{
  "metadata": {
    "title": "Agile Testing",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 577,
    "conversion_date": "2025-12-25T18:10:43.693430",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Agile Testing.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 2-9)",
      "start_page": 2,
      "end_page": 9,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank\n\nAGILE TESTING\n\nThis page intentionally left blank\n\nAGILE TESTING\n\nA PRACTICAL GUIDE FOR TESTERS AND AGILE TEAMS\n\nLisa Crispin Janet Gregory\n\nUpper Saddle River, NJ • Boston • Indianapolis • San Francisco New York • Toronto • Montreal • London • Munich • Paris • Madrid Capetown • Sydney • Tokyo • Singapore • Mexico City\n\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed with initial capital letters or in all capitals.\n\nThe authors and publisher have taken care in the preparation of this book, but make no expressed or implied warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or consequential damages in connection with or arising out of the use of the information or programs contained herein.\n\nThe publisher offers excellent discounts on this book when ordered in quantity for bulk purchases or special sales, which may include electronic versions and/or custom covers and content particular to your business, training goals, marketing focus, and branding interests. For more information, please contact:\n\nU.S. Corporate and Government Sales (800) 382-3419 corpsales@pearsontechgroup.com\n\nFor sales outside the United States, please contact:\n\nInternational Sales international@pearson.com\n\nVisit us on the Web: informit.com/aw\n\nLibrary of Congress Cataloging-in-Publication Data:\n\nCrispin, Lisa.\n\nAgile testing : a practical guide for testers and agile teams /\n\nLisa Crispin, Janet Gregory. — 1st ed.\n\np.\n\ncm.\n\nIncludes bibliographical references and index. ISBN-13: 978-0-321-53446-0 (pbk. : alk. paper) ISBN-10: 0-321-53446-8 (pbk. : alk. paper)\n\n1. Computer software— II. Title.\n\nTesting.\n\n2. Agile software development.\n\nI. Gregory, Janet.\n\nQA76.76.T48C75 2009 005.1—dc22\n\n2008042444\n\nCopyright © 2009 Pearson Education, Inc.\n\nAll rights reserved. Printed in the United States of America. This publication is protected by copyright, and permission must be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. For information regarding permissions, write to:\n\nPearson Education, Inc. Rights and Contracts Department 501 Boylston Street, Suite 900 Boston, MA 02116 Fax (617) 671-3447\n\nISBN-13: 978-0-321-53446-0 ISBN-10: 0-321-53446-8 Text printed in the United States on recycled paper at R.R. Donnelley in Crawfordsville, Indiana. First printing, December 2008\n\nTo my husband, Bob Downing—you’re the bee’s knees!\n\n—Lisa\n\nTo Jack, Dana, and Susan, and to all the writers in my family.\n\n—Janet\n\nAnd to all our favorite donkeys and dragons.\n\n—Lisa and Janet\n\nThis page intentionally left blank\n\nCONTENTS\n\nForeword by Mike Cohn\n\nForeword by Brian Marick\n\nPreface\n\nAcknowledgments\n\nAbout the Authors\n\nPart I\n\nIntroduction\n\nChapter 1 What Is Agile Testing, Anyway?\n\nAgile Values What Do We Mean by “Agile Testing”? A Little Context for Roles and Activities on an Agile Team\n\nCustomer Team Developer Team Interaction between Customer and Developer Teams\n\nHow Is Agile Testing Different?\n\nWorking on Traditional Teams Working on Agile Teams Traditional vs. Agile Testing\n\nWhole-Team Approach Summary\n\nChapter 2\n\nTen Principles for Agile Testers\n\nWhat’s an Agile Tester? The Agile Testing Mind-Set\n\nxxiii\n\nxxv\n\nxxvii\n\nxxxvii\n\nxli\n\n1\n\n3 3 4 7 7 7 8 9 9 10 12 15 17\n\n19 19 20\n\nix",
      "page_number": 2
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 10-17)",
      "start_page": 10,
      "end_page": 17,
      "detection_method": "topic_boundary",
      "content": "x\n\nCONTENTS\n\nApplying Agile Principles and Values Provide Continuous Feedback Deliver Value to the Customer Enable Face-to-Face Communication Have Courage Keep It Simple Practice Continuous Improvement Respond to Change Self-Organize Focus on People Enjoy Adding Value Summary\n\nPart II\n\nOrganizational Challenges\n\nChapter 3\n\nCultural Challenges\n\nOrganizational Culture Quality Philosophy Sustainable Pace Customer Relationships Organization Size Empower Your Team\n\nBarriers to Successful Agile Adoption by Test/QA Teams\n\nLoss of Identity Additional Roles Lack of Training Not Understanding Agile Concepts Past Experience/Attitude Cultural Differences among Roles\n\nIntroducing Change Talk about Fears Give Team Ownership Celebrate Success\n\nManagement Expectations\n\nCultural Changes for Managers Speaking the Manager’s Language\n\nChange Doesn’t Come Easy\n\nBe Patient Let Them Feel Pain\n\n21 22 22 23 25 26 27 28 29 30 31 31 33\n\n35\n\n37 37 38 40 41 42 44 44 44 45 45 45 48 48 49 49 50 50 52 52 55 56 56 56\n\nBuild Your Credibility Work On Your Own Professional Development Beware the Quality Police Mentality Vote with Your Feet\n\nSummary\n\nChapter 4\n\nTeam Logistics\n\nTeam Structure\n\nIndependent QA Teams Integration of Testers into an Agile Project Agile Project Teams\n\nPhysical Logistics Resources\n\nTester-Developer Ratio Hiring an Agile Tester\n\nBuilding a Team\n\nSelf-Organizing Team Involving Other Teams Every Team Member Has Equal Value Performance and Rewards What Can You Do?\n\nSummary\n\nChapter 5\n\nTransitioning Typical Processes\n\nSeeking Lightweight Processes Metrics\n\nLean Measurements Why We Need Metrics What Not to Do with Metrics Communicating Metrics Metrics ROI Defect Tracking\n\nWhy Should We Use a Defect Tracking System (DTS)? Why Shouldn’t We Use a DTS? Defect Tracking Tools Keep Your Focus\n\nTest Planning\n\nTest Strategy vs. Test Planning Traceability\n\nCONTENTS\n\nxi\n\n57 57 57 57 58\n\n59 59 60 61 64 65 66 66 67 69 69 69 70 70 71 71\n\n73 73 74 74 75 77 77 78 79 80 82 83 85 86 86 88\n\nxii\n\nCONTENTS\n\nExisting Processes and Models\n\nAudits Frameworks, Models, and Standards\n\nSummary\n\nPart III\n\nThe Agile Testing Quadrants\n\nChapter 6\n\nThe Purpose of Testing\n\nThe Agile Testing Quadrants\n\nTests that Support the Team Tests that Critique the Product\n\nKnowing When a Story Is Done\n\nShared Responsibility Managing Technical Debt Testing in Context Summary\n\nChapter 7\n\nTechnology-Facing Tests that Support the Team\n\nAn Agile Testing Foundation\n\nThe Purpose of Quadrant 1 Tests Supporting Infrastructure\n\nWhy Write and Execute These Tests? Lets Us Go Faster and Do More Making Testers’ Jobs Easier Designing with Testing in Mind Timely Feedback\n\nWhere Do Technology-Facing Tests Stop? What If the Team Doesn’t Do These Tests?\n\nWhat Can Testers Do? What Can Managers Do? It’s a Team Problem\n\nToolkit\n\nSource Code Control IDEs Build Tools Build Automation Tools Unit Test Tools\n\nSummary\n\n88 89 90 93\n\n95\n\n97 97 98 101 104 105 106 106 108\n\n109 109 110 111 112 112 114 115 118 119 121 121 122 123 123 123 124 126 126 126 127\n\nCONTENTS\n\nChapter 8\n\nBusiness-Facing Tests that Support the Team\n\nDriving Development with Business-Facing Tests The Requirements Quandary\n\nCommon Language Eliciting Requirements Advance Clarity Conditions of Satisfaction Ripple Effects\n\nThin Slices, Small Chunks How Do We Know We’re Done? Tests Mitigate Risk Testability and Automation Summary\n\nChapter 9\n\nToolkit for Business-Facing Tests that Support the Team\n\nBusiness-Facing Test Tool Strategy Tools to Elicit Examples and Requirements\n\nChecklists Mind Maps Spreadsheets Mock-Ups Flow Diagrams Software-Based Tools\n\nTools for Automating Tests Based on Examples Tools to Test below the GUI and API Level Tools for Testing through the GUI\n\nStrategies for Writing Tests\n\nBuild Tests Incrementally Keep the Tests Passing Use Appropriate Test Design Patterns Keyword and Data-Driven Tests\n\nTestability\n\nCode Design and Test Design Automated vs. Manual Quadrant 2 Tests\n\nTest Management Summary\n\nChapter 10 Business-Facing Tests that Critique the Product\n\nIntroduction to Quadrant 3 Demonstrations\n\nxiii\n\n129 129 132 134 135 140 142 143 144 146 147 149 150\n\n153 153 155 156 156 159 160 160 163 164 165 170 177 178 179 179 182 183 184 185 186 186\n\n189 190 191\n\nxiv\n\nCONTENTS\n\nScenario Testing Exploratory Testing\n\nSession-Based Testing Automation and Exploratory Testing An Exploratory Tester\n\nUsability Testing\n\nUser Needs and Persona Testing Navigation Check Out the Competition\n\nBehind the GUI API Testing Web Services\n\nTesting Documents and Documentation\n\nUser Documentation Reports\n\nTools to Assist with Exploratory Testing\n\nTest Setup Test Data Generation Monitoring Tools Simulators Emulators\n\nSummary\n\nChapter 11 Critiquing the Product Using Technology-\n\nFacing Tests\n\nIntroduction to Quadrant 4 Who Does It? When Do You Do It? “ility” Testing Security Maintainability Interoperability Compatibility Reliability Installability “ility” Summary\n\nPerformance, Load, Stress, and Scalability Testing\n\nScalability Performance and Load Testing Performance and Load-Testing Tools Baseline\n\n192 195 200 201 201 202 202 204 204 204 205 207 207 207 208 210 211 212 212 213 213 214\n\n217 217 220 222 223 223 227 228 229 230 231 232 233 233 234 234 235\n\nTest Environments Memory Management\n\nSummary\n\nChapter 12 Summary of Testing Quadrants\n\nReview of the Testing Quadrants A System Test Example The Application The Team and the Process\n\nTests Driving Development\n\nUnit Tests Acceptance Tests\n\nAutomation\n\nThe Automated Functional Test Structure Web Services Embedded Testing\n\nCritiquing the Product with Business-Facing Tests\n\nExploratory Testing Testing Data Feeds The End-to-End Tests User Acceptance Testing Reliability Documentation\n\nDocumenting the Test Code Reporting the Test Results Using the Agile Testing Quadrants Summary\n\nPart IV\n\nAutomation\n\nChapter 13 Why We Want to Automate Tests and What\n\nHolds Us Back\n\nWhy Automate?\n\nManual Testing Takes Too Long Manual Processes Are Error Prone Automation Frees People to Do Their Best Work Automated Regression Tests Provide a Safety Net Automated Tests Give Feedback, Early and Often Tests and Examples that Drive Coding Can Do More\n\nCONTENTS\n\nxv\n\n237 237 238\n\n241 241 242 242 243 244 244 245 245 245 247 248 248 248 249 249 250 250 251 251 251 252 253\n\n255\n\n257 258 258 259 259 261 262 262\n\nxvi\n\nCONTENTS\n\nTests Are Great Documentation ROI and Payback\n\nBarriers to Automation—Things that Get in the Way\n\nBret’s List Our List Programmers’ Attitude—“Why Automate?” The “Hump of Pain” (The Learning Curve) Initial Investment Code that’s Always in Flux Legacy Code Fear Old Habits\n\nCan We Overcome These Barriers? Summary\n\nChapter 14 An Agile Test Automation Strategy\n\nAn Agile Approach to Test Automation\n\nAutomation Test Categories Test Automation Pyramid\n\nWhat Can We Automate?\n\nContinuous Integration, Builds, and Deploys Unit and Component Tests API or Web Services Testing Testing behind the GUI Testing the GUI Load Tests Comparisons Repetitive Tasks Data Creation or Setup What Shouldn’t We Automate?\n\nUsability Testing Exploratory Testing Tests that Will Never Fail One-Off Tests\n\nWhat Might Be Hard to Automate? Developing an Automation Strategy—Where Do We Start?\n\nWhere Does It Hurt the Most? Multi-Layered Approach Think about Test Design and Maintenance Choosing the Right Tools\n\n263 264 264 264 265 265 266 267 269 269 269 270 270 271\n\n273 274 274 276 279 280 282 282 282 282 283 283 284 284 285 285 286 286 286 287 288 289 290 292 294\n\nCONTENTS\n\nApplying Agile Principles to Test Automation\n\nKeep It Simple Iterative Feedback Whole-Team Approach Taking the Time to Do It Right Learn by Doing Apply Agile Coding Practices to Tests\n\nSupplying Data for Tests Data Generation Tools Avoid Database Access When Database Access Is Unavoidable or Even Desirable Understand Your Needs Evaluating Automation Tools\n\nIdentifying Requirements for Your Automation Tool One Tool at a Time Choosing Tools Agile-Friendly Tools Implementing Automation Managing Automated Tests\n\nOrganizing Tests Organizing Test Results\n\nGo Get Started Summary\n\nPart V\n\nAn Iteration in the Life of a Tester\n\nChapter 15 Tester Activities in Release or Theme Planning\n\nThe Purpose of Release Planning Sizing\n\nHow to Size Stories The Tester’s Role in Sizing Stories An Example of Sizing Stories\n\nPrioritizing\n\nWhy We Prioritize Stories Testing Considerations While Prioritizing\n\nWhat’s in Scope?\n\nDeadlines and Timelines Focus on Value\n\nxvii\n\n298 298 299 300 301 303 303 304 304 306 307 310 311 311 312 313 316 316 319 319 322 324 324\n\n327\n\n329 330 332 332 333 334 338 338 339 340 340 341",
      "page_number": 10
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 18-25)",
      "start_page": 18,
      "end_page": 25,
      "detection_method": "topic_boundary",
      "content": "xviii\n\nCONTENTS\n\nSystem-Wide Impact Third-Party Involvement\n\nTest Planning\n\nWhere to Start Why Write a Test Plan? Types of Testing Infrastructure Test Environments Test Data Test Results\n\nTest Plan Alternatives\n\nLightweight Test Plans Using a Test Matrix Test Spreadsheet A Whiteboard Automated Test List Preparing for Visibility\n\nTracking Test Tasks and Status Communicating Test Results Release Metrics\n\nSummary\n\nChapter 16 Hit the Ground Running\n\nBe Proactive Beneﬁts Do You Really Need This? Potential Downsides to Advance Preparation\n\nAdvance Clarity\n\nCustomers Speak with One Voice Story Size Geographically Dispersed Teams\n\nExamples Test Strategies Prioritize Defects Resources Summary\n\nChapter 17 Iteration Kickoff\n\nIteration Planning\n\nLearning the Details Considering All Viewpoints\n\n342 342 345 345 345 346 346 347 348 349 350 350 350 353 353 354 354 354 357 358 366\n\n369 369 370 372 373 373 373 375 376 378 380 381 381 382\n\n383 383 384 385\n\nWriting Task Cards Deciding on Workload\n\nTestable Stories Collaborate with Customers High-Level Tests and Examples Reviewing with Customers Reviewing with Programmers Test Cases as Documentation\n\nSummary\n\nChapter 18 Coding and Testing\n\nDriving Development\n\nStart Simple Add Complexity Assess Risk Coding and Testing Progress Together Identify Variations Power of Three Focus on One Story\n\nTests that Critique the Product Collaborate with Programmers\n\nPair Testing “Show Me” Talk to Customers\n\nShow Customers Understand the Business\n\nCompleting Testing Tasks Dealing with Bugs\n\nIs It a Defect or Is It a Feature? Technical Debt Zero Bug Tolerance It’s All about Choices\n\nDecide Which Bugs to Log Choose When to Fix Your Bugs Choose the Media You Should Use to Log a Bug Alternatives and Suggestions for Dealing with Bugs Start Simple\n\nFacilitate Communication\n\nTesters Facilitate Communication Distributed Teams\n\nCONTENTS\n\nxix\n\n389 393 393 396 397 400 400 402 403\n\n405 406 406 407 407 409 410 411 411 412 413 413 413 414 414 415 415 416 417 418 418 419 420 421 423 424 428 429 429 431\n\nxx\n\nCONTENTS\n\nRegression Tests\n\nKeep the Build “Green” Keep the Build Quick Building a Regression Suite Checking the “Big Picture”\n\nResources Iteration Metrics\n\nMeasuring Progress Defect Metrics\n\nSummary\n\nChapter 19 Wrap Up the Iteration\n\nIteration Demo Retrospectives\n\nStart, Stop, and Continue Ideas for Improvements\n\nCelebrate Successes Summary\n\nChapter 20 Successful Delivery\n\nWhat Makes a Product? Planning Enough Time for Testing The End Game\n\nTesting the Release Candidate Test on a Staging Environment Final Nonfunctional Testing Integration with External Applications Data Conversion and Database Updates Installation Testing Communication What If It’s Not Ready?\n\nCustomer Testing\n\nUAT Alpha/Beta Testing\n\nPost-Development Testing Cycles Deliverables Releasing the Product\n\nRelease Acceptance Criteria Release Management Packaging\n\n432 433 433 434 434 434 435 435 437 440\n\n443 443 444 445 447 449 451\n\n453 453 455 456 458 458 458 459 459 461 462 463 464 464 466 467 468 470 470 474 474\n\nCustomer Expectations Production Support Understand Impact to Business\n\nSummary\n\nPart VI\n\nSummary\n\nChapter 21 Key Success Factors\n\nSuccess Factor 1: Use the Whole-Team Approach Success Factor 2: Adopt an Agile Testing Mind-Set Success Factor 3: Automate Regression Testing Success Factor 4: Provide and Obtain Feedback Success Factor 5: Build a Foundation of Core Practices\n\nContinuous Integration Test Environments Manage Technical Debt Working Incrementally Coding and Testing Are Part of One Process Synergy between Practices\n\nSuccess Factor 6: Collaborate with Customers Success Factor 7: Look at the Big Picture Summary\n\nGlossary\n\nBibliography\n\nIndex\n\nCONTENTS\n\nxxi\n\n475 475 475 476\n\n479\n\n481 482 482 484 484 486 486 487 487 488 488 489 489 490 491\n\n493\n\n501\n\n509\n\nThis page intentionally left blank\n\nFOREWORD\n\nBy Mike Cohn\n\n“Quality is baked in,” the programmers kept telling me. As part of a proposed acquisition, my boss had asked me to perform some ﬁnal due diligence on the development team and its product. We’d already established that the company’s recently launched product was doing well in the market, but I was to make sure we were not about to buy more trouble than beneﬁt. So I spent my time with the development team. I was looking for problems that might arise from having rushed the product into release. I wondered, “Was the code clean? Were there modules that could only be worked on by one developer? Were there hundreds or thousands of defects waiting to be discovered?” And when I asked about the team’s approach to testing, “Quality is baked in” was the answer I got.\n\nBecause this rather unusual colloquialism could have meant just about any- thing, I pressed further. What I found was that this was the company founder’s shorthand for expressing one of quality pioneer W. Edwards Dem- ing’s famous fourteen points: Build quality into the product rather than try- ing to test it in later.\n\nThe idea of building quality into their products is at the heart of how agile teams work. Agile teams work in short iterations in part to ensure that the application remains at a known state of quality. Agile teams are highly cross- functional, with programmers, testers, and others working side by side throughout each iteration so that quality can be baked into products through techniques such as acceptance-test driven development, a heavy emphasis on automated testing, and whole-team thinking. Good agile teams bake quality in by building their products continuously, integrating new work within minutes of its being completed. Agile teams utilize techniques such as refac- toring and a preference for simplicity in order to prevent technical debt from accumulating.\n\nxxiii\n\nxxiv\n\nFOREWORD\n\nLearning how to do these things is difﬁcult, and especially so for testers, whose role has been given scant attention in previous books. Fortunately, the book you now hold in your hands answers questions on the mind of every tester who’s beginning to work on an agile project, such as:\n\n(cid:2) What are my roles and responsibilities? (cid:2) How do I work more closely with programmers? (cid:2) How much do we automate, and how do we start automating?\n\nThe experience of Lisa and Janet shines through on every page of the book. However, this book is not just their story. Within this book, they incorporate dozens of stories from real-world agile testers. These stories form the heart of the book and are what makes it so unique. It’s one thing to shout from the ivory tower, “Here’s how to do agile testing.” It’s another to tell the stories of the teams that have struggled and then emerged agile and victorious over challenges such as usability testing, legacy code that resists automation, tran- sitioning testers used to traditional phase-gate development, testing that “keeps up” with short iterations, and knowing when a feature is “done.”\n\nLisa and Janet were there at the beginning, learning how to do agile testing back when the prevailing wisdom was that agile teams didn’t need testers and that programmers could bake quality in by themselves. Over the years and through articles, conference presentations, and working with their clients and teams, Lisa and Janet have helped us see the rich role to be ﬁlled by testers on agile projects. In this book, Lisa and Janet use a test automation pyramid, the agile testing quadrants of Brian Marick (himself another world- class agile tester), and other techniques to show how much was missing from a mind-set that said testing is necessary but testers aren’t.\n\nIf you want to learn how to bake quality into your products or are an aspiring agile tester seeking to understand your role, I can think of no better guides than Lisa and Janet.\n\nFOREWORD\n\nBy Brian Marick\n\nImagine yourself skimming over a landscape thousands of years ago, looking at the people below. They’re barely scraping out a living in a hostile territory, doing some hunting, some ﬁshing, and a little planting. Off in the distance, you see the glitter of a glacier. Moving closer, you see that it’s melting fast and that it’s barely damming a huge lake. As you watch, the lake breaks through, sweeping down a riverbed, carving it deeper, splashing up against cliffs on the far side of the landscape—some of which collapse.\n\nAs you watch, the dazed inhabitants begin to explore the opening. On the other side, there’s a lush landscape, teaming with bigger animals than they’ve ever seen before, some grazing on grass with huge seed heads, some squab- bling over mounds of fallen fruit.\n\nPeople move in. Almost immediately, they begin to live better. But as the years ﬂy past, you see them adapt. They begin to use nets to ﬁsh in the fast- running streams. They learn the teamwork needed to bring down the larger animals, though not without a few deaths along the way. They ﬁnd ever- better ways to cultivate this new grass they’ve come to call “wheat.”\n\nAs you watch, the mad burst of innovation gives way to a stable solution, a good way to live in this new land, a way that’s taught to each new generation. Although just over there, you spy someone inventing the wheel . . .\n\n(cid:2) (cid:2) (cid:2)\n\nIn the early years of this century, the adoption of Agile methods sometimes seemed like a vast dam breaking, opening up a way to a better—more pro- ductive, more joyful—way of developing software. Many early adopters saw beneﬁts right away, even though they barely knew what they were doing.\n\nxxv",
      "page_number": 18
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 26-35)",
      "start_page": 26,
      "end_page": 35,
      "detection_method": "topic_boundary",
      "content": "xxvi\n\nFOREWORD\n\nSome had an easier time of it than others. Programmers were like the hunters in the fable above. Yes, they had to learn new skills in order to hunt bison, but they knew how to hunt rabbits, more or less, and there were plenty of rabbits around. Testers were more like spear-ﬁshers in a land where spear-ﬁshing wouldn’t work. Going from spear-ﬁshing to net-ﬁshing is a much bigger con- ceptual jump than going from rabbit to bison. And, while some of the skills— cleaning ﬁsh, for example—were the same in the new land, the testers had to invent new skills of net-weaving before they could truly pull their weight.\n\nSo testing lagged behind. Fortunately, we had early adopters like Lisa and Janet, people who dove right in alongside the programmers, testers who were not jealous of their role or their independence, downright pleasant people who could ﬁgure out the biggest change of all in Agile testing: the tester’s new social role.\n\nAs a result, we have this book. It’s the stable solution, the good way for testers to live in this new Agile land of ours. It’s not the ﬁnal word—we could use the wheel, and I myself am eager for someone to invent antibiotics—but what’s taught here will serve you well until someone, perhaps Lisa and Janet, brings the next big change.\n\nPREFACE\n\nWe were early adopters of Extreme Programming (XP), testing on XP teams that weren’t at all sure where testers or their brand of testing ﬁt in. At the time, there wasn’t much in the agile (which wasn’t called agile yet) literature about acceptance testing, or how professional testers might contribute. We learned not only from our own experiences but from others in the small agile com- munity. In 2002, Lisa co-wrote Testing Extreme Programming with Tip House, with lots of help from Janet. Since then, agile development has evolved, and the agile testing community has ﬂourished. With so many people contribut- ing ideas, we’ve learned a whole lot more about agile testing.\n\nIndividually and together, we’ve helped teams transition to agile, helped testers learn how to contribute on agile teams, and worked with others in the agile community to explore ways that agile teams can be more successful at testing. Our experiences differ. Lisa has spent most of her time as an agile tester on stable teams working for years at a time on web applications in the retail, telephony, and ﬁnancial industries. Janet has worked with soft- ware organizations developing enterprise systems in a variety of industries. These agile projects have included developing a message-handling system, an environmental-tracking system, a remote data management system (in- cluding an embedded application, with a communication network as well as the application), an oil and gas production accounting application, and ap- plications in the airline transportation industry. She has played different roles—sometimes tester, sometimes coach—but has always worked to better integrate the testers with the rest of the team. She has been with teams from as little as six months to as long as one-and-a-half years.\n\nWith these different points of view, we have learned to work together and complement each other’s skill sets, and we have given many presentations and tutorials together.\n\nxxvii\n\nxxviii\n\nPREFACE\n\nWHY WE WROTE THIS BOOK Several excellent books oriented toward agile development on testing and test patterns have been published (see our bibliography). These books are generally focused on helping the developer. We decided to write a book aimed at helping agile teams be more successful at delivering business value using tests that the business can understand. We want to help testers and quality assurance (QA) professionals who have worked in more traditional development methodologies make the transition to agile development.\n\nWe’ve ﬁgured out how to apply—on a practical, day-to-day level—the fruits of our own experience working with teams of all sizes and a variety of ideas from other agile practitioners. We’ve put all this together in this book to help testers, quality assurance managers, developers, development manag- ers, product owners, and anyone else with a stake in effective testing on agile projects to deliver the software their customers need. However, we’ve fo- cused on the role of the tester, a role that may be adopted by a variety of professionals.\n\nAgile testing practices aren’t limited to members of agile teams. They can be used to improve testing on projects using traditional development method- ologies as well. This book is also intended to help testers working on projects using any type of development methodology.\n\nAgile development isn’t the only way to successfully deliver software. How- ever, all of the successful teams we’ve been on, agile or waterfall, have had several critical commonalities. The programmers write and automate unit and integration tests that provide good code coverage. They are disciplined in the use of source code control and code integration. Skilled testers are in- volved from the start of the development cycle and are given time and re- sources to do an adequate job of all necessary forms of testing. An automated regression suite that covers the system functionality at a higher level is run and checked regularly. The development team understands the customers’ jobs and their needs, and works closely together with the business experts.\n\nPeople, not methodologies or tools, make projects successful. We enjoy agile development because its values, principles, and core practices enable people to do their best work, and testing and quality are central to agile develop- ment. In this book, we explain how to apply agile values and principles to your unique testing situation and enable your teams to succeed. We have more about that in Chapter 1, “What Is Agile Testing, Anyway?” and in Chapter 2, “Ten Principles for Agile Testers.”\n\nPREFACE\n\nHOW WE WROTE THIS BOOK Having experienced the beneﬁts of agile development, we used agile practices to produce this book. As we began work on the book, we talked to agile testers and teams from around the globe to ﬁnd out what problems they en- countered and how they addressed them. We planned how we would cover these areas in the book.\n\nWe made a release plan based on two-week iterations. Every two weeks, we delivered two rough-draft chapters to our book website. Because we aren’t co-located, we found tools to use to communicate, provide “source code con- trol” for our chapters, deliver the product to our customers, and get their feedback. We couldn’t “pair” much real-time, but we traded chapters back and forth for review and revision, and had informal “stand-ups” daily via in- stant message.\n\nOur “customers” were the generous people in the agile community who volun- teered to review draft chapters. They provided feedback by email or (if we were lucky) in person. We used the feedback to guide us as we continued writing and revising. After all the rough drafts were done, we made a new plan to com- plete the revisions, incorporating all the helpful ideas from our “customers.”\n\nOur most important tool was mind maps. We started out by creating a mind map of how we envisioned the whole book. We then created mind maps for each section of the book. Before writing each chapter, we brainstormed with a mind map. As we revised, we revisited the mind maps, which helped us think of ideas we may have missed.\n\nBecause we think the mind maps added so much value, we’ve included the mind map as part of the opening of each chapter. We hope they’ll help you get an overview of all the information included in the chapter, and inspire you to try using mind maps yourself.\n\nOUR AUDIENCE This book will help you if you’ve ever asked any of the following excellent questions, which we’ve heard many times:\n\n(cid:2) If developers are writing tests, what do the testers do? (cid:2) I’m a QA manager, and our company is implementing agile develop- ment (Scrum, XP, DSDM, name your ﬂavor). What’s my role now?\n\nxxix\n\nxxx\n\nPREFACE\n\n(cid:2) I’ve worked as a tester on a traditional waterfall team, and I’m really excited by what I’ve read about agile. What do I need to know to work on an agile team?\n\n(cid:2) What’s an “agile tester”? (cid:2) I’m a developer on an agile team. We’re writing code test-ﬁrst, but our customers still aren’t happy with what we deliver. What are we missing?\n\n(cid:2) I’m a developer on an agile team. We’re writing our code test-ﬁrst. We make sure we have tests for all our code. Why do we need testers? (cid:2) I coach an agile development team. Our QA team can’t keep up with us, and testing always lags behind. Should we just plan to test an iteration behind development?\n\n(cid:2) I’m a software development manager. We recently transitioned to\n\nagile, but all our testers quit. Why?\n\n(cid:2) I’m a tester on a team that’s going agile. I don’t have any program- ming or automation skills. Is there any place for me on an agile team?\n\n(cid:2) How can testing possibly keep up with two-week iterations? (cid:2) What about load testing, performance testing, usability testing, all\n\nthe other “ilities”? Where do these ﬁt in?\n\n(cid:2) We have audit requirements. How does agile development and testing\n\naddress these?\n\nIf you have similar questions and you’re looking for practical advice about how testers contribute to agile teams and how agile teams can do an effective job of testing, you’ve picked up the right book.\n\nThere are many “ﬂavors” of agile development, but they all have much in common. We support the Agile Manifesto, which we explain in Chapter 1, “What Is Agile Testing, Anyway?” Whether you’re practicing Scrum, Extreme Programming, Crystal, DSDM, or your own variation of agile development, you’ll ﬁnd information here to help with your testing efforts.\n\nA User Story for an Agile Testing Book\n\nWhen Robin Dymond, a managing consultant and trainer who has helped many teams adopt lean and agile, heard we were writing this book, he sent us the user story he’d like to have fulﬁlled. It encapsulates many of the re- quirements we planned to deliver.\n\nPREFACE\n\nBook Story 1\n\nAs a QA professional, I can understand the main\n\ndifference between traditional QA professionals and agile\n\nteam members with a QA background, so that I can begin\n\ninternalizing my new responsibilities and deliver value to\n\nthe customer sooner and with less difﬁculty.\n\nAcceptance conditions:\n\nMy concerns and fears about losing control of testing are addressed.\n\nMy concerns and fears about having to write code (never done it) are addressed.\n\nAs a tester I understand my new value to the team.\n\nAs a tester new to Agile, I can easily read about things that are most important to my new role.\n\nAs a tester new to Agile, I can easily ignore things that are less im- portant to my new role.\n\nAs a tester new to Agile, I can easily get further detail about agile testing that is important to MY context.\n\nWere I to suggest a solution to this problem, I think of Scrum versus XP. With Scrum you get a simple view that enables people to quickly adopt Agile. However, Scrum is the tip of the iceberg for successful agile teams. For testers who are new, I would love to see agile testing ideas ex- pressed in layers of detail. What do I need to know today, what should I know tomorrow, and what context-sensitive things should I consider for continuous improvement?\n\nWe’ve tried to provide these layers of detail in this book. We’ll approach agile testing from a few different perspectives: transitioning into agile develop- ment, using an agile testing matrix to guide testing efforts, and explaining all the different testing activities that take place throughout the agile develop- ment cycle.\n\nxxxi\n\nxxxii\n\nPREFACE\n\nHOW TO USE THIS BOOK If you aren’t sure where to start in this book, or you just want a quick over- view, we suggest you read the last chapter, Chapter 22, “Key Success Factors,” and follow wherever it leads you.\n\nPart I: Introduction\n\nIf you want quick answers to questions such as “Is agile testing different than testing on waterfall projects?” or “What’s the difference between a tester on a traditional team and an agile tester?,” start with Part I, which includes the following chapters:\n\n(cid:2) Chapter 1: What Is Agile Testing, Anyway? (cid:2) Chapter 2: Ten Principles for Agile Testers\n\nThese chapters are the “tip of the iceberg” that Robin requested in his user story. They include an overview of how agile differs from a traditional phased approach and explore the “whole team” approach to quality and testing.\n\nIn this part of the book we deﬁne the “agile testing mind-set” and what makes testers successful on agile teams. We explain how testers apply agile values and principles to contribute their particular expertise.\n\nPart II: Organizational Challenges\n\nIf you’re a tester or manager on a traditional QA team, or you’re coaching a team that’s moving to agile, Part II will help you with the organizational chal- lenges faced by teams in transition. The “whole team” attitude represents a lot of cultural changes to team members, but it helps overcome the fear testers have when they wonder how much control they’ll have or whether they’ll be expected to write code.\n\nSome of the questions answered in Part II are:\n\n(cid:2) How can we engage the QA team? (cid:2) What about management’s expectations? (cid:2) How should we structure our agile team, and where do the testers ﬁt? (cid:2) What do we look for when hiring an agile tester? (cid:2) How do we cope with a team distributed across the globe?\n\nPREFACE\n\nPart II also introduces some topics we don’t always enjoy talking about. We explore ideas about how to transition processes and models, such as audits or SOX compliance, that are common in traditional environments.\n\nMetrics and how they’re applied can be a controversial issue, but there are positive ways to use them to beneﬁt the team. Defect tracking easily becomes a point of contention for teams, with questions such as “Do we use a defect- tracking system?” or “When do we log bugs?”\n\nTwo common questions about agile testing from people with traditional test team experience are “What about test plans?” and “Is it true there’s no docu- mentation on agile projects?” Part II clears up these mysteries.\n\nThe chapters in Part II are as follows:\n\n(cid:2) Chapter 3: Cultural Challenges (cid:2) Chapter 4: Team Logistics (cid:2) Chapter 5: Transitioning Typical Processes\n\nPart III: The Agile Testing Quadrants\n\nDo you want more details on what types of testing are done on agile projects? Are you wondering who does what testing? How do you know whether you’ve done all the testing that’s needed? How do you decide what practices, techniques, and tools ﬁt your particular situation? If these are your concerns, check out Part III.\n\nWe use Brian Marick’s Agile Testing Quadrants to explain the purpose of testing. The quadrants help you deﬁne all the different areas your testing should address, from unit level tests to reliability and other “ilities,” and ev- erything in between. This is where we get down into the nitty-gritty of how to deliver a high-quality product. We explain techniques that can help you to communicate well with your customers and better understand their require- ments. This part of the book shows how tests drive development at multiple levels. It also provides tools for your toolkit that can help you to effectively deﬁne, design, and execute tests that support the team and critique the prod- uct. The chapters include the following:\n\n(cid:2) Chapter 6: The Purpose of Testing (cid:2) Chapter 7: Technology-Facing Tests that Support the Team\n\nxxxiii\n\nxxxiv\n\nPREFACE\n\n(cid:2) Chapter 8: Business-Facing Tests that Support the Team (cid:2) Chapter 9: Toolkit for Business-Facing Tests that Support the Team (cid:2) Chapter 10: Business-Facing Tests that Critique the Product (cid:2) Chapter 11: Critiquing the Product Using Technology-Facing Tests (cid:2) Chapter 12: Summary of Testing Quadrants\n\nPart IV: Automation\n\nTest automation is a central focus of successful agile teams, and it’s a scary topic for lots of people (we know, because it’s had us running scared before!). How do you squeeze test automation into short iterations and still get all the stories completed?\n\nPart IV gets into the details of when and why to automate, how to overcome barriers to test automation, and how to develop and implement a test auto- mation strategy that works for your team. Because test automation tools change and evolve so rapidly, our aim is not to explain how to use speciﬁc tools, but to help you select and use the right tools for your situation. Our agile test automation tips will help you with difﬁcult challenges such as test- ing legacy code.\n\nThe chapters are as follows:\n\n(cid:2) Chapter 13: Why We Want to Automate Tests and What Holds Us Back (cid:2) Chapter 14: An Agile Test Automation Strategy\n\nPart V: An Iteration in the Life of a Tester\n\nIf you just want to get a feel for what testers do throughout the agile develop- ment cycle, or you need help putting together all the information in this book, go to Part V. Here we chronicle an iteration, and more, in the life of an agile tester. Testers contribute enormous value throughout the agile software devel- opment cycles. In Part V, we explain the activities that testers do on a daily ba- sis. We start with planning releases and iterations to get each iteration off to a good start, and move through the iteration—collaborating with the customer and development teams, testing, and writing code. We end the iteration by de- livering new features and ﬁnding ways for the team to improve the process.\n\nThe chapters break down this way:\n\n(cid:2) Chapter 15: Tester Activities in Release or Theme Planning (cid:2) Chapter 16: Hit the Ground Running\n\nPREFACE\n\n(cid:2) Chapter 17: Iteration Kickoff (cid:2) Chapter 18: Coding and Testing (cid:2) Chapter 19: Wrap Up the Iteration (cid:2) Chapter 20: Successful Delivery\n\nPart VI: Summary\n\nIn Chapter 21, “Key Success Factors,” we present seven key factors agile teams can use for successful testing. If you’re having trouble deciding where to start with agile testing, or how to work on improving what you’re doing now, these success factors will give you some direction.\n\nOther Elements\n\nWe’ve also included a glossary we hope you will ﬁnd useful, as well as refer- ences to books, articles, websites, and blogs in the bibliography.\n\nJUST START DOING IT—TODAY! Agile development is all about doing your best work. Every team has unique challenges. We’ve tried to present all the information that we think may help agile testers, their teams, managers, and customers. Apply the techniques that you think are appropriate for your situation. Experiment constantly, evaluate the results, and come back to this book to see what might help you improve. Our goal is to help testers and agile teams enjoy delivering the best and most valuable product they can.\n\nWhen we asked Dierk König, founder and project manager of Canoo Web- Test, what he thought was the number one success factor for agile testing, he answered: “Start doing it—today!” You can take a baby step to improve your team’s testing right now. Go get started!\n\nxxxv",
      "page_number": 26
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 36-43)",
      "start_page": 36,
      "end_page": 43,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank\n\nACKNOWLEDGMENTS\n\nSo many people have helped us with this book that it’s hard to know whom to thank ﬁrst. Chris Guzikowski gave us the opportunity to write this book and kept encouraging us along the way. When we were deciding whether to take on such a mammoth task, Mike Cohn gave us the sage advice that the best reason to write a book is that you have something to say. We sure have lots to say about agile testing. Fortunately, so do lots of other people who were will- ing to lend us a hand.\n\nMany thanks to Brian Marick and Mike Cohn for writing such kind fore- words. We’re honored that Mike selected our book for his signature series. We’re grateful for the many ideas and observations of his that are included in this book.\n\nBrian Marick’s “Agile Testing Matrix” has guided both of us in our agile projects for several years, and it provides the core of Part III. Thank you, Brian, for thinking up the quadrants (and so many other contributions to agile testing) and letting us use them here.\n\nWe made constant use of the agile value of feedback. Many thanks to our of- ﬁcial reviewers: Jennitta Andrea, Gerard Meszaros, Ron Jeffries, and Paul Du- vall. Each one had unique and insightful comments that helped us greatly improve the book. Gerard also helped us be more consistent and correct in our testing terminology, and contributed some agile testing success stories.\n\nSpecial thanks to two reviewers and top-notch agile testers who read every word we wrote and spent hours discussing the draft chapters with us in per- son: Pierre Veragen and Paul Rogers. Many of the good ideas in this book are theirs.\n\nxxxvii\n\nxxxviii\n\nACKNOWLEDGMENTS\n\nWe interviewed several teams to learn what advice they would give new agile teams and testers, and solicited success stories and “lessons learned” from colleagues in the agile testing community. Heartfelt thanks to our many con- tributors of sidebars and quotes, as well as providers of helpful feedback, in- cluding (in no particular order) Robin Dymond, Bret Pettichord, Tae Chang, Bob Galen, Erika Boyer, Grig Gheorghiu, Erik Bos, Mark Benander, Jonathan Rasmusson, Andy Pols, Dierk König, Rafael Santos, Jason Holzer, Christophe Louvion, David Reed, John Voris, Chris McMahon, Declan Whelan, Michael Bolton, Elisabeth Hendrickson, Joe Yakich, Andrew Glover, Alessandro Col- lino, Coni Tartaglia, Markus Gärtner, Megan Sumrell, Nathan Silberman, Mike Thomas, Mike Busse, Steve Perkins, Joseph King, Jakub Oleszkiewicz, Pierre Veragen (again), Paul Rogers (again), Jon Hagar, Antony Marcano, Patrick Wilson-Welsh, Patrick Fleisch, Apurva Chandra, Ken De Souza, and Carol Vaage.\n\nMany thanks also to the rest of our community of unofﬁcial reviewers who read chapters, gave feedback and ideas, and let us bounce ideas off of them, including Tom Poppendieck, Jun Bueno, Kevin Lawrence, Hannu Kokko, Titus Brown, Wim van de Goor, Lucas Campos, Kay Johansen, Adrian Howard, Henrik Kniberg, Shelly Park, Robert Small, Senaka Suriyaachchi, and Erik Petersen. And if we’ve neglected to list you here, it’s not that we value your contribution any less, it’s just that we didn’t keep good enough notes! We hope you will see how your time and effort paid off in the ﬁnished book.\n\nWe appreciate the groundwork laid by the agile pioneers who have helped us and our teams succeed with agile. You’ll ﬁnd some of their works in the bibli- ography. We are grateful for the agile teams that have given us so many open source test tools that help all of our teams deliver so much value. Some of those tools are also listed in the bibliography.\n\nThanks to Mike Thomas for taking many of the action photos of an agile team that appear in this book. We hope these photos show those of you new to agile testing and development that there’s no big mystery—it’s just good people getting together to discuss, demo, and draw pictures.\n\nThanks so much to our Addison-Wesley editorial and production team who patiently answered many questions and turned this into the professional- looking book you see here, including Raina Chrobak, Chris Zahn, John Fuller, Sally Gregg, Bonnie Granat, Diane Freed, Jack Lewis, and Kim Arney.\n\nLisa’s Story\n\nACKNOWLEDGMENTS\n\nI’m eternally grateful to Janet for agreeing to write this book with me. She kept us organized and on track so we could juggle book writing with our full-time jobs and personal lives. I’m fortunate to have a writing partner whose experience is complementary to mine. Like any successful agile project, this is a true team effort. This has been hard work, but thanks to Janet it has been a lot of fun, too.\n\nI’d like to thank the members of my current team at ePlan Services Inc. (formerly known as Fast401k), which I joined (thanks to Mike Cohn, our ﬁrst leader) in 2003. All of us learned so much working for Mike that ﬁrst year, and it’s a testa- ment to his leadership that we continue to improve and help the business grow. Thanks to my awesome teammates who have each helped me become a better tester and agile team member, and who were all good sports while Mike Thomas took action photos of us: Nanda Lankapalli, Tony Sweets, Jeff Thuss, Lisa Owens, Mike Thomas, Vince Palumbo, Mike Busse, Nehru Kaja, Trevor Sterritt, Steve Kives, and former but still beloved team members Joe Yakich, Jason Kay, Jennifer Riefen- berg, Matt Tierney, and Charles LeRose. I also have been lucky enough to work with the best customer team anywhere. They are too numerous to mention here, but many thanks to them, and in particular to Steve Perkins, Anne Olguin, and Zachary Shannon, who help us focus on delivering value. Thanks also to Mark and Dan Gutrich, founders and leaders of ePlan Services, for giving us all the opportu- nity to succeed with agile development.\n\nThanks to Kay and Zhon Johansen for teaching me about mind maps at Agile 2006. I hope we have put this skill to good use in creating this book.\n\nMuch gratitude to all my friends and family, whom I neglected terribly during the many months spent writing this book, and who nevertheless supported me con- stantly. There are too many to mention, but I must specially thank Anna Blake for her constant understanding and provision of donkey therapy. Chester and Ernest, the donkeys of my heart, have kept pulling me along. Dodger didn’t make the whole book-writing journey in this world, but his memory continues to lift me up. My little poodle and muse Tango was by my side every minute that I worked on this book at home, joined occasionally by Bruno, Bubba, Olive, Squiggy, Starsky, Bobcat, and Patty. Thanks to my parents for being proud of me and not complain- ing about my neglect of them during this book-writing time.\n\nI know that my husband, Bob Downing, took a deep breath when I exclaimed, “I have the chance to write another book about agile testing,” but he nevertheless encouraged me constantly and made it possible for me to ﬁnd the time to write. He kept the “no-kill shelter” running, kept our lives rolling, kept my spirits up, and sustained me with many fabulous meals. He is the light of my life.\n\nxxxix\n\n—Lisa\n\nxl\n\nACKNOWLEDGMENTS\n\nJanet’s Story\n\nLisa and I made a great team; each of us had our own strengths. When one of us faltered and took some time to recoup, the other picked up the slack. I learned so much from Lisa (thanks go to her for giving me this opportunity), but I also found I learned a lot from myself. Just the process of articulating my thoughts helped to clarify things that had been rumbling around in my brain for a long time. The mindmaps helped immensely, so thanks to Kenji Hiranabe, who gave a work- shop at Agile 2007 and made me realize what a powerful yet simple tool mind maps can be.\n\nThis book-writing journey was an amazing experience. Thanks to the people on all the teams I worked with who provided so many of the examples in this book.\n\nIt’s been a pretty special year all the way around. During the year (or so) it took to write this book, my family increased in size. My two daughters, Dana and Susan, each gave me a grandson—those were some of the times Lisa picked up the slack. I would like to thank my granddaughter Lauren (currently three) for making me leave my computer and play. It kept me sane. Thanks to my sister Colleen who gave me long-distance encouragement many mornings using instant messenger when I was feeling overwhelmed with the sheer number of hours I was putting in.\n\nAnd a very special thanks to Jack, my husband, who moved his ofﬁce downstairs when I took over the existing one. There were times when I am sure he felt ne- glected and wondered if he even had a wife as he spend many long hours alone. However, he was there with me the whole way, encouraging and supporting me in this endeavor.\n\n—Janet\n\nABOUT THE AUTHORS\n\nLisa Crispin is an agile testing practitioner and coach. She specializes in showing testers and agile teams how testers can add value and how to guide development with business-facing tests. Her mission is to bring agile joy to the software testing world and testing joy to the agile development world. Lisa joined her ﬁrst agile team in 2000, having enjoyed many years working as a programmer, analyst, tester, and QA director. Since 2003, she’s been a tester on a Scrum/XP team at ePlan Services, Inc. She frequently leads tutorials and workshops on agile testing at conferences in North America and Eu- rope. Lisa regularly contributes articles about agile testing to publications such as Better Software magazine, IEEE Software, and Methods and Tools. Lisa also co-authored Testing Extreme Programming (Addison-Wesley, 2002) with Tip House.\n\nFor more about Lisa’s work, visit her websites, www.lisa.crispin.com and www.agiletester.ca, or email her at lisa@agiletester.ca.\n\nJanet Gregory is the founder of DragonFire Inc., an agile quality process consultancy and training ﬁrm. Her passion is helping teams build quality systems. For the past ten years, she has worked as a coach and tester, intro- ducing agile practices into both large and small companies. Her focus is working with business users and testers to understand their role in agile projects. Janet’s programming background is a deﬁnite plus when she part- ners with developers on her agile teams to implement innovative agile test automation solutions. Janet is a frequent speaker at agile and testing software conferences, and she is a major contributor to the North American agile test- ing community.\n\nFor more about Janet’s work, visit her websites at www.janetgregory.ca, www.janetgregory.blogspot.com, and www.agiletester.ca, or you can email her at janet@agiletester.ca.\n\nxli\n\nThis page intentionally left blank\n\nPart I INTRODUCTION\n\nIn the ﬁrst two chapters, we provide an overview of agile testing, highlighting how agile testing differs from testing in a traditional phased or “waterfall” approach. We explore the “whole team” approach to quality and testing.",
      "page_number": 36
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 44-51)",
      "start_page": 44,
      "end_page": 51,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank\n\nChapter 1\n\nWHAT IS AGILE TESTING, ANYWAY?\n\nWhole-Team Approach\n\nAgile Values\n\nWhat Is Agile Testing, Anyway?\n\nWhat We Mean by “Agile Testing”\n\nWorking on Traditional Teams\n\nCustomer Team\n\nWorking on Agile Teams\n\nHow Is Agile Testing Different\n\nA Little Context for Roles and Activities\n\nDeveloper Team\n\nTraditional vs. Agile teams\n\nInteraction\n\nLike a lot of terminology, “agile development” and “agile testing” mean different things to different people. In this chapter, we explain our view of agile, which reﬂects the Agile Manifesto and general principles and values shared by different agile methods. We want to share a common language with you, the reader, so we’ll go over some of our vocabulary. We compare and contrast agile develop- ment and testing with the more traditional phased approach. The “whole team” approach promoted by agile development is central to our attitude toward qual- ity and testing, so we also talk about that here.\n\nAGILE VALUES “Agile” is a buzzword that will probably fall out of use someday and make this book seem obsolete. It’s loaded with different meanings that apply in dif- ferent circumstances. One way to deﬁne “agile development” is to look at the Agile Manifesto (see Figure 1-1).\n\nUsing the values from the Manifesto to guide us, we strive to deliver small chunks of business value in extremely short release cycles.\n\n3\n\n4\n\nCHAPTER 1\n\nChapter 21, “Key Success Factors,” lists key success factors for agile testing.\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\nManifesto for Agile Software Development\n\nWe are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:\n\nIndividuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan\n\nThat is, while there is value in the items on the right, we value the items on the left more.\n\nFigure 1-1 Agile Manifesto\n\nWe use the word “agile” in this book in a broad sense. Whether your team is practicing a particular agile method, such as Scrum, XP, Crystal, DSDM, or FDD, to name a few, or just adopting whatever principles and practices make sense for your situation, you should be able to apply the ideas in this book. If you’re delivering value to the business in a timely manner with high-quality software, and your team continually strives to improve, you’ll ﬁnd useful in- formation here. At the same time, there are particular agile practices we feel are crucial to any team’s success. We’ll talk about these throughout the book.\n\nWHAT DO WE MEAN BY “AGILE TESTING”? You might have noticed that we use the term “tester” to describe a person whose main activities revolve around testing and quality assurance. You’ll also see that we often use the word “programmer” to describe a person whose main activities revolve around writing production code. We don’t intend that these terms sound narrow or insigniﬁcant. Programmers do more than turn a speciﬁcation into a program. We don’t call them “developers,” because ev-\n\nWHAT DO WE MEAN BY “AGILE TESTING”?\n\neryone involved in delivering software is a developer. Testers do more than perform “testing tasks.” Each agile team member is focused on delivering a high-quality product that provides business value. Agile testers work to en- sure that their team delivers the quality their customers need. We use the terms “programmer” and “tester” for convenience.\n\nSeveral core practices used by agile teams relate to testing. Agile program- mers use test-driven development (TDD), also called test-driven design, to write quality production code. With TDD, the programmer writes a test for a tiny bit of functionality, sees it fail, writes the code that makes it pass, and then moves on to the next tiny bit of functionality. Programmers also write code integration tests to make sure the small units of code work together as intended. This essential practice has been adopted by many teams, even those that don’t call themselves “agile,” because it’s just a smart way to think through your software design and prevent defects. Figure 1-2 shows a sample unit test result that a programmer might see.\n\nThis book isn’t about unit-level or component-level testing, but these types of tests are critical to a successful project. Brian Marick [2003] describes these types of tests as “supporting the team,” helping the programmers know what code to write next. Brian also coined the term “technology-facing tests,” tests that fall into the programmer’s domain and are described using pro- grammer terms and jargon. In Part II, we introduce the Agile Testing Quad- rants and examine the different categories of agile testing. If you want to learn more about writing unit and component tests, and TDD, the bibliogra- phy will steer you to some good resources.\n\nFigure 1-2 Sample unit test output\n\n5\n\n6\n\nCHAPTER 1\n\nLisa’s Story\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\nIf you want to know how agile values, principles, and practices applied to test- ing can help you, as a tester, do your best work, and help your team deliver more business value, please keep reading. If you’ve bothered to pick up this book, you’re probably the kind of professional who continually strives to grow and learn. You’re likely to have the mind-set that a good agile team needs to succeed. This book will show you ways to improve your organization’s prod- uct, provide the most value possible to your team, and enjoy your job.\n\nDuring a break from working on this chapter, I talked to a friend who works in quality assurance for a large company. It was a busy time of year, and management expected everyone to work extra hours. He said, “If I thought working 100 extra hours would solve our problems, I’d work ‘til 7 every night until that was done. But the truth was, it might take 4,000 extra hours to solve our problems, so working extra feels pointless.” Does this sound familiar?\n\nIf you’ve worked in the software industry long, you’ve probably had the op- portunity to feel like Lisa’s friend. Working harder and longer doesn’t help when your task is impossible to achieve. Agile development acknowledges the reality that we only have so many good productive hours in a day or week, and that we can’t plan away the inevitability of change.\n\nAgile development encourages us to solve our problems as a team. Business people, programmers, testers, analysts—everyone involved in software devel- opment—decides together how best to improve their product. Best of all, as testers, we’re working together with a team of people who all feel responsible for delivering the best possible quality, and who are all focused on testing. We love doing this work, and you will too.\n\nWhen we say “agile testing” in this book, we’re usually talking about business- facing tests, tests that deﬁne the business experts’ desired features and func- tionality. We consider “customer-facing” a synonym for “business-facing.” “Testing” in this book also includes tests that critique the product and focus on discovering what might be lacking in the ﬁnished product so that we can improve it. It includes just about everything beyond unit and component level testing: functional, system, load, performance, security, stress, usability, exploratory, end-to-end, and user acceptance. All these types of tests might be appropriate to any given project, whether it’s an agile project or one using more traditional methodologies.\n\n—Lisa\n\nA LITTLE CONTEXT FOR ROLES AND ACTIVITIES ON AN AGILE TEAM\n\nAgile testing doesn’t just mean testing on an agile project. Some testing ap- proaches, such as exploratory testing, are inherently agile, whether it’s done an agile project or not. Testing an application with a plan to learn about it as you go, and letting that information guide your testing, is in line with valuing working software and responding to change. Later chapters discuss agile forms of testing as well as “agile testing” practices.\n\nA LITTLE CONTEXT FOR ROLES AND ACTIVITIES ON AN AGILE TEAM We’ll talk a lot in this book about the “customer team” and the “developer team.” The difference between them is the skills they bring to delivering a product.\n\nCustomer Team\n\nThe customer team includes business experts, product owners, domain ex- perts, product managers, business analysts, subject matter experts—every- one on the “business” side of a project. The customer team writes the stories or feature sets that the developer team delivers. They provide the examples that will drive coding in the form of business-facing tests. They communi- cate and collaborate with the developer team throughout each iteration, an- swering questions, drawing examples on the whiteboard, and reviewing ﬁnished stories or parts of stories.\n\nTesters are integral members of the customer team, helping elicit require- ments and examples and helping the customers express their requirements as tests.\n\nDeveloper Team\n\nEveryone involved with delivering code is a developer, and is part of the de- veloper team. Agile principles encourage team members to take on multiple activities; any team member can take on any type of task. Many agile practi- tioners discourage specialized roles on teams and encourage all team mem- bers to transfer their skills to others as much as possible. Nevertheless, each team needs to decide what expertise their projects require. Programmers, system administrators, architects, database administrators, technical writers, security specialists, and people who wear more than one of these hats might be part of the team, physically or virtually.\n\n7\n\n8\n\nCHAPTER 1\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\nTesters are also on the developer team, because testing is a central compo- nent of agile software development. Testers advocate for quality on behalf of the customer and assist the development team in delivering the maximum business value.\n\nInteraction between Customer and Developer Teams\n\nThe customer and developer teams work closely together at all times. Ideally, they’re just one team with a common goal. That goal is to deliver value to the organization. Agile projects progress in iterations, which are small develop- ment cycles that typically last from one to four weeks. The customer team, with input from the developers, will prioritize stories to be developed, and the developer team will determine how much work they can take on. They’ll work together to deﬁne requirements with tests and examples, and write the code that makes the tests pass. Testers have a foot in each world, understand- ing the customer viewpoint as well as the complexities of the technical imple- mentation (see Figure 1-3).\n\nSome agile teams don’t have any members who deﬁne themselves as “testers.” However, they all need someone to help the customer team write business- facing tests for the iteration’s stories, make sure the tests pass, and make sure that adequate regression tests are automated. Even if a team does have testers, the entire agile team is responsible for these testing tasks. Our experience with agile teams has shown that testing skills and experience are vital to project success and that testers do add value to agile teams.\n\nInteraction of Roles\n\nProgrammer\n\nDomain Expert\n\nTester\n\nFigure 1-3 Interaction of roles\n\nHOW IS AGILE TESTING DIFFERENT?\n\nHOW IS AGILE TESTING DIFFERENT? We both started working on agile teams at the turn of the millennium. Like a lot of testers who are new to agile, we didn’t know what to expect at ﬁrst. To- gether with our respective agile teams, we’ve worked on we’ve learned a lot about testing on agile projects. We’ve also implemented ideas and practices suggested by other agile testers and teams. Over the years, we’ve shared our experiences with other agile testers as well. We’ve facilitated workshops and led tutorials at agile and testing conferences, talked with local user groups, and joined countless discussions on agile testing mailing lists. Through these experiences, we’ve identiﬁed differences between testing on agile teams and testing on traditional waterfall development projects. Agile development has transformed the testing profession in many ways.\n\nWorking on Traditional Teams\n\nNeither working closely with programmers nor getting involved with a project from the earliest phases was new to us. However, we were used to strictly enforced gated phases of a narrowly deﬁned software development life cycle, starting with release planning and requirements deﬁnition and usually ending with a rushed testing phase and a delayed release. In fact, we often were thrust into a gatekeeper role, telling business managers, “Sorry, the requirements are frozen; we can add that feature in the next release.”\n\nAs leaders of quality assurance teams, we were also often expected to act as gatekeepers of quality. We couldn’t control how the code was written, or even if any programmers tested their code, other than by our personal efforts at collaboration. Our post-development testing phases were expected to boost quality after code was complete. We had the illusion of control. We usually had the keys to production, and sometimes we had the power to postpone releases or stop them from going forward. Lisa even had the title of “Quality Boss,” when in fact she was merely the manager of the QA team.\n\nOur development cycles were generally long. Projects at a company that pro- duced database software might last for a year. The six-month release cycles Lisa experienced at an Internet start-up seemed short at the time, although it was still a long time to have frozen requirements. In spite of much process and discipline, diligently completing one phase before moving on to the next, it was plenty of time for the competition to come out ahead, and the applications were not always what the customers expected.\n\n9",
      "page_number": 44
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 52-59)",
      "start_page": 52,
      "end_page": 59,
      "detection_method": "topic_boundary",
      "content": "10\n\nCHAPTER 1\n\nLisa’s Story\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\nTraditional teams are focused on making sure all the speciﬁed requirements are delivered in the ﬁnal product. If everything isn’t ready by the original tar- get release date, the release is usually postponed. The development teams don’t usually have input about what features are in the release, or how they should work. Individual programmers tend to specialize in a particular area of the code. Testers study the requirements documents to write their test plans, and then they wait for work to be delivered to them for testing.\n\nWorking on Agile Teams\n\nTransitioning to the short iterations of an agile project might produce initial shock and awe. How can we possibly deﬁne requirements and then test and deliver production-ready code in one, two, three, or four weeks? This is par- ticularly tough for larger organizations with separate teams for different func- tions and even harder for teams that are geographically dispersed. Where do all these various programmers, testers, analysts, project managers, and count- less specialties ﬁt in a new agile project? How can we possibly code and test so quickly? Where would we ﬁnd time for difﬁcult efforts such as automating tests? What control do we have over bad code getting delivered to production?\n\nWe’ll share our stories from our ﬁrst agile experiences to show you that ev- eryone has to start somewhere.\n\nMy ﬁrst agile team embraced Extreme Programming (XP), not without some “learn- ing experiences.” Serving as the only professional tester on a team of eight pro- grammers who hadn’t learned how to automate unit tests was disheartening. The ﬁrst two-week iteration felt like jumping off a cliff.\n\nFortunately, we had a good coach, excellent training, a supportive community of agile practitioners with ideas to share, and time to learn. Together we ﬁgured out some ins and outs of how to integrate testing into an agile project—indeed, how to drive the project with tests. I learned how I could use my testing skills and experience to add real value to an agile team.\n\nThe toughest thing for me (the former Quality Boss) to learn was that the custom- ers, not I, decided on quality criteria for the product. I was horriﬁed after the ﬁrst iteration to ﬁnd that the code crashed easily when two users logged in concur- rently. My coach patiently explained, over my strident objections, that our cus- tomer, a start-up company, wanted to be able to show features to potential customers. Reliability and robustness were not yet the issue.\n\nI learned that my job was to help the customers tell us what was valuable to them during each iteration, and to write tests to ensure that’s what they got.\n\n—Lisa\n\nJanet’s Story\n\nHOW IS AGILE TESTING DIFFERENT?\n\nMy ﬁrst foray into the agile world was also an Extreme Programming (XP) engage- ment. I had just come from an organization that practiced waterfall with some extremely bad practices, including giving the test team a day or so to test six months of code. In my next job as QA manager, the development manager and I were both learning what XP really meant. We successfully created a team that worked well together and managed to automate most of the tests for the func- tionality. When the organization downsized during the dot-com bust, I found myself in a new position at another organization as the lone tester with about ten developers on an XP project.\n\nOn my ﬁrst day of the project, Jonathan Rasmusson, one of the developers, came up to me and asked me why I was there. The team was practicing XP, and the pro- grammers were practicing test-ﬁrst and automating all their own tests. Participating in that was a challenge I couldn’t resist. The team didn’t know what value I could add, but I knew I had unique abilities that could help the team. That experience changed my life forever, because I gained an understanding of the nuances of an agile project and determined then that my life’s work was to make the tester role a more fulﬁlling one.\n\nRead Jonathan’s Story Jonathan Rasmusson, now an Agile Coach at Rasmusson Software Consulting, but Janet’s coworker on her second agile team, explains how he learned how agile testers add value.\n\nSo there I was, a young hotshot J2EE developer excited and pumped to be developing software the way it should be developed—using XP. Until one day, in walks a new team member—a tester. It seems management thought it would be good to have a QA resource on the team.\n\nThat’s ﬁne. Then it occurred to me that this poor tester would have noth- ing to do. I mean, as a developer on an XP project, I was writing the tests. There was no role for QA here as far as I could see.\n\nSo of course I went up and introduced myself and asked quite pointedly what she was going to do on the project, because the developers were writing all the tests. While I can’t remember exactly how Janet responded, the next six months made it very clear what testers can do on agile projects.\n\nWith the automation of the tedious, low-level boundary condition test cases, Janet as a tester was now free to focus on much greater value- add areas like exploratory testing, usability, and testing the app in ways developers hadn’t originally anticipated. She worked with the\n\n11\n\n—Janet\n\n12\n\nCHAPTER 1\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\ncustomer to help write test cases that deﬁned success for upcoming sto- ries. She paired with developers looking for gaps in tests.\n\nBut perhaps most importantly, she helped reinforce an ethos of quality and culture, dispensing happy-face stickers to those developers who had done an exceptional job (these became much sought-after badges of honor displayed prominently on laptops).\n\nWorking with Janet taught me a great deal about the role testers play on agile projects, and their importance to the team.\n\nAgile teams work closely with the business and have a detailed understanding of the requirements. They’re focused on the value they can deliver, and they might have a great deal of input into prioritizing features. Testers don’t sit and wait for work; they get up and look for ways to contribute throughout the development cycle and beyond.\n\nIf testing on an agile project felt just like testing on a traditional project, we wouldn’t feel the need to write a book. Let’s compare and contrast these test- ing methods.\n\nTraditional vs. Agile Testing\n\nIt helps to start by looking at similarities between agile testing and testing in traditional software development. Consider Figure 1-4.\n\nIn the phased approach diagram, it is clear that testing happens at the end, right before release. The diagram is idealistic, because it gives the impression there is as much time for testing as there is for coding. In many projects, this is not the case. The testing gets “squished” because coding takes longer than expected, and because teams get into a code-and-ﬁx cycle at the end.\n\nAgile is iterative and incremental. This means that the testers test each incre- ment of coding as soon as it is ﬁnished. An iteration might be as short as one week, or as long as a month. The team builds and tests a little bit of code, making sure it works correctly, and then moves on to next piece that needs to be built. Programmers never get ahead of the testers, because a story is not “done” until it has been tested. We’ll talk much more about this throughout the book.\n\nThere’s tremendous variety in the approaches to projects that agile teams take. One team might be dedicated to a single project or might be part of another\n\nHOW IS AGILE TESTING DIFFERENT?\n\nPhased or gated—for example, Waterfall\n\nRequirements\n\nSpecifications\n\nCode\n\nTesting\n\nRelease\n\nTime\n\nF\n\nAgile: Iterative & incremental\n\nD\n\nE\n\nD\n\nEach story is expanded, coded, and tested • Possible release after each iteration\n\nC\n\nC\n\nC\n\nA B\n\nA\n\nB\n\nA\n\nB\n\nA\n\nB\n\nIt 0\n\nIt 1\n\nIt 2\n\nIt 3\n\nIt 4\n\nFigure 1-4 Traditional testing vs. agile testing\n\nbigger project. No matter how big your project is, you still have to start some- where. Your team might take on an epic or feature, a set of related stories at an estimating meeting, or you might meet to plan the release. Regardless of how a project or subset of a project gets started, you’ll need to get a high-level un- derstanding of it. You might come up with a plan or strategy for testing as you prepare for a release, but it will probably look quite different from any test plan you’ve done before.\n\nEvery project, every team, and sometimes every iteration is different. How your team solves problems should depend on the problem, the people, and the tools you have available. As an agile team member, you will need to be adaptive to the team’s needs.\n\nRather than creating tests from a requirements document that was created by business analysts before anyone ever thought of writing a line of code, some- one will need to write tests that illustrate the requirements for each story days or hours before coding begins. This is often a collaborative effort between a\n\n13\n\n14\n\nCHAPTER 1\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\nbusiness or domain expert and a tester, analyst, or some other development team member. Detailed functional test cases, ideally based on examples pro- vided by business experts, ﬂesh out the requirements. Testers will conduct manual exploratory testing to ﬁnd important bugs that deﬁned test cases might miss. Testers might pair with other developers to automate and exe- cute test cases as coding on each story proceeds. Automated functional tests are added to the regression test suite. When tests demonstrating minimum functionality are complete, the team can consider the story ﬁnished.\n\nIf you attended agile conferences and seminars in the early part of this de- cade, you heard a lot about TDD and acceptance testing but not so much about other critical types of testing, such as load, performance, security, us- ability, and other “ility” testing. As testers, we thought that was a little weird, because all these types of testing are just as vital on agile projects as they are on projects using any other development methodology. The real difference is that we like to do these tests as early in the development process as we can so that they can also drive design and coding.\n\nIf the team actually releases each iteration, as Lisa’s team does, the last day or two of each iteration is the “end game,” the time when user acceptance test- ing, training, bug ﬁxing, and deployments to staging environments can oc- cur. Other teams, such as Janet’s, release every few iterations, and might even have an entire iteration’s worth of “end game” activities to verify release readiness. The difference here is that all the testing is not left until the end.\n\nAs a tester on an agile team, you’re a key player in releasing code to produc- tion, just as you might have been in a more traditional environment. You might run scripts or do manual testing to verify all elements of a release, such as database update scripts, are in place. All team members participate in ret- rospectives or other process improvement activities that might occur for ev- ery iteration or every release. The whole team brainstorms ways to solve problems and improve processes and practices.\n\nAgile projects have a variety of ﬂavors. Is your team starting with a clean slate, in a greenﬁeld (new) development project? If so, you might have fewer challenges than a team faced with rewriting or building on a legacy system that has no automated regression suite. Working with a third party brings additional testing challenges to any team.\n\nWhatever ﬂavor of development you’re using, pretty much the same ele- ments of a software development life cycle need to happen. The difference\n\nWHOLE-TEAM APPROACH\n\nwith agile is that time frames are greatly shortened, and activities happen concurrently. Participants, tests, and tools need to be adaptive.\n\nThe most critical difference for testers in an agile project is the quick feed- back from testing. It drives the project forward, and there are no gatekeepers ready to block project progress if certain milestones aren’t met.\n\nWe’ve encountered testers who resist the transition to agile development, fearing that “agile development” equates with chaos, lack of discipline, lack of documentation, and an environment that is hostile to testers. While some teams do seem to use the “agile” buzzword to justify simply doing whatever they want, true agile teams are all about repeatable quality as well as efﬁ- ciency. In our experience, an agile team is a wonderful place to be a tester.\n\nWHOLE-TEAM APPROACH One of the biggest differences in agile development versus traditional devel- opment is the agile “whole-team” approach. With agile, it’s not only the testers or a quality assurance team who feel responsible for quality. We don’t think of “departments,” we just think of the skills and resources we need to deliver the best possible product. The focus of agile development is producing high- quality software in a time frame that maximizes its value to the business. This is the job of the whole team, not just testers or designated quality assurance professionals. Everyone on an agile team gets “test-infected.” Tests, from the unit level on up, drive the coding, help the team learn how the application should work, and let us know when we’re “done” with a task or story.\n\nAn agile team must possess all the skills needed to produce quality code that delivers the features required by the organization. While this might mean in- cluding specialists on the team, such as expert testers, it doesn’t limit particu- lar tasks to particular team members. Any task might be completed by any team member, or a pair of team members. This means that the team takes re- sponsibility for all kinds of testing tasks, such as automating tests and man- ual exploratory testing. It also means that the whole team thinks constantly about designing code for testability.\n\nThe whole-team approach involves constant collaboration. Testers collabo- rate with programmers, the customer team, and other team specialists—and not just for testing tasks, but other tasks related to testing, such as building infrastructure and designing for testability. Figure 1-5 shows a developer re- viewing reports with two customers and a tester (not pictured).\n\n15\n\n16\n\nCHAPTER 1\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\nFigure 1-5 A developer discusses an issue with customers\n\nThe whole-team approach means everyone takes responsibility for testing tasks. It means team members have a range of skill sets and experience to em- ploy in attacking challenges such as designing for testability by turning ex- amples into tests and into code to make those tests pass. These diverse viewpoints can only mean better tests and test coverage.\n\nMost importantly, on an agile team, anyone can ask for and receive help. The team commits to providing the highest possible business value as a team, and the team does whatever is needed to deliver it. Some folks who are new to ag- ile perceive it as all about speed. The fact is, it’s all about quality—and if it’s not, we question whether it’s really an “agile” team.\n\nYour situation is unique. That’s why you need to be aware of the potential testing obstacles your team might face and how you can apply agile values and principles to overcome them.\n\nSUMMARY\n\nSUMMARY Understanding the activities that testers perform on agile teams helps you show your own team the value that testers can add. Learning the core prac- tices of agile testing will help your team deliver software that delights your customers.\n\nIn this chapter, we’ve explained what we mean when we use the term “agile testing.\n\n(cid:2) We showed how the Agile Manifesto relates to testing, with its empha- sis on individuals and interactions, working software, customer col- laboration, and responding to change.\n\n(cid:2) We provided some context for this book, including some other terms we use such as “tester,” “programmer,” “customer,” and related terms so that we can speak a common language.\n\n(cid:2) We explained how agile testing, with its focus on business value and delivering the quality customers require, is different from traditional testing, which focuses on conformance to requirements.\n\n(cid:2) We introduced the “whole-team” approach to agile testing, which\n\nmeans that everyone involved with delivering software is responsible for delivering high-quality software.\n\n(cid:2) We advised taking a practical approach by applying agile values and principles to overcome agile testing obstacles that arise in your unique situation.\n\n17",
      "page_number": 52
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 60-67)",
      "start_page": 60,
      "end_page": 67,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank\n\nChapter 2\n\nTEN PRINCIPLES FOR AGILE TESTERS\n\nAdding Value\n\nProvide Continuous Feedback\n\nWhat Is an Agile Tester?\n\nDeliver Value to the Customer\n\nEnable Face-to-Face Communication\n\nHave Courage\n\nTen Principles for Agile Testers\n\nKeep It Simple\n\nPractice Continuous Improvement\n\nApplying Agile Principles and Values\n\nThe Agile Tesing Mind-Set\n\nRespond to Change\n\nSelf-Organize\n\nFocus on People\n\nEnjoy\n\nEveryone on an agile team is a tester. Anyone can pick up testing tasks. If that’s true, then what is special about an agile tester? If I deﬁne myself as a tester on an agile team, what does that really mean? Do agile testers need different skill sets than testers on traditional teams? What guides them in their daily activities?\n\nIn this chapter, we talk about the agile testing mind-set, show how agile val- ues and principles guide testing, and give an overview of how testers add value on agile teams.\n\nWHAT’S AN AGILE TESTER? We deﬁne an agile tester this way: a professional tester who embraces change, collaborates well with both technical and business people, and understands the concept of using tests to document requirements and drive development. Agile testers tend to have good technical skills, know how to collaborate with\n\n19\n\n20\n\nCHAPTER 2\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\nothers to automate tests, and are also experienced exploratory testers. They’re willing to learn what customers do so that they can better under- stand the customers’ software requirements.\n\nWho’s an agile tester? She’s a team member who drives agile testing. We know many agile testers who started out in some other specialization. A developer becomes test-infected and branches out beyond unit testing. An exploratory tester, accustomed to working in an agile manner, is attracted to the idea of an agile team. Professionals in other roles, such as business or functional an- alysts, might share the same traits and do much of the same work.\n\nSkills are important, but attitude counts more. Janet likes to say, “Without the attitude, the skill is nothing.” Having had to hire numerous testers for our agile teams, we've put a lot of thought into this and discussed it with others in the agile community. Testers tend to see the big picture. They look at the application more from a user or customer point of view, which means they’re generally customer-focused.\n\nTHE AGILE TESTING MIND-SET What makes a team “agile”? To us, an agile team is one that continually fo- cuses on doing its best work and delivering the best possible product. In our experience, this involves a ton of discipline, learning, time, experimentation, and working together. It’s not for everyone, but it’s ideal for those of us who like the team dynamic and focus on continual improvement.\n\nSuccessful projects are a result of good people allowed to do good work. The characteristics that make someone succeed as a tester on an agile team are probably the same characteristics that make a highly valued tester on any team.\n\nAn agile tester doesn’t see herself as a quality police ofﬁcer, protecting her cus- tomers from inadequate code. She’s ready to gather and share information, to work with the customer or product owner in order to help them express their requirements adequately so that they can get the features they need, and to provide feedback on project progress to everyone.\n\nAgile testers, and maybe any tester with the right skills and mind-set, are continually looking for ways the team can do a better job of producing high- quality software. On a personal level, that might mean attending local user group meetings or roundtables to ﬁnd out what other teams are doing. It\n\nAPPLYING AGILE PRINCIPLES AND VALUES\n\nalso means trying out new tools to help the team do a better job of specify- ing, executing, and automating customer requirements as tests.\n\nThe bottom line is that agile testers, like their agile teammates, enjoy learning new skills and taking on new challenges, and they don’t limit themselves to solving only testing issues. This isn’t just a trait of testers; we see it in all agile team members. Agile testers help the developer and customer teams address any kind of issue that might arise. Testers can provide information that helps the team look back and learn what’s working and what isn’t.\n\nCreativity, openness to ideas, willingness to take on any task or role, focus on the customer, and a constant view of the big picture are just some components of the agile testing mind-set. Good testers have an instinct and understanding for where and how software might fail, and how to track down failures.\n\nTesters might have special expertise and experience in testing, but a good ag- ile tester isn’t afraid to jump into a design discussion with suggestions that will help testability or create a more elegant solution. An agile testing mind- set is one that is results-oriented, craftsman-like, collaborative, eager to learn, and passionate about delivering business value in a timely manner.\n\nAPPLYING AGILE PRINCIPLES AND VALUES Individuals can have a big impact on a project’s success. We’d expect a team with more experienced and higher-skilled members to outperform a less tal- ented team. But a team is more than just its individual members. Agile values and principles promote a focus on the people involved in a project and how they interact and communicate. A team that guides itself with agile values and principles will have higher team morale and better velocity than a poorly functioning team of talented individuals.\n\nThe four value statements in the Agile Manifesto, which we presented at the start of the ﬁrst chapter, show preferences, not ultimatums, and make no statements about what to do or not to do. The Agile Manifesto also includes a list of principles that deﬁne how we approach software development. Our list of agile “testing” principles is partially derived from those principles. Because we both come from the Extreme Programming culture, we’ve adopted many of its values and underlying principles. We’ve also incorporated guidelines and principles that have worked for our teams. Your team’s own values and principles will guide you as you choose practices and make decisions about how you want to work.\n\n21\n\n22\n\nCHAPTER 2\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\nThe principles we think are important for an agile tester are:\n\n(cid:2) Provide continuous feedback. (cid:2) Deliver value to the customer. (cid:2) Enable face-to-face communication. (cid:2) Have courage. (cid:2) Keep it simple. (cid:2) Practice continuous improvement. (cid:2) Respond to change. (cid:2) Self-organize. (cid:2) Focus on people. (cid:2) Enjoy.\n\nProvide Continuous Feedback\n\nGiven that tests drive agile projects, it’s no surprise that feedback plays a big part in any agile team. The tester’s traditional role of “information provider” makes her inherently valuable to an agile team. One of the agile tester’s most important contributions is helping the product owner or customer articulate requirements for each story in the form of examples and tests. The tester then works together with teammates to turn those requirements into execut- able tests. Testers, programmers, and other team members work to run these tests early and often so they’re continually guided by meaningful feedback. We’ll spend a lot of time in this book explaining ways to do this.\n\nWhen the team encounters obstacles, feedback is one way to help remove them. Did we deliver a user interface that didn’t quite meet customer expec- tations? Let’s write a task card reminding us to collaborate with the customer on paper prototypes of the next UI story.\n\nIs management worried about how work is progressing? Display a big visible chart of tests written, run, and passing every day. Display big-picture func- tionality coverage such as test matrices. Having trouble getting the build sta- ble? Lisa’s team displayed the number of days remaining until time to tag the build for release in order to keep everyone focused on ﬁnishing stories in time. After that became a habit, they didn’t need the visual cue anymore.\n\nDeliver Value to the Customer\n\nAgile development is about delivering value in small releases that provide ex- actly the functionality that the customer has most recently prioritized. This usually means limiting scope. It’s easy to get caught up in the customer\n\nLisa’s Story\n\nLisa’s Story\n\nAPPLYING AGILE PRINCIPLES AND VALUES\n\nteam’s desire for cool features. Anyone can question these additions, but a tester often recognizes the impact to the story, because they need to think about the testing repercussions.\n\nOur product owner participates in planning meetings before each iteration. Never- theless, after the iteration has started and we discuss more details about the sto- ries and how to test them, he often brings up an idea that didn’t come out during the planning, such as, “Well, it would really be nice if the selection on this report could include X, Y, and Z and be sorted on A as well.” An innocent request can add a lot of complexity to a story. I often bring in one of the programmers to talk about whether this addition can be handled within the scope of the story we had planned. If not, we ask the product owner to write a card for the next iteration.\n\nAgile testers stay focused on the big picture. We can deliver the most critical functionality in this iteration and add to it later. If we let new features creep in, we risk delivering nothing on time. If we get too caught up with edge cases and miss core functionality on the happy path, we won’t provide the value the business needs.\n\nTo ensure that we deliver some value in each iteration, our team looks at each story to identify the “critical path” or “thin slice” of necessary functionality. We complete those tasks ﬁrst and then go back and ﬂesh out the rest of the features. The worst-case scenario is that only the core functionality gets released. That’s better than delivering nothing or something that works only halfway.\n\nAgile testers take the same approach as that identiﬁed in Lisa’s story. While one of our skills is to identify test cases beyond the “happy path,” we still need to start by making sure the happy path works. We can automate tests for the happy path, and add negative and boundary tests later. Always consider what adds the most value to the customer, and understand your context. If an ap- plication is safety-critical, adding negative tests is absolutely required. The testing time needs to be considered during the estimation process to make sure that enough time is allotted in the iteration to deliver a “safe” feature.\n\nEnable Face-to-Face Communication\n\nNo team works well without good communication. Today, when so many teams are distributed in multiple geographical locations, communication is\n\n23\n\n—Lisa\n\n—Lisa\n\n24\n\nCHAPTER 2\n\nJanet’s Story\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\neven more vital and more of a challenge. The agile tester should look for unique ways to facilitate communication. It is a critical aspect to doing her job well.\n\nWhen I was working with one team, we had a real problem with programmers talking with the product owner and leaving the testers out of the discussion. They often found out about changes after the fact. Part of the problem was that the developers were not sitting with the testers due to logistical problems. Another problem was history. The test team was new, and the product owner was used to going straight to the programmers.\n\nI took the problem to the team, and we created a rule. We found great success with the “Power of Three.” This meant that all discussions about a feature needed a programmer, a tester, and the product owner. It was each person’s responsibility to make sure there was always a representative from each group. If someone saw two people talking, they had the right to butt into the conversation. It didn’t take very long before it was just routine and no one would consider leaving the tester out of a discussion. This worked for us because the team bought into the solution.\n\nAny time there is a question about how a feature should work or what an inter- face should look like, the tester can pull in a programmer and a business expert to talk about it. Testers should never get in the way of any direct customer- developer communication, but they can often help to make sure that com- munication happens.\n\nAgile testers see each story or theme from the customer’s point of view but also understand technical aspects and limitations related to implementing features. They can help customers and developers achieve a common lan- guage. Business people and software people often speak different languages. They have to ﬁnd some common ground in order to work together success- fully. Testers can help them develop a shared language, a project dialect, or team jargon.\n\nBrian Marick (2004) recommends that we use examples to develop this lan- guage. When Lisa’s team digresses into a philosophical discussion during a sprint planning meeting, Lisa asks the product owner for an example or us- age scenario. Testers can encourage whiteboard discussions to work through more examples. These help the customers envision their requirements more clearly. They also help the developers to produce well-designed code to meet those requirements.\n\n—Janet\n\nAPPLYING AGILE PRINCIPLES AND VALUES\n\nFace-to-face communication has no substitute. Agile development depends on constant collaboration. Like other agile team members, the people doing testing tasks will continually seek out customer and technical team members to discuss and collaborate. When an agile tester suspects a hidden assump- tion or a misunderstood requirement, she’ll get a customer and a developer talking about it. If people in a different building or continent need to talk, they look for creative ways to replace face-to-face, real-time conversations.\n\nHave Courage\n\nCourage is a core value in XP, and practices such as test automation and con- tinuous integration allow the team to practice this value. The developers have the courage to make changes and refactor the code because they have the safety net of an automated regression suite. In this section, we talk about the emotional courage that is needed when transitioning to an agile team.\n\nHave you worked in an organization where testers were stuck in their own silo, unable to talk to either business stakeholders or other members of the technical team? While you might jump at the chance to join a collaborative agile environment, you might feel uncomfortable having to go ask the cus- tomer for examples, or ask a programmer to help automate a test or bring up a roadblock during the daily stand-up.\n\nWhen you ﬁrst join an agile team, or when your current team ﬁrsts transi- tions to agile development, it’s normal to experience fear and have a list of questions that need to be answered. How in the world are we going to be able to complete testing tasks for each story in such a short time? How will testing “keep up” with development? How do you know how much testing is enough? Or maybe you’re a functional testing manager or a quality process manager and it’s not clear to you where that role ﬁts on an agile team, and nobody has the answers. Agile testers need courage to ﬁnd the answers to those questions, but there are other reasons as well for having courage.\n\nWe need courage to let ourselves fail, knowing that at least we’ll fail fast and be able to learn from that failure. After we’ve blown an iteration because we didn’t get a stable build, we’ll start thinking of ways to ensure it doesn’t hap- pen again.\n\nWe need courage to allow others to make mistakes, because that’s the only way to learn the lesson.\n\n25",
      "page_number": 60
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 68-77)",
      "start_page": 68,
      "end_page": 77,
      "detection_method": "topic_boundary",
      "content": "26\n\nCHAPTER 2\n\nLisa’s Story\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\nI worked on a project where the agile coach insisted that I be on a separate test- ing team (often a team of one!) whose work wasn’t included in the programmers’ tracking and velocity. I had to just go along and try this. After the release ran into trouble because testing wasn’t ﬁnished, I asked the coach if we could try things my way for an iteration or two. The whole-team approach worked much better. Each story was tested and “done” by the end of the iteration, and the customers were much happier with the results.\n\nWe need courage to ask for help, especially when the person who could pro- vide that help looks pretty busy and stressed-out himself. Climbing out of your old silo and joining in a team responsibility for success or failure takes courage. Asking a question or pointing out what you think is a ﬂaw requires courage, even in a team supported by agile values and principles. Don’t be afraid! Agile teams are open and generally accepting of new ideas.\n\nKeep It Simple\n\nKent Beck’s Extreme Programming Explained advised us to do the simplest thing that could possibly work. That doesn’t mean the ﬁrst thing you try will actually work, but it ought to be simple.\n\nAgile testers and their teams are challenged to not only produce the simplest possible software implementation but to take a simple approach to ensuring that software meets the customer requirements. This doesn’t mean that the team shouldn’t take some time to analyze themes and stories and think through the appropriate architecture and design. It does mean that the team might need to push back to the business side of the team when their re- quirements might be a bit elaborate and a simpler solution will deliver the same value.\n\nSome of us worked in software organizations where we, as testers and quality assurance staff, were asked to set quality standards. We believe this is back- wards, because it’s up to the customer team to decide what level of quality they want to pay for. Testers and other team members should provide information to customers and help them consider all aspects of quality, including nonfunc- tional requirements such as performance and security. The ultimate decisions are up to the customer. The team can help the customer make good decisions by its taking a simple, step-by-step approach to its work. Agile testing means\n\n—Lisa\n\nChapter 9, “Tool- kit for Business- Facing Tests that Support the Team,” and Chap- ter 11, “Critiquing the Product using Technology- Facing Tests,” give examples of test tools.\n\nPart IV, “Test Auto- mation,” explains how to build a “doable” test au- tomation strategy.\n\nAPPLYING AGILE PRINCIPLES AND VALUES\n\ndoing the simplest tests possible to verify that a piece of functionality exists or that the customer’s quality standard (e.g., performance) has been met.\n\nSimple doesn’t mean easy. For testers, it means testing “just enough” with the lightest-weight tools and techniques we can ﬁnd that will do the job. Tools can be as simple as a spreadsheet or a checklist. We need to automate regres- sion tests, but we should push them down to the lowest level possible in or- der to encourage fast feedback. Even simple smoke tests might be enough for business-facing test automation.\n\nExploratory testing can be used to learn about your application and ferret out hard-to-ﬁnd bugs, but start with the basics, time-boxing side trips and evaluating how far to go with edge cases. Simplicity helps us keep our focus on risk, return on investment, and improving in the areas of greatest pain.\n\nPractice Continuous Improvement\n\nLooking for ways to do a better job is part of an agile tester’s mind-set. Of course, the whole team should be thinking this way, because the central core of agile is that the team always tries to do better work. Testers participate in team retrospectives, evaluating what’s working well and what needs to be added or tweaked. Testers bring testing issues up for the whole team to ad- dress. Teams have achieved their greatest improvements in testing and all other areas through the use of process improvement practices such as retro- spectives and impediment backlogs. Some improvement ideas might become task cards. For larger problems, teams focus on one or two issues at a time to make sure they solve the real problem and not just the symptom.\n\nAgile testers and their teams are always on the lookout for tools, skills, or practices that might help them add more value or get a better return on the customer’s investment. The short iterations of agile development make it easier to try something new for a few iterations and see whether it’s worth adopting for the long term.\n\nLearning new skills and growing professionally are important to agile testers. They take advantage of the many available free resources to improve their specialized skills, such as exploratory testing. They go to meetings and con- ferences, join mailing lists, and read articles, blogs, and books to get new ideas. They look for ways to automate (or get help from their coworkers to automate) mundane or repetitive tasks so they have more time to contribute their valuable expertise.\n\n27\n\n28\n\nCHAPTER 2\n\nLisa’s Story\n\nWe’ll talk more about retrospec- tives and how they can help your team practice con- tinuous improve- ment in Chapter 19, “Wrap Up the Iteration.”\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\nPierre Veragren, an SQA Lead at iLevel by Weyerhaeuser, identiﬁed a quality we often see in agile teams ourselves: “AADD,” Agile Attention Deﬁcit Disor- der. Anything not learned quickly might be deemed useless. Agile team mem- bers look for return on investment, and if they don’t see it quickly, they move on. This isn’t a negative characteristic when you’re delivering production- ready software every two weeks or even more often.\n\nRetrospectives are a key agile practice that lets the team use yesterday’s ex- perience to do a better job tomorrow. Agile testers use this opportunity to raise testing-related issues and ask the team to brainstorm ways to address them. This is a way for the team to provide feedback to itself for continual improvement.\n\nOur team had used retrospectives to great beneﬁt, but we felt we needed some- thing new to help us focus on doing a better job. I suggested keeping an “imped- iment backlog” of items that were keeping us from being as productive as we’d like to be. The ﬁrst thing I wrote in the impediment backlog was our test environ- ment’s slow response time. Our system administrator scrounged a couple of bar- gain machines and turned them into new, faster servers for our test environments. Our DBA analyzed the test database performance, found that the one-disk system was the impediment, and our manager gave the go-ahead to install a RAID for bet- ter disk access. Soon we were able to deploy builds and conduct our exploratory testing much faster.\n\nRespond to Change\n\nWhen we worked in a waterfall environment, we got used to saying, “Sorry, we can’t make this change now; the requirements are frozen. We’ll have to put that in the ﬁrst patch release.” It was frustrating for customers because they realized that they didn’t do a great job on deﬁning all their requirements up front.\n\nIn a two-week agile iteration, we might have to say, “OK, write a card for that and we’ll do it in the next iteration or next release,” but customers know they can get their change when they want it because they control the priority.\n\nResponding to change is a key value for agile practitioners, but we’ve found that it’s one of the most difﬁcult concepts for testers. Stability is what testers crave so that they can say, “I’ve tested that; it’s done.” Continuously changing requirements are a tester’s nightmare. However, as agile testers, we have to welcome change. On Wednesday, we might expect to start stories A and B\n\n—Lisa\n\nLisa’s Story\n\nAPPLYING AGILE PRINCIPLES AND VALUES\n\nand then C the next Friday. By Friday, the customer could have re-prioritized and now wants stories A, X, and Y. As long as we keep talking to the cus- tomer, we can handle changes like that because we are working at the same pace with the rest of team.\n\nSome agile teams try to prepare in advance of the next iteration, perhaps by writing high-level test cases, capturing business satisfaction conditions, or documenting examples. It’s a tricky business that might result in wasted time if stories are re-prioritized or greatly changed. However, distributed teams in particular need extra feedback cycles to get ready for the iteration.\n\nOur remote team member used to be our on-site manager. He’s a key player in helping the business write and prioritize stories. He has in-depth knowledge of both the code and the business, which helps him come up with creative solutions to business needs. When he moved to India, we looked for ways to retain the beneﬁt of his expertise. Meetings are scheduled at times when he can participate, and he has regular conference calls with the product owner to talk about upcom- ing stories. We’ve had to switch from low-tech tools such as index cards to online tools that we can all use.\n\nBecause the team was willing to make changes in the way we worked, and looked for tools that helped keep him in the loop with ongoing changes, we were able to retain the beneﬁt of his expertise.\n\nSome teams have analysts who can spend more time with the business ex- perts to do some advance planning. Each team has to strike a balance be- tween brainstorming solutions ahead of time and starting from scratch on the ﬁrst day of each iteration. Agile testers go with the ﬂow and work with the team to accommodate changes.\n\nAutomated testing is one key to the solution. One thing we know for sure: No agile team will succeed doing only manual testing. We need robust automa- tion in order to deliver business value in a time frame that makes it valuable.\n\nSelf-Organize\n\nThe agile tester is part of a self-organizing agile team. The team culture im- bues the agile testing philosophy. When programmers, system administra- tors, analysts, database experts, and the customer team think continually about testing and test automation, testers enjoy a whole new perspective. Au- tomating tests is hard, but it is much easier when you have the whole team\n\n29\n\n—Lisa\n\n30\n\nCHAPTER 2\n\nLisa’s Story\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\nworking together. Any testing issue is easier to address when you have people with multiple skill sets and multiple perspectives attacking it.\n\nMy team is a good example of a self-organizing team. When we implemented Scrum, we had a buggy legacy system and no automated tests. Making any changes to the code was risky at best. Our manager probably had some excellent solutions to the problem, but he didn’t suggest them. Instead, we explored the issues and came up with a plan.\n\nThe programmers would start implementing new stories in a new, testable archi- tecture, using test-driven development. The testers would write manual regression test scripts, and the entire team—programmers, testers, the system administrator, and the DBA—would execute them on the last two days of every iteration. The testers (at the time, this meant me) would work on an automated regression smoke test suite through the user interface. Eventually, the architecture of the new code would let us automate functional tests with a tool such as FitNesse.\n\nWe implemented this plan in baby steps, reﬁning our approach in each iteration. Using the skills of every member of the team was a much better approach than my going off and deciding the automation strategy on my own.\n\nWhen an agile team faces a big problem, perhaps a production showstopper or a broken build, it’s everyone’s problem. The highest-priority issues are problems for the whole team to solve. Team members discuss the issue right away and decide how to and who will ﬁx it.\n\nThere’s no doubt that Lisa’s manager could have mandated that the team take this approach to solving its automation problems, but the team itself can come up with the most workable plan. When the team creates its own ap- proach and commits to it, its members adopt a new attitude toward testing.\n\nFocus on People\n\nProjects succeed when good people are allowed to do their best work. Agile values and principles were created with the aim of enabling individual and team success. Agile team members should feel safe and not have to worry about being blamed for mistakes or losing their jobs. Agile team members re- spect each other and recognize individual accomplishments. Everyone on an agile team should have opportunities to grow and develop their skills. Agile teams work at a sustainable pace that lets them follow disciplined practices and keep a fresh perspective. As the Agile Manifesto states, we value individ- uals and interactions over processes and tools.\n\n—Lisa\n\nADDING VALUE\n\nIn the history of software development, testers haven’t always enjoyed parity with other roles on the development team. Some people saw testers as failed programmers or second-class citizens in the world of software development. Testers who don’t bother to learn new skills and grow professionally contribute to the perception that testing is low-skilled work. Even the term “tester” has been avoided, with job titles such as “Quality Assurance Engineer” or “Quality Analyst” and team names such as “QA Department” given preference.\n\nAgile teams that adhere to the true agile philosophy give all team members equal weight. Agile testers know they contribute unique value to their teams, and development teams have found they are more successful when their team includes people with speciﬁc testing skills and background. For exam- ple, a skilled exploratory tester may discover issues in the system that couldn’t be detected by automated functional tests. Someone with deep test- ing experience might ask important questions that didn’t occur to team members without testing experience. Testing knowledge is one component of any team’s ability to deliver value.\n\nEnjoy\n\nWorking on a team where everyone collaborates, where you are engaged in the project from start to ﬁnish, where business stakeholders work together with the development team, where the whole team takes responsibility for quality and testing, in our opinion, is nothing short of a tester’s Utopia. We’re not alone in believing that everyone should ﬁnd joy in their work. Ag- ile development rewards the agile tester’s passion for her work.\n\nOur jobs as agile testers are particularly satisfying because our viewpoint and skills let us add real value to our teams. In the next section, we’ll explore how.\n\nADDING VALUE What do these principles bring to the team? Together, they bring business value. In agile development, the whole team takes responsibility for deliver- ing high-quality software that delights customers and makes the business more proﬁtable. This, in turn, brings new advantages for the business.\n\nTeam members wear many hats, and agile development tends to avoid classi- fying people by specialty. Even with short iterations and frequent releases, it’s easy to develop a gap between what the customer team expects and what the team delivers. Using tests to drive development helps to prevent this, but you still need the right tests.\n\n31\n\n32\n\nCHAPTER 2\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\nAgile testers not only think about the system from the viewpoint of stakehold- ers who will live with the solution but they also have a grasp of technical constraints and implementation details that face the development team. Pro- grammers focus on making things work. If they’re coding to the right require- ments, customers will be happy. Unfortunately, customers aren’t generally good at articulating their requirements. Driving development with the wrong tests won’t deliver the desired outcome. Agile testers ask questions of both customers and developers early and often, and help shape the answers into the right tests.\n\nAgile testers take a much more integrated, team-oriented approach than testers on traditional waterfall projects. They adapt their skills and experi- ence to the team and project. A tester who views programmers as adversaries, or sits and waits for work to come to her, or expects to spend more time planning than doing, is likely to cling to skills she learned on traditional projects and won’t last long on an agile team.\n\nPeril: You’re Not “Really” Part of the Team\n\nIf you’re a tester, and you’re not invited to attend planning sessions, stand- ups, or design meetings, you might be in a situation where testers are viewed as somehow apart from the development team. If you are invited to these meetings but you’re not speaking up, then you’re probably creating a percep- tion that you aren’t really part of the team. If business experts are writing sto- ries and deﬁning requirements all by themselves, you aren’t participating as a tester who’s a member of an agile team.\n\nIf this is your situation, your team is at risk. Hidden assumptions are likely to go undetected until late in the release cycle. Ripple effects of a story on other parts of the system aren’t identiﬁed until it’s too late. The team isn’t making the best use of every team member’s skills, so it’s not going to be able to produce the best possible software. Communication might break down, and it’ll be hard to keep up with what the programmers and customers are doing. The team risks being divided in an unhealthy way between developers and testers, and there’s more potential that the development team will become isolated from the customer team.\n\nHow can you avoid this peril? See if you can arrange to be located near the developers. If you can’t, at least come to their area to talk and pair test. Ask them to show you what they’re working on. Ask them to look at the test cases you’ve written. Invite yourself to meetings if nobody else has invited you. Make yourself useful by testing and providing feedback, and become a neces- sity to the team.\n\nSUMMARY\n\nHelp customers develop their stories and acceptance tests. Push the “whole team” attitude, and ask the team to work on testing problems. If your team is having trouble adapting to agile development, suggest experimenting with some new ideas for an iteration or two. Propose adopting the “Power of Three” rule to promote good communication. Use the information in this book to show that testers can help agile teams succeed beyond their wildest expectations.\n\nDuring story estimating and planning sessions, agile testers look at each fea- ture from multiple perspectives: business, end user, production support, and programmer. They consider the problems faced by the business and how the software might address them. They raise questions that ﬂush out assump- tions made by the customer and developer teams. At the start of each itera- tion, they help to make sure the customer provides clear requirements and examples, and they help the development team turn those into tests. The tests drive development, and test results provide feedback on the team’s progress. Testers help to raise issues so that no testing is overlooked; it’s more than functional testing. Customers don’t always know that they should men- tion their performance and reliability needs or security concerns, but testers think to ask about those. Testers also keep the testing approach and tools as simple and lightweight as possible. By the end of the iteration, testers verify that the minimum testing was completed.\n\nLines between roles on an agile team are blurred. Other team members might be skilled at the same activities that testers perform. For example, ana- lysts and programmers also write business-facing tests. As long as all testing activities are performed, an agile team doesn’t necessarily require members who identify themselves primarily as testers. However, we have found that teams beneﬁt from the skills that professional testers have developed. The ag- ile principles and values we’ve discussed will help any team do a good job of testing and delivering value.\n\nSUMMARY In this chapter, we covered principles for agile testers and the values we think an agile tester needs to possess in order to contribute effectively to an agile team.\n\n(cid:2) An “agile testing mind-set” is customer-focused, results-oriented,\n\ncraftsman-like, collaborative, creative, eager to learn, and passionate about delivering business value in a timely manner.\n\n(cid:2) Attitude is important, and it blurs the lines between testers, program-\n\nmers, and other roles on an agile team.\n\n33\n\n34\n\nCHAPTER 2\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\n(cid:2) Agile testers apply agile values and principles such as feedback, com- munication, courage, simplicity, enjoyment, and delivering value in order to help the team identify and deliver the customer requirements for each story.\n\n(cid:2) Agile testers add value to their teams and their organizations with\n\ntheir unique viewpoint and team-oriented approach.\n\nPart II ORGANIZATIONAL CHALLENGES\n\nWhen software development organizations implement agile development, the testing or QA team often takes the longest to make the transition. Inde- pendent QA teams have become entrenched in many organizations. When they start to adapt to a new agile organization, they encounter cultural differ- ences that are difﬁcult for them to accept. In Part II, we talk about introduc- ing change and some of the barriers you might encounter when transitioning to agile. Training is a big part of what organizations making the transition need, and it’s often forgotten. It’s also hard to see how existing processes such as audits and process improvement frameworks will work in the agile envi- ronment. Going from an independent QA team to an integrated agile team is a huge change.\n\nChapter 4, “Team Logistics,” talks about the team structure, such as where a tester actually ﬁts into the team, and the never-ending question about tester- developer ratio. We’ll also talk about hiring testers and what to look for in a successful agile tester.\n\nTraditional testing activities, such as logging bugs, keeping track of metrics, and writing test plans, might not seem like a good ﬁt in an agile project. We introduce some of the typical processes that might need special care and at- tention and discuss how to adapt existing quality processes.\n\nYou can expect to ﬁnd ways that testers and test teams accustomed to a tra- ditional waterfall type of development environment can change their orga- nizational structure and culture to beneﬁt from and add value to agile development.",
      "page_number": 68
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 78-86)",
      "start_page": 78,
      "end_page": 86,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank\n\nChapter 3\n\nCULTURAL CHALLENGES\n\nBe Patient\n\nQuality Philosophy\n\nLet Them Feel the Pain\n\nSustainable Pace\n\nBuild Your Credibility\n\nProfessional Development\n\nChange Doesn’t Come Easy\n\nOrganizational Culture\n\nCustomer Relationships\n\nOrganization Size\n\nBeware the Quality Police Mentality\n\nEmpower Your Team\n\nVote with Your Feet\n\nCultural Challenges\n\nCultural Change for Managers\n\nSpeaking the Manager’s Language\n\nManagement Expectations\n\nLoss of Identity\n\nAdditional Roles\n\nBarriers to Success\n\nLack of Training\n\nNot Understanding Agile Concepts\n\nTalk about Fears\n\nPast Experiences\n\nGive Team Ownership\n\nIntroducing Change\n\nCultural Differences among Roles\n\nCelebrate Success\n\nMany organizational inﬂuences can impact a project, whether it uses an agile or a traditional phased or gated approach. Organizational and team culture can block a smooth transition to an agile approach. In this chapter, we discuss fac- tors that can directly affect a tester’s role on an agile team.\n\nORGANIZATIONAL CULTURE An organizational culture is deﬁned by its values, norms, and assumptions. An organization’s culture governs how people communicate, interrelate, and make decisions, and it is easily seen by observing employee behavior.\n\n37\n\n38\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nThe culture of an organization can impact the success of an agile team. Agile teams are best suited for organizations that allow independent thinking. For example, if a company has a hierarchical structure and encourages a directive management style for all its projects, agile teams will probably struggle. Past experiences of the organization will also affect the success of a new agile team. If a company tried agile and had poor results, people will be suspicious of trying it again, citing examples of why it didn’t work. They might even ac- tively campaign against it.\n\nOrganizational culture is too frequently not considered when attempts are made to implement an agile process, leaving people wondering why it didn’t work as promised. It’s hard to change established processes, especially if indi- viduals feel they have a stake in the status quo. Each functional group devel- ops a subculture and processes that meet their needs. They’re comfortable with the way they work. Fear is a powerful emotion, and if it is not addressed, it can jeopardize the transition to agile. If team members feel that a new agile process threatens their jobs, they’ll resist the change.\n\nWe’ll talk speciﬁcally about how organizational culture affects testers work- ing in an agile environment. The bibliography contains resources that deal with other cultural aspects that may affect teams.\n\nQuality Philosophy\n\nConsider an organization’s quality philosophy in terms of how it determines the acceptable level of software quality. Does it tolerate poor quality? Does it take customers’ quality requirements into account, or is it just concerned with getting the product into the customers’ hands as fast as it can?\n\nWhen an organization lacks an overall quality philosophy and pressures teams to get the product out without regard to quality, testers feel the pinch. A team that tries to use agile development in such an environment faces an uphill battle.\n\nSome organizations have strong, independent test teams that wield a lot of power. These teams, and their managers, might perceive that agile develop- ment will take that power away. They might fear that agile runs contrary to their quality philosophy. Evaluate your organization’s quality philosophy and the philosophy of the teams that enforce it.\n\nORGANIZATIONAL CULTURE\n\nPeril: Quality Police Mentality\n\nIf an existing QA team has assumed the role of “Quality Police,” its members usually enforce quality by making sure code reviews are completed and bugs are religiously entered into the defect-tracking systems. They keep metrics about the number of bugs found, and then are charged with making the ﬁnal decision as to whether to release the product.\n\nWe’ve talked to testers who brag about accomplishments such as going over a development manager’s head to force a programmer to follow coding stan- dards. We’ve even heard of testers who spend their time writing bugs about requirements that aren’t up to their standards. This kind of attitude won’t ﬂy on a collaborative agile team. It fosters antagonistic behavior.\n\nAnother risk of the “Quality Police” role is that the team doesn’t buy into the concept of building quality in, and the programmers start using testers as a safety net. The team starts communicating through the bug-tracking system, which isn’t a very effective means of communicating, so the team never “jells.”\n\nRead on for ways to help avoid this peril.\n\nCompanies in which everyone values quality will have an easier time transi- tioning to agile. If any one group has assumed ownership of quality, they’ll have to learn to share that with everyone else on the team in order to succeed.\n\nWhole-Team Ownership of Quality In Chapter 1, “What Is Agile Testing, Anyway?,” we talked about the whole- team approach to quality. For many testers and QA teams, this means a mind shift from owning quality to having a participatory role in deﬁning and maintaining quality. Such a drastic shift in attitude is difﬁcult for many testers and QA teams.\n\nTesters who have been working in a traditional setting might have a hard time adjusting to their new roles and activities. If they’ve come from an orga- nization where development and QA have an adversarial relationship, it may be difﬁcult to change from being an afterthought (if thought of at all) to be- ing an integral part of the team. It can be difﬁcult for both programmers and testers to learn to trust each other.\n\nSkills and Adaptability Much has been observed about programmers who can’t adapt to agile prac- tices—but what about testers who are used to building test scripts according to a requirements document? Can they learn to ask the questions as the code\n\n39\n\n40\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nis being built? Testers who don’t change their approach to testing have a hard time working closely with the rest of the development team.\n\nTesters who are used to doing only manual testing through the user interface might not understand the automated approach that is intrinsic to agile de- velopment. These testers need a lot of courage in order to face their changing roles, because changing means developing new skill sets outside their com- fort zones.\n\nFactors that Help Although there are many cultural issues to consider, most QA teams have a focus on process improvement, and agile projects encourage continuous im- provements and adaptability through the use of tools like retrospectives. Most quality assurance professionals are eager to take what they’ve learned and make it better. These people are adaptable enough to not only survive, but to thrive in an agile project.\n\nIf your organization focuses on learning, it will encourage continual process improvement. It will likely adopt agile much more quickly than organiza- tions that put more value on how they react to crises than on improving their processes.\n\nIf you are a tester in an organization that has no effective quality philosophy, you probably struggle to get quality practices accepted. The agile approach will provide you with a mechanism for introducing good quality-oriented practices.\n\nTesters need time and training, just like everyone else who is learning to work on an agile project. If you’re managing a team that includes testers, be sure to give them plenty of support. Testers are often not brought in at the beginning of a greenﬁeld project and are then expected to just ﬁt into a team that has been working together for months. To help testers adjust, you may need to bring in an experienced agile testing coach. Hiring someone who has previ- ously worked on an agile team and can serve as a mentor and teacher will help testers integrate with the new agile culture, whether they’re transition- ing to agile along with an existing team or joining a new agile development team.\n\nSustainable Pace\n\nTraditional test teams are accustomed to fast and furious testing at the end of a project, which translates into working weekends and evenings. During this end-of-project testing phase, some organizations regularly ask their teams to\n\nORGANIZATIONAL CULTURE\n\nput in 50, 60, or more hours each week to try to meet a deadline. Organiza- tions often look at overtime as a measure of an individual’s commitment. This conﬂicts with agile values that revolve around enabling people to do their best work all the time.\n\nIn agile projects, you are encouraged to work at a sustainable pace. This means that teams work at a consistent pace that sustains a constant velocity that permits maintaining a high-quality standard. New agile teams tend be overly optimistic about what they can accomplish and sign up for too much work. After an iteration or two, they learn to sign up for just enough work so no overtime is needed to complete their tasks. A 40-hour week is the normal sustainable pace for XP teams; it is the amount of effort that, if put in week in and week out, allows people to accomplish the most work over the long haul while delivering good value.\n\nTeams might need to work for short bursts of unsustainable pace now and then, but it should be the exception, not the norm. If overtime is required for short periods, the whole team should be working extra hours. If it’s the last day of the sprint and some stories aren’t tested, the whole team should stay late to ﬁnish the testing, not just the testers. Use the practices and techniques recommended throughout this book to learn how to plan testing along with development and allow testing to “keep up” with coding. Until your team gets better at managing its workload and velocity, budget in extra time to help even out the pace.\n\nCustomer Relationships\n\nIn traditional software development, the relationship between the develop- ment teams and their customers is more like a vendor-supplier relationship. Even if the customer is internal, it can feel more like two separate companies than two teams working on a common goal of producing business value.\n\nAgile development depends on close involvement from customers or, at the very least, their proxies. Agile teams have invited customers to collaborate, work in the same locations if possible, and be intimately involved with the development process. Both sides learn each other’s strengths and weaknesses.\n\nThis change in the relationships needs to be recognized by both sides, and it doesn’t matter whether the customer is internal or external. An open rela- tionship is critical to the success of an agile project, where the relationship between the customer team and the development team is more like a part- nership than a vendor-supplier relationship.\n\n41\n\n42\n\nCHAPTER 3\n\nJanet’s Story\n\n(cid:2) CULTURAL CHALLENGES\n\nIn a large project I was on recently, the customer was actually a consortium of ﬁve companies, with one of them being the software company creating the software. Each of the companies supplied three of their best domain experts to represent their needs. There was regular communication between these on-site users and their own organizations, and they were also an integral part of the team they worked with on a daily basis.\n\nA steering committee with representatives from all ﬁve companies was kept in the loop on progress and was brought in when decisions needed to be made at a higher level.\n\nHaving a few representative domain experts, while keeping all stakeholders continually informed, is one approach to successful developer-customer col- laboration. We’ll talk about others in Part V. Customers are critical to the success of your agile project. They prioritize what will be built and have the ﬁnal say in the quality of the product. Testers work closely with customers to learn requirements and deﬁne acceptance tests that will prove that condi- tions of satisfaction are met. Testing activities are key to the development team-customer team relationship. That’s why testing expertise is so essential to agile teams.\n\nOrganization Size\n\nThe size of an organization can have great impact on how projects are run and how the structure of a company matures. The larger the organization, the more hierarchical the structure tends to be. As top-down communication channels are developed, the reporting structures become directive and less compatible with collaboration between technology and business.\n\nCommunication Challenges Some agile processes provide ways to facilitate inter-team communication. For example, Scrum has the “Scrum of Scrums,” where representatives from multiple teams coordinate on a daily basis.\n\nIf you work in a large organization where the test teams or other specialized resources are separate from the programming teams, work to ﬁnd ways to keep in constant touch. For example, if your database team is completely sep- arate, you need to ﬁnd a way to work closely with the database specialists in order to get what you need in a timely manner.\n\n—Janet\n\nChapter 16, “Hit the Ground Run- ning,” describes how one large organization uses functional analysts to mitigate prob- lems due to re- mote customers.\n\nChapter 15, “Tester Activities in Release or Theme Planning,” and Chapter 16, “Hit the Ground Run- ning,” talk about what testers can do to help with planning and co- ordinating with other teams.\n\nORGANIZATIONAL CULTURE\n\nAnother problem that tends to be more common in large companies is that customers might not be as accessible as they are in smaller companies. This is a big obstacle when you try to gather requirements and examples and seek to get customer involvement throughout the development cycle. One solution is to have testers or analysts with domain expertise act as customer proxies. Communication tools can help deal with such situations as well. Look for creative ways to overcome the problems inherent in big companies.\n\nConﬂicting Cultures within the Organization With large software development shops, agile development is often ﬁrst im- plemented in one team or just a few teams. If your agile team has to coordi- nate with other teams using other approaches such as phased or gated development, you have an extra set of challenges. If some of the external teams tend to be dysfunctional, it’s even harder. Even when an entire company adopts agile, some teams make the transition more successfully than others.\n\nYour team might also run into resistance from specialist teams that are feel- ing protective of their particular silos. Lisa talked to a team whose members could not get any help from their company’s conﬁguration management team, which was obviously a major obstacle. Some development teams are barred from talking directly to customers.\n\nIf third parties are working on the same system your team is working on, their cultures can also cause conﬂicts. Perhaps your team is the third party, and you’re developing software for a client. You will need to think about how to mitigate culture-based differences. Part V goes into more detail about working with other teams and third parties, but here are a few ideas to get you started.\n\nAdvanced Planning If you have to coordinate with other teams, you will need to spend time during release planning, or before the start of an iteration, to work with them. You need time to adapt your own processes to work with oth- ers’ processes, and they might need to change their processes to accommodate your requests. Consider arranging access to shared resources such as perfor- mance test specialists or load test environments, and plan your own work around others’ schedules. Your stakeholders might expect certain deliverables, such as formal test plans, that your own agile process doesn’t include. Some ex- tra planning will help you to work through these cultural differences.\n\nAct Now, Apologize Later We hesitate to make suggestions that might cause trouble, but often in a large organization, the bureaucratic wheels turn so\n\n43\n\n44\n\nCHAPTER 3\n\nChapter 4, “Team Logistics,” talks more about sepa- rate functional teams and how they affect the agile tester.\n\n(cid:2) CULTURAL CHALLENGES\n\nslowly that your team might have to ﬁgure out and implement its own solu- tions. For example, the team that couldn’t get cooperation from the conﬁgura- tion management team simply implemented its own internal build process and kept working on getting it integrated with the ofﬁcially sanctioned process.\n\nIf there aren’t ofﬁcial channels to get what you need, it’s time to get cre- ative. Maybe testers have never talked directly to customers before. Try to arrange a meeting yourself, or ﬁnd someone who can act as a customer proxy or go-between.\n\nEmpower Your Team\n\nIn an agile project, it is important for each development team to feel empow- ered to make decisions. If you’re a manager and you want your agile teams to succeed, set them free to act and react creatively. The culture of an organiza- tion must adapt to this change for an agile project to be successful.\n\nBARRIERS TO SUCCESSFUL AGILE ADOPTION BY TEST/QA TEAMS Any change faces barriers to success. Organizational culture, as we discussed in the previous section, might be the largest obstacle to overcome. Once or- ganizational culture has become well established, it’s very hard to change. It took time for it to form, and once in place, employees become committed to the culture, which makes it extremely resistant to alteration.\n\nThis section discusses speciﬁc barriers to adoption of agile development methods that can be encountered by your testers and QA teams.\n\nLoss of Identity\n\nTesters cling to the concept of an independent QA team for many reasons, but the main reason is fear, speciﬁcally:\n\n(cid:2) Fear that they will lose their QA identity (cid:2) Fear that if they report to a development manager, they will lose sup-\n\nport and programmers will get priority\n\n(cid:2) Fear that they lack the skills to work in an agile team and will lose\n\ntheir jobs\n\n(cid:2) Fear that when they’re dispersed into development teams they won’t\n\nget the support they need\n\n(cid:2) Fear that they, and their managers, will get lost in the new organization",
      "page_number": 78
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 87-94)",
      "start_page": 87,
      "end_page": 94,
      "detection_method": "topic_boundary",
      "content": "Chapter 4, “Team Logistics,” covers ideas that can be used to help peo- ple adapt.\n\nBARRIERS TO SUCCESSFUL AGILE ADOPTION BY TEST/QA TEAMS\n\nWe often hear of QA managers asking questions such as, “My company is im- plementing agile development. How does my role ﬁt in?” This is directly re- lated to the “loss of identity” fears.\n\nAdditional Roles\n\nWe know from experience that new teams are often missing specialists or ex- pertise that might be key to their success. Lisa’s team has run into obstacles so large that the only thing to do was sit back and ask, “What role are we missing on our team that is holding us back? What do we need? Another de- veloper, another tester, a database designer?” We all know that testing is a vast ﬁeld. Maybe you need someone experienced in testing on an agile team. Or maybe you need a performance testing specialist. It’s critical that you take the time to analyze what roles your product needs to be successful, and if you need to ﬁll them from outside the team, do it.\n\nIt’s critical that everyone already on the product team understand their role or ﬁgure out what their role is now that they’re part of a new agile team. Do- ing this requires time and training.\n\nLack of Training\n\nWe hosted a session in the “Conference within a Conference” at Agile 2007 that asked people what testing-related problems they were having on their agile teams. One of the attendees told us that they split up their test organiza- tion as advocated by the agile literature. However, they put the testers into development units without any training; within three months, all of the testers had quit because they didn’t understand their new roles. Problems like these can be prevented with the right training and coaching.\n\nWhen we started working with our ﬁrst agile teams, there weren’t many re- sources available to help us learn what agile testers should do or how we should work together with our teams. Today, you can ﬁnd many practitio- ners who can help train testers to adapt to an agile environment and help test teams make the agile transition. Local user groups, conferences, seminars, online instruction, and mailing lists all provide valuable resources to testers and managers wanting to learn. Don’t be afraid to seek help when you need it. Good coaching gives a good return on your investment.\n\nNot Understanding Agile Concepts\n\nNot all agile teams are the same. There are lots of different approaches to agile development, such as XP, Scrum, Crystal, FDD, DSDM, OpenUP, and various\n\n45\n\n46\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nmixes of those. Some self-titled “agile” teams are not, in our opinion, really practicing agile. Plenty of teams simply adopt practices that work for them regardless of the original source, or they invent their own. That’s ﬁne, but if they don’t follow any of the core agile values and principles, we question giv- ing them an agile label. Releasing every month and dispensing with docu- mentation does not equate to agile development!\n\nIf different team members have opposing notions of what constitutes “agile,” which practices they should use, or how those practices are supposed to be practiced, there’s going to be trouble. For example, if you’re a tester who is pushing for the team to implement continuous integration, but the pro- grammers simply refuse to try, you’re in a bad spot. If you’re a programmer who is unsuccessful at getting involved in some practices, such as driving de- velopment with business-facing tests, you’re also in for conﬂict.\n\nThe team must reach consensus on how to proceed in order to make a suc- cessful transition to agile. Many of the agile development practices are syner- gistic, so if they are used in isolation, they might not provide the beneﬁts that teams are looking for. Perhaps the team can agree to experiment with certain practices for a given number of iterations and evaluate the results. It could decide to seek external input to help them understand the practices and how they ﬁt together. Diverse viewpoints are good for a team, but everyone needs to be headed in the same direction.\n\nSeveral people we’ve talked to described the “mini-waterfall” phenomenon that often occurs when a traditional software development organization im- plements an agile development process. The organization replaces a six- month or year-long development cycle with a two- or four-week one, and just tries to squeeze all of the traditional SDLC phases into that short period. Nat- urally, they keep having the same problems as they had before. Figure 3-1 shows an “ideal” version of the mini-waterfall where there is a code-and-ﬁx phase and then testing—the testing comes after coding is completed but be- fore the next iteration starts. However, what really happens is that testing gets squeezed into the end of the iteration and usually drags over into the next iter- ation. The programmers don’t have much to ﬁx yet, so they start working on the next iteration. Before long, some teams are always an iteration “behind” with their testing, and release dates get postponed just as they always did.\n\nEveryone involved with delivering the product needs time and training to understand the concepts behind agile as well as the core practices. Experi- enced coaches can be used to give hands-on training in practices new to the team, such as test-driven development. In larger organizations, functional\n\nSee the bibliogra- phy for a link to more information about XP Radar charts.\n\nBARRIERS TO SUCCESSFUL AGILE ADOPTION BY TEST/QA TEAMS\n\n47\n\nRequirements\n\nRequirements\n\nRequirements\n\nRequirements\n\nCode\n\nCode & Fix\n\nCode\n\nCode & Fix\n\nCode\n\nCode & Fix\n\nTest\n\nTest\n\nTest\n\nIteration 1\n\nIteration 2\n\nIteration 3\n\nFigure 3-1 A mini-waterfall process\n\ntest managers can become practice leads and can provide support and re- sources so that testers learn how to communicate and collaborate with their new teams. Programmers and other team members need similar help from their functional managers. Strong leadership will help teams ﬁnd ways to mi- grate away from “mini-waterfall” to true collaborative development, where coding and testing are integrated into one process.\n\nXP has developed a radar chart to help teams determine their level of adapta- tion to key XP practices. They measure ﬁve different key practices: team, pro- gramming, planning, customer, and pairing, and they show the level of adaptation to practices by teams. Figure 3-2 shows two such charts. The chart\n\nProgramming\n\nProgramming\n\nTeam\n\nPlanning\n\nTeam\n\nPlanning\n\nPairing\n\nCustomer\n\nPairing\n\nCustomer\n\nFigure 3-2 XP Radar charts\n\n48\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\non the left shows successful adaptation, while the chart on the right shows that there are some problem areas.\n\nPast Experience/Attitude\n\nLots of people have been through changes that didn’t stick. Some develop- ment organizations have lived through a succession of the “methodology du jour.” They throw up their hands and wonder, “Why should we do it again?” People get stuck in their old, unsuccessful patterns. Even when they try something new, they might revert to bad old habits when under stress. The following are just a few examples of people resisting change due to past expe- rience and their perception of “the way things are”:\n\n(cid:2) A tester sat in his cube and wouldn’t talk with the programmers about problems he was having. He complained that programmers didn’t understand what he wanted.\n\n(cid:2) A tester couldn’t shake his existing attitude that programmers didn’t know how to write good code, or how to test it. His condescending at- titude was clear to all, and his credibility as a tester was challenged. (cid:2) A customer threw up his hands when the programmers did some-\n\nthing he didn’t like, because they “always” do what they want anyhow.\n\nWhen faced with a transition to agile development, people like this often leave without giving the new process a chance. Agile development isn’t for everyone, but training and time to experiment can help adjust attitudes. Ask everyone to be part of the solution, and work together to ﬁnd out what processes and prac- tices work best for their particular situations. The self-organizing team can be a powerful tool to use to reassure all members of the development team that they’re in control of their own destiny.\n\nCultural Differences among Roles\n\nEach new agile team member is making the transition from a different per- spective. Programmers are often used to writing production code and getting it released as quickly as possible. System administrators and database experts might be accustomed to working in their own silo, performing requests on their own schedule. Customers may never have talked directly with develop- ment team members. Testers might be used to coming in at the end of the project and not interacting much at all with programmers.\n\nIt’s no wonder a transition to agile can be scary. Teams can come up with rules and guidelines to help them communicate and work well together. For\n\nLisa’s Story\n\nINTRODUCING CHANGE\n\nexample, Lisa joined a new agile team whose rule was that if someone asked you to pair with her, you had to agree. You might not be able to do it right that minute, but as soon as you could free yourself up, you had to go help your teammate.\n\nIdentify what people doing different activities need, and ﬁnd ways to provide it. Customers need some way to know how development is progressing and whether their conditions of satisfaction are being met. Developers need to know business priorities and requirements. Testers need ways to capture ex- amples and turn them into tests. All team members want to feel they are val- ued, ﬁrst-class team members. Each team member also needs to feel safe and to feel free to raise issues and try new ideas. Understanding the viewpoint of each role helps teams through the transition.\n\nINTRODUCING CHANGE When implementing any change, be aware of the side effects. The ﬁrst stage may be chaos; your team isn’t sure what the new processes are, some groups are loyal to old ways, and some people are unsure and disruptive. People mis- take this chaotic stage for the new status quo. To avoid this, explain the change model up front and set expectations. Expect and accept perceived chaos as you implement agile processes. Find the areas of the most pain, and determine what practices will solve the problem so that you can get some im- mediate progress out of the chaos.\n\nTalk about Fears\n\nWhen you start iterative development, use retrospectives to provide people with a place to talk about their fears and a place in which they can give feed- back. Let people know that it’s normal to be fearful. Be open; teach them it is acceptable to say they are fearful or uncomfortable. Discuss each source of fear, learn from the discussion, make decisions, and move on. Fear is a com- mon response to change. Forcing people to do something they don’t want is detrimental to positive change. Lead by example.\n\nJanet and I each joined our ﬁrst XP teams at a time when many XP practitioners didn’t see any place for testers on an XP team. XP had a “Customer Bill of Rights” and a “Programmer Bill of Rights,” but the “Tester Bill of Rights” was conspicuously absent. Tip House and I came up with our own “Tester Bill of Rights” in order to give testers the support and courage to succeed on agile teams. Over the years, many testers have told us how much this helped them and their teams learn how testers work together with other team members. I don’t like too many rules, but they can\n\n49\n\n50\n\nCHAPTER 3\n\nChapter 18, “Cod- ing and Testing,” covers how testers and programmers work together throughout the development process.\n\n(cid:2) CULTURAL CHALLENGES\n\nbe a good thing when they help the team to overcome cultural barriers and to understand how to work in new ways. The following list presents a “Tester Bill of Rights.” We encourage you to use it to help testers integrate into agile teams.\n\nYou have the right to bring up issues related to testing, quality, and process at any time.\n\nYou have the right to ask questions of customers, programmers, and other team members and receive timely answers.\n\nYou have the right to ask for and receive help from anyone on the project teams, including programmers, managers, and customers.\n\nYou have the right to estimate testing tasks and have these included in story estimates.\n\nYou have the right to the tools you need to perform testing tasks in a timely manner.\n\nYou have the right to expect your entire team, not just yourself, to be responsi- ble for quality and testing.\n\nGive Team Ownership\n\nA critical success factor is whether the team takes ownership and has the abil- ity to customize its approach. People can change their attitudes and their perceptions if they are given the right help. Lisa was able to observe Mike Cohn work with her team as a coach. As a self-organizing team, the team had to identify and solve its own problems. Mike made sure they had the time and resources to experiment and improve. He made sure that the business understood that quality was more important than quantity or speed. Every team, even a self-organizing team, needs a leader who can effectively interact with the organization’s management team.\n\nCelebrate Success\n\nImplementing change takes time and can be frustrating, so be sure to cele- brate all successes your team achieves. Pat yourselves on the back when you meet your goal to write high-level test cases for all stories by the fourth day of the iteration. Get the team together for a trivia game or lunch when you’ve just delivered an iteration’s worth of work. Acknowledgment is important if you want a change to stick.\n\nIntegrating testers into development teams while letting them continue to re- port to a supportive QA manager is one way to ease the transition to agile de- velopment. Testers can ﬁnd ways to move from an adversarial relationship with programmers to a collaborative one. They can show how they can help\n\n—Lisa\n\nINTRODUCING CHANGE\n\nOvercoming Resistance to Agile\n\nMark Benander, a Quality Assurance Team Lead with Quickofﬁce, was on his fourth project on an agile team. The ﬁrst was a major rewrite of their entire application, with a team of eight developers, one tester, and no test auto- mation tool. He told us about his experiences in overcoming his concerns about agile development, especially about reporting to a development manager.\n\nWe were in a matrix management type of system, where a tester reports to a development manager, but the test manager is still ofﬁ- cially the supervisor. This comforted me somewhat, but the majority of the issues I expected to occur, such as being overruled whenever I found an issue, never did. My concern wasn’t that I’d really end up thinking like a developer and just releasing anything, but that my manager, who was not a tester, wouldn’t care as much, and might not back up my concerns with the application.\n\nUltimately, I think I ended up thinking slightly more like a developer, being less concerned about some of the small bugs. My better understanding of the application’s workings made me understand that the risk and cost of ﬁxing it was potentially much more risky than the beneﬁt. I believe that thinking like this isn’t a bad thing as long as we are always mindful of the end customer impact, not just the internal cost.\n\nThe corollary to my thinking more like a developer is that the developers began thinking more like testers. I’m actually a fan of the adversarial role of the tester, but in a relaxed way. I actually give the developers gold stars (the little sticker kind you used to get on your spelling test in second grade) when they implement an area of code that is especially solid and user friendly, and I give out pink stars when they “implement” a bug that is especially heinous. They groan when I come over, wondering what I’ve found now, and take great joy in “making my job boring” by testing their code themselves and giving me nothing to ﬁnd. Needless to say, you need the right group to be able to work with this kind of faux- hostile attitude. I’ve never been in another company where this would have worked, but I’ve never worked in another company where spontaneous nerf gunﬁghts broke out either.\n\nMark’s experience matches our own and that of many other testers we’ve met who’ve moved from traditional to agile development. If you’re a tester who just joined an agile team, keep an open mind and consider how your team- mates might have different viewpoints.\n\n51\n\n52\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nthe team understand the customers’ needs and deliver appropriate business value. They can host enjoyable activities to build good team interactions. Hav- ing cookies or chocolate available for teammates is a good way to get them to walk over to your desk! Patience and a sense of fun are big advantages.\n\nMANAGEMENT EXPECTATIONS When we think of challenges involved with adopting agile, we generally think of the actual team and the issues it encounters. However, for success- ful agile adoption, management buy-in is critical. In a phased project, man- agement gets regular updates and sign-off documents indicating the end of each phase. Upper-level managers might not understand how they’ll be able to gauge agile project progress. They might fear a loss of control or lack of “process.”\n\nCultural Changes for Managers\n\nIn an agile project, expectations change. In her previous life in waterfall projects, Janet remembers hearing comments like “this feature is 90% done” for weeks. Those types of metrics are meaningless in agile projects. There are no sign-offs to mark the end of a phase, and the “doneness” of a project isn’t measured by gates.\n\nMeaningful metrics are determined by each project team. In Scrum, sprint and release burndown charts track story completion and can give managers a measure of progress, but not any hard “dates” to use for billing customers. Test matrices can be used to track functionality test coverage but do not pro- vide sign-off documentation.\n\nThe other change that is difﬁcult for some managers to understand is letting the teams make their own technical decisions and manage their own work- loads. It’s no longer the manager who decides what is good enough. It is the team (which includes the customer) that deﬁnes the level of quality necessary to deliver a successful application.\n\nAgile teams estimate and work in smaller chunks of time than traditional teams. Rather than building in contingency, teams need to plan enough time for good design and execution in order to ensure that technical debt does not increase. Rather than managing the team’s activities at a low level, managers of agile teams focus on removing obstacles so that team members can do their best work.",
      "page_number": 87
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 95-104)",
      "start_page": 95,
      "end_page": 104,
      "detection_method": "topic_boundary",
      "content": "Janet’s Story\n\nMANAGEMENT EXPECTATIONS\n\nI asked the vice president in charge of a large agile project what he found to be the most difﬁcult part in the new agile environment from a management perspec- tive. He said that in a traditional waterfall type project, the reports all showed that everything was going according to plan until the very end, and then everything was in a panic state and “nothing worked.”\n\nIn the agile project, there were problems every day that needed to be addressed. Agile projects were more work on a consistent basis, but at least he was getting realistic reports. There were no surprises at the end of the project.\n\nBusiness stakeholders don’t like surprises. If they can be convinced to give the team enough time and resources to make the transition, they’ll ﬁnd that agile development lets them plan more accurately and achieve business goals in steady increments.\n\nSometimes it’s actually management that drives the decision to start doing agile development. The business leaders at Lisa’s company chose to try agile development in order to solve its software crisis. To be effective, they needed to have a different set of management expectations. They needed to be sensi- tive to the difﬁculty of making big changes, especially in an organization that wasn’t functioning well.\n\nIn all cases, managers need lots of patience during what might be a long tran- sition to a high-functioning agile team. It’s their job to make sure they pro- vide the necessary resources and that they enable every individual to learn how to do high-quality work.\n\nA Testing Manager’s Transition Tale\n\nTae Chang manages a team at DoubleClick that conducts end-to-end testing to ensure that all integration points, both up and downstream from the target of change, are covered. When they implemented Scrum, the development teams were reorganized into numerous application teams. Communication problems resulted in missed dependencies, so Tae’s team stepped up to help make sure problems were detected early.\n\nTae told us, “I believe agile development effectively magniﬁed the importance of cross-team communication and a coordinated end-to-end testing effort. It was not easy to work out a noninvasive (in terms of ﬁtting into current sprint\n\n53\n\n—Janet\n\n54\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nstructure) integration testing process; in fact, we are still tweaking it, but the overall beneﬁt of such a testing effort is apparent.” Their teams began to slide into the “mini-waterfall” trap. “In retrospect,” explains Tae, “one of the rea- sons for this is because we started with the agile process before internalizing agile practices.”\n\nKnowing that test automation and continuous integration were key, the teams at DoubleClick came up with new ideas, such as a specialized build and automation team to help the development teams cope. They brought in expert training to help them learn TDD and pair programming. They started taking steps to address their legacy system’s technical debt.\n\nTae’s team attends all of the sprint planning and review sessions, using both formal and informal communication to facilitate cross-functional communica- tion and coordinate testing and releases. He has found that it helps to keep meetings small, short, and relevant. He’s also a proponent of everyone sitting together in an open work area, as opposed to sectioned-off cubes.\n\nTae offers the following advice to testers making the transition to agile:\n\n“Agile development in general will initially frustrate testers in that they will not have access to full requirements documentation or deﬁned stages of testing. In my view of agile development, at any given moment, the tester will be engaged in tasks from multiple stages of the traditional development process. A tester can be sitting in a design session with engineering and product management (she should be taking notes here and start thinking of areas of risk where proposed code change will most likely impact) and on the same day work on automating and running test cases for the proposed changes. It's a change in mind-set, and some people are quicker to adapt than others.”\n\nTae’s experience mirrors our own and that of many other teams we’ve talked to.\n\nIf you’re a QA manager, be prepared to help your testers overcome their frus- trations with moving from deﬁned, sequential testing stages to fast-paced it- erations where they perform widely varied tasks on any given day. Help them adapt to the idea that testing is no longer a separate activity that occurs after development but that testing and coding are integrated activities.\n\nIf you’re a tester or other team member who isn’t getting the support you need in your transition to agile development, think about the difﬁculties your managers might be having in understanding agile development. Help them to understand what kinds of support you need.\n\nLisa’s Story\n\nMANAGEMENT EXPECTATIONS\n\nSpeaking the Manager’s Language\n\nWhat do business managers understand best? It’s the bottom line—the ROI (return on investment). To get the support you need from your manage- ment, frame your needs in a context that they can understand. Your team’s velocity translates into new features to make the business more proﬁtable. If you need time and funds to learn and implement an automated test tool, ex- plain to management that over time, automated regression tests will let your team go faster and deliver more functionality in each iteration.\n\nMy team needs big blocks of time to do risky refactoring, such as trying to split the code base into multiple modules that can be built independently. We also need time to upgrade to the latest versions of our existing tools, or to try out new tools. All of these tasks are difﬁcult to integrate into a two-week sprint when we’re also trying to deliver stories for the business.\n\nWe explained to our management that if these “engineering” tasks were put off too long, our technical debt would accumulate and our velocity would slow. The number of story points delivered each iteration would decline, and new stories would take longer to code. It would take longer and longer for the business to get the new features it needed in order to attract customers.\n\nIt was hard for the business to agree to let us devote a two-week iteration every six months to do the internal work we needed to manage our technical debt, but over time they could see the results in our velocity. Recently, one of the managers actually asked if we might need to have “engineering sprints” more often. Both the product and the team are growing, and the business wants to make sure we grow our infrastructure and tools, too.\n\nLike all members of an agile team, managers need to learn a lot of new con- cepts and ﬁgure out how they ﬁt as team members. Use big visible charts (or their virtual equivalents, as needed) to make sure they can follow the progress of each iteration and release. Look for ways to maximize ROI. Often, the busi- ness will ask for a complex and expensive feature when there is a simpler and quicker solution that delivers similar value. Make sure you explain how your team’s work affects the bottom line. Collaborate with them to ﬁnd the best way for stakeholders to express the requirements for each new feature.\n\nBudget limitations are a reality most teams face. When resources are limited, your team needs to be more creative. The whole-team approach helps. Per- haps, like Lisa’s team, your team has a limited budget to buy software, and so\n\n55\n\n—Lisa\n\n56\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nyou tend to look at open-source test automation tools that usually don’t have a large up-front purchase cost. A tool that uses the same language as the ap- plication won’t help the non-programming testers unless the programmers collaborate with them to automate the tests. Leveraging all of the expertise on the team helps you work within the business limitations.\n\nAs with all challenges your team encounters, experiment with new ways that the development team and management can help each other to build a valu- able product. At the same time, regardless of your development approach, you might have to make sure that some processes, such as conformance to audit requirements, receive the necessary attention.\n\nCHANGE DOESN’T COME EASY Agile development might seem fast-paced, but change can seem glacial. Teams that are new to agile will be slow to master some practices they’ve committed to using. We’ve met many testers who are frustrated that their “agile” develop- ment cycles are actually mini-waterfall cycles. These testers are still getting squeezed; it just happens more often. Iterations are over before stories can be tested. Programmers refuse or aren’t able to adopt critical practices such as TDD or pairing. The team leaves responsibility for quality in the hands of the testers, who are powerless to make changes to the process.\n\nThere’s no magic that you can use to get your team to make positive changes, but we have some tips for testers who want to get their teams to change in positive ways.\n\nBe Patient\n\nNew skills such as TDD are hard. Find ways to help your team get time to master them. Find changes you can make independently while you wait. For example, while programmers learn to write unit tests, implement a GUI test tool that you can use with minimal help. Help the team make baby steps. Re- member that when people panic, they go back to their old habits, even though those habits didn’t work. Focus on tiny positive increments.\n\nLet Them Feel Pain\n\nSometimes you just have to watch the train wreck. If your suggestions for im- provement were rebuffed, and the team fails, bring your suggestion up again and ask the team to consider trying it for a few iterations. People are most willing to change in the areas where they feel the most pain.\n\nSee the bibliogra- phy for some good resources on being an effec- tive change agent for your team.\n\nCHANGE DOESN’T COME EASY\n\nBuild Your Credibility\n\nYou might now be working with programmers who haven’t worked closely with testers before. Show them how you can help. Go to them with issues you’ve found rather than opening bug reports. Ask them to review code with you before they check it in. When they realize you’re contributing real value, they’re more likely to listen to your ideas.\n\nWork On Your Own Professional Development\n\nRead books and articles, go to user group meetings and conferences, and learn a new tool or scripting language. Start learning the language your ap- plication is coded in, and ask the programmers on your team if you can pair with them or if they’ll tutor you. Your coworkers will respect your de- sire to improve your skills. If your local user group is willing to listen to your presentation on agile testing, or a software newsletter publishes your automation article, your teammates might notice you have something worth hearing too.\n\nBeware the Quality Police Mentality\n\nBe a collaborator, not an enforcer. It might bug you if programmers don’t follow coding standards, but it’s not your job to make sure that they do so. Raise your issues with the team and ask for their help. If they ignore a critical problem that is really hurting the team, you might need to go to your coach or manager for help. But do that in a “please help me ﬁnd a solution” vein rather than a “make these people behave” one. If you’re seeing a problem, chances are high that others see it too.\n\nVote with Your Feet\n\nYou’ve been patient. You’ve tried every approach you can think of, but your management doesn’t understand agile development. The programmers still throw buggy, untestable code “over the wall,” and that code is released as is despite your best efforts, including working 14-hour days. Nobody cares about quality, and you feel invisible despite your best efforts. It might be time to look for a better team. Some teams are happy the way they are and simply don’t feel enough pain to want to change. Lisa worked on a team that thrived on chaos, because there were frequent opportunities to ﬁgure out why the server crashed and be a hero. Despite a successful project using agile prac- tices, they went back to their old habits, and Lisa ﬁnally gave up trying to change them.\n\n57\n\n58\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nSUMMARY In this chapter, we talked about how cultural issues can affect whether testers and their teams can make a successful transition to doing agile development.\n\n(cid:2) Consider organizational culture before making any kind of change. (cid:2) Testers have an easier time integrating into agile teams when their\n\nwhole organization values quality, but testers with a “quality police” mind-set will struggle.\n\n(cid:2) Some testers might have trouble adjusting to the “whole team” own- ership of quality, but a team approach helps overcome cultural differences.\n\n(cid:2) Customer teams and developer teams must work closely together, and we showed how testers can be key in facilitating this relationship. (cid:2) Large organizations that tend to have more isolated specialist teams face particular cultural challenges in areas such as communication and collaboration.\n\n(cid:2) Major barriers to success for testers for agile adoption include fear, loss of identity, lack of training, previous negative experiences with new development processes, and cultural differences among roles. (cid:2) To help introduce change and promote communication, we suggest encouraging team members to discuss fears and celebrating every success, no matter how small.\n\n(cid:2) Guidelines such as a “Tester Bill of Rights” give testers conﬁdence to raise issues and help them feel safe as they learn and try new ideas. (cid:2) Managers face their own cultural challenges, and they need to provide\n\nsupport and training to help testers succeed on agile teams.\n\n(cid:2) Testers can help teams accommodate manager expectations by pro- viding the information managers need to track progress and deter- mine ROI.\n\n(cid:2) Change doesn’t come easy, so be patient, and work on improving\n\nyour own skills so you can help your team.\n\nChapter 4\n\nTEAM LOGISTICS\n\nSelf-Organizing Team\n\nInvolving Other Teams\n\nIndependent QA Team\n\nEvery Team Member Has Equal Value\n\nBuilding a Team\n\nTeam Structure\n\nIntegrating Testers into Agile Project\n\nPerformance and Rewards\n\nAgile Project Teams\n\nWhat You Can Do\n\nTeam Logistics\n\nTester-Developer Ratio\n\nResources\n\nHiring an Agile Tester\n\nPhysical Logistics\n\nAgile teams stress that face-to-face communication is critical to the success of a project. They also encourage using the “whole-team” approach. What does this mean to the testers? This chapter talks about some of the issues involving team structure and physical logistics. There’s more to creating a cohesive team than just moving chairs and desks.\n\nTEAM STRUCTURE Having separate functional groups can make life difﬁcult for agile teams. Constant communication is critical. Team members need to work closely with one another, whether the work is done virtually or in the same physical location.\n\nWe use the terms “QA team” and “test team” interchangeably here. It can be argued whether “QA teams” are really doing quality assurance or not, but the term has become a common one attached to test teams, so we use it too.\n\n59\n\n60\n\nCHAPTER 4\n\n(cid:2) TEAM LOGISTICS\n\nIndependent QA Teams\n\nMany organizations, both large and small, think it is important to have an in- dependent QA or test team in order to get an honest opinion about the quality of a product. We’re often asked the questions, “Is there a place for a test organi- zation in the whole-team approach?” and “If so, what is its role?”\n\nSome of the reasons we’re given for wanting to keep the QA team separate from the development team are:\n\n(cid:2) It is important to have that independent check and audit role. (cid:2) The team can provide an unbiased and outside view relating to the\n\nquality of the product.\n\n(cid:2) If testers work too closely with developers, they will start to think like\n\ndevelopers and lose their customer viewpoint.\n\n(cid:2) If the testers and developers report to the same person, there is a dan- ger that the priority becomes delivering any code rather than deliver- ing tested code.\n\nTeams often confuse “independent” with “separate.” If the reporting struc- ture, budgets, and processes are kept in discrete functional areas, a division between the programmers and testers is inevitable. This can lead to friction, competition, and an “us versus them” attitude. Time is wasted on duplicate meetings, programmers and testers don’t share a common goal, and infor- mation sharing is nonexistent.\n\nThere are reasons for having a QA manager and an independent test team. However, we suggest changing the reasons as well as the structure. Rather than keeping the testers separate as an independent team to test the applica- tion after coding, think about the team as a community of testers. Provide a learning organization to help your testers with career development and a place to share ideas and help each other. If the QA manager becomes a prac- tice leader in the organization, that person will be able to teach the skills that testers need to become stronger and better able to cope with the ever-changing environment.\n\nWe don’t believe that integrating the testers with the project teams prevents testers from doing their jobs well. In fact, testers on agile teams feel very strongly about their role as customer advocate and also feel they can inﬂu- ence the rest of the team in quality thinking.\n\nTEAM STRUCTURE\n\nIntegration of Testers into an Agile Project\n\nThe whole-team approach in agile development has provoked many organi- zations that have adopted agile development to disband their independent QA teams and send their testers to work with the project groups. While this sounds great, some organizations have found that it doesn’t work as ex- pected. More than one organization has had most, if not all, of their testers quit when they found themselves on an agile development team with no idea what they should be doing.\n\nDevelopers get training on pair programming, test-driven development, and other agile practices, while testers often seem to get no training at all. Many organizations fail to recognize that testers also need training on pair testing, working with incomplete and changing requirements, automation, and all of the other new skills that are required. It’s critical that testers receive training and coaching so that they can acquire the skills and understanding that will help them succeed, such as how to work with customers to write business- facing tests. Programmers also might need coaching to understand the im- portance of business-facing tests and the whole-team approach to writing and automating tests.\n\nJanet has helped integrate several independent test teams into agile projects. She ﬁnds that it can take up to six months for most testers to start feeling conﬁdent about working with the new process.\n\nThe pairing of programmers and testers can only improve communication about the quality of the product. Developers often need to observe the be- havior of the application on the tester’s workstation if that behavior can’t be reproduced in the development environment. Testers can sometimes sit down with the developer to reproduce a problem more easily and quickly than they can by trying to record the steps in a defect. This interaction re- duces the time spent on non-oral communication.\n\nComments we’ve heard from testers on this subject include the following:\n\n(cid:2) “Being closer to the development of the product makes me a better\n\ntester.”\n\n(cid:2) “Going to lunch with developers builds a better team, one that wants\n\nand likes to work together.”\n\n61\n\n62\n\nCHAPTER 4\n\nLisa’s Story\n\n(cid:2) TEAM LOGISTICS\n\nOne major advantage of an integrated project team is that there’s only one budget and one schedule. There is no “testing” time to cut if all of the func- tionality is not ﬁnished. If there is no time to test a new feature, then there is no time to develop it in the ﬁrst place. The whole-team approach to taking responsibility for quality is very powerful, as we point out throughout this book.\n\nI once joined an XP team that had been depending solely on unit-level testing and had never had anyone in a tester role before. Their customer wasn’t all that happy with the results, so they decided to hire a tester. While I attended daily stand-ups, I wasn’t allowed to talk about testing tasks. Testing time wasn’t included in story estimates, and testing tasks weren’t part of iteration planning. Stories were marked “done” as soon as coding tasks were complete.\n\nAfter the team missed the release date, which was planned for after three two- week iterations, I asked the team’s coach to try the whole-team approach to test- ing. Testing tasks went up on the board along with coding tasks. Stories were no longer considered done until testing tasks were ﬁnished. Programmers took on testing tasks, and I was a full participant in daily stand-ups. The team had no more issues meeting the release plans they set.\n\nTesters need to be full-ﬂedged members of the development team, and test- ing tasks need to be given the same attention as other tasks. Again, the whole-team approach to testing goes a long way toward ensuring that testing tasks are completed by the end of each iteration and release. Be sure to use retrospectives to evaluate what testers need to integrate with their new agile team and what skills they might need to acquire. For example, testers might need more support from programmers, or from someone who’s an expert in a particular type of testing.\n\nA smart approach to planning the organizational changes for agile develop- ment makes all the difference to a successful transition. Ask the QA and devel- opment managers to ﬁgure out their own roles in the new agile organization. Let them plan how they will help their testers and developers be productive on the new agile teams. Provide training in agile practices that the team doesn’t know. Make sure all of the teams can communicate with each other. Provide a framework that lets each team learn as it goes, and the teams will ﬁnd a way to succeed.\n\n—Lisa",
      "page_number": 95
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 105-114)",
      "start_page": 105,
      "end_page": 114,
      "detection_method": "topic_boundary",
      "content": "See the bibliogra- phy for a link to some of Chris- tophe’s writings on managing agile teams.\n\nTEAM STRUCTURE\n\nTransitioning QA and Engineering Teams—Case Study\n\nChristophe Louvion is a CTO and agile coach for high-proﬁle Internet compa- nies. He told us about one experience he had while helping his company implement agile development. As the agile coach, he wanted to truly imple- ment agile development and avoid the common “small waterfall” mistake, where the developers spend a week writing code and the testers spend the next week testing it.\n\nHis company at the time was an organization of about 120 engineers, including the internal IT departments. Before transitioning to Scrum, the company was organized functionally. There were directors of QA and Engineering, and the idea of product-based teams was hard for management to accept. The manag- ers of these teams struggled with the following question: “What is my job now?” Christophe turned this around on the managers and said: “You tell me.”\n\nHe worked with the Engineering and QA managers to help them ﬁgure out what their jobs would be in the new agile environment. Only when they were able to speak with one voice did they all go to the teams and explain their ﬁndings.\n\nIn the new agile organization, managers deal with speciﬁc domain knowl- edge, resources, prioritization, and problems that arise. The Engineering and QA managers work hand-in-hand on a daily basis to resolve these types of issues. Christophe and the two managers looked at what prevented testers from being productive in the ﬁrst week of the two-week iteration and taught them how to help with design.\n\nFor the programmers, the question was “How do I make it so that the code is easy to test?” The engineers weren’t trained in continuous integration, because they were used to working in phased cycles. They needed lots of training in test-driven design, continuous integration, and other practices. Their managers ensured that they got this training.\n\nConﬁguration management (CM) experts were brought in to help with the build process. The CM team is separate from Engineering and QA at the com- pany, and it provides the framework for everything in the build process, including database objects, hardware, and conﬁgurations. Once the build process framework was implemented, integrating coding and testing was much easier to talk about.\n\nHaving management ﬁgure out their new roles ﬁrst, and then getting a build process framework in place with everything in source code control, were key to the successful transition to agile. Another success factor was having repre- sentatives from all teams—Engineering, QA, the CM, network, and the system administrator groups and product teams—participate in daily stand-ups and planning activities. This way, when testing issues came up, they could be addressed by everyone who could help. As Christophe says, their approach integrates everyone and puts a focus on testing.\n\n63\n\n64\n\nCHAPTER 4\n\n(cid:2) TEAM LOGISTICS\n\nAgile Project Teams\n\nAgile project teams are generally considered cross-functional, because each team has members from many different backgrounds. The difference be- tween a traditional cross-functional team and an agile team is the approach to the whole-team effort. Members are not just “representing” their func- tions in the team but are becoming true members of the team for as long as the project or permanent team exists (see Figure 4-1).\n\nBecause projects vary in size, project teams might vary in structure. Organi- zations with large projects or many projects that happen simultaneously are having success using a matrix-type structure. People from different func- tional areas combine to form a virtual team while still reporting back to their individual organizational structures. In a large organization, a pool of testers might move from project to project. Some specialists, such as security or per- formance testers, might be shared among several teams. If you’re starting up a project, identify all of the resources the project will need. Determine the number of testers required and the skill set needed before you start. The testers start with the team and keep working until the project is complete, and at that time they go on to the next project.\n\nWhile testers are part of the team, their day-to-day work is managed the same as the rest of the project team’s work. A tester can bounce new ideas off of the larger tester community, which includes testers on different project teams across a large organization. All testers can share knowledge and ideas. In organizations that practice performance reviews, the QA manager (if there is one) might drive the reviews and get input from the project team.\n\nFunctional Teams\n\nAgile Team\n\nProgrammers\n\nBusiness Analysts\n\nProgrammer\n\nDomain Expert\n\nTesters\n\nTester\n\nFigure 4-1 Traditional functional teams structure vs. agile team structure\n\nJanet’s Story\n\nPHYSICAL LOGISTICS\n\nAs with any new team, it takes a while for a team to jell. If the length of the project is short and the teams are constantly changing, the organization needs to be aware that the ﬁrst iteration or two of every project will include the new team members getting used to working with each other. Refactor your organization as needed, and remember to include your customers. The best teams are those that have learned to work together and have developed trust with one another.\n\nPHYSICAL LOGISTICS Many organizations that are thinking of adopting agile try to create project teams without co-locating the team in an open-plan environment. To sup- port agile values and principles, teams work better when they have ready ac- cess to all team members, easy visibility of all project progress charts, and an environment that fosters communication.\n\nTesters and customers sitting close to the programmers enable the necessary communalization. If logistics prohibit co-location, teams can be inventive.\n\nI worked in a team where space prevented all team members from sitting together. The programmers had an area where they could pair-program with ease, but the testers and the customer were seated in another area. At ﬁrst, it was the testers that made the trip to the storyboard area where the programmers sat to participate in stand-ups and whenever they had a question for one of the pro- grammers. Few of the programmers made the trip (about 50 feet) to the testers’ area. I started keeping a candy dish handy with treats and encouraged the devel- opers to take some as often as they wanted. But there was one rule—they needed to ask a question of one of the testers if they came for candy. Over time, the walk got shorter for all team members. No one side was doing all of the walk- ing, and communication ﬂourished.\n\nTeam size offers different types of challenges to the organization. Small teams mean small areas, so it is usually easier to co-locate members. Large teams might be spread globally, and virtual communication tools are needed. Co- locating large teams usually means renovating existing space, which some or- ganizations are reluctant to do. Understand your constraints, and try to ﬁnd solutions to the problems your team encounters rather than just accepting things as “the way it is.”\n\n65\n\n—Janet\n\n66\n\nCHAPTER 4\n\nJanet’s Story\n\n(cid:2) TEAM LOGISTICS\n\nOne team I worked on started in one corner of the ﬂoor, but expanded over the course of three years, gradually taking over 70% of the ﬂoor. Walls were taken down, ofﬁces removed, and large open areas created. The open areas and pods of teams worked well, but all the open space meant wall space was lost. Windows became story boards and whiteboards, and rolling whiteboards were ordered that could be used as teams needed them.\n\nCo-located teams don’t always live in a perfect world, and distributed teams have a another set of challenges. Distributed teams need technology to help them communicate and collaborate. Teleconferencing, video conferencing, webcams, and instant messaging are some tools that can promote real-time collaboration for teams in multiple locations.\n\nWhether teams are co-located or distributed, the same questions usually come up about what resources are needed on an agile team and how to ob- tain them. We’ll discuss these in the next section.\n\nRESOURCES New agile team members and their managers have lots of questions about the makeup of the team. Can we use the same testers that we had with our tradi- tional projects, or do we need to hire a different type of tester? How many testers will we need? Do we need people with other specialized skills? In this section, we talk a little about these questions.\n\nTester-Developer Ratio\n\nThere have been many discussions about the “right” ratio of the number of testers to the number of developers. This ratio has been used by organiza- tions to determine how many testers are needed for a project so that they can hire accordingly. As with traditional projects, there is no “right” ratio, and each project needs to be evaluated on its own. The number of testers needed will vary and depends upon the complexity of the application, the skill set of the testers, and the tools used.\n\nWe have worked on teams with a tester-developer ratio of anywhere from 1:20 to 1:1. Here are a couple of our experiences.\n\n—Janet\n\nJanet’s Story\n\nLisa’s Story\n\nRESOURCES\n\nI worked on a project with a 1:10 ratio that developed a message-handling sys- tem. There was very little GUI, and I manually tested that part of the application, looking at usability and how well it matched the customer’s expectations. The pro- grammers did all of the automated regression testing while I worked with them to validate the effectiveness of the test cases written. I pair-tested stories with the developers, including load testing speciﬁc stories.\n\nI never felt that I didn’t have enough time to do the testing I needed to, because the developers believed that quality was the whole team’s responsibility.\n\nI was once the only professional tester on a team of up to 20 programmers devel- oping a content management system on an Internet shopping website. The team began to get really productive when the programmers took on responsibility for both manual testing and test automation. One or two programmers wore a “tester hat” for each iteration, writing customer-facing tests ahead of coding and per- forming manual tests. Additional programmers picked up the test automation tasks during the iteration.\n\nConversely, my current team has had two testers for every three to ﬁve program- mers. The web-based ﬁnancial application we produce has highly complex busi- ness logic, is high risk, and test intensive. Testing tasks often add up to the same amount of time as programming tasks. Even with a relatively high tester–programmer ratio, programmers do much of the functional test automation and sometimes pick up manual testing tasks. Specialized testing tasks such as writing high-level test cases and detailed customer-facing tests are usually done by the testers.\n\nRather than focus on a ratio, teams should evaluate the testing skills they need and ﬁnd the appropriate resources. A team that takes responsibility for testing can continually evaluate whether it has the expertise and bandwidth it needs. Use retrospectives to identify whether there’s a problem that hiring more testers would solve.\n\nHiring an Agile Tester\n\nAs we discussed in Chapter 2, “Ten Principles for Agile Testers,” there are cer- tain qualities that make a tester suited to working on an agile team. We don’t want to go into a lot of detail about what kind of tester to hire, because every team’s need is different. However, we do believe that attitude is an important factor. Here’s a story of how Lisa’s team struggled to hire a new agile tester.\n\n67\n\n—Janet\n\n—Lisa\n\n68\n\nCHAPTER 4\n\nLisa’s Story\n\n(cid:2) TEAM LOGISTICS\n\nOur ﬁrst attempt at recruiting another tester was not very successful. The ﬁrst job posting elicited many responses, and we interviewed three candidates without ﬁnding a good ﬁt. The programmers wanted someone “techie,” but we also needed someone with the skills to collaborate with business people and help them to produce examples and requirements. We struggled to determine the con- tent of the job posting in order to attract candidates with the right attitude and mind-set.\n\nAfter soliciting opinions and suggestions from Janet and other colleagues in the agile testing community, we decided to look for a tester with the mind-set that is described in Chapter 2. We changed the job posting to include items such as these:\n\nExperience writing black box and GUI test cases, designing tests to mitigate risks, and helping business experts deﬁne requirements\n\nExperience writing simple SQL queries and insert/update statements and basic grasp of Oracle or another relational database\n\nAt least one year of experience with some scripting or programming language and/or open source test tools\n\nAbility to use basic Unix commands\n\nExperience collaborating with programmers and business experts\n\nExperience in context-based, exploratory, or scenario testing a plus\n\nAbility to work as part of a self-organizing team in which you determine your tasks on a daily basis in coordination with coworkers rather than waiting for work to be assigned to you\n\nThese requirements brought candidates more suited to an agile testing job. I pro- ceeded carefully with screening, ruling out people with a “quality police” mental- ity. Testers who pursued professional development and showed interest in agile development were more likely to have the right mind-set. The team needed someone who would be strong in the area of test tools and automation, so a pas- sion for learning was paramount.\n\nThis more creative approach to recruiting a tester paid off. At that time, it wasn’t easy to ﬁnd good “agile tester” candidates, but subsequent searches went more smoothly. We found that posting the tester position in less obvious places, such as a Ruby mailing list or the local agile user group, helped reach a wider range of suit- able candidates.\n\nHiring an agile tester taught me a lot about the agile testing mind-set. There are testers with very good skill sets who would be valuable to any traditional test team but would not be a good ﬁt on an agile team because of their attitude toward testing.\n\n—Lisa\n\nBUILDING A TEAM\n\nWe need to consider more than just the roles that testers and programmers perform on the team. No matter what role you’re trying to ﬁll, the most im- portant consideration is how that person will ﬁt on your team. With the agile whole-team approach, specialists on the team might be asked to step outside their areas of expertise and pitch in on other activities. Each team member needs to have a strong focus on quality and delivering business value. Con- sider more than just technical skills when you’re expanding your team.\n\nBUILDING A TEAM We’ve talked a lot about the whole-team approach. But changes like that don’t just happen. We get asked questions like, “How do we get the team to jell?” or “How do we promote the whole-team approach?” One of the big ones is: “How do we keep everyone motivated and focused on the goal of de- livering business value?”\n\nSelf-Organizing Team\n\nIn our experience, teams make the best progress when they’re empowered to identify and solve their own problems. If you’re a manager, resist the tempta- tion to impose all your good ideas on the team. There are problems, such as personnel issues, that are best solved by managers, and there are times a coach needs to provide strong encouragement and lead the team when it needs leadership. It takes time for a new agile team to learn how to prioritize and solve its problems, but it’s okay for the team to make mistakes and stum- ble a few times. We think a high-functioning team has to grow itself. If you’re a tester, you’re in a good position to help the team ﬁgure out ways to get fast feedback, use practices such as retrospectives to prioritize and address issues, and ﬁnd the techniques that help your team produce better software.\n\nInvolving Other Teams\n\nYou might need to get other teams on board to help your team succeed. Set up meetings; ﬁnd ways to communicate as much as possible. Use a Scrum of Scrums to keep multiple teams coordinated, or just get involved with the other teams. If you have to bring in an expert to help with security testing, pair with that expert and learn as much as you can, and help them learn about your project.\n\nIf teams are scattered in different locations and time zones, ﬁgure out how to get as much direct communication as possible. Maybe representatives from\n\n69\n\n70\n\nCHAPTER 4\n\nChapter 9, “Tool- kit for Business- Facing Tests that Support the Team,” gives exam- ples of tools that help remote teams collaborate.\n\n(cid:2) TEAM LOGISTICS\n\neach team can adjust their hours once or twice a week so that they can tele- conference once a week. Make a phone call instead of sending an email whenever possible. Lisa’s team adjusted its planning meeting times to include a remote team member who works late at night. They schedule meetings for a time where his day overlaps with the rest of the team’s day.\n\nEvery Team Member Has Equal Value\n\nEvery team member has equal value to the team. If testers or any other team members feel left out or less valued, the whole-team approach is doomed. Make sure testers are invited to all meetings. If you’re a tester and someone forgets to invite you to a meeting, invite yourself. Nontechnical testers might think they’ll be out of place or overwhelmed at a design meeting, but some- times they ask good questions that the techies didn’t think of.\n\nTesters have a right to ask for and get help. If you’re a tester stuck on an auto- mation problem, have the courage to ask a team member for help. That per- son might be busy right now, but he or she must commit to helping you in a reasonable amount of time. If you’re a manager or leader on your team, make sure this is happening, and raise the issue to the team if it’s not.\n\nPerformance and Rewards\n\nMeasuring and rating performance on an individual basis risks undermining team collaboration. We don’t want a programmer to feel she shouldn’t take on a testing task because she’s rated on delivering production code. We don’t want a system administrator to be so busy making sure her individual goals are met that she can’t help with a test environment problem.\n\nConversely, a good performer who was trying to work well with the team shouldn’t be knocked because the rest of the team didn’t pull together. This is a time when a manager needs to step up and help the team ﬁnd its way. If major bugs made it to production, nobody should blame the testers. Instead, the whole team should analyze what happened and start taking steps to pre- vent a recurrence.\n\nThe development team needs to keep the business needs in mind. Set goals that serve the business, increase proﬁtability, and make the customers hap- pier. Work closely with the business so that your successes help the whole company succeed.\n\nRead about the “Shout-Out Shoe- box” idea in Chap- ter 19, “Wrap Up the Iteration.”\n\nSUMMARY\n\nAs we mentioned in Chapter 3, “Cultural Challenges,” celebrate every suc- cess, however small. A celebration might be a high-ﬁve, a company-provided lunch, or maybe just leaving work early to socialize a bit. The ScrumMaster on Lisa’s team hands out gold stars at stand-up meetings for special accom- plishments. Acknowledge the people who help you and your team.\n\nTeams can ﬁnd novel ways to recognize each other’s contributions. Iteration review and demonstration meetings, where both the development team and customer team are present, are a good setting for recognizing both individual and team achievements.\n\nWhat Can You Do?\n\nIf you’re a new tester on an agile team, especially a new agile team, what can you do to help the team overcome organizational challenges and succeed? How can you ﬁt in with the team and contribute your particular skills and experience?\n\nPut the ten principles we described in Chapter 2 to work. Courage is espe- cially important. Get up and go talk to people; ask how you can help. Reach out to team members and other teams with direct communication. Notice impediments and ask the team to help remove them.\n\nAgile development works because it gets obstacles out of our path and lets us do our best work. We can feel proud and satisﬁed, individually and as a team. When we follow agile principles, we collaborate well, use feedback to help im- prove how we work, and always look for new and better ways to accomplish our goals. All this means we can continually improve the quality of our product.\n\nSUMMARY In this chapter, we looked at ways to build a team and a structure for success- ful agile testing and development.\n\n(cid:2) Consider the importance of team structure; while testers might need an independent mind-set, putting them on a separate team can be counterproductive.\n\n(cid:2) Testers need access to a larger community of testers for learning and trying out new ideas. QA teams might be able to create this commu- nity within their organization.\n\n71\n\n72\n\nCHAPTER 4\n\n(cid:2) TEAM LOGISTICS\n\n(cid:2) It is important for the whole team to be located together, to foster collaboration; if the team is distributed, provide tools to promote communication. (cid:2) Hire for attitude. (cid:2) There is no right tester–developer ratio. The right answer is, “It de-\n\npends on your situation.”\n\n(cid:2) Teams need to self-organize, identify and ﬁnd solutions to their own problems, and look for ways to improve. They can’t wait for someone to tell them what to do.\n\n(cid:2) Management should reward performance in a way that promotes the team’s effort to deliver business value but not penalize good individ- ual performance if the team is struggling.\n\n(cid:2) Testers can use agile principles to improve their own skills and in- crease their value to the team. They need to be proactive and ﬁnd ways that they can contribute.",
      "page_number": 105
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 115-122)",
      "start_page": 115,
      "end_page": 122,
      "detection_method": "topic_boundary",
      "content": "Chapter 5\n\nTRANSITIONING TYPICAL PROCESSES\n\nSeeking Lightweight Processes\n\nLean Measurements\n\nAudits\n\nFrameworks, Models, and Standards\n\nExisting Processes\n\nMetrics\n\nWhy Do We Need Metrics?\n\nWhat Not to Do with Metrics\n\nCommunicating\n\nMetrics ROI\n\nTransitioning Typical Processes\n\nTest Strategy vs. Test Plan\n\nTest Planning\n\nTraceability\n\nWhy Use a DTS?\n\nDefect Tracking\n\nWhy Not?\n\nDefect Tracking Tools\n\nThere are many processes in a traditional project that don’t transition well to agile because they require heavyweight documentation or are an inherent part of the phased and gated process and require sign-offs at the end of each stage. Like anything else, there are no hard and fast rules for transitioning your processes to a more agile or lightweight process. In this chapter, we discuss a few of those processes, and give you alternatives and guidance on how to work with them in an agile project. You’ll ﬁnd more examples and details about these alter- natives in Parts III, IV, and V.\n\nSEEKING LIGHTWEIGHT PROCESSES When teams are learning how to use agile processes, some of the more tradi- tional processes can be lost in the shufﬂe. Most testers who are used to working\n\n73\n\n74\n\nCHAPTER 5\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nwith traditional phased and gated development methodologies are accustomed to producing and using metrics, recording defects in a formal defect tracking system, and writing detailed test plans. Where do those ﬁt in agile development?\n\nMany software organizations must comply with audit systems or quality pro- cess models. Those requirements don’t usually disappear just because you start using agile development practices. In fact, some people worry that agile development will be incompatible with such models and standards as CMMI and ISO 9000.\n\nIt might be more fun to talk about everything that’s new and different when testing on an agile project, but we still need ways to measure progress, track de- fects, and plan testing. We also need to be prepared to work with our organiza- tion’s quality models. The key is to keep these processes lightweight enough to help us deliver value in a timely manner. Let’s start by looking at metrics.\n\nMETRICS Metrics can be controversial, and we spend a lot of time talking about them. Metrics can be a pit of wasted effort, numbers for the sake of numbers. They are sometimes used in harmful ways, although they don’t have to be bad. They can guide your team and help it to measure your team’s progress to- ward its goals. Let’s take a look at how to use metrics to help agile testers and their teams.\n\nLean Measurements\n\nLean software development practitioners look for ways to reduce the number of measurements and ﬁnd measurements that will drive the right behaviors. Implementing Lean Software Development: From Concept to Cash, by Mary and Tom Poppendieck, is an excellent resource that teaches how to apply lean initiatives to your testing and development efforts.\n\nAccording to the Poppendiecks [2007], a fundamental lean measurement is the time it takes to go “from concept to cash,” from a customer’s feature re- quest to delivered software. They call this measurement “cycle time.” The fo- cus is on the team’s ability to “repeatedly and reliably” deliver new business value. Then the team tries to continuously improve their process and reduce the cycle time.\n\nMeasurements such as cycle time that involve the whole team are more likely to drive you toward success than are measures conﬁned to isolated roles or\n\nMETRICS\n\ngroups. How long does it usually take to ﬁx a defect? What can the team do to reduce that latency, the amount of time it takes? These types of metrics en- courage collaboration in order to make improvements.\n\nAnother lean measurement the Poppendiecks explain in their book is ﬁnan- cial return. If the team is developing a proﬁtable product, it needs to under- stand how it can work to achieve the most proﬁt. Even if the team is developing internal software or some other product whose main goal isn’t proﬁt, it still needs to look at ROI to make sure it is delivering the best value. Identify the business goals and ﬁnd ways to measure what the team delivers. Is the company trying to attract new customers? Keep track of how many new accounts sign on as new features are released.\n\nLean development looks for ways to delight customers, which ought to be the goal for all software development. The Poppendiecks give examples of simple ways you can measure whether your customers are delighted.\n\nWe like the lean metrics, because they’re congruent with our goal to deliver business value. Why are we interested in metrics at all? We’ll go into that in the next section.\n\nWhy We Need Metrics\n\nThere are good reasons to collect and track metrics. There are some really bad ones too. Anyone can use good metrics in terrible ways, such as using them as the basis for an individual team member’s performance evaluation. However, without metrics, how do you measure your progress?\n\nWhen metrics are used as guideposts—telling the team when it’s getting off track or providing feedback that it’s on the right track—they’re worth gath- ering. Is our number of unit tests going up every day? Why did the code cov- erage take a dive from 75% to 65%? It might have been a good reason— maybe we got rid of unused code that was covered by tests. Metrics can alert us to problems, but in isolation they don’t usually provide value.\n\nMetrics that measure milestones along a journey to achieve team goals are useful. If our goal is to increase unit test code coverage by 3%, we might run the code coverage every time we check in to make sure we didn’t slack on unit tests. If we don’t achieve the desired improvement, it’s more important to ﬁgure out why than to lament whatever amount our bonus was reduced as a result. Rather than focus on individual measurements, we should focus on the goal and the trending toward reaching that goal.\n\n75\n\n76\n\nCHAPTER 5\n\nLisa’s Story\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nMetrics help the team, customers included, to track progress within the iter- ation and within the release or epic. If we’re using a burndown chart, and we’re burning up instead of down, that’s a red ﬂag to stop, take a look at what’s happening, and make sure we understand and address the problems. Maybe the team lacked important information about a story. Metrics, in- cluding burndown charts, shouldn’t be used as a form of punishment or source of blame. For example, questions like “Why were your estimates too low?” or “Why can’t you ﬁnish all of the stories?” would be better coming from the team and phrased as “Why were our estimates so low?” and “Why didn’t we get our stories ﬁnished?”\n\nMetrics, used properly, can be motivating for a team. Lisa’s team tracks the number of unit tests run in each build. Big milestones—100 tests, 1000 tests, 3000 tests—are a reason to celebrate. Having that number of unit tests go up every day is a nice bit of feedback for the development and customer teams. However, it is important to recognize that the number itself means nothing. For example, the tests might be poorly written, or to have a well tested prod- uct, maybe we need 10,000 tests. Numbers don’t work in isolation.\n\nPierre Veragen told me about a team he worked on that was allergic to metrics. The team members decided to stop measuring how much code their tests cov- ered. When they decided to measure again after six months, they were stunned to discover the rate had dropped from 40% to 12%.\n\nHow much is it costing you to not use the right metrics?\n\nWhen you’re trying to ﬁgure out what to measure, ﬁrst understand what problem you are trying to solve. When you know the problem statement, you can set a goal. These goals need to be measurable. “Reduce average response time on the XYZ application to 1.5 seconds with 20 concurrent users” works better than “Improve the XYZ application performance.” If your goals are measurable, the measurements you need to gather to track the metrics will be obvious.\n\nRemember to use metrics as a motivating force and not for beating down a team’s morale. This wisdom bears repeating: Focus on the goal, not the met- rics. Maybe you’re not using the right metrics to measure whether you’re achieving your team’s objectives, or perhaps you’re not interpreting them in context. An increased number of defect reports might mean the team is do- ing a better job of testing, not that they are writing more buggy code. If your\n\n—Lisa\n\nLisa’s Story\n\nMETRICS\n\nmetrics aren’t helping you to understand your progress toward your goal, you might have the wrong metrics.\n\nWhat Not to Do with Metrics\n\nMark Twain popularized the saying, which he attributed to Benjamin Dis- raeli, “There are three kinds of lies: lies, damned lies, and statistics.” Measur- able goals are a good thing; if you can’t gauge them in some way, you can’t tell if you achieved them. On the other hand, using metrics to judge individual or team performance is dangerous. Statistics by themselves can be twisted into any interpretation and used in detrimental ways.\n\nTake lines of code, a traditional software measuring stick. Are more lines of code a good thing, meaning the team has been productive, or a bad thing, meaning the team is writing inefﬁcient spaghetti-style code?\n\nWhat about number of defects found? Does it make any sense to judge testers by the number of defects they found? How does that help them do their jobs better? Is it safe to say that a development team that produces a higher num- ber of defects per lines of code is doing a bad job? Or that a team that ﬁnds more defects is doing a good job? Even if that thought holds up, how moti- vating is it for a team to be whacked over the head with numbers? Will that make the team members start writing defect-free code?\n\nCommunicating Metrics\n\nWe know that whatever we measure is bound to change. How many tests are running and passing? How many days until we need a “build on the shelf”? Is the full build passing? Metrics we can’t see and easily interpret aren’t worth having. If you want to track the number of passing tests, make sure that met- ric is visible in the right way, to the right people. Big visible charts are the most effective way of displaying metrics we know.\n\nMy previous team had goals concerned with the number of unit tests. However, the number of unit tests passing wasn’t communicated to anyone; there were no big visible charts or build emails that referred to that number. Interestingly, the team never got traction on automating unit tests.\n\nAt my current company, everyone in the company regularly gets a report of the number of passing tests at the unit, behind-the-GUI, and GUI levels (see Tables 5-1 and 5-2 for examples). Business people do notice when that number goes down instead of up. Over time, the team has grown a huge number of useful tests.\n\n77\n\n—Lisa\n\n78\n\nCHAPTER 5\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nTable 5-1 Starting and Ending Metrics\n\nMetric\n\nAt Start\n\nAt End\n\nNCSS – Whitney\n\n69943\n\n69887\n\nNCSS – Ghidrah\n\n41044\n\n41978\n\nNumber of JUnit tests\n\n3001\n\n3062\n\nNumber of Canoo/Watir assertions\n\n3215\n\n3215\n\nNumber of FitNesse assertions\n\n57319\n\n61585\n\nTable 5-2 Daily Build Results\n\nDate\n\nBuild Result\n\nFriday 1/25/2008\n\nPassed 3026 JUnits\n\nMonday 1/28/2008\n\nPassed 3026 JUnits\n\nTuesday 1/29/2008\n\nPassed 3027 JUnits\n\nWednesday 1/30/2008\n\nPassed 3033 JUnits\n\nThursday 1/31/2008\n\nPassed 3040 JUnits\n\nFriday 2/1/2008\n\nPassed 3058 JUnits\n\nMonday 2/4/2008\n\nPassed 3059 JUnits\n\nTuesday 2/5/2008\n\nPassed 3060 JUnits\n\nWednesday 2/6/2008\n\nPassed 3062 JUnits\n\nThursday 2/7/2008\n\nPassed 3062 JUnits\n\nAre your metrics worth the trouble? Don’t measure for the sake of producing numbers. Think about what you’ll learn from those numbers. In the next section, we consider the return on investment you can expect from metrics.\n\nMetrics ROI\n\nWhen you identify the metrics you need, make sure you can obtain them at a reasonable cost. If your continual build delivers useful numbers, it delivers good value. You’re running the build anyway, and if it gives us extra informa- tion, that’s gravy. If you need a lot of extra work to get information, ask your- self if it’s worth the trouble.\n\nLisa’s team went to a fair amount of trouble to track actual time spent per story versus estimated time. What did they learn other than the obvious fact\n\nDEFECT TRACKING\n\nthat estimates are just that? Not much. Some experienced teams ﬁnd they can dispense with the sprint burndown chart because the task board gives them enough information to gauge their progress. They can use the time spent estimating tasks and calculating the remaining hours on more pro- ductive activities.\n\nThis doesn’t mean we recommend that you stop tracking these measure- ments. New teams need to understand their velocity and burndown rate, so that they can steadily improve.\n\nDefect rates are traditional software metrics, and they might not have much value on a team that’s aiming for zero defects. There’s not much value in knowing the rate of bugs found and ﬁxed during development, because ﬁnd- ing and ﬁxing them is an integral part of development. If a tester shows a de- fect to the programmer who’s working on the code, and a unit test is written and the bug is ﬁxed right away, there’s often no need to log a defect. On the other hand, if many defects reach production undetected, there can be value in tracking the number to know if the team improves.\n\nWhen it started to rewrite its buggy legacy application, Lisa’s team set a goal of no more than six high-severity bugs in new code reported after the code is in production over a six-month period. Having a target that was straightfor- ward and easy to track helped motivate the team to ﬁnd ways to head bugs off during development and exceed this objective.\n\nFigure each metric’s return on investment and decide whether to track or maintain it. Does the effort spent collecting it justify the value it delivers? Can it be easily communicated and understood? As always, do what works for your situation. Experiment with keeping a particular metric for a few sprints and evaluate whether it’s paying off.\n\nOne common metric that relates to software quality is the defect rate. In the next section, we look at reasons to track defects, or to not track defects, and what we can learn from them.\n\nDEFECT TRACKING One of the questions that are asked by every new agile team is, “Do we still track bugs in a defect tracking system?” There’s no simple answer, but we’ll give you our opinion on the matter and offer some alternatives so that you can determine what ﬁts your team.\n\n79\n\n80\n\nCHAPTER 5\n\nJanet’s Story\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nWhy Should We Use a Defect Tracking System (DTS)?\n\nA lot of us testers have used defect tracking as the only way to communicate the issues we saw, and it’s easy to keep using the tools we are familiar with. A DTS is a convenient place to keep track of not only the defect but the priori- ties, severities, and status, and to see who it is assigned to. Many agile practi- tioners say that we don’t need to do this anymore, that we can track defects on cards or some other simple mechanism. We could write a test to show the failure, ﬁx the code, and keep the test in our regression suite.\n\nHowever, there are reasons to keep using a tool to record defects and how they were ﬁxed. Let’s explore some of them now.\n\nConvenience One of the concerns about not keeping a defect tracking system is that there is no place to keep all of the details of the bug. Testers are used to recording a bug with lots of information, such as how to reproduce it, what environment it was found in, or what operating system or browser was used. All of this in- formation cannot ﬁt on a card, so how do you capture those details? If you are relying only on cards, you also need conversation. But with conversation, de- tails get lost, and sometimes a tester forgets exactly what was done—especially if the bug was found a few days prior to the programmer tackling the issue.\n\nA DTS is also a convenient place to keep all supplemental documentation, such as screen prints or uploaded ﬁles.\n\nKnowledge Base We have heard reasons to track defects such as, “We need to be able to look at old bug reports.” We tried to think of reasons why you would ever need to look at old bug reports, and as we were working on this chapter, Janet found an example.\n\nWhen I was testing the pre-seating algorithm at WestJet, I found an anomaly. I asked Sandra, another tester, if she had ever come across the issue before. San- dra vaguely recalled something about it but not exactly what the circumstances were. She quickly did a search in Bugzilla and found the issue right away. It had been closed as invalid because the business had decided that it wasn’t worth the time it would take to ﬁx it, and the impact was low.\n\nBeing able to look it up saved me from running around trying to ask questions or reentering the bug and getting it closed again. Because the team members sit",
      "page_number": 115
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 123-130)",
      "start_page": 123,
      "end_page": 130,
      "detection_method": "topic_boundary",
      "content": "Lisa’s Story\n\nDEFECT TRACKING\n\nclose to each other, our talking led to another conversation with the business analyst on the team. This conversation sparked the idea of a FAQ page, an out- standing issues list, or something along that line that would provide new testers a place to ﬁnd all of the issues that had been identiﬁed but for which the decision had been made not to address them.\n\nThis story shows that although the bug database can be used as a knowledge base, there might be other mechanisms for keeping business decisions and their background information. If an issue is old enough to have been lost track of, maybe we should rewrite it and bring it up again. The circumstances may have changed, and the business might decide it is now worthwhile to ﬁx the bug.\n\nThe types of bugs that are handy to keep in a DTS are the ones that are inter- mittent and take a long time to track down. These bugs present themselves infrequently, and there are usually gaps in time during which the investiga- tion stalls for lack of information. A DTS is a place where information can be captured about what was ﬁgured out so far. It can also contain logs, traces, and so on. This can be valuable information when someone on the team ﬁ- nally has time to look at the problem or the issue becomes more critical.\n\nThe information in bug reports can be used later for several purposes. Here’s a story from Lisa’s team on how it uses its information.\n\nOne developer from our team serves on a “production support” rotation for each iteration. Production support requests come in from the business side for manual ﬁxes of past mistakes or production problems that need manual intervention. The “production support person” researches the problem and notes whatever was done to ﬁx it in the bug report. These notes usually include a SQL statement and information about the cause. If anyone encounters the same error or situation later, the solution can be easily found in the DTS. If certain types of problems seem to occur frequently, the team can use the DTS for research and analysis. Even though our team is small, we deal with a lot of legacy code, and we can’t rely on people’s memory to keep track of every problem and ﬁx.\n\nRemembering the cause of defects or what was done to fulﬁll a special re- quest is even harder when the team is particularly large or isn’t co-located. Customers might also be interested in the solutions to their problems.\n\n81\n\n—Janet\n\n—Lisa\n\n82\n\nCHAPTER 5\n\nChapter 18, “Cod- ing and Testing,” explores metrics related to defect rates.\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nLarge or Distributed Teams If projects are so large that defects found by one team might affect other teams, a DTS is probably a good choice. Of course, to be useful it needs to be accessible to all members of the team. Face-to-face communication is always our ﬁrst choice, but when circumstances make that impractical, we need aids such as a DTS.\n\nCustomer Support When there are defects that have been reported by the customer after the re- lease, the customer usually wants to know when they’ve been ﬁxed. It’s in- valuable for the help desk or technical support to know what was ﬁxed in a given release. They can also ﬁnd defects that are still outstanding at release time and let the customers know. A DTS makes it much simpler to pull this information together.\n\nMetrics There are reasons to track defect rates. There are also reasons why you wouldn’t track a defect. For example, we don’t think that a bug should be counted as a defect if it never makes it out of the iteration. This, of course, brings up another discussion about what should we track and why, but we won’t discuss that here.\n\nTraceability Another reason we’ve heard for having a DTS is traceability, linking defects to test cases. We’re not sure that this is a valid reason. Not all defects are linked to test cases, nor should they be. For example, errors like spelling mis- takes might not need speciﬁc test cases. Maybe the product was not intuitive to use; this is a very real bug that often goes unreported. How do you write a test to determine if something is usable? Exploratory testing might ﬁnd bugs in edge conditions that are not worth the effort of creating automated tests.\n\nIf it is an automated test case that caught a bug, then the need to record that defect is further reduced, because it will be caught again if ever reintroduced. The need for traceability is gone. So, maybe we don’t need to track defects.\n\nWhy Shouldn’t We Use a DTS?\n\nAgile and Lean provide us with practices and principles that help reduce the need for a DTS. If the process is solid, and all of the people are committed to delivering a quality product, defects should be rare and very simply tracked.\n\nIn Chapter 15, “Coding and Test- ing,” we’ll explain how tester and programmers work together on bugs.\n\nJanet’s Story\n\nAntony will share his ideas about the hidden back- log when we cover iteration planning in Chap- ter 18, “Coding and Testing.”\n\nDEFECT TRACKING\n\nAs a Communication Tool Defect tracking systems certainly don’t promote communication between programmers and testers. They can make it easy to avoid talking directly to each other.\n\nWaste of Time and Inventory We tend to put lots of information into the DTS in addition to all of the steps to reproduce the defect. Depending on the bug, it can take a long time to write these steps so that the programmer can reproduce it as well. Then there is the triage, and someone has to make comments, interpret the defect, at- tempt to reproduce it, (ideally) ﬁx it, write more comments, and assign it back to the person who reported it. Finally, the ﬁx can be veriﬁed. This whole cycle can double if the programmer misunderstood the problem in the ﬁrst place. The cost of a single defect report can become exorbitant.\n\nHow much easier would it be if we as testers could just talk to the program- mer and show what we found, with the developer then ﬁxing the defect right away? We’ll talk more about that later.\n\nDefects in a DTS become a queue or a mini product backlog. According to lean principles, this inventory of defects is a waste. As a team, we should be thinking of ways to reduce this waste.\n\nIn 2004, Antony Marcano, author of TestingReﬂections.com, wrote a blog post about the idea of not using a bug-tracking system. When it was discussed on mail- ing lists, he was ﬂamed by many testers as introducing something similar to heresy. He ﬁnds he has a different reception now, because the idea is making its way into the mainstream of agile thinking.\n\nHe suggests that bug-tracking systems in agile teams are just “secret backlogs.”\n\nDefect Tracking Tools\n\nIf you do decide to use a DTS, choose it carefully. Understand your needs and keep it simple. You will want everyone on the team to use it. If it becomes overhead or hard to use, people will ﬁnd ways to work around it. As with all tools used by your agile development team, you should consider the whole team’s opinion. If anyone from the customer team enters bug reports, get his or her opinion too.\n\n83\n\n—Janet\n\n84\n\nCHAPTER 5\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nOne of the simplest tools that Janet has used is Alcea’s FIT IssueTrack. It is conﬁgurable, does not make you follow a predeﬁned process, and is easy to get metrics out of. Do your homework and ﬁnd the tool that works for you. There are a variety of open source defect-tracking systems, hosted systems, and integrated enterprise systems available.\n\nWhether or not you use a DTS, you want to make defects as visible as possible.\n\nWe use a commercial DTS, but we ﬁnd value in keeping bugs visible. We color-code bugs and include them as tasks in our story board, shown in Figure 5-1. Yellow cards denote normal bugs, and red cards denote either high production bugs or “test stopper” development bugs—both categories need to be addressed right away. A quick look at the board lets us see how many bugs are in the To Do, WIP, Verify and Done columns. Other cards are color-coded as well: blue for story cards, green for test task cards, and white for development tasks. Striped cards are for tasks added after iteration planning. Yellow and red bug cards stand out easily.\n\nFigure 5-1 Story board with color-coded cards\n\nDuring the time we were writing this book, my team converted to a virtual story board because one of our team members became a remote team member, but we retained this color-coding concept.\n\n—Lisa\n\nLisa’s Story\n\nDEFECT TRACKING\n\nWe usually recommend experimenting with different tools, using each one for a few iterations, but this is trickier with bug-tracking systems, because you need to port all of the bugs that are in one system to the new one that you’re trying on for size. Spend some time thinking about what you need in a DTS, what purposes it will serve, and evaluate alternatives judiciously.\n\nMy team used a web-based DTS that was basically imposed upon it by manage- ment. We found it somewhat cumbersome to use, lacking in basic features such as time-stamping updates to the bug reports, and we chafed at the license restric- tions. We testers were especially frustrated by the fact that our license limited us to three concurrent users, so sessions were set to time out quickly.\n\nThe team set aside time to evaluate different DTS alternatives. At ﬁrst, the selec- tion seemed mind-boggling. However, we couldn’t ﬁnd one tool that met all our requirements. Every tool seemed to be missing something important, or we heard negative reports from people who had used the tool. We were concerned about the effort needed to convert the existing bug database into a new system.\n\nThe issue was forced when our DTS actually crashed. We had stopped paying for support a couple of years earlier, but the system administrator decided to see what enhancements the vendor had made in the tool. He found that a lot of shortcomings we had experienced had been addressed. For example, all updates were now time stamped. A client application was available that wasn’t subject to session timeouts and had enhanced features that were particularly valuable to the testers.\n\nBy going with our existing tool and paying for the upgrade and maintenance, plus a license allowing more concurrent users, we got help with converting our existing data to the new version and got a working system easily and at a low cost. A bonus was that our customers weren’t faced with having to learn a new system.\n\nSometimes the best tool is the one you already have if you just look to see how it has improved!\n\nAs with all your tool searches, look to others in your community, such as user groups and mailing lists, for recommendations. Deﬁne your criteria before you start looking, and experiment as much as you can. If you choose the wrong tool, cut your losses and start researching alternatives.\n\nKeep Your Focus\n\nDecisions about reporting and tracking defects are important, but don’t lose track of your main target. You want to deliver the best quality product you can, and you want to deliver value to the business in a timely manner. Projects\n\n85\n\n—Lisa\n\n86\n\nCHAPTER 5\n\nChapter 18, “Cod- ing and Testing,” covers alterna- tives and shows you different ways to attack your bug problems.\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nsucceed when people are allowed to do their best work. Concentrate on im- proving communication and building collaboration. If you encounter a lot of defects, investigate the source of the problem. If you need a DTS to do that, use it. If your team works better by documenting defects in executable tests and ﬁxing them right away, do that. If some combination enables you continually improve, go with it. The main thing to remember is that it has to work for your whole team.\n\nDefect tracking is one of the typical quality processes that generate the most questions and controversy in agile testing. Another big source of confusion is whether agile projects need documents such as test plans or traceability ma- trices. Let’s consider that next.\n\nTEST PLANNING Traditional phased software methodologies stress the importance of test plans as part of the overall documentation needs. They’re intended to outline the objectives, scope, approach, and focus of the software testing effort for stakeholders. The completed document is intended to help people outside the test group understand the “why” and “how” of product validation. In this section, we look at test plans and other aspects of preparing and tracking the testing effort for an agile project.\n\nTest Strategy vs. Test Planning\n\nIn an agile project, teams don’t rely on heavy documentation to communi- cate what the testers need to do. Testers work hand in hand with the rest of the team so that the testing efforts are visible to all in the form of task cards. So the question often put to us is, “Is there still a need for test plans?” To an- swer that question, let’s ﬁrst take a look at the difference between a test plan and a test strategy or approach.\n\nThe more information that is contained in a document, the less likely it is that someone is going to read it all. Consider what information is really necessary for the stakeholders. Think about how often it is used and what it is used for.\n\nWe like to think of a test strategy as a static document that seldom changes, while a test plan is created new and is speciﬁc to each new project.\n\nTest Strategy A strategy is a long-term plan of action, the key word being “long-term.” If your organization wants documentation about your overall test approach to\n\nJanet’s Story\n\nIn Chapter 15, “Tester Activities in Release or Theme Planning,” we show examples and discuss alter- natives you can use when you are plan- ning the release.\n\nTEST PLANNING\n\nprojects, consider taking this information and putting it in a static document that doesn’t change much over time. There is a lot of information that is not project speciﬁc and can be extracted into a Test Strategy or Test Approach document.\n\nThis document can then be used as a reference and needs to be updated only if processes change. A test strategy document can be used to give new em- ployees a high-level understanding of how your test processes work.\n\nI have had success with this approach at several organizations. Processes that were common to all projects were captured into one document. Using this format answered most compliance requirements. Some of the topics that were covered were:\n\nTesting Practices • Story Testing • Solution Veriﬁcation Testing • User Acceptance Testing • Exploratory Testing • Load and Performance Testing • Test Automation • Test Results • Defect Tracking Process • Test Tools • Test Environments\n\nTest Plan The power of planning is to identify possible issues and dependencies, to bring risks to the surface to be talked about and to be addressed, and to think about the big picture. Test planning is no different. A team should think about risks and dependencies and the big picture for each project before it starts.\n\nWhether your team decides to create a test plan document or not, the plan- ning should be done. Each project is different, so don’t expect that the same solution will ﬁt all.\n\nSometimes our customers insist on a test plan document. If you’re contract- ing to develop an application, a test plan might be part of a set of deliver- ables that also include items such as a requirements document and a design document.\n\n87\n\n—Janet\n\n88\n\nCHAPTER 5\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nTalk of test plans often leads to talk of traceability. Did someone execute all planned testing of the desired behavior on the delivered code? How do re- quirements and test plans relate to the actual testing and ﬁnal functionality?\n\nTraceability\n\nIn traditional projects, we used to need traceability matrices to determine whether we had actually tested all of the requirements. If a requirement changed, we needed to know that we had changed the appropriate test cases. With very large requirements documents, this was the only way that a test team knew it had good coverage.\n\nIn an agile project, we don’t have those restrictions. We build functionality in tiny, well-deﬁned steps. We work with the team closely and know when something changes. If the programmers work test-ﬁrst, we know there are unit tests for all of the small chunks of work. We can then collaborate with the customer to deﬁne acceptance tests. We test each story as the program- mer works on it, so we know that nothing goes untested.\n\nThere might be requirements for some kind of traceability for regulated in- dustries. If there is, we suggest that you really look at what problem manage- ment is trying to solve. When you understand what is needed, you should try to make the solution as simple as possible. There are multiple ways to pro- vide traceability. Source code check-in comments can refer to the wiki page containing the requirements or test cases, or to a defect number. You can put comments in unit tests tying the test to the location or identiﬁer of the re- quirement. The tests can be integrated directly with the requirements in a tool such as FitNesse. Your team can easily ﬁnd the way that works best for your customers’ needs.\n\nDocuments such as traceability matrices might be needed to fulﬁll require- ments imposed by the organization’s audit standards or quality models. Let’s consider how these directives get along with agile development.\n\nEXISTING PROCESSES AND MODELS This question is often asked: “Can traditional quality models and processes coexist with agile development methods?” In theory, there is no reason why they can’t. In reality, there is often not a choice. Quality models often fall into",
      "page_number": 123
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 131-138)",
      "start_page": 131,
      "end_page": 138,
      "detection_method": "topic_boundary",
      "content": "Lisa’s Story\n\nEXISTING PROCESSES AND MODELS\n\nthe domain of the traditional QA team, and they can follow testers into the new agile structure as well. It might not be easy to ﬁt these into a new agile development model. Let’s look at a few typical quality processes and how testers and their teams might accommodate them.\n\nAudits\n\nDifferent industries have different audit requirements. Quality assurance teams in traditional development organizations are often tasked with provid- ing information for auditors and ensuring compliance with audit require- ments. The Sarbanes-Oxley Act of 2002, enacted in response to high-proﬁle corporate ﬁnancial scandals, sets out requirements for maintaining business records. Ensuring compliance usually falls to the IT departments. SAS 70 is another widely recognized auditing standard for service organizations. These are just a couple of examples of the type of audit controls that affect develop- ment teams.\n\nLarger organizations have specialized teams that control compliance and work with auditors, but development teams are often asked to provide infor- mation. Examples include what testing has been performed on a given soft- ware release, or proving that different accounts reconcile. Testers can be tasked with writing test plans to evaluate the effectiveness of control activities.\n\nOur company undergoes regular SAS 70 audits. Whenever one is scheduled, we write a story card for providing support for the audit. Most of this work falls to the system administrators, but I provide support to the business people who work with the auditor. Sometimes we’re required to demonstrate system functionality in our demo environment. I can provide data for the demos and help if questions arise. I might also be asked to provide details about how we tested a particular piece of functionality.\n\nSome of our internal processes are required to conform with SAS 70 require- ments. For example, every time we release to production, we ﬁll out a form with information about which build was released, how many tests at each level were run on it, who did the release, and who veriﬁed it.\n\nTesters who are part of an agile team should be dedicated to that team. If their help is needed in providing information for an audit or helping to ensure\n\n89\n\n—Lisa\n\n90\n\nCHAPTER 5\n\nSee the bibliogra- phy for informa- tion about CMMI and agile development.\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\ncompliance, write stories for this and plan them along with the rest of the team’s work. Work together with the compliance and internal audit teams to understand your team’s responsibilities.\n\nFrameworks, Models, and Standards\n\nThere are many quality models, but we’ll look at two to show how you can adapt your agile process to ﬁt within their constraints.\n\n1. The Capability Maturity Model Integration (CMMI) aims to help or- ganizations improve their process but doesn’t dictate speciﬁc develop- ment practices to accomplish the improvements.\n\n2. Information Technology Infrastructure Library (ITIL) is a set of best practices for IT service management intended to help organizations develop an effective quality process.\n\nBoth of these models can coexist happily with agile development. They’re rooted in the same goal, making software development projects succeed.\n\nLet’s look at CMMI, a framework for measuring the maturity of your pro- cess. It deﬁnes each level by measuring whether the process is unknown, de- ﬁned, documented, permanent, or optimized. Agile projects have a deﬁned process, although not all teams document what they do. For example, man- aging your requirements with index cards on a release planning wall with a single customer making the ﬁnal decisions is a deﬁned process as long as you do it all the time.\n\nRetrospectives are aimed at constant process improvement, and teams should be always be looking for ways to optimize processes. If the only thing your team is lacking is documentation, then think about including your process into your test strategy documentation.\n\nAsk yourself what the minimum amount of documentation you could give to satisfy the CMMI requirements would be. Janet has had success with using diagrams like the one in Figure 5-2.\n\nIf ITIL has been introduced into your organization and affects change man- agement, adapt your process to accommodate it. You might even ﬁnd the new process beneﬁcial.\n\nJanet’s Story\n\nEXISTING PROCESSES AND MODELS\n\nProject Initiation\n\nGet an understanding of the project\n\nRelease Planning\n\nParticipate in sizing stories Create test plans\n\nEach Iteration\n\n1 ... . X\n\nParticipate in sprint planning, estimating tasks Write and execute story tests Pair-test with other testers, developers Business validation (customers) Automate new functional test cases Run automated regression test cases Run project load tests Demo to the stakeholders\n\nThe End Game (System Test)\n\nRelease mgmt tests mock deploy on staging Smoke test on staging Perform load test (if needed) Complete regression test Business testers perform UAT Participate in release readiness\n\nRelease to Prod/ Support\n\nParticipate in release to production Participate in retrospectives\n\nFigure 5-2 Documenting the test strategy\n\nWhen I worked in one organization that had a central call center to handle all of the customers’ support calls, management implemented ITIL for the service part of the organization. We didn’t think it would affect the development team until the change management team realized that the number of open problems was steadily increasing. No one understood why the number kept going up, so we held a series of problem-solving sessions. First, we mapped out the process cur- rently in effect.\n\nThe call center staff reported an incident in their tracking system. They tried to solve the customer’s problem immediately. Often, that meant providing a work- around for a software defect. The call center report was closed, but a problem\n\n91\n\n92\n\nCHAPTER 5\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nreport in Remedy was then opened, and someone in the development team was sent an email. If the defect was accepted by the development team, a defect was entered into Bugzilla to be ﬁxed.\n\nThere was no loop back to the problem issue to close it when the defect was ﬁnally ﬁxed. We held several brainstorming sessions with all involved stakeholders to determine the best and easiest solution to that problem.\n\nThe problem statement to solve was, “How does the project team report back to the problem and change management folks to tell them when the bug was actu- ally ﬁxed?”\n\nThere were a couple of ways we could have solved the problem. One option was to reference the Remedy ticket in Bugzilla and put hooks into Remedy so that when we closed the Bugzilla defect, Remedy would detect it and close the Rem- edy ticket. Of course, some of the bugs were never addressed, which meant the Remedy tickets stayed open forever.\n\nWe actually found a better solution for the whole team, including the problem change folks. We brainstormed a lot of different ideas but decided that when a bug was opened in Bugzilla, we could close the Remedy ticket, because we realis- tically would never go back to the original complaint and tell the customer who reported it, or when the ﬁx was done.\n\nThe change request that covered the release would automatically include all soft- ware ﬁxes, so it followed the change management process as well.\n\nIf your organization is using some kind of process model or quality standards management, educate yourself about it, and work with the appropriate spe- cialists in your organization. Maintain the team’s focus on delivering high- quality software that provides real business value, and see how you can work within the model.\n\nProcess improvement models and frameworks emphasize discipline and con- formance to process. Few software development methodologies require more discipline than agile development. Standards simply enable you to measure your progress toward your goal. Agile’s focus is on doing your best work and constantly improving. Agile development is compatible with achieving what- ever standards you set for yourself or borrow from a process improvement measurement tool.\n\nSeparate your measurement goals and standards from your means to im- prove those measurements. Set goals, and know what metrics you need to measure success for areas that need improvement. Try using task cards for\n\n—Janet\n\nSUMMARY\n\nactivities that provide the improvements in order to ensure they get the visi- bility they need.\n\nWorking with existing quality processes and models is one of the biggest cultural issues you may face as you transition to agile development. All of these changes are hard, but when your whole team gets involved, none are insurmountable.\n\nSUMMARY In this chapter, we looked at traditional quality-oriented processes and how they can be adapted for an agile environment.\n\n(cid:2) The right metrics can help you to make sure your team is on track to achieve its goals and provide a good return on your investment in them.\n\n(cid:2) Metrics should be visible, providing necessary milestones upon which\n\nto make decisions.\n\n(cid:2) The reasons to use a defect tracking system include for convenience,\n\nfor use as a knowledge base, and for traceability.\n\n(cid:2) Defect tracking systems are too often used as a communication tool, and entering and tracking unnecessary bugs can be considered wasteful.\n\n(cid:2) All tools, including the DTS, need to be used by the whole team, so\n\nconsider all perspectives when choosing a tool.\n\n(cid:2) A test strategy is a long-term overall test approach that can be put in a\n\nstatic document; a test plan should be unique to the project.\n\n(cid:2) Think about alternatives before blindly accepting the need for speciﬁc documents. For example, the agile approach to developing in small, incremental chunks, working closely together, might remove the need for formal traceability documents. Linking the source code control system comments to tests might be another way.\n\n(cid:2) Traditional quality processes and process improvement models, such as SAS 70 audits and CMMI standards, can coexist with agile develop- ment and testing. Teams need to be open to thinking outside the box and work together to solve their problems.\n\n93\n\nThis page intentionally left blank\n\nPart III THE AGILE TESTING QUADRANTS\n\nSoftware quality has many dimensions, each requiring a different testing ap- proach. How do we know all the different types of tests we need to do? How do we know when we’re “done” testing? Who does which tests and how? In this part, we explain how to use the Agile Testing Quadrants to make sure your team covers all needed categories of testing.\n\nOf course, testing requires tools, and we’ve included examples of tools to use, strategies for using those tools effectively, and guidelines about when to use them. Tools are easier to use when used with code that’s designed for testabil- ity. These concerns and more are discussed in this part of the book.\n\nThis page intentionally left blank",
      "page_number": 131
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 139-148)",
      "start_page": 139,
      "end_page": 148,
      "detection_method": "topic_boundary",
      "content": "Chapter 6\n\nTHE PURPOSE OF TESTING\n\nTests That Support the Team\n\nContext-Driven\n\nOverview of Quadrants\n\nTests That Critique the Product\n\nQuadrant Intro— Purpose of Testing\n\nShared Responsibility\n\nManaging Technical Debt\n\nKnowing When We’re Done\n\nFitting All Types into “Doneness”\n\nWhy do we test? The answer might seem obvious, but in fact, it’s pretty complex. We test for a lot of reasons: to ﬁnd bugs, to make sure the code is reliable, and sometimes just to see if the code’s usable. We do different types of testing to accomplish different goals. Software product quality has many components. In this chapter, we introduce the Agile Testing Quadrants. The rest of the chapters in Part III go into detail on each of the quadrants. The Agile Testing Quadrants matrix helps testers ensure that they have considered all of the different types of tests that are needed in order to deliver value.\n\nTHE AGILE TESTING QUADRANTS In Chapter 1, “What Is Agile Testing, Anyway?,” we introduced Brian Marick’s terms for different categories of tests that accomplish different purposes. Fig- ure 6-1 is a diagram of the agile testing quadrants that shows how each of the four quadrants reﬂects the different reasons we test. On one axis, we divide the matrix into tests that support the team and tests that critique the product. The other axis divides them into business-facing and technology-facing tests.\n\n97\n\n98\n\nCHAPTER 6\n\n(cid:2) THE PURPOSE OF TESTING\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ2 Q3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 6-1 Agile Testing Quadrants\n\nThe order in which we’ve numbered these quadrants has no relationship to when the different types of testing are done. For example, agile development starts with customer tests, which tell the team what to code. The timing of the various types of tests depends on the risks of each project, the customers’ goals for the product, whether the team is working with legacy code or on a greenﬁeld project, and when resources are available to do the testing.\n\nTests that Support the Team\n\nThe quadrants on the left include tests that support the team as it develops the product. This concept of testing to help the programmers is new to many testers and is the biggest difference between testing on a traditional project and testing on an agile project. The testing done in Quadrants 1 and 2 are more requirements speciﬁcation and design aids than what we typically think of as testing.\n\nChapter 8, “Busi- ness-Facing Tests that Support the Team,” explains business conditions of satisfaction.\n\nTHE AGILE TESTING QUADRANTS\n\nQuadrant 1 The lower left quadrant represents test-driven development, which is a core agile development practice.\n\nUnit tests verify functionality of a small subset of the system, such as an object or method. Component tests verify the behavior of a larger part of the system, such as a group of classes that provide some service [Meszaros, 2007]. Both types of tests are usually automated with a member of the xUnit family of test automation tools. We refer to these tests as programmer tests, developer- facing tests, or technology-facing tests. They enable the programmers to mea- sure what Kent Beck has called the internal quality of their code [Beck, 1999].\n\nA major purpose of Quadrant 1 tests is test-driven development (TDD) or test-driven design. The process of writing tests ﬁrst helps programmers de- sign their code well. These tests let the programmers conﬁdently write code to deliver a story’s features without worrying about making unintended changes to the system. They can verify that their design and architecture de- cisions are appropriate. Unit and component tests are automated and written in the same programming language as the application. A business expert probably couldn’t understand them by reading them directly, but these tests aren’t intended for customer use. In fact, internal quality isn’t negotiated with the customer; it’s deﬁned by the programmers. Programmer tests are normally part of an automated process that runs with every code check-in, giving the team instant, continual feedback about their internal quality.\n\nQuadrant 2 The tests in Quadrant 2 also support the work of the development team, but at a higher level. These business-facing tests, also called customer-facing tests and customer tests, deﬁne external quality and the features that the custom- ers want.\n\nLike the Quadrant 1 tests, they also drive development, but at a higher level. With agile development, these tests are derived from examples provided by the customer team. They describe the details of each story. Business-facing tests run at a functional level, each one verifying a business satisfaction con- dition. They’re written in a way business experts can easily understand using the business domain language. In fact, the business experts use these tests to deﬁne the external quality of the product and usually help to write them. It’s possible this quadrant could duplicate some of the tests that were done at the unit level; however, the Quadrant 2 tests are oriented toward illustrating and conﬁrming desired system behavior at a higher level.\n\n99\n\n100\n\nCHAPTER 6\n\nLisa’s Story\n\n(cid:2) THE PURPOSE OF TESTING\n\nMost of the business-facing tests that support the development team also need to be automated. One of the most important purposes of tests in these two quadrants is to provide information quickly and enable fast trouble- shooting. They must be run frequently in order to give the team early feed- back in case any behavior changes unexpectedly. When possible, these automated tests run directly on the business logic in the production code without having to go through a presentation layer. Still, some automated tests must verify the user interfaces and any APIs that client applications might use. All of these tests should be run as part of an automated continu- ous integration, build, and test process.\n\nThere is another group of tests that belongs in this quadrant as well. User in- teraction experts use mock-ups and wireframes to help validate proposed GUI (graphical user interface) designs with customers and to communicate those designs to the developers before they start to code them. The tests in this group are tests that help support the team to get the product built right but are not automated. As we’ll see in the following chapters, the quadrants help us identify all of the different types of tests we need to use in order to help drive coding.\n\nSome people use the term “acceptance tests” to describe Quadrant 2 tests, but we believe that acceptance tests encompass a broader range of tests that in- clude Quadrants 3 and 4. Acceptance tests verify that all aspects of the sys- tem, including qualities such as usability and performance, meet customer requirements.\n\nUsing Tests to Support the Team The quick feedback provided by Quadrants 1 and 2 automated tests, which run with every code change or addition, form the foundation of an agile team. These tests ﬁrst guide development of functionality, and when auto- mated, then provide a safety net to prevent refactoring and the introduction of new code from causing unexpected results.\n\nWe run our automated tests that support the team (the left half of the quadrants) in separate build processes. Unit and component tests run in our “ongoing” build, which takes about eight minutes to ﬁnish. Although the programmers run the unit tests before they check in, the build might still fail due to integration problems or environmental differences. As soon as we see the “build failed” email, the person who checked in the offending code ﬁxes the problem. Business-facing functional tests run in our “full build,” which also runs continually, kicking off every time a code change is checked in. It ﬁnishes in less than two hours. That’s still pretty quick feedback, and again, a build failure means immediate action to ﬁx the\n\nTHE AGILE TESTING QUADRANTS\n\nproblem. With these builds as a safety net, our code is stable enough to release every day of the iteration if we so choose.\n\nThe tests in Quadrants 1 and 2 are written to help the team deliver the busi- ness value requested by the customers. They verify that the business logic and the user interfaces behave according to the examples provided by the cus- tomers. There are other aspects to software quality, some of which the custom- ers don’t think about without help from the technical team. Is the product competitive? Is the user interface as intuitive as it needs to be? Is the applica- tion secure? Are the users happy with how the user interface works? We need different tests to answer these types of questions.\n\nTests that Critique the Product\n\nIf you’ve been in a customer role and had to express your requirements for a software feature, you know how hard it can be to know exactly what you want until you see it. Even if you’re conﬁdent about how the feature should work, it can be hard to describe it so that programmers fully understand it.\n\nThe word “critique” isn’t intended in a negative sense. A critique can include both praise and suggestions for improvement. Appraising a software product involves both art and science. We review the software in a constructive man- ner, with the goal of learning how we can improve it. As we learn, we can feed new requirements and tests or examples back to the process that supports the team and guide development.\n\nQuadrant 3 Business-facing examples help the team design the desired product, but at least some of our examples will probably be wrong. The business experts might overlook functionality, or not get it quite right if it isn’t their ﬁeld of expertise. The team might simply misunderstand some examples. Even when the programmers write code that makes the business-facing tests pass, they might not be delivering what the customer really wants.\n\nThat is where the tests to critique the product in the third and fourth quad- rants come into play. Quadrant 3 classiﬁes the business-facing tests that exer- cise the working software to see if it doesn’t quite meet expectations or won’t stand up to the competition. When we do business-facing tests to critique the product, we try to emulate the way a real user would work the application. This is manual testing that only a human can do. We might use some automated\n\n101\n\n—Lisa\n\n102\n\nCHAPTER 6\n\n(cid:2) THE PURPOSE OF TESTING\n\nscripts to help us set up the data we need, but we have to use our senses, our brains, and our intuition to check whether the development team has delivered the business value required by the customers.\n\nOften, the users and customers perform these types of tests. User Acceptance Testing (UAT) gives customers a chance to give new features a good workout and see what changes they may want in the future, and it’s a good way to gather new story ideas. If your team is delivering software on a contract basis to a client, UAT might be a required step in approving the ﬁnished stories.\n\nUsability testing is an example of a type of testing that has a whole science of its own. Focus groups might be brought in, studied as they use the applica- tion, and interviewed in order to gather their reactions. Usability testing can also include navigation from page to page or even something as simple as the tabbing order. Knowledge of how people use systems is an advantage when testing usability.\n\nExploratory testing is central to this quadrant. During exploratory testing sessions, the tester simultaneously designs and performs tests, using critical thinking to analyze the results. This offers a much better opportunity to learn about the application than scripted tests. We’re not talking about ad hoc test- ing, which is impromptu and improvised. Exploratory testing is a more thoughtful and sophisticated approach than ad hoc testing. It is guided by a strategy and operates within deﬁned constraints. From the start of each project and story, testers start thinking of scenarios they want to try. As small chunks of testable code become available, testers analyze test results, and as they learn, they ﬁnd new areas to explore. Exploratory testing works the sys- tem in the same ways that the end users will. Testers use their creativity and intuition. As a result, it is through this type of testing that many of the most serious bugs are usually found.\n\nQuadrant 4 The types of tests that fall into the fourth quadrant are just as critical to agile development as to any type of software development. These tests are technol- ogy-facing, and we discuss them in technical rather than business terms. Technology-facing tests in Quadrant 4 are intended to critique product char- acteristics such as performance, robustness, and security. As we’ll describe in Chapter 11, “Critiquing the Product using Technology-Facing Tests,” your team already possesses many of the skills needed to do these tests. For exam- ple, programmers might be able to leverage unit tests into performance tests with a multi-threaded engine. However, creating and running these tests might require the use of specialized tools and additional expertise.\n\nTHE AGILE TESTING QUADRANTS\n\nIn the past, we’ve heard complaints that agile development seems to ignore the technology-facing tests that critique the product. These complaints might be partly due to agile’s emphasis on having customers write and prior- itize stories. Nontechnical customer team members often assume that the de- velopers will take care of concerns such as speed and security, and that the programmers are intent on producing only the functionality prioritized by the customers.\n\nIf we know the requirements for performance, security, interaction with other systems, and other nonfunctional attributes before we start coding, it’s easier to design and code with that in mind. Some of these might be more important than actual functionality. For example, if an Internet retail website has a one-minute response time, the customers won’t wait to appreciate the fact that all of the features work properly. Technology-facing tests that cri- tique the product should be considered at every step of the development cy- cle and not left until the very end. In many cases, such testing should even be done before functional testing.\n\nIn recent years we’ve seen many new lightweight tools appropriate to an agile development project become available to support tests. Automation tools can be used to create test data, set up test scenarios for manual testing, drive se- curity tests, and help make sense of results. Automation is mandatory for some efforts such as load and performance testing.\n\nChecking Nonfunctional Requirements\n\nAlessandro Collino, a computer science and information engineer with Onion S.p.A., who works on agile projects, illustrates why executing tests that cri- tique the product early in the development process is critical to project success.\n\nOur Scrum/XP team used TDD to develop a Java application that would convert one form of XML to another. The application performed complex calculations on the data. For each simple story, we wrote a unit test to check the conversion of one element into the required format, imple- mented the code to make the test pass, and refactored as needed.\n\nWe also wrote acceptance tests that read subsets of the original XML ﬁles from disk, converted them, and wrote them back. The ﬁrst time we ran the application on a real ﬁle to be converted, we got an out-of- memory error. The DOM parser we used for the XML conversion couldn’t handle such a large ﬁle. All of our tests used small subsets of the actual ﬁles; we hadn’t thought to write unit tests using large datasets.\n\n103\n\n104\n\nCHAPTER 6\n\n(cid:2) THE PURPOSE OF TESTING\n\nDoing TDD gave us quick feedback on whether the code was working per the functional requirements, but the unit tests didn’t test any non- functional requirements such as capacity, performance, scalability, and usability. If you use TDD to also check nonfunctional requirements, in this case, capacity, you’ll have quick feedback and be able to avoid expen- sive mistakes.\n\nAlessandro’s story is a good example of how the quadrant numbering doesn’t imply the order in which tests are done. When application performance is critical, plan to test with production-level loads as soon as testable code is available.\n\nWhen you and your team plan a new release or project, discuss which types of tests from Quadrants 3 and 4 you need, and when they should be done. Don’t leave essential activities such as load or usability testing to the end, when it might be too late to rectify problems.\n\nUsing Tests that Critique the Product The information produced during testing to review the product should be fed back into the left side of our matrix and used to create new tests to drive future development. For example, if the server fails under a normal load, new stories and tests to drive a more scalable architecture will be needed. Using the quadrants will help you plan tests that critique the product as well as tests that drive development. Think about why you are testing to make sure that the tests are performed at the optimum stage of development.\n\nThe short iterations of agile development give your team a chance to learn and experiment with the different testing quadrants. If you ﬁnd out too late that your design doesn’t scale, start load testing earlier with the next story or project. If the iteration demo reveals that the team misunderstood the cus- tomer’s requirements, maybe you’re not doing a good enough job of writing customer tests to guide development. If the team puts off needed refactoring, maybe the unit and component tests aren’t providing enough coverage. Use the agile testing quadrants to help make sure all necessary testing is done at the right time.\n\nKNOWING WHEN A STORY IS DONE For most products, we need all four categories of testing to feel conﬁdent we’re delivering the right value. Not every story requires security testing, but you don’t want to omit it because you didn’t think of it.\n\nLisa’s Story\n\nKNOWING WHEN A STORY IS DONE\n\nMy team uses “stock” cards to ensure that we always consider all different types of tests. When unit testing wasn’t yet a habit, we wrote a unit test card for each story on the board. Our “end to end” test card reminds the programmers to complete the job of integration testing and to make sure all of the parts of the code work together. A “security” card also gets considered for each story, and if appropriate, put on the board to keep everyone conscious of keeping data safe. A task card to show the user interface to customers makes sure that we don’t forget to do this as early as possible, and it helps us start exploratory testing along with the customers early, too. All of these cards help us address all the different aspects of product quality.\n\nTechnology-facing tests that extend beyond a single story get their own row on the story board. We use stories to evaluate load test tools and to establish perfor- mance baselines to kick off our load and performance-testing efforts.\n\nThe technology-facing and business-facing tests that drive development are central to agile development, whether or not you actually write task cards for them. They give your team the best chance of getting each story “done.” Iden- tifying the tasks needed to perform the technology-facing and business- facing tests that critique the product ensures that you’ll learn what the prod- uct is missing. A combination of tests from all four quadrants will let the team know when each feature has met the customer’s criteria for functional- ity and quality.\n\nShared Responsibility\n\nOur product teams need a wide range of expertise to cover all of the agile testing quadrants. Programmers should write the technology-facing tests that support programming, but they might need help at different times from testers, database designers, system administrators, and conﬁguration special- ists. Testers take primary charge of the business-facing tests in tandem with the customers, but programmers participate in designing and automating tests, while usability and other experts might be called in as needed. The fourth quadrant, with technology-facing tests that critique the product, may require more specialists. No matter what resources have to be brought in from outside the development team, the team is still responsible for getting all four quadrants of testing done.\n\nWe believe that a successful team is one where everybody participates in the crafting of the product and that everyone shares the team’s internal pain when things go wrong. Implementing the practices and tools that enable us\n\n105\n\n—Lisa\n\n106\n\nCHAPTER 6\n\n(cid:2) THE PURPOSE OF TESTING\n\nto address all four quadrants of testing can be painful at times, but the joy of implementing a successful product is worth the effort.\n\nMANAGING TECHNICAL DEBT Ward Cunningham coined the term “technical debt” in 1992, but we’ve cer- tainly experienced it throughout our careers in software development! Tech- nical debt builds up when the development team takes shortcuts, hacks in quick ﬁxes, or skips writing or automating tests because it’s under the gun. The code base gets harder and harder to maintain. Like ﬁnancial debt, “inter- est” compounds in the form of higher maintenance costs and lower team ve- locity. Programmers are afraid to make any changes, much less attempt refactoring to improve the code, for fear of breaking it. Sometimes this fear exists because they can’t understand the coding to start with, and sometimes it is because there are no tests to catch mistakes.\n\nEach quadrant in the agile testing matrix plays a role in keeping technical debt to a manageable level. Technology-facing tests that support coding and design help keep code maintainable. An automated build and integration process that runs unit tests is a must for minimizing technical debt. Catching unit-level defects during coding will free testers to focus on business-facing tests in order to guide the team and improve the product. Timely load and stress testing lets the teams know whether their architecture is up to the job.\n\nBy taking the time and applying resources and practices to keep technical debt to a minimum, a team will have time and resources to cover the testing needed to ensure a quality product. Applying agile principles to do a good job of each type of testing at each level will, in turn, minimize technical debt.\n\nTESTING IN CONTEXT Categorizations and deﬁnitions such as we ﬁnd in the agile testing matrix help us make sure we plan for and accomplish all of the different types of testing we need. However, we need to bear in mind that each organization, product, and team has its own unique situation, and each needs to do what works for it in its individual situation. As Lisa’s coworker Mike Busse likes to say, “It’s a tool, not a rule.” A single product or project’s needs might evolve drastically over time. The quadrants are a helpful way to make sure your team is considering all of the different aspects of testing that go into “doneness.”",
      "page_number": 139
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 149-157)",
      "start_page": 149,
      "end_page": 157,
      "detection_method": "topic_boundary",
      "content": "For more on con- text-driven test- ing, see www .context-driven- testing.com.\n\nTESTING IN CONTEXT\n\nWe can borrow important principles from the context-driven school of test- ing when planning testing for each story, iteration, and release.\n\n(cid:2) The value of any practice depends on its context. (cid:2) There are good practices in context, but there are no best practices. (cid:2) People, working together, are the most important part of any project’s\n\ncontext.\n\n(cid:2) Projects unfold over time in ways that are often not predictable. (cid:2) The product is a solution. If the problem isn’t solved, the product\n\ndoesn’t work.\n\n(cid:2) Good software testing is a challenging intellectual process. (cid:2) Only through judgment and skill, exercised cooperatively throughout the entire project, are we able to do the right things at the right times to effectively test our products.\n\nThe quadrants help give context to agile testing practices, but you and your team will have to adapt as you go. Testers help provide the feedback the team needs to adjust and work better. Use your skills to engage the customers throughout each iteration and release. Be conscious of when your team needs roles or knowledge beyond what it currently has available.\n\nThe Agile Testing Quadrants provide a checklist to make sure you’ve covered all your testing bases. Examine the answers to questions such as these:\n\n(cid:2) Are we using unit and component tests to help us ﬁnd the right de-\n\nsign for our application?\n\n(cid:2) Do we have an automated build process that runs our automated unit\n\ntests for quick feedback?\n\n(cid:2) Do our business-facing tests help us deliver a product that matches\n\ncustomers’ expectations?\n\n(cid:2) Are we capturing the right examples of desired system behavior? Do\n\nwe need more? Are we basing our tests on these examples?\n\n(cid:2) Do we show prototypes of UIs and reports to the users before we start coding them? Can the users relate them to how the ﬁnished software will work?\n\n(cid:2) Do we budget enough time for exploratory testing? How do we tackle\n\nusability testing? Are we involving our customers enough?\n\n(cid:2) Do we consider technological requirements such as performance and security early enough in the development cycle? Do we have the right tools to do “ility” testing?\n\n107\n\n108\n\nCHAPTER 6\n\n(cid:2) THE PURPOSE OF TESTING\n\nUse the matrix as a map to get started. Experiment, and use retrospectives to keep improving your efforts to guide development with tests and build on what you learn about your product through testing.\n\nSUMMARY In this chapter we introduced the Agile Testing Quadrants as a convenient way to categorize tests. The four quadrants serve as guidelines to ensure that all facets of product quality are covered in the testing and developing process.\n\n(cid:2) Tests that support the team can be used to drive requirements. (cid:2) Tests that critique the product help us think about all facets of appli-\n\ncation quality.\n\n(cid:2) Use the quadrants to know when you’re done, and ensure the whole team shares responsibility for covering the four quadrants of the matrix.\n\n(cid:2) Managing technical debt is an essential foundation for any software development team. Use the quadrants to think about the different dimensions.\n\n(cid:2) Context should always guide our testing efforts.\n\nChapter 7\n\nTECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nSource Code Control\n\nIDEs\n\nBuild Tools\n\nBuild Automation Tools\n\nToolkit\n\nFoundation of Agile Testing\n\nPurpose of Unit Tests\n\nSupporting Infrastructure\n\nUnit Test Tools\n\nGo Faster, Do More\n\nTechnology-Facing Tests that Support the Team\n\nWhy These Tests?\n\nValue to Testers\n\nDesigning for Testing\n\nTimely Feedback\n\nWhat Testers Can Do\n\nWhat Managers Can Do\n\nWhat If You Don’t Do Them?\n\nQuadrant 1 Tests vs. Quadrant 2\n\nWhere Does One Stop & the Other Start?\n\nTeam Approach\n\nWe use the Agile Testing Quadrants as a guide to help us cover all the types of testing we need and to help us make sure we have the right resources to succeed at each type. In this chapter, we look at tests in the ﬁrst quadrant, technology- facing tests that support the team, and at tools to support this testing. The activ- ities in this quadrant form the core of agile development.\n\nAN AGILE TESTING FOUNDATION We discuss Quadrant 1 ﬁrst because the technology-facing tests that support the team form the foundation of agile development and testing. See Figure 7-1 for a reminder of the Agile Testing Quadrants with this quadrant highlighted. Quadrant 1 is about much more than testing. The unit and component tests we talk about in Quadrant 1 aren’t the ﬁrst tests written for each story, but they help guide design and development. Without a foundation of test-driven\n\n109\n\n110\n\nCHAPTER 7\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ2 Q3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 7-1 The Agile Testing Quadrants, highlighting Quadrant 1\n\ndesign, automated unit and component tests, and a continuous integration process to run the tests, it’s hard to deliver value in a timely manner. All of the testing in the other quadrants can’t make up for inadequacies in this one. We’ll talk about the other quadrants in the next few chapters and explain how they all ﬁt together.\n\nTeams need the right tools and processes to create and execute technology- facing tests that guide development. We’ll give some examples of the types of tools needed in the last section of this chapter.\n\nThe Purpose of Quadrant 1 Tests\n\nUnit tests and component tests ensure quality by helping the programmer understand exactly what the code needs to do, and by providing guidance in\n\nAN AGILE TESTING FOUNDATION\n\nthe right design. They help the team to focus on the story that’s being deliv- ered and to take the simplest approach that will work. Unit tests verify the behavior of parts as small as a single object or method [Meszaros, 2007]. Component tests help solidify the overall design of a deployable part of the system by testing the interaction between classes or methods.\n\nDeveloping unit tests can be an essential design tool when using TDD. When an agile programmer starts a coding task, she writes a test that captures the behavior of a tiny bit of code and then works on the code until the test passes. By building the code in small test-code-test increments, the programmer has a chance to think through the functionality that the customer needs. As questions come up, she can ask the customer. She can pair with a tester to help make sure all aspects of that piece of code, and its communication with other units, are tested.\n\nThe term test-driven development misleads practitioners who don’t under- stand that it’s more about design than testing. Code developed test-ﬁrst is naturally designed for testability. Quadrant 1 activities are all aimed at pro- ducing software with the highest possible internal quality.\n\nWhen teams practice TDD, they minimize the number of bugs that have to be caught later on. Most unit-level bugs are prevented by writing the test be- fore the code. Thinking through the design by writing the unit test means the system is more likely to meet customer requirements. When post-development testing time is occupied with ﬁnding and ﬁxing bugs that could have been detected by programmer tests, there’s no time to ﬁnd the serious issues that might adversely affect the business. The more bugs that leak out of our cod- ing process, the slower our delivery will be, and in the end, it is the quality that will suffer. That’s why the programmer tests in Quadrant 1 are so criti- cal. While every team should adopt practices that work for its situation, a team without these core agile practices is unlikely to beneﬁt much from agile values and principles.\n\nSupporting Infrastructure\n\nSolid source code control, conﬁguration management, and continuous inte- gration are essential to getting value from programmer tests that guide devel- opment. They enable the team to always know exactly what’s being tested. Continuous integration gives us a way to run tests every time new code is checked in. When a test fails, we know who checked in the change that caused the failure, and that person can quickly ﬁx the problem. Continuous\n\n111\n\n112\n\nCHAPTER 7\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nintegration saves time and motivates each programmer to run the tests be- fore checking in the new code. A continuous integration and build process delivers a deployable package of code for us to test.\n\nAgile projects that lack these core agile practices tend to turn into “mini- waterfalls.” The development cycles are shorter, but code is still being thrown “over the wall” to testers who run out of time to test because the code is of poor quality. The term waterfall isn’t necessarily derogatory. We’ve worked on successful “waterfall” projects where the programmers stringently auto- mate unit tests, practice continuous integration, and use automated builds to run tests. These successful “waterfall” projects also involve customers and testers throughout the development cycle. When we code without appropri- ate practices and tools, regardless of what we call the process, we’re not going to deliver high-quality code in a timely manner.\n\nWHY WRITE AND EXECUTE THESE TESTS? We’re not going into any details here about how to do TDD, or the best ways to write unit and component tests. There are several excellent books on those sub- jects. Our goal is to explain why these activities are important to agile testers. Let’s explore some reasons to use technology-facing tests that support the team.\n\nLets Us Go Faster and Do More\n\nSpeed should never be the end goal of an agile development team. Trying to do things fast and meet tight deadlines without thinking about the quality causes us to cut corners and revert to old, bad habits. If we cut corners, we’ll build up more technical debt, and probably miss the deadline anyway. Hap- pily, though, speed is a long-term side effect of producing code with the highest possible internal quality. Continuous builds running unit tests notify the team of failure within a few minutes of the problem check-in, and the mistake can be found and ﬁxed quickly. A safety net of automated unit and code integration tests enables the programmers to refactor frequently. This keeps the code at a reasonable standard of maintainability and delivers the best value for the time invested. Technical debt is kept as low as possible.\n\nIf you’ve worked as a tester on a project where unit testing was neglected, you know how easy it is to spend all of your time ﬁnding unit-level defects. You might ﬁnd so many bugs while testing the “happy path” that you never have time to test more complex scenarios and edge cases. The release deadline is pushed back as the “ﬁnd and ﬁx” cycle drags on, or testing is just stopped and a buggy product is foisted off on unsuspecting customers.\n\nWHY WRITE AND EXECUTE THESE TESTS?\n\nOur years on agile teams have been Utopian in contrast to this scenario. Driving coding practices with tests means that the programmers probably understood the story’s requirements reasonably well. They’ve talked exten- sively with the customers and testers to clarify the desired behaviors. All parties understand the changes being made. By the time the team has com- pleted all of the task cards for coding a story, or a thin, testable slice of one, the feature has been well covered by unit and component tests. Usually the programmers have made sure at least one path through the story works end to end.\n\nThis means that we, as testers, waste little time ﬁnding low-level bugs. We’re likely to try scenarios the programmers hadn’t thought of and to spend our time on higher-level business functionality. Well-designed code is usually ro- bust and testable. If we ﬁnd a defect, we show it to the programmer, who writes a unit test to reproduce the bug and then ﬁxes it quickly. We actually have time to focus on exploratory testing and the other types of in-depth tests to give the code a good workout and learn more about how it should work. Often, the only “bugs” we ﬁnd are requirements that everyone on our team missed or misunderstood. Even those are found quickly if the customer is involved and has regular demos and test opportunities. After a development team has mas- tered TDD, the focus for improvement shifts from bug prevention to ﬁguring out better ways to elicit and capture requirements before coding.\n\nTest-First Development vs. Test-Driven Development\n\nGerard Meszaros [Meszaros 2007, pp. 813–814] offers the following descrip- tion of how test-ﬁrst development differs from test-driven development:\n\n“Unlike test-driven development, test-ﬁrst development merely says that the tests are written before the production code; it does not imply that the production code is made to work one test at a time (emergent design). Test-ﬁrst development can be applied at the unit test or customer test level, depending on which tests we have chosen to automate.”\n\nErik Bos [2008] observes that test-ﬁrst development involves both test-ﬁrst programming and test-ﬁrst design, but there’s a subtle difference:\n\n“With test-ﬁrst design, the design follows the tests, whereas you can do test-ﬁrst programming of a design that you ﬁrst write down on a white- board. On larger projects, we tend to do more design via whiteboard discussions; the team discusses the architecture around a whiteboard, and codes test-ﬁrst based on this design. On smaller projects, we do practice test-driven design.”\n\n113\n\n114\n\nCHAPTER 7\n\nLisa’s Story\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nThere are several different philosophies about when to write tests and for what purpose. It’s up to each team to agree on the approach that helps it achieve its quality objectives, although there is common agreement in the ag- ile community that TDD deﬁnitely helps a team achieve better-quality soft- ware. This is an important way that programmer tests support the team. Let’s look at some more ways.\n\nMaking Testers’ Jobs Easier\n\nThe core practices related to programmer tests make lots of testing activities easier to accomplish. Programmers work in their own sandboxes, where they can test new code without affecting anyone else’s work. They don’t check in code until it has passed a suite of regression tests in their sandbox.\n\nThe team thinks about test environments and what to use for test data. Unit tests usually work with fake or mock objects instead of actual databases for speed, but programmers still need to test against realistic data. Testers can help them identify good test data. If the unit tests represent real-life data, fewer issues will be found later.\n\nHere’s a small example. When my current team ﬁrst adopted agile development, we didn’t have any automated tests. We had no way to produce a deployable code package, and we had no rudimentary test environments or test databases. I didn’t have any means to produce a build myself, either. We decided to start writ- ing code test-ﬁrst and committed to automating tests at all levels where appropri- ate, but we needed some infrastructure ﬁrst.\n\nOur ﬁrst priority was to implement a continuous build process, which was done in a couple of days. Each build sent an email with a list of checked-in ﬁles and com- ments about the updates. I could now choose which build to deploy and test. The next priority was to provide independent test environments so that tests run by one person would not interfere with other tests. The new database expert cre- ated new schemas to meet testing needs and a “seed” database of canonical, production-like data. These schemas could be refreshed on demand quickly with a clean set of data. Each team member, including me, got a unique and indepen- dent test environment.\n\nEven before the team mastered TDD, the adopted infrastructure was in place to support executing tests. This infrastructure enabled the team to start testing much more effectively. Another aspect of trying to automate testing was dealing with a legacy application that was difﬁcult to test. The decisions that were made to enable TDD also helped with customer-facing tests. We decided to start rewriting the system in a new architecture that facilitated testing and test automation, not only at the unit level but at all levels.\n\n—Lisa\n\nWHY WRITE AND EXECUTE THESE TESTS?\n\nWriting tests and writing code with those tests in mind means programmers are always consciously making code testable. All of these good infrastructure- related qualities spill over to business-facing tests and tests that critique the product. The whole team is continually thinking of ways to improve design and make testing easier.\n\nDesigning with Testing in Mind\n\nOne advantage of driving development with tests is that code is written with the express intention of making the tests pass. The team has to think, right from the beginning, about how it will execute and automate tests for every story it codes. Test-driven development means that programmers will write each test before they write the code to make it pass.\n\nWriting “testable code” is a simple concept, but it’s not an easy task, espe- cially if you’re working on old code that has no automated tests and isn’t de- signed for testability. Legacy systems often have business logic, I/O, database, and user interface layers intertwined. There’s no easy way to hook in to auto- mate a test below the GUI or at the unit level.\n\nA common approach in designing a testable architecture is to separate the different layers that perform different functions in the application. Ideally, you would want to access each layer directly with a test ﬁxture and test algo- rithms with different inputs. To do this, you isolate the business logic into its own layer, using fake objects instead of trying to access other applications or the actual database. If the presentation layer can be separated from underly- ing business logic and database access, you can quickly test input validation without testing underlying logic.\n\nLayered Architectures and Testability\n\nLisa’s team took the “strangler application” approach to creating a testable sys- tem where tests could be use to drive coding. Mike Thomas, the team’s senior architect, explains how their new layered architecture enabled a testable design.\n\nA layered architecture divides a code base into horizontal slices that contain similar functionality, often related to a technology. The slices at the highest level are the most speciﬁc and depend upon the slices below, which are more general. For example, many layered code bases have slices such as the following: UI, business logic, and data access.\n\n115",
      "page_number": 149
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 158-165)",
      "start_page": 158,
      "end_page": 165,
      "detection_method": "topic_boundary",
      "content": "116\n\nCHAPTER 7\n\nSee the bibliogra- phy for more infor- mation on Alastair Cockburn’s Ports and Adapters pattern.\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nHorizontal layering is just one way to organize a code base: Another is domain-oriented slices (such as payroll or order entry), which are gener- ally thought of as “vertical.” These layering approaches can be com- bined, of course, and all can be used to enhance testability.\n\nLayering has advantages for testing, but only if the mechanism for “con- necting” the slices provides ﬂexibility. If a code base has tightly coupled slices via such mechanisms as direct concrete class dependencies and static methods, it is difﬁcult to isolate a unit for testing, despite the layer- ing. This makes most automated tests into integration tests, which can be complicated and can run slowly. In many cases, testing can only be accomplished by running the entire system.\n\nContrast this with a code base where the layers are separated by inter- faces. Each slice depends only upon interfaces deﬁned in the slice beneath it rather than on speciﬁc classes. Dependencies on such inter- faces are easy to satisfy with test doubles at test time: mocks, stubs, and so on. Unit testing is thus simpliﬁed because each unit can truly be iso- lated. For example, the UI can be tested against mock business layer objects, and the business layer can be tested against mock data access objects, avoiding live database access.\n\nThe layered approach has allowed Lisa’s team to succeed in automating tests at all levels and drive development with both technology-facing and business- facing tests.\n\nAnother example of an approach to testable design is Alistair Cockburn’s Ports and Adapters pattern [Cockburn, 2005]. This pattern’s intent is to “cre- ate your application to work without either a UI or a database so you can run automated regression tests against the application, work when the database becomes unavailable, and link applications together without any user in- volvement.” Ports accept outside events, and a technology-speciﬁc adapter converts it into a message that can be understood by the application. In turn, the application sends output via a port to an adapter, which creates the sig- nals needed by the receiving human or automated users. Applications de- signed using this pattern can be driven by automated test scripts as easily as by actual users.\n\nIt’s more obvious how to code test-ﬁrst on a greenﬁeld project. Legacy sys- tems, which aren’t covered by automated unit tests, present a huge challenge. It’s hard to write unit tests for code that isn’t designed for testability, and it’s hard to change code that isn’t safeguarded with unit tests. Many teams have\n\nThe bibliography has links to more articles about “res- cue” and “stran- gler” approaches to legacy code.\n\nFor more informa- tion about Pre- senter First development, see the bibliography.\n\nWHY WRITE AND EXECUTE THESE TESTS?\n\nfollowed the “legacy code rescue” techniques explained by Michael Feathers in Working Effectively with Legacy Code [Feathers, 2005]. Other teams, such as Lisa’s, aim to “strangle” their legacy code. This strategy stems from Martin Fowler’s “strangler application” [Fowler, 2004]. New stories were coded test- ﬁrst in a new architecture while the old system was still maintained. Over time, much of the system has been converted to the new architecture, with the goal of eventually doing away with the old system.\n\nAgile testing in a legacy mainframe type of environment presents particular challenges, not the least of which is the lack of availability of publications and information about how to do it successfully. COBOL, mainframes, and their ilk are still widely used. Let agile principles and values guide your team as you look for ways to enable automated testing in your application. You might have to adapt some techniques; for example, maybe you can’t write code test- ﬁrst, but you can test soon after writing the code. When it’s the team’s prob- lem to solve, and not just the testers’ problem, you’ll ﬁnd a way to write tests.\n\nTesting Legacy Systems\n\nJohn Voris, a developer with Crown Cork and Seal, works in the RPG lan- guage, a cousin of COBOL, which runs on the operating system previously known as AS 400 and now known as System i. John was tasked with merging new code with a vendor code base. He applied tenets of Agile, Lean, and IBM-recommended coding practices to come up with an approach he calls “ADEPT” for “AS400 Displays for External Prototyping and Testing.” While he isn’t coding test-ﬁrst, he’s testing “Minutes Afterward.” Here’s how he summed up his approach:\n\nWrite small, single-purpose modules (not monolithic programs), and refactor existing programs into modules. Use a Presenter First develop- ment approach (similar to the Model View Presenter or Model View Controller pattern).\n\nDeﬁne parameter interfaces for the testing harness based on screen formats and screen ﬁelds. The only drawback here is numbers are deﬁned as zoned decimals rather than packed hexadecimal, but this is offset by the gain in productivity.\n\n“Minutes after” coding each production module, create a testing pro- gram using the screen format to test via the UI. The UI interface for the test is created prior to the production program, because the UI testing interface is the referenced interface for the production module. The impetus for running a test looms large for the programmer, because most of the coding for the test is already done.\n\n117\n\n118\n\nCHAPTER 7\n\nFor more about RPGUnit, see www .RPGUnit.org.\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nUse standard test data sets, which are unchanging, canonical test data, to drive the tests.\n\nThis approach, in which the test programs are almost auto-generated, lends itself to automation with a record/playback tool that would cap- ture data inputs and outputs, with tests run in a continuous build, using RPGUnit.\n\nYour team can ﬁnd an approach to designing for testability that works for you. The secret is the whole-team commitment to testing and quality. When a team is constantly working to write tests and make them pass, it ﬁnds a way to get it done. Teams should take time to consider how they can create an ar- chitecture that will make automated tests easy to create, inexpensive to main- tain, and long-lived. Don’t be afraid to revisit the architecture if automated tests don’t return enough value for the investment in them.\n\nTimely Feedback\n\nThe biggest value of unit tests is in the speed of their feedback. In our opin- ion, a continuous integration and build process that runs the unit tests should ﬁnish within ten minutes. If each programmer checks code in several times a day, a longer build and test process will cause changes to start stack- ing up. As a tester, it can be frustrating to have to wait a long time for new functionality or a bug ﬁx. If there’s a compile error or unit test failure, the de- lay gets even worse, especially if it’s almost time to go home!\n\nA build and test process that runs tests above the unit level, such as func- tional API tests or GUI tests, is going to take longer. Have at least one build process that runs quickly, and a second that runs the slower tests. There should be at least one daily “build” that runs all of the slower functional tests. However, even that can be unwieldy. When a test fails and the problem is ﬁxed, how long will it take to know for sure that the build passes again?\n\nIf your build and test process takes too long, ask your team to analyze the cause of the slowdown and take steps to speed up the build. Here are a few examples.\n\n(cid:2) Database access usually consumes lots of time, so consider using fake objects, where possible, to replace the database, especially at the unit level.\n\n(cid:2) Move longer-running integration and database-access tests to the sec-\n\nondary build and test process.\n\n(cid:2) See if tests can run in parallel so that they ﬁnish faster.\n\nLisa’s Story\n\nWHERE DO TECHNOLOGY-FACING TESTS STOP?\n\n(cid:2) Run the minimum tests needed for regression testing your system. (cid:2) Distribute tasks across multiple build machines. (cid:2) Upgrade the hardware and software that run the build. (cid:2) Find the area that takes the most time and take incremental steps to\n\nspeed it up.\n\nEarly in my current team’s agile evolution, we had few unit tests, so we included a few GUI smoke tests in our continual build, which kicked off on every check-in to the source code control system. When we had enough unit tests to feel good about knowing when code was broken, we moved the GUI tests and the FitNesse functional tests into a separate build and test process that ran at night, on the same machine as our continual build.\n\nOur continual ongoing build started out taking less than 10 minutes, but soon was taking more than 15 minutes to complete. We wrote task cards to diagnose and ﬁx the problem. The unit tests that the programmers had written early on weren’t well designed, because nobody was sure of the best way to write unit tests. Time was budgeted to refactor the unit tests, use mock data access objects instead of the real database, and redesign tests for speed. This got the build to around eight minutes. Every time it has started to creep up, we’ve addressed the problem with refactoring, removing unnecessary tests, upgrading the hardware, and choosing different software that helped the build run faster.\n\nAs our functional tests covered more code, the nightly build broke more often. Because the nightly build ran on the same machine as the continual ongoing one, the only way to verify that the build was “green” again was to stop the ongoing build, which removed our fast feedback. This started to waste everyone’s time. We bought and set up another build machine for the longer build, which now also runs continuously. This was much less expensive than spending so much time keeping two builds running on the same machine, and now we get quick feed- back from our functional tests as well.\n\nWow, multiple continuous build and test processes providing constant feed- back—it sounds like a dream to a lot of testers. Regression bugs will be caught early, when they’re cheapest to ﬁx. This is a great reason for writing technology- facing tests. Can we get too carried away with them, though? Let’s look at the line between technology-facing tests and business-facing tests.\n\nWHERE DO TECHNOLOGY-FACING TESTS STOP? We often hear people worry that the customer-facing tests will overlap so much with the technology-facing tests that the team will waste time. We know that\n\n119\n\n—Lisa\n\n120\n\nCHAPTER 7\n\nChapter 13, “Why We Want to Auto- mate Tests and What Holds Us Back,” talks more about the ROI of the different types of tests.\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nbusiness-facing tests might cover a bit of the same ground as unit or code inte- gration tests, but they have such different purposes that waste isn’t a worry.\n\nFor example, we have a story to calculate a loan amortization schedule and display it to a user who’s in the process of requesting a loan. A unit test for this story would likely test for illegal arguments, such as an annual payment frequency if the business doesn’t allow it. There might be a unit test to ﬁgure the anticipated loan payment start date given some deﬁnition of amount, in- terest rate, start date, and frequency. Unit-level tests could cover different combinations of payment frequency, amount, interest date, term, and start date in order to prove that the amortization calculation is correct. They could cover scenarios such as leap years. When these tests pass, the program- mer feels conﬁdent about the code.\n\nEach unit test is independent and tests one dimension at a time. This means that when a unit test fails, the programmer can identify the problem quickly and solve the issue just as quickly. The business-facing tests very seldom cover only one dimension, because they are tackled from a business point of view.\n\nThe business-facing tests for this story would deﬁne more details for the business rules, the presentation in the user interface, and error handling. They would verify that payment details, such as the principal and interest ap- plied, display correctly in the user interface. They would test validations for each ﬁeld on the user interface, and specify error handling for situations such as insufﬁcient balance or ineligibility. They could test a scenario where an ad- ministrator processes two loan payments on the same day, which might be harder to simulate at the unit level.\n\nThe business-facing tests cover more complex user scenarios and verify that the end user will have a good experience. Push tests to lower levels whenever possible; if you identify a test case that can be automated at the unit level, that’s almost always a better return on investment.\n\nIf multiple areas or layers of the application are involved, it might not be pos- sible to automate at the unit level. Both technology-facing and business- facing levels might have tests around the date of the ﬁrst loan payment, but they check for different reasons. The unit test would check the calculation of the date, and the business-facing test would verify that it displays correctly in the borrower’s loan report.\n\nLearning to write Quadrant 1 tests is hard. Many teams making the transi- tion to agile development start out with no automated unit tests, not even a\n\nLisa’s Story\n\nWHAT IF THE TEAM DOESN’T DO THESE TESTS?\n\ncontinuous integration and build process. In the next section, we suggest ac- tions agile testers can take if their teams don’t tackle Quadrant 1 tests.\n\nWHAT IF THE TEAM DOESN’T DO THESE TESTS? Many an organization has decided to try agile development, or at least stated that intention, without understanding how to make a successful transition. When we’re in a tester role, what can we do to help the development team implement TDD, continuous integration, and other practices that are key to successful development?\n\nOur experience over the years has been that if we aren’t programmers our- selves, we don’t necessarily have much credibility when we urge the program- mers to adopt practices such as TDD. If we could sit down and show them how to code test-ﬁrst, that would be persuasive, but many of us testers don’t have that kind of experience. We’ve also found that evangelizing doesn’t work. It’s not that hard to convince someone conceptually that TDD is a good idea. It’s much trickier to help them get traction actually coding test-ﬁrst.\n\nWhat Can Testers Do?\n\nIf you’re a tester on a so-called “agile” team that isn’t even automating unit tests or producing continuous builds—or at a minimum, doing builds on a daily basis—you’re going to get frustrated pretty quickly. Don’t give up; keep brainstorming for a way to get traction on a positive transition. Try using so- cial time or other relaxing activity to take some quality time to see what new ideas you can generate to get all team members on board.\n\nOne trap to avoid is having testers write the unit tests. Because TDD is really- more of a design activity, it’s essential that the person writing the code also write the tests, before writing the code. Programmers also need the immediate feedback that automated unit tests give. Unit tests written by someone else af- ter the code is written might still guard against regression defects, but they won’t have the most valuable beneﬁts of tests written by the programmer.\n\nWhenever I’ve wanted to effect change, I’ve turned to the patterns in Fearless Change by Mary Lynn Manns and Linda Rising [2004]. After working on two XP teams, I joined a team that professed a desire to become agile but wasn’t making strides toward solid development practices. I found several patterns in Fearless Change to try to move the team toward agile practices.\n\n121\n\n122\n\nCHAPTER 7\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\n“Ask for Help” was one pattern that helped me. This pattern says, in part: “Since the task of introducing a new idea into an organization is a big job, look for peo- ple and resources to help your efforts” [Manns and Rising, 2004]. When I wanted my team to start using FitNesse, I identiﬁed the programmer who was most sympa- thetic to my cause and asked him to pair with me to write FitNesse tests for the story he was working on. He told the other programmers about the beneﬁts he derived from the FitNesse tests, which encouraged them to try it too. Most people want to help, and agile is all about the team working together, so there’s no rea- son to go it alone.\n\n“Brown Bag” is another change pattern that my teams have put to good use. For example, my current team held several brown bag sessions where they wrote unit tests together. “Guru on Your Side” is a productive pattern in which you enlist the help of a well-respected team member who might understand what you’re trying to achieve. A previous team I was on was not motivated to write unit tests. The most experienced programmer on the team agreed with me that test-driven development was a good idea, and he set an example for the rest of the team.\n\nWe think you’ll ﬁnd that there’s always someone on an agile team who’s sympa- thetic to your cause. Enlist that person’s support, especially if the team perceives him or her as a senior-level guru.\n\nAs a tester on an agile team, there’s a lot you can do to act as a change agent, but your potential impact is limited. In some cases, strong management sup- port is the key to driving the team to engage in Quadrant 1 activities.\n\nWhat Can Managers Do?\n\nIf you’re managing a development team, you can do a lot to encourage test- driven development and unit test automation. Work with the product owner to make quality your goal, and communicate the quality criteria to the team. Encourage the programmers to take time to do their best work instead of worrying about meeting a deadline. If a delivery date is in jeopardy, push to reduce the scope, not the quality. Your job is to explain to the business man- agers how making quality a priority will ensure that they get optimum busi- ness value.\n\nGive the team time to learn, and provide expert, hands-on training. Bring in an experienced agile development coach or hire someone with experience in using these practices who can transfer those skills to the rest of the team. Budget time for major refactoring, for brainstorming about the best ap- proach to writing unit and code integration tests, and for evaluating, install- ing, and upgrading tools. Test managers should work with development\n\n—Lisa\n\nMore about retro- spectives and pro- cess improvement in Chapter 19, “Wrap Up the Iteration.”\n\nTOOLKIT\n\nmanagers to encourage practices that enhance testability and allow testers to write executable tests. Test managers can also make sure testers have time to learn how to use the automation tools and frameworks that the team decides to implement.\n\nIt’s a Team Problem\n\nWhile you can ﬁnd ways to be an effective change agent, the best thing to do is involve the whole team in solving the problems. If you aren’t already doing retrospectives after every iteration, propose trying this practice or some other type of process improvement. At the retrospective, raise issues that are hampering successful delivery. For example, “We aren’t ﬁnishing testing tasks before the end of the iteration” is a problem for the whole team to address. If one reason for not ﬁnishing is the high number of unit-level bugs, suggest experimenting with TDD, but allow programmers to propose their own ways to address the problem. Encourage the team to try a new approach for a few iterations and see how it works.\n\nTechnology-facing tests that support the team’s development process are an important foundation for all of the testing that needs to happen. If the team isn’t doing an adequate job with the tests in this quadrant, the other types of testing will be much more difﬁcult. This doesn’t mean you can’t get value from the other quadrants on their own—it just means it will be harder to do so because the team’s code will lack internal quality and everything will take longer.\n\nTechnology-facing tests can’t be done without the right tools and infrastruc- ture. In the next section, we look at examples of the types of tools a team needs to be effective with Quadrant 1 tests.\n\nTOOLKIT There’s no magical tool that will ensure success. However, tools can help good people do their best work. Building up the right infrastructure to sup- port technology-facing tests is critical. There’s a huge selection of excellent tools available, and they improve all the time. Your team must ﬁnd the tools that work best for your situation.\n\nSource Code Control\n\nSource code control is known by other names too, such as version control or revision control. It’s certainly not new, or unique to agile development, but\n\n123",
      "page_number": 158
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 166-174)",
      "start_page": 166,
      "end_page": 174,
      "detection_method": "topic_boundary",
      "content": "124\n\nCHAPTER 7\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nno software development team can succeed without it. That’s why we’re dis- cussing it here. Without source code control, you’ll never be sure what you’re testing. Did the programmer change only the module he said he changed, or did he forget changes he made to other modules? You can’t back out un- wanted or erroneous changes without some kind of versioning system. Source code control keeps different programmers from walking on each other’s changes to the same modules. Without versioning, you can’t be sure what code to release to production.\n\nSoftware Conﬁguration Management Patterns: Effective Teamwork, Practical Integrations [2003], by Stephen Berczuk and Brad Appleton, is a good resource to use to learn how and why to use source code control. Source code control is essential to any style of software development.\n\nUse source code control for automated test scripts, too. It’s important to tie the automated tests with the corresponding code version that they tested in case you need to rerun tests against that version in the future. When you label or tag a build, make sure you label or tag the test code too, even if it doesn’t get released to production.\n\nTeams can organize their code hierarchy to provide a repository for produc- tion code, corresponding unit tests, and higher-level test scripts. Doing this might require some brainstorming and experimenting in order to get the right structure.\n\nThere are many terriﬁc options to choose from. Open source systems such as CVS and Subversion (SVN) are easy to implement, integrate with a continu- ous build process and IDEs, and are robust. Vendor tools such as IBM Ratio- nal ClearCase and Perforce might add features that compensate for the increased overhead they often bring.\n\nSource code control is tightly integrated with development environments. Let’s look at some IDEs used by agile teams.\n\nIDEs\n\nA good IDE (integrated development environment) can be helpful for pro- grammers and testers on an agile team. The IDE integrates with the source code control system to help prevent problems with versioning and changes walking on each other. The editors inside an IDE are speciﬁc to the program- ming language and ﬂag errors even as you write the code. Most importantly, IDEs provide support for refactoring.\n\nLisa’s Story\n\nTOOLKIT\n\nProgrammers who use an IDE tend to have strong personal preferences. However, sometimes an organization decrees that all programmers must use a speciﬁc IDE. This might be because of licensing, or it might be intended to encourage open pair programming. It is easier to pair with another program- mer if the other person uses the same IDE, but it’s generally not essential for the same one to be used. Most tools work similarly, so it’s not hard to change from one IDE to another in order to meet new needs or take advantage of new features. Some diehards still prefer to use tried-and-true technology such as vi, vim, or emacs with make ﬁles rather than an IDE.\n\nOpen source IDEs such as Eclipse and NetBeans are widely used by agile teams, along with proprietary systems such as Visual Studio and IntelliJ IDEA. IDEs have plug-ins to support different languages and tools. They work as well with test scripts as they do with production code.\n\nOn my current team, some programmers were using IntelliJ IDEA, while others used Eclipse. Environmental differences in rare cases caused issues, such as tests passing in the IDE but not the full build, or check-ins via the IDE causing havoc in the source code control system. Generally, though, use of different IDEs caused no problems. Interestingly, over time most of the Eclipse users switched. Pairing with the IntelliJ users led them to prefer it.\n\nI use Eclipse to work with the automated test scripts as well as to research issues with the production code. The Ruby plug-in helps us with our Ruby and Watir scripts, and the XML editor helps with our Canoo WebTest scripts. We can run unit tests and do builds through the IDE. Programmers on the team helped me set up and start using Eclipse, and it has saved huge amounts of time. Maintaining the automated tests is much easier, and the IDE’s “synchronize” view helps me remem- ber to check in all of the modules I’ve changed.\n\nTest tools are starting to come out with their own IDEs or plug-ins to work with existing IDEs such as Eclipse. Take advantage of these powerful, time-saving, quality-promoting tools.\n\nTesters who aren’t automating tests through an IDE, but who want to be able to look at changed snippets of code, can use tools such as FishEye that enable the testers to get access to the code through the automated build.\n\nAs of this writing, IDEs have added support for dynamic languages such as Ruby, Groovy, and Python. Programmers who use dynamic languages may prefer lighter-weight tools, but they still need good tools that support good coding practices, such as TDD and refactoring.\n\n125\n\n—Lisa\n\n126\n\nCHAPTER 7\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nRegardless of the development environment and tools being used, agile teams need a framework that will integrate code changes from different pro- grammers, run the unit tests to verify no regression bugs have occurred, and provide the code in a deployable format.\n\nBuild Tools\n\nYour team needs some way to build the software and create a deployable jar, war, or other type of ﬁle. This can be done with shell-based tools such as make, but those tools have limitations, such as the platforms where they work. Agile teams that we know use tools such as ant, Nant, and Maven to build their projects. These tools not only manage the build but also provide easy ways to report and document build results, and they integrate easily with build automation and test tools. They also integrate with IDEs.\n\nBuild Automation Tools\n\nContinuous integration is a core practice for agile teams. You need a way to not only build the project but also run automated tests on each build to make sure nothing broke. A fully automated and reproducible build that runs many times a day is a key success factor for agile teams. Automated build tools provide features such as email notiﬁcation of build results, and they in- tegrate with build and source code control tools.\n\nCommonly used tools as of the writing of this book include the open source tools CruiseControl, CruiseControl.net, CruiseControl.rb, and Hudson. Other open source and proprietary tools available at publication time are AnthillPro, Bamboo, BuildBeat, CI Factory, Team City, and Pulse, just to name a few.\n\nWithout an automated build process you’ll have a hard time deploying code for testing as well as releasing. Build management and build automation tools are easy to implement and absolutely necessary for successful agile projects. Make sure you get your build process going early, even before you start coding. Experiment with different tools when you ﬁnd you need more features than your current process provides.\n\nUnit Test Tools\n\nUnit test tools are speciﬁc to the language in which you’re coding. “xUnit” tools are commonly used by agile teams, and there’s a ﬂavor for many differ- ent languages, including JUnit for Java, NUnit for .NET, Test::Unit for Perl and Ruby, and PyUnit for Python.\n\nSee Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” for more information on behavior- driven develop- ment tools.\n\nSee the bibliogra- phy for links and books to help your team search for the right unit test tools.\n\nSUMMARY\n\nBehavior-driven development is another ﬂavor of test-driven development, spelling out expected behavior to drive tests with tools such as RSpec and easyb.\n\nGUI code can and should be developed test-ﬁrst as well. Some tools for rich- client unit testing are TestNG, Abbot, and SWTBot.\n\nTools such as EasyMock and Ruby/Mock help with implementing mock ob- jects and test stubs, an integral part of well-designed unit tests.\n\nThe tools programmers use to write technology-facing tests can also be used for business-facing tests. Whether they are suited for that purpose in your project depends on the needs of your team and your customers.\n\nSUMMARY In this chapter, we explained the purpose of technology-facing tests that sup- port the team, and we talked about what teams need to use them effectively.\n\n(cid:2) Technology-facing tests that support programming let the team pro- duce the highest quality code possible; they form the foundation for all other types of testing.\n\n(cid:2) The beneﬁts of this quadrant’s tests include going faster and doing more, but speed and quantity should never be the ultimate goal. (cid:2) Programmers write technology-facing tests that support the team and provide great value to testers by enhancing the internal quality and testability of the system.\n\n(cid:2) Teams that fail to implement the core practices related to agile devel-\n\nopment are likely to struggle.\n\n(cid:2) Legacy systems usually present the biggest obstacles to test-driven\n\ndevelopment, but these problems can be overcome with incremental approaches.\n\n(cid:2) If your team doesn’t now do these tests, you can help them get\n\nstarted by engaging other team members and getting support from management.\n\n(cid:2) There can be some overlap between technology-facing tests and business-facing tests that support the team. However, when faced with a choice, push tests to the lowest level in order to maximize ROI. (cid:2) Teams should set up continuous integration, build, and test processes\n\nin order to provide feedback as quickly as possible.\n\n(cid:2) Agile teams require tools for tasks such as source code control, test automation, IDEs, and build management in order to facilitate technology-facing tests that support the team.\n\n127\n\nThis page intentionally left blank\n\nChapter 8\n\nBUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nTestability and Automation\n\nDriving Development with Business-Facing Tests\n\nCommon Language\n\nEliciting Requirements\n\nPost Conditions\n\nPeril: Forgetting the Big Picture\n\nTest Mitigate Risks\n\nBusiness-Facing Tests that Support the Team\n\nThe Requirements Quandary\n\nAdvance Clarity\n\nConditions of Satisfaction\n\nRipple Effects\n\nCustomer Availability\n\nKnowing When We’re Done\n\nThin Slices, Small Chunks\n\nIn the last chapter, we talked about programmer tests, those low-level tests that help programmers make sure they have written the code right. How do they know the right thing to build? In phased and gated methodologies, we try to solve that by gathering requirements up front and putting as much detail in them as possible. In projects using agile practices, we put all our faith in story cards and tests that customers understand in order to help code the right thing. These “understandable” tests are the subject of this chapter.\n\nDRIVING DEVELOPMENT WITH BUSINESS-FACING TESTS Yikes, we’re starting an iteration with no more information than what ﬁts on an index card, something like what’s shown in Figure 8-1.\n\nThat’s not much information, and it’s not meant to be. Stories are a brief de- scription of desired functionality and an aid to planning and prioritizing work. On a traditional waterfall project, the development team might be\n\n129\n\n130\n\nCHAPTER 8\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nStory PA-2\n\nAs an internet shopper on LotsO’Stuff.xx, I want free\n\nshipping when my order exceeds the free shipping\n\nthreshold, so that I can take advantage of ordering\n\nmore at one time.\n\nFigure 8-1 Story to set up conversation\n\ngiven a wordy requirements document that includes every detail of the fea- ture set. On an agile project, the customer team and development team strike up a conversation based on the story. The team needs requirements of some kind, and they need them at a level that will let them start writing working code almost immediately. To do this, we need examples to turn into tests that will conﬁrm what the customer really wants.\n\nThese business-facing tests address business requirements. These tests help provide the big picture and enough details to guide coding. Business-facing tests express requirements based on examples and use a language and format that both the customer and development teams can understand. Examples form the basis of learning the desired behavior of each feature, and we use those examples as the basis for our story tests in Quadrants 2 (see Figure 8-2).\n\nBusiness-facing tests are also called “customer-facing,” “story,” “customer,” and “acceptance” tests. The term “acceptance test” is particularly confusing, because it makes some people think only of “user acceptance tests.” In the context of agile development, acceptance tests generally refer to the business- facing tests, but the term could also include the technology-facing tests from Quadrant 4, such as the customer’s criteria for system performance or secu- rity. In this chapter, we’re discussing only the business-facing tests that sup- port the team by guiding development and providing quick feedback.\n\nAs we explained in the previous two chapters, the order in which we present these four quadrants isn’t related to the order in which we might perform\n\nPart V, “An Itera- tion in the Life,” examines the or- der in which we perform tests from the different quadrants.\n\nDRIVING DEVELOPMENT WITH BUSINESS-FACING TESTS\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nQ2\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 8-2 The Agile Testing Quadrants, highlighting Quadrant 2\n\nactivities from each quadrant. The business-facing tests in Quadrant 2 are written for each story before coding is started, because they help the team understand what code to write. Like the tests in Quadrant 1, these tests drive development, but at a higher level. Quadrant 1 activities ensure internal qual- ity, maximize team productivity, and minimize technical debt. Quadrant 2 tests deﬁne and verify external quality, and help us know when we’re done.\n\nThe customer tests to drive coding are generally written in an executable for- mat, and automated, so that team members can run the tests as often as they like in order to see if the functionality works as desired. These tests, or some subset of them, will become part of an automated regression suite so that fu- ture development doesn’t unintentionally change system behavior.\n\nAs we discuss the stories and examples of desired behavior, we must also deﬁne nonfunctional requirements such as performance, security, and usability. We’ll\n\n131\n\n132\n\nCHAPTER 8\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nalso make note of scenarios for manual exploratory testing. We’ll talk about these other types of testing activities in the chapters on Quadrants 3 and 4.\n\nWe hear lots of questions relating to how agile teams get requirements. How do we know what the code we write should do? How do we obtain enough information to start coding? How do we get the customers to speak with one voice and present their needs clearly? Where do we start on each story? How do we get customers to give us examples? How do we use those to write story tests?\n\nThis chapter explains our strategy for creating business-facing tests that sup- port the team as it develops each story. Let’s start by talking more about requirements.\n\nTHE REQUIREMENTS QUANDARY Just about every development team we’ve known, agile or not, struggles with requirements. Teams on traditional waterfall projects might invest months in requirements gathering only to have them be wrong or quickly get out of date. Teams in chaos mode might have no requirements at all, with the pro- grammers making their best guess as to how a feature should work.\n\nAgile development embraces change, but what happens when requirements change during an iteration? We don’t want a long requirements-gathering period before we start coding, but how can we be sure we (and our custom- ers) really understand the details of each story?\n\nIn agile development, new features usually start out life as stories, or groups of stories, written by the customer team. Story writing is not about ﬁguring out implementation details, although high-level discussions can have an im- pact on dependencies and how many stories are created. It’s helpful if some members of the technical team can participate in story-writing sessions so that they can have input into the functionality stories and help ensure that technical stories are included as part of the backlog. Programmers and testers can also help customers break stories down to appropriate sizes, suggest al- ternatives that might be more practical to implement, and discuss dependen- cies between stories.\n\nStories by themselves don’t give much detail about the desired functionality. They’re usually just a sentence that expresses who wants the feature, what the feature is, and why they want it. “As an Internet shopper, I need a way to delete items from my shopping cart so I don’t have to buy unwanted items” leaves a",
      "page_number": 166
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 175-182)",
      "start_page": 175,
      "end_page": 182,
      "detection_method": "topic_boundary",
      "content": "THE REQUIREMENTS QUANDARY\n\nlot to the imagination. Stories are only intended as a starting point for an on- going dialogue between business experts and the development team. If team members understand what problem the customer is trying to solve, they can suggest alternatives that might be simpler to use and implement.\n\nIn this dialogue between customers and developers, agile teams expand on stories until they have enough information to write appropriate code. Testers help elicit examples and context for each story, and help customers write story tests. These tests guide programmers as they write the code and help the team know when it has met the customers’ conditions of satisfaction. If your team has use cases, they can help to supplement the example or coach- ing test to clarify the needed functionality (see Figure 8-3).\n\nIn agile development, we accept that we’ll never understand all of the re- quirements for a story ahead of time. After the code that makes the story tests pass is completed, we still need to do more testing to better understand the requirements and how the features should work.\n\nAfter customers have a chance to see what the team is delivering, they might have different ideas about how they want it to work. Often customers have a vague idea of what they want and a hard time deﬁning exactly what that is. The team works with the customer or customer proxy for an iteration and might deliver just a kernel of a solution. The team keeps reﬁning the function- ality over multiple iterations until it has deﬁned and delivered the feature.\n\nBeing able to iterate is one reason agile development advocates small releases and developing one small chunk at a time. If our customer is unhappy with the behavior of the code we deliver in this iteration, we can quickly rectify that in the next, if they deem it important. Requirements changes are pretty much inevitable.\n\nWe must learn as much as we can about our customers’ wants and needs. If our end users work in our location, or it’s feasible to travel to theirs, we should sit with them, work alongside them, and be able to do their jobs if we can. Not only will we understand their requirements better but we might even identify requirements they didn’t think to state.\n\nStory\n\n+\n\nExample/ Coaching Test\n\n+\n\nConversation\n\n=\n\nRequirement\n\nFigure 8-3 The makeup of a requirement\n\n133\n\n134\n\nCHAPTER 8\n\nMore on Fit in Chapter 9, “Tool- kit for Business- Facing Tests that Support the Team.”\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nTests need to include more than the customers’ stated requirements. We need to test for post conditions, impact on the system as a whole, and integration with other systems. We identify risks and mitigate those with tests as needed. All of these factors guide our coding.\n\nCommon Language\n\nWe can also use our tests to provide a common language that’s understood by both the development team and the business experts. As Brian Marick [2004] points out, a shared language helps the business people envision the features they want. It helps the programmers craft well-designed code that’s easy to extend. Real-life examples of desired and undesired behavior can be expressed so that they’re understood by both the business and technical sides. Pictures, ﬂow diagrams, spreadsheets, and prototypes are accessible to people with different backgrounds and viewpoints. We can use these tools to ﬁnd examples and then easily turn those examples into tests. The tests need to be written in a way that’s comprehensible to a business user reading them yet still executable by the technical team.\n\nBusiness-facing tests also help deﬁne scope, so that everyone knows what is part of the story and what isn’t. Many of the test frameworks now allow teams to create a domain language and deﬁne tests using that language. Fit (Functional for Integrated Framework) is one of those.\n\nThe Perfect Customer\n\nAndy Pols allowed us to reprint this story from his blog [Pols, 2008]. In it, he shows how his customer demanded a test, wrote it, and realized the story was out of scope.\n\nOn a recent project, our customer got so enthusiastic about our Fit tests that he got extremely upset when I implemented a story without a Fit test. He refused to let the system go live until we had the Fit test in place.\n\nThe story in question was very technical and involved sending a particular XML message to an external system. We just could not work out what a Fit test would look like for this type of requirement. Placing the expected XML message, with all its gory detail, in the Fit test would not have been helpful because this is a technical artifact and of no interest to the busi- ness. We could not work out what to do. The customer was not around to discuss this, so I just went ahead and implemented the story (very naughty!).\n\nTHE REQUIREMENTS QUANDARY\n\nWhat the customer wanted was to be sure that we were sending the correct product information in the XML message. To resolve the issue, I suggested that we have a Fit test that shows how the product attributes get mapped onto the XML message using Xpath, although I still thought this was too technical for a business user.\n\nWe gave the customer a couple of links to explain what XPath was so that he could explore whether this was a good solution for him. To my amazement, he was delighted with XPath (I now know who to turn to when I have a problem with XPath) and ﬁlled in the Fit test.\n\nThe interesting bit for me is that as soon as he knew what the message looked like and how it was structured, he realized that it did not really support the business—we were sending information that was outside our scope of our work and that should have been supplied by another system. He was also skeptical about the speed at which the external team could add new products due to the complex nature of the XML.\n\nMost agile people we tell this story to think we have the “perfect customer!”\n\nEven if your customers aren’t perfect, involving them in writing customer tests gives them a chance to identify functionality that’s outside the scope of the story. We try to write customer tests that customers can read and compre- hend. Sometimes we set the bar too low. Collaborate with your customers to ﬁnd a tool and format for writing tests that works for both the customer and development teams.\n\nIt’s ﬁne to say that our customers will provide to us the examples that we need to have in order for us to understand the value that each story should deliver. But what if they don’t know how to explain what they want? In the next section, we’ll suggest ways to help customers deﬁne their conditions of satisfaction.\n\nEliciting Requirements\n\nIf you’ve ever been a customer requesting a particular software feature, you know how hard it is to articulate exactly what you want. Often, you don’t re- ally know exactly what you want until you can see, feel, touch and use it. We have lots of ways to help our customers get clarity about what they want.\n\nAsk Questions Start by asking questions. Testers can be especially good at asking a variety of questions because they are conscious of the big picture, the business-facing\n\n135\n\n136\n\nCHAPTER 8\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nand technical aspects of the story, and are always thinking of the end user ex- perience. Types of general questions to ask are:\n\n(cid:2) Is this story solving a problem? (cid:2) If so, what’s the problem we’re trying to solve? (cid:2) Could we implement a solution that doesn’t solve the problem? (cid:2) How will the story bring value to the business? (cid:2) Who are the end users of the feature? (cid:2) What value will they get out of it? (cid:2) What will users do right before and right after they use that feature? (cid:2) How do we know we’re done with this story?\n\nOne question Lisa likes to ask is, “What’s the worst thing that could happen?” Worst-case scenarios tend to generate ideas. They also help us consider risk and focus our tests on critical areas. Another good question is, “What’s the best thing that could happen?” This question usually generates our happy path test, but it might also uncover some hidden assumptions.\n\nUse Examples Most importantly, ask the customer to give you examples of how the feature should work. Let’s say the story is about deleting items out of an online shop- ping cart. Ask the customer to draw a picture on a whiteboard of how that delete function might look. Do they want any extra features, such as a conﬁr- mation step, or a chance to save the item in case they want to retrieve it later? What would they expect to see if the deletion couldn’t be done?\n\nExamples can form the basis for our tests. Our challenge is to capture exam- ples, which might be expressed in the business domain language, as tests that can actually be executed. Some customers are comfortable expressing exam- ples using a test tool such as Fit or FitNesse as long as they can write them in their domain language.\n\nLet’s explore the difference between an example and a test with a simple story (see Figure 8-4). People often get confused between these two terms.\n\nAn example would look something like this:\n\nThere are 5 items on a page. I want to select item 1 for $20.25 and put it in the shopping cart. I click to the next page, which has 5 more items. I select a second item on that page for $5.38 and put it in my shopping cart. When I say I’m done shopping, it will show both the item from the\n\nTHE REQUIREMENTS QUANDARY\n\n137\n\nStory PA-2\n\nAs a shopper, I want to add items to the shopping\n\ncart so I can pay for them all at once.\n\nFigure 8-4 Story to use as a base for examples and tests\n\nﬁrst page and the item from the second page in my shopping cart, with the total of $25.63\n\nThe test could be quite a bit different. We’ll use a Fit type format in Table 8-1 to show you how the test could be represented.\n\nThe test captures the example in an executable format. It might not use exactly the same inputs, but it encapsulates the sample user scenario. More test cases can be written to test boundary conditions, edge cases, and other scenarios.\n\nMultiple Viewpoints Each example or test has one point of view. Different people will write differ- ent tests or examples from their unique perspectives. We’d like to capture as many different viewpoints as we can, so think about your users.\n\nTable 8-1 Test for Story PA-2\n\nInputs\n\nExpected Results\n\nID\n\nItem\n\nPrice\n\nTotal Cost\n\n# of Items\n\n001\n\nItem A\n\n20.25\n\n20.25\n\n1\n\n002\n\nItem D\n\n0.01\n\n20.26\n\n2\n\n003\n\nItem F\n\n100.99\n\n121.25\n\n3\n\n138\n\nCHAPTER 8\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nGetting the requirements right is an area where team members in many dif- ferent roles can jump in to help. Business analysts, subject matter experts, programmers, and various members of the customer team all have some- thing to contribute. Think about other stakeholders, such as your production support team. They have a very unique perspective.\n\nWe often forget about nonfunctional requirements such as “How long does the system need to be up? What happens if it fails? If we have middleware that passes messages, do we expect messages to be large enough that we might need to consider loss during transmission? Or will they be a constant size? What happens if there is no trafﬁc for hours? Does the system need to warn someone?” Testing for these types of requirements usually falls into quadrants 3 and 4, but we still need to write tests to make sure they get done.\n\nAll of the examples that customers give to the team add up quickly. Do we re- ally have to turn all of these into executable tests? Not as long as we have the customers there to tell us if the code is working the way they want. With tech- niques such as paper prototyping, designs can be tested before a line of code is written.\n\nWizard of Oz Testing\n\nGerard Meszaros, a Certiﬁed ScrumMaster (Practicing) and Agile Coach, shared his story about Wizard of Oz Testing on Agile Projects. He describes a good example of how artifacts we generate to elicit requirements can help communicate meaning in an unambiguous form.\n\nWe thought we were ready to release our software. We had been build- ing it one iteration at a time under the guidance of an on-site customer who had prioritized the functionality based on what he needed to enter into integration testing with his business partners. We consciously deferred the master data maintenance and reporting functionality to later iterations to ensure we had the functionality needed for integration testing ready. The integration testing went ﬁne, with just a few defects logged (all related to missing or misunderstood functionality). In the meantime, we implemented the master data maintenance in parallel with integration testing in the last few iterations. When we went into accep- tance testing with the business users, we got a rude shock: They hated the maintenance and reporting functionality! They logged so many defects and “must-have improvements” that we had to delay the release by a month. So much for coming up with a plan that would allow us to deliver early!\n\nTHE REQUIREMENTS QUANDARY\n\nWhile we were reimplementing the master data maintenance, I attended the Agile 2005 conference and took a tutorial by Jeff Patton. One of the exercises was building paper prototypes of the UI for a sample applica- tion. Then we “tested” the paper prototypes with members of the other groups as our users and found out how badly ﬂawed our UI designs were. Déjà vu! The tutorial resembled my reality.\n\nOn my return to the project back home, I took the project manager I was mentoring in agile development aside and suggested that paper proto- typing and “Wizard of Oz” testing (the Wizard of Oz reference is to a human being acting as a computer—sort of the “man behind the cur- tain”) might have avoided our one-month setback. After a very short dis- cussion, we decided to give it a try on our release 2 functionality. We stayed late a couple of evenings and designed the UI using screenshots from the R1 functionality overlaid with hand-drawn R2 functionality. It was a long time since either of us had used scissors and glue sticks, and it was fun!\n\nFor the Wizard of Oz testing with users, we asked our on-site customers to ﬁnd some real users with whom to do the testing. They also came up with some realistic sample tasks for the users to try to execute. We put the sample data into Excel spreadsheets and printed out various combi- nations of data grids to use the in the testing. Some future users came to town for a conference. We hijacked pairs of them for an hour each and did our testing.\n\nI acted as the “wizard,” playing the part of the computer (“it’s a 286 pro- cessor so don’t expect the response times to be very good”). The on-site customer introduced the problem and programmers acted as observers, recording the missteps the users made as “possible defects.” After just a few hours, we had huge amounts of valuable data about which parts of our UI design worked well and which parts needed rethinking. And there was little argument about which was which! We repeated the usability testing with other users when we had alpha versions of the application available and gained further valuable insights. Our business customer found the exercise so valuable that on a subsequent project the business team set about doing the paper prototyping and Wizard of Oz testing with no prompting from the development team. This might have been inﬂuenced somewhat by the ﬁrst e-mail we got from a real user 30 minutes after going live: “I love this application!!!”\n\nDeveloping user interfaces test-ﬁrst can seem like an intimidating effort. The Wizard of Oz technique can be done before writing a single line of code. The team can test user interaction with the system and gather plenty of informa- tion to understand the desired system behavior. It’s a great way to facilitate communication between the customer and development teams.\n\n139\n\n140\n\nCHAPTER 8\n\nLisa’s Story\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nClose, constant collaboration between the customer team and the developer team is key to obtaining examples on which to base customer tests that drive coding. Communication is a core agile value, and we talk about it more in the next section.\n\nCommunicate with Customers In an ideal world, our customers are available to us all day, every day. In real- ity, many teams have limited access to their business experts, and in many cases, the customers are in a different location or time zone. Do whatever you can to have face-to face conversations. When you can’t, conference calls, phone conversations, emails, instant messages, cameras, and other commu- nication tools will have to substitute. Fortunately, more tools to facilitate re- mote communication are available all the time. We’ve heard of teams, such as Erika Boyer’s team at iLevel by Weyerhaeuser, that use webcams that can be controlled by the folks in the remote locations. Get as close to you can to di- rect conversation.\n\nI worked on a team where the programmers were spread through three time zones and the customers were in a different one. We sent different programmers, testers, and analysts to the customer site for every iteration, so that each team member had “face time” with the customers at least every third iteration. This built trust and conﬁdence between the developer and customer teams. The rest of the time we used phone calls, open conference calls, and instant messages to ask questions. With continual ﬁne-tuning based on retrospective discussions, we suc- ceeded in satisfying and even delighting the customers.\n\nEven when customers are available and lines of communication are wide open, communication needs to be managed. We want to talk to each member of the customer team, but they all have different viewpoints. If we get several different versions of how a piece of functionality should work, we won’t know what to code. Let’s consider ways to get customers to agree on the con- ditions of satisfaction for each story.\n\nAdvance Clarity\n\nIf your customer team consists of people from different parts of the organiza- tion, there may be conﬂicting opinions among them about exactly what’s in- tended by a particular story. In Lisa’s company, business development wants\n\n—Lisa",
      "page_number": 175
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 183-191)",
      "start_page": 183,
      "end_page": 191,
      "detection_method": "topic_boundary",
      "content": "Lisa’s Story\n\nJanet’s Story\n\nTHE REQUIREMENTS QUANDARY\n\nfeatures that generate revenue, operations wants features that cut down on phone support calls, and ﬁnance wants features that streamline accounting, cash management, and reporting. It’s amazing how many unique interpreta- tions of the same story can emerge from people who have differing viewpoints.\n\nAlthough we had a product owner when we ﬁrst implemented Scrum, we still got different directives from different customers. Management decided to appoint a vice president with extensive domain and operations knowledge as the new product owner. He is charged with getting all of the stakeholders to agree on each story’s implications up front. He and the rest of the customer team meet reg- ularly to discuss upcoming themes and stories, and to agree on priorities and con- ditions of satisfaction. He calls this “advance clarity.”\n\nA Product Owner is a role in Scrum. He’s responsible not only for achieving advance clarity but also for acting as the “customer representative” in priori- tizing stories. There’s a downside, though. When you funnel the needs of many different viewpoints through one person, something can be lost. Ide- ally, the development team should sit together with the customer team and learn how to do the customer’s work. If we understand the customer’s needs well enough to perform its daily tasks, we have a much better chance of pro- ducing software that properly supports those tasks.\n\nOur team didn’t implement the product owner role at ﬁrst and used the domain experts on the team to determine prioritization and clarity. It worked well, but the achieving consensus took many meetings because each person had different experiences. The product was better for it, but there were trade-offs. The many meetings meant the domain experts were not always available for answering ques- tions from the programmers, so coding was slower than anticipated.\n\nThere were four separate project teams working on the same product, but each one was focused on different features. After several retrospectives, and a lot of problem-solving sessions, each project team appointed a Product Owner. The number of meetings was cut down signiﬁcantly because most business decisions were made by the domain experts on their particular project. Meetings were held for all of the domain experts if there were any differences of opinion, and the Product Owner facilitated bringing consensus on an issue. Decisions were made much faster, the domain experts were more available for answering questions by the team, and were able to keep up with the acceptance tests.\n\n141\n\n—Lisa\n\n—Janet\n\n142\n\nCHAPTER 8\n\nChapter 9, “Tool- kit for Business- Facing Tests that Support the Team,” includes example check- lists as well as other tools for expressing requirements.\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nHowever your team chooses to bring together varying viewpoints, it is im- portant that there is only “one voice of the customer” presented to the team.\n\nWe said that product owners provide conditions of satisfaction. Let’s look more closely at what we mean.\n\nConditions of Satisfaction\n\nThere are conditions of satisfaction for the whole release as well as for each feature or story. Acceptance tests help deﬁne the story acceptance. Your devel- opment team can’t successfully deliver what the business wants unless condi- tions of satisfaction for a story are agreed to up front. The customer team needs to “speak with one voice.” If you’re getting different requirements from different stakeholders, you might need to push back and put off the story until you have a ﬁrm list of business satisfaction conditions. Ask the customer rep- resentative to provide a minimum amount of information on each story so that you can start every iteration with a productive conversation.\n\nThe best way to understand the customer team’s requirements is to talk with the customers face to face. Because everyone struggles with “requirements,” there are tools to help the customer team work through each story. Condi- tions of satisfaction should include not only the features that the story deliv- ers but also the impacts on the larger system.\n\nLisa’s product owner uses a checklist format to sort out issues such as:\n\n(cid:2) Business satisfaction conditions (cid:2) Impact on existing functions such as the website, documents,\n\ninvoices, forms, or reports\n\n(cid:2) Legal considerations (cid:2) The impact on regularly scheduled processes (cid:2) References to mock-ups for UI stories (cid:2) Help text, or who will provide it (cid:2) Test cases (cid:2) Data migration (as appropriate) (cid:2) Internal communication that needs to happen (cid:2) External communication to business partners and vendors\n\nThe product owner uses a template to put this information on the team’s wiki so that it can be used as team members learn about the stories and start writing tests.\n\nJanet’s Story\n\nChapter 16, “Hit the Ground Run- ning,” and Chap- ter 17, “Iteration Kickoff,” give ex- amples of when and how teams can plan cus- tomer tests and explore the wider impact of each story.\n\nTHE REQUIREMENTS QUANDARY\n\nThese conditions are based on key assumptions and decisions made by the customer team for a story. They generally come out of conversations with the customer about high-level acceptance criteria for each story. Discussing con- ditions of satisfaction helps identify risky assumptions and increases the team’s conﬁdence in writing and correctly estimating all of the tasks that are needed to complete the story.\n\nRipple Effects\n\nIn agile development, we focus on one story at a time. Each story is usually a small component of the overall application, but it might have a big ripple ef- fect. A new story drops like a little stone into the application water, and we might not think about what the resulting waves might run into. It’s easy to lose track of the big picture when we’re focusing on a small number of stories in each iteration.\n\nLisa’s team ﬁnds it helpful to make a list of all of the parts of the system that might be affected by a story. The team can check each “test point” to see what requirements and test cases it might generate. A small and innocent story might have a wide-ranging impact, and each part of the application that it touches might present another level of complexity. You need to be aware of all the potential impacts of any code change. Making a list is a good place to start. In the ﬁrst few days of the iteration, the team can research and analyze affected areas further and see whether any more task cards are needed to cover them all.\n\nIn one project I was on, we used a simple spreadsheet that listed all of the high- level functionality of the application under test. During release planning, and at the start of each new iteration, we reviewed the list and thought about how the new or changing functionality would affect those areas. That became the starting point for determining what level of testing needed to be done in each functional area. This impact analysis was in addition to the actual story testing and enabled our team to see the big picture and the impact of the changes to the rest of the system.\n\nStories that look small but that impact unexpected areas of the system can come back to bite you. If your team forgets to consider all dependencies, and if the new code intersects with existing functionality, your story might take much longer than planned to ﬁnish. Make sure your story tests include the less obvious fallout from implementing the new functionality.\n\n143\n\n—Janet\n\n144\n\nCHAPTER 8\n\nSee Chapter 10, “Business-Facing Tests that Critique the Product,” for more about ex- ploratory testing.\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nTake time to identify the central value each story provides and ﬁgure out an incremental approach to developing it. Plan small increments of writing tests, writing code, and testing the code some more. This way, your Quadrant 2 tests ensure you’ll deliver the minimum value as planned.\n\nTHIN SLICES, SMALL CHUNKS Writing stories is a tricky business. When the development team estimates new stories, it might ﬁnd some stories too big, so it will ask the customer team to go back and break them into smaller stories. Stories can be too small as well, and might need to be combined with others or simply treated as tasks. Agile development, including testing, takes on one small chunk of functionality at a time.\n\nWhen your team embarks on a new project or theme, ask the product owner to bring all of the related stories to a brainstorming session prior to the ﬁrst iteration for that theme. Have the product owner and other interested stake- holders explain the stories. You might ﬁnd that some stories need to be sub- divided or that additional stories need to be written to ﬁll in gaps.\n\nAfter you understand what value each story should deliver and how it ﬁts in the context of the system, you can break the stories down into small, man- ageable pieces. You can write customer tests to deﬁne those small increments, while keeping in mind the impact on the larger application.\n\nA smart incremental approach to writing customer tests that guide develop- ment is to start with the “thin slice” that follows a happy path from one end to the other. Identifying a thin slice, also called a “steel thread” or “tracer bul- let,” can be done on a theme level, where it’s used to verify the overall archi- tecture. This steel thread connects all of the components together, and after it’s solid, more functionality can be added.\n\nWe ﬁnd this strategy works at the story level, too. The sooner you can build the end-to-end path, the sooner you can do meaningful testing, get feedback, start automating tests, and start exploratory testing. Begin with a thin slice of the most stripped-down functionality that can be tested. This can be thought of as the critical path. For a user interface, this might start with simply navigating from one page to the next. We can show this to the customer and see whether the ﬂow makes sense. We could write a simple automated GUI test. For the free-shipping threshold story at the beginning of this chapter, we might start by verifying the logic used to sum up the order total and determine whether it\n\nSee Part IV, “Auto- mation,” for more about regression test automation.\n\nLisa’s Story\n\nTHIN SLICES, SMALL CHUNKS\n\nqualiﬁes for free shipping, without worrying about how it will look on the UI. We could automate tests for it with a functional test tool such as FitNesse.\n\nAfter the thin slice is working, we can write customer tests for the next chunk or layer of functionality, and write the code that makes those tests pass. Now we’ll have feedback for this small increment, too. Maybe we add the UI to display the checkout page showing that the order qualiﬁed for free shipping, or add the layer to persist updates to the database. We can add on to the auto- mated tests we wrote for the ﬁrst pass. It’s a process of “write tests—write code—run tests—learn.” If you do this, you know that all of the code your team produces satisﬁes the customer and works properly at each stage.\n\nMy team has found that we have to focus on accomplishing a simple thin slice and add to it in tiny increments. Before we did this, we tended to get stuck on one part of the story. For example, if we had a UI ﬂow that included four screens, we’d get so involved in the ﬁrst one that we might not get to the last one, and there was no working end-to-end path. By starting with an end-to-end happy path and adding functionality a step at a time, we can be sure of delivering the minimum value needed.\n\nHere’s an example of our process. The story was to add a new conditional step to the process of establishing a company’s retirement plan. This step allows users to select mutual fund portfolios, but not every user has access to this feature. The retirement plan establishment functionality is written in old, poorly designed leg- acy code. We planned to write the new page in the new architecture, but linking the new and old code together is tricky and error prone. We broke the story down into slices that might look tiny but that allowed us to manage risk and mini- mize the time needed to code and test the story. Figure 8-5 shows a diagram of incremental steps planned for this story.\n\nThe #1 thin slice is to insert a new, empty page based on a property. While it’s not much for our customers to look at, it lets us test the bridge between old and new code, and then verify that the plan establishment navigation still works properly. Slice #2 introduces some business logic: If no mutual fund portfolios are available for the company, skip to the fund selection step, which we’re not changing yet. If there are fund portfolios available, display them on the new step 3. In slice #3, we change the fund selection step, adding logic to display the funds that make up the portfolios. Slice #4 adds navigational elements between various steps in the establishment process.\n\nWe wrote customer tests to deﬁne each slice. As the programmers completed each one, we manually tested it and showed it to our customers. Any problems found were ﬁxed immediately. We wrote an automated GUI test for slice #1, and added to it as the remaining steps were ﬁnished. The story was difﬁcult because of the old legacy code interacting with the new architecture, but the stepwise approach made implementation smooth, and saved time.\n\n145\n\n146\n\nCHAPTER 8\n\nCheck the bibliog- raphy for Gerard Meszaros’s article “Using Storyo- types to Split Bloated XP Stories.”\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nFigure 8-5 Incremental steps\n\nWhen we draw diagrams such as this to break stories into slices, we upload pho- tos of them to our team wiki so our remote team member can see them too. As each step is ﬁnished, we check it off in order to provide instant visual feedback.\n\nIf the task of writing customer tests for a story seems confusing or over- whelming, your team might need to break the story into smaller steps or chunks. Finishing stories a small step at a time helps spread out the testing effort so that it doesn’t get pushed to the end of the iteration. It also gives you a better picture of your progress and helps you know when you’re done—a subject we’ll explore in the next section.\n\nHOW DO WE KNOW WE’RE DONE? We have our business-facing tests that support the team—those tests that have been written to ensure the conditions of satisfaction have been met. They start with the happy path and show that the story meets the intended\n\n—Lisa\n\nTESTS MITIGATE RISK\n\nneed. They cover various user scenarios and ensure that other parts of the system aren’t adversely affected. These tests have been run, and they pass (or at least they’ve identiﬁed issues to be ﬁxed).\n\nAre we done now? We could be, but we’re not sure yet. The true test is whether the software’s user can perform the action the story was supposed to provide. Activities from Quadrants 3 and 4, such as exploratory testing, us- ability testing, and performance testing will help us ﬁnd out. For now, we just need to do some customer tests to ensure that we have captured all of the requirements. The business users or product owners are the right people to determine whether every requirement has been delivered, so they’re the right people to do the exploring at this stage.\n\nWhen the tests all pass and any missed requirements have been identiﬁed, we are done for the purpose of supporting the programmers in their quest for code that does the “right thing.” It does not mean we are done testing. We’ll talk much more about that in the chapters that follow.\n\nAnother goal of customer tests is to identify high-risk areas and make sure the code is written to solidify those. Risk management is an essential practice in any software development methodology, and testers play a role in identify- ing and mitigating risks.\n\nTESTS MITIGATE RISK Customer tests are written not only to deﬁne expected behavior of the code but to manage risk. Driving development with tests doesn’t mean we’ll iden- tify every single requirement up front or be able to predict perfectly when we’re done. It does give us a chance to identify risks and mitigate them with executable test cases. Risk analysis isn’t a new technique. Agile development inherently mitigates some risks by prioritizing business value into small, tested deliverable pieces and by having customer involvement in incremental acceptance. However, we should still brainstorm potential events, the proba- bility they might occur, and the impact on the organization if they do hap- pen so that the right mitigation strategy can be employed.\n\nCoding to predeﬁned tests doesn’t work well if the tests are for improbable edge cases. While we don’t want to test only the happy path, it’s a good place to start. After the happy path is known, we can deﬁne the highest risk scenar- ios—cases that not only have a bad outcome but also have a good possibility of happening.\n\n147\n\n148\n\nCHAPTER 8\n\nLisa’s Story\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nIn addition to asking the customer team questions such as “What’s the worst thing that could happen?,” ask the programmers questions like these: “What are the post conditions of this section of code? What should be persisted in the database? What behavior should we look for down the line?” Specify tests to cover potentially risky outcomes of an action.\n\nMy team considers worst-case scenarios in order to help us identify customer tests. For example, we planned a story to rewrite the ﬁrst step of a multistep account creation wizard with a couple of new options. We asked ourselves questions such as the following: “When the user submits that ﬁrst page, what data is inserted in the database? Are any other updates triggered? Do we need to regression test the entire account setup process? What about activities the user account might do after setup?” We might need to test the entire life cycle of the account. We don’t have time to test more than necessary, so decisions about what to test are critical. The right tests help us mitigate the risk brought by the change.\n\nProgrammers can identify fragile parts of the code. Does the story involve stitching together legacy code with a new architecture? Does the code being changed interact with another system or depend on third-party software? By discussing potential impacts and risky areas with programmers and other team members, we can plan appropriate testing activities.\n\nThere’s another risk. We might get so involved writing detailed test cases up front that the team loses the forest in the trees; that is, we can forget the big picture while we concentrate on details that might prove irrelevant.\n\nPeril: Forgetting the Big Picture\n\nIt’s easy to slip into the habit of testing only individual stories or basing your testing on what the programmer tells you about the code. If you ﬁnd yourself ﬁnding integration problems between stories late in the release or that a lot of requirements are missing after the story is “done,” take steps to mitigate this peril.\n\nAlways consider how each individual story impacts other parts of the system. Use realistic test data, use concrete examples as the basis of your tests, and have a lot of whiteboard discussions (or their virtual equivalent) in order to make sure everyone understands the story. Make sure the programmers don’t start coding before any tests are written, and use exploratory testing to ﬁnd gaps between stories.\n\nRemember the end goal and the big picture.\n\n—Lisa\n\nTESTABILITY AND AUTOMATION\n\nAs an agile team, we work in short iterations, so it’s important to time-box the time spent writing tests before we start. After each iteration is completed, take the time to evaluate whether more detail up front would have helped. Were there enough tests to keep the team on track? Was there a lot of wasted time because the story was misunderstood? Lisa’s team has found it best to write high-level story tests before coding, to write detailed test cases once coding starts, and then to do exploratory testing on the code as it’s delivered in order to give the team more information and help make needed adjustments.\n\nJanet worked on a project that had some very intensive calculations. The time spent creating detailed examples and tests before coding started, in or- der to ensure that the calculations were done correctly, was time well spent. Understanding the domain, and the impact of each story, is critical to assess- ing the risk and choosing the correct mitigation strategy.\n\nWhile business-facing tests can help mitigate risks, other types of tests are also critical. For example, many of the most serious issues are usually uncov- ered during manual exploratory testing. Performance, security, stability, and usability are also sources of risk. Tests to mitigate these other risks are dis- cussed in the chapters on Quadrants 3 and 4.\n\nExperiment and ﬁnd ways that your team can balance using up-front detail and keeping focused on the big picture. The beauty of short agile iterations is that you have frequent opportunities to evaluate how your process is working so that you can make continual improvements.\n\nTESTABILITY AND AUTOMATION When programmers on an agile team get ready to do test-driven develop- ment, they use the business-facing tests for the story in order to know what to code. Working from tests means that everyone thinks about the best way to design the code to make testing easier. The business-facing tests in Quad- rant 2 are expressed as automated tests. They need to be clearly understood, easy to run, and provide quick feedback; otherwise, they won’t get used.\n\nIt’s possible to write manual test scripts for the programmers to execute be- fore they check in code so that they can make sure they satisﬁed the cus- tomer’s conditions, but it’s not realistic to expect they’ll go to that much trouble for long. When meaningful business value has to be delivered every two weeks or every 30 days, information has to be direct and automatic. In- experienced agile teams might accept the need to drive coding with auto- mated tests at the developer test level more easily than at the customer test\n\n149",
      "page_number": 183
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 192-200)",
      "start_page": 192,
      "end_page": 200,
      "detection_method": "topic_boundary",
      "content": "150\n\nCHAPTER 8\n\nPart IV, “Test Auto- mation,” will guide you as you de- velop an automa- tion strategy.\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nlevel. However, without the customer tests, the programmers have a much harder time knowing what unit tests to write.\n\nEach agile team must ﬁnd a process of writing and automating business- facing tests that drive development. Teams that automate only technology- facing tests ﬁnd that they can have bug-free code that doesn’t do what the customer wants. Teams that don’t automate any tests will anchor themselves with technical debt.\n\nQuadrant 2 contains a lot of different types of tests and activities. We need the right tools to facilitate gathering, discussing, and communicating exam- ples and tests. Simple tools such as paper or a whiteboard work well for gath- ering examples if the team is co-located. More sophisticated tools help teams write business-facing tests that guide development in an executable, autom- atable format. In the next chapter, we’ll look at the kinds of tools needed to elicit examples, and to write, communicate, and execute business-facing tests that support the team.\n\nSUMMARY In this chapter, we looked at ways to support the team during the coding pro- cess with business-facing tests.\n\n(cid:2) In agile development, examples and business-facing tests, rather than traditional requirements documents, tell the team what code to write. (cid:2) Working on thin slices of functionality, in short iterations, gives cus- tomers the opportunity to see and use the application and adjust their requirements as needed.\n\n(cid:2) An important area where testers contribute is helping customers ex- press satisfaction conditions and create examples of desired, and un- desired, behavior for each story.\n\n(cid:2) Ask open-ended questions to help the customer think of all of the de- sired functionality and to prevent hiding important assumptions. (cid:2) Help the customers achieve consensus on desired behavior for stories that accommodate the various viewpoints of different parts of the business.\n\n(cid:2) Help customers develop tools (e.g., a story checklist) to express infor-\n\nmation such as business satisfaction conditions.\n\n(cid:2) The development and customer teams should think through all of the parts of the application that a given story affects, keeping the overall system functionality in mind.\n\nSUMMARY\n\n(cid:2) Work with your team to break feature sets into small, manageable sto-\n\nries and paths within stories.\n\n(cid:2) Follow a pattern of “write test—write code—run tests—learn” in a\n\nstep-by-step manner, building on each pass through the functionality.\n\n(cid:2) Use tests and examples to mitigate risks of missing functionality or\n\nlosing sight of the big picture.\n\n(cid:2) Driving coding with business-facing tests makes the development team\n\nconstantly aware of the need to implement a testable application. (cid:2) Business-facing tests that support the team must be automated for quick and easy feedback so that teams can deliver value in short iterations.\n\n151\n\nThis page intentionally left blank\n\nChapter 9\n\nTOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nBusiness-Facing Tool Strategy Introduction\n\nTest Management\n\nChecklists\n\nMind maps\n\nCode Design and Test\n\nAutomated vs. Manual Tests\n\nTestability\n\nToolkit for Business-Facing Tests that Support the Team\n\nTools to Elicit Examples and Requirements\n\nSpreadsheets\n\nMock-Ups\n\nFlow Diagrams\n\nBuild Tests Incrementally\n\nSoftware-Based Tools\n\nKeep the Tests Passing\n\nDesign Patterns\n\nStrategies for Writing Tests\n\nBelow the GUI/API\n\nKeyword and Data-Driven Tests\n\nTools for Automating Tests Based on Examples\n\nUsing the GUI\n\nHome-Brewed Frameworks\n\nIn the previous chapter, we talked about how to approach business or functional testing to support the team in its effort to build the right software. In this chap- ter, we’ll examine some of the tools you can use to help your team succeed with Quadrant 2 tests.\n\nBUSINESS-FACING TEST TOOL STRATEGY How do we capture the business-facing tests that help the programmers know what to code? Face-to-face conversations between programmers and custom- ers are usually the best way, but even when customers are part of your team, they don’t have all day to hang out with programmers and explain features. If any customer or developer team members are in different locations, im- promptu hallway conversations might not be feasible. Besides, six months\n\n153\n\n154\n\nCHAPTER 9\n\nFor more informa- tion about a gen- eral approach to test automation, see Chapter 14, “An Agile Test Automation Strategy.”\n\nLisa’s Story\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nfrom now, we might want a way to remember why we coded a piece of func- tionality a certain way. If some of our team members are in different locations, we’re deﬁnitely going to need some way to share information electronically.\n\nAs agile development has gained in popularity, we have more and more tools to help us capture examples and use them to write executable tests. The tools available are changing too fast for us to include an inventory of them in this book, but we can offer some examples of tools and some strategies for using them to help provide business-facing tests that support the team’s develop- ment of new stories. Some of the tools we discuss here aren’t new, or speciﬁc to agile development, but they work well in an agile project.\n\nYour strategy for selecting the tools you need should be based on your team’s skill set, the technology your application uses, your team’s automation prior- ities, time and budget constraints, and other concerns unique to your situa- tion. Your selection of a tool or tools should not be based on the latest and coolest tool offered by a salesman. You might need many different tools to solve different problems.\n\nWe encourage customers to do some advance preparation and to be ready to explain examples for each story during iteration planning. Testers are in a good position to help customers ﬁgure out how to provide the right amount of detail at the beginning of the iteration. It’s hard to strike just the right balance.\n\nSoon after our team chose to use FitNesse for specifying and automating business- facing tests, our product owner and I tried to make good use of the new tool. We had an extremely complex epic coming up. We spent many hours writing detailed test cases for highly complex business rules weeks in advance of the iteration where the ﬁrst story of the epic was started. We felt good about getting a running start on developing the new functionality.\n\nWhen they started working on these stories, the programmers complained that they couldn’t get the big picture from these detailed tests. The tests were also designed in a way that was incompatible with the actual code design. I ended up spending hours refactoring them. It wasn’t a complete waste of time, because at least I under- stood the stories well and we had a number of test cases we could use eventually, but it wasn’t the right approach for our team. Trial and error has shown us that high- level tests combined with a few examples of desired and undesired behavior are the best way for the programmers to know what to start coding.\n\n—Lisa\n\nTOOLS TO ELICIT EXAMPLES AND REQUIREMENTS\n\nExperiment with different levels of up-front detail in test cases to ﬁgure out what works best for your team. Whatever level of detail you’re after, you need some way to help customers ﬁnd and express examples of desired system be- havior. In the next section, we look at the types of tools that can do that.\n\nTOOLS TO ELICIT EXAMPLES AND REQUIREMENTS As we pointed out in Chapter 8, stories are only a starting place for a pro- longed conversation about the desired behavior. Having correctly sized sto- ries where the feature, user, and purpose are clearly stated gives us a head start. They aren’t very detailed, because as Mike Cohn [2004] points out, it’s best to defer collecting details until the story is included in an iteration. Col- lecting details for a story that might never be included is a waste of resources. We like the “role, function, business value” pattern for user stories that Mike Cohn describes in User Stories Applied, as in:\n\nAs a (role), I want (function) so that (business value).\n\nThis format doesn’t work for everyone, so we encourage you to experiment and see what works best in your situation. Regardless of how your user sto- ries read, you need some way to ﬂesh those stories out with examples and business-facing tests that guide development.\n\nOne simple story can have a wide-ranging impact, not only on the applica- tion, but across the organization, its clients, its associates, vendors, or part- ners. If we change an API, we have to notify any customers or vendors who might be using it. If we plan a UI change, we want, or might even be contrac- tually obligated, to give a certain amount of advance notice to users. Stories may affect legal concerns or impact external reporting. New features often mean new or updated documentation. Of course, changed functionality is likely to affect other parts of the system.\n\nThe software development team, including the testers, should help the cus- tomer capture and communicate all of the requirements related to each story or theme. Developing new features, only to be prevented from releasing them for legal reasons or because a business partner wasn’t informed in time, is a frustrating waste of time (just ask Lisa!). Lean development teaches us to avoid waste while we develop software.\n\n155\n\n156\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nWhat tools can help us illustrate desired behavior with examples, brainstorm potential implementations and ripple effects, and create requirements we can turn into tests? Some examples are:\n\n(cid:2) Checklists (cid:2) Mind maps (cid:2) Spreadsheets (cid:2) Mock-ups (cid:2) Flow diagrams (cid:2) Software-based tools\n\nThe list includes a number of simple tools that aren’t unique to agile testing but that shouldn’t be neglected. In agile development, simple solutions are usually best. Let’s look at these in more detail.\n\nChecklists\n\nChecklists are one way for product owners to make sure they correctly assess and communicate all of the aspects of a story. The product owner for Lisa’s team, Steve Perkins, came up with his own “story checklist” to make sure he and the stakeholders think through everything affected by the story. He cre- ated a template on the team wiki for this purpose. The checklist speciﬁes the conditions of satisfaction—what the business needs from the story. It also in- cludes impacts on existing functions such as the website, documents, admin- istrative forms, account statements, and other components of the system and the daily operation of the business. The checklist makes sure the team doesn’t miss requirements such as data migration, notiﬁcations, legal considerations, and communications to vendors and business partners because they forgot to consider them. Figure 9-1 shows a sample story checklist.\n\nMind Maps\n\nMind maps are a simple but effective way to search out ideas that that might not occur to you in a simple brainstorming session. Mind maps are diagrams created to represent concepts, words, or ideas linked to a central key concept. We used mind maps to organize this book.\n\nIt really doesn’t matter whether you purchase a tool such as the one we used or draw on a whiteboard or a big piece of paper. The effect is the same. Mind maps enable you to generate ideas and work in a way that is consistent with the way you think about problems.\n\nFigure 9-1 Sample story checklist\n\nTOOLS TO ELICIT EXAMPLES AND REQUIREMENTS\n\n157\n\n158\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nHow about an example? We’re discussing the story shown in Figure 9-2.\n\nStory PA-3\n\nAs a shopper on our site, I want to delete items\n\nout of my shopping cart so I don’t purchase extra\n\nitems that I decide I don't want.\n\nFigure 9-2 Shopping cart delete story\n\nWe gather around the whiteboard and start asking questions. Where should the deleted items go? Should they be saved for later approval, or should they just disappear? What should the screen look like after we delete an item? Figure 9-3 shows an example of the sort of mind map we might draw on a whiteboard.\n\nFigure 9-3 Example mind map for shopping cart delete story",
      "page_number": 192
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 201-208)",
      "start_page": 201,
      "end_page": 208,
      "detection_method": "topic_boundary",
      "content": "TOOLS TO ELICIT EXAMPLES AND REQUIREMENTS\n\nSpreadsheets\n\nWhen possible, tools for specifying business-facing tests should ﬁt well with your business domain. For example, spreadsheets are widely used by ﬁnan- cial services companies, so for a project in the ﬁnancial services area it makes sense to use spreadsheets to deﬁne examples of the functionality that a story should deliver.\n\nCustomers can write a few high-level test cases to help round out a story prior to the start of the iteration, possibly using some type of checklist. Some cus- tomer teams simply write a couple of tests, maybe a happy path and a negative test, on the back of each story card. Some write more detailed examples in spreadsheets or whatever format they’re comfortable working with.\n\nSteve Perkins, the product owner for Lisa’s team, often illustrates complex calculations and algorithms in spreadsheets, which the team can turn into tests later. Figure 9-4 shows one of his worksheets, which performs calcula- tions on the input values to produce the values in the ADR and ACR col- umns. This format is easy to get into an automated test framework (refer to Figure 9-8 for the corresponding FitNesse example).\n\nLook at tools already used by your business experts and see whether they can be adapted to document examples of desired feature behavior to help the de- velopment team better understand the story.\n\nJanet has worked with several teams that have used spreadsheets as input into their Fit tests. This allows customers to work in a tool that is familiar to them but not waste any effort in translating them to an automation tool.\n\nFigure 9-4 Spreadsheet example from product owner\n\n159\n\n160\n\nCHAPTER 9\n\nSee Chapter 8 for Gerard Meszaros’ description of using paper proto- types and Wizard of Oz testing.\n\nLisa’s Story\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nMock-Ups\n\nMock-ups can take many forms. Paper prototypes are a simple but effective way to test how screens will work together. Drawing on a whiteboard can ac- complish the same goal, but it can’t be passed around. Screenshots from ex- isting applications can form the basis of a discussion about how to add a new feature and where it will ﬁt into the UI. You may have used tools like these in other development methodologies. The big difference in agile development is that we create and discuss the mock-ups just as we’re about to start writing the code, rather than weeks or months beforehand. We can be conﬁdent that the mock-up represents what the customers want right now.\n\nWe use simple approaches to creating mock-ups so that we aren’t tempted to in- vest time coding before we’re ﬁnished working through the mock-up. Often, we draw a UI or workﬂow on the whiteboard and then take photos of it to upload to our team wiki so our remote team member can also see it. At other times, a cus- tomer or our product owner draws the mock-up on paper or modiﬁes an existing UI page or report to show what should be added and changed. The paper mock- ups are scanned in and posted on the wiki.\n\nA picture’s worth a thousand words, even in agile software development. Mock- ups show the customer’s desires more clearly than a narrative possibly could. They provide a good focal point for discussing desired code behavior.\n\nFigure 9-5 shows an example of a mock-up that Lisa’s team used to mock up a new report—simply by marking up an existing report that’s similar.\n\nMock-ups don’t need to be fancy or pretty, or to take a lot of time to create. They do need to be understandable to both the customer and developer teams.\n\nFlow Diagrams\n\nSimple diagramming tools are helpful, whether the team is co-located or not. It’s often a good idea to capture in a more permanent form a workﬂow or de- cision tree worked out during a discussion. Flow diagrams can become the basis of a user scenario that might help you tie two or three user stories to- gether. Let’s look at the shipping order story again that we introduced in Chapter 8 (see Figure 9-6).\n\n—Lisa\n\nTOOLS TO ELICIT EXAMPLES AND REQUIREMENTS\n\nFigure 9-5 Sample report mock-up\n\nStory PA-1\n\nAs an Internet shopper on LotsO'Stuff.xx,\n\nI want free shipping when my order exceeds the free\n\nshipping threshold, so that I can take advantage\n\nof ordering more at one time.\n\nFigure 9-6 Story for shipping charges\n\n161\n\n162\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nFigure 9-7 shows a very simple ﬂowchart of a decision process for whether a customer’s order is eligible for free shipping based on a threshold order amount. Because we’ve discussed this story with our customer, we’ve found out that the customer’s order must not only exceed a threshold dollar amount but also must be to one address only, and it must weigh less than a shipping weight threshold. If all of these conditions are satisﬁed, the cus- tomer’s order will ship free; otherwise, the customer will have to select from the “choose shipping options” page.\n\nCheck-Out\n\nOrder Total > Free Shipping Threshold?\n\nYes\n\nNo\n\nShip to One Address?\n\nNo\n\nShipping Options Page\n\nNo\n\nYes\n\nOrder Weight < Max Shipping Threshold?\n\nYes\n\nCheck-Out\n\nFigure 9-7 Flow chart for qualifying for free shipping option\n\nTOOLS TO ELICIT EXAMPLES AND REQUIREMENTS\n\nVisuals such as ﬂow diagrams and mind maps are good ways to describe an overview of a story’s functionality, especially if they’re drawn by a group of customers, programmers, and testers. In agile development, we create these diagrams as we’re about to start writing tests and code. From these, the team can immediately start digging down to the detailed requirements.\n\nSoftware-Based Tools\n\nIf we’re in a different location than our customers, we need tools to help us converse with them. Distributed teams tell us that desktop sharing is the num- ber one tool that helps them deal with working in separate locations. Windows NetMeeting and VNC are examples of tools that let two team members in dif- ferent locations pair-test. Video conferencing tools such as WebEx and Skype enable collaboration and demos between remote teams and customers. Online whiteboards such as Scriblink and interactive whiteboard tools such as Mimeo facilitate distributed whiteboard discussions.\n\nMore tools that are geared for direct use by product owners and business ex- perts are becoming available, and many teams develop their own. Tools such as Fit (Framework for Integrated Tests) and FitNesse were designed to facili- tate collaboration and communication between the customer and develop- ment teams. We’re hearing about more teams where the customers actually write the tests in a tool such as those.\n\nNotes from a Distributed Team\n\nPierre Veragen and Erika Boyer of iLevel by Weyerhaeuser told us that every iteration begins with everyone on the team writing acceptance tests. That’s how they start their iteration planning. Most interesting is the fact that their product owners, who are mechanical engineers, write FitNesse tests them- selves. Pierre explains that an advantage of a tool such as FitNesse is the abil- ity to use their own domain language in the FitNesse tests. It doesn’t matter what they end up choosing as a UI. They can test all of their complex calcula- tions in the tests.\n\nWith this process, tests can be written before writing the testing code or the system under test. It’s true test-driven development. Behavior changes and bug ﬁxes can follow.\n\nSome teams build their own frameworks that allow customers, business analysts, and testers to document examples that can be directly turned into executable tests. These are often based on open source tools such as xUnit, Fit, Selenium, and Watir. We like this approach, because it saves time and\n\n163\n\n164\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nresources. When you’re delivering production-ready code in short itera- tions, you need a streamlined process.\n\nOnline forum tools are a good alternative to email conversations for ongoing discussions about features or technical concerns, especially for teams that don’t all sit together. Emails often get missed or lost, people have to remem- ber to choose “Reply all,” and it can be hard to put together the details of the discussion later. Lisa’s team uses an online forum to elicit opinions about dif- ferent tools, propose different behavior for features, and conduct philosoph- ical discussions such as whether to track defects.\n\nFinding the right electronic tools is particularly vital for distributed teams. Instant messaging, the telephone, VoIP, and Skype help us communicate, but they lack the visual component. Some global teams ask their members to meet at nonstandard hours so that they can have real-time conversations, but frameworks for written and visual communication are still critical.\n\nWikis are a common tool used to enhance communication and record dis- cussions and decisions. Wikis enable users to edit web page content in a web browser. Users can add hyperlinks and easily create new pages. You can up- load mock-ups, samples, and pictures of whiteboard drawings and make them easily visible on Wiki pages. The hierarchical organization can get tricky to maintain, but there are lots of open source and vendor wiki software packages available that make managing your knowledgebase and sharing in- formation easier to administer. If your wiki knowledgebase has grown to the point where it’s hard to ﬁnd anything, hire a technical writer to transform it into organized, usable documentation.\n\nOpen source and commercial tools provide ways to let teams collaborate on re- quirements and test cases online. We can’t emphasize enough the need for you to identify tools that might be helpful, to experiment with them for a few itera- tions, and to decide how well they work for you. Your team’s needs will change with time, so always be open to trying new techniques and frameworks.\n\nThese tools help create the conversation about the story. With these tech- niques, and as much real-time conversation and visual sharing as we can manage, we can deﬁne the right product from the get-go.\n\nTOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES What about test tools? We like the collaboration inherent with tools such as Fit and FitNesse. However, in our opinion, any tool that gets testers and pro-\n\nTOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES\n\ngrammers, programmers and customers, and testers and customers talking is a great one. We know teams where customers actually write tests in Fit, Fit- Nesse, Expect, or other tools. This works when the tool has been set up in a manner that’s clear to everyone writing tests, with the domain language easy to understand and the appropriate ﬁxtures provided.\n\nTools to Test below the GUI and API Level\n\nThere are a multitude of open source tools that enable you to test below the GUI or at the API layer. We are listing just a few, but your team will need to determine the right tool for you.\n\nUnit-Level Test Tools Some teams use the xUnit tools such as JUnit or NUnit for business-facing tests as well as technology-facing tests. If the testers and customers are com- fortable with these tools, and they provide for all of the functional testing be- hind the GUI needed, they’re ﬁne. To make these tools more customer- friendly, teams might build a framework on top of the unit-level tools that testers and customers can use to specify tests.\n\nJanet has worked on a couple of applications like that. One was a message handling system that was being deployed in an organization. The program- mers used JUnit for all of the component and integration testing. They built a load test framework that could make use of the JUnit tests, so no other test- ing tools were needed. The GUI front end was so small that Janet was able to test it manually. It made no sense to automate the GUI testing in this case.\n\nBehavior-driven development (BDD) tools are also suited to this purpose, because they use a more natural language for specifying the tests. Behavior- driven development is a variation of test-driven development, pioneered by Dan North [2006], and evolved by many others. It’s related to domain-driven design, with a focus on the domain rather than on the technology, and driv- ing design with a model. Instead of the word “test” or “assert,” BDD uses the word “should.” By thinking in terms of behavior, it’s natural to write speciﬁ- cations ahead of code. Test speciﬁcations use a domain-speciﬁc language to provide tests that customers can read but that can also be easily automated.\n\nSome of the many BDD tools available as of this writing include easyb and JBehave for the Java platform, NBehave and NSpec for .NET, and RSpec for Ruby. These tools, like the XUnit tools, are intended for use by programmers to guide coding, but they can also be used to express business-facing tests that drive development, involving customers more closely in the development process.\n\n165\n\n166\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nBehavior-Driven Development\n\nAndrew Glover, president of Stelligent Incorporated and author of books including Continuous Integration and Java Testing Patterns, explains the think- ing behind one of the BDD tools, easyb.\n\nassertEquals(42.50, order.price(), 0.0). Without examining the context in which this statement appears, this code is somewhat incom- prehensible. Now imagine you don’t even read code—that is, you are a stakeholder asking (actually paying) for new features. The previous code statement might as well be Farsi (assuming you can’t actually read Farsi!).\n\norder.price().shouldBe 42.50. While the context in which this state- ment appears is still absent, this line of code is a bit more coherent. In fact, it reads like a normal sentence (and this time knowledge of Farsi isn’t required!). Stakeholders, in this case, could understand this code if they chose to read it; on top of that, it turns out that this line of code essentially matches what they asked for in the ﬁrst place. This line of code describes behavior in a more literal manner too—the code uses a normal everyday phrase like shouldBe, which is distinctly different than the previously written assertEquals.\n\nBoth lines of code from the previous paragraphs convey the same mean- ing and indeed validate the same requirement, yet the latter one comes awfully close to leveraging the customer’s language. This is a fundamental point of the notion of behavior-driven development,which strives to more appropriately validate a software system by thinking in terms of the term “should” rather than test. In fact, by focusing on behavior and closely modeling behavior after what stakeholders ask for, behavior- driven development converges on the idea of executable documenta- tion. Indeed, through leveraging a stakeholder’s language, there is a de- creased impedance mismatch between what he wants and what he ultimately receives; moreover, employing a stakeholder’s language facili- tates a deeper level of collaboration between all parties. Listen to how a conversation might go:\n\nStakeholder: For the next release of our online store, our Gold- level customers should receive a discount when they make a pur- chase.\n\nDeveloper: What kind of discount—what criteria do they have to meet in order to receive it?\n\nStakeholder: When they have at least $50 dollars in their shopping cart.\n\nDeveloper: Does the discount increase based upon the amount, or is it ﬁxed regardless of the value of the shopping cart?\n\nStakeholder: Good question—the discount is ﬁxed at 15% regardless of price. So, given a Gold-level customer, when the shopping cart totals $50 or more, it should receive a 15% discount off the total price.",
      "page_number": 201
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 209-216)",
      "start_page": 209,
      "end_page": 216,
      "detection_method": "topic_boundary",
      "content": "TOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES\n\nThe last statement of the stakeholder is key—note how the requirement has been speciﬁed and the means for validating it. In fact, the stake- holder has essentially narrated a speciﬁc scenario in a larger story related to discounts.\n\nGiven this scenario, a developer can take the stakeholder’s comments— word for word—and execute them. For example, one behavior-driven development framework, dubbed easyb, facilitates system validation through a domain-speciﬁc language that supports stories and scenarios. For example:\n\nscenario “Gold-level customer with $50 in shopping cart”, { given “ a Gold-level customer” when “their shopping cart totals $50 or more” then “ they should receive a 15% discount off the total price” }\n\nOf course, this particular scenario doesn’t actually do anything (other than capturing the stakeholder’s requirements, which is still quite im- portant!); consequently, it is considered pending. This status alone conveys valuable information—stakeholders can, ﬁrst and foremost, see their words as a means to validate their requests, and secondly, gauge if their requirement has been fulﬁlled. After this scenario has been implemented, it can, of course, take on two other states—suc- cess or failure, both of which serve to convey further status information to interested parties.\n\nNow, with a collaborative scenario deﬁned, development can proceed to the implementation—the beauty in this case is that they can directly implement the desired behavior inline with the requirements, like this:\n\nscenario \"Gold-level customer with $50 in shopping cart\", { given \"a Gold-level customer\", { customer = new GoldCustomer() } when \"their shopping cart totals $50 or more\", { customer.shoppingCart << new Item(\"widget\", 50.00) } then \"they should receive a 15% discount off the total price\" , { customer.orderPrice.shouldBe 42.50 } }\n\nThis scenario is now executable within the context of the application it serves to validate! The scenario leverages the customer’s exact words, too; what’s more, regardless of the customer’s ability to read code, the code it- self leverages natural language: customer.orderPrice.shouldBe 42.50.\n\n167\n\n168\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nBy leveraging the customer’s language, the customer has the ability to collaboratively facilitate in validating the system he or she wants built. Also, with development leveraging the stakeholders’ language, there is a direct link between what stakeholders ask for and what they receive. And you don’t even need to understand Farsi to see the beneﬁt in that.\n\nTwo of the most common questions we’re asked by new agile teams are, “What about documentation?” and “How can test automation keep up with development in two-week iterations?” Tools such as easyb answer that ques- tion with executable documentation using a domain-speciﬁc language that everyone on both the customer and developer teams understands.\n\nThe goal of business-facing tests that support the team is to promote com- munication and collaboration between customers and developers, and to en- able teams to deliver real value in each iteration. Some teams do this best with unit-level tools, and others adapt better to functional-level test tools.\n\nAPI-Layer Functional Test Tools Before Lisa joined her ﬁrst agile team, testing “behind the GUI” was a con- cept that sounded good, but she’d never had the opportunity to try it. Fit, and FitNesse, which is built on top of Fit, are functional test tools that grew from the need for the customer team to be able to write and understand the business-facing tests that drive development. With these tools, teams can test business logic without involving the presentation layer.\n\nFit and FitNesse. Fit (Framework for Integrated Tests) is an open source testing framework that promotes collaboration, which makes it a good tool to help reﬁne requirements. The invention of Ward Cunningham, Fit has en- joyed an illustrious roster of contributing developers. Fit enables customers, testers, and programmers to use examples to specify what they expect the system to do. When the tests run, Fit automatically compares customers’ ex- pectations to actual results.\n\nWith Fit, customers can provide guidance using their subject matter exper- tise to deﬁne the examples that the programmers can code against. The pro- grammers participate by writing the ﬁxtures that do the actual checks against the examples. These ﬁxtures use the data speciﬁed in the examples to run with the actual program.\n\nFit tests are automated by ﬁxtures that pass the test inputs to the production code and then accept the outputs, which it then compares with expected re- sults. The test results are color-coded, so it’s easy to spot a failure or exception.\n\nTOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES\n\nLearn more about Fit at ﬁt.c2.com.\n\nFit tests are written as HTML tables, but teams can customize Fit so that tests can be written in spreadsheets or whatever form the customers, testers, and analysts ﬁnd usable.\n\nLearn more about FitNesse at www.ﬁtnesse.org.\n\nFitNesse is a web server, a wiki, and a software testing tool that is based on Fit. Originally developed by Robert C. “Uncle Bob” Martin and Micah Mar- tin, it’s an open source tool with an active developer community. The main difference between FitNesse and Fit is that FitNesse tests are written in wiki markup instead of HTML tables, which some users ﬁnd easier. It also sup- ports creating tests in spreadsheets and importing those into the tests.\n\nFigure 9-8 shows part of the FitNesse test that was built from the example in Figure 9-4. More inputs were added to make the production code run, but the essential test data is from the spreadsheet. The test results are color-coded green when they pass, red when they fail.\n\nAnother beneﬁt of a Fit or FitNesse type of tool is that it promotes collabora- tion among different team members in order to come up with the right tests\n\nFigure 9-8 Automated FitNesse test from customer example\n\n169\n\n170\n\nCHAPTER 9\n\nSee the ”System Test” example in Chapter 12, “Summary of Test- ing Quadrants,” to see how Janet’s team used Ruby Test::Unit to test web services.\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nto guide development. Customers, programmers, testers, and others work to- gether to specify and automate the tests.\n\nTesting Web Services. Web services is just another form of an API that en- ables other applications to access your application. Let’s talk about some of the tools you can use to test various inputs into your system.\n\nCrossCheck. CrossCheck is one example of a tool for testing web ser- vices. You supply the WSDL (Web Services Description Language); CrossCheck compiles the page and then presents you with a tabbed menu that contains textboxes for you to ﬁll in. It has a Run mode where you can add your tests to a suite and then run the suite. Neither Lisa or Janet have tried this tool, but it was noted on the Yahoo agile-testing group as a tool to use for testing web services if you were running the same data through each time.\n\nRuby Test::Unit. One project Janet was on used Ruby’s unit testing framework, Test::Unit, to test web services, with great success. In fact, the team was able to test early to give the programmers immediate feedback, which helped with the ﬁnal design.\n\nsoapUI. Another tool suggested for testing web services is soapUI. It has a steep learning curve but can be used for performance and load testing. Because it can loop though rows in an Excel spreadsheet or text ﬁle, it can be used for data-driven testing.\n\nTests that work at the layers below the presentation layer are well suited for writing and automating customer tests that guide coding. Some practitioners haven’t gotten the value they expected from story test-driven development. Brian Marick [2008] hypothesized that an application built with program- mer test-driven development, example-heavy business-facing design that re- lies heavily on whiteboard discussions, a small set of automated sanity tests, and lots of exploratory testing could be a less expensive and equally effective approach. Whichever approach you take, if you’re testing an application with a user interface, you’ll need some automation at the GUI level.\n\nTools for Testing through the GUI\n\nWait a minute. How can we use GUI tests to drive development, because the GUI won’t be ready until the story is complete? It sound counterintuitive, but automated GUI tests are important to help us while we’re developing new functionality. Test frameworks can be used to specify test cases for a GUI tool before the code is written. In addition, you can automate GUI tests be-\n\nTOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES\n\nA Tool Selection Rationale\n\nDavid Reed, a test automation engineer, and his team went with soapUI Pro to automate testing for their web services. Here are some reasons he gave for choosing this particular tool.\n\nIt has an open source version, so you can try it out it for free. You can learn it, kick the tires, expand stuff, and learn its strengths and weaknesses.\n\nIt was easy to ﬁgure out what requests to make for what service.\n\nThe assertions provided for verifying the results from requests are great and expandable. One really helpful one is verifying that the response comes back in an acceptable amount of time, raising an error if it doesn’t.\n\nThe Pro version takes a lot of the hassle out of designing XPath queries to verify results. It also adds some nice touches for retrieving database data.\n\nIt’s expandable with Groovy, a Java-based scripting language. (They’re working on a Java application, so it pays to have Java-friendly tools.)\n\nDevelopers can use it without sneering at it as a “test tool.”\n\nIt’s easily integrated with our continuous integration environment.\n\nIt has a feature to check code coverage.\n\nThe price is right.\n\nfore coding is ﬁnished, either by using HTML mock-ups or by developing an end-to-end bare-bones slice through all of the screens that simply navi- gates but doesn’t provide all of the functionality yet. Even if you’re not using a lot of automated story tests to drive development, manual exploratory testing that helps us learn about the functionality and provides immediate feedback gets pretty tedious and slow without any assistance from automa- tion. Let’s look at the types of GUI test tools that help drive development us- ing business-facing tests.\n\nRecord/Playback Tools Record/playback tools are appealing because you can usually learn how to record a script and play it back quickly, and you can create lots of scripts in a short time. However, they have drawbacks. Early GUI test tools recorded mouse movements using X-Y screen coordinates. Scripts using those tools might also be sensitive to changes in screen resolution, color depth, and even where the window is placed on the screen.\n\n171\n\n172\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nMost modern GUI test tools use objects to recognize the controls in a graph- ical application, like buttons, menus, and text input widgets, so they can refer to them symbolically rather than with raw screen coordinates. This makes the application much more testable, because it’s more robust standing up to changes. A button might move to a different part of the screen, but the test can still ﬁnd it based on its object name.\n\nEven with improved object recognition, scripts created with record/playback are usually brittle and expensive to maintain. Recording can be a good way to start creating a script. Testers or programmers who know the tool’s scripting language can refactor the recorded script into an object-oriented model that’s easier to use and maintain. Historically, record/playback tools used propri- etary scripting languages, which programmers aren’t interested in learning. It’s also more difﬁcult to change the design patterns used in the tests.\n\nSome script-based tools such as the ones we’ll talk about in the next few sec- tions offer a record feature to help people get a quick start on writing the test script. However, with those tools, the recorded scripts aren’t intended for straight playback; they’re just a starting point to creating a well-designed and easily maintained suite of tests.\n\nMany agile teams prefer tools and scripting languages that let them create their own domain-speciﬁc language (DSL). This makes tests much easier for busi- ness experts to understand and even write. Let’s look at some of these next.\n\nAgile Open Source Test Tools Each of the tools in this section was originally written by an agile develop- ment team that needed a GUI test tool and couldn’t ﬁnd any third-party tools that worked for its situation. With these tools, you can write scripts that use web applications just like a human user. They ﬁll in text ﬁelds, select from lists, and click checkboxes and buttons. They provide a variety of ways to verify correct navigation and contents of pages, such as tool-speciﬁc verify steps or XPath. Some of these tools have a higher learning curve than simple record/playback tools, but the extra investment of time usually pays off in scripts with a low total cost of ownership.\n\nRuby with Watir. Watir (Web Application Testing in Ruby) is a simple open source Ruby library for automating web browsers that works with Internet Explorer on Windows. There are different ﬂavors for other browsers, includ- ing FireWatir for Firefox and SafariWatir for Safari.\n\nJanet’s Story\n\nTOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES\n\nI worked on a project that developed a three-layer test framework using Ruby and Watir. The ﬁrst layer was a common set of libraries, and the second layer was to ac- cess the pages and provide navigation. The third and top layer created a domain language using ﬁxture-type methods that mapped to the business needs. This al- lowed the manual testers to write high-level automated tests for workﬂows before coding was completed. If a ﬁxture didn’t exist because of new functionality, the test could be created and the action word for the missing ﬁxture could be “dummied” in. As soon as the ﬁxture was coded, the test could be run as an acceptance test.\n\nA very simple example of using Ruby with Watir incorporates the idea of DSL. Methods were created to simplify the tests so that any of the testers could actually create an automated script without knowing any Ruby or Watir.\n\nThis next example shows a test, and then two of the methods used in the test.\n\ndef test_create_new_user\n\nlogin 'administrator','admin' navigate_to_tab 'Manage Users' click_button \"Create New User\" set_text_field \"userFirstNameInput\", \"Ruby\" set_text_field \"userLastNameInput\", \"RubyTester\" click_button \"Save Changes\" verify_text “Saved changes” end\n\n# methods created to support easier test writing def navigate_to_tab(menuItemName) @browser.link(:text,menuItemName).click end\n\ndef set_text_field(id, value) @browser.text_field(:id,id).set value end\n\nA third level could easily be added if create_new_user was called more than once. Just extract the common code that the test could call:\n\ncreate_new_user (Ruby, RubyTester)\n\nThese tests were well suited to guiding development and providing quick feed- back. Making tests easy for testers and customers to write, while keeping the au- tomation framework designed for optimum maintainability, reduced the total cost of ownership of the tests.\n\nThere are always drawbacks to any tool you use. For example, there are limi- tations to using objects. Sometimes programmers use custom controls or a new toolkit that your tool might not understand.\n\n173\n\n—Janet\n\n174\n\nCHAPTER 9\n\nJanet’s Story\n\nSee Chapter 14, “An Agile Test Automation Strategy,” for an example of using Selenium RC to create a domain- speciﬁc test auto- mation framework.\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nI started a new job as QA manager, and after much deliberation we decided to drop the vendor tool that the team had been using for a couple of years. We could not ﬁgure out what tests were actually being run, or what the real coverage was. We decided to start automating the tests using Ruby and Watir. The automa- tion went fairly quickly at ﬁrst, but then the tests started failing. We spent a lot of time changing the tests to reﬂect new object names. The developers were just us- ing the default WebLogic object names, which would change every time a new object was added to the page. The testers went to the developers to ask if they could change the way they were coding. It took a little convincing, but when the developers realized the problems their practice was causing, they changed their habits. Over time, all of the defaults were changed, and each object had an as- signed name. The tests became much more robust, and we spent much less time in maintenance mode.\n\nImplementing a new test automation tool usually requires some experimen- tation to get a good balance of testable code and well-designed test scripts. Involving the whole team makes this much easier. Watir is one example of a GUI test tool that we’ve found is well suited to agile projects. Let’s look at a couple more, Selenium and Canoo WebTest.\n\nSelenium. Selenium is another open source tool, actually a suite of tools, for testing web applications. The tests can be written as HTML tables or coded in a number of popular programming languages, and can be run directly in most modern web browsers. A Firefox plug-in called “Selenium IDE” provides a way to learn the tool quickly. A recorder is provided to help create the tests, includ- ing writing assertions. Tests can be written in several different common pro- gramming and scripting languages, including Java, C#, and Ruby.\n\nCanoo WebTest. In WebTest scripts, tests are speciﬁed as “steps” in XML ﬁles, simulating a user’s actions through a web UI. Here’s an example of how a WebTest script might invoke a page and verify the results:\n\n<setInputField description=\"set query\" name=\"q\" value=\"Agile Tester\"/> <clickButton description=\"submit query\" label=\"Google Search\"/> <verifyText description=\"check for result\" text=\"Lisa Crispin\" /> <verifyText description=\"check for result\" text=\"Janet Gregory\" />\n\nRather than driving an actual browser, as Selenium and Watir do, WebTest simulates the desired browser using HtmlUnit. The advantage of specifying tests as opposed to coding test scripts, is because there’s no logic in them, you don’t have to test the test.\n\n—Janet",
      "page_number": 209
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 217-224)",
      "start_page": 217,
      "end_page": 224,
      "detection_method": "topic_boundary",
      "content": "Lisa’s Story\n\nTOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES\n\nMy team chose WebTest to automate smoke tests for our legacy application for several reasons. Because the scripts are written in XML, the programmers on the team were comfortable using the tool. It uses Ant to run the tests, so integrating it into the continuous build process was simple. It’s easy to learn, and the tests can be designed in a modular fashion, so they’re fairly easy to maintain. WebTest sup- ports testing PDF ﬁles, emails, and Excel ﬁles, all of which are widely used in our application.\n\nBeing accustomed to powerful commercial test tools, I was skeptical of the con- cept of specifying tests, as opposed to programming them. I was amazed at how effective the simple tests were at catching regression bugs. It’s possible to put logic into the tests using Groovy or other scripting languages, but we’ve only found the need in a few cases.\n\nWriting a few tests per iteration, I automated smoke tests for all of the critical areas of our application in eight months. These simple tests ﬁnd regression bugs regu- larly. We refactor the tests frequently, so they are relatively easy to maintain. Our ROI on these tests has been tremendous.\n\nSelenium, WebTest, and Watir are just three examples of the many open source tools available for GUI testing as of the time we wrote this book. Many teams write their own test automation frameworks. Let’s look at an ex- ample in the next section.\n\n“Home-Brewed” Test Automation Tools Bret Pettichord [2004] coined the term “home-brewed” for the tools agile teams create to meet their own unique testing needs. This allows even more customization than an open source tool. The goal of these tools is usually to provide a way for nontechnical customer team members and testers to write tests that are actually executable by the automated tool. Home-brewed tools are tailored to the exact needs of the project. They can be designed to mini- mize the total cost of ownership. They’re often built on top of existing open source tools.\n\nJanet has been involved in a few projects that have used Ruby and Watir to create a full framework for functional testing. These frameworks allowed cus- tomers to specify tests that were then turned into a functional regression suite.\n\nNo test tool guarantees success. In fact, the history of test automation is lit- tered with failed attempts. Having the whole team think about the best tools to use is a big help, but no matter what tool you use, you need a smart ap- proach to writing tests. We’ll discuss that in the next section.\n\n175\n\n—Lisa\n\n176\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nPAS Functional Testing\n\nThis next story is about one project Janet worked on that enjoyed success with home-brewed test automation.\n\nPAS is a production accounting application for the oil and gas industry. Using gross meter readings and contract agreements, it must calculate ownership of the various products down to a very precise level (i.e., the components in gas). There are literally thousands of interactions between the combinations available in conﬁguring the system and the actual outputs visible to a user. Given the large number of interactions, PAS has employed many complemen- tary strategies for testing.\n\nJoseph King, one of the initial programmers and agile coach for the team, tells us the story of how they accomplished their functional testing.\n\nAt the lowest level, there are developer functional tests that exercise speciﬁc functions via an API and verify the results using another read-only user API. There are currently over 24,000 tests implemented in JUnit that every developer must run before they can ”check in” their changes to the source code.\n\nThe next level is a set of GUI tests that test the marshalling of user data back and forth to the API, particularly around ”master-data” creation and updates. There are currently over 500 of these tests implemented using Watij (an open source library similar to Watir but using Java) and JUnit that run multiple times a day.\n\nThe ﬁnal level of testing is a set of integration tests created by the users that run in a Fit-like harness. Users identify dense test cases that reﬂect real-world cases covering many of the functions that work together to produce ﬁnancial and regulatory outputs. These test cases are then tran- scribed into import templates and then processed using a domain lan- guage that mirrors the way end customers think about their processes.\n\nFor example, after an end customer has created the conﬁguration of fa- cilities and contracts they wish to exercise in their test, they work with a developer to use the domain language to process their facilities in the correct order. The end users also supply a set of expected outputs that are then veriﬁed using a read-only API. These outputs can contain thousands of numbers, any of which can change for seemingly minor reasons in an evolving product. It is a constant challenge to sort through what is a legitimate business change from what is a defect. There are currently over 400 integration tests, and they run twice per day, providing feedback to the end customers and developers.\n\nExploratory testing is done continuously throughout the development cycle and is augmented at the end of releases.\n\nOur ﬁrst attempt at PASFIT (which is what we called the functional test framework) was a spreadsheet of color-coded inputs and outputs. We\n\nSTRATEGIES FOR WRITING TESTS\n\nthen generated Java code based on the color of the cells to create the data in PAS. That proved difﬁcult to maintain, partly because the applica- tion was in major ﬂux both at the GUI and database level.\n\nOur next iteration of PASFIT didn’t evolve for nearly a year after the previ- ous attempt. After we had a more stable set of database views and GUI, we were able to create an engine that used simple imperative language (i.e., a script) to do actions with arguments against a GUI (e.g., Go to Bal- ancing Page, Balance Battery: Oil, Water). The script evolved into following the thought process of a production accountant and became a domain- speciﬁc language. The engine was written using Ruby and Watir, and an instruction from the script was basically a Ruby method that was invoked dynamically so that it was easy to update. After the script ran, the frame- work then loaded a snapshot of the views that the test wished to com- pare and did a simple row-by-row, cell-by-cell comparison of what was to be asserted and what actually happened. Eventually this was enhanced in the spreadsheet to use Pivot tables to enable the users to focus in on only the results they wished to assert for their test. All in all it has been quite successful, although the requirements for our application mean that 300 tests take about 12 hours to run, which is a long time.\n\nGetting the business more involved in maintaining the regression tests has also been difﬁcult, but when it happens it is very good. Currently, we have a stand-up where the business users and the developers meet for 15 minutes to pick up any of the scenario tests that are breaking that day. It is quite effective in that people often know when they come to the stand-up what they might have broken the day before. Future enhancements are likely to include asserting against actual user reports instead of the views and running a migration each night against the sce- nario script.\n\nPASFIT achieved a balance between letting business experts write tests in a DSL and automating those tests with a highly complex application. Success came with some trial and error. Teams that write their own test frameworks need time to experiment to ﬁnd the right solution for both the business and the development team.\n\nSTRATEGIES FOR WRITING TESTS The best tools in the world won’t help if you don’t use them wisely. Test tools might make it very easy to specify tests, but whether you’re specifying the right tests at the right time is up to you. Lisa’s team found that too much de- tail up front clouded the big picture to such a degree that the programmers didn’t know what to code. This won’t be true for every team, and at some point we do need details. The latest time to provide them is when a program- mer picks up a coding task card and starts working on a story.\n\n177\n\n178\n\nCHAPTER 9\n\nLisa’s Story\n\nChapter 18, “Cod- ing and Testing,” goes into more detail about how testers and pro- grammers work together to test and code.\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nWriting detailed test cases that communicate desired behavior effectively re- quires both art and science. Poorly expressed scenarios and poorly designed test cases can create more confusion than they resolve. Experiment so that you can ﬁnd the right level of detail and the right test design for each story. Let’s look at some strategies to help you use tools successfully to write useful business-facing tests.\n\nBuild Tests Incrementally\n\nAfter we have deﬁned our high-level acceptance tests so that the programmer knows what to start coding, we can start elaborating on the rest of the story tests. We can work closely with the programmer to ensure we automate the best possible way.\n\nWhen a programmer starts working on the programming tasks for a story, start writing detailed tests. For those of us who enjoy testing, it’s tempting to go for the biggest “smells” right away, the areas where we think the code might be fragile. Resist the temptation. Make sure the most obvious use case is work- ing ﬁrst. Write a simple, happy path automated test to show the code accom- plishes the most basic task that it should. After that test passes, you can start getting more creative. Writing the business-facing tests is an iterative process.\n\nI start writing executable business-facing tests that support the team by writing a simple FitNesse test based on examples that the product owner provides. I show this to the programmer working on the code. He can make suggestions for changes right then, or he might modify the test himself as appropriate when he’s ready to automate it. Discussing the test often leads the programmer to realize he missed or misunderstood a requirement. We might need another three-way con- versation with the customer. The programmer updates the code accordingly. We can also show the test to the product owner to make sure we captured the be- havior correctly.\n\nAfter the simple test passes, I write more tests, covering more business rules. I write some more complex tests, run them, and the programmer updates the code or tests as needed. The story is ﬁlling out to deliver all of the desired value.\n\nConﬁne each test to one business rule or condition. At some point you can automate or manually perform more complex scenarios, but start by cover- ing each condition with a simple test. If you’ve followed our recommended thin slice or steel thread pattern, the ﬁrst set of tests should prove the ﬁrst\n\n—Lisa\n\nLisa’s Story\n\nSTRATEGIES FOR WRITING TESTS\n\nthin slice end-to-end. As your automated tests pass, add them to the regres- sion suite that runs in a frequent build process.\n\nKeep the Tests Passing\n\nAfter a test passes, it shouldn’t fail unless the requirements were changed. If that happens, the test should be updated before the code is altered. Of course, if a test was forgotten as part of a requirement change, we expect it to fail. It did its job as change detector. At this time, the test will likely need to change to get it passing.\n\nWhenever a test fails in a continuous integration and build process, the team’s highest priority (other than a critical production problem) should be to get the build passing again. Don’t comment out the failing test and ﬁx it later; that’s the road to perdition. Soon you’ll have dozens of commented-out tests and a lot of technical debt. Everyone on the team should stop what they’re doing and make sure the build goes “green” again. Determine if a bug has been introduced, or if the test simply needs to be updated to accommo- date intentionally changed behavior. Fix the problem, check it in, and make sure all of the tests pass.\n\nEarly on in our agile efforts, my team wasn’t ﬁxing broken tests fast enough. I wrote “Tests are not temporary!” on the whiteboard to remind everyone that once a test passes, it needs to keep passing. A few days later, the words “but testers are!” had been added to get back at me. We did get much better at keep- ing our builds “green” after that.\n\nOne passing test leads to another. Keep your tests current and maintainable with refactoring. Extend them to cover other test cases. The various combi- nations and scenarios might or might not become part of the regression suite after they pass. We want our regression suite to run in a timely manner, and having too many tests for edge cases would slow it down.\n\nUse Appropriate Test Design Patterns\n\nWhen designing tests, look at different patterns and choose the ones that work for you. Keep them as simple as you can. Before you can design tests, you have to identify the ones you need. Pierre Veragen coined the term test genesis patterns to note the patterns that help you think of tests. Examples and use cases feed into our test genesis patterns.\n\n179\n\n—Lisa\n\n180\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nBuild/Operate/Check Lisa’s team often goes with a build/operate/check pattern: Build the input data, in memory or actually in the database, depending on the purpose of the test; invoke the production code to operate on those inputs; and check the re- sults of that operation. Some teams call this setup/execute/validate. For exam- ple, to test the invoice presented to a new account holder, set up the fees to be charged, input the properties of the account that relate to fee amounts, run the code that calculates the fees, and then check to see what fees were actually charged. See Figure 9-9 for an example of a test that sets up a loan with a spec- iﬁed amount, interest rate, term, payment frequency, and service start date and then checks the resulting amortization schedule. The test data is built in memory, which makes for a speedy test. A “teardown” ﬁxture (not shown) re- moves the test data from memory so it won’t interfere with subsequent tests.\n\nIf there’s a need to test the application’s data access layer, tests can run using an actual database. Each test can insert the test data it needs, operate on it, check results, and delete the data. Testing with data in a real database can be a means of automating a test against legacy code whose data access and busi- ness logic layers aren’t easily separated.\n\nFigure 9-9 Example test with build/operate/check pattern\n\nSTRATEGIES FOR WRITING TESTS\n\nNotice that the “check” table in the example uses a declarative style, with each row forming an independent test case, without changing the state of the system. Each row in our example tests a line in the loan amortization sched- ule. In the next section, we’ll look at patterns that are in a procedural style, with steps that change or test the state of the system.\n\nTime-Based, Activity, and Event Patterns Sometimes a timeline-based procedural pattern reﬂects the business better. For example, when testing a loan, we want to make sure interest and princi- pal are applied correctly for each payment. The amount of interest depends on the date the payment was received and the date of the last payment pro- cessed. We want a test that simulates taking out a loan for a certain dollar amount, interest rate, and time period, and then over time simulates the bor- rower sending in payments, which are received and processed. Figure 9-10 shows a simple example of a FitLibrary “DoFixture” test that takes out a loan, checks the payment amount, posts the borrower’s payments, receives the payments and processes them, and then checks the interest, principal, and loan balance amount. It also checks the loan default state.\n\nDepending on the domain, a time- or event-based approach might simulate the actual business processes better and be more understandable to business experts than a declarative type test. Other customers might ﬁnd the declara- tive table style simpler to understand, because it hides the procedural details. Different patterns work best for different situations, so experiment with them.\n\nFigure 9-10 Sample time-based test\n\n181\n\n182\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nLearning More Your team should educate itself on test patterns that help drive program- ming. Finding the right pattern for each type of test ensures the test commu- nicates clearly, is easy to maintain, and runs in an optimal amount of time. See the bibliography for more invaluable resources on test design, such as Gerard Meszaros’s xUnit Test Patterns: Refactoring Test Code.\n\nBring programmers and testers together to brainstorm test approaches and to help decide what tests can be automated and how the code should be de- signed to support testing. Business logic and algorithms should be accessible by test ﬁxtures, without having to go through a user interface or batch sched- uling process. This enables test-driven development, which in turn produces testable architecture.\n\nA common approach to automating tests is by driving tests with keywords or action words. This can be used with tools such as Fit and FitNesse, or Ruby with Watir. We’ll explain this next.\n\nKeyword and Data-Driven Tests\n\nData-driven testing is a tool that can help reduce test maintenance and en- able you to share your test automation with manual testers. There are many times when you want to run the same test code over and over, repeating only the inputs and expected results. Spreadsheets or tables, such as those sup- ported by Fit, are excellent ways to specify inputs. The test ﬁxture, method, or script can loop through each data value one at a time, matching expected results to actual results. By using data-driven tests, you are actually using ex- amples to show what the application is supposed to do.\n\nKeyword-driven testing is another tool used in automated testing, where pre- deﬁned keywords are used to deﬁne actions. These actions correspond to a process related to the application. It is the ﬁrst step in creating a domain test- ing language. These keywords (or action words) represent a very simple spec- iﬁcation language that non-programmers can use to develop automated tests. You still need programmers or technical automation specialists to im- plement the ﬁxtures that the action words act on. If these keywords are ex- tended to emulate the domain language, customers and nontechnical testers can specify tests that map to the workﬂow more easily.\n\nThe sample spreadsheet in Figure 9-11 shows how one company used action words to automate their test setup. The same action words can be used to test. The words Signup, Signoff, and CCDeposit are words that are domain-",
      "page_number": 217
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 225-233)",
      "start_page": 225,
      "end_page": 233,
      "detection_method": "topic_boundary",
      "content": "TESTABILITY\n\nScriptID\n\nLogging\n\nEnvironment\n\nSite\n\nLang\n\nEmail\n\n8\n\nON\n\nSTAGING\n\nGlobal\n\nEnglish\n\nON\n\nTest ID\n\nDescription\n\nClassName\n\nAction\n\nInput 1\n\nInput 2\n\nInput 3\n\nInput 4\n\n# Signup Customer\n\n123\n\nCdn customer\n\nMember\n\nSignup\n\nJanet\n\nGregory\n\nCalgary\n\n123 St\n\n123\n\nReg complete\n\nMember\n\nsignoff\n\nTRUE\n\n123\n\nLog out\n\nMember\n\nLog_out\n\nTRUE\n\n# Perform CC Deposit\n\nSetup\n\nDescription\n\nClassName\n\nAction\n\nInput 1\n\nInput 2\n\nInput 3\n\nInput 4\n\n234\n\nLog in mbr\n\nMember\n\nLogin\n\nget.AcctId\n\nget.ID_greg\n\nget.pwd_\n\n234\n\nSubmit CC Txn\n\nMember\n\nCCDeposit\n\nVISA\n\n4444333322\n\n02\n\n2008\n\n234\n\nMember Logout\n\nMember\n\nlog_out\n\nEND\n\nFigure 9-11 Sample test spreadsheet with action words\n\nspeciﬁc. Their users could easily write tests without understanding the un- derlying code.\n\nCombining data-driven and keyword-driven testing techniques can be very powerful. Fit and FitNesse use both keywords and data to drive tests. The other tools we’ve described in this chapter can also accommodate this approach.\n\nAny test strategy can run into trouble if the code isn’t designed to be easily tested. Let’s take a look at testability concerns.\n\nTESTABILITY Business-facing tests built with appropriate design patterns and written ahead of any coding help the team achieve a testable code design. The programmers\n\n183\n\nInput 5\n\nT1T 2A2\n\nInput 5\n\n25.86\n\n184\n\nCHAPTER 9\n\nJanet’s Story\n\nLisa’s Story\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nstart by looking at the business-facing tests, perhaps together with a tester, ana- lyst, or customer, so that the need to execute those tests are always in their minds as they proceed with their test-driven design. They can build so that the tests provide inputs and control run-time conditions.\n\nI ran into a snag when I was trying to automate some GUI workﬂow with Ruby and Watir. The calendar pop-up feature was not recognized, and the data ﬁeld was read-only. I took my problem to one of the programmers. We paired to- gether so that he could see the issue I was having. The ﬁrst thing he did was to understand the calendar feature. He thought it would be too difﬁcult to auto- mate the test, so he suggested another alternative. He created a new method that would “fool” the input ﬁeld so it would accept a date into the text ﬁeld. We knew the risk was no automation on the calendar, but for simplicity’s sake we went with his option.\n\nNot all code is testable using automation, but work with the programmers to ﬁnd alternative solutions to your problems.\n\nLet’s look at techniques that promote design of testable code.\n\nCode Design and Test Design\n\nIn Chapter 7, “Technology-Facing Tests that Support the Team,” we explained how test-driven development at the unit level ensures a testable architecture. This is true for business-facing tests as well. The layered architecture Lisa’s team designed works just as well for functional testing. Testing can be done directly against the business logic without involving the user interface, and if appropriate, without involving the database layer. This doesn’t mean that the database layer doesn’t need to be tested. It still needs to be tested, just maybe somewhere else.\n\nTestability has to be considered when coding the presentation layer as well. GUI test tools work better on well-designed code developed with good practices.\n\nWhen I ﬁrst started trying to automate GUI tests using Canoo WebTest, I discovered that the HTML and JavaScript used in the system didn’t comply with standards and contained many errors. WebTest and the tool it’s built on, HtmlUnit, required cor- rect, standard HTML and Javascript. Specifying tests depended on good HTML\n\n—Janet\n\nIn Part IV, “Test Au- tomation,” we’ll dive into develop- ing a successful test automation strategy and look at considerations such as building your own tools versus using third- party or open source tools.\n\nTESTABILITY\n\npractices such as giving each element a unique ID. The programmers started writ- ing HTML and JavaScript (and later, Ajax) with the test tool in mind, making test automation much easier. They also started validating their HTML and making sure it was up to industry standards. This also reduced the possibility of the application having problems in different browsers and browser versions.\n\nCoding and testing are part of one process in agile development. Code design and test design are complementary and interdependent. It’s a chicken-and- egg scenario: You can’t write tests without a testable code design, and you can’t write code without well-designed tests that clearly communicate re- quirements and are compatible with the system architecture. This is why we always consider coding and testing together. When we estimate stories, we in- clude time for both coding and testing, and when we plan each iteration and story, we budget time to design both tests and code. If automating a test proves difﬁcult, evaluate the code design. If programmers are writing code that doesn’t match customer expectations, the problem might be poorly de- signed tests.\n\nAutomated vs. Manual Quadrant 2 Tests\n\nWe’ve assumed that at least a good-sized portion of the tests that guide pro- gramming will be automated. Manual test scenarios can also drive program- ming if you share them with the programmers early. The earlier you turn them into automated tests, the faster you will realize the beneﬁt. Most man- ual tests fall more into the “critique product” quadrant where we might learn things about the story we hadn’t anticipated with the initial set of tests.\n\nThat doesn’t stop us from writing tests that might not be appropriate for au- tomation. Don’t sweat the details when you’re writing tests. You might come up with one-off tests that are important to do but not important to repeat over and over in a regression suite. You might start thinking about end-to- end scenarios or springboards to exploratory test sessions that might be facil- itated with some automation but need an intelligent human to conduct them in full. You’ll ﬁgure that out later. Right now, we want to make sure we cap- ture the customer’s critical requirements.\n\nStart with a simple approach, see how it works, and build on it. The impor- tant thing is to get going writing business-facing tests to support the team as you develop your product.\n\n185\n\n—Lisa\n\n186\n\nCHAPTER 9\n\nChapter 14, “An Agile Test Automa- tion Strategy,” goes into more detail on how to manage auto- mated tests.\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nTEST MANAGEMENT If we’re automating tests, it makes sense to present them in the automation tool framework, even if they’re not yet executable. We want some way for all tests, even those that won’t be automated, to be accessible to everyone on the development team and understandable to our customers. There are lots of options available that let everyone on the team see tests. Wikis are a common way to share test cases, and some tools such as FitNesse use a wiki or similar tool, enabling narrative requirements, examples, and executable tests to co- exist in one place.\n\nTests should be included in your source code control, so that you can track which versions of the tests go with which versions of the code. At the very least, have some kind of version control for your tests. Some teams use test management tools or comprehensive test frameworks that might integrate with requirements management, defect tracking, or other components.\n\nSUMMARY In this chapter, we’ve looked at tools you might want in your toolkit to help create business-facing tests that help drive development and guidelines to make sure the tools help rather than get in the way. The tools and guidelines included the following:\n\n(cid:2) Teams need the right tools to elicit requirements and examples,\n\nfrom the big picture down to details, including checklists, mind maps, spreadsheets, mock-ups, ﬂow diagrams, and various software-based tools.\n\n(cid:2) Tools to express examples and automate tests, below and through the GUI, are also essential to agile test automation. Some of these tools include unit test tools, behavior-driven development tools, FitNesse, Ruby with Watir, Selenium, and Canoo WebTest.\n\n(cid:2) “Home brewed” test automation helps teams keep the total cost of\n\nownership of their automated tests low.\n\n(cid:2) Driving development with business-facing tests is one way agile teams\n\nare motivated to design testable code.\n\n(cid:2) Test strategies for building your automation should include building your tests incrementally and making sure they always pass. Design patterns can be used to help you create effective tests.\n\nSUMMARY\n\n(cid:2) Keyword and data-driven testing is a common approach that works\n\nwith the tools we’ve discussed in this chapter.\n\n(cid:2) Consider testability in your code design, and choose your test tools\n\nwisely, because they need to work with your code.\n\n(cid:2) We need some way to organize tests so that they can be used effec-\n\ntively and put into version control.\n\n187\n\nThis page intentionally left blank\n\nChapter 10\n\nBUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nUsing Automated Functional Test Tools for ET\n\nTest Setup\n\nMonitoring Tools\n\nToolkit\n\nTest Data Generation\n\nSimulators\n\nIntroduction\n\nEmulators\n\nDemonstrations\n\nWorkflow Testing\n\nUser Documentation\n\nReports\n\nDocumentation\n\nBusiness-Facing/ Critique Product\n\nScenario Testing\n\nDomain Knowledge\n\nEnd-to-End Scenarios\n\nSoap Opera Testing\n\nAPI\n\nBehind the GUI\n\nWeb Services\n\nLearning by Testing\n\nUser Needs and Persona Testing\n\nExploratory Testing\n\nSession-Based Testing\n\nAutomate\n\nNavigation\n\nUsability\n\nAn Exploratory Tester\n\nCheck Out the Competition\n\nThis chapter covers the third quadrant of the testing matrix. In Chapter 8, “Business-Facing Tests that Support the Team,” we talked about the second quadrant and how to use business-facing tests to support programming. In this chapter, we show you how to critique the product with different types of business-facing tests. We’ll also talk about tools that might help with these activities.\n\n189\n\n190\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nINTRODUCTION TO QUADRANT 3 Remember that business-facing tests are those you could describe in terms that would (or should) be of interest to a business expert. When we mention testing in traditional phased approaches, it pretty much always means cri- tiquing the product after it is built. By now, you might think that in agile de- velopment this part of testing should be easy. After all, we just spent all that time making sure it works as expected. The requirements have all been tested as they were built, including security and other nonfunctional requirements, right? All that’s left is to possibly ﬁnd some obscure or interesting bugs.\n\nAs testers, we know that people make mistakes. No matter how hard we try to get it right the ﬁrst time, we sometimes get it wrong. Maybe we used an ex- ample that didn’t test what we thought it did. Or maybe we recorded a wrong expected result so the test passed, but it was a false positive. The business ex- pert might have forgotten some things that real users needed. The best cus- tomer may not know what she wants (or doesn’t want) until she sees it.\n\nCritiquing or evaluating the product is what testers or business users do when they assess and make judgments about the product. These evaluators form perceptions based on whether they like the way it behaves, the look and feel, or the workﬂow of new screens. It is easier to see, feel, and touch a product and respond than to imagine what it will look like when it is described to you.\n\nIt’s difﬁcult to automate business-facing tests that critique the product, because such testing relies on human intellect, experience, and instinct. However, auto- mated tools can assist with aspects of Quadrant 3 tests (see Figure 10-1), such as test data setup. The last section of this chapter contains examples of the types of tools that help teams focus on the important aspects of evaluating the product’s value.\n\nWhile much of the testing we discuss in this chapter is manual, don’t make the mistake of thinking that this manual testing will be enough to produce high-quality software and that you can get away with not automating your regression tests. You won’t have time to do any Quadrant 3 tests if you haven’t automated the tests in Quadrants 1 and 2.\n\nEvaluating or critiquing the product is about manipulating the system under test and trying to recreate actual experiences of the end users. Understanding different business scenarios and workﬂows helps to make the experience more realistic.\n\nDEMONSTRATIONS\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nQ2\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 10-1 Quadrant 3 tests\n\nDEMONSTRATIONS We recommend showing customers what you’re developing early and often. As soon as a rudimentary UI or report is available during story development, show it to the product owner or other domain expert on the team. However, not everyone on the business side will get a chance to see the iteration’s deliv- erables until the iteration demo. End-of-iteration demonstrations are an op- portunity for the business users and domain experts to see what has been delivered in the iteration and revise their priorities. It gives them a chance to say, “That’s what I said, but it’s not what I meant.” This is a form of critiquing the product.\n\n191",
      "page_number": 225
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 234-242)",
      "start_page": 234,
      "end_page": 242,
      "detection_method": "topic_boundary",
      "content": "192\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nJanet’s Story\n\nI worked on a project that had ﬁve separate teams of eight to ten members, all developing the same system. Even though they were on the same ﬂoor, communi- cation was an issue. There were many dependencies and overlaps, so the pro- grammers depended on team-lead meetings to share information. However, the business users and testers needed to see what was being developed by other teams. They relied on end-of-iteration demonstrations given by each team to learn what the other teams were doing.\n\nDemonstrations to the executives or upper management can instill conﬁ- dence in your project as well. One of the downfalls of a phased project is there is nothing to see until the very end, and management has to place all of its trust in the development team’s reports. The incremental and iterative na- ture of agile development gives you a chance to demonstrate business value as you produce it, even before you release it. A live demonstration can be a very powerful tool if the participants are actively asking questions about the new features.\n\nRather than waiting until the end of the iteration, you can use any opportu- nity to demonstrate your changes. A recent project Janet worked on used reg- ularly scheduled meetings with the business users to demonstrate new features in order to get immediate feedback. Any desired changes were fed into the next iteration.\n\nChapter 19, “Wrap Up the Iteration,” talks about end- of-iteration dem- onstrations and reviews.\n\nChoose a frequency for your demonstrations that works for your team so that the feedback loop is quick enough for you to incorporate changes into the release.\n\nInformal demos can be even more productive. Sit down with a business ex- pert and show her the story your team is currently coding. Do some explor- atory testing together. We’ve heard of teams that get their stakeholders to do some exploratory testing after each iteration demo in order to help them think of reﬁnements and future stories to change or build on the functional- ity just delivered.\n\nSCENARIO TESTING Business users can help deﬁne plausible scenarios and workﬂows that can mimic end user behavior. Real-life domain knowledge is critical to creating accurate scenarios. We want to test the system from end to end but not neces- sarily as a black box.\n\n—Janet\n\nChapter 14, “An Agile Test Automa- tion Strategy,” ex- amines different approaches to ob- taining test data.\n\nSCENARIO TESTING\n\nOne good technique for helping the team understand the business and user needs is “soap opera testing,” a term coined by Hans Buwalda [2003]. The idea here is to take a scenario that is based on real life, exaggerate it in a man- ner similar to the way TV soap operas exaggerate behavior and emotions, and compress it into a quick sequence of events. Think about questions like, “What’s the worst thing that can happen, and how did it happen?”\n\nSoap Opera Test Example\n\nLisa worked on an Internet retail site, where she found soap opera tests to be effective. Here’s an example of a soap opera scenario to test inventory, preorder, and backorder processes of an Internet retailer’s warehouse.\n\nThe most popular toy at our online toy store this holiday season is the Super Tester Action Figure. We have 20 preorders awaiting receipt of the items in our warehouse. Finally, Jane, a warehouse supervisor, receives 100 Super Tester Action ﬁgures. She updates the inventory system to show it is available inventory against the purchase order and no longer a preorder. Our website now shows Super Tester Action Figures available for delivery in time for the holidays. The system releases the preorders, which are sent to the warehouse. Meanwhile, Joe, the forklift driver, is distracted by his cell phone, and accidentally crashes into the shelf con- taining the Super Tester Action Figures. All appear to be smashed up beyond recognition. Jane, horriﬁed, removes the 100 items from avail- able inventory. Meanwhile, more orders for this popular toy have piled up in the system item. Sorting through the debris, Jane and Joe ﬁnd that 14 of the action ﬁgures have actually survived intact. Jane adds them back into the available inventory.\n\nThis scenario tests several processes in the system, including preorder, purchase order receipt, backorder, warehouse cancels, and preorder release. How many Super Tester toys will show as available on the shop- ping website at the end of all that? While executing the scenario, we’ll probably ﬁnd other areas we want to investigate; maybe the purchase order application is difﬁcult to use or the warehouse inventory updates aren’t reﬂected properly in the website. Thinking up and executing these types of tests will teach us more about what our users and other external customers need than running predeﬁned functional tests on narrower areas of the application. As a bonus, it’s fun!\n\nAs a tester, we often “make up” test data, but it is usually simple so we can easily check our results. When testing different scenarios, both the data and the ﬂow need to be realistic. Find out if the data comes from another system or if it’s input manually. Get a sample if you can by asking the customers to provide data for testing. Real data will ﬂow through the system and can be checked along the way. In large systems, it will behave differently depending on what decisions are made.\n\n193\n\n194\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nTools to help deﬁne the scenarios and workﬂows can be simple. Data ﬂow or process ﬂow diagrams will help identify some of the common scenarios. These scenarios can help you think through a complex problem if you take the time. Consider the users and their motivation.\n\nLisa’s Story\n\nOur team planned to rewrite the core functionality of the application that pro- cesses the daily buys and sells of mutual funds. These trades are the result of re- tirement plan participants making contributions, exchanging balances from one fund to another, or withdrawing money from their accounts. Lisa’s coworker, Mike Thomas, studied the existing trade processing ﬂow and diagrammed it so that the team could understand it well before trying to rewrite the code. Figure 10-2 shows a portion of the ﬂow diagram. WT stands for the custodian who does the actual trading. Three different ﬁle types are downloaded and translated into read- able format: CFM, PRI, and POS. Each of these ﬁles feeds into a different part of the application to perform processing and produce various outputs: settled trades, a ticker exception report, and a fund position report.\n\n—Lisa\n\nTrade Processing\n\nDownload/Translate\n\nWT\n\nDownload/Translate\n\nCFM File\n\nDownload/Translate\n\nPOS File\n\nTP1 Settle Trades\n\nPRI File\n\nTP3 Upload/Verify Positions\n\nSettled Txns\n\nTP2 Update Prices\n\nOmnibus Fund Position Rpt\n\nTicker Exception Rpt\n\nFigure 10-2 Sample portion of a process ﬂow diagram\n\nWhen testing end-to-end, make spot checks to make sure the data, status ﬂags, calculations, and so on are behaving as expected. Use ﬂow diagrams and other visual aids to help you understand the functionality. Many organi-\n\nThe bibliography lists more resources you should investi- gate to learn more about exploratory testing.\n\nSee the bibliogra- phy for some links to more about Rapid Software Testing.\n\nEXPLORATORY TESTING\n\nzations depend on reports to make decisions, and those reports seem to be the last thing we verify. If your scenarios have been identiﬁed correctly, you might be able to use your application reports to provide a ﬁnal check.\n\nEXPLORATORY TESTING Exploratory testing (ET) is an important approach to testing in the agile world. As an investigative tool, it’s a critical supplement to the story tests and our automated regression suite. It is a sophisticated, thoughtful approach to testing without a script, and it enables you to go beyond the obvious varia- tions that have already been tested. Exploratory testing combines learning, test design, and test execution into one test approach. We apply heuristics and techniques in a disciplined way so that the “doing” reveals more implica- tions that just thinking about a problem. As you test, you learn more about the system under test and can use that information to help design new tests.\n\nExploratory testing is not a means of evaluating the software through ex- haustive testing. It is meant to add another dimension to your testing. You do just enough to see if the “done” stories are really done to your satisfaction.\n\nA valuable side effect of exploratory testing is the learning that comes out of it. It reveals areas of the product that could use more automated tests and brings up ideas for new or modiﬁed features that lead to new stories.\n\nExploratory Testing Explained\n\nMichael Bolton is a trainer and consultant in rapid and exploratory testing approaches. He teaches a course called Rapid Software Testing, which he co- writes with senior author James Bach. Here’s Michael’s deﬁnition of explor- atory testing.\n\nCem Kaner didn’t invent exploratory testing, but he identiﬁed and named it in 1983, in the ﬁrst edition of Testing Computer Software, as an approach all testers use when their brains are engaged in their work. He and James Bach, the other leading advocate of the approach, have long deﬁned exploratory testing as “simultaneous test design, test execution, and learning.” Kaner also deﬁnes exploratory testing more explicitly as “a style of testing that emphasizes the freedom and responsibility of the in- dividual tester to continually optimize the value of her work by treating learning, test design, test execution, and test result interpretation as ac- tivities that continue in parallel throughout the project.” That’s quite a mouthful. What does it mean?\n\n195\n\n196\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nThe most important thing to remember about exploratory testing is that it’s not a test technique on its own. Instead, it’s an approach or a mind- set that can be applied to any test technique. The second thing to remember is that exploratory testing is not merely about test execution; testers can also take an exploratory approach when they’re designing new tests at the beginning of the iteration or analyzing the results of tests that have already been performed. A third important note is that exploratory testing isn’t sloppy or slapdash or unprepared testing. An exploratory approach might require very extensive and elaborate prepa- ration for certain tests—and an exploratory tester’s knowledge and skill set, developed over years, is an often invisible yet important form of preparation. An exploratory test might be performed manually, or might employ extensive use of test automation—that is, any use of tools to support testing. So if exploratory testing isn’t a technique, nor test exe- cution, nor spontaneous, nor manual, what is it that makes a test activity exploratory? The answer lies in the cognitive engagement of the tester— how the tester responds to a situation that is continuously changing.\n\nSuppose that a tester is given the mission to test a conﬁguration dialog for a text editor. A tester using an exploratory approach would use spec- iﬁcations and conversations about the desired behavior to inform test ideas, but would tend to record these ideas in less detail than a tester using a scripted approach. A skilled tester doesn’t generally need much explicit instruction unless the test ideas require some speciﬁc actions or data. If so, they might be written down or supplied to a program that could exercise them quickly. Upon seeing the dialog, the exploratory tester would interact with it, usually performing tests in accordance with the original test ideas—but she might also turn her attention to other ideas based on new problems or risks in the dialog as it appeared in front of her. Can two settings conﬂict in a way not covered by existing tests? The exploratory tester immediately investigates by performing a test on the spot. Does the dialog have a usability issue that could inter- fere with a user’s work ﬂow? The exploratory tester quickly considers a variety of users and scenarios and evaluates the signiﬁcance of the prob- lem. Is there a delay upon pressing the OK button? The exploratory tester performs a few more tests to seek a general pattern. Is there a possibility that some conﬁguration options might not be possible on another platform? The exploratory tester notes the need for additional testing and moves on. Upon receiving new builds, the exploratory tester would tend to deemphasize repetition and emphasize variation in order to discover problems missed by older tests that are no longer revealing interesting information. This approach, which has always been fruitful, is even more powerful in environments where the need for repeated test- ing is handled by the developers’ low-level, automated regression tests.\n\nExploratory testing is characterized by the degree to which the tester is under her own control, making informed choices about what he or she\n\nEXPLORATORY TESTING\n\nis going to do next, and where the last outcome of the last activity con- sciously informs the next choice. Exploratory and scripted approaches are at the opposite poles of a continuum. At the extreme end of the scripted mind-set, the decision as to what to do next comes exclusively from someone else, at some point in the past. In the exploratory mind-set, the decision to continue on the same line of inquiry or to choose a new path comes entirely from the individual tester, in the moment in which the activity occurs, The result of the last test strongly informs the tester’s choices for the next test. Other inﬂuences include the stakeholders for whom test information might be important, the quality criteria that are important to stakeholders, the test coverage that stakeholders seek, spe- ciﬁc risks associated with the item being tested, the needs of the end user of the product, the skills of the tester, the skills of the developers, the state of the item under test, the schedule for the project, the equip- ment and tools that are available to the tester, and the extent to which she can use them effectively—and that’s only a partial list.\n\nNo test activity performed by a thinking human is entirely scripted. Humans have an extraordinary capacity to recognize things even when people are telling them not to, and as a result we can be distracted and diverted—but we can learn and adapt astonishingly quickly to new infor- mation and investigate its causes and effects. Machines only recognize what they’ve been programmed to recognize. When they’re confronted with a surprising test result, at best they ignore it; at worst, they crash or destroy data.\n\nYet no test activity performed on behalf of a client is entirely exploratory, either. The exploratory tester is initially driven by the testing mission, which is typically set out by the client early in the project. Exploratory work can also be guided by checklists, strategy models, coverage out- lines, risk lists—ideas that might come from other people at other times. The more that the tester is controlled by these ideas rather than guided by them, the more testing takes on a scripted approach.\n\nGood exploration requires continuous investigation of the product by engaged human testers, in collaboration with the rest of the project community, rather than following a procedurally structured approach, performed exclusively by automation. Exploration emphasizes individuals and interactions over processes and tools. In an agile environment, where code is produced test-ﬁrst and is covered with automated regres- sion tests, testers can have not only the conﬁdence but also the man- date to develop new tests and seek out new problems in the moment. Exploration emphasizes responding to change versus following a plan. Exploratory approaches use variation to drive an active search for prob- lems instead of scripted manual or automated test cases that merely conﬁrm what we already knew. Exploration emphasizes working soft- ware over comprehensive documentation. And to be effective, good\n\n197\n\n198\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nexploration requires frequent feedback between testers, developers, customers, and the rest of the project community, not merely repetition of tests that were prepared at the beginning of the iteration, before we had learned important things about the project. Exploration empha- sizes customer collaboration over negotiated contracts. Exploratory approaches are fundamentally agile.\n\nExploratory testing embraces the same values as agile development. It’s an im- portant part of the “agile testing mind-set” and critical to any team’s success.\n\nPeople unfamiliar with exploratory testing often confuse it with ad hoc test- ing. Exploratory testing isn’t sitting down at a keyboard and typing away. Unskilled “black box” testers may not know how to do exploratory testing.\n\nExploratory testing starts with a charter of what aspects of the functionality will be explored. It requires critical thinking, interpreting the results, and comparing them to expectations or similar systems. Following “smells” when testing is an important component. Testers take notes during their explor- atory testing sessions so that they can reproduce any issues they see and do more investigation as needed.\n\nTechnique: Exploratory Testing and Information Evaluation\n\nJon Hagar, an experienced exploratory tester, learner, and trainer, shares some activities, characteristics, and skills that are vital to effective exploratory testing.\n\nExploratory testing uses the tester’s understanding of the system, along with critical thinking, to deﬁne focused, experimental “tests” which can be run in short time frames and then fed back into the test planning process.\n\nAn agile team has many opportunities to do exploratory testing, since each development cycle creates production-ready, working software. Starting early in each development cycle, consider exploratory tests based on:\n\nRisk (analysis): The critical things you and the customer/user think can go wrong or be potential problems that will make people unhappy.\n\nModels (mental or otherwise) of how software should behave: You and/or the customer have a great expectation about what the newly produced function should do or look like, so you test that.\n\nPast experience: Think about how similar systems have failed (or suc- ceeded) in predictable patterns that can be reﬁned into a test, and explore it.\n\nEXPLORATORY TESTING\n\nWhat your development team is telling you: Talk to your developers and ﬁnd out what “is important to us.”\n\nMost importantly: What you learn (see and observe) as you test. As a tester on an agile team, a big part of your job is to constantly learn about your product, your team, and your customer. As you learn, you should quickly see tests based on such things as customer needs, common mistakes the team seems to be making, or good/ bad characteristics of the product.\n\nSome tests might be good candidates for automated regression suites. Some might just answer your exploratory charter and be “done.” The agile team must critically think about what they are learning and “evolve” tests accordingly. The most important aspect here is to be “brain on” while testing, where you are looking for the “funny,” unexpected, or new, which automated tests would miss. Use automation for what it is good at (repetitive tasks) and use agile humans for what we are good at (seeing, thinking, and dealing with the unexpected).\n\nSeveral components are typically needed for useful exploratory testing:\n\nTest Design: An exploratory tester as a good test designer under- stands the many test methods. You should be able to call different methods into play on the ﬂy during the exploration. This agility is a big advantage of exploratory testing over automated (scripted) pro- cedures, where things must be thought out in advance.\n\nCareful Observation: Exploratory testers are good observers. They watch for the unusual and unexpected and are careful about as- sumptions of correctness. They might observe subtle software characteristics or patterns that drive them to change the test in real time.\n\nCritical Thinking: The ability to think openly and with agility is a key reason to have thinking humans doing nonautomated exploratory testing. Exploratory testers are able to review and redirect a test into unexpected directions on the ﬂy. They should also be able to ex- plain their logic of looking for defects and to provide clear status on testing. Critical thinking is a learned human skill.\n\nDiverse Ideas: Experienced testers and subject matter experts can produce more and better ideas. Exploratory testers can build on this diversity during testing. One of the key reasons for exploratory tests is to use critical thinking to drive the tests in unexpected directions and ﬁnd errors.\n\nRich Resources: Exploratory testers should develop a large set of tools, techniques, test data, friends, and information sources upon which they can draw. The agile test team members should grow their exploratory resources throughout a project and throughout their careers.\n\n199\n\n200\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nTo help you understand a day in the life of an agile exploratory tester, here is a short tester’s story:\n\nI arrived at 8:00 a.m. and reviewed what had happened the night before during automated testing. The previous night’s automated tests found some minor but interesting errors. A password ﬁeld on a login form had accepted a special character, which should have been rejected by the validation. I created an outline as a starting point for my “attack” (a top- level plan and/or risk list).\n\nAs I thought about my “plan of attack,” I sketched a small state model of the problem on a ﬂip chart and showed this to a developer and my team’s customer rep. I designed a test incorporating their suggestions, using some data stress inputs that I expected the validation to reject (a 1 MG ﬁle of special characters). I executed my test with my stress input, and, sure enough, the system rejected them as expected. I tried a differ- ent data set and the system failed with a buffer overﬂow in the database. I was learning, and we were on the trail of a potentially serious security bug. As the day went on, I explored different inputs to the password ﬁeld and worked with the team to get the bug ﬁxed.\n\nYou can learn from automated test results as well as from exploratory testing. Each type of testing feeds into the other. Develop a broad range of skills so you’ll be able to identify important issues and write tests to prevent them from reoccurring.\n\nThe term exploratory testing was popularized by the “context-driven school” of testing. It’s a highly disciplined activity, and it can be learned. Session-based test management is one method of testing that’s designed to make explor- atory testing auditable and measurable [Bach, 2003].\n\nSession-Based Testing\n\nSession-based testing combines accountability and exploratory testing. It gives a framework to a tester’s exploratory testing experience so that they can report results in a consistent way.\n\nJanet’s Story\n\nJames Bach [2003] compares exploratory testing to putting together a jigsaw puz- zle. When I ﬁrst read his article with the jigsaw puzzle analogy, exploratory testing made perfect sense to me.\n\nI start a jigsaw puzzle by dumping out all of the pieces of the puzzle and then sorting them into the different colors and edge pieces. Next, I put the edge pieces together, which gives me a framework in which to start. The edge of the jigsaw is analogous both to the mission statement, which helps me focus, and to the time- boxing of a session, which keeps me within certain limits.",
      "page_number": 234
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 243-250)",
      "start_page": 243,
      "end_page": 250,
      "detection_method": "topic_boundary",
      "content": "For more informa- tion on session- based testing, check the bibliog- raphy for work by Jonathan Bach.\n\nEXPLORATORY TESTING\n\nSession-based testing is a form of exploratory testing, but it is time-boxed and a bit more structured. I learned about session-based testing from Jonathan Bach and found it gave me the structure I needed to do exploratory testing well. I use the same skills as I do for a jigsaw puzzle: I look for patterns in color or shapes or perhaps something that just doesn’t look right, an anomaly. My thought process can take those patterns and make sense of them, using heuristics I have devel- oped to help me solve a puzzle.\n\nLike solving the jigsaw puzzle by putting together the outside pieces ﬁrst, we can use session-based testing to give us the framework in which we work. In session-based testing, we create a mission or a charter and then time-box our session so we can focus on what’s important. Too often as testers, we can go off track and end up chasing a bug that might or might not be important to what we are currently testing.\n\nSessions are divided into three kinds of tasks: test design and execution, bug investigation and reporting, and session setup. We measure the time we spend on setup versus actual test execution so that we know where we spend the most time. We can capture results in a consistent manner so that we can report back to the team.\n\nAutomation and Exploratory Testing\n\nWe can combine exploratory testing with test automation as well. Jonathan Kohl, in his article “Man and Machine” [2007], talks about interactive test automation to assist exploratory testing. Use automation to do test set up, data generation, repetitive tasks, or to progress along a workﬂow to the place you want to start. Then you start using your testing skills and experience to ﬁnd the really “good” bugs, the insidious ones that otherwise escape atten- tion. You can also use an automated test suite to explore. Just modify it a bit, watch the results as it runs, modify it again, and watch what happens.\n\nAn Exploratory Tester\n\nWith exploratory testing, each tester has a different approach to a problem, and has a unique style of working. However, there are certain attributes that make for a good exploratory tester. A good tester:\n\n(cid:2) Is systematic, but pursues “smells” (anomalies, pieces that aren’t\n\nconsistent)\n\n(cid:2) Learns to recognize problems through the use of Oracles (principle or\n\nmechanism by which we recognize a problem)\n\n201\n\n—Janet\n\n202\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\n(cid:2) Chooses a theme or role or mission statement to focus testing (cid:2) Time-boxes sessions and side trips (cid:2) Thinks about what the expert or novice user would do (cid:2) Explores together with domain experts (cid:2) Checks out similar or competitive applications\n\nExploratory testing helps us learn about the behavior of an application. Testers generally know a lot about the application they’re testing. How do they judge whether the application is usable by users who are less technical or not familiar with it? Usability testing is vital for many software systems. We’ll talk about that in the next section.\n\nUSABILITY TESTING There are two types of usability testing. The ﬁrst type is the kind that is done up front by the user experience folks, using tools such as wire frames to help drive programming. Those types of tests belong in Quadrant 2. In this sec- tion, we’re talking about the kind of usability testing that critiques the prod- uct. We use tools such as personas and our intuition to help us look at the product with the end user in mind.\n\nUser Needs and Persona Testing\n\nLet’s look at an online shopping example. We think about who will use the site. Will it be people who have shopped online before, or will it be brand new users who have no idea how to proceed? We’re guessing it will be a mix- ture of both, as well as others. Take the time to ask your marketing group to get the demographics of the end users. The numbers might help you plan your testing.\n\nOne approach to using personas is for your team to invent several different users of your application representing different experience levels and needs. For our Internet retail application, we might have the following personas:\n\n(cid:2) Nancy Newbie, a senior citizen who is new to Internet shopping and\n\nnervous about identity theft\n\n(cid:2) Hudson Hacker, who looks for ways to cheat the checkout page (cid:2) Enrico Executive, who does all his shopping online and ships gifts to\n\nall his clients worldwide\n\n(cid:2) Betty Bargain, who’s looking for great deals (cid:2) Debbie Ditherer, who has a hard time deciding what items she really\n\nwants to order\n\nJanet’s Story\n\nUSABILITY TESTING\n\nWe might hang photos representing these different personas and their biog- raphies in our work area so that we always keep them in mind. We can test the same scenario as each persona in turn and see what different experiences they might encounter.\n\nAnother way to approach persona testing, which we learned from Brian Mar- ick and Elisabeth Hendrickson, is to pick a ﬁctional character or famous ce- lebrity and imagine how they would use our application. Would the Queen of England be able to navigate our checkout process? How might Homer Simpson search for the item he wants?\n\nReal World Projects: Personas\n\nThe OneNote team at Microsoft uses personas as part of their testing process. Mike Tholfsen [2008], the Test Manager for OneNote, says they use seven per- sonas that might use OneNote, speciﬁc customer types such as Attorneys, Students, Real Estate Agents, and Salespersons. The personas they create contain information such as:\n\nGeneral job description • “A Day in the Life” • Primary uses for OneNote • List of features the persona might use • Potential notebook structures • Other applications used • Conﬁguration and hardware environment\n\nYou can also just assume the roles of novice, intermediate, and expert users as you explore the application. Can users ﬁgure out what they are supposed to do without instructions? If you have a lot of ﬁrst-time users, you might need to make the interface very simple.\n\nWhen I ﬁrst started testing a new production accounting system, I found it very difﬁ- cult to understand the ﬂow, but the production accountants on the team loved it. After I worked with it for a while, I understood the complexity behind the applica- tion and knew why it didn’t have to be intuitive for a ﬁrst-time user. This was a good lesson for me, because I always assumed applications had to be user-friendly.\n\nIf your application is custom-built for speciﬁc types of users, it might need to be “smart” rather than intuitive. Training sessions might be sufﬁcient to get\n\n203\n\n—Janet\n\n204\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nover the initial lack of usability so that the interface can be designed for max- imum efﬁciency and utility.\n\nNavigation\n\nNavigation is another aspect of usability testing. It’s incredibly important to test links and make sure the tabbing order makes sense. If a user has a choice of applications or websites, and has a bad ﬁrst experience, they likely won’t use your application again. Some of this testing is automatable, but it’s im- portant to test the actual user experience.\n\nSee Chapter 8, “Business-Facing Tests that Support the Team,” for an example of Wiz- ard of Oz testing, which is one ap- proach to design- ing for usability.\n\nIf you have access to the end users, get them involved in testing the naviga- tion. Pair with a real user, or watch one actually use the application and take notes. When you’re designing a new user interface, consider using focus groups to evaluate different interfaces. You can start with mock-ups and ﬂows drawn on paper, get opinions, and try HTML mock-ups next, to get early feedback.\n\nCheck Out the Competition\n\nWhen evaluating your application for usability, think about other applica- tions that are similar. How do they accomplish tasks? Do you consider them user-friendly or intuitive? If you can get access to competing software, take some time to research how those applications work and compare them with your product. For example, you’re testing a user interface that takes a date range, and it has a pop-up calendar feature to select the date. Take a look at how a similar calendar function works on an airline reservation website.\n\nSee the bibliogra- phy for links to articles by Jeff Patton, Gerard Meszaros, and others on usability testing.\n\nUsability testing is a fairly specialized ﬁeld. If you’re producing an internal application to be used by a few users who will be trained in its use, you prob- ably don’t need to invest much in usability testing. If you’re writing the on- line directory assistance for a phone company, usability might be your main focus, so you need to learn as much as you can about it, or bring in a usabil- ity expert.\n\nChapter 9, “Tool- kit for Business- Facing Tests that Support the Team,” provides more detail about tools that facili- tate these tests.\n\nBEHIND THE GUI In a presentation titled “Man and Machine” [2007], Jonathan Kohl talked about alternatives for testing interfaces. Instead of always thinking about test- ing through the user interface, consider attacking the problem in other ways. Think about testing the whole system from every angle that you can ap- proach. Consider using tools like simulators or emulators.\n\nLisa’s Story\n\nBEHIND THE GUI\n\nAPI Testing\n\nIn Chapter 8 and Chapter 9, we talked about testing behind the GUI to drive development. In this section, we show that you can extend your tests for the API in order to try different permutations and combinations.\n\nAn API (application programming interface) is a collection of functions that can be executed by other software applications or components. The end user is usually never aware that an API exists; she simply interacts with the inter- face on top.\n\nEach API call has a speciﬁc function with a number of parameters that accept different inputs. Each variation will return a different result. The easy tests are simple inputs. The more complicated testing patterns occur when the pa- rameters work together to give many possible variations. Sometimes param- eters are optional, so it’s important that you understand the possibilities. Boundary conditions should be considered as well, for both the inputs and expected results. For example, use both valid and invalid strings for parame- ters, vary the content, and vary the length of the strings’ input.\n\nAnother way to test is to vary the order of the API calls. Changing the se- quence might produce unexpected results and reveal bugs that would never be found through UI testing. You can control the tests much more easily than when using the UI.\n\nMy team was working on a set of stories to enable retirement plan sponsors to upload payroll contribution ﬁles. We wrote FitNesse test cases to illustrate the ﬁle parsing rules, and the programmer wrote unit tests for those as well. When the coding for the parser was complete, we wanted to throw a lot more combi- nations of data at the parser, including some really bizarre ones, and see what happened. We could use the same ﬁxture as we used for our tests to drive de- velopment, enter all of the crazy combinations we could think of, and see the results. We tested about 100 variations of both valid and invalid data. Figure 10-3 shows an example of just a few of the tests we tried. We found several errors in the code this way.\n\nWe didn’t keep all of these tests in the regression suite because they were just a means of quickly trying every combination we could think of. We could have done these tests in a semi-automated, ad hoc manner too, not bothering to type the expected results into the result checking table, and just eyeballing the outputs to make sure they looked correct.\n\n205\n\n—Lisa\n\n206\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nFigure 10-3 Sample of parsing rules test\n\nJanet’s Story\n\nI recently worked with a web application that interfaces to a legacy system through a well-deﬁned API. Due to the design of the legacy system and the fact that the data is hard to replicate, the team hasn’t yet found a way to automate this testing. However, we could look in the log ﬁles to verify the correct inputs were passed and the expected result was returned. Valuable exploratory testing of APIs is possible with or without beneﬁt of automation.\n\nAPI calls can be developed early in an application life cycle, which means testing can occur early as well. Testing through an API can give conﬁdence in the system before a UI is ever developed. Because this type of testing can be automated, you will need to work with your programmers to understand all of the parameters and the purpose of each function. If your programmers or automation team develop a test harness that is easy to use, you should be able to methodically create a suite of test cases that exercises the functionality.\n\n—Janet\n\nWeb services generally require plenty of security, stress, and reliabil- ity testing. See Chapter 12, “Cri- tiquing the Product using Technology- Facing Tests,” for more on these types of tests.\n\nChapter 12, “Sum- mary of Testing Quadrants,” has an example of testing web services.\n\nTESTING DOCUMENTS AND DOCUMENTATION\n\nWeb Services\n\nWeb services are a services-based architecture that provides an external inter- face so that others can access the system. There might be multiple stakeholders, and you may not even know who will be using your product. Your testing will need to conﬁrm the quality of service that the external customers expect.\n\nConsider levels of service that have been promised to clients when you are creating your test plans. Make time for exploratory testing to simulate the different ways users might access the web services.\n\nThe use of web services standards also offers other implications for current testing tools. As with API calls, web services-based integration highlights the importance of validating interface points. However, we also need to consider message formats and processing, queuing times, and message response times.\n\nUsing testing tools that utilize GUI-driven automation is simply inadequate for a web services project. A domain-speciﬁc language that encapsulates im- plementation details “behind the scenes” works well for testing web services.\n\nTESTING DOCUMENTS AND DOCUMENTATION One of the components of the system that is often overlooked during testing is documentation. As agile developers, we may value working software over doc- umentation, but we still value documentation! User manuals and online help need validation just as much as software. Your team may employ specialists such as technical writers who create and verify documentation. As with all other components of the product, your whole team is responsible for the qual- ity of the documentation, and that includes both hard copy and electronic.\n\nUser Documentation\n\nYour team might do Quadrant 2 tests to support the team as they produce documentation; in fact we encourage it. Lisa’s team writes code that pro- duces documents whose contents are speciﬁed by government regulations, and programmers can write much of the code test-ﬁrst. However, it’s difﬁcult for automated tests to judge whether a document is formatted correctly or uses a readable font. They also can’t evaluate whether the contents of docu- ments such as user manuals are accurate or useful. Because documentation has many subjective components, validating it is more of a critiquing activity.\n\n207\n\n208\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nJanet’s Story\n\nTechnical writers and testers can work very closely together. Stephanie, a techni- cal writer I worked with on one project, talked with the programmers to under- stand how the application worked. She would also work through the application to make sure she wrote it down correctly. This seemed to be a duplication of the testing effort, so Stephanie and I sat down and ﬁgured out a better approach.\n\nWe decided to work together on the stories as they were developed. For some stories Stephanie was lead “tester,” and sometimes I took that role. If I was lead, I’d create my test conditions and examples and Stephanie would use those as her basis for the documentation. When Stephanie was lead, she would write her doc- umentation, and then I would use that to determine the test cases.\n\nDoing it this way enabled the documentation to be tested and the tests to be challenged before they were ever executed. Working hand in hand like this proved to be a very successful experiment. The resulting documentation matched the software’s behavior and was much more useful to the end users.\n\nDon’t forget to check the help text too. Are the links to help text easily identi- ﬁable? Are they consistent throughout the user interface? Is the help text pre- sented clearly? If it opens in a pop-up, and users block pop-ups in their browsers, what’s the impact? Does the help cover all of the topics needed? On Lisa’s projects, help text tends to be a low priority, so it often doesn’t get done at all. That’s a business decision, but if you feel an area of the application needs extra help text or documentation, raise the issue to your team and your customers.\n\nReports\n\nAnother system component that’s often overlooked from a testing perspec- tive is reports. Reports are critical to many users for decision-making pur- poses but are often left until the very end, and either don’t get done or are poorly executed. Reports might be tailored to meet speciﬁc customer needs, but there are many third-party tools available for generating reports. Reports may be part of the application itself or be generated through a separate re- porting system for end users.\n\nWe discuss testing reports along with the other Quadrant 3 test activities in order to critique the product, but we recommend that you also write Quad- rant 2 report tests that will guide the coding and help the team understand the customer’s needs as it produces reports. They can certainly be written test-ﬁrst. Like documents, though, you need to look at a report to know if it’s easy enough to read and presents information in an understandable way.\n\n—Janet",
      "page_number": 243
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 251-258)",
      "start_page": 251,
      "end_page": 258,
      "detection_method": "topic_boundary",
      "content": "Lisa’s Story\n\nSee Chapter 8, “Business-Facing Tests that Support the Team,” for in- formation about using thin slices.\n\nTESTING DOCUMENTS AND DOCUMENTATION\n\nOne of the biggest challenges when testing reports is not the formatting but getting the right data. When you try to create test data for reports, it can be dif- ﬁcult to get a good cross section of realistic data. It also is usually the edge cases that make the reports fail, so incorporating that extra data is not feasible. In most cases, it’s best to use production data (or data copied from the produc- tion system into a test environment) to test the different reporting variations.\n\nOur application includes a number of reports, many of which help companies meet governmental compliance requirements. While we have automated smoke tests for each report, any change to a report, or even an upgrade in the tool we use to generate reports, requires extensive manual and visual testing. We have to watch like hawks: Has a number been truncated by one character? Did a piece of text run over to the next page? Is the right data included? Wrong or missing data can mean trouble with the regulatory agency.\n\nAnother challenge is verifying the data contained in the report. If I were to use the same query that the report uses, it doesn’t prove anything. I sometimes struggle to come up with my own SQL queries to compare the actual data with what shows up on a report. We budget extra time to test reports, even the simple-looking ones.\n\nBecause reports are so subjective, we ﬁnd that different stakeholders have differ- ent preferences for how the data is presented. The plan administrator who has to explain a report to a user on the phone has a different idea of what’s easy to understand than the company lawyer who decides what data needs to be on the report. Our product owner helps us get consensus from all areas of the business.\n\nThe contents and formatting of a report are important, of course, but for online reports, the speed at which they come up is critical too. Our plan administrators wanted complete freedom to specify any date range for some transaction history reports. Our DBA, who coded the reports, warned that for a large company’s retirement plan, data for more than a few months worth of transactions could take several minutes to render. Over time, companies grew, they had more and more transactions, and eventually the user interface started timing out before it could deliver the report. When testing, try out worst-case scenarios, which could eventually become the most common scenario.\n\nIf you’re tackling a project that involves lots of reports, don’t give in to the temptation to leave them to the end. Include some reports in each iteration if you can. One report could be a single story or maybe even broken up into a couple of stories. Use mock-ups to help the customers decide on report con- tents and formatting. Find the “thin slice” or “critical path” in the report, code that ﬁrst, and show it to your customer before you add the next slice. Incre- mental development works as well with reports as it does with other software.\n\n209\n\n—Lisa\n\n210\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nSometimes your customers themselves aren’t sure how a report should look or how to approach it incrementally. And sometimes nobody on the team anticipates how hard the testing effort will prove to be.\n\nLisa’s Story\n\nLike other ﬁnancial accounts, retirement plans need to provide periodic state- ments to account holders that detail all of the money going into and out of the ac- count. These statements show the change in value between the beginning and ending balances and other pertinent information, such as the names of account beneﬁciaries. Our company wanted to improve the account statements, both as a marketing tool and to reduce the number of calls from account holders who didn’t understand their statements.\n\nWe didn’t have access to our direct competitors’ account statements, so the product owner asked for volunteers to bring in account statements from banks and other ﬁnancial institutions in order to get ideas. Months of discussions and experimentation with mock-ups produced a new statement format, which included data that wasn’t on the report previously, such as performance results for each mutual fund.\n\nStories for developing the new account statement were distributed throughout two quarters worth of iterations. During the ﬁrst quarter, stories to collect new data were done. Testing proved much harder than we thought. We used FitNesse tests to verify capturing the different data elements, which lulled us into a false sense of security. It was hard to cover all of the variations, and we missed some with the automated tests. We also didn’t anticipate that the changes to collect new data could have an adverse effect on the data that already displayed on the existing statements.\n\nAs a result, we didn’t do adequate manual testing of the account statements. Sub- tle errors slipped past us. When the job to produce quarterly statements ran, calls started coming in from customers. We had a mad scramble to diagnose and ﬁx the errors in both code and data. The whole project was delayed by a quarter while we ﬁgured out better ways to test and added internal checks and better logging to the code.\n\nShort iterations mean that it can be hard to make time for adequate explor- atory testing and other Quadrant 3 activities. Let’s look at tools that might help speed up this testing and make time for vital manual and visual tests.\n\nTOOLS TO ASSIST WITH EXPLORATORY TESTING Exploratory testing is manual testing. Some of the best testing happens be- cause a person is paying attention to details that often get missed if we are fol-\n\n—Lisa\n\nSee the bibliogra- phy for references to Jonathan Kohl’s writings on using human and auto- mation power to- gether for optimal testing.\n\nTOOLS TO ASSIST WITH EXPLORATORY TESTING\n\nlowing a script. Intuition is something that we cannot make a machine learn. However, there are many tools that can assist us in our quest for excellence.\n\nTools shouldn’t replace human interaction; they should enhance the experi- ence. Tools can provide testers with more power to ﬁnd the hard-to-repro- duce bugs that often get ﬁled away because no one can get a handle on them. Exploratory testing is unconventional, so why shouldn’t the tools be as well? Think about low-effort, high-value ways that tools can be incorporated into your testing.\n\nComputers are good at doing repetitive tasks and performing calculations. These are two areas where they are much better than humans, so let’s use them for those tasks. Because testing needs to keep pace with coding, any time advantage we can gain is a bonus.\n\nIn the next few sections, we’ll look at some areas where automation can le- verage exploratory testing. The ones we cover are test setup, test data genera- tion, monitoring, simulators, and emulators.\n\nTest Setup\n\nLet’s think about what we do when we test. We’ve just found a bug, but not one that is easily reproducible. We’re pretty sure it happens as a result of in- teractions between components. We go back to the beginning and try one scenario after another. Soon we’ve spent the whole day just trying to repro- duce this one bug.\n\nAsk yourself how you can make this easier. We’ve found that one of the most time-consuming tasks is the test setup and getting to the right starting point for your actual test. If you use session-based testing, then you already know how much time you spend setting up the test, because you have been track- ing that particular time waster. This is an excellent opportunity for some automation.\n\nThe tools used for business-facing tests that support the team described in Chapter 9 are also valuable for manual exploratory testing. Automated functional test scripts can be run to set up data and scenarios to launch ex- ploratory testing sessions. Tests conﬁgured to accept runtime parameters are particularly powerful for setting up a starting point for evaluating the product.\n\n211\n\n212\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nLisa’s Story\n\nOur Watir test scripts all accept a number of runtime parameters. When I need a retirement plan with a speciﬁc set of options, and speciﬁc types of participants, I can kick off a Watir script or two with some variables set on the command line. When the scripts stop, I have a browser session with all of the data I need for test- ing already set up. This is so fast that I can test permutations I’d never get to using all-manual keystrokes.\n\nThe test scripts you use for functional regression testing and for guiding de- velopment aren’t the only tools that help take the tedium out of manual ex- ploratory testing. There are other tools to help set up test data as well as to help you evaluate the outputs of your testing sessions.\n\nWhatever tool you are using, think about how it can be adapted to run the scenario over and over with different inputs plugged in. Janet has also suc- cessfully used Ruby with Watir to set up tests to run multiple times to help identify bugs. Tools that drive the browser or UI in much the same way that an end user would makes your testing more reliable because you can play it back on your monitor and watch for anything that might not look as it should during the setup. When you get to the place where the test actually starts, you can then use your excellent testing abilities to track down the source of the bug.\n\nTest Data Generation\n\nPerlClip is an example of a tool that you can use to test a text ﬁeld with differ- ent kinds of inputs. James Bach provides it free of charge on his website, www.satisﬁce.com, and it can be very helpful in validating ﬁelds. For exam- ple, if you have a ﬁeld that will accept a maximum input of 200 characters, testing this ﬁeld and its boundaries manually would be very tedious. Use Per- lClip to create a string, put it in your automation library, and have your auto- mation tool call the string to test the value.\n\nMonitoring Tools\n\nTools like the Unix/Linux command tail -f, or James Bach’s LogWatch, can help monitor log ﬁles for error conditions. IDEs also provide log analysis tools. Many error messages are never displayed on the screen, so if you’re testing via the GUI, you never see them. Get familiar with tools like these, be- cause they can make your testing more effective and efﬁcient. If you are not\n\n—Lisa\n\nSee the ”System Test” example in Chapter 12, “Sum- mary of Testing Quadrants,” to see how a simulator was critical to the testing the whole system.\n\nTOOLS TO ASSIST WITH EXPLORATORY TESTING\n\nsure where your system logs warnings and errors, ask your developers. They probably have lots of ideas about how you can monitor the system.\n\nSimulators\n\nSimulators are tools used to create data that represent key characteristics and behavior of real data for the system under test. If you do not have access to real data for your system, simulated data will sometimes work almost as well. The other advantage of using a simulator is for pumping data into a system over time. It can be used to help generate error conditions that are difﬁcult to create under normal circumstances and can reduce time in boundary testing.\n\nSetting up data and test scenarios is half of the picture. You also need to have a way to watch the outcomes of your testing. Let’s consider some tools for that purpose.\n\nEmulators\n\nAn emulator duplicates the functionality of a system so that it behaves like the system under test. There are many reasons to use an emulator. When you need to test code that interfaces with other systems or devices, emulators are invaluable.\n\nTwo Examples of Emulators\n\nWestJet, a Canadian airline company, provides the capability for guests to use their mobile devices to check in at airports that support the feature. When testing this application, it is better for both the programmers and the testers to test various devices as early as possible. To make this feasible, they use downloadable emulators to test the Web Check-in application quickly and often during an iteration. Real devices, which are expensive to use, can then be used sparingly to verify already tested functionality.\n\nThe team also created another type of emulator to help test against the leg- acy system being interfaced with. The programmers on the legacy system have different priorities and delivery schedules, and a backlog of requests. To prevent this from holding up new development, the programmers on the web application have created a type of emulator for the API into the legacy system that returns predetermined values for speciﬁc API calls. They develop against this emulator, and when the real changes are available, they test and make any modiﬁcations then. This change in process has enabled them to move ahead much more quickly than was previously possible. It has proved to be a simple but very powerful tool.\n\n213\n\n214\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nEmulators are one tool that helps to keep testing and coding moving together hand-in-hand. Using them is one way for testing to keep up with develop- ment in short iterations. As you plan your releases and iterations, think about the types of tools that might help with creating production-like test scenarios. See if you can use the tools you’re already using for automating tests to drive development as aids to exploratory testing.\n\nDriving development with tests is critical to any project’s success. However, we humans won’t always get all of the requirements for desired system be- havior entirely correct. Our business experts themselves can miss important aspects of functionality or interaction with other parts of the system when they provide examples of how a feature should work. We have to use tech- niques to help both the customer and developer teams learn more about the system so they can keep improving the product.\n\nSUMMARY A large part of the testing effort is spent critiquing the product from a busi- ness perspective. This chapter gave you some ideas about the types of tests you can do to make your testing efforts more effective.\n\n(cid:2) Demonstrate software to stakeholders in order to get early feedback\n\nthat will help direct building the right stuff.\n\n(cid:2) Use scenarios and workﬂows to test the whole system from end to\n\nend.\n\n(cid:2) Use exploratory testing to supplement automation and to take advan-\n\ntage of human intellect and perceptions.\n\n(cid:2) Without usability in mind when testing and coding, applications can become shelfware. Always be aware of how the system is being used.\n\n(cid:2) Testing behind the GUI is the most effective way of getting at the application functionality. Do some research to see how you can approach your application.\n\n(cid:2) Incorporate all kinds of tests to make a good regression suite. (cid:2) Don’t forget about testing documentation and reports. (cid:2) Automation tools can perform tedious and repetitive tasks, such as data and test scenario setup, and free up more time for important manual exploratory testing.\n\n(cid:2) Tools you’re already using to automate functional tests might also be\n\nuseful to leverage exploratory tests.\n\nSUMMARY\n\n(cid:2) Monitoring, resource usage, and log analysis tools built into operat- ing systems and IDEs help testers appraise the application’s behavior. (cid:2) Simulators and emulators enable exploratory testing even when you\n\ncan’t duplicate the exact production environment.\n\n(cid:2) Even when tests are used to drive development, requirements for de- sired behavior or interaction with other systems can be missed or misunderstood. Quadrant 3 activities help teams keep adding value to the product.\n\n215\n\nThis page intentionally left blank",
      "page_number": 251
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 259-266)",
      "start_page": 259,
      "end_page": 266,
      "detection_method": "topic_boundary",
      "content": "Chapter 11\n\nCRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nScalability\n\nPerformance and Load\n\nPerformance and Load Tools\n\nLoad Type Tests\n\nBaseline\n\nWhat Is It?\n\nMemory Management\n\nSecurity\n\nMaintainability\n\nCritique Product Using Technology-Facing Tests\n\nWho Does It?\n\nInteroperability\n\n“ility” Testing\n\nCompatibility\n\nReliability\n\nWhen Do You Do It?\n\nInstallability\n\nThis chapter is focused on the bottom right corner of our testing quadrant. We’ve looked at driving development with both business-facing and technology-facing tests. After the code is written, we are no longer driving the development but are looking at ways to critique the product. In the previous chapter, we examined ways to critique from a business point of view. Now we look at ways to critique from a technology-facing point of view. These tests are an important means of evaluating whether our product delivers the right business value.\n\nINTRODUCTION TO QUADRANT 4 Individual stories are pieces of the puzzle, but there’s more to an application than that. The technology-facing tests that critique the product are more\n\n217\n\n218\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nconcerned with the nonfunctional requirements than the functional ones. We worry about deﬁciencies in the product from a technical point of view. Rather than using the business domain language, we describe requirements using a programming domain vocabulary. This is the province of Quadrant 4 (see Figure 11-1).\n\nNonfunctional requirements include conﬁguration issues, security, perfor- mance, memory management, the “ilities” (e.g., reliability, interoperability, and scalability), recovery, and even data conversion. Not all projects are con- cerned about all of these issues, but it is a good idea to have a checklist to make sure the team thinks about them and asks the customer how important each one is.\n\nOur customer should think about all of the quality attributes and factors that are important and make informed trade-offs. However, many customers focus\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ2 Q3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 11-1 Quadrant 4 tests\n\nLisa’s Story\n\nINTRODUCTION TO QUADRANT 4\n\non the business side of the application and don’t understand the criticality of many nonfunctional requirements in their role of helping to deﬁne the level of quality needed for the product. They might assume that the development team will just take care of issues such as performance, reliability, and security.\n\nWe believe that the development team has a responsibility to explain the consequences of not addressing these nonfunctional or cross-functional re- quirements. We’re really all part of one product team that wants to deliver good value, and these technology-oriented factors might expose make-or- break issues.\n\nMany of these nonfunctional and cross-functional issues are deemed low- risk for many applications and so are not added to the test plan. However, when you are planning your project, you should think about the risks in each of these areas, address them in your test plan, and include the tools and re- sources needed for testing them in your project plan.\n\nIn the past, I’ve been asked by specialists in areas such as performance and secu- rity testing why they didn’t hear much about “ility” testing at agile conferences or in publications about agile development. Like Janet, I’ve always seen these areas of testing as critical, so this wasn’t my perception. But as I thought about it, I had to agree that this wasn’t a much-discussed topic at the time (although that’s changed recently).\n\nWhy would agile discussions not include such important considerations as load testing? My theory is that it’s because agile development is driven by customers, from user stories. Customers simply assume that software will be designed to properly accommodate the potential load, at a reasonable rate of performance. It doesn’t always occur to them to verbalize those concerns. If not asked to address them, programmers may or may not think to prioritize them. I believe that one area where testers have contributed greatly to agile teams is in bringing up ques- tions such as, “How many concurrent users should the application support?” and “What’s the average response time required?”\n\nBecause the types of testing in this quadrant are so diverse, we’ll give exam- ples of tools that might be helpful as we go along instead of a separate toolkit section. Tools, whether homegrown or acquired, are essential to succeed with Quadrant 4 testing efforts. Still, the people doing the work count, so let’s consider who on an agile team can perform these tests.\n\n219\n\n—Lisa\n\n220\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nWHO DOES IT? All of the agile literature talks about teams being generalists; anyone should be able to pick up a task and do it. We know that isn’t always practical, but the idea is to be able to share the knowledge so that people don’t become silos of information.\n\nHowever, there are many tasks that need specialized knowledge. A good exam- ple is security testing. We’re not talking about security within an application, such as who has access rights to administer it. Because that type of security is really part of the functional requirements and will be covered by regular sto- ries, verifying that it works falls within the ﬁrst three quadrants. We’re talking about probing for external security ﬂaws and knowing the types of vulnerabili- ties in systems that hackers exploit. That is a specialized skill set.\n\nPerformance testing can be done by testers and programmers collaborating and building simple tools for their speciﬁc needs. Some organizations pur- chase load-testing tools that require team members who specialize in that tool to build the scripts and analyze and interpret the results. It can be difﬁ- cult for a software development organization, especially a small one, to have enough resources to duplicate an accurate production-level load for a test, so external providers of performance testing may be needed.\n\nChapter 15, “Tester Activities in Release or Theme Planning,” ex- plains how to plan to work with ex- ternal teams.\n\nLarger organizations may have groups such as database experts that your team can use to help with data conversion, security groups that will help you identify risks to your application, or a production support team that can help you test recovery or failover. Build a close relationship with these specialists. You’ll need to work together as a virtual team to gather the information you need about your product.\n\nThe more diverse the skill sets are in your team, the less likely you are to need outside consultants to help you. Identify the resources you need for each project. Many teams ﬁnd that a good technical tester or toolsmith can take on many of these tasks. If someone already on the team can learn whatever special- ized knowledge is required, great; otherwise, bring in the expertise you need.\n\nSkills within the Team\n\nJason Holzer, Product Owner for Property Testing (performance, security, sta- bility, and reliability) at Ultimate Software, tells us that a good programmer can write a multithreaded engine to call a function concurrently and test per- formance. Jason feels that agile teams do have the skills to do their own per- formance testing; they just may not realize it.\n\nWHO DOES IT?\n\nPerformance testing does require a controlled, dedicated environment. Some specialized tools are needed, such as a proﬁler to measure code per- formance. But, in Jason’s view, performance, stability, scalability, and reliability (PSR) tests can, and should, be done at the unit level. There’s a mind-set that holds that these tests are too complex and require specialists when in fact the teams do possess the necessary skills.\n\nJason ﬁnds that awareness of the “PSR” aspects of code needs to be part of the team’s culture.\n\nIf stakeholders place a high priority on performance, stability, scalability, and the like, Jason recommends that the team talk about ways to verify these aspects of the application. When teams understand the priority of qualities such as performance and reliability, they ﬁgure out how to improve their code to ensure them. They don’t need to depend on an outside, specialized team. Jason explains his viewpoint.\n\nThe potential resistance I see today to this plan is that someone believes that programmers don’t know how to PSR test and that there will need to be a great deal of training. In my opinion, a more accurate statement is that programmers are not aware that PSR testing is a high priority and a key to quality. I don't think it has anything to do with knowing how to PSR test. PSR testing is a combination of math, science, analysis, program- ming, and problem solving. I am willing to bet that if you conducted a competition at any software development organization where you asked every team to implement a tree search algorithm, and the team with the fastest algorithm would win, that every team will do PSR testing and pro- vide PSR metrics without teaching them anything new.\n\nPSR testing is really just telling me “How fast?” (performance), “How long?” (stability), “How often?” (reliability), and “How much?” (scalabil- ity). So, as long as the awareness is there and the organization is seri- ously asking those questions with everything they develop, then PSR testing is successfully integrated into a team.\n\nTake a second look at the skills that your team already possesses, and brain- storm about the types of “ility” testing that can be done with the resources you already have. If you need outside teams, plan for that in your release and iteration planning.\n\nRegardless of whether or not your team brings in additional resources for these types of tests, your team is still responsible for making sure the mini- mum testing is done. The information these tests provide may result in new stories and tasks in areas such as changing the architecture for better scalabil- ity or implementing a system-wide security solution. Be sure to complete the feedback loop from tests that critique the product to tests that drive changes that will improve the nonfunctional aspects of the product.\n\n221\n\n222\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nJust because this is the fourth out of four agile testing quadrants doesn’t mean these tests come last. Your team needs to think about when to do per- formance, security, and “ility” tests so that you ensure your product delivers the right business value.\n\nWHEN DO YOU DO IT? As with functional testing, the sooner technology-facing tests that support the team are completed, the cheaper it is to ﬁx any issues that are found. However, many of the cross-functional tests are expensive and hard to do in small chunks.\n\nTechnical stories can be written to address speciﬁc requirements, such as: “As user Abby, I need to retrieve report X in less than 20 seconds so that I can make a decision quickly.” This story is about performance and requires spe- cialized tests to be written, and it can be done along with the story to code the report, or in a later iteration.\n\nConsider a separate row on your story board for tasks needed by the product as a whole. Lisa’s team uses this area to put cards such as “Evaluate load test tools” or “Establish a performance test baseline.” Janet has successfully used different colored cards to show that the story is meant for one of the expert roles borrowed from other areas of the organization.\n\nSome performance tests might need to wait until much of the application is built if you are trying to baseline full end-to-end workﬂows. If performance and reliability are a top priority, you need to ﬁnd a way to test those early in the project. Prioritize stories so that a steel thread or thin slice is complete early. You should be able to create a performance test that can be run and continue to run as you add more and more functionality to the workﬂow. This may enable you to catch performance issues early and redesign the sys- tem architecture for improvements. For many applications, correct function- ality is irrelevant without the necessary performance.\n\nThe time to think about your nonfunctional tests is during release or theme planning. Plan to start early, tackling small increments as needed. For each iteration, see what tasks your team needs in order to determine whether the code design is reliable, scalable, usable, and secure. In the next section, we’ll look at some different types of Quadrant 4 tests.\n\n“ILITY” TESTING\n\nPerformance Testing from the Start\n\nKen De Souza, a software developer/tester at NCR [2008], responded to a question on the agile-testing mailing list about when to do stress and perfor- mance testing in an agile project with an explanation of how he approaches performance testing.\n\nI'd suggest designing your performance tests from the start. We build data from the ﬁrst iteration, and we run a simple performance test to make sure it all holds together. This is more to see that the functionality of the performance scripts holds together.\n\nI used JMeter because I can hook FTP, SOAP, HTTP, RegEx, and so on, all from a few threads, with just one instance running. I can test out my calls right from the start (or at least have the infrastructure in place to do it).\n\nMy eventual goal is that when the product is close to releasing, I don't have to nurse the performance test; I just have to crank up the threads and let go. All my metrics and tasks have already been tested out for months, so I'm fairly certain that anyone can run my performance test.\n\nPerformance testing can be approached using agile principles to build the tools and test components incrementally. As with software features, focus on getting the performance information you need, one small chunk at a time.\n\n“ILITY” TESTING If we could just focus on the desired behavior and functionality of the appli- cation, life would be so simple. Unfortunately, we have to be concerned with qualities such as security, maintainability, interoperability, compatibility, re- liability, and installability. Let’s take a look at some of these “ilities.”\n\nSecurity\n\nOK, it doesn’t end in -ility, but we include it in the “ility” bucket because we use technology-facing tests to appraise the security aspects of the product. Security is a top priority for every organization these days. Every organiza- tion needs to ensure the conﬁdentiality and integrity of their software. They want to verify concepts such as no repudiation, a guarantee that the message has been sent by the party that claims to have sent it and received by the party that claims to have received it. The application needs to perform the correct authentication, conﬁrming each user’s identity, and authorization, in order\n\n223\n\n224\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nto allow the user access only to the services they’re authorized to use. Testing so many different aspects of security isn’t easy.\n\nIn the rush to deliver functionality, both business experts and development teams in newly started organizations may not be thinking of security ﬁrst. They just want to get some software working so they can do business. Autho- rization is often the only aspect of security testing that they consider as part of business functionality.\n\nLisa’s Story\n\nMy current team is a case in point. The business was interested in automating func- tionality to manage 401(k) plans. They did take pains to secure the software and data, but it wasn’t a testing priority. When I “got religion” after hearing some good presentations about security testing at conferences, I bought a book on security testing and started hacking around on the site. I found some serious issues, which we ﬁxed, but we realized we needed a comprehensive approach to ensuring security. We wrote stories to implement this. We also started including a “security” task card with every story so that we’d be mindful of security needs while devel- oping and testing.\n\nBudgeting this type of work has to be a business priority. There’s a range of alternatives available, depending on your company’s priorities and resources. Understand your needs and the risks before you invest a lot of time and energy.\n\nJanet’s Story\n\nOne team that I worked with has a separate corporate security team. Whenever functionality is added to the application that might expose a security ﬂaw, the cor- porate team runs the application through a security test application and produces a report for the team. It performs static testing using a canned black-box probe on the code and has exposed a few weak areas that the developers were able to address. It does not give an overall picture of the security level for the application, but that was not deemed a major concern.\n\nTesters who are skilled in security testing can perform security risk-based testing, which is driven by analyzing the architectural risk, attack patterns, or abuse and misuse cases. When specialized skills are required, bring in what you need, but the team is still responsible for making sure the testing gets done.\n\n—Lisa\n\n—Janet",
      "page_number": 259
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 267-274)",
      "start_page": 267,
      "end_page": 274,
      "detection_method": "topic_boundary",
      "content": "See http://en .wikipedia.org/wiki/ Buffer_overﬂow and http://en .wikipedia.org/wiki/ Format_string_ vulnerabilities for more information.\n\nSee http://en .wikipedia.org/wiki/ List_of_tools_for_ static_code_analysis for a list of tools that can be used for static code analysis.\n\nMore resources on this subject can be found at: www .fuzzing.org/ category/fuzzing- book/ and www .fuzzing.org/fuzzing- software\n\n“ILITY” TESTING\n\nThere are a variety of automated tools to help with security veriﬁcation. Static analysis tools, which can examine the code without executing the ap- plication, can detect potential security ﬂaws in the code that might not oth- erwise show up for years. Dynamic analysis tools, which run in real time, can test for vulnerabilities such as SQL injection and cross-site scripting. Manual exploratory testing by a knowledgeable security tester is indispensable to de- tect issues that automated tests can miss.\n\nSecurity Testing Perspectives\n\nSecurity testing is a vast topic on its own. Grig Gheorghiu shares some high- lights about resources that can help agile teams with security testing.\n\nJust like functional testing, security testing can be done from two per- spectives: from the inside out (white-box testing) and from the outside in (black-box testing). Inside-out security testing assumes that the source code for the application under test is available to the testers. The code can be analyzed statically with a variety of tools that try to discover com- mon coding errors that can make the application vulnerable to attacks such as buffer overﬂows or format string attacks.\n\nThe fact that the testers have access to the source code of the applica- tion also means that they can map what some books call \"the attack sur- face\" of the application, which is the list of all of the inputs and resources used by the program under test. Armed with a knowledge of the attack surface, testers can then apply a variety of techniques that attempt to break the security of the application. A very effective class of such tech- niques is called fuzzing and is based on fault injection. Using this tech- nique, the testers try to make the application fail by feeding it various types of inputs (hence the term fault injection). These inputs can be care- fully crafted strings used in SQL injection attacks, random byte changes in given input ﬁles, or random strings fed as command line arguments.\n\nThe outside-in approach is the one mostly used by attackers who try to penetrate into the servers or the network hosting your application. As a security tester, you need to have the same mind-set that attackers do, which means that you have to use your creativity in discovering and ex- ploiting vulnerabilities in your own application. You also need to stay up- to-date with the latest security news and updates related to the platform/ operating system your application runs on, which is not an easy task.\n\nSo what are agile testers to do when faced with the apparently insur- mountable task of testing the security of their application? Here are some practical, pragmatic steps that anybody can follow:\n\n1. Adopt a continuous integration (CI) process that periodically runs a suite of automated tests against your application.\n\n225\n\n226\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\n2. Learn how to use one or more open source static code analysis tools. Add a step to your CI process that consists of running these tools against your application code. Mark the step as failed if the tools ﬁnd any critical vulnerabilities.\n\n3. Install an automated security vulnerability scanner such as Nessus (http://www.nessus.org/nessus/). Nessus can be run in a command- line, non-GUI mode, which makes it suitable for inclusion in a CI tool. Add a step to your CI process that consists of running Nessus against your application. Capture the Nessus output in a ﬁle and parse that ﬁle for any high-importance security holes found by the scanner. Mark the step as failed when any such holes are found.\n\n4. Learn how to use one or more open source fuzzing tools. Add a step to your CI process that consists of running these tools against your application code. Mark the step as failed if the tools ﬁnd any critical vulnerabilities.\n\nAs with any automated testing effort, running these tools is no guarantee that your code and your application will be free of security defects. However, running these tools will go a long way toward improving the quality of your application in terms of security. As always, the 80/20 rule applies. These tools will probably ﬁnd the 80% most common security bugs out there while requiring 20% of your security budget.\n\nTo ﬁnd the remaining 20% of the security defects, you're well advised to spend the other 80% of your security budget on high-quality secu- rity experts. They will be able to test your application security thor- oughly by the use of techniques such as SQL injection, code injection, remote code inclusion, and cross-site scripting. While there are some tools that try to automate some of these techniques, they are no match for a trained professional who takes the time to understand the inner workings of your application in order to craft the perfect attack against it.\n\nSecurity testing can be intimidating, so budget time to adopt a hacker mind-set and decide on the right approach to the task at hand. Use the resources Grig suggests to educate yourself. Take advantage of these tools and techniques in order to achieve security tests with a reasonable return on investment.\n\nJust this brief look at security testing shows why specialized training and tools are so important to do a good job of it. For most organizations, this testing is absolutely required. One security intrusion might be enough to take a company out of business. Even if the probability were low, the stakes are too high to put off these tests.\n\n“ILITY” TESTING\n\nCode that costs a lot to maintain might not kill an organization’s proﬁtability as quickly as a security breach, but it could lead to a long, slow death. In the next section we consider ways to verify maintainability.\n\nMaintainability\n\nMaintainability is not something that is easy to test. In traditional projects, it’s often done by the use of full code reviews or inspections. Agile teams of- ten use pair programming, which has built-in continual code review. There are other ways to make sure the code and tests stay maintainable.\n\nWe encourage development teams to develop standards and guidelines that they follow for application code, the test frameworks, and the tests them- selves. Teams that develop their own standards, rather than having them set by some other independent team, will be more likely to follow them because they make sense to them.\n\nThe kinds of standards we mean include naming conventions for method names or test names. All guidelines should be simple to follow and make maintainability easier. Examples are: “Success is always zero and failure must be a negative value,” “Each class or module should have only one single re- sponsibility,” or “All functions must be single entry, single exit.”\n\nStandards for developing the GUI also make the application more testable and maintainable, because testers know what to expect and don’t need to wonder whether a behavior is right or wrong. It also adds to testability if you are automating tests from the GUI. Simple standards such as, “Use names for all GUI objects rather than defaulting to the computer assigned identiﬁer” or “You cannot have two ﬁelds with the same name on a page” help the team achieve a level where the code is maintainable, as are the automated tests that provide coverage for it.\n\nMaintainable code supports shared code ownership. It is much easier for a programmer to move from one area to another if all code is written in the same style and easily understood by everyone on the team. Complexity adds risk and also makes code harder to understand. The XP value of simplicity should be applied to code. Simple coding standards can also include guide- lines such as, “Avoid duplication—Don’t copy-paste methods.” These same concepts apply to test frameworks and the tests themselves.\n\nMaintainability is an important factor for automated tests as well. Test tools have lagged behind programming tools in features that make them easy to\n\n227\n\n228\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nmaintain, such as IDE plug-ins to make writing and maintaining test scripts simpler and more efﬁcient. That’s changing fast, so look for tools that pro- vide easy refactoring and search-and-replace, and for other utilities that make it easy to modify the scripts.\n\nDatabase maintainability is also important. The database design needs to be ﬂexible and usable. Every iteration might bring tasks to add or remove tables, columns, constraints, or triggers, or to do some kind of data conversion. These tasks become a bottleneck if the database design is poor or the data- base is cluttered with invalid data.\n\nLisa’s Story\n\nA serious regression bug went undetected and caused production problems. We had a test that should have caught the bug. However, a constraint was missing from the schema used by the regression suite. Our test schemas had grown hap- hazardly over the years. Some had columns that no longer existed in the produc- tion schema. Some were missing various constraints, triggers, and indices. Our DBA had to manually make changes to each schema as needed for each story instead of running the same script in each schema to update it. We budgeted time over several sprints to recreate all of the test schemas so that they were identical and also matched production.\n\nPlan time to evaluate the database’s impact on team velocity, and refactor it just as you do production and test code. Maintainability of all aspects of the application, test, and execution environments is more a matter of assessment and refactoring than direct testing. If your velocity is going down, is it be- cause parts of the code are hard to work on, or is it that the database is difﬁ- cult to modify?\n\nInteroperability\n\nInteroperability refers to the capability of diverse systems and organizations to work together and share information. Interoperability testing looks at end-to-end functionality between two or more communicating systems. These tests are done in the context of the user—human or a software applica- tion—and look at functional behavior.\n\nIn agile development, interoperability testing can be done early in the devel- opment cycle. We have a working, deployable system at the end of each itera- tion so that we can deploy and set up testing with other systems.\n\n—Lisa\n\nIn Chapter 20, “Successful Deliv- ery,” we discuss more about the importance of this level of testing.\n\n“ILITY” TESTING\n\nQuadrant 1 includes code integration tests, which are tests between compo- nents, but there is a whole other level of integration tests in enterprise systems. You might ﬁnd yourself integrating systems through open or proprietary inter- faces. The API you develop for your system might enable your users to easily set up a framework for them to test easily. Easier testing for your customer makes for faster acceptance.\n\nIn one project Janet worked on, test systems were set up at the customer’s site so that they could start to integrate them with their own systems early. Inter- faces to existing systems were changed as needed and tested with each new deployment.\n\nIf the system your team works on has to work together with external systems, you may not be able to represent them all in your test environments except with stubs and drivers that simulate the behavior of the other systems or equipment. This is one situation where testing after development is complete might be unavoidable. You might have to schedule test time in a test environ- ment shared by several teams.\n\nConsider all of the systems with which yours needs to communicate, and make sure you plan ahead to have an appropriate environment for testing them together. You’ll also need to plan resources for testing that your applica- tion is compatible with the various operating systems, browsers, clients, serv- ers, and hardware with which it might be used. We’ll discuss compatibility testing next.\n\nCompatibility\n\nThe type of project you’re working on dictates how much compatibility test- ing is required. If you have a web application and your customers are world- wide, you will need to think about all types of browsers and operating systems. If you are delivering a custom enterprise application, you can prob- ably reduce the amount of compatibility testing, because you might be able to dictate which versions are supported.\n\nAs each new screen is developed as part of a user interface story, it is a good idea to check its operability in all supported browsers. A simple task can be added to the story to test on all browsers.\n\nOne organization that Janet worked at had to test compatibility with reading software for the visual impaired. Although the company had no formal test\n\n229\n\n230\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nlab, it had test machines available near the team area for easy access. The testers made periodic checks to make sure that new functionality was still compatible with the third-party tools. It was easy to ﬁx problems that were discovered early during development.\n\nHaving test machines available with different operating systems or browsers or third-party applications that need to work with the system under test makes it easier for the testers to ensure compatibility with each new story or at the end of an iteration. When you start a new theme or project, think about the resources you might need to verify compatibility. If you’re starting on a brand new product, you might have to build up a test lab for it. Make sure your team gets information on your end users’ hardware, operating systems, browsers, and versions of each. If the percentage of use of a new browser version has grown large enough, it might be time to start including that version in your compatibility testing.\n\nWhen you select or create functional test tools, make sure there’s an easy way to run the same script with different versions of browsers, operating systems, and hardware. For example, Lisa’s team could use the same suite of GUI regression tests on each of the servers running on Windows, Solaris, and Linux. Func- tional test scripts can also be used for reliability testing. Let’s look at that next.\n\nReliability\n\nReliability of software can be referred to as the ability of a system to perform and maintain its functions in routine circumstances as well as unexpected circumstances. The system also must perform and maintain its functions with consistency and repeatability. Reliability analysis answers the question, “How long will it run before it breaks?” Some statistics used to measure reli- ability are:\n\n(cid:2) Mean time to failure: The average or mean time between initial oper- ation and the ﬁrst occurrence of a failure or malfunction. In other words, how long can the system run before it fails the ﬁrst time?\n\n(cid:2) Mean time between failures: A statistical measure of reliability, this is calculated to indicate the anticipated average time between failures. The longer the better.\n\nIn traditional projects, we used to schedule weeks of reliability testing that tried to run simulations that matched a regular day’s work. Now, we should be able to deliver at the end of every iteration, so how can we schedule reli- ability tests?\n\n“ILITY” TESTING\n\nWe have automated unit and acceptance tests running on a regular basis. To do a reliability test, we simply need to use those same tests and run them over and over. Ideally, you would use statistics gathered that show daily usage, cre- ate a script that mirrors the usage, and run it on a stable build for however long your team thinks is adequate to prove stability. You can input random data into the tests to simulate production use and make sure the application doesn’t crash because of invalid inputs. Of course, you might want to mirror peak usage to make sure that it handles busy times as well.\n\nYou can create stories in each iteration to develop these scripts and add new functionality as it is added to the application. Your acceptance tests could be very speciﬁc such as, “Functionality X must perform 10,000 operations in a 24-hour period for a minimum of 3 days.”\n\nBeware: Running a thousand tests without any serious problems doesn’t mean you have reliable software. You have to run the right tests. To make a reliability test effective, think about your application and how it is used all day, every day, over a period of time. Specify tests that are aimed at demon- strating that your application will be able to meet your customers’ needs, even during peak times.\n\nAsk the customer team for their reliability criteria in the form of measurable goals. For example, they might consider the system reliable if ten or fewer er- rors occur for every 10,000 transactions, or the web application is available 99.999% of the time. Recovery from power outages and other disasters might be part of the reliability objectives, and will be stated in the form of Service Level Agreements. Know what they are. Some industries have their own soft- ware reliability standards and guidelines.\n\nDriving development with the right programmer and customer tests should enhance the application’s reliability, because this usually leads to better de- sign and fewer defects. Write additional stories and tasks as needed to deliver a system that meets the organization’s reliability standards.\n\nYour product might be reliable after it’s up and running, but it also needs to be installable by all users, in all supported environments. This is another area where following agile principles gives us an advantage.\n\nInstallability\n\nOne of the cornerstones of a successful agile team is continuous integration. This means that a build is ready for testing anytime during the day. Many\n\n231\n\n232\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nteams choose to deploy one or more of the successful builds into test envi- ronments on a daily basis.\n\nAutomating the deployment creates repeatability and makes deployment a non-event. This is exciting to us because we have experienced weeks of trying to integrate and install a new system. We know that if we build once and de- ploy the same build to multiple environments, we have developed consistency.\n\nJanet’s Story\n\nOn one project I worked on, the deployment was automatic and was tested on multiple environments in the development cycle. However, there were issues when deploying to the customer site. We added a step to the end game so that the support group would take the release and do a complete install test as if it were the customer’s site. We were able to walk through the deployment notes and eliminated many of the issues the customer would have otherwise seen.\n\nChapter 20, “Suc- cessful Delivery,” has more on instal- lation testing.\n\nAs with any other functionality, risks associated with installation need to be evaluated and the amount of testing determined accordingly. Our advice is to do it early and often, and automate the process if possible.\n\n“ility” Summary\n\nThere are other “ilities” to test, depending on your product’s domain. Safety- critical software, such as that used in medical devices and aircraft control sys- tems, requires extensive safety testing, and the regression tests probably would contain tests related to safety. System redundancy and failover tests would be especially important for such a product. Your team might need to look at industry data around software-related safety issues and use extra code reviews. Conﬁgurability, auditability, portability, robustness, and extensibil- ity are just a few of the qualities your team might need to evaluate with tech- nology-facing tests.\n\nWhatever “ility” you need to test, use an incremental approach. Start by elic- iting the customer team’s requirements and examples of their objectives for that particular area of quality. Write business-facing tests to make sure the code is designed to meet those goals. In the ﬁrst iteration, the team might do some research and come up with a test strategy to evaluate the existing qual- ity level of the product. The next step might be to create a suitable test envi- ronment, to research tools, or to start with some manual tests.\n\n—Janet",
      "page_number": 267
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 275-282)",
      "start_page": 275,
      "end_page": 282,
      "detection_method": "topic_boundary",
      "content": "Janet’s Story\n\nPERFORMANCE, LOAD, STRESS, AND SCALABILITY TESTING\n\nAs you learn how the application measures up to the customers’ require- ments, close the loop with new Quadrant 1 and 2 tests that drive the applica- tion closer to the goals for that particular property. An incremental approach is also recommended for performance, load, and other tests that are ad- dressed in the next section.\n\nPERFORMANCE, LOAD, STRESS, AND SCALABILITY TESTING Performance, load, stress, and scalability testing all fall into Quadrant 4 be- cause of their technology focus. Often specialized skills are required, al- though many teams have ﬁgured out ways to do their own testing in these areas. Let’s talk about scalability ﬁrst, because it is often forgotten.\n\nScalability\n\nScalability testing veriﬁes the application remains reliable when more users are added. What that really means is, “Can your system handle the capacity of a growing customer base?” It sounds simple, but really isn’t, and is a problem that an agile team usually can’t solve by itself.\n\nIt is important to think about the whole system and not just the application itself. For example, the network is often the bottleneck, because it can’t han- dle the increased throughput. What about the database? Will it scale? Will the hardware you are using handle the new loads being considered? Is it simple just to add new hardware, or is it the bottleneck?\n\nIn one organization I was recently working in, their customer base had grown very quickly, and the solution they had invested in had reached its capacity due to hardware constraints. It was not a simple matter of adding a new server, because the solution was not designed that way. The system needed to be monitored to restart services during peak usage.\n\nTo grow, the organization had to actually change solutions to accommodate its future growth, but this was not recognized until problems started to happen.\n\nIdeally, the organization would have replaced the old system before it was an issue. This is an example of why it is important to understand your system and its capability, as well as future growth projections.\n\n233\n\n—Janet\n\n234\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nYou will need to go outside the team to get the answers you require to address scalability issues, so plan ahead.\n\nPerformance and Load Testing\n\nPerformance testing is usually done to help identify bottlenecks in a system or to establish a baseline for future testing. It is also done to ensure compli- ance with performance goals and requirements, and to help stakeholders make informed decisions related to the overall quality of the application be- ing tested.\n\nLoad testing evaluates system behavior as more and more users access the system at the same time. Stress testing evaluates the robustness of the appli- cation under higher-than-expected loads. Will the application scale as the business grows? Characteristics such as response time can be more critical than functionality for some applications.\n\nGrig Gheorghiu [2005] emphasizes the need for clearly deﬁned expectations to get value from performance testing. He says, “If you don’t know where you want to go in terms of the system, then it matters little which direction you take (remember Alice and the Cheshire Cat?).” For example, you probably want to know the number of concurrent users and the acceptable response time for a web application.\n\nPerformance and Load-Testing Tools\n\nSee the bibliogra- phy for links to sites where you can research tools.\n\nAfter you’ve deﬁned your performance goals, you can use a variety of tools to put a load on the system and check for bottlenecks. This can be done at the unit level, with tools such as JUnitPerf, httperf, or a home-grown harness. Apache JMeter, The Grinder, Pounder, ftptt, and OpenWebLoad are more examples of the many open source performance and load test tools available at the time of this writing. Some of these, such as JMeter, can be used on a variety of server types, from SOAP to LDAP to POP3 mail. Plenty of com- mercial tool options are available too, including NeoLoad, WebLoad, eValid LoadTest, LoadRunner, and SOATest.\n\nUse these tools to look for performance bottlenecks. Lisa’s team uses JProﬁler to look for application bottlenecks and memory leaks, and JConsole to analyze database usage. Similar tools exist for .NET and other environments, including .NET Memory Proﬁler and ANTS Proﬁler Pro. As Grig points out, there are database-speciﬁc proﬁlers to pinpoint performance issues at the database level; ask your database experts to work with you. Your system administrators can\n\nPERFORMANCE, LOAD, STRESS, AND SCALABILITY TESTING\n\nhelp you use shell commands such as top, or tools such as PerfMon to monitor CPU, memory, swap, disk I/O, and other hardware resources. Similar tools are available at the network level, for example, NetScout.\n\nYou can also use the tools the team is most familiar with. In one project, Janet worked very closely with one of the programmers to create the tests. She helped him to deﬁne the tests needed based on customer’s performance and load expectations, and he automated them using JUnit. Together they analyzed the results to report back to the customer.\n\nEstablishing a baseline is a good ﬁrst step for evaluating performance. The next section explores this aspect of performance testing.\n\nBaseline\n\nPerformance tuning can turn into a big project, so it is essential to provide a baseline that you can compare against new versions of the software on perfor- mance. Even if performance isn’t your biggest concern at the moment, don’t ignore it. It’s a good idea to get a performance baseline so that you know later which direction your response time is headed. Lisa’s company hosts a website that has had a small load on it. They got a load test baseline on the site so that as it grew, they’d know how performance was being affected.\n\nPerformance Baseline Test Results\n\nLisa’s coworker Mike Busse took on the task of obtaining performance base- lines for their web application that manages retirement plans. He evaluated load test tools, implemented one (JMeter), and set about to get a baseline. He reported the results both in a high-level summary and a spreadsheet with detailed results.\n\nThe tests simulated slowly increasing the load up to 100 concurrent users. Three test scripts, each for a common user activity, were used, and they were run separately and all together. Data gathered included:\n\nMaximum time of a transaction\n\nMaximum number of busy connections.\n\nA plot of the max time of a transaction against the number of users (see Figure 11-2 for an example of a chart)\n\nNumber of users who were on the system when the max time of a transaction equaled eight seconds\n\n235\n\n236\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nAll Three Tests Run Together\n\n50000\n\n45000\n\n) s d n o c e s i l l i\n\nM n i ( e m i T\n\n40000\n\n35000 30000 25000 20000\n\n15000\n\n10000\n\nAverage Max Poly. (Max)\n\n5000\n\n0\n\n3\n\n10\n\n20\n\n40\n\n60\n\n80\n\n100\n\n# of Users\n\nFigure 11-2 Max and average transaction times at different user loads.\n\nAn important aspect of reporting results was providing deﬁnitions of terms such as transaction and connection in order to make the results meaningful to everyone. For example, maximum time of a transaction is deﬁned as the longest transaction of all transactions completed during the test.\n\nMike’s report also included assumptions made for the performance test:\n\nEight seconds is a transaction threshold that we would not like to cross.\n\nThe test web server is equivalent to either of the two web servers in production.\n\nThe load the system can handle, as determined by these tests, can be doubled in production because the load is distributed between two web servers.\n\nThe distribution of tasks in the test that combines all three tests is accu- rate to a reasonable degree.\n\nMike also identiﬁed shortcomings with the performance baseline. More than one transaction can contribute to loading a page, meaning that the max page load time could be longer than the max time of a transaction. The test machine doesn’t duplicate the production environment, which has two machines and load-balancing software to distribute the transactions.\n\nThe report ended with a conclusion about the number of concurrent users that the production system could support. This serves as a guideline to be aware of as the production load increases. The current load is less than half of this number, but there are unknowns, such as whether the production users are all active or have neglected to log out.\n\nPERFORMANCE, LOAD, STRESS, AND SCALABILITY TESTING\n\nMake sure your performance tests adequately mimic production conditions. Make results meaningful by deﬁning each test and metric, explaining how the results correlate to the production environment and what can be done with the results, and providing results in graphical form.\n\nIf there are speciﬁc performance criteria that have been deﬁned for speciﬁc functionality, we suggest that performance testing be done as part of the iter- ation to ensure that issues are found before it is too late to ﬁx them.\n\nBenchmarking can be done at any time during a release. If new functionality is added that might affect the performance, such as complicated queries, re- run the tests to make sure there are no adverse effects. This way, you have time to optimize the query or code early in the cycle when the development team is still familiar with the feature.\n\nAny performance, load, or stress test won’t be meaningful unless it’s run in an environment that mimics the production environment. Let’s talk more about environments.\n\nTest Environments\n\nFinal runs of the performance tests will help customers make decisions about accepting their product. For accurate results, tests need to be run on equip- ment that is similar to that of production. Often teams will use smaller ma- chines and extrapolate the results to decide if the performance is sufﬁcient for the business needs. This should be clearly noted when reporting test results.\n\nStressing the application to see what load it can take before it crashes can also be done anytime during the release, but usually it is not considered high-priority by customers unless you have a mission-critical system with lots of load.\n\nOne resource that is affected by increasing load is memory. In the next sec- tion, we discuss memory management.\n\nMemory Management\n\nMemory is usually described in terms of the amount (normally the mini- mum or maximum) of memory to be used for RAM, ROM, hard drives, and so on. You should be aware of memory usage and watch for leaks, because they can cause catastrophic failures when the application is in production during peak usage. Some programming languages are more susceptible to memory issues, so understanding the strengths and weaknesses of the code\n\n237\n\n238\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nwill assist you in knowing what to watch for. Testing for memory issues can be done as part of performance, load, and stress testing.\n\nGarbage collection is one tool used to release memory back to the program. However, it can mask severe memory issues. If you see the available memory steadily decreasing with usage and then all of a sudden increasing to maxi- mum available, you might suspect the garbage collection has kicked in. Watch for anomalies in the pattern or whether the system starts to get slow under heavy usage. You may need to monitor for a while and work with the pro- grammers to ﬁnd the issue. The ﬁx might be something simple, such as sched- uling the garbage collection more often or setting the trigger level higher.\n\nWhen you are working with the programmers on a story, ask them if they ex- pect problems with memory. You can test speciﬁcally if you know there might be a risk in the area. Watching for memory leaks is not always easy, but there are tools to help. This is an area where programmers should have tools easily available. Collaborate with them to verify that the application is free of memory issues. Perform the performance and load tests described in the pre- vious section to verify that there aren’t any memory problems.\n\nYou don’t have to be an expert on how to do technology-facing testing that critiques the product to help your team plan for it and execute it. Your team can evaluate what tests it needs from this quadrant. Talk about these tests as you plan your release; you can create a test plan speciﬁcally for performance and load if you’ve not done it before. You will need time to obtain the exper- tise needed, either by acquiring it through identifying and learning the skills, or by bringing in outside help. As with all development efforts, break tech- nology-facing tests into small tasks that can be addressed and built upon each iteration.\n\nSUMMARY In this chapter, we’ve explored the fourth agile testing quadrant, the technology- facing tests that critique the product.\n\n(cid:2) The developer team should evaluate whether it has, or can acquire,\n\nthe expertise to do these tests, or if it needs to plan to bring in external resources.\n\n(cid:2) An incremental approach to these tests, completing tasks in each iter- ation, ensures time to address any issues that arise and avoid produc- tion problems.\n\nSUMMARY\n\n(cid:2) The team should consider various types of “ility” testing, including security, maintainability, interoperability, compatibility, reliability, and installability testing, and should execute these tests at appropriate times.\n\n(cid:2) Performance, scalability, stress, and load testing should be done from\n\nthe beginning of the project.\n\n(cid:2) Research the memory management issues that might impact your product, and plan tests to verify the application is free of memory issues.\n\n239\n\nThis page intentionally left blank",
      "page_number": 275
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 283-290)",
      "start_page": 283,
      "end_page": 290,
      "detection_method": "topic_boundary",
      "content": "Chapter 12\n\nSUMMARY OF TESTING QUADRANTS\n\nTest Code\n\nThe Team and the Process\n\nReporting Test Results\n\nDocumentation\n\nThe System Explained\n\nThe Application\n\nQuadrant Summary: Testing on a Real Agile Project\n\nDriving Development\n\nUnit Tests\n\nAcceptance Tests\n\nExploratory Testing\n\nTesting Data Feeds\n\nFunctional Automation\n\nEnd-to-End Tests\n\nCritiquing the Product\n\nAutomation\n\nWeb Services\n\nUAT\n\nEmbedded Test Framework\n\nReliability Tests\n\nIn Chapter 6, we introduced the testing quadrants, and in the chapters that fol- lowed we talked about how to use the concepts in your agile project. In this chap- ter, we’ll bring it all together with an example of an agile team that used tests from all four quadrants.\n\nREVIEW OF THE TESTING QUADRANTS We’ve just spent ﬁve chapters talking about each of the quadrants (see Fig- ure 12-1) and examples of tools you can use for the different types of testing. The next trick is to know which tests your project needs and when to do them. In this chapter, we’ll walk you through a real-life example of an agile project that used tests from all four agile testing quadrants.\n\n241\n\n242\n\nCHAPTER 12\n\n(cid:2)\n\nSUMMARY OF TESTING QUADRANTS\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ2 Q3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 12-1 Agile Testing Quadrants\n\nA SYSTEM TEST EXAMPLE The following story is about one organization’s success in testing its whole system using a variety of home-grown and open source tools. Janet worked with this team, and Paul Rogers was the primary test architect. This is Paul’s story.\n\nThe Application\n\nThe system solves the problem of monitoring remote oil and gas production wells. The solution combines a remote monitoring device that can transmit data and receive adjustments from a central monitoring station using a pro- prietary protocol over a satellite communication channel.\n\nFigure 12-2 shows the architecture of the Remote Data Monitoring system. The measurement devices on the oil wells, Remote Terminal Units (RTU), use a va-\n\nA SYSTEM TEST EXAMPLE\n\n243\n\nSite\n\nRTU\n\nH/W\n\nSatellite Modem\n\nSatellite\n\nClient\n\nTCP Network Bridge\n\nNetwork Bridges\n\nData on JMS Queue\n\nWeb Services\n\n3rd-Party Application\n\nMessage Routing and Distribution\n\nData Processing\n\nData Management\n\nPresentation and Navigation\n\nWeb Server\n\nApplication Server\n\nAdmin User\n\nDBServer\n\nDatabase\n\nMessage Delivery Center Email, Fax, Photo\n\nBasic User\n\nFigure 12-2 Remote data monitoring system architecture\n\nriety of protocols to communicate with the measurement device. This data from each RTU is transmitted via satellite to servers located at the client’s main ofﬁce. It is then made available to users via a web interface. A notiﬁcation sys- tem, via email, fax, or phone, is available when a particular reading is outside of normal operational limits. A Java Message Service (JMS) feed and web ser- vices are also available to help integration with clients’ other applications.\n\nThe software application was a huge legacy system that had few unit tests. The team was slowly rebuilding the application with new technology.\n\nThe Team and the Process\n\nThe team consisted of four software programmers, two ﬁrmware program- mers, three to four testers, a product engineer, and an off-site manager. The “real” customer was in another country. The development team uses XP\n\n244\n\nCHAPTER 12\n\n(cid:2)\n\nSUMMARY OF TESTING QUADRANTS\n\npractices, including pair programming and TDD. The customer team used the defect-tracking system for the backlog, but most of the visibility of the stories was through index cards. Story cards were used during iteration planning meetings, and the task board tracked the progress.\n\nScrum was used as the outside reporting mechanism to the organization and the customers. The team had two week iterations and released the product about every four months. This varied depending on the functionality being developed. Retrospectives were held as part of every iteration planning ses- sion, and action was taken on the top three priority items discussed.\n\nContinuous integration through CruiseControl provided constant builds for the testers and the demonstrations held at the end of every iteration. Each tester had a local environment for testing the web application, but there were three test environments available to the system. The ﬁrst one was to test new stories and was updated as needed with the latest build. The second one was for testing client-reported issues, because it had the last version released to the clients. The third environment was a full stand-alone test environment that was available for testing full deploys, communication links, and the ﬁrmware and hardware. It was on this environment that we ran our load and reliability tests.\n\nTESTS DRIVING DEVELOPMENT The tests driving development included unit test and acceptance tests.\n\nUnit Tests\n\nChapter 7, “Technology- Facing Tests that Support the Team,” explains more about unit testing and TDD.\n\nUnit tests are technology-facing tests that support programming. Those that are developed as part of test-driven development not only help the program- mer get the story right but also help to design the system.\n\nThe programmers on the Remote Data Monitoring project bought into Test Driven Development (TDD) and pair programming wholeheartedly. All new functionality was developed and tested using pair programming. All stories delivered to the testers were supported by unit tests, and very few bugs were found after coding was complete. The bugs that were found were generally integration-related.\n\nHowever, when the team ﬁrst started, the legacy system had few unit tests to support refactoring. As process changes were implemented, the developers\n\nSee Chapter 8, “Business-Facing Tests that Support the Team,” for more about driv- ing development with acceptance tests.\n\nAUTOMATION\n\ndecided to start ﬁxing the problem. Every time they touched a piece of code in the legacy system, they added unit tests and refactored the code as neces- sary. Gradually, the legacy system became more stable and was able to with- stand major refactoring when it was needed. We experienced the power of unit tests!\n\nAcceptance Tests\n\nThe product engineer (the customer proxy) took ownership of creating the acceptance tests. These tests varied in format depending on the actual story. Although he struggled at ﬁrst, the product engineer got pretty good at giving the tests to the programmers before they started coding. The team created a test template, which evolved over time, that met both the programmers’ and the testers’ needs.\n\nThe tests were sometimes informally written, but they included data, re- quired setup if it wasn’t immediately obvious, different variations that were critical to the story, and some examples. The team found that examples helped clarify the expectations for many of the stories.\n\nThe test team automated the acceptance tests as soon as possible, usually at the same time as the stories were being developed. Of course, the product engineer was available to answer any questions that came up during development.\n\nThese acceptance tests served three purposes. They were business-facing tests that supported development because they were given to the team before coding started. Secondly, they were used by the test team as the basis of automation that fed into the regression suite and provided future ideas for exploratory test- ing. The third purpose was to conﬁrm that the implementation met the needs of the customer. The product engineer did this solution veriﬁcation.\n\nAUTOMATION Automation involved the functional test structure, web services, and embed- ded testing.\n\nThe Automated Functional Test Structure\n\nRuby was used with Watir as the tool of choice for the functional automation framework. It was determined to have the most ﬂexibility and opportunity for customization that was required for the system under test.\n\n245\n\n246\n\nCHAPTER 12\n\n(cid:2)\n\nSUMMARY OF TESTING QUADRANTS\n\nThe automated test code included three distinct layers, shown in Figure 12-3. The lowest layer, Layer 1, included Watir and other classes, such as loggers that wrote to the log ﬁles.\n\nThe second layer, Layer 2, was the page access layer, where classes that contained code to access individual web pages lived. For example, in the application under test (AUT) there was a login page, a create user page, and an edit user page. Classes written in Ruby contained code that could perform certain functions in the AUT, such as a class that logs into the application, a class to edit a user, and a class to assign access rights to a user. These classes contained no data. For exam- ple, the log-in class didn’t know what username to log in with.\n\nThe third and top layer, Layer 3, was the test layer, and it contained the data needed to perform a test. It called Layer 2 classes, which in turned called Layer 1.\n\nFor example, the actual test would call LogIn and pass Janet as the user name and Passw0rd as the password. This meant you could feed in many different data sets easily.\n\nLogIn (‘Janet’, ‘Passw0rd’)\n\nLayer 3—Test Layer\n\nLayer 2—Page Access Layer\n\nLayer 1—IE Controller (Watir)\n\nFigure 12-3 Functional test layers\n\nAUTOMATION\n\nLayer 2 also knew how to handle the error messages the application gener- ated. For example, when an invalid username was entered on the login page, the login class detected the error message and then passed the problem back to the tests in Layer 3.\n\nThis means the same Layer 2 classes could be used for both happy path testing and for negative testing. In the negative case, Layer 3 would expect Layer 2 to return a failure, and would then check to see if the test failed for the correct rea- son by accessing the error messages that Layer Two scraped from the browser.\n\nThe functional tests used Ruby with Watir to control the DOM on the browser and could access almost all of the objects in the page. The automated test suite was run on nightly builds to give the team consistent feedback on high-level application behavior. This was a lifesaver as the team continued to build out the unit tests. This architecture efﬁciently accommodated the busi- ness-facing tests that support the team.\n\nWeb Services\n\nWeb services were used by clients to interface with some of their other appli- cations. The development group used Ruby to write a client to test each service they developed. For these tests, Ruby’s unit testing framework, Test::Unit, was used.\n\nThe web services tests were expanded by the test team to cover more than 1,000 different test cases, and took just minutes to run. They gave the team an amazing amount of coverage in a short period of time.\n\nThe team demonstrated the test client to the customers, who decided to use it as well. However, the customers subsequently decided it didn’t work for them, so they started writing their own tests, albeit in a much more ad hoc fashion using Ruby.\n\nThey used IRB, the interactive interface provided by Ruby, and fed values in an exploratory method. It gave the customer an interactive environment for discovering what worked and what didn't. It also let them get familiar with Ruby and how we were testing, and it gave them much more conﬁdence in our tests. Much of their User Acceptance Testing was done using IRB.\n\nThree different slants on the web services tests served three different pur- poses. The programmers used it to help test their client and drive their develop- ment. The testers used it to critique the product in a very efﬁcient automated\n\n247\n\n248\n\nCHAPTER 12\n\n(cid:2)\n\nSUMMARY OF TESTING QUADRANTS\n\nmanner, and the customers were able to test the web services delivered to them using IRB.\n\nEmbedded Testing\n\nIn addition to the web interface, the RDM system consisted of a small em- bedded device that communicated with measuring equipment using various protocols. Using Ruby, various tests were developed to test part of its admin- istrative interface. This interface was a command-line system similar to FTP.\n\nThese data-driven tests were contained in an Excel spreadsheet. A Ruby script would read commands from Excel using the OLE interface and send them to the embedded device. The script would then compare the response from the device with the expected result, also held in the spreadsheet. Errors were highlighted in red. These automated tests took approximately one hour to run, while doing the same tests manually would take eight hours.\n\nWhile this provided a lot of test coverage, it didn’t actually test the reason the device was used, which was to read data from RTUs. A simulator was written in Ruby with a FOX (FXRuby) GUI. This allowed mock data to be fed into the device. Because the simulator could be controlled remotely, it was incorpo- rated into automated tests that exercised the embedded device’s ability to read data, respond to error conditions, and generate alarms when the input data exceeded a predetermined threshold.\n\nEmbedded testing is highly technical, but with the power provided by the simulator, the whole team was able to participate in testing the device. The simulator was written to support testing for the test team, but the program- mer for the ﬁrmware found it valuable and used it to help with his develop- ment efforts as well. That was a positive unexpected side effect. Quadrant 2 tests that support the team may incorporate a variety of technologies, as they did in this project.\n\nCRITIQUING THE PRODUCT WITH BUSINESS-FACING TESTS The business-facing tests that critique the product are outlined in this section.\n\nExploratory Testing\n\nThe automated tests were simple and easy for everyone on the team to use. Individual test scripts could be run to set up speciﬁc conditions, allowing ef-",
      "page_number": 283
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 291-298)",
      "start_page": 291,
      "end_page": 298,
      "detection_method": "topic_boundary",
      "content": "Exploratory test- ing, usability test- ing, and other Quadrant 3 tests are discussed in Chapter 10, “Business-Facing Tests that Critique the Product.”\n\nCRITIQUING THE PRODUCT WITH BUSINESS-FACING TESTS\n\nfective exploratory testing to be done without having to spend a lot of time manually entering data. This worked for all three test frameworks: func- tional, web services, and embedded.\n\nThe team performed exploratory testing to supplement the automated test suites and get the best coverage possible. This human interaction with the system found issues that automation didn’t ﬁnd.\n\nUsability testing was not a critical requirement for the system, but the testers watched so that the interface made sense and ﬂowed smoothly. The testers used exploratory testing extensively to critique the product. The product en- gineer also used exploratory testing for his solution veriﬁcation tests.\n\nTesting Data Feeds\n\nAs shown in Figure 12-2, the data from the system is available on a JMS queue, as well as the web browser. To test the JMS queue, the development group wrote a Java proxy. It connected to a queue and printed any arriving data to the console. They also wrote a Ruby client that received this data via a pipe, which was then available in the Ruby automated test system.\n\nEmails were automatically sent when alarm conditions were encountered. The alarm emails contained both plain text email and email with attach- ments. The MIME attachments contained data useful for testing, so a Ruby email client that supported attachments was written.\n\nThe End-to-End Tests\n\nQuadrant 3 includes end-to-end functional testing that demonstrates the de- sired behavior of every part of the system. From the beginning, it was apparent that correct operation of the whole Remote Data Monitoring system could only be determined when all components were used. Once the simulator, em- bedded device tests, web services tests, and application tests were written, it was a relatively simple matter to combine them to produce an automated test of the entire system. Once again, Excel spreadsheets were used to hold the test data, and Ruby classes were written to access the data and expected results.\n\nThe end-to-end tests were complicated by the unpredictable response of the satellite transmission path. A predeﬁned timeout value was set, and if the test’s actual value did not match the expected value, the test would cycle until it matched or the timeout was reached. When the timeout expired, the test was deemed to have failed. Most transmission issues were found and eliminated\n\n249\n\n250\n\nCHAPTER 12\n\n(cid:2)\n\nSUMMARY OF TESTING QUADRANTS\n\nthis way. It would have been highly unlikely that they would have been found with manual testing, because they were sporadic issues.\n\nBecause end-to-end tests such as these can be fragile, they may not be kept as part of the automated regression suite. If all of the components of the system are well covered with automated regression tests, automated end-to-end tests might not be necessary. However, due to the nature of this system, it wasn’t possible to do a full test without automation.\n\nUser Acceptance Testing\n\nUser Acceptance Testing (UAT) is the ﬁnal critique of the product by the cus- tomer, who should have been involved in the project from the start. In this example, the real customer was in France, thousands of miles from the devel- opment team. The team had to be inventive to have a successful UAT. The customer came to work with the team members a couple of times during the year and so was able to interact with the team a little easier than if they’d never met.\n\nAfter the team introduced agile development, Janet went to France to facili- tate the ﬁrst UAT at the customer site. It worked fairly well, and the release was accepted after a few critical issues were ﬁxed. The team learned a lot from that experience.\n\nThe second UAT sign-off was done in-house. To prepare, the team worked with the customer to develop a set of tests the customer could perform to ver- ify new functionality. The customer was able to test the application through- out the development cycle, so UAT didn’t produce any issues. The customer came, ran through the tests, and signed off in a day.\n\nWe cannot stress the importance of working with the customer enough. Even though the product engineer was the proxy for the customer, it was crucial to get face time with the actual customer. The relationship that had been built over time was critical to the success of the project. Janet strongly believes that the UAT succeeded because the customer knew what the team was doing along the way.\n\nReliability\n\nReliability, one of the “ilities” addressed by Quadrant 4 tests, was a critical factor of the system because it was monitoring remote sites that were often\n\nSee Chapter 10, “Business-Facing Tests that Critique the Product,” for more about Quad- rant 4 tests such as reliability testing.\n\nChapter 16, “Hit the Ground Run- ning,” gives more examples of ways teams report test results.\n\nChapter 18, “Cod- ing and Testing,” also discusses uses of big visible charts.\n\nDOCUMENTATION\n\ninaccessible, especially in winter. The simulator that was developed for test- ing the embedded system was set up on a separate environment, and was run for weeks at a time measuring stability (yet another “ility”) of the whole sys- tem. Corrections to the system design could be planned and coded as needed. This is a good example of why you shouldn’t wait until the end of the project to do the technology-facing tests that critique the product.\n\nDOCUMENTATION The approach taken to documentation is presented in this section.\n\nDocumenting the Test Code\n\nDuring development, it became clear that a formal documentation system was needed for the test code. The simplest solution was to use RDoc, similar to Javadoc, but for Ruby. RDoc extracted tagged comments from the source code and generated web pages with details of ﬁles, classes, and methods. The documents were generated every night using a batch ﬁle and were available to the complete team. It was easy to ﬁnd what test ﬁxtures were created.\n\nThe documentation of the test code helped to document the tests and make it easier to ﬁnd what we were testing and what the tests did. It was very pow- erful and easy to use.\n\nReporting the Test Results\n\nAlthough comprehensive testing was being performed, there was little evi- dence of this outside of the test team. The logs generated during automated tests provided good information to track down problems but were not suit- able for a wider audience.\n\nTo raise the visibility of the tests being performed, the test team developed a logging and reporting system using Apache, PHP, and mySQL. When a test ran, it logged the result into the database. A web front end allowed project stakeholders to determine what tests were run, the pass/failure rate, and other information.\n\nWe also believed in making our progress visible (good or bad) as much as possible. To this end we created charts and graphs along the way and posted them in common areas. Figure 12-4 shows some of the charts we created.\n\n251\n\n252\n\nCHAPTER 12\n\n(cid:2)\n\nSUMMARY OF TESTING QUADRANTS\n\nFigure 12-4 Big visible charts used by the remote monitoring system project team\n\nUSING THE AGILE TESTING QUADRANTS This example demonstrates how testing practices from all four agile testing quadrants are combined during the life of a complex development project to achieve successful delivery. The experience of this team illustrates many of the principles we have been emphasizing. The whole team, including pro- grammers, testers, customer proxy, and the actual customer, contributed to efforts to solve automation problems. They experimented with different ap- proaches. They combined their homegrown and open source tools in differ- ent ways to perform testing at all levels, from the unit level to end-to-end system testing and UAT. The success of the project demonstrates the success of the testing approach.\n\nAs you plan each epic, release, or iteration, work with your customer team to understand the business priorities and analyze risks. Use the quadrants to help identify all of the different types of testing that will be needed and when they should be performed. Is performance the most important criteria? Is the\n\nSUMMARY\n\nhighest priority the ability to interface with other systems? Is usability per- haps the most important aspect?\n\nInvest in a test architecture that accommodates the complexity of the system under test. Plan to obtain necessary resources and expertise at the right time for specialized tests. For each type of test, your team should work together to choose tools that solve your testing problems. Use retrospectives to continu- ally evaluate whether your team has the resources it needs to succeed, and whether all necessary tests are being speciﬁed in time to serve their purpose, and automated appropriately.\n\nDoes end-to-end testing seem impossible to do? Is your team ﬁnding it hard to write unit tests? As Janet’s team did, get everyone experimenting with dif- ferent approaches and tools. The quadrants provide a framework for produc- tive brainstorming on creative ways to achieve the testing that will let the team deliver value to the business.\n\nSUMMARY In this chapter, we described a real project that used tests from all four agile testing quadrants to overcome difﬁcult testing challenges. We used examples from this project to show how teams can succeed with all types of testing. Some important lessons from the Remote Data Monitoring System project are:\n\n(cid:2) The whole team should choose or create tools that solve each testing\n\nproblem.\n\n(cid:2) Combinations of common business tools such as spreadsheets and custom-written test scripts may be needed to accomplish complex tests.\n\n(cid:2) Invest time in building the right test architecture that works for all\n\nteam members.\n\n(cid:2) Find ways to keep customers involved in all types of testing, even if\n\nthey’re in a remote location.\n\n(cid:2) Report test results in a way that keeps all stakeholders informed about\n\nthe iteration and project progress.\n\n(cid:2) Don’t forget to document . . . but only what is useful. (cid:2) Think about all four quadrants of testing throughout your develop-\n\nment cycles.\n\n(cid:2) Use lessons learned during testing to critique the product in order to\n\ndrive development in subsequent iterations.\n\n253\n\nThis page intentionally left blank\n\nPart IV AUTOMATION\n\nTest automation is a core agile practice. Agile projects depend on automation. Good-enough automation frees the team to deliver high-quality code fre- quently. It provides a framework that lets the team maximize its velocity while maintaining a high standard. Source code control, automated builds and test suites, deployment, monitoring, and a variety of scripts and tools eliminate tedium, ensure reliability, and allow the team to do its best work at all times.\n\nAutomation is also a vast topic. It includes tasks like writing simple shell scripts, setting up session properties, and creating robust automated tests. The range and number of automated tools seem to grow exponentially as we learn about better ways to produce software. Happily, the number of excel- lent books that teach ways to automate appears to grow just as fast.\n\nThis book is focused on the tester’s role in agile development. Because auto- mation is key to successful agile development, we need to talk about it, but we can’t begin to cover every aspect of the subject. What we do want to explain is why you, as a tester, must embrace automation, and how you and your team can overcome the many obstacles that can hamper your automation efforts. This section describes how you can apply agile values, principles, and prac- tices to grow a practical automation strategy, overcome barriers, and get trac- tion on test automation.\n\nThis page intentionally left blank",
      "page_number": 291
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 299-306)",
      "start_page": 299,
      "end_page": 306,
      "detection_method": "topic_boundary",
      "content": "Culture\n\nWhole-Team Approach\n\nChapter 13\n\nWHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nManual Testing Takes Too Long\n\nReduce Error-Prone Testing Tasks\n\nFree Up Time to Do Best Work\n\nReasons to Automate\n\nSafety Net\n\nProvide Feedback Early and Often\n\nTests & Examples that Drive Coding Can Do More\n\nTests Provide Documentation\n\nROI/Investment\n\nWhy Automate?\n\nBret’s List\n\nAttitude—Why Should We Automate?\n\nCan We Overcome Barriers?\n\nObstacles to Watch For\n\nHump of Pain\n\nInitial Investment\n\nCode in Flux\n\nLegacy Code\n\nFear\n\nOld Habits\n\nWhy do we automate testing, the build process, deployment, and other tasks? Agile teams focus on always having working software, which enables them to release production-ready software as often as needed. Achieving this goal requires constant testing. In this chapter, we look at reasons we want to auto- mate and the challenges that make it hard to get traction on automation.\n\n257\n\n258\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nWHY AUTOMATE? There are multiple reasons to automate besides our saying you need to have automation to be successful using agile. Our list includes the following:\n\n(cid:2) Manual testing takes too long. (cid:2) Manual processes are error prone. (cid:2) Automation frees people to do their best work. (cid:2) Automated regression tests provide a safety net. (cid:2) Automated tests give feedback early and often. (cid:2) Tests and examples that drive coding can do more. (cid:2) Tests provide documentation. (cid:2) Automation can be a good return on investment.\n\nLet’s explore each of these in a little more detail.\n\nManual Testing Takes Too Long\n\nThe most basic reason a team wants to automate is that it simply takes too long to complete all of the necessary testing manually. As your application gets bigger and bigger, the time to test everything grows longer and longer, sometimes exponentially, depending on the complexity of the AUT (Applica- tion under test).\n\nAgile teams are able to deliver production-ready software at the end of each short iteration by having production-ready software every day. Running a full suite of passing regression tests at least daily is an indispensable practice, and you can’t do it with manual regression testing. If you don’t have any au- tomation now, you’ll have to regression test manually, but don’t let that stop you from starting to automate it.\n\nIf you execute your regression testing manually, it takes more and more time testing every day, every iteration. In order for testing to keep pace with cod- ing, either the programmers have to take time to help with manual regres- sion testing, or the team has to hire more testers. Inevitably, both technical debt and frustration will grow.\n\nIf the code doesn’t even have to pass a suite of automated unit level regres- sion tests, the testers will probably spend much of their time researching, try- ing to reproduce and report those simple bugs, and less time ﬁnding potentially serious system level bugs. In addition, because the team isn’t do- ing test-ﬁrst development, code design is more likely to be less testable and may not provide the functionality desired by the business.\n\nWHY AUTOMATE?\n\nManually testing a number of different scenarios can take a lot of time, espe- cially if you’re keying inputs into a user interface. Setting up data for a variety of complex scenarios can be an overwhelming task if you have no automated way to speed it up. As a result, only a limited number of scenarios may be tested, and important defects can be missed.\n\nManual Processes Are Error Prone\n\nManual testing gets repetitive, especially if you’re following scripted tests, and manual tests get boring very quickly. It’s way too easy to make mistakes and overlook even simple bugs. Steps and even entire tests will be skipped. If the team’s facing a tight deadline, there’s a temptation to cut corners, and the result is a missed problem.\n\nBecause manual testing is slow, you might still be testing at midnight on the last day of the iteration. How many bugs will you notice then?\n\nAutomated builds, deployment, version control, and monitoring also go a long way toward mitigating risk and making your development process more consistent. Automating these scripted tests eliminate the possibility of errors, because each test is done exactly the same way every time.\n\nThe adage of “build once, deploy to many” is a tester’s dream come true. Au- tomation of the build and deploy processes allow you to know exactly what you are testing on any given environment.\n\nAutomation Frees People to Do Their Best Work\n\nWriting code test-ﬁrst helps programmers understand requirements and de- sign code accordingly. Having continual builds run all of the unit tests and the functional regression tests means more time to do interesting exploratory testing. Automating the setup for exploratory tests means even more time to probe into potentially weak parts of the system. Because you didn’t spend time executing tedious manual scripts, you have the energy to do a good job, thinking of different scenarios and learning more about how the application works.\n\nIf we’re thinking constantly about how to automate tests for a ﬁx or new fea- ture, we’re more likely to think of testability and a quality design rather than a quick hack that might prove fragile. That means better code and better tests.\n\nAutomating tests can actually help with consistency across the application.\n\n259\n\n260\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nJanet’s Story\n\nSee Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” Chap- ter 12, “Summary of Testing Quad- rants,” and Chap- ter 14, “An Agile Test Automation Strategy,” for more information about Ruby and Watir.\n\nJason (one of my fellow testers) and I were working on some GUI automation scripts using Ruby and Watir, and were adding constants for button names for the tests. We quickly realized that the buttons on each page were not consistently named. We were able to get them changed and resolved those consistency issues very quickly, and had an easy way to enforce the naming conventions.\n\nBooks such as Pragmatic Project Automation [2004] can guide you in auto- mating daily development chores and free your team for important activities such as exploratory testing.\n\nGiving Testers Better Work\n\nChris McMahon described the beneﬁts he’s experienced due to regression test automation in a posting to the agile-testing mailing list in November 2007:\n\nOur UI regression test automation has grown 500% since April [of 2007]. This allows us to focus the attention of real human beings on more inter- esting testing.\n\nChris went on to explain, “Now that we have a lot of automation, we have the leisure to really think about what human tests need doing. For any testing that isn’t trivial, we have just about institutionalized a test- idea brainstorming session before beginning execution.” Usually, Chris and his teammates pair either two testers or one tester and a developer. Sometimes a tester generates ideas and gets them reviewed, via a mind- map, a wiki page, or a list in the release notes. Chris observed, “We almost always come up with good test ideas by pairing that wouldn’t have been found by either individual independently.”\n\nReferring to their frequent releases of signiﬁcant features, Chris says, “Thanks to the good test automation, we have the time to invest in mak- ing certain that the whole product is attractive and functional for real people. Without the automation, testing this product would be both boring and stupid. As it is, we testers have signiﬁcant and interesting work to do for each release.”\n\nWe agree with Chris that the most exciting part of test automation is the way it expands our ability to improve the product through innovative exploratory testing.\n\n—Janet\n\nJanet’s Story\n\nWHY AUTOMATE?\n\nProjects succeed when good people are free to do their best work. Automat- ing tests appropriately makes that happen. Automated regression tests that detect changes to existing functionality and provide immediate feedback are a primary component of this.\n\nAutomated Regression Tests Provide a Safety Net\n\nMost practitioners who’ve been in the software business for a few years know the feeling of dread when they’re faced with ﬁxing a bug or implementing a new feature in poorly designed code that isn’t covered by automated tests. Squeeze one end of the balloon here and another part of it bulges out. Will it break?\n\nKnowing the code has sufﬁcient coverage by automated regression tests gives a great feeling of conﬁdence. Sure, a change might produce an unexpected ef- fect, but we’ll know about it within a matter of minutes if it’s at the unit level, or hours if at a higher functional level. Making the change test-ﬁrst means thinking through the changed behavior before writing the code and writing a test to verify it, which adds to that conﬁdence.\n\nI recently had a conversation with one of the testers on my team who questioned the value of automated tests. My ﬁrst answer was “It’s a safety net” for the team. However, he challenged that premise. Don’t we just become reliant on the tests rather than ﬁxing the root cause of the problem?\n\nIt made me think a bit more about my answer. He was right in one sense; if we be- come complacent about our testing challenges and depend solely on automated tests to ﬁnd our issues, and then just ﬁx them enough for the test to pass, we do ourselves a disservice.\n\nHowever, if we use the tests to identify problem areas and ﬁx them the right way or refactor as needed, then we are using the safety net of automation in the right way. Automation is critical to the success of an agile project, especially as the ap- plication grows in size.\n\nWhen they don’t have an automated suite of tests acting as a safety net, the programmers may start viewing the testers themselves as a safety net. It’s easy to imagine that Joe Programmer’s thought process goes like this: “I ought to go back and add some automated unit tests for formatEmployeeInfo, but I know Susie Tester is going to check every page where it’s used manually. She’ll see if anything is off, so I’d just be duplicating her effort.”\n\n261\n\n—Janet\n\n262\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nIt’s nice that a programmer would think so highly of the tester’s talents, but Joe is headed down a slippery slope. If he doesn’t automate these unit tests, which other tests might he skip? Susie is going to be awfully busy eyeballing all those pages.\n\nTeams that have good coverage from automated regression tests can make changes to the code fearlessly. They don’t have to wonder, “If I change this formatEmployeeInfo module, will I break something in the user interface?” The tests will tell them right away whether or not they broke anything. They can go lots faster than teams relying exclusively on manual testing.\n\nAutomated Tests Give Feedback, Early and Often\n\nAfter an automated test for a piece of functionality passes, it must continue to pass until the functionality is intentionally changed. When we plan changes in the application, we change the tests to accommodate them. When an auto- mated test fails unexpectedly, a regression defect may have been introduced by a code change. Running an automated suite of tests every time new code is checked in helps ensure that regression bugs will be caught quickly. Quick feedback means the change is still fresh in some programmer’s mind, so trou- bleshooting will go more quickly than if the bug weren’t found until some test- ing phase weeks later. Failing fast means bugs are cheaper to ﬁx.\n\nAutomated tests run regularly and often act as your change detector. They al- low the team an opportunity to know what has changed since the last build. For example, were there any negative side effects with the last build? If your automation suite has sufﬁcient coverage, it can easily tell far-reaching effects that manual testers can never hope to ﬁnd.\n\nMore often than not, if regression tests are not automated, they won’t get run every iteration, let alone every day. The problem arises very quickly during the end game, when the team needs to complete all of the regression tests. Bugs that would have been caught early are found late in the game. Many of the beneﬁts of testing early are lost.\n\nTests and Examples that Drive Coding Can Do More\n\nIn Chapter 7, “Technology-Facing Tests that Support the Team,” we talked about using tests and examples to drive coding. We’ve talked about how im- portant it is to drive coding with both unit and customer tests. We also want to stress that if these tests are automated, they become valuable for a different reason. They become the base for a very strong regression suite.\n\nLisa’s Story\n\nThe bibliography contains an article by Jennitta Andrea [2008] on team et- iquette for TDD.\n\nWHY AUTOMATE?\n\nAfter my team got a handle on unit tests, refactoring, continuous integration, and other technology-facing practices, we were able to catch regression bugs and in- correctly implemented functionality during development.\n\nOf course, this didn’t mean our problems were completely solved; we still some- times missed or misunderstood requirements. However, having an automation framework in place enabled us to start focusing on doing a better job of capturing requirements in up-front tests. We also had more time for exploratory testing. Over time, our defect rate declined dramatically, while our customers’ delight in the delivered business value went up.\n\nTDD and SDD (story test-driven development) keep teams thinking test- ﬁrst. During planning meetings, they talk about the tests and the best way to do them. They design code to make the tests pass, so testability is never an is- sue. The automated test suite grows along with the code base, providing a safety net for constant refactoring. It’s important that the whole team prac- tices TDD and consistently writes unit tests, or the safety net will have holes.\n\nThe team also doesn’t accrue too much technical debt, and their velocity is bound to be stable or even increase over time. That’s one of the reasons why the business managers should be happy to let software teams take the time to implement good practices correctly.\n\nTests Are Great Documentation\n\nIn Part III, we explained how agile teams use examples and tests to guide de- velopment. When tests that illustrate examples of desired behavior are auto- mated, they become “living” documentation of how the system actually works. It’s good to have narrative documentation about how a piece of func- tionality works, but nobody can argue with an executable test that shows in red and green how the code operates on a given set of inputs.\n\nIt’s hard to keep static documentation up to date, but if we don’t update our automated tests when the system changes, the tests fail. We need to ﬁx them to keep our build process “green.” This means that automated tests are always an accurate picture of how our code works. That’s just one of the ways our investment in automation pays off.\n\n263\n\n—Lisa\n\n264\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nROI and Payback\n\nAll of the reasons just presented contribute to the bottom line and the pay- back of automation. Automation provides consistency to a project and gives the team opportunity to test differently and push the limits of the applica- tion. Automation means extra time for testers and team members to concen- trate on getting the right product out to market in a timely manner.\n\nAn important component of test automation payback is the way defects are ﬁxed. Teams that rely on manual tests tend to ﬁnd bugs long after the code containing the bug is written. They get into the mode of ﬁxing the “bug of the day,” instead of looking at the root cause of the bug and redesigning the code accordingly. When programmers run the automated test suite in their own sandbox, the automated regression tests ﬁnd the bugs before the code is checked in, so there’s time to correct the design. That’s a much bigger pay- back, and it’s how you reduce technical debt and develop solid code.\n\nBARRIERS TO AUTOMATION—THINGS THAT GET IN THE WAY Back in 2001, Bret Pettichord [2001] listed seven problems that plague auto- mation. They are still applicable, but are intended for teams that do not in- corporate automation as part of their development. And of course, because you are doing agile, you are doing that, right?\n\nWe would like to think that everyone has included automation tasks as part of each story, but the reality is that you probably wouldn’t be reading this section if you had it all under control. We’ve included Bret’s list to show what problems you probably have if you don’t include automation as part of the everyday project deliverables.\n\nBret’s List\n\nBret’s list of automation problems looks like this:\n\n(cid:2) Only using spare time for test automation doesn’t give it the focus it\n\nneeds.\n\n(cid:2) There is a lack of clear goals. (cid:2) There is a lack of experience. (cid:2) There is high turnover, because you lose any experience you may have. (cid:2) A reaction to desperation is often the reason why automation is cho- sen, in which case it can be more of a wish than a realistic proposal.",
      "page_number": 299
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 307-314)",
      "start_page": 307,
      "end_page": 314,
      "detection_method": "topic_boundary",
      "content": "Lisa’s Story\n\nBARRIERS TO AUTOMATION—THINGS THAT GET IN THE WAY\n\n(cid:2) There can be a reluctance to think about testing; the fun is in the\n\nautomating, not in the testing.\n\n(cid:2) Focusing on solving the technology problem can cause you to lose\n\nsight of whether the result meets the testing need.\n\nWe think there are some other problems that teams run into when trying to automate. Even if we do try to include automation in our project deliverables, there are other barriers to success. In the next section, we present our list of obstacles to successful test automation.\n\nOur List\n\nOur list of barriers to successful test automation is based on the experiences we’ve had with our own agile teams as well as that of the other teams we know.\n\n(cid:2) Programmers’ attitude (cid:2) The “Hump of Pain” (cid:2) Initial investment (cid:2) Code that’s always in ﬂux (cid:2) Legacy systems (cid:2) Fear (cid:2) Old habits\n\nProgrammers’ Attitude—“Why Automate?”\n\nProgrammers who are used to working in a traditional environment, where some separate, unseen QA team does all of the testing, may not even give functional test automation a lot of thought. Some programmers don’t bother to test much because they have the QA team as a safety net to catch bugs be- fore release. Long waterfall development cycles make testing even more re- mote to programmers. By the time the unseen testers are doing their job, the programmers have moved on to the next release. Defects go into a queue to be ﬁxed later at great expense, and nobody is accountable for having pro- duced them. Even programmers who have adopted test-driven development and are used to automating tests at the unit level may not think about how acceptance tests beyond the unit level get done.\n\nI once joined an XP team of skilled programmers practicing test-driven development that had a reasonable suite of unit tests running in an automated build process. They had never automated any business-facing tests, so one day I started a discussion about what tools they might use to automate functional business-facing regression tests. The programmers wanted to know why we needed to automate these tests.\n\n265\n\n266\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nAt the end of the ﬁrst iteration, when everyone was executing the acceptance tests by hand, I pointed out that there would be all these tests to do again in the next iteration as regression tests, in addition to the tests for all of the new stories. In the third iteration, there would be three times as many tests. To a tester, it seems ridiculously obvious, but sometimes programmers need to do the manual tests before they understand the compulsion to automate them.\n\nEducation is the key to getting programmers and the rest of the team to un- derstand the importance of automation.\n\nThe “Hump of Pain” (The Learning Curve)\n\nIt’s hard to learn test automation, especially to learn how to do it in a way that produces a good return on the resources invested in it. A term we’ve heard Brian Marick use to describe the initial phase of automation that developers (including testers) have to overcome is the “hump of pain” (see Figure 13-1). This phrase refers to the struggle that most teams go through when adopting automation.\n\nNew teams are often expected to adopt practices such as TDD and refactor- ing, which are difﬁcult to learn. Without good coaching, plenty of time to master new skills, and strong management support, they’re easily discour-\n\nFigure 13-1 Hump of pain of the automation learning curve\n\n—Lisa\n\nLisa’s Story\n\nBARRIERS TO AUTOMATION—THINGS THAT GET IN THE WAY\n\naged. If they have extra obstacles to learning, such as having to work with poorly designed legacy code, it may seem impossible to ever get traction on test automation.\n\nMy team at ePlan Services originally tried to write unit tests for a legacy system that deﬁnitely wasn’t written with testing in mind. They found this to be a difﬁcult, if not impossible, task, so they decided to code all new stories in a new, testable ar- chitecture. Interestingly, about a year later, they discovered it wasn’t really that hard to write unit tests for the old code. The problem was they didn’t know how to write unit tests at all, and it was easier to learn on a well-designed architecture. Writing unit-level tests became simply a natural part of writing code.\n\nThe hump of pain may occur because you are building your domain-speciﬁc testing framework or learning your new functional test tool. You may want to bring in an expert to help you get it set up right.\n\nYou know your team has overcome the “hump” when automation becomes, if not easy, at least a natural and ingrained process. Lisa has worked on three teams that successfully adopted TDD and functional test automation. Each time, the team needed lots of time, training, commitment, and encourage- ment to get traction on the practices.\n\nInitial Investment\n\nEven with the whole team working on the problem, automation requires a big investment, one that may not pay off right away. It takes time and re- search to decide on what test frameworks to use and whether to build them in-house or use externally produced tools. New hardware and software are probably required. Team members may take a while to ramp up on how to use automated test harnesses.\n\nMany people have experienced test automation efforts that didn’t pay off. Their organization may have purchased a vendor capture-playback tool, given it to the QA team, and expected it to solve all of the automation prob- lems. Such tools often sit on a shelf gathering dust. There may have been thousands of lines of GUI test scripts generated, with no one left who knows what they do, or the test scripts that are impossible to maintain are no longer useful.\n\n267\n\n—Lisa\n\n268\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nJanet’s Story\n\nI walked into an organization as a new QA manager. One of my tasks was to eval- uate the current automated test scripts and increase the test coverage. A vendor tool had been purchased a few years earlier, and the testers who had developed the initial suite were no longer with the organization. One of the new testers hired was trying to learn the tool and was adding tests to the suite.\n\nThe ﬁrst thing I did was ask this tester to do an assessment on the test suite to see what the coverage actually was. She spent a week just trying to understand how the tests were organized. I started poking around as well and found that that the existing tests were very poorly designed and had very little value.\n\nWe stopped adding more tests and instead spent a little bit of time understanding what the goal was for our test automation. As it turned out, the vendor tool could not do what we really needed it to do, so we cancelled the licenses and found an open source tool that met our needs.\n\nWe still had to spend time learning the new open source tool, but that investment would have been made if we’d stayed with the original vendor tool anyhow, be- cause no one on the team knew how to use the original tool.\n\nTest design skills have a huge impact on whether automation pays off right away. Poor practices produce tests that are hard to understand and maintain, and may produce hard-to-interpret results or false failures that take time to research. Teams with inadequate training and skills might decide the return on their automation investment isn’t worth their time.\n\nGood test design practices produce simple, well-designed, continually refac- tored, maintainable tests. Libraries of test modules and objects build up over time and make automating new tests quicker. See Chapter 14 for some hints on and guidelines for test design for automation.\n\nWe know it’s not easy to capture metrics. For example, trying to capture the time it takes to write and maintain automated tests versus the time it takes to run the same regression tests manually is almost impossible. Similarly, trying to capture how much it costs to ﬁx defects within minutes of introducing them versus how much it costs to ﬁnd and ﬁx problems after the end of the iteration is also quite difﬁcult. Many teams don’t make the effort to track this information. Without numbers showing that automating requires less effort and provides more value, it’s harder for teams to convince management that an investment in automation is worthwhile. A lack of metrics that demon- strate automation’s return on investment also makes it harder to change a team’s habits.\n\n—Janet\n\nIn Chapter 14, “An Agile Test Automa- tion Strategy,” we’ll look at ways to organize auto- mated tests.\n\nBARRIERS TO AUTOMATION—THINGS THAT GET IN THE WAY\n\nCode that’s Always in Flux\n\nAutomating tests through the user interface is tricky, because UIs tend to change frequently during development. That’s one reason that simple record and playback techniques are rarely a good choice for an agile project.\n\nIf the team is struggling to produce a good design on the underlying business logic and database access, and major rework is done frequently, it might be hard to keep up even with tests automated behind the GUI at the API level. If little thought is given to testing while designing the system, it might be difﬁ- cult and expensive to ﬁnd a way to automate tests. The programmers and testers need to work together to get a testable application.\n\nAlthough the actual code and implementation, like the GUI, tends to change frequently in agile development, the intent of code rarely changes. Organiz- ing test code by the application’s intent, rather than by its implementation, allows you to keep up with development.\n\nLegacy Code\n\nIn our experience, it’s much easier to get traction on automation if you’re writ- ing brand new code in an architecture designed with testing in mind. Writing tests for existing code that has few or no tests is a daunting task at best. It seems virtually impossible to a team new to agile and new to test automation.\n\nIt is sometimes a Catch-22. You want to automate tests so you can refactor some of the legacy code, but the legacy code isn’t designed for testability, so it is hard to automate tests even at the unit level.\n\nIf your team faces this type of challenge and doesn’t budget plenty of time to brainstorm about how to tackle it, it’ll be tough to start automating tests ef- fectively. Chapter 14 gives strategies to address these issues.\n\nFear\n\nTest automation is scary to those who’ve never mastered it, and even to some who have. Programmers may be good at writing production code, but they might not be very experienced at writing automated tests. Testers may not have a strong programming background, and they don’t trust their potential test automation skills.\n\nNon-programming testers have often gotten the message that they have nothing to offer in the agile world. We believe otherwise. No individual tester\n\n269\n\n270\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nshould need to worry about how to do automation. It’s a team problem, and there are usually plenty of programmers on the team who can help. The trick is to embrace learning new ideas. Take one day at a time.\n\nOld Habits\n\nWhen iterations don’t proceed smoothly and the team can’t complete all of the programming and testing tasks by the end of an iteration, team members may panic. We’ve observed that when people go into panic mode, they fall into comfortable old habits, even if those habits never produced good results.\n\nSo we may say, “We are supposed to deliver on February 1. If we want to meet that date, we don’t have time to automate any tests. We’ll have to do whatever manual tests can be done in that amount of time and hope for the best. We can always automate the tests later.”\n\nThis is the road to perdition. Some manual tests can get done, but maybe not the important manual exploratory tests that would have found the bug that cost the company hundreds of thousands of dollars in lost sales. Then, be- cause we didn’t ﬁnish with our test automation tasks, those tasks carry over to the next iteration, reducing the amount of business value we can deliver. As iterations proceed, the situation continues to deteriorate.\n\nSee Chapter 3, “Cultural Chal- lenges,” for some ideas on making changes to the team culture in order to facilitate agile practices.\n\nCAN WE OVERCOME THESE BARRIERS? The agile whole-team approach is the foundation to overcoming automa- tion challenges. Programmers who are new to agile are probably used to be- ing rewarded for delivering code, whether it’s buggy or not, as long as they meet deadlines. Test-driven development is oriented more toward design than testing, so business-facing tests may still not enter their consciousness. It takes leadership and a team commitment to quality to get everyone think- ing about how to write, use, and run both technology-facing and business- facing tests. Getting the whole team involved in test automation may be a cultural challenge.\n\nIn the next chapter, we show how to use agile values and principles to over- come some of the problems we’ve described in this chapter.\n\nSUMMARY\n\nSUMMARY In this chapter, we analyzed some important factors related to test automation:\n\n(cid:2) We need automation to provide a safety net, provide us with essential feedback, keep technical debt to a minimum, and help drive coding. (cid:2) Fear, lack of knowledge, negative past experiences with automation, rapidly changing code, and legacy code are among the common bar- riers to automation.\n\n(cid:2) Automating regression tests, running them in an automated build\n\nprocess, and ﬁxing root causes of defects reduces technical debt and permits growth of solid code.\n\n(cid:2) Automating regression tests and tedious manual tasks frees the team\n\nfor more important work, such as exploratory testing.\n\n(cid:2) Teams with automated tests and automated build processes enjoy a\n\nmore stable velocity.\n\n(cid:2) Without automated regression tests, manual regression testing will continue to grow in scope and eventually may simply be ignored. (cid:2) Team culture and history may make it harder for programmers to pri- oritize automation of business-facing tests than coding new features. Using agile principles and values helps the whole team overcome bar- riers to test automation.\n\n271\n\nThis page intentionally left blank",
      "page_number": 307
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 315-322)",
      "start_page": 315,
      "end_page": 322,
      "detection_method": "topic_boundary",
      "content": "Chapter 14\n\nAN AGILE TEST AUTOMATION STRATEGY\n\nAutomation Test Categories\n\nTest Pyramid\n\nQuadrants\n\nOrganizing Tests\n\nOrganizing Test Results\n\nManaging Automated Tests\n\nConinuous Integration, Builds, and Deploys\n\nUnit and Component Tests\n\nImplementing Automation\n\nAPI or Web Services\n\nBehind the GUI\n\nIdentifying Requirements\n\nWhat Can We Automate?\n\nGUI\n\nLoad\n\nOne Tool at a Time\n\nChoosing Tools\n\nEvaluating Automation Tools\n\nComparisons\n\nRepetitive Tasks\n\nAgile-Friendly Tools\n\nData Creation or Setup\n\nDeveloping an Automation Strategy\n\nData Generation Tools\n\nUsability\n\nAvoid Database Access\n\nWhen Database Access Is Unavoidable\n\nSupplying Data for Tests\n\nWhat Shouldn’t We Automate?\n\nExploratory\n\nTests That Will Never Fail\n\nUnderstand Your Needs\n\nOne-Off Tests\n\nKeep It Simple\n\nIterative Feedback\n\nWhat Might Be Hard to Automate?\n\nWhole-Team Approach\n\nTake Time to Do It Right\n\nApply Agile Principles to Automation\n\nWhat Hurts the Most?\n\nLearn by Doing\n\nApplying Agile Coding Practices\n\nDeveloping a Strategy—Where Do We Start?\n\nMulti-Layered Approach\n\nTest Design and Maintenance\n\nChoosing the Right Tools\n\n273\n\n274\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nAs we explored each of the Agile Testing Quadrants in Part III, we gave examples of tools that can help those different testing efforts succeed. Many of those tools are for automating tests. As we described in the previous chapter, teams face plenty of obstacles in their quest for successful test automation. Better tools become available all the time, but the trick is to choose the right tools and learn to use them effec- tively. Test automation requires thoughtful investment and incremental improve- ment. In this chapter, we explain how you can apply agile values and principles to get traction in starting or improving your automation efforts.\n\nAN AGILE APPROACH TO TEST AUTOMATION Here you are, reading this chapter on how to get your test automation strat- egy working, maybe hoping for that silver bullet, or an answer to all your questions. We hate to disappoint you, but we need to tell you right up front, there is no silver bullet. There is no one answer that works for every team. Don’t lose heart though, because we have some ideas to help you get started.\n\nFirst, we suggest approaching your automation problems as you would any problem. Deﬁne the problem you are trying to solve. To help you ﬁgure that out, we ﬁrst talk about some basics of test automation and reintroduce some terms.\n\nAutomation Test Categories\n\nIn Part III, we introduced the Agile Testing Quadrants and talked about each quadrant and the purpose of the tests in each quadrant. In this section, we look at the quadrants in a different light. Let’s look carefully at the quadrants (see Figure 14-1).\n\nYou can see that we’ve labeled both quadrants that support the team (Q1 and Q2) as using automation. In Quadrant 4, the tools used for critiquing the product from a technology point of view also usually require automated tools. In Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” we discussed some of the tools that can be used for automating business-facing tests in the quest for supporting the team. In fact, the only quadrant that is not labeled as using automation is Quadrant 3—the business-facing tests that cri- tique the product. However, as we discussed in Chapter 10, “Business-Facing Tests that Critique the Product,” tools may be useful for some of that testing. For example, automation can help set up test data and user scenarios, and ana- lyze logged activity.\n\nSee Chapter 8, “Business-Facing Tests that Support the Team,” for more about Wiz- ard of Oz testing.\n\nAN AGILE APPROACH TO TEST AUTOMATION\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ2 Q3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 14-1 Agile Testing Quadrants\n\nUse the quadrants to help you identify the different types of automation tools you might need for each project, even for each iteration. We ﬁnd it helpful to go through each quadrant and make a checklist of what tools might be needed. Let’s say we’re about to redesign a UI. We look at Quadrant 1. How can it be coded test-ﬁrst? Do we know how to unit test our presentation layer? Do we need a new tool to help with that? Now on to Quadrant 2. We’ll need to do some prototyping; should we just use paper, or should we plan a Wizard of Oz type activity? What tool will we use to create executable business-facing tests to guide development? Do we have regression test scripts that will need updating or replacing? We know that one of our Quadrant 3 activities will be usability testing. That takes some advance planning. We might want tools to help track the users’ activities so we can analyze them further. Thinking about Quadrant 4, we realize that we have load test scripts that use the old UI, so we have to budget time to update them for the new one.\n\n275\n\n276\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nAs we emphasized in Part III, “Using the Agile Testing Quadrants,” the order of quadrants doesn’t relate to the order in which we do the testing. As we make our checklist of tools needed for each type of test, we think about when we want to test so we know when to have our automation tools ready. For ex- ample, a team designing a new architecture would plan to do a spike and run scalability test against it as soon as possible. They will need to spend time during the ﬁrst iteration of the project ﬁnding and implementing a perfor- mance test tool.\n\nThe quadrants help us ﬁgure out what tools we might need, but with so many different automation options at different levels, a strategy for where to do which types of testing and how to organize the tests is essential. To deliver value quickly and often, our automation efforts need a high ROI. The test pyramid helps us optimize our test investment.\n\nTest Automation Pyramid\n\nFigure 14-2 illustrates the “test automation pyramid.” We like the version that Mike Cohn introduced, which shows the foundation layer made up of technology-facing unit and component tests. We recognize that many teams will struggle with this idea, because it seems the opposite of what many teams currently have. Many test teams have been taught the “V” model of testing, where activities such as component, system, and release testing are done in sequence after coding activities. Other teams have an inverted pyra- mid, with the majority of the tests in the functional or presentation layer.\n\nThe agile test automation pyramid shows three different layers of automated tests. The lowest tier is the foundation that supports all of the rest. It’s mainly made up of robust unit tests and component tests, the technology-facing tests that support the team. This layer represents the bulk of the automated tests. They’re generally written in the same language as the system under test, using the xUnit family of tools. After a team has mastered the art of TDD, these tests are by far the quickest and least expensive to write. They provide the quickest feedback, too, making them highly valuable. They have the big- gest ROI by far of any type of test.\n\nSee Chapter 7, “Technology- Facing Tests that Support the Team” for more about unit and compo- nent tests.\n\nIn agile development, we try to push as many tests as possible to this layer. While business-facing tests tend to go in one of the higher levels, we imple- ment them at the unit level when it makes sense. If they’re tests the customers don’t have to be able to read, and they can be coded much more quickly as unit tests, it’s a good option. Other types of technology-facing tests such as performance tests may also be possible at the unit level.\n\nSee Chapter, 8, \"Business-Facing Tests that Support the Team,\" for more about business-facing tests that support the team.\n\nAN AGILE APPROACH TO TEST AUTOMATION\n\nManual Tests\n\nGUI Tests\n\nAcceptance Tests (API Layer)\n\nUnit Tests/Component Tests\n\nFigure 14-2 Test automation pyramid\n\nThe middle tier in the pyramid is the layer that includes most of the auto- mated business-facing tests written to support the team. These are the func- tional tests that verify that we are “building the right thing.” The tests in this layer may include “story” tests, “acceptance” tests, and tests that cover larger sets of functionality than the unit test layer. These tests operate at the API level or “behind the GUI,” testing the functionality directly without going through the GUI. We write test cases that set up inputs and ﬁxtures that feed the inputs into the production code, accept the outputs, and compare them to expected results. Because these tests bypass the presentation layer, they are less expensive to write and maintain than tests that use the interface.\n\nWe try to write them in a domain-speciﬁc language that the customers can understand, so they take more work than unit-level tests. They also generally run more slowly, because each test covers more ground than a unit test and may access the database or other components. The feedback they provide is not as quick as the unit-level tests, but it is still much faster than we could get operating through the user interface. Therefore, their ROI is not as high as the tests that form the base of the pyramid, but it’s higher than the top layer.\n\n277\n\n278\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nWe have more about these tests in Chapter 8, “Business-Facing Tests that Support the Team,” and Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” where we discuss the business-facing tests that support the team and the tools that effec- tively capture these tests.\n\nFit and FitNesse are examples of tools used for the middle layer of the pyra- mid. Home-grown test harnesses that use spreadsheets or other business- friendly means for deﬁning test cases are also common.\n\nThe top tier represents what should be the smallest automation effort, be- cause the tests generally provide the lowest ROI. These tests are the ones done through the GUI, the ones that actually operate and manipulate the presentation layer. They are written after the code is completed, and so are usually written to critique the product and go directly to the regression suite.\n\nThese tests are traditionally more expensive to write, although there are new tools that help reduce the investment needed. Because components of the user interface tend to be changed often, these tests are much more brittle than tests that work at a functional or unit level. For example, just renaming HTML elements could cause a test script to fail. Operating through the user interface also slows these tests down, compared to tests in the lower levels of the pyramid that operate directly on production code. The tests in the top layer do provide important feedback, but a suite of GUI tests may take hours to run rather than the few minutes required for unit-level test suites. We want to minimize the number of tests at this layer, so they should only form the tip of the pyramid.\n\nNo matter how many automated tests they have, most systems also need man- ual testing activities, such as exploratory testing and user acceptance testing. We don’t want to forget these, so we’ve illustrated them with the little cloud at the tip of the pyramid. The bulk of our regression testing must be automated or our manual testing won’t give us a good return on investment either.\n\nPatrick Wilson-Welsh [2008] adds a descriptive dimension to the test auto- mation pyramid with a “three little pigs” metaphor. The bottom foundation layer is made of bricks. The tests are solid, and not vulnerable to the hufﬁng and pufﬁng of the Big Bad Wolf. The middle layer is made of sticks. They need rearranging more often than the brick layer to stay strong. The tests in the top layer are made of straw. It’s hard to get them to stay in place, and the wolf can easily blow them around. If we have too many tests made out of straw, we’re going to spend lots of time putting them back into shape.\n\nMost new agile teams don’t start with this shape pyramid—it’s usually in- verted, a left-over from previous projects. GUI test tools are often easier to learn, so teams start out with a lot of tests in their top “straw” layer. As we mentioned in the previous chapter, the “hump of pain” that most program- mers have to overcome to master unit test automation means that the team\n\nSee the bibliogra- phy for a link to Patrick Wilson- Welsh’s discussion of “ﬂipping the test pyramid” right-side up.\n\nWHAT CAN WE AUTOMATE?\n\nmay start out with only a few bricks. The ﬁxtures that automate functional tests in the middle layer are easy to write if the system is designed with those tests in mind, so the sticks might pile up faster than the bricks. As teams mas- ter TDD and unit test automation, the bottom layer starts to grow. When they get traction, a team using TDD will quickly build out the brick foundation of the test pyramid.\n\nThe testing pyramid is a good place to start looking at how test automation can help an agile team. Programmers tend to focus on the bottom of the pyr- amid, and they need plenty of time and training to get over the “hump of pain” and get to the point where TDD is natural and quick. In traditional teams, testers usually have no choice but to automate tests at the GUI level. The whole-team approach used by agile teams means that testers pair with programmers and help them get better at writing tests, which in turn solidi- ﬁes that brick foundation layer of the pyramid. Because tests drive develop- ment, the whole team is always designing for maximum testability, and the pyramid can grow to the right shape.\n\nProgrammers pair with testers to automate functional-level tests, ﬁlling out the middle layer. For example, a tester and customer may prepare a 400-row spreadsheet of test cases for a web services application. The programmer can help ﬁgure out a way to automate those tests. Different team members may have expertise in areas such as generating test data or using tools such as Ex- cel macros, and all that knowledge spreads around the team. Working to- gether, the team ﬁnds the best combinations of tools, test cases, and test data.\n\nInvolving the programmers in ﬁnding cost-effective ways to automate the top-level GUI tests has multiple beneﬁts. These efforts may give program- mers a better understanding of the system’s “big picture,” and testers can learn how to create more pliable, less straw-like GUI tests.\n\nThe more a team can work together and share knowledge, the stronger the team, the application, and the tests will become. The Big Bad Wolf won’t stand a chance. Let’s start by looking at what kind of tests we can automate and then at what we shouldn’t even try.\n\nWHAT CAN WE AUTOMATE? Most types of testing you can think of beneﬁt from automation. Manual unit tests don’t go far toward preventing regression failures, because performing a suite of manual tests before every check-in just isn’t practical. You can’t de- sign code test-ﬁrst through manual unit tests either. When programmers\n\n279\n\n280\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\ncan’t run tests quickly at the touch of a button, they may not be motivated enough to run tests at all. We could manually test that different units of code work together correctly, but automated component tests are a much more ef- fective safety net.\n\nManual exploratory testing is an effective way to ﬁnd functional defects, but if we don’t have enough automated business-facing regression tests, we prob- ably spend all of our time madly trying to keep up with manual regression testing. Let’s talk about all of the different kinds of testing that can be done well with automation.\n\nTo run automated tests, you need some kind of automated framework that allows programmers to check in code often, run tests on that code, and create deployable ﬁles. Let’s consider this ﬁrst.\n\nContinuous Integration, Builds, and Deploys\n\nSee Chapter 7, “Technology- Facing Tests that Support the Team,” for examples of build automation tools.\n\nAny tedious or repetitive task involved in developing software is a candidate for automation. We’ve talked about the importance of an automated build process. You can’t build your automated test pyramid without this. Your team needs the immediate feedback from the unit-level tests to stay on track. Getting automated build emails listing every change checked in is a big help to testers because they know when a build is ready to test without having to bother the programmers.\n\nPeril: Waiting for Tuesday’s Build\n\nIn a traditional environment, it is normal for testers to wait for a stable build, even if that means waiting until next Tuesday. In an agile environment, if testers don’t keep up with the developers, the stories get tested late in the game. If the developers don’t get the feedback, such as suggestions and bugs, the testers can lose credibility with the developers. Bugs won’t be discovered un- til the developers are already on another story and do not want to be inter- rupted to ﬁx them until later.\n\nBugs pile up, and automation suffers because it can’t be completed. Velocity is affected because a story cannot be marked “done” until it is tested. This makes it harder to plan the next iteration. At the end of the release cycle, your story testing runs into the end game and you may not have a successful release. At the very least, you will have a stressful release.",
      "page_number": 315
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 323-330)",
      "start_page": 323,
      "end_page": 330,
      "detection_method": "topic_boundary",
      "content": "WHAT CAN WE AUTOMATE?\n\nAn automated deployment process also speeds up testing and reduces errors. In fact, the day Janet was editing this chapter, she messed up the deployment because it was a manual process. It was pretty simple, but she was new to the project and moved the ﬁle to the wrong place. Getting an automated deploy- ment process in place went on Janet’s list of things to get done right away. Lisa’s team implemented its continuous integration and build framework ﬁrst thing, and found it fairly easy and quick to do, although it requires con- tinual care and feeding. Other teams, especially those with large, complex systems, face much bigger hurdles.\n\nWe’ve talked with teams who had build times of two hours or more. This meant that a programmer would have to wait for two hours after checking in code to get validation that his check-in didn’t break any preexisting function- ality. That is a long time to wait.\n\nMost agile teams ﬁnd an ongoing build longer than eight to ten minutes to be unworkable. Even 15 minutes is much too long to wait for feedback, be- cause check-ins will start stacking up, and testers will wait a long time to get the latest, greatest build. Can you imagine how the developers working with a build that takes two hours feel as they approach the end of an iteration or release cycle? If they break any functionality, they’ll have to wait two more hours to learn whether or not they had ﬁxed it.\n\nMany times, long builds are the result of accessing the database or trying to test through the interface. Thousands of tests running against a large code- base can tax the resources of the machine running the build. Do some pro- ﬁling of your tests and see where the bottleneck is. For example, if it is the database access that is causing most of the problems, try mocking out the real database and use an in-memory one instead. Conﬁgure the build pro- cess to distribute tests across several machines. See if different software could help manage resources better. Bring in experts from outside your team to help if needed.\n\nThe key to speeding up a continuous integration and build process is to take one small step at a time. Introduce changes one at a time so that you can measure each success separately and know you are on the right track. To start with, you may want to simply remove the most costly (in terms of time) tests to run nightly instead of on every build.\n\nA fast-running continuous integration and build process gives the greatest ROI of any automation effort. It’s the ﬁrst thing every team needs to automate.\n\n281\n\n282\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nSee the bibliogra- phy for links to build automation tools and books with more infor- mation about im- proving the build process.\n\nChapter 7, “Technology- Facing Tests that Support the Team,” goes into detail about some of the tools that can be used.\n\nWhen it’s in place, the team has a way to get quick feedback from the auto- mated tests. Next, we look at different types of tests that should be automated.\n\nUnit and Component Tests\n\nWe can’t overemphasize the importance of automating the unit tests. If your programmers are using TDD as a mechanism to write their tests, then they are not only creating a great regression suite, but they are using them to de- sign high-quality, robust code. If your team is not automating unit tests, its chances of long-term success are slim. Make unit-level test automation and continuous integration your ﬁrst priority.\n\nAPI or Web Services Testing\n\nTesting an API or web services application is easiest using some form of auto- mation. Janet has been on teams that have successfully used Ruby to read in a spreadsheet with all of the permutations and combinations of input variables and compare the outputs with the expected results stored in the spread- sheets. These data-driven tests are easy to write and maintain.\n\nOne customer of Janet’s used Ruby’s IRB (Interactive Ruby Shell) feature to test the web services for acceptance tests. The team was willing to share its scripts with the customer team, but the business testers preferred to watch to see what happened if inputs were changed on the ﬂy. Running tests interac- tively in a semiautomated manner allowed that.\n\nTesting behind the GUI\n\nSee Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” for speciﬁc tool examples.\n\nTesting behind the GUI is easier to automate than testing the GUI itself. Be- cause the tests aren’t affected by changes to the presentation layer and work on more stable business logic code, they’re more stable. Tools for this type of testing typically provide for writing tests in a declarative format, using tables or spreadsheets. The ﬁxtures that get the production code to operate on the test inputs and return the results can generally be written quickly. This is a prime area for writing business-facing tests, understandable to both custom- ers and developers that drive development.\n\nTesting the GUI\n\nEven a thin GUI with little or no business logic needs to be tested. The fast pace of agile development, delivering new functionality each iteration, man- dates some automated regression tests at the GUI level for most projects.\n\nSee Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” for ex- amples of GUI test frameworks.\n\nSee Chapter 11, “Critiquing the Product Using Technology- Facing Tests,” for examples of load test automation tools.\n\nRead more about source code man- agement tools and IDEs in Chapter 7, “Technology- Facing Tests that Support the Team.”\n\nWHAT CAN WE AUTOMATE?\n\nTool selection is key for successful GUI automation. The automated scripts need to be ﬂexible and easy to maintain. Janet has used Ruby and Watir very successfully when the framework was developed using good coding practices, just as if it were a production application. Time was put into developing the libraries so that there was not a lot of rework or duplication in the code, and changes needed could be made in one place. Making the code easy to main- tain increased the ROI on these tests.\n\nA point about testability here—make sure the programmers name their ob- jects or assign IDs to them. If they rely on system-generated identiﬁers, then every time a new object is added to the page, the IDs will change, requiring changes to the tests.\n\nKeep the tests to just the actual interface. Check things like making sure the buttons really work and do what they are supposed to. Don’t try to try to test business functionality. Other types of tests that can be automated easily are link checkers. There is no need for someone to manually go through every link on every page to make sure they hit the right page. Look for the low- hanging fruit, automate the things that are simple to automate ﬁrst, and you’ll have more time for the bigger challenges.\n\nLoad Tests\n\nSome types of testing can’t be done without automation. Manual load tests aren’t usually feasible or accurate, although we’ve all tried it at one time or another. Performance testing requires both monitoring tools and a way to drive actions in the system under test. You can’t generate a high-volume at- tack to verify whether a website can be hacked or can handle a large load without some tool framework.\n\nComparisons\n\nVisually checking an ASCII ﬁle output by a system process is much easier if you ﬁrst parse the ﬁle and display it in a human-readable format. A script to compare output ﬁles to make sure no unintentional changes were made is a lot faster and more accurate than trying to compare them manually. File comparison tools abound, ranging from the free diff to proprietary tools such as WinDiff. Source code management tools, and IDEs have their own built-in comparison tools. These are essential items in every tester’s toolbox. Don’t forget about creating scripts for comparing database tables when do- ing testing for your data warehouse or data migration projects.\n\n283\n\n284\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nRepetitive Tasks\n\nAs we work with our customers to better understand the business and learn what’s valuable to them, we might see opportunities to automate some of their tasks. Lisa’s company needed to mail several forms with a cover letter to all of their clients. The programmers could not only generate the forms but could also concatenate them with the cover letter and greatly speed up the mailing effort. Lisa’s fellow tester, Mike Busse, wrote a spreadsheet macro to do complex calculations for allocating funds that the retirement plan admin- istrators had been doing manually. A lot of manual checklists can be replaced with an automated script. Automation isn’t just for testing.\n\nData Creation or Setup\n\nAnother useful area for automation is data creation or setup. If you are con- stantly setting up your data, automate the process. Often, we need to repeat something multiple times to be able to recreate a bug. If that can be auto- mated, you will be guaranteed to have the same results each time.\n\nLisa’s Story\n\nMany of our test schemas, including the ones used for automated regression suites, use canonical data. This canonical or “seed” data was originally taken from production. Some tables in the database, such as lookup tables, don’t change, so they never need to be refreshed with a new copy. Other tables, such as those containing retirement plan, employee, and transaction information, need to start from Ground Zero whenever a regression suite runs.\n\nOur database developer wrote a stored procedure to refresh each test schema from the “seed” schema. We testers may specify the tables we want refreshed in a special table called REFRESH_TABLE_LIST. We have an ant target for each test schema to run the stored procedure that refreshes the data. The automated builds use this target, but we use it ourselves whenever we want to clean up our test schema and start over.\n\nMany of our regression tests create their own data on top of the “seed” data. Our Watir tests create all of the data they need and include logic that makes them re- runnable no matter what data is present. For example, the script that tests an em- ployee requesting a loan from his or her retirement plan ﬁrst cancels any existing loans so a new one can be taken out.\n\nFitNesse tests that test the database layer also create their own data. We use a special schema where we have removed most constraints, so we don’t have to add every column of every table. The tests only add the data that’s pertinent to the functionality being tested. Each test tears down the data it created, so subse- quent tests aren’t affected, and each test is independent and rerunnable.\n\n—Lisa\n\nWe discuss some logging and moni- toring tools in Chapter 10, “Business-Facing Tests that Critique the Product.”\n\nJanet’s Story\n\nWHAT SHOULDN’T WE AUTOMATE?\n\nCleaning up test data is as important as generating it. Your data creation toolkit should include ways to tear down the test data so it doesn’t affect a different test or prevent rerunning the same test.\n\nWe’ve looked at major areas where automation is required or at least useful. Our opinion is that whenever you need to do a test or some testing-related activity, ﬁrst decide whether it can be aided by automation. In some cases, automation won’t be appropriate. Let’s look at some of those.\n\nWHAT SHOULDN’T WE AUTOMATE? Some testing needs human eyes, ears, and intelligence. Usability and explor- atory testing are two that fall into that category. Other tests that may not jus- tify the automation investment are one-off tests and those that will never fail.\n\nUsability Testing\n\nReal usability testing requires someone to actually use the software. Automa- tion might be helpful in setting up scenarios to subsequently examine for us- ability. Observing users in action, debrieﬁng them on their experiences, and judging the results is a job for a person who understands that usability as- pects of software cannot be automated. Logging user actions is helpful for usability testing.\n\nWe had evaluated several GUI tools but decided to use Ruby with Watir. We kept our tests limited to GUI functions only. One of our tests was checking to make sure that correct validation messages were displaying on the screen. I was running the tests and happened to be watching the screen because I hadn’t seen this particu- lar test that one of the other testers created. My eyes caught something weird, but the test passed, so I replayed it again. One of the programmers had added a “$” to the screen, and the error message was displayed offset because of it. The correct message was displayed, just not in the right place. In this instance, the value in watching the tests run was huge because we were preparing to release fairly soon, and we probably wouldn’t have caught that particular problem.\n\nIt is possible to automate tests that make sure the GUI never changes, but you need to ask yourself whether it’s worth the cost. Do you really care that a but- ton has changed positions by one pixel? Do the results justify the effort? We don’t think you should automate “look and feel” testing, because an automated\n\n285\n\n—Janet\n\n286\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nscript can only look for what you tell it to see. Automation would miss visual problems that would jump out at a human.\n\nExploratory Testing\n\nSee Chapter 10, “Business-Facing Tests that Critique the Product,” for more on explor- atory testing and tools that can fa- cilitate it.\n\nSimilarly, exploratory testing may be speeded up with scripts to create test data and jump through some setup steps, but it requires a skilled tester to de- sign and execute the tests. One major goal of exploratory testing is to learn more about the product by doing, and then use that information to improve future development. Automated scripts won’t do that for you. However, as we’ve said before, you won’t have time for exploratory testing without a lot of other automated tests.\n\nTests that Will Never Fail\n\nWe’ve heard an argument that tests that will never fail don’t need to be auto- mated. If a requirement is so obvious that there’s only one way to implement it, and no programmer will ever look at that code later without knowing ex- actly what it should do, the chances of someone introducing a defect in that code are next to nothing. Let’s say we have a form with address ﬁelds. Do we need an automated regression test to verify that the second street address line is not required? After we’ve veriﬁed it manually, how likely is it that someone will accidentally change it to a required ﬁeld later? Even if someone did, it wouldn’t be a catastrophic event. Someone else would notice it and people could work around it easily until it was ﬁxed.\n\nThen again, a test for it would be easy to include. And programmer tricks such as copy/paste errors happen all the time. If you feel comfortable that one-time manual testing does the job and that the risk of future failures doesn’t justify automating regression tests, don’t automate them. If your de- cision turns out to be wrong, you’ll get another chance to automate them later. If you aren’t sure, and it’s not terribly difﬁcult to automate, go for it.\n\nSee Chapter 18, “Coding and Test- ing,” for more about risk analysis and how it relates to testing.\n\nIf you’re testing a life-critical system, even a very small risk of a regression failure is too much. Use risk analysis to help decide what tests should be automated.\n\nOne-Off Tests\n\nMost times, manually executing a one-off test is sufﬁcient. If automating a test doesn’t have payoff, why do it? Sometimes automation is worth doing for a one-off test.\n\nLisa’s Story\n\nWHAT MIGHT BE HARD TO AUTOMATE?\n\nWe recently did a story to pop up a warning message dialog when posting a pay- roll, but the message should only come up during the ﬁrst two weeks of January. Automating a test for this functionality would require some way to simulate that the current date was between January 1 and January 15. That’s not terribly hard to do, but the consequences of a failure were fairly trivial, and we had more criti- cal stories to deliver that iteration. Automating that test at that time just didn’t have enough value to justify the cost, and the risk factor was low. We decided to test it manually.\n\nThere are other cases where doing a one-off test seems the most intuitive but automation is a better choice. We host sites for different business partners, and each one has unique content, look, and feel. Values in the database drive the correct behavior and content for each brand. Some of the data, such as fee schedules based on asset values and numbers of participants, are highly com- plex. It’s much easier and much more accurate to verify this data using FitNesse tests. We have a set of ﬁxtures that let us specify keys for the partner “brand” that we want to test. We can easily plug in the appropriate expected results from the spreadsheets that the business development staff creates for each new partner. These tests aren’t part of our regression suite. They’re used one time only to vali- date the new brand.\n\nTedious tasks may be worth automating, even if you don’t do them often. Weigh the automation cost against the amount of valuable time eaten up by manually doing the test. If it’s easy to do manually, and automating wouldn’t be quick, just keep it manual.\n\nWHAT MIGHT BE HARD TO AUTOMATE? When code isn’t written test-ﬁrst, or at least with test automation in mind, it’s much harder to automate. Older systems tend to fall into this category, but no doubt plenty of new code with the same untestable characteristics is still being produced.\n\nIf you’re faced with working on existing code that doesn’t already have auto- mated tests, you’re in for an uphill battle, but a winnable one. Legacy code may have I/O, database access, as well as business logic and presentation code intertwined. It may not be clear where to hook into the code to automate a test. How do you get started automating tests on such a system? You certainly can’t plan on automating everything below the GUI, because much of the logic is in the presentation layer.\n\n287\n\n—Lisa\n\n288\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nThere are at least a couple of different approaches that work well. The “hump of pain” we talked about in Chapter 13, “Why We Want to Automate Tests and What Holds Us Back,” is intimidating, but it can be overcome, and then test automation will become much easier. Michael Feathers’ Working Effectively With Legacy Code [2004] explains how to build a test harness around existing code bases and refactor them to accommodate automation. Even with legacy code, you can write tests to protect against introducing new problems. This approach can work even on systems that lack structure or aren’t object-oriented.\n\nChapter 7, “Technology- Facing Tests that Support the Team,“ goes into more detail about different agile approaches to legacy code.\n\nLisa’s team decided on a different but equally effective approach. The team members started ”strangling” the legacy code by writing all new features in a new test-friendly architecture. They’re gradually replacing all of the old code with code written test-ﬁrst. When they do work on old code to ﬁx bugs, or in the cases where the old code needs updating, they simply add unit tests for all of the code they change. A GUI smoke test suite covers the critical functions of the rest of the legacy system that has no unit tests.\n\nAs with any automation project, approach the hard-to-automate code one piece at a time, and address the highest risk areas ﬁrst. Solve the testability problem and ﬁnd a way to write unit-level tests. The effort will pay off.\n\nDEVELOPING AN AUTOMATION STRATEGY—WHERE DO WE START? A simple, step-by-step approach sounds incompatible with an automation strategy, but in agile testing we try to understand the problem ﬁrst. Deciding where and how to start with automation requires a bit of thought and discus- sion. As your team looks at testing challenges, you’ll need to consider where automation is appropriate. Before you start searching for a particular auto- mation tool, you’ll want to identify your requirements.\n\nYou need to understand what problem you are trying to solve. What are you trying to automate? For example, if you have no test automation of any kind, and you start by buying an expensive commercial test tool thinking it will au- tomate all your functional tests, you may be starting in the wrong place.\n\nWe suggest you start at the beginning. Look for your biggest gain. The biggest bang for the buck is deﬁnitely the unit tests that the programmers can do. In- stead of starting at the top of the test pyramid, you may want to start at the bottom, making sure that the basics are in place. You also need to consider",
      "page_number": 323
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 331-340)",
      "start_page": 331,
      "end_page": 340,
      "detection_method": "topic_boundary",
      "content": "DEVELOPING AN AUTOMATION STRATEGY—WHERE DO WE START?\n\nthe different types of tests you need to automate, and when you’ll need to have tools ready to use.\n\nIn this section, we assume you have automated Quadrant 1 unit and compo- nent tests in place, and are looking to automate your business-facing tests in Quadrants 2 and 3, or your Quadrant 4 technology-facing tests that critique the product. We’ll help you design a good strategy for building your automa- tion resources.\n\nThink about the skills and experience on your team. Who needs the automa- tion, and why? What goals are you trying to achieve? Understanding some of these issues may affect your choice of tools and what effort you expend. There is a section on evaluating tools at the end of this chapter.\n\nAutomation is scary, especially if you’re starting from scratch, so where do we begin?\n\nWhere Does It Hurt the Most?\n\nTo ﬁgure out where to focus your automation efforts next, ask your team, “What’s the greatest area of pain?” or, for some teams, “What’s the greatest area of boredom?” Can you even get code deployed in order to test it? Do team members feel conﬁdent about changing the code, or do they lack any safety net of automated tests? Maybe your team members are more ad- vanced, have mastered TDD, and have a full suite of unit tests. But they don’t have a good framework for specifying business-facing tests, or can’t quite get a handle on automating them. Perhaps you do have some GUI tests, but they’re extremely slow and are costing a lot to maintain.\n\nPeril: Trying to Test Everything Manually\n\nIf you’re spending all your time retesting features that you’ve tested before, not getting to new features, and needing to add more and more testing, you’re suffering from a severe lack of test automation. This peril means that testers don’t have time to participate in design and implementation discus- sions, regression bugs may creep in unnoticed, testing can’t keep up anymore with development, and testers get stuck in a rut. Developers aren’t getting in- volved in the business-facing testing, and testers don’t have time to ﬁgure out a better way to solve the testing problems.\n\nYour team can ﬁx this by developing an automation strategy, as we describe in this chapter. The team starts designing for testability and chooses and imple- ments appropriate automation tools. Testers get an opportunity to develop their technical skills.\n\n289\n\n290\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nChapter 18, “Cod- ing and Testing,” has more informa- tion on a simple approach to risk analysis.\n\nWherever it hurts the most, that’s the place to start your automation efforts. For example, if your team is struggling to even deliver deployable code, you need to implement an automated build process. Nothing’s worse than twid- dling your thumbs while you wait for some code to test.\n\nBut, if performance puts the existence of your organization in danger, per- formance testing has to be the top priority. It’s back to understanding what problem you are trying to solve. Risk analysis is your friend here.\n\nJanet’s Story\n\nI worked on a legacy system that was trying to address some quality issues as well as add new features for our main customer. There were no automated unit or functional tests for the existing application, but we needed to refactor the code to address the quality issues. The team members decided to tackle it one piece at a time. As they chose a chunk of functionality to refactor, the programmers wrote unit tests, made sure they passed, and then rewrote the code until the tests passed again. At the end of the refactoring, they had testable, well-written code and the tests to go with them. The testers wrote the higher-level functional tests at the same time. Within a year, most of the poor-quality legacy code had been re- written, and the team had achieved good test coverage just by tackling one chunk at a time.\n\nTest automation won’t pay off unless other good development practices are in place. Continuous integration running a robust suite of unit tests is a ﬁrst step toward automating other tests. Code that’s continually refactored for main- tainability and good design will help increase the ROI on automation. Refac- toring can’t happen without that good unit test coverage. These development practices also need to be applied to the automated functional test scripts.\n\nMulti-Layered Approach\n\nWhile we recommend mastering one tool at a time, don’t expect too much out of any one tool. Use the right tool for each need. The tool that works best for unit tests may or may not be appropriate to automate functional tests. GUI, load, performance, and security testing may each require a different tool or tools.\n\nMike Cohn’s test pyramid concept (see Figure 14-2) has helped our teams put their automation efforts where they do the most good. We want to maximize the tests that have the best ROI. If the system architecture is designed for test- ability, test automation will be less expensive, especially at the unit level.\n\n—Janet\n\nLisa’s Story\n\nDEVELOPING AN AUTOMATION STRATEGY—WHERE DO WE START?\n\nTests that go through the user interface usually have the lowest ROI, because they’re expensive to maintain, but we do need them. They make up the small tip of our pyramid We may choose to automate some of these tests, but the majority of GUI tests are deﬁned in business terms and probably are best left as human interaction tests (i.e., manual tests).\n\nThe middle layer represents the functional tests that work directly with produc- tion code, without a GUI or other layer in between. While they’re not as inex- pensive to automate as unit-level tests, and provide feedback a bit more slowly, the right tools allow them to have a good ROI. The fact that these tests can be written in a language the business experts understand adds to their value.\n\nThere are many different layers in the application that can be tested indepen- dently. In his book xUnit Test Patterns [2007], Gerard Meszaros refers to this as the Layer Test pattern. He cautions that when trying to test all of the layers of the application separately, we still have to verify that the layers are hooked up correctly, and this may require at least one test of the business logic through the presentation layer.\n\nAs my team built our automation framework one step at a time, we gathered an arsenal of tools. After implementing a continuous build framework with Ant and CruiseControl, we mastered JUnit for unit testing. We knew that unit test automa- tion is the quickest and cheapest way to automate, and provides the fastest feed- back to the programmers.\n\nOur legacy system had no automated tests, so we built a GUI regression test suite with Canoo WebTest. This provided good payback because the WebTest scripts were speciﬁed, not programmed. They were quick to write and easy to maintain.\n\nAfter JUnit and WebTest were in place, we experimented with FitNesse and found it worked well for functional testing behind the GUI. We found automating with FitNesse to go relatively quickly. Although FitNesse tests are signiﬁcantly more ex- pensive to produce and maintain than unit tests, their value in driving develop- ment and promoting collaboration among customers, programmers, and testers kept the ROI high.\n\nAll of these tools were easy to learn, implement, and integrate with the build pro- cess, and provided continual feedback about our regression issues. They were im- portant considerations when we were deciding on our test automation strategy.\n\nWhen evaluating the payback of your automation efforts, consider less tangi- ble aspects such as whether the tool promoted collaboration between the\n\n291\n\n—Lisa\n\n292\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\ntechnical and customer teams. A primary reason to write tests is to help guide development. If the process of writing your automated acceptance tests results in a thorough understanding of business requirements, that’s plenty of payback, even if the tests never ﬁnd a single regression bug later on.\n\nThink about Test Design and Maintenance\n\nThink about all of the manual test scripts you’ve written in your life. Don’t you just wish those all would have been automated? Wouldn’t your life have been a lot easier? We believe that all scripted tests should be automated. Let’s get started converting those manual scripted tests.\n\nAfter you get started, it can be quite easy to automate tests. For example, when you have a working FitNesse ﬁxture, adding more test cases requires little ef- fort. This is great when you have a lot of different permutations to test. You’ll probably test more conditions than you would if all your testing was done manually. When Lisa’s team members rewrote their retirement plan’s loan sys- tem, they could test hundreds of different possibilities for loan payment pro- cessing via FitNesse tests. What happens when three loan payments are processed on the same day? If someone doesn’t make any payments for three months, and then sends in a large payment, is the interest calculated and ap- plied correctly? It was easy to write automated tests to ﬁnd out.\n\nThat’s a great advantage, but it has a down side. Now the team has dozens, or even hundreds, of test cases to maintain. What if the rules about calculating the amount of interest for a loan payment change a year from now? This could require updating every test. If your test tool doesn’t easily accommo- date making changes to existing tests, your big suite of automated tests can turn into a headache.\n\nEnd-to-end tests are particularly tricky to automate because they have the most potential to need maintenance as business rules change. How do we balance the need for automation with the cost?\n\nChapter 8, “Business-Facing Tests that Support the Team,” explains more about thin slices.\n\nTest Design Remember to start with the thin slice or steel thread of the feature you’re test- ing. Approach automation just as programmers approach coding. Get one small unit of the steel thread working, and then move on to the next. After you’ve covered the whole thin slice, go back and ﬂesh it out.\n\nSee Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” for more information about effective test design.\n\nDEVELOPING AN AUTOMATION STRATEGY—WHERE DO WE START?\n\nChoose your test pattern thoughtfully. Automate all of the test cases you need, but no more, and automate them at the lowest level that you can. Limit the scope of each test case to one test condition or one business rule. Under- stand the purpose of the test. Avoid dependencies between tests, because they quickly increase complexity and maintenance expense.\n\nConsider Options As we’ve mentioned before, the lower the level at which you automate a test, the better the ROI. Push test automation as far down the pyramid as you can. If you have good coverage in your unit and code integration tests, you don’t need to automate as many functional tests. With solid coverage at the lower levels, it might be enough to do end-to-end tests manually to verify the sys- tem’s behavior. Use risk analysis to help you decide.\n\nUser Interface The user interface does need to be tested. In some situations, test automation at the GUI level is critical. Perhaps your team is using third-party GUI con- trols, and you aren’t sure how they will behave. If your risk and ROI analysis supports a lot of automation at the GUI level, make the investment.\n\nIf you do automate at the higher levels, don’t go overboard and automate ev- ery possible path through the system. You don’t have to keep every auto- mated test created during the development phase in the regression suite; consider tradeoffs of build time and the chance of ﬁnding defects. Focus your efforts on covering every important path through the code at the unit, code integration, and functional levels. You’ll get a much better payback.\n\nStrike a Balance Striking a balance isn’t an agile principle, it’s just common sense. You need a good-enough solution right now, but it doesn’t have to be perfect. Does the tool provide the results you need right now? Does it provide an adequate re- turn on the resources needed to use it for automation? If so, go ahead and use it, and budget time later to look for alternatives. You can improve your auto- mation framework over time. The most important factor is whether your au- tomation tools ﬁt your particular situation right now.\n\nDon’t slide the other way, and think, “OK, we can generate a bunch of scripts with this record tool, get our immediate testing done, and refactor the scripts later to make them maintainable.” While you don’t need to keep searching for the perfectly ideal automation solution, you do need a solution that doesn’t\n\n293\n\n294\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nadd to your team’s technical debt. Find a balance between “It ﬁnds the bugs we need to know about and doesn’t cost too much to maintain” and “This is the most elegant and cool solution we can ﬁnd.”\n\nChoosing the Right Tools\n\nIt’s cool that we have so many tools available to help us solve our automation problems. Don’t go for more sophistication than you need. Lisa’s coworkers have found that a spreadsheet that retrieves data from the database and per- forms calculations independently of the system is a powerful tool, both for driving development and for verifying the application’s calculations.\n\nWe usually minimize test automation at the GUI layer, but there are situa- tions where more GUI automation is appropriate. If the user makes a change at X, what else changes? Some problems only manifest themselves at the GUI level. Lisa tested a bug ﬁx that addressed a back-end problem when retire- ment plan participants requested a distribution of money from their ac- counts. The change was surrounded by unit tests, but it was a GUI regression test that failed when the distribution form failed to pop up upon request. Nobody anticipated that a back-end change could affect the GUI, so they probably wouldn’t have bothered to test it manually. That’s why you need GUI regression tests, too.\n\nWe’ve talked about some disadvantages of record/playback tools, but they’re appropriate in the right situation. You may be using a record/playback tool for a good reason: Maybe your legacy code already has a suite of automated tests created in that tool, your team has a lot of expertise in the tool, or your management wants you to use it for whatever reason. You can use recorded scripts as a starting point, then break the scripts into modules, replace hard- coded data with parameters where appropriate, and assemble tests using the modules as building blocks. Even if you don’t have much programming ex- perience, it’s not hard to identify the blocks of script that should be in a mod- ule. Login, for example, is an obvious choice.\n\nRecord/playback may also be appropriate for legacy systems that are de- signed in such a way that makes unit testing difﬁcult and hand-scripting tests from scratch too costly. It’s possible to build a record and playback capability into the application, even a legacy application. With the right design, and the use of some human-readable format for the recorded interaction, it’s even possible to build playback tests before the code is built.\n\nDEVELOPING AN AUTOMATION STRATEGY—WHERE DO WE START?\n\nGUI Test Automation: From the Dark Ages to Successful Automation in an Agile Environment\n\nPierre Veragen, SQA Lead at iLevel by Weyerhaeuser, explains how his team used a tool that provided both record/playback and scripting capability pro- ductively in a waterfall environment, and then leveraged it when the com- pany adopted agile development.\n\nBack in our waterfall development days, in 2000, we started doing GUI test automation using a record-playback approach. We quickly accumu- lated tens of thousands of lines of recorded scripts that didn’t meet our testing needs. When I took over 18 months later, I quickly became con- vinced that the record-playback approach was for the dinosaurs.\n\nWhen we had a chance to obtain a new test automation tool at the end of 2003, we carefully evaluated tools with these criteria in mind: record capability to help us understand the scripting language, and the ability to build an object-oriented library to cover most of our needs, including test reporting. At the time, TestPartner from CompuWare fulﬁlled all of our requirements.\n\nWe started using TestPartner on a highly complex, CAD-with-engineering application, built in Visual Basic 6, still using a waterfall process. Before we started automating tests, our releases were quickly followed by one or more patches. We focused our automation efforts toward checking the engineering calculations through the GUI, and later, the actual posi- tion of the CAD details. These tests included hundreds of thousands of individual veriﬁcation points, which could never have been done by hand. Within a year, having added a solid set of manual tests of the user interaction, in addition to our automated tests, we were releasing robust software without the usual follow-up patches. We felt conﬁdent about our combination of manual and automated tests, which didn’t include a single line of recorded scripts.\n\nIn 2004, our group moved to Visual Basic .NET. I spent several months adapting our TestPartner library to activate .NET controls. In 2006, we adopted an Agile methodology. Building on lessons previously learned in the non-Agile world, we achieved astonishing results with test automa- tion. By the end of 2006, team members were able to produce maintain- able GUI test scripts and library components after just a few days of training. At the same time, the team embraced unit testing with NUnit and user acceptance tests with FitNesse.\n\nAs of this writing, issues are caught at all three levels of our automated testing: Unit, FitNesse, and GUI. The issues found by each of the three testing tiers are of a different nature. Because everything is automated and triggered automatically, issues are caught really fast, in true Agile fashion. Each part of our test automation is bringing value.\n\n295\n\n296\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nSome people feel resources would be better spent on architecture and design, so that GUI test automation isn’t needed. In our development group, each team made its own decision about whether to automate GUI tests.\n\nIn case you decide to use GUI test automation, here’s some advice: Stay away from recorded scripts, invest in maintainability, and minimize the required GUI testing with a good architecture of the application. It is my experience that investing in good GUI test automation practices will al- ways pay off.\n\nPierre’s advice reﬂects well how good development practices, especially those followed in agile development projects, apply to automated test develop- ment as well as to production code development.\n\nBuilt-In Record & Playback\n\nGerard Meszaros, agile coach and author of xUnit Test Patterns [2007], de- scribes a situation where the simplest approach turned out to be record/ playback. We’ve mentioned drawbacks to record/playback tools, but if you design your code to support them, they can be the best approach.\n\nI was asked to help a team that was porting a “safety sensitive” applica- tion from OS2 to Windows. The business was very concerned about the amount of time it would take it to retest the ported system and the like- lihood that the team would miss important bugs. The system was de- signed to only offer the user valid choices that would not compromise safety. They considered using a test recording tool to record tests on the old system and play them back on the new system, but there were no test recording tools available for both OS2 and Windows that could deal with windows drawn using ASCII characters. After reviewing the architec- ture of the system, we determined that writing xUnit tests would not be a cost-effective way to test the system because much of the business logic was embedded in the user interface logic, and refactoring the code to separate them would be too risky and time-consuming. Instead, we proposed building a Record & Playback test capability right into the system before we ported it.\n\nEven though the rest of the project was milestone-driven, we developed the built-in test mechanism in a very agile way. Each screen required at least one new hook and sometimes several. We started with the most frequently used screens, adding the necessary hooks to record the user’s actions and the systems responses to them into an XML ﬁle. We also added the hooks to play back the XML and determine the test results. Initially, we focused our efforts on proving the concept by hooking only\n\nSee more exam- ples of speciﬁc tools for business- facing tests in Chapter 9, “Tool- kit for Business- Facing Tests that Support the Team.”\n\nDEVELOPING AN AUTOMATION STRATEGY—WHERE DO WE START?\n\nthe screens we needed to record and then playing back a simple but re- alistic test. After everyone was convinced the approach would work, we prioritized the screens with respect to how much beneﬁt it would pro- vide. We implemented the hooks one by one until we could automate a signiﬁcant portion of the tests. We also built an XSLT stylesheet that would format the XML in a Fit-like way, with green cells indicating ac- ceptable results and red cells indicating a failed test step.\n\nIn the meantime, the client was identifying the test scenarios that needed test cases. As we ﬁnished enough screens to record a particular test, the client would “acceptance test” our hooks by recording and playing back (still on OS2) the test(s) that were waiting for those hooks. When all of the hooks were in place, we could go ahead and port the code, including the test hooks, from OS2 to Windows. After verifying successful playback on OS2, the client would move the XML test ﬁles over to Windows and run them against the ported version of the code. The client found this quite easy to do and was able to record a large number of tests in a relatively short period of time. Because the tests were recording actions and responses in business terms, the tests were fairly easy to understand. The client loved the capability, and still raves about how much effort it saved and how much more conﬁdence it has in the product. “Not only did this save tens of man-years of testing effort, but it even uncovered hidden unknown bugs in the legacy system, which we had considered to be the gold standard.”\n\nIn Gerard’s story, the team worked together to retroﬁt testability onto a sys- tem that wasn’t designed for testability. They gave their customers a way to capture their test scenarios on one platform and play them back on both platforms to verify the successful port. This is a stellar example of the whole- team approach. When everyone on the team collaborates on a test automa- tion solution, there’s a much better chance it’s going to succeed.\n\nSome agile teams get value from commercial or open source test tools, while others prefer a completely customized approach. Many testers ﬁnd value writ- ing simple scripts in a scripting language such as Ruby, or a shell, to automate mundane but necessary tasks, generate test data, or drive other tools. Books such as Everyday Scripting with Ruby for Teams, Testers, and You give a road- map for this approach. If you’re a tester without a strong programming back- ground, we encourage you to pick up a book, ﬁnd an online tutorial, or take a class on a scripting language, and see how easy it can be to write useful scripts.\n\nWhat we’re trying to tell you is that you can use many different tools. Look at the problem you are trying to solve and decide as a team the easiest and most effective way to solve it. Every so often, step back and take a look at the tools\n\n297\n\n298\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nyou’re using. Is everyone on the team happy with them? Are you missing prob- lems because you don’t have the right tools? Budget time to explore new tools and see if they might ﬁll gaps or replace a tool that isn’t paying off.\n\nIf your team is new to agile development, or working on a brand-new project, you might be faced with choosing tools and setting up test environments during the early iterations, when you also might be working on high-risk sto- ries. Don’t expect to be able to deliver much business value if you’re still cre- ating your test infrastructure. Plan in lots of time for evaluating tools, setting up build processes, and experimenting with different test approaches.\n\nAPPLYING AGILE PRINCIPLES TO TEST AUTOMATION Every team, every project, and every organization has a unique situation with unique automation challenges. Each has its own culture, history, resources, business pressures, products, and experience. No matter what your team’s situation, you can use the agile principles and values discussed in Chapter 2 to help you ﬁnd solutions. Concepts such as courage, feedback, simplicity, communication, continuous improvement, and responding to change aren’t just agile ideas—they’re qualities that are common to all successful teams.\n\nKeep It Simple\n\nThe agile maxim of “do the simplest thing that could possibly work” applies to tests as well as code. Keep the test design simple, keep the scope minimal, and use the simplest tool that will do the job.\n\nSimplicity is a core agile value for a good reason. The best place to start is the simplest approach you can think of. However, doing the simplest thing doesn’t mean doing the easiest thing. It involves really thinking about what you need now and taking baby steps to get there. By keeping things simple, if you do make a bad choice, you won’t go too far off track before realizing the error of your ways.\n\nIt’s easy to get involved in a task and slip away from the basics into some in- triguing challenge. Weigh the ROI of every automation task before you do it. Automation is fun (when you get past the scary part of getting started). It’s tempting to try something difﬁcult just because you can. Like all other as- pects of testing in an agile development project, the only way to keep up is to do only the minimum required.\n\nUse the simplest tool you can get away with. Remember the test pyramid. If a customer-facing test can be most easily automated at the unit level, do it",
      "page_number": 331
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 341-348)",
      "start_page": 341,
      "end_page": 348,
      "detection_method": "topic_boundary",
      "content": "Lisa’s Story\n\nAPPLYING AGILE PRINCIPLES TO TEST AUTOMATION\n\nthere. Lisa sometimes writes test cases in FitNesse, only to learn the program- mers can automate them much faster as JUnit tests. Conversely, sometimes the programmers use FitNesse for TDD instead of JUnit, because the code they’re writing lends itself to testing in one of the FitNesse ﬁxture formats.\n\nIterative Feedback\n\nShort iterations allow us to experiment with various automation approaches, evaluate results, and change course as quickly as needed. Commit to an auto- mation effort, such as developing a test framework in-house or implement- ing an open source tool for at least a couple of iterations. After each iteration, look at what’s working and what’s not working. Think of ideas to overcome problems, and try those in the next iteration. If it’s not the right solution, try something else for a few iterations. Don’t get sucked into a quagmire where you’ve put so many resources into a tool, and have so many tests that use it, that you feel you can’t switch tools. Between the many open source and com- mercial tools, plus programmers’ ability to write home-grown test tools, there’s no reason to settle for less than the optimum tool.\n\nOne of my early XP teams struggled to ﬁnd a good way to automate customer- facing acceptance tests for a Java-based web application. This was back when there were far fewer tool options for agile teams. First, we tried an open source tool that simulated a browser, but it lacked the features we required. It just wasn’t quite robust enough. We discussed this at the next retrospective.\n\nWe decided to try using the unit testing tool for testing behind the GUI for the next two iterations. By committing to two iterations, we felt we were giving our- selves enough time to give the tool a good try, but not so much time that we would have too much invested if it weren’t the right solution. The customers found the unit tests hard to read, and there was logic in the GUI we couldn’t test with this tool.\n\nAfter another discussion during our retrospective, we then committed to two iter- ations of using a vendor GUI test tool I had used extensively on previous projects. The Java programmers found it slow going because the tool used a proprietary scripting language, but it worked well enough to do the minimum automation needed. After two iterations, we decided that it wasn’t ideal, but at the time there weren’t a lot of other options, and it was the best one we had.\n\nIn hindsight, we should have kept looking for a better option. Perhaps we could have developed our own test harness. We were able to automate about 60% of the regression tests above the unit level using the vendor tool, which seemed great at the time. If we had pushed ourselves a little more, we might have done a lot better.\n\n299\n\n—Lisa\n\n300\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nUse iterations to your advantage. They facilitate a step-wise approach. If your idea’s a dud, you’ll know quickly and have a chance to try a different one. Don’t be afraid to keep looking, but don’t keep looking for the perfect solu- tion if one you try performs adequately.\n\nWhole-Team Approach\n\nAgile development can’t work without automation. Fortunately, the whole- team approach, which we explored in Chapter 1, means that a wider range of skills and resources are available to ﬁnd and implement a useful automation strategy. Attacking the problem as a team means it’s more likely that code will be designed for testability. Programmers, testers, and other team members will collaborate to automate tests, bringing multiple viewpoints and skill sets to the effort.\n\nThe whole-team approach helps overcome the fear barrier. Automation tasks can be overwhelming to start with. Knowing there are other people with dif- ferent skills and experience to help gives us courage. Being able to ask for and receive help gives us conﬁdence that we can achieve adequate coverage with our automated tests.\n\nLisa’s Story\n\nMy current team made a commitment to automating regression tests at all levels where it made sense. Here are some examples of where I’ve asked for help to suc- ceed with automation.\n\nEarly on, when we had no automated tests at all and the developers were trying to master test-driven development, we settled on Canoo WebTest for the GUI smoke tests. I needed a bit of help understanding how to conﬁgure WebTest to run in our environment, and I needed a lot of help to run the tests from the auto- mated build process. I asked our system administrator (who was also one of the programmers) to help. We quickly got a suite of tests running in the build.\n\nLater, I really wanted to try FitNesse for functional testing behind the GUI. I had to be patient while the programmers were still getting traction with the automated unit tests. The team agreed to try the tool, but it was hard to ﬁnd time to start us- ing it. I picked a story that seemed suited to FitNesse tests, and asked the pro- grammer working on the story if I could pair with him to try some FitNesse tests. He agreed, and we got some tests automated in FitNesse. The programmer found it easy and worthwhile, and gave a good report to the rest of the team.\n\nAfter that, it wasn’t hard to approach each programmer, suggest writing FitNesse tests for the story he was working on, and let him see the results. The FitNesse tests found test cases the programmer hadn’t thought of, and they saw the bene- ﬁt right away. When everyone on the team had some experience with the tool, they were not only happy to automate the tests, but started designing code in a way that would make writing FitNesse ﬁxtures easier.\n\nChapter 11, “Critiquing the Product Using Technology- Facing Tests,” talks about technology- facing tests such as these and differ- ent approaches to handling them.\n\nAPPLYING AGILE PRINCIPLES TO TEST AUTOMATION\n\nWhen our Ruby expert, who designed most of our Watir test suite, left the com- pany, I was quite concerned about maintaining our huge suite of tests as well as being able to code new ones. My Ruby expertise was not as good as his (plus, we were down to just one tester, so time was an issue). Every programmer on the team went out, bought a book on Ruby, and helped when I had problems updat- ing scripts to work when the code changed. One programmer even wrote a new script to test a new story when I didn’t have time for that task. When we hired a new tester, he and I were able to handle the care and feeding of the Watir scripts, so the programmers no longer needed to take on those tasks.\n\nI know I can ask teammates for help with automation issues, and the entire team sees automation as a priority, so the programmers always think about testability when designing the code. This is an example of the whole-team approach at work.\n\nSpecialized technology-facing tests such as security or load testing might re- quire bringing in experts from outside the team. Some companies have spe- cialist teams that are available as shared resources to product teams. Even while taking advantage of these resources, agile teams should still take re- sponsibility for making sure all types of testing are done. They may also be surprised to ﬁnd that team members may have the skills needed if they take a creative approach.\n\nSome organizations have independent test teams that do post-development testing. They may be testing to ensure the software integrates with other sys- tems, or conducting other specialized testing such as large-scale performance testing. Development teams should work closely with these other teams, us- ing feedback from all testing efforts to improve code design and facilitate automation.\n\nTaking the Time to Do It Right\n\nSolving problems and implementing good solutions takes time. We must help our management understand that without enough time to do things the right way, our technical debt will grow, and our velocity will slow. Implementing solutions the “right” way takes time up front but will save time in the long term. Consider the time it takes for brainstorming ideas, solutions, formal training, and for on-the-job learning.\n\nYour organization’s management is understandably interested in producing results as quickly as possible. If management is reluctant to give the team time to implement automation, explain the trade-offs clearly. Delivering some fea- tures in the short term without automated regression tests to make sure they\n\n301\n\n—Lisa\n\n302\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nkeep working will have a big cost down the line. As your team accumulates technical debt, you’ll be less able to deliver the business value management needs. Work toward a compromise. For example, cut the scope of a feature but keep the essential value, and use automation to deliver and maintain a better product.\n\nWe always have deadlines, and we always feel pressed for time. The tempta- tion to just go back to doing things the way we always have, like executing re- gression tests manually and hoping for the best, is always there even though we know that doesn’t work. There is never enough time to go back and ﬁx things. During your next planning meeting, budget time to make meaningful progress on your automation efforts.\n\nLisa’s Story\n\nOur team focuses on taking time for good design, a strong set of automated tests, and ample time for exploratory testing. Quality, not speed, has always been our goal. Our production problems cost a lot to ﬁx, so the whole company is on board to take the time to prevent them. Sometimes we don’t pick the right de- sign, and we aren’t afraid to rip it out and replace it when we realize it.\n\nNaturally there are business tradeoffs, and the business decides whether to pro- ceed with known risks. We work to explain all of the risks clearly and give exam- ples of potential scenarios.\n\nHere are a couple of recent examples of taking the time to do things right. We started a theme to make major changes to the account statements for the retire- ment plans. One of the programmers, Vince Palumbo, took on the task of collect- ing additional data to be used for the statements. He decided to write robust unit tests for the data collection functionality, even though this meant the story would have to continue on to the next iteration. Writing the unit tests took a great deal of time and effort, and even with the tests, the code was extremely complex and difﬁcult to do. A couple of iterations later, another programmer, Nanda Lankala- palli, picked up another story related to the data collection and was pleasantly surprised to ﬁnd new unit tests. He was able to make his changes quickly, and the testing effort was greatly reduced because the unit tests were in place.\n\nLater, we found we had missed an edge case where some calculations for the change in account value were incorrect. The combination of automated unit tests and a great deal of exploratory testing were not enough to catch all of the scenar- ios. Still, having the tests meant Vince could write his corrected code test-ﬁrst and feel more conﬁdent that the code was now correct.\n\nAnother recent example concerned processing of incoming checks. The business wanted to shorten the two-step process to one step, which meant the money would be invested in the retirement plan accounts two days earlier than was then possible. The existing process was all written in legacy code, without unit tests. We discussed whether to rewrite the processing in the new architecture. Our product owner was concerned about the amount of time this might take. We felt\n\nAPPLYING AGILE PRINCIPLES TO TEST AUTOMATION\n\nit would take just as long to change the existing code as to completely rewrite it, because the old code was difﬁcult to understand and had no unit tests at all. We decided on the rewrite, which not only reduced the risk of problems in this criti- cal functionality but also gave us the opportunity to provide a couple of extra fea- tures at little extra cost. So far, this strategy has proven worthwhile.\n\nAllow yourself to succeed. Work at a sustainable pace. Take the time to refac- tor as you go or you’ll end up with a mess eventually. As testers, we always have many different tasks to do. If you’re learning a new tool or trying to au- tomate new tests, don’t multitask. Find a big block of time and focus. This is hard, but switching gears constantly is harder.\n\nIf business stakeholders are impatient for your team to “just get it done,” ana- lyze the problem with them. What are the risks? How much will a production problem cost? What are the beneﬁts of releasing a quick hack? How much technical debt will it add? What’s the long-term return on investment of a solid design supported with automated tests? How will each approach affect company proﬁtability and customer satisfaction? What about the intangible costs, such as the effect that doing poor-quality work has on team morale? Sometimes the business will be right, but we’re betting that you’ll usually ﬁnd that up-front investment pays off.\n\nLearn by Doing\n\nEveryone learns in different ways, but when you’ve decided how you’re going to automate a test, jump in and start doing it. In Everyday Scripting with Ruby for Teams, Testers, and You [2007], Brian Marick advises to learn to program by writing a program. Make mistakes! The more problems you have, the more you’ll learn. Getting someone to pair with you will help speed up learn- ing, even if neither one of you is familiar with the tool or the language.\n\nIf you don’t have anyone to pair with, talk to the “rubber ducky”: Imagine you’re describing the problem to a coworker. The process of explaining can often make the cause of the problem jump into view. Simply reading a test aloud to yourself can help you ﬁnd the weaknesses in it.\n\nApply Agile Coding Practices to Tests\n\nTests are just as valuable as production code. In fact, production code isn’t much without tests to support it. Treat your tests the same way you treat all code. Keep it in the same source code control tool as your production code.\n\n303\n\n—Lisa\n\n304\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nYou should always be able to identify the versions of test scripts that go with a particular version of code.\n\nPairing, refactoring, simple design, modular and object-oriented design, good standards, keeping tests as independent as possible—all of the qualities of good code are also qualities of good automated tests. Agile development is some- times perceived by the uninformed to be chaotic or lax, when in fact it is highly disciplined. Undertake your automation tasks with the greatest discipline, pro- ceeding in small steps, checking in each step that succeeds. If you’re program- ming automated scripts, write them test-ﬁrst, just as any agile programmer would write production code. Keep simplicity in mind, though. Don’t write fancy test scripts with lots of logic unless there’s a good ROI. Those tests need testing and cost more to maintain. Specify tests when you can instead of coding them, and always go with the simplest approach possible.\n\nWe can’t emphasize it enough: Test automation is a team effort. The varying experience, skills, and perspectives of different team members can work to- gether to come up with the best approach to automation. Innovate—be cre- ative. Do what works for your unique situation, no matter what the “common wisdom” says.\n\nAutomation tools are just one piece of the puzzle. Test environments and test data are essential components. Let’s look at test data next.\n\nSUPPLYING DATA FOR TESTS No matter what tool we use to automate tests, the tests need data to process. Ideally, they need realistic data that matches production data. However, pro- duction databases usually contain lots and lots of data, and they can be highly complex. Also, database access slows down tests exponentially. Like so much of agile testing, it’s a balancing act.\n\nData Generation Tools\n\nAs we write this book, there are several cool tools available to generate test data for all kinds of input ﬁelds and boundary conditions. Open source and commercial tools such as Data Generator, databene benerator, testgen, Datatect, and Turbo Data are available to generate ﬂat ﬁles or generate data directly to database tables. These tools can generate huge varieties of differ- ent types of data, such as names and addresses.\n\nLisa’s Story\n\nSUPPLYING DATA FOR TESTS\n\nIt’s also fairly easy to generate test data with a home-grown script, using a scripting language such as Ruby or Python, a tool such a Fit or FitNesse, or a shell script.\n\nOur Watir scripts create randomized test data inputs, both to ensure they are re- runnable (they’re unlikely to create an employee with the same SSN twice), and to provide a variety of data and scenarios. The script that creates new retirement plans produces plans with about 200 different combinations of options. The script that tests taking out a loan randomly generates the frequency, reason, and term of the loan, and veriﬁes that the expected payment is correct.\n\nWe have utility scripts to create comma-separated ﬁles for testing uploads. For example, there are several places in the system that upload census ﬁles with new employee information. If I need a test ﬁle with 1,000 new employees with random investment allocations to a retirement plan, I can simply run the script and specify the number of employees, the mutual funds they’re investing in, and the ﬁle name. Each record will have a randomly generated Social Security Number, name, address, beneﬁciaries, salary deferral amounts, and investment fund allocations. Here’s a snippet of the code to generate the investment calculations.\n\n# 33% of the time maximize the number of funds chosen, 33% of the time # select a single fund, and 33% of the time select from 2-4 funds fund_hash = case rand(3) when 0: a.get_random_allocations(@fund_list.clone) when 1: a.get_random_allocations(@fund_list.clone, 1) when 2: a.min_percent = 8; a.get_random_allocations(@fund_list.clone, rand(3) + 2) end emp['fund_allocations'] = fund_hash_to_string(fund_hash)\n\nScripts like these have dual uses, both as regression tests that cover a lot of differ- ent scenarios and exploratory test tools that create test data and build test sce- narios. They aren’t hard to learn to write (see the section “Learning by Doing“ earlier in this chapter).\n\nScripts and tools to generate test data don’t have to be complex. For example, PerlClip simply generates text into the Windows clipboard so it can be pasted in where needed. Any solution that removes enough tedium to let you dis- cover potential issues about the application is worth trying. “The simplest thing that could possibly work” deﬁnitely applies to creating data for tests. You want to keep your tests as simple and fast as possible.\n\n305\n\n—Lisa\n\n306\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nAvoid Database Access\n\nYour ﬁrst choice for testing should try to have tests that can run completely in-memory. They will still need to set up and tear down test data, but the data won’t store in a database. Each test is independent and runs as quickly as any test could. Database access means I/O and disks are inherently slow. Ev- ery read to the database slows down your test run. If your goal is to give fast feedback to the team, then you want your tests to run as quickly as possible. A fake object such as an in-memory database lets the test do what it needs to do and still give instant feedback.\n\nLisa’s Story\n\nComprehensive explanations and examples of vari- ous types of test doubles can be found in xUnit Test Patterns. See the bibliography for more information on that and tools for working with test stubs and with mock and fake objects.\n\nOne of our build processes runs only unit-level tests, and we try to keep its run- time less than eight minutes, for optimum feedback. The tests substitute fake ob- jects for the real database in most cases. Tests that are actually testing the database layer, such as persisting data to the database, use a small schema with canonical data originally copied from the production database. The data is realis- tic, but the small amount makes access faster.\n\nAt the functional test level, our FitNesse test ﬁxtures build data in-memory wher- ever possible. These tests run quickly, and the results appear almost instanta- neously. When we need to test the database layer, or if we need to test legacy code that’s not accessible independently of the database layer, we usually write FitNesse tests that set up and tear down their own data using a home-grown data ﬁxture. These tests are necessary, but they run slowly and are expensive to main- tain, so we keep them to the absolute minimum needed to give us conﬁdence. We want our build that runs business-facing tests to provide feedback within a couple of hours in order to keep us productive.\n\nTools such as DbFit and NdbUnit can simplify database testing and en- able test-driven database develop- ment; see the bib- liography for more resources.\n\nBecause it’s so difﬁcult to get traction on test automation, it would be easy to say “OK, we’ve got some tests, and they do take hours to run, but it’s better than no tests.” Database access is a major contributor to slow tests. Keep tak- ing small steps to fake the database where you can, and test as much logic as possible without involving the database. If this is difﬁcult, reevaluate your system architecture and see if it can be organized better for testing.\n\nIf you’re testing business logic, algorithms, or calculations in code, you’re in- terested in the behavior of the code itself given certain inputs; you don’t care where the data comes from as long as it accurately represents real data. If this is the case, build test data that is part of the test and can be accessed in mem- ory, and let the production code operate from that. Simulate database access and objects, and focus on the purpose of the test. Not only will the tests run faster, but they’ll be easier to write and maintain.\n\n—Lisa",
      "page_number": 341
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 349-356)",
      "start_page": 349,
      "end_page": 356,
      "detection_method": "topic_boundary",
      "content": "Lisa’s Story\n\nSUPPLYING DATA FOR TESTS\n\nWhen generating data for a test, use values that reﬂect the intent of the test, where possible. Unless you’re completely conﬁdent that each test is indepen- dent, generate unique test values for each test. For example, use timestamps as part of the ﬁeld values. Unique data is another safety net to keep tests from infecting each other with stray data. When you need large amounts of data, try generating the data randomly, but always clean it up at the end of the test so that it doesn’t bleed into the next test. We recognize that sometimes you need to test very speciﬁc types of data. In these cases, randomly generated data would defeat the purpose of the test. But you may be able to use enough randomization to ensure that each test has unique inputs.\n\nWhen Database Access Is Unavoidable or Even Desirable\n\nIf the system under test relies heavily on the database, this naturally has to be tested. If the code you’re testing reads from and/or writes to the database, at some point you need to test that, and you’ll probably want at least some re- gression tests that verify the database layer of code.\n\nSetup/Teardown Data for Each Test Our preferred approach is to have every test add the data it needs to a test schema, operate on the data, verify the results in the database, and then de- lete all of that test data so the test can be rerun without impacting other sub- sequent tests. This supports the idea that tests are independent of each other.\n\nWe use a generic data ﬁxture that lets the person writing the test specify the data- base table, columns, and values for the columns in order to add data. Another ge- neric data lookup ﬁxture lets us enter a table name and SQL where clause to verify the actual persisted data. We can also use the generic data ﬁxture to delete data using the table name and a key value. Figure 14-3 shows an example of a table that uses a data ﬁxture to build test data in the database. It populates the table\n\nFigure 14-3 Example of a table using a data ﬁxture to build test data in the database\n\n307\n\n308\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\n“all fund” with the speciﬁed columns and values. It’s easy for those of us writing test cases to populate the tables with all of the data we need.\n\nNote that the schemas we use for these tests have most of their constraints re- moved, so we only have to populate the tables and columns pertinent to the functionality being tested. This makes maintenance a little easier, too. The down- side is that the test is a bit less realistic, but tests using other tools verify the func- tionality with a realistic environment.\n\nThe downside to creating test data this way is that whenever a change is made in the database, such as a new column with a required value, all of the data ﬁxture tables in the tests that populate that table will have to be changed. These tests can be burdensome to write and maintain, so we only use them when absolutely needed. We try to design the tests to keep maintenance costs down. For exam- ple, the data ﬁxture in Figure 14-3 is in an “include” library and can be included into the tests that need it. Let’s say we add a new column, “fund_category.” We only need to add it to this “include” table, rather than in 20 different tests that use it.\n\nCanonical Data Another alternative is having test schemas that can quickly be refreshed with data from a canonical or seed database. The idea is that this seed data is a repre- sentative sample of real production data. Because it’s a small amount of data, it can be quickly rebuilt each time a suite of regression tests needs to be run.\n\nThis approach also increases the time it takes to run tests, but it’s just a few minutes at the start of the regression suite rather than taking time out of each individual test. The tests will still be slower than tests that don’t access the da- tabase, but they’ll be faster than tests that have to laboriously populate every column in every table.\n\nCanonical data has many uses. Testers and programmers can have their own test schema to refresh at will. They can conduct both manual and automated tests without stepping on anyone else’s testing. If the data is carefully chosen, the data will be more realistic than the limited amount of data each test can build for itself.\n\nOf course, as with practically everything, there’s a downside. Canonical data can be a pain to keep up. When you need new test scenarios, you have to iden- tify production data that will work, or make up the data you need and add it to the seed schema. You have to scrub the data, mask real peoples’ identifying characteristics, making it innocuous for security reasons. Every time you add\n\n—Lisa\n\nSUPPLYING DATA FOR TESTS\n\na table or column to the production database, you must update your test sche- mas accordingly. You might have to roll date-sensitive data forward every year, or do other large-scale maintenance. You have to carefully select which tables should be refreshed and which tables don’t need refreshing, such as lookup ta- bles. If you have to add data to increase test coverage, the refresh will take longer to do, increasing the time of the build process that triggers it. As we’ve been emphasizing, it’s important that your automated builds provide feed- back in a timely manner, so longer and longer database refreshes lengthen your feedback cycle. You also lose the test independence with canonical data, so if one test fails, others may follow suit.\n\nLisa’s team members run their GUI test suites and some of their functional regression tests against schemas refreshed each run with canonical data. On rare occasions, tests fail unexpectedly because of an erroneous update to the seed data. Deciding whether to “roll” data forward, so that, for example, 2008’s rows become 2009’s rows, gets to be a headache. So far, the ROI on using ca- nonical data has been acceptable for the team. Janet’s current team also uses seed data for its “middle layer” testing on local builds. It works well for fast feedback during the development cycle. However, the test environment and the staging environments use a migrated copy of production data. The downside is that the regression tests can only be run on local copies of the build. The risk is low because they practice “build once, deploy to many.”\n\nProduction-Like Data The ability to test a system that is as much like production as possible is essen- tial to most software development teams. However, running a suite of auto- mated regression tests against a copy of a production database would probably run too slowly to be useful feedback. Besides, you couldn’t really depend on any data remaining stable as you bring over new copies to stay up-to-date. Generally, when you’re talking about functional or end-to-end testing, a clone of the production database is most useful for manual exploratory testing.\n\nStress, performance, and load testing, which are automation–intensive, need an environment that closely simulates production in order to provide results that can translate to actual operations. Usability, security, and reliability are other examples of testing that needs a production-like system, although they may not involve much automation.\n\nThere is always a trade-off; your production database might be huge, so it is expensive and slow, but it provides the most accurate test data available. If your organization can afford hardware and software to store multiple copies of production data for testing purposes, this is ideal. Small companies may\n\n309\n\n310\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nhave resource constraints that might limit the amount of data that can be stored in test and staging environments. In this case, you’ll need to decide how much test data you can support and plan how to copy enough relevant data to make the test representative of what’s used in “real life.” Or you may consider making the investment in hardware, which is getting less expensive every day, to support a real production style environment. Otherwise, your test results might be misleading. As we mentioned with the canonical data, you may need to scrub the data before using it.\n\nData Migration Data migration needs to be tested against a real database. The database up- grade scripts need to be run against real data and against the last known re- lease of the database schema.\n\nTesting a Database Migration\n\nPaul Rogers, an automation test architect, tells this story of testing an eye- opening database migration [2008]:\n\nJust yesterday, I ran a Rails migration against my test database. The developers had written it, tested it, and checked it using their develop- ment databases. My test database was probably 20,000 times larger. The migration for them took seconds. For me, well, I stopped it after three hours, at probably 10% complete. The programmers needed to redo their migration strategy.\n\nI doubt this would have shown up on an in-memory database, so for me, a real database in this instance was deﬁnitely the right choice. In fact, this is likely to feed into things we need to consider before releasing, such as how long does a deployment take, or how long does the data- base update take. We can then use this to estimate how much down time we will need for the actual upgrade.\n\nThis is another example of how we must strike a balance between tests that deliver quick feedback and tests that realistically reﬂect events that might occur in production.\n\nUnderstand Your Needs\n\nIf you understand the purpose of your tests, you can better evaluate your needs. For example, if you don’t need to test stored procedures or SQL que- ries directly for speed, consider tools such as in-memory databases, which work just like real databases but greatly speed up your tests. When you need to simulate the actual production environment, make a copy of the entire\n\nEVALUATING AUTOMATION TOOLS\n\nproduction database, if necessary. Quick feedback is the goal, so balance test- ing realistic scenarios with ﬁnding defects as efﬁciently as possible.\n\nEVALUATING AUTOMATION TOOLS The ﬁrst step in choosing an automation tool is to make a list of everything the tool needs to do for you. Let’s consider how you can decide on your test tool requirements.\n\nIdentifying Requirements for Your Automation Tool\n\nAfter deciding on the next automation challenge to tackle, think about your tool needs. What tools do you already have? If you need additional ones, you probably want something that integrates well with your existing testing and development infrastructure. Do you need a tool to easily integrate into the continuous build process? Will your existing hardware support the automa- tion you need to do? Setting up a second build process to run functional tests may require additional machinery.\n\nWho’s going to use the test tool you’re hoping to implement? Will non- programmers be writing test cases? Do your programmers want a tool they feel comfortable with as well? Do you have distributed team members who need to collaborate?\n\nWho will be automating and maintaining the tests? The skills already on your team are important. How much time do you have to get a tool installed and learn how to use it? If your application is written in Java, a tool that uses Java for scripting may be the most appropriate. Do team members have experi- ence with particular tools? Is there a separate test team with expertise in a certain tool? If you’re starting the transition to agile development and you al- ready have a team of test automators, it may make sense to leverage their ex- pertise and keep using the tools they know.\n\nYour tool requirements are dependent on your development environment. If you’re testing a web application, and the tool you choose doesn’t support SSL or AJAX, you may have a problem. Not every test tool can test web services applications. Embedded system testing can need different tools again. The case study in Chapter 12, “Summary of Testing Quadrants,” shows one way to use Ruby to test an embedded application.\n\nOf course, the type of testing you’re automating is key. Security testing prob- ably needs highly specialized tools. There are many existing open source and\n\n311\n\n312\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nvendor tools for performance, so the job of selecting one isn’t overwhelming. As you master one challenge, you’ll be better prepared for the next. It took Lisa’s team a couple of years to develop robust regression test suites at the unit, integration, and functional levels. Performance testing was their next area of pain. Lessons learned from the earlier automation efforts helped them do a better job of identifying requirements for a test tool, such as ease of reporting results, compatibility with existing frameworks, and scripting language.\n\nWrite a checklist that captures all your tool requirements. Some of them might conﬂict with or contradict each other—“The tool needs to be easy enough so that customers can specify tests” or “The tests should be easy to automate.” Write them down so you can ﬁnd the right balance. Then start doing your research.\n\nOne Tool at a Time\n\nYou’re going to need different tools to serve different purposes. Implement- ing new tools and learning the best way to use them can get overwhelming pretty quickly. Try one tool at a time, addressing your greatest area of pain. Give it enough time for a fair trial and evaluate the results. If it’s working for you, master that tool before you go on to the next area of pain and the next tool. Multitasking might work for some situations, but new technology de- mands full attention.\n\nWhen you’ve settled on a tool to address a particular need, take a step back and see what else you need. What’s the next automation challenge facing your team? Will the tool you just selected for another purpose work for that need, too, or do you need to start a new selection process?\n\nThe bibliography contains websites that help with tool searches and evaluation.\n\nIf you’ve decided to look outside your own organization for tools, the ﬁrst step is to ﬁnd time to try some out. Start with some basic research: Internet searches, articles and other publications about tools, and mailing lists are good places to get ideas. Compile a list of tools to consider. If your team uses a wiki or online forum tool, post information about tools and start a discus- sion about pros and cons.\n\nBudget time for evaluating tools. Some teams have an “engineering sprint” or “refactoring iteration” every few months where, rather than delivering stories prioritized by the business, they get to work on reducing technical debt, up- grading tool versions, and trying out new tools. If your team doesn’t have these yet, make a case to your management to get them. Reducing your tech-\n\nLisa’s Story\n\nChapter 11, “Critiquing the Product Using Technology- Facing Tests, shows an example of the results pro- duced by the per- formance test tool chosen, JMeter.\n\nEVALUATING AUTOMATION TOOLS\n\nnical debt and establishing a good testing infrastructure will improve your velocity in the future and free time for exploratory testing. If you never have time to make code easier to maintain or upgrade tools, technical debt will drag down your velocity until it comes to a halt.\n\nWhen you have a list of tools that may meet your requirements, narrow the possibilities down to one or two, learn how to use each one well enough to try it, and do a spike: Try a simple but representative scenario that you can throw away. Evaluate the results against the requirements. Use retrospectives to consider pros and cons.\n\nWhat resources do you need to implement and use the tool? What impact will the tool have on the team’s productivity and velocity? What risks does it pose? What will it allow you to do in the long term that you can’t do now?\n\nPick your top candidate and commit to trying it for some period of time— long enough to get some competency with it. Make sure you try all your mis- sion-critical functionality. For example, if your application uses a lot of Ajax, make sure you can automate tests using the tool. In retrospectives, look at what worked and what didn’t. Be open to the idea that it might not be right and that you have to throw it out and start over. Don’t feel you have to keep on with the tool because you have so much invested in it already.\n\nWe all know that there’s no “silver bullet” that can solve all your automation problems. Lower your expectations and open your mind. Creative solutions rely on art as much as science.\n\nWhen conducting the performance test tool search, we turned to an agile testing mailing list for suggestions. Many people offered their experiences, and some even offered to help learning and implementing a tool. We searched for a tool that used Java for scripting, had a minimal learning curve, and presented results in a useful graphical format. We listed tools and their pros and cons on the team wiki. We budgeted time for trial runs. Lisa’s coworker, Mike Busse, tried the top two candidates and showed highlights to the rest of the team. A tool was chosen by team consensus and has proven to be a good ﬁt.\n\nChoosing Tools\n\nWe’re lucky to have an already vast range and ever-growing set of tools to choose from: home-grown, open source, vendor tools, or a combination of\n\n313\n\n—Lisa\n\n314\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nany, are all viable alternatives. With so many choices, the trick is knowing where to look and ﬁnding time to try tools out to see if they ﬁt your require- ments. Because we can’t predict the future, it may be hard to judge the ROI of each potential solution, but an iterative approach to evaluating them helps get to the right one.\n\nShould You Grow Your Own? Does your application present unique testing challenges, such as embedded software or integration with outside systems? Do team members have the skills, time, and inclination to write their own test framework or build one on top of an existing open source tool? If so, home-grown test tools may be the best ﬁt.\n\nA happy result (or perhaps a major success factor) of agile development is that many programmers are “test infected.” Today’s development tools and languages make automation frameworks easier to build. Ruby, Groovy, Rails, and many languages and frameworks lend themselves to automation. Exist- ing open source tools such as Fit and HtmlUnit can be leveraged, with cus- tom frameworks built on top of them.\n\nHome-grown tools have many advantages. They’re deﬁnitely programmer- friendly. If your team is writing its own automation frameworks, they’ll be precisely customized to the needs of your development and customer teams, and integrated with your existing build process and other infrastructure— and you can make them as easy to execute and interpret results as you need.\n\nHome-grown doesn’t mean free, of course. A small team may not have the bandwidth to write and support tools as well as develop production code. A large organization with unique requirements may be able to put together a team of automation specialists who can collaborate with testers, customers, programmers, and others. If your needs are so unique that no existing tool supports them, home-grown may be your only option.\n\nOpen Source Tools Many teams who wrote their own tools have generously made them available to the open source community. Because these tools were written by test- infected programmers whose needs weren’t met by vendor tools, they are usually lightweight and appropriate for agile development. Many of these tools are developed test-ﬁrst, and you can download the test suite along with the source code, making customization easier and safer. These tools have a broad appeal, with features useful to both programmers and testers. The",
      "page_number": 349
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 357-365)",
      "start_page": 357,
      "end_page": 365,
      "detection_method": "topic_boundary",
      "content": "See Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” for more about open source test auto- mation tools.\n\nSee the bibliog- raphy for a full discussion by Elisabeth Hendrickson on this subject.\n\nEVALUATING AUTOMATION TOOLS\n\nprice is right, although it’s important to remember that purchase price is only a fraction of any tool’s cost.\n\nNot all open source tools are well documented, and training can be an issue. However, we see seminars and tutorials on using these tools at many confer- ences and user group meetings. Some open source tools have excellent user manuals and even have online tutorials and scheduled classes available.\n\nIf you’re considering an open source solution, look for an active developer and user community. Is there a mailing list with lots of bandwidth? Are new features released often? Is there a way to report bugs, and does anyone ﬁx them? Some of these tools have better support and faster response on bugs than vendor tools. Why? The people writing them are also using them, and they need those features to test their own products.\n\nVendor Tools Commercial tools are perceived as a safe bet. It’s hard to criticize someone for selecting a well-known tool that’s been around for years. They’re likely to come with manuals, support, and training. For testers or other users who lack a technical background, the initial ramp-up might be faster. Some are quite robust and feature-rich. Your company may already own one and have a team of specialists who know how to use it.\n\nAlthough they are changing with the times, vendor tools are historically pro- grammer-unfriendly. They tend to use proprietary scripting languages that programmers don’t want to spend time learning. They also tend to be heavy- weight. The test scripts may be brittle, easily broken by minor changes to the application, and expensive to maintain. Most of these tools are recording scripts for subsequent playback. Record/playback scripts are notoriously costly from a maintenance perspective.\n\nElisabeth Hendrickson [2008] points out that specialized tools such as these may create a need for test automation specialists. Silos such as these can work against agile teams. We need tools that facilitate test-ﬁrst, rather than test-last development. Test tools shouldn’t stand in the way of change.\n\nIf you have people already expert in a vendor tool, and a use for a tool that might be used only by a subset of the development team or a team separate from development, a vendor tool could make lots of sense. Lisa’s ﬁrst two XP teams used a vendor tool with some degree of success.\n\n315\n\n316\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nAs of this writing, better functional test tools and IDEs are emerging. These fa- cilitate test maintenance tasks with features such as global search/replace. Twist is an example of a tool implemented as a collection of plug-ins to the Eclipse IDE, so it can take advantage of powerful editing and refactoring features.\n\nAgile-Friendly Tools\n\nPart III, “Using Agile Testing Quadrants,” and particularly Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” contain examples of test automation tools that work well on agile projects.\n\nElisabeth Hendrickson [2008] lists some characteristics of effective agile test automation tools. These tools should:\n\n(cid:2) Support starting the test automation effort immediately, using a test-\n\nﬁrst approach\n\n(cid:2) Separate the essence of the test from the implementation details (cid:2) Support and encourage good programming practices for the code\n\nportion of the test automation\n\n(cid:2) Support writing test automation code using real languages, with\n\nreal IDEs\n\n(cid:2) Foster collaboration\n\nIMPLEMENTING AUTOMATION While you’re evaluating tools, think about how quickly your top priority au- tomation need must be addressed. Where will you get the support to help implement it? What training does the team need, and how much time will be available to devote to it? How quickly do you have to ramp up on this tool?\n\nKeep all of these constraints in mind when you’re looking at tools. You might have to settle for a less robust tool than you really want in order to get vital automation going in the short term. Remember that nothing’s permanent. You can build your automation effort step-by-step. Many teams experience unsuccessful attempts before ﬁnding the right combination of tools, skills, and infrastructure.\n\nSelenium at Work\n\nJoe Yakich, a software engineer with test automation experience, describes how a team he worked with implemented a test automation effort with Sele- nium, an open source test automation tool.\n\nThe software company I worked for—let's call it XYZ Corp—had a prob- lem. The product, an enterprise-level web-based application, was a powerful, mature offering. Development projects were managed using\n\nIMPLEMENTING AUTOMATION\n\nAgile and Scrum, and a talented stable of engineers churned out new features quickly. The company was growing steadily.\n\nSo, what was the problem? XYZ was facing a future where software test- ing efforts might not be able to keep pace with the development effort. Software quality issues might slow adoption of the product or—worse yet—cause existing customers to look elsewhere.\n\nTest automation seemed like an obvious way to mitigate these risks, and XYZ was fully aware of it. In fact, they had attempted to create a test au- tomation suite twice before, and failed.\n\nThe third time, XYZ chose to use Selenium RC, driven by the Ruby pro- gramming language. Selenium RC—the RC is for “Remote Control”—is a tool for test automation. Selenium RC consists of a server component and client libraries. The Java server component acts as an HTTP proxy, making the Selenium Core JavaScript appear to originate from the web- server of the application under test (AUT). The server can start and stop browser sessions (supported browsers include nearly all modern brows- ers, including Internet Explorer, Firefox, and Safari) and interpret com- mands to interact with elements such as buttons, links, and input ﬁelds. The client libraries allow test scripts to be written in Java, .NET, Perl, Python, and Ruby.\n\nOur team chose Ruby because it's a purely object-oriented, dynamic, in- terpreted language with a syntax that is elegant, expressive, and tersely powerful. Most importantly, Ruby is an ideal tool for the creation of a Do- main Speciﬁc Language (DSL). Ruby is malleable enough for the program- mer to ﬁrst choose the structure and syntax of the DSL and then craft an implementation, as opposed to a more rigid language that might impose constraints on that freedom. One of our goals was to create an automa- tion framework—a DSL—hiding complex detail. We wanted to be able to say things like\n\neditor.save\n\nin our tests instead of\n\ns.click(\"//table[@class='edit']/tbody/tr[0]//img[@src='save.gif']\")\n\nNot only is the former more readable, it’s also far more maintainable. The XPath expression in the latter can be put in a library method to be called as needed. Using a DSL that employs the nouns and verbs of the applica- tion allows an engineer writing a test to focus on the test, not the under- lying complexity of interacting with on-screen controls.\n\nXYZ created an automation team to build the framework and tests. Cre- ating the framework itself was a time-consuming, technically challenging\n\n317\n\n318\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\ntask. Some of the framework classes themselves were complicated enough to warrant unit tests of their own. After a sufﬁcient amount of test framework was constructed, we began work on actual application tests, using the Ruby RSpec library. RSpec is itself a DSL for test speciﬁca- tions. One of its strengths is the use of simple declarative statements to describe behavior and expectations. One might, for example, write a test using the statement\n\n“A user should be able to save data in an editor by clicking Save”\n\nﬁlling in the body of the test with calls to the Selenium-based test frame- work we had created.\n\nNearly a year later, we had automated nearly two thousand test cases. Although the majority of the application was covered by automation, other portions of the application required manual testing—we had been forced to make choices and prioritize our efforts. Every week the test suite took longer to run than the preceding week; it now took nearly six hours to complete, and we had begun to think about running tests in parallel. We had not yet managed to expand our testing across all of the browsers supported by the application. The enthusiasm that automation generated had waned somewhat, and we found it necessary to carefully manage expectations, both with upper management and with other en- gineers. Despite these issues, Selenium was a clear win, for had we not invested heavily in test automation, testing at XYZ would have required hiring an army of test engineers (which would have been prohibitively expensive even had we been able to ﬁnd enough qualiﬁed applicants).\n\nNot everything can be automated, because of budgetary or technical reasons. In addition, exploratory testing is invaluable and should not be neglected. It should be noted, however, that these drawbacks are shared by every other test automation tool currently available, and most of the other automation tools that can rival Selenium's automation prow- ess are commercial products that cannot match its price: free.\n\nGood development practices are key to any automation effort. Use an ob- ject-oriented approach. As you build your library of test objects, adding new tests becomes easier. A domain speciﬁc language helps make business-facing tests understandable to customers, while lowering the costs of writing and maintaining automated test scripts.\n\nGood object-oriented design isn’t the only key to building a suite of main- tainable automated tests that pay off. You also need to run the tests often enough to get the feedback your team needs.. Whatever tools we choose must be integrated with our build process. Easy-to-interpret results should come to us automatically.\n\nMANAGING AUTOMATED TESTS\n\nThe tools we choose have to work on our platforms, and must share and play well with our other tools. We have to continually tweak them to help with our current issues. Is the build breaking every day? Maybe we need to hook our results up to an actual trafﬁc light to build team awareness of its status. Did a business-facing test fail? It should be plain exactly what failed, and where. We don’t have extra time to spend isolating problems.\n\nThese concerns are an essential part of the picture, but still only part of the picture. We need tools that help us devise test environments that mimic pro- duction. We need ways to keep these test environments independent, unaf- fected by changes programmers might be making.\n\nBuilding test infrastructure can be a big investment, but it’s one our agile team needs to make to get a jump on test automation. Hardware, software, and tools need to be identiﬁed and implemented. Depending on your company’s re- sources, this might be a long-term project. Brainstorm ways to cope in the short term, while you plan how to put together the infrastructure you really need to minimize risk, maximize velocity, and deliver the best possible product.\n\nMANAGING AUTOMATED TESTS Let’s say we need a way to ﬁnd the test that veriﬁes a particular scenario, to understand what each test does, and to know what part of the application it veriﬁes. Perhaps we need to satisfy an audit requirement for traceability from each requirement to its code and tests. Automated tests need to be main- tained and controlled in the same way as production source code. When you tag your production code for release, the tests that veriﬁed that functionality need to be part of the tag.\n\nHere’s an example where that comes in handy. We just found a problem in the code under development. Is it a new problem, or has it been lurking in the code for a while and somehow missed by the test? We can deploy the tag that’s in production, try to reproduce the problem, and investigate why the tests didn’t catch it. Lisa’s team recently had a situation where the regression suite missed a bug because a database constraint was missing in the test schema. That kind of problem is hard to pinpoint if you aren’t tying your test code versions to your production code versions.\n\nOrganizing Tests\n\nMany tools come with their own means of organization. For example, Fit- Nesse comes with its own wiki, with a hierarchical organization, and built-in\n\n319\n\n320\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nversion control. As of this writing, FitNesse is starting to provide support for source code control tools such as Subversion. Scripts written in other test tools, such as Watir and Canoo WebTest, can and should be maintained within the same source code control system as production code, just as the unit tests are.\n\nOrganizing Tests with the Project under Test\n\nWe asked some agile testing experts how they manage tests. Dierk König, founder and project manager of Canoo WebTest, explained how his teams have managed their automated tests to satisfy the needs of both the devel- opment and customer teams.\n\nWe always organize our tests alongside the project under test. That is, test sources are stored together with the project sources in the exact same repository, using the same mechanisms for revision control, tagging, and sharing the test base.\n\nWebTest comes with a standard layout of how to organize tests and test data in directories. You can adapt this to any structure you fancy, but the \"convention over conﬁguration\" shows its strength here. In large projects, every sub-project maintains its own test base in a “webtest” subdirectory that follows the convention.\n\nWhenever a client did not follow this approach, the experience was very painful for all people involved. We have seen huge databases of test de- scriptions that did not even feature a proper revision control (i.e., where you could, for example, see diffs to old releases or who changed which test for what reason).\n\nKeep in mind that tests are made up from modules so that you can elimi- nate duplication of test code; otherwise the maintenance will kill you. And before changing any module, you need to know where it is used.\n\nIn short: Make sure the master of your tests and your test data is in a text format that is versioned together with your code under test.\n\nNontechnical personnel (for example, management, QA) may require more high-level information about test coverage, latest test results, or even means of triggering a test run. Don't let these valid requirements undermine the engineering approach to test automation. Instead, write little tools, for example, web-based report applications, that address these needs.\n\nThe ability of customers to access information about tests is as important as the ability to keep test and production code coordinated. As Dierk pointed out, you might not be able to do all this with the same tool.\n\nMANAGING AUTOMATED TESTS\n\nTest management helps your team answer questions such as the following:\n\n(cid:2) Which test cases have been automated? (cid:2) Which still need automating? (cid:2) Which tests are currently running as part of a regression suite? (cid:2) Which tests cover what functional areas? (cid:2) How is feature XYZ designed to work? (cid:2) Who wrote this test case? When? Who changed it last? (cid:2) How long has this test been part of the regression suite?\n\nBecause one of the primary reasons we write tests is to guide development, we need to organize tests so that everyone on the team can ﬁnd the appropri- ate tests for each story and easily identify what functionality the tests cover. Because we use tests as documentation, it’s critical that anyone on either the development or customer team can ﬁnd a particular test quickly when there’s a question about how the system should behave. We might need multiple tools to satisfy different test management goals.\n\nIt’s easy to lose control of test scripts. When a test fails, you need to pinpoint the problem quickly. You may need to know what changes have been made recently to the test script, which is easy with the history available in a source code control system. Your customer team also needs a way to keep track of project progress, to understand how much of the code is covered with tests, and possibly to run tests themselves. Test management systems, like the tests themselves, should promote communication and collaboration among team members and between different teams.\n\nTest Transparency\n\nDeclan Whelan, a software developer and agile coach, uses a test manage- ment approach designed to keep tests visible to testers, developers, manag- ers, and other teams.\n\nWe treat all test artifacts the same as source code from an organizational and revision control perspective. We use Subversion, and anyone who wants to run or edit the tests simply checks them out.\n\nThe latest Fit tests are available on a Conﬂuence Wiki. We did this to sup- port collaboration (team is distributed) and to leverage the strong capa- bilities of Conﬂuence. Having the tests visible on the wiki was also helpful to others such as managers and other teams who did not want to check it out from the repository.\n\n321\n\n322\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nPrior to this, the QA team maintained test cases on a drive that was not ac- cessible to anyone outside of QA. This meant that developers could not easily see what was being tested. Making the tests visible, transparent, and supported by a version control system (Subversion) really helped to break down barriers between developers and testers on the team.\n\nMake sure your tests are managed with solid version control, but augment that with a way for everyone to use the tests in ways that drive the project forward and ensure the right value is delivered.\n\nOrganizing Test Results\n\nEveryone involved with delivering software needs easy access to tests and test results. Another aspect of managing tests is keeping track of what tests are from prior iterations and need to keep passing, versus tests that are driving development in the current iteration and may not be passing yet. A continu- ous integration and build process runs tests for quick feedback on progress and to catch regression failures. Figure 14-4 shows an example of a test result report that’s understandable at a glance. One test failed, and the cause of the failure is clearly stated.\n\nIf you’re driving development with tests, and some of those tests aren’t pass- ing yet, this shouldn’t fail a build. Some teams, such as Lisa’s, simply keep new tests out of the integration and build process until they pass for the ﬁrst time. After that, they always need to pass. Other teams use rules in the build process itself to ignore failures from tests written to cover the code currently being developed.\n\nAs with any test automation tool, you can solve your test management prob- lems with home-grown, open source, or commercial systems. The same cri- teria we described in the section on evaluating test tools can be applied to selecting a test management approach.\n\nTest management is yet another area where agile values and principles, to- gether with the whole-team approach, applies. Start simple. Experiment in small steps until you ﬁnd the right combination of source code control, re- positories, and build management that keeps tests and production code in synch. Evaluate your test management approach often, and make sure it ac- commodates all of the different users of tests. Identify what’s working and what’s missing, and plan tasks or even stories to try another tool or process to ﬁll any gaps. Remember to keep test management lightweight and maintain- able so that everyone will use it.\n\nMANAGING AUTOMATED TESTS\n\nFigure 14-4 Test results from a home-grown test management tool\n\nManaging Tests For Feedback\n\nMegan Sumrell, an agile trainer and coach, describes how her team coordi- nates its build process and tests for optimum feedback.\n\nWe create a FitNesse test suite for each sprint. In that suite, we create a subwiki for each user story that holds its tests. As needed, we create a setup and teardown per test or suite. If for some reason we don't com- plete a user story in the sprint, then we move the tests to the suite for sprint in which we do complete the story.\n\nWe scripted the following rule into our build: If any of the suites from the previous sprint fail, then the build breaks. However, if tests in the current sprint are failing, then do not fail the build.\n\n323",
      "page_number": 357
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 366-373)",
      "start_page": 366,
      "end_page": 373,
      "detection_method": "topic_boundary",
      "content": "324\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nEach test suite has a lengthy setup process, so when our FitNesse tests started taking longer than 10 minutes to run, our continuous integration build became too slow. We used symbolic links to create a suite of tests that serve as our smoke tests, running as part of our continuous integra- tion build process. We run the complete set of FitNesse tests on a sepa- rate machine. We set it up to check the build server every ﬁve minutes. If a new build existed, then it would pull the build over and run the whole set of FitNesse tests. When it was done, it would then check the build server again every ﬁve minutes and after a new build existed, it would re- peat the process.\n\nMegan’s team took advantage of features built into their tools, such as sym- bolic links to organize FitNesse test suites for different purposes—one for a smoke test, others for complete regression testing. The team members get immediate feedback from the smoke tests, and they’ll know within an hour whether there’s a bug that the smoke tests missed.\n\nGO GET STARTED Don’t be afraid to get something—anything—in place, even if it’s somewhat deﬁcient. The most important factor in success is to just get started. Many, if not most, successful teams have started with a poor process but managed to turn an inadequate process into something truly essential to the team’s suc- cess, one piece at a time. As with so many aspects of agile testing, improving in tiny increments is the key to success.\n\nIf you don’t start somewhere, you’ll never get traction on automation. Get the whole team together and start an experiment. Without the right level of test automation, your team can’t do its best work. You need the right test au- tomation to deliver business value frequently. A year or two from now, you’ll wonder why you thought test automation was so hard.\n\nSUMMARY In this chapter, we considered how to apply agile values, principles, and prac- tices to develop an automation strategy. We discussed the following subjects related to automation:\n\n(cid:2) Use the agile testing quadrants to help identify where you need test\n\nautomation, and when you’ll need it.\n\n(cid:2) The test automation pyramid can help your team make the right in-\n\nvestments in test automation that will pay off the most.\n\nSUMMARY\n\n(cid:2) Apply agile values, principles, and practices to help your team get\n\ntraction on test automation.\n\n(cid:2) Repetitive tasks, continuous integration and build processes, unit\n\ntests, functional tests, load tests, and data creation are all good candi- dates for automation.\n\n(cid:2) Quadrant 3 tests such as usability testing and exploratory testing may beneﬁt from some automation to set up test scenarios and analyze re- sults, but human instincts, critical thinking, and observation can’t be automated.\n\n(cid:2) A simple, whole-team approach, using iterative feedback, and taking\n\nenough time can help you get started on a good solution.\n\n(cid:2) When developing an automation strategy, start with the greatest area of pain, consider a multi-layered approach, and strive for continu- ously revisiting and improving your strategy rather than achieving perfection from the start.\n\n(cid:2) Consider risk and ROI when deciding what to automate. (cid:2) Take time to learn by doing; apply agile coding practices to tests. (cid:2) Decide whether you can simply build inputs in-memory, or whether\n\nyou need production-style data in a database.\n\n(cid:2) Supply test data that will allow tests to be independent, rerunnable,\n\nand as fast as possible.\n\n(cid:2) Take on one tool need at a time, identify your requirements, and de-\n\ncide what type of tool to choose or build that ﬁts your needs.\n\n(cid:2) Use good development practices for test automation, and take time\n\nfor good test design.\n\n(cid:2) Automated tools need to ﬁt into the team’s development infrastructure. (cid:2) Version-control automated tests along with the production code that\n\nthey verify.\n\n(cid:2) Good test management ensures that tests can provide effective docu-\n\nmentation of the system and of development progress.\n\n(cid:2) Get started on test automation today.\n\n325\n\nThis page intentionally left blank\n\nPart V AN ITERATION IN THE LIFE OF A TESTER\n\nWhenever we do tutorials, webinars, or Q&A sessions with participants who are relatively new to agile development, we’re always asked questions such as “What do testers do during the ﬁrst part of an iteration before anything’s ready to test?” or “Where does user acceptance testing ﬁt into an agile release cycle?” It’s easy to expound on theories of who should do what and when, in an agile process, but we ﬁnd giving concrete examples from our own experi- ence is the best help we can give agile newbies. Through our talking to many different agile teams, we’ve learned that there’s a lot of commonality in what works well for agile development and testing.\n\nIn this part of the book, we’ll follow an agile tester’s life throughout an itera- tion. Actually, we’ll explore more than just an iteration. We’ll start with what testers do during release or theme planning, when the team looks at the work it will do for several upcoming iterations. We’ll give examples of what testers can do to help the team members hit the ground running when they start the iteration. We’ll show how coding and testing are part of one integrated pro- cess of delivering software, and we’ll describe how testers and programmers work closely and incrementally. We’ll explain different ways that testers can help their teams stay on track and gauge progress, including useful ap- proaches to metrics and handling defects. We’ll look at testing-related activi- ties involved in wrapping up an iteration and ﬁnding ways to improve for the next one. Finally, we’ll examine a tester’s role in a successful release, includ- ing the end game, UAT, packaging, documentation, and training.\n\n328\n\nPART V (cid:2) AN ITERATION IN THE LIFE OF A TESTER\n\nThe activities described in this slice-of-life look at agile testing can be per- formed by anyone on the team, not only testing specialists. On some teams, all team members can, and do, perform any task, be it development, testing, database, infrastructure, or other tasks. For simplicity, in this section we’ll as- sume we’re following someone whose primary role is testing as they help to deliver high-quality software.\n\nChapter 15\n\nTESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nTracking Test Tasks\n\nPurpose of Release Planning\n\nCommunicating Test Results\n\nPreparing for Visibility\n\nRelease Metrics\n\nSizing Stories\n\nSizing\n\nTester’s Role\n\nAn Example\n\nLIghtweight Test Plan\n\nTest Matrix\n\nSpreadsheet\n\nTest Plan Alternatives\n\nRelease/Theme Planning\n\nPrioritizing\n\nWhy Do We Prioritize?\n\nTesting Considerations\n\nWhiteboard\n\nAutomated Test List\n\nDeadlines and Timelines\n\nFocus on Value\n\nWhat’s In-Scope?\n\nWhere to Start?\n\nSystem-Wide Impact\n\nWhy Test Plan?\n\nThird-Party Involvement\n\nTypes of Testing\n\nInfrastructure\n\nTest Planning\n\nTest Environments\n\nTest Data\n\nTest Results\n\nAgile development teams complete stories and deliver production-ready software in every iteration but plan the big picture or a larger chunk of functionality in advance. A theme, epic, or project may encompass several iterations. In this chapter, we look at what testers do when their team takes time to plan their release. We also consider ways to track whether our development is proceeding as anticipated, or if course corrections are needed.\n\n329\n\n330\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nTHE PURPOSE OF RELEASE PLANNING One reason software teams try agile development is because they know long- range plans don’t work. Most business environments are volatile, and priori- ties change every week, or even every day. Agile development is supposed to avoid “big design up front.” Most of us have experienced making plans that turned out to be a waste of effort. But we have to have some understanding of what our customer is looking for and how we might deliver it in order to get off to a good start. Fortunately, an agile approach can make planning a useful way to give us a head start on knowing how we will deliver the product.\n\nAgile Planning Applied\n\nJanet’s sister, Carol Vaage, teaches ﬁrst grade when she isn’t directing confer- ences. She relates her ﬁrst experience with using agile practices to organize a conference:\n\nMy table is loaded with binders and to-do lists, and a feeling of being overwhelmed freezes me into inaction. I am Conference Director and the task right now seems onerous. When my sister offers to help me, I agree, because I am desperate to get this planning under control. I welcome Janet to my clutter, show her my pages of hand-written lists of things that need to get done, explain the huge tasks waiting for my attention, and share how my committee works.\n\nJanet showed me in simple language how to separate each task onto a sticky note and use color coordination for different responsibilities and different individuals. She explained about the columns of “To Do,” “In Progress,” “To Review,” and “Done.” I had never heard of the word itera- tion before but fully understood about a timeline. She recommended two-week blocks of time, but I chose one-week iterations. We set up a wall for my planning board, and Janet left me to pull it together and to add the tasks needed.\n\nIn the six days since Janet has been here, ten tasks have been moved from the To-Do column to In-Progress. Three tasks are Done, and speciﬁc time-related tasks have been blocked by the correct time period. The most positive thing is that as I add more tasks in the To-Do column, I am not feeling overwhelmed. I understand that all I need to do is initiate the steps to start it, and then the job becomes easier. The feeling of chaos is gone; I see progress and understand that there is still much work to be done. The timeline is clear, the tasks are discrete and concrete. And the most difﬁcult task of all, ﬁnding a way to coordinate the video confer- ence for our keynote speaker has been tackled. This system works!\n\nAgile planning and tracking practices are useful for more than software devel- opment. A little time carefully invested, and simple tools used in organizing and planning the testing activities and resources for a release, will help the team deliver high-quality software.\n\nTHE PURPOSE OF RELEASE PLANNING\n\nXP teams may take a day every few months for release planning. Other agile teams do advance planning when getting ready to start on a theme, epic, or major feature, which we think of as a related group of stories. They work to understand the theme or release at a high level. What is the customer’s vision of what we should be delivering? What’s the purpose of the release? What’s the big picture? What value will it deliver to the business, to the customers? What other teams or projects are involved and require coordination? When will UAT take place? When will code be released to staging, to production? What metrics do we need to know if we’re on track? These general questions are addressed in release planning.\n\nSome teams don’t spend much time doing release planning activities. Priori- ties change quickly, even within a particular theme of features. Nobody wants to do too much work up front that ends up being wasted. Some teams just look at the ﬁrst couple of stories to make sure they can get a running start. At the very least, teams want to know enough to get their system archi- tecture pointed in the right direction and get started on the ﬁrst few stories.\n\nThese planning meetings aren’t intended to plan every iteration of the release in detail. And we know we can’t predict exactly how many stories we can complete each iteration. However, we do have an idea of our average velocity, so we can get a general idea of the possible scope of the release. The team talks about the features and stories, trying to get a 20,000-foot view of what can go into the release and how many iterations it might take to complete. Both of us like Mike Cohn’s approach to release planning in his book Agile Estimating and Planning [2005]. Stories that the business wants to include are sized relative to each other, and then features are prioritized according to the value they deliver. The team may identify “thin slices” through the fea- tures to determine what stories absolutely have to be done, what’s in scope, what “nice-to-haves” could be put off until later. They look at dependencies between stories, relative risk, and other factors that determine the order in which features should be coded. The order in which stories are coded is as important, or sometimes more important, than the size of the stories. Teams want to deliver value the ﬁrst iteration of the release.\n\nRelease planning is a chance for the developers and customers to consider the impact of the planned features on the larger system, clarify assumptions, and look at dependencies that might affect what stories are done ﬁrst. They may think about testing at a high level and whether new resources such as test en- vironments and software will be needed.\n\nLet’s follow our agile tester through release planning activities and see how she contributes value through her unique perspective and focus.\n\n331",
      "page_number": 366
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 374-382)",
      "start_page": 374,
      "end_page": 382,
      "detection_method": "topic_boundary",
      "content": "332\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nSIZING Agile teams estimate the relative size of each story. Some teams size as they go, delaying the estimation until the iteration where they’ll actually complete the story. Others have meetings to estimate stories even in advance of release planning. Some developer and customer teams sit together to write and esti- mate the size of stories all at one time. The goal of sizing is for the program- mers to give the business an idea of the cost of each story and to help them prioritize and plan the ﬁrst few iterations. High-functioning teams who’ve worked together for years may take a less formal approach. For new agile teams, learning to size stories takes a lot of practice and experience. It’s not important to get each story sized correctly but to be close enough to give cus- tomers some idea of how big the stories are so they can prioritize with better information. Over time, variations on individual story sizing will average out, and we ﬁnd that a theme or related group of stories takes about the amount of time expected.\n\nHow to Size Stories\n\nAs far as how to calculate story size, different teams use different techniques, but again, we like Mike Cohn’s approach to determining story size. We size in story points, ideal days, or simply “small, medium, large.” The relative size of each story to others is the important factor. For example, adding an input ﬁeld to an existing user interface is obviously much smaller than developing a brand new screen from scratch.\n\nIf the business knows the average velocity (the number of story points the team completes each iteration) and has the initial size estimates of each story it wants to get done, it has an idea of how long it might take to implement a given theme. As with any other development methodology, there are no guarantees, because estimates are just that. Still, the business can plan well enough to conduct its usual activities.\n\nOur teams use planning poker (explained in Mike Cohn’s book Agile Estimat- ing and Planning) to estimate story size. In planning poker, each team mem- ber has a deck of cards. Each card has a number of points on it. The process begins with the customer or product owner reading a story and explaining its purpose and the value it will deliver. He might list a few conditions of satisfac- tion or high-level test cases. After a brief discussion, team members each hold up a point card that represents how “big” they think the story is from their perspective. They discuss any big differences in point value and estimate again until they reach consensus. Figure 15-1 shows team members talking about\n\nSIZING\n\nFigure 15-1 Planning poker\n\nthe point values they each just displayed. This needs to be a quick process— long discussions about details don’t result in more accurate size estimates.\n\nSome teams ﬁgure the relative sizes of stories by how many people are needed to complete a given story in a set amount of time. Others estimate how many ideal days one person would need to ﬁnish it. Use a measurement that makes sense to all team members and one that provides consistency among estimates.\n\nThe Tester’s Role in Sizing Stories\n\nOne of our favorite sayings is, “No story is done until it’s tested.” However, we’ve run across teams where testing wasn’t included in estimates of story size. In some cases, testing a piece of functionality might take longer than coding it.\n\nIn our experience, testers usually have a different viewpoint than other team members. They often have a broad understanding of the domain and can quickly identify “ripple effects” that one story might have on the rest of the system. They also tend to think of activities not directly related to develop- ment that might need to be done, such as training users on a new or changed interface.\n\n333\n\n334\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nLisa’s Story\n\nWhat does a tester do during the story sizing process? I think quickly about the story from several viewpoints. What business problem is the story solving, or what business value does it deliver? If this isn’t clear, I ask the product owner questions. How will the end user actually use the feature? If it’s still not clear, I ask the prod- uct owner for a quick example. I might ask, “What’s the worst thing that could go wrong with it?” This negative approach helps gauge the story’s risk. What testing considerations might affect the story’s size? If test data will be hard to obtain or the story involves a third party, testing might take longer than coding. I try to quickly ﬂush out any hidden assumptions. Are there dependencies or special se- curity risks? Will this part of the application need to handle a big load?\n\nMany stories aren’t big enough to warrant that much thought. Usually, we don’t need much detail to get an idea of relative size. However, your team can really get burned if a story is underestimated by a factor of ﬁve or ten. We once gave a rela- tively small estimate to a story that ended up being at least ten times the size. These are the disasters we want to avoid by asking good questions.\n\nTesters need to be part of the sizing process. Some teams think that only pro- grammers should participate, but when testers are active participants, they can help to get a much more accurate story sizing, which is in the best inter- ests of the whole team.\n\nAn Example of Sizing Stories\n\nLet’s imagine we have the story in Figure 15-2 to size up.\n\nStory PA-3\n\nAs a shopper on our site, I want to delete items\n\nout of my shopping cart so I don’t purchase\n\nextra items that I decide I don't want.\n\nFigure 15-2 Story to delete items\n\n—Lisa\n\nSIZING\n\nAfter the product owner reads the story, the following discussion ensues:\n\nProduct Owner: “We just want some easy way for users to delete items, but we don’t have a speciﬁc implementation in mind.” Tester: “Should they be able to delete several items at once?” Product Owner: “Oh, yes, just make it as easy as possible.” Tester: “What if they accidentally delete an item they wanted to buy?” Product Owner: “Is there some way the deleted items can be saved for later retrieval?” Programmer: “Sure, but you should write a new story for that. For now, we should start with the basic delete functionality.” Tester: “Last release we implemented a wish list feature. Do you want users to be able to move items from their shopping basket to their wish list? That would be a new story also.” Product Owner: “Yes, those are two more stories we want to do, for sure. I’ll write those down, we can size them also. But we could deﬁnitely put them off until the next release, if we have to.” Tester: “What’s the worst thing that could happen with this feature?” Product Owner: “If they can’t ﬁgure out how to delete, they might just abandon their whole shopping basket. It has to be really easy and obvious.”\n\nThe ScrumMaster calls for an estimate. The team understands they’re sizing only the basic story for deleting items, not for doing something else with the deleted items. They quickly agree on a point value.\n\nLet’s look at another story. (See Figure 15-3.)\n\nStory PA-4\n\nAs a customer, I want to know how much my\n\norder will cost to ship based on the shipping\n\nspeed I choose so that I can choose a different\n\nshipping speed if I want to.\n\nFigure 15-3 Story on shipping speed\n\n335\n\n336\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nTester: “What are the shipping speeds the user can choose?”\n\nProduct Owner: “Standard 5-day, 2-day, and next-day.”\n\nProgrammer: “We should probably start by only offering one speed, and calculating that cost. Then we can easily implement the other two speeds.”\n\nProduct Owner: “It’s ﬁne to break it up like that.”\n\nTester: “Will we use BigExpressShipping’s API to calculate cost based on weight and destination?”\n\nProgrammer: “That would be the easiest.”\n\nThe team holds up their point cards. The tester and one of the programmers hold up an 8; the other developers hold up a 5.\n\nScrumMaster: “Why did you two choose 8?”\n\nTester: “We’ve never used BigExpressShipping’s cost API before, and I’m not sure how that will impact our testing. We have to ﬁnd out how to ac- cess their system for testing.”\n\nOther Programmer with 8: “I agree, I think the testing effort is more intense than the coding effort for this story.”\n\nThe team agrees to size the story as eight points.\n\nThis sizing process may occur before the planning meeting, and if the sto- ries were sized or estimated a long time ago, the team might want to make sure they feel comfortable with the story sizes. Teams may have changed or may be more experienced. Either of those factors can make a team change the estimates.\n\nThere are many times when a story will have a large testing component, and the coding effort is small. At other times, the reverse will be true. It’s impor- tant to consider all perspectives.\n\nLisa’s Story\n\nSIZING\n\nOur team grew to dread story sizing meetings, because we got into overly long discussions about details, and the meetings always lasted long past the scheduled time. Since then, our ScrumMaster has found ways to keep us on track. She uses an egg timer to time discussions, and stops them each time the sand runs out to see if we think we really need more time to ask questions. Our product owner has also learned what information we need for estimating and usually has what we need. We also learned to only work on stories that were likely to come up in the next few iterations.\n\nWith all of our meetings, little traditions have grown to make the meetings more fun. Someone always brings treats to iteration planning meetings. In stand-up meetings, we pass around a combination penlight and laser pointer, so each of us holds it as we report on what we’re working on. We always end story sizing meet- ings with a competition to see who can throw his or her deck of planning poker cards into the small plastic tub where they live. Figure 15-4 shows this goofy but fun meeting-ending activity. Always remember the agile value of enjoyment and have some fun with your meetings.\n\nFigure 15-4 A meeting-ending tradition\n\n337\n\n—Lisa\n\n338\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nPRIORITIZING The purpose of the release planning meeting is also to get an idea of what stories the team will try to ﬁnish by the release date. The customers prioritize the stories, but there may be dependencies, so it makes sense to do certain stories ﬁrst, even if they aren’t the highest priority. It is important that the team understands the possibility that not all of the stories will get completed by the release date. One of the basic premises of agile is to deliver working software, so it is important to have the highest-value stories completed ﬁrst so that the software we do deliver meets the customer’s needs.\n\nWhy We Prioritize Stories\n\nEveryone’s goal is to deliver real value in each iteration. Testers can help the team pick out the core functionality that has to work. In Chapter 8, we ex- plained the “thin slice” or “steel thread” concept, identifying one path through the functionality to code and test ﬁrst, and adding more features after the ﬁrst critical path works. This concept applies at the release level, too. The order of the stories is critical. Lisa’s team will sometimes break up a story and pull out a core part of a feature to do in the ﬁrst iteration.\n\nSome teams that don’t do full-blown release planning do take time to look at the stories and decide which two or three should be ﬁrst. That way, they de- liver business value in the very ﬁrst iteration of the release.\n\nLet’s look at an example.\n\nIf our theme is providing the ability for an online shopper to choose shipping options and then calculate the shipping cost based on weight, shipping speed, and destination, it may be a good idea to complete sim- ple stories or even subsets of stories so that the checkout process can pro- ceed end-to-end. Start by only allowing standard 5-day shipping, items less than 10 pounds, and destinations in the continental United States. When the user can get the shipping cost for that scenario and check out, the team can decide the next priorities. They may include heavyweight items, faster shipping speeds, shipping to Hawaii and Alaska, and ship- ping to Canada and Mexico.\n\nBy providing this thin slice ﬁrst, the testers have something to start testing immediately. The programmers have also tested their design and code inte- gration steps and so have a solid idea of how things will work when the whole feature is complete.\n\nPRIORITIZING\n\nTesting Considerations While Prioritizing\n\nIt is important that the team understands the big picture or theme. In our ex- ample, team members know the stories for shipping outside the continental United States will come later. This knowledge may affect how they imple- ment the ﬁrst story. This doesn’t mean they have to plan for every eventual- ity, but if they know they need more shipping options, they may implement a drop-down list rather than a basic text ﬁeld. No need to make more work or rework than necessary.\n\nDuring release planning, we also consider the relative risk of the stories. If certain stories have many unknowns, it might be best to include them in an early iteration, so there’s time to recover if a story “blows up” and takes much more time than estimated. The same may apply to a story which, if not com- pleted or implemented incorrectly, would have a costly negative impact. Scheduling it early will leave more time for testing.\n\nIf new technology or software is needed, it might be good to learn it by devel- oping a straightforward story and plan more difﬁcult ones for later itera- tions. This new technology may or may not affect your test automation. You may want more time to check out the impact. If the features are all brand new and the team needs more time to understand how they should work, plan to do less than your average velocity for the ﬁrst iteration. That way, you’ll have more time to write tests that will correctly guide development. Identify risks and decide what approach makes the most sense from a testing perspective as well as a development perspective. This is one of the reasons it is important to include the whole team in the planning sessions.\n\nLooking at the stories from a testing viewpoint is essential. This is where testers add the most value. The team needs to develop in small, testable chunks in order to help decide what stories are tentatively planned for which iteration. The key here is testable. Many new agile teams think small chunks means doing all of the database work ﬁrst, or all of the conﬁguration stuff. Testable doesn’t necessarily mean it needs a GUI either. For example, the al- gorithm that calculates shipping cost is an independent piece of code that can be tested independently of any user interface but requires extensive test- ing. That might be a good story for the ﬁrst iteration. It can be tested as free- standing code and then later tested in combination with the UI and other parts of the system.\n\nThe testers may lobby for getting an end-to-end tracer bullet through the code quickly, so they can build an automation framework, and then ﬂesh it\n\n339\n\n340\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nout as the story development proceeds. If there are stories that present a big testing challenge, it might be good to do those early on. For example, if the release includes implementing a new third-party tool to create documents from templates and dynamic data, there are many permutations to test. If the team is unfamiliar with the tool, the testers can ask the team to consider do- ing those stories in the ﬁrst iteration of the release.\n\nWHAT’S IN SCOPE? Agile teams continually manage scope in order to meet business deadlines while preserving quality. High-value stories are the ﬁrst priority. Stories that are “nice-to-haves” might be elbowed out of the release.\n\nLisa’s Story\n\nOur team’s customers list their stories in priority order and then draw a line be- tween the stories that must be done before the release can occur, and the ones that could safely be put off. They call the less important stories “below the line,” and those stories may never get done.\n\nFor example, when we undertook the theme to allow retirement plan participants to borrow money from their retirement accounts, there was a “below the line” story to send emails to any participants whose loans are changing status to “pend- ing default” or “default.” When the loan is in “default” status, the borrower must pay taxes and penalties on the balance. The email would be extremely helpful to the borrowers, but it wasn’t as important to our business as the software to re- quest, approve and distribute loans, or process loan payments.\n\nThe email story didn’t make it into the release. It wasn’t done until more than two years later, after enough complaints from people who didn’t know their loans were going into default until it was too late.\n\nJanet worked with a team whose customers were under the misplaced as- sumption that all of the features would get into their release and that when they were prioritizing, they were just picking which stories got done ﬁrst. When the rest of the team realized the misunderstanding, they also imple- mented the idea of stories above and below the line. It helped to track progress as well as make the stories that were dropped below the line very visible.\n\nDeadlines and Timelines\n\nMany domains revolve around ﬁxed dates on the calendar. Retail businesses make most of their proﬁt during the holiday season. An Internet retail site is smart to have all new features implemented by October 1. Implementing a\n\n—Lisa",
      "page_number": 374
    },
    {
      "number": 46,
      "title": "Segment 46 (pages 383-392)",
      "start_page": 383,
      "end_page": 392,
      "detection_method": "topic_boundary",
      "content": "Janet’s Story\n\nLisa’s Story\n\nWHAT’S IN SCOPE?\n\nnew feature close to the peak buying period is risky. Lisa’s company’s cus- tomers must complete government-required tasks during certain periods of the year. When it’s too late for a feature to get released this year, it often gets put off for the next year, because more urgent priorities must be addressed. Regulatory changes have speciﬁc timelines, and organizations have no choice about the timeline.\n\nWhile working on this book, I was planning a release with my team at WestJet. We had several possible stories and worked with the customers to decide what the release would look like. We had one regulatory change that was light work for the programmers, but heavy for the testers. It needed to be in production by a certain date, so the other stories we were considering for the release took that into consideration.\n\nWe decided to create a small maintenance release with just that one major fea- ture, along with a few bugs from the backlog so the release of the regulatory change would not be jeopardized. While the testers completed their testing, the rest of the team started some of elaboration stories for the next release.\n\nAn alternative plan could have been that the programmers chip in and help test and ﬁt in more features. However, the whole team decided that this plan would work the best with the least amount of risk.\n\nFocus on Value\n\nIt’s rather easy for a team to start discussing a complex story and lose sight of what value the features actually deliver. Release planning is the time to start asking for examples and use cases of how the features will be used, and what value they’ll provide. Drawing ﬂowcharts or sample calculations on the white- board can help pinpoint the core functionality.\n\nOur product owner wrote a story to provide a warning if an employer overrides the date a participant becomes eligible to contribute to a retirement account after the participant has already made contributions.\n\nThe warning needed to be incorporated into the legacy UI code, which didn’t easily accommodate it. The team discussed how it might be implemented, but every option was fairly costly. Not only would coding be tricky, but a lot of time was needed to test it adequately and update existing automated tests. This fea- ture wouldn’t provide much value to the business, just a bit of help to the end users. The release was already pretty close to the limit on features.\n\nOne of the programmers suggested providing a report of participants who met the criteria so the plan administrators could simply call the employers who may\n\n341\n\n—Janet\n\n342\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nneed to make corrections. The report story was much smaller than the warning story, could easily ﬁt into the release, and was acceptable to the customer.\n\nThere is no guarantee that these initial “guesstimates” at what will be in a given release will hold up over time. That is why customers needs to under- stand their priorities, take checkpoints at the end of every iteration, and re- evaluate the priorities of remaining stories.\n\nSystem-Wide Impact\n\nWe talked about the “ripple effects” in Chapter 8, “Busi- ness Facing Tests that Support the Team.”\n\nOne of our jobs as testers is to keep the big picture in mind. The agile tester thinks about how each story might affect the system as a whole, or other sys- tems that ours has to work with. For example, if the toy warehouse makes a change to its inventory software, and the new code has a bug that overstates the number of items in stock, the website might sell more of the hot new doll than there are available, disappointing thousands of children and their par- ents at Christmas. When risk is high, listing areas of the system that might be affected for a theme or group of stories might be a worthwhile exercise even during release planning.\n\nContact points between our system and that of partners or vendors always merit consideration. Even a minor change to a csv or xml ﬁle format could have a huge impact if we don’t communicate it correctly to partners who ftp ﬁles to us. Stories that mean changes for third parties need to be done early enough in the release cycle to let the third parties make necessary changes.\n\nFigure 15-5 shows a simpliﬁed diagram of a new system that touches many pieces of the existing system. Different tools might be needed to test the integrations.\n\nTesters who have worked with some of the other systems or understand what testing needs to happen on those systems can offer valuable insight into the impact of a new story. Often, stories will need to be delayed until a future re- lease if the impact has not been explored. This is a good time to recall previ- ous releases that didn’t end so well.\n\nThird-Party Involvement\n\nWorking with vendor tools, partners, or other contractor teams on a big project complicates release planning. If anyone outside your team is respon- sible for some part of the project, that’s one piece that’s out of your control. If\n\n—Lisa\n\nWHAT’S IN SCOPE?\n\nABC System\n\nProfiles (NS Tool 1)\n\nStandard Reports\n\nSAP\n\nOverhead (NS Tool 2)\n\nLegacy System\n\nGas System\n\nNew System\n\nWeb Reports\n\nOMS\n\nFuel System\n\nConversion (NS Tool 3)\n\nFigure 15-5 System impacts\n\nyou need to coordinate with others, including possible new users of the sys- tem, it’s best to start early.\n\nLisa’s team has written several interfaces to allow users to upload data to their systems. In each case, they had to get the proposed ﬁle format out to the us- ers early to make sure it would work for them. Other projects involved send- ing data to partners or vendors. These required extra planning to arrange testing with their test systems and getting their feedback on whether data was valid and correctly formatted.\n\nIf you’re using a third-party product as part of your solution, you might as- sume it has been tested, but that might be a poor assumption. You will need to budget extra time to test your application in conjunction with the vendor software. If there’s a problem in the other company’s software, it might take a long time to resolve. Lisa’s team uses third-party software for critical tasks such as document creation. If a theme includes modifying or creating new documents, they plan extra time to upgrade the software if needed, and extra\n\n343\n\n344\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\ntime for testing in case ﬁxes are needed. If possible, bring the third-party software into the project early, and start end-to-end testing. The more you can work with the interface, the better off you’ll be.\n\nOther third-party software that we often forget about until it’s too late is our own testing environments. Sometimes a team will incorporate new code that takes advantages of new features in their chosen language. For example, if team members are using AJAX or JavaScript, they may need to upgrade the software development kit they’re using. This means that a team will have to upgrade its production runtime environment as well, so take that into con- sideration and test early.\n\nClients or partners might have concerns about a release that isn’t within your own team’s scope. Lisa’s team was once prevented at the very last minute from releasing a new feature because a partner didn’t have time to okay the change with their legal advisors. The programmers had to quickly devise a way to turn the functionality off without requiring extensive additional test- ing. Interestingly, partners who aren’t using agile development sometimes have trouble meeting their own deadlines. They might be unprepared when your team meets the deadline.\n\nJanet’s Story\n\nI worked on a project to implement a feature that required a new piece of hard- ware for scanning a new 2D bar code. The team decided to implement in stages because it was not known when the scanners would be available for full testing, but the customer wanted the code ready for when the scanners arrived.\n\nThe initial phase was programmer-intensive because there was a lot of research to be done. After they determined how they would implement the feature, the story was created to add it into the code. However, we knew we couldn’t thoroughly test it until the scanners were available. The code was ready to test, but instead of backing it all out, we only needed to worry about testing that the feature could be turned off for the release. The next release would require more testing, but only if the scanners were available. The testing of the story was kept in the prod- uct backlog so we would not forget to do it.\n\nIf you’ll be working with other teams developing different components of the same system, or related systems, budget time to coordinate with them. It’s a good idea to designate a member from each team to coordinate together.\n\nRelease planning is the time to identify extra roles you need on your team, ad- ditional resources, and time needed for out-of-the-ordinary circumstances.\n\n—Janet\n\nChapter 8, \"Business-Facing Tests that Support the Team,\" ex- plains how to identify steel threads or thin slices in a story or theme.\n\nTEST PLANNING\n\nTEST PLANNING We can’t expect to plan the iterations in a release at a detailed level. We can get an idea of the theme’s steel threads, prioritize stories, and make a guess at what stories will be in which iteration. Detailed test planning needs to wait for iteration planning. Still, we need to think about testing at a high level, and try to budget enough time for it. We might even take time separately from the release planning meeting to strategize our testing for the release. In Chapter 8, Business-Facing Tests that Support the Team, we mentioned one of the perils of agile testing: “forgetting the big picture.” Test Planning will help you with that problem.\n\nWhere to Start\n\nDuring release planning, it’s helpful to know the business conditions of satis- faction for each story or high-level user acceptance test case. When stories need clariﬁcation, agile testers ask for examples. At this stage, examples will be high-level, covering just the basics, but enough to be able to size and pri- oritize the story. Drawing ﬂowcharts or writing calculations on the white- board and discussing them helps us identify project-speciﬁc testing issues.\n\nAt a minimum, the team needs to understand the top-priority stories that are scheduled to be performed ﬁrst. Lightweight planning might involve only looking at those core stories with the understanding that more time will be needed for deﬁning additional tests.\n\nAs we get a sense of which stories will probably be included in the release, we can start thinking about the scope of the testing. What assumptions have been made that might affect testing? Use of third-party software, such as the example of using a shipping company’s shipping calculation API, affects test planning. Are there any unusual risks in this release that will impact testing? If we have stories to implement batch jobs, and we’ve never had any batch processing in the system before, there are probably new frameworks that im- pact testing. We need to budget time to learn them.\n\nWhy Write a Test Plan?\n\nIn release planning, we talk about the purpose of the release, what’s in scope, and what assumptions we’re making. We do some quick risk analysis and plan our test approach to address those risks. We consider automation and what we need for test environments and test data. We certainly want to iden- tify milestones and deliverables. Hmmm, this is starting to sound like . . . a test plan!\n\n345\n\n346\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nSee Chapter 5, “Transitioning Typical Processes,” for more about test plans and test strategies.\n\nIf, like ourselves, you spent time working in a traditional waterfall environ- ment, you might have wasted time writing bulky test plans that nobody read and nobody bothered to maintain. In agile development, we want our test plans to serve our needs. Your customer might require a test plan for each re- lease for compliance reasons. Even if it’s not a required deliverable, it can be useful. Keep it concise and lightweight. It only has to serve your purposes during this release. Address the testing issues that are speciﬁc to this release or project. Include your risk analysis and identify assumptions. Outline the critical success factors that your customer has identiﬁed. Think about what people need to know related to testing, and remove anything extraneous.\n\nEven if you don’t create a formal test plan, be sure you have made note of all these different testing factors involved in the release. You’ll want to keep them in mind during every iteration planning session. The biggest beneﬁt in test planning is the planning itself. It allows you to consider and address issues such as test data requirements, infrastructure, or even what test results are re- quired. Test planning is a risk mitigation strategy. Let’s consider some of these issues.\n\nTypes of Testing\n\nIn Part III, we covered the four quadrants of testing and talked about all of the different types of testing you can do during your project. Release plan- ning is a good time to consider these different needs. Do you need to plan to bring in a load test tool, or will there be the need to build some kind of spe- cialty test harness?\n\nIt could be that your next release is just an extension of your last, and you will just carry on creating your examples, automating your story tests, and doing the rest of the testing as you’ve been doing. You are one of the lucky ones. For those of you who are starting a brand new project with no previous processes in place, now is the time to consider what testing you will need. We don’t mean you have to decide how to test each story, but look at the big pic- ture and think about the quadrants. Will you need to plan for a special UAT, or will the iteration demos be enough? It is important to raise these issues early so the team can plan for them.\n\nInfrastructure\n\nWhile you are doing your test planning, you need to consider your infra- structure. Infrastructure can mean your continuous integration setup, test environments, and test database. It can mean how you promote your builds\n\nLisa’s Story\n\nTEST PLANNING\n\nto your test environments. It might mean your test lab, if you have one, or having a separate server to run all your automation tests. These are generally pieces of infrastructure that need some lead time to get in place. This is the time to make a plan.\n\nSome types of testing might require extra effort. My team had a tool to do perfor- mance testing and some scripts, but we lacked a production-style environment where we could control all of the variables that might affect performance. For ex- ample, the test database was shared by testers, programmers, and two build pro- cesses. Slower performance might simply mean someone was running database- intensive tests. We used our staging environment to get a baseline, but it was miss- ing some of the components of production. We set a six-month goal to acquire hardware and software for a proper test environment and get it set up. We wrote a task card or two each iteration to establish the environment step by step.\n\nWhatever your needs are, make sure you understand them and can plan for what you need. If you don’t have the right infrastructure, then you will waste time trying to get it together and cause a bottleneck in mid-iteration.\n\nTest Environments\n\nAs we look at the types of features in the next release, we might see the need for a whole new test environment. Think about specialized test environments you may need as well. Will you need more tools? Do you need to expand your test lab so that you can test with different browsers and operating systems? This is the time to think about all testing considerations.\n\nIf you’re planning your ﬁrst release, test environments are a key consider- ation. You might need a story or iteration just to set up the infrastructure you need. We’ve started more than one project where the only place we could test was the development environment. We found that doesn’t work very well, because the environment is never stable enough for effective testing.\n\nJust as programmers have their own sandboxes to work and test in, it works well if each tester has that same availability and control. We recognize that not all applications lend themselves to this, but at the very least, you need to know what build you’re testing. You also need test data that others will not walk over with their tests. If you don’t have a testing sandbox that’s under your own control, take time to plan what you need to establish for your test environments. Brainstorm with your team about how you can obtain the\n\n347\n\n—Lisa\n\n348\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nnecessary hardware and software. It might take time, so develop a Plan B for getting something done while waiting for the infrastructure you need.\n\nIf you’re working on a large system, you may have to queue up along with other teams to get time on a test or staging environment that includes all of the various pieces of software with which yours must work. This staging en- vironment should mimic your production system as much as possible. If your organization doesn’t have someone responsible for creating environ- ments, your team might need extra roles dedicated to obtaining the test envi- ronments you need. These roles might involve working with other teams as well. Release planning is the time to consider all of these testing infrastruc- ture requirements.\n\nTest Data\n\nRelease or theme planning is also a good time to think about what test data you might need during the project.\n\nUsing test data that closely resembles real data is generally a good practice. Plan for the data you need. We’ve had the opportunity in several organiza- tions to use a copy of production data. Real data provides a good base for different scenarios for exploratory testing. Production data may need to be “scrubbed” before it’s used for testing in order to remove any sensitive in- formation such as identiﬁcation or bank account numbers. The data needs to be altered to hide the original values but remain valid so that it doesn’t violate database restrictions. Because it takes time for database experts to port production data to a test environment, be sure they’re included in your planning.\n\nJanet’s Story\n\nIn one of the organizations I was working with, we used two different baseline test data schemes. For our individual test environments, we used Fit ﬁxtures to load predeﬁned data. We tried to make this data as close to production as possi- ble, but we also seeded it with some very speciﬁc test data. Every time we checked out a new version of code, we were able to reload a base set of data. In this way, we also tested the database schema as well to see if anything had changed.\n\nFor our more stable test environment where we wanted data persisted, we used the data migration scripts that the programmers developed as they made data- base changes. These migration scripts were eventually used for the initial cut over from production and by then we were pretty certain they were correct.\n\n—Janet\n\nChapter 14, “An Agile Test Automa- tion Strategy,” ex- plores different approaches to ob- taining test data\n\nTEST PLANNING\n\nEnlist your customers’ support in obtaining meaningful test data. If you’re working on a story that involves sending a ﬁle to a third-party vendor, your business expert can ﬁnd out what data the vendor expects in the ﬁle. Lisa’s team developed features to allow retirement plan brokers to offer their cus- tomers portfolios of mutual funds. They asked the product owner to provide samples of portfolios, including a name, description, and set of funds for each. This helped them test with realistic data.\n\nTest data tends to get stale and out of date over time. Older data, even if it came from production, may no longer accurately reﬂect current production data. A “passing” test using data that’s no longer valid gives a misleading sense of conﬁdence. Continually review your test data needs. Refresh data or create it using a new approach, as needed.\n\nTest data requirements vary according to the type of testing. Regression tests can usually create their own data or run against a small representational set of data that can be refreshed to a known state quickly. Exploratory testing may need a complete replica of production type data.\n\nTest Results\n\nDifferent teams have different requirements for test result reporting. Think about how you are going to report test results at this stage of the game so that you can do so effectively when the time comes to do the actual reporting. Your organization may have audit compliance requirements, or maybe your customer just wants to know how you tested. Understand your needs so that you can choose the approach that is right for your team.\n\nThere are many ways to report test results. There are vendor tools that will record both automated and manual results. Your team may ﬁnd a way to per- sist the results from tools such as Fit, or you may just choose to keep a big visible manual chart.\n\nThe approach that a few teams have taken is to create home-grown test result applications. For example, a simple Ruby application written with Ruby on Rails for the database or a MySQL database with a PHP front end can make a very simple but easy-to-use test management system.\n\nA tool such as this can be very simple or can include added complexity such as the capability to categorize your tests. The important thing is the test re- sults. If your automated tests record their pass or fail result along with the er- ror, you have some history to help determine fragility of the test.\n\n349\n\n350\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nYour team can conﬁgure your automated build process to provide test results from each build, by email, or a feedback utility or web interface that team members can view online. Results over time can be summarized in a variety of formats that make progress visible. One of Lisa’s teams produced a daily graph of tests written, run, and passing that was posted in the team’s work area. An- other produced a daily calendar with the number of unit tests passing every day. Even simple visual results are effective.\n\nWe talk about some of the metrics you can use later in this chapter.\n\nTEST PLAN ALTERNATIVES We’ve talked about why to test plan and what you should consider. Now we talk about some of the alternatives to the heavy test plans you may be used to. Whatever type of test plan your organization uses, make it yours. Use it in a way that beneﬁts your team, and make sure you meet your customer’s needs. As with any document your team produces, it should fulﬁll a purpose.\n\nLightweight Test Plans\n\nIf your organization or customer insists on a test plan for SOX compliance or other regulatory needs, consider a lightweight test plan that covers the neces- sities but not any extras. Do not repeat items that have already been included in the Project Plan or Project Charter. A sample Test Plan might look some- thing like the one shown in Figure 15-6.\n\nA test plan should not cover every eventuality or every story, and it is not meant to address traceability. It should be a tool to help you think about test- ing risks to your project. It should not replace face-to-face conversation with your customer or the rest of your team.\n\nUsing a Test Matrix\n\nJanet uses release planning to work with the testers and customers to develop a high-level test matrix. A test matrix is a simple way to communicate the big picture concerning what functionality you want to test. It gives your team a quick overview of the testing required.\n\nA test matrix is just a list of functionality down the side and test conditions across the top. When thinking about test conditions and functionality, con- sider the whole application and any impact the new or changed functionality",
      "page_number": 383
    },
    {
      "number": 47,
      "title": "Segment 47 (pages 393-400)",
      "start_page": 393,
      "end_page": 400,
      "detection_method": "topic_boundary",
      "content": "TEST PLAN ALTERNATIVES\n\nProject ABC Test Plan Prepared by: Janet Gregory and Lisa Crispin\n\nIntroduction The Test Plan is intended as a baseline to identify what is deemed in and out of scope for testing, and what the risks and assumptions are.\n\nResourcing\n\nTester Janet Lisa\n\n% Committed 100% 50%\n\nIn Scope Testing includes all new functionality, identiﬁed high-risk regression suite functionality, UAT, and Load Testing. Localization is part of this project. Manual regression tests deemed low priority will be run if time permits.\n\nOut of Scope Actual translation testing is outsourced, so it is not part of this test plan.\n\nNew Functionality The following functionality is being changed in this release.\n\nFeature Description Adding new toggle for language selection on home page\n\nDepth of Testing Testing all 5 languages (English, Spanish, French, Italian, and German). Testing that we are able to dynamically switch languages.\n\nPerformance & Load Testing Load testing will concentrate on the following areas. Load testing details will be found in the Load Test Plan document [link to Load Test Plan].\n\nUAT (User Acceptance Testing) UAT will be performed and coordinated with the Paris ofﬁce as well as the Calgary ofﬁce. Users will be chosen for their expertise in select areas and transactions as well as being ﬂuent in one of the following languages: German, Italian, Spanish, or French.\n\nInfrastructure Considerations The test lab will need all 5 languages installed and available for testing.\n\nAssumptions Translation has been tested before being delivered to project team.\n\nRisks The following risks have been identiﬁed and the appropriate action identiﬁed to miti- gate their impact on the project. The impact (or severity) of the risk is based on how the project would be affected if the risk was triggered.\n\n# 1\n\nRisk Users aren’t ready for UAT\n\nImpact HIgh\n\nMitigation Plan\n\nFigure 15-6 Sample Test Plan\n\n351\n\n352\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nmight have on the rest of the application. Testers sitting with customers and thinking about test conditions is what is important.\n\nIt can also be a mechanism to track coverage and can be as detailed as you like. A high-level test matrix can be used by the team to show the customer team or management what has been tested already and what is left. A more detailed test matrix can be used by the team to show what is planned for test- ing and track the progress of the testing. After the matrix has been created, it becomes easy to ﬁll in the squares when testing is done. Keep it simple. Be- cause we like big visible charts that are easy to read, we recommend colors that mean something to your team. For example, green (G) means testing is done and the team is happy with it, while yellow (Y) might mean some test- ing has been done but more exploratory testing is needed if there is time. Red (R) means something is broken. A white square means it hasn’t been tested yet, and a gray (not applicable) square means it doesn’t need to be tested.\n\nLet’s look at an example. We have a small release we want to put out that cal- culates shipping costs. In Figure 15-7, different pieces of functionality are represented on one axis, and properties of the shipment are represented on the other. Individual cells are color-coded to show which cases are tested and which need more attention. All of the cells for “<= 2 lbs” are ﬁnished, the top three cells for > 4 lbs are done but need more exploratory testing, and the “Ship to Alaska”/“>4 lbs” cell denotes a possible issue.\n\nShipping Test Matrix\n\nTest Conditions\n\nFunctionality\n\nn o i t a n\n\ni t s e D e\n\nl\n\ng n\n\ni\n\nS\n\ns n o i t a n\n\ni t s e D e p i t l\n\nl\n\nu M\n\ns s e r d d A\n\nl a c i s y h P\n\ns b\n\nl\n\n2 = <\n\ns b\n\nl\n\n4\n\n–\n\n2\n\ns b\n\nl\n\n4 >\n\ny a D e m a S\n\ny a D t x e N\n\ns y a D s s e n i s u B\n\n5 <\n\nShip within US\n\nG\n\nG\n\nY\n\nShip to Canada\n\nG\n\nY\n\nShip to Hawaii\n\nG\n\nY\n\nShip to Alaska\n\nG\n\nR\n\nShipping estimates\n\nG\n\nFigure 15-7 A sample test matrix\n\ns e t a m i t s E g n p p h S\n\ni\n\ni\n\nn/a\n\nJanet’s Story\n\nTEST PLAN ALTERNATIVES\n\nI had an unexpected side effect from using a test matrix in one project I was on. The customers and testers put the test matrix together, and had thought of all af- fected functionality for the project and the high-level test conditions they would need. As expected, the act of planning brought a lot of issues out that would have been missed until later.\n\nWhen they hung the matrix on the wall in their team area, Dave, the developer team lead, expressed an interest. One of the testers explained the matrix to him, and I was surprised when he said it was very useful for them as well. Dave said “I didn’t know that this functionality would affect this area. We need to make sure our unit tests touch on this as well.”\n\nLooking back on this, I shouldn’t have been surprised, but I had never had that ex- perience with the programmers before.\n\nA test matrix is a very powerful tool and can be used to help address trace- ability issues if your team has those problems. Think about what makes sense for your team and adapt it for your team and what makes sense to you.\n\nTest Spreadsheet\n\nJanet has also seen a spreadsheet format used with some success. For exam- ple, at WestJet, the ﬁrst tab in a workbook was a high-level list of functional- ity that existed in the application. For each row, the team determined if the project affected that piece of functionality. If so, they gave a rating of the ex- pected impact. After the impact of the changes had been determined, deci- sions about test environments, test data, or UAT could then be made.\n\nTabs were used for risks and assumptions but could be used for anything your team may need. A ﬂexible format such as a spreadsheet means you can tailor it to work for you.\n\nThis information can be used in a number of different ways. It can be used to determine where to concentrate your exploratory testing efforts, or maybe to help create a high-level test matrix to make sure you touch on all of the areas during your testing.\n\nA Whiteboard\n\nIf your team is informal and has small releases, any kind of documentation may be too much. Sometimes it’s enough to list the risks and assumptions on a whiteboard or on index cards. Janet has used a whiteboard to manage risks,\n\n353\n\n—Janet\n\n354\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nand it worked quite well. If a risk actually became an issue, the result was documented and crossed off. It was easy to add new risks and mitigation strategies, and the list was visible to the whole team. This could also be done on a wiki page.\n\nWe cannot stress enough that you need to know your team and its needs.\n\nAutomated Test List\n\nSometimes you may be required to present more information to your cus- tomers, such as a list of test cases. If your team has a tool from which you could extract a list of test case names, you could provide this list easily to any- one who needed it. This would present more of a traditional type detailed test plan but wouldn’t be available until after tests were actually written. We don’t recommend spending any time on this because we don’t see added value, but sometimes this list may be required for risk assessment or auditability.\n\nPREPARING FOR VISIBILITY If your team is just getting started with agile development, make sure you have necessary infrastructure in place for your early iterations. You may change the way you are tracking progress as you go along, and your retrospectives will help you bring these issues to light. If you’re having problems completing the work planned for each iteration, maybe you need more visible charts or visual aids to help you gauge progress and make mid-iteration adjustments. Do your customers have some way to know how the iteration is progressing and which stories are done? Take time before the each iteration to evaluate whether you’re getting the right kind of feedback to keep track of testing.\n\nTracking Test Tasks and Status\n\nThe effective agile teams we know all follow this simple rule: “No story is done until it’s tested.” This rule can be expanded to say that not only must the story be tested, the code must be checked in, it must have automated tests that are run by a continual build process, it must be documented, or whatever your team’s “doneness” criteria are. At any time during an iteration, you need to be able to quickly assess how much testing work remains on each story, and which stories are “done.” Story or task boards are ideal for this purpose, especially if they use color-coding to denote test tasks vs. development and other types of tasks. Cork boards, steel sheets with magnets, poster-sized sticky notes, or whiteboards all work ﬁne. Give each story its own row, and order them by pri- ority. Have columns for “to do,” “work in progress,” “verify,” and “done.”\n\nJanet’s Story\n\nJanet’s Story\n\nPREPARING FOR VISIBILITY\n\nI started with team members who had been doing agile for a few months with only a couple of programmers and one tester. They had been using XPlanner to track their tasks and stories, and it was working ok for them. At the same time I came on board, a couple of new programmers were added, and the stand-ups became less effective; the team was not completing the stories it had planned. I suggested a storyboard, and although they were skeptical about keeping two sets of “tasks,” they said they would try it.\n\nWe took an open wall and used stickies to create our story board. We started hav- ing stand-ups in front of the story board and our discussion became more spe- ciﬁc. It provided a nice visible way of knowing when the tasks were done and what was left to do. After a couple of months, the team grew again and we had to move the story board into an ofﬁce. We also moved our stand-ups and our test re- sult charts there. However, the constant visibility was lost, and programmers and testers stopped moving their tasks.\n\nWe had to reevaluate what we wanted to do. One size does not ﬁt all teams. Make sure you plan for what is right for your team.\n\nSome teams use different colored index cards for the different types of tasks: green for testing, white for coding, yellow and red for bugs. Other teams use one card per development task, and add different colored stickers to show that testing is in progress or show that there are bugs to resolve. Use any method that lets you see at a quick glance how many stories are “done,” with all cod- ing, database, testing, and other tasks completed. As the iteration progresses, it’s easy to see if the team is on track, or if you need to pull a story out or have programmers pitch in on testing tasks.\n\nOur story board (shown in Figure 15-8) wasn’t very big, and we didn’t have a lot of wall space to expand to have the regular column-type task board. Instead, we decided to use stickers to designate the status.\n\nWhite cards, such as those shown in the ﬁrst row of Figure 15-8, were regular tasks, blue cards designated technical stories such as refactoring or spikes, and pink cards, shown toward the right-hand side of the board as the darkest color, were bugs that need to be addressed. It is easy to see that this picture was taken at the beginning of an iteration because there are no colored circles on each card. In the top right-hand corner, you can see the legend. Blue stickers meant it has been coded, green would indicate done (tested), and red meant the task has been deemed not completed or a bug was rejected as not ﬁxed. As a task or story was completed (i.e., green sticker), it was moved to the right of the board.\n\n355\n\n—Janet\n\n—Janet\n\n356\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nFigure 15-8 Example story board\n\nLisa’s Story\n\nFor more than four years, our story board was a couple of sheets of sheet metal, painted in company colors, using color-coded index cards attached to the board with magnets. Figure 15-9 shows a picture of it early in an iteration. Our task cards were also color-coded: white for development tasks, green for coding tasks, yel- low and red for bugs, and striped for cards not originally planned in the iteration. The board was so effective in indicating our progress that we eventually stopped bothering with a task burndown chart. It let us focus on completing one story at a time. We also used it to post other big visible charts, such as a big red sign show- ing the build had failed. We loved our board.\n\nThen, one of our team members moved overseas. We tried using a spreadsheet along with our physical story board, but our remote teammate found the spread- sheet too hard to use. We tried several software packages designed for Scrum teams, but they were so different from our real story board that we couldn’t ad- just to using them. We ﬁnally found a product (Mingle) that looked and worked enough like our physical board that everyone, including our remote person, could use it. We painted our old story board white, and now we can project the story board on the wall during stand-up meetings.\n\n—Lisa\n\nPREPARING FOR VISIBILITY\n\nFigure 15-9 Another sample story board\n\nDistributed teams need some kind of online story board. This might be a spreadsheet, or specialized software that mimics a physical story board as Mingle does.\n\nCommunicating Test Results\n\nEarlier, we talked about planning how to track test results. Now we want to talk about effectively communicating them. Test results are one of the most important ways to measure progress, see whether new tests are being written and run for each story, and whether they’re all passing. Some teams post big visible charts of the number of tests written, run, and passed. Others have their build process email automated test results to team members and stake- holders. Some continuous integration tools provide GUI tools to monitor builds and build results.\n\nWe’ve heard of teams that have a projector hooked up to the machine that runs FitNesse tests on a continuous build and displays the test results at all times. Test results are a concrete depiction of the team’s progress. If the number of\n\n357\n\n358\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\ntests doesn’t go up every day or every iteration, that might indicate a problem. Either the team isn’t writing tests (assuming they’re developing test-ﬁrst), or they aren’t getting much code completed. Of course, it’s possible they are rip- ping out old code and the tests that went with it. It’s important to analyze why trends are going the wrong way. The next section gives you some ideas about the types of metrics you may want to gather and display.\n\nHowever your team decides they want to communicate your progress, make sure you think about it up front and everyone gets value from it.\n\nRelease Metrics\n\nWe include this section here, because it is important to understand what metrics you want to gather from the very beginning of a release. These met- rics should give you continual feedback about how development is proceed- ing, so that you can respond to unexpected events and change your process as needed. Remember, you need to understand what problem you are trying to solve with your metrics so that you can track the right ones. The metrics we talk about here are just some examples that you may choose to track.\n\nNumber of Passing Tests Many agile teams track the number of tests at each level: unit, functional, story tests, GUI, load, and so on. The trend is more important than the num- ber. We get a warm fuzzy feeling seeing the number of tests go up. A number without context is just a number, though. For example, if a team says it has 1000 tests, what does that mean? Do 1000 tests give 10% or 90% coverage? What happens when code that has tests is removed?\n\nTracking the number of tests written, running, and passing at a story level is one way to show a story’s status. The number of tests written shows progress of tests to drive development. Knowing how many tests aren’t passing yet gives you an idea of how much code still needs to be written.\n\nAfter a test passes, it needs to stay “green” as long as the functionality is present in the code. Graphs of the number of tests passing and failing over time show whether there’s a problem with regression failures and also show the growth of the code base. Again, it’s the trend that’s important. Watch for anomalies.\n\nThese types of measurements can be reported simply and still be effective.",
      "page_number": 393
    },
    {
      "number": 48,
      "title": "Segment 48 (pages 401-408)",
      "start_page": 401,
      "end_page": 408,
      "detection_method": "topic_boundary",
      "content": "PREPARING FOR VISIBILITY\n\nFigure 15-10 Full build result email from Lisa’s team\n\nLisa’s Story\n\nMy team emails a color-coded calendar out every day showing whether the “full build” with the full suite of regression tests passed each day (see Figure 15-10). Two “red” days in a row (the darkest color) are a cause for concern and noticed by management as well as the development team. Seeing the visual test results helps the organization pull together to ﬁx the failing tests or any other problems causing the build to not run, such as hardware or database issues.\n\nThere are different ways to measure the number of tests. Choose one and try to stay consistent across the board with all types of tests, otherwise your met- rics may get confusing. Measuring the number of test scripts or classes is one way, but each one may contain multiple individual test cases or “asserts,” so it may be more accurate to count those.\n\nIf you’re going to count tests, be sure to report the information so that it can be used. Build emails or build status UIs can communicate the number of tests run, passed, and failed at various levels. The customer team may be content\n\n359\n\n—Lisa\n\n360\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nto see this information only at the end of each sprint, in the sprint review, or an email.\n\nWhatever metrics you choose to gather, be sure the team buys into them.\n\nJanet’s Story\n\nI started a new contract with a team that had been doing agile for a couple of years, and they had developed a large number of automated functional tests. I started keeping track of the number of tests passing each day. The team didn’t see a problem when the trending showed fewer and fewer tests were passing. The unit tests were maintained and were doing what they were supposed to do, so the team felt conﬁdent in the release. It seemed this happened with every re- lease, and the team would spend the last week before the release to make all of the tests pass. It was costly to maintain the tests, but the team didn’t want to slow down to ﬁx them. Everyone was okay with this except me.\n\nI did not see how ﬁxing the tests at that late date could ensure the right expected results were captured. I felt that we ran the risk of getting false positives.\n\nAt the start of the next release cycle, I got the team to agree to try ﬁxing the tests as they broke. It didn’t take long for the team to realize that it wasn’t so tough to ﬁx the tests as soon as we knew they were broken, and we found a lot of issues early that hadn’t usually been caught until much later. The team soon set a goal of having 95% of the tests passing at all times.\n\nWe also realized how brittle the tests were. The team made a concerted effort to refactor some of the more complex tests and eliminate redundant ones. Over time, the number of high-level tests was reduced, but the quality and coverage was increased.\n\nWe started out measuring passing rates, but we ended up with far more.\n\nDon’t get so caught up in the actual measurements that you don’t recognize other side effects of the trending. Be open to adjusting what you are measur- ing if the need is there.\n\nCode Coverage Code coverage is another traditional metric. How much of our code is exer- cised by our tests? There are excellent commercial and open source code cov- erage tools available, and these can be integrated into your build process so that you know right away if coverage has gone up or down. As with most met- rics, the trend is the thing to watch. Figure 15-11 shows a sample code coverage report.\n\n—Janet\n\nPREPARING FOR VISIBILITY\n\n361\n\nGHIDRAH\n\nOverall Coverage Summary\n\nName all classes\n\nClass, % 95% (1727/1809)\n\nMethod, % 77% (13605/17678)\n\nBlock, % 72% (201131/279707)\n\nLine, % 75% (43454.5/58224)\n\nOverall Stats Summary\n\ntotal packages: total executable ﬁles: total classes: total methods: total executable lines:\n\n240 1329 1809 17678 58224\n\nWHITNEY\n\nOverall Coverage Summary\n\nName all classes\n\nClass, % 15% (109/737)\n\nMethod, % 8% (669/8760)\n\nBlock, % 4% (16292/363257)\n\nLine, % 5% (3713.7/80358)\n\nOverall Stats Summary\n\ntotal packages: total executable ﬁles: total classes: total methods: total executable lines:\n\n46 655 737 8760 80358\n\nFigure 15-11 Sample code coverage report from Lisa’s team. “Ghidrah” is the new architecture; “Whitney” is the legacy system.\n\nFigures 15-12 and 15-13 are two examples of trends that work together. Figure 15-12 shows a trend of the total number of methods each iteration. Fig- ure 15-12 is the matching code coverage. These examples show why graphs need to be looked at in context. If you only look at the ﬁrst graph showing the number of methods, you’ll only get half the story. The number of meth- ods is increasing, which looks good, but the coverage is actually decreasing. We do not know the reason for the decreased coverage, but it should be a trigger to ask the team, “Why?”\n\nRemember that these tools can only measure coverage of the code you’ve written. If some functionality was missed, your code coverage report will not\n\n362\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nFigure 15-12 Number of methods trend\n\nFigure 15-13 Test coverage\n\nPREPARING FOR VISIBILITY\n\nbring that to light. You might have 80% code coverage with your tests, but you’re missing 10% of the code you should have. Driving development with tests helps avoid this problem, but don’t value code coverage statistics more than they deserve.\n\nKnow What You Are Measuring\n\nAlessandro Collino, a computer science and information engineer with Onion S.p.A. who works on agile projects, told us about an experience where code coverage fell suddenly and disastrously. His agile team developed middle- ware for a real-time operating system on an embedded system. He explained:\n\nA TDD approach was followed to develop a great number of good unit tests oriented to achieve good code coverage. We wrote many effective acceptance tests to check all of the complex functionalities. After that, we instrumented the code with a code coverage tool and reached a statement coverage of 95%.\n\nThe code that couldn’t be tested was veriﬁed by inspection, leading them to declare 100% of statement coverage after ten four-week sprints.\n\nAfter that, the customer required to us to add a small feature before we delivered the software product. We implemented this request and ap- plied the code optimization of the compiler.\n\nThis time, when we ran the acceptance tests, the result was disastrous; 47% of acceptance tests failed, and the statement coverage had fallen down to 62%!\n\nWhat happened? The problem turned out to be due to enabling compiler optimization but with an incorrect setting. Because of this, a key value was read once as the application started up and was stored in a CPU register. Even when the variable was modiﬁed in memory, the value in the CPU register was never replaced. The routine kept reading this same stale value instead of the correct updated value, causing tests to fail.\n\nAlessandro concludes, “The lesson learned from this example is that the en- abling of the compiler optimization options should be planned at the beginning of the project. It’s a mistake to activate them at the ﬁnal stages of the project.”\n\nGood metrics require some good planning. Extra effort can give you more meaningful data. Pierre Veragen’s team members use a break-test baseline technique to learn if their code coverage metric is meaningful. They manu- ally introduce a ﬂaw into each method and then run their tests to make sure the tests catch the problem. Some tests just make sure the code returns some value, any value. Pierre’s team makes sure the tests return the correct value. In this way, they can determine whether their test coverage is good enough.\n\n363\n\n364\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nCode coverage is just one small part of the puzzle. Use it as such. It doesn’t tell you how good your tests are but only if a certain chunk of code was run during the test. It does not tell you if different paths through the application were run, either. Understand your application and try identifying your highest risk areas, and set a coverage goal that is higher for those areas than for low-risk areas. Don’t forget to include your functional tests in the coverage report as well.\n\nDefect Metrics As your team sets goals related to defects, use appropriate metrics to measure progress toward those goals. There are trends that you will want to monitor for the whole release, and there are ones that are iteration-speciﬁc. For exam- ple, if you’re trying to achieve zero defects, you may want to track the open bugs at the end of each iteration, or how many bugs were found after devel- opment but before release. Most of us are interested in knowing how many defects have been reported after the code is in production, which is some- thing completely different altogether. These issues will tell you after the fact how well your team did on the last release, but not how well you are doing on the current release. They may give you some indication of what processes you need to change to reduce the number of defects. Lisa’s team is more con- cerned with production defects found in the “new” code that was rewritten in the new architecture. They’re working hard to produce this new code with zero defects, so they need to know how well they’re doing. They expect that bugs will be found fairly often in the legacy system, where only the most crit- ical functionality is covered by automated GUI smoke tests, and there are few automated unit and behind-the-GUI tests.\n\nKnowing the defect rate of legacy code might be good justiﬁcation for refac- toring or rewriting it, but the team’s top priority is doing a good job with the new code, so they group bugs by “new” and “old” code, and focus on the “new” bugs.\n\nMore on defect tracking systems can be found in Chapter 5, “Transi- tioning Traditional Processes.”\n\nMake sure your bug database can track what you want to measure. You may have to make some changes in both the database and your process to get the data you need. For example, if you want to measure how many defects were found in production after a release, you have to make sure you have environ- ment and version as mandatory ﬁelds, or make sure that people who enter bugs always ﬁll them in.\n\nBecause defect tracking systems are often used for purposes besides tracking bugs, be sure not to muddle the numbers. A request for a manual update to the database doesn’t necessarily reﬂect an issue with the existing code. Use your defect tracking tool properly to ensure that your metrics are meaningful.\n\nLisa’s Story\n\nPREPARING FOR VISIBILITY\n\nPeriodically evaluate the metrics you’re reporting and see if they’re still relevant. Figure 15-14 shows two defect reports that Lisa’s team used for years. When we ﬁrst transitioned to agile, managers and others looked at these reports to see the progress that resulted from the new process. Four years later, our ScrumMaster found that nobody was reading these reports anymore, so we quit producing them. By that time, rates of new defects had reduced dramatically, and nobody really cared about the old defects still hanging about in the legacy code.\n\nFigure 15-14 Sample defect reports used (and no longer used) by Lisa’s team\n\n365\n\n—Lisa\n\n366\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nRelease planning is a good time to evaluate the ROI of the metrics you’ve been tracking. How much effort are you spending to gather and report the metrics? Do they tell you what you need to know? Does the code you release meet your team’s standards for internal quality? Is the code coverage percent- age going up? Is the team meeting its goals for reducing the number of de- fects that get out to production? If not, was there a good reason?\n\nMetrics are just one piece of the puzzle. Use your release, theme, or project planning meetings to refocus on delivering business value when the business needs it. Take some time to learn about the features you’re about to develop. Don’t get caught up with committing to your plans—the situation is bound to change. Instead, prepare for doing the right activities and getting the right resources in time to meet the customers’ priorities.\n\nSUMMARY As your team puts together its plan for a new theme or release, keep the main points of this chapter in mind.\n\n(cid:2) When sizing a story, consider different viewpoints, including business value, risk, technical implementation, and how the feature will be used. Ask clarifying questions, but don’t get bogged down in details. (cid:2) Testers can help identify the “thin slice” or “critical path” through a\n\nfeature set to help prioritize stories. Schedule high-risk stories early if they might require extra testing early.\n\n(cid:2) The size of testing effort for a story helps determine whether that\n\nstory is in scope for the release.\n\n(cid:2) Testers can help the team think about how new stories will impact the\n\nlarger system.\n\n(cid:2) Plan for extra testing time and resources when features may affect sys-\n\ntems or subsystems developed by outside teams.\n\n(cid:2) As the team identiﬁes the scope of the release, evaluate the scope of\n\ntesting and budget enough time and resources for it.\n\n(cid:2) Spend some time during release planning to address infrastructure,\n\ntest environment, and test data concerns.\n\n(cid:2) A lightweight, agile test plan can help make sure all of the testing con- siderations are addressed during the life of the release or project. (cid:2) Consider alternatives to test plans that might be more appropriate for your team; test matrices, spreadsheets, or even a whiteboard may be sufﬁcient.",
      "page_number": 401
    },
    {
      "number": 49,
      "title": "Segment 49 (pages 409-416)",
      "start_page": 409,
      "end_page": 416,
      "detection_method": "topic_boundary",
      "content": "SUMMARY\n\n(cid:2) Formal release planning may not be appropriate for your situation. In the absence of release planning, consider identifying and discussing at least the ﬁrst few stories that should be done ﬁrst.\n\n(cid:2) Plan for what metrics you want to capture for the life of the release; think about what problem you are trying to solve and capture only those metrics that are meaningful for your team.\n\n367\n\nThis page intentionally left blank\n\nChapter 16\n\nHIT THE GROUND RUNNING\n\nBenefits\n\nResources\n\nBe Proactive\n\nDo You Really Need This?\n\nPotential Downsides to Advanced Preparation\n\nCustomers Speak with One Voice\n\nPrioritize Defects\n\nBefore the Iteration\n\nAdvance Clarity\n\nStory Size\n\nGeographically Dispersed Customers and Teams\n\nTest Strategies\n\nExamples\n\nIn agile development, we generally like to do tasks “just in time.” We can’t see around the curves in the road ahead, so we focus on the activities at hand. Then again, we want to hit the ground running when we start each new iteration. That may require a little preparation. Baking is a good analogy here. You decide you want to bake cookies because someone is coming over. Before you start, you make sure you have the right ingredients. If you don’t, you either go buy what you need, or you choose a different kind to make.\n\nDon’t go overboard—if a pre-iteration activity doesn’t save time during the iteration, or help you do a better job, don’t do it before the iteration. Do what is appropriate for your team, and keep experimenting. Maybe you’ll do some of these activities after the iteration starts instead. Here are some ideas to think about that might help you “bake quality in” to your product.\n\nBE PROACTIVE In Chapter 2, “Ten Principles of an Agile Tester,” we explained how agile testers have to shift their mind-set. Instead of waiting for work to come to us, we develop a proactive attitude where we get up and go look for ways to\n\n369\n\n370\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\ncontribute. With the fast and constant pace of agile development, it’s easy to get immersed in the current iteration’s stories. We are so busy making sure we’ve covered the features with up-front tests, performing exploratory test- ing to be sure we’ve understood the business requirements, and automating adequate regression tests, it’s hard to think of anything else. However, it’s sometimes appropriate to take a bit of time to help our customers and our team prepare for the next iteration. When our team is about to break new ground, or work on complex and risky stories, some work before the itera- tion can help maximize our team’s velocity and minimize frustration.\n\nWe sure don’t want to spend all our time in meetings, or planning for stories that might be re-prioritized. However, if we can make our iteration planning go faster, and reduce the risk of the stories we’re about to undertake, it’s worth doing some research and brainstorming before we start the iteration.\n\nBeneﬁts\n\nWorking on stories in advance of the iteration may be especially useful for teams that are split across different geographic locations. By working ahead, there’s time to get information to everyone and give them a chance to give their input.\n\nThe problem is that we’re so busy during each short agile iteration that it’s hard to ﬁnd time to meet about the next iteration’s stories, much less start writing test cases. If your iterations always go smoothly, with stories delivered incrementally and plenty of time to test, and the delivered software matches customer expectations, you may not need to take time to prepare in advance. If your team has trouble ﬁnishing stories, or ends up with big mismatches be- tween actual and desired behavior of features, a little advance planning may save you time during the iteration.\n\nLisa’s Story\n\nOur team used to feel we didn’t have time to plan in advance for the next itera- tion. After many experiences of misunderstanding stories and having them far ex- ceed estimations, and ﬁnding most “bugs” were missed requirements, we decided to budget time in the iteration to start talking about the next one. Now the whole team, including the product owner and other customers as needed, meet for an hour or less the day before the planning meeting for our next sprint.\n\nWe laughingly call this the “pre-planning” meeting. We go over the stories for the next iteration. The product owner explains the purpose of each story. He goes over the business conditions of satisfaction and other items in his story checklists, and gives examples of desired behavior. We brainstorm about potential risks and dependencies, and identify steel threads where appropriate.\n\nBE PROACTIVE\n\nSometimes it’s enough to spend a few minutes listening to the product owner’s explanation of the stories. At other times, we take time to diagram thin slices of a story on the whiteboard. Figure 16-1 shows an example diagram where we got into both the details of the UI ﬂow and the database tables. Note the numbers for the “threads.” Thread #1 is our critical path. Thread #2 is the second layer, and so on. We upload photos of these diagrams to the wiki so our remote developer can see them too.\n\nWe can start thinking about what task cards we might write the next day and what approach we might take to each story. For some reason, being able to rumi- nate about the stories overnight makes us more productive in the actual iteration planning meeting the next day. After doing this for a few iterations, we were spending less time overall in planning the iteration, even though we had two meetings to do it.\n\nSometimes we pull in other customers to discuss stories that affect them directly. If they aren’t available right then, we still have time before our iteration planning meeting to talk to them to clarify the story.\n\nIn one pre-planning meeting, our product owner introduced a story about ob- taining performance data for mutual funds. We would send a ﬁle to the vendor containing a list of mutual funds, and the vendor would provide an XML ﬁle on a website with all the latest performance information for those funds. We would then upload that data into our database.\n\nFigure 16-1 Sample planning whiteboard diagram\n\n371\n\n372\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\nIn the pre-planning meeting, we asked questions such as, “What’s the format of the ﬁle we send the vendor?” “Is the ‘as-of’ date for each fund always the last day of the month?” “Is there any security on the website that contains the XML?” “Will we ever get a record for the same fund and “as-of” date that has new data, or can we ignore records with a date we already have in our database?”\n\nBy the next day’s iteration planning meeting, the product owner had obtained an- swers to all our questions. Writing task cards went quickly, and coding could pro- ceed with correct assumptions.\n\nOften, we ﬁnd a much simpler solution to a story when we discuss it in the pre- iteration planning discussion. We found that to go fast, we needed to slow down ﬁrst.\n\nDo You Really Need This?\n\nYour team may not need much or any advance preparation. Pierre Veragen and Erika Boyer described to Lisa how their teams at iLevel by Weyerhaeuser write user acceptance tests together at their iteration kickoff meeting.\n\nThese tests, which were written on a wiki page or some similar tool along with the story narrative, are used later when team members write task cards for each story and start writing more tests and code. Examples are turned into executable tests. Because the tests change as the team learns more about the story, the team may opt not to maintain the original ones that were writ- ten at the start. Keep it simple to start with, and dig into details later.\n\nLisa subsequently observed one of their planning sessions and saw ﬁrst-hand how effective this technique was. Even when the product manager provides concrete examples, turning them into tests may ﬂush out missing require- ments. Their team did not need to do this before the iteration planning ses- sion, but it is not the case with all teams.\n\nLisa’s Story\n\nMy team liked the practice of writing tests together, but because we were writing task cards during iteration planning, we decided to write user acceptance tests together during the pre-planning meeting. We found this kept our discussions fo- cused and we understood each story more quickly. We also did a better job of delivering exactly what the customer had in mind. Our customers noticed a differ- ence in quality, and our product owner encouraged us to continue this practice.\n\n—Lisa\n\n—Lisa\n\nADVANCE CLARITY\n\nExperiment with short pre-iteration discussions and test-writing sessions. It’ll take you several iterations to ﬁnd your team’s rhythm, and ﬁnd out if ad- vance story discussions make you more productive during the iteration.\n\nPotential Downsides to Advance Preparation\n\nThere’s a risk to “working ahead.” You could spend time learning more de- tails about a feature only to have the business people re-prioritize at the last minute and put that feature off indeﬁnitely. Invest preparation time when it’s appropriate. When you know you have a complex theme or story coming up, and it has a hard deadline such as Lisa’s team had with the statement story, consider spending some time up front checking out different viewpoints. The only reason to discuss stories in advance is to save time during iteration planning and during development. A deeper understanding of the feature be- havior can speed up testing and coding, and can help make sure you deliver the right functionality.\n\nIf your situation is so dynamic that stories might be re-prioritized the day that the iteration starts, it isn’t worth trying to do this planning. Instead, make sure you budget time for these discussions during your planning meeting.\n\nADVANCE CLARITY Lisa’s product owner, Steve Perkins, came up with the term “advance clarity.” Different parts of each organization have different priorities and agendas. For example, Business Development is looking for new features to attract new business, while Operations is prioritizing features that would reduce the number of phone calls from users. The development team tries to understand the range of business needs and get a feel for each individual’s job.\n\nWith many different agendas, someone needs to decide what stories should be implemented in the next iteration. Because there are many ways to imple- ment any given story, someone has to decide the speciﬁc requirements and capture them in the form of examples, conditions of satisfaction, and test cases. Steve gets everyone together to agree on the value they want from each story, and to provide “advance clarity.”\n\nCustomers Speak with One Voice\n\nScrum provides the helpful role of the product owner to help all the custom- ers “Speak with One Voice.” Whether or not you’re on a Scrum team, ﬁnd some way to help your customers agree on the priority of the stories and how\n\n373\n\n374\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\nthe components of each story ought to be implemented. Management sup- port is crucial, because any person in this role needs time and authority to get everyone on the same page.\n\nOther teams use business analysts to help ﬂesh out the stories before the next iteration. In one organization Janet worked with, the customers were not available full-time to answer questions, but each team had a business analyst that worked with the customers to ﬂesh out the requirements before the iter- ation planning meeting. If there were any questions that she could not an- swer at the meeting, the team either called the customer directly or the analyst followed up immediately after the meeting.\n\nAs a tester, you want to sit in on story writing and prioritization meetings. Ask questions that help the customers focus on the core functionality, the critical business value they need. Help participants stay focused on concrete examples that crystallize the meaning of the stories. In meetings that involve multiple customers, it is critical to have a strong facilitator and a method for determining consensus.\n\nAs with code, stories are best if they have the bare minimum. For example, an Internet shopping cart needs some way to delete unwanted items, but the ability to move items from the cart to a “save for later” list can probably wait. It may be helpful to talk about this before the iteration, so that the team is clear on what tasks need to be planned. Focus on the simplest thing ﬁrst and use an example to make it clear.\n\nGet All Viewpoints Getting requirements from different customers for a story, each of whom has a different agenda, might create chaos. That’s why it’s essential for someone on the customer team to get consensus and coordinate all points of view. This doesn’t mean we shouldn’t get input from different customers. As a tester, you’re considering each story from multiple points of view. It helps to know what the story means to people in different roles.\n\nLisa’s Story\n\nWhen my company decided to redesign the retirement plan participants’ quarterly account statements, different people on the business side wanted changes for different reasons. The plan administrators wanted a clearly understandable layout that would minimize the number of calls from confused participants to customer support.\n\nFor example, they wanted the statement to show the date and amount of the par- ticipant’s most recent contribution. This helps the participant know whether her",
      "page_number": 409
    },
    {
      "number": 50,
      "title": "Segment 50 (pages 417-424)",
      "start_page": 417,
      "end_page": 424,
      "detection_method": "topic_boundary",
      "content": "Lisa’s Story\n\nADVANCE CLARITY\n\nemployer is late in posting contributions to the accounts. Business development wanted jazzy new features that they could sell to potential customers, such as graphs of performance by fund category. Our legal person needed some new text and data on the statement to satisfy federal regulations.\n\nWhile the product owner balanced all the different needs and presented the ﬁnal statement layout, it was still important for our team to understand the purpose behind each new piece of information. We needed to talk directly to business ex- perts in the plan administration, business development, and legal areas, and to the product owner. A tester and a programmer met with each group to gather the different viewpoints. By doing this before starting on stories to gather and display data, we understood the requirements much more clearly and even made sugges- tions to produce the information more efﬁciently.\n\nMake sure you are as efﬁcient as possible in collecting this data. Sometimes it is important for the whole team to understand the need, and sometimes it is sufﬁcient for one or two of the team members to do the research.\n\nStory Size\n\nAs you discuss stories for the next iteration with the customer team mem- bers, ask questions to help them make sure each story delivers the value needed. This is a good time to identify new stories they might need to write. Even though the team sized the stories previously, you might ﬁnd a story is bigger than previously thought. You might even discover that a feature can be implemented more simply than planned, and the story can be smaller.\n\nSometimes assumptions are made when the story is sized and on further in- vestigation turn out to be false. Even simple stories deserve a closer look. It’s hard for any one person to remember all the details of an application.\n\nHere are some examples of stories that turned out to be signiﬁcantly bigger or smaller than originally thought.\n\n1. The story was to produce a ﬁle of account statements for all participants in a given company retirement plan, which was to be sent to a vendor who would print and mail the statements. It was originally sized with the assumption that all statements were exactly three pages long. Upon further investigation, we discovered that some participants had four-page statements, but the vendor required that all statements be the same length. Our business experts had to decide whether to have a feature to ﬂag any plans whose participants had four-page statements and deal with those manually, or change the statements to make them all four pages long. That’s a much bigger effort than the original\n\n375\n\n—Lisa\n\n376\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\nstory. After we started developing the story, the customers revealed another requirement: If any participant’s address was missing or invalid, the statement should be mailed to the employer instead. It’s reasonable, but we didn’t know about it when we sized the story.\n\n2. Our customers wanted to start displaying the sales phone number in various locations in the UI. There is a different sales phone number for each partner’s site, and at the time there were about 25 different partner sites. This sounded like such a straightforward story that it wasn’t even given to the team to size. The development manager just assigned it a small point value, and it was just “added” to the iteration. He had assumed the phone number was stored in the database, when in fact it was hard-coded in the HTML of each partner’s “con- tact” page. Storing the correct number for each partner in the database, and changing the code to retrieve the value, made the story twice as big, and there wasn’t room for it in that iteration, so it did not get done.\n\n3. We sized a story for the user interface to allow administrators to submit a request for a batch job to rebalance participant accounts that met a certain condition. It included a conﬁrmation page displaying the number of partici- pants affected. Because the request was queued to run as an asynchronous batch job, the code to determine which participants were affected was in the batch job’s code. Refactoring the code to obtain the number of participants at request time was a big job. After we started working on the story, we asked the primary user of the feature whether he really needed that number upon submitting the request, and he decided it wasn’t necessary. The story became much smaller than originally thought. We always ask questions to ﬁnd out the true business value that the customers want and eliminate components that don’t have a good ROI.\n\nThese stories show that a few questions up front might save time during the iteration that could be spent ﬁguring out what to do with new discoveries. However, we recognize that not all discoveries can be found early. For exam- ple, on the ﬁrst story, a simple question about statement size may have pre- vented last-minute confusion about how to handle four-page statements, but the inaccurate address issue may not have been considered until it was being coded or tested.\n\nWe know there will always be discoveries along the way, but if we can catch the big “gotchas” ﬁrst, that will help the team work as effectively as possible.\n\nGeographically Dispersed Teams\n\nSome preparation for the next iteration may be useful for teams that are split across different locations. Teams that are distributed in multiple locations may do their iteration planning by conference call, online meeting, or telecon- ference. One practice, which a team of Lisa’s used, is to assign each team a\n\n—Lisa\n\nADVANCE CLARITY\n\nsubset of the upcoming stories and have them write task cards in advance. During the planning meeting, everyone can review all the task cards and make changes as needed. The up-front work enhances communication, makes the stories and tasks visible to everyone, and speeds up the planning process.\n\nOf course, this assumes that the team is using an electronic story or task board. Lisa’s team uses Thoughtwork’s Mingle, but there are many other products out there that serve this purpose.\n\nCoping with Geographic Diversity\n\nWe talked to a team we know at a software company that has customers, de- velopers, and testers spread all over the globe. Not only are the customers far away from the technical team but they don’t have bandwidth to be avail- able to answer the development team’s questions. Instead, the team relies on functional analysts who understand both the business side of the applica- tion at a detailed level and the technical implementation of the software. These functional analysts act as liaisons between the business and technical teams.\n\nPatrick Fleisch and Apurva Chandra are consultants who were working with this company and served as functional analysts on a project to develop web- based entitlement software, because they are experts in this domain. They traveled between locations to facilitate communication between stakehold- ers and developers.\n\nThe functional analysts worked in advance of the iteration, sizing and getting stories ready to size, helping the technical team to understand the stories. They entered stories into an online tool and built on them by deﬁning test cases, edge conditions, and other information that helped the technical team understand the story. They documented high-level functionality on a wiki aimed at the business users.\n\nApurva and Patrick played a key role in making the decisions that the techni- cal team needed to get started with the new stories. Their deep business and technical understanding allowed them to provide the team with require- ments they needed to get coding, because the actual customers weren’t available to them. David Reed, a tester and automation engineer, told us how he relied on Apurva and Patrick for the information he needed to perform and automate tests. While agile principles say to collaborate closely with the customer, in some situations you have to be creative and ﬁnd another way to get clear business requirements.\n\nIf customers aren’t readily available to answer questions and make decisions, other domain experts who are accessible at all times should be empowered to guide the team by determining priorities and expressing desired system\n\n377\n\n378\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\nbehavior with examples. Testers and business analysts are often called upon to do these activities.\n\nEXAMPLES You may notice that we talk about examples in just about every chapter of this book. Examples are an effective way to learn about and illustrate desired (and undesired) functionality; it’s worth using them throughout your devel- opment cycle. Our motto was coined by Brian Marick: “An example would be handy right about now.” (See Figure 16-2.) Start your discussions about fea- tures and stories with a realistic example. The idea has taken off, so that at a recent workshop for functional testing we were discussing ideas around call- ing it “Example-Driven Development.”\n\nWhen Lisa’s team members meet with their product owner to talk about the next iteration, they ask him for examples of desired behavior for each story. This keeps the discussion at a concrete level and is a fast way to learn how the new features should work. Have a whiteboard handy while you do this, and start drawing. If some team members are in a distant location, consider using tools that allow everyone to see whiteboard diagrams and participate in the discussion. Go through real examples with your customers or their proxies. As during release planning, consider different points of view: the business, end users, developers, and business partners. Unlike release planning, you are looking at far more detail because these are the stories you are planning for the next iteration.\n\nUsing examples, you can write high-level tests to ﬂesh out each story a bit more. You may not need to do this before the iteration starts, but for com-\n\nFigure 16-2 Brian Marick’s example sticker\n\nEXAMPLES\n\nplex stories, it can be a good idea to write at least one happy path and one negative path test case in advance. Let’s consider the story in Figure 16-3.\n\nStory PA-3\n\nAs a shopper on our site, I want to delete items\n\nout of my shopping cart, so I don’t purchase extra\n\nitems I don't want.\n\nFigure 16-3 Story for deleting items from shopping cart\n\nThe product owner sketches out the desired UI on the whiteboard. There’s a “delete” checkbox next to each item and an “update cart” button. The user can select one or more items and click the button to remove the items. The high-level tests might be:\n\n(cid:2) When the user clicks the delete checkbox next to the item and clicks the “update cart” button, the page refreshes showing the item is no longer in the cart.\n\n(cid:2) When the user clicks the delete checkboxes next to every item in the cart and clicks the “update cart” button, the page refreshes showing an empty cart. (This will generate questions—should the user be di- rected to another page? Should a “keep shopping” button display?) (cid:2) When the user clicks the “update cart” button without checking an item for delete, the page is refreshed and nothing is removed from the cart.\n\nAsk your customers to write down examples and high-level test cases before the iteration. This can help them think through the stories more and help de- ﬁne their conditions of satisfaction. It also helps them identify which features are critical, and which might be able to wait. It also helps to deﬁne when the story is done and manage expectations among the team.\n\n379\n\n380\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\nFigure 16-4 Sample customer mock-up\n\nFigure 16-4 shows a sample mock-up, where the product owner marked changes on the existing page. Be careful about using an existing screenshot from an old system, because you will run the risk of having a new system look exactly like the old one even if that is not what you wanted.\n\nMock-ups are essential for stories involving the UI or a report. Ask your cus- tomers to draw up their ideas about how the page should look. Share these ideas with the team. One idea is to scan them in and upload them on the wiki so everyone has access. Use those as a starting point and do more paper pro- totypes, or draw them on the whiteboard. These can be photographed and uploaded for remote team members to see.\n\nTEST STRATEGIES As you learn about the stories for the next iteration, think about how to ap- proach testing them. Do they present any special automation challenge? Are any new tools needed?\n\nLisa’s Story\n\nRecently, our company needed to replace the voice response unit hardware and interactive voice interface software. A contractor was to provide the software to operate the voice application, but it needed to interact via stored procedures with the database.\n\nChapter 9, “Tool- kit for Business- Facing Tests that Support the Team,” Chapter 10, “Business-Facing Tests that Critique the Product,” and Chapter 11, “Critiquing the Product Using Technology- Facing Tests,” pro- vide examples of tools for different types of testing.\n\nRESOURCES\n\nThis was a big departure from any software we’d worked on before, so it was helpful to have an extra day to research how other teams have tested this type of application before the ﬁrst iteration planning that involved a story related to this project. During the iteration planning session, we were able to write tasks that were pertinent to the testing needed and give better estimates.\n\nWhen your team embarks on a new type of software, you may decide to do a development spike to see what you can learn about how to develop it. At the same time, try a test spike to help make sure you’ll know how to drive the de- velopment with tests and how to test the resulting software. If a major new epic or feature is coming up, write some cards to research it and hold brain- storming meetings an iteration or two in advance. That helps you know what stories and tasks to plan when you actually start coding. One idea is to have a “scout” team that looks at what technical solutions might work for upcom- ing stories or themes.\n\nPRIORITIZE DEFECTS In our ideal world, we want zero defects at the end of each iteration and deﬁ- nitely at the end of the release. However, we recognize that we don’t live in an ideal world. Sometimes we have legacy system defects to worry about, and sometimes ﬁxing a defect is just not high enough value for the business to ﬁx. What happens to these defects? We’ll talk about strategies in Chapter 18, “Coding and Testing,” but for now, let’s just consider that we have defects to deal with.\n\nBefore the next iteration is an ideal time to review outstanding issues with the customer and triage the value of ﬁxing versus leaving them in the system. Those that are deemed necessary to be ﬁxed should be scheduled into the next iteration.\n\nRESOURCES Another thing to double-check before the iteration is whether your team has all the resources you need to complete any high-risk stories. Do you need any experts who are shared with other projects? For example, you may need a se- curity expert if one of the stories poses a security risk or is for a security fea- ture. If load testing will be done, you may need to have a special tool, or have help from a load testing specialist from another team, or even a vendor who provides load testing services. This is your last chance to plan ahead.\n\n381\n\n—Lisa\n\n382\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\nSUMMARY Your team may or may not need to do any preparation in advance of an iter- ation. Because priorities change fast in agile development, you don’t want to waste time planning stories that may be postponed. However, if you’re about to implement some new technology, embark on a complex new theme, hope to save time in iteration planning, or your team is divided into different loca- tions, you might ﬁnd some up-front planning and research to be productive. As a tester, you can do the following:\n\n(cid:2) Help the customers achieve “advance clarity”—consensus on the desired behavior of each story—by asking questions and getting examples.\n\n(cid:2) Be proactive, learn about complex stories in advance of the iteration,\n\nand make sure they’re sized correctly.\n\n(cid:2) You don’t always need advance preparation to be able to hit the\n\nground running in the next iteration. Don’t do any preparation that doesn’t save time during the iteration or ensure more success at meet- ing customer requirements.\n\n(cid:2) Coordinate between different locations and facilitate communication.\n\nThere are many tools to help with this.\n\n(cid:2) Obtain examples to help illustrate each story. (cid:2) Develop test strategies in advance of the next iteration for new and\n\nunusual features.\n\n(cid:2) Triage and prioritize existing defects to determine whether any should\n\nbe scheduled for the next iteration.\n\n(cid:2) Determine whether any necessary testing resources not currently at\n\nhand need to be lined up for the next iteration.",
      "page_number": 417
    },
    {
      "number": 51,
      "title": "Segment 51 (pages 425-434)",
      "start_page": 425,
      "end_page": 434,
      "detection_method": "topic_boundary",
      "content": "Chapter 17\n\nITERATION KICKOFF\n\nReview with Customers\n\nLearning the Details\n\nReview with Programmers\n\nTest Cases as Documentation\n\nHigh-Level Tests and Examples\n\nIteration Planning\n\nConsidering All Viewpoints\n\nWriting Task Cards\n\nDeciding on Workload\n\nIteration Kickoff\n\nCollaborate with Customers\n\nTestable Stories\n\nAgile testers play an essential role during iteration planning, helping to plan testing and development tasks. As the iteration gets under way, testers actively collaborate with customers and developers, writing the high-level tests that help guide development, eliciting and illustrating examples, making sure stories are testable. Let’s take a closer look at the agile tester’s activities at the beginning of each iteration.\n\nITERATION PLANNING Most teams kick off their new iteration with a planning session. This might be preceded by a retrospective, or “lessons learned” session, to look back to see what worked well and what didn’t in the previous iteration. Although the retrospective’s action items or “start, stop, continue” suggestions will affect the iteration that’s about to start, we’ll talk about the retrospective as an end- of-iteration activity in Chapter 19, “Wrap Up the Iteration.”\n\nWhile planning the work for the iteration, the development team discusses one story at a time, writing and estimating all of the tasks needed to implement\n\n383\n\n384\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nFigure 17-1 Iteration planning meeting\n\nthat story. If you’ve done some work ahead of time to prepare for the iteration, this planning session will likely go fairly quickly.\n\nTeams new to agile development often need a lot of time for their iteration planning sessions. Iteration planning often took a whole day when Lisa’s team ﬁrst started out. Now they are done in two or three hours, which in- cludes time for the retrospective. Lisa’s team uses a projector to display user acceptance test cases and conditions of satisfaction from their wiki so that everyone on the team can see them. They also project their online story board tool, where they write the task cards. Another traditional component of their planning meetings is a plate of treats that they take turns providing. Figure 17-1 shows an iteration planning meeting in progress.\n\nLearning the Details\n\nIdeally, the product owner and/or other customer team members participate in the iteration planning, answering questions and providing examples de- scribing requirements of each story. If nobody from the business side can at- tend, team members who work closely with the customers, such as analysts and testers, can serve as proxies. They explain details and make decisions on behalf of the customers, or take note of questions to get answered quickly. If\n\nITERATION PLANNING\n\nyour team went over stories with the customers in advance of the iteration, you may think you don’t need them on hand during the iteration planning session. However, we suggest that they be available just in case you do have extra questions.\n\nAs we’ve emphasized throughout the book, use examples to help the team understand each story, and turn these examples into tests that drive coding. Address stories in priority order. If you haven’t previously gone over stories with the customers, the product owner or other person representing the cus- tomer team ﬁrst reads each story to be planned. They explain the purpose of the story, the value it will deliver, and give examples of how it will be used. This might involve passing around examples or writing on a whiteboard. UI and report stories may already have wire frames or mock-ups that the team can study.\n\nA practice that helps some teams is to write user acceptance tests for each story together, during the iteration planning. Along with the product owner and possibly other stakeholders, they write high-level tests that, when pass- ing, will show that the story is done. This could also be done shortly in ad- vance of iteration planning as part of the iteration “prep work.”\n\nStories should be sized so they’ll take no more than a few days to complete. When we get small stories to test on a regular basis, we do not have them all ﬁnished at once and stacked up at the end of the iteration waiting to be tested. If a story has made it past release planning and pre-iteration discus- sions and is still too large, this is the ﬁnal chance to break it up into smaller pieces. Even a small story can be complex. The team may go through an exer- cise to identify the thin slices or critical path through the functionality. Use examples to guide you, and ﬁnd the most basic user scenarios.\n\nAgile testers, along with other team members, are alert to “scope creep.” Don’t be afraid to raise a red ﬂag when a story seems to be growing in all directions. Lisa’s team makes a conscious effort to point out “bling,” or “nice to have” components, which aren’t central to the story’s functionality. Those can be put off until last, or postponed, in case the story takes longer than planned to ﬁnish.\n\nConsidering All Viewpoints\n\nAs a tester, you’ll try to put each story into the context of the larger system and assess the potential of unanticipated impacts on other areas. As you did in the release planning meeting, put yourself in the different mind-sets of\n\n385\n\n386\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nuser, business stakeholder, programmer, technical writer, and everyone in- volved in creating and using the functionality. Now you’re working at a de- tailed level.\n\nIn the release planning chapter, we used this example story:\n\nAs a customer, I want to know how much my order will cost to ship based on the shipping speed I choose, so I can change if I want to.\n\nWe decided to take a “thin slice” and change this story to assume there is only one shipping speed. The other shipping speeds will be later stories. For this story, we need to calculate shipping cost based on item weight and destina- tion, and we decided to use BigExpressShipping’s API for the calculation. Our story is now as shown in Figure 17-2.\n\nStory PA-5\n\nAs a customer, I want to know how much my order\n\nwill cost to ship for standard 5-day delivery based\n\non weight and destination, so I can decide if\n\nthat’s the shipping option I want.\n\nFigure 17-2 Story shipping speed for 5-day delivery\n\nThe team starts discussing the story.\n\nTester: “Does this story apply for all items available on the site? Are any items too heavy or otherwise disqualiﬁed for 5-day delivery?\n\nProduct Owner: “5-day ground is available for all our items. It’s the overnight and 2-day that are restricted to less than 25 lbs.”\n\nTester: “What’s the goal here, from the business perspective? Making it easy to ﬁgure the cost to speed up the checkout? Are you hoping to\n\nITERATION PLANNING\n\nencourage them to check the other shipping methods—are those more proﬁtable?”\n\nProduct Owner: “Ease of use is our main goal, we want the checkout process to be quick, and we want the user to easily determine the total cost of the order so they won’t be afraid to complete the purchase.”\n\nProgrammer: “We could have the 5-day shipping cost display as a de- fault as soon as the user enters the shipping address. When we do the stories for the other shipping options, we can put buttons to pop up those costs quickly.”\n\nProduct Owner: “That’s what we want, get the costs up front. We’re go- ing to market our site as the most customer friendly.”\n\nTester: “Is there any way the user can screw up? What will they do on this page?”\n\nProduct Owner: “When we add the other shipping options, they can opt to change their shipping option. But for now, it’s really straightfor- ward. We already have validation to make sure their postal code matches the city they enter for the shipping address.”\n\nTester: “What if they realize they messed up their shipping address? Maybe they accidentally gave the billing address. How can they get back to change the shipping address?”\n\nProgrammer: “We’ll put buttons to edit billing and shipping addresses, so it will be very easy for the user to correct errors. We’ll show both ad- dresses on this page where the shipping cost displays. We can extend this later when we add the multiple shipping addresses option.”\n\nTester: “That would make the UI easy to use. I know when I shop on- line, it bugs me to not be able to see the shipping cost until the order conﬁrmation. If the shipping is ridiculously expensive and I don’t want to continue, I’ve already wasted time. We want to make sure users can’t get stuck in the checkout process, get frustrated, and just give up. So, the next page they’ll see is the order conﬁrmation page. Is there any chance the shipping cost could be different when the user gets to that page?”\n\nProgrammer: “No, the API that gives us the estimated cost should al- ways match the actual cost, as long as the same items are still in the shop- ping cart.”\n\nProduct Owner: “If they change quantities or delete any items, we need to make sure the shipping cost is immediately changed to reﬂect that.”\n\n387\n\n388\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nAs you can see by the conversation, a lot of clariﬁcation came to light. Every- one on the team now has a common understanding of the story. It’s impor- tant to talk about all aspects of the story. Writing user acceptance tests as a group is a good way to make sure the development team understands the customer requirements. Let’s continue monitoring this conversation.\n\nTester: “Let’s just write up some quick tests to make sure we get it right.”\n\nCustomer: “OK, how’s this example?\n\nI can select two items with a 5-day shipping option and see my costs immediately.\n\nTester: “Great start, but we won’t know where to ship it to at that point. How about a more generic test like:\n\nVerify the 5-day shipping cost displays as the default as soon as the user enters a shipping address.\n\nCustomer: “That works for me.”\n\nConsidering All of the Facets\n\nPaul Rogers recounts a situation during an iteration planning meeting, where a performance issue came up for a story that appeared to be straightforward and quick.\n\nDuring our iteration meeting, one of the stories we were discussing was for adding some new images to part of a web application. This discussion ensued.\n\nProduct Owner: “I’d like to also get in the story for additional images.”\n\nDeveloper 1: “OK, who has ideas on how long it will take?”\n\nDeveloper 2: “It’s fairly quick, maybe half a day.”\n\nDeveloper 3: “But what about the database changes?”\n\nDeveloper 2: “I included those in the estimate.”\n\nDeveloper 1: “OK, let’s go with half a day.”\n\nMe: “Hang on. We looked at some performance issues last iteration. If we add all those images, we will be taking a performance hit.\n\nLisa’s Story\n\nITERATION PLANNING\n\nDeveloper 1: “OK, we should think about that some more. Maybe there are other ways of implementing it.\n\nDeveloper 2: “Why don’t we do a quick spike, add the mock images, and run another performance test?”\n\nIt was really good that this discussion before even starting on a story gave us some ideas of what problems we may encounter.\n\nAnyone who’s uncertain about either the impact of a story on the rest of the system or the difﬁculty of developing the functionality can, and should, raise an issue during iteration planning. It’s better to address uncertainty early on and then do more research or a spike to get more information.\n\nAsking questions based on different viewpoints will help to clarify the story and allow the team to do a better job.\n\nWriting Task Cards\n\nWhen your team has a good understanding of a story, you can start writing and estimating task cards. Because agile development drives coding with tests, we write both testing and development task cards at the same time.\n\nIf you have done any pre-planning, you may have some task cards already written out. If not, write them during the iteration planning meeting. It doesn’t matter who writes the task cards, but everyone on the team should review them and get a chance to give their input. We recognize that tasks may be added as we begin coding, but recognizing most of the tasks and estimat- ing them during the meeting gives the team a good sense of what is involved.\n\nWhen our team is ready to start writing task cards, programmers usually come up with the coding task cards. The testers write testing task cards at the same time.\n\nI usually start with a card to write high-level test cases. I ask the programmers whether the story can be tested behind the GUI, and write testing task cards accordingly. This usually means a test card to “Write FitNesse test cases” and a developer task card to “Write FitNesse ﬁxture,” unless the ﬁxtures already exist. Sometimes all of the behind-GUI tests can be covered more easily in unit tests, so it is always good to ask whether this is the case.\n\nWe put anything the team needs to remember during the iteration on a task card. “Show UI to Anne” or “Send test ﬁles to Joe” go up on the story board along with all of the other tasks.\n\n389\n\n390\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nI estimate the testing task cards as I go, and ask the team for feedback on the cards and the estimates. Sometimes we divide into groups, and each group takes some stories and writes task cards for them. We always review all cards together, along with the estimated time. If development time is relatively low compared to testing time, or vice versa, that provokes a discussion. We reach a consensus as to whether we think all aspects of the story have been covered with task cards. If there are still some unknowns, we simply postpone writing the task cards until we have the information.\n\nThe testing and development cards all go on the story board in the “to do” col- umn. Anyone on the team can sign up for any card. Some testing task cards move to the “work in progress” or “done” column before coding cards start to move, so that programmers have some tests to guide their coding. As coding task cards are moved to the “done” column, the cards for testing the “done” functionality are moved into “work in progress.”\n\nJanet uses an approach similar to this, but the programmer’s coding card stays in the “To Test” column until the testing task has been completed. Both cards move at the same time to the “Done” column.\n\nThree test cards for Story PA-5 (Figure 17-2), displaying the shipping cost for 5- day delivery based on weight and destination, that Lisa’s team might write are:\n\n(cid:2) Write FitNesse tests for calculating 5-day ship cost based on weight\n\nand destination.\n\n(cid:2) Write WebTest tests for displaying the 5-day ship cost. (cid:2) Manually test displaying the 5-day delivery ship cost.\n\nSome teams prefer to write testing tasks directly on the development task cards. It’s a simple solution, because the task is obviously not “done” until the testing is ﬁnished. You’re trying to avoid a “mini-waterfall” approach where testing is done last, and the programmer feels she is done because she “sent the story to QA.” See what approach works best for your team.\n\nIf the story heavily involves outside parties or shared resources, write task cards to make sure those tasks aren’t forgotten, and make the estimates gen- erous enough to allow for dependencies and events beyond the team’s con- trol. Our hypothetical team working on the shipping cost story has to work with the shipper’s cost calculation API.\n\n—Lisa\n\nITERATION PLANNING\n\nTester: “Does anyone know who we work with at BigExpressShipping to get specs on their API? What do we pass to them, just the weight and postal code? Do we already have access for testing this?”\n\nScrum Master: “Joe at BigExpressShipping is our contact, and he’s al- ready sent this document specifying input and output format. They still need to authorize access from our test system, but that should be done in a couple of days.”\n\nTester: “Oh good, we need that information to write test cases. We’ll write a test card just to verify that we can access their API and get a ship- ping cost back. But how do we know the cost is really correct?”\n\nScrum Master: “Joe has provided us with some test cases for weight and postal code and expected cost, so we can send those inputs and check for the correct output. We also have this spreadsheet showing rates for some different postal codes.”\n\nTester: “We should allow lots of time for just making sure we’re access- ing their API correctly. I’m going to put a high estimate on this card to verify using the API for testing. Maybe the developer card for the inter- face to the API should have a pretty conservative estimate, too.”\n\nWhen writing programmer task cards, make sure that coding task estimates include time for writing unit tests and for all necessary testing by program- mers. A card for “end-to-end” testing helps make sure that programmers working on different, independent tasks verify that all of the pieces work to- gether. Testers should help make sure all necessary cards are written and that they have reasonable estimates. You don’t want second-guess estimates, but if the testing estimates are twice as high as the coding estimates, it might be worth talking about.\n\nSome teams keep testing tasks to a day’s work or less and don’t bother to write estimated hours on the card. If a task card is still around after a day’s work, the team talks about why that happened and writes new cards to go forward. This might cut down on overhead and record-keeping, but if you are entering tasks into your electronic system, it may not. Do what makes sense for your team.\n\nEstimating time for bug ﬁxing is always tricky as well. If existing defects are pulled in as stories, it is pretty simple. But what about the bugs that are found as part of the iteration?\n\n391\n\n392\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nJanet’s Story\n\nWith new agile teams, I found that they always seem to end up with time spent on bugs that wasn’t allotted as part of their estimates for the stories. Over time, pro- grammers learn how much time they typically spend ﬁxing bugs from a story, and can just add half a day or a couple hours to their tasks for that purpose. Retesting bug ﬁxes adds time to tester’s estimates as well.\n\nUntil the team members get a handle on this, it may be appropriate to track the time spent on ﬁxing and testing bugs separately. My current team adds a story in XPlanner with tasks for ﬁxing and testing those bugs that didn’t get caught immedi- ately. We are tracking the time so we can better estimate down the road.\n\nHowever your team chooses to estimate time spent for ﬁxing defects during the iteration, whether it is included in the story estimate or tracked sepa- rately, make sure it is done consistently.\n\nAnother item to consider when estimating testing tasks is test data. The be- ginning of an iteration is almost too late to think about the data you need to test with. As we mentioned in Chapter 15, “Tester Activities in Release or Theme Planning,” think about test data during release planning, and ask the customers to help identify and obtain it. Certainly think about it as you prep for the next iteration. When the iteration starts, whatever test data is missing must be created or obtained, so don’t forget to allow for this in estimates.\n\nLisa’s Story\n\nWe worked on a theme related to quarterly account statements for participants in retirement plans. We were modifying a monthly job that takes a “snapshot” of each participant’s account on the speciﬁed date. The snapshot relies on a huge amount of data in the production database, including thousands of daily transac- tions. We planned ahead.\n\nFor the ﬁrst iteration, we did a few stories in the theme knowing we could only test a few cases using some individual retirement plans for which we had data in the test database. We also knew we needed a larger-scale test, with all of the re- tirement plans in the database and at least an entire month’s worth of data. We wrote a task card to copy enough production data to produce one monthly “snapshot” and made sure the data was scrubbed to protect privacy.\n\nThen we planned the full-blown test in the next iteration. This data enabled the testers to ﬁnd problems that were undetectable earlier when only partial data was available. It was a nice balance of “just enough” data to do most of the cod- ing and the full amount available in time to verify the complete functionality. Be- cause the team planned ahead, the bugs were ﬁxed in time for the critical release.\n\n—Janet\n\n—Lisa",
      "page_number": 425
    },
    {
      "number": 52,
      "title": "Segment 52 (pages 435-442)",
      "start_page": 435,
      "end_page": 442,
      "detection_method": "topic_boundary",
      "content": "TESTABLE STORIES\n\nDeciding on Workload\n\nWe, as the technical team, control our own workload. As we write tasks for each story and post them on our (real or virtual) story board, we add up the estimated hours or visually check the number of cards. How much work can we take on? In XP, we can’t exceed the number of story points we completed in the last iteration. In Scrum, we commit to a set of stories based on the ac- tual time we think we need to complete them.\n\nLisa’s current team has several years of experience in their agile process and ﬁnds they sometimes waste time writing task cards for stories they may not have time to do during the iteration. They start with enough stories to keep everyone busy. As people start to free up, they pull in more stories and plan the related tasks. They might have some stories ready “on deck” to bring in as soon as they ﬁnish the initial ones. This sounds easy, but it is difﬁcult to do until you’ve learned enough to be more conﬁdent about story sizes and team velocity, and know what your team can and cannot do in a given amount of time and in speciﬁc circumstances.\n\nYour job as tester is to make sure enough time is allocated to testing, and to remind the team that testing and quality are the responsibility of the whole team. When the team decides how many stories they can deliver in the itera- tion, the question isn’t “How much coding can we ﬁnish?” but “How much coding and testing can we complete?” There will be times when a story is easy to code but the testing will be very time consuming. As a tester, it is impor- tant that you only accept as many stories into the iteration as can be tested.\n\nIf you have to commit, commit conservatively. It’s always better to bring in another story than to have to drop one. If you have high-risk stories that are hard to estimate, or some tasks are unknown or need more research, write task cards for an extra story or two and have them ready on the sidelines to bring in mid-iteration.\n\nAs a team, we’re always going to do our best. We need to remember that no story is done until it’s tested, so plan accordingly.\n\nTESTABLE STORIES When you are looking at stories, and the programmers start to think about implementation, always think how you can test them. An example goes a long way toward “testing the testability.” What impact will it have on my test- ing? Part III, “The Agile Testing Quadrants,” gives a lot of examples of how to\n\n393\n\n394\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\ndesign the application to enable effective testing. This is your last opportu- nity to think about testability of a story before coding begins.\n\nJanet’s Story\n\nOne team I worked with told me about issues they had in the previous release. The team was rewriting the ﬁrst step of a multistep process. What they didn’t an- ticipate was that when the development on the new step started, the rest of the process broke. No testing could be done on any other changes in that iteration until the whole ﬁrst step was ﬁnished.\n\nTestability had not been considered when planning the story. In the next release, when they decided to rewrite the second step, they learned from their previous mistake. The programmers created an extra button on the page that allowed the testers to either call the new page (in ﬂux) or the old page to allow them test other stories.\n\nRemember to ask, “How can we test this?” if it is not obvious to you.\n\nDuring iteration planning, think about what kind of variations you will need to test. That may drive other questions.\n\nJanet’s Story\n\nDuring one iteration planning meeting that I was in, the programmers started talk- ing about implementation and drawing pictures on the whiteboard to show what they were thinking.\n\nI thought about it for a bit and asked the question, “Can it be done more simply? The permutations and combinations for testing your proposed implementation will make testing horrendous.”\n\nThe programmers thought about it for a couple of minutes and suggested an alter- native that not only met the customer’s needs, but was simpler and easier to test. It was a win-win combination for everyone.\n\nWhen testability is an issue, make it the team’s problem to solve. Teams that start their planning by writing test task cards probably have an advantage here, because as they think about their testing tasks, they’ll ask how the story can be tested. Can any functionality be tested behind the GUI? Is it possible to do the business-facing tests at the unit level? Every agile team should be thinking test-ﬁrst. As your team writes developer task cards for a story, think about how to test the story and how to automate testing for it. If the pro- grammers aren’t yet in the habit of coding TDD or automating unit tests, try\n\n—Janet\n\n—Janet\n\nLisa’s Story\n\nLisa’s Story\n\nTESTABLE STORIES\n\nwriting a “XUnit” task card for each story. Write programming task cards for any test automation ﬁxtures that will be needed. Think about application changes that could help with testing, such as runtime properties and APIs.\n\nThe application that I work on has many time- and date-dependent activities. The programmers added a runtime server property to the web application to set the server date. I can specify a date and time override, and when the server starts up, it behaves accordingly. This allows kicking off monthly or quarterly pro- cesses with a simple override. This property has helped in testing a wide variety of stories.\n\nMarkus Gärtner [2008] told us his team has a similar property, a “DATE_OFFSET” counted in “days to advance.” However, this was only used by the Java compo- nents of the application where the business logic lives. The back-end systems in C and C++ don’t use the date offset, which caused a problem.\n\nIf you have similar issues because other teams are developing parts of the sys- tem, write a task card to discuss the problem with the other team and come up with a coordinated solution. If working with the other team isn’t an op- tion, budget time to brainstorm another solution. At the very least, be mind- ful of the limitations, and adjust testing estimates accordingly and manage the associated risk.\n\nWe started a project to replace our company’s interactive voice response (IVR) system, which allows retirement plan participants to obtain account information and manage accounts by phone. We contracted with another company to write the system in Java, with the intention that our team would maintain it after a cer- tain time period.\n\nWe spent some time brainstorming what testing would be needed and how to do it. Presumably, the contractor would test things like the text-to-speech functional- ity, but we had to supply stored procedures to retrieve appropriate data from the database.\n\nOur ﬁrst step was to negotiate with the contractor to deliver small chunks of fea- tures on an iteration basis, so they could be tested as the project progressed and the work would be spread out evenly over the life of the contract. We decided to test the stored procedures using FitNesse ﬁxtures, and explored the options. We settled on PL/SQL to access the stored procedures. A programmer was tasked with getting up to speed on PL/SQL to tackle the test automation.\n\nThe team aimed for a step-by-step approach. By allocating plenty of time for tasks at the start, we allowed for the steep learning curves involved.\n\n395\n\n—Lisa\n\n396\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nInterestingly, the contractor delivered an initial build for the ﬁrst iteration but was not able to deliver the increments of code for the next few iterations. We ended up canceling the contract and postponing the project until we could ﬁnd a better solution. By forcing the contractor to work in increments, we discovered right away that it couldn’t deliver. What if we had let them take six months to write the whole application? It probably wouldn’t have ended well. We put what we learned to good use in researching a better approach.\n\nWhen you’re embarking on something new to the team, such as a new tem- plating framework or reporting library, remember to include it as a risk in your test plan. Hopefully, your team considered the testability before choos- ing a new framework or tool, and selected one that enhanced your ability to test. Be generous with your testing task estimates with everything new, in- cluding new domains, because there are lots of unknowns. Sometimes new domain knowledge or new technology means a steep learning curve.\n\nCOLLABORATE WITH CUSTOMERS Working closely with customers, or customer proxies such as functional ana- lysts, is one of our most important activities as agile testers. As you kick off the iteration, your customer collaboration will also kick into high gear. This is the time to do all those good activities described in Chapter 8, “Business- Facing Tests that Support the Team.” Ask the customers for examples, ask open-ended questions about each story’s functionality and behavior, have discussions around the whiteboard, and then turn those examples into tests to drive coding.\n\nEven if your product owner and/or other customers explained the stories be- fore and during iteration planning, it’s sometimes helpful to go over them brieﬂy one more time as the iteration starts. Not everyone may have heard it before, and the customer may have more information.\n\nLisa’s Story\n\nWe start writing high-level acceptance tests the ﬁrst day of the iteration. Because we go over all stories with the product owner the day before the iteration and write user acceptance tests as a team for the more complex stories, we have a pretty good idea of what’s needed. However, the act of writing more test cases of- ten brings up new questions. We go over the high-level tests and any questions we have with the product owner, who has also been thinking more about the stories.\n\n—Lisa\n\nHIGH-LEVEL TESTS AND EXAMPLES\n\nOne example of this was a story that involved a ﬁle of monetary distributions to plan participants who withdraw money from their retirement accounts. This ﬁle is sent to a partner who uses the information to cut checks to the participant. The amounts in some of the records were not reconciling correctly in the partner’s sys- tem, and the partner asked for a new column with an amount to allow them to do a reconciliation.\n\nAfter the iteration planning meeting, our product owner became concerned that the new column wasn’t the right solution and brought up his misgivings in the story review meeting. He and a tester studied the problem further and found that instead of adding a new amount, a calculation needed to be changed. This was actually a bigger story, but it addressed a core issue with the distributions. The team discussed the larger story and wrote new task cards. It was worth taking a little time to discuss the story further, because the initial understanding turned out to be wrong.\n\nGood communication usually takes work. If you’re not taking enough op- portunities to ask questions and review test cases, go ahead and schedule reg- ular meetings to do so. If there’s not much to discuss, the meetings will go quickly. Time in a meeting for an insightful discussion can save coding and testing time later, because you’re more certain of the requirements.\n\nHIGH-LEVEL TESTS AND EXAMPLES We want “big picture” tests to help the programmers get started in the right direction on a story. As usual, we recommend starting with examples and turning them into tests. You’ll have to experiment to see how much detail is appropriate at the acceptance test level before coding starts. Lisa’s team has found that high-level tests drawn from examples are what they need to kick off a story.\n\nHigh-level tests should convey the main purpose behind the story. They may include examples of both desired and undesired behavior. For our earlier Story PA-5 (Figure 17-2) that asks to show the shipping cost for 5-day deliv- ery based on the order’s weight and destination, our high-level tests might include:\n\n(cid:2) Verify that the 5-day shipping cost displays as the default as soon as\n\nthe user enters a shipping address.\n\n(cid:2) Verify that the estimated shipping cost matches the shipping cost on\n\nthe ﬁnal invoice.\n\n397\n\n—Lisa\n\n398\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\n(cid:2) Verify that the user can click a button to change the shipping address,\n\nand when this is done, the updated shipping cost displays.\n\n(cid:2) Verify that if the user deletes items from the cart or adds items to the\n\ncart, the updated shipping option is displayed.\n\nSee the bibliogra- phy for links to more information on graphical tests and model-driven development.\n\nDon’t conﬁne yourself to words on a wiki page when you write high-level tests. For example, a test matrix such as the one shown in Figure 15-7 might work better. Some people express tests graphically, using workﬂow drawings and pictures. Brian Marick [2007] has a technique to draw graphical tests that can be turned into Ruby test scripts. Model-driven development provides an- other way to express high-level scope for a story. Use cases are another possi- ble avenue for expressing desired behavior at the “big picture” level.\n\nA Picture Is Worth a Thousand Words\n\nThe saying “A picture says a thousand words” can also be applied to test cases and test validations.\n\nPaul Rogers [2008] has been experimenting with some cool ideas around this and explains his team’s approach to its problem in the following sidebar. Fig- ure 17-3 shows the UI model he describes.\n\nThe application I work on is very graphical in its nature. It allows a user to modify a web page by adding “photo enhancements” such as glasses, hats, or speech bubbles to images, or by highlighting the text in the web page with a highlighter pen effect.\n\nThere is a complex set of business rules as to what additions can be ap- plied to images, how and where they are afﬁxed, and how they can be rotated. To explain the tests for these rules, it was much simpler to draw a sketch of a typical web page with the different types of additions and add small notes to each picture.\n\nText highlighting also posed many challenges. Most problematic were the areas where text highlighting covered only part of an HTML tag. To de- scribe what should be expected in many different situations, we created different web pages and printed them out.\n\nUsing real pen highlighters, we highlighted the areas we expected to show as highlighted after starting and ending in certain areas. This way, we had an easy-to-read regression test.\n\nLow-tech tools can take the mystery out of complex application design. Find ways to express business rules as simply as possible, and share those with the entire team.\n\nSee the sample mock-up of UI changes in Chap- ter 16, “Hit the Ground Running.”\n\nSee Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” for some ideas on tools to gather and communicate requirements.\n\nHIGH-LEVEL TESTS AND EXAMPLES\n\nFigure 17-3 Sample of UI modeling technique\n\nMock-ups can convey requirements for a UI or report quickly and clearly. If an existing report needs modifying, take a screenshot of the report and use highlighters, pen, pencil, or whatever tools are handy. If you want to capture it electronically, try the Windows Paint program or other graphical tool to draw the changes and post it on the wiki page that describes the report’s requirements.\n\nDistributed teams need high-level tests available electronically, while co- located teams might work well from drawings on a whiteboard, or even from having the customer sit with them and tell them the requirements as they code.\n\nWhat’s important as you begin the iteration is that you quickly learn the ba- sic requirements for each story and express them in context in a way that works for the whole team. Most agile teams we’ve talked to say their biggest\n\n399\n\n400\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nproblem is to understand each story well enough to deliver exactly what the customer wanted. They might produce code that’s technically bug-free but doesn’t quite match the customer’s desired functionality. Or they may end up doing a lot of rework on one story during the iteration as the customer clari- ﬁes requirements, and run out of time to complete another story as a result.\n\nSee Chapter 8, “Business-Facing Tests that Support the Team,” for more about what makes up a requirement.\n\nPut time and effort into experimenting with different ways to capture and express the high-level tests in a way that ﬁts your domain and environment. Janet likes to say that a requirement is a combination of the story + conver- sation + a user scenario or supporting picture if needed + a coaching test or example.\n\nReviewing with Customers\n\nEarlier in this chapter we talked about the importance of constant customer collaboration. Reviewing high-level tests with customers is a good opportu- nity for enforced collaboration and enhanced communication, especially for a new agile team. After your team is in the habit of continually talking about stories, requirements, and test cases, you might not need to sit down and go over every test case.\n\nIf your team is contracting to develop software, requirements and test cases might be formal deliverables that you have to present. Even if they aren’t, it’s a good idea to provide the test cases in a format that the customers can easily read on their own and understand.\n\nReviewing with Programmers\n\nYou can have all of the diagrams and wiki pages in the world, but if nobody looks at them, they won’t help. Direct communication is always best. Sit down with the programmers and go over the high-level tests and requirements. Go over whiteboard diagrams or paper prototypes together. Figure 17-4 shows a tester and a programmer discussing a diagram of thin slices or threads through a user workﬂow. If you’re working with a team member in another location, ﬁnd a way to schedule a phone conversation. If team members have trouble understanding the high-level tests and requirements, you’ll know to try a dif- ferent approach next time.\n\nProgrammers with good domain knowledge may understand a story right away and be able to start coding even before high-level tests are written. Even so, it’s always a good idea to review the stories from the customer and tester perspective with the programmers. Their understanding of the story might",
      "page_number": 435
    },
    {
      "number": 53,
      "title": "Segment 53 (pages 443-450)",
      "start_page": 443,
      "end_page": 450,
      "detection_method": "topic_boundary",
      "content": "Chapter 2, “Ten Principles for Ag- ile Testers,” intro- duces the “Power of Three” rule.\n\nHIGH-LEVEL TESTS AND EXAMPLES\n\nFigure 17-4 A whiteboard discussion\n\nbe different than yours, and it’s important to look at mismatches. Remember the “Power of Three” rule and grab a customer if there are two opinions you can’t reconcile. The test cases also help put the story in context with the rest of the application. Programmers can use the tests to help them to code the story correctly. This is the main reason you want to get this done as close to the start of the iteration as you can—before programmers start to code.\n\nDon’t forget to ask the programmers what they think you might have missed. What are the high-risk areas of the code? Where do they think the testing should be focused? Getting more technical perspective will help with design- ing detailed test cases. If you’ve created a test matrix, you may want to review the impacted areas again as well.\n\nOne beneﬁcial side effect of reviewing the tests with the programmers is the cross-learning that happens. You as a tester are exposed to what they are thinking, and they learn some techniques for testing that they would not have otherwise encountered. As programmers, they may get a better under- standing of what high-level tests they hadn’t considered.\n\n401\n\n402\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nTest Cases as Documentation\n\nHigh-level test cases, along with the executable tests you’ll write during the iteration, will form the core of your application’s documentation. Require- ments will change during and after this iteration, so make sure your execut- able test cases are easy to maintain. People unfamiliar with agile development often have the misconception that there’s no documentation. In fact, agile projects produce usable documentation that contains executable tests and thus is always up to date.\n\nThe great advantage of having executable tests as part of your requirements document is that it’s hard to argue with their results.\n\nLisa’s Story\n\nFrequently, product owners, plan administrators, or business development man- agers will come and ask me a question such as, “What’s the system supposed to do if someone submits a loan payment for zero dollars?” or “Why didn’t everyone in this plan get a 3% nonelective contribution?”\n\nShowing them a FitNesse test that replicates the scenario is much more powerful than just showing them narrative requirements. Maybe the system wasn’t designed the way it should have been, but the test illustrates how it actually works, because we can clearly see the results of the inputs and operations. This has saved a lot of arguments on the level of “I thought it worked this way.”\n\nIf they decide the functionality, as implemented, is incorrect, we can change the expected outputs of the test and write a story to implement code to make the test pass again with the new expectations. You can’t do that with a requirements document.\n\nOrganizing the test cases and tests isn’t always straightforward. Many teams document tests and requirements on a wiki. The downside to a wiki’s ﬂexi- bility is that you can end up with a jumble of hierarchies. You might have trouble ﬁnding the particular requirement or example you need.\n\nChapter 14, “An Agile Test Automa- tion Strategy,” has more on test management.\n\nLisa’s team periodically revisits its wiki documentation and FitNesse tests, and refactors the way they’re organized. If you’re having trouble organizing your re- quirements and test cases, budget some time to research new tools that might help. Hiring a skilled technical writer is a good way to get your valuable test cases and examples into a usable repository of easy-to-ﬁnd information.\n\n—Lisa\n\nSUMMARY\n\nSUMMARY The iteration planning session sets the tone for the whole iteration. In this chapter, we looked at what agile testers do to help kick off the iteration to a good start.\n\n(cid:2) During iteration planning, testers help the team learn about the sto-\n\nries by asking questions and considering all viewpoints.\n\n(cid:2) Task cards need to be written along with development task cards and\n\nestimated realistically.\n\n(cid:2) Another way of tackling testing tasks is to write them directly on the\n\ndeveloper task cards.\n\n(cid:2) Teams should commit to the work for which they can complete all of\n\nthe testing tasks, because no story is done until it’s fully tested.\n\n(cid:2) The start of an iteration is the last chance to ensure that the stories are\n\ntestable and that adequate test data is provided.\n\n(cid:2) Testers collaborate with customers to explore stories in detail and write high-level test cases to let programmers kick off coding.\n\n(cid:2) Testers review high-level tests and requirements with programmers to\n\nmake sure they are communicating well.\n\n(cid:2) Tests form the core of the application’s documentation.\n\n403\n\nThis page intentionally left blank\n\nChapter 18\n\nCODING AND TESTING\n\nMeasuring Progress\n\nDefect Metrics\n\nIteration Metrics\n\nStart Simple\n\nResources\n\nAdd Complexity\n\nAssess Risk\n\nKeep the Build “Green”\n\nDriving Development\n\nTesting and Coding Progress Together\n\nKeep the Build Quick\n\nBuilding a Regression Suite\n\nRegression Tests\n\nIdentify Variations\n\nPower of Three\n\nChecking the “Big Picture”\n\nFocus on One Story\n\nTesters Facilitate Communication\n\nDistributed Teams\n\nFacilitate Communication\n\nCoding and Testing\n\nTests that Critique the Product\n\nWhich Bugs to Log?\n\nCollaborate with Programmers\n\nPair Testing\n\nShow Me\n\nWhen to Fix Bugs?\n\nWhat Media Do We Use to Log Bugs?\n\nAlternatives & Suggestions\n\nIt’s All about Choices\n\nTalk to Customers\n\nShow Customers\n\nUnderstand Business Needs\n\nStart Simple\n\nDefect vs. Feature\n\nTechnical Debt\n\nDealing with Bugs\n\nCompleting Testing Tasks\n\nAddress the Testing Crunch\n\nAnyone Can Do Testing Tasks\n\nZero Bug Tolerance\n\nOur agile tester has helped plan the release, size stories appropriately, and make sure they’re testable. She, along with colleagues on the customer and develop- ment team, has turned examples of desired behavior for each story into high- level user acceptance tests. She and her team have lined up the resources and infrastructure needed to deliver business value. Now, team members have picked up task cards and started writing code. What do testers do next, especially before any stories are ready to test?\n\n405\n\n406\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nDRIVING DEVELOPMENT The beginning of coding is a good time to start writing detailed tests. The high-level tests written before the iteration, or in the ﬁrst couple days of it, provide enough information for the programmers to start their own test- driven development. So now we have a bit of breathing room, but if we don’t move quickly, coding could get way ahead of testing and go off in the wrong direction.\n\nNow’s the time to start writing executable tests that illustrate the details about a story in order to keep development moving forward smoothly and help testing keep pace with coding. Like the high-level tests, we base detailed tests on examples provided by the customers.\n\nAt this point, we’re mainly writing tests that will be automated, but we’re also thinking ahead to the important exploratory testing we need to do as coding is completed.\n\nStart Simple\n\nAs testers, we’re easily distracted by interesting code smells and edge cases. However, if we’re using tests to guide coding, we have to start with the basics. Write the simplest happy path test you can in order to show that the core functionality works.\n\nChapter 14, “An Agile Automation Strategy,” gives pointers for select- ing the right tools.\n\nWhy executable tests? We’re working on an extremely tight schedule, and neither the programmers nor the testers have time to stop and run manual tests over and over. They do have time to click a button and run an auto- mated test. That test needs to fail in a way that makes the cause as obvious as possible. Ideally, we would give these tests to the programmers so that they could execute them as they code. That is one reason why picking the right automation framework is so important.\n\nFor some stories, automating the tests might take a long time. By keeping the ﬁrst test simple, you keep the focus on designing the automation solution. When the simple test works, it’s worth putting time into more complex test cases.\n\nWe stress the importance of automation, but Janet has worked with teams that have successfully used manual tests in the form of checklists or spread- sheets to give the programmers the information they need to start. However, to be successful in the long run, these tests do need to be automated.\n\nDRIVING DEVELOPMENT\n\nAdd Complexity\n\nAs soon as the happy path test works, start adding more test cases. Add boundary and edge conditions. The tests may show that the programmers misunderstood a requirement, or they may show that a tester did, or maybe the requirement’s true meaning eluded everyone. The important thing is that everyone talks about it and gets on track.\n\nAs testers think of new scenarios to validate with executable tests, they also think about potential scenarios for manual exploratory testing. Make a note of these for later pursuit.\n\nRemember the purpose of these tests. They should provide examples that tell the programmers what code to write. As the code evolves, your tests can chal- lenge it more, but resist the temptation to immediately follow smells into edge cases. Get the basics working ﬁrst. If you think of more cases based on some risk analysis, you can always add extra tests later.\n\nAssess Risk\n\nTesters have used risk analysis to help prioritize testing for a long time, and consideration for risk is already built into agile development. High-risk sto- ries may get higher size estimates, and teams consider risk as they prioritize stories during release and iteration planning.\n\nSome quick risk analysis can help you decide what testing to do ﬁrst and where to focus your efforts. We never have time to test everything, and we can use risk analysis to ﬁgure out how much testing is just enough.\n\nIf you have a really complex story, you may want to start by listing all of the potential risks related to the story. These aren’t limited to functionality. Con- sider security, performance, usability, and other “ilities.” Next, for each item, rate the impact on the business if it were to occur, using a scale of 1 to 5 (or whatever scale works for you): 1 being a low impact, 5 being a critical nega- tive impact.\n\nNow, consider the likelihood of each item occurring, using the same scale: 1 for not at all likely to happen, and 5 for items that probably will come up. Multiply the two ratings together to get the total risk rating for each item. This makes it easy to pick out the areas where your team should focus its test- ing efforts ﬁrst. Low-risk items can be left for last, or, because their impact is low or they’re highly unlikely to occur, may not be addressed at all.\n\n407\n\n408\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nYour domain makes a huge difference here. If you’re testing software that runs in heart pacemakers, you probably need to cover all risks with your testing no matter how low or unlikely they are. If you’re testing an internal company web application to be used by a few trained subject matter experts, you may be able to skip over scenarios that are unlikely or have an obvious workaround.\n\nConsider the story in Figure 18-1.\n\nStory PA-5\n\nAs a customer, I want to know how much my order\n\nwill cost to ship for based on the shipping speed\n\nI select so that I can choose a different shipping\n\nspeed if I want to.\n\nFigure 18-1 Story on shipping speeds\n\nFigure 18-2 shows a possible risk assessment for this shipping cost story.\n\n# 1 2 3\n\n4\n\n5\n\n6 7 8\n\nItem Incorrect cost displayed User can’t choose different shipping option Item isn’t eligible for selected shipping option, but selection allowed Estimated cost doesn’t match actual cost at checkout Invalid postal code entered and not caught by validation User can’t understand shipping option rules User can’t change shipping address User changes shipping address, but cost doesn’t change accordingly\n\nImpact 4 5 3\n\n3\n\n4\n\n2 5 5\n\nProbability 2 1 2\n\n4\n\n1\n\n3 2 4\n\nFigure 18-2 Sample risk assessment\n\nItem 8 is the highest-risk item, so we’d want to be sure to test changing ship- ping addresses and verify the updated costs. We might want to automate an\n\nRisk 8 5 6\n\n12\n\n4\n\n6 10 20",
      "page_number": 443
    },
    {
      "number": 54,
      "title": "Segment 54 (pages 451-458)",
      "start_page": 451,
      "end_page": 458,
      "detection_method": "topic_boundary",
      "content": "DRIVING DEVELOPMENT\n\nend-to-end test with this scenario. We’re not too worried about item 5; maybe we have already tested our postal code validation and feel good about it, so we don’t need to test it more. You may even have a very low-risk item that you chose not to test.\n\nHistory is usually a good teacher. Take note of past issues and make sure they don’t happen again.\n\nCoding and Testing Progress Together\n\nAt this point in the iteration, coding and testing continue hand in hand. Testers, programmers, database experts, and other team members collabo- rate to develop the stories, following the guidelines provided by examples and tests. Different team members may contribute their particular expertise, but all of them feel responsible for making sure each story is ﬁnished. All of them learn about the story and learn from each other as work progresses.\n\nLet’s look at how a team might work on the shipping cost story in Figure 18-1. Patty Programmer picks up a task card to code the estimated shipping cost calculations. She already understands the story pretty well from earlier dis- cussions, but she may look at the wiki pages or back of the story card where the testers wrote down some narrative describing the purpose of the story, some examples of how it should work, and some high-level tests to make sure she has a good idea of where to start. Tammy Tester sees that coding work has begun and starts to write behind-the-GUI test cases for the cost calculations.\n\nThe team had agreed during planning to start by calculating the 5-day ship- ping cost based on the shipping address and item weight. Items can only be shipped within continental North America, but that validation will be done in the presentation layer, so the cost calculation tests can assume only valid destinations are considered for input. They’re using a cost calculation API provided by the shipping partner, and Tammy asks Patty where to ﬁnd the al- gorithms so she can ﬁgure the cost herself in order to write the tests. Tammy writes the simplest test case she can think of in their behind-the-GUI test tool. We show it as a simple table in Figure 18-3.\n\nWeight 5 lbs\n\nDestination Postal Code 80104\n\nCost 7.25\n\nFigure 18-3 Simple happy path test\n\n409\n\n410\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nPatty hasn’t ﬁnished the code that would make this test pass yet, so Tammy starts working on another testing task for the story, setting up the test envi- ronment to work with the shipping partner’s test system.\n\nIdentify Variations\n\nBecause this story and the test are so straightforward, Patty and Tammy don’t discuss the test design and tweak it as they might on more complex stories. They also haven’t needed to ask the product owner more questions yet. Patty calls Tammy over to show her that the simple test is now working. Tammy writes up more test cases, trying different weights and destinations within the United States. Those all work ﬁne. She tries a Canadian postal code, and the test gets an exception. She shows this to Patty, who realizes that the API defaults to U.S. postal codes, and requires a country code for codes in Can- ada and Mexico. She hadn’t written any unit tests yet for other countries. They revise the test inputs, and Patty pairs with Paul Programmer to change the code that calls the API. Now the test looks something like Figure 18-4.\n\nWeight 5 lbs 5 lbs\n\nDestination Postal Code 80104 T2J 2M7\n\nCoutry Code US CA\n\nFigure 18-4 Revised happy path test\n\nThis simple example illustrates the iterative back-and-forth between cod- ing and testing. Different teams take different approaches. Patty and Tammy might pair on both the coding and testing. Tammy might pair with Paul to write the ﬁxture to automate the test. Tammy might be in a remote ofﬁce, using an online collaboration tool to work with Patty. Patty might write the executable story tests herself and then write the code to make them work, practicing true story test-driven development. The point is that testing and coding are part of one development process in which all team members participate.\n\nTammy can continue to identify new test cases, including edge cases and boundary conditions, until she feels all risk areas have been covered by the minimum amount and variety of test cases. She might test with the heaviest item available on the website sent to the most expensive destination. She might test having a large quantity of the same item. Some edge cases may be so unlikely that she doesn’t bother with them, or she decides to run a test but after it passes doesn’t include it in the regression suite. Some tests might be better done manually after a UI is available.\n\nCost 7.25 9.40\n\nDRIVING DEVELOPMENT\n\nPower of Three\n\nPatty has written unit tests with Hawaii as the shipping destination, but Tammy believes that only continental destinations are acceptable. Neither of them is sure whether military post ofﬁce box destinations are acceptable. They go see Polly Product-Owner to ask what she thinks. They’re using the Power of Three. When disagreements or questions arise, having three differ- ent viewpoints is an effective way to make sure you get a good solution and you won’t have to rehash the issue later. If one of the participants in the dis- cussion isn’t familiar with the topic, the others will have to organize their thoughts to explain it clearly, which is always helpful. Involving people in dif- ferent roles helps make sure that changes to requirements don’t ﬂy under the radar and surprise team members later.\n\nWhen unexpected problems arise, as they always do, the Power of Three rule is a great place to start. You may need to pull in more people, or even the whole team, depending on the severity or complexity of the issue. What if the shipping partner’s API proves to be so slow that the response time on the website will be unacceptable? Both the development team and the customer team need to quickly explore alternative solutions.\n\nFocus on One Story\n\nPaul looks for a programming task to work on. Although the UI tasks for the estimated shipping cost story are still in the “to do” column on the task board, he’s more interested in the story to delete items out of the shopping cart, so he picks up one of those cards. Nobody has time to start writing the executable tests for that story, so he plunges ahead on his own.\n\nNow the team has two stories going. They don’t really know how much time it will take to ﬁnish either story. A much better approach would be for Paul to start working on a UI task for the ﬁrst story so that story can be ﬁnished sooner. When a story’s done (meaning all of the code is written and tested), you know exactly how much work is left to do on it: zero. If disaster struck and no other stories got ﬁnished this iteration, there is at least one completed story to release.\n\nCompleting the whole story isn’t a testing concept, but it’s one that testers should promote and follow. If a programmer has started coding on a story, make sure someone has also started working on testing tasks for that story. This is a bal- ancing act. What if nobody has written even high-level tests for the delete items story? Maybe that’s the highest testing priority? Usually, ﬁnishing a story should be the goal before the team can move on to the next story.\n\n411\n\n412\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nUnless the team is very small, there is always more than one story in progress at any given time. It might be more difﬁcult, but try to focus on ﬁnishing one story at a time. Patty is about to wrap up the shipping cost story, and Paul has moved on to the delete items story. Patty runs into a snag, and she isn’t sure how to solve it. Paul helps her to ﬁnish the code so that Tammy can ﬁnish her exploratory testing and they can mark the story “done.” Now they have a bet- ter idea of how much they have left to ﬁnish this iteration (or at least, how much they don’t still have to work on).\n\nSometimes, several different stories can be done at the same time if a pro- grammer and tester pair up to complete each story together. This works if the stories are small and independent. What you don’t want to see is program- mers starting coding without testing tasks being completed at the same time.\n\nTESTS THAT CRITIQUE THE PRODUCT As soon as testable chunks of code are available, and the automated tests that guided their coding pass, take time to explore the functionality more deeply. Try different scenarios and learn more about the code’s behavior. You should have task cards for tests that critique the product, both business- and technology- facing. The story’s not “done” until all of these types of tests are complete.\n\nThis becomes more important when all tasks except testing are complete for a story. Now you should be able to test from one end of the story’s thread to the other end, with all of the variations in between. Don’t put this testing off. You may ﬁnd requirements that were in the story but were missed with the tests that drove development and are thus missing in the code. Now’s the time to write those missing tests and code. Fill in all of the gaps and add more value while the team is still focused on the story. Doing this later will cost much more.\n\nChapter 10, “Business-Facing Tests that Critique the Product,” and Chapter 11, “Tecnology-Facing Tests that Critique the Product,” will help you make sure you cover all of the necessary tests that critique the product.\n\nBe aware that some of what you learn in testing the ﬁnal story may be consid- ered “nice to have,” perhaps making the functionality easier to use or faster, items that weren’t part of the original story. Consult with your customer. If there’s time to add it in the iteration, and the business can use the extra value, go ahead. These additions are much cheaper to add now. But don’t jeopardize other stories by spending too much time adding “bling” that doesn’t have a big ROI.\n\nIf your exploratory testing leads the team and the customers to realize that signiﬁcant functionality wasn’t covered by the stories, write new stories for future iterations. Keep a tight rein on “scope creep” or your team won’t have time to deliver the value you planned originally.\n\nCOLLABORATE WITH PROGRAMMERS\n\nTechnology-facing tests to critique the product are often best done during coding. This is the time to know if the design doesn’t scale, or if there are se- curity holes.\n\nCOLLABORATE WITH PROGRAMMERS Our vignette describing a team writing and using detailed tests to drive cod- ing shows how closely testers and programmers collaborate. This continues as coding and testing proceed. Working together enhances the team’s ability to deliver the right product and provides many opportunities to transfer skills. Programmers learn new ways of testing, and they’ll be better at testing their own code as they write it. Testers learn more about the process of cod- ing and how the right tests might make it easier.\n\nPair Testing\n\nPaul Programmer has completed the user interface for the estimated ship- ping options story, but he hasn’t checked it in yet. He asks Tammy to come sit with him and demonstrates how the end user would enter the shipping ad- dress during the checkout process. The estimated shipping cost displays right away. Tammy changes the shipping address and sees the new cost appear. She enters a postal code that doesn’t match the rest of the address and sees the appropriate error message appear. The UI looks good to both of them, so Paul checks in the code, and Tammy continues with her exploratory manual testing of it.\n\nJanet likes to have the programmer “drive” during these pair testing sessions while she watches what happens. She ﬁnds that it is far more effective than taking control of the keyboard and mouse while the programmer watches.\n\n“Show Me”\n\nTammy is especially concerned with changing the shipping address and having the estimated cost recalculate, because they identiﬁed that as a risky area. She ﬁnds that if she displays the estimated cost, goes ahead to the billing address page, and then comes back to change the shipping address, the estimated costs don’t change properly. She gets Paul to come observe this behavior. He realizes there is a problem with session caching and goes back to ﬁx it.\n\nShowing someone a problem and working through it together is much more effective than ﬁling a bug in a defect tracking system and waiting for someone to have time to look at it. It’s harder to do if the team isn’t co-located. If team\n\n413\n\n414\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nmembers are working in vastly different time zones, it’s even harder. Stick to the most direct communication available to you. One of Lisa’s teammates is in a time zone 121⁄2 hours ahead. He works late into his nighttime, and when needed, he calls Lisa and they work through test results and examples together.\n\nThe bibliography contains refer- ences for further reading on this subject.\n\nThe simple act of showing the GUI to another person may help Paul realize he’s implemented some erroneous behavior. Similarly, if Tammy is having trouble getting her GUI test script to work, explaining the problem might be enough for her to realize what’s causing it. If there is nobody available to look at what you’ve just coded or help you debug a problem, it sometimes helps to explain it out loud to yourself. “Rubber Ducking” and “Thinking Out Loud” are surprisingly effective ways to solve your own problems. Janet likes to have her own little rubber duck sitting on her desk to remind herself to think be- fore she asks.\n\nTALK TO CUSTOMERS It’s shockingly easy for development team members to get their heads down cranking out stories and forget to keep customers in the loop. In addition to consulting business experts when we have questions, we need to show them what we’ve delivered so far.\n\nHopefully, you were able to review test cases with customers, or with some- one who could represent the customer, before coding began. If not, it’s never too late. For situations where customers need to be more involved with the details of the executable tests, be sure to ﬁnd test tools that work for them as well as for technical team members.\n\nAs we described in the last two chapters, you may have already gone over mock-ups or paper prototypes with your customers. If tasks to mock up a re- port or interface remain in the iteration plan, remember to keep the process simple. For example, don’t code an HTML prototype when drawing on a whiteboard will do just as well. We want to keep the process as simple as pos- sible; simplicity is a core value.\n\nShow Customers\n\nAs soon as a coded user interface or report is ready, even if it’s still rudimentary, lacking all features or displaying hard-coded data, show it to the appropriate customers. Nobody can explain exactly what they want ahead of time. They need to see, feel, and use the application to know if it’s right. You may not be\n\nLisa’s Story\n\nCOMPLETING TESTING TASKS\n\nable to implement big changes mid-iteration, but if you start early, there may be time for minor tweaks, and your customers will know what to expect.\n\nThe iteration review meeting is a great opportunity to show what the team delivered and get feedback for the next iteration, but don’t wait until then to get input from customers. Keep them involved throughout the iteration.\n\nUnderstand the Business\n\nAlthough we get caught up in the fast pace of iterations, we also need to stop and take time to understand the business better. Spend some time talking to business people about their jobs and what aspects might be enhanced with new software features. The better you understand your customer’s business, the better you can be at providing a good product.\n\nMy team budgets time for each development team member to sit with the retire- ment plan administration team members as they do their daily work. Not only do we understand those jobs better, but we often identify small changes in the appli- cation that will make the administrator’s work easier.\n\nSimple additions such as a bit of extra data provided, an additional search ﬁlter, or changing the order of a display can make a big difference to a tedious and de- tailed process. We also document what we learn with ﬂow charts and wiki pages so that other team members can beneﬁt.\n\nSome teams actually sit with the business people permanently so that they are involved with the actual business on a daily basis.\n\nCOMPLETING TESTING TASKS Agile testers are proactive. We don’t sit and wait for work to come to us. Testers who are accustomed to a waterfall process may feel there’s nothing to do until a story is 100% complete. That’s rarely true during an agile iteration. Work with programmers so that they produce some testable piece of code early on. The shipping cost algorithm presented earlier is a good example. It can be tested completely in isolation, without needing to access the database or the user interface. Alternatively, the user interface could be stubbed out with hard-coded data before the services accessing the real data are complete, and the behavior of the presentation layer can be tested by itself.\n\n415\n\n—Lisa\n\n416\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nPeril: The Testing Crunch\n\nEven experienced agile teams often experience a testing crunch at the end of an iteration. Maybe a story or two turned out to take much longer than ex- pected, or a production problem took time away from development. What happens when tomorrow is the end of your iteration and your task board (real or virtual) is still full of testing cards?\n\nIf you see this, recognize it as a bad smell. Work with the team to determine what the problem may be. Are the programmers not working closely enough with the testers? Were there too many interruptions?\n\nThe way to address this peril is to involve the whole team. Remember that anyone on the team can sign up for testing tasks. In your daily stand-up, you can evaluate whether the team is on track to ﬁnish all of the stories. If multiple stories are in danger of not being completed, choose a story to drop, or re- duce the scope on one or more stories. Focus on completing one story at a time. As the end of the iteration approaches, programmers may have to stop working on new features and start picking up testing tasks instead. Missing some functionality from a release is better than missing the entire release be- cause testing couldn’t be completed on all or most stories.\n\nThe programmers on Lisa’s team regularly automate behind-the-GUI tests in addition to unit and integration tests. They also often write the functional behind-the-GUI test cases. Sometimes they write the initial happy path execut- able test so they can coordinate test and code design; then a tester adds more test cases. Occasionally, they write all of the functional test cases, because the testers don’t have the bandwidth to cover all of the test-intensive stories.\n\nEveryone on the team also must be willing to take on manual testing tasks. If your team is just starting and hasn’t been able to address automation needs yet, the whole team should plan time to execute manual regression test scripts as well as manually testing new features. As Lisa’s team can attest, this task provides great motivation for learning how to design the application to facilitate test automation. Other teams tell us this worked for them as well.\n\nDEALING WITH BUGS We’ve known many teams that struggle with the question of how to track bugs, or whether to track them at all. As Tom and Mary Poppendieck write in their book Implementing Lean Software Development: From Concept to Cash [2006], defect queues are queues of rework and thus collection points for",
      "page_number": 451
    },
    {
      "number": 55,
      "title": "Segment 55 (pages 459-468)",
      "start_page": 459,
      "end_page": 468,
      "detection_method": "topic_boundary",
      "content": "Chapter 5, “Transi- tioning Typical Processes,” talks about why your team may or may not want to use a Defect Tracking System.\n\nDEALING WITH BUGS\n\nwaste. Some teams simply ﬁx bugs as soon as they’re discovered. They write a unit test to reproduce the bug, ﬁx the code so the test passes, check in the test and the bug ﬁx, and go on. If someone breaks that piece of code later, the test will catch the regression.\n\nOther teams ﬁnd value in documenting problems and ﬁxes in a defect track- ing system (DTS), especially problems that weren’t caught until after code was released. They may even look for patterns in the bugs that got to produc- tion and do root cause analysis to learn how to prevent similar issues from recurring. Still, defect systems don’t provide a good forum for face-to-face communication about how to produce higher-quality code.\n\nLisa and her fellow testers prefer to talk to a programmer as soon as a prob- lem is found. If the programmer can ﬁx it immediately, there’s no need to log the bug anywhere. If no programmer is available immediately to work on the problem, and there’s a possibility the bug might be forgotten, they write a card for it or enter it into their DTS.\n\nWe’ve added this section to this chapter because this is when you run into the problem. You have been writing tests ﬁrst, but are ﬁnding problems as you work with the programmer. Do you log a bug? If so, how? You’ve been doing your exploratory testing and found a bug from a story that was marked done. Do you log a bug for that? Let’s discuss more about defects and consider op- tions that are open to you and your team.\n\nIs It a Defect or Is It a Feature?\n\nFirst, let’s talk about defects versus features. The age-old question in software development is, “What is a bug”? Some answers we’ve heard are: It’s a devia- tion from the requirements or it’s behavior that is not what was expected. Of course, there are some really obvious defects such as incorrect output or in- correct error messages. But what really matters is the user’s perception of the quality of the product. If the customer says it is a defect, then it is a defect.\n\nIn agile, we have the opportunity to work with customers to get things ﬁxed to their satisfaction. Customers don’t have to try to think of every possible feature and detail up front. It is okay for them to change their minds when they see something.\n\nIn the end, does it really matter if it is a bug or a feature if it needs to be ﬁxed? The customer chooses priorities and the value proposition. If software quality\n\n417\n\n418\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nis a higher priority for the customer than getting all of the new features, then we should try to ﬁx all defects as we ﬁnd them.\n\nCustomers on the team use their knowledge to give the best advice they can to the team on day-to-day development. However, when a product goes to UAT and is exposed to a larger customer base, there will always be requests in the form of bugs or new enhancements.\n\nTechnical Debt\n\nChapter 6, “The Purpose of Test- ing,” explains how tests help manage technical debt.\n\nOne way of thinking about defects is as technical debt. The longer a defect stays in the system and goes undetected, the greater the impact. It also is true that leaving bugs festering in a code base has a negative effect on code quality, system intuitiveness, system ﬂexibility, team morale, and velocity. Fixing one defect in buggy code may reveal more, so maintenance tasks take longer.\n\nZero Bug Tolerance\n\nJanet encourages teams that she works with to strive for “zero tolerance” to- ward bug counts. New agile teams usually have a hard time believing it can be done. In one organization Janet was working with, she challenged each of the ﬁve project teams to see how close they could come to zero bugs out- standing at the end of each iteration, and zero at release time.\n\nZero Bug Iterations\n\nJakub Oleszkiewicz, the QA manager at NT Services [2008], recounts how his team learned how to ﬁnish each iteration with no bugs carried over to the next one.\n\nI think it really comes down to exceptional communication between the testers, the developers, and the business analysts. Discipline was also key, because we set a goal to close off iterations with fully developed, functional, deployable, and defect-free features while striving to avoid falling into a waterfall trap. To us, avoiding waterfall meant we had to maintain alignment with code and test activities; we tried to plan an it- eration's activities so that a given feature's test cases were designed and automated at the same time as that feature's code was written. We quickly found that we were practicing a form of test-driven develop- ment. I don't think it was pure TDD, because we weren't actually exe- cuting the tests until code was checked in, but we were developing the tests as developers wrote code, and developers were asking us how our tests were structured and what our expected results were.\n\nIT’S ALL ABOUT CHOICES\n\nConversely, we regularly asked the developers how they were imple- menting a given feature. This kind of two-way questioning often elevated inconsistencies in how requirements were interpreted and ultimately highlighted defects in our interpretations before code was actually committed.\n\nEvery morning during our Scrum, we further ensured parity between the functional groups within the team through simple dialogue. Communica- tion was ridiculously good—we sat close to each other, often even at the same computer. When a defect was discovered, the developer was right there observing, taking notes, and talking through the require- ments. A business analyst was always nearby to further validate our thinking. Often within minutes a resolution was checked-in, deployed to the test environment, and veriﬁed.\n\nBoth developers and testers had to be committed to this approach or it wouldn't have worked. Without discipline, the developers could have easily moved forward onto more features and let the bugs slide until the end of the project, risking an incomplete iteration. If we were not co- located as we were, communication would have suffered; likely a bug tracking system or email would have become our primary means of com- municating defects, resulting in longer turn-around times and an in- creased probability of rework.\n\nAs part of any development, you will always need to make trade-offs. Your team may decide to release with some outstanding bugs because it is deemed more important to get new functionality out the door than to ﬁx low-level bugs.\n\nIT’S ALL ABOUT CHOICES Teams have solved the problem of how to handle defects in many different ways. Some teams put all of their bugs on task cards. Other teams have cho- sen to write a card, estimate it, and schedule it as a story. Still others suggest adding a test for every bug—that way you don’t have to record the defect, just the test.\n\nIs there one right way? Of course not! But, how do you know what is right for your team? We have some suggestions to help you choose and decide what is right for you. Think about your team and your product and what might work in your situation. First, we’ll talk about what defects we should log, then we’ll talk a bit about when you should ﬁx them, and ﬁnally we’ll look at what media to choose. The right combination will depend on how far along your team is in its agile journey and how mature your product is.\n\n419\n\n420\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nDecide Which Bugs to Log\n\nNot all bugs need to be logged, but teams often struggle with which ones should be recorded and which ones don’t need to be. We recommend that you avoid creating a defect report if possible. Have a conversation with a real person ﬁrst, and only produce a defect report if it is truly a real problem that demands a change to the product or the programmers just can’t get to it right away.\n\nUnit Test Failures Don’t log unit test failures. If you are part of a team that is practicing TDD (test-driven development) and has good coverage with its unit tests, you know that failed tests during the build should not be logged. A failed test during the continuous integration build is a signal for the programmers to address the problem right away. Logging these bugs would be redundant and a waste of time.\n\nFailures in Higher-Level Regression Tests Many teams have builds that run regression tests above the unit level, such as tests behind the GUI and tests through the GUI. When one of these builds fails, should you log the bug in a DTS?\n\nLisa’s Story\n\nWe have two builds, an “ongoing build” that runs only unit tests, and a “full build” that runs the functional tests behind and through the GUI. When the “full build” breaks, if a developer investigates and tackles the problem right away as some- times happens, usually no bug is logged. The problem is ﬁxed quickly. At other times, the failure is not straightforward. One of the testers investigates, narrows down the problem, and ﬁles a bug that either states the name of the failing test or provides manual steps to recreate the problem.\n\nIn either case, tests are written that reproduce the bug, and the code is ﬁxed to make the tests pass. The tests become part of one of the builds.\n\nFailing tests in themselves are a type of recorded bug. But sometimes, as in Lisa’s case, more information needs to be added to allow for an effective and clean ﬁx, so logging the defect is warranted.\n\nStory Bugs within the Current Iteration Don’t log bugs that can be ﬁxed immediately, especially if you would other- wise record them in an electronic DTS. If your team is working closely with\n\n—Lisa\n\nIT’S ALL ABOUT CHOICES\n\nthe programmers and is practicing pair testing as soon as a story is com- pleted, we strongly recommend that you don’t log those bugs as long as the programmer addresses them right away. As you notice issues, talk them over with the programmer and decide whether they are real issues or not. Talk to the customer if you need to, but make a couple of notes so you remember what you saw so you can adjust your tests if needed.\n\nIf you are using index cards to log bugs, you may want to put an index card up on the task board (or a card on your electronic board) just as a reminder.\n\nPost-Iteration Bugs (Or Those that Can’t Be Fixed Immediately) Do log bugs that can’t be ﬁxed right away. We stress testing early in order to catch as many bugs as possible while the programmers are still working on the story. We know it is cheaper to ﬁx them when caught early; however, sometimes we just don’t catch them right away. The programmer has moved on to another story and can’t drop everything to ﬁx it now. Those are the ones that are good candidates for logging. Sometimes a “bug” is really a missed requirement and needs to be handled as a story—estimated and pri- oritized for a future iteration.\n\nFrom the Legacy System Do log bugs that occur in the legacy system. If your product has been around a long time, it likely has a number of bugs that have been lurking in the back- ground just waiting to be discovered. When you ﬁnd them, you have a couple of choices. If your product owner thinks it is worthwhile to ﬁx them, then log the bugs and they can be prioritized as part of the product backlog. However, if they have been around a long time and cause no issues, your product owner may decide it is not worth ﬁxing them. In this case, don’t bother log- ging them. They will never get addressed anyhow, so don’t waste your time.\n\nFound in Production Do log all production bugs. When your application is in production, all bugs found by the customer should be logged. Depending on their severity, these bugs may be ﬁxed immediately, at the time of the next release, or they’ll be estimated, prioritized, and put in your product backlog.\n\nChoose When to Fix Your Bugs\n\nThere are three options. All bugs you ﬁnd need to be triaged to determine if you ﬁx them now, ﬁx them later, or don’t ﬁx them at all. This triage may be as simple as a discussion with the programmer to determine if they are really\n\n421\n\n422\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nbugs in the story he is working on. The triage may be a discussion with the product owner to determine if there should be another story for the next it- eration. The triage may also be a formal process with the customers to prior- itize which bugs to ﬁx.\n\nFix Now The more bugs you can ﬁx immediately, the less technical debt your applica- tion generates and the less “defect” inventory you have. Defects are also cheaper to ﬁx the sooner they are discovered. In an article in iSixSigma Mag- azine, Mukesh Soni [2008] quotes a report from IBM that the cost to ﬁx an error found after product release was four to ﬁve times as much as one un- covered during design, and up to 100 times more than one identiﬁed in the maintenance phase (see Figure 18-5).\n\nFigure 18-5 shows a statistic based on phased methodology, but the statistic still holds true for agile development. It is cheaper to ﬁx bugs that are found during development than after.\n\nIf a defect is found while developing a new feature, or is a side effect from an- other bug ﬁx, it should be automatically ﬁxed. But, as usual, this is to be ap- plied with prudence. For example, if a bug is found that the programmers say will be difﬁcult to ﬁx and may destabilize the product, it should be taken to the customers to prioritize.\n\nPhase/Stage of the S/W Development in which the Defect is Found\n\n120\n\n100\n\n100x\n\n80\n\n60\n\n40\n\n20\n\n15x\n\n1x\n\n6.5x\n\n0\n\nDesign\n\nImplementation\n\nTesting\n\nMaintenance\n\nFigure 18-5 Relative costs to ﬁx software defects (Source: IBM Systems Sciences Institute)\n\nIT’S ALL ABOUT CHOICES\n\nIf you ﬁx the bugs during development, you lessen the presence of bugs later in the process. Your team velocity can include time to ﬁx bugs. Over time, your team members will get a good idea of how long they spend on ﬁxing bugs found by the testers for a story. Hopefully, there are few. If your team is a new agile team, there may be quite a few bugs that escape development, but as the team gets more comfortable with the tools and the processes, the num- ber of bugs found will lessen. To start, try making the estimate for a story to include two hours or half a day for ﬁxing associated bugs.\n\nFix Later Different teams have different ways of handling defects. Some teams believe that all defects found should be prioritized by the customers before they get put on the list to ﬁx. They believe it is completely up to the customer to deter- mine whether they really are defects, and if so, whether they should be ﬁxed.\n\nNever Fix Your team has recognized a defect, but know it won’t get ﬁxed. Perhaps that section of code needs a complete rewrite later because the functionality will change, or perhaps it is just such a low-priority issue or so obscure that your customers may never ﬁnd it. There are a multitude of reasons why it won’t get ﬁxed. If your triage determines this is the case, we suggest you just close the bug. Don’t keep it open pretending that you will ﬁx it someday.\n\nChoose the Media You Should Use to Log a Bug\n\nWhen we talk about media, we mean the variety of ways you can log a bug. It could be a defect tracking system or index cards, or maybe you choose to have no physical record at all.\n\nIndex Cards Index cards (whether real or virtual cards in an online planning and tracking system) don’t leave a lot of room for a lot of clerical details, but they do give great visibility to outstanding issues when they are pinned on the story board, especially if they are in another color. Some teams use screen prints and staple them to the back of the card or write the details in a text ﬁle, or even record steps in audio form on a hand-held voice recorder.\n\nThere are lots of options, but we would suggest that you pick one that contains enough information to guide someone to reproduce a problem or to focus a discussion when the programmer is ready to ﬁx it. The card is tangible. Five hundred bugs in a DTS are just a number. A stack of 500 cards is impressive.\n\n423\n\n424\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nUse cards in the following circumstances:\n\n(cid:2) You are a disciplined agile team and are ﬁxing all bugs within an\n\niteration.\n\n(cid:2) You want to make bugs visible to the team.\n\nThere is nothing stopping you from having both index cards and a DTS.\n\nDefect Tracking System Use a DTS in the following circumstances:\n\n(cid:2) Your team is distributed. (cid:2) You need to track bugs for audit purposes or to capture them in re-\n\nlease notes.\n\n(cid:2) You have bugs that escape an iteration and you need to remember to\n\nﬁx them later.\n\n(cid:2) You have a legacy system with a large number of defects.\n\nOne way or the other, you will likely want to have some kind of DTS to log some of the bugs. This does not mean you need to log them all. Be smart about which ones you do log.\n\nNone at All Why wouldn’t you log a bug? Most teams that we have worked with have set rules for themselves that no bug is ﬁxed without a unit test. If you also have a functional automation suite, then you can catch the larger bugs with those. The argument is that if there is a test that will catch the bug, you have no need to log the bug. Anything learned from ﬁxing the bug was captured in the test and the code. However, you need to recognize that not all tests are easy to automate.\n\nUse tests to capture bugs in the following circumstance:\n\n(cid:2) Your team is disciplined and writes tests for every bug found.\n\nAlternatives and Suggestions for Dealing with Bugs\n\nAs teams mature, they ﬁnd procedures that work for them. They eliminate redundant tasks. They become more practiced at using story cards, story\n\nJanet’s Story\n\nIT’S ALL ABOUT CHOICES\n\nboards, and project backlogs. They use tests effectively, and learn which bugs to log and what metrics make sense to their team. In this section, we’ll share some ideas that other teams have found work for them.\n\nSet Rules Set rules like, “The number of pink cards (bugs) should never get higher than ten at any one time.” Revisit these each time you have a team retrospective. If your defect rate is going down, no worries. If the trend is the opposite, spend time analyzing the root cause of bugs and create new rules to mitigate those.\n\nFix All Bugs Don’t forget to ﬁx low-priority bugs found during the iteration as well, be- cause they have an effect on future development. In our experience, there seems to be a strong correlation between “low priority” and “quick to ﬁx,” al- though we don’t have hard facts to support that. We suggest stopping small, isolated bugs before they become large, tangled bugs.\n\nCombine Bugs If you ﬁnd a lot of bugs in one area, think about combining them into an en- hancement or story.\n\nWhen I ﬁrst started working at WestJet, I found a lot of small issues with the mobile application. The application worked correctly, but I was confused about the ﬂow. I only found these issues because I was new and had no previous perceptions.\n\nThe team decided to group the issues I had raised and look at the whole issue as a new story. After studying the full problem with all of the known details, the ﬁnal outcome was a solid feature. If the bugs had been ﬁxed piecemeal, the effect would not have been so pretty.\n\nTreat It as a Story If a “bug” is really missed functionality, choose to write a card for the bug and schedule it as a story. These stories are estimated and prioritized just like any other story. Be aware that bug stories may not receive as much attention as the new user stories in the product backlog. It also takes time to create the story, prioritize, and schedule it.\n\n425\n\n—Janet\n\n426\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nThe Hidden Backlog\n\nAntony Marcano, author of www.TestingReﬂections.com, points out that while user stories and their acceptance tests describe desired behavior, de- fect reports describe misbehavior. Behind each misbehavior is a desired be- havior, often not previously deﬁned. Thus, behind every defect report may be a hidden user story. He explains his experiences.\n\nIn Chapter 5, “Transitioning Typical Processes,” we mentioned Antony Marcano’s blog post about defect tracking systems being a hidden backlog in agile teams. Antony shares his ideas about how to bring that secret out into the open.\n\nXP publications suggest that if you ﬁnd a bug you should write an auto- mated test reproducing it. Many teams ﬁle a bug report and then write a separate automated test. I’ve found that this results in duplication of ef- fort—and therefore waste. When we write a bug report, we state the steps, what should have happened (expectation), and what actually happened (anti-expectation). An automated test tells you the same things—steps, expectation, and running it for the ﬁrst time should dem- onstrate the anti-expectation. When you are able to write an automated acceptance test as easily as you write a bug-report and the test commu- nicates as much as the bug report does and your backlogs and story boards allow you to manage the work involved in ﬁxing it, then why write a separate bug report?\n\nBug metrics are all that remain. Bug metrics are traditionally used to help predict when software would be ready for release or highlight whether quality is improving or worsening. In test-ﬁrst approaches, rather than telling us if quality is improving or worsening, it tells us how good we were at predicting tests—that is, how big the gaps were in our original thinking. This is useful information for retrospectives and can be achieved simply by tagging each test with details of when it was identi- ﬁed—story elaboration, post-implementation exploration, or in produc- tion. As for predicting when we will be able to release—when we are completing software of “releasable quality” every iteration—this job is handled by burn-down/burn-up charts and the like.\n\nWith one new project I was working on, I suggested that we start using a bug-tracking system when the need for one was compelling. We cap- tured the output of exploratory testing performed inside the iteration as automated tests rather than bug reports. We determined whether the test belonged to the current story, another story, or whether these tests inspired new stories. We managed these stories as we would any other story and used burn-down charts to predict how much scope would be done by the end of the iteration. We never even set up a bug-tracking system in the end.\n\nThere is a difference between typical user stories and bug-inspired user stories, however. Previously our stories and tests only dealt with missing behaviors (i.e., features we know we want to implement in the future).",
      "page_number": 459
    },
    {
      "number": 56,
      "title": "Segment 56 (pages 469-476)",
      "start_page": 469,
      "end_page": 476,
      "detection_method": "topic_boundary",
      "content": "Janet’s Story\n\nIT’S ALL ABOUT CHOICES\n\nNow, they also started to represent misbehaviors. We found it useful to in- clude summary information about the misbehavior in our proposed user story to help the customer prioritize it better. For example:\n\nAs a registered user, I want to be prevented from accessing the system if my password is entered using the incorrect case, so that I can feel safer that no one else can guess my password, rather than being allowed to access the system.\n\nThe “rather than” was understood by the customer to mean \"that's something that happens currently\"— which is a misbehavior rather than merely a yet-to-be-implemented behavior.\n\nUsing this test-only approach to capturing bugs, I’ve noticed that bug- inspired stories are prioritized more as equals to the new-feature user stories, whereas before they often gave more attention to the “cool new features” in the product backlog than the misbehaviors described in the bug tracking. That's when I realized that bug-tracking systems are essen- tially hidden, or secret backlogs.\n\nOn some teams, however, the opposite is true. Fix-all-bugs policies can give more attention to bugs at the expense of perhaps more important new features in the main backlog.\n\nNow, if I'm coaching a team mid-project, I help them to ﬁnd better and faster ways of writing automated tests. I help them use those improve- ments in writing bug-derived automated tests. I help them ﬁnd the ap- propriate story—new or existing—and help them harness the aggregate information useful to retrospectives. Eventually, they come to the same realization that I did: Traditional bug tracking starts to feel wasteful and redundant. That's when they decide that they no longer want or need a hidden backlog.\n\nIf bugs are simply logged in a DTS, important information might be effectively lost from the project. When we write acceptance tests to drive development, we tend to focus on desired behavior. Learning about undesired behavior from a defect, and turning that into stories is a vital addition to producing the right functionality.\n\nBlue, Green, and Red Stickers Each team needs to determine the process that works for it, and how to make that process easily visible. The following story is about one process that worked for Janet.\n\nA few years ago, I worked on a legacy system with lots of bugs already logged against the system before agile was introduced. One of the developers was adamant that he would not use a defect-tracking system. He ﬁrmly believed they\n\n427\n\n428\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nwere a waste of time. However, the testers needed the defects logged because there were so many.\n\nThe team worked out a compromise that worked for everyone. Bugs that were found during pair testing with the programmers were not recorded, because they were ﬁxed right away. All others were logged in the DTS. Bugs that needed to be ﬁxed in the current iteration were recorded on pink cards with the summary and bug number and then put on the story board. All others became part of the prod- uct backlog.\n\nThe programmers could look at details in the system but also asked testers for more information, if required. Because the issues were on the story board, they became part of the daily stand-ups and discussions. When a bug was ﬁxed, the programmers wrote the ﬁx and any extra information on the back of the card. They put a blue sticker on the card so the testers knew it was ready for testing. A green sticker meant it had been veriﬁed as ﬁxed, and a red sticker meant it wasn’t ﬁxed and needed more work. Of course, there were lots of conversations be- tween the testers and the programmers. James, one of the programmers, and I had a lot of fun with one bug that just wouldn’t stay ﬁxed. By the end, the card looked like it had a caterpillar on it—blue, red, blue, red, blue, and ﬁnally green. We were all quite excited when that bug was squashed.\n\nThe testers closed bugs and did most of the administration, because the DTS was their requirement. After a while, the programmers started entering what they ﬁxed into the defect-tracking system because it was easier than writing on the card. The team still continued to use the cards because of the visibility. It was easy to see at a glance how many outstanding bugs there were in the iteration or on the backlog.\n\nThis approach worked for this team because there was a lot of discipline in the team, and most new bugs were ﬁxed in the iteration if they were part of the new or changed functionality. The only bugs that went into the backlog were legacy bugs that were deemed low risk.\n\nStart Simple\n\nWe suggest using as simple a system as possible and applying complexity as required. Code produced test-ﬁrst is, in our experience, fairly free of bugs by the time it’s checked in. If you’re ﬁnding a lot of bugs in new code, your team needs to ﬁgure out why, and take action. Try to shorten the cycle of coding, integrating and testing so that programmers get immediate feed- back about code quality. Perhaps some buggy section of legacy code needs to be redesigned before it mires your team in technical debt. Maybe you need to work more closely with the business experts to understand the de- sired functionality.\n\n—Janet\n\nMore on retro- spectives in Chap- ter 19, “Wrap Up the Iteration.”\n\nFACILITATE COMMUNICATION\n\nAnother idea might be to create an ongoing “start, stop, continue” list so that you can remember some of the issues during the iteration retrospective.\n\nFACILITATE COMMUNICATION The daily stand-up helps teams maintain the close communication they need. Everyone on the team learns the current status of tasks and stories, and can help each other with obstacles. Often, hearing programmers describe tasks they’re working on provides a clue that they may have misunderstood the customer’s requirements. That signals the need for a group discussion af- ter the stand-up. If a tester needs help with a testing issue that’s come up, she might ask the team to stay after the stand-up to talk about it. Missed tasks are often identiﬁed during stand-ups, and new cards can be written on the spot.\n\nThe stand-up is a good time to look at progress. Use big, visible charts such as story boards, burndown charts, and other visual cues to help keep focus and know your status. If the end of the iteration is drawing near, and coding on a story seems “stuck,” raise a red ﬂag and ask the team what can be done about it. Perhaps some pairing or extra help will get things going. Lisa has of- ten noted when there’s a lot of testing left to do and time is running out. She asks for help to pick up the slack. The whole team focuses on what needs to be done to complete each story and talks about the best approach.\n\nWhen teams use an electronic medium for keeping track of stories, there is a tendency to forget the story board. Janet ﬁnds that having both may seem like a duplication of effort, but the visibility of progress to the team far out- weighs the extra overhead of writing up the task cards and moving them as they are completed. Having the story board gives your team focus during the stand-ups or when you are talking to someone outside the team about your progress.\n\nTesters Facilitate Communication\n\nTesters can help keep the iteration progressing smoothly by helping make sure everyone is communicating enough. Talk to programmers when they start working on a story, and make sure they understand it. Lisa ﬁnds that she can write all of the tests and examples she wants on the team wiki, but if nobody bothers to read them, they don’t help. When in doubt, she goes over requirements and tests with the programmer who picks up the task cards.\n\nProgrammers will always have questions as they develop a story, even if they understand the business and the story well. It’s best if a customer is available\n\n429\n\n430\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nto answer questions, because that is the most direct communication. Testers shouldn’t get in the way of that; however, we’ve observed that business ex- perts sometimes have trouble explaining a requirement, or a programmer simply gets the wrong idea and can’t get on the same page with the customer. The Power of Three applies here. Testers can help customers and program- mers ﬁnd a common language.\n\nA Little Friendly Competition\n\nGerard Meszaros, well-known agile coach and author of xUnit Test Patterns [2007], shared this story about a team he was working with and how a game solved a communication issue.\n\nWe were having trouble getting the developers to talk to the business people about their assumptions. When they did talk, the tester often got left out of the loop. The tester would sometimes discuss something with the business but never pass it on to the developer. Our project manager, Janice, decided to try to change the behavior through friendly competition.\n\nAll of the developers were given blue poker chips with a “D” written on them. All of the testers got a red chip with a “T” on them, and the busi- ness people got yellow chips with a “B” on them. Whenever someone met with a counterpart from another area, he or she could exchange one chip with each person. The goal was to get the most complete sets of chips: T-B-D. The winner got a custom-made T-B-D trophy decorated with the three kinds of chips. The end result was that everyone was much keener to meet with each other because they would get more chips!\n\nFind creative ways to get the business experts and programmers to talk and agree upon requirements. If a poker chip game gets them talking, embrace it.\n\nFacilitating communication usually involves drawing on a whiteboard, mock- ing up interfaces, listing other areas that might be affected, or working through real examples. Whenever communication appears to reach a dead end, or con- fusion is rampant, ask for a new example and focus on that.\n\nLisa’s Story\n\nWhen retirement plan participants want to withdraw money from their accounts, many complex vesting rules and government regulations come into play. It gets worse if the participant has withdrawn money in the past. Working on a story to calculate a participant’s vested balance, my team members all had different ideas on the correct algorithm, even though the product owner had worked through\n\nChapter 9, “Tool- kit for Business- Facing Tests that Support the Team,” talks about some tools that can help distrib- uted teams\n\nLisa’s Story\n\nFACILITATE COMMUNICATION\n\nseveral examples at the beginning of the iteration. My fellow tester, Mike, asked the product owner to work through a new example, and several programmers and testers joined the session. It took a couple of rather tortuous hours of writing numbers and ﬂowcharts on a whiteboard, but eventually they arrived at the cor- rect formula, and everyone was on the same page.\n\nWork through as many examples as you need until the team understands enough different aspects of the system. Try a different format if it’s not work- ing. For example, if pictures drawn on the whiteboard aren’t sufﬁcient to un- derstand the story, try spreadsheets or some other format that’s familiar to the business experts.\n\nDistributed Teams\n\nAs we’ve noted in other chapters, having team members in different locations and different time zones means you have to work harder at communication. Phones, email, and instant messaging form the basics of communication, but better collaboration tools are developed all the time.\n\nOne of the programmers on our team, who is also a manager, moved to India. Nanda works late into the evening there, so he’s available for the Denver team in the mornings. He has a cell phone with a local Denver phone number, so it’s easy to talk to him by phone as well as by instant message and email. We schedule meetings where we discuss stories, such as estimating meetings, brainstorming sessions, and iteration planning, early in the morning so he can participate. Al- though the team can’t be as productive as we were when we were co-located, we’re still able to beneﬁt from Nanda’s domain expertise and deep knowledge of the software.\n\nIf Nanda hires more team members in India, we may have to address more com- plex issues, such as coordinating integration and builds. We may consider more sophisticated technical solutions to communication problems.\n\nYou will need to experiment to see what works for your distributed team. Use retrospectives to evaluate whether collaboration and communication need improving, and brainstorm ways to improve. You, as a tester, may have a lot of experience in helping with process improvement projects. Just think about improving communication as one of those continual improvement needs.\n\n431\n\n—Lisa\n\n—Lisa\n\n432\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nA Remote Tester’s Story\n\nSometimes, the testers are the remote team members. Erika Boyer of iLevel by Weyerhaeuser lives on the East Coast and works with a team in Denver. She’s a tester by profession, but on her team all tasks are up for grabs. She might write ﬁxtures to automate a FitNesse test or pair with a programmer to write production code. Being able to get in touch with people when she needs them is an issue. If she doesn’t get a response when she instant-messages a coworker, she phones; every work area in the Denver ofﬁce has a phone. It’s not foolproof, because everyone could be in the break room at a going-away party and forgot to tell her. Teams in different locations have to make a spe- cial effort to keep each other informed.\n\nBecause Erika starts working a few hours before the team’s daily stand-up, she needs work she can do alone during that time. She works with any team members who come in early in Denver and converses with other program- mers late in the day about work she’ll do the next morning.\n\nErika is able to see the team’s tasks using a tool on their intranet that shows each task, its status, and its percentage complete. With a few extra accom- modations, the team (which has other remote members) is able to keep up good communication.\n\nEven from a distance, Erika has been able to transfer testing skills to the pro- grammers but has found they think differently than testers. Her team uses these varying perspectives to their advantage by rotating all types of tasks among all of the team members.\n\nSuccessful teams keep remote members “in the loop” and share skills and ex- pertise. Distributed teams face extra challenges in successfully completing testing activities, but some minor adjustments, thoughtfulness on the part of all team members, and good communication tools help ensure that remote testers can be productive.\n\nWe all need to be able to communicate well with each other for our projects to succeed. When teams are in diverse geographic locations, they might have to work twice as hard to stay in constant touch.\n\nREGRESSION TESTS Unless you’re on a team that’s just starting its automation efforts, you have automated regression tests covering stories from previous iterations. Hope- fully, these are running as part of a continual build process, or at least part of a daily build process. If they aren’t, ask your team to make implementing this critical infrastructure a priority, and brainstorm with them how this might be done. Plan time in the next iteration to start a build process.\n\nREGRESSION TESTS\n\nKeep the Build “Green”\n\nProgrammers should run all automated unit tests before checking in new code. However, unit tests may fail in the continual build, either because some- one forgot to run them before check-in, or because of a difference in runtime environment or IDE. We have unit tests for a reason, so whenever one fails, the team’s highest priority (apart from a showstopper production issue) should be to ﬁx it and get the build working again.\n\nTeams take different approaches to make sure their build stays “green.” Lisa’s team has a build process that emails results after every build. If the build fails, the person who checked in the failure usually ﬁxes it right away. If it’s not clear why the build failed, team members will get together to investigate. Their ScrumMaster has a stuffed toy that she puts on the desk of the person who “broke the build,” as a visual reminder that it has to be ﬁxed right away.\n\nSome teams use a trafﬁc light, ambient orb, GUI build monitoring tool, or other electronic visual way to show the build status. When the lights turn red, it’s time to stop new development and ﬁx the build. Another technique is to have a screen pop up in everyone’s IDE showing that the build has failed, and the popup won’t go away until you click “Ok, I’ll ﬁx the build.” Have some fun with it, but keeping the build running is serious business.\n\nIn extreme cases, you may have to temporarily comment out a failing test un- til it can be diagnosed, but this is a dangerous practice, especially for a novice team. Everyone on the team should stop what they’re doing if necessary until the build works again.\n\nKeep the Build Quick\n\nThe build needs to provide immediate feedback, so keep it short. If the build takes longer than the average frequency of code check-ins, builds start to stack up, and testers can’t get the code they need to test. The XP guideline for build time is ten minutes [Fowler, 2006]. Lisa’s team tries to keep the build less than eight minutes, because they check in so often.\n\nTests that take too long, such as tests that update the database, functional tests above the unit level, or GUI test scripts, should run in a separate build process. If the team is limited in hardware, they might have to run the “full” build with the full suite of tests at night and the “ongoing” build that has only unit tests continually during working hours. Having a separate, continual “full” build with all of the regression test suites is worth the investment. Lisa’s team gets feedback every 90 minutes from their “full” build, and this has\n\n433\n\n434\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nproven invaluable in heading off regression issues. This secondary suite of tests does not stop a programmer from checking in their code.\n\nBuilding a Regression Suite\n\nDuring the iteration, you’re automating new tests. As soon as these pass, add them to the regression suite, as appropriate. You may not need every edge case or permutation included in the regression suite, and you want to keep the regression suites fast enough to provide timely feedback. As each story is completed, tests that conﬁrm its functionality should be included in the re- gression suite and be part of the regular build cycle.\n\nThe regression tests themselves must be under some form of version control. It’s best to keep them in the same source code control system as the produc- tion code. That way, when you tag the code for production release, the tag also contains all of the versions of the tests that worked with the code. At minimum, keep a daily backup of the test code.\n\nWhen tests have been added to the regression suite, their purpose changes. They no longer exist to help drive development, and they are not expected to ﬁnd new bugs. There sole purpose in life is to detect unexpected changes or side effects in the system.\n\nChecking the “Big Picture”\n\nHopefully, you wrote task cards to test the story in the context of the larger ap- plication and regression test other parts of the system to ensure the new story hasn’t had a negative effect. You may have automated some of those end-to- end tests like the example in Chapter 12, “Summary of Testing Quadrants.”\n\nBut sometimes, even if you have a large suite of regression tests, manual ex- ploratory testing can be appropriate. The story isn’t “done” until you’ve com- pleted these tasks as well.\n\nRESOURCES As you start the iteration, make sure that test environments, test data, and test tools are in place to accommodate testing this iteration’s stories. Hope-",
      "page_number": 469
    },
    {
      "number": 57,
      "title": "Segment 57 (pages 477-484)",
      "start_page": 477,
      "end_page": 484,
      "detection_method": "topic_boundary",
      "content": "Chapter 15, “Tester Activities in Release or Theme Planning,” talks about useful metrics to keep.\n\nITERATION METRICS\n\nfully you’ve anticipated these needs, but some requirements might only be- come obvious when you start working on a story. Collaborate with database experts, system administrators, and other team members to set up any addi- tional infrastructure needed.\n\nYou may have brought in outside resources for this iteration to help with per- formance, usability, security, or other forms of testing. Include them in stand-ups and discussions with the customers as needed. Pair with them and help them understand the team’s objectives. This is an opportunity to pick up new skills.\n\nITERATION METRICS In Chapter 5, “Transitioning Typical Processes,” we talked a bit about the purpose of metrics, but because metrics are critical to understanding how your coding and testing activities are progressing, we’ll delve into them more here. Know what problem you are trying to solve before you start measuring data points and going to all the work of analyzing the results. In this section, we’ll cover some of the typical measurements that teams gather through the iteration.\n\nMeasuring Progress\n\nYou need some way to know how much work your team has completed at any point in the iteration and an idea of how much work is left to do. You need to know when it becomes obvious that some stories can’t be completed and the team needs a Plan B. Iteration burndown charts and estimated versus actual time for tasks are examples used to measure team progress. They may or may not provide value for your particular team.\n\nStory or task boards are a good visual way to know the iteration’s status, es- pecially if color coding is used. If too many test task cards are still in the “to do” column or not enough coding task cards have been moved to “Done” or “Tested,” it’s time for the team to think of ways to make sure all of the testing is completed. Maybe some team members need to stop coding and start tak- ing on testing tasks, or maybe one story or a less critical part of a story needs to be put off until the next iteration so that testing for all the other stories can be ﬁnished.\n\n435\n\n436\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nThis can be accomplished with virtual story boards as well as physical ones. Get creative with your visual effects so that problems are instantly visible. Re- member that no story is “done” until it’s tested at all appropriate levels. Teams may have other criteria for when a story is “done,” such as whether it has been peer reviewed or the automated regression tests are completed. On the story board shown in Figure 18-6, the “Done” column for each story row is the rightmost column. The column just to the left of it is the “Verify” col- umn. The story isn’t considered “done” until all the cards, including testing task cards, are in that “Done” column. A glance at the board is enough to know which stories are ﬁnished.\n\nEven teams that don’t track burndown at the task level can do so at the story level. Knowing how much work the team can do each iteration (its velocity) helps with the overall release plan, and the reprioritizing for each iteration. It simply may be enough to know the number of stories completed in an it- eration if they tend to average out to the same size. Although plans are ten- tative at best, it’s helpful to get an idea of about how many stories can be\n\nFigure 18-6 Story board showing iteration stories and tasks\n\nJanet’s Story\n\nITERATION METRICS\n\ncompleted by a hard release date or what stories might get done in the up- coming quarter.\n\nDefect Metrics\n\nWe talked about defect metrics in Chapter 15, “Tester Activities in Release or Theme Planning” giving you some high level ideas about what to track. Gathering metrics on defects can be very time consuming so always consider the goal before you start to measure. What is the purpose of the metrics you would like to gather? How long will you need to follow the trend before you know if they are useful?\n\nDefect containment is always a favorite metric to capture. When was the de- fect found? In traditional projects, it is much easier as you have “hard” re- quirements and coding phases. When the whole team is responsible for quality, and everyone is working together throughout, it is much harder to determine “when” the defect was injected into the system.\n\nWe would like to challenge the idea of this type of metric as not necessary in agile development. However, if you ﬁnd a lot of bugs are slipping through, you may want to start tracking what type of bugs they are so you can address the root cause. For example, if the bugs could have been caught with unit tests, then maybe the programmers need more training on writing unit tests. If the bugs are missed or misunderstood requirements, then maybe not enough time is spent in iteration planning, or acceptance tests aren’t detailed enough.\n\nIf you are practicing zero tolerance for defects, then you probably have no need to be tracking defects during coding and testing. A simple card on the story board will give you all the information you need.\n\nWhatever metrics you choose to measure, go for simplicity.\n\nIn one organization I was with, we tracked the number of defects logged in the DTS over several releases. These were defects that escaped the iteration or were found in the legacy system. Figure 18-7 shows the trend over a year and a half.\n\nAt the beginning, the number of issues found right after it was released to QA for ﬁnal testing was high (33 issues found in one month). The customers found even more issues during UAT which lasted over two months because they were not\n\n437\n\n438\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nFigure 18-7 Sample Defect Trend (but stopped after a while)\n\nconﬁdent in the quality of the release. In the month that zero defects were re- ported, we were just starting a new release so there was no new functionality to test. Over the next year, fewer and fewer defects were logged and it becomes im- possible to tell where an actual release happened by just looking at the trend.\n\nThis graph was used to show customers that the team was becoming consistent with their testing and their releases. Once the team and customers had faith the numbers were not going up, the metrics were no longer needed and were dropped.\n\nDon’t be afraid to stop using metrics when they are no longer useful. If the problem they were initially gathered for no longer exists, there is no reason to keep gathering them.\n\nYour team may have to provide metrics to upper managers or a Project Man- agement Ofﬁce (PMO), especially if you work for a large organization. Patrick Fleisch, an Accenture Consultant who was working as a functional analyst at a\n\n—Janet\n\nITERATION METRICS\n\nUseful Iteration Metrics\n\nConi Tartaglia, a software test manager at Primavera Systems, Inc., explains some ways she has found to achieve useful iteration metrics.\n\nCollecting metrics at the end of the iteration is particularly useful when many different teams are working on the same product releases. This helps ensure all teams end the iteration with the same standard for “done.” The teams should agree on what should be measured. What fol- lows are some standards for potentially shippable software [Schwaber 2004], and different ways of judging the state of each one.\n\nSprint deliverables are refactored and coded to standards.\n\nUse a static analysis tool. Focus on data that is useful and actionable. De- cide each sprint if corrective action is needed. For example, use an open source tool like FindBugs, and look for an increase each sprint in the number of priority one issues. Correct these accordingly.\n\nSprint deliverables are unit tested.\n\nFor example, look at the code coverage results each sprint. Count the number of packages with unit test coverage falling into ranges of 0%–30% (low coverage), 31%–55% (average coverage), and 56%–100% (high) coverage. Legacy packages may fall into the low coverage range, while coverage for new packages should fall into the 56%–100% range, if you are practicing test driven development. An increase in the high coverage range is desirable.\n\nSprint deliverables have passing, automated acceptance tests.\n\nMap automated acceptance tests to requirements in a quality manage- ment system. At the end of the iteration, generate a coverage report showing that all requirements selected as goals for the iteration have passing tests. Requirements that do not show passing test coverage are not complete. The same approach is easily executed using story cards on a bulletin board. The intent is simply to show that the agreed-upon tests for each requirement or story are passing at the end of the sprint.\n\nSprint deliverables are successfully integrated.\n\nCheck the continuous integration build test results to ensure they are passing. Run other integration tests during the sprint. Make corrections prior to the beginning of the next iteration. Hesitate to start a new itera- tion if integration tests are failing.\n\nSprint deliverables are free of defects.\n\nRequirements completed during the iteration should be free of defects.\n\nCan the product ship in [30] days?\n\n439\n\n440\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nSimply ask yourself this question at the end of each iteration, and pro- ceed into the next iteration according to the answer.\n\nMetrics like this are easy to collect and easy to analyze, and can provide valu- able opportunities to help teams correct their course. They can also conﬁrm the engineering standards the teams have put in place to create potentially shippable software in each iteration.\n\nsoftware company during the time we wrote this book, gave us the following examples of metrics his team provides to their PMO.\n\n(cid:2) Test execution numbers by story and functional area (cid:2) Test automation status (number of tests automated vs. manual) (cid:2) Line graph of the number of tests passing/failing over time (cid:2) Summary and status of each story (cid:2) Defect metrics\n\nGathering and reporting metrics such as these may result in signiﬁcant over- head. Look for the simplest ways to satisfy the needs of your organization.\n\nSUMMARY At this point in our example iteration, our agile tester works closely with pro- grammers, customers, and other team members to produce stories in small testing-coding-reviewing-testing increments. Some points to keep in mind are:\n\n(cid:2) Coding and testing are part of one process during the iteration. (cid:2) Write detailed tests for a story as soon as coding begins. (cid:2) Drive development by starting with a simple test; when the simple tests pass, write more complex test cases to further guide coding. (cid:2) Use simple risk assessment techniques to help focus testing efforts. (cid:2) Use the “Power of Three” when requirements aren’t clear or opinions\n\nvary.\n\n(cid:2) Focus on completing one story at a time. (cid:2) Collaborate closely with programmers so that testing and coding are\n\nintegrated.\n\n(cid:2) Tests that critique the product are part of development. (cid:2) Keep customers in the loop throughout the iteration; let them review\n\nearly and often.\n\n(cid:2) Everyone on the team can work on testing tasks.\n\nSUMMARY\n\n(cid:2) Testers can facilitate communication between the customer team and\n\ndevelopment team.\n\n(cid:2) Determine what the best “bug ﬁxing” choice for your team is, but a\n\ngood goal is to aim to have no bugs by release time.\n\n(cid:2) Add new automated tests to the regression suite and schedule it to run\n\noften enough to provide adequate feedback.\n\n(cid:2) Manual exploratory testing helps ﬁnd missing requirements after all\n\nthe application has been coded.\n\n(cid:2) Collaborate with other experts to get the resources and infrastructure\n\nneeded to complete testing.\n\n(cid:2) Consider what metrics you need during the iteration; progress and\n\ndefect metrics are two examples.\n\n441\n\nThis page intentionally left blank",
      "page_number": 477
    },
    {
      "number": 58,
      "title": "Segment 58 (pages 485-492)",
      "start_page": 485,
      "end_page": 492,
      "detection_method": "topic_boundary",
      "content": "Chapter 19\n\nWRAP UP THE ITERATION\n\nIteration Demo\n\nWrap Up the Iteration\n\nStart, Stop, Continue\n\nCelebrate Successes\n\nRetrospectives\n\nIdeas for Improvement\n\nWe’ve completed an iteration. What do testers do as the team wraps up this iter- ation and prepares for the next? We like to focus on how we and the rest of our team can improve and deliver a better product next time.\n\nITERATION DEMO One of the pleasures of agile development is the chance to show completed stories to customers at the end of each iteration. Customers get to see a real, live, working application. They get to ask questions and give feedback. Every- one involved in the project, from both the business and technical sides, gets to enjoy a sense of accomplishment.\n\nOn Lisa’s team, the testers conduct the iteration review. Among all the team members, they’ve usually worked on the most stories. They have a natural role as information providers, and they have a good idea what the customers need to know about the new functionality. Having testers show off the deliv- erables is a common practice, although there is no hard and fast rule. The business experts on the team are a good choice for conducting the demo too, because they have the best understanding of how the software meets the business needs and they’ll feel greater ownership of the product. The Scrum- Master, a programmer, or a business analyst could demonstrate the new fea- tures and often does. Janet encourages rotating this honor.\n\n443\n\n444\n\nCHAPTER 19\n\n(cid:2) WRAP UP THE ITERATION\n\nListening to the Customers\n\nPierre Veragen explains how his team uses iteration demonstrations.\n\n“We shut up and listen to our customers. It’s all about the chemistry of the group’s presentation. Somehow, sharing the moment brings brains together—we look at things from a different perspective. The event gives birth to ideas and concepts. Some die as the next person speaks; some live on and become that great idea that differentiates the product.”\n\nThe demo is a chance to show off the new stories, but the feedback custom- ers provide is the biggest reason to do them.\n\nAnyone may note the comments made by customers as they participate in the demo, but testers are good candidates. They may notice previously unde- tected inconsistencies as the demo progresses. As questions come up, cus- tomers might decide they want to change something minor, such as help text, or something bigger, such as how a feature behaves. Minor changes can usu- ally be made into tasks and dealt with in the next iteration, but some changes are big enough to turn into stories to plan into future releases.\n\nIteration demos (called sprint reviews in the Scrum world) are a super op- portunity to get everyone talking and thinking about the application. Take advantage of it. Review meetings are usually short and can be under half an hour. If there’s time left over after demonstrating new stories, ask customers if they’ve experienced any problems with the previous release that they haven’t reported. Do they have any general concerns, do they need help understand- ing how to use a feature, or have any new issues arisen? Of course, you can talk to customers anytime, but having most of the stakeholders in the room with the development team can lead to interesting ideas.\n\nRETROSPECTIVES Agile development means continually improving the way you work, and ret- rospectives are an excellent place to start identifying what and how you can do better. We recommend taking time at the end of each iteration and release cycle to look back and talk about what went well, what didn’t, and what you might like to try in the next iteration. There are different approaches for con- ducting retrospective sessions. No matter what approach you use, it’s key that each team member feels safe, everyone is respected, and there’s no ﬁnger- pointing or blame.\n\nAgile Retrospec- tives: Making Good Teams Great [2006] has imagi- native ideas for making retrospec- tives more pro- ductive (see the bibliography).\n\nRETROSPECTIVES\n\nThe whole idea is to make the process better, one baby step at a time.\n\nStart, Stop, Continue\n\nOne common exercise used in iteration retrospectives is “start, stop, continue.” The team asks itself: “What went well during this past iteration? What hap- pened that shouldn’t happen again? What can we start doing to help with things that didn’t go well?” Each team member can suggest things to start do- ing to improve, things to stop doing that weren’t working, and things that are helping that should be continued. A facilitator or ScrumMaster lists them on a whiteboard or big piece of paper. Post them in a location where everyone can read them again during the iteration. Figure 19-1 shows a “stop, start, and con- tinue” retrospective in progress. The ScrumMaster (standing) is writing stop, start, and continue suggestions on the big piece of paper on the story board.\n\nSome teams start this process ahead of time. All team members write “start,” “stop,” and “continue” items on sticky notes, and then during the retrospec- tive meeting they put the stickies on the board and group them by topic. “Start, stop, continue” is just one example of the terms you might use. Some other ideas are: “Things that went well,” “Things to improve,” “Enjoyable,” “Frustrating,” and “To Try.” Use whatever names that work for you. It can be\n\nFigure 19-1 A retrospective in progress\n\n445\n\n446\n\nCHAPTER 19\n\n(cid:2) WRAP UP THE ITERATION\n\nhard to remember the past two weeks, much less an entire release, if that’s what your retrospective covers. Research different creative approaches to re- ﬂecting on your team’s experiences.\n\nHere’s a sample “stop, start, continue” list from Lisa’s team:\n\nStart: (cid:2) Sending out next sprint’s stories to us earlier. (cid:2) Don’t do lazy, single-record processing. Think of every service call as\n\na remote call.\n\n(cid:2) Communicate any database changes to everyone.\n\nStop: (cid:2) Accepting stories without complete requirements.\n\nContinue: (cid:2) Running FitNesse tests for the code you’re working on. (cid:2) Documenting what came up in meeting or informal discussions. (cid:2) Communicating better with each other. (cid:2) Showing mock-ups early. (cid:2) Doing FitNesse driven development.\n\nIf the list of “start, stop, continue” items is long, it’s a good idea to choose one or two to focus on for the new iteration. To prioritize the items, give each team member “n” votes they can assign to items. The ten people on Lisa’s team each get three votes, and they can apply them all to one item if they feel that’s most important, or they can vote for two or three different items. The items with the most votes are noted as the focus items. Janet has had success with this way of prioritizing as well.\n\nIn addition to “start, stop, continue” items, the team may simply write task cards for actions to be undertaken the next iteration. For example, if the ongo- ing build is too slow, write a card to “get ongoing build under ten minutes.”\n\nIn the next iteration, take some time to look at the one or two focus items you wanted to improve. At the end of that iteration, take a checkpoint to see if you improved. If not, ask why. Should you try something different? Is it still impor- tant? It could be it has dropped in importance or really wasn’t important in the big picture. If you thought you improved on a problem area and it resurfaces, you’ll have to decide to do something about it or else quit talking about it.\n\nRETROSPECTIVES\n\nWe’ve found that retrospectives are a simple and highly effective way for teams to identify and address issues. The retrospective meeting is a perfect opportunity to raise testing-related issues. Bring up the issues in an objective, non-blaming way. The team can discuss each problem, what might be caus- ing it, and write down some ideas to ﬁx it.\n\nIdeas for Improvements\n\nLet’s take a look at some of those items that made it onto the list for improve- ment. Too many times, a team will identify really big issues but never follow up and actually do something about them. For example, maybe a lot of unit- level bugs are discovered after the programmers have claimed coding was complete.\n\nThe team may decide the programmers aren’t covering enough code with unit tests. They might write an action item to run the code coverage tool be- fore they check in new code, or start writing a “unit tests” task card for each story to make sure they’re completed. Perhaps the team didn’t ﬁnish all the test automation tasks before the iteration ended. As they discuss the prob- lem, the team ﬁnds that the initial executable tests were too complex, and they need to focus on writing and automating a simple test ﬁrst, or pair for a better test design. Make sure the action items are concrete.\n\nAgile teams try to solve their own problems and set guidelines to help them- selves improve. Action items aimed at one problem may help with others. When Lisa’s team had trouble ﬁnishing stories and getting them tested dur- ing each iteration, it came up with various rules over the course of a few retrospectives:\n\n(cid:2) Finish high-level test cases for all stories by the fourth day of the\n\niteration.\n\n(cid:2) Deliver one story to test by the fourth day of the iteration. (cid:2) Focus on ﬁnishing one story at a time. (cid:2) 100% of features must be checked in by close of business on the next-\n\nto-last day of the iteration.\n\nThese rules did more than help the team ﬁnish testing tasks. They facilitated a ﬂow and rhythm that helped the team work at a steady, sustainable pace over the course of each iteration.\n\nBegin the next retrospective meeting by reviewing the action items to see what items were beneﬁcial. Lisa’s team puts happy, sad, or neutral faces next to\n\n447\n\n448\n\nCHAPTER 19\n\n(cid:2) WRAP UP THE ITERATION\n\nitems to denote whether the team tried them and found them successful. The team should ﬁgure out the reasons behind any sad faces. Were some items simply forgotten? Did time constraints keep the team from trying a new activ- ity? Did it just seem to be less of a good idea later? These discussions might lead to changing the improvement item or evolving it into a new one.\n\nWhen the actions for improvement become a habit to the team, they no longer need to be written on the “stop, start, and continue” list. “Start” items that work well may be moved to the “Continue” column. Some ideas don’t work, or prove to be unnecessary, and those can also be taken off the list for the next iteration.\n\nRefer to your ideas for improvement and action items during the iteration. Post them in a location (on a wall or online) where everyone sees them often. Lisa’s team sometimes goes through the list during a mid-iteration stand-up meeting. If you think of new improvement ideas during the iteration, write them down, possibly even on the existing list, so you won’t forget for the next iteration.\n\nIt’s a good idea to keep track of things that get in your way throughout the it- eration. Keep an impediment backlog on some big visible chart. Talk about the impediments in each iteration, and write task cards or take action to eliminate them.\n\nAn Approach to Process Improvement\n\nRafael Santos, VP of Software Development and Chief ScrumMaster at Ultimate Software, and Jason Holzer, the Chief PSR (Performance, Security, Reliability) Architect, explained to us that their teams found retrospectives that used the “stop, start, and continue” model ineffective. They made “stop, start, and continue” lists, but those didn’t provide enough focus to address issues.\n\nInstead, the ScrumMaster kept an impediment backlog, and the team found that worked better than retrospectives. Impediments may be related to test- ing or tools.\n\nSee the bibliogra- phy for good re- sources for lean development practices.\n\nThey also do value stream mapping to ﬁnd the biggest “wait time,” and use the “ﬁve whys” from Toyota to understand which impediment is the biggest or which constraint needs to be addressed.\n\nOne example shared was that in a team with three programmers and one tester, the biggest problem was a testing bottleneck. Rafael asked the team what the tester does and wrote those items on a whiteboard. Then he asked the programmers which of those things on the board they couldn’t do. There was only one item they felt they couldn’t handle. This helped the programmers\n\nCELEBRATE SUCCESSES\n\nunderstand how everyone on the development team, not only the testers, could be responsible for testing tasks. This was a highly effective exercise.\n\nCreative approaches like this help new agile teams tackle difﬁcult testing chal- lenges. Retrospectives are a good environment for experimenting.\n\nUse retrospectives as an opportunity to raise testing-related issues and get the whole team thinking about possible solutions. We’ve been pleasantly sur- prised with the innovative ideas that come out of an entire team focusing on how to improve the way it works.\n\nCELEBRATE SUCCESSES Agile development practices tend to moderate the highs and lows that exist in more traditional or chaotic processes. If your waterfall team ﬁnally man- ages to push a release out the door after a year-long cycle ending in a two- month stressful ﬁx-and-test cycle, everyone may be ready to celebrate the event with a big party—or they might just collapse for a couple of weeks. Ag- ile teams that release every two weeks tend to stay in their normal coding and testing groove, starting on the next set of stories after drawing just enough breath to hold an iteration review and retrospective. This is nice, but you know what they say about all work and no play.\n\nMake sure your team takes at least a little time to pat itself on the back and recognize its achievements. Even small successes deserve a reward. Enjoy- ment is a vital agile value, and a little motivation helps your team continue on its successful path. For some reason, this can be hard to do. Many agile teams have trouble taking time to celebrate success. Sometimes you’re eager to get going with the next iteration and don’t take time to congratulate your- selves on the previous accomplishments.\n\nLisa’s team ends an iteration every other Thursday and conducts its retro- spective, iteration review, and release the following day. After their meetings conclude, they usually engage in something they call “Friday Fun.” This sometimes consists of playing a silly trivia or board game, going out for a drink, or playing a round of miniature golf. Getting a chance to relax and have a good laugh has a team-building side beneﬁt.\n\nFor bigger milestones, such as a big release or achieving a test coverage goal, the whole company has a party to celebrate, bringing in catered food or going out\n\n449\n\n450\n\nCHAPTER 19\n\n(cid:2) WRAP UP THE ITERATION\n\nto a restaurant on Friday afternoon. This is a nice reward and recognizes for ev- eryone on both the business and technical teams.\n\nIf yours is a new agile team, motivate yourselves by rewarding small accom- plishments. Cheer the rising number of unit tests passing in each build. Oooh and aaah over the chart showing actual burn down matching the pro- jected burn down. Ring a bell when the broken unit tests in the build are ﬁxed (okay, that one might be annoying, but recognize it in some way.)\n\nCelebrate your individual successes, too. Congratulate your coworker for completing the project’s ﬁrst performance test baseline. Give your DBA a gold star for implementing a production back-up system. Give yourself a treat for solving that hard test-automation problem. Bring cookies to your next meeting with the customers. Recognize the programmer who gave you a JavaScript harness that sped up testing of some GUI validations. Use your imagination.\n\nThe Shout-Out Shoebox\n\nWe love the celebration idea we got from Megan Sumrell, an agile trainer and coach. She shared this with an agile testing Open Space session at Agile 2007.\n\nCelebrating accomplishments is something I am pretty passionate about on teams. On a recent project, we implemented the Shout-Out Shoe- box. I took an old shoebox and decorated it. Then, I just cut a slit in the top of the lid so people could put their shout-outs in the box. The box is open to the entire team during the course of the sprint.\n\nAnytime team members want to give a “shout-out” to another team member, they can write it on a card and put it in the box. They can range from someone helping you with a difﬁcult task to someone going above and beyond the call of duty. If you have distributed team members, en- courage them to email their shout-outs to your ScrumMaster who can then put them in the box as well.\n\nAt the end of our demo, someone from the team gets up and reads all of the cards out of the box. This is even better if you have other stakehold- ers at your demo. That way, folks on your team are getting public recog- nition for their work in front of a larger audience. You can also include small give-aways for folks, too.\n\nIt may be a cliché, but little things can mean a lot. The Shout-Out Shoebox is a great way to recognize the value different team members contribute.",
      "page_number": 485
    },
    {
      "number": 59,
      "title": "Segment 59 (pages 493-502)",
      "start_page": 493,
      "end_page": 502,
      "detection_method": "topic_boundary",
      "content": "SUMMARY\n\nTaking time to celebrate successes lets your team take a step back, get a fresh perspective, and renew its energy so it can keep improving your product. Give team members a chance to appreciate each other’s contributions. Don’t fall into a routine where everyone has their head down working all the time.\n\nIn agile development, we get a chance to stop and get a new perspective at the end of each short iteration. We can make minor course corrections, decide to try out a new test tool, think of better ways to elicit examples from custom- ers, or identify the need for a particular type of testing expertise.\n\nSUMMARY\n\n(cid:2) In this chapter, we looked at some activities to wrap up the iteration\n\nor release.\n\n(cid:2) The iteration review is an excellent opportunity to get feedback and\n\ninput from the customer team.\n\n(cid:2) Retrospectives are a critical practice to help your team improve. (cid:2) Look at all areas where the team can improve, but focus on one or two\n\nat a time.\n\n(cid:2) Find a way to keep improvement items in mind during the iteration. (cid:2) Celebrate both big and small successes, and recognize the contribu-\n\ntions from different roles and activities.\n\n(cid:2) Take advantage of the opportunity after each iteration to identify testing-related obstacles, and think of ways to overcome them.\n\n451\n\nThis page intentionally left blank\n\nChapter 20\n\nSUCCESSFUL DELIVERY\n\nWhat Makes a Product?\n\nProduction Support\n\nCustomer Expectations\n\nPlanning Enough Time for Testing\n\nRelease Acceptance Criteria\n\nRelease Management\n\nReleasing the Product\n\nTesting the Release Candidate\n\nPackaging\n\nTesting on a Staging Environment\n\nFinal Nonfunctioning Testing\n\nSuccessful Delivery\n\nEnd Game\n\nIntegration with External Applications\n\nData Conversion, Database Updates\n\nInstallation Testing\n\nDeliverables\n\nCommunication\n\nPost-Development Testing Cycles\n\nWhat If It’s Not Ready?\n\nUAT\n\nCustomer Testing\n\nAlpha/Beta Testing\n\nIn this chapter, we share what you as a tester can do to help your team and your organization successfully deliver a high-quality product. The same process and tools can be used for shrink-wrapped products, customized solutions, or internal development products. Agile testers can make unique contributions that help both the customer and developer team deﬁne and produce the value that the business needs.\n\nWHAT MAKES A PRODUCT? Many of the books on agile development talk about the actual development cycle but neglect to talk about what makes a product and what it takes to suc- cessfully deliver that product. It’s not enough to just code, test, and say it’s\n\n453\n\n454\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\ndone. It’s like buying something from a store: If there is great service to go with the purchase, how much more likely are you to go back and buy there again?\n\nJanet’s Story\n\nI was talking to my friend, Ron, who buys and sells coins. Over the years he has de- veloped a very good reputation in the industry and has turned away prospective clients because he is so busy.\n\nWhen I asked him his secret, he said, “It’s not a secret. I just work with my custom- ers to make them feel comfortable and establish a trusting relationship with them. In the end, both I and my customer need to be happy with the deal. It only takes one unhappy customer to break my reputation.”\n\nAgile teams can learn from Ron’s experience. If we treat our customers with re- spect and deliver a product they are happy with, we will have a good relationship with them, hopefully for many years.\n\nOur goal is to deliver value to the business in a timely manner. We don’t want just to meet requirements but also to delight our customers. Before we re- lease, we want to make sure all of the deliverables are ready and polished up appropriately. Hopefully, you started planning early to meet not only the code requirements but to plan for training, documentation, and everything that goes into making a high-value product.\n\nFit and Finish\n\nConi Tartaglia, a software test manager with Primavera Systems, Inc., explains “ﬁt and ﬁnish” deliverables.\n\nIt is helpful to have a “Fit and Finish” checklist. Sometimes ﬁt and ﬁnish items aren’t ready to be included in the product until close to the end. It may be necessary to rebuild parts of the product to include items such as new artwork, license or legal agreements, digital signatures for execut- ables, copyright dates, trademarks, and logos.\n\nIt is helpful to assemble these during the last full development iteration and incorporate them into the product while continuous integration build cycles are running so that extra builds are not needed later.\n\nBusiness value is the goal of agile development. This can include lots beyond the production code. Teams need to plan for all aspects of product delivery.\n\n—Janet\n\nPLANNING ENOUGH TIME FOR TESTING\n\nImagine yourself in the middle of getting your release ready for production. You’ve just ﬁnished your last iteration and are wrapping up your last story test. Your automated regression suite has been running on every new build, or at least on every nightly build. What you do now will depend on how dis- ciplined your process has been. If you’ve been keeping to the “zero tolerance” for bugs, you’re probably in pretty good shape.\n\nIf you’re one of those teams that thinks you can leave bugs until the end to ﬁx, you’re probably not in such good shape and may need to introduce an it- eration for “hardening” or bug ﬁxes. We don’t recommend this, but if your team has a lot of outstanding bugs that have been introduced during the de- velopment cycle, you need to get those addressed before you go into the end game. We ﬁnd that new teams tend to fall into this trap.\n\nIn addition, there are lots of varied components to any release, some in the software, some not. You have customers who need to install and learn to use the new features. Think about all those elements that are critical to a success- ful release, because it’s time to wrap up all those loose ends and hone your product.\n\nBob Galen, an agile coach and end-game expert, observes that agile develop- ment may not have seeped into every organizational nook and cranny. He notes, “Agile testers can serve as a conduit or facilitator when it comes to physical delivery of the software.”\n\nPLANNING ENOUGH TIME FOR TESTING Because testing and coding are part of one process in agile development, we’d prefer not to make special plans for extra testing time, but in real life we might need some extra time.\n\nMost teams accumulate some technical debt, despite the best intentions, espe- cially if they’re working with legacy code. To maintain velocity, your team may need to plan a refactoring iteration at regular intervals to add tests, upgrade tools, and reduce technical debt. Lisa’s team conducts a refactoring sprint about every six months. While the business doesn’t usually receive any direct beneﬁts at the end of a refactoring sprint, the business experts understand that these special sprints result in better test coverage, a solid base for future development, reduced technical debt, and a higher overall team velocity.\n\n455\n\n456\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nSome teams resort to “hardening” iterations, where they spend time only ﬁnd- ing and ﬁxing bugs, and they don’t introduce any new functionality. This is a last resort for keeping the application and its infrastructure solid. New teams may need an extra iteration to complete testing tasks, and if so, they budget time for that in the release plan.\n\nUse retrospectives and other process improvement practices to learn ways to integrate testing and coding so that the code produced in each iteration is production-ready. When that goal is achieved, work to ensure that a stable build that could be released to production is available every day. Lisa’s team members thought that this was an unattainable goal in the days when they struggled to get any stable build before release, but it was only a couple of years before almost every build was release-worthy.\n\nWhen your build is stable, you are ready to enter the “End Game.”\n\nTHE END GAME What is the end game? We’ve heard people call the time right before delivery many things, but the “end game” seems to ﬁt best. It’s the time when the team applies the ﬁnishing touches to the product. You’re dotting your i’s and crossing your t’s. It’s the last stretch before the delivery ﬁnish line. It’s not meant to be a bug-ﬁx cycle, because you shouldn’t have any outstanding bugs by then, but that doesn’t mean you might not have one or two to ﬁx.\n\nYou might have groups in your organization that you didn’t involve in your earlier planning. Now it’s time to work closely with the folks that administer the staging and production environments, the conﬁguration managers, the database administrators outside of your team, and everyone who plays a role in moving the software from development to staging and production. If you weren’t working with them early this time, consider talking to these folks during your next release planning sessions, and keep in touch with them throughout the development cycle.\n\nBob Galen tells us that the testers on his team have partnered with the opera- tions group that manages the staging and production environments. Because the operations group is remote, it ﬁnds that having guidance from the agile team is particularly valuable.\n\nThere are always system-level tests that can’t be automated, or are not worth automating. More often than not, your staging environment is the only place\n\nLisa’s Story\n\nTHE END GAME\n\nwhere you can do some system-level integration tests or system-level load and stress testing. We suggest that you allot some time after development for these types of ﬁnishing tasks. Don’t code right up to the end.\n\nPlan as much time for the end game as you need. Janet has found that the length of time needed for the end game varies with the maturity of the team and the size of the application. It may be that only one day is needed to ﬁnish the extra tasks, but it may be one week or sometimes as much as a whole two-week iteration. The team from the example used in Chapter 12, “Sum- mary of Testing Quadrants,” scheduled two weeks, because it was a complex system that required a fair bit of setup and system testing.\n\nWhen I worked on a team developing applications for a client, we had to follow the client’s release schedule. Testing with other parts of the larger system was only possible during certain two-week windows, every six or eight weeks. Our team completed two or three iterations, ﬁnishing all of the stories for each as if they were releasing each iteration.\n\nThen we entered a testing window where we could coordinate system testing with other development teams, assist the client with UAT, and plan the actual release. This constituted our end game.\n\nIf you have a large organization, you might have ten or ﬁfteen teams develop- ing software for individual products or for separate areas of functionality for the same application. These areas or products may all need to release to- gether, so an integrated end game is necessary. This does not mean that you leave the integration until the very end. Coordination with the other teams will be critical all along your development cycle, and if you have a test inte- gration system, we recommend that you be sure that you have tried to inte- grate long before the end game.\n\nYou also may have considerations beyond your team, for example, working with software delivered by external teams at the enterprise level.\n\nUse this end-game time to do some ﬁnal exploratory testing. Step back and look at the whole system and do some end-to-end scenarios. Such testing will conﬁrm that the application is working correctly, give you added conﬁdence in the product, and provide information for the next iteration or release.\n\n457\n\n—Lisa\n\n458\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nTesting the Release Candidate\n\nWe recommend that the automated regression testing be done against every release candidate. If you’re following our recommendation to run automated regression tests continually on each new build, or at least daily, you’ve already done this. If some of your regression tests are manual, you’ll need to plan time for those or they might not get done. A risk assessment based on changes made to each build will determine what tests need to be run if there is more than one release candidate.\n\nTest on a Staging Environment\n\nWhether you are using traditional or agile development processes, a staging en- vironment that mimics production is vital for ﬁnal testing before release, as well as for testing the release process itself. As part of the end game, your applica- tion should be deployed to staging just like you would deploy it to production, or as your customers would on their environments. In many organizations that Janet has seen, the staging environment is usually shared among multiple projects, and the deployment must be scheduled as part of the release plan- ning. Consider ahead of time how to handle dependencies, integrating with other teams using the staging environment, and working with external third parties. It might feel like “traditional” test planning, but you might be dealing with teams that haven’t embraced agile development.\n\nAlthough agile promotes continuous integration, it is often difﬁcult to inte- grate with third-party products or other applications outside your project’s control. Staging environments can have better controls so that external appli- cations may connect and have access tothird-party test environments. Stag- ing environments can also be used for load and performance testing, mock deploys, fail-over testing, and manual regression tests and exploratory func- tional testing. There are always conﬁguration differences between environ- ments so your staging environment is a good place to test these.\n\nFinal Nonfunctional Testing\n\nLoad testing should be scheduled throughout the project on speciﬁc pieces of the application that you are developing. If your staging environment is in high demand, you may not be able to do full system load testing until the end game.\n\nBy this time, you should be able to do long-running reliability tests on all product functionality. Check for crashes and degradation of performance with normal load. When done at release time, it should be a ﬁnal conﬁrma- tion only.\n\nJanet’s Story\n\nTHE END GAME\n\nFault tolerance and recovery testing is best done on your staging environment as well, because test environments usually don’t have the necessary setup. For these same reasons, you may only be able to test certain aspects of security. One example is https, a secure http connection through encrypted secure sockets. Some organizations may choose to have the necessary certiﬁcates on their staging environment only. Other examples are clustering or data replica- tion. Make sure you involve all parties who need to be included in this testing.\n\nIntegration with External Applications\n\nYour team may be agile, but other product teams in your organization, or third parties your team works with, may not be.\n\nIn one organization that I worked with, the third-party partner that approved credit cards had a test account that could be used, but it was only accessible from the staging environment.\n\nTo test during development, test stubs were created to return speciﬁed results depending on the credit card number used. However, this wasn’t sufﬁcient be- cause the third party sometimes changed functionality on its end that we weren’t aware of. Testing with the actual third party was critical to the success of the project, and it is a key part of the end game.\n\nCoordinate well in advance with other product teams or outside partners that have products that need to integrate with your product. If you have identiﬁed these risks early and done as much up-front testing as possible, the testing done during the end game should be ﬁnal veriﬁcation only. However, there are always last-minute surprises, so you may need to be prepared to make changes to your application.\n\nTools like simulators and mock objects used for testing during development can help alleviate some of the risks, but the sooner you can test with external applications, the lower the risk.\n\nData Conversion and Database Updates\n\nAs we are developing an application, we change ﬁelds, add columns in the da- tabase, or remove obsolete ones. Different teams tackle this in different ways. Some teams re-create the database with each new build. This works for new applications, because there is no existing data. However, after an application exists in production and has associated data, this approach won’t work.\n\n459\n\n—Janet\n\n460\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nAn application needs to consider the data that is part of the product. As with so much in agile development, a joint effort by database experts, program- mers, and testers on the team is required to ensure successful release of data- base changes. Janet has seen a couple of different tactics for dealing with data conversion and backward compatibility. Database scripts can be created by the developers or database administrators as the team makes changes. These scripts become part of the build and are continually tested. Another option is for the team to run “diffs” on the database after all of the database changes have been made.\n\nIf you’re a tester, ask your database administrator/developer to help your team ensure that schemas are kept consistent among the production, test- ing, and staging environments. Find a way to guarantee that all changes made in the test environments will be done in the staging and production environments during release. Keep the schemas matching (except for the new changes still under development) in terms of column names, triggers, constraints, indices, and other components. The same discipline applied to coding and testing also should be applied to database development and maintenance.\n\nLisa’s Story\n\nWe recently had a bug released to production because some of the test schemas, including the one used by regression tests, were missing a constraint. Without the constraint in place, the code didn’t fail. This triggered an effort to make sure the exact same update scripts get run against each schema to make changes for a given release.\n\nIt turned out that different test schemas had small differences, such as old col- umns still remaining in some or columns in different order in different schemas, so it wasn’t possible to run the same script in every environment. Our database ad- ministrator led a major effort to re-create all of the test schemas to be perfectly compatible with production. He creates one script in each iteration with all nec- essary database changes and runs that same script in the staging and production environment when we release. This seems simple, but it’s easy to miss subtle dif- ferences when you’re focused on delivering new features.\n\nAutomating data migrations enhances your ability to test them and reduces the chance for human error. Native database tools such as SQL, stored proce- dures, data import tools such as SQL*Loader and bcp, shell scripts, and Win- dows command ﬁles can be used for automation because they can be cloned and altered easily.\n\n—Lisa",
      "page_number": 493
    },
    {
      "number": 60,
      "title": "Segment 60 (pages 503-511)",
      "start_page": 503,
      "end_page": 511,
      "detection_method": "topic_boundary",
      "content": "Lisa’s Story\n\nTHE END GAME\n\nNo matter how the database update and conversion scripts are created or maintained, they need to be tested. One of the best ways to ensure all of the changes have been captured in the update scripts is to use the customer’s data if it is available. Customers have a habit of using the application in weird and wonderful ways, and the data is not always as clean as we would like it. If the development team cleans up the database and puts extra restrictions on a column, the application on the customer’s site might blow up as soon as a query touches a piece of data that does not match the new restrictions. You need to make sure that any changes you’ve made are still compatible with ex- isting data.\n\nMy team uses the staging environment to test the database update scripts. After the scripts are run, we do manual testing to verify that all changes and data con- versions completed correctly. Some of our GUI test scripts cover a subset of re- gression scenarios. This gives us conﬁdence about releasing to production, where our ability to test is more limited.\n\nWhen planning a data conversion, think about data cleanup as part of the mitigation strategy. You have the opportunity to take the data that was en- tered in some of the “weird and wonderful” ways we mentioned before and massage or manipulate it so it conforms to the new constraints. This type of job can take a long time to do but is often very worthwhile in terms of main- taining data integrity.\n\nNot everyone can do a good enough simulation of production data in the staging environment. If a customer’s data is not available, a mitigation strat- egy is to have a UAT at the customer site. Another way to mitigate risk is to try to avoid large-scale updates and release in smaller stages. Develop new functionality in parallel with the old functionality and use a system property to “turn on” one or the other. The old functionality can continue to work in production until the new functionality is complete. Meanwhile, testing can be done on the new code at each iteration. New columns and tables can be added to production tables without affecting the old code so that the data migration or conversion for the ﬁnal release is minimized.\n\nInstallation Testing\n\nOrganizations often have a separate team that deploys to production or cre- ates the product set. These team members should have the opportunity to\n\n461\n\n—Lisa\n\n462\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\npractice the deployment exactly as they would for production. If they use the deployment to staging as their proving ground, they can work out any of the problems long before they release to the customer.\n\nTesting product installations can also mean testing various installations of shrink-wrapped products to different operating systems or hardware. How does the product behave? Does it do what is expected? How long will the sys- tem need to be down for installation? Can we deploy without taking an out- age? Can we make the user experience as pleasant as possible?\n\nJanet’s Story\n\nI had an experience a while ago that was not so pleasant, and it led me to wish that someone had tested and ﬁxed the issue before I found it. I bought a new lap- top and wanted to transfer my license for one of my applications to the new com- puter. It came with a trial version of the same application, so the transfer should have been easy, but the new PC did not recognize the product key—it kept say- ing it was invalid. I called the support desk and after a bit of diagnostics, I was in- formed they were considered different products, so the key wouldn’t work.\n\nTwo more hours of support time, and the issue was ﬁxed. The trial version had to be removed, an old version had to be reinstalled, the key had to be reentered, and all updates since the original purchase had to be uploaded. How much easier would it have been for the development team to test that scenario and offer the customer an informative message saying, “The trial version is not compatible with your product key.” A message such as that would have let me ﬁgure out the prob- lem and solve it myself rather than taking the support person’s time.\n\nTake the time you need to determine what your requirements are for testing installation. It will be worth it in the end if you satisfy your customers.\n\nCommunication\n\nConstant communication between different development team members is always important, but it’s especially critical as we wrap up the release. Have extra stand-up meetings, if needed, to make sure everything is ready for the release. Write cards for release tasks if there’s any chance some step might be forgotten.\n\nLisa’s Story\n\nMy team releases after each iteration. We usually have a quick stand-up on the last afternoon of the sprint to touch base and identify any loose ends. Before the team had a lot of practice with releases, we wrote release task cards such as “run database update script in staging” and “verify database updates in production.”\n\n—Janet\n\nTHE END GAME\n\nWith more experience at deploying, we no longer need those cards unless we have a new team member who might need an extra reminder. It never hurts to have cards for release tasks, though.\n\nReminders of tasks, whether they are in a full implementation plan or just written on task cards as Lisa’s team does, are often necessary. On simple im- plementations, a whiteboard works well.\n\nWhat If It’s Not Ready?\n\nBy constantly tracking progress in many forms, such as builds, regression test suites, story boards, and burndown charts, a team usually knows well in ad- vance when it’s in trouble on a release. There’s time to drop stories and read- just. Still, last-minute disasters can happen. What if the build machine breaks on the last day of the iteration? What if the test database crashes so that ﬁnal testing can’t be completed? What if a showstopper bug isn’t detected until ﬁ- nal functional testing?\n\nWe strongly advise against adding extra days to an iteration, because it will eat into the next iteration or release development. An experienced team might be ﬂexible enough to do this, but it can derail a new team. Still, des- perate times call for desperate measures. If you release every two weeks, you may simply be able to skip doing the actual release, budget time into the next iteration to correct the problems and ﬁnish up, and release on the next scheduled date. If testing tasks are being put off or ignored and the release goes ahead, bring up this issue with the team. Did the testing needs change, or is the team taking a chance and sacriﬁcing quality to meet a deadline? The team should cut the release scope if the delivery date is ﬁxed and in jeopardy.\n\nIf your release cycle is longer, more like three months, you should know in advance if your release is in jeopardy. You probably have planned an end game of at least two weeks, which will just be for ﬁnal validation. When you have a longer release cycle, you have more time to determine what you should do, whether it’s dropping functionality or changing the schedule.\n\nIf your organization requires certain functionality to be released on a ﬁxed day and last-minute glitches threaten the release, evaluate your alternatives. See if you can continue on your same development cycle but delay the release itself for a day or a week. Maybe the offending piece of code can be backed out temporarily and a patch done later. The customers have the ultimate say in what will work for the business.\n\n463\n\n—Lisa\n\n464\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nLisa’s Story\n\nOn the rare occasions when our team has faced the problem of last-minute show- stoppers, we’ve used different approaches according to the situation. If there’s nothing critical that has to be released right now, we sometimes skip the release and release two iterations’ worth on the next release day. If something critical has to go in, we delay the release a day or two. Sometimes we can go ahead and re- lease what we have and do a patch release the next day. On one occasion, we decided to have a special one-week iteration to correct the problems, release, and then go back to the normal two-week iteration schedule.\n\nAfter more than four years of practicing agile development, we have a stable build almost 100% of the time, and we feel conﬁdent about being able to release whenever it’s necessary. We needed a lot of discipline and continual improvement to our process in order to feel that a more ﬂexible approach could work for us. It’s also nice to be able to release a valuable bit of functionality early, if we can. What we’ve worked hard to avoid is falling into a death spiral where we can never re- lease on schedule and we’re always playing catch-up.\n\nDon’t beat yourself up if you can’t release on time. Your team is doing its best. Do spend time analyzing why you got behind schedule, or over-committed, and take action to keep it from happening again.\n\nWork to prevent a “no go” situation with good planning, close collaboration, driving coding with tests, and testing as you code. If your tracking shows the release could be in jeopardy, remove the functionality that can’t be ﬁnished, if possible. If something bad and unexpected happens, don’t panic. Involve the whole team and the customer team, and brainstorm about the best solution.\n\nCUSTOMER TESTING There are a couple of different ways in which to involve your customers to get their approval or feedback. User Acceptance Testing can be fairly formal, with sign-offs from the business. It signiﬁes acceptance of a release. Alpha or beta testing is a way to get feedback on a product you are looking to release but which is not quite ready.\n\nUAT\n\nUser Acceptance Testing (UAT) is important in large customized applications as well as internal applications. It’s performed by all affected business depart- ments to verify usability of the system and to conﬁrm existing and new (em- phasis on new) business functionality of the system. Your customers are the\n\n—Lisa\n\nJanet’s Story\n\nCUSTOMER TESTING\n\nones who have to live with the application, so they need to make sure it works on their system and with their data.\n\nIn previous chapters we’ve often talked about getting the customers involved early, but at those times, the testing is done on speciﬁc features under develop- ment. UAT is usually done after the team decides the quality is good enough to release. Sometimes though, the timeline dictates the release cycle. If that is the case, then try moving the UAT cycle up to run parallel with your end game. The application should be stable enough so that your team could deploy to the customer’s test system at the same time as they deploy to staging.\n\nIn one team I joined, the customers were very picky. In fact, the pickiest I had ever seen. They always asked for a full week of UAT just to be sure they had the time to test it all. They had prepared test cases and checked them all, including all the content, both in English and in French. Showstopper bugs included spelling errors such as a missing accent in the French content. Over time, as they gained more conﬁdence in our releases and found fewer and fewer errors, they relaxed their demands but still wanted a week, just in case they couldn’t get to it right away. Their business group was very busy.\n\nOne release came that pushed the timeline. We were being held to the release date but couldn’t get all the functionality in and leave two weeks for the end game. We talked with the business users and we decided to decrease the end game to one week; the business users would perform their UAT while the project team ﬁnished up their system testing and cleanup. The only reason we were able to do this was because of the trust the customer had in our team and the consis- tency of our releases.\n\nThe good news was that, once again, the UAT found no issues that could not wait until the next release.\n\nFigure 20-1 shows an example timeline with a normal UAT at the end of the release cycle. The team starts working on the next release, doing release plan- ning, and starts the ﬁrst iteration with all team members ready to go.\n\nWork with customers so that they understand the process, their role, and what is expected of them. If the UAT is not smooth, then the chances are there will be a high level of support needed. An experienced customer test team may have deﬁned test cases, but most often its testing is ad hoc. Customers may approach their testing as if they were doing their daily job but will probably focus on the new functionality. This is an opportunity to observe how people\n\n465\n\n—Janet\n\n466\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nRelease Timeline Normal Release with UAT\n\nIter 2\n\nIter 3\n\nEnd Game\n\n. . .\n\nRelease 1\n\nCode and Test Code and Test\n\nSystem Test\n\nUAT\n\nIter 1\n\nIter 2\n\n. . .\n\nRelease 2\n\nRelease Planning\n\nCode and Test Code and Test\n\nFigure 20-1 Release timeline with UAT\n\nuse the system and to get feedback from them on what works well and what improvements would help them.\n\nTesters can provide support to the customers who are doing the UAT by re- viewing tests run and defects logged, and by tracking defects to completion. Both of us have found it helpful to provide customers involved in doing UAT with a report of all of the testing done during development, along with the results. That helps them decide where to focus their own testing.\n\nAlpha/Beta Testing\n\nIf you are an organization that distributes software to a large customer base, you may not have a formal UAT. You are much more likely to incorporate al- pha or beta testing. Your team will want to get feedback on new features from your real customers, and this is one mechanism for doing so. Alpha testing is early distribution of new versions of software. Because there are likely to be some major bugs, you need to pick your customers wisely. If you choose this method of customer feedback, make sure your customers understand their role. Alpha testing is to get feedback on the features—not to report bugs.\n\nBeta testing is closer to UAT. It is expected that the release is fairly stable and can actually be used. It may not be “ready for prime time” for most custom- ers, but many customers may feel the new features are worth the risk. Cus-\n\nLisa’s Story\n\nPOST-DEVELOPMENT TESTING CYCLES\n\ntomers should understand that it is not a formal release and that you are asking them to test your product and report bugs.\n\nAs a tester, it is important to understand how customers view the product, because it may affect how you test. Alpha and beta testing may be the only time you get to interact with end users, so take advantage of the chance to learn how well the product meets their needs.\n\nPOST-DEVELOPMENT TESTING CYCLES If you work in a large organization or are developing a component of a large, complex system, you may need to budget time for testing after development is complete. Sometimes the UAT testing, or the test coordination, isn’t as smooth as it could be, so the timeline stretches out. Test environments that include test versions of all production systems may only be available for small, scheduled windows of time. You may need to coordinate test sessions with teams working on other applications that interact with yours. Whatever the reason, you need extra testing time that does not include the whole development team.\n\nI worked on a team developing components of both internal and external appli- cations for a large telecom client. We could only get access to the complete test environment at scheduled intervals. Releases were also tightly scheduled.\n\nThe development team worked in two-week iterations. It could release to the test environment only after every third iteration. At that time, there was a two-week system integration and user acceptance test cycle, followed by the release.\n\nSomeone from my team needed to direct the post-development testing phase. Meanwhile, the developers were starting a new iteration with new features, and they needed a tester to help with that effort.\n\nThe team had to make a special effort to make sure someone in the tester role fol- lowed each release from start to ﬁnish. For example, I worked from start to ﬁnish on release 1. Shauna took over the tester role as the team started work on the ﬁrst iteration of release 2, while I was coordinating system testing and UAT on re- lease 1. Shauna stayed as primary tester for release 2, while I assumed that role for release 3.\n\nFigure 20-2 shows an example timeline where the UAT was extended. This could happen for any number of reasons, and the issue may not always be UAT. Most of the team is ready to start working on the next release, but often\n\n467\n\n—Lisa\n\n468\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nRelease Timeline Overlapping Releases with Extended UAT\n\nTester stays with this release until UAT is completed\n\nIter 2\n\nIter 3\n\nEnd Game\n\n. . .\n\nRelease 1\n\nCode and Test Code and Test\n\nSystem Test\n\nUAT\n\nIf no tester available to work on new release, programmers can do refactoring, spikes\n\nIter 1\n\nIter 2\n\n. . .\n\nRelease 2—Alternative 1\n\nRelease Planning\n\nRefactoring, Spikes\n\nCode and Test\n\nOther tester works on next release\n\nIter 0\n\nIter 1\n\n. . .\n\nRelease 2—Alternative 2\n\nRelease Planning\n\nCode and Test Code and Test\n\nFigure 20-2 Release timeline—alternative approach with extended UAT\n\na tester is still working with customers, completing ﬁnal testing. Sometimes a programmer will be involved as well. There are a couple of options. If the team is large enough, you can probably start the next release while a couple of team members work with the existing release (Release 2—Alternative 2 in Figure 20-2). If you have a small team, you may need to consider an Iteration 0 with programmers doing refactoring or spikes (experiments) on new func- tionality so that the tester working with the customer does not get left behind (Release 2—Alternative 1 in Figure 20-2).\n\nBe creative in dealing with circumstances imposed on your team by the reali- ties of your project. While plans rarely work as expected, planning ahead can still help you make sure the right people are in place to deliver the product in a timely manner.\n\nDELIVERABLES In the ﬁrst section of this chapter we talked about what makes a product. The answer to this will actually depend on the audience: Who is accepting the product, and what are their expectations?\n\nDELIVERABLES\n\nIf your customers need to meet SOX (Sarbanes-Oxley)compliance require- ments, there will be certain deliverables that are required. For example, one cus- tomer Janet has worked with felt test results should be thoroughly documented, and made test results one of their SOX compliance measurement points, while a different customer didn’t measure test results at all. Work with compliance and audit personnel to identify reporting needs as you begin a project.\n\nHow much documentation is enough? Janet always asks two questions before answering that question: “Who is it for?” and “What are they using it for?” If there are no adequate answers to those questions, then consider whether the documentation is really needed.\n\nDeliverables are not always for the end customer, and they aren’t always in the form of software. There are many internal customers, such as the produc- tion support team members. What will they need to make their job easier? Workﬂow diagrams can help them understand new features. They would probably like to know if there are work-arounds in place so they can help customers through problems.\n\nJanet often gets asked about test coverage of code, usually by management. How much of the application is being tested by the unit tests or regression tests? The problem is that the number by itself is just a number, and there are so many reasons why it might be high or low. Also, code coverage doesn’t tell you about features that might have been missed, for which no code exists yet. The audience for a deliverable such as code coverage should not be manage- ment, but the team itself. It can be used to see what areas of the code are not being tested.\n\nTraining could be considered a deliverable as well. Many applications require customized training sessions for customers. Others may only need online help or a user manual. Training could determine the success of your product, so it’s important to consider. Lisa’s team often writes task cards for either a tester or the product owner to make sure training materials and sessions are arranged. Some people may feel training isn’t the job of testers or anyone else on the development team. However, agile teams aim to work as closely as possible with the business. Testers often have the domain expertise to be able to at least identify training that might be needed for new or updated features. Even if training isn’t the tester’s responsibility, she can raise the issue if the business isn’t planning training sessions.\n\nMany agile teams have technical writers as part of the team that write online help or electronic forms of documentation. One application even included\n\n469",
      "page_number": 503
    },
    {
      "number": 61,
      "title": "Segment 61 (pages 512-519)",
      "start_page": 512,
      "end_page": 519,
      "detection_method": "topic_boundary",
      "content": "470\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\ntraining videos to help get started, and different members of the team were the trainers. It is the responsibility of the team to create a successful product.\n\nNonsoftware Deliverables\n\nConi Tartaglia, software test manager at Primavera Systems, Inc., reﬂects on what has worked for her team in delivering items that aren’t code but are necessary for a successful release.\n\nAside from the software, what is the team delivering? It is helpful to have a conversation with the people outside of the development team who may be concerned with this question. Groups such as Legal, Product Marketing, Training, and Customer Support will want to contribute to the list of deliverables.\n\nAfter there is agreement on what is being delivered, assembly of the components can begin, and the Release Management function can pro- vide conﬁrmation of the delivery through execution of a release check- list. If the release is an update to an existing product, testers can check the deliverables from previous releases to ensure nothing critical is left out of the update package. Deliverables can include legal notices, docu- mentation, translations, and third-party software that are provided as a courtesy to the customers.\n\nAgile teams are delivering value, not just software. We work together with the customer team to improve all aspects of the product.\n\nThere are no hard and fast rules to what should be delivered with the prod- uct. Think of deliverables as something that adds value to your product. Who should be the recipient of the deliverable, and when does it make the most sense to deliver it?\n\nRELEASING THE PRODUCT When we talk about releasing the product, we mean making it available to the customer in whatever format that may take. Your organization might have a website that gets updated or a custom application that is delivered to a few large customers. Maybe the product is shrink-wrapped and delivered to millions of PCs around the world, or downloaded off the Internet.\n\nRelease Acceptance Criteria\n\nHow do you know when you’re done? Acceptance criteria are a traditional way of deﬁning when to accept the product. Performance criteria may have\n\nRELEASING THE PRODUCT\n\nto be met. We capture these for each story at the start of each iteration, and we may also specify them for larger feature sets when we begin a theme or epic. Customers may set quality criteria such as a certain percentage of code covered by automated tests, or that certain tests must pass. Line items such as having zero critical bugs, or zero bugs with serious impact to the system, are often part of the release criteria. The customers need to decide how they’ll know when there’s enough value in the product. Testers can help them deﬁne release criteria that accomplish their goals.\n\nAgile teams work to attain the spirit of the quality goals, not just the letter. They don’t downgrade the severity of bugs to medium so they can say they achieved the criterion of no high-severity bugs. Instead, they frequently look at bug trends and think of ways to ensure that high-severity bugs don’t occur in production.\n\nYour quality level should be negotiated with your customer up front so that there are no unpleasant surprises. The acceptance tests your team and your customers deﬁned, using real examples, should serve as milestones for progress toward release. If your customer has a very low tolerance for bugs, and 100% of those acceptance tests must be passing, your iteration velocity should take that into consideration. If new features are more important than bug ﬁxes, well, maybe you will be shipping with bugs.\n\nA Tale of Multitiered “Doneness”\n\nBob Galen, agile coach and author of Software Endgames, explains how his teams deﬁne release acceptance criteria and evaluate whether they’ve been met.\n\nI’ve joined several new agile teams over the past few years, and I’ve seen a common pattern within those teams. My current team does a wonder- ful job of establishing criteria at a user story or feature level—basically deﬁning acceptance criteria. We’ve worked hard at reﬁning our accep- tance criteria. Initially they were developed from the Product Owners’ perspective, and often they were quite ambiguous and ill-deﬁned. The testers decided they could really assist the customers in reﬁning their tests to be much more relevant, clear, and testable. That collaboration proved to be a signiﬁcant win at the story level, and the Product Owners really valued the engagement and help.\n\nQuite often the testers would also automate the user story acceptance tests, running them during each sprint but also demonstrating overall ac- ceptance during the sprint review.\n\n471\n\n472\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nOne problem we had, though, was getting this same level of clarity for “doneness” at a story level to extend beyond the individual stories. We found that often, when we approached the end of a Sprint or the end game of a release, we would have open expectations of what the team was supposed to accomplish within the sprint. For example, we would deliver stories that were thoroughly tested “in the small”; that is, the functionality of those stories was tested but the stories were not inte- grated into our staging environment for broader testing. That wasn’t part of our “understanding,” but external stakeholders had that expectation of the teams’ deliverables.\n\nThe way the teams solved this problem was to look at our criteria as a multitiered set of guiding goals that wrap each phase, if you will, of agile development. An example of this is shown in Table 20-1.\n\nDeﬁning doneness at these individual levels has proven to work for our teams and has signiﬁcantly improved our ability to quantify and meet all of our various customer expectations. Keep in mind that there is a con- nection among all of the criteria, so deﬁning at one level really helps de- ﬁne the others. We often start at the Release Criteria level and work our way “backwards.”\n\nAgile development doesn’t work if stories, iterations, or releases aren’t “done.” “Doneness” includes testing, and testing is often the thing that gets postponed when time is tight. Make sure your success criteria at every level includes all of the necessary testing to guide and validate development.\n\nTable 20-1 Different Levels of Doneness\n\nActivity Basic Team Work Products\n\nUser Story Level\n\nSprint or Iteration Level\n\nRelease Level\n\nCriteria Doneness criteria\n\nAcceptance tests\n\nDoneness criteria\n\nRelease criteria\n\nExample Pairing or pair inspections of code prior to check-in, or to development, execu- tion, and passing of unit tests Development of FitNesse-based accep- tance tests with the customer AND their successful execution and passing Deﬁning a Sprint Goal that clariﬁes the feature development and all external dependencies associated with a sprint Deﬁning a broad set of conditions (artifacts, testing activities or coverage levels, results/metrics, collaboration with other groups, meeting compliance levels, etc.) that, if met, would mean the release could occur\n\nRELEASING THE PRODUCT\n\nEach project, each team, each business is unique. Agile teams work with the business experts to decide when they’re ready to deliver software to produc- tion. If the release deadline is set in stone, the business will have to modify scope. If there’s enough ﬂexibility to release when the software has enough value, the teams can decide when the quality criteria have been met and the software can go to production.\n\nChallenging Release Candidate Builds\n\nConi Tartaglia’s team uses a checklist to evaluate each release candidate build. The checklist might specify that the release candidate build:\n\nIncludes all features that provide business value for the release, includ- ing artwork, logos, legal agreements, and documentation\n\nMeets all build acceptance criteria • Has proof that all agreed-upon tests (acceptance, integration, regres- sion, nonfunctional, UAT) have passed\n\nHas no open defect reports\n\nConi’s team challenges the software they might ship with a ﬁnal set of inspec- tions and agreed-upon “release acceptance tests,” or “RATS.” She explains:\n\nThe key phrase is “agreed-upon tests.” By agreeing on the tests in ad- vance, the scope for the release checklist is well deﬁned. Include system- level, end-to-end tests in the RATS, and select from the compatibility ros- ter tests, which will really challenge the release candidate build. Perfor- mance tests can also be included in RATs. Agree in advance on the content of the automation suites as well as a subset of manual tests for each RAT.\n\nAgree in advance which tests will be repeated if a RAT succeeds in caus- ing the failure of a release candidate build. If the software has survived several iterations of continuously run automated regression tests, passing these ﬁnal challenges should be a breeze.\n\nDeﬁning acceptance criteria is ultimately up to the customers. Testers are in a unique position to help the customer and development teams agree on the criteria that optimize product quality.\n\nTraditional software development works in long time frames, with deadlines set far in advance and hurdles to clear from one phase to the next. Agile de- velopment lets us produce quality software in small increments and release as necessary. The development and customer teams can work closely to deﬁne and decide what to release and when. Testers can play a critical role in this goal-setting process.\n\n473\n\n474\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nRelease Management\n\nMany organizations have a release management team, but if you don’t, some- one still does the work. Many times in a small organization it is the QA man- ager who fulﬁlls this role. The person leading the release may hold a release readiness meeting with the stakeholders to evaluate readiness.\n\nA release readiness checklist is a great tool to use to walk through what is im- portant to your team. The intention of this checklist is to help the team ob- jectively determine what was completed and identify the risks associated with not completing a task.\n\nFor example, if training is not required because the changes made to the prod- uct were transparent to the end user, then the risk is low. However, if there were signiﬁcant changes to the process for how a new user is created in the system, the risk would be very high to the production support or help teams, and may warrant a delay. The needs of all stakeholders must be considered.\n\nRelease notes are important for any product release. The formality of these depends on the audience. If your product is aimed at developers, then a “read me” text ﬁle is probably ﬁne. In other cases, you may want to make them more formal. Whatever the media, they should address the needs of the audi- ence. Don’t provide a lot of added information that isn’t needed.\n\nWhen Janet gets a new release, one of the ﬁrst things she does is check the version and all of the components. “Did I get what they said they gave me? Are there special instructions I need to consider before installing, such as de- pendencies or upgrade scripts?” Those are good simple questions to answer in release notes. Other things to include are the new features that the cus- tomer should look for.\n\nRelease notes should give special consideration to components that aren’t part of what your development team delivered, such as a help ﬁle or user manuals prepared by a different team. Sometimes old release notes get left on the release media, which may or may not be useful to the end user. Consider what is right for your team and your application.\n\nPackaging\n\nWe’ve talked a lot about continual integration. We tend to take it for granted and forget what good conﬁguration management means. “Build once, deploy multiple times” is part of what gives us conﬁdence when we release. We know that the build we tested in staging is the same build that the customer tested\n\nCUSTOMER EXPECTATIONS\n\nin UAT and is the build we release to production. This is critical for a success- ful release.\n\nIf the product is intended for an external customer, the installation should be easy, because the installation may be the ﬁrst look at the product that cus- tomer has. Know your audience and its tolerance level for errors. How will the product be delivered? For example, if it is to be downloaded off the Inter- net, then it should be a simple download and install. If it is a huge enterprise system, then maybe your organization needs to send a support person with the product to help with the install.\n\nCUSTOMER EXPECTATIONS Before we spring new software on our customers, we’d better be certain they are ready for it. We must be sure they know what new functionality to expect and that they have some means to deal with problems that arise.\n\nProduction Support\n\nMany organizations have a production or operations support team that maintains the code and supports customers after it’s in production. If your company has a production support team, that group is your ﬁrst customer. Make it your partner as well. Production support teams receive defect reports and enhancement requests from the customers, and they can work with your team to identify high-risk areas.\n\nVery often the production support team is the team that accepts the release from the development team. If your organization has this type of hand-off, it is important that your development team works closely with the production support team to make it a smooth transition. Make sure the production sup- port team understands how to use the system’s log ﬁles and the messaging and monitoring systems in order to keep track of operations and identify problems quickly.\n\nUnderstand Impact to Business\n\nEvery time a deployment to production requires an outage, the product is unavailable to your customer. If your product is a website, this may be a huge impact. If your product is an independent product to be downloaded onto a PC, the impact is low. Agile teams release frequently to maximize value to the business, and small releases have a lower risk of a large negative impact. It’s common sense to work with the business to time releases for time periods\n\n475\n\n476\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nthat minimize disruption. Automate and streamline deployment processes as much as possible to keep downtime windows small. A quick deployment process is also helpful during development in short iterations where we may deploy a dozen times in one day.\n\nInternational Considerations\n\nMarkus Gärtner, an “agile affected” testing group lead, explains his team’s ap- proach to timing its releases:\n\nWe build telecommunications software for mobiles, so we usually install our software at night, when no one is likely to make calls. This might be during our ofﬁce hours, when we're handling a customer in Australia, but usually it is during our nighttime.\n\nMy colleagues who do the actual installation—there are three within our team—are most likely to appear late during next day's ofﬁce hours be- cause we don't have a separate group for these tasks.\n\nAs businesses and development teams become more global, release timing gets more complicated. Fortunately, production conﬁgurations can make re- leases easier. If your production environment has multiple application servers, you may be able to bring them down one at a time for release without dis- rupting users.\n\nNew releases should be as transparent as possible to the customer. The fewer emergency releases or patches required after a release, the more conﬁdence your customer will have in both the product and the development team.\n\nLearn from each release and take actions to make the next one go more smoothly. Get all roles, such as system and database administrators, involved in the planning. Evaluate each release and think of ways to improve the next one.\n\nSUMMARY This chapter covered the following points:\n\n(cid:2) Successful delivery of a product includes more than just the applica- tion you are building. Plan the non-software deliverables such as doc- umentation, legal notices, and training.\n\n(cid:2) The end game is an opportunity to put the spit and polish, the ﬁnal\n\nﬁnishing touches, on your product.\n\nSUMMARY\n\n(cid:2) Other groups may be responsible for environments, tools, and other components of the end game and release. Coordinate with them ahead of time.\n\n(cid:2) Be sure to test database update scripts, data conversions, and other\n\nparts of the installation.\n\n(cid:2) UAT is an opportunity for customers to test against their data and to\n\nbuild their conﬁdence in the product.\n\n(cid:2) Budget time for extra cycles as needed, such as post-development\n\ncycles to coordinate testing with outside parties.\n\n(cid:2) Establish release acceptance criteria during release planning so that\n\nyou can know when you’re ready to release.\n\n(cid:2) Testers often are involved in managing releases and testing the\n\npackaging.\n\n(cid:2) When releasing the product, consider the whole package—what the\n\ncustomer needs and expects.\n\n(cid:2) Learn from each release, and adapt to improve your processes.\n\n477",
      "page_number": 512
    },
    {
      "number": 62,
      "title": "Segment 62 (pages 520-527)",
      "start_page": 520,
      "end_page": 527,
      "detection_method": "topic_boundary",
      "content": "This page intentionally left blank\n\nPart VI SUMMARY\n\nIn Chapter 21, “Key Success Factors,” we pull things together and summarize the agile approach to testing.\n\nThis page intentionally left blank\n\nChapter 21\n\nKEY SUCCESS FACTORS\n\nLook at the Big Picture\n\nUse the Whole-Team Approach\n\nCollaborate with Customers\n\nAdopt an Agile Testing Mind-Set\n\nKey Success Factors\n\nContinuous Integration\n\nTest Environments\n\nAutomate Regression Testing\n\nManage Technical Debt\n\nWorking Incrementally\n\nBuild a Foundation of Core Agile Practices\n\nProvide and Obtain Feedback\n\nCoding and Testing Are Part of One Process\n\nSynergy between Practices\n\nHaving traveled through an iteration and beyond, following an agile tester as she engages in many activities, we can now pick out some key factors that help testers succeed on agile teams and help agile teams succeed at delivering a high-quality product. We think agile testers have something special to offer. “Agile-infected” testers learn how to apply agile practices and principles to help their whole team produce a better product. “Test-infected” programmers on agile teams learn how to use testing to produce better work. Lines between roles are blurred, but that’s a good thing. Everyone is focused on quality.\n\nWe have gleaned some critical testing guidelines for agile teams and testers\n\nfrom our own trial and error as well as from teams with which we’ve worked. These guidelines are built on the agile testing matrix, on our experience of learning to overcome cultural and organizational obstacles, our adventures in performing the tester role on agile teams, and our experience of ﬁguring out how best to use test automation. We like lucky numbers, so in this chapter we present seven key factors that help an agile tester succeed.\n\nWe asked a small group of people who were reviewing some of our chapters to suggest the order in which to present these success factors. The results varied quite a bit, although many (but not all) agreed on the top two. Pick the success factor that will give you the biggest return on investment, and start working on it today.\n\n481\n\n482\n\nCHAPTER 21\n\n(cid:2) KEY SUCCESS FACTORS\n\nSUCCESS FACTOR 1: USE THE WHOLE-TEAM APPROACH When the whole development team takes responsibility for testing and qual- ity, you have a large variety of skill sets and experience levels taking on what- ever testing issues might arrive. Test automation isn’t a big problem to a group of skilled programmers. When testing is a team priority, and anyone can sign up for testing tasks, the team designs testable code.\n\nMaking testers truly part of the development team means giving them the support and training they need to adapt to the fast pace of agile develop- ment. They have time to acquire new skills in order to collaborate closely with members of both the development and customer teams.\n\nIf you manage an agile team, use the suggestions in Part II, “Organizational Challenges,” to help your team adopt the whole-team approach. Remember that quality, not speed, is the goal of agile development. Your team needs testers to help customers clarify requirements, turn those into tests that guide development, and provide a unique viewpoint that will promote delivery of a solid product. Make sure the testers can transfer their skills and expertise to the rest of the team. Make sure they aren’t pigeonholed in a role such as only doing manual testing. Make sure that when they ask for help (which may re- quire considerable courage on their part), their team members give it. The reverse is true, too; a tester should step up whenever someone needs assis- tance that they can provide.\n\nSee Chapter 2, “Ten Principles for Agile Testers,” for an example of how the “Power of Three” works.\n\nIf you’re a tester on an agile team, and there are planning meetings and de- sign discussions happening that don’t include you, or the business users are struggling to deﬁne their stories and requirements alone, it’s time to get up and go talk to the rest of the team. Sit with the programmers, invite yourself to meetings, and propose trying the “Power of Three” by involving a tester, a programmer, and a business expert. Be useful, giving feedback and helping the customers provide examples. Make your problems the team’s problems, and make their problems yours. Ask your teammates to adopt a whole-team approach.\n\nSUCCESS FACTOR 2: ADOPT AN AGILE TESTING MIND-SET In Chapter 2, “Ten Principles for Agile Testers,” we cautioned agile testers to lose any “Quality Police” mind-set they might have brought with them. You’re on an agile team now, where programmers test and testers do what- ever they can think of to help the team deliver the best possible product. As\n\nSee Chapter 2, “Ten Principles for Agile Testers,” for more about the agile testing mindset.\n\nSUCCESS FACTOR 2: ADOPT AN AGILE TESTING MIND-SET\n\nwe emphasized in Chapter 2, an agile testing attitude is proactive, creative, open to new ideas, and willing to take on any task. The agile tester constantly hones her craft, is always ready to collaborate, trusts her instincts, and is pas- sionate about helping the team and the business succeed.\n\nWe don’t mean that you should put on your Super Tester cape and go protect the world from bugs. There’s no room for big egos on agile teams. Your teammates share your passion for quality. Focus on the team’s goals and do what you can to help everyone do their best work.\n\nUse agile principles and values to guide you. Always try the simplest ap- proach to meeting a testing need. Be courageous in seeking help and experi- menting with new ideas. Focus on delivering value. Communicate as directly and as often as possible. Be ﬂexible in responding to change. Remember that agile development is people-centric, and that we should all enjoy our work. When in doubt, go back to the values and principles to decide what to do.\n\nAn important component of the agile testing mind-set is the drive to contin- ually ﬁnd better ways to work. A successful agile tester constantly polishes her craft. Read good books, blogs, and articles to get new ideas and skills. At- tend local user group meetings. Participate in mailing list discussions to get feedback on problems or new ideas. If your company won’t pay for you to at- tend a good conference, put what you’ve learned into an experience report to exchange for a free conference registration. Giving back to your testing and agile development communities will help you, too.\n\nExperiment with new practices, tools, and techniques. Encourage your team to try new approaches. Short iterations are ideally suited to experimentation. You might fail, but it’ll be fast, and you can try something else.\n\nIf you manage agile testers or an agile team, give them time to learn and pro- vide support for the training they need. Remove obstacles so that they can do their best work.\n\nWhen you’re faced with problems that impact testing, bring those problems to the team. Ask the team to brainstorm ways to overcome these obstacles. Retrospectives are one place to talk about issues and how to resolve them. Keep an impediment backlog and address one or two in every iteration. Use big visible charts, or their virtual equivalents, to ensure that everyone is aware of problems that arise and that everyone can track the progress of cod- ing and testing.\n\n483\n\n484\n\nCHAPTER 21\n\n(cid:2) KEY SUCCESS FACTORS\n\nSUCCESS FACTOR 3: AUTOMATE REGRESSION TESTING Can an agile team succeed with no test automation? Maybe, but the success- ful teams that we know rely on automated regression tests. As we’ve said of- ten in this book, if you’re spending all your time doing manual regression testing, you’ll never have time for the important exploratory testing that will ferret out the damaging behaviors lurking in the code.\n\nSee Part II for more on the Agile Test- ing Quadrants.\n\nAgile development uses tests to guide development. In order to write code to make a test pass, you need a quick and easy way to run the test. Without the short feedback cycle and safety net regression that suites provide, your team will soon become mired in technical debt, with a growing defect queue and ever-slowing velocity.\n\nSee Chapter 14, “Automation Strat- egy,” for more on the test automa- tion pyramid.\n\nAutomating regression tests is a team effort. The whole team should choose appropriate tools for each type of test. Thinking about tests up front will let programmers design code for ease of test automation. Use the Agile Testing Quadrants and test automation pyramid to help you automate different types of tests effectively.\n\nRemember to start simply. You’ll be surprised at how much value some basic automated smoke tests or automated unit tests can provide.\n\nSee the bibliogra- phy for resources on promoting change.\n\nTest automation is a team effort. It’s also hard, at least at ﬁrst. There’s often a big “hump of pain” to overcome. If you manage a development or testing team, make sure you’re providing enough support in the form of time, train- ing, and motivation. If you’re a tester on a team with no automation, and the programmers are too frantic trying to write production code to stop and think about testing, you have a big challenge ahead of you. Experiment with different ways of getting support from management and from team members to start some tiny automation effort.\n\nSUCCESS FACTOR 4: PROVIDE AND OBTAIN FEEDBACK Feedback is a core agile value. The short iterations of agile are designed to provide constant feedback in order to keep the team on track. Testers are in a unique position to help provide feedback in the form of automated test re- sults, discoveries made during exploratory testing, and observations of actual users of the system.\n\nSUCCESS FACTOR 4: PROVIDE AND OBTAIN FEEDBACK\n\nAgile Is All about Feedback\n\nBret Pettichord, CTO of WatirCraft and co-author of Lessons Learned in Soft- ware Testing, shared these thoughts on the importance of feedback to agile development.\n\nAgile methods allow your team to get feedback regarding the software you are building. That’s the point. The feedback works on several levels. Pair programming gives developers instant feedback on their code. Sto- ries represent units of work where testers and analysts can give feed- back to developers. Iteration releases facilitate feedback from outside the team. Most agile practices are valuable because they create feed- back loops that allow teams to adapt.\n\nA lot of teams adopt Agile with a grab-bag approach without quite real- izing the point of the practices. They pair-program without discussion or changing drivers. They send code to QA that the testers can’t test be- cause the story boundaries are arbitrary; they can’t tell whether they found a bug or just the end of the story. Iterations become schedule milestones rather than real opportunities to improve alignment and ad- just objectives.\n\nThe reason Agile teams can do with less planning is because feedback al- lows you to make sure that you are on course. If you don’t have mean- ingful feedback, then you’re not agile. You’re just in a new form of chaos.\n\nOn my last project, we deﬁned our stories so that they made sense to everyone on the team. Our analysts, testers, and developers could all understand and review individual stories. But we found that we had to create a larger grouping, which we called features, to facilitate meaning- ful review from outside our team. We made sure all the stories in a fea- ture were complete before soliciting feedback from outside the team.\n\nBeing able to give and receive meaningful feedback is often a challenge for people. Yet it is crucial to success with Agile.\n\nAgile teams get into terrible binds when executives or clients hand them a list of requirements at the start, tell them to use Agile (because it’s faster), and then don’t want to participate in the feedback process.\n\nAgile isn’t faster all by itself. Agile is only a beneﬁt in a world that ac- knowledges the value of adapting. And that adaptability needs to go all the way to whoever is funding the project. It is not enough for the team to be agile. The sponsors need to be agile too. Are all of the require- ments really required? Do we know exactly what the software needs to look like from the start?\n\n485",
      "page_number": 520
    },
    {
      "number": 63,
      "title": "Segment 63 (pages 528-535)",
      "start_page": 528,
      "end_page": 535,
      "detection_method": "topic_boundary",
      "content": "486\n\nCHAPTER 21\n\n(cid:2) KEY SUCCESS FACTORS\n\nAgile is faster because feedback allows you to ﬁnd and focus on the most valuable features. If you are certain you know what needs to be built, don’t use Agile. If you don’t have time to gather and act on feed- back from customers, then don’t use Agile. If you are sure that everyone understands exactly what needs to be done from the start, then don’t use Agile.\n\nAgile practices build a technical and organizational infrastructure to facil- itate getting and acting on feedback. If you aren’t going to adapt to feedback, then this infrastructure is waste that will only slow you down.\n\nTo us, the value of agile development isn’t that it’s faster but that it delivers enough value quickly enough to help the business grow and succeed. Testers play a key role in providing the feedback that allows that to happen.\n\nTesters need feedback too. How do you know that you have the right exam- ples of desired behavior from the customers? How do you know if the test cases you wrote reﬂected these examples correctly? Can the programmers understand what to code by looking at the examples you’ve captured and the tests you’ve created?\n\nOne of the most valuable skills you can learn is how to ask for feedback on your own work. Ask the programmers if they get enough information to un- derstand requirements and whether that information guides their coding. Ask customers if they feel their quality criteria are being met. Take time in both the iteration planning meetings and retrospectives to talk about these issues and suggest ways to improve.\n\nSUCCESS FACTOR 5: BUILD A FOUNDATION OF CORE PRACTICES An old saying in the testing business is, “You can’t test quality into the prod- uct.” This is, of course, true of agile development as well. We feel you can’t deliver high-quality software without following some fundamental practices. While we think of these as agile practices, they’ve been around longer than the term “agile development,” and they’re simply core practices of successful software development.\n\nContinuous Integration\n\nEvery development team needs source code management and continuous in- tegration to be successful. You can’t test effectively if you don’t know exactly\n\nSee the bibliog- raphy for more information about continu- ous integration.\n\nSUCCESS FACTOR 5: BUILD A FOUNDATION OF CORE PRACTICES\n\nwhat you’re testing, and you can’t test at all if you have no code you can de- ploy. All team members need to check in their work at least once a day. Every integration must be veriﬁed by an automated build that includes tests to pro- vide rapid feedback about the state of the software.\n\nImplementing a continuous integration process should be one of the ﬁrst priorities of any software development team. If your team doesn’t have at least a daily veriﬁed build, stop what you’re doing and get one started. It’s that important. It doesn’t have to be perfect to start with. If you have a huge system to integrate, it’s deﬁnitely more challenging. In general, though, it’s not that difﬁcult. There’s a plethora of outstanding tools, both open source and commercial, available for this purpose.\n\nTest Environments\n\nYou can’t test productively without a test environment that you control. You need to know what build is deployed, what database schema is being used, whether anyone else is updating that schema, and what other processes are running on the machine.\n\nHardware is getting less expensive all the time, and more open source software is available that can be used for test environments. Your team must make the investment so that you can effectively conduct automated and manual explor- atory tests quickly and efﬁciently. If there’s a problem with the test environ- ment, speak up and let it be a problem for the team to solve creatively.\n\nManage Technical Debt\n\nEven good software development teams, feeling time pressure, neglect refac- toring or resort to quick ﬁxes and hacks to solve a problem quickly. As the code becomes more confusing and hard to maintain, more bugs creep in, and it doesn’t take long before the team’s velocity is consumed by bug ﬁxes and trying to make sense out of the code in order to add new features. Your team must constantly evaluate the amount of technical debt dragging it down and work on reducing and preventing it.\n\nPeople often say, “Our management won’t give us time to do things right, we don’t have time to refactor, and we’re under tight deadlines.” However, it’s not hard to make a clear business case showing what growing technical debt is costing the company. There are many ways to measure code and defect rates that can translate technical debt into its impact on the bottom line. Merely pointing to your decreasing velocity may be enough. Businesses need\n\n487\n\n488\n\nCHAPTER 21\n\n(cid:2) KEY SUCCESS FACTORS\n\ntheir software development teams to remain consistently productive. They may have to reduce the scope of their desired features in order to allow enough time for good, test-guided code design and good practices such as continual small refactoring.\n\nGood coverage from automated regression tests is key to minimizing technical debt. If these are lacking, budget time in each iteration to build up the auto- mated tests, plan a “refactoring iteration” to upgrade or add necessary tools, and write tests and do major refactoring efforts. In every iteration, take the time to guide code with tests, refactor the code you’re touching as needed, and add automated tests where they’re missing. Increase your estimates to account for this work. In the long run, the team will be able to go much faster.\n\nWorking Incrementally\n\nOne reason agile teams are able to create a quality product is that they work on a small scale. Stories represent a few days of work, and each story may be broken into several thin slices or steel threads and built step-by-step. This al- lows testing each small piece and then incrementally testing as the pieces are put together.\n\nRead more about small chunks and thin slices in Chap- ter 8, “Business- Facing Tests that Support the Team.”\n\nIf your team members are tempted to take on a large chunk of functionality at once, encourage them to look at a stepwise approach. Ask questions: “What’s the central business value in this story? What’s the most basic path through this piece of code? What would come next?” Suggest writing task cards to code and test the small pieces, get a proof of concept for your design, and conﬁrm your test and test automation strategy.\n\nCoding and Testing Are Part of One Process\n\nPeople who are new to agile often ask agile testers, “What do you do until all the stories are ﬁnished and you can test?” Experienced agile practitioners say, “Testers must be involved throughout the whole iteration, the whole devel- opment process. Otherwise it doesn’t work.”\n\nTesters write tests, based on examples provided by customers, to help program- mers understand the story and get started. Tests and examples provide a com- mon language that everyone involved in producing the software understands. Testers and programmers collaborate closely as coding proceeds, and they both also collaborate closely with the customers. Programmers show testers the functionality they’ve written, and testers show programmers the unexpected behaviors they’ve found. Testers write more tests as coding proceeds, program-\n\nRead more about coding and test- ing in Chapter 18, “Coding and Testing.”\n\nSUCCESS FACTOR 6: COLLABORATE WITH CUSTOMERS\n\nmers make them pass, and testers do more exploratory testing to learn whether the right value has been delivered. Each agile iteration consists of dozens of constant, quick, incremental test-code-test-code-test iterations.\n\nWhen this collaboration and feedback cycle is disturbed, and testing is sepa- rated from development, bad things happen. If a story is tested in the itera- tion after which it was coded and bugs are found, the programmer has to stop working on the new story, remember how the code worked for the last iteration’s story, ﬁx it, and wait for someone to test the ﬁx. There are few facts in software development, but we know for sure that bugs are cheaper to ﬁx the sooner they’re found.\n\nWhen coding is constantly guided by tests, and testing happens alongside coding, we’re much more likely to achieve the behavior and provide the value that the customer wanted. Testing is a team responsibility. If your team doesn’t share this view, ask everyone to think about their focus on quality, their desire to deliver the best possible product, and what steps they can take to ensure that the team achieves its goals.\n\nSynergy between Practices\n\nA single agile development practice such as continuous integration can make a difference, but the combination of multiple agile practices is greater than the sum of the parts. Test-driven design, collective code ownership, and continu- ous integration together deliver rapid feedback, continually improving code design and the ability to deliver business value quickly. Automating tests is good, but using automated tests to drive development, followed up by explor- atory testing to detect gaps or weaknesses, is many levels of magnitude better.\n\nSome practices don’t work well in isolation. Refactoring is impossible with- out automated tests. It’s possible to do small releases in a mini-waterfall fash- ion and avoid all beneﬁts of agile development. If your on-site customer isn’t empowered to make decisions, her value to the team is limited.\n\nAgile practices were designed to complement each other. Take time to under- stand the purpose of each one, consider what is needed to take full advantage of each practice, and make thoughtful decisions about what works for your team.\n\nSUCCESS FACTOR 6: COLLABORATE WITH CUSTOMERS Some of the greatest value that testers contribute to agile teams is helping customers clarify and prioritize requirements, illustrating the requirements\n\n489\n\n490\n\nCHAPTER 21\n\n(cid:2) KEY SUCCESS FACTORS\n\nwith concrete examples of desired behavior and user scenarios, and turning those examples into executable tests. Testers speak the domain language of the business and the technical language of the development team. We make good facilitators and translators.\n\nNever get in the way of direct communication between programmers and customers. Do encourage as much direct communication as possible. Use the “Power of Three.” When requirements are missed or misunderstood, a cus- tomer, programmer, and tester need to work together to get questions an- swered. Get the customers talking in front of a whiteboard or its virtual equivalent as often as necessary. If customers are scattered around the cam- pus, the country, or the globe, use every tool you can ﬁnd to enhance com- munication and collaboration. Teleconferences, instant messages, and wikis aren’t an ideal replacement for face-to-face conversation, but they beat send- ing emails or not talking at all.\n\nSUCCESS FACTOR 7: LOOK AT THE BIG PICTURE This is a generalization, of course, but we’ve found that testers tend to look at the big picture, and usually from a customer point of view. Programmers usually have to focus on delivering the story they’re working on now, and while they may be using tests to guide them, they have to focus on the techni- cal implementation of the requirements.\n\nThis big-picture viewpoint is a huge contribution to the team. Test-driven development, done well, delivers solid code that may, in isolation, be free of defects. What if that new feature causes some apparently unrelated part of the application to break? Someone has to consider the impact to the larger system and bring that to the team’s attention. What if we’ve overlooked some little detail that will irritate the customers? The new UI may be ﬂawlessly coded, but if the background color makes the text hard to read, that’s what the end user’s going to notice.\n\nPart III explains how to use the Agile Testing Quadrants.\n\nUse the Agile Testing Quadrants as a guide to help you plan testing that will cover all the angles. Use the test pyramid idea to ensure good ROI from your test automation. Guiding development with tests helps make sure you don’t miss something big, but it’s not perfect. Use exploratory testing to learn more about how the application should work, and what direction your test- ing needs to take. Make your test environments as similar as possible to pro- duction, using data that reﬂects the real world. Be diligent about re-creating a production-style situation for activities such as load testing.\n\nSUMMARY\n\nIt’s easy for everyone on the team to narrowly focus only on the task or story at hand. That’s a drawback of working on small chunks of functionality at a time. Help your team take a step back now and then to evaluate how your current stories ﬁt into the grand scheme of the business. Keep asking your- selves how you can do a better job of delivering real value.\n\nSUMMARY Testing and quality are the responsibility of the whole team, but testers bring a special viewpoint and unique skills. As a tester, your passion for delivering a product that delights your customers will carry you through the frustrations you and your team may encounter. Don’t be afraid to be an agent for continual improvement. Let agile principles and values guide you as you work with the customer and development teams, adding value throughout each iteration.\n\nIn this concluding chapter, we looked at seven key factors for successful agile testing:\n\n1. 2. 3. 4. 5. 6. 7.\n\nUse the whole-team approach. Adopt an agile testing mind-set. Automate regression testing. Provide and obtain feedback. Build a foundation of core practices. Collaborate with customers. Look at the big picture.\n\n491\n\nThis page intentionally left blank\n\nGLOSSARY\n\nThis glossary contains the authors’ deﬁnitions of terms used throughout this book.\n\nAcceptance Test Acceptance tests are tests that deﬁne the business value each story must deliver. They may verify functional requirements or nonfunctional requirements such as performance or reliability. Although they are used to help guide development, it is at a higher level than the unit-level tests used for code design in test-driven development. Acceptance test is a broad term that may include both business-facing and technology-facing tests.\n\nApplication programming interface (API) APIs enable other software to invoke some piece of functionality. The API may consist of functions, proce- dures, or classes that support requests made by other programs.\n\nBuild A build is the process of converting source code into a deployable arti- fact that can be installed to run the application. The term “build” also refers to the deployable artifact.\n\nComponent A component is a larger part of the overall system that may be separately deployable. For example, on the Windows platform, dynamic linked libraries (DLLs) are used as components, Java Archives (JAR ﬁles) are components on the Java platform, and a service-oriented architecture (SOA) uses Web Services as components.\n\nComponent Test A component test veriﬁes a component’s behavior. Compo- nent tests help with component design by testing interactions between objects.\n\nConditions of Satisfaction Conditions of satisfaction, also called satisfac- tion conditions or conditions of business satisfaction, are key assumptions and decisions made by the customer team to deﬁne the desired behavior of\n\n493",
      "page_number": 528
    },
    {
      "number": 64,
      "title": "Segment 64 (pages 536-543)",
      "start_page": 536,
      "end_page": 543,
      "detection_method": "topic_boundary",
      "content": "494\n\nGLOSSARY\n\nthe code delivered for a given story. Conditions of satisfaction are criteria by which the outcome of a story can be measured. They evolve during conversa- tions with the customer about high-level acceptance criteria for each story. Discussing conditions of satisfaction helps identify risky assumptions and in- creases the team’s conﬁdence in writing and correctly estimating all the tasks to complete the story.\n\nContext-Driven Testing Context-driven testing follows seven principles, the ﬁrst being that the value of any practice depends on its context. Every new project and every new application may require different ways of ap- proaching a project. All seven practices can be found on the website www.context-driven-testing.com/.\n\nCustomer Team The customer team identiﬁes and prioritizes the features needed by the business. In Scrum, these features become epics or themes, which are further broken into stories and comprise the product backlog. Cus- tomer teams include all stakeholders outside of the development team, such as business experts, subject-matter experts, and end users. Testers and developers work closely with the customer team to specify examples of desired behavior for each story and turn those examples into tests to guide development.\n\nCustomer Test A customer test veriﬁes the behavior of a slice or piece of functionality that is visible to the customer and related directly back to a story or feature. The terms “business-facing test” and “customer-facing test” refer to the same type of test as customer test.\n\nDevelopment Team The development team is the technical team that pro- duces the software requested by the customer team. Everyone involved in de- livering software is a developer, including programmers, testers, database experts, system administrators, technical writers, architects, usability experts, and analysts. This development team works together to produce the software and deliver value to the business, whether they are a co-located team or a vir- tual team.\n\nEpic An epic is a piece of functionality, or feature, described by the customer and is an item on the product backlog. An epic is broken up into related sto- ries that are then sized and estimated. Some teams use the term “theme” in- stead of epic.\n\nExploratory Testing Exploratory testing is interactive testing that combines test design with test execution and focuses on learning about the application.\n\nGLOSSARY\n\nSee Chapter 10, “Business-Facing Tests that Critique the Product,” for an ex- tensive deﬁnition of exploratory testing.\n\nFake Object A fake object replaces the functionality of the depended-on component with a simpler implementation. It emulates the behavior of the real depended-on component but is easier to use for testing purposes.\n\nFeature A feature is a piece of functionality described by the customer and is an item on the product backlog. A feature is broken up into related stories that are then sized and estimated. In agile development, the terms “epic” or “theme” are often used in place of “feature.\"\n\nFunctional Test Functional tests verify the system’s expected behavior given a set of inputs and/or actions.\n\nGreenﬁeld Greenﬁeld projects are new application development projects starting from scratch with no existing code base. There are no constraints, so development teams have many options open to them.\n\nIntegrated Development Environment (IDE) An Integrated Development Environment, or IDE, is a set of tools that support programming and testing. It usually includes an editor, compiler or intepreter debugger, refactoring ca- pabilities, and build automation tools. IDEs usually enable integration with a source code control system and provide language-speciﬁc support to help with code design.\n\nIteration An iteration is a short development cycle, generally from one to four weeks, at the end of which production-ready code can potentially be delivered. Several iterations, each one the same length, may be needed to deliver an entire theme or epic. Some teams actually release the code to production each iteration, but even if the code isn’t released, it is ready for release.\n\nJava Messaging Service (JMS) The Java Messaging Service (JMS) API is a messaging standard that enables application components based on the Java 2 Platform, Enterprise Edition (J2EE) to create, send, receive, and read messages.\n\nLegacy System A legacy system is one that does not have any (or few) auto- mated regression tests. Introducing changes in legacy code, or refactoring it, might be risky because there are no tests to catch unintended changes in sys- tem behavior.\n\n495\n\n496\n\nGLOSSARY\n\nMultipurpose Internet Mail Extensions (MIME) Multipurpose Internet Mail Extensions, or MIME, extend the format of Internet mail to enable non-textual messages, multipart message bodies, and non-US-ASCII textual messages and headers.\n\nMock Object A mock object simulates the responses of an existing object. It helps with designing and testing interactions between objects, replacing a real component so that a test can verify its indirect outputs.\n\nProduct Backlog Product Backlog is a Scrum term for the prioritized mas- ter list of all functionality desired in the product. This backlog grows over time as the organization thinks of new features they may need.\n\nProduct Owner Product Owner is a Scrum term for the person responsi- ble for prioritizing the product backlog, or stories. He or she is typically someone from a marketing role or a key business expert involved with development.\n\nQuality Assurance (QA) Team Quality Assurance, or QA, can be deﬁned as actions taken to ensure compliance with a quality standard. In software de- velopment, the term “QA Team” is often used to refer to the team that does software testing. Test teams (see Test Team) provide stakeholders with infor- mation related to the quality of the software product. They perform activities to learn how the system under test should behave and verify that it behaves as expected. In agile development, these activities are fully integrated with de- velopment activities. Testers are often part of the development team along with everyone else involved in developing the software.\n\nProduction Code Production code is the code for the system that is, or will be, used in production, as distinguished from the code that is written to test it. Test code invokes or operates on production code to verify its behavior.\n\nRefactoring Refactoring is changing code, without changing its functional- ity, to make it more maintainable, easier to read, easier to test, or easier to extend.\n\nRegression Test A regression test veriﬁes that the behavior of the system un- der test hasn’t changed. Regression tests are usually written as unit tests to drive coding or acceptance tests to deﬁne desired system behavior. Once the tests pass, they become part of a regression test suite, to guard against unin- tended changes being introduced. Regression tests should be automated to ensure continual feedback.\n\nGLOSSARY\n\nRelease Candidate A release candidate is a version or build of the product that can potentially be released to production. The release candidate may un- dergo further testing or be augmented with documentation or other materials.\n\nReturn on Investment (ROI) Return on investment, or ROI, is a term bor- rowed from the world of ﬁnancial investments and is a measure of the efﬁ- ciency of an investment. ROI can be calculated in different ways, but it’s basically the difference between the gain from an investment and the cost of that investment, divided by the cost of that investment. In testing, ROI is the beneﬁt gained from a testing activity such as automating a test, weighed against the cost of producing and maintaining that test or activity.\n\nSOAP SOAP is a protocol for exchanging XML-based messages over net- works, normally using HTTP/HTTPS. It forms the foundation layer of the web services protocol stack, providing a basic messaging framework upon which abstract layers can be built. A common SOAP messaging pattern is the Remote Procedure Call (RPC) pattern, in which the client network node sends a request message to the server node, and the server immediately sends a response to the client.\n\nStory A user story is a short description of functionality told from the per- spective of the user that is valuable to either the user or the customer team. Stories are traditionally written on index cards. The card typically contains a one-line description of the feature. For example, “As a shopper, I can put items in my shopping cart so that I can check out with them later” is a story. Cards are only useable in combination with subsequent conversations be- tween the customer team and the development team and some veriﬁcation that the story has been implemented through writing and running tests.\n\nStory Test A story test deﬁnes expected behavior for the code to be delivered by the story. Story tests may be business-facing, specifying the functional re- quirements, or technology-facing, such as security or performance tests. These tests are used to guide development as well as to verify the delivered code. Most agile practitioners use the term “story test” synonymously with “acceptance test,” although the term “acceptance test” might be used for tests that verify behavior at a higher level than one story.\n\nStory Board The story board, also called the task board, is used to track the work the team does during an iteration. Task cards, which may be color-coordinated for the type of task, are written for each story. These cards, along with a visual cue of some kind, provide an easy mechanism for seeing the current status of an iteration’s progress. It may use columns or\n\n497\n\n498\n\nGLOSSARY\n\ndifferent colored stickers on cards for different states such as “To do,” “Work in Progress,” “Verify,” and “Done.” The story board might be a physical board on a wall or a virtual online board.\n\nTask Tasks are pieces of work needed to ﬁnish a story. A task might be action needed to implement a small piece of a story, or it might be for building a bit of infrastructure, or testing that encompasses more than one story. Generally it should represent a day or less of work.\n\nTechnical Debt Ward Cunningham ﬁrst introduced this metaphor. When a team produces software without using good practices such as TDD, continu- ous integration, and refactoring, it may incur technical debt. Like ﬁnancial debt, technical debt accrues interest that will cost the team more at a later date. Sometimes this debt may be worthwhile, such as to take advantage of a sudden business opportunity. Usually, though, technical debt compounds and slows the team’s velocity. Less and less business value can be produced in each iteration because the code lacks a safety net of automated regression tests or has become difﬁcult to understand and maintain.\n\nTest Double A test double is any object or component that’s installed in place of the real component for the express purpose of running a test. Test doubles include dummy objects, mock objects, test stubs, and fake objects.\n\nTest-Driven Development (TDD) In test-driven development, the pro- grammer writes and automates a small unit test before writing the small piece of code that will make the test pass. The production code is made to work one test at a time.\n\nTest-First Development In test-ﬁrst development, tests are written in ad- vance of the corresponding production code, but the code is not necessarily made to work one test at a time. Customer or story tests may be used in test- ﬁrst development as well as unit tests.\n\nTest Stub A test stub is an object that replaces a real component needed by the system under test with a test-speciﬁc object that feeds desired indirect in- puts into the system under test. This enables the test to verify logic indepen- dently of the other components.\n\nTest Team A test team performs activities that help deﬁne and subsequently verify the desired behavior of the system under test. The test team provides information to the stakeholders about the external quality of the system, the risks that may be present, and potential risk mitigation strategies. In agile de-\n\nGLOSSARY\n\nvelopment, these activities are fully integrated with development activities. Testers are often part of the development team along with everyone else in- volved in developing the software.\n\nTester A tester provides information to stakeholders about the software be- ing developed. A tester helps customers deﬁne functional and nonfunc- tional requirements and quality criteria, and helps turn these into tests that guide development and verify desired behavior. Testers perform a wide vari- ety of activities related to delivering high-quality software, such as test auto- mation and exploratory testing. In agile development, everyone on the development team performs testing activities. Team members who identify themselves as testers work closely with other members of both the developer and customer teams.\n\nTheme A theme is the same as an epic or feature. It is a piece of functionality described by the customer and placed in the product backlog to be broken up into stories that are sized and estimated.\n\nUnit Test A unit test veriﬁes the behavior of a small part of the overall sys- tem. It may be as small as a single object or method that is a consequence of one or more design decisions.\n\nVelocity A development team’s velocity is the amount of value it delivers in each iteration, measured in story points, ideal days, or hours. Generally, only completed stories are included in the velocity. Velocity is helpful to the busi- ness in planning for future features and releases. Agile teams use their veloc- ity for the previous iteration to help determine the amount of work they can take on in the next iteration.\n\nWeb Service Description Language (WSDL) Web Service Description Lan- guage (WDSL) is an XML format for describing network services as a set of endpoints operating on messages containing either document-oriented or procedure-oriented information.\n\n499\n\nThis page intentionally left blank\n\nBIBLIOGRAPHY\n\nBOOKS, ARTICLES, PAPERS, AND BLOG POSTINGS Agile Alliance. “Principles Behind the Agile Manifesto,” www.agilemanifesto .org/principles.html, 2001.\n\nAlles, Micah, David Crosby, Carl Erickson, Brian Harleton, Michael Marsiglia, Greg Pattison, and Curt Stienstra. “Presenter First: Organizing Complex GUI Applications for Test-Driven Development,” Agile 2006, Minneapolis, MN, July 2006.\n\nAmbler, Scott. Agile Database Techniques: Effective Strategies for the Agile Soft- ware Developer, Wiley, 2003.\n\nAstels, David. Test-Driven Development: A Practical Guide, Prentice Hall, 2003.\n\nBach, James. “Exploratory Testing Explained,” www.satisﬁce.com/articles/ et-article.pdf, 2003.\n\nBach, Jonathan. “Session-Based Test Management,” Software Testing and Quality Engineering Magazine, November, 2000, www.satisﬁce.com/articles/ sbtm.pdf.\n\nBeck, Kent. Extreme Programming Explained: Embrace Change, Addison- Wesley, 2000.\n\nBeck, Kent, and Andres, Cynthia. Extreme Programming Explained: Embrace Change. 2nd Edition, Addison-Wesley, 2004.\n\nBerczuk, Stephen and Brad Appleton. Software Conﬁguration Management Patterns: Effective Teamwork, Practical Integration, Addison-Wesley, 2003.\n\n501",
      "page_number": 536
    },
    {
      "number": 65,
      "title": "Segment 65 (pages 544-551)",
      "start_page": 544,
      "end_page": 551,
      "detection_method": "topic_boundary",
      "content": "502\n\nBIBLIOGRAPHY\n\nBolton, Michael. “Testing Without a Map,” Better Software, January 2005, www.developsense.com/articles/Testing%20Without%20A%20Map.pdf.\n\nBos, Erik and Christ Vriens. “An Agile CMM,” in Extreme Programming and Agile Methods–XP/Agile Universe 2004, 4th Conference on Extreme Pro- gramming and Agile Methods, Calgary, Canada, August 15–18, 2004, Pro- ceedings, ed. Carmen Zannier, Hakan Erdogmus, Lowell Lindstrom, pp. 129–138, Springer, 2004.\n\nBoutelle, Jonathan. “Usability Testing for Agile Development,” www .jonathanboutelle.com/mt/archives/2005/08/usability_testi_1.html, 2005.\n\nBrown, Titus. “The (Lack of) Testing Death Spiral,” http://ivory.idyll.org/ blog/mar-08/software-quality-death-spiral.html, 2008.\n\nBuwalda, Hans. “Soap Opera Testing,” Better Software Magazine, February 2004, www.logigear.com/resources/articles_lg/soap_opera_testing.asp.\n\nClark, Mike. Pragmatic Project Automation: How to Build, Deploy and Moni- tor Java Apps, The Pragmatic Programmers, 2004.\n\nCohn, Mike. User Stories Applied for Agile Software Development, Addison- Wesley, 2004.\n\nCohn, Mike. Agile Estimating and Planning, Prentice Hall, 2005.\n\nCrispin, Lisa and Tip House. Testing Extreme Programming, Addison-Wesley 2002.\n\nCrispin, Lisa. Articles “Hiring an Agile Tester,” “An Agile Tool Selection Strat- egy for Web Testing Tools,” “Driving Software Quality: How Test-Driven De- velopment Impacts Software Quality,” http://lisa.crispin.home.att.net.\n\nDeMarco, Tom and Timothy Lister. Managing Risk on Software Projects, Dor- set House, 2003.\n\nDerby, Esther and Larsen, Diana. Agile Retrospectives: Making Good Teams Great, Pragmatic Bookshelf, 2006.\n\nDerby, Esther and Rothman, Johanna. Behind Closed Doors: Secrets of Great Management, Pragmatic Bookshelf, 2006.\n\nBIBLIOGRAPHY\n\nDe Souza, Ken. “A tester in developer’s clothes” blog, http://kendesouza .blogspot.com.\n\nDustin, Elfriede, Chris Wysopal, Lucas Nelson, and Dino Dia Zovi. The Art of Software Security Testing: Identifying Software Security Flaws, Symantec Press, 2006.\n\nDustin, Elfriede. “Teamwork Tackles the Quality Goal,” Software Test & Per- formance, Volume 2, Issue 200, March 2005.\n\nDuvall, Paul, Steve Matyas, and Andrew Glover. Continuous Integration: Im- proving Software Quality and Reducing Risk, Addison-Wesley, 2007.\n\nEckstein, Jutta. Agile Software Development in the Large: Diving Into the Deep, Dorset House, 2004.\n\nEvans, Eric. Domain-Driven Design: Tackling Complexity in the Heart of Soft- ware, Addison-Wesley, 2003.\n\nFeathers, Michael. Working Effectively with Legacy Code, Prentice Hall, 2004.\n\nFreeman, Steve and Nat Pryce. “Mock Objects,” www.mockobjects.com.\n\nFowler, Martin. “Continuous Integration,” http://martinfowler.com/articles/ continuousIntegration.html, 2006.\n\nFowler, Martin. “StranglerApplication,” www.martinfowler.com/bliki/ StranglerApplication.html, 2004.\n\nFowler, Martin, “TechnicalDebt,” http://martinfowler.com/bliki/ TechnicalDebt.html, 2003.\n\nGårtner, Markus, Blog, http://blog.shino.de.\n\nGalen, Robert. Software Endgames: Eliminating Defects, Controlling Change, and the Countdown to On-Time Delivery, Dorset House, 2005.\n\nGhiorghiu, Grig. “Performance vs. load vs. stress testing,” http:// agiletesting.blogspot.com/2005/02/performance-vs-load-vs-stress- testing.html, 2005.\n\nGhirghiu, Grig. “Agile Testing” blog, http://agiletesting.blogspot.com.\n\n503\n\n504\n\nBIBLIOGRAPHY\n\nHagar, Jon. Software Testing Papers, www.swtesting.com/hagar_papers_ index.html.\n\nHendrickson, Elisabeth. “Tester Developers, Developer Testers,” http:// testobsessed.com/2007/01/17/tester-developers-developer-testers/, 2007.\n\nHendrickson, Elisabeth. “Test Heuristics Cheat Sheet,” http://testobsessed.com/ wordpress/wp-content/uploads/2007/02/testheuristicscheatsheetv1.pdf, 2007.\n\nHendrickson, Elisabeth. “Agile-Friendly Test Automation Tools/Frame- works,” http://testobsessed.com/2008/04/29/agile-friendly-test-automation- toolsframeworks, 2008.\n\nHighsmith, Jim. Agile Project Management: Creating Innovative Products, Addison-Wesley, 2004.\n\nHunt, Andrew and David Thomas. The Pragmatic Programmer: From Jour- neyman to Master, Addison-Wesley, 1999.\n\nKaner, Cem, James Bach, and Bret Pettichord. Lessons Learned in Software Testing, Wiley, 2001.\n\nKerth, Norman. Project Retrospectives: A Handbook for Team Reviews, Dorset House, 2001.\n\nKniberg, Henrik. “How to Catch Up on Test Automation,” http:// blog.crisp.se/henrikkniberg/2008/01/03/1199386980000.html, 2008.\n\nKniberg, Henrik. Scrum and XP from the Trenches, Lulu.com, 2007.\n\nKoenig, Dierk, Andrew Glover, Paul King, Guillaume Laforge, and Jon Skeet. Groovy in Action, Manning Publications, 2007.\n\nKohl, Jonathan.“Man and Machine,” Better Software magazine, December 2007.\n\nKohl, Jonathan. Blog and articles, www.kohl.ca/.\n\nLouvion, Christophe. Blog, www.runningagile.com.\n\nBIBLIOGRAPHY\n\nManns, Mary Lynn and Linda Rising. Fearless Change: Patterns for Introduc- ing New Ideas, Addison-Wesley, 2004.\n\nMarick, Brian. Everyday Scripting with Ruby: For Teams, Testers and You, Pragmatic Bookshelf, 2007.\n\nMarick, Brian, “My Agile Testing Project,” www.exampler.com/old-blog/ 2003/ 08/21/, 2003.\n\nMarick, Brian. “An Alternative to Business-Facing TDD,” www.exampler .com/ blog/category/aa-ftt, 2008.\n\nMarick, Brian. Blog and articles on agile testing, http://exampler.com.\n\nMarcano, Antony. Blog, www.testingreﬂections.com.\n\nMeszaros, Gerard. XUnit Test Patterns: Refactoring Test Code, Addison- Wesley, 2007.\n\nMeszaros, Gerard and Janice Aston. “Adding Usability Testing to an Agile Project,” Agile 2006, Minneapolis, MN, 2006, http://papers.gerardmeszaros .com/AgileUsabilityPaper.pdf.\n\nMeszaros, Gerard, Ralph Bohnet, and Jennitta Andrea. “Agile Regression Testing Using Record & Playback,” XP/Agile Universe 2003, New Orleans, LA, 2003, http://agileregressiontestpaper.gerardmeszaros.com.\n\nMeszaros, Gerard. “Using Storyotypes to Split Bloated XP Stories,” http:// storyotypespaper.gerardmeszaros.com.\n\nMugridge, Rick and Ward Cunningham. Fit for Developing Software: Frame- work for Integrated Tests, Prentice Hall, 2005.\n\nNewkirk, James and Alexei Vorontsov. Test-Driven Development in Microsoft .NET, Microsoft Professional, 2004.\n\nNielsen, Jakob. “Time Budgets for Usability Sessions,” www.useit.com/ alertbox/usability_sessions.html, 2005.\n\nNorth, Dan. “Introducing BDD,” http://dannorth.net/introducing-bdd, 2006.\n\n505\n\n506\n\nBIBLIOGRAPHY\n\nPatterson, Kerry, Joseph Gernny, Ben McMillan, Al Switzler and Stephen R. Covey. Crucial Conversations: Tools for Talking when the Stakes are High, McGraw-Hill, 2002.\n\nPatton, Jeff. “Test Software Before You Code,” StickyMinds.com, August 2006, www.stickyminds.com/sitewide.asp?Function=edetail&ObjectType= COL&ObjectId=11104.\n\nPatton, Jeff. “Holistic Agile Product Design and Development,” www.ag- ileproductdesign.com/blog/agile_product_development.html, 2006.\n\nPols, Andy. “The Perfect Customer,” www.pols.co.uk/archives/category/ testing, 2008.\n\nPettichord, Bret. “Homebrew Test Automation,” www.io.com/~wazmo/ papers/homebrew_test_automation_200409.pdf, 2004.\n\nPettichord, Bret. “Seven Steps to Test Automation Success,” www.io .com/ ~wazmo/papers/seven_steps.html, 2001.\n\nPoppendieck, Mary and Tom Poppendieck. Implementing Lean Software Development: From Concept to Cash, Addison-Wesley, 2006.\n\nPoppendieck, Mary and Tom Poppendieck. Lean Software Development: An Agile Toolkit, Addison-Wesley, 2003.\n\nRainsberger, J. B. JUnit Recipes: Practical Methods for Programmer Testing, Manning Publications, 2004.\n\nRasmusson, Jonathan. “Introducing XP into Greenﬁeld Projects: Lessons Learned,” IEEE Software, 2003, http://rasmusson.ﬁles.wordpress.com/2008/ 01/s3021.pdf.\n\nRobbins, Stephen and Tim Judge. Essentials of Organizational Behavior, 9th Edition, Prentice Hall, 2007.\n\nSchwaber, Ken. Agile Project Management with Scrum, Microsoft Press, 2004.\n\nShore, James and Shane Warden. The Art of Agile Development, O’Reilly Me- dia, 2007.\n\nBIBLIOGRAPHY\n\nSoni, Mukesh. “Defect Prevention: Reducing Costs and Enhancing Quality,” iSixSigma, http://software.isixsigma.com/library/content/c060719b.asp.\n\nSumrell, Megan. “’Shout-Out’ Shoebox – Boosting Team Morale,” http:// megansumrell.wordpress.com/2007/08/27/shout-out-shoebox-boosting- team-morale, 2007.\n\nSutherland, Jeff, Carsten Ruseng Jakobsen, and Kent Johnson. “Scrum and CMMI Level 5: The Magic Potion for Code Warriors,” Agile 2007, Washington, DC, 2007, http://jeffsutherland.com/scrum/ Sutherland-ScrumCMMI6pages.pdf.\n\nTabaka, Jean. Collaboration Explained: Facilitation Skills for Software Project Leaders, Addison-Wesley, 2006.\n\nThomas, Mike. “Strangling Legacy Code,” Better Software magazine, October 2005, http://samoht.com/wiki_downloads/StranglingLegacyCodeArticle.pdf.\n\nTholfsen, Mike. “The Rise of the Customer Champions,” STAREAST, May 7–9, 2008.\n\nVoris, John. ADEPT AS400 Displays for External Prototyping and Testing, www.AdeptTesting.org.\n\nWake, Bill. “XP Radar Chart,” http://xp123.com/xplor/xp0012b/index.shtml, 2001.\n\nVriens, Christ. “Certifying for CMM Level 2 and ISO9001 with XP@Scrum,” in ADC 2003: Proceedings of the Agile Development Conference, 25–28 June 2003, Salt Lake City, UT, USA, 120–124, IEEE, 2003.\n\nTOOL REFERENCES Abbot Java GUI Test Framework, http://abbot.sourceforge.net/doc/ overview.shtml.\n\nAdzik, Gojko. DbFit: Test-driven Database Development, http://gojko.net/ ﬁtnesse/dbﬁt/.\n\nFaught, Danny. “Test Tools List,” http://testingfaqs.org, 2008.\n\n507\n\n508\n\nBIBLIOGRAPHY\n\nCanoo WebTest, Open Source Tool for Automated Testing of Web Applica- tions, http://webtest.canoo.com.\n\neasyb, Behavior Driven Development Framework for the Java Platform, www.easyb.org/.\n\nFit, Framework for Integrated Test, http://ﬁt.c2.com.\n\nJUnit, Resources for Test-Driven Development, www.junit.org.\n\nJUnitPerf, JUnit Test Decorators for Performance and Scalability Testing, http://clarkware.com/software/JUnitPerf.html.\n\nFitNesse, Fully Integrated Standalone Wiki and Acceptance Testing Frame- work, www.ﬁtnesse.org.\n\nHower, Rick, Software QA and Testing Tools Info, www.softwareqatest.com/ qattls1.html.\n\nNUnit, Unit-testing Framework for .NET Languages, http://nunit.org/ index.php.\n\nOpen Source Software Testing Tools, News and Discussion. www .opensourcetesting.org/.\n\nRpgUnit, RPG Regression Testing Framework, www.RPGunit.org.\n\nSelenium, Web Application Testing System, http://selenium.openqa.org.\n\nsoapUI, Web Services Testing Tool, www.soapui.org.\n\nSource Conﬁguration Management, http://better-scm.berlios.de.\n\nSubversion, Open Source Version Control System, http://subversion.tigris.org/.\n\nUnit Testing Frameworks. http://en.wikipedia.org/wiki/List_of_unit_ testing_frameworks.\n\nWatir, Web Application Testing in Ruby, http://wtr.rubyforge.org, http:// watircraft.com.\n\nINDEX\n\nA Abbot GUI test tool, 127 Acceptance tests. See also Business-facing tests\n\ndeﬁnition, 501 Remote Data Monitoring system example,\n\n245\n\nUAT (user acceptance testing) compared\n\nwith, 130\n\nAd hoc testing, 198 Adaptability, skills and, 39–40 ADEPT (AS400 Displays for External\n\nPrototyping and Testing), 117–118\n\nAdvance clarity\n\ncustomers speaking with one voice,\n\n373–374\n\ndetermining story size, 375–376 gathering all viewpoints regarding\n\nrequirements, 374–375\n\noverview of, 140–142, 373\n\nAdvance preparation downside of, 373 how much needed, 372–373\n\nAgile development\n\nAgile manifesto and, 3–4 barriers to. See Barriers to adopting agile\n\ndevelopment team orientation of, 6\n\nAgile Estimating and Planning (Cohn), 331, 332 Agile manifesto\n\npeople focus, 30 statement of, 4 value statements in, 21\n\nAgile principles. See Principles, for agile\n\ntesters\n\nAgile testers. See also Testers\n\nagile testing mind-set, 482–483 deﬁnition, 4 giving all team members equal weight, 31 hiring, 67–69 what they are, 19–20\n\nAgile testing\n\ndeﬁnition, 6 as mind-set, 20–21 what we mean, 4–7\n\nAgile values, 3–4 Alcea’s FIT IssueTrack, 84 Alpha tests, 466–467 ant, 284\n\nas build tool, 126 continual builds and, 175, 291\n\nAnthillPro, 126 ANTS Proﬁler Pro, 234 Apache JMeter. See JMeter API-layer functional test tools, 168–170\n\nFit and FitNesse, 168–170 overview of, 168 testing web Services, 170\n\nAPI testing\n\nautomating, 282 overview of, 205–206\n\nAPIs (application programming interfaces),\n\n501\n\nAppleton, Brad, 124 Application under test (AUT), 246\n\n509",
      "page_number": 544
    },
    {
      "number": 66,
      "title": "Segment 66 (pages 552-559)",
      "start_page": 552,
      "end_page": 559,
      "detection_method": "topic_boundary",
      "content": "510\n\nINDEX\n\nApplications\n\nintegration testing with external applications,\n\n459\n\nRemote Data Monitoring system example,\n\n242–243\n\nArchitecture\n\nincremental approach to testing, 114 layered, 116 Quadrant 1 tests and, 99 scalability and, 104, 221 testable, 30, 115, 182, 184, 267\n\nAS400 Displays for External Prototyping and\n\nTesting (ADEPT), 117–118\n\nAssumptions, hidden\n\nagile testers response to, 25 failure to detect, 32 questions that uncover, 136 worst-case scenarios and, 334\n\nAttitude\n\nagile testing mind-set, 482–483 barriers to adopting agile development, 48 vs. skills, 20\n\nAudits, compliance with audit requirements,\n\n89–90\n\nAUT (application under test), 143, 225, 246,\n\n317\n\nAuthorization, security testing and, 224 Automated regression testing key success factors, 484 release candidates and, 458 as a safety net, 261–262\n\nAutomated test lists, test plan alternatives,\n\n353–354 Automation\n\ncode ﬂux and, 269 of deployment, 232 driving development with, 262–263 of exploratory testing, 201 fear of, 269–270 feedback from, 262 freeing people for other work, 259–261 of functional test structure, 245–247 home-brewed test, 175 investment required, 267–268\n\nlearning curve, 266–267 legacy code and, 269 maintainability and, 227–228 manual testing vs., 258–259 obstacles to, 264–265 old habits and, 270 overview of, 255 programmers’ attitude regarding,\n\n265–266\n\nreasons for, 257–258 responding to change and, 29 ROI and, 264 task cards and, 394–395 testability and, 149–150 tests as documentation, 263–264\n\nAutomation strategy\n\nagile coding practices and, 303–304 applying one tool at a time, 312–313 data generation tools, 304–305 database access and, 306–310 design and maintenance and, 292–294 developing, 288–289 identifying tool requirements, 311–312 implementing, 316–319 iterative approach, 299–300 keep it simple, 298–299 learning by doing, 303 managing automated tests, 319 multi-layered approach to, 290–292 organizing test results, 322–324 organizing tests, 319–322 overview of, 273 principles, 298 record/playback tools and, 294, 296–297 starting with area of greatest pain,\n\n289–290\n\ntaking time to do it right, 301–303 test automation pyramid, 276–279 test categories, 274–276 tool selection, 294–298, 313–316 understanding purpose of tests and, 310–311 what can be automated, 279–285 what might be difﬁcult to automate,\n\n287–288\n\nwhat should not be automated, 285–287 whole team approach, 300–301\n\nAutomation tools, 164–177\n\nAPI-layer functional test tools, 168–170 builds and, 126 GUI test tools, 170–176 overview of, 164–165 unit-level test tools, 165–168 web services test tool, 170\n\nB Bach, James, 195, 200, 212 Bach, Jonathan, 201 Back-end testing\n\nbehind the GUI, 282 non-UI testing, 204–205\n\nBamboo, 126 Barriers to adopting agile development, 44–49\n\nconﬂicting or multiple roles, 45 cultural differences among roles, 48–49 lack of training, 45 lack of understanding of agile concepts,\n\n45–48\n\nloss of identity, 44–45 overview of, 44 past experience and attitudes, 48\n\nBaselines\n\nbreak-test baseline technique, 363 performance, 235–237\n\nBatch\n\nﬁles, 251 processing, 345 scheduling process, 182\n\nBDD (Behavior-driven development)\n\neasyb tool, 166–168 tools for Quadrant 1 tests, 127\n\nBeck, Kent, 26, 99 Benander, Mark, 51 Benchmarking, 237 Berczuk, Stephen, 124 Beta testing, 466–467 Big picture\n\nagile testers focus on, 23 high-level tests and examples, 397–402\n\nINDEX\n\nkey success factors, 490–491 peril of forgetting, 148 regression tests and, 434\n\nBolton, Michael, 195 Bos, Erik, 114 Boundary conditions API testing and, 205 automation and, 11 data generation tools and, 304 identifying test variations, 410 writing test cases for, 137 Boyer, Erika, 140, 163, 372, 432 Brainstorming\n\nautomation giving testers better work,\n\n260\n\nprior to iteration, 370, 381 quadrants as framework for, 253 taking time for, 301 testers, 121\n\nBreak-test baseline technique, 363 Browsers, compatibility testing and, 230 Budget limits, 55 Bug tracking. See Defect tracking Bugs. See Defects Build\n\nautomating, 280–282 challenging release candidate builds, 473 deﬁnition, 501 incremental, 178–179 speeding up, 118–119\n\nBuild automation tools, 126, 282 Build/Operate/Check pattern, 180 Build tools, 126 BuildBeat, 126 Business analysts, 374 Business expert role\n\nagreement regarding requirements, 428,\n\n430\n\ncommon language and, 134, 291, 414 on customer team, 6–7 iteration demo and, 443 language of, 291 Power of Three and, 482 tools geared to, 134\n\n511\n\n512\n\nINDEX\n\nBusiness-facing tests agile testing as, 6 Quadrants 2 & 3, 97–98 technology-facing tests compared with, 120\n\nBusiness-facing tests, critiquing the product\n\n(Quadrant 3), 189–215\n\nacceptance tests, 245 API testing, 205–206 demonstrations, 191–192 emulator tools, 213–214 end-to-end tests, 249–250 exploratory testing, 195–202, 248–249 generating test data, 212 GUI testing, 204 monitoring tools, 212–213 overview of, 189–191 reports, 208–210 scenario testing, 192–195 session-based testing, 200–201 setting up tests, 211–212 simulator tools, 213 tools for exploratory testing, 210–211 usability testing, 202–204 user acceptance testing, 250 user documentation, 207–208 web services testing, 207\n\nBusiness-facing tests, supporting team\n\n(Quadrant 2), 129–151 advance clarity, 140–142 automating functional tests, 245–247 common language and, 134–135 conditions of satisfaction and, 142–143 doneness, 146–147 driving development with, 129–132 eliciting requirements, 135–140 embedded testing, 248 incremental approach, 144–146 requirements quandary and, 132–134 ripple effects, 143–144 risk mitigation and, 147–149 testability and automation, 149–150 toolkit for. See Toolkit (Quadrant 2) web services testing, 247–248\n\nBusiness impact, 475–476\n\nBusiness value\n\nadding value, 31–33 as goal of agile development, 5–8, 69, 454 metrics and, 75 release cycles and, 3 role, function, business value pattern, 155 team approach and, 16\n\nBusse, Mike, 106, 235, 284, 313 Buwalda, Hans, 193\n\nC Canonical data, automating databases and,\n\n308–309 Canoo WebTest\n\nautomating GUI tests, 184, 186 GUI regression test suite, 291 GUI smoke tests, 300 GUI test tools, 174–175 organizing tests and, 320 scripts and, 320 XML Editor for, 125\n\nCapability Maturity Model Integration\n\n(CMMI), 90–91 Capture-playback tool, 267 Celebrating successes\n\nchange implementation and, 50–52 iteration wrap up and, 449–451\n\nChandra, Apurva, 377 Chang, Tae, 53–54 Change\n\ncelebrating successes, 50–52 giving team ownership, 50 introducing, 49 not coming easy, 56–57 responsiveness to, 28–29 talking about fears, 49–50\n\nChecklists\n\nrelease readiness, 474 tools for eliciting examples and requirements,\n\n156\n\nCI. See Continuous integration (CI) CI Factory, 126 CMMI (Capability Maturity Model\n\nIntegration), 90–91\n\nCo-location, team logistics and, 65–66 Coaches\n\nadjusting to agile culture and, 40 learning curve and, 266 providing encouragement, 69 skill development and, 122 training and, 45–46 Cockburn, Alistair, 115 Code\n\nautomation and code ﬂux, 269 automation and legacy code, 269 automation strategy and, 303–304 documentation of, 251 standards, 227 writing testable, 115\n\nCode coverage, release metrics, 360–364 Coding and testing, 405–441 adding complexity, 407 alternatives for dealing with bugs, 424–428 choosing when to ﬁx bugs, 421–423 collaborating with programmers, 413–414 dealing with bugs, 416–419 deciding which bugs to log, 420–421 driving development and, 406 facilitating communication, 429–432 focusing on one story, 411–412 identifying variations, 410 iteration metrics, 435–440 media for logging bugs, 423–424 overview of, 405 Power of Three for resolving differences in\n\nviewpoint, 411\n\nregression testing and, 432–434 resources, 434–435 risk assessment, 407–409 as simultaneous process, 409–410,\n\n488–489\n\nstarting simple, 406, 428–429 talking to customers, 414–415 tests that critique the product, 412–413\n\nCohn, Mike, 50, 155, 276, 296, 331, 332 Collaboration\n\nwith customers, 396–397 key success factors, 489–490\n\nINDEX\n\nwith programmers, 413–414 whole team approach, 15–16\n\nCollino, Alessandro, 103, 363 Communication\n\ncommon language and, 134–135 with customer, 140, 396–397 DTS (Defect Tracking System) and, 83 facilitating, 23–25, 429–432 product delivery and, 462–463 size as challenge to, 42–43 between teams, 69–70 test results, 357–358\n\nComparisons, automating, 283 Compatibility testing, 229–230 Component tests\n\nautomating, 282 deﬁnition, 501 supporting function of, 5\n\nConditions of satisfaction\n\nbusiness-facing tests and, 142–143 deﬁnition, 501–502 Context-driven testing\n\ndeﬁnition, 502 quadrants and, 106–107 Continuous build process\n\nfailure notiﬁcation and, 112 feedback and, 119 FitNesse tests and, 357 implementing, 114 integrating tools with, 175, 311 source code control and, 124 what testers can do, 121\n\nContinuous feedback principle, 22 Continuous improvement principle, 27–28 Continuous integration (CI)\n\nautomating, 280–282 as core practice, 486–487 installability and, 231–232 Remote Data Monitoring system example, 244 running tests and, 111–112\n\nConversion, data migration and, 460–461 Core practices\n\ncoding and testing as one process, 488–489 continuous integration, 486–487\n\n513\n\n514\n\nINDEX\n\nCore practices, continued\n\nincremental approach, 488 overview of, 486 synergy between practices, 489 technical debt management, 487–488 test environments, 487\n\nCourage, principles, 25–26, 71 Credibility, building, 57 Critiquing the product\n\nbusiness facing tests. See Business-facing tests, critiquing the product (Quadrant 3)\n\ntechnology-facing tests. See\n\nTechnology-facing tests, critiquing the product (Quadrant 4) CrossCheck, testing Web Services, 170 CruiseControl, 126, 244, 291 Cultural change, 37. See also Organizations Cunningham, Ward, 106, 168, 506 Customer expectations\n\nbusiness impact and, 475–476 production support, 475\n\nCustomer-facing test. See Business-facing tests Customer support, DTS (Defect Tracking\n\nSystem) and, 82\n\nCustomer team\n\ndeﬁnition, 502 interaction between customer and developer\n\nteams, 8 overview of, 7 Customer testing\n\nAlpha/Beta testing, 466–467 deﬁnition, 502 overview of, 464 UAT (user acceptance testing), 464–466\n\nCustomers\n\ncollaborating with, 396–397, 489–490 considering all viewpoints during iteration\n\nplanning, 388–389 delivering value to, 22–23 importance of communicating with, 140,\n\n414–415, 444\n\niteration demo, 191–192, 443–444 participation in iteration planning,384–385\n\nrelationship with, 41–42 reviewing high-level tests with, 400 speaking with one voice, 373–374\n\nCVS, source code control and, 124\n\nD Data\n\nautomating creation or setup, 284–285 cleanup, 461 conversion, 459–461 release planning and, 348 writing task cards and, 392\n\nData-driven tests, 182–183 Data feeds, testing, 249 Data generation tools, 304–305 Data migration, automating, 310, 460 Databases\n\navoiding access when running tests, 306–310 canonical data and automation, 308–309 maintainability and, 228 product delivery and updates, 459–461 production-like data and automation,\n\n309–310\n\nsetting up/tearing down data for each\n\nautomated test, 307–308\n\ntesting data migration, 310\n\nDe Souza, Ken, 223 Deadlines, scope and, 340–341 Defect metrics\n\noverview of, 437–440 release metrics, 364–366\n\nDefect tracking, 79–86\n\nDTS (Defect Tracking System), 79–83 keeping focus and, 85–86 overview of, 79 reasons for, 79 tools for, 83–85\n\nDefect Tracking System. See DTS (Defect\n\nTracking System)\n\nDefects\n\nalternatives for dealing with bugs,\n\n424–428\n\nchoosing when to ﬁx bugs, 421–423\n\ndealing with bugs, 416–419 deciding which bugs to log, 420–421 media for logging bugs, 423–424 metrics and, 79 TDD (test-driven development) and, 490 writing task cards and, 391–392 zero bug tolerance, 79, 418–419\n\nDeliverables\n\n“ﬁt and ﬁnish” deliverables, 454 nonsoftware, 470 overview of, 468–470\n\nDelivering product\n\nAlpha/Beta testing, 466–467 business impact and, 475–476 communication and, 462–463 customer expectations, 475 customer testing, 464 data conversion and database updates, 459–461 deliverables, 468–470 end game, 456–457 installation testing, 461–462 integration with external applications, 459 nonfunctional testing and, 458–459 overview of, 453 packaging, 474–475 planning time for testing, 455–456 post-development testing cycles, 467–468 production support, 475 release acceptance criteria, 470–473 release management, 470, 474 releasing product, 470 staging environment and, 458 testing release candidates, 458 UAT (user acceptance testing), 464–466 what if it is not ready, 463–464 what makes a product, 453–455\n\nDemos/demonstrations\n\nof an iteration, 443–444 value to customers, 191–192 Deployment, automating, 280–282 Design\n\nautomation strategy and, 292–294 designing with testing in mind, 115–118\n\nINDEX\n\nDetailed test cases\n\nart and science of writing, 178 big picture approach and, 148–149 designing with, 401\n\nDeveloper team\n\ninteraction between customer and\n\ndeveloper teams, 8\n\noverview of, 7–8\n\nDevelopment\n\nagile development, 3–4, 6 automated tests driving, 262–263 business-facing tests driving,\n\n129–132 coding driving, 406 post-development testing cycles,\n\n467–468\n\nDevelopment spikes, 381 Development team, 502 diff tool, 283 Distributed teams, 431–432\n\ndefect tracing systems, and, 82 physical logistics, 66 online high level tests for, 399 online story board for, 357 responding to change, 29 software-based tools to elicit examples and\n\nrequirements, and, 163–164\n\nDocumentation\n\nautomated tests as source of, 263–264 problems and ﬁxes, 417 reports, 208–210 of test code, 251 tests as, 402 user documentation, 207–208\n\nDoneness\n\nknowing when a story is done, 104–105 multitiered, 471–472\n\nDriving development with tests. See TDD\n\n(test-driven development) DTS (Defect Tracking System), 80–83\n\nbeneﬁts of, 80–82 choosing media for logging bugs, 424 documenting problems and ﬁxes, 417\n\n515\n\n516\n\nINDEX\n\nDTS (Defect Tracking System), continued\n\nlogging bugs and, 420 reason for not using, 82–83\n\nDymond, Robin, xxx Dynamic analysis, security testing tools, 225\n\nE easyb behavior-driven development tool, 165–168 EasyMock, 127 Eclipse, 125, 316 Edge cases\n\nidentifying variations, 410 not having time for, 112 starting simple and then adding complexity,\n\n406–407 test cases for, 137\n\nEmbedded system, Remote Data Monitoring\n\nexample, 248\n\nEmpowerment, of teams, 44 Emulator tools, 213–214 End game\n\nAgile testing, 91 iteration, 14 product delivery and, 456–457 release and, 327\n\nEnd-to-end tests, 249–250 Enjoyment, principle of, 31 Environment, test environment, 347–348 Epic. See also Themes deﬁnition, 502 features becoming, 502 iterations in, 76, 329 planning, 252\n\nePlan Services, Inc., xli, 267 Errors, manual testing and, 259 Estimating story size, 332–338 eValid, 234 Event-based patterns, test design patterns, 181 Everyday Scripting with Ruby for Teams, Testers,\n\nand You (Marick), 297, 303\n\nExample-driven development, 378–380 Examples\n\nfor eliciting requirements, 136–137 tools for eliciting examples and requirements,\n\n155–156\n\nExecutable tests, 406 Exploratory testing (ET)\n\nactivities, characteristics, and skills (Hagar),\n\n198–200\n\nattributes of exploratory tester,\n\n201–202 automation of, 201 deﬁnition, 502–503 end game and, 457 explained (Bolton), 195–198 manual testing and, 280 monitoring tools, 212 overview of, 26, 195 Remote Data Monitoring system example,\n\n248–249\n\nsession-based testing and, 200–201 setup, 211–212 simulators and emulators, 212–213 tests that critique the product, 412–413 tools for, 210–212 tools for generating test data, 212 what should not be automated, 286\n\nExternal quality, business facing tests deﬁning,\n\n99, 131\n\nExternal teams, 43, 457 Extreme Programming. See XP (Extreme\n\nProgramming)\n\nExtreme Programming Explained (Beck),\n\n26\n\nF Face-to-face communication, 23–25 Failover tests, 232 Failure, courage to learn from, 25 Fake objects, 115, 118, 306, 502–503 Fault tolerance, product delivery and, 459 Fear\n\nbarriers to automation, 269–270 change and, 49–50\n\nFearless Change (Manns and Rising), 121 Feathers, Michael, 117, 288 Features\n\ndefects vs., 417–418 deﬁnition, 502–503 focusing on value, 341\n\nFeedback\n\nautomated tests providing, 262 continuous feedback principle, 22 iterative approach and, 299–300 key success factors, 484–486 managing tests for, 323–324 Quadrant 1 tests and, 118–119 “Fit and ﬁnish” deliverables, 454 Fit (Framework for Integrated Test),\n\n134–135\n\nAPI-layer functional test tools, 168–169 automation test pyramid and, 278\n\nFIT IssueTrack, Alcea, 83–84 FitNesse\n\nadvantages of, 163 API-layer functional test tools, 169–170 automating functional tests with, 30,\n\n145\n\nbusiness-facing tests with, 154, 178 collaboration and, 164 continual builds and, 119, 357 data veriﬁcation with, 287 doneness and, 472 encouraging use of, 122 examples and, 136, 169 feedback and, 323–324 ﬁle parsing rules illustrated with, 205 functional testing behind the GUI, 291,\n\n300\n\nhome-grown scripts and, 305 JUnit compared with, 299 keywords or actions words for automating\n\ntests, 182–183\n\nmanual vs. automated testing, 210 memory demands of, 306 organizing tests and, 319–320 overview of, 168–170 remote testing and, 432 “start, stop, continue” list, 446 support for source code control tools,\n\n320\n\ntest automation pyramid and, 278 test cards and, 389–390 test cases as documentation, 402 test design and maintenance, 292\n\nINDEX\n\ntesting database layer with, 284 testing stories, 395 traceability requirements and, 88 user acceptance testing, 295 wikis and, 186\n\nFleisch, Patrick, 377, 440 Flow diagrams\n\nscenario testing and, 194–195 tools for eliciting examples and requirements,\n\n160–163 Fowler, Martin, 117 Framework for Integrated Test. See Fit\n\n(Framework for Integrated Test)\n\nFrameworks, 90–93 ftptt, 234 Functional analysts, 386 Functional testing\n\ncompatibility issues and, 230 deﬁnition, 502–503 end-to-end tests, 249–250 layers, 246 nonfunctional tests compared with, 225 Remote Data Monitoring system example,\n\n245–247\n\nG Galen, Bob, 455–456, 471 Gärtner, Markus, 395, 476 Geographically dispersed teams\n\ncoping with, 376–378 facilitating communication and,\n\n431–432\n\nGheorghiu, Grig, 225–226, 234 Glover, Andrew, 166 Greenﬁeld projects\n\ncode testing and, 116 deﬁnition, 502–503\n\nGUI (graphical user interface)\n\nautomation strategy and, 293 code ﬂux and, 269 standards, 227 GUI smoke tests\n\nCanoo WebTest and, 300 continual builds and, 119 defect metrics, 437\n\n517",
      "page_number": 552
    },
    {
      "number": 67,
      "title": "Segment 67 (pages 560-567)",
      "start_page": 560,
      "end_page": 567,
      "detection_method": "topic_boundary",
      "content": "518\n\nINDEX\n\nGUI test tools, 170–176\n\nCanoo Web Test, 174–175 “home-brewed” test automation tools,\n\n175\n\nopen source test tools, 172 overview of, 170–171 record/playback tools, 171–172 Ruby with Watir, 172–174 Selenium, 174\n\nGUI testing\n\nAPI testing, 205–206 automating, 282–283, 295–296 automation test pyramid and, 278 GUI smoke tests, 119, 300, 437 overview of, 204 Web service testing, 207\n\nH Hagar, Jon, 198 Hardware\n\ncompatibility and, 229 cost of test environments, 487 functional testing and, 230 investing in automation and, 267 production environment and, 310 scalability and, 233 test infrastructure, 319 testing product installation, 462 Hendrickson, Elisabeth, 203, 315–316 High-level test cases, 397–402\n\nmockups, 398–399 overview of, 397–398 reviewing with customers, 400 reviewing with programmers,\n\n400–401\n\ntest cases as documentation, 402\n\nHiring a tester, 67–69 Holzer, Jason, 220, 448 Home-grown test tool\n\nautomation tools, 314 GUI test tools, 175 test results, 323\n\nhttperf, 234 Hudson, 126\n\nI IBM Rational ClearCase, 124 IDEs (Integrated Development Environments)\n\ndeﬁnition, 502–503 log analysis tools, 212 tools for Quadrant 1 tests, 124–126\n\n“ility” testing\n\ncompatibility testing, 229–230 installability testing, 231–232 interoperability testing, 228–229 maintainability testing, 227–228 reliability testing, 230–231, 250–251 security testing, 223–227\n\nImpact, system-wide, 342 Implementing Lean Software Development: From Concept to Cash (Poppendieck), 74, 416\n\nImprovement\n\napproach to process improvement, 448–449 continuous improvement principle, 27–28 ideas for improvement from retrospectives,\n\n447–449\n\nIncremental development\n\nbuilding tests incrementally, 178–179 as core practice, 488 “ilities” tests and, 232 thin slices, small chunks, 144–146 traditional vs. agile testing, 12–13\n\nIndex cards, logging bugs on, 423 Infrastructure\n\nQuadrant 1 tests, 111–112 test infrastructure, 319 test plans and, 346–347 Installability testing, 231–232 Installation testing, 461–462 Integrated Development Environments. See\n\nIDEs (Integrated Development Environments) Integration testing\n\ninteroperability and, 229 product and external applications, 459\n\nIntelliJ IDEA, 125 Internal quality\n\nmeasuring internal quality of code, 99 meeting team standards, 366\n\nQuadrant 1 tests and, 111 speed and, 112\n\nInteroperability testing, 228–229 Investment, automation requiring,\n\n267–268\n\nIteration\n\nautomation strategy and, 299–300 deﬁnition, 502–503 demo, 443–444 life of a tester and, 327 pre-iteration activities. See Pre-iteration\n\nactivities\n\nprioritizing stories and, 338 review, 415, 435–437 traditional vs. agile testing, 12–13\n\nIteration kickoff, 383–403\n\ncollaboration with customers, 396–397 considering all viewpoints, 385–389 controlling workload, 393 high-level tests and examples, 397–402 iteration planning, 383–384 learning project details, 384–385 overview of, 383 testable stories, 393–396 writing task cards, 389–392\n\nIteration metrics, 435–440 defect metrics, 437–440 measuring progress with, 435–437 overview of, 435 usefulness of, 439–440\n\nIteration planning\n\nconsidering all viewpoints, 385–389 controlling workload, 393 learning project details, 384–385 overview of, 383–384 writing task cards, 389–392 Iteration review meeting, 415 Iteration wrap up, 443–451\n\ncelebrating successes, 449–451 demo of iteration, 443–444 ideas for improvement, 447–449 retrospectives, 444–445 “start, stop, continue” exercise for retrospectives, 445–447\n\nINDEX\n\nITIL (Information Technology Infrastructure\n\nLibrary), 90–91\n\nJ JBehave, 165 JConsole, 234 JMeter\n\nperformance baseline tests, 235 performance testing, 223, 234, 313\n\nJMS (Java Messaging Service)\n\ndeﬁnition, 502–503 integration with external applications and,\n\n243\n\ntesting data feeds and, 249\n\nJProﬁler, 234 JUnit\n\nFitNesse as alternative for TDD, 299 functional testing, 176 load testing tools, 234–235 unit test tools, 126, 165, 291\n\nJUnitPerf, 234 Just in time development, 369. See also\n\nPre-iteration activities\n\nK Key success factors\n\nagile testing mind-set, 482–483 automating regression testing, 484 big picture approach, 490–491 coding and testing as one process, 488–489 collaboration with customers, 489–490 continuous integration (CI), 486–487 feedback, 484–486 foundation of core practices, 486 incremental approach (thin slices, small\n\nchunks), 488 overview of, 481 synergy between practices, 489 technical debt management, 487–488 test environments, 487 whole team approach, 482 Keyword-driven tests, 182–183 King, Joseph, 176 Knowledge base, DTS, 80–81\n\n519\n\n520\n\nINDEX\n\nKohl, Jonathan, 201, 204, 211 König, Dierk, 320\n\nL Language, need for common, 134–135 Layered architecture, 116 Lean measurements, metrics, 74–75 Learning\n\nautomation strategy and, 303 continuous improvement principle, 27\n\nLearning curve, automation and, 266–267, 303 Legacy code, 269 Legacy code rescue (Feathers), 117 Legacy systems ccde, 269 deﬁnition, 502–503 logging bugs and, 421 testing, 117\n\nLessons Learned in Software Testing (Pettichord),\n\n485\n\nLessons learned sessions, 383. See also\n\nRetrospectives\n\nLightweight processes, 73–74 Lightweight test plans, 350 Load testing. See Performance and load testing LoadRunner, 234 LoadTest, 234 Logistics, physical, 65–66 LogWatch tool, 212 Loss of identity, QA teams fearing, 44–45 Louvion, Christophe, 63\n\nM Maintainability testing, 227–228 Management, 52–55\n\nadvance clarity and, 373–374 cultural change and, 52–54 overview of, 52 providing metrics to, 440\n\nManagers\n\ncultural changes for, 52–54 how to inﬂuence testing, 122–123 speaking managerís language, 55\n\nManns, Mary Lynn, 121–122\n\nManual testing\n\nautomation vs., 258–259 peril of, 289\n\nMarcano, Antony, 83, 426 Marick, Brian, 5, 24, 97, 134, 170, 203, 303 Martin, Micah, 169 Martin, Robert C., 169 Matrices\n\nhigh-level tests and, 398–399 text matrices, 350–353\n\nMaven, 126 McMahon, Chris, 260 Mean time between failure, reliability testing,\n\n230\n\nMean time to failure, reliability testing, 230 Media, for logging bugs, 423–424 Meetings\n\ndemonstrations, 71, 192 geographically dispersed, 376 iteration kickoff, 372 iteration planning, 23–24, 244, 331, 384, 389 iteration review, 71, 415 pre-planning, 370–372 release planning, 338, 345 retrospective, 447 scheduling, 70 sizing process and, 336–337 standup, 177, 429, 462 team participation and, 32 test planning, 263 Memory leaks, 237–238 Memory management testing, 237–238 Meszaros, Gerald, 99, 111, 113, 138, 146, 182,\n\n204, 291, 296, 430\n\nMetrics, 74–79\n\ncode coverage, 360–364 communication of, 77–78 defect metrics, 364–366, 437–440 iteration metrics, 435–440 justifying investment in automation, 268 lean measurements, 74–75 overview of, 74 passing tests, 358–360 reasons for tracking defects, 52, 75–77, 82\n\nrelease metrics, 358 ROI and, 78–79 what not to do with, 77 XP radar charts, 47–48\n\nMilestones, celebrating successes, 449–450 MIME (Multipurpose Internet Mail\n\nExtensions) deﬁnition, 504 testing data feeds and, 249\n\nMind maps, 156–158 Mind-set\n\nagile testing as, 20–21 key success factors, 482–483 pro-active, 369–370\n\n“Mini-waterfall” phenomenon, 46–47 Mock objects\n\ndeﬁnition, 504 risk alleviation and, 459 tools for implementing, 127 unit tests and, 114\n\nMock-ups\n\nfacilitating communication and, 430 high-level tests and, 398–399 stories and, 380 tools for eliciting examples and requirements,\n\n160\n\nModel-driven development, 398 Models\n\nquality models, 90–93 UI modeling example, 399 Monitoring tools, 212–213, 235 Multi-layered approach, automation strategy,\n\n290–292\n\nMultipurpose Internet Mail Extensions\n\n(MIME) deﬁnition, 504 testing data feeds and, 249\n\nN Naming conventions, 227 Nant, 126 Navigation, usability testing and, 204 NBehave, 165 NeoLoad, 234\n\nINDEX\n\nNessus, vulnerability scanner, 226 .NET Memory Proﬁler, 234 NetBeans, 125 NetScout, 235 Non-functional testing. See also\n\nTechnology-facing tests, critiquing the product (Quadrant 4)\n\ndelivering product and, 458–459 functional testing compared with, 225 requirements, 218–219 when to perform, 222\n\nNorth, Dan, 165 NSpec, 165 NUnit, 126, 165\n\nO Oleszkiewicz, Jakub, 418 One-off tests, 286–287 Open source tools\n\nagile open source test tools, 172–175 automation and, 314–315 GUI test tools, 172 IDEs, 124–125 OpenWebLoad, 234 Operating systems (OSs), compatibility testing\n\nand, 230\n\nOrganizations, 37–44\n\nchallenges of agile development, 35 conﬂicting cultures, 43 customer relationships and, 41–42 overview of, 37–38 quality philosophy, 38–40 size and, 42–43 sustainable pace of testing and, 40–41 team empowerment, 44\n\nOSs (operating systems), compatibility testing\n\nand, 230\n\nOwnership, giving team ownership, 50\n\nP Packaging, product delivery and, 474–475 Pair programming\n\ncode review and, 227 developers trained in, 61\n\n521\n\n522\n\nINDEX\n\nPair programming, continued\n\nIDEs and, 125 team approach and, 244\n\nPair testing, 413 Passing tests, release metrics, 358–360 PerfMon, 235 Perforce, 124 Performance and load testing\n\nautomating, 283 baselines, 235–237 memory management testing, 237–238 overview of, 234 product delivery and, 458 scalability testing, 233–234 test environment, 237 tools for, 234–235 when to perform, 223 who performs the test, 220–221 Performance, rewards and, 70–71 Perils\n\nforgetting the big picture, 148 quality police mentality, 39 the testing crunch, 416 waiting for Tuesdayís build, 280 youíre not really part of the team, 32\n\nPerkins, Steve, 156, 159, 373 PerlClip\n\ndata generation tools, 305 tools for generating test data, 212\n\nPersona testing, 202–204 Pettichord, Bret, 175, 264, 485 Phased and gated development, 73–74, 129 Physical logistics, 65–66 Planning\n\nadvance, 43 iteration. See Iteration planning release/theme planning. See Release planning testing. See Test planning\n\nPMO (Project Management Ofﬁce), 440 Pols, Andy, 134 Ports and Adapters pattern (Cockburn), 115 Post-development testing, 467–468 Post-iteration bugs, 421 Pounder, 234\n\nPower of Three\n\nbusiness expert and, 482 ﬁnding a common language, 430 good communication and, 33, 490 problem solving and, 24 resolving differences in viewpoint, 401, 411 whole team approach and, 482 Pragmatic Project Automation, 260 Pre-iteration activities, 369–382\n\nadvance clarity, 373 beneﬁts of working on stories in advance,\n\n370–372\n\ncustomers speaking with one voice, 373–374 determining story size, 375–376 evaluating amount of advance preparation\n\nneeded, 372–373\n\nexamples, 378–380 gathering all viewpoints regarding\n\nrequirements, 374–375\n\ngeographically dispersed team and,\n\n376–378 overview of, 369 prioritizing defects, 381 pro-active mindset, 369–370 resources, 381 test strategies and, 380–381 Pre-planning meeting, 370–372 Principles, automation\n\nagile coding practices, 303–304 iterative approach, 299–300 keep it simple, 298–299 learning by doing, 303 overview of, 298 taking time to do it right, 301–303 whole team approach, 300–301\n\nPrinciples, for agile testers continuous feedback, 22 continuous improvement, 27–28 courage, 25–26 delivering value to customer, 22–23 enjoyment, 31 face-to-face communication, 23–25 keeping it simple, 26–27 overview of, 21–22\n\npeople focus, 30 responsive to change, 28–29 self-organizing, 29–30 Prioritizing defects, 381 Prioritizing stories, 338–340 Pro-active mindset, 369–370 Product\n\nbusiness value, 31–33 delivery. See Delivering product tests that critique (Q3 & Q4), 101–104 what makes a product, 453–455\n\nProduct owner\n\nconsidering all viewpoints during iteration\n\nplanning, 386–389\n\ndeﬁnition, 504 iteration planning and, 384 Scrum roles, 141, 373 tools geared to, 134\n\nProduction\n\nlogging bugs and, 421 support, 475 Production code\n\nautomation test pyramid and, 277–278 deﬁnition, 504 delivering value to, 70 programmers writing, 48 source code control and, 434 synchronization with testing, 322 test-ﬁrst development and, 113 tests supporting, 303–304\n\nProduction-like data, automating databases\n\nand, 309–310\n\nProfessional development, 57 Proﬁling tools, 234 Programmers\n\nattitude regarding automation, 265–266 big picture tests, 397 collaboration with, 413–414 considering all viewpoints during iteration\n\nplanning, 387–389\n\nfacilitating communication and, 429–430 reviewing high-level tests with, 400–401 tester-developer ratio, 66–67 testers compared with, 4, 5\n\nINDEX\n\ntraining, 61 writing task cards and, 391\n\nProject Management Ofﬁce (PMO), 440 Projects, PAS example, 176–177 Prototypes\n\naccessible as common language, 134 mock-ups and, 160 paper, 22, 138–139, 380, 400, 414 paper vs. Wizard of Oz type, 275 UI (user interface), 107\n\nPulse, 126 PyUnit unit test tool for Python, 126\n\nQ QA (quality assurance)\n\ndeﬁnition, 504 in job titles, 31 independent QA team, 60 interchangeable with “test,” 59 whole team approach, 39 working on traditional teams, 9\n\nQuadrant 1. See Technology-facing tests, supporting team (Quadrant 1) Quadrant 2. See Business-facing tests, supporting team (Quadrant 2)\n\nQuadrant 3. See Business-facing tests, critiquing\n\nthe product (Quadrant 3)\n\nQuadrant 4. See Technology-facing tests, critiquing the product (Quadrant 4)\n\nQuadrants\n\nautomation test categories, 274–276 business facing (Q2 & Q3), 97–98 context-driven testing and, 106–108 critiquing the product (Q3 & Q4), 104 managing technical debt, 106 overview of, 97–98 as planning guide, 490 purpose of testing and, 97 Quadrant 1 summary, 99 Quadrant 2 summary, 99–100 Quadrant 3 summary, 101–102 Quadrant 4 summary, 102–104 shared responsibility and, 105–106 story completion and, 104–105\n\n523\n\n524\n\nINDEX\n\nQuadrants, continued\n\nsupporting the team (Q1 & Q2), 100–101 technology facing (Q1 & Q4), 97–98\n\nQuality\n\ncustomer role in setting quality standards, 26 models, 90–93 organizational philosophy regarding,\n\n38–40\n\nQuality assurance. See QA (quality assurance) Quality police mentality, 57 Questions, for eliciting requirements,\n\n135–136\n\nR Radar charts, XP, 47–48 Rasmusson, Jonathan, 11 Record/playback tools\n\nautomation strategy and, 294, 296–297 GUI test tools, 171–172\n\nRecovery testing, 459 Redundancy tests, 232 Reed, David, 171, 377 Refactoring\n\ndeﬁnition, 504 IDEs supporting, 124–126\n\nRegression suite, 434 Regression tests, 432–434\n\nautomated regression tests as a safety net,\n\n261–262\n\nautomating as success factor, 484 checking big picture, 434 deﬁnition, 504 exploratory testing and, 212 keeping the build “green,” 433 keeping the build quick, 433–434 logging bugs and, 420 regression suite and, 434 release candidates and, 458\n\nRelease\n\nacceptance criteria, 470–473 end game, 327, 456–457 management, 474 product delivery, 470 what if it is not ready, 463–464\n\nRelease candidates\n\nchallenging release candidate builds,\n\n473 deﬁnition, 505 testing, 458 Release metrics\n\ncode coverage, 360–364 defect metrics, 364–366 overview of, 358 passing tests, 358–360\n\nRelease notes, 474 Release planning, 329–367\n\noverview of, 329 prioritizing and, 338–340 purpose of, 330–331 scope, 340–344 sizing and, 332–337 test plan alternatives, 350–354 test planning, 345–350 visibility and, 354–366\n\nReliability testing\n\noverview of, 230–231 Remote Data Monitoring system example,\n\n250–251\n\nRemote Data Monitoring system example\n\nacceptance tests, 245 application, 242–243 applying test quadrants, 252–253 automated functional test structure,\n\n245–247\n\ndocumenting test code, 251 embedded testing, 248 end-to-end tests, 249–250 exploratory testing, 248–249 overview of, 242 reliability testing, 250–251 reporting test results, 251 team and process, 243–244 testing data feeds, 249 unit tests, 244–245 user acceptance testing, 250 web services, 247–248\n\nRemote team member. See Geographically\n\ndispersed teams\n\nRepetitive tasks, automating, 284 Reports\n\ndocumentation and, 208–210 Remote Data Monitoring system example,\n\n251 Repository, 124 Requirements\n\nbusiness-facing tests addressing, 130 documentation of, 402 gathering all viewpoints regarding\n\nrequirements, 374–375\n\nhow to elicit, 135–140 nonfunctional, 218–219 quandary, 132–134 tools for eliciting examples and requirements,\n\n155–156\n\nResources\n\ncompleting stories and, 381 hiring agile tester, 67–69 overview of, 66 tester-developer ratio, 66–67 testing and, 434–435\n\nResponse time API, 411 load testing and, 234–235 measurable goals and, 76 web services and, 207\n\nRetrospectives\n\ncontinuous improvement and, 28 ideas for improvement, 447–449 iteration planning and, 383 overview of, 444–445 process improvement and, 90 “start, stop, and continue” exercise,\n\n445–447\n\nReturn on investment. See ROI (return on\n\ninvestment)\n\nRewards, performance and, 70–71 Rich-client unit testing tools, 127 Rising, Linda, 121–122 Risk\n\nrisk analysis, 198, 286, 290, 345–346 risk assessment, 407–409 test mitigating, 147–149\n\nINDEX\n\nRogers, Paul, 242, 310, 388, 398 ROI (return on investment)\n\nautomation and, 264 deﬁnition, 505 lean measurement and, 75 metrics and, 78–79 speaking managerís language, 55\n\nRole, function, business value pattern, 155 Roles\n\nconﬂicting or multiple roles, 45 cultural differences among, 48–49 customer team, 7 developer team, 7–8 interaction of, 8\n\nRPGUnit, 118 RSpec, 165, 318 Ruby Test::Unit, 170 Ruby with Watir\n\nfunctional testing, 247 GUI testing, 285 identifying defects with, 212 keywords or actions words for automating\n\ntests, 182\n\noverview of, 172–174 test automation with, 186\n\nRubyMock, 127 Rules, managing bugs and, 425\n\nS Safety tests, 232 Santos, Rafael, 448 Satisfaction conditions. See Conditions of\n\nsatisfaction\n\nScalability testing, 233–234 Scenario testing, 192–193\n\nﬂow diagrams and, 194–195 overview of, 192–195 soap opera tests, 193\n\nScope, 340–344\n\nbusiness-facing tests deﬁning, 134 deadlines and timelines and, 340–341 focusing on value, 341–342 overview of, 340 system-wide impact, 342\n\n525",
      "page_number": 560
    },
    {
      "number": 68,
      "title": "Segment 68 (pages 568-575)",
      "start_page": 568,
      "end_page": 575,
      "detection_method": "topic_boundary",
      "content": "526\n\nINDEX\n\nScope, continued\n\ntest plans and, 345 third-party involvement and, 342–344\n\nScope creep, 385, 412 Scripts\n\nautomating comparisons, 283 as automation tools, 297 conversion scripts, 461 data generation tools, 305 exploratory testing and, 211–212\n\nScrum\n\nproduct owner role, 141, 373 Remote Data Monitoring system example,\n\n244\n\nsprint reviews, 444\n\nScrumMaster\n\napproach to process improvement, 448–449 sizing stories and, 336–337 writing task cards and, 391\n\nSDD (story test-driven development)\n\nidentifying variations, 410 overview of, 262–263 test-ﬁrst development and, 263 testing web services and, 170\n\nSecurity testing\n\noutside-in approach of attackers, 225 overview of, 223–227 specialized knowledge required for, 220\n\nSelenium\n\nGUI test tools, 174–175 implementing automation, 316–318 open source tools, 163 test automation with, 186, 316\n\nSelf-organization\n\nprinciples, 29–30 self-organizing teams, 69 Session-based testing, 200–201 Setup\n\nautomating, 284–285 exploratory testing, 211–212\n\nShared resources access to, 43 specialists as, 301 writing tasks and, 390\n\nShared responsibility, 105–106 Shout-Out Shoebox, 450 “Show me,” collaboration with programmers,\n\n413–414\n\nSimplicity\n\nautomation and, 298–299 coding, 406 logging bugs and, 428–429 principle of “keeping it simple,” 26–27\n\nSimulator tools\n\nembedded testing and, 248 overview of, 213\n\nSize, organizational, 42–43 Sizing stories, 332–337 example of, 334–337 how to, 332–333 overview of, 332 tester’s role in, 333–334\n\nSkills\n\nadaptability and, 39–40 vs. attitude, 20 continuous improvement principle,\n\n27\n\nwho performs tests and, 220–221\n\nSmall chunks, incremental development,\n\n144–146\n\nSOAP\n\ndeﬁnition, 505 performance tests and, 223, 234\n\nSoap opera tests, 193 soapUI\n\ndeﬁnition, 505 performance tests and, 223, 234 testing Web Services, 170–171\n\nSOATest, 234 Software-based tools, 163 Software Conﬁguration Management Patterns: Effective Teamwork, Practical Integrations (Berczuk and Appleton), 124\n\nSoftware Endgames (Galen), 471 Source code control beneﬁts of, 255 overview of, 123–124 tools for, 124, 320\n\nSOX compliance, 469 Speak with one voice, customers, 373–374 Specialization, 220–221 Speed as a goal, 112 Spikes, development and test, 381 Spreadsheets\n\ntest spreadsheets, 353 tools for eliciting examples and requirements,\n\n159\n\nSprint reviews, 444. See also Demos/\n\ndemonstrations\n\nSQL*Loader, 460 Stability testing, 28 Staging environment, 458 Stand-up meetings, 177, 429, 462 Standards\n\nmaintainability and, 227 quality models and, 90–93\n\n“Start, stop, continue” exercise, retrospectives,\n\n445–447\n\nStatic analysis, security testing tools, 225 Steel thread, incremental development, 144,\n\n338, 345\n\nStories. See also Business-facing tests\n\nbeneﬁts of working on in advance of\n\niterations, 370–372\n\nbriefness of, 129–130 business-facing tests as, 130 determining story size, 375–376 focusing on one story when coding,\n\n411–412\n\nidentifying variations, 410 knowing when a story is done,\n\n104–105\n\nlogging bugs and, 420–421 mock-ups and, 380 prioritizing, 338–340 resources and, 381 scope and, 340 sizing. See Sizing stories starting simple, 133, 406 story tests deﬁned, 505 system-wide impact of, 342 test plans and, 345\n\nINDEX\n\ntest strategies and, 380–381 testable, 393–396 treating bugs as, 425\n\nStory boards\n\nburndown charts, 429 deﬁnition, 505–506 examples, 356–357 online, 357, 384 physical, 356 stickers and, 355 tasks, 222, 355, 436 virtual, 357, 384, 393 work in progress, 390\n\nStory cards\n\naudits and, 89 dealing with bugs and, 424–425 iteration planning and, 244 story narrative on, 409\n\nStory test-driven development. See SDD (story test-driven development) Strangler application (Fowler), 116–117 Strategy\n\nautomation. See Automation strategy test planning vs. test strategy, 86–87 test strategies, 380–381 Strategy, for writing tests\n\nbuilding tests incrementally, 178–179 iteration planning and, 372 keep the tests passing, 179 overview of, 177–178 test design patterns, 179–183 testability and, 183–185 Stress testing. See Load testing Subversion (SVN), 124, 320 Success factors. See Key success factors Successes, celebrating\n\nchange implementation and, 50–52 iteration wrap up and, 449–451\n\nSumrell, Megan, 365, 450 Sustainable pace, of testing, 40–41, 303 SVN (Subversion), 124, 320 SWTBot GUI test tool, 127 Synergy, between practices, 489 System, system-wide impact of story, 342\n\n527\n\n528\n\nINDEX\n\nT tail-f, 212 Tartaglia, Coni, 439, 454, 470, 473 Task boards. See Story boards Task cards\n\nautomating testing and, 394–395 iteration planning and, 389–392 product delivery and, 462–463\n\nTasks\n\ncompleting testing tasks, 415–416 deﬁnition, 505–506\n\nTDD (test-driven development)\n\nautomated tests driving, 262–263 defects and, 490 deﬁnition, 506 overview of, 5 Test-First Development compared with,\n\n113–114\n\nunit tests and, 111, 244–245\n\nTeam City, 126 Team structure, 59–65\n\nagile project teams, 64–65 independent QA team, 60 integration of testers into agile project, 61–63 overview of, 59 traditional functional structure vs agile\n\nstructure, 64\n\nTeams\n\nautomation as team effort, 484 building, 69–71 celebrating success, 50–52 co-located, 65–66 controlling workload and, 393 customer, 7 developer, 7–8 empowerment of, 44 facilitating communication and, 429–432 geographically dispersed, 376–378, 431–432 giving all team members equal weight, 31 giving ownership to, 50 hiring agile tester for, 67–69 interaction between customer and developer\n\nteams, 8\n\niteration planning and, 384–385\n\nlogistics, 59 problem solving and, 123 Remote Data Monitoring system example,\n\n243–244\n\nshared responsibility and, 105–106 traditional, 9–10 using tests to support Quadrants 1 and 2,\n\n100–101\n\nwhole team approach. See Whole team\n\napproach\n\nworking on agile teams, 10–12\n\nTeardown, for tests, 307–308 Technical debt\n\ndefects as, 418 deﬁnition, 506 managing, 106, 487–488\n\nTechnology-facing tests\n\noverview of, 5 Quadrants 1 & 4, 97–98\n\nTechnology-facing tests, critiquing the product\n\n(Quadrant 4), 217–239\n\nbaselines, 235–237 coding and testing and, 412–413 compatibility testing, 229–230 installability testing, 231–232 interoperability testing, 228–229 maintainability testing, 227–228 memory management testing, 237–238 overview of, 217–219 performance and load testing, 234 performance and load testing tools, 234–235 reliability testing, 230–231, 250–251 scalability testing, 233–234 security testing, 223–227 test environment and, 237 when to use, 222–223 who performs the test, 220–222\n\nTechnology-facing tests, supporting team\n\n(Quadrant 1) build tools, 126 designing with testing in mind, 115–118 ease of accomplishing tasks, 114–115 IDEs for, 124–126 infrastructure supporting, 111–112\n\noverview of, 109–110 purpose of, 110–111 source code control, 123–124 speed as beneﬁt of, 112–114 timely feedback, 118–119 toolkit for, 123 unit test tools, 126–127 unit tests, 244–245 what to do if team doesn’t perform these\n\ntests, 121–123\n\nwhere/when to stop, 119–121\n\nTest automation pyramid\n\nmulti-layered approach to automation and,\n\n290–291\n\noverview of, 276–279 three little pigs metaphor, 278\n\nTest behind UI, 282 Test cases\n\nadding complexity, 407 as documentation, 402 example-driven development, 379 identifying variations, 410 starting simple, 406\n\nTest coverage (and/or code coverage),\n\n360–364\n\nTest design patterns, 179–183\n\nBuild/Operate/Check pattern, 180 data-driven and keyword-driven tests,\n\n182–183 overview of, 179 test genesis patterns (Veragen), 179 time-based, activity, and event patterns, 181\n\nTest doubles\n\ndeﬁnition, 506 layered architectures and, 116\n\nTest-driven development. See TDD (test-driven\n\ndevelopment)\n\nTest environments, 237, 487 Test-First Development\n\ndeﬁnition, 506 TDD (test-driven development) compared\n\nwith, 113–114\n\nTest management, 186 Test management toolkit (Quadrant 2), 186\n\nINDEX\n\nTest plan alternatives, 350-354 Test planning, 345–350\n\nautomated test lists, test plan alternatives,\n\n353–354\n\ninfrastructure and, 346–347 overview of, 86, 345 reasons for writing, 345–346 test environment and, 347–348 test plan alternatives, 350–354 test plans, lightweight 350 test plan sample, 351 test strategy vs., 86–88 traceability and, 88 types of tests and, 346 where to start, 345\n\nTest results\n\ncommunicating, 357–358 organizing, 322–324 release planning and, 349–350\n\nTest skills. See Skills Test spikes, 381 Test spreadsheets, 353 Test strategy\n\niterations, pre-iteration activities and,\n\n380–381 test plan vs., 86–88\n\nTest stubs\n\ndeﬁnition, 506 integration with external applications and,\n\n459\n\nunit tests and, 127\n\nTest teams, 506–507. See also Teams Test tools. See also Toolkits\n\nAPI-layer functional, 168–170 exploratory testing, 210–211 generating test data with, 212 GUI tests, 170–176 home-brewed, 175 home-grown, 314 IDEs, 124–126 performance testing, 234–235 security testing, 225 unit-level tests, 126–127, 165–168 web service tests, 170\n\n529\n\n530\n\nINDEX\n\nTest types\n\nalpha/beta, 466–467 exploratory. See Exploratory testing (ET) functional. See Functional testing GUI. See GUI testing integration, 229, 459 load. See Load testing performance. See Performance and load\n\ntesting\n\nreliability, 230–231, 250–251 security, 220, 223–227 stress. See Load testing unit. See Unit testing usability. See Usability testing user acceptance testing. See UAT (user\n\nacceptance testing)\n\nTest writing strategy. See Strategy, for writing\n\ntests\n\nTestability, 183–185\n\nautomated vs. manual Quadrant 2 tests, 185 automation and, 149–150 code design and test design and, 184–185 overview of, 183 of stories, 393–396\n\nTesters\n\nadding value, 12 agile testers, 4, 19–20 agile testing mindset, 20–21 automation allowing focus on more\n\nimportant work, 260\n\ncollaboration with customers, 396–397 considering all viewpoints during iteration\n\nplanning, 386–389\n\ncontrolling workload and, 393 deﬁnition, 507 facilitating communication, 429–430 feedback and, 486 hiring agile tester, 67–69 how to inﬂuence testing, 121–122 integration of testers into agile project,\n\n61–63\n\niterations and, 327 making job easier, 114–115 sizing stories, 333–334\n\ntester-developer ratio, 66–67 writing task cards and, 391\n\nTester's bill of rights, 49–50 Testing\n\ncoding and testing simultaneously, 409–410 completing testing tasks, 415–416 identifying variations, 410 managing, 320–322 organizing test results, 322–324 organizing tests, 319–322 planning time for, 455–456 post-development cycles, 467–468 quadrants. See Quadrants release candidates, 458 risk assessment and, 407–409 sustainable pace of, 40–41 traditional vs. agile, 12–15 transparency of tests, 321–322\n\nTesting in context\n\ncontext-driven testing and, 106–108 deﬁnition, 502\n\nTestNG GUI test tool, 127 Tests that never fail, 286 Text matrices, 350–353 The Grinder, 234 Themes. See also Release planning\n\ndeﬁnition, 507 prioritizing stories and, 339 writing task cards and, 392\n\nThin slices, incremental development and, 338 Third parties\n\ncompatibility testing and, 230 release planning and, 342–344 software, 163 Tholfsen, Mike, 203 Thomas, Mike, 116, 194 Three little pigs metaphor, 278 Timelines, scope and, 340–341 Toolkit (Quadrant 1) build tools, 126 IDEs, 124–126 overview of, 123 source code control, 123–124 unit test tools, 126–127\n\nToolkit (Quadrant 2)\n\nAPI-layer functional test tools, 168–170 automation tools, 164–165 building tests incrementally, 178–179 checklists, 156 ﬂow diagrams, 160–163 GUI test tools, 170–176 keep the tests passing, 179 mind maps, 156–158 mock-ups, 160 software-based tools, 163 spreadsheets, 159 strategies for writing tests, 177–178 test design patterns, 179–183 test management, 186 testability and, 183–185 tool strategy, 153–155 tools for eliciting examples and requirements,\n\n155–156\n\nunit-level test tools, 165–168 Web service test tool, 170\n\nToolkit (Quadrant 3)\n\nemulator tools, 213–214 monitoring tools, 212–213 simulator tools, 213 user acceptance testing, 250\n\nToolkit (Quadrant 4) baselines, 235–237 performance and load testing tools, 234–235\n\nTools\n\nAPI-layer functional test tools, 168–170 automation, 164–165 data generation, 304–305 defect tracking, 83–85 eliciting examples and requirements,\n\n155–156, 159–163\n\nemulator tools, 213–214 exploratory testing, 210–211 generating test data, 212 GUI test tools, 170–176 home-brewed, 175 home-grown, 314 IDEs, 124–126 load testing, 234–235\n\nINDEX\n\nmonitoring, 212–213 open source, 172, 314–315 performance testing, 234–235 for product owners and business experts, 134 security testing, 225 simulators, 213 software-based, 163 unit-level tests, 126–127, 165–168 vendor/commercial, 315–316 web service test tool, 170\n\nTools, automation\n\nagile-friendly, 316 applying one tool at a time, 312–313 home-brewed, 175 home-grown, 314 identifying tool requirements, 311–312 open source, 314–315 selecting, 294–298 vendors, 315–316\n\nTraceability\n\nDTS and, 82 matrices, 86 test planning and, 88\n\nTracking, test tasks and status, 354–357 Traditional processes, transitioning. See\n\nTransitioning traditional processes to agile\n\nTraditional teams, 9–10 Traditional vs. agile testing, 12–15 Training\n\nas deliverable, 469 lack of, 45\n\nTransitioning traditional processes to agile, 73–93\n\ndefect tracking. See Defect tracking existing process and, 88–92 lean measurements, 74–75 lightweight processes and, 73–74 metrics and, 74–79 overview of, 73 test planning. See Test planning\n\nU UAT (user acceptance testing)\n\npost-development testing cycles, 467–468 product delivery and, 464–466\n\n531\n\n532\n\nINDEX\n\nUAT (user acceptance testing), continued\n\nin Quadrant 3, 102 release planning for, 331, 346 Remote Data Monitoring system example,\n\n250\n\nin test plan, 351 tryng out new features and, 102 writing at iteration kickoff meeting, 372\n\nUI (user interface). See also GUI (graphical user\n\ninterface)\n\nautomation strategy and, 293 modeling and, 399\n\nUnit test tools, 165–168. See also by individual\n\nunit tools\n\nbehavior-driven development tools, 166–168 list of, 126–127 overview of, 165\n\nUnit testing\n\nautomating, 282 BDD (Behavior-driven development), 165–168 deﬁnition, 507 metrics and, 76 supporting function of, 5 TDD (test-driven development) and, 111 technology-facing tests, 120 tools for Quadrant 1 tests, 126–127\n\nUsability testing, 202–204\n\nchecking out applications of competitors, 204 navigation and, 204 overview of, 202 users needs and persona testing, 202–204 what should not be automated, 285–286\n\nUse cases, 398 User acceptance testing. See UAT (user\n\nacceptance testing) User documentation, 207–208 User interface (UI). See also GUI (graphical user\n\ninterface)\n\nautomation strategy and, 293 modeling and, 399 User story. See Story User story card. See Story card\n\nUser Stories Applied for Agile Software\n\nDevelopment (Cohn), 155\n\nV Vaage, Carol, 330 Value\n\nadding, 31–33 delivering to customer, 22–23 focusing on, 341–342 testers adding, 12\n\nValues, agile, 3–4. See also Principles, for agile\n\ntesters\n\nVariations, coding and testing and, 410 Velocity\n\nautomation and, 255, 484 burnout rate and, 79 database impact on, 228 defects and, 487 deﬁnition, 507 maximizing, 370 sustainable pace of testing and, 41 taking time to do it right, 301 technical debt and, 106, 313, 418, 506\n\nVendors\n\nautomation tools, 315–316 capture-playback tool, 267 IDEs, 125 planning and, 342–344 source code control tools, 124 working with, 142, 349\n\nVeragen, Pierre, 76, 163, 179, 295, 363, 372,\n\n444\n\nVersion control, 123–124, 186. See also\n\nSource Code Control Viewpoints. See also Big picture\n\nconsidering all viewpoints during iteration\n\nplanning, 385–389\n\ngathering all viewpoints regarding\n\nrequirements, 374–375\n\nPower of Three and, 411 using multiple viewpoints in eliciting\n\nrequirement, 137–138\n\nVisibility, 354–366\n\ncode coverage, 360–364 communicating test results, 357–358 defect metrics, 364–366 number of passing tests, 358–360\n\noverview of, 354 release metrics, 358 tracking test tasks and status, 354–357\n\nVisual Studio, 125 Voris, John, 117\n\nW Waterfall approach, to development\n\nagile development compared with, 12–13 ìmini-waterfallî phenomenon, 46–47 successes of, 112 test plans and, 346\n\nWatir (Web Application Testing in Ruby), 163, 172–174, 320. See also Ruby with Watir Web Services Description Language (WSDL),\n\n507\n\nWeb service testing automating, 282 overview of, 207 Remote Data Monitoring system example,\n\n247–248 tools for, 170–171\n\nWebLoad, 234 Whelan, Declan, 321 Whiteboards\n\nexample-driven development, 379 facilitating communication, 430 modeling, 399 planning diagram, 371 reviewing high-level tests with programmers,\n\n400–401\n\ntest plan alternatives, 353–354\n\nWhole team approach, 325\n\nadvantages of, 26 agile vs. traditional development, 15–16 automation strategy and, 300–301 budget limits and, 55 ﬁnding enjoyment in work and, 31 key success factors, 482, 491 pairing testers with programmers, 279\n\nINDEX\n\nshared responsibility and, 105–106 team building and, 69 team structure and, 59–62 to test automation, 270 test management and, 322 traditional cross-functional team compared\n\nwith, 64\n\nvalue of team members and, 70\n\nWiki\n\nas communication tool, 164 graphical documentation of examples,\n\n398–399 mockups, 160, 380 requirements, 402 story checklists and, 156 test cases, 372 traceability and, 88\n\nWilson-Welsh, Patrick, 278 Wizard of Oz Testing, 138–139 Workﬂow diagrams, 398 Working Effectively With Legacy Code (Feathers),\n\n117, 288\n\nWorkload, 393 Worst-case scenarios, 136, 334 Writing tests, strategy for. See Strategy, for\n\nwriting tests\n\nWSDL (Web Services Description Language),\n\n507\n\nX XP (Extreme Programming)\n\nagile team embracing, 10–11 courage as core value in, 25\n\nxUnit, 126–127\n\nY Yakich, Joe, 316\n\nZ Zero bug tolerance, 79, 418–419\n\n533",
      "page_number": 568
    },
    {
      "number": 69,
      "title": "Segment 69 (pages 576-577)",
      "start_page": 576,
      "end_page": 577,
      "detection_method": "topic_boundary",
      "content": "FREE Online Edition\n\nYour purchase of Agile Testing includes access to a free online edition for 45 days through the Safari Books Online subscription service. Nearly every Addison-Wesley Professional book is available online through Safari Books Online, along with more than 5,000 other technical books and videos from publishers such as, Cisco Press, Exam Cram, IBM Press, O’Reilly, Prentice Hall, Que, and Sams.\n\nSAFARI BOOKS ONLINE allows you to search for a speciﬁ c answer, cut and paste code, download chapters, and stay current with emerging technologies.\n\nActivate your FREE Online Edition at www.informit.com/safarifree\n\nSTEP 1:\n\nEnter the coupon code: BTTZRBI.\n\nSTEP 2:\n\nNew Safari users, complete the brief registration form. Safari subscribers, just log in.\n\nIf you have difﬁ culty registering on Safari or accessing the online edition, please e-mail customer-service@safaribooksonline.com",
      "page_number": 576
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "Praise for Agile Testing\n\n“As Agile methods have entered the mainstream, we’ve learned a lot about how the testing discipline ﬁts into Agile projects. Lisa and Janet give us a solid look at what to do, and what to avoid, in Agile testing.”\n\n—Ron Jeffries, www.XProgramming.com\n\n“An excellent introduction to agile and how it affects the software test community!”\n\n—Gerard Meszaros, Agile Practice Lead and Chief Test Strategist at Solution Frameworks, Inc., an agile coaching and lean software development consultancy\n\n“In sports and music, people know the importance of practicing technique until it becomes a part of the way they do things. This book is about some of the most funda- mental techniques in software development—how to build quality into code—tech- niques that should become second nature to every development team. The book provides both broad and in-depth coverage of how to move testing to the front of the development process, along with a liberal sprinkling of real-life examples that bring the book to life.”\n\n—Mary Poppendieck, Author of Lean Software Development and Implementing Lean Software Development\n\n“Refreshingly pragmatic. Chock-full of wisdom. Absent of dogma. This book is a game- changer. Every software professional should read it.”\n\n—Uncle Bob Martin, Object Mentor, Inc.\n\n“With Agile Testing, Lisa and Janet have used their holistic sensibility of testing to de- scribe a culture shift for testers and teams willing to elevate their test effectiveness. The combination of real-life project experiences and speciﬁc techniques provide an excellent way to learn and adapt to continually changing project needs.”\n\n—Adam Geras, M.Sc. Developer-Tester, Ideaca Knowledge Services\n\n“On Agile projects, everyone seems to ask, ‘But, what about testing?’ Is it the develop- ment team’s responsibility entirely, the testing team, or a collaborative effort between developers and testers? Or, ‘How much testing should we automate?’ Lisa and Janet have written a book that ﬁnally answers these types of questions and more! Whether you’re a tester, developer, or manager, you’ll learn many great examples and stories from the real-world work experiences they’ve shared in this excellent book.”\n\n—Paul Duvall, CTO of Stelligent and co-author of Continuous Integration: Improving Software Quality and Reducing Risk\n\n“Finally a book for testers on Agile teams that acknowledges there is not just one right way! Agile Testing provides comprehensive coverage of the issues testers face when they move to Agile: from tools and metrics to roles and process. Illustrated with numerous stories and examples from many contributors, it gives a clear picture of what successful Agile testers are doing today.”\n\n—Bret Pettichord, Chief Technical Ofﬁcer of WatirCraft and Lead Developer of Watir",
      "content_length": 2805,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "AGILE TESTING",
      "content_length": 13,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "AGILE TESTING\n\nA PRACTICAL GUIDE FOR TESTERS AND AGILE TEAMS\n\nLisa Crispin Janet Gregory\n\nUpper Saddle River, NJ • Boston • Indianapolis • San Francisco New York • Toronto • Montreal • London • Munich • Paris • Madrid Capetown • Sydney • Tokyo • Singapore • Mexico City",
      "content_length": 269,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Many of the designations used by manufacturers and sellers to distinguish their products are claimed as trademarks. Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed with initial capital letters or in all capitals.\n\nThe authors and publisher have taken care in the preparation of this book, but make no expressed or implied warranty of any kind and assume no responsibility for errors or omissions. No liability is assumed for incidental or consequential damages in connection with or arising out of the use of the information or programs contained herein.\n\nThe publisher offers excellent discounts on this book when ordered in quantity for bulk purchases or special sales, which may include electronic versions and/or custom covers and content particular to your business, training goals, marketing focus, and branding interests. For more information, please contact:\n\nU.S. Corporate and Government Sales (800) 382-3419 corpsales@pearsontechgroup.com\n\nFor sales outside the United States, please contact:\n\nInternational Sales international@pearson.com\n\nVisit us on the Web: informit.com/aw\n\nLibrary of Congress Cataloging-in-Publication Data:\n\nCrispin, Lisa.\n\nAgile testing : a practical guide for testers and agile teams /\n\nLisa Crispin, Janet Gregory. — 1st ed.\n\np.\n\ncm.\n\nIncludes bibliographical references and index. ISBN-13: 978-0-321-53446-0 (pbk. : alk. paper) ISBN-10: 0-321-53446-8 (pbk. : alk. paper)\n\n1. Computer software— II. Title.\n\nTesting.\n\n2. Agile software development.\n\nI. Gregory, Janet.\n\nQA76.76.T48C75 2009 005.1—dc22\n\n2008042444\n\nCopyright © 2009 Pearson Education, Inc.\n\nAll rights reserved. Printed in the United States of America. This publication is protected by copyright, and permission must be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. For information regarding permissions, write to:\n\nPearson Education, Inc. Rights and Contracts Department 501 Boylston Street, Suite 900 Boston, MA 02116 Fax (617) 671-3447\n\nISBN-13: 978-0-321-53446-0 ISBN-10: 0-321-53446-8 Text printed in the United States on recycled paper at R.R. Donnelley in Crawfordsville, Indiana. First printing, December 2008",
      "content_length": 2360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "To my husband, Bob Downing—you’re the bee’s knees!\n\n—Lisa\n\nTo Jack, Dana, and Susan, and to all the writers in my family.\n\n—Janet\n\nAnd to all our favorite donkeys and dragons.\n\n—Lisa and Janet",
      "content_length": 192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "CONTENTS\n\nForeword by Mike Cohn\n\nForeword by Brian Marick\n\nPreface\n\nAcknowledgments\n\nAbout the Authors\n\nPart I\n\nIntroduction\n\nChapter 1 What Is Agile Testing, Anyway?\n\nAgile Values What Do We Mean by “Agile Testing”? A Little Context for Roles and Activities on an Agile Team\n\nCustomer Team Developer Team Interaction between Customer and Developer Teams\n\nHow Is Agile Testing Different?\n\nWorking on Traditional Teams Working on Agile Teams Traditional vs. Agile Testing\n\nWhole-Team Approach Summary\n\nChapter 2\n\nTen Principles for Agile Testers\n\nWhat’s an Agile Tester? The Agile Testing Mind-Set\n\nxxiii\n\nxxv\n\nxxvii\n\nxxxvii\n\nxli\n\n1\n\n3 3 4 7 7 7 8 9 9 10 12 15 17\n\n19 19 20\n\nix",
      "content_length": 676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "x\n\nCONTENTS\n\nApplying Agile Principles and Values Provide Continuous Feedback Deliver Value to the Customer Enable Face-to-Face Communication Have Courage Keep It Simple Practice Continuous Improvement Respond to Change Self-Organize Focus on People Enjoy Adding Value Summary\n\nPart II\n\nOrganizational Challenges\n\nChapter 3\n\nCultural Challenges\n\nOrganizational Culture Quality Philosophy Sustainable Pace Customer Relationships Organization Size Empower Your Team\n\nBarriers to Successful Agile Adoption by Test/QA Teams\n\nLoss of Identity Additional Roles Lack of Training Not Understanding Agile Concepts Past Experience/Attitude Cultural Differences among Roles\n\nIntroducing Change Talk about Fears Give Team Ownership Celebrate Success\n\nManagement Expectations\n\nCultural Changes for Managers Speaking the Manager’s Language\n\nChange Doesn’t Come Easy\n\nBe Patient Let Them Feel Pain\n\n21 22 22 23 25 26 27 28 29 30 31 31 33\n\n35\n\n37 37 38 40 41 42 44 44 44 45 45 45 48 48 49 49 50 50 52 52 55 56 56 56",
      "content_length": 999,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Build Your Credibility Work On Your Own Professional Development Beware the Quality Police Mentality Vote with Your Feet\n\nSummary\n\nChapter 4\n\nTeam Logistics\n\nTeam Structure\n\nIndependent QA Teams Integration of Testers into an Agile Project Agile Project Teams\n\nPhysical Logistics Resources\n\nTester-Developer Ratio Hiring an Agile Tester\n\nBuilding a Team\n\nSelf-Organizing Team Involving Other Teams Every Team Member Has Equal Value Performance and Rewards What Can You Do?\n\nSummary\n\nChapter 5\n\nTransitioning Typical Processes\n\nSeeking Lightweight Processes Metrics\n\nLean Measurements Why We Need Metrics What Not to Do with Metrics Communicating Metrics Metrics ROI Defect Tracking\n\nWhy Should We Use a Defect Tracking System (DTS)? Why Shouldn’t We Use a DTS? Defect Tracking Tools Keep Your Focus\n\nTest Planning\n\nTest Strategy vs. Test Planning Traceability\n\nCONTENTS\n\nxi\n\n57 57 57 57 58\n\n59 59 60 61 64 65 66 66 67 69 69 69 70 70 71 71\n\n73 73 74 74 75 77 77 78 79 80 82 83 85 86 86 88",
      "content_length": 987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "xii\n\nCONTENTS\n\nExisting Processes and Models\n\nAudits Frameworks, Models, and Standards\n\nSummary\n\nPart III\n\nThe Agile Testing Quadrants\n\nChapter 6\n\nThe Purpose of Testing\n\nThe Agile Testing Quadrants\n\nTests that Support the Team Tests that Critique the Product\n\nKnowing When a Story Is Done\n\nShared Responsibility Managing Technical Debt Testing in Context Summary\n\nChapter 7\n\nTechnology-Facing Tests that Support the Team\n\nAn Agile Testing Foundation\n\nThe Purpose of Quadrant 1 Tests Supporting Infrastructure\n\nWhy Write and Execute These Tests? Lets Us Go Faster and Do More Making Testers’ Jobs Easier Designing with Testing in Mind Timely Feedback\n\nWhere Do Technology-Facing Tests Stop? What If the Team Doesn’t Do These Tests?\n\nWhat Can Testers Do? What Can Managers Do? It’s a Team Problem\n\nToolkit\n\nSource Code Control IDEs Build Tools Build Automation Tools Unit Test Tools\n\nSummary\n\n88 89 90 93\n\n95\n\n97 97 98 101 104 105 106 106 108\n\n109 109 110 111 112 112 114 115 118 119 121 121 122 123 123 123 124 126 126 126 127",
      "content_length": 1026,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "CONTENTS\n\nChapter 8\n\nBusiness-Facing Tests that Support the Team\n\nDriving Development with Business-Facing Tests The Requirements Quandary\n\nCommon Language Eliciting Requirements Advance Clarity Conditions of Satisfaction Ripple Effects\n\nThin Slices, Small Chunks How Do We Know We’re Done? Tests Mitigate Risk Testability and Automation Summary\n\nChapter 9\n\nToolkit for Business-Facing Tests that Support the Team\n\nBusiness-Facing Test Tool Strategy Tools to Elicit Examples and Requirements\n\nChecklists Mind Maps Spreadsheets Mock-Ups Flow Diagrams Software-Based Tools\n\nTools for Automating Tests Based on Examples Tools to Test below the GUI and API Level Tools for Testing through the GUI\n\nStrategies for Writing Tests\n\nBuild Tests Incrementally Keep the Tests Passing Use Appropriate Test Design Patterns Keyword and Data-Driven Tests\n\nTestability\n\nCode Design and Test Design Automated vs. Manual Quadrant 2 Tests\n\nTest Management Summary\n\nChapter 10 Business-Facing Tests that Critique the Product\n\nIntroduction to Quadrant 3 Demonstrations\n\nxiii\n\n129 129 132 134 135 140 142 143 144 146 147 149 150\n\n153 153 155 156 156 159 160 160 163 164 165 170 177 178 179 179 182 183 184 185 186 186\n\n189 190 191",
      "content_length": 1208,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "xiv\n\nCONTENTS\n\nScenario Testing Exploratory Testing\n\nSession-Based Testing Automation and Exploratory Testing An Exploratory Tester\n\nUsability Testing\n\nUser Needs and Persona Testing Navigation Check Out the Competition\n\nBehind the GUI API Testing Web Services\n\nTesting Documents and Documentation\n\nUser Documentation Reports\n\nTools to Assist with Exploratory Testing\n\nTest Setup Test Data Generation Monitoring Tools Simulators Emulators\n\nSummary\n\nChapter 11 Critiquing the Product Using Technology-\n\nFacing Tests\n\nIntroduction to Quadrant 4 Who Does It? When Do You Do It? “ility” Testing Security Maintainability Interoperability Compatibility Reliability Installability “ility” Summary\n\nPerformance, Load, Stress, and Scalability Testing\n\nScalability Performance and Load Testing Performance and Load-Testing Tools Baseline\n\n192 195 200 201 201 202 202 204 204 204 205 207 207 207 208 210 211 212 212 213 213 214\n\n217 217 220 222 223 223 227 228 229 230 231 232 233 233 234 234 235",
      "content_length": 985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Test Environments Memory Management\n\nSummary\n\nChapter 12 Summary of Testing Quadrants\n\nReview of the Testing Quadrants A System Test Example The Application The Team and the Process\n\nTests Driving Development\n\nUnit Tests Acceptance Tests\n\nAutomation\n\nThe Automated Functional Test Structure Web Services Embedded Testing\n\nCritiquing the Product with Business-Facing Tests\n\nExploratory Testing Testing Data Feeds The End-to-End Tests User Acceptance Testing Reliability Documentation\n\nDocumenting the Test Code Reporting the Test Results Using the Agile Testing Quadrants Summary\n\nPart IV\n\nAutomation\n\nChapter 13 Why We Want to Automate Tests and What\n\nHolds Us Back\n\nWhy Automate?\n\nManual Testing Takes Too Long Manual Processes Are Error Prone Automation Frees People to Do Their Best Work Automated Regression Tests Provide a Safety Net Automated Tests Give Feedback, Early and Often Tests and Examples that Drive Coding Can Do More\n\nCONTENTS\n\nxv\n\n237 237 238\n\n241 241 242 242 243 244 244 245 245 245 247 248 248 248 249 249 250 250 251 251 251 252 253\n\n255\n\n257 258 258 259 259 261 262 262",
      "content_length": 1092,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "xvi\n\nCONTENTS\n\nTests Are Great Documentation ROI and Payback\n\nBarriers to Automation—Things that Get in the Way\n\nBret’s List Our List Programmers’ Attitude—“Why Automate?” The “Hump of Pain” (The Learning Curve) Initial Investment Code that’s Always in Flux Legacy Code Fear Old Habits\n\nCan We Overcome These Barriers? Summary\n\nChapter 14 An Agile Test Automation Strategy\n\nAn Agile Approach to Test Automation\n\nAutomation Test Categories Test Automation Pyramid\n\nWhat Can We Automate?\n\nContinuous Integration, Builds, and Deploys Unit and Component Tests API or Web Services Testing Testing behind the GUI Testing the GUI Load Tests Comparisons Repetitive Tasks Data Creation or Setup What Shouldn’t We Automate?\n\nUsability Testing Exploratory Testing Tests that Will Never Fail One-Off Tests\n\nWhat Might Be Hard to Automate? Developing an Automation Strategy—Where Do We Start?\n\nWhere Does It Hurt the Most? Multi-Layered Approach Think about Test Design and Maintenance Choosing the Right Tools\n\n263 264 264 264 265 265 266 267 269 269 269 270 270 271\n\n273 274 274 276 279 280 282 282 282 282 283 283 284 284 285 285 286 286 286 287 288 289 290 292 294",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "CONTENTS\n\nApplying Agile Principles to Test Automation\n\nKeep It Simple Iterative Feedback Whole-Team Approach Taking the Time to Do It Right Learn by Doing Apply Agile Coding Practices to Tests\n\nSupplying Data for Tests Data Generation Tools Avoid Database Access When Database Access Is Unavoidable or Even Desirable Understand Your Needs Evaluating Automation Tools\n\nIdentifying Requirements for Your Automation Tool One Tool at a Time Choosing Tools Agile-Friendly Tools Implementing Automation Managing Automated Tests\n\nOrganizing Tests Organizing Test Results\n\nGo Get Started Summary\n\nPart V\n\nAn Iteration in the Life of a Tester\n\nChapter 15 Tester Activities in Release or Theme Planning\n\nThe Purpose of Release Planning Sizing\n\nHow to Size Stories The Tester’s Role in Sizing Stories An Example of Sizing Stories\n\nPrioritizing\n\nWhy We Prioritize Stories Testing Considerations While Prioritizing\n\nWhat’s in Scope?\n\nDeadlines and Timelines Focus on Value\n\nxvii\n\n298 298 299 300 301 303 303 304 304 306 307 310 311 311 312 313 316 316 319 319 322 324 324\n\n327\n\n329 330 332 332 333 334 338 338 339 340 340 341",
      "content_length": 1113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "xviii\n\nCONTENTS\n\nSystem-Wide Impact Third-Party Involvement\n\nTest Planning\n\nWhere to Start Why Write a Test Plan? Types of Testing Infrastructure Test Environments Test Data Test Results\n\nTest Plan Alternatives\n\nLightweight Test Plans Using a Test Matrix Test Spreadsheet A Whiteboard Automated Test List Preparing for Visibility\n\nTracking Test Tasks and Status Communicating Test Results Release Metrics\n\nSummary\n\nChapter 16 Hit the Ground Running\n\nBe Proactive Beneﬁts Do You Really Need This? Potential Downsides to Advance Preparation\n\nAdvance Clarity\n\nCustomers Speak with One Voice Story Size Geographically Dispersed Teams\n\nExamples Test Strategies Prioritize Defects Resources Summary\n\nChapter 17 Iteration Kickoff\n\nIteration Planning\n\nLearning the Details Considering All Viewpoints\n\n342 342 345 345 345 346 346 347 348 349 350 350 350 353 353 354 354 354 357 358 366\n\n369 369 370 372 373 373 373 375 376 378 380 381 381 382\n\n383 383 384 385",
      "content_length": 950,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Writing Task Cards Deciding on Workload\n\nTestable Stories Collaborate with Customers High-Level Tests and Examples Reviewing with Customers Reviewing with Programmers Test Cases as Documentation\n\nSummary\n\nChapter 18 Coding and Testing\n\nDriving Development\n\nStart Simple Add Complexity Assess Risk Coding and Testing Progress Together Identify Variations Power of Three Focus on One Story\n\nTests that Critique the Product Collaborate with Programmers\n\nPair Testing “Show Me” Talk to Customers\n\nShow Customers Understand the Business\n\nCompleting Testing Tasks Dealing with Bugs\n\nIs It a Defect or Is It a Feature? Technical Debt Zero Bug Tolerance It’s All about Choices\n\nDecide Which Bugs to Log Choose When to Fix Your Bugs Choose the Media You Should Use to Log a Bug Alternatives and Suggestions for Dealing with Bugs Start Simple\n\nFacilitate Communication\n\nTesters Facilitate Communication Distributed Teams\n\nCONTENTS\n\nxix\n\n389 393 393 396 397 400 400 402 403\n\n405 406 406 407 407 409 410 411 411 412 413 413 413 414 414 415 415 416 417 418 418 419 420 421 423 424 428 429 429 431",
      "content_length": 1083,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "xx\n\nCONTENTS\n\nRegression Tests\n\nKeep the Build “Green” Keep the Build Quick Building a Regression Suite Checking the “Big Picture”\n\nResources Iteration Metrics\n\nMeasuring Progress Defect Metrics\n\nSummary\n\nChapter 19 Wrap Up the Iteration\n\nIteration Demo Retrospectives\n\nStart, Stop, and Continue Ideas for Improvements\n\nCelebrate Successes Summary\n\nChapter 20 Successful Delivery\n\nWhat Makes a Product? Planning Enough Time for Testing The End Game\n\nTesting the Release Candidate Test on a Staging Environment Final Nonfunctional Testing Integration with External Applications Data Conversion and Database Updates Installation Testing Communication What If It’s Not Ready?\n\nCustomer Testing\n\nUAT Alpha/Beta Testing\n\nPost-Development Testing Cycles Deliverables Releasing the Product\n\nRelease Acceptance Criteria Release Management Packaging\n\n432 433 433 434 434 434 435 435 437 440\n\n443 443 444 445 447 449 451\n\n453 453 455 456 458 458 458 459 459 461 462 463 464 464 466 467 468 470 470 474 474",
      "content_length": 995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Customer Expectations Production Support Understand Impact to Business\n\nSummary\n\nPart VI\n\nSummary\n\nChapter 21 Key Success Factors\n\nSuccess Factor 1: Use the Whole-Team Approach Success Factor 2: Adopt an Agile Testing Mind-Set Success Factor 3: Automate Regression Testing Success Factor 4: Provide and Obtain Feedback Success Factor 5: Build a Foundation of Core Practices\n\nContinuous Integration Test Environments Manage Technical Debt Working Incrementally Coding and Testing Are Part of One Process Synergy between Practices\n\nSuccess Factor 6: Collaborate with Customers Success Factor 7: Look at the Big Picture Summary\n\nGlossary\n\nBibliography\n\nIndex\n\nCONTENTS\n\nxxi\n\n475 475 475 476\n\n479\n\n481 482 482 484 484 486 486 487 487 488 488 489 489 490 491\n\n493\n\n501\n\n509",
      "content_length": 768,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "FOREWORD\n\nBy Mike Cohn\n\n“Quality is baked in,” the programmers kept telling me. As part of a proposed acquisition, my boss had asked me to perform some ﬁnal due diligence on the development team and its product. We’d already established that the company’s recently launched product was doing well in the market, but I was to make sure we were not about to buy more trouble than beneﬁt. So I spent my time with the development team. I was looking for problems that might arise from having rushed the product into release. I wondered, “Was the code clean? Were there modules that could only be worked on by one developer? Were there hundreds or thousands of defects waiting to be discovered?” And when I asked about the team’s approach to testing, “Quality is baked in” was the answer I got.\n\nBecause this rather unusual colloquialism could have meant just about any- thing, I pressed further. What I found was that this was the company founder’s shorthand for expressing one of quality pioneer W. Edwards Dem- ing’s famous fourteen points: Build quality into the product rather than try- ing to test it in later.\n\nThe idea of building quality into their products is at the heart of how agile teams work. Agile teams work in short iterations in part to ensure that the application remains at a known state of quality. Agile teams are highly cross- functional, with programmers, testers, and others working side by side throughout each iteration so that quality can be baked into products through techniques such as acceptance-test driven development, a heavy emphasis on automated testing, and whole-team thinking. Good agile teams bake quality in by building their products continuously, integrating new work within minutes of its being completed. Agile teams utilize techniques such as refac- toring and a preference for simplicity in order to prevent technical debt from accumulating.\n\nxxiii",
      "content_length": 1892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "xxiv\n\nFOREWORD\n\nLearning how to do these things is difﬁcult, and especially so for testers, whose role has been given scant attention in previous books. Fortunately, the book you now hold in your hands answers questions on the mind of every tester who’s beginning to work on an agile project, such as:\n\n(cid:2) What are my roles and responsibilities? (cid:2) How do I work more closely with programmers? (cid:2) How much do we automate, and how do we start automating?\n\nThe experience of Lisa and Janet shines through on every page of the book. However, this book is not just their story. Within this book, they incorporate dozens of stories from real-world agile testers. These stories form the heart of the book and are what makes it so unique. It’s one thing to shout from the ivory tower, “Here’s how to do agile testing.” It’s another to tell the stories of the teams that have struggled and then emerged agile and victorious over challenges such as usability testing, legacy code that resists automation, tran- sitioning testers used to traditional phase-gate development, testing that “keeps up” with short iterations, and knowing when a feature is “done.”\n\nLisa and Janet were there at the beginning, learning how to do agile testing back when the prevailing wisdom was that agile teams didn’t need testers and that programmers could bake quality in by themselves. Over the years and through articles, conference presentations, and working with their clients and teams, Lisa and Janet have helped us see the rich role to be ﬁlled by testers on agile projects. In this book, Lisa and Janet use a test automation pyramid, the agile testing quadrants of Brian Marick (himself another world- class agile tester), and other techniques to show how much was missing from a mind-set that said testing is necessary but testers aren’t.\n\nIf you want to learn how to bake quality into your products or are an aspiring agile tester seeking to understand your role, I can think of no better guides than Lisa and Janet.",
      "content_length": 2012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "FOREWORD\n\nBy Brian Marick\n\nImagine yourself skimming over a landscape thousands of years ago, looking at the people below. They’re barely scraping out a living in a hostile territory, doing some hunting, some ﬁshing, and a little planting. Off in the distance, you see the glitter of a glacier. Moving closer, you see that it’s melting fast and that it’s barely damming a huge lake. As you watch, the lake breaks through, sweeping down a riverbed, carving it deeper, splashing up against cliffs on the far side of the landscape—some of which collapse.\n\nAs you watch, the dazed inhabitants begin to explore the opening. On the other side, there’s a lush landscape, teaming with bigger animals than they’ve ever seen before, some grazing on grass with huge seed heads, some squab- bling over mounds of fallen fruit.\n\nPeople move in. Almost immediately, they begin to live better. But as the years ﬂy past, you see them adapt. They begin to use nets to ﬁsh in the fast- running streams. They learn the teamwork needed to bring down the larger animals, though not without a few deaths along the way. They ﬁnd ever- better ways to cultivate this new grass they’ve come to call “wheat.”\n\nAs you watch, the mad burst of innovation gives way to a stable solution, a good way to live in this new land, a way that’s taught to each new generation. Although just over there, you spy someone inventing the wheel . . .\n\n(cid:2) (cid:2) (cid:2)\n\nIn the early years of this century, the adoption of Agile methods sometimes seemed like a vast dam breaking, opening up a way to a better—more pro- ductive, more joyful—way of developing software. Many early adopters saw beneﬁts right away, even though they barely knew what they were doing.\n\nxxv",
      "content_length": 1727,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "xxvi\n\nFOREWORD\n\nSome had an easier time of it than others. Programmers were like the hunters in the fable above. Yes, they had to learn new skills in order to hunt bison, but they knew how to hunt rabbits, more or less, and there were plenty of rabbits around. Testers were more like spear-ﬁshers in a land where spear-ﬁshing wouldn’t work. Going from spear-ﬁshing to net-ﬁshing is a much bigger con- ceptual jump than going from rabbit to bison. And, while some of the skills— cleaning ﬁsh, for example—were the same in the new land, the testers had to invent new skills of net-weaving before they could truly pull their weight.\n\nSo testing lagged behind. Fortunately, we had early adopters like Lisa and Janet, people who dove right in alongside the programmers, testers who were not jealous of their role or their independence, downright pleasant people who could ﬁgure out the biggest change of all in Agile testing: the tester’s new social role.\n\nAs a result, we have this book. It’s the stable solution, the good way for testers to live in this new Agile land of ours. It’s not the ﬁnal word—we could use the wheel, and I myself am eager for someone to invent antibiotics—but what’s taught here will serve you well until someone, perhaps Lisa and Janet, brings the next big change.",
      "content_length": 1287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "PREFACE\n\nWe were early adopters of Extreme Programming (XP), testing on XP teams that weren’t at all sure where testers or their brand of testing ﬁt in. At the time, there wasn’t much in the agile (which wasn’t called agile yet) literature about acceptance testing, or how professional testers might contribute. We learned not only from our own experiences but from others in the small agile com- munity. In 2002, Lisa co-wrote Testing Extreme Programming with Tip House, with lots of help from Janet. Since then, agile development has evolved, and the agile testing community has ﬂourished. With so many people contribut- ing ideas, we’ve learned a whole lot more about agile testing.\n\nIndividually and together, we’ve helped teams transition to agile, helped testers learn how to contribute on agile teams, and worked with others in the agile community to explore ways that agile teams can be more successful at testing. Our experiences differ. Lisa has spent most of her time as an agile tester on stable teams working for years at a time on web applications in the retail, telephony, and ﬁnancial industries. Janet has worked with soft- ware organizations developing enterprise systems in a variety of industries. These agile projects have included developing a message-handling system, an environmental-tracking system, a remote data management system (in- cluding an embedded application, with a communication network as well as the application), an oil and gas production accounting application, and ap- plications in the airline transportation industry. She has played different roles—sometimes tester, sometimes coach—but has always worked to better integrate the testers with the rest of the team. She has been with teams from as little as six months to as long as one-and-a-half years.\n\nWith these different points of view, we have learned to work together and complement each other’s skill sets, and we have given many presentations and tutorials together.\n\nxxvii",
      "content_length": 1975,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "xxviii\n\nPREFACE\n\nWHY WE WROTE THIS BOOK Several excellent books oriented toward agile development on testing and test patterns have been published (see our bibliography). These books are generally focused on helping the developer. We decided to write a book aimed at helping agile teams be more successful at delivering business value using tests that the business can understand. We want to help testers and quality assurance (QA) professionals who have worked in more traditional development methodologies make the transition to agile development.\n\nWe’ve ﬁgured out how to apply—on a practical, day-to-day level—the fruits of our own experience working with teams of all sizes and a variety of ideas from other agile practitioners. We’ve put all this together in this book to help testers, quality assurance managers, developers, development manag- ers, product owners, and anyone else with a stake in effective testing on agile projects to deliver the software their customers need. However, we’ve fo- cused on the role of the tester, a role that may be adopted by a variety of professionals.\n\nAgile testing practices aren’t limited to members of agile teams. They can be used to improve testing on projects using traditional development method- ologies as well. This book is also intended to help testers working on projects using any type of development methodology.\n\nAgile development isn’t the only way to successfully deliver software. How- ever, all of the successful teams we’ve been on, agile or waterfall, have had several critical commonalities. The programmers write and automate unit and integration tests that provide good code coverage. They are disciplined in the use of source code control and code integration. Skilled testers are in- volved from the start of the development cycle and are given time and re- sources to do an adequate job of all necessary forms of testing. An automated regression suite that covers the system functionality at a higher level is run and checked regularly. The development team understands the customers’ jobs and their needs, and works closely together with the business experts.\n\nPeople, not methodologies or tools, make projects successful. We enjoy agile development because its values, principles, and core practices enable people to do their best work, and testing and quality are central to agile develop- ment. In this book, we explain how to apply agile values and principles to your unique testing situation and enable your teams to succeed. We have more about that in Chapter 1, “What Is Agile Testing, Anyway?” and in Chapter 2, “Ten Principles for Agile Testers.”",
      "content_length": 2628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "PREFACE\n\nHOW WE WROTE THIS BOOK Having experienced the beneﬁts of agile development, we used agile practices to produce this book. As we began work on the book, we talked to agile testers and teams from around the globe to ﬁnd out what problems they en- countered and how they addressed them. We planned how we would cover these areas in the book.\n\nWe made a release plan based on two-week iterations. Every two weeks, we delivered two rough-draft chapters to our book website. Because we aren’t co-located, we found tools to use to communicate, provide “source code con- trol” for our chapters, deliver the product to our customers, and get their feedback. We couldn’t “pair” much real-time, but we traded chapters back and forth for review and revision, and had informal “stand-ups” daily via in- stant message.\n\nOur “customers” were the generous people in the agile community who volun- teered to review draft chapters. They provided feedback by email or (if we were lucky) in person. We used the feedback to guide us as we continued writing and revising. After all the rough drafts were done, we made a new plan to com- plete the revisions, incorporating all the helpful ideas from our “customers.”\n\nOur most important tool was mind maps. We started out by creating a mind map of how we envisioned the whole book. We then created mind maps for each section of the book. Before writing each chapter, we brainstormed with a mind map. As we revised, we revisited the mind maps, which helped us think of ideas we may have missed.\n\nBecause we think the mind maps added so much value, we’ve included the mind map as part of the opening of each chapter. We hope they’ll help you get an overview of all the information included in the chapter, and inspire you to try using mind maps yourself.\n\nOUR AUDIENCE This book will help you if you’ve ever asked any of the following excellent questions, which we’ve heard many times:\n\n(cid:2) If developers are writing tests, what do the testers do? (cid:2) I’m a QA manager, and our company is implementing agile develop- ment (Scrum, XP, DSDM, name your ﬂavor). What’s my role now?\n\nxxix",
      "content_length": 2125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "xxx\n\nPREFACE\n\n(cid:2) I’ve worked as a tester on a traditional waterfall team, and I’m really excited by what I’ve read about agile. What do I need to know to work on an agile team?\n\n(cid:2) What’s an “agile tester”? (cid:2) I’m a developer on an agile team. We’re writing code test-ﬁrst, but our customers still aren’t happy with what we deliver. What are we missing?\n\n(cid:2) I’m a developer on an agile team. We’re writing our code test-ﬁrst. We make sure we have tests for all our code. Why do we need testers? (cid:2) I coach an agile development team. Our QA team can’t keep up with us, and testing always lags behind. Should we just plan to test an iteration behind development?\n\n(cid:2) I’m a software development manager. We recently transitioned to\n\nagile, but all our testers quit. Why?\n\n(cid:2) I’m a tester on a team that’s going agile. I don’t have any program- ming or automation skills. Is there any place for me on an agile team?\n\n(cid:2) How can testing possibly keep up with two-week iterations? (cid:2) What about load testing, performance testing, usability testing, all\n\nthe other “ilities”? Where do these ﬁt in?\n\n(cid:2) We have audit requirements. How does agile development and testing\n\naddress these?\n\nIf you have similar questions and you’re looking for practical advice about how testers contribute to agile teams and how agile teams can do an effective job of testing, you’ve picked up the right book.\n\nThere are many “ﬂavors” of agile development, but they all have much in common. We support the Agile Manifesto, which we explain in Chapter 1, “What Is Agile Testing, Anyway?” Whether you’re practicing Scrum, Extreme Programming, Crystal, DSDM, or your own variation of agile development, you’ll ﬁnd information here to help with your testing efforts.\n\nA User Story for an Agile Testing Book\n\nWhen Robin Dymond, a managing consultant and trainer who has helped many teams adopt lean and agile, heard we were writing this book, he sent us the user story he’d like to have fulﬁlled. It encapsulates many of the re- quirements we planned to deliver.",
      "content_length": 2079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "PREFACE\n\nBook Story 1\n\nAs a QA professional, I can understand the main\n\ndifference between traditional QA professionals and agile\n\nteam members with a QA background, so that I can begin\n\ninternalizing my new responsibilities and deliver value to\n\nthe customer sooner and with less difﬁculty.\n\nAcceptance conditions:\n\nMy concerns and fears about losing control of testing are addressed.\n\nMy concerns and fears about having to write code (never done it) are addressed.\n\nAs a tester I understand my new value to the team.\n\nAs a tester new to Agile, I can easily read about things that are most important to my new role.\n\nAs a tester new to Agile, I can easily ignore things that are less im- portant to my new role.\n\nAs a tester new to Agile, I can easily get further detail about agile testing that is important to MY context.\n\nWere I to suggest a solution to this problem, I think of Scrum versus XP. With Scrum you get a simple view that enables people to quickly adopt Agile. However, Scrum is the tip of the iceberg for successful agile teams. For testers who are new, I would love to see agile testing ideas ex- pressed in layers of detail. What do I need to know today, what should I know tomorrow, and what context-sensitive things should I consider for continuous improvement?\n\nWe’ve tried to provide these layers of detail in this book. We’ll approach agile testing from a few different perspectives: transitioning into agile develop- ment, using an agile testing matrix to guide testing efforts, and explaining all the different testing activities that take place throughout the agile develop- ment cycle.\n\nxxxi",
      "content_length": 1619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "xxxii\n\nPREFACE\n\nHOW TO USE THIS BOOK If you aren’t sure where to start in this book, or you just want a quick over- view, we suggest you read the last chapter, Chapter 22, “Key Success Factors,” and follow wherever it leads you.\n\nPart I: Introduction\n\nIf you want quick answers to questions such as “Is agile testing different than testing on waterfall projects?” or “What’s the difference between a tester on a traditional team and an agile tester?,” start with Part I, which includes the following chapters:\n\n(cid:2) Chapter 1: What Is Agile Testing, Anyway? (cid:2) Chapter 2: Ten Principles for Agile Testers\n\nThese chapters are the “tip of the iceberg” that Robin requested in his user story. They include an overview of how agile differs from a traditional phased approach and explore the “whole team” approach to quality and testing.\n\nIn this part of the book we deﬁne the “agile testing mind-set” and what makes testers successful on agile teams. We explain how testers apply agile values and principles to contribute their particular expertise.\n\nPart II: Organizational Challenges\n\nIf you’re a tester or manager on a traditional QA team, or you’re coaching a team that’s moving to agile, Part II will help you with the organizational chal- lenges faced by teams in transition. The “whole team” attitude represents a lot of cultural changes to team members, but it helps overcome the fear testers have when they wonder how much control they’ll have or whether they’ll be expected to write code.\n\nSome of the questions answered in Part II are:\n\n(cid:2) How can we engage the QA team? (cid:2) What about management’s expectations? (cid:2) How should we structure our agile team, and where do the testers ﬁt? (cid:2) What do we look for when hiring an agile tester? (cid:2) How do we cope with a team distributed across the globe?",
      "content_length": 1835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "PREFACE\n\nPart II also introduces some topics we don’t always enjoy talking about. We explore ideas about how to transition processes and models, such as audits or SOX compliance, that are common in traditional environments.\n\nMetrics and how they’re applied can be a controversial issue, but there are positive ways to use them to beneﬁt the team. Defect tracking easily becomes a point of contention for teams, with questions such as “Do we use a defect- tracking system?” or “When do we log bugs?”\n\nTwo common questions about agile testing from people with traditional test team experience are “What about test plans?” and “Is it true there’s no docu- mentation on agile projects?” Part II clears up these mysteries.\n\nThe chapters in Part II are as follows:\n\n(cid:2) Chapter 3: Cultural Challenges (cid:2) Chapter 4: Team Logistics (cid:2) Chapter 5: Transitioning Typical Processes\n\nPart III: The Agile Testing Quadrants\n\nDo you want more details on what types of testing are done on agile projects? Are you wondering who does what testing? How do you know whether you’ve done all the testing that’s needed? How do you decide what practices, techniques, and tools ﬁt your particular situation? If these are your concerns, check out Part III.\n\nWe use Brian Marick’s Agile Testing Quadrants to explain the purpose of testing. The quadrants help you deﬁne all the different areas your testing should address, from unit level tests to reliability and other “ilities,” and ev- erything in between. This is where we get down into the nitty-gritty of how to deliver a high-quality product. We explain techniques that can help you to communicate well with your customers and better understand their require- ments. This part of the book shows how tests drive development at multiple levels. It also provides tools for your toolkit that can help you to effectively deﬁne, design, and execute tests that support the team and critique the prod- uct. The chapters include the following:\n\n(cid:2) Chapter 6: The Purpose of Testing (cid:2) Chapter 7: Technology-Facing Tests that Support the Team\n\nxxxiii",
      "content_length": 2092,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "xxxiv\n\nPREFACE\n\n(cid:2) Chapter 8: Business-Facing Tests that Support the Team (cid:2) Chapter 9: Toolkit for Business-Facing Tests that Support the Team (cid:2) Chapter 10: Business-Facing Tests that Critique the Product (cid:2) Chapter 11: Critiquing the Product Using Technology-Facing Tests (cid:2) Chapter 12: Summary of Testing Quadrants\n\nPart IV: Automation\n\nTest automation is a central focus of successful agile teams, and it’s a scary topic for lots of people (we know, because it’s had us running scared before!). How do you squeeze test automation into short iterations and still get all the stories completed?\n\nPart IV gets into the details of when and why to automate, how to overcome barriers to test automation, and how to develop and implement a test auto- mation strategy that works for your team. Because test automation tools change and evolve so rapidly, our aim is not to explain how to use speciﬁc tools, but to help you select and use the right tools for your situation. Our agile test automation tips will help you with difﬁcult challenges such as test- ing legacy code.\n\nThe chapters are as follows:\n\n(cid:2) Chapter 13: Why We Want to Automate Tests and What Holds Us Back (cid:2) Chapter 14: An Agile Test Automation Strategy\n\nPart V: An Iteration in the Life of a Tester\n\nIf you just want to get a feel for what testers do throughout the agile develop- ment cycle, or you need help putting together all the information in this book, go to Part V. Here we chronicle an iteration, and more, in the life of an agile tester. Testers contribute enormous value throughout the agile software devel- opment cycles. In Part V, we explain the activities that testers do on a daily ba- sis. We start with planning releases and iterations to get each iteration off to a good start, and move through the iteration—collaborating with the customer and development teams, testing, and writing code. We end the iteration by de- livering new features and ﬁnding ways for the team to improve the process.\n\nThe chapters break down this way:\n\n(cid:2) Chapter 15: Tester Activities in Release or Theme Planning (cid:2) Chapter 16: Hit the Ground Running",
      "content_length": 2160,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "PREFACE\n\n(cid:2) Chapter 17: Iteration Kickoff (cid:2) Chapter 18: Coding and Testing (cid:2) Chapter 19: Wrap Up the Iteration (cid:2) Chapter 20: Successful Delivery\n\nPart VI: Summary\n\nIn Chapter 21, “Key Success Factors,” we present seven key factors agile teams can use for successful testing. If you’re having trouble deciding where to start with agile testing, or how to work on improving what you’re doing now, these success factors will give you some direction.\n\nOther Elements\n\nWe’ve also included a glossary we hope you will ﬁnd useful, as well as refer- ences to books, articles, websites, and blogs in the bibliography.\n\nJUST START DOING IT—TODAY! Agile development is all about doing your best work. Every team has unique challenges. We’ve tried to present all the information that we think may help agile testers, their teams, managers, and customers. Apply the techniques that you think are appropriate for your situation. Experiment constantly, evaluate the results, and come back to this book to see what might help you improve. Our goal is to help testers and agile teams enjoy delivering the best and most valuable product they can.\n\nWhen we asked Dierk König, founder and project manager of Canoo Web- Test, what he thought was the number one success factor for agile testing, he answered: “Start doing it—today!” You can take a baby step to improve your team’s testing right now. Go get started!\n\nxxxv",
      "content_length": 1422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "ACKNOWLEDGMENTS\n\nSo many people have helped us with this book that it’s hard to know whom to thank ﬁrst. Chris Guzikowski gave us the opportunity to write this book and kept encouraging us along the way. When we were deciding whether to take on such a mammoth task, Mike Cohn gave us the sage advice that the best reason to write a book is that you have something to say. We sure have lots to say about agile testing. Fortunately, so do lots of other people who were will- ing to lend us a hand.\n\nMany thanks to Brian Marick and Mike Cohn for writing such kind fore- words. We’re honored that Mike selected our book for his signature series. We’re grateful for the many ideas and observations of his that are included in this book.\n\nBrian Marick’s “Agile Testing Matrix” has guided both of us in our agile projects for several years, and it provides the core of Part III. Thank you, Brian, for thinking up the quadrants (and so many other contributions to agile testing) and letting us use them here.\n\nWe made constant use of the agile value of feedback. Many thanks to our of- ﬁcial reviewers: Jennitta Andrea, Gerard Meszaros, Ron Jeffries, and Paul Du- vall. Each one had unique and insightful comments that helped us greatly improve the book. Gerard also helped us be more consistent and correct in our testing terminology, and contributed some agile testing success stories.\n\nSpecial thanks to two reviewers and top-notch agile testers who read every word we wrote and spent hours discussing the draft chapters with us in per- son: Pierre Veragen and Paul Rogers. Many of the good ideas in this book are theirs.\n\nxxxvii",
      "content_length": 1624,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "xxxviii\n\nACKNOWLEDGMENTS\n\nWe interviewed several teams to learn what advice they would give new agile teams and testers, and solicited success stories and “lessons learned” from colleagues in the agile testing community. Heartfelt thanks to our many con- tributors of sidebars and quotes, as well as providers of helpful feedback, in- cluding (in no particular order) Robin Dymond, Bret Pettichord, Tae Chang, Bob Galen, Erika Boyer, Grig Gheorghiu, Erik Bos, Mark Benander, Jonathan Rasmusson, Andy Pols, Dierk König, Rafael Santos, Jason Holzer, Christophe Louvion, David Reed, John Voris, Chris McMahon, Declan Whelan, Michael Bolton, Elisabeth Hendrickson, Joe Yakich, Andrew Glover, Alessandro Col- lino, Coni Tartaglia, Markus Gärtner, Megan Sumrell, Nathan Silberman, Mike Thomas, Mike Busse, Steve Perkins, Joseph King, Jakub Oleszkiewicz, Pierre Veragen (again), Paul Rogers (again), Jon Hagar, Antony Marcano, Patrick Wilson-Welsh, Patrick Fleisch, Apurva Chandra, Ken De Souza, and Carol Vaage.\n\nMany thanks also to the rest of our community of unofﬁcial reviewers who read chapters, gave feedback and ideas, and let us bounce ideas off of them, including Tom Poppendieck, Jun Bueno, Kevin Lawrence, Hannu Kokko, Titus Brown, Wim van de Goor, Lucas Campos, Kay Johansen, Adrian Howard, Henrik Kniberg, Shelly Park, Robert Small, Senaka Suriyaachchi, and Erik Petersen. And if we’ve neglected to list you here, it’s not that we value your contribution any less, it’s just that we didn’t keep good enough notes! We hope you will see how your time and effort paid off in the ﬁnished book.\n\nWe appreciate the groundwork laid by the agile pioneers who have helped us and our teams succeed with agile. You’ll ﬁnd some of their works in the bibli- ography. We are grateful for the agile teams that have given us so many open source test tools that help all of our teams deliver so much value. Some of those tools are also listed in the bibliography.\n\nThanks to Mike Thomas for taking many of the action photos of an agile team that appear in this book. We hope these photos show those of you new to agile testing and development that there’s no big mystery—it’s just good people getting together to discuss, demo, and draw pictures.\n\nThanks so much to our Addison-Wesley editorial and production team who patiently answered many questions and turned this into the professional- looking book you see here, including Raina Chrobak, Chris Zahn, John Fuller, Sally Gregg, Bonnie Granat, Diane Freed, Jack Lewis, and Kim Arney.",
      "content_length": 2526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "Lisa’s Story\n\nACKNOWLEDGMENTS\n\nI’m eternally grateful to Janet for agreeing to write this book with me. She kept us organized and on track so we could juggle book writing with our full-time jobs and personal lives. I’m fortunate to have a writing partner whose experience is complementary to mine. Like any successful agile project, this is a true team effort. This has been hard work, but thanks to Janet it has been a lot of fun, too.\n\nI’d like to thank the members of my current team at ePlan Services Inc. (formerly known as Fast401k), which I joined (thanks to Mike Cohn, our ﬁrst leader) in 2003. All of us learned so much working for Mike that ﬁrst year, and it’s a testa- ment to his leadership that we continue to improve and help the business grow. Thanks to my awesome teammates who have each helped me become a better tester and agile team member, and who were all good sports while Mike Thomas took action photos of us: Nanda Lankapalli, Tony Sweets, Jeff Thuss, Lisa Owens, Mike Thomas, Vince Palumbo, Mike Busse, Nehru Kaja, Trevor Sterritt, Steve Kives, and former but still beloved team members Joe Yakich, Jason Kay, Jennifer Riefen- berg, Matt Tierney, and Charles LeRose. I also have been lucky enough to work with the best customer team anywhere. They are too numerous to mention here, but many thanks to them, and in particular to Steve Perkins, Anne Olguin, and Zachary Shannon, who help us focus on delivering value. Thanks also to Mark and Dan Gutrich, founders and leaders of ePlan Services, for giving us all the opportu- nity to succeed with agile development.\n\nThanks to Kay and Zhon Johansen for teaching me about mind maps at Agile 2006. I hope we have put this skill to good use in creating this book.\n\nMuch gratitude to all my friends and family, whom I neglected terribly during the many months spent writing this book, and who nevertheless supported me con- stantly. There are too many to mention, but I must specially thank Anna Blake for her constant understanding and provision of donkey therapy. Chester and Ernest, the donkeys of my heart, have kept pulling me along. Dodger didn’t make the whole book-writing journey in this world, but his memory continues to lift me up. My little poodle and muse Tango was by my side every minute that I worked on this book at home, joined occasionally by Bruno, Bubba, Olive, Squiggy, Starsky, Bobcat, and Patty. Thanks to my parents for being proud of me and not complain- ing about my neglect of them during this book-writing time.\n\nI know that my husband, Bob Downing, took a deep breath when I exclaimed, “I have the chance to write another book about agile testing,” but he nevertheless encouraged me constantly and made it possible for me to ﬁnd the time to write. He kept the “no-kill shelter” running, kept our lives rolling, kept my spirits up, and sustained me with many fabulous meals. He is the light of my life.\n\nxxxix\n\n—Lisa",
      "content_length": 2915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "xl\n\nACKNOWLEDGMENTS\n\nJanet’s Story\n\nLisa and I made a great team; each of us had our own strengths. When one of us faltered and took some time to recoup, the other picked up the slack. I learned so much from Lisa (thanks go to her for giving me this opportunity), but I also found I learned a lot from myself. Just the process of articulating my thoughts helped to clarify things that had been rumbling around in my brain for a long time. The mindmaps helped immensely, so thanks to Kenji Hiranabe, who gave a work- shop at Agile 2007 and made me realize what a powerful yet simple tool mind maps can be.\n\nThis book-writing journey was an amazing experience. Thanks to the people on all the teams I worked with who provided so many of the examples in this book.\n\nIt’s been a pretty special year all the way around. During the year (or so) it took to write this book, my family increased in size. My two daughters, Dana and Susan, each gave me a grandson—those were some of the times Lisa picked up the slack. I would like to thank my granddaughter Lauren (currently three) for making me leave my computer and play. It kept me sane. Thanks to my sister Colleen who gave me long-distance encouragement many mornings using instant messenger when I was feeling overwhelmed with the sheer number of hours I was putting in.\n\nAnd a very special thanks to Jack, my husband, who moved his ofﬁce downstairs when I took over the existing one. There were times when I am sure he felt ne- glected and wondered if he even had a wife as he spend many long hours alone. However, he was there with me the whole way, encouraging and supporting me in this endeavor.\n\n—Janet",
      "content_length": 1654,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "ABOUT THE AUTHORS\n\nLisa Crispin is an agile testing practitioner and coach. She specializes in showing testers and agile teams how testers can add value and how to guide development with business-facing tests. Her mission is to bring agile joy to the software testing world and testing joy to the agile development world. Lisa joined her ﬁrst agile team in 2000, having enjoyed many years working as a programmer, analyst, tester, and QA director. Since 2003, she’s been a tester on a Scrum/XP team at ePlan Services, Inc. She frequently leads tutorials and workshops on agile testing at conferences in North America and Eu- rope. Lisa regularly contributes articles about agile testing to publications such as Better Software magazine, IEEE Software, and Methods and Tools. Lisa also co-authored Testing Extreme Programming (Addison-Wesley, 2002) with Tip House.\n\nFor more about Lisa’s work, visit her websites, www.lisa.crispin.com and www.agiletester.ca, or email her at lisa@agiletester.ca.\n\nJanet Gregory is the founder of DragonFire Inc., an agile quality process consultancy and training ﬁrm. Her passion is helping teams build quality systems. For the past ten years, she has worked as a coach and tester, intro- ducing agile practices into both large and small companies. Her focus is working with business users and testers to understand their role in agile projects. Janet’s programming background is a deﬁnite plus when she part- ners with developers on her agile teams to implement innovative agile test automation solutions. Janet is a frequent speaker at agile and testing software conferences, and she is a major contributor to the North American agile test- ing community.\n\nFor more about Janet’s work, visit her websites at www.janetgregory.ca, www.janetgregory.blogspot.com, and www.agiletester.ca, or you can email her at janet@agiletester.ca.\n\nxli",
      "content_length": 1868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Part I INTRODUCTION\n\nIn the ﬁrst two chapters, we provide an overview of agile testing, highlighting how agile testing differs from testing in a traditional phased or “waterfall” approach. We explore the “whole team” approach to quality and testing.",
      "content_length": 249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "Chapter 1\n\nWHAT IS AGILE TESTING, ANYWAY?\n\nWhole-Team Approach\n\nAgile Values\n\nWhat Is Agile Testing, Anyway?\n\nWhat We Mean by “Agile Testing”\n\nWorking on Traditional Teams\n\nCustomer Team\n\nWorking on Agile Teams\n\nHow Is Agile Testing Different\n\nA Little Context for Roles and Activities\n\nDeveloper Team\n\nTraditional vs. Agile teams\n\nInteraction\n\nLike a lot of terminology, “agile development” and “agile testing” mean different things to different people. In this chapter, we explain our view of agile, which reﬂects the Agile Manifesto and general principles and values shared by different agile methods. We want to share a common language with you, the reader, so we’ll go over some of our vocabulary. We compare and contrast agile develop- ment and testing with the more traditional phased approach. The “whole team” approach promoted by agile development is central to our attitude toward qual- ity and testing, so we also talk about that here.\n\nAGILE VALUES “Agile” is a buzzword that will probably fall out of use someday and make this book seem obsolete. It’s loaded with different meanings that apply in dif- ferent circumstances. One way to deﬁne “agile development” is to look at the Agile Manifesto (see Figure 1-1).\n\nUsing the values from the Manifesto to guide us, we strive to deliver small chunks of business value in extremely short release cycles.\n\n3",
      "content_length": 1366,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "4\n\nCHAPTER 1\n\nChapter 21, “Key Success Factors,” lists key success factors for agile testing.\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\nManifesto for Agile Software Development\n\nWe are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value:\n\nIndividuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan\n\nThat is, while there is value in the items on the right, we value the items on the left more.\n\nFigure 1-1 Agile Manifesto\n\nWe use the word “agile” in this book in a broad sense. Whether your team is practicing a particular agile method, such as Scrum, XP, Crystal, DSDM, or FDD, to name a few, or just adopting whatever principles and practices make sense for your situation, you should be able to apply the ideas in this book. If you’re delivering value to the business in a timely manner with high-quality software, and your team continually strives to improve, you’ll ﬁnd useful in- formation here. At the same time, there are particular agile practices we feel are crucial to any team’s success. We’ll talk about these throughout the book.\n\nWHAT DO WE MEAN BY “AGILE TESTING”? You might have noticed that we use the term “tester” to describe a person whose main activities revolve around testing and quality assurance. You’ll also see that we often use the word “programmer” to describe a person whose main activities revolve around writing production code. We don’t intend that these terms sound narrow or insigniﬁcant. Programmers do more than turn a speciﬁcation into a program. We don’t call them “developers,” because ev-",
      "content_length": 1737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "WHAT DO WE MEAN BY “AGILE TESTING”?\n\neryone involved in delivering software is a developer. Testers do more than perform “testing tasks.” Each agile team member is focused on delivering a high-quality product that provides business value. Agile testers work to en- sure that their team delivers the quality their customers need. We use the terms “programmer” and “tester” for convenience.\n\nSeveral core practices used by agile teams relate to testing. Agile program- mers use test-driven development (TDD), also called test-driven design, to write quality production code. With TDD, the programmer writes a test for a tiny bit of functionality, sees it fail, writes the code that makes it pass, and then moves on to the next tiny bit of functionality. Programmers also write code integration tests to make sure the small units of code work together as intended. This essential practice has been adopted by many teams, even those that don’t call themselves “agile,” because it’s just a smart way to think through your software design and prevent defects. Figure 1-2 shows a sample unit test result that a programmer might see.\n\nThis book isn’t about unit-level or component-level testing, but these types of tests are critical to a successful project. Brian Marick [2003] describes these types of tests as “supporting the team,” helping the programmers know what code to write next. Brian also coined the term “technology-facing tests,” tests that fall into the programmer’s domain and are described using pro- grammer terms and jargon. In Part II, we introduce the Agile Testing Quad- rants and examine the different categories of agile testing. If you want to learn more about writing unit and component tests, and TDD, the bibliogra- phy will steer you to some good resources.\n\nFigure 1-2 Sample unit test output\n\n5",
      "content_length": 1817,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "6\n\nCHAPTER 1\n\nLisa’s Story\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\nIf you want to know how agile values, principles, and practices applied to test- ing can help you, as a tester, do your best work, and help your team deliver more business value, please keep reading. If you’ve bothered to pick up this book, you’re probably the kind of professional who continually strives to grow and learn. You’re likely to have the mind-set that a good agile team needs to succeed. This book will show you ways to improve your organization’s prod- uct, provide the most value possible to your team, and enjoy your job.\n\nDuring a break from working on this chapter, I talked to a friend who works in quality assurance for a large company. It was a busy time of year, and management expected everyone to work extra hours. He said, “If I thought working 100 extra hours would solve our problems, I’d work ‘til 7 every night until that was done. But the truth was, it might take 4,000 extra hours to solve our problems, so working extra feels pointless.” Does this sound familiar?\n\nIf you’ve worked in the software industry long, you’ve probably had the op- portunity to feel like Lisa’s friend. Working harder and longer doesn’t help when your task is impossible to achieve. Agile development acknowledges the reality that we only have so many good productive hours in a day or week, and that we can’t plan away the inevitability of change.\n\nAgile development encourages us to solve our problems as a team. Business people, programmers, testers, analysts—everyone involved in software devel- opment—decides together how best to improve their product. Best of all, as testers, we’re working together with a team of people who all feel responsible for delivering the best possible quality, and who are all focused on testing. We love doing this work, and you will too.\n\nWhen we say “agile testing” in this book, we’re usually talking about business- facing tests, tests that deﬁne the business experts’ desired features and func- tionality. We consider “customer-facing” a synonym for “business-facing.” “Testing” in this book also includes tests that critique the product and focus on discovering what might be lacking in the ﬁnished product so that we can improve it. It includes just about everything beyond unit and component level testing: functional, system, load, performance, security, stress, usability, exploratory, end-to-end, and user acceptance. All these types of tests might be appropriate to any given project, whether it’s an agile project or one using more traditional methodologies.\n\n—Lisa",
      "content_length": 2590,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "A LITTLE CONTEXT FOR ROLES AND ACTIVITIES ON AN AGILE TEAM\n\nAgile testing doesn’t just mean testing on an agile project. Some testing ap- proaches, such as exploratory testing, are inherently agile, whether it’s done an agile project or not. Testing an application with a plan to learn about it as you go, and letting that information guide your testing, is in line with valuing working software and responding to change. Later chapters discuss agile forms of testing as well as “agile testing” practices.\n\nA LITTLE CONTEXT FOR ROLES AND ACTIVITIES ON AN AGILE TEAM We’ll talk a lot in this book about the “customer team” and the “developer team.” The difference between them is the skills they bring to delivering a product.\n\nCustomer Team\n\nThe customer team includes business experts, product owners, domain ex- perts, product managers, business analysts, subject matter experts—every- one on the “business” side of a project. The customer team writes the stories or feature sets that the developer team delivers. They provide the examples that will drive coding in the form of business-facing tests. They communi- cate and collaborate with the developer team throughout each iteration, an- swering questions, drawing examples on the whiteboard, and reviewing ﬁnished stories or parts of stories.\n\nTesters are integral members of the customer team, helping elicit require- ments and examples and helping the customers express their requirements as tests.\n\nDeveloper Team\n\nEveryone involved with delivering code is a developer, and is part of the de- veloper team. Agile principles encourage team members to take on multiple activities; any team member can take on any type of task. Many agile practi- tioners discourage specialized roles on teams and encourage all team mem- bers to transfer their skills to others as much as possible. Nevertheless, each team needs to decide what expertise their projects require. Programmers, system administrators, architects, database administrators, technical writers, security specialists, and people who wear more than one of these hats might be part of the team, physically or virtually.\n\n7",
      "content_length": 2133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "8\n\nCHAPTER 1\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\nTesters are also on the developer team, because testing is a central compo- nent of agile software development. Testers advocate for quality on behalf of the customer and assist the development team in delivering the maximum business value.\n\nInteraction between Customer and Developer Teams\n\nThe customer and developer teams work closely together at all times. Ideally, they’re just one team with a common goal. That goal is to deliver value to the organization. Agile projects progress in iterations, which are small develop- ment cycles that typically last from one to four weeks. The customer team, with input from the developers, will prioritize stories to be developed, and the developer team will determine how much work they can take on. They’ll work together to deﬁne requirements with tests and examples, and write the code that makes the tests pass. Testers have a foot in each world, understand- ing the customer viewpoint as well as the complexities of the technical imple- mentation (see Figure 1-3).\n\nSome agile teams don’t have any members who deﬁne themselves as “testers.” However, they all need someone to help the customer team write business- facing tests for the iteration’s stories, make sure the tests pass, and make sure that adequate regression tests are automated. Even if a team does have testers, the entire agile team is responsible for these testing tasks. Our experience with agile teams has shown that testing skills and experience are vital to project success and that testers do add value to agile teams.\n\nInteraction of Roles\n\nProgrammer\n\nDomain Expert\n\nTester\n\nFigure 1-3 Interaction of roles",
      "content_length": 1682,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "HOW IS AGILE TESTING DIFFERENT?\n\nHOW IS AGILE TESTING DIFFERENT? We both started working on agile teams at the turn of the millennium. Like a lot of testers who are new to agile, we didn’t know what to expect at ﬁrst. To- gether with our respective agile teams, we’ve worked on we’ve learned a lot about testing on agile projects. We’ve also implemented ideas and practices suggested by other agile testers and teams. Over the years, we’ve shared our experiences with other agile testers as well. We’ve facilitated workshops and led tutorials at agile and testing conferences, talked with local user groups, and joined countless discussions on agile testing mailing lists. Through these experiences, we’ve identiﬁed differences between testing on agile teams and testing on traditional waterfall development projects. Agile development has transformed the testing profession in many ways.\n\nWorking on Traditional Teams\n\nNeither working closely with programmers nor getting involved with a project from the earliest phases was new to us. However, we were used to strictly enforced gated phases of a narrowly deﬁned software development life cycle, starting with release planning and requirements deﬁnition and usually ending with a rushed testing phase and a delayed release. In fact, we often were thrust into a gatekeeper role, telling business managers, “Sorry, the requirements are frozen; we can add that feature in the next release.”\n\nAs leaders of quality assurance teams, we were also often expected to act as gatekeepers of quality. We couldn’t control how the code was written, or even if any programmers tested their code, other than by our personal efforts at collaboration. Our post-development testing phases were expected to boost quality after code was complete. We had the illusion of control. We usually had the keys to production, and sometimes we had the power to postpone releases or stop them from going forward. Lisa even had the title of “Quality Boss,” when in fact she was merely the manager of the QA team.\n\nOur development cycles were generally long. Projects at a company that pro- duced database software might last for a year. The six-month release cycles Lisa experienced at an Internet start-up seemed short at the time, although it was still a long time to have frozen requirements. In spite of much process and discipline, diligently completing one phase before moving on to the next, it was plenty of time for the competition to come out ahead, and the applications were not always what the customers expected.\n\n9",
      "content_length": 2548,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "10\n\nCHAPTER 1\n\nLisa’s Story\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\nTraditional teams are focused on making sure all the speciﬁed requirements are delivered in the ﬁnal product. If everything isn’t ready by the original tar- get release date, the release is usually postponed. The development teams don’t usually have input about what features are in the release, or how they should work. Individual programmers tend to specialize in a particular area of the code. Testers study the requirements documents to write their test plans, and then they wait for work to be delivered to them for testing.\n\nWorking on Agile Teams\n\nTransitioning to the short iterations of an agile project might produce initial shock and awe. How can we possibly deﬁne requirements and then test and deliver production-ready code in one, two, three, or four weeks? This is par- ticularly tough for larger organizations with separate teams for different func- tions and even harder for teams that are geographically dispersed. Where do all these various programmers, testers, analysts, project managers, and count- less specialties ﬁt in a new agile project? How can we possibly code and test so quickly? Where would we ﬁnd time for difﬁcult efforts such as automating tests? What control do we have over bad code getting delivered to production?\n\nWe’ll share our stories from our ﬁrst agile experiences to show you that ev- eryone has to start somewhere.\n\nMy ﬁrst agile team embraced Extreme Programming (XP), not without some “learn- ing experiences.” Serving as the only professional tester on a team of eight pro- grammers who hadn’t learned how to automate unit tests was disheartening. The ﬁrst two-week iteration felt like jumping off a cliff.\n\nFortunately, we had a good coach, excellent training, a supportive community of agile practitioners with ideas to share, and time to learn. Together we ﬁgured out some ins and outs of how to integrate testing into an agile project—indeed, how to drive the project with tests. I learned how I could use my testing skills and experience to add real value to an agile team.\n\nThe toughest thing for me (the former Quality Boss) to learn was that the custom- ers, not I, decided on quality criteria for the product. I was horriﬁed after the ﬁrst iteration to ﬁnd that the code crashed easily when two users logged in concur- rently. My coach patiently explained, over my strident objections, that our cus- tomer, a start-up company, wanted to be able to show features to potential customers. Reliability and robustness were not yet the issue.\n\nI learned that my job was to help the customers tell us what was valuable to them during each iteration, and to write tests to ensure that’s what they got.\n\n—Lisa",
      "content_length": 2728,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Janet’s Story\n\nHOW IS AGILE TESTING DIFFERENT?\n\nMy ﬁrst foray into the agile world was also an Extreme Programming (XP) engage- ment. I had just come from an organization that practiced waterfall with some extremely bad practices, including giving the test team a day or so to test six months of code. In my next job as QA manager, the development manager and I were both learning what XP really meant. We successfully created a team that worked well together and managed to automate most of the tests for the func- tionality. When the organization downsized during the dot-com bust, I found myself in a new position at another organization as the lone tester with about ten developers on an XP project.\n\nOn my ﬁrst day of the project, Jonathan Rasmusson, one of the developers, came up to me and asked me why I was there. The team was practicing XP, and the pro- grammers were practicing test-ﬁrst and automating all their own tests. Participating in that was a challenge I couldn’t resist. The team didn’t know what value I could add, but I knew I had unique abilities that could help the team. That experience changed my life forever, because I gained an understanding of the nuances of an agile project and determined then that my life’s work was to make the tester role a more fulﬁlling one.\n\nRead Jonathan’s Story Jonathan Rasmusson, now an Agile Coach at Rasmusson Software Consulting, but Janet’s coworker on her second agile team, explains how he learned how agile testers add value.\n\nSo there I was, a young hotshot J2EE developer excited and pumped to be developing software the way it should be developed—using XP. Until one day, in walks a new team member—a tester. It seems management thought it would be good to have a QA resource on the team.\n\nThat’s ﬁne. Then it occurred to me that this poor tester would have noth- ing to do. I mean, as a developer on an XP project, I was writing the tests. There was no role for QA here as far as I could see.\n\nSo of course I went up and introduced myself and asked quite pointedly what she was going to do on the project, because the developers were writing all the tests. While I can’t remember exactly how Janet responded, the next six months made it very clear what testers can do on agile projects.\n\nWith the automation of the tedious, low-level boundary condition test cases, Janet as a tester was now free to focus on much greater value- add areas like exploratory testing, usability, and testing the app in ways developers hadn’t originally anticipated. She worked with the\n\n11\n\n—Janet",
      "content_length": 2547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "12\n\nCHAPTER 1\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\ncustomer to help write test cases that deﬁned success for upcoming sto- ries. She paired with developers looking for gaps in tests.\n\nBut perhaps most importantly, she helped reinforce an ethos of quality and culture, dispensing happy-face stickers to those developers who had done an exceptional job (these became much sought-after badges of honor displayed prominently on laptops).\n\nWorking with Janet taught me a great deal about the role testers play on agile projects, and their importance to the team.\n\nAgile teams work closely with the business and have a detailed understanding of the requirements. They’re focused on the value they can deliver, and they might have a great deal of input into prioritizing features. Testers don’t sit and wait for work; they get up and look for ways to contribute throughout the development cycle and beyond.\n\nIf testing on an agile project felt just like testing on a traditional project, we wouldn’t feel the need to write a book. Let’s compare and contrast these test- ing methods.\n\nTraditional vs. Agile Testing\n\nIt helps to start by looking at similarities between agile testing and testing in traditional software development. Consider Figure 1-4.\n\nIn the phased approach diagram, it is clear that testing happens at the end, right before release. The diagram is idealistic, because it gives the impression there is as much time for testing as there is for coding. In many projects, this is not the case. The testing gets “squished” because coding takes longer than expected, and because teams get into a code-and-ﬁx cycle at the end.\n\nAgile is iterative and incremental. This means that the testers test each incre- ment of coding as soon as it is ﬁnished. An iteration might be as short as one week, or as long as a month. The team builds and tests a little bit of code, making sure it works correctly, and then moves on to next piece that needs to be built. Programmers never get ahead of the testers, because a story is not “done” until it has been tested. We’ll talk much more about this throughout the book.\n\nThere’s tremendous variety in the approaches to projects that agile teams take. One team might be dedicated to a single project or might be part of another",
      "content_length": 2271,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "HOW IS AGILE TESTING DIFFERENT?\n\nPhased or gated—for example, Waterfall\n\nRequirements\n\nSpecifications\n\nCode\n\nTesting\n\nRelease\n\nTime\n\nF\n\nAgile: Iterative & incremental\n\nD\n\nE\n\nD\n\nEach story is expanded, coded, and tested • Possible release after each iteration\n\nC\n\nC\n\nC\n\nA B\n\nA\n\nB\n\nA\n\nB\n\nA\n\nB\n\nIt 0\n\nIt 1\n\nIt 2\n\nIt 3\n\nIt 4\n\nFigure 1-4 Traditional testing vs. agile testing\n\nbigger project. No matter how big your project is, you still have to start some- where. Your team might take on an epic or feature, a set of related stories at an estimating meeting, or you might meet to plan the release. Regardless of how a project or subset of a project gets started, you’ll need to get a high-level un- derstanding of it. You might come up with a plan or strategy for testing as you prepare for a release, but it will probably look quite different from any test plan you’ve done before.\n\nEvery project, every team, and sometimes every iteration is different. How your team solves problems should depend on the problem, the people, and the tools you have available. As an agile team member, you will need to be adaptive to the team’s needs.\n\nRather than creating tests from a requirements document that was created by business analysts before anyone ever thought of writing a line of code, some- one will need to write tests that illustrate the requirements for each story days or hours before coding begins. This is often a collaborative effort between a\n\n13",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "14\n\nCHAPTER 1\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\nbusiness or domain expert and a tester, analyst, or some other development team member. Detailed functional test cases, ideally based on examples pro- vided by business experts, ﬂesh out the requirements. Testers will conduct manual exploratory testing to ﬁnd important bugs that deﬁned test cases might miss. Testers might pair with other developers to automate and exe- cute test cases as coding on each story proceeds. Automated functional tests are added to the regression test suite. When tests demonstrating minimum functionality are complete, the team can consider the story ﬁnished.\n\nIf you attended agile conferences and seminars in the early part of this de- cade, you heard a lot about TDD and acceptance testing but not so much about other critical types of testing, such as load, performance, security, us- ability, and other “ility” testing. As testers, we thought that was a little weird, because all these types of testing are just as vital on agile projects as they are on projects using any other development methodology. The real difference is that we like to do these tests as early in the development process as we can so that they can also drive design and coding.\n\nIf the team actually releases each iteration, as Lisa’s team does, the last day or two of each iteration is the “end game,” the time when user acceptance test- ing, training, bug ﬁxing, and deployments to staging environments can oc- cur. Other teams, such as Janet’s, release every few iterations, and might even have an entire iteration’s worth of “end game” activities to verify release readiness. The difference here is that all the testing is not left until the end.\n\nAs a tester on an agile team, you’re a key player in releasing code to produc- tion, just as you might have been in a more traditional environment. You might run scripts or do manual testing to verify all elements of a release, such as database update scripts, are in place. All team members participate in ret- rospectives or other process improvement activities that might occur for ev- ery iteration or every release. The whole team brainstorms ways to solve problems and improve processes and practices.\n\nAgile projects have a variety of ﬂavors. Is your team starting with a clean slate, in a greenﬁeld (new) development project? If so, you might have fewer challenges than a team faced with rewriting or building on a legacy system that has no automated regression suite. Working with a third party brings additional testing challenges to any team.\n\nWhatever ﬂavor of development you’re using, pretty much the same ele- ments of a software development life cycle need to happen. The difference",
      "content_length": 2714,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "WHOLE-TEAM APPROACH\n\nwith agile is that time frames are greatly shortened, and activities happen concurrently. Participants, tests, and tools need to be adaptive.\n\nThe most critical difference for testers in an agile project is the quick feed- back from testing. It drives the project forward, and there are no gatekeepers ready to block project progress if certain milestones aren’t met.\n\nWe’ve encountered testers who resist the transition to agile development, fearing that “agile development” equates with chaos, lack of discipline, lack of documentation, and an environment that is hostile to testers. While some teams do seem to use the “agile” buzzword to justify simply doing whatever they want, true agile teams are all about repeatable quality as well as efﬁ- ciency. In our experience, an agile team is a wonderful place to be a tester.\n\nWHOLE-TEAM APPROACH One of the biggest differences in agile development versus traditional devel- opment is the agile “whole-team” approach. With agile, it’s not only the testers or a quality assurance team who feel responsible for quality. We don’t think of “departments,” we just think of the skills and resources we need to deliver the best possible product. The focus of agile development is producing high- quality software in a time frame that maximizes its value to the business. This is the job of the whole team, not just testers or designated quality assurance professionals. Everyone on an agile team gets “test-infected.” Tests, from the unit level on up, drive the coding, help the team learn how the application should work, and let us know when we’re “done” with a task or story.\n\nAn agile team must possess all the skills needed to produce quality code that delivers the features required by the organization. While this might mean in- cluding specialists on the team, such as expert testers, it doesn’t limit particu- lar tasks to particular team members. Any task might be completed by any team member, or a pair of team members. This means that the team takes re- sponsibility for all kinds of testing tasks, such as automating tests and man- ual exploratory testing. It also means that the whole team thinks constantly about designing code for testability.\n\nThe whole-team approach involves constant collaboration. Testers collabo- rate with programmers, the customer team, and other team specialists—and not just for testing tasks, but other tasks related to testing, such as building infrastructure and designing for testability. Figure 1-5 shows a developer re- viewing reports with two customers and a tester (not pictured).\n\n15",
      "content_length": 2601,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "16\n\nCHAPTER 1\n\n(cid:2) WHAT IS AGILE TESTING, ANYWAY?\n\nFigure 1-5 A developer discusses an issue with customers\n\nThe whole-team approach means everyone takes responsibility for testing tasks. It means team members have a range of skill sets and experience to em- ploy in attacking challenges such as designing for testability by turning ex- amples into tests and into code to make those tests pass. These diverse viewpoints can only mean better tests and test coverage.\n\nMost importantly, on an agile team, anyone can ask for and receive help. The team commits to providing the highest possible business value as a team, and the team does whatever is needed to deliver it. Some folks who are new to ag- ile perceive it as all about speed. The fact is, it’s all about quality—and if it’s not, we question whether it’s really an “agile” team.\n\nYour situation is unique. That’s why you need to be aware of the potential testing obstacles your team might face and how you can apply agile values and principles to overcome them.",
      "content_length": 1023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "SUMMARY\n\nSUMMARY Understanding the activities that testers perform on agile teams helps you show your own team the value that testers can add. Learning the core prac- tices of agile testing will help your team deliver software that delights your customers.\n\nIn this chapter, we’ve explained what we mean when we use the term “agile testing.\n\n(cid:2) We showed how the Agile Manifesto relates to testing, with its empha- sis on individuals and interactions, working software, customer col- laboration, and responding to change.\n\n(cid:2) We provided some context for this book, including some other terms we use such as “tester,” “programmer,” “customer,” and related terms so that we can speak a common language.\n\n(cid:2) We explained how agile testing, with its focus on business value and delivering the quality customers require, is different from traditional testing, which focuses on conformance to requirements.\n\n(cid:2) We introduced the “whole-team” approach to agile testing, which\n\nmeans that everyone involved with delivering software is responsible for delivering high-quality software.\n\n(cid:2) We advised taking a practical approach by applying agile values and principles to overcome agile testing obstacles that arise in your unique situation.\n\n17",
      "content_length": 1262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Chapter 2\n\nTEN PRINCIPLES FOR AGILE TESTERS\n\nAdding Value\n\nProvide Continuous Feedback\n\nWhat Is an Agile Tester?\n\nDeliver Value to the Customer\n\nEnable Face-to-Face Communication\n\nHave Courage\n\nTen Principles for Agile Testers\n\nKeep It Simple\n\nPractice Continuous Improvement\n\nApplying Agile Principles and Values\n\nThe Agile Tesing Mind-Set\n\nRespond to Change\n\nSelf-Organize\n\nFocus on People\n\nEnjoy\n\nEveryone on an agile team is a tester. Anyone can pick up testing tasks. If that’s true, then what is special about an agile tester? If I deﬁne myself as a tester on an agile team, what does that really mean? Do agile testers need different skill sets than testers on traditional teams? What guides them in their daily activities?\n\nIn this chapter, we talk about the agile testing mind-set, show how agile val- ues and principles guide testing, and give an overview of how testers add value on agile teams.\n\nWHAT’S AN AGILE TESTER? We deﬁne an agile tester this way: a professional tester who embraces change, collaborates well with both technical and business people, and understands the concept of using tests to document requirements and drive development. Agile testers tend to have good technical skills, know how to collaborate with\n\n19",
      "content_length": 1242,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "20\n\nCHAPTER 2\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\nothers to automate tests, and are also experienced exploratory testers. They’re willing to learn what customers do so that they can better under- stand the customers’ software requirements.\n\nWho’s an agile tester? She’s a team member who drives agile testing. We know many agile testers who started out in some other specialization. A developer becomes test-infected and branches out beyond unit testing. An exploratory tester, accustomed to working in an agile manner, is attracted to the idea of an agile team. Professionals in other roles, such as business or functional an- alysts, might share the same traits and do much of the same work.\n\nSkills are important, but attitude counts more. Janet likes to say, “Without the attitude, the skill is nothing.” Having had to hire numerous testers for our agile teams, we've put a lot of thought into this and discussed it with others in the agile community. Testers tend to see the big picture. They look at the application more from a user or customer point of view, which means they’re generally customer-focused.\n\nTHE AGILE TESTING MIND-SET What makes a team “agile”? To us, an agile team is one that continually fo- cuses on doing its best work and delivering the best possible product. In our experience, this involves a ton of discipline, learning, time, experimentation, and working together. It’s not for everyone, but it’s ideal for those of us who like the team dynamic and focus on continual improvement.\n\nSuccessful projects are a result of good people allowed to do good work. The characteristics that make someone succeed as a tester on an agile team are probably the same characteristics that make a highly valued tester on any team.\n\nAn agile tester doesn’t see herself as a quality police ofﬁcer, protecting her cus- tomers from inadequate code. She’s ready to gather and share information, to work with the customer or product owner in order to help them express their requirements adequately so that they can get the features they need, and to provide feedback on project progress to everyone.\n\nAgile testers, and maybe any tester with the right skills and mind-set, are continually looking for ways the team can do a better job of producing high- quality software. On a personal level, that might mean attending local user group meetings or roundtables to ﬁnd out what other teams are doing. It",
      "content_length": 2419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "APPLYING AGILE PRINCIPLES AND VALUES\n\nalso means trying out new tools to help the team do a better job of specify- ing, executing, and automating customer requirements as tests.\n\nThe bottom line is that agile testers, like their agile teammates, enjoy learning new skills and taking on new challenges, and they don’t limit themselves to solving only testing issues. This isn’t just a trait of testers; we see it in all agile team members. Agile testers help the developer and customer teams address any kind of issue that might arise. Testers can provide information that helps the team look back and learn what’s working and what isn’t.\n\nCreativity, openness to ideas, willingness to take on any task or role, focus on the customer, and a constant view of the big picture are just some components of the agile testing mind-set. Good testers have an instinct and understanding for where and how software might fail, and how to track down failures.\n\nTesters might have special expertise and experience in testing, but a good ag- ile tester isn’t afraid to jump into a design discussion with suggestions that will help testability or create a more elegant solution. An agile testing mind- set is one that is results-oriented, craftsman-like, collaborative, eager to learn, and passionate about delivering business value in a timely manner.\n\nAPPLYING AGILE PRINCIPLES AND VALUES Individuals can have a big impact on a project’s success. We’d expect a team with more experienced and higher-skilled members to outperform a less tal- ented team. But a team is more than just its individual members. Agile values and principles promote a focus on the people involved in a project and how they interact and communicate. A team that guides itself with agile values and principles will have higher team morale and better velocity than a poorly functioning team of talented individuals.\n\nThe four value statements in the Agile Manifesto, which we presented at the start of the ﬁrst chapter, show preferences, not ultimatums, and make no statements about what to do or not to do. The Agile Manifesto also includes a list of principles that deﬁne how we approach software development. Our list of agile “testing” principles is partially derived from those principles. Because we both come from the Extreme Programming culture, we’ve adopted many of its values and underlying principles. We’ve also incorporated guidelines and principles that have worked for our teams. Your team’s own values and principles will guide you as you choose practices and make decisions about how you want to work.\n\n21",
      "content_length": 2583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "22\n\nCHAPTER 2\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\nThe principles we think are important for an agile tester are:\n\n(cid:2) Provide continuous feedback. (cid:2) Deliver value to the customer. (cid:2) Enable face-to-face communication. (cid:2) Have courage. (cid:2) Keep it simple. (cid:2) Practice continuous improvement. (cid:2) Respond to change. (cid:2) Self-organize. (cid:2) Focus on people. (cid:2) Enjoy.\n\nProvide Continuous Feedback\n\nGiven that tests drive agile projects, it’s no surprise that feedback plays a big part in any agile team. The tester’s traditional role of “information provider” makes her inherently valuable to an agile team. One of the agile tester’s most important contributions is helping the product owner or customer articulate requirements for each story in the form of examples and tests. The tester then works together with teammates to turn those requirements into execut- able tests. Testers, programmers, and other team members work to run these tests early and often so they’re continually guided by meaningful feedback. We’ll spend a lot of time in this book explaining ways to do this.\n\nWhen the team encounters obstacles, feedback is one way to help remove them. Did we deliver a user interface that didn’t quite meet customer expec- tations? Let’s write a task card reminding us to collaborate with the customer on paper prototypes of the next UI story.\n\nIs management worried about how work is progressing? Display a big visible chart of tests written, run, and passing every day. Display big-picture func- tionality coverage such as test matrices. Having trouble getting the build sta- ble? Lisa’s team displayed the number of days remaining until time to tag the build for release in order to keep everyone focused on ﬁnishing stories in time. After that became a habit, they didn’t need the visual cue anymore.\n\nDeliver Value to the Customer\n\nAgile development is about delivering value in small releases that provide ex- actly the functionality that the customer has most recently prioritized. This usually means limiting scope. It’s easy to get caught up in the customer",
      "content_length": 2123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "Lisa’s Story\n\nLisa’s Story\n\nAPPLYING AGILE PRINCIPLES AND VALUES\n\nteam’s desire for cool features. Anyone can question these additions, but a tester often recognizes the impact to the story, because they need to think about the testing repercussions.\n\nOur product owner participates in planning meetings before each iteration. Never- theless, after the iteration has started and we discuss more details about the sto- ries and how to test them, he often brings up an idea that didn’t come out during the planning, such as, “Well, it would really be nice if the selection on this report could include X, Y, and Z and be sorted on A as well.” An innocent request can add a lot of complexity to a story. I often bring in one of the programmers to talk about whether this addition can be handled within the scope of the story we had planned. If not, we ask the product owner to write a card for the next iteration.\n\nAgile testers stay focused on the big picture. We can deliver the most critical functionality in this iteration and add to it later. If we let new features creep in, we risk delivering nothing on time. If we get too caught up with edge cases and miss core functionality on the happy path, we won’t provide the value the business needs.\n\nTo ensure that we deliver some value in each iteration, our team looks at each story to identify the “critical path” or “thin slice” of necessary functionality. We complete those tasks ﬁrst and then go back and ﬂesh out the rest of the features. The worst-case scenario is that only the core functionality gets released. That’s better than delivering nothing or something that works only halfway.\n\nAgile testers take the same approach as that identiﬁed in Lisa’s story. While one of our skills is to identify test cases beyond the “happy path,” we still need to start by making sure the happy path works. We can automate tests for the happy path, and add negative and boundary tests later. Always consider what adds the most value to the customer, and understand your context. If an ap- plication is safety-critical, adding negative tests is absolutely required. The testing time needs to be considered during the estimation process to make sure that enough time is allotted in the iteration to deliver a “safe” feature.\n\nEnable Face-to-Face Communication\n\nNo team works well without good communication. Today, when so many teams are distributed in multiple geographical locations, communication is\n\n23\n\n—Lisa\n\n—Lisa",
      "content_length": 2465,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "24\n\nCHAPTER 2\n\nJanet’s Story\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\neven more vital and more of a challenge. The agile tester should look for unique ways to facilitate communication. It is a critical aspect to doing her job well.\n\nWhen I was working with one team, we had a real problem with programmers talking with the product owner and leaving the testers out of the discussion. They often found out about changes after the fact. Part of the problem was that the developers were not sitting with the testers due to logistical problems. Another problem was history. The test team was new, and the product owner was used to going straight to the programmers.\n\nI took the problem to the team, and we created a rule. We found great success with the “Power of Three.” This meant that all discussions about a feature needed a programmer, a tester, and the product owner. It was each person’s responsibility to make sure there was always a representative from each group. If someone saw two people talking, they had the right to butt into the conversation. It didn’t take very long before it was just routine and no one would consider leaving the tester out of a discussion. This worked for us because the team bought into the solution.\n\nAny time there is a question about how a feature should work or what an inter- face should look like, the tester can pull in a programmer and a business expert to talk about it. Testers should never get in the way of any direct customer- developer communication, but they can often help to make sure that com- munication happens.\n\nAgile testers see each story or theme from the customer’s point of view but also understand technical aspects and limitations related to implementing features. They can help customers and developers achieve a common lan- guage. Business people and software people often speak different languages. They have to ﬁnd some common ground in order to work together success- fully. Testers can help them develop a shared language, a project dialect, or team jargon.\n\nBrian Marick (2004) recommends that we use examples to develop this lan- guage. When Lisa’s team digresses into a philosophical discussion during a sprint planning meeting, Lisa asks the product owner for an example or us- age scenario. Testers can encourage whiteboard discussions to work through more examples. These help the customers envision their requirements more clearly. They also help the developers to produce well-designed code to meet those requirements.\n\n—Janet",
      "content_length": 2504,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "APPLYING AGILE PRINCIPLES AND VALUES\n\nFace-to-face communication has no substitute. Agile development depends on constant collaboration. Like other agile team members, the people doing testing tasks will continually seek out customer and technical team members to discuss and collaborate. When an agile tester suspects a hidden assump- tion or a misunderstood requirement, she’ll get a customer and a developer talking about it. If people in a different building or continent need to talk, they look for creative ways to replace face-to-face, real-time conversations.\n\nHave Courage\n\nCourage is a core value in XP, and practices such as test automation and con- tinuous integration allow the team to practice this value. The developers have the courage to make changes and refactor the code because they have the safety net of an automated regression suite. In this section, we talk about the emotional courage that is needed when transitioning to an agile team.\n\nHave you worked in an organization where testers were stuck in their own silo, unable to talk to either business stakeholders or other members of the technical team? While you might jump at the chance to join a collaborative agile environment, you might feel uncomfortable having to go ask the cus- tomer for examples, or ask a programmer to help automate a test or bring up a roadblock during the daily stand-up.\n\nWhen you ﬁrst join an agile team, or when your current team ﬁrsts transi- tions to agile development, it’s normal to experience fear and have a list of questions that need to be answered. How in the world are we going to be able to complete testing tasks for each story in such a short time? How will testing “keep up” with development? How do you know how much testing is enough? Or maybe you’re a functional testing manager or a quality process manager and it’s not clear to you where that role ﬁts on an agile team, and nobody has the answers. Agile testers need courage to ﬁnd the answers to those questions, but there are other reasons as well for having courage.\n\nWe need courage to let ourselves fail, knowing that at least we’ll fail fast and be able to learn from that failure. After we’ve blown an iteration because we didn’t get a stable build, we’ll start thinking of ways to ensure it doesn’t hap- pen again.\n\nWe need courage to allow others to make mistakes, because that’s the only way to learn the lesson.\n\n25",
      "content_length": 2403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "26\n\nCHAPTER 2\n\nLisa’s Story\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\nI worked on a project where the agile coach insisted that I be on a separate test- ing team (often a team of one!) whose work wasn’t included in the programmers’ tracking and velocity. I had to just go along and try this. After the release ran into trouble because testing wasn’t ﬁnished, I asked the coach if we could try things my way for an iteration or two. The whole-team approach worked much better. Each story was tested and “done” by the end of the iteration, and the customers were much happier with the results.\n\nWe need courage to ask for help, especially when the person who could pro- vide that help looks pretty busy and stressed-out himself. Climbing out of your old silo and joining in a team responsibility for success or failure takes courage. Asking a question or pointing out what you think is a ﬂaw requires courage, even in a team supported by agile values and principles. Don’t be afraid! Agile teams are open and generally accepting of new ideas.\n\nKeep It Simple\n\nKent Beck’s Extreme Programming Explained advised us to do the simplest thing that could possibly work. That doesn’t mean the ﬁrst thing you try will actually work, but it ought to be simple.\n\nAgile testers and their teams are challenged to not only produce the simplest possible software implementation but to take a simple approach to ensuring that software meets the customer requirements. This doesn’t mean that the team shouldn’t take some time to analyze themes and stories and think through the appropriate architecture and design. It does mean that the team might need to push back to the business side of the team when their re- quirements might be a bit elaborate and a simpler solution will deliver the same value.\n\nSome of us worked in software organizations where we, as testers and quality assurance staff, were asked to set quality standards. We believe this is back- wards, because it’s up to the customer team to decide what level of quality they want to pay for. Testers and other team members should provide information to customers and help them consider all aspects of quality, including nonfunc- tional requirements such as performance and security. The ultimate decisions are up to the customer. The team can help the customer make good decisions by its taking a simple, step-by-step approach to its work. Agile testing means\n\n—Lisa",
      "content_length": 2414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Chapter 9, “Tool- kit for Business- Facing Tests that Support the Team,” and Chap- ter 11, “Critiquing the Product using Technology- Facing Tests,” give examples of test tools.\n\nPart IV, “Test Auto- mation,” explains how to build a “doable” test au- tomation strategy.\n\nAPPLYING AGILE PRINCIPLES AND VALUES\n\ndoing the simplest tests possible to verify that a piece of functionality exists or that the customer’s quality standard (e.g., performance) has been met.\n\nSimple doesn’t mean easy. For testers, it means testing “just enough” with the lightest-weight tools and techniques we can ﬁnd that will do the job. Tools can be as simple as a spreadsheet or a checklist. We need to automate regres- sion tests, but we should push them down to the lowest level possible in or- der to encourage fast feedback. Even simple smoke tests might be enough for business-facing test automation.\n\nExploratory testing can be used to learn about your application and ferret out hard-to-ﬁnd bugs, but start with the basics, time-boxing side trips and evaluating how far to go with edge cases. Simplicity helps us keep our focus on risk, return on investment, and improving in the areas of greatest pain.\n\nPractice Continuous Improvement\n\nLooking for ways to do a better job is part of an agile tester’s mind-set. Of course, the whole team should be thinking this way, because the central core of agile is that the team always tries to do better work. Testers participate in team retrospectives, evaluating what’s working well and what needs to be added or tweaked. Testers bring testing issues up for the whole team to ad- dress. Teams have achieved their greatest improvements in testing and all other areas through the use of process improvement practices such as retro- spectives and impediment backlogs. Some improvement ideas might become task cards. For larger problems, teams focus on one or two issues at a time to make sure they solve the real problem and not just the symptom.\n\nAgile testers and their teams are always on the lookout for tools, skills, or practices that might help them add more value or get a better return on the customer’s investment. The short iterations of agile development make it easier to try something new for a few iterations and see whether it’s worth adopting for the long term.\n\nLearning new skills and growing professionally are important to agile testers. They take advantage of the many available free resources to improve their specialized skills, such as exploratory testing. They go to meetings and con- ferences, join mailing lists, and read articles, blogs, and books to get new ideas. They look for ways to automate (or get help from their coworkers to automate) mundane or repetitive tasks so they have more time to contribute their valuable expertise.\n\n27",
      "content_length": 2791,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "28\n\nCHAPTER 2\n\nLisa’s Story\n\nWe’ll talk more about retrospec- tives and how they can help your team practice con- tinuous improve- ment in Chapter 19, “Wrap Up the Iteration.”\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\nPierre Veragren, an SQA Lead at iLevel by Weyerhaeuser, identiﬁed a quality we often see in agile teams ourselves: “AADD,” Agile Attention Deﬁcit Disor- der. Anything not learned quickly might be deemed useless. Agile team mem- bers look for return on investment, and if they don’t see it quickly, they move on. This isn’t a negative characteristic when you’re delivering production- ready software every two weeks or even more often.\n\nRetrospectives are a key agile practice that lets the team use yesterday’s ex- perience to do a better job tomorrow. Agile testers use this opportunity to raise testing-related issues and ask the team to brainstorm ways to address them. This is a way for the team to provide feedback to itself for continual improvement.\n\nOur team had used retrospectives to great beneﬁt, but we felt we needed some- thing new to help us focus on doing a better job. I suggested keeping an “imped- iment backlog” of items that were keeping us from being as productive as we’d like to be. The ﬁrst thing I wrote in the impediment backlog was our test environ- ment’s slow response time. Our system administrator scrounged a couple of bar- gain machines and turned them into new, faster servers for our test environments. Our DBA analyzed the test database performance, found that the one-disk system was the impediment, and our manager gave the go-ahead to install a RAID for bet- ter disk access. Soon we were able to deploy builds and conduct our exploratory testing much faster.\n\nRespond to Change\n\nWhen we worked in a waterfall environment, we got used to saying, “Sorry, we can’t make this change now; the requirements are frozen. We’ll have to put that in the ﬁrst patch release.” It was frustrating for customers because they realized that they didn’t do a great job on deﬁning all their requirements up front.\n\nIn a two-week agile iteration, we might have to say, “OK, write a card for that and we’ll do it in the next iteration or next release,” but customers know they can get their change when they want it because they control the priority.\n\nResponding to change is a key value for agile practitioners, but we’ve found that it’s one of the most difﬁcult concepts for testers. Stability is what testers crave so that they can say, “I’ve tested that; it’s done.” Continuously changing requirements are a tester’s nightmare. However, as agile testers, we have to welcome change. On Wednesday, we might expect to start stories A and B\n\n—Lisa",
      "content_length": 2686,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "Lisa’s Story\n\nAPPLYING AGILE PRINCIPLES AND VALUES\n\nand then C the next Friday. By Friday, the customer could have re-prioritized and now wants stories A, X, and Y. As long as we keep talking to the cus- tomer, we can handle changes like that because we are working at the same pace with the rest of team.\n\nSome agile teams try to prepare in advance of the next iteration, perhaps by writing high-level test cases, capturing business satisfaction conditions, or documenting examples. It’s a tricky business that might result in wasted time if stories are re-prioritized or greatly changed. However, distributed teams in particular need extra feedback cycles to get ready for the iteration.\n\nOur remote team member used to be our on-site manager. He’s a key player in helping the business write and prioritize stories. He has in-depth knowledge of both the code and the business, which helps him come up with creative solutions to business needs. When he moved to India, we looked for ways to retain the beneﬁt of his expertise. Meetings are scheduled at times when he can participate, and he has regular conference calls with the product owner to talk about upcom- ing stories. We’ve had to switch from low-tech tools such as index cards to online tools that we can all use.\n\nBecause the team was willing to make changes in the way we worked, and looked for tools that helped keep him in the loop with ongoing changes, we were able to retain the beneﬁt of his expertise.\n\nSome teams have analysts who can spend more time with the business ex- perts to do some advance planning. Each team has to strike a balance be- tween brainstorming solutions ahead of time and starting from scratch on the ﬁrst day of each iteration. Agile testers go with the ﬂow and work with the team to accommodate changes.\n\nAutomated testing is one key to the solution. One thing we know for sure: No agile team will succeed doing only manual testing. We need robust automa- tion in order to deliver business value in a time frame that makes it valuable.\n\nSelf-Organize\n\nThe agile tester is part of a self-organizing agile team. The team culture im- bues the agile testing philosophy. When programmers, system administra- tors, analysts, database experts, and the customer team think continually about testing and test automation, testers enjoy a whole new perspective. Au- tomating tests is hard, but it is much easier when you have the whole team\n\n29\n\n—Lisa",
      "content_length": 2434,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "30\n\nCHAPTER 2\n\nLisa’s Story\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\nworking together. Any testing issue is easier to address when you have people with multiple skill sets and multiple perspectives attacking it.\n\nMy team is a good example of a self-organizing team. When we implemented Scrum, we had a buggy legacy system and no automated tests. Making any changes to the code was risky at best. Our manager probably had some excellent solutions to the problem, but he didn’t suggest them. Instead, we explored the issues and came up with a plan.\n\nThe programmers would start implementing new stories in a new, testable archi- tecture, using test-driven development. The testers would write manual regression test scripts, and the entire team—programmers, testers, the system administrator, and the DBA—would execute them on the last two days of every iteration. The testers (at the time, this meant me) would work on an automated regression smoke test suite through the user interface. Eventually, the architecture of the new code would let us automate functional tests with a tool such as FitNesse.\n\nWe implemented this plan in baby steps, reﬁning our approach in each iteration. Using the skills of every member of the team was a much better approach than my going off and deciding the automation strategy on my own.\n\nWhen an agile team faces a big problem, perhaps a production showstopper or a broken build, it’s everyone’s problem. The highest-priority issues are problems for the whole team to solve. Team members discuss the issue right away and decide how to and who will ﬁx it.\n\nThere’s no doubt that Lisa’s manager could have mandated that the team take this approach to solving its automation problems, but the team itself can come up with the most workable plan. When the team creates its own ap- proach and commits to it, its members adopt a new attitude toward testing.\n\nFocus on People\n\nProjects succeed when good people are allowed to do their best work. Agile values and principles were created with the aim of enabling individual and team success. Agile team members should feel safe and not have to worry about being blamed for mistakes or losing their jobs. Agile team members re- spect each other and recognize individual accomplishments. Everyone on an agile team should have opportunities to grow and develop their skills. Agile teams work at a sustainable pace that lets them follow disciplined practices and keep a fresh perspective. As the Agile Manifesto states, we value individ- uals and interactions over processes and tools.\n\n—Lisa",
      "content_length": 2565,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "ADDING VALUE\n\nIn the history of software development, testers haven’t always enjoyed parity with other roles on the development team. Some people saw testers as failed programmers or second-class citizens in the world of software development. Testers who don’t bother to learn new skills and grow professionally contribute to the perception that testing is low-skilled work. Even the term “tester” has been avoided, with job titles such as “Quality Assurance Engineer” or “Quality Analyst” and team names such as “QA Department” given preference.\n\nAgile teams that adhere to the true agile philosophy give all team members equal weight. Agile testers know they contribute unique value to their teams, and development teams have found they are more successful when their team includes people with speciﬁc testing skills and background. For exam- ple, a skilled exploratory tester may discover issues in the system that couldn’t be detected by automated functional tests. Someone with deep test- ing experience might ask important questions that didn’t occur to team members without testing experience. Testing knowledge is one component of any team’s ability to deliver value.\n\nEnjoy\n\nWorking on a team where everyone collaborates, where you are engaged in the project from start to ﬁnish, where business stakeholders work together with the development team, where the whole team takes responsibility for quality and testing, in our opinion, is nothing short of a tester’s Utopia. We’re not alone in believing that everyone should ﬁnd joy in their work. Ag- ile development rewards the agile tester’s passion for her work.\n\nOur jobs as agile testers are particularly satisfying because our viewpoint and skills let us add real value to our teams. In the next section, we’ll explore how.\n\nADDING VALUE What do these principles bring to the team? Together, they bring business value. In agile development, the whole team takes responsibility for deliver- ing high-quality software that delights customers and makes the business more proﬁtable. This, in turn, brings new advantages for the business.\n\nTeam members wear many hats, and agile development tends to avoid classi- fying people by specialty. Even with short iterations and frequent releases, it’s easy to develop a gap between what the customer team expects and what the team delivers. Using tests to drive development helps to prevent this, but you still need the right tests.\n\n31",
      "content_length": 2437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "32\n\nCHAPTER 2\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\nAgile testers not only think about the system from the viewpoint of stakehold- ers who will live with the solution but they also have a grasp of technical constraints and implementation details that face the development team. Pro- grammers focus on making things work. If they’re coding to the right require- ments, customers will be happy. Unfortunately, customers aren’t generally good at articulating their requirements. Driving development with the wrong tests won’t deliver the desired outcome. Agile testers ask questions of both customers and developers early and often, and help shape the answers into the right tests.\n\nAgile testers take a much more integrated, team-oriented approach than testers on traditional waterfall projects. They adapt their skills and experi- ence to the team and project. A tester who views programmers as adversaries, or sits and waits for work to come to her, or expects to spend more time planning than doing, is likely to cling to skills she learned on traditional projects and won’t last long on an agile team.\n\nPeril: You’re Not “Really” Part of the Team\n\nIf you’re a tester, and you’re not invited to attend planning sessions, stand- ups, or design meetings, you might be in a situation where testers are viewed as somehow apart from the development team. If you are invited to these meetings but you’re not speaking up, then you’re probably creating a percep- tion that you aren’t really part of the team. If business experts are writing sto- ries and deﬁning requirements all by themselves, you aren’t participating as a tester who’s a member of an agile team.\n\nIf this is your situation, your team is at risk. Hidden assumptions are likely to go undetected until late in the release cycle. Ripple effects of a story on other parts of the system aren’t identiﬁed until it’s too late. The team isn’t making the best use of every team member’s skills, so it’s not going to be able to produce the best possible software. Communication might break down, and it’ll be hard to keep up with what the programmers and customers are doing. The team risks being divided in an unhealthy way between developers and testers, and there’s more potential that the development team will become isolated from the customer team.\n\nHow can you avoid this peril? See if you can arrange to be located near the developers. If you can’t, at least come to their area to talk and pair test. Ask them to show you what they’re working on. Ask them to look at the test cases you’ve written. Invite yourself to meetings if nobody else has invited you. Make yourself useful by testing and providing feedback, and become a neces- sity to the team.",
      "content_length": 2715,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "SUMMARY\n\nHelp customers develop their stories and acceptance tests. Push the “whole team” attitude, and ask the team to work on testing problems. If your team is having trouble adapting to agile development, suggest experimenting with some new ideas for an iteration or two. Propose adopting the “Power of Three” rule to promote good communication. Use the information in this book to show that testers can help agile teams succeed beyond their wildest expectations.\n\nDuring story estimating and planning sessions, agile testers look at each fea- ture from multiple perspectives: business, end user, production support, and programmer. They consider the problems faced by the business and how the software might address them. They raise questions that ﬂush out assump- tions made by the customer and developer teams. At the start of each itera- tion, they help to make sure the customer provides clear requirements and examples, and they help the development team turn those into tests. The tests drive development, and test results provide feedback on the team’s progress. Testers help to raise issues so that no testing is overlooked; it’s more than functional testing. Customers don’t always know that they should men- tion their performance and reliability needs or security concerns, but testers think to ask about those. Testers also keep the testing approach and tools as simple and lightweight as possible. By the end of the iteration, testers verify that the minimum testing was completed.\n\nLines between roles on an agile team are blurred. Other team members might be skilled at the same activities that testers perform. For example, ana- lysts and programmers also write business-facing tests. As long as all testing activities are performed, an agile team doesn’t necessarily require members who identify themselves primarily as testers. However, we have found that teams beneﬁt from the skills that professional testers have developed. The ag- ile principles and values we’ve discussed will help any team do a good job of testing and delivering value.\n\nSUMMARY In this chapter, we covered principles for agile testers and the values we think an agile tester needs to possess in order to contribute effectively to an agile team.\n\n(cid:2) An “agile testing mind-set” is customer-focused, results-oriented,\n\ncraftsman-like, collaborative, creative, eager to learn, and passionate about delivering business value in a timely manner.\n\n(cid:2) Attitude is important, and it blurs the lines between testers, program-\n\nmers, and other roles on an agile team.\n\n33",
      "content_length": 2567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "34\n\nCHAPTER 2\n\n(cid:2) TEN PRINCIPLES FOR AGILE TESTERS\n\n(cid:2) Agile testers apply agile values and principles such as feedback, com- munication, courage, simplicity, enjoyment, and delivering value in order to help the team identify and deliver the customer requirements for each story.\n\n(cid:2) Agile testers add value to their teams and their organizations with\n\ntheir unique viewpoint and team-oriented approach.",
      "content_length": 418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "Part II ORGANIZATIONAL CHALLENGES\n\nWhen software development organizations implement agile development, the testing or QA team often takes the longest to make the transition. Inde- pendent QA teams have become entrenched in many organizations. When they start to adapt to a new agile organization, they encounter cultural differ- ences that are difﬁcult for them to accept. In Part II, we talk about introduc- ing change and some of the barriers you might encounter when transitioning to agile. Training is a big part of what organizations making the transition need, and it’s often forgotten. It’s also hard to see how existing processes such as audits and process improvement frameworks will work in the agile envi- ronment. Going from an independent QA team to an integrated agile team is a huge change.\n\nChapter 4, “Team Logistics,” talks about the team structure, such as where a tester actually ﬁts into the team, and the never-ending question about tester- developer ratio. We’ll also talk about hiring testers and what to look for in a successful agile tester.\n\nTraditional testing activities, such as logging bugs, keeping track of metrics, and writing test plans, might not seem like a good ﬁt in an agile project. We introduce some of the typical processes that might need special care and at- tention and discuss how to adapt existing quality processes.\n\nYou can expect to ﬁnd ways that testers and test teams accustomed to a tra- ditional waterfall type of development environment can change their orga- nizational structure and culture to beneﬁt from and add value to agile development.",
      "content_length": 1600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "Chapter 3\n\nCULTURAL CHALLENGES\n\nBe Patient\n\nQuality Philosophy\n\nLet Them Feel the Pain\n\nSustainable Pace\n\nBuild Your Credibility\n\nProfessional Development\n\nChange Doesn’t Come Easy\n\nOrganizational Culture\n\nCustomer Relationships\n\nOrganization Size\n\nBeware the Quality Police Mentality\n\nEmpower Your Team\n\nVote with Your Feet\n\nCultural Challenges\n\nCultural Change for Managers\n\nSpeaking the Manager’s Language\n\nManagement Expectations\n\nLoss of Identity\n\nAdditional Roles\n\nBarriers to Success\n\nLack of Training\n\nNot Understanding Agile Concepts\n\nTalk about Fears\n\nPast Experiences\n\nGive Team Ownership\n\nIntroducing Change\n\nCultural Differences among Roles\n\nCelebrate Success\n\nMany organizational inﬂuences can impact a project, whether it uses an agile or a traditional phased or gated approach. Organizational and team culture can block a smooth transition to an agile approach. In this chapter, we discuss fac- tors that can directly affect a tester’s role on an agile team.\n\nORGANIZATIONAL CULTURE An organizational culture is deﬁned by its values, norms, and assumptions. An organization’s culture governs how people communicate, interrelate, and make decisions, and it is easily seen by observing employee behavior.\n\n37",
      "content_length": 1222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "38\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nThe culture of an organization can impact the success of an agile team. Agile teams are best suited for organizations that allow independent thinking. For example, if a company has a hierarchical structure and encourages a directive management style for all its projects, agile teams will probably struggle. Past experiences of the organization will also affect the success of a new agile team. If a company tried agile and had poor results, people will be suspicious of trying it again, citing examples of why it didn’t work. They might even ac- tively campaign against it.\n\nOrganizational culture is too frequently not considered when attempts are made to implement an agile process, leaving people wondering why it didn’t work as promised. It’s hard to change established processes, especially if indi- viduals feel they have a stake in the status quo. Each functional group devel- ops a subculture and processes that meet their needs. They’re comfortable with the way they work. Fear is a powerful emotion, and if it is not addressed, it can jeopardize the transition to agile. If team members feel that a new agile process threatens their jobs, they’ll resist the change.\n\nWe’ll talk speciﬁcally about how organizational culture affects testers work- ing in an agile environment. The bibliography contains resources that deal with other cultural aspects that may affect teams.\n\nQuality Philosophy\n\nConsider an organization’s quality philosophy in terms of how it determines the acceptable level of software quality. Does it tolerate poor quality? Does it take customers’ quality requirements into account, or is it just concerned with getting the product into the customers’ hands as fast as it can?\n\nWhen an organization lacks an overall quality philosophy and pressures teams to get the product out without regard to quality, testers feel the pinch. A team that tries to use agile development in such an environment faces an uphill battle.\n\nSome organizations have strong, independent test teams that wield a lot of power. These teams, and their managers, might perceive that agile develop- ment will take that power away. They might fear that agile runs contrary to their quality philosophy. Evaluate your organization’s quality philosophy and the philosophy of the teams that enforce it.",
      "content_length": 2340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "ORGANIZATIONAL CULTURE\n\nPeril: Quality Police Mentality\n\nIf an existing QA team has assumed the role of “Quality Police,” its members usually enforce quality by making sure code reviews are completed and bugs are religiously entered into the defect-tracking systems. They keep metrics about the number of bugs found, and then are charged with making the ﬁnal decision as to whether to release the product.\n\nWe’ve talked to testers who brag about accomplishments such as going over a development manager’s head to force a programmer to follow coding stan- dards. We’ve even heard of testers who spend their time writing bugs about requirements that aren’t up to their standards. This kind of attitude won’t ﬂy on a collaborative agile team. It fosters antagonistic behavior.\n\nAnother risk of the “Quality Police” role is that the team doesn’t buy into the concept of building quality in, and the programmers start using testers as a safety net. The team starts communicating through the bug-tracking system, which isn’t a very effective means of communicating, so the team never “jells.”\n\nRead on for ways to help avoid this peril.\n\nCompanies in which everyone values quality will have an easier time transi- tioning to agile. If any one group has assumed ownership of quality, they’ll have to learn to share that with everyone else on the team in order to succeed.\n\nWhole-Team Ownership of Quality In Chapter 1, “What Is Agile Testing, Anyway?,” we talked about the whole- team approach to quality. For many testers and QA teams, this means a mind shift from owning quality to having a participatory role in deﬁning and maintaining quality. Such a drastic shift in attitude is difﬁcult for many testers and QA teams.\n\nTesters who have been working in a traditional setting might have a hard time adjusting to their new roles and activities. If they’ve come from an orga- nization where development and QA have an adversarial relationship, it may be difﬁcult to change from being an afterthought (if thought of at all) to be- ing an integral part of the team. It can be difﬁcult for both programmers and testers to learn to trust each other.\n\nSkills and Adaptability Much has been observed about programmers who can’t adapt to agile prac- tices—but what about testers who are used to building test scripts according to a requirements document? Can they learn to ask the questions as the code\n\n39",
      "content_length": 2394,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "40\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nis being built? Testers who don’t change their approach to testing have a hard time working closely with the rest of the development team.\n\nTesters who are used to doing only manual testing through the user interface might not understand the automated approach that is intrinsic to agile de- velopment. These testers need a lot of courage in order to face their changing roles, because changing means developing new skill sets outside their com- fort zones.\n\nFactors that Help Although there are many cultural issues to consider, most QA teams have a focus on process improvement, and agile projects encourage continuous im- provements and adaptability through the use of tools like retrospectives. Most quality assurance professionals are eager to take what they’ve learned and make it better. These people are adaptable enough to not only survive, but to thrive in an agile project.\n\nIf your organization focuses on learning, it will encourage continual process improvement. It will likely adopt agile much more quickly than organiza- tions that put more value on how they react to crises than on improving their processes.\n\nIf you are a tester in an organization that has no effective quality philosophy, you probably struggle to get quality practices accepted. The agile approach will provide you with a mechanism for introducing good quality-oriented practices.\n\nTesters need time and training, just like everyone else who is learning to work on an agile project. If you’re managing a team that includes testers, be sure to give them plenty of support. Testers are often not brought in at the beginning of a greenﬁeld project and are then expected to just ﬁt into a team that has been working together for months. To help testers adjust, you may need to bring in an experienced agile testing coach. Hiring someone who has previ- ously worked on an agile team and can serve as a mentor and teacher will help testers integrate with the new agile culture, whether they’re transition- ing to agile along with an existing team or joining a new agile development team.\n\nSustainable Pace\n\nTraditional test teams are accustomed to fast and furious testing at the end of a project, which translates into working weekends and evenings. During this end-of-project testing phase, some organizations regularly ask their teams to",
      "content_length": 2364,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "ORGANIZATIONAL CULTURE\n\nput in 50, 60, or more hours each week to try to meet a deadline. Organiza- tions often look at overtime as a measure of an individual’s commitment. This conﬂicts with agile values that revolve around enabling people to do their best work all the time.\n\nIn agile projects, you are encouraged to work at a sustainable pace. This means that teams work at a consistent pace that sustains a constant velocity that permits maintaining a high-quality standard. New agile teams tend be overly optimistic about what they can accomplish and sign up for too much work. After an iteration or two, they learn to sign up for just enough work so no overtime is needed to complete their tasks. A 40-hour week is the normal sustainable pace for XP teams; it is the amount of effort that, if put in week in and week out, allows people to accomplish the most work over the long haul while delivering good value.\n\nTeams might need to work for short bursts of unsustainable pace now and then, but it should be the exception, not the norm. If overtime is required for short periods, the whole team should be working extra hours. If it’s the last day of the sprint and some stories aren’t tested, the whole team should stay late to ﬁnish the testing, not just the testers. Use the practices and techniques recommended throughout this book to learn how to plan testing along with development and allow testing to “keep up” with coding. Until your team gets better at managing its workload and velocity, budget in extra time to help even out the pace.\n\nCustomer Relationships\n\nIn traditional software development, the relationship between the develop- ment teams and their customers is more like a vendor-supplier relationship. Even if the customer is internal, it can feel more like two separate companies than two teams working on a common goal of producing business value.\n\nAgile development depends on close involvement from customers or, at the very least, their proxies. Agile teams have invited customers to collaborate, work in the same locations if possible, and be intimately involved with the development process. Both sides learn each other’s strengths and weaknesses.\n\nThis change in the relationships needs to be recognized by both sides, and it doesn’t matter whether the customer is internal or external. An open rela- tionship is critical to the success of an agile project, where the relationship between the customer team and the development team is more like a part- nership than a vendor-supplier relationship.\n\n41",
      "content_length": 2535,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "42\n\nCHAPTER 3\n\nJanet’s Story\n\n(cid:2) CULTURAL CHALLENGES\n\nIn a large project I was on recently, the customer was actually a consortium of ﬁve companies, with one of them being the software company creating the software. Each of the companies supplied three of their best domain experts to represent their needs. There was regular communication between these on-site users and their own organizations, and they were also an integral part of the team they worked with on a daily basis.\n\nA steering committee with representatives from all ﬁve companies was kept in the loop on progress and was brought in when decisions needed to be made at a higher level.\n\nHaving a few representative domain experts, while keeping all stakeholders continually informed, is one approach to successful developer-customer col- laboration. We’ll talk about others in Part V. Customers are critical to the success of your agile project. They prioritize what will be built and have the ﬁnal say in the quality of the product. Testers work closely with customers to learn requirements and deﬁne acceptance tests that will prove that condi- tions of satisfaction are met. Testing activities are key to the development team-customer team relationship. That’s why testing expertise is so essential to agile teams.\n\nOrganization Size\n\nThe size of an organization can have great impact on how projects are run and how the structure of a company matures. The larger the organization, the more hierarchical the structure tends to be. As top-down communication channels are developed, the reporting structures become directive and less compatible with collaboration between technology and business.\n\nCommunication Challenges Some agile processes provide ways to facilitate inter-team communication. For example, Scrum has the “Scrum of Scrums,” where representatives from multiple teams coordinate on a daily basis.\n\nIf you work in a large organization where the test teams or other specialized resources are separate from the programming teams, work to ﬁnd ways to keep in constant touch. For example, if your database team is completely sep- arate, you need to ﬁnd a way to work closely with the database specialists in order to get what you need in a timely manner.\n\n—Janet",
      "content_length": 2244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "Chapter 16, “Hit the Ground Run- ning,” describes how one large organization uses functional analysts to mitigate prob- lems due to re- mote customers.\n\nChapter 15, “Tester Activities in Release or Theme Planning,” and Chapter 16, “Hit the Ground Run- ning,” talk about what testers can do to help with planning and co- ordinating with other teams.\n\nORGANIZATIONAL CULTURE\n\nAnother problem that tends to be more common in large companies is that customers might not be as accessible as they are in smaller companies. This is a big obstacle when you try to gather requirements and examples and seek to get customer involvement throughout the development cycle. One solution is to have testers or analysts with domain expertise act as customer proxies. Communication tools can help deal with such situations as well. Look for creative ways to overcome the problems inherent in big companies.\n\nConﬂicting Cultures within the Organization With large software development shops, agile development is often ﬁrst im- plemented in one team or just a few teams. If your agile team has to coordi- nate with other teams using other approaches such as phased or gated development, you have an extra set of challenges. If some of the external teams tend to be dysfunctional, it’s even harder. Even when an entire company adopts agile, some teams make the transition more successfully than others.\n\nYour team might also run into resistance from specialist teams that are feel- ing protective of their particular silos. Lisa talked to a team whose members could not get any help from their company’s conﬁguration management team, which was obviously a major obstacle. Some development teams are barred from talking directly to customers.\n\nIf third parties are working on the same system your team is working on, their cultures can also cause conﬂicts. Perhaps your team is the third party, and you’re developing software for a client. You will need to think about how to mitigate culture-based differences. Part V goes into more detail about working with other teams and third parties, but here are a few ideas to get you started.\n\nAdvanced Planning If you have to coordinate with other teams, you will need to spend time during release planning, or before the start of an iteration, to work with them. You need time to adapt your own processes to work with oth- ers’ processes, and they might need to change their processes to accommodate your requests. Consider arranging access to shared resources such as perfor- mance test specialists or load test environments, and plan your own work around others’ schedules. Your stakeholders might expect certain deliverables, such as formal test plans, that your own agile process doesn’t include. Some ex- tra planning will help you to work through these cultural differences.\n\nAct Now, Apologize Later We hesitate to make suggestions that might cause trouble, but often in a large organization, the bureaucratic wheels turn so\n\n43",
      "content_length": 2960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "44\n\nCHAPTER 3\n\nChapter 4, “Team Logistics,” talks more about sepa- rate functional teams and how they affect the agile tester.\n\n(cid:2) CULTURAL CHALLENGES\n\nslowly that your team might have to ﬁgure out and implement its own solu- tions. For example, the team that couldn’t get cooperation from the conﬁgura- tion management team simply implemented its own internal build process and kept working on getting it integrated with the ofﬁcially sanctioned process.\n\nIf there aren’t ofﬁcial channels to get what you need, it’s time to get cre- ative. Maybe testers have never talked directly to customers before. Try to arrange a meeting yourself, or ﬁnd someone who can act as a customer proxy or go-between.\n\nEmpower Your Team\n\nIn an agile project, it is important for each development team to feel empow- ered to make decisions. If you’re a manager and you want your agile teams to succeed, set them free to act and react creatively. The culture of an organiza- tion must adapt to this change for an agile project to be successful.\n\nBARRIERS TO SUCCESSFUL AGILE ADOPTION BY TEST/QA TEAMS Any change faces barriers to success. Organizational culture, as we discussed in the previous section, might be the largest obstacle to overcome. Once or- ganizational culture has become well established, it’s very hard to change. It took time for it to form, and once in place, employees become committed to the culture, which makes it extremely resistant to alteration.\n\nThis section discusses speciﬁc barriers to adoption of agile development methods that can be encountered by your testers and QA teams.\n\nLoss of Identity\n\nTesters cling to the concept of an independent QA team for many reasons, but the main reason is fear, speciﬁcally:\n\n(cid:2) Fear that they will lose their QA identity (cid:2) Fear that if they report to a development manager, they will lose sup-\n\nport and programmers will get priority\n\n(cid:2) Fear that they lack the skills to work in an agile team and will lose\n\ntheir jobs\n\n(cid:2) Fear that when they’re dispersed into development teams they won’t\n\nget the support they need\n\n(cid:2) Fear that they, and their managers, will get lost in the new organization",
      "content_length": 2175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Chapter 4, “Team Logistics,” covers ideas that can be used to help peo- ple adapt.\n\nBARRIERS TO SUCCESSFUL AGILE ADOPTION BY TEST/QA TEAMS\n\nWe often hear of QA managers asking questions such as, “My company is im- plementing agile development. How does my role ﬁt in?” This is directly re- lated to the “loss of identity” fears.\n\nAdditional Roles\n\nWe know from experience that new teams are often missing specialists or ex- pertise that might be key to their success. Lisa’s team has run into obstacles so large that the only thing to do was sit back and ask, “What role are we missing on our team that is holding us back? What do we need? Another de- veloper, another tester, a database designer?” We all know that testing is a vast ﬁeld. Maybe you need someone experienced in testing on an agile team. Or maybe you need a performance testing specialist. It’s critical that you take the time to analyze what roles your product needs to be successful, and if you need to ﬁll them from outside the team, do it.\n\nIt’s critical that everyone already on the product team understand their role or ﬁgure out what their role is now that they’re part of a new agile team. Do- ing this requires time and training.\n\nLack of Training\n\nWe hosted a session in the “Conference within a Conference” at Agile 2007 that asked people what testing-related problems they were having on their agile teams. One of the attendees told us that they split up their test organiza- tion as advocated by the agile literature. However, they put the testers into development units without any training; within three months, all of the testers had quit because they didn’t understand their new roles. Problems like these can be prevented with the right training and coaching.\n\nWhen we started working with our ﬁrst agile teams, there weren’t many re- sources available to help us learn what agile testers should do or how we should work together with our teams. Today, you can ﬁnd many practitio- ners who can help train testers to adapt to an agile environment and help test teams make the agile transition. Local user groups, conferences, seminars, online instruction, and mailing lists all provide valuable resources to testers and managers wanting to learn. Don’t be afraid to seek help when you need it. Good coaching gives a good return on your investment.\n\nNot Understanding Agile Concepts\n\nNot all agile teams are the same. There are lots of different approaches to agile development, such as XP, Scrum, Crystal, FDD, DSDM, OpenUP, and various\n\n45",
      "content_length": 2523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "46\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nmixes of those. Some self-titled “agile” teams are not, in our opinion, really practicing agile. Plenty of teams simply adopt practices that work for them regardless of the original source, or they invent their own. That’s ﬁne, but if they don’t follow any of the core agile values and principles, we question giv- ing them an agile label. Releasing every month and dispensing with docu- mentation does not equate to agile development!\n\nIf different team members have opposing notions of what constitutes “agile,” which practices they should use, or how those practices are supposed to be practiced, there’s going to be trouble. For example, if you’re a tester who is pushing for the team to implement continuous integration, but the pro- grammers simply refuse to try, you’re in a bad spot. If you’re a programmer who is unsuccessful at getting involved in some practices, such as driving de- velopment with business-facing tests, you’re also in for conﬂict.\n\nThe team must reach consensus on how to proceed in order to make a suc- cessful transition to agile. Many of the agile development practices are syner- gistic, so if they are used in isolation, they might not provide the beneﬁts that teams are looking for. Perhaps the team can agree to experiment with certain practices for a given number of iterations and evaluate the results. It could decide to seek external input to help them understand the practices and how they ﬁt together. Diverse viewpoints are good for a team, but everyone needs to be headed in the same direction.\n\nSeveral people we’ve talked to described the “mini-waterfall” phenomenon that often occurs when a traditional software development organization im- plements an agile development process. The organization replaces a six- month or year-long development cycle with a two- or four-week one, and just tries to squeeze all of the traditional SDLC phases into that short period. Nat- urally, they keep having the same problems as they had before. Figure 3-1 shows an “ideal” version of the mini-waterfall where there is a code-and-ﬁx phase and then testing—the testing comes after coding is completed but be- fore the next iteration starts. However, what really happens is that testing gets squeezed into the end of the iteration and usually drags over into the next iter- ation. The programmers don’t have much to ﬁx yet, so they start working on the next iteration. Before long, some teams are always an iteration “behind” with their testing, and release dates get postponed just as they always did.\n\nEveryone involved with delivering the product needs time and training to understand the concepts behind agile as well as the core practices. Experi- enced coaches can be used to give hands-on training in practices new to the team, such as test-driven development. In larger organizations, functional",
      "content_length": 2879,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "See the bibliogra- phy for a link to more information about XP Radar charts.\n\nBARRIERS TO SUCCESSFUL AGILE ADOPTION BY TEST/QA TEAMS\n\n47\n\nRequirements\n\nRequirements\n\nRequirements\n\nRequirements\n\nCode\n\nCode & Fix\n\nCode\n\nCode & Fix\n\nCode\n\nCode & Fix\n\nTest\n\nTest\n\nTest\n\nIteration 1\n\nIteration 2\n\nIteration 3\n\nFigure 3-1 A mini-waterfall process\n\ntest managers can become practice leads and can provide support and re- sources so that testers learn how to communicate and collaborate with their new teams. Programmers and other team members need similar help from their functional managers. Strong leadership will help teams ﬁnd ways to mi- grate away from “mini-waterfall” to true collaborative development, where coding and testing are integrated into one process.\n\nXP has developed a radar chart to help teams determine their level of adapta- tion to key XP practices. They measure ﬁve different key practices: team, pro- gramming, planning, customer, and pairing, and they show the level of adaptation to practices by teams. Figure 3-2 shows two such charts. The chart\n\nProgramming\n\nProgramming\n\nTeam\n\nPlanning\n\nTeam\n\nPlanning\n\nPairing\n\nCustomer\n\nPairing\n\nCustomer\n\nFigure 3-2 XP Radar charts",
      "content_length": 1191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "48\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\non the left shows successful adaptation, while the chart on the right shows that there are some problem areas.\n\nPast Experience/Attitude\n\nLots of people have been through changes that didn’t stick. Some develop- ment organizations have lived through a succession of the “methodology du jour.” They throw up their hands and wonder, “Why should we do it again?” People get stuck in their old, unsuccessful patterns. Even when they try something new, they might revert to bad old habits when under stress. The following are just a few examples of people resisting change due to past expe- rience and their perception of “the way things are”:\n\n(cid:2) A tester sat in his cube and wouldn’t talk with the programmers about problems he was having. He complained that programmers didn’t understand what he wanted.\n\n(cid:2) A tester couldn’t shake his existing attitude that programmers didn’t know how to write good code, or how to test it. His condescending at- titude was clear to all, and his credibility as a tester was challenged. (cid:2) A customer threw up his hands when the programmers did some-\n\nthing he didn’t like, because they “always” do what they want anyhow.\n\nWhen faced with a transition to agile development, people like this often leave without giving the new process a chance. Agile development isn’t for everyone, but training and time to experiment can help adjust attitudes. Ask everyone to be part of the solution, and work together to ﬁnd out what processes and prac- tices work best for their particular situations. The self-organizing team can be a powerful tool to use to reassure all members of the development team that they’re in control of their own destiny.\n\nCultural Differences among Roles\n\nEach new agile team member is making the transition from a different per- spective. Programmers are often used to writing production code and getting it released as quickly as possible. System administrators and database experts might be accustomed to working in their own silo, performing requests on their own schedule. Customers may never have talked directly with develop- ment team members. Testers might be used to coming in at the end of the project and not interacting much at all with programmers.\n\nIt’s no wonder a transition to agile can be scary. Teams can come up with rules and guidelines to help them communicate and work well together. For",
      "content_length": 2419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Lisa’s Story\n\nINTRODUCING CHANGE\n\nexample, Lisa joined a new agile team whose rule was that if someone asked you to pair with her, you had to agree. You might not be able to do it right that minute, but as soon as you could free yourself up, you had to go help your teammate.\n\nIdentify what people doing different activities need, and ﬁnd ways to provide it. Customers need some way to know how development is progressing and whether their conditions of satisfaction are being met. Developers need to know business priorities and requirements. Testers need ways to capture ex- amples and turn them into tests. All team members want to feel they are val- ued, ﬁrst-class team members. Each team member also needs to feel safe and to feel free to raise issues and try new ideas. Understanding the viewpoint of each role helps teams through the transition.\n\nINTRODUCING CHANGE When implementing any change, be aware of the side effects. The ﬁrst stage may be chaos; your team isn’t sure what the new processes are, some groups are loyal to old ways, and some people are unsure and disruptive. People mis- take this chaotic stage for the new status quo. To avoid this, explain the change model up front and set expectations. Expect and accept perceived chaos as you implement agile processes. Find the areas of the most pain, and determine what practices will solve the problem so that you can get some im- mediate progress out of the chaos.\n\nTalk about Fears\n\nWhen you start iterative development, use retrospectives to provide people with a place to talk about their fears and a place in which they can give feed- back. Let people know that it’s normal to be fearful. Be open; teach them it is acceptable to say they are fearful or uncomfortable. Discuss each source of fear, learn from the discussion, make decisions, and move on. Fear is a com- mon response to change. Forcing people to do something they don’t want is detrimental to positive change. Lead by example.\n\nJanet and I each joined our ﬁrst XP teams at a time when many XP practitioners didn’t see any place for testers on an XP team. XP had a “Customer Bill of Rights” and a “Programmer Bill of Rights,” but the “Tester Bill of Rights” was conspicuously absent. Tip House and I came up with our own “Tester Bill of Rights” in order to give testers the support and courage to succeed on agile teams. Over the years, many testers have told us how much this helped them and their teams learn how testers work together with other team members. I don’t like too many rules, but they can\n\n49",
      "content_length": 2547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "50\n\nCHAPTER 3\n\nChapter 18, “Cod- ing and Testing,” covers how testers and programmers work together throughout the development process.\n\n(cid:2) CULTURAL CHALLENGES\n\nbe a good thing when they help the team to overcome cultural barriers and to understand how to work in new ways. The following list presents a “Tester Bill of Rights.” We encourage you to use it to help testers integrate into agile teams.\n\nYou have the right to bring up issues related to testing, quality, and process at any time.\n\nYou have the right to ask questions of customers, programmers, and other team members and receive timely answers.\n\nYou have the right to ask for and receive help from anyone on the project teams, including programmers, managers, and customers.\n\nYou have the right to estimate testing tasks and have these included in story estimates.\n\nYou have the right to the tools you need to perform testing tasks in a timely manner.\n\nYou have the right to expect your entire team, not just yourself, to be responsi- ble for quality and testing.\n\nGive Team Ownership\n\nA critical success factor is whether the team takes ownership and has the abil- ity to customize its approach. People can change their attitudes and their perceptions if they are given the right help. Lisa was able to observe Mike Cohn work with her team as a coach. As a self-organizing team, the team had to identify and solve its own problems. Mike made sure they had the time and resources to experiment and improve. He made sure that the business understood that quality was more important than quantity or speed. Every team, even a self-organizing team, needs a leader who can effectively interact with the organization’s management team.\n\nCelebrate Success\n\nImplementing change takes time and can be frustrating, so be sure to cele- brate all successes your team achieves. Pat yourselves on the back when you meet your goal to write high-level test cases for all stories by the fourth day of the iteration. Get the team together for a trivia game or lunch when you’ve just delivered an iteration’s worth of work. Acknowledgment is important if you want a change to stick.\n\nIntegrating testers into development teams while letting them continue to re- port to a supportive QA manager is one way to ease the transition to agile de- velopment. Testers can ﬁnd ways to move from an adversarial relationship with programmers to a collaborative one. They can show how they can help\n\n—Lisa",
      "content_length": 2443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "INTRODUCING CHANGE\n\nOvercoming Resistance to Agile\n\nMark Benander, a Quality Assurance Team Lead with Quickofﬁce, was on his fourth project on an agile team. The ﬁrst was a major rewrite of their entire application, with a team of eight developers, one tester, and no test auto- mation tool. He told us about his experiences in overcoming his concerns about agile development, especially about reporting to a development manager.\n\nWe were in a matrix management type of system, where a tester reports to a development manager, but the test manager is still ofﬁ- cially the supervisor. This comforted me somewhat, but the majority of the issues I expected to occur, such as being overruled whenever I found an issue, never did. My concern wasn’t that I’d really end up thinking like a developer and just releasing anything, but that my manager, who was not a tester, wouldn’t care as much, and might not back up my concerns with the application.\n\nUltimately, I think I ended up thinking slightly more like a developer, being less concerned about some of the small bugs. My better understanding of the application’s workings made me understand that the risk and cost of ﬁxing it was potentially much more risky than the beneﬁt. I believe that thinking like this isn’t a bad thing as long as we are always mindful of the end customer impact, not just the internal cost.\n\nThe corollary to my thinking more like a developer is that the developers began thinking more like testers. I’m actually a fan of the adversarial role of the tester, but in a relaxed way. I actually give the developers gold stars (the little sticker kind you used to get on your spelling test in second grade) when they implement an area of code that is especially solid and user friendly, and I give out pink stars when they “implement” a bug that is especially heinous. They groan when I come over, wondering what I’ve found now, and take great joy in “making my job boring” by testing their code themselves and giving me nothing to ﬁnd. Needless to say, you need the right group to be able to work with this kind of faux- hostile attitude. I’ve never been in another company where this would have worked, but I’ve never worked in another company where spontaneous nerf gunﬁghts broke out either.\n\nMark’s experience matches our own and that of many other testers we’ve met who’ve moved from traditional to agile development. If you’re a tester who just joined an agile team, keep an open mind and consider how your team- mates might have different viewpoints.\n\n51",
      "content_length": 2533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "52\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nthe team understand the customers’ needs and deliver appropriate business value. They can host enjoyable activities to build good team interactions. Hav- ing cookies or chocolate available for teammates is a good way to get them to walk over to your desk! Patience and a sense of fun are big advantages.\n\nMANAGEMENT EXPECTATIONS When we think of challenges involved with adopting agile, we generally think of the actual team and the issues it encounters. However, for success- ful agile adoption, management buy-in is critical. In a phased project, man- agement gets regular updates and sign-off documents indicating the end of each phase. Upper-level managers might not understand how they’ll be able to gauge agile project progress. They might fear a loss of control or lack of “process.”\n\nCultural Changes for Managers\n\nIn an agile project, expectations change. In her previous life in waterfall projects, Janet remembers hearing comments like “this feature is 90% done” for weeks. Those types of metrics are meaningless in agile projects. There are no sign-offs to mark the end of a phase, and the “doneness” of a project isn’t measured by gates.\n\nMeaningful metrics are determined by each project team. In Scrum, sprint and release burndown charts track story completion and can give managers a measure of progress, but not any hard “dates” to use for billing customers. Test matrices can be used to track functionality test coverage but do not pro- vide sign-off documentation.\n\nThe other change that is difﬁcult for some managers to understand is letting the teams make their own technical decisions and manage their own work- loads. It’s no longer the manager who decides what is good enough. It is the team (which includes the customer) that deﬁnes the level of quality necessary to deliver a successful application.\n\nAgile teams estimate and work in smaller chunks of time than traditional teams. Rather than building in contingency, teams need to plan enough time for good design and execution in order to ensure that technical debt does not increase. Rather than managing the team’s activities at a low level, managers of agile teams focus on removing obstacles so that team members can do their best work.",
      "content_length": 2262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "Janet’s Story\n\nMANAGEMENT EXPECTATIONS\n\nI asked the vice president in charge of a large agile project what he found to be the most difﬁcult part in the new agile environment from a management perspec- tive. He said that in a traditional waterfall type project, the reports all showed that everything was going according to plan until the very end, and then everything was in a panic state and “nothing worked.”\n\nIn the agile project, there were problems every day that needed to be addressed. Agile projects were more work on a consistent basis, but at least he was getting realistic reports. There were no surprises at the end of the project.\n\nBusiness stakeholders don’t like surprises. If they can be convinced to give the team enough time and resources to make the transition, they’ll ﬁnd that agile development lets them plan more accurately and achieve business goals in steady increments.\n\nSometimes it’s actually management that drives the decision to start doing agile development. The business leaders at Lisa’s company chose to try agile development in order to solve its software crisis. To be effective, they needed to have a different set of management expectations. They needed to be sensi- tive to the difﬁculty of making big changes, especially in an organization that wasn’t functioning well.\n\nIn all cases, managers need lots of patience during what might be a long tran- sition to a high-functioning agile team. It’s their job to make sure they pro- vide the necessary resources and that they enable every individual to learn how to do high-quality work.\n\nA Testing Manager’s Transition Tale\n\nTae Chang manages a team at DoubleClick that conducts end-to-end testing to ensure that all integration points, both up and downstream from the target of change, are covered. When they implemented Scrum, the development teams were reorganized into numerous application teams. Communication problems resulted in missed dependencies, so Tae’s team stepped up to help make sure problems were detected early.\n\nTae told us, “I believe agile development effectively magniﬁed the importance of cross-team communication and a coordinated end-to-end testing effort. It was not easy to work out a noninvasive (in terms of ﬁtting into current sprint\n\n53\n\n—Janet",
      "content_length": 2263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "54\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nstructure) integration testing process; in fact, we are still tweaking it, but the overall beneﬁt of such a testing effort is apparent.” Their teams began to slide into the “mini-waterfall” trap. “In retrospect,” explains Tae, “one of the rea- sons for this is because we started with the agile process before internalizing agile practices.”\n\nKnowing that test automation and continuous integration were key, the teams at DoubleClick came up with new ideas, such as a specialized build and automation team to help the development teams cope. They brought in expert training to help them learn TDD and pair programming. They started taking steps to address their legacy system’s technical debt.\n\nTae’s team attends all of the sprint planning and review sessions, using both formal and informal communication to facilitate cross-functional communica- tion and coordinate testing and releases. He has found that it helps to keep meetings small, short, and relevant. He’s also a proponent of everyone sitting together in an open work area, as opposed to sectioned-off cubes.\n\nTae offers the following advice to testers making the transition to agile:\n\n“Agile development in general will initially frustrate testers in that they will not have access to full requirements documentation or deﬁned stages of testing. In my view of agile development, at any given moment, the tester will be engaged in tasks from multiple stages of the traditional development process. A tester can be sitting in a design session with engineering and product management (she should be taking notes here and start thinking of areas of risk where proposed code change will most likely impact) and on the same day work on automating and running test cases for the proposed changes. It's a change in mind-set, and some people are quicker to adapt than others.”\n\nTae’s experience mirrors our own and that of many other teams we’ve talked to.\n\nIf you’re a QA manager, be prepared to help your testers overcome their frus- trations with moving from deﬁned, sequential testing stages to fast-paced it- erations where they perform widely varied tasks on any given day. Help them adapt to the idea that testing is no longer a separate activity that occurs after development but that testing and coding are integrated activities.\n\nIf you’re a tester or other team member who isn’t getting the support you need in your transition to agile development, think about the difﬁculties your managers might be having in understanding agile development. Help them to understand what kinds of support you need.",
      "content_length": 2607,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Lisa’s Story\n\nMANAGEMENT EXPECTATIONS\n\nSpeaking the Manager’s Language\n\nWhat do business managers understand best? It’s the bottom line—the ROI (return on investment). To get the support you need from your manage- ment, frame your needs in a context that they can understand. Your team’s velocity translates into new features to make the business more proﬁtable. If you need time and funds to learn and implement an automated test tool, ex- plain to management that over time, automated regression tests will let your team go faster and deliver more functionality in each iteration.\n\nMy team needs big blocks of time to do risky refactoring, such as trying to split the code base into multiple modules that can be built independently. We also need time to upgrade to the latest versions of our existing tools, or to try out new tools. All of these tasks are difﬁcult to integrate into a two-week sprint when we’re also trying to deliver stories for the business.\n\nWe explained to our management that if these “engineering” tasks were put off too long, our technical debt would accumulate and our velocity would slow. The number of story points delivered each iteration would decline, and new stories would take longer to code. It would take longer and longer for the business to get the new features it needed in order to attract customers.\n\nIt was hard for the business to agree to let us devote a two-week iteration every six months to do the internal work we needed to manage our technical debt, but over time they could see the results in our velocity. Recently, one of the managers actually asked if we might need to have “engineering sprints” more often. Both the product and the team are growing, and the business wants to make sure we grow our infrastructure and tools, too.\n\nLike all members of an agile team, managers need to learn a lot of new con- cepts and ﬁgure out how they ﬁt as team members. Use big visible charts (or their virtual equivalents, as needed) to make sure they can follow the progress of each iteration and release. Look for ways to maximize ROI. Often, the busi- ness will ask for a complex and expensive feature when there is a simpler and quicker solution that delivers similar value. Make sure you explain how your team’s work affects the bottom line. Collaborate with them to ﬁnd the best way for stakeholders to express the requirements for each new feature.\n\nBudget limitations are a reality most teams face. When resources are limited, your team needs to be more creative. The whole-team approach helps. Per- haps, like Lisa’s team, your team has a limited budget to buy software, and so\n\n55\n\n—Lisa",
      "content_length": 2637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "56\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nyou tend to look at open-source test automation tools that usually don’t have a large up-front purchase cost. A tool that uses the same language as the ap- plication won’t help the non-programming testers unless the programmers collaborate with them to automate the tests. Leveraging all of the expertise on the team helps you work within the business limitations.\n\nAs with all challenges your team encounters, experiment with new ways that the development team and management can help each other to build a valu- able product. At the same time, regardless of your development approach, you might have to make sure that some processes, such as conformance to audit requirements, receive the necessary attention.\n\nCHANGE DOESN’T COME EASY Agile development might seem fast-paced, but change can seem glacial. Teams that are new to agile will be slow to master some practices they’ve committed to using. We’ve met many testers who are frustrated that their “agile” develop- ment cycles are actually mini-waterfall cycles. These testers are still getting squeezed; it just happens more often. Iterations are over before stories can be tested. Programmers refuse or aren’t able to adopt critical practices such as TDD or pairing. The team leaves responsibility for quality in the hands of the testers, who are powerless to make changes to the process.\n\nThere’s no magic that you can use to get your team to make positive changes, but we have some tips for testers who want to get their teams to change in positive ways.\n\nBe Patient\n\nNew skills such as TDD are hard. Find ways to help your team get time to master them. Find changes you can make independently while you wait. For example, while programmers learn to write unit tests, implement a GUI test tool that you can use with minimal help. Help the team make baby steps. Re- member that when people panic, they go back to their old habits, even though those habits didn’t work. Focus on tiny positive increments.\n\nLet Them Feel Pain\n\nSometimes you just have to watch the train wreck. If your suggestions for im- provement were rebuffed, and the team fails, bring your suggestion up again and ask the team to consider trying it for a few iterations. People are most willing to change in the areas where they feel the most pain.",
      "content_length": 2321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "See the bibliogra- phy for some good resources on being an effec- tive change agent for your team.\n\nCHANGE DOESN’T COME EASY\n\nBuild Your Credibility\n\nYou might now be working with programmers who haven’t worked closely with testers before. Show them how you can help. Go to them with issues you’ve found rather than opening bug reports. Ask them to review code with you before they check it in. When they realize you’re contributing real value, they’re more likely to listen to your ideas.\n\nWork On Your Own Professional Development\n\nRead books and articles, go to user group meetings and conferences, and learn a new tool or scripting language. Start learning the language your ap- plication is coded in, and ask the programmers on your team if you can pair with them or if they’ll tutor you. Your coworkers will respect your de- sire to improve your skills. If your local user group is willing to listen to your presentation on agile testing, or a software newsletter publishes your automation article, your teammates might notice you have something worth hearing too.\n\nBeware the Quality Police Mentality\n\nBe a collaborator, not an enforcer. It might bug you if programmers don’t follow coding standards, but it’s not your job to make sure that they do so. Raise your issues with the team and ask for their help. If they ignore a critical problem that is really hurting the team, you might need to go to your coach or manager for help. But do that in a “please help me ﬁnd a solution” vein rather than a “make these people behave” one. If you’re seeing a problem, chances are high that others see it too.\n\nVote with Your Feet\n\nYou’ve been patient. You’ve tried every approach you can think of, but your management doesn’t understand agile development. The programmers still throw buggy, untestable code “over the wall,” and that code is released as is despite your best efforts, including working 14-hour days. Nobody cares about quality, and you feel invisible despite your best efforts. It might be time to look for a better team. Some teams are happy the way they are and simply don’t feel enough pain to want to change. Lisa worked on a team that thrived on chaos, because there were frequent opportunities to ﬁgure out why the server crashed and be a hero. Despite a successful project using agile prac- tices, they went back to their old habits, and Lisa ﬁnally gave up trying to change them.\n\n57",
      "content_length": 2405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "58\n\nCHAPTER 3\n\n(cid:2) CULTURAL CHALLENGES\n\nSUMMARY In this chapter, we talked about how cultural issues can affect whether testers and their teams can make a successful transition to doing agile development.\n\n(cid:2) Consider organizational culture before making any kind of change. (cid:2) Testers have an easier time integrating into agile teams when their\n\nwhole organization values quality, but testers with a “quality police” mind-set will struggle.\n\n(cid:2) Some testers might have trouble adjusting to the “whole team” own- ership of quality, but a team approach helps overcome cultural differences.\n\n(cid:2) Customer teams and developer teams must work closely together, and we showed how testers can be key in facilitating this relationship. (cid:2) Large organizations that tend to have more isolated specialist teams face particular cultural challenges in areas such as communication and collaboration.\n\n(cid:2) Major barriers to success for testers for agile adoption include fear, loss of identity, lack of training, previous negative experiences with new development processes, and cultural differences among roles. (cid:2) To help introduce change and promote communication, we suggest encouraging team members to discuss fears and celebrating every success, no matter how small.\n\n(cid:2) Guidelines such as a “Tester Bill of Rights” give testers conﬁdence to raise issues and help them feel safe as they learn and try new ideas. (cid:2) Managers face their own cultural challenges, and they need to provide\n\nsupport and training to help testers succeed on agile teams.\n\n(cid:2) Testers can help teams accommodate manager expectations by pro- viding the information managers need to track progress and deter- mine ROI.\n\n(cid:2) Change doesn’t come easy, so be patient, and work on improving\n\nyour own skills so you can help your team.",
      "content_length": 1850,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "Chapter 4\n\nTEAM LOGISTICS\n\nSelf-Organizing Team\n\nInvolving Other Teams\n\nIndependent QA Team\n\nEvery Team Member Has Equal Value\n\nBuilding a Team\n\nTeam Structure\n\nIntegrating Testers into Agile Project\n\nPerformance and Rewards\n\nAgile Project Teams\n\nWhat You Can Do\n\nTeam Logistics\n\nTester-Developer Ratio\n\nResources\n\nHiring an Agile Tester\n\nPhysical Logistics\n\nAgile teams stress that face-to-face communication is critical to the success of a project. They also encourage using the “whole-team” approach. What does this mean to the testers? This chapter talks about some of the issues involving team structure and physical logistics. There’s more to creating a cohesive team than just moving chairs and desks.\n\nTEAM STRUCTURE Having separate functional groups can make life difﬁcult for agile teams. Constant communication is critical. Team members need to work closely with one another, whether the work is done virtually or in the same physical location.\n\nWe use the terms “QA team” and “test team” interchangeably here. It can be argued whether “QA teams” are really doing quality assurance or not, but the term has become a common one attached to test teams, so we use it too.\n\n59",
      "content_length": 1183,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "60\n\nCHAPTER 4\n\n(cid:2) TEAM LOGISTICS\n\nIndependent QA Teams\n\nMany organizations, both large and small, think it is important to have an in- dependent QA or test team in order to get an honest opinion about the quality of a product. We’re often asked the questions, “Is there a place for a test organi- zation in the whole-team approach?” and “If so, what is its role?”\n\nSome of the reasons we’re given for wanting to keep the QA team separate from the development team are:\n\n(cid:2) It is important to have that independent check and audit role. (cid:2) The team can provide an unbiased and outside view relating to the\n\nquality of the product.\n\n(cid:2) If testers work too closely with developers, they will start to think like\n\ndevelopers and lose their customer viewpoint.\n\n(cid:2) If the testers and developers report to the same person, there is a dan- ger that the priority becomes delivering any code rather than deliver- ing tested code.\n\nTeams often confuse “independent” with “separate.” If the reporting struc- ture, budgets, and processes are kept in discrete functional areas, a division between the programmers and testers is inevitable. This can lead to friction, competition, and an “us versus them” attitude. Time is wasted on duplicate meetings, programmers and testers don’t share a common goal, and infor- mation sharing is nonexistent.\n\nThere are reasons for having a QA manager and an independent test team. However, we suggest changing the reasons as well as the structure. Rather than keeping the testers separate as an independent team to test the applica- tion after coding, think about the team as a community of testers. Provide a learning organization to help your testers with career development and a place to share ideas and help each other. If the QA manager becomes a prac- tice leader in the organization, that person will be able to teach the skills that testers need to become stronger and better able to cope with the ever-changing environment.\n\nWe don’t believe that integrating the testers with the project teams prevents testers from doing their jobs well. In fact, testers on agile teams feel very strongly about their role as customer advocate and also feel they can inﬂu- ence the rest of the team in quality thinking.",
      "content_length": 2262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "TEAM STRUCTURE\n\nIntegration of Testers into an Agile Project\n\nThe whole-team approach in agile development has provoked many organi- zations that have adopted agile development to disband their independent QA teams and send their testers to work with the project groups. While this sounds great, some organizations have found that it doesn’t work as ex- pected. More than one organization has had most, if not all, of their testers quit when they found themselves on an agile development team with no idea what they should be doing.\n\nDevelopers get training on pair programming, test-driven development, and other agile practices, while testers often seem to get no training at all. Many organizations fail to recognize that testers also need training on pair testing, working with incomplete and changing requirements, automation, and all of the other new skills that are required. It’s critical that testers receive training and coaching so that they can acquire the skills and understanding that will help them succeed, such as how to work with customers to write business- facing tests. Programmers also might need coaching to understand the im- portance of business-facing tests and the whole-team approach to writing and automating tests.\n\nJanet has helped integrate several independent test teams into agile projects. She ﬁnds that it can take up to six months for most testers to start feeling conﬁdent about working with the new process.\n\nThe pairing of programmers and testers can only improve communication about the quality of the product. Developers often need to observe the be- havior of the application on the tester’s workstation if that behavior can’t be reproduced in the development environment. Testers can sometimes sit down with the developer to reproduce a problem more easily and quickly than they can by trying to record the steps in a defect. This interaction re- duces the time spent on non-oral communication.\n\nComments we’ve heard from testers on this subject include the following:\n\n(cid:2) “Being closer to the development of the product makes me a better\n\ntester.”\n\n(cid:2) “Going to lunch with developers builds a better team, one that wants\n\nand likes to work together.”\n\n61",
      "content_length": 2209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "62\n\nCHAPTER 4\n\nLisa’s Story\n\n(cid:2) TEAM LOGISTICS\n\nOne major advantage of an integrated project team is that there’s only one budget and one schedule. There is no “testing” time to cut if all of the func- tionality is not ﬁnished. If there is no time to test a new feature, then there is no time to develop it in the ﬁrst place. The whole-team approach to taking responsibility for quality is very powerful, as we point out throughout this book.\n\nI once joined an XP team that had been depending solely on unit-level testing and had never had anyone in a tester role before. Their customer wasn’t all that happy with the results, so they decided to hire a tester. While I attended daily stand-ups, I wasn’t allowed to talk about testing tasks. Testing time wasn’t included in story estimates, and testing tasks weren’t part of iteration planning. Stories were marked “done” as soon as coding tasks were complete.\n\nAfter the team missed the release date, which was planned for after three two- week iterations, I asked the team’s coach to try the whole-team approach to test- ing. Testing tasks went up on the board along with coding tasks. Stories were no longer considered done until testing tasks were ﬁnished. Programmers took on testing tasks, and I was a full participant in daily stand-ups. The team had no more issues meeting the release plans they set.\n\nTesters need to be full-ﬂedged members of the development team, and test- ing tasks need to be given the same attention as other tasks. Again, the whole-team approach to testing goes a long way toward ensuring that testing tasks are completed by the end of each iteration and release. Be sure to use retrospectives to evaluate what testers need to integrate with their new agile team and what skills they might need to acquire. For example, testers might need more support from programmers, or from someone who’s an expert in a particular type of testing.\n\nA smart approach to planning the organizational changes for agile develop- ment makes all the difference to a successful transition. Ask the QA and devel- opment managers to ﬁgure out their own roles in the new agile organization. Let them plan how they will help their testers and developers be productive on the new agile teams. Provide training in agile practices that the team doesn’t know. Make sure all of the teams can communicate with each other. Provide a framework that lets each team learn as it goes, and the teams will ﬁnd a way to succeed.\n\n—Lisa",
      "content_length": 2481,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "See the bibliogra- phy for a link to some of Chris- tophe’s writings on managing agile teams.\n\nTEAM STRUCTURE\n\nTransitioning QA and Engineering Teams—Case Study\n\nChristophe Louvion is a CTO and agile coach for high-proﬁle Internet compa- nies. He told us about one experience he had while helping his company implement agile development. As the agile coach, he wanted to truly imple- ment agile development and avoid the common “small waterfall” mistake, where the developers spend a week writing code and the testers spend the next week testing it.\n\nHis company at the time was an organization of about 120 engineers, including the internal IT departments. Before transitioning to Scrum, the company was organized functionally. There were directors of QA and Engineering, and the idea of product-based teams was hard for management to accept. The manag- ers of these teams struggled with the following question: “What is my job now?” Christophe turned this around on the managers and said: “You tell me.”\n\nHe worked with the Engineering and QA managers to help them ﬁgure out what their jobs would be in the new agile environment. Only when they were able to speak with one voice did they all go to the teams and explain their ﬁndings.\n\nIn the new agile organization, managers deal with speciﬁc domain knowl- edge, resources, prioritization, and problems that arise. The Engineering and QA managers work hand-in-hand on a daily basis to resolve these types of issues. Christophe and the two managers looked at what prevented testers from being productive in the ﬁrst week of the two-week iteration and taught them how to help with design.\n\nFor the programmers, the question was “How do I make it so that the code is easy to test?” The engineers weren’t trained in continuous integration, because they were used to working in phased cycles. They needed lots of training in test-driven design, continuous integration, and other practices. Their managers ensured that they got this training.\n\nConﬁguration management (CM) experts were brought in to help with the build process. The CM team is separate from Engineering and QA at the com- pany, and it provides the framework for everything in the build process, including database objects, hardware, and conﬁgurations. Once the build process framework was implemented, integrating coding and testing was much easier to talk about.\n\nHaving management ﬁgure out their new roles ﬁrst, and then getting a build process framework in place with everything in source code control, were key to the successful transition to agile. Another success factor was having repre- sentatives from all teams—Engineering, QA, the CM, network, and the system administrator groups and product teams—participate in daily stand-ups and planning activities. This way, when testing issues came up, they could be addressed by everyone who could help. As Christophe says, their approach integrates everyone and puts a focus on testing.\n\n63",
      "content_length": 2958,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "64\n\nCHAPTER 4\n\n(cid:2) TEAM LOGISTICS\n\nAgile Project Teams\n\nAgile project teams are generally considered cross-functional, because each team has members from many different backgrounds. The difference be- tween a traditional cross-functional team and an agile team is the approach to the whole-team effort. Members are not just “representing” their func- tions in the team but are becoming true members of the team for as long as the project or permanent team exists (see Figure 4-1).\n\nBecause projects vary in size, project teams might vary in structure. Organi- zations with large projects or many projects that happen simultaneously are having success using a matrix-type structure. People from different func- tional areas combine to form a virtual team while still reporting back to their individual organizational structures. In a large organization, a pool of testers might move from project to project. Some specialists, such as security or per- formance testers, might be shared among several teams. If you’re starting up a project, identify all of the resources the project will need. Determine the number of testers required and the skill set needed before you start. The testers start with the team and keep working until the project is complete, and at that time they go on to the next project.\n\nWhile testers are part of the team, their day-to-day work is managed the same as the rest of the project team’s work. A tester can bounce new ideas off of the larger tester community, which includes testers on different project teams across a large organization. All testers can share knowledge and ideas. In organizations that practice performance reviews, the QA manager (if there is one) might drive the reviews and get input from the project team.\n\nFunctional Teams\n\nAgile Team\n\nProgrammers\n\nBusiness Analysts\n\nProgrammer\n\nDomain Expert\n\nTesters\n\nTester\n\nFigure 4-1 Traditional functional teams structure vs. agile team structure",
      "content_length": 1942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Janet’s Story\n\nPHYSICAL LOGISTICS\n\nAs with any new team, it takes a while for a team to jell. If the length of the project is short and the teams are constantly changing, the organization needs to be aware that the ﬁrst iteration or two of every project will include the new team members getting used to working with each other. Refactor your organization as needed, and remember to include your customers. The best teams are those that have learned to work together and have developed trust with one another.\n\nPHYSICAL LOGISTICS Many organizations that are thinking of adopting agile try to create project teams without co-locating the team in an open-plan environment. To sup- port agile values and principles, teams work better when they have ready ac- cess to all team members, easy visibility of all project progress charts, and an environment that fosters communication.\n\nTesters and customers sitting close to the programmers enable the necessary communalization. If logistics prohibit co-location, teams can be inventive.\n\nI worked in a team where space prevented all team members from sitting together. The programmers had an area where they could pair-program with ease, but the testers and the customer were seated in another area. At ﬁrst, it was the testers that made the trip to the storyboard area where the programmers sat to participate in stand-ups and whenever they had a question for one of the pro- grammers. Few of the programmers made the trip (about 50 feet) to the testers’ area. I started keeping a candy dish handy with treats and encouraged the devel- opers to take some as often as they wanted. But there was one rule—they needed to ask a question of one of the testers if they came for candy. Over time, the walk got shorter for all team members. No one side was doing all of the walk- ing, and communication ﬂourished.\n\nTeam size offers different types of challenges to the organization. Small teams mean small areas, so it is usually easier to co-locate members. Large teams might be spread globally, and virtual communication tools are needed. Co- locating large teams usually means renovating existing space, which some or- ganizations are reluctant to do. Understand your constraints, and try to ﬁnd solutions to the problems your team encounters rather than just accepting things as “the way it is.”\n\n65\n\n—Janet",
      "content_length": 2347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "66\n\nCHAPTER 4\n\nJanet’s Story\n\n(cid:2) TEAM LOGISTICS\n\nOne team I worked on started in one corner of the ﬂoor, but expanded over the course of three years, gradually taking over 70% of the ﬂoor. Walls were taken down, ofﬁces removed, and large open areas created. The open areas and pods of teams worked well, but all the open space meant wall space was lost. Windows became story boards and whiteboards, and rolling whiteboards were ordered that could be used as teams needed them.\n\nCo-located teams don’t always live in a perfect world, and distributed teams have a another set of challenges. Distributed teams need technology to help them communicate and collaborate. Teleconferencing, video conferencing, webcams, and instant messaging are some tools that can promote real-time collaboration for teams in multiple locations.\n\nWhether teams are co-located or distributed, the same questions usually come up about what resources are needed on an agile team and how to ob- tain them. We’ll discuss these in the next section.\n\nRESOURCES New agile team members and their managers have lots of questions about the makeup of the team. Can we use the same testers that we had with our tradi- tional projects, or do we need to hire a different type of tester? How many testers will we need? Do we need people with other specialized skills? In this section, we talk a little about these questions.\n\nTester-Developer Ratio\n\nThere have been many discussions about the “right” ratio of the number of testers to the number of developers. This ratio has been used by organiza- tions to determine how many testers are needed for a project so that they can hire accordingly. As with traditional projects, there is no “right” ratio, and each project needs to be evaluated on its own. The number of testers needed will vary and depends upon the complexity of the application, the skill set of the testers, and the tools used.\n\nWe have worked on teams with a tester-developer ratio of anywhere from 1:20 to 1:1. Here are a couple of our experiences.\n\n—Janet",
      "content_length": 2040,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Janet’s Story\n\nLisa’s Story\n\nRESOURCES\n\nI worked on a project with a 1:10 ratio that developed a message-handling sys- tem. There was very little GUI, and I manually tested that part of the application, looking at usability and how well it matched the customer’s expectations. The pro- grammers did all of the automated regression testing while I worked with them to validate the effectiveness of the test cases written. I pair-tested stories with the developers, including load testing speciﬁc stories.\n\nI never felt that I didn’t have enough time to do the testing I needed to, because the developers believed that quality was the whole team’s responsibility.\n\nI was once the only professional tester on a team of up to 20 programmers devel- oping a content management system on an Internet shopping website. The team began to get really productive when the programmers took on responsibility for both manual testing and test automation. One or two programmers wore a “tester hat” for each iteration, writing customer-facing tests ahead of coding and per- forming manual tests. Additional programmers picked up the test automation tasks during the iteration.\n\nConversely, my current team has had two testers for every three to ﬁve program- mers. The web-based ﬁnancial application we produce has highly complex busi- ness logic, is high risk, and test intensive. Testing tasks often add up to the same amount of time as programming tasks. Even with a relatively high tester–programmer ratio, programmers do much of the functional test automation and sometimes pick up manual testing tasks. Specialized testing tasks such as writing high-level test cases and detailed customer-facing tests are usually done by the testers.\n\nRather than focus on a ratio, teams should evaluate the testing skills they need and ﬁnd the appropriate resources. A team that takes responsibility for testing can continually evaluate whether it has the expertise and bandwidth it needs. Use retrospectives to identify whether there’s a problem that hiring more testers would solve.\n\nHiring an Agile Tester\n\nAs we discussed in Chapter 2, “Ten Principles for Agile Testers,” there are cer- tain qualities that make a tester suited to working on an agile team. We don’t want to go into a lot of detail about what kind of tester to hire, because every team’s need is different. However, we do believe that attitude is an important factor. Here’s a story of how Lisa’s team struggled to hire a new agile tester.\n\n67\n\n—Janet\n\n—Lisa",
      "content_length": 2502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "68\n\nCHAPTER 4\n\nLisa’s Story\n\n(cid:2) TEAM LOGISTICS\n\nOur ﬁrst attempt at recruiting another tester was not very successful. The ﬁrst job posting elicited many responses, and we interviewed three candidates without ﬁnding a good ﬁt. The programmers wanted someone “techie,” but we also needed someone with the skills to collaborate with business people and help them to produce examples and requirements. We struggled to determine the con- tent of the job posting in order to attract candidates with the right attitude and mind-set.\n\nAfter soliciting opinions and suggestions from Janet and other colleagues in the agile testing community, we decided to look for a tester with the mind-set that is described in Chapter 2. We changed the job posting to include items such as these:\n\nExperience writing black box and GUI test cases, designing tests to mitigate risks, and helping business experts deﬁne requirements\n\nExperience writing simple SQL queries and insert/update statements and basic grasp of Oracle or another relational database\n\nAt least one year of experience with some scripting or programming language and/or open source test tools\n\nAbility to use basic Unix commands\n\nExperience collaborating with programmers and business experts\n\nExperience in context-based, exploratory, or scenario testing a plus\n\nAbility to work as part of a self-organizing team in which you determine your tasks on a daily basis in coordination with coworkers rather than waiting for work to be assigned to you\n\nThese requirements brought candidates more suited to an agile testing job. I pro- ceeded carefully with screening, ruling out people with a “quality police” mental- ity. Testers who pursued professional development and showed interest in agile development were more likely to have the right mind-set. The team needed someone who would be strong in the area of test tools and automation, so a pas- sion for learning was paramount.\n\nThis more creative approach to recruiting a tester paid off. At that time, it wasn’t easy to ﬁnd good “agile tester” candidates, but subsequent searches went more smoothly. We found that posting the tester position in less obvious places, such as a Ruby mailing list or the local agile user group, helped reach a wider range of suit- able candidates.\n\nHiring an agile tester taught me a lot about the agile testing mind-set. There are testers with very good skill sets who would be valuable to any traditional test team but would not be a good ﬁt on an agile team because of their attitude toward testing.\n\n—Lisa",
      "content_length": 2543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "BUILDING A TEAM\n\nWe need to consider more than just the roles that testers and programmers perform on the team. No matter what role you’re trying to ﬁll, the most im- portant consideration is how that person will ﬁt on your team. With the agile whole-team approach, specialists on the team might be asked to step outside their areas of expertise and pitch in on other activities. Each team member needs to have a strong focus on quality and delivering business value. Con- sider more than just technical skills when you’re expanding your team.\n\nBUILDING A TEAM We’ve talked a lot about the whole-team approach. But changes like that don’t just happen. We get asked questions like, “How do we get the team to jell?” or “How do we promote the whole-team approach?” One of the big ones is: “How do we keep everyone motivated and focused on the goal of de- livering business value?”\n\nSelf-Organizing Team\n\nIn our experience, teams make the best progress when they’re empowered to identify and solve their own problems. If you’re a manager, resist the tempta- tion to impose all your good ideas on the team. There are problems, such as personnel issues, that are best solved by managers, and there are times a coach needs to provide strong encouragement and lead the team when it needs leadership. It takes time for a new agile team to learn how to prioritize and solve its problems, but it’s okay for the team to make mistakes and stum- ble a few times. We think a high-functioning team has to grow itself. If you’re a tester, you’re in a good position to help the team ﬁgure out ways to get fast feedback, use practices such as retrospectives to prioritize and address issues, and ﬁnd the techniques that help your team produce better software.\n\nInvolving Other Teams\n\nYou might need to get other teams on board to help your team succeed. Set up meetings; ﬁnd ways to communicate as much as possible. Use a Scrum of Scrums to keep multiple teams coordinated, or just get involved with the other teams. If you have to bring in an expert to help with security testing, pair with that expert and learn as much as you can, and help them learn about your project.\n\nIf teams are scattered in different locations and time zones, ﬁgure out how to get as much direct communication as possible. Maybe representatives from\n\n69",
      "content_length": 2312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "70\n\nCHAPTER 4\n\nChapter 9, “Tool- kit for Business- Facing Tests that Support the Team,” gives exam- ples of tools that help remote teams collaborate.\n\n(cid:2) TEAM LOGISTICS\n\neach team can adjust their hours once or twice a week so that they can tele- conference once a week. Make a phone call instead of sending an email whenever possible. Lisa’s team adjusted its planning meeting times to include a remote team member who works late at night. They schedule meetings for a time where his day overlaps with the rest of the team’s day.\n\nEvery Team Member Has Equal Value\n\nEvery team member has equal value to the team. If testers or any other team members feel left out or less valued, the whole-team approach is doomed. Make sure testers are invited to all meetings. If you’re a tester and someone forgets to invite you to a meeting, invite yourself. Nontechnical testers might think they’ll be out of place or overwhelmed at a design meeting, but some- times they ask good questions that the techies didn’t think of.\n\nTesters have a right to ask for and get help. If you’re a tester stuck on an auto- mation problem, have the courage to ask a team member for help. That per- son might be busy right now, but he or she must commit to helping you in a reasonable amount of time. If you’re a manager or leader on your team, make sure this is happening, and raise the issue to the team if it’s not.\n\nPerformance and Rewards\n\nMeasuring and rating performance on an individual basis risks undermining team collaboration. We don’t want a programmer to feel she shouldn’t take on a testing task because she’s rated on delivering production code. We don’t want a system administrator to be so busy making sure her individual goals are met that she can’t help with a test environment problem.\n\nConversely, a good performer who was trying to work well with the team shouldn’t be knocked because the rest of the team didn’t pull together. This is a time when a manager needs to step up and help the team ﬁnd its way. If major bugs made it to production, nobody should blame the testers. Instead, the whole team should analyze what happened and start taking steps to pre- vent a recurrence.\n\nThe development team needs to keep the business needs in mind. Set goals that serve the business, increase proﬁtability, and make the customers hap- pier. Work closely with the business so that your successes help the whole company succeed.",
      "content_length": 2421,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Read about the “Shout-Out Shoe- box” idea in Chap- ter 19, “Wrap Up the Iteration.”\n\nSUMMARY\n\nAs we mentioned in Chapter 3, “Cultural Challenges,” celebrate every suc- cess, however small. A celebration might be a high-ﬁve, a company-provided lunch, or maybe just leaving work early to socialize a bit. The ScrumMaster on Lisa’s team hands out gold stars at stand-up meetings for special accom- plishments. Acknowledge the people who help you and your team.\n\nTeams can ﬁnd novel ways to recognize each other’s contributions. Iteration review and demonstration meetings, where both the development team and customer team are present, are a good setting for recognizing both individual and team achievements.\n\nWhat Can You Do?\n\nIf you’re a new tester on an agile team, especially a new agile team, what can you do to help the team overcome organizational challenges and succeed? How can you ﬁt in with the team and contribute your particular skills and experience?\n\nPut the ten principles we described in Chapter 2 to work. Courage is espe- cially important. Get up and go talk to people; ask how you can help. Reach out to team members and other teams with direct communication. Notice impediments and ask the team to help remove them.\n\nAgile development works because it gets obstacles out of our path and lets us do our best work. We can feel proud and satisﬁed, individually and as a team. When we follow agile principles, we collaborate well, use feedback to help im- prove how we work, and always look for new and better ways to accomplish our goals. All this means we can continually improve the quality of our product.\n\nSUMMARY In this chapter, we looked at ways to build a team and a structure for success- ful agile testing and development.\n\n(cid:2) Consider the importance of team structure; while testers might need an independent mind-set, putting them on a separate team can be counterproductive.\n\n(cid:2) Testers need access to a larger community of testers for learning and trying out new ideas. QA teams might be able to create this commu- nity within their organization.\n\n71",
      "content_length": 2090,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "72\n\nCHAPTER 4\n\n(cid:2) TEAM LOGISTICS\n\n(cid:2) It is important for the whole team to be located together, to foster collaboration; if the team is distributed, provide tools to promote communication. (cid:2) Hire for attitude. (cid:2) There is no right tester–developer ratio. The right answer is, “It de-\n\npends on your situation.”\n\n(cid:2) Teams need to self-organize, identify and ﬁnd solutions to their own problems, and look for ways to improve. They can’t wait for someone to tell them what to do.\n\n(cid:2) Management should reward performance in a way that promotes the team’s effort to deliver business value but not penalize good individ- ual performance if the team is struggling.\n\n(cid:2) Testers can use agile principles to improve their own skills and in- crease their value to the team. They need to be proactive and ﬁnd ways that they can contribute.",
      "content_length": 864,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Chapter 5\n\nTRANSITIONING TYPICAL PROCESSES\n\nSeeking Lightweight Processes\n\nLean Measurements\n\nAudits\n\nFrameworks, Models, and Standards\n\nExisting Processes\n\nMetrics\n\nWhy Do We Need Metrics?\n\nWhat Not to Do with Metrics\n\nCommunicating\n\nMetrics ROI\n\nTransitioning Typical Processes\n\nTest Strategy vs. Test Plan\n\nTest Planning\n\nTraceability\n\nWhy Use a DTS?\n\nDefect Tracking\n\nWhy Not?\n\nDefect Tracking Tools\n\nThere are many processes in a traditional project that don’t transition well to agile because they require heavyweight documentation or are an inherent part of the phased and gated process and require sign-offs at the end of each stage. Like anything else, there are no hard and fast rules for transitioning your processes to a more agile or lightweight process. In this chapter, we discuss a few of those processes, and give you alternatives and guidance on how to work with them in an agile project. You’ll ﬁnd more examples and details about these alter- natives in Parts III, IV, and V.\n\nSEEKING LIGHTWEIGHT PROCESSES When teams are learning how to use agile processes, some of the more tradi- tional processes can be lost in the shufﬂe. Most testers who are used to working\n\n73",
      "content_length": 1187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "74\n\nCHAPTER 5\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nwith traditional phased and gated development methodologies are accustomed to producing and using metrics, recording defects in a formal defect tracking system, and writing detailed test plans. Where do those ﬁt in agile development?\n\nMany software organizations must comply with audit systems or quality pro- cess models. Those requirements don’t usually disappear just because you start using agile development practices. In fact, some people worry that agile development will be incompatible with such models and standards as CMMI and ISO 9000.\n\nIt might be more fun to talk about everything that’s new and different when testing on an agile project, but we still need ways to measure progress, track de- fects, and plan testing. We also need to be prepared to work with our organiza- tion’s quality models. The key is to keep these processes lightweight enough to help us deliver value in a timely manner. Let’s start by looking at metrics.\n\nMETRICS Metrics can be controversial, and we spend a lot of time talking about them. Metrics can be a pit of wasted effort, numbers for the sake of numbers. They are sometimes used in harmful ways, although they don’t have to be bad. They can guide your team and help it to measure your team’s progress to- ward its goals. Let’s take a look at how to use metrics to help agile testers and their teams.\n\nLean Measurements\n\nLean software development practitioners look for ways to reduce the number of measurements and ﬁnd measurements that will drive the right behaviors. Implementing Lean Software Development: From Concept to Cash, by Mary and Tom Poppendieck, is an excellent resource that teaches how to apply lean initiatives to your testing and development efforts.\n\nAccording to the Poppendiecks [2007], a fundamental lean measurement is the time it takes to go “from concept to cash,” from a customer’s feature re- quest to delivered software. They call this measurement “cycle time.” The fo- cus is on the team’s ability to “repeatedly and reliably” deliver new business value. Then the team tries to continuously improve their process and reduce the cycle time.\n\nMeasurements such as cycle time that involve the whole team are more likely to drive you toward success than are measures conﬁned to isolated roles or",
      "content_length": 2324,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "METRICS\n\ngroups. How long does it usually take to ﬁx a defect? What can the team do to reduce that latency, the amount of time it takes? These types of metrics en- courage collaboration in order to make improvements.\n\nAnother lean measurement the Poppendiecks explain in their book is ﬁnan- cial return. If the team is developing a proﬁtable product, it needs to under- stand how it can work to achieve the most proﬁt. Even if the team is developing internal software or some other product whose main goal isn’t proﬁt, it still needs to look at ROI to make sure it is delivering the best value. Identify the business goals and ﬁnd ways to measure what the team delivers. Is the company trying to attract new customers? Keep track of how many new accounts sign on as new features are released.\n\nLean development looks for ways to delight customers, which ought to be the goal for all software development. The Poppendiecks give examples of simple ways you can measure whether your customers are delighted.\n\nWe like the lean metrics, because they’re congruent with our goal to deliver business value. Why are we interested in metrics at all? We’ll go into that in the next section.\n\nWhy We Need Metrics\n\nThere are good reasons to collect and track metrics. There are some really bad ones too. Anyone can use good metrics in terrible ways, such as using them as the basis for an individual team member’s performance evaluation. However, without metrics, how do you measure your progress?\n\nWhen metrics are used as guideposts—telling the team when it’s getting off track or providing feedback that it’s on the right track—they’re worth gath- ering. Is our number of unit tests going up every day? Why did the code cov- erage take a dive from 75% to 65%? It might have been a good reason— maybe we got rid of unused code that was covered by tests. Metrics can alert us to problems, but in isolation they don’t usually provide value.\n\nMetrics that measure milestones along a journey to achieve team goals are useful. If our goal is to increase unit test code coverage by 3%, we might run the code coverage every time we check in to make sure we didn’t slack on unit tests. If we don’t achieve the desired improvement, it’s more important to ﬁgure out why than to lament whatever amount our bonus was reduced as a result. Rather than focus on individual measurements, we should focus on the goal and the trending toward reaching that goal.\n\n75",
      "content_length": 2436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "76\n\nCHAPTER 5\n\nLisa’s Story\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nMetrics help the team, customers included, to track progress within the iter- ation and within the release or epic. If we’re using a burndown chart, and we’re burning up instead of down, that’s a red ﬂag to stop, take a look at what’s happening, and make sure we understand and address the problems. Maybe the team lacked important information about a story. Metrics, in- cluding burndown charts, shouldn’t be used as a form of punishment or source of blame. For example, questions like “Why were your estimates too low?” or “Why can’t you ﬁnish all of the stories?” would be better coming from the team and phrased as “Why were our estimates so low?” and “Why didn’t we get our stories ﬁnished?”\n\nMetrics, used properly, can be motivating for a team. Lisa’s team tracks the number of unit tests run in each build. Big milestones—100 tests, 1000 tests, 3000 tests—are a reason to celebrate. Having that number of unit tests go up every day is a nice bit of feedback for the development and customer teams. However, it is important to recognize that the number itself means nothing. For example, the tests might be poorly written, or to have a well tested prod- uct, maybe we need 10,000 tests. Numbers don’t work in isolation.\n\nPierre Veragen told me about a team he worked on that was allergic to metrics. The team members decided to stop measuring how much code their tests cov- ered. When they decided to measure again after six months, they were stunned to discover the rate had dropped from 40% to 12%.\n\nHow much is it costing you to not use the right metrics?\n\nWhen you’re trying to ﬁgure out what to measure, ﬁrst understand what problem you are trying to solve. When you know the problem statement, you can set a goal. These goals need to be measurable. “Reduce average response time on the XYZ application to 1.5 seconds with 20 concurrent users” works better than “Improve the XYZ application performance.” If your goals are measurable, the measurements you need to gather to track the metrics will be obvious.\n\nRemember to use metrics as a motivating force and not for beating down a team’s morale. This wisdom bears repeating: Focus on the goal, not the met- rics. Maybe you’re not using the right metrics to measure whether you’re achieving your team’s objectives, or perhaps you’re not interpreting them in context. An increased number of defect reports might mean the team is do- ing a better job of testing, not that they are writing more buggy code. If your\n\n—Lisa",
      "content_length": 2551,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Lisa’s Story\n\nMETRICS\n\nmetrics aren’t helping you to understand your progress toward your goal, you might have the wrong metrics.\n\nWhat Not to Do with Metrics\n\nMark Twain popularized the saying, which he attributed to Benjamin Dis- raeli, “There are three kinds of lies: lies, damned lies, and statistics.” Measur- able goals are a good thing; if you can’t gauge them in some way, you can’t tell if you achieved them. On the other hand, using metrics to judge individual or team performance is dangerous. Statistics by themselves can be twisted into any interpretation and used in detrimental ways.\n\nTake lines of code, a traditional software measuring stick. Are more lines of code a good thing, meaning the team has been productive, or a bad thing, meaning the team is writing inefﬁcient spaghetti-style code?\n\nWhat about number of defects found? Does it make any sense to judge testers by the number of defects they found? How does that help them do their jobs better? Is it safe to say that a development team that produces a higher num- ber of defects per lines of code is doing a bad job? Or that a team that ﬁnds more defects is doing a good job? Even if that thought holds up, how moti- vating is it for a team to be whacked over the head with numbers? Will that make the team members start writing defect-free code?\n\nCommunicating Metrics\n\nWe know that whatever we measure is bound to change. How many tests are running and passing? How many days until we need a “build on the shelf”? Is the full build passing? Metrics we can’t see and easily interpret aren’t worth having. If you want to track the number of passing tests, make sure that met- ric is visible in the right way, to the right people. Big visible charts are the most effective way of displaying metrics we know.\n\nMy previous team had goals concerned with the number of unit tests. However, the number of unit tests passing wasn’t communicated to anyone; there were no big visible charts or build emails that referred to that number. Interestingly, the team never got traction on automating unit tests.\n\nAt my current company, everyone in the company regularly gets a report of the number of passing tests at the unit, behind-the-GUI, and GUI levels (see Tables 5-1 and 5-2 for examples). Business people do notice when that number goes down instead of up. Over time, the team has grown a huge number of useful tests.\n\n77\n\n—Lisa",
      "content_length": 2400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "78\n\nCHAPTER 5\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nTable 5-1 Starting and Ending Metrics\n\nMetric\n\nAt Start\n\nAt End\n\nNCSS – Whitney\n\n69943\n\n69887\n\nNCSS – Ghidrah\n\n41044\n\n41978\n\nNumber of JUnit tests\n\n3001\n\n3062\n\nNumber of Canoo/Watir assertions\n\n3215\n\n3215\n\nNumber of FitNesse assertions\n\n57319\n\n61585\n\nTable 5-2 Daily Build Results\n\nDate\n\nBuild Result\n\nFriday 1/25/2008\n\nPassed 3026 JUnits\n\nMonday 1/28/2008\n\nPassed 3026 JUnits\n\nTuesday 1/29/2008\n\nPassed 3027 JUnits\n\nWednesday 1/30/2008\n\nPassed 3033 JUnits\n\nThursday 1/31/2008\n\nPassed 3040 JUnits\n\nFriday 2/1/2008\n\nPassed 3058 JUnits\n\nMonday 2/4/2008\n\nPassed 3059 JUnits\n\nTuesday 2/5/2008\n\nPassed 3060 JUnits\n\nWednesday 2/6/2008\n\nPassed 3062 JUnits\n\nThursday 2/7/2008\n\nPassed 3062 JUnits\n\nAre your metrics worth the trouble? Don’t measure for the sake of producing numbers. Think about what you’ll learn from those numbers. In the next section, we consider the return on investment you can expect from metrics.\n\nMetrics ROI\n\nWhen you identify the metrics you need, make sure you can obtain them at a reasonable cost. If your continual build delivers useful numbers, it delivers good value. You’re running the build anyway, and if it gives us extra informa- tion, that’s gravy. If you need a lot of extra work to get information, ask your- self if it’s worth the trouble.\n\nLisa’s team went to a fair amount of trouble to track actual time spent per story versus estimated time. What did they learn other than the obvious fact",
      "content_length": 1480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "DEFECT TRACKING\n\nthat estimates are just that? Not much. Some experienced teams ﬁnd they can dispense with the sprint burndown chart because the task board gives them enough information to gauge their progress. They can use the time spent estimating tasks and calculating the remaining hours on more pro- ductive activities.\n\nThis doesn’t mean we recommend that you stop tracking these measure- ments. New teams need to understand their velocity and burndown rate, so that they can steadily improve.\n\nDefect rates are traditional software metrics, and they might not have much value on a team that’s aiming for zero defects. There’s not much value in knowing the rate of bugs found and ﬁxed during development, because ﬁnd- ing and ﬁxing them is an integral part of development. If a tester shows a de- fect to the programmer who’s working on the code, and a unit test is written and the bug is ﬁxed right away, there’s often no need to log a defect. On the other hand, if many defects reach production undetected, there can be value in tracking the number to know if the team improves.\n\nWhen it started to rewrite its buggy legacy application, Lisa’s team set a goal of no more than six high-severity bugs in new code reported after the code is in production over a six-month period. Having a target that was straightfor- ward and easy to track helped motivate the team to ﬁnd ways to head bugs off during development and exceed this objective.\n\nFigure each metric’s return on investment and decide whether to track or maintain it. Does the effort spent collecting it justify the value it delivers? Can it be easily communicated and understood? As always, do what works for your situation. Experiment with keeping a particular metric for a few sprints and evaluate whether it’s paying off.\n\nOne common metric that relates to software quality is the defect rate. In the next section, we look at reasons to track defects, or to not track defects, and what we can learn from them.\n\nDEFECT TRACKING One of the questions that are asked by every new agile team is, “Do we still track bugs in a defect tracking system?” There’s no simple answer, but we’ll give you our opinion on the matter and offer some alternatives so that you can determine what ﬁts your team.\n\n79",
      "content_length": 2262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "80\n\nCHAPTER 5\n\nJanet’s Story\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nWhy Should We Use a Defect Tracking System (DTS)?\n\nA lot of us testers have used defect tracking as the only way to communicate the issues we saw, and it’s easy to keep using the tools we are familiar with. A DTS is a convenient place to keep track of not only the defect but the priori- ties, severities, and status, and to see who it is assigned to. Many agile practi- tioners say that we don’t need to do this anymore, that we can track defects on cards or some other simple mechanism. We could write a test to show the failure, ﬁx the code, and keep the test in our regression suite.\n\nHowever, there are reasons to keep using a tool to record defects and how they were ﬁxed. Let’s explore some of them now.\n\nConvenience One of the concerns about not keeping a defect tracking system is that there is no place to keep all of the details of the bug. Testers are used to recording a bug with lots of information, such as how to reproduce it, what environment it was found in, or what operating system or browser was used. All of this in- formation cannot ﬁt on a card, so how do you capture those details? If you are relying only on cards, you also need conversation. But with conversation, de- tails get lost, and sometimes a tester forgets exactly what was done—especially if the bug was found a few days prior to the programmer tackling the issue.\n\nA DTS is also a convenient place to keep all supplemental documentation, such as screen prints or uploaded ﬁles.\n\nKnowledge Base We have heard reasons to track defects such as, “We need to be able to look at old bug reports.” We tried to think of reasons why you would ever need to look at old bug reports, and as we were working on this chapter, Janet found an example.\n\nWhen I was testing the pre-seating algorithm at WestJet, I found an anomaly. I asked Sandra, another tester, if she had ever come across the issue before. San- dra vaguely recalled something about it but not exactly what the circumstances were. She quickly did a search in Bugzilla and found the issue right away. It had been closed as invalid because the business had decided that it wasn’t worth the time it would take to ﬁx it, and the impact was low.\n\nBeing able to look it up saved me from running around trying to ask questions or reentering the bug and getting it closed again. Because the team members sit",
      "content_length": 2409,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Lisa’s Story\n\nDEFECT TRACKING\n\nclose to each other, our talking led to another conversation with the business analyst on the team. This conversation sparked the idea of a FAQ page, an out- standing issues list, or something along that line that would provide new testers a place to ﬁnd all of the issues that had been identiﬁed but for which the decision had been made not to address them.\n\nThis story shows that although the bug database can be used as a knowledge base, there might be other mechanisms for keeping business decisions and their background information. If an issue is old enough to have been lost track of, maybe we should rewrite it and bring it up again. The circumstances may have changed, and the business might decide it is now worthwhile to ﬁx the bug.\n\nThe types of bugs that are handy to keep in a DTS are the ones that are inter- mittent and take a long time to track down. These bugs present themselves infrequently, and there are usually gaps in time during which the investiga- tion stalls for lack of information. A DTS is a place where information can be captured about what was ﬁgured out so far. It can also contain logs, traces, and so on. This can be valuable information when someone on the team ﬁ- nally has time to look at the problem or the issue becomes more critical.\n\nThe information in bug reports can be used later for several purposes. Here’s a story from Lisa’s team on how it uses its information.\n\nOne developer from our team serves on a “production support” rotation for each iteration. Production support requests come in from the business side for manual ﬁxes of past mistakes or production problems that need manual intervention. The “production support person” researches the problem and notes whatever was done to ﬁx it in the bug report. These notes usually include a SQL statement and information about the cause. If anyone encounters the same error or situation later, the solution can be easily found in the DTS. If certain types of problems seem to occur frequently, the team can use the DTS for research and analysis. Even though our team is small, we deal with a lot of legacy code, and we can’t rely on people’s memory to keep track of every problem and ﬁx.\n\nRemembering the cause of defects or what was done to fulﬁll a special re- quest is even harder when the team is particularly large or isn’t co-located. Customers might also be interested in the solutions to their problems.\n\n81\n\n—Janet\n\n—Lisa",
      "content_length": 2461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "82\n\nCHAPTER 5\n\nChapter 18, “Cod- ing and Testing,” explores metrics related to defect rates.\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nLarge or Distributed Teams If projects are so large that defects found by one team might affect other teams, a DTS is probably a good choice. Of course, to be useful it needs to be accessible to all members of the team. Face-to-face communication is always our ﬁrst choice, but when circumstances make that impractical, we need aids such as a DTS.\n\nCustomer Support When there are defects that have been reported by the customer after the re- lease, the customer usually wants to know when they’ve been ﬁxed. It’s in- valuable for the help desk or technical support to know what was ﬁxed in a given release. They can also ﬁnd defects that are still outstanding at release time and let the customers know. A DTS makes it much simpler to pull this information together.\n\nMetrics There are reasons to track defect rates. There are also reasons why you wouldn’t track a defect. For example, we don’t think that a bug should be counted as a defect if it never makes it out of the iteration. This, of course, brings up another discussion about what should we track and why, but we won’t discuss that here.\n\nTraceability Another reason we’ve heard for having a DTS is traceability, linking defects to test cases. We’re not sure that this is a valid reason. Not all defects are linked to test cases, nor should they be. For example, errors like spelling mis- takes might not need speciﬁc test cases. Maybe the product was not intuitive to use; this is a very real bug that often goes unreported. How do you write a test to determine if something is usable? Exploratory testing might ﬁnd bugs in edge conditions that are not worth the effort of creating automated tests.\n\nIf it is an automated test case that caught a bug, then the need to record that defect is further reduced, because it will be caught again if ever reintroduced. The need for traceability is gone. So, maybe we don’t need to track defects.\n\nWhy Shouldn’t We Use a DTS?\n\nAgile and Lean provide us with practices and principles that help reduce the need for a DTS. If the process is solid, and all of the people are committed to delivering a quality product, defects should be rare and very simply tracked.",
      "content_length": 2300,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "In Chapter 15, “Coding and Test- ing,” we’ll explain how tester and programmers work together on bugs.\n\nJanet’s Story\n\nAntony will share his ideas about the hidden back- log when we cover iteration planning in Chap- ter 18, “Coding and Testing.”\n\nDEFECT TRACKING\n\nAs a Communication Tool Defect tracking systems certainly don’t promote communication between programmers and testers. They can make it easy to avoid talking directly to each other.\n\nWaste of Time and Inventory We tend to put lots of information into the DTS in addition to all of the steps to reproduce the defect. Depending on the bug, it can take a long time to write these steps so that the programmer can reproduce it as well. Then there is the triage, and someone has to make comments, interpret the defect, at- tempt to reproduce it, (ideally) ﬁx it, write more comments, and assign it back to the person who reported it. Finally, the ﬁx can be veriﬁed. This whole cycle can double if the programmer misunderstood the problem in the ﬁrst place. The cost of a single defect report can become exorbitant.\n\nHow much easier would it be if we as testers could just talk to the program- mer and show what we found, with the developer then ﬁxing the defect right away? We’ll talk more about that later.\n\nDefects in a DTS become a queue or a mini product backlog. According to lean principles, this inventory of defects is a waste. As a team, we should be thinking of ways to reduce this waste.\n\nIn 2004, Antony Marcano, author of TestingReﬂections.com, wrote a blog post about the idea of not using a bug-tracking system. When it was discussed on mail- ing lists, he was ﬂamed by many testers as introducing something similar to heresy. He ﬁnds he has a different reception now, because the idea is making its way into the mainstream of agile thinking.\n\nHe suggests that bug-tracking systems in agile teams are just “secret backlogs.”\n\nDefect Tracking Tools\n\nIf you do decide to use a DTS, choose it carefully. Understand your needs and keep it simple. You will want everyone on the team to use it. If it becomes overhead or hard to use, people will ﬁnd ways to work around it. As with all tools used by your agile development team, you should consider the whole team’s opinion. If anyone from the customer team enters bug reports, get his or her opinion too.\n\n83\n\n—Janet",
      "content_length": 2335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "84\n\nCHAPTER 5\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nOne of the simplest tools that Janet has used is Alcea’s FIT IssueTrack. It is conﬁgurable, does not make you follow a predeﬁned process, and is easy to get metrics out of. Do your homework and ﬁnd the tool that works for you. There are a variety of open source defect-tracking systems, hosted systems, and integrated enterprise systems available.\n\nWhether or not you use a DTS, you want to make defects as visible as possible.\n\nWe use a commercial DTS, but we ﬁnd value in keeping bugs visible. We color-code bugs and include them as tasks in our story board, shown in Figure 5-1. Yellow cards denote normal bugs, and red cards denote either high production bugs or “test stopper” development bugs—both categories need to be addressed right away. A quick look at the board lets us see how many bugs are in the To Do, WIP, Verify and Done columns. Other cards are color-coded as well: blue for story cards, green for test task cards, and white for development tasks. Striped cards are for tasks added after iteration planning. Yellow and red bug cards stand out easily.\n\nFigure 5-1 Story board with color-coded cards\n\nDuring the time we were writing this book, my team converted to a virtual story board because one of our team members became a remote team member, but we retained this color-coding concept.\n\n—Lisa",
      "content_length": 1370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "Lisa’s Story\n\nDEFECT TRACKING\n\nWe usually recommend experimenting with different tools, using each one for a few iterations, but this is trickier with bug-tracking systems, because you need to port all of the bugs that are in one system to the new one that you’re trying on for size. Spend some time thinking about what you need in a DTS, what purposes it will serve, and evaluate alternatives judiciously.\n\nMy team used a web-based DTS that was basically imposed upon it by manage- ment. We found it somewhat cumbersome to use, lacking in basic features such as time-stamping updates to the bug reports, and we chafed at the license restric- tions. We testers were especially frustrated by the fact that our license limited us to three concurrent users, so sessions were set to time out quickly.\n\nThe team set aside time to evaluate different DTS alternatives. At ﬁrst, the selec- tion seemed mind-boggling. However, we couldn’t ﬁnd one tool that met all our requirements. Every tool seemed to be missing something important, or we heard negative reports from people who had used the tool. We were concerned about the effort needed to convert the existing bug database into a new system.\n\nThe issue was forced when our DTS actually crashed. We had stopped paying for support a couple of years earlier, but the system administrator decided to see what enhancements the vendor had made in the tool. He found that a lot of shortcomings we had experienced had been addressed. For example, all updates were now time stamped. A client application was available that wasn’t subject to session timeouts and had enhanced features that were particularly valuable to the testers.\n\nBy going with our existing tool and paying for the upgrade and maintenance, plus a license allowing more concurrent users, we got help with converting our existing data to the new version and got a working system easily and at a low cost. A bonus was that our customers weren’t faced with having to learn a new system.\n\nSometimes the best tool is the one you already have if you just look to see how it has improved!\n\nAs with all your tool searches, look to others in your community, such as user groups and mailing lists, for recommendations. Deﬁne your criteria before you start looking, and experiment as much as you can. If you choose the wrong tool, cut your losses and start researching alternatives.\n\nKeep Your Focus\n\nDecisions about reporting and tracking defects are important, but don’t lose track of your main target. You want to deliver the best quality product you can, and you want to deliver value to the business in a timely manner. Projects\n\n85\n\n—Lisa",
      "content_length": 2639,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "86\n\nCHAPTER 5\n\nChapter 18, “Cod- ing and Testing,” covers alterna- tives and shows you different ways to attack your bug problems.\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nsucceed when people are allowed to do their best work. Concentrate on im- proving communication and building collaboration. If you encounter a lot of defects, investigate the source of the problem. If you need a DTS to do that, use it. If your team works better by documenting defects in executable tests and ﬁxing them right away, do that. If some combination enables you continually improve, go with it. The main thing to remember is that it has to work for your whole team.\n\nDefect tracking is one of the typical quality processes that generate the most questions and controversy in agile testing. Another big source of confusion is whether agile projects need documents such as test plans or traceability ma- trices. Let’s consider that next.\n\nTEST PLANNING Traditional phased software methodologies stress the importance of test plans as part of the overall documentation needs. They’re intended to outline the objectives, scope, approach, and focus of the software testing effort for stakeholders. The completed document is intended to help people outside the test group understand the “why” and “how” of product validation. In this section, we look at test plans and other aspects of preparing and tracking the testing effort for an agile project.\n\nTest Strategy vs. Test Planning\n\nIn an agile project, teams don’t rely on heavy documentation to communi- cate what the testers need to do. Testers work hand in hand with the rest of the team so that the testing efforts are visible to all in the form of task cards. So the question often put to us is, “Is there still a need for test plans?” To an- swer that question, let’s ﬁrst take a look at the difference between a test plan and a test strategy or approach.\n\nThe more information that is contained in a document, the less likely it is that someone is going to read it all. Consider what information is really necessary for the stakeholders. Think about how often it is used and what it is used for.\n\nWe like to think of a test strategy as a static document that seldom changes, while a test plan is created new and is speciﬁc to each new project.\n\nTest Strategy A strategy is a long-term plan of action, the key word being “long-term.” If your organization wants documentation about your overall test approach to",
      "content_length": 2446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "Janet’s Story\n\nIn Chapter 15, “Tester Activities in Release or Theme Planning,” we show examples and discuss alter- natives you can use when you are plan- ning the release.\n\nTEST PLANNING\n\nprojects, consider taking this information and putting it in a static document that doesn’t change much over time. There is a lot of information that is not project speciﬁc and can be extracted into a Test Strategy or Test Approach document.\n\nThis document can then be used as a reference and needs to be updated only if processes change. A test strategy document can be used to give new em- ployees a high-level understanding of how your test processes work.\n\nI have had success with this approach at several organizations. Processes that were common to all projects were captured into one document. Using this format answered most compliance requirements. Some of the topics that were covered were:\n\nTesting Practices • Story Testing • Solution Veriﬁcation Testing • User Acceptance Testing • Exploratory Testing • Load and Performance Testing • Test Automation • Test Results • Defect Tracking Process • Test Tools • Test Environments\n\nTest Plan The power of planning is to identify possible issues and dependencies, to bring risks to the surface to be talked about and to be addressed, and to think about the big picture. Test planning is no different. A team should think about risks and dependencies and the big picture for each project before it starts.\n\nWhether your team decides to create a test plan document or not, the plan- ning should be done. Each project is different, so don’t expect that the same solution will ﬁt all.\n\nSometimes our customers insist on a test plan document. If you’re contract- ing to develop an application, a test plan might be part of a set of deliver- ables that also include items such as a requirements document and a design document.\n\n87\n\n—Janet",
      "content_length": 1877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "88\n\nCHAPTER 5\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nTalk of test plans often leads to talk of traceability. Did someone execute all planned testing of the desired behavior on the delivered code? How do re- quirements and test plans relate to the actual testing and ﬁnal functionality?\n\nTraceability\n\nIn traditional projects, we used to need traceability matrices to determine whether we had actually tested all of the requirements. If a requirement changed, we needed to know that we had changed the appropriate test cases. With very large requirements documents, this was the only way that a test team knew it had good coverage.\n\nIn an agile project, we don’t have those restrictions. We build functionality in tiny, well-deﬁned steps. We work with the team closely and know when something changes. If the programmers work test-ﬁrst, we know there are unit tests for all of the small chunks of work. We can then collaborate with the customer to deﬁne acceptance tests. We test each story as the program- mer works on it, so we know that nothing goes untested.\n\nThere might be requirements for some kind of traceability for regulated in- dustries. If there is, we suggest that you really look at what problem manage- ment is trying to solve. When you understand what is needed, you should try to make the solution as simple as possible. There are multiple ways to pro- vide traceability. Source code check-in comments can refer to the wiki page containing the requirements or test cases, or to a defect number. You can put comments in unit tests tying the test to the location or identiﬁer of the re- quirement. The tests can be integrated directly with the requirements in a tool such as FitNesse. Your team can easily ﬁnd the way that works best for your customers’ needs.\n\nDocuments such as traceability matrices might be needed to fulﬁll require- ments imposed by the organization’s audit standards or quality models. Let’s consider how these directives get along with agile development.\n\nEXISTING PROCESSES AND MODELS This question is often asked: “Can traditional quality models and processes coexist with agile development methods?” In theory, there is no reason why they can’t. In reality, there is often not a choice. Quality models often fall into",
      "content_length": 2261,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Lisa’s Story\n\nEXISTING PROCESSES AND MODELS\n\nthe domain of the traditional QA team, and they can follow testers into the new agile structure as well. It might not be easy to ﬁt these into a new agile development model. Let’s look at a few typical quality processes and how testers and their teams might accommodate them.\n\nAudits\n\nDifferent industries have different audit requirements. Quality assurance teams in traditional development organizations are often tasked with provid- ing information for auditors and ensuring compliance with audit require- ments. The Sarbanes-Oxley Act of 2002, enacted in response to high-proﬁle corporate ﬁnancial scandals, sets out requirements for maintaining business records. Ensuring compliance usually falls to the IT departments. SAS 70 is another widely recognized auditing standard for service organizations. These are just a couple of examples of the type of audit controls that affect develop- ment teams.\n\nLarger organizations have specialized teams that control compliance and work with auditors, but development teams are often asked to provide infor- mation. Examples include what testing has been performed on a given soft- ware release, or proving that different accounts reconcile. Testers can be tasked with writing test plans to evaluate the effectiveness of control activities.\n\nOur company undergoes regular SAS 70 audits. Whenever one is scheduled, we write a story card for providing support for the audit. Most of this work falls to the system administrators, but I provide support to the business people who work with the auditor. Sometimes we’re required to demonstrate system functionality in our demo environment. I can provide data for the demos and help if questions arise. I might also be asked to provide details about how we tested a particular piece of functionality.\n\nSome of our internal processes are required to conform with SAS 70 require- ments. For example, every time we release to production, we ﬁll out a form with information about which build was released, how many tests at each level were run on it, who did the release, and who veriﬁed it.\n\nTesters who are part of an agile team should be dedicated to that team. If their help is needed in providing information for an audit or helping to ensure\n\n89\n\n—Lisa",
      "content_length": 2289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "90\n\nCHAPTER 5\n\nSee the bibliogra- phy for informa- tion about CMMI and agile development.\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\ncompliance, write stories for this and plan them along with the rest of the team’s work. Work together with the compliance and internal audit teams to understand your team’s responsibilities.\n\nFrameworks, Models, and Standards\n\nThere are many quality models, but we’ll look at two to show how you can adapt your agile process to ﬁt within their constraints.\n\n1. The Capability Maturity Model Integration (CMMI) aims to help or- ganizations improve their process but doesn’t dictate speciﬁc develop- ment practices to accomplish the improvements.\n\n2. Information Technology Infrastructure Library (ITIL) is a set of best practices for IT service management intended to help organizations develop an effective quality process.\n\nBoth of these models can coexist happily with agile development. They’re rooted in the same goal, making software development projects succeed.\n\nLet’s look at CMMI, a framework for measuring the maturity of your pro- cess. It deﬁnes each level by measuring whether the process is unknown, de- ﬁned, documented, permanent, or optimized. Agile projects have a deﬁned process, although not all teams document what they do. For example, man- aging your requirements with index cards on a release planning wall with a single customer making the ﬁnal decisions is a deﬁned process as long as you do it all the time.\n\nRetrospectives are aimed at constant process improvement, and teams should be always be looking for ways to optimize processes. If the only thing your team is lacking is documentation, then think about including your process into your test strategy documentation.\n\nAsk yourself what the minimum amount of documentation you could give to satisfy the CMMI requirements would be. Janet has had success with using diagrams like the one in Figure 5-2.\n\nIf ITIL has been introduced into your organization and affects change man- agement, adapt your process to accommodate it. You might even ﬁnd the new process beneﬁcial.",
      "content_length": 2084,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Janet’s Story\n\nEXISTING PROCESSES AND MODELS\n\nProject Initiation\n\nGet an understanding of the project\n\nRelease Planning\n\nParticipate in sizing stories Create test plans\n\nEach Iteration\n\n1 ... . X\n\nParticipate in sprint planning, estimating tasks Write and execute story tests Pair-test with other testers, developers Business validation (customers) Automate new functional test cases Run automated regression test cases Run project load tests Demo to the stakeholders\n\nThe End Game (System Test)\n\nRelease mgmt tests mock deploy on staging Smoke test on staging Perform load test (if needed) Complete regression test Business testers perform UAT Participate in release readiness\n\nRelease to Prod/ Support\n\nParticipate in release to production Participate in retrospectives\n\nFigure 5-2 Documenting the test strategy\n\nWhen I worked in one organization that had a central call center to handle all of the customers’ support calls, management implemented ITIL for the service part of the organization. We didn’t think it would affect the development team until the change management team realized that the number of open problems was steadily increasing. No one understood why the number kept going up, so we held a series of problem-solving sessions. First, we mapped out the process cur- rently in effect.\n\nThe call center staff reported an incident in their tracking system. They tried to solve the customer’s problem immediately. Often, that meant providing a work- around for a software defect. The call center report was closed, but a problem\n\n91",
      "content_length": 1547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "92\n\nCHAPTER 5\n\n(cid:2) TRANSITIONING TYPICAL PROCESSES\n\nreport in Remedy was then opened, and someone in the development team was sent an email. If the defect was accepted by the development team, a defect was entered into Bugzilla to be ﬁxed.\n\nThere was no loop back to the problem issue to close it when the defect was ﬁnally ﬁxed. We held several brainstorming sessions with all involved stakeholders to determine the best and easiest solution to that problem.\n\nThe problem statement to solve was, “How does the project team report back to the problem and change management folks to tell them when the bug was actu- ally ﬁxed?”\n\nThere were a couple of ways we could have solved the problem. One option was to reference the Remedy ticket in Bugzilla and put hooks into Remedy so that when we closed the Bugzilla defect, Remedy would detect it and close the Rem- edy ticket. Of course, some of the bugs were never addressed, which meant the Remedy tickets stayed open forever.\n\nWe actually found a better solution for the whole team, including the problem change folks. We brainstormed a lot of different ideas but decided that when a bug was opened in Bugzilla, we could close the Remedy ticket, because we realis- tically would never go back to the original complaint and tell the customer who reported it, or when the ﬁx was done.\n\nThe change request that covered the release would automatically include all soft- ware ﬁxes, so it followed the change management process as well.\n\nIf your organization is using some kind of process model or quality standards management, educate yourself about it, and work with the appropriate spe- cialists in your organization. Maintain the team’s focus on delivering high- quality software that provides real business value, and see how you can work within the model.\n\nProcess improvement models and frameworks emphasize discipline and con- formance to process. Few software development methodologies require more discipline than agile development. Standards simply enable you to measure your progress toward your goal. Agile’s focus is on doing your best work and constantly improving. Agile development is compatible with achieving what- ever standards you set for yourself or borrow from a process improvement measurement tool.\n\nSeparate your measurement goals and standards from your means to im- prove those measurements. Set goals, and know what metrics you need to measure success for areas that need improvement. Try using task cards for\n\n—Janet",
      "content_length": 2493,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "SUMMARY\n\nactivities that provide the improvements in order to ensure they get the visi- bility they need.\n\nWorking with existing quality processes and models is one of the biggest cultural issues you may face as you transition to agile development. All of these changes are hard, but when your whole team gets involved, none are insurmountable.\n\nSUMMARY In this chapter, we looked at traditional quality-oriented processes and how they can be adapted for an agile environment.\n\n(cid:2) The right metrics can help you to make sure your team is on track to achieve its goals and provide a good return on your investment in them.\n\n(cid:2) Metrics should be visible, providing necessary milestones upon which\n\nto make decisions.\n\n(cid:2) The reasons to use a defect tracking system include for convenience,\n\nfor use as a knowledge base, and for traceability.\n\n(cid:2) Defect tracking systems are too often used as a communication tool, and entering and tracking unnecessary bugs can be considered wasteful.\n\n(cid:2) All tools, including the DTS, need to be used by the whole team, so\n\nconsider all perspectives when choosing a tool.\n\n(cid:2) A test strategy is a long-term overall test approach that can be put in a\n\nstatic document; a test plan should be unique to the project.\n\n(cid:2) Think about alternatives before blindly accepting the need for speciﬁc documents. For example, the agile approach to developing in small, incremental chunks, working closely together, might remove the need for formal traceability documents. Linking the source code control system comments to tests might be another way.\n\n(cid:2) Traditional quality processes and process improvement models, such as SAS 70 audits and CMMI standards, can coexist with agile develop- ment and testing. Teams need to be open to thinking outside the box and work together to solve their problems.\n\n93",
      "content_length": 1863,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "Part III THE AGILE TESTING QUADRANTS\n\nSoftware quality has many dimensions, each requiring a different testing ap- proach. How do we know all the different types of tests we need to do? How do we know when we’re “done” testing? Who does which tests and how? In this part, we explain how to use the Agile Testing Quadrants to make sure your team covers all needed categories of testing.\n\nOf course, testing requires tools, and we’ve included examples of tools to use, strategies for using those tools effectively, and guidelines about when to use them. Tools are easier to use when used with code that’s designed for testabil- ity. These concerns and more are discussed in this part of the book.",
      "content_length": 694,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "Chapter 6\n\nTHE PURPOSE OF TESTING\n\nTests That Support the Team\n\nContext-Driven\n\nOverview of Quadrants\n\nTests That Critique the Product\n\nQuadrant Intro— Purpose of Testing\n\nShared Responsibility\n\nManaging Technical Debt\n\nKnowing When We’re Done\n\nFitting All Types into “Doneness”\n\nWhy do we test? The answer might seem obvious, but in fact, it’s pretty complex. We test for a lot of reasons: to ﬁnd bugs, to make sure the code is reliable, and sometimes just to see if the code’s usable. We do different types of testing to accomplish different goals. Software product quality has many components. In this chapter, we introduce the Agile Testing Quadrants. The rest of the chapters in Part III go into detail on each of the quadrants. The Agile Testing Quadrants matrix helps testers ensure that they have considered all of the different types of tests that are needed in order to deliver value.\n\nTHE AGILE TESTING QUADRANTS In Chapter 1, “What Is Agile Testing, Anyway?,” we introduced Brian Marick’s terms for different categories of tests that accomplish different purposes. Fig- ure 6-1 is a diagram of the agile testing quadrants that shows how each of the four quadrants reﬂects the different reasons we test. On one axis, we divide the matrix into tests that support the team and tests that critique the product. The other axis divides them into business-facing and technology-facing tests.\n\n97",
      "content_length": 1400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "98\n\nCHAPTER 6\n\n(cid:2) THE PURPOSE OF TESTING\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ2 Q3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 6-1 Agile Testing Quadrants\n\nThe order in which we’ve numbered these quadrants has no relationship to when the different types of testing are done. For example, agile development starts with customer tests, which tell the team what to code. The timing of the various types of tests depends on the risks of each project, the customers’ goals for the product, whether the team is working with legacy code or on a greenﬁeld project, and when resources are available to do the testing.\n\nTests that Support the Team\n\nThe quadrants on the left include tests that support the team as it develops the product. This concept of testing to help the programmers is new to many testers and is the biggest difference between testing on a traditional project and testing on an agile project. The testing done in Quadrants 1 and 2 are more requirements speciﬁcation and design aids than what we typically think of as testing.",
      "content_length": 1395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "Chapter 8, “Busi- ness-Facing Tests that Support the Team,” explains business conditions of satisfaction.\n\nTHE AGILE TESTING QUADRANTS\n\nQuadrant 1 The lower left quadrant represents test-driven development, which is a core agile development practice.\n\nUnit tests verify functionality of a small subset of the system, such as an object or method. Component tests verify the behavior of a larger part of the system, such as a group of classes that provide some service [Meszaros, 2007]. Both types of tests are usually automated with a member of the xUnit family of test automation tools. We refer to these tests as programmer tests, developer- facing tests, or technology-facing tests. They enable the programmers to mea- sure what Kent Beck has called the internal quality of their code [Beck, 1999].\n\nA major purpose of Quadrant 1 tests is test-driven development (TDD) or test-driven design. The process of writing tests ﬁrst helps programmers de- sign their code well. These tests let the programmers conﬁdently write code to deliver a story’s features without worrying about making unintended changes to the system. They can verify that their design and architecture de- cisions are appropriate. Unit and component tests are automated and written in the same programming language as the application. A business expert probably couldn’t understand them by reading them directly, but these tests aren’t intended for customer use. In fact, internal quality isn’t negotiated with the customer; it’s deﬁned by the programmers. Programmer tests are normally part of an automated process that runs with every code check-in, giving the team instant, continual feedback about their internal quality.\n\nQuadrant 2 The tests in Quadrant 2 also support the work of the development team, but at a higher level. These business-facing tests, also called customer-facing tests and customer tests, deﬁne external quality and the features that the custom- ers want.\n\nLike the Quadrant 1 tests, they also drive development, but at a higher level. With agile development, these tests are derived from examples provided by the customer team. They describe the details of each story. Business-facing tests run at a functional level, each one verifying a business satisfaction con- dition. They’re written in a way business experts can easily understand using the business domain language. In fact, the business experts use these tests to deﬁne the external quality of the product and usually help to write them. It’s possible this quadrant could duplicate some of the tests that were done at the unit level; however, the Quadrant 2 tests are oriented toward illustrating and conﬁrming desired system behavior at a higher level.\n\n99",
      "content_length": 2712,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "100\n\nCHAPTER 6\n\nLisa’s Story\n\n(cid:2) THE PURPOSE OF TESTING\n\nMost of the business-facing tests that support the development team also need to be automated. One of the most important purposes of tests in these two quadrants is to provide information quickly and enable fast trouble- shooting. They must be run frequently in order to give the team early feed- back in case any behavior changes unexpectedly. When possible, these automated tests run directly on the business logic in the production code without having to go through a presentation layer. Still, some automated tests must verify the user interfaces and any APIs that client applications might use. All of these tests should be run as part of an automated continu- ous integration, build, and test process.\n\nThere is another group of tests that belongs in this quadrant as well. User in- teraction experts use mock-ups and wireframes to help validate proposed GUI (graphical user interface) designs with customers and to communicate those designs to the developers before they start to code them. The tests in this group are tests that help support the team to get the product built right but are not automated. As we’ll see in the following chapters, the quadrants help us identify all of the different types of tests we need to use in order to help drive coding.\n\nSome people use the term “acceptance tests” to describe Quadrant 2 tests, but we believe that acceptance tests encompass a broader range of tests that in- clude Quadrants 3 and 4. Acceptance tests verify that all aspects of the sys- tem, including qualities such as usability and performance, meet customer requirements.\n\nUsing Tests to Support the Team The quick feedback provided by Quadrants 1 and 2 automated tests, which run with every code change or addition, form the foundation of an agile team. These tests ﬁrst guide development of functionality, and when auto- mated, then provide a safety net to prevent refactoring and the introduction of new code from causing unexpected results.\n\nWe run our automated tests that support the team (the left half of the quadrants) in separate build processes. Unit and component tests run in our “ongoing” build, which takes about eight minutes to ﬁnish. Although the programmers run the unit tests before they check in, the build might still fail due to integration problems or environmental differences. As soon as we see the “build failed” email, the person who checked in the offending code ﬁxes the problem. Business-facing functional tests run in our “full build,” which also runs continually, kicking off every time a code change is checked in. It ﬁnishes in less than two hours. That’s still pretty quick feedback, and again, a build failure means immediate action to ﬁx the",
      "content_length": 2757,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "THE AGILE TESTING QUADRANTS\n\nproblem. With these builds as a safety net, our code is stable enough to release every day of the iteration if we so choose.\n\nThe tests in Quadrants 1 and 2 are written to help the team deliver the busi- ness value requested by the customers. They verify that the business logic and the user interfaces behave according to the examples provided by the cus- tomers. There are other aspects to software quality, some of which the custom- ers don’t think about without help from the technical team. Is the product competitive? Is the user interface as intuitive as it needs to be? Is the applica- tion secure? Are the users happy with how the user interface works? We need different tests to answer these types of questions.\n\nTests that Critique the Product\n\nIf you’ve been in a customer role and had to express your requirements for a software feature, you know how hard it can be to know exactly what you want until you see it. Even if you’re conﬁdent about how the feature should work, it can be hard to describe it so that programmers fully understand it.\n\nThe word “critique” isn’t intended in a negative sense. A critique can include both praise and suggestions for improvement. Appraising a software product involves both art and science. We review the software in a constructive man- ner, with the goal of learning how we can improve it. As we learn, we can feed new requirements and tests or examples back to the process that supports the team and guide development.\n\nQuadrant 3 Business-facing examples help the team design the desired product, but at least some of our examples will probably be wrong. The business experts might overlook functionality, or not get it quite right if it isn’t their ﬁeld of expertise. The team might simply misunderstand some examples. Even when the programmers write code that makes the business-facing tests pass, they might not be delivering what the customer really wants.\n\nThat is where the tests to critique the product in the third and fourth quad- rants come into play. Quadrant 3 classiﬁes the business-facing tests that exer- cise the working software to see if it doesn’t quite meet expectations or won’t stand up to the competition. When we do business-facing tests to critique the product, we try to emulate the way a real user would work the application. This is manual testing that only a human can do. We might use some automated\n\n101\n\n—Lisa",
      "content_length": 2425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "102\n\nCHAPTER 6\n\n(cid:2) THE PURPOSE OF TESTING\n\nscripts to help us set up the data we need, but we have to use our senses, our brains, and our intuition to check whether the development team has delivered the business value required by the customers.\n\nOften, the users and customers perform these types of tests. User Acceptance Testing (UAT) gives customers a chance to give new features a good workout and see what changes they may want in the future, and it’s a good way to gather new story ideas. If your team is delivering software on a contract basis to a client, UAT might be a required step in approving the ﬁnished stories.\n\nUsability testing is an example of a type of testing that has a whole science of its own. Focus groups might be brought in, studied as they use the applica- tion, and interviewed in order to gather their reactions. Usability testing can also include navigation from page to page or even something as simple as the tabbing order. Knowledge of how people use systems is an advantage when testing usability.\n\nExploratory testing is central to this quadrant. During exploratory testing sessions, the tester simultaneously designs and performs tests, using critical thinking to analyze the results. This offers a much better opportunity to learn about the application than scripted tests. We’re not talking about ad hoc test- ing, which is impromptu and improvised. Exploratory testing is a more thoughtful and sophisticated approach than ad hoc testing. It is guided by a strategy and operates within deﬁned constraints. From the start of each project and story, testers start thinking of scenarios they want to try. As small chunks of testable code become available, testers analyze test results, and as they learn, they ﬁnd new areas to explore. Exploratory testing works the sys- tem in the same ways that the end users will. Testers use their creativity and intuition. As a result, it is through this type of testing that many of the most serious bugs are usually found.\n\nQuadrant 4 The types of tests that fall into the fourth quadrant are just as critical to agile development as to any type of software development. These tests are technol- ogy-facing, and we discuss them in technical rather than business terms. Technology-facing tests in Quadrant 4 are intended to critique product char- acteristics such as performance, robustness, and security. As we’ll describe in Chapter 11, “Critiquing the Product using Technology-Facing Tests,” your team already possesses many of the skills needed to do these tests. For exam- ple, programmers might be able to leverage unit tests into performance tests with a multi-threaded engine. However, creating and running these tests might require the use of specialized tools and additional expertise.",
      "content_length": 2776,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "THE AGILE TESTING QUADRANTS\n\nIn the past, we’ve heard complaints that agile development seems to ignore the technology-facing tests that critique the product. These complaints might be partly due to agile’s emphasis on having customers write and prior- itize stories. Nontechnical customer team members often assume that the de- velopers will take care of concerns such as speed and security, and that the programmers are intent on producing only the functionality prioritized by the customers.\n\nIf we know the requirements for performance, security, interaction with other systems, and other nonfunctional attributes before we start coding, it’s easier to design and code with that in mind. Some of these might be more important than actual functionality. For example, if an Internet retail website has a one-minute response time, the customers won’t wait to appreciate the fact that all of the features work properly. Technology-facing tests that cri- tique the product should be considered at every step of the development cy- cle and not left until the very end. In many cases, such testing should even be done before functional testing.\n\nIn recent years we’ve seen many new lightweight tools appropriate to an agile development project become available to support tests. Automation tools can be used to create test data, set up test scenarios for manual testing, drive se- curity tests, and help make sense of results. Automation is mandatory for some efforts such as load and performance testing.\n\nChecking Nonfunctional Requirements\n\nAlessandro Collino, a computer science and information engineer with Onion S.p.A., who works on agile projects, illustrates why executing tests that cri- tique the product early in the development process is critical to project success.\n\nOur Scrum/XP team used TDD to develop a Java application that would convert one form of XML to another. The application performed complex calculations on the data. For each simple story, we wrote a unit test to check the conversion of one element into the required format, imple- mented the code to make the test pass, and refactored as needed.\n\nWe also wrote acceptance tests that read subsets of the original XML ﬁles from disk, converted them, and wrote them back. The ﬁrst time we ran the application on a real ﬁle to be converted, we got an out-of- memory error. The DOM parser we used for the XML conversion couldn’t handle such a large ﬁle. All of our tests used small subsets of the actual ﬁles; we hadn’t thought to write unit tests using large datasets.\n\n103",
      "content_length": 2547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "104\n\nCHAPTER 6\n\n(cid:2) THE PURPOSE OF TESTING\n\nDoing TDD gave us quick feedback on whether the code was working per the functional requirements, but the unit tests didn’t test any non- functional requirements such as capacity, performance, scalability, and usability. If you use TDD to also check nonfunctional requirements, in this case, capacity, you’ll have quick feedback and be able to avoid expen- sive mistakes.\n\nAlessandro’s story is a good example of how the quadrant numbering doesn’t imply the order in which tests are done. When application performance is critical, plan to test with production-level loads as soon as testable code is available.\n\nWhen you and your team plan a new release or project, discuss which types of tests from Quadrants 3 and 4 you need, and when they should be done. Don’t leave essential activities such as load or usability testing to the end, when it might be too late to rectify problems.\n\nUsing Tests that Critique the Product The information produced during testing to review the product should be fed back into the left side of our matrix and used to create new tests to drive future development. For example, if the server fails under a normal load, new stories and tests to drive a more scalable architecture will be needed. Using the quadrants will help you plan tests that critique the product as well as tests that drive development. Think about why you are testing to make sure that the tests are performed at the optimum stage of development.\n\nThe short iterations of agile development give your team a chance to learn and experiment with the different testing quadrants. If you ﬁnd out too late that your design doesn’t scale, start load testing earlier with the next story or project. If the iteration demo reveals that the team misunderstood the cus- tomer’s requirements, maybe you’re not doing a good enough job of writing customer tests to guide development. If the team puts off needed refactoring, maybe the unit and component tests aren’t providing enough coverage. Use the agile testing quadrants to help make sure all necessary testing is done at the right time.\n\nKNOWING WHEN A STORY IS DONE For most products, we need all four categories of testing to feel conﬁdent we’re delivering the right value. Not every story requires security testing, but you don’t want to omit it because you didn’t think of it.",
      "content_length": 2370,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "Lisa’s Story\n\nKNOWING WHEN A STORY IS DONE\n\nMy team uses “stock” cards to ensure that we always consider all different types of tests. When unit testing wasn’t yet a habit, we wrote a unit test card for each story on the board. Our “end to end” test card reminds the programmers to complete the job of integration testing and to make sure all of the parts of the code work together. A “security” card also gets considered for each story, and if appropriate, put on the board to keep everyone conscious of keeping data safe. A task card to show the user interface to customers makes sure that we don’t forget to do this as early as possible, and it helps us start exploratory testing along with the customers early, too. All of these cards help us address all the different aspects of product quality.\n\nTechnology-facing tests that extend beyond a single story get their own row on the story board. We use stories to evaluate load test tools and to establish perfor- mance baselines to kick off our load and performance-testing efforts.\n\nThe technology-facing and business-facing tests that drive development are central to agile development, whether or not you actually write task cards for them. They give your team the best chance of getting each story “done.” Iden- tifying the tasks needed to perform the technology-facing and business- facing tests that critique the product ensures that you’ll learn what the prod- uct is missing. A combination of tests from all four quadrants will let the team know when each feature has met the customer’s criteria for functional- ity and quality.\n\nShared Responsibility\n\nOur product teams need a wide range of expertise to cover all of the agile testing quadrants. Programmers should write the technology-facing tests that support programming, but they might need help at different times from testers, database designers, system administrators, and conﬁguration special- ists. Testers take primary charge of the business-facing tests in tandem with the customers, but programmers participate in designing and automating tests, while usability and other experts might be called in as needed. The fourth quadrant, with technology-facing tests that critique the product, may require more specialists. No matter what resources have to be brought in from outside the development team, the team is still responsible for getting all four quadrants of testing done.\n\nWe believe that a successful team is one where everybody participates in the crafting of the product and that everyone shares the team’s internal pain when things go wrong. Implementing the practices and tools that enable us\n\n105\n\n—Lisa",
      "content_length": 2638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "106\n\nCHAPTER 6\n\n(cid:2) THE PURPOSE OF TESTING\n\nto address all four quadrants of testing can be painful at times, but the joy of implementing a successful product is worth the effort.\n\nMANAGING TECHNICAL DEBT Ward Cunningham coined the term “technical debt” in 1992, but we’ve cer- tainly experienced it throughout our careers in software development! Tech- nical debt builds up when the development team takes shortcuts, hacks in quick ﬁxes, or skips writing or automating tests because it’s under the gun. The code base gets harder and harder to maintain. Like ﬁnancial debt, “inter- est” compounds in the form of higher maintenance costs and lower team ve- locity. Programmers are afraid to make any changes, much less attempt refactoring to improve the code, for fear of breaking it. Sometimes this fear exists because they can’t understand the coding to start with, and sometimes it is because there are no tests to catch mistakes.\n\nEach quadrant in the agile testing matrix plays a role in keeping technical debt to a manageable level. Technology-facing tests that support coding and design help keep code maintainable. An automated build and integration process that runs unit tests is a must for minimizing technical debt. Catching unit-level defects during coding will free testers to focus on business-facing tests in order to guide the team and improve the product. Timely load and stress testing lets the teams know whether their architecture is up to the job.\n\nBy taking the time and applying resources and practices to keep technical debt to a minimum, a team will have time and resources to cover the testing needed to ensure a quality product. Applying agile principles to do a good job of each type of testing at each level will, in turn, minimize technical debt.\n\nTESTING IN CONTEXT Categorizations and deﬁnitions such as we ﬁnd in the agile testing matrix help us make sure we plan for and accomplish all of the different types of testing we need. However, we need to bear in mind that each organization, product, and team has its own unique situation, and each needs to do what works for it in its individual situation. As Lisa’s coworker Mike Busse likes to say, “It’s a tool, not a rule.” A single product or project’s needs might evolve drastically over time. The quadrants are a helpful way to make sure your team is considering all of the different aspects of testing that go into “doneness.”",
      "content_length": 2417,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "For more on con- text-driven test- ing, see www .context-driven- testing.com.\n\nTESTING IN CONTEXT\n\nWe can borrow important principles from the context-driven school of test- ing when planning testing for each story, iteration, and release.\n\n(cid:2) The value of any practice depends on its context. (cid:2) There are good practices in context, but there are no best practices. (cid:2) People, working together, are the most important part of any project’s\n\ncontext.\n\n(cid:2) Projects unfold over time in ways that are often not predictable. (cid:2) The product is a solution. If the problem isn’t solved, the product\n\ndoesn’t work.\n\n(cid:2) Good software testing is a challenging intellectual process. (cid:2) Only through judgment and skill, exercised cooperatively throughout the entire project, are we able to do the right things at the right times to effectively test our products.\n\nThe quadrants help give context to agile testing practices, but you and your team will have to adapt as you go. Testers help provide the feedback the team needs to adjust and work better. Use your skills to engage the customers throughout each iteration and release. Be conscious of when your team needs roles or knowledge beyond what it currently has available.\n\nThe Agile Testing Quadrants provide a checklist to make sure you’ve covered all your testing bases. Examine the answers to questions such as these:\n\n(cid:2) Are we using unit and component tests to help us ﬁnd the right de-\n\nsign for our application?\n\n(cid:2) Do we have an automated build process that runs our automated unit\n\ntests for quick feedback?\n\n(cid:2) Do our business-facing tests help us deliver a product that matches\n\ncustomers’ expectations?\n\n(cid:2) Are we capturing the right examples of desired system behavior? Do\n\nwe need more? Are we basing our tests on these examples?\n\n(cid:2) Do we show prototypes of UIs and reports to the users before we start coding them? Can the users relate them to how the ﬁnished software will work?\n\n(cid:2) Do we budget enough time for exploratory testing? How do we tackle\n\nusability testing? Are we involving our customers enough?\n\n(cid:2) Do we consider technological requirements such as performance and security early enough in the development cycle? Do we have the right tools to do “ility” testing?\n\n107",
      "content_length": 2311,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "108\n\nCHAPTER 6\n\n(cid:2) THE PURPOSE OF TESTING\n\nUse the matrix as a map to get started. Experiment, and use retrospectives to keep improving your efforts to guide development with tests and build on what you learn about your product through testing.\n\nSUMMARY In this chapter we introduced the Agile Testing Quadrants as a convenient way to categorize tests. The four quadrants serve as guidelines to ensure that all facets of product quality are covered in the testing and developing process.\n\n(cid:2) Tests that support the team can be used to drive requirements. (cid:2) Tests that critique the product help us think about all facets of appli-\n\ncation quality.\n\n(cid:2) Use the quadrants to know when you’re done, and ensure the whole team shares responsibility for covering the four quadrants of the matrix.\n\n(cid:2) Managing technical debt is an essential foundation for any software development team. Use the quadrants to think about the different dimensions.\n\n(cid:2) Context should always guide our testing efforts.",
      "content_length": 1022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Chapter 7\n\nTECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nSource Code Control\n\nIDEs\n\nBuild Tools\n\nBuild Automation Tools\n\nToolkit\n\nFoundation of Agile Testing\n\nPurpose of Unit Tests\n\nSupporting Infrastructure\n\nUnit Test Tools\n\nGo Faster, Do More\n\nTechnology-Facing Tests that Support the Team\n\nWhy These Tests?\n\nValue to Testers\n\nDesigning for Testing\n\nTimely Feedback\n\nWhat Testers Can Do\n\nWhat Managers Can Do\n\nWhat If You Don’t Do Them?\n\nQuadrant 1 Tests vs. Quadrant 2\n\nWhere Does One Stop & the Other Start?\n\nTeam Approach\n\nWe use the Agile Testing Quadrants as a guide to help us cover all the types of testing we need and to help us make sure we have the right resources to succeed at each type. In this chapter, we look at tests in the ﬁrst quadrant, technology- facing tests that support the team, and at tools to support this testing. The activ- ities in this quadrant form the core of agile development.\n\nAN AGILE TESTING FOUNDATION We discuss Quadrant 1 ﬁrst because the technology-facing tests that support the team form the foundation of agile development and testing. See Figure 7-1 for a reminder of the Agile Testing Quadrants with this quadrant highlighted. Quadrant 1 is about much more than testing. The unit and component tests we talk about in Quadrant 1 aren’t the ﬁrst tests written for each story, but they help guide design and development. Without a foundation of test-driven\n\n109",
      "content_length": 1407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "110\n\nCHAPTER 7\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ2 Q3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 7-1 The Agile Testing Quadrants, highlighting Quadrant 1\n\ndesign, automated unit and component tests, and a continuous integration process to run the tests, it’s hard to deliver value in a timely manner. All of the testing in the other quadrants can’t make up for inadequacies in this one. We’ll talk about the other quadrants in the next few chapters and explain how they all ﬁt together.\n\nTeams need the right tools and processes to create and execute technology- facing tests that guide development. We’ll give some examples of the types of tools needed in the last section of this chapter.\n\nThe Purpose of Quadrant 1 Tests\n\nUnit tests and component tests ensure quality by helping the programmer understand exactly what the code needs to do, and by providing guidance in",
      "content_length": 1284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "AN AGILE TESTING FOUNDATION\n\nthe right design. They help the team to focus on the story that’s being deliv- ered and to take the simplest approach that will work. Unit tests verify the behavior of parts as small as a single object or method [Meszaros, 2007]. Component tests help solidify the overall design of a deployable part of the system by testing the interaction between classes or methods.\n\nDeveloping unit tests can be an essential design tool when using TDD. When an agile programmer starts a coding task, she writes a test that captures the behavior of a tiny bit of code and then works on the code until the test passes. By building the code in small test-code-test increments, the programmer has a chance to think through the functionality that the customer needs. As questions come up, she can ask the customer. She can pair with a tester to help make sure all aspects of that piece of code, and its communication with other units, are tested.\n\nThe term test-driven development misleads practitioners who don’t under- stand that it’s more about design than testing. Code developed test-ﬁrst is naturally designed for testability. Quadrant 1 activities are all aimed at pro- ducing software with the highest possible internal quality.\n\nWhen teams practice TDD, they minimize the number of bugs that have to be caught later on. Most unit-level bugs are prevented by writing the test be- fore the code. Thinking through the design by writing the unit test means the system is more likely to meet customer requirements. When post-development testing time is occupied with ﬁnding and ﬁxing bugs that could have been detected by programmer tests, there’s no time to ﬁnd the serious issues that might adversely affect the business. The more bugs that leak out of our cod- ing process, the slower our delivery will be, and in the end, it is the quality that will suffer. That’s why the programmer tests in Quadrant 1 are so criti- cal. While every team should adopt practices that work for its situation, a team without these core agile practices is unlikely to beneﬁt much from agile values and principles.\n\nSupporting Infrastructure\n\nSolid source code control, conﬁguration management, and continuous inte- gration are essential to getting value from programmer tests that guide devel- opment. They enable the team to always know exactly what’s being tested. Continuous integration gives us a way to run tests every time new code is checked in. When a test fails, we know who checked in the change that caused the failure, and that person can quickly ﬁx the problem. Continuous\n\n111",
      "content_length": 2590,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "112\n\nCHAPTER 7\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nintegration saves time and motivates each programmer to run the tests be- fore checking in the new code. A continuous integration and build process delivers a deployable package of code for us to test.\n\nAgile projects that lack these core agile practices tend to turn into “mini- waterfalls.” The development cycles are shorter, but code is still being thrown “over the wall” to testers who run out of time to test because the code is of poor quality. The term waterfall isn’t necessarily derogatory. We’ve worked on successful “waterfall” projects where the programmers stringently auto- mate unit tests, practice continuous integration, and use automated builds to run tests. These successful “waterfall” projects also involve customers and testers throughout the development cycle. When we code without appropri- ate practices and tools, regardless of what we call the process, we’re not going to deliver high-quality code in a timely manner.\n\nWHY WRITE AND EXECUTE THESE TESTS? We’re not going into any details here about how to do TDD, or the best ways to write unit and component tests. There are several excellent books on those sub- jects. Our goal is to explain why these activities are important to agile testers. Let’s explore some reasons to use technology-facing tests that support the team.\n\nLets Us Go Faster and Do More\n\nSpeed should never be the end goal of an agile development team. Trying to do things fast and meet tight deadlines without thinking about the quality causes us to cut corners and revert to old, bad habits. If we cut corners, we’ll build up more technical debt, and probably miss the deadline anyway. Hap- pily, though, speed is a long-term side effect of producing code with the highest possible internal quality. Continuous builds running unit tests notify the team of failure within a few minutes of the problem check-in, and the mistake can be found and ﬁxed quickly. A safety net of automated unit and code integration tests enables the programmers to refactor frequently. This keeps the code at a reasonable standard of maintainability and delivers the best value for the time invested. Technical debt is kept as low as possible.\n\nIf you’ve worked as a tester on a project where unit testing was neglected, you know how easy it is to spend all of your time ﬁnding unit-level defects. You might ﬁnd so many bugs while testing the “happy path” that you never have time to test more complex scenarios and edge cases. The release deadline is pushed back as the “ﬁnd and ﬁx” cycle drags on, or testing is just stopped and a buggy product is foisted off on unsuspecting customers.",
      "content_length": 2687,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "WHY WRITE AND EXECUTE THESE TESTS?\n\nOur years on agile teams have been Utopian in contrast to this scenario. Driving coding practices with tests means that the programmers probably understood the story’s requirements reasonably well. They’ve talked exten- sively with the customers and testers to clarify the desired behaviors. All parties understand the changes being made. By the time the team has com- pleted all of the task cards for coding a story, or a thin, testable slice of one, the feature has been well covered by unit and component tests. Usually the programmers have made sure at least one path through the story works end to end.\n\nThis means that we, as testers, waste little time ﬁnding low-level bugs. We’re likely to try scenarios the programmers hadn’t thought of and to spend our time on higher-level business functionality. Well-designed code is usually ro- bust and testable. If we ﬁnd a defect, we show it to the programmer, who writes a unit test to reproduce the bug and then ﬁxes it quickly. We actually have time to focus on exploratory testing and the other types of in-depth tests to give the code a good workout and learn more about how it should work. Often, the only “bugs” we ﬁnd are requirements that everyone on our team missed or misunderstood. Even those are found quickly if the customer is involved and has regular demos and test opportunities. After a development team has mas- tered TDD, the focus for improvement shifts from bug prevention to ﬁguring out better ways to elicit and capture requirements before coding.\n\nTest-First Development vs. Test-Driven Development\n\nGerard Meszaros [Meszaros 2007, pp. 813–814] offers the following descrip- tion of how test-ﬁrst development differs from test-driven development:\n\n“Unlike test-driven development, test-ﬁrst development merely says that the tests are written before the production code; it does not imply that the production code is made to work one test at a time (emergent design). Test-ﬁrst development can be applied at the unit test or customer test level, depending on which tests we have chosen to automate.”\n\nErik Bos [2008] observes that test-ﬁrst development involves both test-ﬁrst programming and test-ﬁrst design, but there’s a subtle difference:\n\n“With test-ﬁrst design, the design follows the tests, whereas you can do test-ﬁrst programming of a design that you ﬁrst write down on a white- board. On larger projects, we tend to do more design via whiteboard discussions; the team discusses the architecture around a whiteboard, and codes test-ﬁrst based on this design. On smaller projects, we do practice test-driven design.”\n\n113",
      "content_length": 2640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "114\n\nCHAPTER 7\n\nLisa’s Story\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nThere are several different philosophies about when to write tests and for what purpose. It’s up to each team to agree on the approach that helps it achieve its quality objectives, although there is common agreement in the ag- ile community that TDD deﬁnitely helps a team achieve better-quality soft- ware. This is an important way that programmer tests support the team. Let’s look at some more ways.\n\nMaking Testers’ Jobs Easier\n\nThe core practices related to programmer tests make lots of testing activities easier to accomplish. Programmers work in their own sandboxes, where they can test new code without affecting anyone else’s work. They don’t check in code until it has passed a suite of regression tests in their sandbox.\n\nThe team thinks about test environments and what to use for test data. Unit tests usually work with fake or mock objects instead of actual databases for speed, but programmers still need to test against realistic data. Testers can help them identify good test data. If the unit tests represent real-life data, fewer issues will be found later.\n\nHere’s a small example. When my current team ﬁrst adopted agile development, we didn’t have any automated tests. We had no way to produce a deployable code package, and we had no rudimentary test environments or test databases. I didn’t have any means to produce a build myself, either. We decided to start writ- ing code test-ﬁrst and committed to automating tests at all levels where appropri- ate, but we needed some infrastructure ﬁrst.\n\nOur ﬁrst priority was to implement a continuous build process, which was done in a couple of days. Each build sent an email with a list of checked-in ﬁles and com- ments about the updates. I could now choose which build to deploy and test. The next priority was to provide independent test environments so that tests run by one person would not interfere with other tests. The new database expert cre- ated new schemas to meet testing needs and a “seed” database of canonical, production-like data. These schemas could be refreshed on demand quickly with a clean set of data. Each team member, including me, got a unique and indepen- dent test environment.\n\nEven before the team mastered TDD, the adopted infrastructure was in place to support executing tests. This infrastructure enabled the team to start testing much more effectively. Another aspect of trying to automate testing was dealing with a legacy application that was difﬁcult to test. The decisions that were made to enable TDD also helped with customer-facing tests. We decided to start rewriting the system in a new architecture that facilitated testing and test automation, not only at the unit level but at all levels.\n\n—Lisa",
      "content_length": 2798,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "WHY WRITE AND EXECUTE THESE TESTS?\n\nWriting tests and writing code with those tests in mind means programmers are always consciously making code testable. All of these good infrastructure- related qualities spill over to business-facing tests and tests that critique the product. The whole team is continually thinking of ways to improve design and make testing easier.\n\nDesigning with Testing in Mind\n\nOne advantage of driving development with tests is that code is written with the express intention of making the tests pass. The team has to think, right from the beginning, about how it will execute and automate tests for every story it codes. Test-driven development means that programmers will write each test before they write the code to make it pass.\n\nWriting “testable code” is a simple concept, but it’s not an easy task, espe- cially if you’re working on old code that has no automated tests and isn’t de- signed for testability. Legacy systems often have business logic, I/O, database, and user interface layers intertwined. There’s no easy way to hook in to auto- mate a test below the GUI or at the unit level.\n\nA common approach in designing a testable architecture is to separate the different layers that perform different functions in the application. Ideally, you would want to access each layer directly with a test ﬁxture and test algo- rithms with different inputs. To do this, you isolate the business logic into its own layer, using fake objects instead of trying to access other applications or the actual database. If the presentation layer can be separated from underly- ing business logic and database access, you can quickly test input validation without testing underlying logic.\n\nLayered Architectures and Testability\n\nLisa’s team took the “strangler application” approach to creating a testable sys- tem where tests could be use to drive coding. Mike Thomas, the team’s senior architect, explains how their new layered architecture enabled a testable design.\n\nA layered architecture divides a code base into horizontal slices that contain similar functionality, often related to a technology. The slices at the highest level are the most speciﬁc and depend upon the slices below, which are more general. For example, many layered code bases have slices such as the following: UI, business logic, and data access.\n\n115",
      "content_length": 2350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "116\n\nCHAPTER 7\n\nSee the bibliogra- phy for more infor- mation on Alastair Cockburn’s Ports and Adapters pattern.\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nHorizontal layering is just one way to organize a code base: Another is domain-oriented slices (such as payroll or order entry), which are gener- ally thought of as “vertical.” These layering approaches can be com- bined, of course, and all can be used to enhance testability.\n\nLayering has advantages for testing, but only if the mechanism for “con- necting” the slices provides ﬂexibility. If a code base has tightly coupled slices via such mechanisms as direct concrete class dependencies and static methods, it is difﬁcult to isolate a unit for testing, despite the layer- ing. This makes most automated tests into integration tests, which can be complicated and can run slowly. In many cases, testing can only be accomplished by running the entire system.\n\nContrast this with a code base where the layers are separated by inter- faces. Each slice depends only upon interfaces deﬁned in the slice beneath it rather than on speciﬁc classes. Dependencies on such inter- faces are easy to satisfy with test doubles at test time: mocks, stubs, and so on. Unit testing is thus simpliﬁed because each unit can truly be iso- lated. For example, the UI can be tested against mock business layer objects, and the business layer can be tested against mock data access objects, avoiding live database access.\n\nThe layered approach has allowed Lisa’s team to succeed in automating tests at all levels and drive development with both technology-facing and business- facing tests.\n\nAnother example of an approach to testable design is Alistair Cockburn’s Ports and Adapters pattern [Cockburn, 2005]. This pattern’s intent is to “cre- ate your application to work without either a UI or a database so you can run automated regression tests against the application, work when the database becomes unavailable, and link applications together without any user in- volvement.” Ports accept outside events, and a technology-speciﬁc adapter converts it into a message that can be understood by the application. In turn, the application sends output via a port to an adapter, which creates the sig- nals needed by the receiving human or automated users. Applications de- signed using this pattern can be driven by automated test scripts as easily as by actual users.\n\nIt’s more obvious how to code test-ﬁrst on a greenﬁeld project. Legacy sys- tems, which aren’t covered by automated unit tests, present a huge challenge. It’s hard to write unit tests for code that isn’t designed for testability, and it’s hard to change code that isn’t safeguarded with unit tests. Many teams have",
      "content_length": 2733,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "The bibliography has links to more articles about “res- cue” and “stran- gler” approaches to legacy code.\n\nFor more informa- tion about Pre- senter First development, see the bibliography.\n\nWHY WRITE AND EXECUTE THESE TESTS?\n\nfollowed the “legacy code rescue” techniques explained by Michael Feathers in Working Effectively with Legacy Code [Feathers, 2005]. Other teams, such as Lisa’s, aim to “strangle” their legacy code. This strategy stems from Martin Fowler’s “strangler application” [Fowler, 2004]. New stories were coded test- ﬁrst in a new architecture while the old system was still maintained. Over time, much of the system has been converted to the new architecture, with the goal of eventually doing away with the old system.\n\nAgile testing in a legacy mainframe type of environment presents particular challenges, not the least of which is the lack of availability of publications and information about how to do it successfully. COBOL, mainframes, and their ilk are still widely used. Let agile principles and values guide your team as you look for ways to enable automated testing in your application. You might have to adapt some techniques; for example, maybe you can’t write code test- ﬁrst, but you can test soon after writing the code. When it’s the team’s prob- lem to solve, and not just the testers’ problem, you’ll ﬁnd a way to write tests.\n\nTesting Legacy Systems\n\nJohn Voris, a developer with Crown Cork and Seal, works in the RPG lan- guage, a cousin of COBOL, which runs on the operating system previously known as AS 400 and now known as System i. John was tasked with merging new code with a vendor code base. He applied tenets of Agile, Lean, and IBM-recommended coding practices to come up with an approach he calls “ADEPT” for “AS400 Displays for External Prototyping and Testing.” While he isn’t coding test-ﬁrst, he’s testing “Minutes Afterward.” Here’s how he summed up his approach:\n\nWrite small, single-purpose modules (not monolithic programs), and refactor existing programs into modules. Use a Presenter First develop- ment approach (similar to the Model View Presenter or Model View Controller pattern).\n\nDeﬁne parameter interfaces for the testing harness based on screen formats and screen ﬁelds. The only drawback here is numbers are deﬁned as zoned decimals rather than packed hexadecimal, but this is offset by the gain in productivity.\n\n“Minutes after” coding each production module, create a testing pro- gram using the screen format to test via the UI. The UI interface for the test is created prior to the production program, because the UI testing interface is the referenced interface for the production module. The impetus for running a test looms large for the programmer, because most of the coding for the test is already done.\n\n117",
      "content_length": 2789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "118\n\nCHAPTER 7\n\nFor more about RPGUnit, see www .RPGUnit.org.\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nUse standard test data sets, which are unchanging, canonical test data, to drive the tests.\n\nThis approach, in which the test programs are almost auto-generated, lends itself to automation with a record/playback tool that would cap- ture data inputs and outputs, with tests run in a continuous build, using RPGUnit.\n\nYour team can ﬁnd an approach to designing for testability that works for you. The secret is the whole-team commitment to testing and quality. When a team is constantly working to write tests and make them pass, it ﬁnds a way to get it done. Teams should take time to consider how they can create an ar- chitecture that will make automated tests easy to create, inexpensive to main- tain, and long-lived. Don’t be afraid to revisit the architecture if automated tests don’t return enough value for the investment in them.\n\nTimely Feedback\n\nThe biggest value of unit tests is in the speed of their feedback. In our opin- ion, a continuous integration and build process that runs the unit tests should ﬁnish within ten minutes. If each programmer checks code in several times a day, a longer build and test process will cause changes to start stack- ing up. As a tester, it can be frustrating to have to wait a long time for new functionality or a bug ﬁx. If there’s a compile error or unit test failure, the de- lay gets even worse, especially if it’s almost time to go home!\n\nA build and test process that runs tests above the unit level, such as func- tional API tests or GUI tests, is going to take longer. Have at least one build process that runs quickly, and a second that runs the slower tests. There should be at least one daily “build” that runs all of the slower functional tests. However, even that can be unwieldy. When a test fails and the problem is ﬁxed, how long will it take to know for sure that the build passes again?\n\nIf your build and test process takes too long, ask your team to analyze the cause of the slowdown and take steps to speed up the build. Here are a few examples.\n\n(cid:2) Database access usually consumes lots of time, so consider using fake objects, where possible, to replace the database, especially at the unit level.\n\n(cid:2) Move longer-running integration and database-access tests to the sec-\n\nondary build and test process.\n\n(cid:2) See if tests can run in parallel so that they ﬁnish faster.",
      "content_length": 2472,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "Lisa’s Story\n\nWHERE DO TECHNOLOGY-FACING TESTS STOP?\n\n(cid:2) Run the minimum tests needed for regression testing your system. (cid:2) Distribute tasks across multiple build machines. (cid:2) Upgrade the hardware and software that run the build. (cid:2) Find the area that takes the most time and take incremental steps to\n\nspeed it up.\n\nEarly in my current team’s agile evolution, we had few unit tests, so we included a few GUI smoke tests in our continual build, which kicked off on every check-in to the source code control system. When we had enough unit tests to feel good about knowing when code was broken, we moved the GUI tests and the FitNesse functional tests into a separate build and test process that ran at night, on the same machine as our continual build.\n\nOur continual ongoing build started out taking less than 10 minutes, but soon was taking more than 15 minutes to complete. We wrote task cards to diagnose and ﬁx the problem. The unit tests that the programmers had written early on weren’t well designed, because nobody was sure of the best way to write unit tests. Time was budgeted to refactor the unit tests, use mock data access objects instead of the real database, and redesign tests for speed. This got the build to around eight minutes. Every time it has started to creep up, we’ve addressed the problem with refactoring, removing unnecessary tests, upgrading the hardware, and choosing different software that helped the build run faster.\n\nAs our functional tests covered more code, the nightly build broke more often. Because the nightly build ran on the same machine as the continual ongoing one, the only way to verify that the build was “green” again was to stop the ongoing build, which removed our fast feedback. This started to waste everyone’s time. We bought and set up another build machine for the longer build, which now also runs continuously. This was much less expensive than spending so much time keeping two builds running on the same machine, and now we get quick feed- back from our functional tests as well.\n\nWow, multiple continuous build and test processes providing constant feed- back—it sounds like a dream to a lot of testers. Regression bugs will be caught early, when they’re cheapest to ﬁx. This is a great reason for writing technology- facing tests. Can we get too carried away with them, though? Let’s look at the line between technology-facing tests and business-facing tests.\n\nWHERE DO TECHNOLOGY-FACING TESTS STOP? We often hear people worry that the customer-facing tests will overlap so much with the technology-facing tests that the team will waste time. We know that\n\n119\n\n—Lisa",
      "content_length": 2651,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "120\n\nCHAPTER 7\n\nChapter 13, “Why We Want to Auto- mate Tests and What Holds Us Back,” talks more about the ROI of the different types of tests.\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nbusiness-facing tests might cover a bit of the same ground as unit or code inte- gration tests, but they have such different purposes that waste isn’t a worry.\n\nFor example, we have a story to calculate a loan amortization schedule and display it to a user who’s in the process of requesting a loan. A unit test for this story would likely test for illegal arguments, such as an annual payment frequency if the business doesn’t allow it. There might be a unit test to ﬁgure the anticipated loan payment start date given some deﬁnition of amount, in- terest rate, start date, and frequency. Unit-level tests could cover different combinations of payment frequency, amount, interest date, term, and start date in order to prove that the amortization calculation is correct. They could cover scenarios such as leap years. When these tests pass, the program- mer feels conﬁdent about the code.\n\nEach unit test is independent and tests one dimension at a time. This means that when a unit test fails, the programmer can identify the problem quickly and solve the issue just as quickly. The business-facing tests very seldom cover only one dimension, because they are tackled from a business point of view.\n\nThe business-facing tests for this story would deﬁne more details for the business rules, the presentation in the user interface, and error handling. They would verify that payment details, such as the principal and interest ap- plied, display correctly in the user interface. They would test validations for each ﬁeld on the user interface, and specify error handling for situations such as insufﬁcient balance or ineligibility. They could test a scenario where an ad- ministrator processes two loan payments on the same day, which might be harder to simulate at the unit level.\n\nThe business-facing tests cover more complex user scenarios and verify that the end user will have a good experience. Push tests to lower levels whenever possible; if you identify a test case that can be automated at the unit level, that’s almost always a better return on investment.\n\nIf multiple areas or layers of the application are involved, it might not be pos- sible to automate at the unit level. Both technology-facing and business- facing levels might have tests around the date of the ﬁrst loan payment, but they check for different reasons. The unit test would check the calculation of the date, and the business-facing test would verify that it displays correctly in the borrower’s loan report.\n\nLearning to write Quadrant 1 tests is hard. Many teams making the transi- tion to agile development start out with no automated unit tests, not even a",
      "content_length": 2842,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "Lisa’s Story\n\nWHAT IF THE TEAM DOESN’T DO THESE TESTS?\n\ncontinuous integration and build process. In the next section, we suggest ac- tions agile testers can take if their teams don’t tackle Quadrant 1 tests.\n\nWHAT IF THE TEAM DOESN’T DO THESE TESTS? Many an organization has decided to try agile development, or at least stated that intention, without understanding how to make a successful transition. When we’re in a tester role, what can we do to help the development team implement TDD, continuous integration, and other practices that are key to successful development?\n\nOur experience over the years has been that if we aren’t programmers our- selves, we don’t necessarily have much credibility when we urge the program- mers to adopt practices such as TDD. If we could sit down and show them how to code test-ﬁrst, that would be persuasive, but many of us testers don’t have that kind of experience. We’ve also found that evangelizing doesn’t work. It’s not that hard to convince someone conceptually that TDD is a good idea. It’s much trickier to help them get traction actually coding test-ﬁrst.\n\nWhat Can Testers Do?\n\nIf you’re a tester on a so-called “agile” team that isn’t even automating unit tests or producing continuous builds—or at a minimum, doing builds on a daily basis—you’re going to get frustrated pretty quickly. Don’t give up; keep brainstorming for a way to get traction on a positive transition. Try using so- cial time or other relaxing activity to take some quality time to see what new ideas you can generate to get all team members on board.\n\nOne trap to avoid is having testers write the unit tests. Because TDD is really- more of a design activity, it’s essential that the person writing the code also write the tests, before writing the code. Programmers also need the immediate feedback that automated unit tests give. Unit tests written by someone else af- ter the code is written might still guard against regression defects, but they won’t have the most valuable beneﬁts of tests written by the programmer.\n\nWhenever I’ve wanted to effect change, I’ve turned to the patterns in Fearless Change by Mary Lynn Manns and Linda Rising [2004]. After working on two XP teams, I joined a team that professed a desire to become agile but wasn’t making strides toward solid development practices. I found several patterns in Fearless Change to try to move the team toward agile practices.\n\n121",
      "content_length": 2423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "122\n\nCHAPTER 7\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\n“Ask for Help” was one pattern that helped me. This pattern says, in part: “Since the task of introducing a new idea into an organization is a big job, look for peo- ple and resources to help your efforts” [Manns and Rising, 2004]. When I wanted my team to start using FitNesse, I identiﬁed the programmer who was most sympa- thetic to my cause and asked him to pair with me to write FitNesse tests for the story he was working on. He told the other programmers about the beneﬁts he derived from the FitNesse tests, which encouraged them to try it too. Most people want to help, and agile is all about the team working together, so there’s no rea- son to go it alone.\n\n“Brown Bag” is another change pattern that my teams have put to good use. For example, my current team held several brown bag sessions where they wrote unit tests together. “Guru on Your Side” is a productive pattern in which you enlist the help of a well-respected team member who might understand what you’re trying to achieve. A previous team I was on was not motivated to write unit tests. The most experienced programmer on the team agreed with me that test-driven development was a good idea, and he set an example for the rest of the team.\n\nWe think you’ll ﬁnd that there’s always someone on an agile team who’s sympa- thetic to your cause. Enlist that person’s support, especially if the team perceives him or her as a senior-level guru.\n\nAs a tester on an agile team, there’s a lot you can do to act as a change agent, but your potential impact is limited. In some cases, strong management sup- port is the key to driving the team to engage in Quadrant 1 activities.\n\nWhat Can Managers Do?\n\nIf you’re managing a development team, you can do a lot to encourage test- driven development and unit test automation. Work with the product owner to make quality your goal, and communicate the quality criteria to the team. Encourage the programmers to take time to do their best work instead of worrying about meeting a deadline. If a delivery date is in jeopardy, push to reduce the scope, not the quality. Your job is to explain to the business man- agers how making quality a priority will ensure that they get optimum busi- ness value.\n\nGive the team time to learn, and provide expert, hands-on training. Bring in an experienced agile development coach or hire someone with experience in using these practices who can transfer those skills to the rest of the team. Budget time for major refactoring, for brainstorming about the best ap- proach to writing unit and code integration tests, and for evaluating, install- ing, and upgrading tools. Test managers should work with development\n\n—Lisa",
      "content_length": 2738,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "More about retro- spectives and pro- cess improvement in Chapter 19, “Wrap Up the Iteration.”\n\nTOOLKIT\n\nmanagers to encourage practices that enhance testability and allow testers to write executable tests. Test managers can also make sure testers have time to learn how to use the automation tools and frameworks that the team decides to implement.\n\nIt’s a Team Problem\n\nWhile you can ﬁnd ways to be an effective change agent, the best thing to do is involve the whole team in solving the problems. If you aren’t already doing retrospectives after every iteration, propose trying this practice or some other type of process improvement. At the retrospective, raise issues that are hampering successful delivery. For example, “We aren’t ﬁnishing testing tasks before the end of the iteration” is a problem for the whole team to address. If one reason for not ﬁnishing is the high number of unit-level bugs, suggest experimenting with TDD, but allow programmers to propose their own ways to address the problem. Encourage the team to try a new approach for a few iterations and see how it works.\n\nTechnology-facing tests that support the team’s development process are an important foundation for all of the testing that needs to happen. If the team isn’t doing an adequate job with the tests in this quadrant, the other types of testing will be much more difﬁcult. This doesn’t mean you can’t get value from the other quadrants on their own—it just means it will be harder to do so because the team’s code will lack internal quality and everything will take longer.\n\nTechnology-facing tests can’t be done without the right tools and infrastruc- ture. In the next section, we look at examples of the types of tools a team needs to be effective with Quadrant 1 tests.\n\nTOOLKIT There’s no magical tool that will ensure success. However, tools can help good people do their best work. Building up the right infrastructure to sup- port technology-facing tests is critical. There’s a huge selection of excellent tools available, and they improve all the time. Your team must ﬁnd the tools that work best for your situation.\n\nSource Code Control\n\nSource code control is known by other names too, such as version control or revision control. It’s certainly not new, or unique to agile development, but\n\n123",
      "content_length": 2297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "124\n\nCHAPTER 7\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nno software development team can succeed without it. That’s why we’re dis- cussing it here. Without source code control, you’ll never be sure what you’re testing. Did the programmer change only the module he said he changed, or did he forget changes he made to other modules? You can’t back out un- wanted or erroneous changes without some kind of versioning system. Source code control keeps different programmers from walking on each other’s changes to the same modules. Without versioning, you can’t be sure what code to release to production.\n\nSoftware Conﬁguration Management Patterns: Effective Teamwork, Practical Integrations [2003], by Stephen Berczuk and Brad Appleton, is a good resource to use to learn how and why to use source code control. Source code control is essential to any style of software development.\n\nUse source code control for automated test scripts, too. It’s important to tie the automated tests with the corresponding code version that they tested in case you need to rerun tests against that version in the future. When you label or tag a build, make sure you label or tag the test code too, even if it doesn’t get released to production.\n\nTeams can organize their code hierarchy to provide a repository for produc- tion code, corresponding unit tests, and higher-level test scripts. Doing this might require some brainstorming and experimenting in order to get the right structure.\n\nThere are many terriﬁc options to choose from. Open source systems such as CVS and Subversion (SVN) are easy to implement, integrate with a continu- ous build process and IDEs, and are robust. Vendor tools such as IBM Ratio- nal ClearCase and Perforce might add features that compensate for the increased overhead they often bring.\n\nSource code control is tightly integrated with development environments. Let’s look at some IDEs used by agile teams.\n\nIDEs\n\nA good IDE (integrated development environment) can be helpful for pro- grammers and testers on an agile team. The IDE integrates with the source code control system to help prevent problems with versioning and changes walking on each other. The editors inside an IDE are speciﬁc to the program- ming language and ﬂag errors even as you write the code. Most importantly, IDEs provide support for refactoring.",
      "content_length": 2353,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Lisa’s Story\n\nTOOLKIT\n\nProgrammers who use an IDE tend to have strong personal preferences. However, sometimes an organization decrees that all programmers must use a speciﬁc IDE. This might be because of licensing, or it might be intended to encourage open pair programming. It is easier to pair with another program- mer if the other person uses the same IDE, but it’s generally not essential for the same one to be used. Most tools work similarly, so it’s not hard to change from one IDE to another in order to meet new needs or take advantage of new features. Some diehards still prefer to use tried-and-true technology such as vi, vim, or emacs with make ﬁles rather than an IDE.\n\nOpen source IDEs such as Eclipse and NetBeans are widely used by agile teams, along with proprietary systems such as Visual Studio and IntelliJ IDEA. IDEs have plug-ins to support different languages and tools. They work as well with test scripts as they do with production code.\n\nOn my current team, some programmers were using IntelliJ IDEA, while others used Eclipse. Environmental differences in rare cases caused issues, such as tests passing in the IDE but not the full build, or check-ins via the IDE causing havoc in the source code control system. Generally, though, use of different IDEs caused no problems. Interestingly, over time most of the Eclipse users switched. Pairing with the IntelliJ users led them to prefer it.\n\nI use Eclipse to work with the automated test scripts as well as to research issues with the production code. The Ruby plug-in helps us with our Ruby and Watir scripts, and the XML editor helps with our Canoo WebTest scripts. We can run unit tests and do builds through the IDE. Programmers on the team helped me set up and start using Eclipse, and it has saved huge amounts of time. Maintaining the automated tests is much easier, and the IDE’s “synchronize” view helps me remem- ber to check in all of the modules I’ve changed.\n\nTest tools are starting to come out with their own IDEs or plug-ins to work with existing IDEs such as Eclipse. Take advantage of these powerful, time-saving, quality-promoting tools.\n\nTesters who aren’t automating tests through an IDE, but who want to be able to look at changed snippets of code, can use tools such as FishEye that enable the testers to get access to the code through the automated build.\n\nAs of this writing, IDEs have added support for dynamic languages such as Ruby, Groovy, and Python. Programmers who use dynamic languages may prefer lighter-weight tools, but they still need good tools that support good coding practices, such as TDD and refactoring.\n\n125\n\n—Lisa",
      "content_length": 2638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "126\n\nCHAPTER 7\n\n(cid:2) TECHNOLOGY-FACING TESTS THAT SUPPORT THE TEAM\n\nRegardless of the development environment and tools being used, agile teams need a framework that will integrate code changes from different pro- grammers, run the unit tests to verify no regression bugs have occurred, and provide the code in a deployable format.\n\nBuild Tools\n\nYour team needs some way to build the software and create a deployable jar, war, or other type of ﬁle. This can be done with shell-based tools such as make, but those tools have limitations, such as the platforms where they work. Agile teams that we know use tools such as ant, Nant, and Maven to build their projects. These tools not only manage the build but also provide easy ways to report and document build results, and they integrate easily with build automation and test tools. They also integrate with IDEs.\n\nBuild Automation Tools\n\nContinuous integration is a core practice for agile teams. You need a way to not only build the project but also run automated tests on each build to make sure nothing broke. A fully automated and reproducible build that runs many times a day is a key success factor for agile teams. Automated build tools provide features such as email notiﬁcation of build results, and they in- tegrate with build and source code control tools.\n\nCommonly used tools as of the writing of this book include the open source tools CruiseControl, CruiseControl.net, CruiseControl.rb, and Hudson. Other open source and proprietary tools available at publication time are AnthillPro, Bamboo, BuildBeat, CI Factory, Team City, and Pulse, just to name a few.\n\nWithout an automated build process you’ll have a hard time deploying code for testing as well as releasing. Build management and build automation tools are easy to implement and absolutely necessary for successful agile projects. Make sure you get your build process going early, even before you start coding. Experiment with different tools when you ﬁnd you need more features than your current process provides.\n\nUnit Test Tools\n\nUnit test tools are speciﬁc to the language in which you’re coding. “xUnit” tools are commonly used by agile teams, and there’s a ﬂavor for many differ- ent languages, including JUnit for Java, NUnit for .NET, Test::Unit for Perl and Ruby, and PyUnit for Python.",
      "content_length": 2321,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "See Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” for more information on behavior- driven develop- ment tools.\n\nSee the bibliogra- phy for links and books to help your team search for the right unit test tools.\n\nSUMMARY\n\nBehavior-driven development is another ﬂavor of test-driven development, spelling out expected behavior to drive tests with tools such as RSpec and easyb.\n\nGUI code can and should be developed test-ﬁrst as well. Some tools for rich- client unit testing are TestNG, Abbot, and SWTBot.\n\nTools such as EasyMock and Ruby/Mock help with implementing mock ob- jects and test stubs, an integral part of well-designed unit tests.\n\nThe tools programmers use to write technology-facing tests can also be used for business-facing tests. Whether they are suited for that purpose in your project depends on the needs of your team and your customers.\n\nSUMMARY In this chapter, we explained the purpose of technology-facing tests that sup- port the team, and we talked about what teams need to use them effectively.\n\n(cid:2) Technology-facing tests that support programming let the team pro- duce the highest quality code possible; they form the foundation for all other types of testing.\n\n(cid:2) The beneﬁts of this quadrant’s tests include going faster and doing more, but speed and quantity should never be the ultimate goal. (cid:2) Programmers write technology-facing tests that support the team and provide great value to testers by enhancing the internal quality and testability of the system.\n\n(cid:2) Teams that fail to implement the core practices related to agile devel-\n\nopment are likely to struggle.\n\n(cid:2) Legacy systems usually present the biggest obstacles to test-driven\n\ndevelopment, but these problems can be overcome with incremental approaches.\n\n(cid:2) If your team doesn’t now do these tests, you can help them get\n\nstarted by engaging other team members and getting support from management.\n\n(cid:2) There can be some overlap between technology-facing tests and business-facing tests that support the team. However, when faced with a choice, push tests to the lowest level in order to maximize ROI. (cid:2) Teams should set up continuous integration, build, and test processes\n\nin order to provide feedback as quickly as possible.\n\n(cid:2) Agile teams require tools for tasks such as source code control, test automation, IDEs, and build management in order to facilitate technology-facing tests that support the team.\n\n127",
      "content_length": 2482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "Chapter 8\n\nBUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nTestability and Automation\n\nDriving Development with Business-Facing Tests\n\nCommon Language\n\nEliciting Requirements\n\nPost Conditions\n\nPeril: Forgetting the Big Picture\n\nTest Mitigate Risks\n\nBusiness-Facing Tests that Support the Team\n\nThe Requirements Quandary\n\nAdvance Clarity\n\nConditions of Satisfaction\n\nRipple Effects\n\nCustomer Availability\n\nKnowing When We’re Done\n\nThin Slices, Small Chunks\n\nIn the last chapter, we talked about programmer tests, those low-level tests that help programmers make sure they have written the code right. How do they know the right thing to build? In phased and gated methodologies, we try to solve that by gathering requirements up front and putting as much detail in them as possible. In projects using agile practices, we put all our faith in story cards and tests that customers understand in order to help code the right thing. These “understandable” tests are the subject of this chapter.\n\nDRIVING DEVELOPMENT WITH BUSINESS-FACING TESTS Yikes, we’re starting an iteration with no more information than what ﬁts on an index card, something like what’s shown in Figure 8-1.\n\nThat’s not much information, and it’s not meant to be. Stories are a brief de- scription of desired functionality and an aid to planning and prioritizing work. On a traditional waterfall project, the development team might be\n\n129",
      "content_length": 1401,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "130\n\nCHAPTER 8\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nStory PA-2\n\nAs an internet shopper on LotsO’Stuff.xx, I want free\n\nshipping when my order exceeds the free shipping\n\nthreshold, so that I can take advantage of ordering\n\nmore at one time.\n\nFigure 8-1 Story to set up conversation\n\ngiven a wordy requirements document that includes every detail of the fea- ture set. On an agile project, the customer team and development team strike up a conversation based on the story. The team needs requirements of some kind, and they need them at a level that will let them start writing working code almost immediately. To do this, we need examples to turn into tests that will conﬁrm what the customer really wants.\n\nThese business-facing tests address business requirements. These tests help provide the big picture and enough details to guide coding. Business-facing tests express requirements based on examples and use a language and format that both the customer and development teams can understand. Examples form the basis of learning the desired behavior of each feature, and we use those examples as the basis for our story tests in Quadrants 2 (see Figure 8-2).\n\nBusiness-facing tests are also called “customer-facing,” “story,” “customer,” and “acceptance” tests. The term “acceptance test” is particularly confusing, because it makes some people think only of “user acceptance tests.” In the context of agile development, acceptance tests generally refer to the business- facing tests, but the term could also include the technology-facing tests from Quadrant 4, such as the customer’s criteria for system performance or secu- rity. In this chapter, we’re discussing only the business-facing tests that sup- port the team by guiding development and providing quick feedback.\n\nAs we explained in the previous two chapters, the order in which we present these four quadrants isn’t related to the order in which we might perform",
      "content_length": 1944,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "Part V, “An Itera- tion in the Life,” examines the or- der in which we perform tests from the different quadrants.\n\nDRIVING DEVELOPMENT WITH BUSINESS-FACING TESTS\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nQ2\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 8-2 The Agile Testing Quadrants, highlighting Quadrant 2\n\nactivities from each quadrant. The business-facing tests in Quadrant 2 are written for each story before coding is started, because they help the team understand what code to write. Like the tests in Quadrant 1, these tests drive development, but at a higher level. Quadrant 1 activities ensure internal qual- ity, maximize team productivity, and minimize technical debt. Quadrant 2 tests deﬁne and verify external quality, and help us know when we’re done.\n\nThe customer tests to drive coding are generally written in an executable for- mat, and automated, so that team members can run the tests as often as they like in order to see if the functionality works as desired. These tests, or some subset of them, will become part of an automated regression suite so that fu- ture development doesn’t unintentionally change system behavior.\n\nAs we discuss the stories and examples of desired behavior, we must also deﬁne nonfunctional requirements such as performance, security, and usability. We’ll\n\n131",
      "content_length": 1663,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "132\n\nCHAPTER 8\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nalso make note of scenarios for manual exploratory testing. We’ll talk about these other types of testing activities in the chapters on Quadrants 3 and 4.\n\nWe hear lots of questions relating to how agile teams get requirements. How do we know what the code we write should do? How do we obtain enough information to start coding? How do we get the customers to speak with one voice and present their needs clearly? Where do we start on each story? How do we get customers to give us examples? How do we use those to write story tests?\n\nThis chapter explains our strategy for creating business-facing tests that sup- port the team as it develops each story. Let’s start by talking more about requirements.\n\nTHE REQUIREMENTS QUANDARY Just about every development team we’ve known, agile or not, struggles with requirements. Teams on traditional waterfall projects might invest months in requirements gathering only to have them be wrong or quickly get out of date. Teams in chaos mode might have no requirements at all, with the pro- grammers making their best guess as to how a feature should work.\n\nAgile development embraces change, but what happens when requirements change during an iteration? We don’t want a long requirements-gathering period before we start coding, but how can we be sure we (and our custom- ers) really understand the details of each story?\n\nIn agile development, new features usually start out life as stories, or groups of stories, written by the customer team. Story writing is not about ﬁguring out implementation details, although high-level discussions can have an im- pact on dependencies and how many stories are created. It’s helpful if some members of the technical team can participate in story-writing sessions so that they can have input into the functionality stories and help ensure that technical stories are included as part of the backlog. Programmers and testers can also help customers break stories down to appropriate sizes, suggest al- ternatives that might be more practical to implement, and discuss dependen- cies between stories.\n\nStories by themselves don’t give much detail about the desired functionality. They’re usually just a sentence that expresses who wants the feature, what the feature is, and why they want it. “As an Internet shopper, I need a way to delete items from my shopping cart so I don’t have to buy unwanted items” leaves a",
      "content_length": 2465,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "THE REQUIREMENTS QUANDARY\n\nlot to the imagination. Stories are only intended as a starting point for an on- going dialogue between business experts and the development team. If team members understand what problem the customer is trying to solve, they can suggest alternatives that might be simpler to use and implement.\n\nIn this dialogue between customers and developers, agile teams expand on stories until they have enough information to write appropriate code. Testers help elicit examples and context for each story, and help customers write story tests. These tests guide programmers as they write the code and help the team know when it has met the customers’ conditions of satisfaction. If your team has use cases, they can help to supplement the example or coach- ing test to clarify the needed functionality (see Figure 8-3).\n\nIn agile development, we accept that we’ll never understand all of the re- quirements for a story ahead of time. After the code that makes the story tests pass is completed, we still need to do more testing to better understand the requirements and how the features should work.\n\nAfter customers have a chance to see what the team is delivering, they might have different ideas about how they want it to work. Often customers have a vague idea of what they want and a hard time deﬁning exactly what that is. The team works with the customer or customer proxy for an iteration and might deliver just a kernel of a solution. The team keeps reﬁning the function- ality over multiple iterations until it has deﬁned and delivered the feature.\n\nBeing able to iterate is one reason agile development advocates small releases and developing one small chunk at a time. If our customer is unhappy with the behavior of the code we deliver in this iteration, we can quickly rectify that in the next, if they deem it important. Requirements changes are pretty much inevitable.\n\nWe must learn as much as we can about our customers’ wants and needs. If our end users work in our location, or it’s feasible to travel to theirs, we should sit with them, work alongside them, and be able to do their jobs if we can. Not only will we understand their requirements better but we might even identify requirements they didn’t think to state.\n\nStory\n\n+\n\nExample/ Coaching Test\n\n+\n\nConversation\n\n=\n\nRequirement\n\nFigure 8-3 The makeup of a requirement\n\n133",
      "content_length": 2368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "134\n\nCHAPTER 8\n\nMore on Fit in Chapter 9, “Tool- kit for Business- Facing Tests that Support the Team.”\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nTests need to include more than the customers’ stated requirements. We need to test for post conditions, impact on the system as a whole, and integration with other systems. We identify risks and mitigate those with tests as needed. All of these factors guide our coding.\n\nCommon Language\n\nWe can also use our tests to provide a common language that’s understood by both the development team and the business experts. As Brian Marick [2004] points out, a shared language helps the business people envision the features they want. It helps the programmers craft well-designed code that’s easy to extend. Real-life examples of desired and undesired behavior can be expressed so that they’re understood by both the business and technical sides. Pictures, ﬂow diagrams, spreadsheets, and prototypes are accessible to people with different backgrounds and viewpoints. We can use these tools to ﬁnd examples and then easily turn those examples into tests. The tests need to be written in a way that’s comprehensible to a business user reading them yet still executable by the technical team.\n\nBusiness-facing tests also help deﬁne scope, so that everyone knows what is part of the story and what isn’t. Many of the test frameworks now allow teams to create a domain language and deﬁne tests using that language. Fit (Functional for Integrated Framework) is one of those.\n\nThe Perfect Customer\n\nAndy Pols allowed us to reprint this story from his blog [Pols, 2008]. In it, he shows how his customer demanded a test, wrote it, and realized the story was out of scope.\n\nOn a recent project, our customer got so enthusiastic about our Fit tests that he got extremely upset when I implemented a story without a Fit test. He refused to let the system go live until we had the Fit test in place.\n\nThe story in question was very technical and involved sending a particular XML message to an external system. We just could not work out what a Fit test would look like for this type of requirement. Placing the expected XML message, with all its gory detail, in the Fit test would not have been helpful because this is a technical artifact and of no interest to the busi- ness. We could not work out what to do. The customer was not around to discuss this, so I just went ahead and implemented the story (very naughty!).",
      "content_length": 2462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "THE REQUIREMENTS QUANDARY\n\nWhat the customer wanted was to be sure that we were sending the correct product information in the XML message. To resolve the issue, I suggested that we have a Fit test that shows how the product attributes get mapped onto the XML message using Xpath, although I still thought this was too technical for a business user.\n\nWe gave the customer a couple of links to explain what XPath was so that he could explore whether this was a good solution for him. To my amazement, he was delighted with XPath (I now know who to turn to when I have a problem with XPath) and ﬁlled in the Fit test.\n\nThe interesting bit for me is that as soon as he knew what the message looked like and how it was structured, he realized that it did not really support the business—we were sending information that was outside our scope of our work and that should have been supplied by another system. He was also skeptical about the speed at which the external team could add new products due to the complex nature of the XML.\n\nMost agile people we tell this story to think we have the “perfect customer!”\n\nEven if your customers aren’t perfect, involving them in writing customer tests gives them a chance to identify functionality that’s outside the scope of the story. We try to write customer tests that customers can read and compre- hend. Sometimes we set the bar too low. Collaborate with your customers to ﬁnd a tool and format for writing tests that works for both the customer and development teams.\n\nIt’s ﬁne to say that our customers will provide to us the examples that we need to have in order for us to understand the value that each story should deliver. But what if they don’t know how to explain what they want? In the next section, we’ll suggest ways to help customers deﬁne their conditions of satisfaction.\n\nEliciting Requirements\n\nIf you’ve ever been a customer requesting a particular software feature, you know how hard it is to articulate exactly what you want. Often, you don’t re- ally know exactly what you want until you can see, feel, touch and use it. We have lots of ways to help our customers get clarity about what they want.\n\nAsk Questions Start by asking questions. Testers can be especially good at asking a variety of questions because they are conscious of the big picture, the business-facing\n\n135",
      "content_length": 2340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "136\n\nCHAPTER 8\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nand technical aspects of the story, and are always thinking of the end user ex- perience. Types of general questions to ask are:\n\n(cid:2) Is this story solving a problem? (cid:2) If so, what’s the problem we’re trying to solve? (cid:2) Could we implement a solution that doesn’t solve the problem? (cid:2) How will the story bring value to the business? (cid:2) Who are the end users of the feature? (cid:2) What value will they get out of it? (cid:2) What will users do right before and right after they use that feature? (cid:2) How do we know we’re done with this story?\n\nOne question Lisa likes to ask is, “What’s the worst thing that could happen?” Worst-case scenarios tend to generate ideas. They also help us consider risk and focus our tests on critical areas. Another good question is, “What’s the best thing that could happen?” This question usually generates our happy path test, but it might also uncover some hidden assumptions.\n\nUse Examples Most importantly, ask the customer to give you examples of how the feature should work. Let’s say the story is about deleting items out of an online shop- ping cart. Ask the customer to draw a picture on a whiteboard of how that delete function might look. Do they want any extra features, such as a conﬁr- mation step, or a chance to save the item in case they want to retrieve it later? What would they expect to see if the deletion couldn’t be done?\n\nExamples can form the basis for our tests. Our challenge is to capture exam- ples, which might be expressed in the business domain language, as tests that can actually be executed. Some customers are comfortable expressing exam- ples using a test tool such as Fit or FitNesse as long as they can write them in their domain language.\n\nLet’s explore the difference between an example and a test with a simple story (see Figure 8-4). People often get confused between these two terms.\n\nAn example would look something like this:\n\nThere are 5 items on a page. I want to select item 1 for $20.25 and put it in the shopping cart. I click to the next page, which has 5 more items. I select a second item on that page for $5.38 and put it in my shopping cart. When I say I’m done shopping, it will show both the item from the",
      "content_length": 2297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "THE REQUIREMENTS QUANDARY\n\n137\n\nStory PA-2\n\nAs a shopper, I want to add items to the shopping\n\ncart so I can pay for them all at once.\n\nFigure 8-4 Story to use as a base for examples and tests\n\nﬁrst page and the item from the second page in my shopping cart, with the total of $25.63\n\nThe test could be quite a bit different. We’ll use a Fit type format in Table 8-1 to show you how the test could be represented.\n\nThe test captures the example in an executable format. It might not use exactly the same inputs, but it encapsulates the sample user scenario. More test cases can be written to test boundary conditions, edge cases, and other scenarios.\n\nMultiple Viewpoints Each example or test has one point of view. Different people will write differ- ent tests or examples from their unique perspectives. We’d like to capture as many different viewpoints as we can, so think about your users.\n\nTable 8-1 Test for Story PA-2\n\nInputs\n\nExpected Results\n\nID\n\nItem\n\nPrice\n\nTotal Cost\n\n# of Items\n\n001\n\nItem A\n\n20.25\n\n20.25\n\n1\n\n002\n\nItem D\n\n0.01\n\n20.26\n\n2\n\n003\n\nItem F\n\n100.99\n\n121.25\n\n3",
      "content_length": 1082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "138\n\nCHAPTER 8\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nGetting the requirements right is an area where team members in many dif- ferent roles can jump in to help. Business analysts, subject matter experts, programmers, and various members of the customer team all have some- thing to contribute. Think about other stakeholders, such as your production support team. They have a very unique perspective.\n\nWe often forget about nonfunctional requirements such as “How long does the system need to be up? What happens if it fails? If we have middleware that passes messages, do we expect messages to be large enough that we might need to consider loss during transmission? Or will they be a constant size? What happens if there is no trafﬁc for hours? Does the system need to warn someone?” Testing for these types of requirements usually falls into quadrants 3 and 4, but we still need to write tests to make sure they get done.\n\nAll of the examples that customers give to the team add up quickly. Do we re- ally have to turn all of these into executable tests? Not as long as we have the customers there to tell us if the code is working the way they want. With tech- niques such as paper prototyping, designs can be tested before a line of code is written.\n\nWizard of Oz Testing\n\nGerard Meszaros, a Certiﬁed ScrumMaster (Practicing) and Agile Coach, shared his story about Wizard of Oz Testing on Agile Projects. He describes a good example of how artifacts we generate to elicit requirements can help communicate meaning in an unambiguous form.\n\nWe thought we were ready to release our software. We had been build- ing it one iteration at a time under the guidance of an on-site customer who had prioritized the functionality based on what he needed to enter into integration testing with his business partners. We consciously deferred the master data maintenance and reporting functionality to later iterations to ensure we had the functionality needed for integration testing ready. The integration testing went ﬁne, with just a few defects logged (all related to missing or misunderstood functionality). In the meantime, we implemented the master data maintenance in parallel with integration testing in the last few iterations. When we went into accep- tance testing with the business users, we got a rude shock: They hated the maintenance and reporting functionality! They logged so many defects and “must-have improvements” that we had to delay the release by a month. So much for coming up with a plan that would allow us to deliver early!",
      "content_length": 2561,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "THE REQUIREMENTS QUANDARY\n\nWhile we were reimplementing the master data maintenance, I attended the Agile 2005 conference and took a tutorial by Jeff Patton. One of the exercises was building paper prototypes of the UI for a sample applica- tion. Then we “tested” the paper prototypes with members of the other groups as our users and found out how badly ﬂawed our UI designs were. Déjà vu! The tutorial resembled my reality.\n\nOn my return to the project back home, I took the project manager I was mentoring in agile development aside and suggested that paper proto- typing and “Wizard of Oz” testing (the Wizard of Oz reference is to a human being acting as a computer—sort of the “man behind the cur- tain”) might have avoided our one-month setback. After a very short dis- cussion, we decided to give it a try on our release 2 functionality. We stayed late a couple of evenings and designed the UI using screenshots from the R1 functionality overlaid with hand-drawn R2 functionality. It was a long time since either of us had used scissors and glue sticks, and it was fun!\n\nFor the Wizard of Oz testing with users, we asked our on-site customers to ﬁnd some real users with whom to do the testing. They also came up with some realistic sample tasks for the users to try to execute. We put the sample data into Excel spreadsheets and printed out various combi- nations of data grids to use the in the testing. Some future users came to town for a conference. We hijacked pairs of them for an hour each and did our testing.\n\nI acted as the “wizard,” playing the part of the computer (“it’s a 286 pro- cessor so don’t expect the response times to be very good”). The on-site customer introduced the problem and programmers acted as observers, recording the missteps the users made as “possible defects.” After just a few hours, we had huge amounts of valuable data about which parts of our UI design worked well and which parts needed rethinking. And there was little argument about which was which! We repeated the usability testing with other users when we had alpha versions of the application available and gained further valuable insights. Our business customer found the exercise so valuable that on a subsequent project the business team set about doing the paper prototyping and Wizard of Oz testing with no prompting from the development team. This might have been inﬂuenced somewhat by the ﬁrst e-mail we got from a real user 30 minutes after going live: “I love this application!!!”\n\nDeveloping user interfaces test-ﬁrst can seem like an intimidating effort. The Wizard of Oz technique can be done before writing a single line of code. The team can test user interaction with the system and gather plenty of informa- tion to understand the desired system behavior. It’s a great way to facilitate communication between the customer and development teams.\n\n139",
      "content_length": 2871,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "140\n\nCHAPTER 8\n\nLisa’s Story\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nClose, constant collaboration between the customer team and the developer team is key to obtaining examples on which to base customer tests that drive coding. Communication is a core agile value, and we talk about it more in the next section.\n\nCommunicate with Customers In an ideal world, our customers are available to us all day, every day. In real- ity, many teams have limited access to their business experts, and in many cases, the customers are in a different location or time zone. Do whatever you can to have face-to face conversations. When you can’t, conference calls, phone conversations, emails, instant messages, cameras, and other commu- nication tools will have to substitute. Fortunately, more tools to facilitate re- mote communication are available all the time. We’ve heard of teams, such as Erika Boyer’s team at iLevel by Weyerhaeuser, that use webcams that can be controlled by the folks in the remote locations. Get as close to you can to di- rect conversation.\n\nI worked on a team where the programmers were spread through three time zones and the customers were in a different one. We sent different programmers, testers, and analysts to the customer site for every iteration, so that each team member had “face time” with the customers at least every third iteration. This built trust and conﬁdence between the developer and customer teams. The rest of the time we used phone calls, open conference calls, and instant messages to ask questions. With continual ﬁne-tuning based on retrospective discussions, we suc- ceeded in satisfying and even delighting the customers.\n\nEven when customers are available and lines of communication are wide open, communication needs to be managed. We want to talk to each member of the customer team, but they all have different viewpoints. If we get several different versions of how a piece of functionality should work, we won’t know what to code. Let’s consider ways to get customers to agree on the con- ditions of satisfaction for each story.\n\nAdvance Clarity\n\nIf your customer team consists of people from different parts of the organiza- tion, there may be conﬂicting opinions among them about exactly what’s in- tended by a particular story. In Lisa’s company, business development wants\n\n—Lisa",
      "content_length": 2349,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Lisa’s Story\n\nJanet’s Story\n\nTHE REQUIREMENTS QUANDARY\n\nfeatures that generate revenue, operations wants features that cut down on phone support calls, and ﬁnance wants features that streamline accounting, cash management, and reporting. It’s amazing how many unique interpreta- tions of the same story can emerge from people who have differing viewpoints.\n\nAlthough we had a product owner when we ﬁrst implemented Scrum, we still got different directives from different customers. Management decided to appoint a vice president with extensive domain and operations knowledge as the new product owner. He is charged with getting all of the stakeholders to agree on each story’s implications up front. He and the rest of the customer team meet reg- ularly to discuss upcoming themes and stories, and to agree on priorities and con- ditions of satisfaction. He calls this “advance clarity.”\n\nA Product Owner is a role in Scrum. He’s responsible not only for achieving advance clarity but also for acting as the “customer representative” in priori- tizing stories. There’s a downside, though. When you funnel the needs of many different viewpoints through one person, something can be lost. Ide- ally, the development team should sit together with the customer team and learn how to do the customer’s work. If we understand the customer’s needs well enough to perform its daily tasks, we have a much better chance of pro- ducing software that properly supports those tasks.\n\nOur team didn’t implement the product owner role at ﬁrst and used the domain experts on the team to determine prioritization and clarity. It worked well, but the achieving consensus took many meetings because each person had different experiences. The product was better for it, but there were trade-offs. The many meetings meant the domain experts were not always available for answering ques- tions from the programmers, so coding was slower than anticipated.\n\nThere were four separate project teams working on the same product, but each one was focused on different features. After several retrospectives, and a lot of problem-solving sessions, each project team appointed a Product Owner. The number of meetings was cut down signiﬁcantly because most business decisions were made by the domain experts on their particular project. Meetings were held for all of the domain experts if there were any differences of opinion, and the Product Owner facilitated bringing consensus on an issue. Decisions were made much faster, the domain experts were more available for answering questions by the team, and were able to keep up with the acceptance tests.\n\n141\n\n—Lisa\n\n—Janet",
      "content_length": 2644,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "142\n\nCHAPTER 8\n\nChapter 9, “Tool- kit for Business- Facing Tests that Support the Team,” includes example check- lists as well as other tools for expressing requirements.\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nHowever your team chooses to bring together varying viewpoints, it is im- portant that there is only “one voice of the customer” presented to the team.\n\nWe said that product owners provide conditions of satisfaction. Let’s look more closely at what we mean.\n\nConditions of Satisfaction\n\nThere are conditions of satisfaction for the whole release as well as for each feature or story. Acceptance tests help deﬁne the story acceptance. Your devel- opment team can’t successfully deliver what the business wants unless condi- tions of satisfaction for a story are agreed to up front. The customer team needs to “speak with one voice.” If you’re getting different requirements from different stakeholders, you might need to push back and put off the story until you have a ﬁrm list of business satisfaction conditions. Ask the customer rep- resentative to provide a minimum amount of information on each story so that you can start every iteration with a productive conversation.\n\nThe best way to understand the customer team’s requirements is to talk with the customers face to face. Because everyone struggles with “requirements,” there are tools to help the customer team work through each story. Condi- tions of satisfaction should include not only the features that the story deliv- ers but also the impacts on the larger system.\n\nLisa’s product owner uses a checklist format to sort out issues such as:\n\n(cid:2) Business satisfaction conditions (cid:2) Impact on existing functions such as the website, documents,\n\ninvoices, forms, or reports\n\n(cid:2) Legal considerations (cid:2) The impact on regularly scheduled processes (cid:2) References to mock-ups for UI stories (cid:2) Help text, or who will provide it (cid:2) Test cases (cid:2) Data migration (as appropriate) (cid:2) Internal communication that needs to happen (cid:2) External communication to business partners and vendors\n\nThe product owner uses a template to put this information on the team’s wiki so that it can be used as team members learn about the stories and start writing tests.",
      "content_length": 2280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "Janet’s Story\n\nChapter 16, “Hit the Ground Run- ning,” and Chap- ter 17, “Iteration Kickoff,” give ex- amples of when and how teams can plan cus- tomer tests and explore the wider impact of each story.\n\nTHE REQUIREMENTS QUANDARY\n\nThese conditions are based on key assumptions and decisions made by the customer team for a story. They generally come out of conversations with the customer about high-level acceptance criteria for each story. Discussing con- ditions of satisfaction helps identify risky assumptions and increases the team’s conﬁdence in writing and correctly estimating all of the tasks that are needed to complete the story.\n\nRipple Effects\n\nIn agile development, we focus on one story at a time. Each story is usually a small component of the overall application, but it might have a big ripple ef- fect. A new story drops like a little stone into the application water, and we might not think about what the resulting waves might run into. It’s easy to lose track of the big picture when we’re focusing on a small number of stories in each iteration.\n\nLisa’s team ﬁnds it helpful to make a list of all of the parts of the system that might be affected by a story. The team can check each “test point” to see what requirements and test cases it might generate. A small and innocent story might have a wide-ranging impact, and each part of the application that it touches might present another level of complexity. You need to be aware of all the potential impacts of any code change. Making a list is a good place to start. In the ﬁrst few days of the iteration, the team can research and analyze affected areas further and see whether any more task cards are needed to cover them all.\n\nIn one project I was on, we used a simple spreadsheet that listed all of the high- level functionality of the application under test. During release planning, and at the start of each new iteration, we reviewed the list and thought about how the new or changing functionality would affect those areas. That became the starting point for determining what level of testing needed to be done in each functional area. This impact analysis was in addition to the actual story testing and enabled our team to see the big picture and the impact of the changes to the rest of the system.\n\nStories that look small but that impact unexpected areas of the system can come back to bite you. If your team forgets to consider all dependencies, and if the new code intersects with existing functionality, your story might take much longer than planned to ﬁnish. Make sure your story tests include the less obvious fallout from implementing the new functionality.\n\n143\n\n—Janet",
      "content_length": 2664,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "144\n\nCHAPTER 8\n\nSee Chapter 10, “Business-Facing Tests that Critique the Product,” for more about ex- ploratory testing.\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nTake time to identify the central value each story provides and ﬁgure out an incremental approach to developing it. Plan small increments of writing tests, writing code, and testing the code some more. This way, your Quadrant 2 tests ensure you’ll deliver the minimum value as planned.\n\nTHIN SLICES, SMALL CHUNKS Writing stories is a tricky business. When the development team estimates new stories, it might ﬁnd some stories too big, so it will ask the customer team to go back and break them into smaller stories. Stories can be too small as well, and might need to be combined with others or simply treated as tasks. Agile development, including testing, takes on one small chunk of functionality at a time.\n\nWhen your team embarks on a new project or theme, ask the product owner to bring all of the related stories to a brainstorming session prior to the ﬁrst iteration for that theme. Have the product owner and other interested stake- holders explain the stories. You might ﬁnd that some stories need to be sub- divided or that additional stories need to be written to ﬁll in gaps.\n\nAfter you understand what value each story should deliver and how it ﬁts in the context of the system, you can break the stories down into small, man- ageable pieces. You can write customer tests to deﬁne those small increments, while keeping in mind the impact on the larger application.\n\nA smart incremental approach to writing customer tests that guide develop- ment is to start with the “thin slice” that follows a happy path from one end to the other. Identifying a thin slice, also called a “steel thread” or “tracer bul- let,” can be done on a theme level, where it’s used to verify the overall archi- tecture. This steel thread connects all of the components together, and after it’s solid, more functionality can be added.\n\nWe ﬁnd this strategy works at the story level, too. The sooner you can build the end-to-end path, the sooner you can do meaningful testing, get feedback, start automating tests, and start exploratory testing. Begin with a thin slice of the most stripped-down functionality that can be tested. This can be thought of as the critical path. For a user interface, this might start with simply navigating from one page to the next. We can show this to the customer and see whether the ﬂow makes sense. We could write a simple automated GUI test. For the free-shipping threshold story at the beginning of this chapter, we might start by verifying the logic used to sum up the order total and determine whether it",
      "content_length": 2704,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "See Part IV, “Auto- mation,” for more about regression test automation.\n\nLisa’s Story\n\nTHIN SLICES, SMALL CHUNKS\n\nqualiﬁes for free shipping, without worrying about how it will look on the UI. We could automate tests for it with a functional test tool such as FitNesse.\n\nAfter the thin slice is working, we can write customer tests for the next chunk or layer of functionality, and write the code that makes those tests pass. Now we’ll have feedback for this small increment, too. Maybe we add the UI to display the checkout page showing that the order qualiﬁed for free shipping, or add the layer to persist updates to the database. We can add on to the auto- mated tests we wrote for the ﬁrst pass. It’s a process of “write tests—write code—run tests—learn.” If you do this, you know that all of the code your team produces satisﬁes the customer and works properly at each stage.\n\nMy team has found that we have to focus on accomplishing a simple thin slice and add to it in tiny increments. Before we did this, we tended to get stuck on one part of the story. For example, if we had a UI ﬂow that included four screens, we’d get so involved in the ﬁrst one that we might not get to the last one, and there was no working end-to-end path. By starting with an end-to-end happy path and adding functionality a step at a time, we can be sure of delivering the minimum value needed.\n\nHere’s an example of our process. The story was to add a new conditional step to the process of establishing a company’s retirement plan. This step allows users to select mutual fund portfolios, but not every user has access to this feature. The retirement plan establishment functionality is written in old, poorly designed leg- acy code. We planned to write the new page in the new architecture, but linking the new and old code together is tricky and error prone. We broke the story down into slices that might look tiny but that allowed us to manage risk and mini- mize the time needed to code and test the story. Figure 8-5 shows a diagram of incremental steps planned for this story.\n\nThe #1 thin slice is to insert a new, empty page based on a property. While it’s not much for our customers to look at, it lets us test the bridge between old and new code, and then verify that the plan establishment navigation still works properly. Slice #2 introduces some business logic: If no mutual fund portfolios are available for the company, skip to the fund selection step, which we’re not changing yet. If there are fund portfolios available, display them on the new step 3. In slice #3, we change the fund selection step, adding logic to display the funds that make up the portfolios. Slice #4 adds navigational elements between various steps in the establishment process.\n\nWe wrote customer tests to deﬁne each slice. As the programmers completed each one, we manually tested it and showed it to our customers. Any problems found were ﬁxed immediately. We wrote an automated GUI test for slice #1, and added to it as the remaining steps were ﬁnished. The story was difﬁcult because of the old legacy code interacting with the new architecture, but the stepwise approach made implementation smooth, and saved time.\n\n145",
      "content_length": 3204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "146\n\nCHAPTER 8\n\nCheck the bibliog- raphy for Gerard Meszaros’s article “Using Storyo- types to Split Bloated XP Stories.”\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nFigure 8-5 Incremental steps\n\nWhen we draw diagrams such as this to break stories into slices, we upload pho- tos of them to our team wiki so our remote team member can see them too. As each step is ﬁnished, we check it off in order to provide instant visual feedback.\n\nIf the task of writing customer tests for a story seems confusing or over- whelming, your team might need to break the story into smaller steps or chunks. Finishing stories a small step at a time helps spread out the testing effort so that it doesn’t get pushed to the end of the iteration. It also gives you a better picture of your progress and helps you know when you’re done—a subject we’ll explore in the next section.\n\nHOW DO WE KNOW WE’RE DONE? We have our business-facing tests that support the team—those tests that have been written to ensure the conditions of satisfaction have been met. They start with the happy path and show that the story meets the intended\n\n—Lisa",
      "content_length": 1125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "TESTS MITIGATE RISK\n\nneed. They cover various user scenarios and ensure that other parts of the system aren’t adversely affected. These tests have been run, and they pass (or at least they’ve identiﬁed issues to be ﬁxed).\n\nAre we done now? We could be, but we’re not sure yet. The true test is whether the software’s user can perform the action the story was supposed to provide. Activities from Quadrants 3 and 4, such as exploratory testing, us- ability testing, and performance testing will help us ﬁnd out. For now, we just need to do some customer tests to ensure that we have captured all of the requirements. The business users or product owners are the right people to determine whether every requirement has been delivered, so they’re the right people to do the exploring at this stage.\n\nWhen the tests all pass and any missed requirements have been identiﬁed, we are done for the purpose of supporting the programmers in their quest for code that does the “right thing.” It does not mean we are done testing. We’ll talk much more about that in the chapters that follow.\n\nAnother goal of customer tests is to identify high-risk areas and make sure the code is written to solidify those. Risk management is an essential practice in any software development methodology, and testers play a role in identify- ing and mitigating risks.\n\nTESTS MITIGATE RISK Customer tests are written not only to deﬁne expected behavior of the code but to manage risk. Driving development with tests doesn’t mean we’ll iden- tify every single requirement up front or be able to predict perfectly when we’re done. It does give us a chance to identify risks and mitigate them with executable test cases. Risk analysis isn’t a new technique. Agile development inherently mitigates some risks by prioritizing business value into small, tested deliverable pieces and by having customer involvement in incremental acceptance. However, we should still brainstorm potential events, the proba- bility they might occur, and the impact on the organization if they do hap- pen so that the right mitigation strategy can be employed.\n\nCoding to predeﬁned tests doesn’t work well if the tests are for improbable edge cases. While we don’t want to test only the happy path, it’s a good place to start. After the happy path is known, we can deﬁne the highest risk scenar- ios—cases that not only have a bad outcome but also have a good possibility of happening.\n\n147",
      "content_length": 2437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "148\n\nCHAPTER 8\n\nLisa’s Story\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nIn addition to asking the customer team questions such as “What’s the worst thing that could happen?,” ask the programmers questions like these: “What are the post conditions of this section of code? What should be persisted in the database? What behavior should we look for down the line?” Specify tests to cover potentially risky outcomes of an action.\n\nMy team considers worst-case scenarios in order to help us identify customer tests. For example, we planned a story to rewrite the ﬁrst step of a multistep account creation wizard with a couple of new options. We asked ourselves questions such as the following: “When the user submits that ﬁrst page, what data is inserted in the database? Are any other updates triggered? Do we need to regression test the entire account setup process? What about activities the user account might do after setup?” We might need to test the entire life cycle of the account. We don’t have time to test more than necessary, so decisions about what to test are critical. The right tests help us mitigate the risk brought by the change.\n\nProgrammers can identify fragile parts of the code. Does the story involve stitching together legacy code with a new architecture? Does the code being changed interact with another system or depend on third-party software? By discussing potential impacts and risky areas with programmers and other team members, we can plan appropriate testing activities.\n\nThere’s another risk. We might get so involved writing detailed test cases up front that the team loses the forest in the trees; that is, we can forget the big picture while we concentrate on details that might prove irrelevant.\n\nPeril: Forgetting the Big Picture\n\nIt’s easy to slip into the habit of testing only individual stories or basing your testing on what the programmer tells you about the code. If you ﬁnd yourself ﬁnding integration problems between stories late in the release or that a lot of requirements are missing after the story is “done,” take steps to mitigate this peril.\n\nAlways consider how each individual story impacts other parts of the system. Use realistic test data, use concrete examples as the basis of your tests, and have a lot of whiteboard discussions (or their virtual equivalent) in order to make sure everyone understands the story. Make sure the programmers don’t start coding before any tests are written, and use exploratory testing to ﬁnd gaps between stories.\n\nRemember the end goal and the big picture.\n\n—Lisa",
      "content_length": 2568,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "TESTABILITY AND AUTOMATION\n\nAs an agile team, we work in short iterations, so it’s important to time-box the time spent writing tests before we start. After each iteration is completed, take the time to evaluate whether more detail up front would have helped. Were there enough tests to keep the team on track? Was there a lot of wasted time because the story was misunderstood? Lisa’s team has found it best to write high-level story tests before coding, to write detailed test cases once coding starts, and then to do exploratory testing on the code as it’s delivered in order to give the team more information and help make needed adjustments.\n\nJanet worked on a project that had some very intensive calculations. The time spent creating detailed examples and tests before coding started, in or- der to ensure that the calculations were done correctly, was time well spent. Understanding the domain, and the impact of each story, is critical to assess- ing the risk and choosing the correct mitigation strategy.\n\nWhile business-facing tests can help mitigate risks, other types of tests are also critical. For example, many of the most serious issues are usually uncov- ered during manual exploratory testing. Performance, security, stability, and usability are also sources of risk. Tests to mitigate these other risks are dis- cussed in the chapters on Quadrants 3 and 4.\n\nExperiment and ﬁnd ways that your team can balance using up-front detail and keeping focused on the big picture. The beauty of short agile iterations is that you have frequent opportunities to evaluate how your process is working so that you can make continual improvements.\n\nTESTABILITY AND AUTOMATION When programmers on an agile team get ready to do test-driven develop- ment, they use the business-facing tests for the story in order to know what to code. Working from tests means that everyone thinks about the best way to design the code to make testing easier. The business-facing tests in Quad- rant 2 are expressed as automated tests. They need to be clearly understood, easy to run, and provide quick feedback; otherwise, they won’t get used.\n\nIt’s possible to write manual test scripts for the programmers to execute be- fore they check in code so that they can make sure they satisﬁed the cus- tomer’s conditions, but it’s not realistic to expect they’ll go to that much trouble for long. When meaningful business value has to be delivered every two weeks or every 30 days, information has to be direct and automatic. In- experienced agile teams might accept the need to drive coding with auto- mated tests at the developer test level more easily than at the customer test\n\n149",
      "content_length": 2667,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "150\n\nCHAPTER 8\n\nPart IV, “Test Auto- mation,” will guide you as you de- velop an automa- tion strategy.\n\n(cid:2) BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nlevel. However, without the customer tests, the programmers have a much harder time knowing what unit tests to write.\n\nEach agile team must ﬁnd a process of writing and automating business- facing tests that drive development. Teams that automate only technology- facing tests ﬁnd that they can have bug-free code that doesn’t do what the customer wants. Teams that don’t automate any tests will anchor themselves with technical debt.\n\nQuadrant 2 contains a lot of different types of tests and activities. We need the right tools to facilitate gathering, discussing, and communicating exam- ples and tests. Simple tools such as paper or a whiteboard work well for gath- ering examples if the team is co-located. More sophisticated tools help teams write business-facing tests that guide development in an executable, autom- atable format. In the next chapter, we’ll look at the kinds of tools needed to elicit examples, and to write, communicate, and execute business-facing tests that support the team.\n\nSUMMARY In this chapter, we looked at ways to support the team during the coding pro- cess with business-facing tests.\n\n(cid:2) In agile development, examples and business-facing tests, rather than traditional requirements documents, tell the team what code to write. (cid:2) Working on thin slices of functionality, in short iterations, gives cus- tomers the opportunity to see and use the application and adjust their requirements as needed.\n\n(cid:2) An important area where testers contribute is helping customers ex- press satisfaction conditions and create examples of desired, and un- desired, behavior for each story.\n\n(cid:2) Ask open-ended questions to help the customer think of all of the de- sired functionality and to prevent hiding important assumptions. (cid:2) Help the customers achieve consensus on desired behavior for stories that accommodate the various viewpoints of different parts of the business.\n\n(cid:2) Help customers develop tools (e.g., a story checklist) to express infor-\n\nmation such as business satisfaction conditions.\n\n(cid:2) The development and customer teams should think through all of the parts of the application that a given story affects, keeping the overall system functionality in mind.",
      "content_length": 2395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "SUMMARY\n\n(cid:2) Work with your team to break feature sets into small, manageable sto-\n\nries and paths within stories.\n\n(cid:2) Follow a pattern of “write test—write code—run tests—learn” in a\n\nstep-by-step manner, building on each pass through the functionality.\n\n(cid:2) Use tests and examples to mitigate risks of missing functionality or\n\nlosing sight of the big picture.\n\n(cid:2) Driving coding with business-facing tests makes the development team\n\nconstantly aware of the need to implement a testable application. (cid:2) Business-facing tests that support the team must be automated for quick and easy feedback so that teams can deliver value in short iterations.\n\n151",
      "content_length": 676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "Chapter 9\n\nTOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nBusiness-Facing Tool Strategy Introduction\n\nTest Management\n\nChecklists\n\nMind maps\n\nCode Design and Test\n\nAutomated vs. Manual Tests\n\nTestability\n\nToolkit for Business-Facing Tests that Support the Team\n\nTools to Elicit Examples and Requirements\n\nSpreadsheets\n\nMock-Ups\n\nFlow Diagrams\n\nBuild Tests Incrementally\n\nSoftware-Based Tools\n\nKeep the Tests Passing\n\nDesign Patterns\n\nStrategies for Writing Tests\n\nBelow the GUI/API\n\nKeyword and Data-Driven Tests\n\nTools for Automating Tests Based on Examples\n\nUsing the GUI\n\nHome-Brewed Frameworks\n\nIn the previous chapter, we talked about how to approach business or functional testing to support the team in its effort to build the right software. In this chap- ter, we’ll examine some of the tools you can use to help your team succeed with Quadrant 2 tests.\n\nBUSINESS-FACING TEST TOOL STRATEGY How do we capture the business-facing tests that help the programmers know what to code? Face-to-face conversations between programmers and custom- ers are usually the best way, but even when customers are part of your team, they don’t have all day to hang out with programmers and explain features. If any customer or developer team members are in different locations, im- promptu hallway conversations might not be feasible. Besides, six months\n\n153",
      "content_length": 1359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "154\n\nCHAPTER 9\n\nFor more informa- tion about a gen- eral approach to test automation, see Chapter 14, “An Agile Test Automation Strategy.”\n\nLisa’s Story\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nfrom now, we might want a way to remember why we coded a piece of func- tionality a certain way. If some of our team members are in different locations, we’re deﬁnitely going to need some way to share information electronically.\n\nAs agile development has gained in popularity, we have more and more tools to help us capture examples and use them to write executable tests. The tools available are changing too fast for us to include an inventory of them in this book, but we can offer some examples of tools and some strategies for using them to help provide business-facing tests that support the team’s develop- ment of new stories. Some of the tools we discuss here aren’t new, or speciﬁc to agile development, but they work well in an agile project.\n\nYour strategy for selecting the tools you need should be based on your team’s skill set, the technology your application uses, your team’s automation prior- ities, time and budget constraints, and other concerns unique to your situa- tion. Your selection of a tool or tools should not be based on the latest and coolest tool offered by a salesman. You might need many different tools to solve different problems.\n\nWe encourage customers to do some advance preparation and to be ready to explain examples for each story during iteration planning. Testers are in a good position to help customers ﬁgure out how to provide the right amount of detail at the beginning of the iteration. It’s hard to strike just the right balance.\n\nSoon after our team chose to use FitNesse for specifying and automating business- facing tests, our product owner and I tried to make good use of the new tool. We had an extremely complex epic coming up. We spent many hours writing detailed test cases for highly complex business rules weeks in advance of the iteration where the ﬁrst story of the epic was started. We felt good about getting a running start on developing the new functionality.\n\nWhen they started working on these stories, the programmers complained that they couldn’t get the big picture from these detailed tests. The tests were also designed in a way that was incompatible with the actual code design. I ended up spending hours refactoring them. It wasn’t a complete waste of time, because at least I under- stood the stories well and we had a number of test cases we could use eventually, but it wasn’t the right approach for our team. Trial and error has shown us that high- level tests combined with a few examples of desired and undesired behavior are the best way for the programmers to know what to start coding.\n\n—Lisa",
      "content_length": 2797,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "TOOLS TO ELICIT EXAMPLES AND REQUIREMENTS\n\nExperiment with different levels of up-front detail in test cases to ﬁgure out what works best for your team. Whatever level of detail you’re after, you need some way to help customers ﬁnd and express examples of desired system be- havior. In the next section, we look at the types of tools that can do that.\n\nTOOLS TO ELICIT EXAMPLES AND REQUIREMENTS As we pointed out in Chapter 8, stories are only a starting place for a pro- longed conversation about the desired behavior. Having correctly sized sto- ries where the feature, user, and purpose are clearly stated gives us a head start. They aren’t very detailed, because as Mike Cohn [2004] points out, it’s best to defer collecting details until the story is included in an iteration. Col- lecting details for a story that might never be included is a waste of resources. We like the “role, function, business value” pattern for user stories that Mike Cohn describes in User Stories Applied, as in:\n\nAs a (role), I want (function) so that (business value).\n\nThis format doesn’t work for everyone, so we encourage you to experiment and see what works best in your situation. Regardless of how your user sto- ries read, you need some way to ﬂesh those stories out with examples and business-facing tests that guide development.\n\nOne simple story can have a wide-ranging impact, not only on the applica- tion, but across the organization, its clients, its associates, vendors, or part- ners. If we change an API, we have to notify any customers or vendors who might be using it. If we plan a UI change, we want, or might even be contrac- tually obligated, to give a certain amount of advance notice to users. Stories may affect legal concerns or impact external reporting. New features often mean new or updated documentation. Of course, changed functionality is likely to affect other parts of the system.\n\nThe software development team, including the testers, should help the cus- tomer capture and communicate all of the requirements related to each story or theme. Developing new features, only to be prevented from releasing them for legal reasons or because a business partner wasn’t informed in time, is a frustrating waste of time (just ask Lisa!). Lean development teaches us to avoid waste while we develop software.\n\n155",
      "content_length": 2325,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "156\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nWhat tools can help us illustrate desired behavior with examples, brainstorm potential implementations and ripple effects, and create requirements we can turn into tests? Some examples are:\n\n(cid:2) Checklists (cid:2) Mind maps (cid:2) Spreadsheets (cid:2) Mock-ups (cid:2) Flow diagrams (cid:2) Software-based tools\n\nThe list includes a number of simple tools that aren’t unique to agile testing but that shouldn’t be neglected. In agile development, simple solutions are usually best. Let’s look at these in more detail.\n\nChecklists\n\nChecklists are one way for product owners to make sure they correctly assess and communicate all of the aspects of a story. The product owner for Lisa’s team, Steve Perkins, came up with his own “story checklist” to make sure he and the stakeholders think through everything affected by the story. He cre- ated a template on the team wiki for this purpose. The checklist speciﬁes the conditions of satisfaction—what the business needs from the story. It also in- cludes impacts on existing functions such as the website, documents, admin- istrative forms, account statements, and other components of the system and the daily operation of the business. The checklist makes sure the team doesn’t miss requirements such as data migration, notiﬁcations, legal considerations, and communications to vendors and business partners because they forgot to consider them. Figure 9-1 shows a sample story checklist.\n\nMind Maps\n\nMind maps are a simple but effective way to search out ideas that that might not occur to you in a simple brainstorming session. Mind maps are diagrams created to represent concepts, words, or ideas linked to a central key concept. We used mind maps to organize this book.\n\nIt really doesn’t matter whether you purchase a tool such as the one we used or draw on a whiteboard or a big piece of paper. The effect is the same. Mind maps enable you to generate ideas and work in a way that is consistent with the way you think about problems.",
      "content_length": 2072,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "Figure 9-1 Sample story checklist\n\nTOOLS TO ELICIT EXAMPLES AND REQUIREMENTS\n\n157",
      "content_length": 81,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "158\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nHow about an example? We’re discussing the story shown in Figure 9-2.\n\nStory PA-3\n\nAs a shopper on our site, I want to delete items\n\nout of my shopping cart so I don’t purchase extra\n\nitems that I decide I don't want.\n\nFigure 9-2 Shopping cart delete story\n\nWe gather around the whiteboard and start asking questions. Where should the deleted items go? Should they be saved for later approval, or should they just disappear? What should the screen look like after we delete an item? Figure 9-3 shows an example of the sort of mind map we might draw on a whiteboard.\n\nFigure 9-3 Example mind map for shopping cart delete story",
      "content_length": 706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "TOOLS TO ELICIT EXAMPLES AND REQUIREMENTS\n\nSpreadsheets\n\nWhen possible, tools for specifying business-facing tests should ﬁt well with your business domain. For example, spreadsheets are widely used by ﬁnan- cial services companies, so for a project in the ﬁnancial services area it makes sense to use spreadsheets to deﬁne examples of the functionality that a story should deliver.\n\nCustomers can write a few high-level test cases to help round out a story prior to the start of the iteration, possibly using some type of checklist. Some cus- tomer teams simply write a couple of tests, maybe a happy path and a negative test, on the back of each story card. Some write more detailed examples in spreadsheets or whatever format they’re comfortable working with.\n\nSteve Perkins, the product owner for Lisa’s team, often illustrates complex calculations and algorithms in spreadsheets, which the team can turn into tests later. Figure 9-4 shows one of his worksheets, which performs calcula- tions on the input values to produce the values in the ADR and ACR col- umns. This format is easy to get into an automated test framework (refer to Figure 9-8 for the corresponding FitNesse example).\n\nLook at tools already used by your business experts and see whether they can be adapted to document examples of desired feature behavior to help the de- velopment team better understand the story.\n\nJanet has worked with several teams that have used spreadsheets as input into their Fit tests. This allows customers to work in a tool that is familiar to them but not waste any effort in translating them to an automation tool.\n\nFigure 9-4 Spreadsheet example from product owner\n\n159",
      "content_length": 1673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "160\n\nCHAPTER 9\n\nSee Chapter 8 for Gerard Meszaros’ description of using paper proto- types and Wizard of Oz testing.\n\nLisa’s Story\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nMock-Ups\n\nMock-ups can take many forms. Paper prototypes are a simple but effective way to test how screens will work together. Drawing on a whiteboard can ac- complish the same goal, but it can’t be passed around. Screenshots from ex- isting applications can form the basis of a discussion about how to add a new feature and where it will ﬁt into the UI. You may have used tools like these in other development methodologies. The big difference in agile development is that we create and discuss the mock-ups just as we’re about to start writing the code, rather than weeks or months beforehand. We can be conﬁdent that the mock-up represents what the customers want right now.\n\nWe use simple approaches to creating mock-ups so that we aren’t tempted to in- vest time coding before we’re ﬁnished working through the mock-up. Often, we draw a UI or workﬂow on the whiteboard and then take photos of it to upload to our team wiki so our remote team member can also see it. At other times, a cus- tomer or our product owner draws the mock-up on paper or modiﬁes an existing UI page or report to show what should be added and changed. The paper mock- ups are scanned in and posted on the wiki.\n\nA picture’s worth a thousand words, even in agile software development. Mock- ups show the customer’s desires more clearly than a narrative possibly could. They provide a good focal point for discussing desired code behavior.\n\nFigure 9-5 shows an example of a mock-up that Lisa’s team used to mock up a new report—simply by marking up an existing report that’s similar.\n\nMock-ups don’t need to be fancy or pretty, or to take a lot of time to create. They do need to be understandable to both the customer and developer teams.\n\nFlow Diagrams\n\nSimple diagramming tools are helpful, whether the team is co-located or not. It’s often a good idea to capture in a more permanent form a workﬂow or de- cision tree worked out during a discussion. Flow diagrams can become the basis of a user scenario that might help you tie two or three user stories to- gether. Let’s look at the shipping order story again that we introduced in Chapter 8 (see Figure 9-6).\n\n—Lisa",
      "content_length": 2345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "TOOLS TO ELICIT EXAMPLES AND REQUIREMENTS\n\nFigure 9-5 Sample report mock-up\n\nStory PA-1\n\nAs an Internet shopper on LotsO'Stuff.xx,\n\nI want free shipping when my order exceeds the free\n\nshipping threshold, so that I can take advantage\n\nof ordering more at one time.\n\nFigure 9-6 Story for shipping charges\n\n161",
      "content_length": 308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "162\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nFigure 9-7 shows a very simple ﬂowchart of a decision process for whether a customer’s order is eligible for free shipping based on a threshold order amount. Because we’ve discussed this story with our customer, we’ve found out that the customer’s order must not only exceed a threshold dollar amount but also must be to one address only, and it must weigh less than a shipping weight threshold. If all of these conditions are satisﬁed, the cus- tomer’s order will ship free; otherwise, the customer will have to select from the “choose shipping options” page.\n\nCheck-Out\n\nOrder Total > Free Shipping Threshold?\n\nYes\n\nNo\n\nShip to One Address?\n\nNo\n\nShipping Options Page\n\nNo\n\nYes\n\nOrder Weight < Max Shipping Threshold?\n\nYes\n\nCheck-Out\n\nFigure 9-7 Flow chart for qualifying for free shipping option",
      "content_length": 878,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "TOOLS TO ELICIT EXAMPLES AND REQUIREMENTS\n\nVisuals such as ﬂow diagrams and mind maps are good ways to describe an overview of a story’s functionality, especially if they’re drawn by a group of customers, programmers, and testers. In agile development, we create these diagrams as we’re about to start writing tests and code. From these, the team can immediately start digging down to the detailed requirements.\n\nSoftware-Based Tools\n\nIf we’re in a different location than our customers, we need tools to help us converse with them. Distributed teams tell us that desktop sharing is the num- ber one tool that helps them deal with working in separate locations. Windows NetMeeting and VNC are examples of tools that let two team members in dif- ferent locations pair-test. Video conferencing tools such as WebEx and Skype enable collaboration and demos between remote teams and customers. Online whiteboards such as Scriblink and interactive whiteboard tools such as Mimeo facilitate distributed whiteboard discussions.\n\nMore tools that are geared for direct use by product owners and business ex- perts are becoming available, and many teams develop their own. Tools such as Fit (Framework for Integrated Tests) and FitNesse were designed to facili- tate collaboration and communication between the customer and develop- ment teams. We’re hearing about more teams where the customers actually write the tests in a tool such as those.\n\nNotes from a Distributed Team\n\nPierre Veragen and Erika Boyer of iLevel by Weyerhaeuser told us that every iteration begins with everyone on the team writing acceptance tests. That’s how they start their iteration planning. Most interesting is the fact that their product owners, who are mechanical engineers, write FitNesse tests them- selves. Pierre explains that an advantage of a tool such as FitNesse is the abil- ity to use their own domain language in the FitNesse tests. It doesn’t matter what they end up choosing as a UI. They can test all of their complex calcula- tions in the tests.\n\nWith this process, tests can be written before writing the testing code or the system under test. It’s true test-driven development. Behavior changes and bug ﬁxes can follow.\n\nSome teams build their own frameworks that allow customers, business analysts, and testers to document examples that can be directly turned into executable tests. These are often based on open source tools such as xUnit, Fit, Selenium, and Watir. We like this approach, because it saves time and\n\n163",
      "content_length": 2509,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "164\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nresources. When you’re delivering production-ready code in short itera- tions, you need a streamlined process.\n\nOnline forum tools are a good alternative to email conversations for ongoing discussions about features or technical concerns, especially for teams that don’t all sit together. Emails often get missed or lost, people have to remem- ber to choose “Reply all,” and it can be hard to put together the details of the discussion later. Lisa’s team uses an online forum to elicit opinions about dif- ferent tools, propose different behavior for features, and conduct philosoph- ical discussions such as whether to track defects.\n\nFinding the right electronic tools is particularly vital for distributed teams. Instant messaging, the telephone, VoIP, and Skype help us communicate, but they lack the visual component. Some global teams ask their members to meet at nonstandard hours so that they can have real-time conversations, but frameworks for written and visual communication are still critical.\n\nWikis are a common tool used to enhance communication and record dis- cussions and decisions. Wikis enable users to edit web page content in a web browser. Users can add hyperlinks and easily create new pages. You can up- load mock-ups, samples, and pictures of whiteboard drawings and make them easily visible on Wiki pages. The hierarchical organization can get tricky to maintain, but there are lots of open source and vendor wiki software packages available that make managing your knowledgebase and sharing in- formation easier to administer. If your wiki knowledgebase has grown to the point where it’s hard to ﬁnd anything, hire a technical writer to transform it into organized, usable documentation.\n\nOpen source and commercial tools provide ways to let teams collaborate on re- quirements and test cases online. We can’t emphasize enough the need for you to identify tools that might be helpful, to experiment with them for a few itera- tions, and to decide how well they work for you. Your team’s needs will change with time, so always be open to trying new techniques and frameworks.\n\nThese tools help create the conversation about the story. With these tech- niques, and as much real-time conversation and visual sharing as we can manage, we can deﬁne the right product from the get-go.\n\nTOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES What about test tools? We like the collaboration inherent with tools such as Fit and FitNesse. However, in our opinion, any tool that gets testers and pro-",
      "content_length": 2590,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "TOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES\n\ngrammers, programmers and customers, and testers and customers talking is a great one. We know teams where customers actually write tests in Fit, Fit- Nesse, Expect, or other tools. This works when the tool has been set up in a manner that’s clear to everyone writing tests, with the domain language easy to understand and the appropriate ﬁxtures provided.\n\nTools to Test below the GUI and API Level\n\nThere are a multitude of open source tools that enable you to test below the GUI or at the API layer. We are listing just a few, but your team will need to determine the right tool for you.\n\nUnit-Level Test Tools Some teams use the xUnit tools such as JUnit or NUnit for business-facing tests as well as technology-facing tests. If the testers and customers are com- fortable with these tools, and they provide for all of the functional testing be- hind the GUI needed, they’re ﬁne. To make these tools more customer- friendly, teams might build a framework on top of the unit-level tools that testers and customers can use to specify tests.\n\nJanet has worked on a couple of applications like that. One was a message handling system that was being deployed in an organization. The program- mers used JUnit for all of the component and integration testing. They built a load test framework that could make use of the JUnit tests, so no other test- ing tools were needed. The GUI front end was so small that Janet was able to test it manually. It made no sense to automate the GUI testing in this case.\n\nBehavior-driven development (BDD) tools are also suited to this purpose, because they use a more natural language for specifying the tests. Behavior- driven development is a variation of test-driven development, pioneered by Dan North [2006], and evolved by many others. It’s related to domain-driven design, with a focus on the domain rather than on the technology, and driv- ing design with a model. Instead of the word “test” or “assert,” BDD uses the word “should.” By thinking in terms of behavior, it’s natural to write speciﬁ- cations ahead of code. Test speciﬁcations use a domain-speciﬁc language to provide tests that customers can read but that can also be easily automated.\n\nSome of the many BDD tools available as of this writing include easyb and JBehave for the Java platform, NBehave and NSpec for .NET, and RSpec for Ruby. These tools, like the XUnit tools, are intended for use by programmers to guide coding, but they can also be used to express business-facing tests that drive development, involving customers more closely in the development process.\n\n165",
      "content_length": 2625,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "166\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nBehavior-Driven Development\n\nAndrew Glover, president of Stelligent Incorporated and author of books including Continuous Integration and Java Testing Patterns, explains the think- ing behind one of the BDD tools, easyb.\n\nassertEquals(42.50, order.price(), 0.0). Without examining the context in which this statement appears, this code is somewhat incom- prehensible. Now imagine you don’t even read code—that is, you are a stakeholder asking (actually paying) for new features. The previous code statement might as well be Farsi (assuming you can’t actually read Farsi!).\n\norder.price().shouldBe 42.50. While the context in which this state- ment appears is still absent, this line of code is a bit more coherent. In fact, it reads like a normal sentence (and this time knowledge of Farsi isn’t required!). Stakeholders, in this case, could understand this code if they chose to read it; on top of that, it turns out that this line of code essentially matches what they asked for in the ﬁrst place. This line of code describes behavior in a more literal manner too—the code uses a normal everyday phrase like shouldBe, which is distinctly different than the previously written assertEquals.\n\nBoth lines of code from the previous paragraphs convey the same mean- ing and indeed validate the same requirement, yet the latter one comes awfully close to leveraging the customer’s language. This is a fundamental point of the notion of behavior-driven development,which strives to more appropriately validate a software system by thinking in terms of the term “should” rather than test. In fact, by focusing on behavior and closely modeling behavior after what stakeholders ask for, behavior- driven development converges on the idea of executable documenta- tion. Indeed, through leveraging a stakeholder’s language, there is a de- creased impedance mismatch between what he wants and what he ultimately receives; moreover, employing a stakeholder’s language facili- tates a deeper level of collaboration between all parties. Listen to how a conversation might go:\n\nStakeholder: For the next release of our online store, our Gold- level customers should receive a discount when they make a pur- chase.\n\nDeveloper: What kind of discount—what criteria do they have to meet in order to receive it?\n\nStakeholder: When they have at least $50 dollars in their shopping cart.\n\nDeveloper: Does the discount increase based upon the amount, or is it ﬁxed regardless of the value of the shopping cart?\n\nStakeholder: Good question—the discount is ﬁxed at 15% regardless of price. So, given a Gold-level customer, when the shopping cart totals $50 or more, it should receive a 15% discount off the total price.",
      "content_length": 2775,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "TOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES\n\nThe last statement of the stakeholder is key—note how the requirement has been speciﬁed and the means for validating it. In fact, the stake- holder has essentially narrated a speciﬁc scenario in a larger story related to discounts.\n\nGiven this scenario, a developer can take the stakeholder’s comments— word for word—and execute them. For example, one behavior-driven development framework, dubbed easyb, facilitates system validation through a domain-speciﬁc language that supports stories and scenarios. For example:\n\nscenario “Gold-level customer with $50 in shopping cart”, { given “ a Gold-level customer” when “their shopping cart totals $50 or more” then “ they should receive a 15% discount off the total price” }\n\nOf course, this particular scenario doesn’t actually do anything (other than capturing the stakeholder’s requirements, which is still quite im- portant!); consequently, it is considered pending. This status alone conveys valuable information—stakeholders can, ﬁrst and foremost, see their words as a means to validate their requests, and secondly, gauge if their requirement has been fulﬁlled. After this scenario has been implemented, it can, of course, take on two other states—suc- cess or failure, both of which serve to convey further status information to interested parties.\n\nNow, with a collaborative scenario deﬁned, development can proceed to the implementation—the beauty in this case is that they can directly implement the desired behavior inline with the requirements, like this:\n\nscenario \"Gold-level customer with $50 in shopping cart\", { given \"a Gold-level customer\", { customer = new GoldCustomer() } when \"their shopping cart totals $50 or more\", { customer.shoppingCart << new Item(\"widget\", 50.00) } then \"they should receive a 15% discount off the total price\" , { customer.orderPrice.shouldBe 42.50 } }\n\nThis scenario is now executable within the context of the application it serves to validate! The scenario leverages the customer’s exact words, too; what’s more, regardless of the customer’s ability to read code, the code it- self leverages natural language: customer.orderPrice.shouldBe 42.50.\n\n167",
      "content_length": 2196,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "168\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nBy leveraging the customer’s language, the customer has the ability to collaboratively facilitate in validating the system he or she wants built. Also, with development leveraging the stakeholders’ language, there is a direct link between what stakeholders ask for and what they receive. And you don’t even need to understand Farsi to see the beneﬁt in that.\n\nTwo of the most common questions we’re asked by new agile teams are, “What about documentation?” and “How can test automation keep up with development in two-week iterations?” Tools such as easyb answer that ques- tion with executable documentation using a domain-speciﬁc language that everyone on both the customer and developer teams understands.\n\nThe goal of business-facing tests that support the team is to promote com- munication and collaboration between customers and developers, and to en- able teams to deliver real value in each iteration. Some teams do this best with unit-level tools, and others adapt better to functional-level test tools.\n\nAPI-Layer Functional Test Tools Before Lisa joined her ﬁrst agile team, testing “behind the GUI” was a con- cept that sounded good, but she’d never had the opportunity to try it. Fit, and FitNesse, which is built on top of Fit, are functional test tools that grew from the need for the customer team to be able to write and understand the business-facing tests that drive development. With these tools, teams can test business logic without involving the presentation layer.\n\nFit and FitNesse. Fit (Framework for Integrated Tests) is an open source testing framework that promotes collaboration, which makes it a good tool to help reﬁne requirements. The invention of Ward Cunningham, Fit has en- joyed an illustrious roster of contributing developers. Fit enables customers, testers, and programmers to use examples to specify what they expect the system to do. When the tests run, Fit automatically compares customers’ ex- pectations to actual results.\n\nWith Fit, customers can provide guidance using their subject matter exper- tise to deﬁne the examples that the programmers can code against. The pro- grammers participate by writing the ﬁxtures that do the actual checks against the examples. These ﬁxtures use the data speciﬁed in the examples to run with the actual program.\n\nFit tests are automated by ﬁxtures that pass the test inputs to the production code and then accept the outputs, which it then compares with expected re- sults. The test results are color-coded, so it’s easy to spot a failure or exception.",
      "content_length": 2618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "TOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES\n\nLearn more about Fit at ﬁt.c2.com.\n\nFit tests are written as HTML tables, but teams can customize Fit so that tests can be written in spreadsheets or whatever form the customers, testers, and analysts ﬁnd usable.\n\nLearn more about FitNesse at www.ﬁtnesse.org.\n\nFitNesse is a web server, a wiki, and a software testing tool that is based on Fit. Originally developed by Robert C. “Uncle Bob” Martin and Micah Mar- tin, it’s an open source tool with an active developer community. The main difference between FitNesse and Fit is that FitNesse tests are written in wiki markup instead of HTML tables, which some users ﬁnd easier. It also sup- ports creating tests in spreadsheets and importing those into the tests.\n\nFigure 9-8 shows part of the FitNesse test that was built from the example in Figure 9-4. More inputs were added to make the production code run, but the essential test data is from the spreadsheet. The test results are color-coded green when they pass, red when they fail.\n\nAnother beneﬁt of a Fit or FitNesse type of tool is that it promotes collabora- tion among different team members in order to come up with the right tests\n\nFigure 9-8 Automated FitNesse test from customer example\n\n169",
      "content_length": 1252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "170\n\nCHAPTER 9\n\nSee the ”System Test” example in Chapter 12, “Summary of Test- ing Quadrants,” to see how Janet’s team used Ruby Test::Unit to test web services.\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nto guide development. Customers, programmers, testers, and others work to- gether to specify and automate the tests.\n\nTesting Web Services. Web services is just another form of an API that en- ables other applications to access your application. Let’s talk about some of the tools you can use to test various inputs into your system.\n\nCrossCheck. CrossCheck is one example of a tool for testing web ser- vices. You supply the WSDL (Web Services Description Language); CrossCheck compiles the page and then presents you with a tabbed menu that contains textboxes for you to ﬁll in. It has a Run mode where you can add your tests to a suite and then run the suite. Neither Lisa or Janet have tried this tool, but it was noted on the Yahoo agile-testing group as a tool to use for testing web services if you were running the same data through each time.\n\nRuby Test::Unit. One project Janet was on used Ruby’s unit testing framework, Test::Unit, to test web services, with great success. In fact, the team was able to test early to give the programmers immediate feedback, which helped with the ﬁnal design.\n\nsoapUI. Another tool suggested for testing web services is soapUI. It has a steep learning curve but can be used for performance and load testing. Because it can loop though rows in an Excel spreadsheet or text ﬁle, it can be used for data-driven testing.\n\nTests that work at the layers below the presentation layer are well suited for writing and automating customer tests that guide coding. Some practitioners haven’t gotten the value they expected from story test-driven development. Brian Marick [2008] hypothesized that an application built with program- mer test-driven development, example-heavy business-facing design that re- lies heavily on whiteboard discussions, a small set of automated sanity tests, and lots of exploratory testing could be a less expensive and equally effective approach. Whichever approach you take, if you’re testing an application with a user interface, you’ll need some automation at the GUI level.\n\nTools for Testing through the GUI\n\nWait a minute. How can we use GUI tests to drive development, because the GUI won’t be ready until the story is complete? It sound counterintuitive, but automated GUI tests are important to help us while we’re developing new functionality. Test frameworks can be used to specify test cases for a GUI tool before the code is written. In addition, you can automate GUI tests be-",
      "content_length": 2681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "TOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES\n\nA Tool Selection Rationale\n\nDavid Reed, a test automation engineer, and his team went with soapUI Pro to automate testing for their web services. Here are some reasons he gave for choosing this particular tool.\n\nIt has an open source version, so you can try it out it for free. You can learn it, kick the tires, expand stuff, and learn its strengths and weaknesses.\n\nIt was easy to ﬁgure out what requests to make for what service.\n\nThe assertions provided for verifying the results from requests are great and expandable. One really helpful one is verifying that the response comes back in an acceptable amount of time, raising an error if it doesn’t.\n\nThe Pro version takes a lot of the hassle out of designing XPath queries to verify results. It also adds some nice touches for retrieving database data.\n\nIt’s expandable with Groovy, a Java-based scripting language. (They’re working on a Java application, so it pays to have Java-friendly tools.)\n\nDevelopers can use it without sneering at it as a “test tool.”\n\nIt’s easily integrated with our continuous integration environment.\n\nIt has a feature to check code coverage.\n\nThe price is right.\n\nfore coding is ﬁnished, either by using HTML mock-ups or by developing an end-to-end bare-bones slice through all of the screens that simply navi- gates but doesn’t provide all of the functionality yet. Even if you’re not using a lot of automated story tests to drive development, manual exploratory testing that helps us learn about the functionality and provides immediate feedback gets pretty tedious and slow without any assistance from automa- tion. Let’s look at the types of GUI test tools that help drive development us- ing business-facing tests.\n\nRecord/Playback Tools Record/playback tools are appealing because you can usually learn how to record a script and play it back quickly, and you can create lots of scripts in a short time. However, they have drawbacks. Early GUI test tools recorded mouse movements using X-Y screen coordinates. Scripts using those tools might also be sensitive to changes in screen resolution, color depth, and even where the window is placed on the screen.\n\n171",
      "content_length": 2197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "172\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nMost modern GUI test tools use objects to recognize the controls in a graph- ical application, like buttons, menus, and text input widgets, so they can refer to them symbolically rather than with raw screen coordinates. This makes the application much more testable, because it’s more robust standing up to changes. A button might move to a different part of the screen, but the test can still ﬁnd it based on its object name.\n\nEven with improved object recognition, scripts created with record/playback are usually brittle and expensive to maintain. Recording can be a good way to start creating a script. Testers or programmers who know the tool’s scripting language can refactor the recorded script into an object-oriented model that’s easier to use and maintain. Historically, record/playback tools used propri- etary scripting languages, which programmers aren’t interested in learning. It’s also more difﬁcult to change the design patterns used in the tests.\n\nSome script-based tools such as the ones we’ll talk about in the next few sec- tions offer a record feature to help people get a quick start on writing the test script. However, with those tools, the recorded scripts aren’t intended for straight playback; they’re just a starting point to creating a well-designed and easily maintained suite of tests.\n\nMany agile teams prefer tools and scripting languages that let them create their own domain-speciﬁc language (DSL). This makes tests much easier for busi- ness experts to understand and even write. Let’s look at some of these next.\n\nAgile Open Source Test Tools Each of the tools in this section was originally written by an agile develop- ment team that needed a GUI test tool and couldn’t ﬁnd any third-party tools that worked for its situation. With these tools, you can write scripts that use web applications just like a human user. They ﬁll in text ﬁelds, select from lists, and click checkboxes and buttons. They provide a variety of ways to verify correct navigation and contents of pages, such as tool-speciﬁc verify steps or XPath. Some of these tools have a higher learning curve than simple record/playback tools, but the extra investment of time usually pays off in scripts with a low total cost of ownership.\n\nRuby with Watir. Watir (Web Application Testing in Ruby) is a simple open source Ruby library for automating web browsers that works with Internet Explorer on Windows. There are different ﬂavors for other browsers, includ- ing FireWatir for Firefox and SafariWatir for Safari.",
      "content_length": 2600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "Janet’s Story\n\nTOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES\n\nI worked on a project that developed a three-layer test framework using Ruby and Watir. The ﬁrst layer was a common set of libraries, and the second layer was to ac- cess the pages and provide navigation. The third and top layer created a domain language using ﬁxture-type methods that mapped to the business needs. This al- lowed the manual testers to write high-level automated tests for workﬂows before coding was completed. If a ﬁxture didn’t exist because of new functionality, the test could be created and the action word for the missing ﬁxture could be “dummied” in. As soon as the ﬁxture was coded, the test could be run as an acceptance test.\n\nA very simple example of using Ruby with Watir incorporates the idea of DSL. Methods were created to simplify the tests so that any of the testers could actually create an automated script without knowing any Ruby or Watir.\n\nThis next example shows a test, and then two of the methods used in the test.\n\ndef test_create_new_user\n\nlogin 'administrator','admin' navigate_to_tab 'Manage Users' click_button \"Create New User\" set_text_field \"userFirstNameInput\", \"Ruby\" set_text_field \"userLastNameInput\", \"RubyTester\" click_button \"Save Changes\" verify_text “Saved changes” end\n\n# methods created to support easier test writing def navigate_to_tab(menuItemName) @browser.link(:text,menuItemName).click end\n\ndef set_text_field(id, value) @browser.text_field(:id,id).set value end\n\nA third level could easily be added if create_new_user was called more than once. Just extract the common code that the test could call:\n\ncreate_new_user (Ruby, RubyTester)\n\nThese tests were well suited to guiding development and providing quick feed- back. Making tests easy for testers and customers to write, while keeping the au- tomation framework designed for optimum maintainability, reduced the total cost of ownership of the tests.\n\nThere are always drawbacks to any tool you use. For example, there are limi- tations to using objects. Sometimes programmers use custom controls or a new toolkit that your tool might not understand.\n\n173\n\n—Janet",
      "content_length": 2145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "174\n\nCHAPTER 9\n\nJanet’s Story\n\nSee Chapter 14, “An Agile Test Automation Strategy,” for an example of using Selenium RC to create a domain- speciﬁc test auto- mation framework.\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nI started a new job as QA manager, and after much deliberation we decided to drop the vendor tool that the team had been using for a couple of years. We could not ﬁgure out what tests were actually being run, or what the real coverage was. We decided to start automating the tests using Ruby and Watir. The automa- tion went fairly quickly at ﬁrst, but then the tests started failing. We spent a lot of time changing the tests to reﬂect new object names. The developers were just us- ing the default WebLogic object names, which would change every time a new object was added to the page. The testers went to the developers to ask if they could change the way they were coding. It took a little convincing, but when the developers realized the problems their practice was causing, they changed their habits. Over time, all of the defaults were changed, and each object had an as- signed name. The tests became much more robust, and we spent much less time in maintenance mode.\n\nImplementing a new test automation tool usually requires some experimen- tation to get a good balance of testable code and well-designed test scripts. Involving the whole team makes this much easier. Watir is one example of a GUI test tool that we’ve found is well suited to agile projects. Let’s look at a couple more, Selenium and Canoo WebTest.\n\nSelenium. Selenium is another open source tool, actually a suite of tools, for testing web applications. The tests can be written as HTML tables or coded in a number of popular programming languages, and can be run directly in most modern web browsers. A Firefox plug-in called “Selenium IDE” provides a way to learn the tool quickly. A recorder is provided to help create the tests, includ- ing writing assertions. Tests can be written in several different common pro- gramming and scripting languages, including Java, C#, and Ruby.\n\nCanoo WebTest. In WebTest scripts, tests are speciﬁed as “steps” in XML ﬁles, simulating a user’s actions through a web UI. Here’s an example of how a WebTest script might invoke a page and verify the results:\n\n<setInputField description=\"set query\" name=\"q\" value=\"Agile Tester\"/> <clickButton description=\"submit query\" label=\"Google Search\"/> <verifyText description=\"check for result\" text=\"Lisa Crispin\" /> <verifyText description=\"check for result\" text=\"Janet Gregory\" />\n\nRather than driving an actual browser, as Selenium and Watir do, WebTest simulates the desired browser using HtmlUnit. The advantage of specifying tests as opposed to coding test scripts, is because there’s no logic in them, you don’t have to test the test.\n\n—Janet",
      "content_length": 2850,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Lisa’s Story\n\nTOOLS FOR AUTOMATING TESTS BASED ON EXAMPLES\n\nMy team chose WebTest to automate smoke tests for our legacy application for several reasons. Because the scripts are written in XML, the programmers on the team were comfortable using the tool. It uses Ant to run the tests, so integrating it into the continuous build process was simple. It’s easy to learn, and the tests can be designed in a modular fashion, so they’re fairly easy to maintain. WebTest sup- ports testing PDF ﬁles, emails, and Excel ﬁles, all of which are widely used in our application.\n\nBeing accustomed to powerful commercial test tools, I was skeptical of the con- cept of specifying tests, as opposed to programming them. I was amazed at how effective the simple tests were at catching regression bugs. It’s possible to put logic into the tests using Groovy or other scripting languages, but we’ve only found the need in a few cases.\n\nWriting a few tests per iteration, I automated smoke tests for all of the critical areas of our application in eight months. These simple tests ﬁnd regression bugs regu- larly. We refactor the tests frequently, so they are relatively easy to maintain. Our ROI on these tests has been tremendous.\n\nSelenium, WebTest, and Watir are just three examples of the many open source tools available for GUI testing as of the time we wrote this book. Many teams write their own test automation frameworks. Let’s look at an ex- ample in the next section.\n\n“Home-Brewed” Test Automation Tools Bret Pettichord [2004] coined the term “home-brewed” for the tools agile teams create to meet their own unique testing needs. This allows even more customization than an open source tool. The goal of these tools is usually to provide a way for nontechnical customer team members and testers to write tests that are actually executable by the automated tool. Home-brewed tools are tailored to the exact needs of the project. They can be designed to mini- mize the total cost of ownership. They’re often built on top of existing open source tools.\n\nJanet has been involved in a few projects that have used Ruby and Watir to create a full framework for functional testing. These frameworks allowed cus- tomers to specify tests that were then turned into a functional regression suite.\n\nNo test tool guarantees success. In fact, the history of test automation is lit- tered with failed attempts. Having the whole team think about the best tools to use is a big help, but no matter what tool you use, you need a smart ap- proach to writing tests. We’ll discuss that in the next section.\n\n175\n\n—Lisa",
      "content_length": 2593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "176\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nPAS Functional Testing\n\nThis next story is about one project Janet worked on that enjoyed success with home-brewed test automation.\n\nPAS is a production accounting application for the oil and gas industry. Using gross meter readings and contract agreements, it must calculate ownership of the various products down to a very precise level (i.e., the components in gas). There are literally thousands of interactions between the combinations available in conﬁguring the system and the actual outputs visible to a user. Given the large number of interactions, PAS has employed many complemen- tary strategies for testing.\n\nJoseph King, one of the initial programmers and agile coach for the team, tells us the story of how they accomplished their functional testing.\n\nAt the lowest level, there are developer functional tests that exercise speciﬁc functions via an API and verify the results using another read-only user API. There are currently over 24,000 tests implemented in JUnit that every developer must run before they can ”check in” their changes to the source code.\n\nThe next level is a set of GUI tests that test the marshalling of user data back and forth to the API, particularly around ”master-data” creation and updates. There are currently over 500 of these tests implemented using Watij (an open source library similar to Watir but using Java) and JUnit that run multiple times a day.\n\nThe ﬁnal level of testing is a set of integration tests created by the users that run in a Fit-like harness. Users identify dense test cases that reﬂect real-world cases covering many of the functions that work together to produce ﬁnancial and regulatory outputs. These test cases are then tran- scribed into import templates and then processed using a domain lan- guage that mirrors the way end customers think about their processes.\n\nFor example, after an end customer has created the conﬁguration of fa- cilities and contracts they wish to exercise in their test, they work with a developer to use the domain language to process their facilities in the correct order. The end users also supply a set of expected outputs that are then veriﬁed using a read-only API. These outputs can contain thousands of numbers, any of which can change for seemingly minor reasons in an evolving product. It is a constant challenge to sort through what is a legitimate business change from what is a defect. There are currently over 400 integration tests, and they run twice per day, providing feedback to the end customers and developers.\n\nExploratory testing is done continuously throughout the development cycle and is augmented at the end of releases.\n\nOur ﬁrst attempt at PASFIT (which is what we called the functional test framework) was a spreadsheet of color-coded inputs and outputs. We",
      "content_length": 2864,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "STRATEGIES FOR WRITING TESTS\n\nthen generated Java code based on the color of the cells to create the data in PAS. That proved difﬁcult to maintain, partly because the applica- tion was in major ﬂux both at the GUI and database level.\n\nOur next iteration of PASFIT didn’t evolve for nearly a year after the previ- ous attempt. After we had a more stable set of database views and GUI, we were able to create an engine that used simple imperative language (i.e., a script) to do actions with arguments against a GUI (e.g., Go to Bal- ancing Page, Balance Battery: Oil, Water). The script evolved into following the thought process of a production accountant and became a domain- speciﬁc language. The engine was written using Ruby and Watir, and an instruction from the script was basically a Ruby method that was invoked dynamically so that it was easy to update. After the script ran, the frame- work then loaded a snapshot of the views that the test wished to com- pare and did a simple row-by-row, cell-by-cell comparison of what was to be asserted and what actually happened. Eventually this was enhanced in the spreadsheet to use Pivot tables to enable the users to focus in on only the results they wished to assert for their test. All in all it has been quite successful, although the requirements for our application mean that 300 tests take about 12 hours to run, which is a long time.\n\nGetting the business more involved in maintaining the regression tests has also been difﬁcult, but when it happens it is very good. Currently, we have a stand-up where the business users and the developers meet for 15 minutes to pick up any of the scenario tests that are breaking that day. It is quite effective in that people often know when they come to the stand-up what they might have broken the day before. Future enhancements are likely to include asserting against actual user reports instead of the views and running a migration each night against the sce- nario script.\n\nPASFIT achieved a balance between letting business experts write tests in a DSL and automating those tests with a highly complex application. Success came with some trial and error. Teams that write their own test frameworks need time to experiment to ﬁnd the right solution for both the business and the development team.\n\nSTRATEGIES FOR WRITING TESTS The best tools in the world won’t help if you don’t use them wisely. Test tools might make it very easy to specify tests, but whether you’re specifying the right tests at the right time is up to you. Lisa’s team found that too much de- tail up front clouded the big picture to such a degree that the programmers didn’t know what to code. This won’t be true for every team, and at some point we do need details. The latest time to provide them is when a program- mer picks up a coding task card and starts working on a story.\n\n177",
      "content_length": 2859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "178\n\nCHAPTER 9\n\nLisa’s Story\n\nChapter 18, “Cod- ing and Testing,” goes into more detail about how testers and pro- grammers work together to test and code.\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nWriting detailed test cases that communicate desired behavior effectively re- quires both art and science. Poorly expressed scenarios and poorly designed test cases can create more confusion than they resolve. Experiment so that you can ﬁnd the right level of detail and the right test design for each story. Let’s look at some strategies to help you use tools successfully to write useful business-facing tests.\n\nBuild Tests Incrementally\n\nAfter we have deﬁned our high-level acceptance tests so that the programmer knows what to start coding, we can start elaborating on the rest of the story tests. We can work closely with the programmer to ensure we automate the best possible way.\n\nWhen a programmer starts working on the programming tasks for a story, start writing detailed tests. For those of us who enjoy testing, it’s tempting to go for the biggest “smells” right away, the areas where we think the code might be fragile. Resist the temptation. Make sure the most obvious use case is work- ing ﬁrst. Write a simple, happy path automated test to show the code accom- plishes the most basic task that it should. After that test passes, you can start getting more creative. Writing the business-facing tests is an iterative process.\n\nI start writing executable business-facing tests that support the team by writing a simple FitNesse test based on examples that the product owner provides. I show this to the programmer working on the code. He can make suggestions for changes right then, or he might modify the test himself as appropriate when he’s ready to automate it. Discussing the test often leads the programmer to realize he missed or misunderstood a requirement. We might need another three-way con- versation with the customer. The programmer updates the code accordingly. We can also show the test to the product owner to make sure we captured the be- havior correctly.\n\nAfter the simple test passes, I write more tests, covering more business rules. I write some more complex tests, run them, and the programmer updates the code or tests as needed. The story is ﬁlling out to deliver all of the desired value.\n\nConﬁne each test to one business rule or condition. At some point you can automate or manually perform more complex scenarios, but start by cover- ing each condition with a simple test. If you’ve followed our recommended thin slice or steel thread pattern, the ﬁrst set of tests should prove the ﬁrst\n\n—Lisa",
      "content_length": 2660,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Lisa’s Story\n\nSTRATEGIES FOR WRITING TESTS\n\nthin slice end-to-end. As your automated tests pass, add them to the regres- sion suite that runs in a frequent build process.\n\nKeep the Tests Passing\n\nAfter a test passes, it shouldn’t fail unless the requirements were changed. If that happens, the test should be updated before the code is altered. Of course, if a test was forgotten as part of a requirement change, we expect it to fail. It did its job as change detector. At this time, the test will likely need to change to get it passing.\n\nWhenever a test fails in a continuous integration and build process, the team’s highest priority (other than a critical production problem) should be to get the build passing again. Don’t comment out the failing test and ﬁx it later; that’s the road to perdition. Soon you’ll have dozens of commented-out tests and a lot of technical debt. Everyone on the team should stop what they’re doing and make sure the build goes “green” again. Determine if a bug has been introduced, or if the test simply needs to be updated to accommo- date intentionally changed behavior. Fix the problem, check it in, and make sure all of the tests pass.\n\nEarly on in our agile efforts, my team wasn’t ﬁxing broken tests fast enough. I wrote “Tests are not temporary!” on the whiteboard to remind everyone that once a test passes, it needs to keep passing. A few days later, the words “but testers are!” had been added to get back at me. We did get much better at keep- ing our builds “green” after that.\n\nOne passing test leads to another. Keep your tests current and maintainable with refactoring. Extend them to cover other test cases. The various combi- nations and scenarios might or might not become part of the regression suite after they pass. We want our regression suite to run in a timely manner, and having too many tests for edge cases would slow it down.\n\nUse Appropriate Test Design Patterns\n\nWhen designing tests, look at different patterns and choose the ones that work for you. Keep them as simple as you can. Before you can design tests, you have to identify the ones you need. Pierre Veragen coined the term test genesis patterns to note the patterns that help you think of tests. Examples and use cases feed into our test genesis patterns.\n\n179\n\n—Lisa",
      "content_length": 2291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "180\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nBuild/Operate/Check Lisa’s team often goes with a build/operate/check pattern: Build the input data, in memory or actually in the database, depending on the purpose of the test; invoke the production code to operate on those inputs; and check the re- sults of that operation. Some teams call this setup/execute/validate. For exam- ple, to test the invoice presented to a new account holder, set up the fees to be charged, input the properties of the account that relate to fee amounts, run the code that calculates the fees, and then check to see what fees were actually charged. See Figure 9-9 for an example of a test that sets up a loan with a spec- iﬁed amount, interest rate, term, payment frequency, and service start date and then checks the resulting amortization schedule. The test data is built in memory, which makes for a speedy test. A “teardown” ﬁxture (not shown) re- moves the test data from memory so it won’t interfere with subsequent tests.\n\nIf there’s a need to test the application’s data access layer, tests can run using an actual database. Each test can insert the test data it needs, operate on it, check results, and delete the data. Testing with data in a real database can be a means of automating a test against legacy code whose data access and busi- ness logic layers aren’t easily separated.\n\nFigure 9-9 Example test with build/operate/check pattern",
      "content_length": 1462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "STRATEGIES FOR WRITING TESTS\n\nNotice that the “check” table in the example uses a declarative style, with each row forming an independent test case, without changing the state of the system. Each row in our example tests a line in the loan amortization sched- ule. In the next section, we’ll look at patterns that are in a procedural style, with steps that change or test the state of the system.\n\nTime-Based, Activity, and Event Patterns Sometimes a timeline-based procedural pattern reﬂects the business better. For example, when testing a loan, we want to make sure interest and princi- pal are applied correctly for each payment. The amount of interest depends on the date the payment was received and the date of the last payment pro- cessed. We want a test that simulates taking out a loan for a certain dollar amount, interest rate, and time period, and then over time simulates the bor- rower sending in payments, which are received and processed. Figure 9-10 shows a simple example of a FitLibrary “DoFixture” test that takes out a loan, checks the payment amount, posts the borrower’s payments, receives the payments and processes them, and then checks the interest, principal, and loan balance amount. It also checks the loan default state.\n\nDepending on the domain, a time- or event-based approach might simulate the actual business processes better and be more understandable to business experts than a declarative type test. Other customers might ﬁnd the declara- tive table style simpler to understand, because it hides the procedural details. Different patterns work best for different situations, so experiment with them.\n\nFigure 9-10 Sample time-based test\n\n181",
      "content_length": 1679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "182\n\nCHAPTER 9\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nLearning More Your team should educate itself on test patterns that help drive program- ming. Finding the right pattern for each type of test ensures the test commu- nicates clearly, is easy to maintain, and runs in an optimal amount of time. See the bibliography for more invaluable resources on test design, such as Gerard Meszaros’s xUnit Test Patterns: Refactoring Test Code.\n\nBring programmers and testers together to brainstorm test approaches and to help decide what tests can be automated and how the code should be de- signed to support testing. Business logic and algorithms should be accessible by test ﬁxtures, without having to go through a user interface or batch sched- uling process. This enables test-driven development, which in turn produces testable architecture.\n\nA common approach to automating tests is by driving tests with keywords or action words. This can be used with tools such as Fit and FitNesse, or Ruby with Watir. We’ll explain this next.\n\nKeyword and Data-Driven Tests\n\nData-driven testing is a tool that can help reduce test maintenance and en- able you to share your test automation with manual testers. There are many times when you want to run the same test code over and over, repeating only the inputs and expected results. Spreadsheets or tables, such as those sup- ported by Fit, are excellent ways to specify inputs. The test ﬁxture, method, or script can loop through each data value one at a time, matching expected results to actual results. By using data-driven tests, you are actually using ex- amples to show what the application is supposed to do.\n\nKeyword-driven testing is another tool used in automated testing, where pre- deﬁned keywords are used to deﬁne actions. These actions correspond to a process related to the application. It is the ﬁrst step in creating a domain test- ing language. These keywords (or action words) represent a very simple spec- iﬁcation language that non-programmers can use to develop automated tests. You still need programmers or technical automation specialists to im- plement the ﬁxtures that the action words act on. If these keywords are ex- tended to emulate the domain language, customers and nontechnical testers can specify tests that map to the workﬂow more easily.\n\nThe sample spreadsheet in Figure 9-11 shows how one company used action words to automate their test setup. The same action words can be used to test. The words Signup, Signoff, and CCDeposit are words that are domain-",
      "content_length": 2560,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "TESTABILITY\n\nScriptID\n\nLogging\n\nEnvironment\n\nSite\n\nLang\n\nEmail\n\n8\n\nON\n\nSTAGING\n\nGlobal\n\nEnglish\n\nON\n\nTest ID\n\nDescription\n\nClassName\n\nAction\n\nInput 1\n\nInput 2\n\nInput 3\n\nInput 4\n\n# Signup Customer\n\n123\n\nCdn customer\n\nMember\n\nSignup\n\nJanet\n\nGregory\n\nCalgary\n\n123 St\n\n123\n\nReg complete\n\nMember\n\nsignoff\n\nTRUE\n\n123\n\nLog out\n\nMember\n\nLog_out\n\nTRUE\n\n# Perform CC Deposit\n\nSetup\n\nDescription\n\nClassName\n\nAction\n\nInput 1\n\nInput 2\n\nInput 3\n\nInput 4\n\n234\n\nLog in mbr\n\nMember\n\nLogin\n\nget.AcctId\n\nget.ID_greg\n\nget.pwd_\n\n234\n\nSubmit CC Txn\n\nMember\n\nCCDeposit\n\nVISA\n\n4444333322\n\n02\n\n2008\n\n234\n\nMember Logout\n\nMember\n\nlog_out\n\nEND\n\nFigure 9-11 Sample test spreadsheet with action words\n\nspeciﬁc. Their users could easily write tests without understanding the un- derlying code.\n\nCombining data-driven and keyword-driven testing techniques can be very powerful. Fit and FitNesse use both keywords and data to drive tests. The other tools we’ve described in this chapter can also accommodate this approach.\n\nAny test strategy can run into trouble if the code isn’t designed to be easily tested. Let’s take a look at testability concerns.\n\nTESTABILITY Business-facing tests built with appropriate design patterns and written ahead of any coding help the team achieve a testable code design. The programmers\n\n183\n\nInput 5\n\nT1T 2A2\n\nInput 5\n\n25.86",
      "content_length": 1327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "184\n\nCHAPTER 9\n\nJanet’s Story\n\nLisa’s Story\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nstart by looking at the business-facing tests, perhaps together with a tester, ana- lyst, or customer, so that the need to execute those tests are always in their minds as they proceed with their test-driven design. They can build so that the tests provide inputs and control run-time conditions.\n\nI ran into a snag when I was trying to automate some GUI workﬂow with Ruby and Watir. The calendar pop-up feature was not recognized, and the data ﬁeld was read-only. I took my problem to one of the programmers. We paired to- gether so that he could see the issue I was having. The ﬁrst thing he did was to understand the calendar feature. He thought it would be too difﬁcult to auto- mate the test, so he suggested another alternative. He created a new method that would “fool” the input ﬁeld so it would accept a date into the text ﬁeld. We knew the risk was no automation on the calendar, but for simplicity’s sake we went with his option.\n\nNot all code is testable using automation, but work with the programmers to ﬁnd alternative solutions to your problems.\n\nLet’s look at techniques that promote design of testable code.\n\nCode Design and Test Design\n\nIn Chapter 7, “Technology-Facing Tests that Support the Team,” we explained how test-driven development at the unit level ensures a testable architecture. This is true for business-facing tests as well. The layered architecture Lisa’s team designed works just as well for functional testing. Testing can be done directly against the business logic without involving the user interface, and if appropriate, without involving the database layer. This doesn’t mean that the database layer doesn’t need to be tested. It still needs to be tested, just maybe somewhere else.\n\nTestability has to be considered when coding the presentation layer as well. GUI test tools work better on well-designed code developed with good practices.\n\nWhen I ﬁrst started trying to automate GUI tests using Canoo WebTest, I discovered that the HTML and JavaScript used in the system didn’t comply with standards and contained many errors. WebTest and the tool it’s built on, HtmlUnit, required cor- rect, standard HTML and Javascript. Specifying tests depended on good HTML\n\n—Janet",
      "content_length": 2323,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "In Part IV, “Test Au- tomation,” we’ll dive into develop- ing a successful test automation strategy and look at considerations such as building your own tools versus using third- party or open source tools.\n\nTESTABILITY\n\npractices such as giving each element a unique ID. The programmers started writ- ing HTML and JavaScript (and later, Ajax) with the test tool in mind, making test automation much easier. They also started validating their HTML and making sure it was up to industry standards. This also reduced the possibility of the application having problems in different browsers and browser versions.\n\nCoding and testing are part of one process in agile development. Code design and test design are complementary and interdependent. It’s a chicken-and- egg scenario: You can’t write tests without a testable code design, and you can’t write code without well-designed tests that clearly communicate re- quirements and are compatible with the system architecture. This is why we always consider coding and testing together. When we estimate stories, we in- clude time for both coding and testing, and when we plan each iteration and story, we budget time to design both tests and code. If automating a test proves difﬁcult, evaluate the code design. If programmers are writing code that doesn’t match customer expectations, the problem might be poorly de- signed tests.\n\nAutomated vs. Manual Quadrant 2 Tests\n\nWe’ve assumed that at least a good-sized portion of the tests that guide pro- gramming will be automated. Manual test scenarios can also drive program- ming if you share them with the programmers early. The earlier you turn them into automated tests, the faster you will realize the beneﬁt. Most man- ual tests fall more into the “critique product” quadrant where we might learn things about the story we hadn’t anticipated with the initial set of tests.\n\nThat doesn’t stop us from writing tests that might not be appropriate for au- tomation. Don’t sweat the details when you’re writing tests. You might come up with one-off tests that are important to do but not important to repeat over and over in a regression suite. You might start thinking about end-to- end scenarios or springboards to exploratory test sessions that might be facil- itated with some automation but need an intelligent human to conduct them in full. You’ll ﬁgure that out later. Right now, we want to make sure we cap- ture the customer’s critical requirements.\n\nStart with a simple approach, see how it works, and build on it. The impor- tant thing is to get going writing business-facing tests to support the team as you develop your product.\n\n185\n\n—Lisa",
      "content_length": 2648,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "186\n\nCHAPTER 9\n\nChapter 14, “An Agile Test Automa- tion Strategy,” goes into more detail on how to manage auto- mated tests.\n\n(cid:2) TOOLKIT FOR BUSINESS-FACING TESTS THAT SUPPORT THE TEAM\n\nTEST MANAGEMENT If we’re automating tests, it makes sense to present them in the automation tool framework, even if they’re not yet executable. We want some way for all tests, even those that won’t be automated, to be accessible to everyone on the development team and understandable to our customers. There are lots of options available that let everyone on the team see tests. Wikis are a common way to share test cases, and some tools such as FitNesse use a wiki or similar tool, enabling narrative requirements, examples, and executable tests to co- exist in one place.\n\nTests should be included in your source code control, so that you can track which versions of the tests go with which versions of the code. At the very least, have some kind of version control for your tests. Some teams use test management tools or comprehensive test frameworks that might integrate with requirements management, defect tracking, or other components.\n\nSUMMARY In this chapter, we’ve looked at tools you might want in your toolkit to help create business-facing tests that help drive development and guidelines to make sure the tools help rather than get in the way. The tools and guidelines included the following:\n\n(cid:2) Teams need the right tools to elicit requirements and examples,\n\nfrom the big picture down to details, including checklists, mind maps, spreadsheets, mock-ups, ﬂow diagrams, and various software-based tools.\n\n(cid:2) Tools to express examples and automate tests, below and through the GUI, are also essential to agile test automation. Some of these tools include unit test tools, behavior-driven development tools, FitNesse, Ruby with Watir, Selenium, and Canoo WebTest.\n\n(cid:2) “Home brewed” test automation helps teams keep the total cost of\n\nownership of their automated tests low.\n\n(cid:2) Driving development with business-facing tests is one way agile teams\n\nare motivated to design testable code.\n\n(cid:2) Test strategies for building your automation should include building your tests incrementally and making sure they always pass. Design patterns can be used to help you create effective tests.",
      "content_length": 2312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "SUMMARY\n\n(cid:2) Keyword and data-driven testing is a common approach that works\n\nwith the tools we’ve discussed in this chapter.\n\n(cid:2) Consider testability in your code design, and choose your test tools\n\nwisely, because they need to work with your code.\n\n(cid:2) We need some way to organize tests so that they can be used effec-\n\ntively and put into version control.\n\n187",
      "content_length": 377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "Chapter 10\n\nBUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nUsing Automated Functional Test Tools for ET\n\nTest Setup\n\nMonitoring Tools\n\nToolkit\n\nTest Data Generation\n\nSimulators\n\nIntroduction\n\nEmulators\n\nDemonstrations\n\nWorkflow Testing\n\nUser Documentation\n\nReports\n\nDocumentation\n\nBusiness-Facing/ Critique Product\n\nScenario Testing\n\nDomain Knowledge\n\nEnd-to-End Scenarios\n\nSoap Opera Testing\n\nAPI\n\nBehind the GUI\n\nWeb Services\n\nLearning by Testing\n\nUser Needs and Persona Testing\n\nExploratory Testing\n\nSession-Based Testing\n\nAutomate\n\nNavigation\n\nUsability\n\nAn Exploratory Tester\n\nCheck Out the Competition\n\nThis chapter covers the third quadrant of the testing matrix. In Chapter 8, “Business-Facing Tests that Support the Team,” we talked about the second quadrant and how to use business-facing tests to support programming. In this chapter, we show you how to critique the product with different types of business-facing tests. We’ll also talk about tools that might help with these activities.\n\n189",
      "content_length": 1006,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "190\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nINTRODUCTION TO QUADRANT 3 Remember that business-facing tests are those you could describe in terms that would (or should) be of interest to a business expert. When we mention testing in traditional phased approaches, it pretty much always means cri- tiquing the product after it is built. By now, you might think that in agile de- velopment this part of testing should be easy. After all, we just spent all that time making sure it works as expected. The requirements have all been tested as they were built, including security and other nonfunctional requirements, right? All that’s left is to possibly ﬁnd some obscure or interesting bugs.\n\nAs testers, we know that people make mistakes. No matter how hard we try to get it right the ﬁrst time, we sometimes get it wrong. Maybe we used an ex- ample that didn’t test what we thought it did. Or maybe we recorded a wrong expected result so the test passed, but it was a false positive. The business ex- pert might have forgotten some things that real users needed. The best cus- tomer may not know what she wants (or doesn’t want) until she sees it.\n\nCritiquing or evaluating the product is what testers or business users do when they assess and make judgments about the product. These evaluators form perceptions based on whether they like the way it behaves, the look and feel, or the workﬂow of new screens. It is easier to see, feel, and touch a product and respond than to imagine what it will look like when it is described to you.\n\nIt’s difﬁcult to automate business-facing tests that critique the product, because such testing relies on human intellect, experience, and instinct. However, auto- mated tools can assist with aspects of Quadrant 3 tests (see Figure 10-1), such as test data setup. The last section of this chapter contains examples of the types of tools that help teams focus on the important aspects of evaluating the product’s value.\n\nWhile much of the testing we discuss in this chapter is manual, don’t make the mistake of thinking that this manual testing will be enough to produce high-quality software and that you can get away with not automating your regression tests. You won’t have time to do any Quadrant 3 tests if you haven’t automated the tests in Quadrants 1 and 2.\n\nEvaluating or critiquing the product is about manipulating the system under test and trying to recreate actual experiences of the end users. Understanding different business scenarios and workﬂows helps to make the experience more realistic.",
      "content_length": 2572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "DEMONSTRATIONS\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nQ2\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 10-1 Quadrant 3 tests\n\nDEMONSTRATIONS We recommend showing customers what you’re developing early and often. As soon as a rudimentary UI or report is available during story development, show it to the product owner or other domain expert on the team. However, not everyone on the business side will get a chance to see the iteration’s deliv- erables until the iteration demo. End-of-iteration demonstrations are an op- portunity for the business users and domain experts to see what has been delivered in the iteration and revise their priorities. It gives them a chance to say, “That’s what I said, but it’s not what I meant.” This is a form of critiquing the product.\n\n191",
      "content_length": 1130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "192\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nJanet’s Story\n\nI worked on a project that had ﬁve separate teams of eight to ten members, all developing the same system. Even though they were on the same ﬂoor, communi- cation was an issue. There were many dependencies and overlaps, so the pro- grammers depended on team-lead meetings to share information. However, the business users and testers needed to see what was being developed by other teams. They relied on end-of-iteration demonstrations given by each team to learn what the other teams were doing.\n\nDemonstrations to the executives or upper management can instill conﬁ- dence in your project as well. One of the downfalls of a phased project is there is nothing to see until the very end, and management has to place all of its trust in the development team’s reports. The incremental and iterative na- ture of agile development gives you a chance to demonstrate business value as you produce it, even before you release it. A live demonstration can be a very powerful tool if the participants are actively asking questions about the new features.\n\nRather than waiting until the end of the iteration, you can use any opportu- nity to demonstrate your changes. A recent project Janet worked on used reg- ularly scheduled meetings with the business users to demonstrate new features in order to get immediate feedback. Any desired changes were fed into the next iteration.\n\nChapter 19, “Wrap Up the Iteration,” talks about end- of-iteration dem- onstrations and reviews.\n\nChoose a frequency for your demonstrations that works for your team so that the feedback loop is quick enough for you to incorporate changes into the release.\n\nInformal demos can be even more productive. Sit down with a business ex- pert and show her the story your team is currently coding. Do some explor- atory testing together. We’ve heard of teams that get their stakeholders to do some exploratory testing after each iteration demo in order to help them think of reﬁnements and future stories to change or build on the functional- ity just delivered.\n\nSCENARIO TESTING Business users can help deﬁne plausible scenarios and workﬂows that can mimic end user behavior. Real-life domain knowledge is critical to creating accurate scenarios. We want to test the system from end to end but not neces- sarily as a black box.\n\n—Janet",
      "content_length": 2389,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "Chapter 14, “An Agile Test Automa- tion Strategy,” ex- amines different approaches to ob- taining test data.\n\nSCENARIO TESTING\n\nOne good technique for helping the team understand the business and user needs is “soap opera testing,” a term coined by Hans Buwalda [2003]. The idea here is to take a scenario that is based on real life, exaggerate it in a man- ner similar to the way TV soap operas exaggerate behavior and emotions, and compress it into a quick sequence of events. Think about questions like, “What’s the worst thing that can happen, and how did it happen?”\n\nSoap Opera Test Example\n\nLisa worked on an Internet retail site, where she found soap opera tests to be effective. Here’s an example of a soap opera scenario to test inventory, preorder, and backorder processes of an Internet retailer’s warehouse.\n\nThe most popular toy at our online toy store this holiday season is the Super Tester Action Figure. We have 20 preorders awaiting receipt of the items in our warehouse. Finally, Jane, a warehouse supervisor, receives 100 Super Tester Action ﬁgures. She updates the inventory system to show it is available inventory against the purchase order and no longer a preorder. Our website now shows Super Tester Action Figures available for delivery in time for the holidays. The system releases the preorders, which are sent to the warehouse. Meanwhile, Joe, the forklift driver, is distracted by his cell phone, and accidentally crashes into the shelf con- taining the Super Tester Action Figures. All appear to be smashed up beyond recognition. Jane, horriﬁed, removes the 100 items from avail- able inventory. Meanwhile, more orders for this popular toy have piled up in the system item. Sorting through the debris, Jane and Joe ﬁnd that 14 of the action ﬁgures have actually survived intact. Jane adds them back into the available inventory.\n\nThis scenario tests several processes in the system, including preorder, purchase order receipt, backorder, warehouse cancels, and preorder release. How many Super Tester toys will show as available on the shop- ping website at the end of all that? While executing the scenario, we’ll probably ﬁnd other areas we want to investigate; maybe the purchase order application is difﬁcult to use or the warehouse inventory updates aren’t reﬂected properly in the website. Thinking up and executing these types of tests will teach us more about what our users and other external customers need than running predeﬁned functional tests on narrower areas of the application. As a bonus, it’s fun!\n\nAs a tester, we often “make up” test data, but it is usually simple so we can easily check our results. When testing different scenarios, both the data and the ﬂow need to be realistic. Find out if the data comes from another system or if it’s input manually. Get a sample if you can by asking the customers to provide data for testing. Real data will ﬂow through the system and can be checked along the way. In large systems, it will behave differently depending on what decisions are made.\n\n193",
      "content_length": 3046,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "194\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nTools to help deﬁne the scenarios and workﬂows can be simple. Data ﬂow or process ﬂow diagrams will help identify some of the common scenarios. These scenarios can help you think through a complex problem if you take the time. Consider the users and their motivation.\n\nLisa’s Story\n\nOur team planned to rewrite the core functionality of the application that pro- cesses the daily buys and sells of mutual funds. These trades are the result of re- tirement plan participants making contributions, exchanging balances from one fund to another, or withdrawing money from their accounts. Lisa’s coworker, Mike Thomas, studied the existing trade processing ﬂow and diagrammed it so that the team could understand it well before trying to rewrite the code. Figure 10-2 shows a portion of the ﬂow diagram. WT stands for the custodian who does the actual trading. Three different ﬁle types are downloaded and translated into read- able format: CFM, PRI, and POS. Each of these ﬁles feeds into a different part of the application to perform processing and produce various outputs: settled trades, a ticker exception report, and a fund position report.\n\n—Lisa\n\nTrade Processing\n\nDownload/Translate\n\nWT\n\nDownload/Translate\n\nCFM File\n\nDownload/Translate\n\nPOS File\n\nTP1 Settle Trades\n\nPRI File\n\nTP3 Upload/Verify Positions\n\nSettled Txns\n\nTP2 Update Prices\n\nOmnibus Fund Position Rpt\n\nTicker Exception Rpt\n\nFigure 10-2 Sample portion of a process ﬂow diagram\n\nWhen testing end-to-end, make spot checks to make sure the data, status ﬂags, calculations, and so on are behaving as expected. Use ﬂow diagrams and other visual aids to help you understand the functionality. Many organi-",
      "content_length": 1741,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "The bibliography lists more resources you should investi- gate to learn more about exploratory testing.\n\nSee the bibliogra- phy for some links to more about Rapid Software Testing.\n\nEXPLORATORY TESTING\n\nzations depend on reports to make decisions, and those reports seem to be the last thing we verify. If your scenarios have been identiﬁed correctly, you might be able to use your application reports to provide a ﬁnal check.\n\nEXPLORATORY TESTING Exploratory testing (ET) is an important approach to testing in the agile world. As an investigative tool, it’s a critical supplement to the story tests and our automated regression suite. It is a sophisticated, thoughtful approach to testing without a script, and it enables you to go beyond the obvious varia- tions that have already been tested. Exploratory testing combines learning, test design, and test execution into one test approach. We apply heuristics and techniques in a disciplined way so that the “doing” reveals more implica- tions that just thinking about a problem. As you test, you learn more about the system under test and can use that information to help design new tests.\n\nExploratory testing is not a means of evaluating the software through ex- haustive testing. It is meant to add another dimension to your testing. You do just enough to see if the “done” stories are really done to your satisfaction.\n\nA valuable side effect of exploratory testing is the learning that comes out of it. It reveals areas of the product that could use more automated tests and brings up ideas for new or modiﬁed features that lead to new stories.\n\nExploratory Testing Explained\n\nMichael Bolton is a trainer and consultant in rapid and exploratory testing approaches. He teaches a course called Rapid Software Testing, which he co- writes with senior author James Bach. Here’s Michael’s deﬁnition of explor- atory testing.\n\nCem Kaner didn’t invent exploratory testing, but he identiﬁed and named it in 1983, in the ﬁrst edition of Testing Computer Software, as an approach all testers use when their brains are engaged in their work. He and James Bach, the other leading advocate of the approach, have long deﬁned exploratory testing as “simultaneous test design, test execution, and learning.” Kaner also deﬁnes exploratory testing more explicitly as “a style of testing that emphasizes the freedom and responsibility of the in- dividual tester to continually optimize the value of her work by treating learning, test design, test execution, and test result interpretation as ac- tivities that continue in parallel throughout the project.” That’s quite a mouthful. What does it mean?\n\n195",
      "content_length": 2644,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "196\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nThe most important thing to remember about exploratory testing is that it’s not a test technique on its own. Instead, it’s an approach or a mind- set that can be applied to any test technique. The second thing to remember is that exploratory testing is not merely about test execution; testers can also take an exploratory approach when they’re designing new tests at the beginning of the iteration or analyzing the results of tests that have already been performed. A third important note is that exploratory testing isn’t sloppy or slapdash or unprepared testing. An exploratory approach might require very extensive and elaborate prepa- ration for certain tests—and an exploratory tester’s knowledge and skill set, developed over years, is an often invisible yet important form of preparation. An exploratory test might be performed manually, or might employ extensive use of test automation—that is, any use of tools to support testing. So if exploratory testing isn’t a technique, nor test exe- cution, nor spontaneous, nor manual, what is it that makes a test activity exploratory? The answer lies in the cognitive engagement of the tester— how the tester responds to a situation that is continuously changing.\n\nSuppose that a tester is given the mission to test a conﬁguration dialog for a text editor. A tester using an exploratory approach would use spec- iﬁcations and conversations about the desired behavior to inform test ideas, but would tend to record these ideas in less detail than a tester using a scripted approach. A skilled tester doesn’t generally need much explicit instruction unless the test ideas require some speciﬁc actions or data. If so, they might be written down or supplied to a program that could exercise them quickly. Upon seeing the dialog, the exploratory tester would interact with it, usually performing tests in accordance with the original test ideas—but she might also turn her attention to other ideas based on new problems or risks in the dialog as it appeared in front of her. Can two settings conﬂict in a way not covered by existing tests? The exploratory tester immediately investigates by performing a test on the spot. Does the dialog have a usability issue that could inter- fere with a user’s work ﬂow? The exploratory tester quickly considers a variety of users and scenarios and evaluates the signiﬁcance of the prob- lem. Is there a delay upon pressing the OK button? The exploratory tester performs a few more tests to seek a general pattern. Is there a possibility that some conﬁguration options might not be possible on another platform? The exploratory tester notes the need for additional testing and moves on. Upon receiving new builds, the exploratory tester would tend to deemphasize repetition and emphasize variation in order to discover problems missed by older tests that are no longer revealing interesting information. This approach, which has always been fruitful, is even more powerful in environments where the need for repeated test- ing is handled by the developers’ low-level, automated regression tests.\n\nExploratory testing is characterized by the degree to which the tester is under her own control, making informed choices about what he or she",
      "content_length": 3297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "EXPLORATORY TESTING\n\nis going to do next, and where the last outcome of the last activity con- sciously informs the next choice. Exploratory and scripted approaches are at the opposite poles of a continuum. At the extreme end of the scripted mind-set, the decision as to what to do next comes exclusively from someone else, at some point in the past. In the exploratory mind-set, the decision to continue on the same line of inquiry or to choose a new path comes entirely from the individual tester, in the moment in which the activity occurs, The result of the last test strongly informs the tester’s choices for the next test. Other inﬂuences include the stakeholders for whom test information might be important, the quality criteria that are important to stakeholders, the test coverage that stakeholders seek, spe- ciﬁc risks associated with the item being tested, the needs of the end user of the product, the skills of the tester, the skills of the developers, the state of the item under test, the schedule for the project, the equip- ment and tools that are available to the tester, and the extent to which she can use them effectively—and that’s only a partial list.\n\nNo test activity performed by a thinking human is entirely scripted. Humans have an extraordinary capacity to recognize things even when people are telling them not to, and as a result we can be distracted and diverted—but we can learn and adapt astonishingly quickly to new infor- mation and investigate its causes and effects. Machines only recognize what they’ve been programmed to recognize. When they’re confronted with a surprising test result, at best they ignore it; at worst, they crash or destroy data.\n\nYet no test activity performed on behalf of a client is entirely exploratory, either. The exploratory tester is initially driven by the testing mission, which is typically set out by the client early in the project. Exploratory work can also be guided by checklists, strategy models, coverage out- lines, risk lists—ideas that might come from other people at other times. The more that the tester is controlled by these ideas rather than guided by them, the more testing takes on a scripted approach.\n\nGood exploration requires continuous investigation of the product by engaged human testers, in collaboration with the rest of the project community, rather than following a procedurally structured approach, performed exclusively by automation. Exploration emphasizes individuals and interactions over processes and tools. In an agile environment, where code is produced test-ﬁrst and is covered with automated regres- sion tests, testers can have not only the conﬁdence but also the man- date to develop new tests and seek out new problems in the moment. Exploration emphasizes responding to change versus following a plan. Exploratory approaches use variation to drive an active search for prob- lems instead of scripted manual or automated test cases that merely conﬁrm what we already knew. Exploration emphasizes working soft- ware over comprehensive documentation. And to be effective, good\n\n197",
      "content_length": 3094,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "198\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nexploration requires frequent feedback between testers, developers, customers, and the rest of the project community, not merely repetition of tests that were prepared at the beginning of the iteration, before we had learned important things about the project. Exploration empha- sizes customer collaboration over negotiated contracts. Exploratory approaches are fundamentally agile.\n\nExploratory testing embraces the same values as agile development. It’s an im- portant part of the “agile testing mind-set” and critical to any team’s success.\n\nPeople unfamiliar with exploratory testing often confuse it with ad hoc test- ing. Exploratory testing isn’t sitting down at a keyboard and typing away. Unskilled “black box” testers may not know how to do exploratory testing.\n\nExploratory testing starts with a charter of what aspects of the functionality will be explored. It requires critical thinking, interpreting the results, and comparing them to expectations or similar systems. Following “smells” when testing is an important component. Testers take notes during their explor- atory testing sessions so that they can reproduce any issues they see and do more investigation as needed.\n\nTechnique: Exploratory Testing and Information Evaluation\n\nJon Hagar, an experienced exploratory tester, learner, and trainer, shares some activities, characteristics, and skills that are vital to effective exploratory testing.\n\nExploratory testing uses the tester’s understanding of the system, along with critical thinking, to deﬁne focused, experimental “tests” which can be run in short time frames and then fed back into the test planning process.\n\nAn agile team has many opportunities to do exploratory testing, since each development cycle creates production-ready, working software. Starting early in each development cycle, consider exploratory tests based on:\n\nRisk (analysis): The critical things you and the customer/user think can go wrong or be potential problems that will make people unhappy.\n\nModels (mental or otherwise) of how software should behave: You and/or the customer have a great expectation about what the newly produced function should do or look like, so you test that.\n\nPast experience: Think about how similar systems have failed (or suc- ceeded) in predictable patterns that can be reﬁned into a test, and explore it.",
      "content_length": 2414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "EXPLORATORY TESTING\n\nWhat your development team is telling you: Talk to your developers and ﬁnd out what “is important to us.”\n\nMost importantly: What you learn (see and observe) as you test. As a tester on an agile team, a big part of your job is to constantly learn about your product, your team, and your customer. As you learn, you should quickly see tests based on such things as customer needs, common mistakes the team seems to be making, or good/ bad characteristics of the product.\n\nSome tests might be good candidates for automated regression suites. Some might just answer your exploratory charter and be “done.” The agile team must critically think about what they are learning and “evolve” tests accordingly. The most important aspect here is to be “brain on” while testing, where you are looking for the “funny,” unexpected, or new, which automated tests would miss. Use automation for what it is good at (repetitive tasks) and use agile humans for what we are good at (seeing, thinking, and dealing with the unexpected).\n\nSeveral components are typically needed for useful exploratory testing:\n\nTest Design: An exploratory tester as a good test designer under- stands the many test methods. You should be able to call different methods into play on the ﬂy during the exploration. This agility is a big advantage of exploratory testing over automated (scripted) pro- cedures, where things must be thought out in advance.\n\nCareful Observation: Exploratory testers are good observers. They watch for the unusual and unexpected and are careful about as- sumptions of correctness. They might observe subtle software characteristics or patterns that drive them to change the test in real time.\n\nCritical Thinking: The ability to think openly and with agility is a key reason to have thinking humans doing nonautomated exploratory testing. Exploratory testers are able to review and redirect a test into unexpected directions on the ﬂy. They should also be able to ex- plain their logic of looking for defects and to provide clear status on testing. Critical thinking is a learned human skill.\n\nDiverse Ideas: Experienced testers and subject matter experts can produce more and better ideas. Exploratory testers can build on this diversity during testing. One of the key reasons for exploratory tests is to use critical thinking to drive the tests in unexpected directions and ﬁnd errors.\n\nRich Resources: Exploratory testers should develop a large set of tools, techniques, test data, friends, and information sources upon which they can draw. The agile test team members should grow their exploratory resources throughout a project and throughout their careers.\n\n199",
      "content_length": 2676,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "200\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nTo help you understand a day in the life of an agile exploratory tester, here is a short tester’s story:\n\nI arrived at 8:00 a.m. and reviewed what had happened the night before during automated testing. The previous night’s automated tests found some minor but interesting errors. A password ﬁeld on a login form had accepted a special character, which should have been rejected by the validation. I created an outline as a starting point for my “attack” (a top- level plan and/or risk list).\n\nAs I thought about my “plan of attack,” I sketched a small state model of the problem on a ﬂip chart and showed this to a developer and my team’s customer rep. I designed a test incorporating their suggestions, using some data stress inputs that I expected the validation to reject (a 1 MG ﬁle of special characters). I executed my test with my stress input, and, sure enough, the system rejected them as expected. I tried a differ- ent data set and the system failed with a buffer overﬂow in the database. I was learning, and we were on the trail of a potentially serious security bug. As the day went on, I explored different inputs to the password ﬁeld and worked with the team to get the bug ﬁxed.\n\nYou can learn from automated test results as well as from exploratory testing. Each type of testing feeds into the other. Develop a broad range of skills so you’ll be able to identify important issues and write tests to prevent them from reoccurring.\n\nThe term exploratory testing was popularized by the “context-driven school” of testing. It’s a highly disciplined activity, and it can be learned. Session-based test management is one method of testing that’s designed to make explor- atory testing auditable and measurable [Bach, 2003].\n\nSession-Based Testing\n\nSession-based testing combines accountability and exploratory testing. It gives a framework to a tester’s exploratory testing experience so that they can report results in a consistent way.\n\nJanet’s Story\n\nJames Bach [2003] compares exploratory testing to putting together a jigsaw puz- zle. When I ﬁrst read his article with the jigsaw puzzle analogy, exploratory testing made perfect sense to me.\n\nI start a jigsaw puzzle by dumping out all of the pieces of the puzzle and then sorting them into the different colors and edge pieces. Next, I put the edge pieces together, which gives me a framework in which to start. The edge of the jigsaw is analogous both to the mission statement, which helps me focus, and to the time- boxing of a session, which keeps me within certain limits.",
      "content_length": 2618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "For more informa- tion on session- based testing, check the bibliog- raphy for work by Jonathan Bach.\n\nEXPLORATORY TESTING\n\nSession-based testing is a form of exploratory testing, but it is time-boxed and a bit more structured. I learned about session-based testing from Jonathan Bach and found it gave me the structure I needed to do exploratory testing well. I use the same skills as I do for a jigsaw puzzle: I look for patterns in color or shapes or perhaps something that just doesn’t look right, an anomaly. My thought process can take those patterns and make sense of them, using heuristics I have devel- oped to help me solve a puzzle.\n\nLike solving the jigsaw puzzle by putting together the outside pieces ﬁrst, we can use session-based testing to give us the framework in which we work. In session-based testing, we create a mission or a charter and then time-box our session so we can focus on what’s important. Too often as testers, we can go off track and end up chasing a bug that might or might not be important to what we are currently testing.\n\nSessions are divided into three kinds of tasks: test design and execution, bug investigation and reporting, and session setup. We measure the time we spend on setup versus actual test execution so that we know where we spend the most time. We can capture results in a consistent manner so that we can report back to the team.\n\nAutomation and Exploratory Testing\n\nWe can combine exploratory testing with test automation as well. Jonathan Kohl, in his article “Man and Machine” [2007], talks about interactive test automation to assist exploratory testing. Use automation to do test set up, data generation, repetitive tasks, or to progress along a workﬂow to the place you want to start. Then you start using your testing skills and experience to ﬁnd the really “good” bugs, the insidious ones that otherwise escape atten- tion. You can also use an automated test suite to explore. Just modify it a bit, watch the results as it runs, modify it again, and watch what happens.\n\nAn Exploratory Tester\n\nWith exploratory testing, each tester has a different approach to a problem, and has a unique style of working. However, there are certain attributes that make for a good exploratory tester. A good tester:\n\n(cid:2) Is systematic, but pursues “smells” (anomalies, pieces that aren’t\n\nconsistent)\n\n(cid:2) Learns to recognize problems through the use of Oracles (principle or\n\nmechanism by which we recognize a problem)\n\n201\n\n—Janet",
      "content_length": 2490,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "202\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\n(cid:2) Chooses a theme or role or mission statement to focus testing (cid:2) Time-boxes sessions and side trips (cid:2) Thinks about what the expert or novice user would do (cid:2) Explores together with domain experts (cid:2) Checks out similar or competitive applications\n\nExploratory testing helps us learn about the behavior of an application. Testers generally know a lot about the application they’re testing. How do they judge whether the application is usable by users who are less technical or not familiar with it? Usability testing is vital for many software systems. We’ll talk about that in the next section.\n\nUSABILITY TESTING There are two types of usability testing. The ﬁrst type is the kind that is done up front by the user experience folks, using tools such as wire frames to help drive programming. Those types of tests belong in Quadrant 2. In this sec- tion, we’re talking about the kind of usability testing that critiques the prod- uct. We use tools such as personas and our intuition to help us look at the product with the end user in mind.\n\nUser Needs and Persona Testing\n\nLet’s look at an online shopping example. We think about who will use the site. Will it be people who have shopped online before, or will it be brand new users who have no idea how to proceed? We’re guessing it will be a mix- ture of both, as well as others. Take the time to ask your marketing group to get the demographics of the end users. The numbers might help you plan your testing.\n\nOne approach to using personas is for your team to invent several different users of your application representing different experience levels and needs. For our Internet retail application, we might have the following personas:\n\n(cid:2) Nancy Newbie, a senior citizen who is new to Internet shopping and\n\nnervous about identity theft\n\n(cid:2) Hudson Hacker, who looks for ways to cheat the checkout page (cid:2) Enrico Executive, who does all his shopping online and ships gifts to\n\nall his clients worldwide\n\n(cid:2) Betty Bargain, who’s looking for great deals (cid:2) Debbie Ditherer, who has a hard time deciding what items she really\n\nwants to order",
      "content_length": 2221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "Janet’s Story\n\nUSABILITY TESTING\n\nWe might hang photos representing these different personas and their biog- raphies in our work area so that we always keep them in mind. We can test the same scenario as each persona in turn and see what different experiences they might encounter.\n\nAnother way to approach persona testing, which we learned from Brian Mar- ick and Elisabeth Hendrickson, is to pick a ﬁctional character or famous ce- lebrity and imagine how they would use our application. Would the Queen of England be able to navigate our checkout process? How might Homer Simpson search for the item he wants?\n\nReal World Projects: Personas\n\nThe OneNote team at Microsoft uses personas as part of their testing process. Mike Tholfsen [2008], the Test Manager for OneNote, says they use seven per- sonas that might use OneNote, speciﬁc customer types such as Attorneys, Students, Real Estate Agents, and Salespersons. The personas they create contain information such as:\n\nGeneral job description • “A Day in the Life” • Primary uses for OneNote • List of features the persona might use • Potential notebook structures • Other applications used • Conﬁguration and hardware environment\n\nYou can also just assume the roles of novice, intermediate, and expert users as you explore the application. Can users ﬁgure out what they are supposed to do without instructions? If you have a lot of ﬁrst-time users, you might need to make the interface very simple.\n\nWhen I ﬁrst started testing a new production accounting system, I found it very difﬁ- cult to understand the ﬂow, but the production accountants on the team loved it. After I worked with it for a while, I understood the complexity behind the applica- tion and knew why it didn’t have to be intuitive for a ﬁrst-time user. This was a good lesson for me, because I always assumed applications had to be user-friendly.\n\nIf your application is custom-built for speciﬁc types of users, it might need to be “smart” rather than intuitive. Training sessions might be sufﬁcient to get\n\n203\n\n—Janet",
      "content_length": 2045,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "204\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nover the initial lack of usability so that the interface can be designed for max- imum efﬁciency and utility.\n\nNavigation\n\nNavigation is another aspect of usability testing. It’s incredibly important to test links and make sure the tabbing order makes sense. If a user has a choice of applications or websites, and has a bad ﬁrst experience, they likely won’t use your application again. Some of this testing is automatable, but it’s im- portant to test the actual user experience.\n\nSee Chapter 8, “Business-Facing Tests that Support the Team,” for an example of Wiz- ard of Oz testing, which is one ap- proach to design- ing for usability.\n\nIf you have access to the end users, get them involved in testing the naviga- tion. Pair with a real user, or watch one actually use the application and take notes. When you’re designing a new user interface, consider using focus groups to evaluate different interfaces. You can start with mock-ups and ﬂows drawn on paper, get opinions, and try HTML mock-ups next, to get early feedback.\n\nCheck Out the Competition\n\nWhen evaluating your application for usability, think about other applica- tions that are similar. How do they accomplish tasks? Do you consider them user-friendly or intuitive? If you can get access to competing software, take some time to research how those applications work and compare them with your product. For example, you’re testing a user interface that takes a date range, and it has a pop-up calendar feature to select the date. Take a look at how a similar calendar function works on an airline reservation website.\n\nSee the bibliogra- phy for links to articles by Jeff Patton, Gerard Meszaros, and others on usability testing.\n\nUsability testing is a fairly specialized ﬁeld. If you’re producing an internal application to be used by a few users who will be trained in its use, you prob- ably don’t need to invest much in usability testing. If you’re writing the on- line directory assistance for a phone company, usability might be your main focus, so you need to learn as much as you can about it, or bring in a usabil- ity expert.\n\nChapter 9, “Tool- kit for Business- Facing Tests that Support the Team,” provides more detail about tools that facili- tate these tests.\n\nBEHIND THE GUI In a presentation titled “Man and Machine” [2007], Jonathan Kohl talked about alternatives for testing interfaces. Instead of always thinking about test- ing through the user interface, consider attacking the problem in other ways. Think about testing the whole system from every angle that you can ap- proach. Consider using tools like simulators or emulators.",
      "content_length": 2696,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "Lisa’s Story\n\nBEHIND THE GUI\n\nAPI Testing\n\nIn Chapter 8 and Chapter 9, we talked about testing behind the GUI to drive development. In this section, we show that you can extend your tests for the API in order to try different permutations and combinations.\n\nAn API (application programming interface) is a collection of functions that can be executed by other software applications or components. The end user is usually never aware that an API exists; she simply interacts with the inter- face on top.\n\nEach API call has a speciﬁc function with a number of parameters that accept different inputs. Each variation will return a different result. The easy tests are simple inputs. The more complicated testing patterns occur when the pa- rameters work together to give many possible variations. Sometimes param- eters are optional, so it’s important that you understand the possibilities. Boundary conditions should be considered as well, for both the inputs and expected results. For example, use both valid and invalid strings for parame- ters, vary the content, and vary the length of the strings’ input.\n\nAnother way to test is to vary the order of the API calls. Changing the se- quence might produce unexpected results and reveal bugs that would never be found through UI testing. You can control the tests much more easily than when using the UI.\n\nMy team was working on a set of stories to enable retirement plan sponsors to upload payroll contribution ﬁles. We wrote FitNesse test cases to illustrate the ﬁle parsing rules, and the programmer wrote unit tests for those as well. When the coding for the parser was complete, we wanted to throw a lot more combi- nations of data at the parser, including some really bizarre ones, and see what happened. We could use the same ﬁxture as we used for our tests to drive de- velopment, enter all of the crazy combinations we could think of, and see the results. We tested about 100 variations of both valid and invalid data. Figure 10-3 shows an example of just a few of the tests we tried. We found several errors in the code this way.\n\nWe didn’t keep all of these tests in the regression suite because they were just a means of quickly trying every combination we could think of. We could have done these tests in a semi-automated, ad hoc manner too, not bothering to type the expected results into the result checking table, and just eyeballing the outputs to make sure they looked correct.\n\n205\n\n—Lisa",
      "content_length": 2456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "206\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nFigure 10-3 Sample of parsing rules test\n\nJanet’s Story\n\nI recently worked with a web application that interfaces to a legacy system through a well-deﬁned API. Due to the design of the legacy system and the fact that the data is hard to replicate, the team hasn’t yet found a way to automate this testing. However, we could look in the log ﬁles to verify the correct inputs were passed and the expected result was returned. Valuable exploratory testing of APIs is possible with or without beneﬁt of automation.\n\nAPI calls can be developed early in an application life cycle, which means testing can occur early as well. Testing through an API can give conﬁdence in the system before a UI is ever developed. Because this type of testing can be automated, you will need to work with your programmers to understand all of the parameters and the purpose of each function. If your programmers or automation team develop a test harness that is easy to use, you should be able to methodically create a suite of test cases that exercises the functionality.\n\n—Janet",
      "content_length": 1130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "Web services generally require plenty of security, stress, and reliabil- ity testing. See Chapter 12, “Cri- tiquing the Product using Technology- Facing Tests,” for more on these types of tests.\n\nChapter 12, “Sum- mary of Testing Quadrants,” has an example of testing web services.\n\nTESTING DOCUMENTS AND DOCUMENTATION\n\nWeb Services\n\nWeb services are a services-based architecture that provides an external inter- face so that others can access the system. There might be multiple stakeholders, and you may not even know who will be using your product. Your testing will need to conﬁrm the quality of service that the external customers expect.\n\nConsider levels of service that have been promised to clients when you are creating your test plans. Make time for exploratory testing to simulate the different ways users might access the web services.\n\nThe use of web services standards also offers other implications for current testing tools. As with API calls, web services-based integration highlights the importance of validating interface points. However, we also need to consider message formats and processing, queuing times, and message response times.\n\nUsing testing tools that utilize GUI-driven automation is simply inadequate for a web services project. A domain-speciﬁc language that encapsulates im- plementation details “behind the scenes” works well for testing web services.\n\nTESTING DOCUMENTS AND DOCUMENTATION One of the components of the system that is often overlooked during testing is documentation. As agile developers, we may value working software over doc- umentation, but we still value documentation! User manuals and online help need validation just as much as software. Your team may employ specialists such as technical writers who create and verify documentation. As with all other components of the product, your whole team is responsible for the qual- ity of the documentation, and that includes both hard copy and electronic.\n\nUser Documentation\n\nYour team might do Quadrant 2 tests to support the team as they produce documentation; in fact we encourage it. Lisa’s team writes code that pro- duces documents whose contents are speciﬁed by government regulations, and programmers can write much of the code test-ﬁrst. However, it’s difﬁcult for automated tests to judge whether a document is formatted correctly or uses a readable font. They also can’t evaluate whether the contents of docu- ments such as user manuals are accurate or useful. Because documentation has many subjective components, validating it is more of a critiquing activity.\n\n207",
      "content_length": 2583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "208\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nJanet’s Story\n\nTechnical writers and testers can work very closely together. Stephanie, a techni- cal writer I worked with on one project, talked with the programmers to under- stand how the application worked. She would also work through the application to make sure she wrote it down correctly. This seemed to be a duplication of the testing effort, so Stephanie and I sat down and ﬁgured out a better approach.\n\nWe decided to work together on the stories as they were developed. For some stories Stephanie was lead “tester,” and sometimes I took that role. If I was lead, I’d create my test conditions and examples and Stephanie would use those as her basis for the documentation. When Stephanie was lead, she would write her doc- umentation, and then I would use that to determine the test cases.\n\nDoing it this way enabled the documentation to be tested and the tests to be challenged before they were ever executed. Working hand in hand like this proved to be a very successful experiment. The resulting documentation matched the software’s behavior and was much more useful to the end users.\n\nDon’t forget to check the help text too. Are the links to help text easily identi- ﬁable? Are they consistent throughout the user interface? Is the help text pre- sented clearly? If it opens in a pop-up, and users block pop-ups in their browsers, what’s the impact? Does the help cover all of the topics needed? On Lisa’s projects, help text tends to be a low priority, so it often doesn’t get done at all. That’s a business decision, but if you feel an area of the application needs extra help text or documentation, raise the issue to your team and your customers.\n\nReports\n\nAnother system component that’s often overlooked from a testing perspec- tive is reports. Reports are critical to many users for decision-making pur- poses but are often left until the very end, and either don’t get done or are poorly executed. Reports might be tailored to meet speciﬁc customer needs, but there are many third-party tools available for generating reports. Reports may be part of the application itself or be generated through a separate re- porting system for end users.\n\nWe discuss testing reports along with the other Quadrant 3 test activities in order to critique the product, but we recommend that you also write Quad- rant 2 report tests that will guide the coding and help the team understand the customer’s needs as it produces reports. They can certainly be written test-ﬁrst. Like documents, though, you need to look at a report to know if it’s easy enough to read and presents information in an understandable way.\n\n—Janet",
      "content_length": 2702,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Lisa’s Story\n\nSee Chapter 8, “Business-Facing Tests that Support the Team,” for in- formation about using thin slices.\n\nTESTING DOCUMENTS AND DOCUMENTATION\n\nOne of the biggest challenges when testing reports is not the formatting but getting the right data. When you try to create test data for reports, it can be dif- ﬁcult to get a good cross section of realistic data. It also is usually the edge cases that make the reports fail, so incorporating that extra data is not feasible. In most cases, it’s best to use production data (or data copied from the produc- tion system into a test environment) to test the different reporting variations.\n\nOur application includes a number of reports, many of which help companies meet governmental compliance requirements. While we have automated smoke tests for each report, any change to a report, or even an upgrade in the tool we use to generate reports, requires extensive manual and visual testing. We have to watch like hawks: Has a number been truncated by one character? Did a piece of text run over to the next page? Is the right data included? Wrong or missing data can mean trouble with the regulatory agency.\n\nAnother challenge is verifying the data contained in the report. If I were to use the same query that the report uses, it doesn’t prove anything. I sometimes struggle to come up with my own SQL queries to compare the actual data with what shows up on a report. We budget extra time to test reports, even the simple-looking ones.\n\nBecause reports are so subjective, we ﬁnd that different stakeholders have differ- ent preferences for how the data is presented. The plan administrator who has to explain a report to a user on the phone has a different idea of what’s easy to understand than the company lawyer who decides what data needs to be on the report. Our product owner helps us get consensus from all areas of the business.\n\nThe contents and formatting of a report are important, of course, but for online reports, the speed at which they come up is critical too. Our plan administrators wanted complete freedom to specify any date range for some transaction history reports. Our DBA, who coded the reports, warned that for a large company’s retirement plan, data for more than a few months worth of transactions could take several minutes to render. Over time, companies grew, they had more and more transactions, and eventually the user interface started timing out before it could deliver the report. When testing, try out worst-case scenarios, which could eventually become the most common scenario.\n\nIf you’re tackling a project that involves lots of reports, don’t give in to the temptation to leave them to the end. Include some reports in each iteration if you can. One report could be a single story or maybe even broken up into a couple of stories. Use mock-ups to help the customers decide on report con- tents and formatting. Find the “thin slice” or “critical path” in the report, code that ﬁrst, and show it to your customer before you add the next slice. Incre- mental development works as well with reports as it does with other software.\n\n209\n\n—Lisa",
      "content_length": 3137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "210\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nSometimes your customers themselves aren’t sure how a report should look or how to approach it incrementally. And sometimes nobody on the team anticipates how hard the testing effort will prove to be.\n\nLisa’s Story\n\nLike other ﬁnancial accounts, retirement plans need to provide periodic state- ments to account holders that detail all of the money going into and out of the ac- count. These statements show the change in value between the beginning and ending balances and other pertinent information, such as the names of account beneﬁciaries. Our company wanted to improve the account statements, both as a marketing tool and to reduce the number of calls from account holders who didn’t understand their statements.\n\nWe didn’t have access to our direct competitors’ account statements, so the product owner asked for volunteers to bring in account statements from banks and other ﬁnancial institutions in order to get ideas. Months of discussions and experimentation with mock-ups produced a new statement format, which included data that wasn’t on the report previously, such as performance results for each mutual fund.\n\nStories for developing the new account statement were distributed throughout two quarters worth of iterations. During the ﬁrst quarter, stories to collect new data were done. Testing proved much harder than we thought. We used FitNesse tests to verify capturing the different data elements, which lulled us into a false sense of security. It was hard to cover all of the variations, and we missed some with the automated tests. We also didn’t anticipate that the changes to collect new data could have an adverse effect on the data that already displayed on the existing statements.\n\nAs a result, we didn’t do adequate manual testing of the account statements. Sub- tle errors slipped past us. When the job to produce quarterly statements ran, calls started coming in from customers. We had a mad scramble to diagnose and ﬁx the errors in both code and data. The whole project was delayed by a quarter while we ﬁgured out better ways to test and added internal checks and better logging to the code.\n\nShort iterations mean that it can be hard to make time for adequate explor- atory testing and other Quadrant 3 activities. Let’s look at tools that might help speed up this testing and make time for vital manual and visual tests.\n\nTOOLS TO ASSIST WITH EXPLORATORY TESTING Exploratory testing is manual testing. Some of the best testing happens be- cause a person is paying attention to details that often get missed if we are fol-\n\n—Lisa",
      "content_length": 2639,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "See the bibliogra- phy for references to Jonathan Kohl’s writings on using human and auto- mation power to- gether for optimal testing.\n\nTOOLS TO ASSIST WITH EXPLORATORY TESTING\n\nlowing a script. Intuition is something that we cannot make a machine learn. However, there are many tools that can assist us in our quest for excellence.\n\nTools shouldn’t replace human interaction; they should enhance the experi- ence. Tools can provide testers with more power to ﬁnd the hard-to-repro- duce bugs that often get ﬁled away because no one can get a handle on them. Exploratory testing is unconventional, so why shouldn’t the tools be as well? Think about low-effort, high-value ways that tools can be incorporated into your testing.\n\nComputers are good at doing repetitive tasks and performing calculations. These are two areas where they are much better than humans, so let’s use them for those tasks. Because testing needs to keep pace with coding, any time advantage we can gain is a bonus.\n\nIn the next few sections, we’ll look at some areas where automation can le- verage exploratory testing. The ones we cover are test setup, test data genera- tion, monitoring, simulators, and emulators.\n\nTest Setup\n\nLet’s think about what we do when we test. We’ve just found a bug, but not one that is easily reproducible. We’re pretty sure it happens as a result of in- teractions between components. We go back to the beginning and try one scenario after another. Soon we’ve spent the whole day just trying to repro- duce this one bug.\n\nAsk yourself how you can make this easier. We’ve found that one of the most time-consuming tasks is the test setup and getting to the right starting point for your actual test. If you use session-based testing, then you already know how much time you spend setting up the test, because you have been track- ing that particular time waster. This is an excellent opportunity for some automation.\n\nThe tools used for business-facing tests that support the team described in Chapter 9 are also valuable for manual exploratory testing. Automated functional test scripts can be run to set up data and scenarios to launch ex- ploratory testing sessions. Tests conﬁgured to accept runtime parameters are particularly powerful for setting up a starting point for evaluating the product.\n\n211",
      "content_length": 2310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "212\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nLisa’s Story\n\nOur Watir test scripts all accept a number of runtime parameters. When I need a retirement plan with a speciﬁc set of options, and speciﬁc types of participants, I can kick off a Watir script or two with some variables set on the command line. When the scripts stop, I have a browser session with all of the data I need for test- ing already set up. This is so fast that I can test permutations I’d never get to using all-manual keystrokes.\n\nThe test scripts you use for functional regression testing and for guiding de- velopment aren’t the only tools that help take the tedium out of manual ex- ploratory testing. There are other tools to help set up test data as well as to help you evaluate the outputs of your testing sessions.\n\nWhatever tool you are using, think about how it can be adapted to run the scenario over and over with different inputs plugged in. Janet has also suc- cessfully used Ruby with Watir to set up tests to run multiple times to help identify bugs. Tools that drive the browser or UI in much the same way that an end user would makes your testing more reliable because you can play it back on your monitor and watch for anything that might not look as it should during the setup. When you get to the place where the test actually starts, you can then use your excellent testing abilities to track down the source of the bug.\n\nTest Data Generation\n\nPerlClip is an example of a tool that you can use to test a text ﬁeld with differ- ent kinds of inputs. James Bach provides it free of charge on his website, www.satisﬁce.com, and it can be very helpful in validating ﬁelds. For exam- ple, if you have a ﬁeld that will accept a maximum input of 200 characters, testing this ﬁeld and its boundaries manually would be very tedious. Use Per- lClip to create a string, put it in your automation library, and have your auto- mation tool call the string to test the value.\n\nMonitoring Tools\n\nTools like the Unix/Linux command tail -f, or James Bach’s LogWatch, can help monitor log ﬁles for error conditions. IDEs also provide log analysis tools. Many error messages are never displayed on the screen, so if you’re testing via the GUI, you never see them. Get familiar with tools like these, be- cause they can make your testing more effective and efﬁcient. If you are not\n\n—Lisa",
      "content_length": 2386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "See the ”System Test” example in Chapter 12, “Sum- mary of Testing Quadrants,” to see how a simulator was critical to the testing the whole system.\n\nTOOLS TO ASSIST WITH EXPLORATORY TESTING\n\nsure where your system logs warnings and errors, ask your developers. They probably have lots of ideas about how you can monitor the system.\n\nSimulators\n\nSimulators are tools used to create data that represent key characteristics and behavior of real data for the system under test. If you do not have access to real data for your system, simulated data will sometimes work almost as well. The other advantage of using a simulator is for pumping data into a system over time. It can be used to help generate error conditions that are difﬁcult to create under normal circumstances and can reduce time in boundary testing.\n\nSetting up data and test scenarios is half of the picture. You also need to have a way to watch the outcomes of your testing. Let’s consider some tools for that purpose.\n\nEmulators\n\nAn emulator duplicates the functionality of a system so that it behaves like the system under test. There are many reasons to use an emulator. When you need to test code that interfaces with other systems or devices, emulators are invaluable.\n\nTwo Examples of Emulators\n\nWestJet, a Canadian airline company, provides the capability for guests to use their mobile devices to check in at airports that support the feature. When testing this application, it is better for both the programmers and the testers to test various devices as early as possible. To make this feasible, they use downloadable emulators to test the Web Check-in application quickly and often during an iteration. Real devices, which are expensive to use, can then be used sparingly to verify already tested functionality.\n\nThe team also created another type of emulator to help test against the leg- acy system being interfaced with. The programmers on the legacy system have different priorities and delivery schedules, and a backlog of requests. To prevent this from holding up new development, the programmers on the web application have created a type of emulator for the API into the legacy system that returns predetermined values for speciﬁc API calls. They develop against this emulator, and when the real changes are available, they test and make any modiﬁcations then. This change in process has enabled them to move ahead much more quickly than was previously possible. It has proved to be a simple but very powerful tool.\n\n213",
      "content_length": 2503,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "214\n\nCHAPTER 10\n\n(cid:2) BUSINESS-FACING TESTS THAT CRITIQUE THE PRODUCT\n\nEmulators are one tool that helps to keep testing and coding moving together hand-in-hand. Using them is one way for testing to keep up with develop- ment in short iterations. As you plan your releases and iterations, think about the types of tools that might help with creating production-like test scenarios. See if you can use the tools you’re already using for automating tests to drive development as aids to exploratory testing.\n\nDriving development with tests is critical to any project’s success. However, we humans won’t always get all of the requirements for desired system be- havior entirely correct. Our business experts themselves can miss important aspects of functionality or interaction with other parts of the system when they provide examples of how a feature should work. We have to use tech- niques to help both the customer and developer teams learn more about the system so they can keep improving the product.\n\nSUMMARY A large part of the testing effort is spent critiquing the product from a busi- ness perspective. This chapter gave you some ideas about the types of tests you can do to make your testing efforts more effective.\n\n(cid:2) Demonstrate software to stakeholders in order to get early feedback\n\nthat will help direct building the right stuff.\n\n(cid:2) Use scenarios and workﬂows to test the whole system from end to\n\nend.\n\n(cid:2) Use exploratory testing to supplement automation and to take advan-\n\ntage of human intellect and perceptions.\n\n(cid:2) Without usability in mind when testing and coding, applications can become shelfware. Always be aware of how the system is being used.\n\n(cid:2) Testing behind the GUI is the most effective way of getting at the application functionality. Do some research to see how you can approach your application.\n\n(cid:2) Incorporate all kinds of tests to make a good regression suite. (cid:2) Don’t forget about testing documentation and reports. (cid:2) Automation tools can perform tedious and repetitive tasks, such as data and test scenario setup, and free up more time for important manual exploratory testing.\n\n(cid:2) Tools you’re already using to automate functional tests might also be\n\nuseful to leverage exploratory tests.",
      "content_length": 2284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "SUMMARY\n\n(cid:2) Monitoring, resource usage, and log analysis tools built into operat- ing systems and IDEs help testers appraise the application’s behavior. (cid:2) Simulators and emulators enable exploratory testing even when you\n\ncan’t duplicate the exact production environment.\n\n(cid:2) Even when tests are used to drive development, requirements for de- sired behavior or interaction with other systems can be missed or misunderstood. Quadrant 3 activities help teams keep adding value to the product.\n\n215",
      "content_length": 512,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "Chapter 11\n\nCRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nScalability\n\nPerformance and Load\n\nPerformance and Load Tools\n\nLoad Type Tests\n\nBaseline\n\nWhat Is It?\n\nMemory Management\n\nSecurity\n\nMaintainability\n\nCritique Product Using Technology-Facing Tests\n\nWho Does It?\n\nInteroperability\n\n“ility” Testing\n\nCompatibility\n\nReliability\n\nWhen Do You Do It?\n\nInstallability\n\nThis chapter is focused on the bottom right corner of our testing quadrant. We’ve looked at driving development with both business-facing and technology-facing tests. After the code is written, we are no longer driving the development but are looking at ways to critique the product. In the previous chapter, we examined ways to critique from a business point of view. Now we look at ways to critique from a technology-facing point of view. These tests are an important means of evaluating whether our product delivers the right business value.\n\nINTRODUCTION TO QUADRANT 4 Individual stories are pieces of the puzzle, but there’s more to an application than that. The technology-facing tests that critique the product are more\n\n217",
      "content_length": 1107,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "218\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nconcerned with the nonfunctional requirements than the functional ones. We worry about deﬁciencies in the product from a technical point of view. Rather than using the business domain language, we describe requirements using a programming domain vocabulary. This is the province of Quadrant 4 (see Figure 11-1).\n\nNonfunctional requirements include conﬁguration issues, security, perfor- mance, memory management, the “ilities” (e.g., reliability, interoperability, and scalability), recovery, and even data conversion. Not all projects are con- cerned about all of these issues, but it is a good idea to have a checklist to make sure the team thinks about them and asks the customer how important each one is.\n\nOur customer should think about all of the quality attributes and factors that are important and make informed trade-offs. However, many customers focus\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ2 Q3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 11-1 Quadrant 4 tests",
      "content_length": 1404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "Lisa’s Story\n\nINTRODUCTION TO QUADRANT 4\n\non the business side of the application and don’t understand the criticality of many nonfunctional requirements in their role of helping to deﬁne the level of quality needed for the product. They might assume that the development team will just take care of issues such as performance, reliability, and security.\n\nWe believe that the development team has a responsibility to explain the consequences of not addressing these nonfunctional or cross-functional re- quirements. We’re really all part of one product team that wants to deliver good value, and these technology-oriented factors might expose make-or- break issues.\n\nMany of these nonfunctional and cross-functional issues are deemed low- risk for many applications and so are not added to the test plan. However, when you are planning your project, you should think about the risks in each of these areas, address them in your test plan, and include the tools and re- sources needed for testing them in your project plan.\n\nIn the past, I’ve been asked by specialists in areas such as performance and secu- rity testing why they didn’t hear much about “ility” testing at agile conferences or in publications about agile development. Like Janet, I’ve always seen these areas of testing as critical, so this wasn’t my perception. But as I thought about it, I had to agree that this wasn’t a much-discussed topic at the time (although that’s changed recently).\n\nWhy would agile discussions not include such important considerations as load testing? My theory is that it’s because agile development is driven by customers, from user stories. Customers simply assume that software will be designed to properly accommodate the potential load, at a reasonable rate of performance. It doesn’t always occur to them to verbalize those concerns. If not asked to address them, programmers may or may not think to prioritize them. I believe that one area where testers have contributed greatly to agile teams is in bringing up ques- tions such as, “How many concurrent users should the application support?” and “What’s the average response time required?”\n\nBecause the types of testing in this quadrant are so diverse, we’ll give exam- ples of tools that might be helpful as we go along instead of a separate toolkit section. Tools, whether homegrown or acquired, are essential to succeed with Quadrant 4 testing efforts. Still, the people doing the work count, so let’s consider who on an agile team can perform these tests.\n\n219\n\n—Lisa",
      "content_length": 2525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "220\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nWHO DOES IT? All of the agile literature talks about teams being generalists; anyone should be able to pick up a task and do it. We know that isn’t always practical, but the idea is to be able to share the knowledge so that people don’t become silos of information.\n\nHowever, there are many tasks that need specialized knowledge. A good exam- ple is security testing. We’re not talking about security within an application, such as who has access rights to administer it. Because that type of security is really part of the functional requirements and will be covered by regular sto- ries, verifying that it works falls within the ﬁrst three quadrants. We’re talking about probing for external security ﬂaws and knowing the types of vulnerabili- ties in systems that hackers exploit. That is a specialized skill set.\n\nPerformance testing can be done by testers and programmers collaborating and building simple tools for their speciﬁc needs. Some organizations pur- chase load-testing tools that require team members who specialize in that tool to build the scripts and analyze and interpret the results. It can be difﬁ- cult for a software development organization, especially a small one, to have enough resources to duplicate an accurate production-level load for a test, so external providers of performance testing may be needed.\n\nChapter 15, “Tester Activities in Release or Theme Planning,” ex- plains how to plan to work with ex- ternal teams.\n\nLarger organizations may have groups such as database experts that your team can use to help with data conversion, security groups that will help you identify risks to your application, or a production support team that can help you test recovery or failover. Build a close relationship with these specialists. You’ll need to work together as a virtual team to gather the information you need about your product.\n\nThe more diverse the skill sets are in your team, the less likely you are to need outside consultants to help you. Identify the resources you need for each project. Many teams ﬁnd that a good technical tester or toolsmith can take on many of these tasks. If someone already on the team can learn whatever special- ized knowledge is required, great; otherwise, bring in the expertise you need.\n\nSkills within the Team\n\nJason Holzer, Product Owner for Property Testing (performance, security, sta- bility, and reliability) at Ultimate Software, tells us that a good programmer can write a multithreaded engine to call a function concurrently and test per- formance. Jason feels that agile teams do have the skills to do their own per- formance testing; they just may not realize it.",
      "content_length": 2726,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "WHO DOES IT?\n\nPerformance testing does require a controlled, dedicated environment. Some specialized tools are needed, such as a proﬁler to measure code per- formance. But, in Jason’s view, performance, stability, scalability, and reliability (PSR) tests can, and should, be done at the unit level. There’s a mind-set that holds that these tests are too complex and require specialists when in fact the teams do possess the necessary skills.\n\nJason ﬁnds that awareness of the “PSR” aspects of code needs to be part of the team’s culture.\n\nIf stakeholders place a high priority on performance, stability, scalability, and the like, Jason recommends that the team talk about ways to verify these aspects of the application. When teams understand the priority of qualities such as performance and reliability, they ﬁgure out how to improve their code to ensure them. They don’t need to depend on an outside, specialized team. Jason explains his viewpoint.\n\nThe potential resistance I see today to this plan is that someone believes that programmers don’t know how to PSR test and that there will need to be a great deal of training. In my opinion, a more accurate statement is that programmers are not aware that PSR testing is a high priority and a key to quality. I don't think it has anything to do with knowing how to PSR test. PSR testing is a combination of math, science, analysis, program- ming, and problem solving. I am willing to bet that if you conducted a competition at any software development organization where you asked every team to implement a tree search algorithm, and the team with the fastest algorithm would win, that every team will do PSR testing and pro- vide PSR metrics without teaching them anything new.\n\nPSR testing is really just telling me “How fast?” (performance), “How long?” (stability), “How often?” (reliability), and “How much?” (scalabil- ity). So, as long as the awareness is there and the organization is seri- ously asking those questions with everything they develop, then PSR testing is successfully integrated into a team.\n\nTake a second look at the skills that your team already possesses, and brain- storm about the types of “ility” testing that can be done with the resources you already have. If you need outside teams, plan for that in your release and iteration planning.\n\nRegardless of whether or not your team brings in additional resources for these types of tests, your team is still responsible for making sure the mini- mum testing is done. The information these tests provide may result in new stories and tasks in areas such as changing the architecture for better scalabil- ity or implementing a system-wide security solution. Be sure to complete the feedback loop from tests that critique the product to tests that drive changes that will improve the nonfunctional aspects of the product.\n\n221",
      "content_length": 2855,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "222\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nJust because this is the fourth out of four agile testing quadrants doesn’t mean these tests come last. Your team needs to think about when to do per- formance, security, and “ility” tests so that you ensure your product delivers the right business value.\n\nWHEN DO YOU DO IT? As with functional testing, the sooner technology-facing tests that support the team are completed, the cheaper it is to ﬁx any issues that are found. However, many of the cross-functional tests are expensive and hard to do in small chunks.\n\nTechnical stories can be written to address speciﬁc requirements, such as: “As user Abby, I need to retrieve report X in less than 20 seconds so that I can make a decision quickly.” This story is about performance and requires spe- cialized tests to be written, and it can be done along with the story to code the report, or in a later iteration.\n\nConsider a separate row on your story board for tasks needed by the product as a whole. Lisa’s team uses this area to put cards such as “Evaluate load test tools” or “Establish a performance test baseline.” Janet has successfully used different colored cards to show that the story is meant for one of the expert roles borrowed from other areas of the organization.\n\nSome performance tests might need to wait until much of the application is built if you are trying to baseline full end-to-end workﬂows. If performance and reliability are a top priority, you need to ﬁnd a way to test those early in the project. Prioritize stories so that a steel thread or thin slice is complete early. You should be able to create a performance test that can be run and continue to run as you add more and more functionality to the workﬂow. This may enable you to catch performance issues early and redesign the sys- tem architecture for improvements. For many applications, correct function- ality is irrelevant without the necessary performance.\n\nThe time to think about your nonfunctional tests is during release or theme planning. Plan to start early, tackling small increments as needed. For each iteration, see what tasks your team needs in order to determine whether the code design is reliable, scalable, usable, and secure. In the next section, we’ll look at some different types of Quadrant 4 tests.",
      "content_length": 2340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "“ILITY” TESTING\n\nPerformance Testing from the Start\n\nKen De Souza, a software developer/tester at NCR [2008], responded to a question on the agile-testing mailing list about when to do stress and perfor- mance testing in an agile project with an explanation of how he approaches performance testing.\n\nI'd suggest designing your performance tests from the start. We build data from the ﬁrst iteration, and we run a simple performance test to make sure it all holds together. This is more to see that the functionality of the performance scripts holds together.\n\nI used JMeter because I can hook FTP, SOAP, HTTP, RegEx, and so on, all from a few threads, with just one instance running. I can test out my calls right from the start (or at least have the infrastructure in place to do it).\n\nMy eventual goal is that when the product is close to releasing, I don't have to nurse the performance test; I just have to crank up the threads and let go. All my metrics and tasks have already been tested out for months, so I'm fairly certain that anyone can run my performance test.\n\nPerformance testing can be approached using agile principles to build the tools and test components incrementally. As with software features, focus on getting the performance information you need, one small chunk at a time.\n\n“ILITY” TESTING If we could just focus on the desired behavior and functionality of the appli- cation, life would be so simple. Unfortunately, we have to be concerned with qualities such as security, maintainability, interoperability, compatibility, re- liability, and installability. Let’s take a look at some of these “ilities.”\n\nSecurity\n\nOK, it doesn’t end in -ility, but we include it in the “ility” bucket because we use technology-facing tests to appraise the security aspects of the product. Security is a top priority for every organization these days. Every organiza- tion needs to ensure the conﬁdentiality and integrity of their software. They want to verify concepts such as no repudiation, a guarantee that the message has been sent by the party that claims to have sent it and received by the party that claims to have received it. The application needs to perform the correct authentication, conﬁrming each user’s identity, and authorization, in order\n\n223",
      "content_length": 2272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "224\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nto allow the user access only to the services they’re authorized to use. Testing so many different aspects of security isn’t easy.\n\nIn the rush to deliver functionality, both business experts and development teams in newly started organizations may not be thinking of security ﬁrst. They just want to get some software working so they can do business. Autho- rization is often the only aspect of security testing that they consider as part of business functionality.\n\nLisa’s Story\n\nMy current team is a case in point. The business was interested in automating func- tionality to manage 401(k) plans. They did take pains to secure the software and data, but it wasn’t a testing priority. When I “got religion” after hearing some good presentations about security testing at conferences, I bought a book on security testing and started hacking around on the site. I found some serious issues, which we ﬁxed, but we realized we needed a comprehensive approach to ensuring security. We wrote stories to implement this. We also started including a “security” task card with every story so that we’d be mindful of security needs while devel- oping and testing.\n\nBudgeting this type of work has to be a business priority. There’s a range of alternatives available, depending on your company’s priorities and resources. Understand your needs and the risks before you invest a lot of time and energy.\n\nJanet’s Story\n\nOne team that I worked with has a separate corporate security team. Whenever functionality is added to the application that might expose a security ﬂaw, the cor- porate team runs the application through a security test application and produces a report for the team. It performs static testing using a canned black-box probe on the code and has exposed a few weak areas that the developers were able to address. It does not give an overall picture of the security level for the application, but that was not deemed a major concern.\n\nTesters who are skilled in security testing can perform security risk-based testing, which is driven by analyzing the architectural risk, attack patterns, or abuse and misuse cases. When specialized skills are required, bring in what you need, but the team is still responsible for making sure the testing gets done.\n\n—Lisa\n\n—Janet",
      "content_length": 2351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "See http://en .wikipedia.org/wiki/ Buffer_overﬂow and http://en .wikipedia.org/wiki/ Format_string_ vulnerabilities for more information.\n\nSee http://en .wikipedia.org/wiki/ List_of_tools_for_ static_code_analysis for a list of tools that can be used for static code analysis.\n\nMore resources on this subject can be found at: www .fuzzing.org/ category/fuzzing- book/ and www .fuzzing.org/fuzzing- software\n\n“ILITY” TESTING\n\nThere are a variety of automated tools to help with security veriﬁcation. Static analysis tools, which can examine the code without executing the ap- plication, can detect potential security ﬂaws in the code that might not oth- erwise show up for years. Dynamic analysis tools, which run in real time, can test for vulnerabilities such as SQL injection and cross-site scripting. Manual exploratory testing by a knowledgeable security tester is indispensable to de- tect issues that automated tests can miss.\n\nSecurity Testing Perspectives\n\nSecurity testing is a vast topic on its own. Grig Gheorghiu shares some high- lights about resources that can help agile teams with security testing.\n\nJust like functional testing, security testing can be done from two per- spectives: from the inside out (white-box testing) and from the outside in (black-box testing). Inside-out security testing assumes that the source code for the application under test is available to the testers. The code can be analyzed statically with a variety of tools that try to discover com- mon coding errors that can make the application vulnerable to attacks such as buffer overﬂows or format string attacks.\n\nThe fact that the testers have access to the source code of the applica- tion also means that they can map what some books call \"the attack sur- face\" of the application, which is the list of all of the inputs and resources used by the program under test. Armed with a knowledge of the attack surface, testers can then apply a variety of techniques that attempt to break the security of the application. A very effective class of such tech- niques is called fuzzing and is based on fault injection. Using this tech- nique, the testers try to make the application fail by feeding it various types of inputs (hence the term fault injection). These inputs can be care- fully crafted strings used in SQL injection attacks, random byte changes in given input ﬁles, or random strings fed as command line arguments.\n\nThe outside-in approach is the one mostly used by attackers who try to penetrate into the servers or the network hosting your application. As a security tester, you need to have the same mind-set that attackers do, which means that you have to use your creativity in discovering and ex- ploiting vulnerabilities in your own application. You also need to stay up- to-date with the latest security news and updates related to the platform/ operating system your application runs on, which is not an easy task.\n\nSo what are agile testers to do when faced with the apparently insur- mountable task of testing the security of their application? Here are some practical, pragmatic steps that anybody can follow:\n\n1. Adopt a continuous integration (CI) process that periodically runs a suite of automated tests against your application.\n\n225",
      "content_length": 3253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "226\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\n2. Learn how to use one or more open source static code analysis tools. Add a step to your CI process that consists of running these tools against your application code. Mark the step as failed if the tools ﬁnd any critical vulnerabilities.\n\n3. Install an automated security vulnerability scanner such as Nessus (http://www.nessus.org/nessus/). Nessus can be run in a command- line, non-GUI mode, which makes it suitable for inclusion in a CI tool. Add a step to your CI process that consists of running Nessus against your application. Capture the Nessus output in a ﬁle and parse that ﬁle for any high-importance security holes found by the scanner. Mark the step as failed when any such holes are found.\n\n4. Learn how to use one or more open source fuzzing tools. Add a step to your CI process that consists of running these tools against your application code. Mark the step as failed if the tools ﬁnd any critical vulnerabilities.\n\nAs with any automated testing effort, running these tools is no guarantee that your code and your application will be free of security defects. However, running these tools will go a long way toward improving the quality of your application in terms of security. As always, the 80/20 rule applies. These tools will probably ﬁnd the 80% most common security bugs out there while requiring 20% of your security budget.\n\nTo ﬁnd the remaining 20% of the security defects, you're well advised to spend the other 80% of your security budget on high-quality secu- rity experts. They will be able to test your application security thor- oughly by the use of techniques such as SQL injection, code injection, remote code inclusion, and cross-site scripting. While there are some tools that try to automate some of these techniques, they are no match for a trained professional who takes the time to understand the inner workings of your application in order to craft the perfect attack against it.\n\nSecurity testing can be intimidating, so budget time to adopt a hacker mind-set and decide on the right approach to the task at hand. Use the resources Grig suggests to educate yourself. Take advantage of these tools and techniques in order to achieve security tests with a reasonable return on investment.\n\nJust this brief look at security testing shows why specialized training and tools are so important to do a good job of it. For most organizations, this testing is absolutely required. One security intrusion might be enough to take a company out of business. Even if the probability were low, the stakes are too high to put off these tests.",
      "content_length": 2653,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "“ILITY” TESTING\n\nCode that costs a lot to maintain might not kill an organization’s proﬁtability as quickly as a security breach, but it could lead to a long, slow death. In the next section we consider ways to verify maintainability.\n\nMaintainability\n\nMaintainability is not something that is easy to test. In traditional projects, it’s often done by the use of full code reviews or inspections. Agile teams of- ten use pair programming, which has built-in continual code review. There are other ways to make sure the code and tests stay maintainable.\n\nWe encourage development teams to develop standards and guidelines that they follow for application code, the test frameworks, and the tests them- selves. Teams that develop their own standards, rather than having them set by some other independent team, will be more likely to follow them because they make sense to them.\n\nThe kinds of standards we mean include naming conventions for method names or test names. All guidelines should be simple to follow and make maintainability easier. Examples are: “Success is always zero and failure must be a negative value,” “Each class or module should have only one single re- sponsibility,” or “All functions must be single entry, single exit.”\n\nStandards for developing the GUI also make the application more testable and maintainable, because testers know what to expect and don’t need to wonder whether a behavior is right or wrong. It also adds to testability if you are automating tests from the GUI. Simple standards such as, “Use names for all GUI objects rather than defaulting to the computer assigned identiﬁer” or “You cannot have two ﬁelds with the same name on a page” help the team achieve a level where the code is maintainable, as are the automated tests that provide coverage for it.\n\nMaintainable code supports shared code ownership. It is much easier for a programmer to move from one area to another if all code is written in the same style and easily understood by everyone on the team. Complexity adds risk and also makes code harder to understand. The XP value of simplicity should be applied to code. Simple coding standards can also include guide- lines such as, “Avoid duplication—Don’t copy-paste methods.” These same concepts apply to test frameworks and the tests themselves.\n\nMaintainability is an important factor for automated tests as well. Test tools have lagged behind programming tools in features that make them easy to\n\n227",
      "content_length": 2459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "228\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nmaintain, such as IDE plug-ins to make writing and maintaining test scripts simpler and more efﬁcient. That’s changing fast, so look for tools that pro- vide easy refactoring and search-and-replace, and for other utilities that make it easy to modify the scripts.\n\nDatabase maintainability is also important. The database design needs to be ﬂexible and usable. Every iteration might bring tasks to add or remove tables, columns, constraints, or triggers, or to do some kind of data conversion. These tasks become a bottleneck if the database design is poor or the data- base is cluttered with invalid data.\n\nLisa’s Story\n\nA serious regression bug went undetected and caused production problems. We had a test that should have caught the bug. However, a constraint was missing from the schema used by the regression suite. Our test schemas had grown hap- hazardly over the years. Some had columns that no longer existed in the produc- tion schema. Some were missing various constraints, triggers, and indices. Our DBA had to manually make changes to each schema as needed for each story instead of running the same script in each schema to update it. We budgeted time over several sprints to recreate all of the test schemas so that they were identical and also matched production.\n\nPlan time to evaluate the database’s impact on team velocity, and refactor it just as you do production and test code. Maintainability of all aspects of the application, test, and execution environments is more a matter of assessment and refactoring than direct testing. If your velocity is going down, is it be- cause parts of the code are hard to work on, or is it that the database is difﬁ- cult to modify?\n\nInteroperability\n\nInteroperability refers to the capability of diverse systems and organizations to work together and share information. Interoperability testing looks at end-to-end functionality between two or more communicating systems. These tests are done in the context of the user—human or a software applica- tion—and look at functional behavior.\n\nIn agile development, interoperability testing can be done early in the devel- opment cycle. We have a working, deployable system at the end of each itera- tion so that we can deploy and set up testing with other systems.\n\n—Lisa",
      "content_length": 2355,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "In Chapter 20, “Successful Deliv- ery,” we discuss more about the importance of this level of testing.\n\n“ILITY” TESTING\n\nQuadrant 1 includes code integration tests, which are tests between compo- nents, but there is a whole other level of integration tests in enterprise systems. You might ﬁnd yourself integrating systems through open or proprietary inter- faces. The API you develop for your system might enable your users to easily set up a framework for them to test easily. Easier testing for your customer makes for faster acceptance.\n\nIn one project Janet worked on, test systems were set up at the customer’s site so that they could start to integrate them with their own systems early. Inter- faces to existing systems were changed as needed and tested with each new deployment.\n\nIf the system your team works on has to work together with external systems, you may not be able to represent them all in your test environments except with stubs and drivers that simulate the behavior of the other systems or equipment. This is one situation where testing after development is complete might be unavoidable. You might have to schedule test time in a test environ- ment shared by several teams.\n\nConsider all of the systems with which yours needs to communicate, and make sure you plan ahead to have an appropriate environment for testing them together. You’ll also need to plan resources for testing that your applica- tion is compatible with the various operating systems, browsers, clients, serv- ers, and hardware with which it might be used. We’ll discuss compatibility testing next.\n\nCompatibility\n\nThe type of project you’re working on dictates how much compatibility test- ing is required. If you have a web application and your customers are world- wide, you will need to think about all types of browsers and operating systems. If you are delivering a custom enterprise application, you can prob- ably reduce the amount of compatibility testing, because you might be able to dictate which versions are supported.\n\nAs each new screen is developed as part of a user interface story, it is a good idea to check its operability in all supported browsers. A simple task can be added to the story to test on all browsers.\n\nOne organization that Janet worked at had to test compatibility with reading software for the visual impaired. Although the company had no formal test\n\n229",
      "content_length": 2387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "230\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nlab, it had test machines available near the team area for easy access. The testers made periodic checks to make sure that new functionality was still compatible with the third-party tools. It was easy to ﬁx problems that were discovered early during development.\n\nHaving test machines available with different operating systems or browsers or third-party applications that need to work with the system under test makes it easier for the testers to ensure compatibility with each new story or at the end of an iteration. When you start a new theme or project, think about the resources you might need to verify compatibility. If you’re starting on a brand new product, you might have to build up a test lab for it. Make sure your team gets information on your end users’ hardware, operating systems, browsers, and versions of each. If the percentage of use of a new browser version has grown large enough, it might be time to start including that version in your compatibility testing.\n\nWhen you select or create functional test tools, make sure there’s an easy way to run the same script with different versions of browsers, operating systems, and hardware. For example, Lisa’s team could use the same suite of GUI regression tests on each of the servers running on Windows, Solaris, and Linux. Func- tional test scripts can also be used for reliability testing. Let’s look at that next.\n\nReliability\n\nReliability of software can be referred to as the ability of a system to perform and maintain its functions in routine circumstances as well as unexpected circumstances. The system also must perform and maintain its functions with consistency and repeatability. Reliability analysis answers the question, “How long will it run before it breaks?” Some statistics used to measure reli- ability are:\n\n(cid:2) Mean time to failure: The average or mean time between initial oper- ation and the ﬁrst occurrence of a failure or malfunction. In other words, how long can the system run before it fails the ﬁrst time?\n\n(cid:2) Mean time between failures: A statistical measure of reliability, this is calculated to indicate the anticipated average time between failures. The longer the better.\n\nIn traditional projects, we used to schedule weeks of reliability testing that tried to run simulations that matched a regular day’s work. Now, we should be able to deliver at the end of every iteration, so how can we schedule reli- ability tests?",
      "content_length": 2515,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "“ILITY” TESTING\n\nWe have automated unit and acceptance tests running on a regular basis. To do a reliability test, we simply need to use those same tests and run them over and over. Ideally, you would use statistics gathered that show daily usage, cre- ate a script that mirrors the usage, and run it on a stable build for however long your team thinks is adequate to prove stability. You can input random data into the tests to simulate production use and make sure the application doesn’t crash because of invalid inputs. Of course, you might want to mirror peak usage to make sure that it handles busy times as well.\n\nYou can create stories in each iteration to develop these scripts and add new functionality as it is added to the application. Your acceptance tests could be very speciﬁc such as, “Functionality X must perform 10,000 operations in a 24-hour period for a minimum of 3 days.”\n\nBeware: Running a thousand tests without any serious problems doesn’t mean you have reliable software. You have to run the right tests. To make a reliability test effective, think about your application and how it is used all day, every day, over a period of time. Specify tests that are aimed at demon- strating that your application will be able to meet your customers’ needs, even during peak times.\n\nAsk the customer team for their reliability criteria in the form of measurable goals. For example, they might consider the system reliable if ten or fewer er- rors occur for every 10,000 transactions, or the web application is available 99.999% of the time. Recovery from power outages and other disasters might be part of the reliability objectives, and will be stated in the form of Service Level Agreements. Know what they are. Some industries have their own soft- ware reliability standards and guidelines.\n\nDriving development with the right programmer and customer tests should enhance the application’s reliability, because this usually leads to better de- sign and fewer defects. Write additional stories and tasks as needed to deliver a system that meets the organization’s reliability standards.\n\nYour product might be reliable after it’s up and running, but it also needs to be installable by all users, in all supported environments. This is another area where following agile principles gives us an advantage.\n\nInstallability\n\nOne of the cornerstones of a successful agile team is continuous integration. This means that a build is ready for testing anytime during the day. Many\n\n231",
      "content_length": 2496,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "232\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nteams choose to deploy one or more of the successful builds into test envi- ronments on a daily basis.\n\nAutomating the deployment creates repeatability and makes deployment a non-event. This is exciting to us because we have experienced weeks of trying to integrate and install a new system. We know that if we build once and de- ploy the same build to multiple environments, we have developed consistency.\n\nJanet’s Story\n\nOn one project I worked on, the deployment was automatic and was tested on multiple environments in the development cycle. However, there were issues when deploying to the customer site. We added a step to the end game so that the support group would take the release and do a complete install test as if it were the customer’s site. We were able to walk through the deployment notes and eliminated many of the issues the customer would have otherwise seen.\n\nChapter 20, “Suc- cessful Delivery,” has more on instal- lation testing.\n\nAs with any other functionality, risks associated with installation need to be evaluated and the amount of testing determined accordingly. Our advice is to do it early and often, and automate the process if possible.\n\n“ility” Summary\n\nThere are other “ilities” to test, depending on your product’s domain. Safety- critical software, such as that used in medical devices and aircraft control sys- tems, requires extensive safety testing, and the regression tests probably would contain tests related to safety. System redundancy and failover tests would be especially important for such a product. Your team might need to look at industry data around software-related safety issues and use extra code reviews. Conﬁgurability, auditability, portability, robustness, and extensibil- ity are just a few of the qualities your team might need to evaluate with tech- nology-facing tests.\n\nWhatever “ility” you need to test, use an incremental approach. Start by elic- iting the customer team’s requirements and examples of their objectives for that particular area of quality. Write business-facing tests to make sure the code is designed to meet those goals. In the ﬁrst iteration, the team might do some research and come up with a test strategy to evaluate the existing qual- ity level of the product. The next step might be to create a suitable test envi- ronment, to research tools, or to start with some manual tests.\n\n—Janet",
      "content_length": 2459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "Janet’s Story\n\nPERFORMANCE, LOAD, STRESS, AND SCALABILITY TESTING\n\nAs you learn how the application measures up to the customers’ require- ments, close the loop with new Quadrant 1 and 2 tests that drive the applica- tion closer to the goals for that particular property. An incremental approach is also recommended for performance, load, and other tests that are ad- dressed in the next section.\n\nPERFORMANCE, LOAD, STRESS, AND SCALABILITY TESTING Performance, load, stress, and scalability testing all fall into Quadrant 4 be- cause of their technology focus. Often specialized skills are required, al- though many teams have ﬁgured out ways to do their own testing in these areas. Let’s talk about scalability ﬁrst, because it is often forgotten.\n\nScalability\n\nScalability testing veriﬁes the application remains reliable when more users are added. What that really means is, “Can your system handle the capacity of a growing customer base?” It sounds simple, but really isn’t, and is a problem that an agile team usually can’t solve by itself.\n\nIt is important to think about the whole system and not just the application itself. For example, the network is often the bottleneck, because it can’t han- dle the increased throughput. What about the database? Will it scale? Will the hardware you are using handle the new loads being considered? Is it simple just to add new hardware, or is it the bottleneck?\n\nIn one organization I was recently working in, their customer base had grown very quickly, and the solution they had invested in had reached its capacity due to hardware constraints. It was not a simple matter of adding a new server, because the solution was not designed that way. The system needed to be monitored to restart services during peak usage.\n\nTo grow, the organization had to actually change solutions to accommodate its future growth, but this was not recognized until problems started to happen.\n\nIdeally, the organization would have replaced the old system before it was an issue. This is an example of why it is important to understand your system and its capability, as well as future growth projections.\n\n233\n\n—Janet",
      "content_length": 2147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "234\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nYou will need to go outside the team to get the answers you require to address scalability issues, so plan ahead.\n\nPerformance and Load Testing\n\nPerformance testing is usually done to help identify bottlenecks in a system or to establish a baseline for future testing. It is also done to ensure compli- ance with performance goals and requirements, and to help stakeholders make informed decisions related to the overall quality of the application be- ing tested.\n\nLoad testing evaluates system behavior as more and more users access the system at the same time. Stress testing evaluates the robustness of the appli- cation under higher-than-expected loads. Will the application scale as the business grows? Characteristics such as response time can be more critical than functionality for some applications.\n\nGrig Gheorghiu [2005] emphasizes the need for clearly deﬁned expectations to get value from performance testing. He says, “If you don’t know where you want to go in terms of the system, then it matters little which direction you take (remember Alice and the Cheshire Cat?).” For example, you probably want to know the number of concurrent users and the acceptable response time for a web application.\n\nPerformance and Load-Testing Tools\n\nSee the bibliogra- phy for links to sites where you can research tools.\n\nAfter you’ve deﬁned your performance goals, you can use a variety of tools to put a load on the system and check for bottlenecks. This can be done at the unit level, with tools such as JUnitPerf, httperf, or a home-grown harness. Apache JMeter, The Grinder, Pounder, ftptt, and OpenWebLoad are more examples of the many open source performance and load test tools available at the time of this writing. Some of these, such as JMeter, can be used on a variety of server types, from SOAP to LDAP to POP3 mail. Plenty of com- mercial tool options are available too, including NeoLoad, WebLoad, eValid LoadTest, LoadRunner, and SOATest.\n\nUse these tools to look for performance bottlenecks. Lisa’s team uses JProﬁler to look for application bottlenecks and memory leaks, and JConsole to analyze database usage. Similar tools exist for .NET and other environments, including .NET Memory Proﬁler and ANTS Proﬁler Pro. As Grig points out, there are database-speciﬁc proﬁlers to pinpoint performance issues at the database level; ask your database experts to work with you. Your system administrators can",
      "content_length": 2496,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "PERFORMANCE, LOAD, STRESS, AND SCALABILITY TESTING\n\nhelp you use shell commands such as top, or tools such as PerfMon to monitor CPU, memory, swap, disk I/O, and other hardware resources. Similar tools are available at the network level, for example, NetScout.\n\nYou can also use the tools the team is most familiar with. In one project, Janet worked very closely with one of the programmers to create the tests. She helped him to deﬁne the tests needed based on customer’s performance and load expectations, and he automated them using JUnit. Together they analyzed the results to report back to the customer.\n\nEstablishing a baseline is a good ﬁrst step for evaluating performance. The next section explores this aspect of performance testing.\n\nBaseline\n\nPerformance tuning can turn into a big project, so it is essential to provide a baseline that you can compare against new versions of the software on perfor- mance. Even if performance isn’t your biggest concern at the moment, don’t ignore it. It’s a good idea to get a performance baseline so that you know later which direction your response time is headed. Lisa’s company hosts a website that has had a small load on it. They got a load test baseline on the site so that as it grew, they’d know how performance was being affected.\n\nPerformance Baseline Test Results\n\nLisa’s coworker Mike Busse took on the task of obtaining performance base- lines for their web application that manages retirement plans. He evaluated load test tools, implemented one (JMeter), and set about to get a baseline. He reported the results both in a high-level summary and a spreadsheet with detailed results.\n\nThe tests simulated slowly increasing the load up to 100 concurrent users. Three test scripts, each for a common user activity, were used, and they were run separately and all together. Data gathered included:\n\nMaximum time of a transaction\n\nMaximum number of busy connections.\n\nA plot of the max time of a transaction against the number of users (see Figure 11-2 for an example of a chart)\n\nNumber of users who were on the system when the max time of a transaction equaled eight seconds\n\n235",
      "content_length": 2140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "236\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nAll Three Tests Run Together\n\n50000\n\n45000\n\n) s d n o c e s i l l i\n\nM n i ( e m i T\n\n40000\n\n35000 30000 25000 20000\n\n15000\n\n10000\n\nAverage Max Poly. (Max)\n\n5000\n\n0\n\n3\n\n10\n\n20\n\n40\n\n60\n\n80\n\n100\n\n# of Users\n\nFigure 11-2 Max and average transaction times at different user loads.\n\nAn important aspect of reporting results was providing deﬁnitions of terms such as transaction and connection in order to make the results meaningful to everyone. For example, maximum time of a transaction is deﬁned as the longest transaction of all transactions completed during the test.\n\nMike’s report also included assumptions made for the performance test:\n\nEight seconds is a transaction threshold that we would not like to cross.\n\nThe test web server is equivalent to either of the two web servers in production.\n\nThe load the system can handle, as determined by these tests, can be doubled in production because the load is distributed between two web servers.\n\nThe distribution of tasks in the test that combines all three tests is accu- rate to a reasonable degree.\n\nMike also identiﬁed shortcomings with the performance baseline. More than one transaction can contribute to loading a page, meaning that the max page load time could be longer than the max time of a transaction. The test machine doesn’t duplicate the production environment, which has two machines and load-balancing software to distribute the transactions.\n\nThe report ended with a conclusion about the number of concurrent users that the production system could support. This serves as a guideline to be aware of as the production load increases. The current load is less than half of this number, but there are unknowns, such as whether the production users are all active or have neglected to log out.",
      "content_length": 1839,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "PERFORMANCE, LOAD, STRESS, AND SCALABILITY TESTING\n\nMake sure your performance tests adequately mimic production conditions. Make results meaningful by deﬁning each test and metric, explaining how the results correlate to the production environment and what can be done with the results, and providing results in graphical form.\n\nIf there are speciﬁc performance criteria that have been deﬁned for speciﬁc functionality, we suggest that performance testing be done as part of the iter- ation to ensure that issues are found before it is too late to ﬁx them.\n\nBenchmarking can be done at any time during a release. If new functionality is added that might affect the performance, such as complicated queries, re- run the tests to make sure there are no adverse effects. This way, you have time to optimize the query or code early in the cycle when the development team is still familiar with the feature.\n\nAny performance, load, or stress test won’t be meaningful unless it’s run in an environment that mimics the production environment. Let’s talk more about environments.\n\nTest Environments\n\nFinal runs of the performance tests will help customers make decisions about accepting their product. For accurate results, tests need to be run on equip- ment that is similar to that of production. Often teams will use smaller ma- chines and extrapolate the results to decide if the performance is sufﬁcient for the business needs. This should be clearly noted when reporting test results.\n\nStressing the application to see what load it can take before it crashes can also be done anytime during the release, but usually it is not considered high-priority by customers unless you have a mission-critical system with lots of load.\n\nOne resource that is affected by increasing load is memory. In the next sec- tion, we discuss memory management.\n\nMemory Management\n\nMemory is usually described in terms of the amount (normally the mini- mum or maximum) of memory to be used for RAM, ROM, hard drives, and so on. You should be aware of memory usage and watch for leaks, because they can cause catastrophic failures when the application is in production during peak usage. Some programming languages are more susceptible to memory issues, so understanding the strengths and weaknesses of the code\n\n237",
      "content_length": 2291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "238\n\nCHAPTER 11\n\n(cid:2) CRITIQUING THE PRODUCT USING TECHNOLOGY-FACING TESTS\n\nwill assist you in knowing what to watch for. Testing for memory issues can be done as part of performance, load, and stress testing.\n\nGarbage collection is one tool used to release memory back to the program. However, it can mask severe memory issues. If you see the available memory steadily decreasing with usage and then all of a sudden increasing to maxi- mum available, you might suspect the garbage collection has kicked in. Watch for anomalies in the pattern or whether the system starts to get slow under heavy usage. You may need to monitor for a while and work with the pro- grammers to ﬁnd the issue. The ﬁx might be something simple, such as sched- uling the garbage collection more often or setting the trigger level higher.\n\nWhen you are working with the programmers on a story, ask them if they ex- pect problems with memory. You can test speciﬁcally if you know there might be a risk in the area. Watching for memory leaks is not always easy, but there are tools to help. This is an area where programmers should have tools easily available. Collaborate with them to verify that the application is free of memory issues. Perform the performance and load tests described in the pre- vious section to verify that there aren’t any memory problems.\n\nYou don’t have to be an expert on how to do technology-facing testing that critiques the product to help your team plan for it and execute it. Your team can evaluate what tests it needs from this quadrant. Talk about these tests as you plan your release; you can create a test plan speciﬁcally for performance and load if you’ve not done it before. You will need time to obtain the exper- tise needed, either by acquiring it through identifying and learning the skills, or by bringing in outside help. As with all development efforts, break tech- nology-facing tests into small tasks that can be addressed and built upon each iteration.\n\nSUMMARY In this chapter, we’ve explored the fourth agile testing quadrant, the technology- facing tests that critique the product.\n\n(cid:2) The developer team should evaluate whether it has, or can acquire,\n\nthe expertise to do these tests, or if it needs to plan to bring in external resources.\n\n(cid:2) An incremental approach to these tests, completing tasks in each iter- ation, ensures time to address any issues that arise and avoid produc- tion problems.",
      "content_length": 2441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "SUMMARY\n\n(cid:2) The team should consider various types of “ility” testing, including security, maintainability, interoperability, compatibility, reliability, and installability testing, and should execute these tests at appropriate times.\n\n(cid:2) Performance, scalability, stress, and load testing should be done from\n\nthe beginning of the project.\n\n(cid:2) Research the memory management issues that might impact your product, and plan tests to verify the application is free of memory issues.\n\n239",
      "content_length": 501,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "Chapter 12\n\nSUMMARY OF TESTING QUADRANTS\n\nTest Code\n\nThe Team and the Process\n\nReporting Test Results\n\nDocumentation\n\nThe System Explained\n\nThe Application\n\nQuadrant Summary: Testing on a Real Agile Project\n\nDriving Development\n\nUnit Tests\n\nAcceptance Tests\n\nExploratory Testing\n\nTesting Data Feeds\n\nFunctional Automation\n\nEnd-to-End Tests\n\nCritiquing the Product\n\nAutomation\n\nWeb Services\n\nUAT\n\nEmbedded Test Framework\n\nReliability Tests\n\nIn Chapter 6, we introduced the testing quadrants, and in the chapters that fol- lowed we talked about how to use the concepts in your agile project. In this chap- ter, we’ll bring it all together with an example of an agile team that used tests from all four quadrants.\n\nREVIEW OF THE TESTING QUADRANTS We’ve just spent ﬁve chapters talking about each of the quadrants (see Fig- ure 12-1) and examples of tools you can use for the different types of testing. The next trick is to know which tests your project needs and when to do them. In this chapter, we’ll walk you through a real-life example of an agile project that used tests from all four agile testing quadrants.\n\n241",
      "content_length": 1117,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "242\n\nCHAPTER 12\n\n(cid:2)\n\nSUMMARY OF TESTING QUADRANTS\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ2 Q3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 12-1 Agile Testing Quadrants\n\nA SYSTEM TEST EXAMPLE The following story is about one organization’s success in testing its whole system using a variety of home-grown and open source tools. Janet worked with this team, and Paul Rogers was the primary test architect. This is Paul’s story.\n\nThe Application\n\nThe system solves the problem of monitoring remote oil and gas production wells. The solution combines a remote monitoring device that can transmit data and receive adjustments from a central monitoring station using a pro- prietary protocol over a satellite communication channel.\n\nFigure 12-2 shows the architecture of the Remote Data Monitoring system. The measurement devices on the oil wells, Remote Terminal Units (RTU), use a va-",
      "content_length": 1237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "A SYSTEM TEST EXAMPLE\n\n243\n\nSite\n\nRTU\n\nH/W\n\nSatellite Modem\n\nSatellite\n\nClient\n\nTCP Network Bridge\n\nNetwork Bridges\n\nData on JMS Queue\n\nWeb Services\n\n3rd-Party Application\n\nMessage Routing and Distribution\n\nData Processing\n\nData Management\n\nPresentation and Navigation\n\nWeb Server\n\nApplication Server\n\nAdmin User\n\nDBServer\n\nDatabase\n\nMessage Delivery Center Email, Fax, Photo\n\nBasic User\n\nFigure 12-2 Remote data monitoring system architecture\n\nriety of protocols to communicate with the measurement device. This data from each RTU is transmitted via satellite to servers located at the client’s main ofﬁce. It is then made available to users via a web interface. A notiﬁcation sys- tem, via email, fax, or phone, is available when a particular reading is outside of normal operational limits. A Java Message Service (JMS) feed and web ser- vices are also available to help integration with clients’ other applications.\n\nThe software application was a huge legacy system that had few unit tests. The team was slowly rebuilding the application with new technology.\n\nThe Team and the Process\n\nThe team consisted of four software programmers, two ﬁrmware program- mers, three to four testers, a product engineer, and an off-site manager. The “real” customer was in another country. The development team uses XP",
      "content_length": 1307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "244\n\nCHAPTER 12\n\n(cid:2)\n\nSUMMARY OF TESTING QUADRANTS\n\npractices, including pair programming and TDD. The customer team used the defect-tracking system for the backlog, but most of the visibility of the stories was through index cards. Story cards were used during iteration planning meetings, and the task board tracked the progress.\n\nScrum was used as the outside reporting mechanism to the organization and the customers. The team had two week iterations and released the product about every four months. This varied depending on the functionality being developed. Retrospectives were held as part of every iteration planning ses- sion, and action was taken on the top three priority items discussed.\n\nContinuous integration through CruiseControl provided constant builds for the testers and the demonstrations held at the end of every iteration. Each tester had a local environment for testing the web application, but there were three test environments available to the system. The ﬁrst one was to test new stories and was updated as needed with the latest build. The second one was for testing client-reported issues, because it had the last version released to the clients. The third environment was a full stand-alone test environment that was available for testing full deploys, communication links, and the ﬁrmware and hardware. It was on this environment that we ran our load and reliability tests.\n\nTESTS DRIVING DEVELOPMENT The tests driving development included unit test and acceptance tests.\n\nUnit Tests\n\nChapter 7, “Technology- Facing Tests that Support the Team,” explains more about unit testing and TDD.\n\nUnit tests are technology-facing tests that support programming. Those that are developed as part of test-driven development not only help the program- mer get the story right but also help to design the system.\n\nThe programmers on the Remote Data Monitoring project bought into Test Driven Development (TDD) and pair programming wholeheartedly. All new functionality was developed and tested using pair programming. All stories delivered to the testers were supported by unit tests, and very few bugs were found after coding was complete. The bugs that were found were generally integration-related.\n\nHowever, when the team ﬁrst started, the legacy system had few unit tests to support refactoring. As process changes were implemented, the developers",
      "content_length": 2377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "See Chapter 8, “Business-Facing Tests that Support the Team,” for more about driv- ing development with acceptance tests.\n\nAUTOMATION\n\ndecided to start ﬁxing the problem. Every time they touched a piece of code in the legacy system, they added unit tests and refactored the code as neces- sary. Gradually, the legacy system became more stable and was able to with- stand major refactoring when it was needed. We experienced the power of unit tests!\n\nAcceptance Tests\n\nThe product engineer (the customer proxy) took ownership of creating the acceptance tests. These tests varied in format depending on the actual story. Although he struggled at ﬁrst, the product engineer got pretty good at giving the tests to the programmers before they started coding. The team created a test template, which evolved over time, that met both the programmers’ and the testers’ needs.\n\nThe tests were sometimes informally written, but they included data, re- quired setup if it wasn’t immediately obvious, different variations that were critical to the story, and some examples. The team found that examples helped clarify the expectations for many of the stories.\n\nThe test team automated the acceptance tests as soon as possible, usually at the same time as the stories were being developed. Of course, the product engineer was available to answer any questions that came up during development.\n\nThese acceptance tests served three purposes. They were business-facing tests that supported development because they were given to the team before coding started. Secondly, they were used by the test team as the basis of automation that fed into the regression suite and provided future ideas for exploratory test- ing. The third purpose was to conﬁrm that the implementation met the needs of the customer. The product engineer did this solution veriﬁcation.\n\nAUTOMATION Automation involved the functional test structure, web services, and embed- ded testing.\n\nThe Automated Functional Test Structure\n\nRuby was used with Watir as the tool of choice for the functional automation framework. It was determined to have the most ﬂexibility and opportunity for customization that was required for the system under test.\n\n245",
      "content_length": 2201,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "246\n\nCHAPTER 12\n\n(cid:2)\n\nSUMMARY OF TESTING QUADRANTS\n\nThe automated test code included three distinct layers, shown in Figure 12-3. The lowest layer, Layer 1, included Watir and other classes, such as loggers that wrote to the log ﬁles.\n\nThe second layer, Layer 2, was the page access layer, where classes that contained code to access individual web pages lived. For example, in the application under test (AUT) there was a login page, a create user page, and an edit user page. Classes written in Ruby contained code that could perform certain functions in the AUT, such as a class that logs into the application, a class to edit a user, and a class to assign access rights to a user. These classes contained no data. For exam- ple, the log-in class didn’t know what username to log in with.\n\nThe third and top layer, Layer 3, was the test layer, and it contained the data needed to perform a test. It called Layer 2 classes, which in turned called Layer 1.\n\nFor example, the actual test would call LogIn and pass Janet as the user name and Passw0rd as the password. This meant you could feed in many different data sets easily.\n\nLogIn (‘Janet’, ‘Passw0rd’)\n\nLayer 3—Test Layer\n\nLayer 2—Page Access Layer\n\nLayer 1—IE Controller (Watir)\n\nFigure 12-3 Functional test layers",
      "content_length": 1275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "AUTOMATION\n\nLayer 2 also knew how to handle the error messages the application gener- ated. For example, when an invalid username was entered on the login page, the login class detected the error message and then passed the problem back to the tests in Layer 3.\n\nThis means the same Layer 2 classes could be used for both happy path testing and for negative testing. In the negative case, Layer 3 would expect Layer 2 to return a failure, and would then check to see if the test failed for the correct rea- son by accessing the error messages that Layer Two scraped from the browser.\n\nThe functional tests used Ruby with Watir to control the DOM on the browser and could access almost all of the objects in the page. The automated test suite was run on nightly builds to give the team consistent feedback on high-level application behavior. This was a lifesaver as the team continued to build out the unit tests. This architecture efﬁciently accommodated the busi- ness-facing tests that support the team.\n\nWeb Services\n\nWeb services were used by clients to interface with some of their other appli- cations. The development group used Ruby to write a client to test each service they developed. For these tests, Ruby’s unit testing framework, Test::Unit, was used.\n\nThe web services tests were expanded by the test team to cover more than 1,000 different test cases, and took just minutes to run. They gave the team an amazing amount of coverage in a short period of time.\n\nThe team demonstrated the test client to the customers, who decided to use it as well. However, the customers subsequently decided it didn’t work for them, so they started writing their own tests, albeit in a much more ad hoc fashion using Ruby.\n\nThey used IRB, the interactive interface provided by Ruby, and fed values in an exploratory method. It gave the customer an interactive environment for discovering what worked and what didn't. It also let them get familiar with Ruby and how we were testing, and it gave them much more conﬁdence in our tests. Much of their User Acceptance Testing was done using IRB.\n\nThree different slants on the web services tests served three different pur- poses. The programmers used it to help test their client and drive their develop- ment. The testers used it to critique the product in a very efﬁcient automated\n\n247",
      "content_length": 2332,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "248\n\nCHAPTER 12\n\n(cid:2)\n\nSUMMARY OF TESTING QUADRANTS\n\nmanner, and the customers were able to test the web services delivered to them using IRB.\n\nEmbedded Testing\n\nIn addition to the web interface, the RDM system consisted of a small em- bedded device that communicated with measuring equipment using various protocols. Using Ruby, various tests were developed to test part of its admin- istrative interface. This interface was a command-line system similar to FTP.\n\nThese data-driven tests were contained in an Excel spreadsheet. A Ruby script would read commands from Excel using the OLE interface and send them to the embedded device. The script would then compare the response from the device with the expected result, also held in the spreadsheet. Errors were highlighted in red. These automated tests took approximately one hour to run, while doing the same tests manually would take eight hours.\n\nWhile this provided a lot of test coverage, it didn’t actually test the reason the device was used, which was to read data from RTUs. A simulator was written in Ruby with a FOX (FXRuby) GUI. This allowed mock data to be fed into the device. Because the simulator could be controlled remotely, it was incorpo- rated into automated tests that exercised the embedded device’s ability to read data, respond to error conditions, and generate alarms when the input data exceeded a predetermined threshold.\n\nEmbedded testing is highly technical, but with the power provided by the simulator, the whole team was able to participate in testing the device. The simulator was written to support testing for the test team, but the program- mer for the ﬁrmware found it valuable and used it to help with his develop- ment efforts as well. That was a positive unexpected side effect. Quadrant 2 tests that support the team may incorporate a variety of technologies, as they did in this project.\n\nCRITIQUING THE PRODUCT WITH BUSINESS-FACING TESTS The business-facing tests that critique the product are outlined in this section.\n\nExploratory Testing\n\nThe automated tests were simple and easy for everyone on the team to use. Individual test scripts could be run to set up speciﬁc conditions, allowing ef-",
      "content_length": 2194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "Exploratory test- ing, usability test- ing, and other Quadrant 3 tests are discussed in Chapter 10, “Business-Facing Tests that Critique the Product.”\n\nCRITIQUING THE PRODUCT WITH BUSINESS-FACING TESTS\n\nfective exploratory testing to be done without having to spend a lot of time manually entering data. This worked for all three test frameworks: func- tional, web services, and embedded.\n\nThe team performed exploratory testing to supplement the automated test suites and get the best coverage possible. This human interaction with the system found issues that automation didn’t ﬁnd.\n\nUsability testing was not a critical requirement for the system, but the testers watched so that the interface made sense and ﬂowed smoothly. The testers used exploratory testing extensively to critique the product. The product en- gineer also used exploratory testing for his solution veriﬁcation tests.\n\nTesting Data Feeds\n\nAs shown in Figure 12-2, the data from the system is available on a JMS queue, as well as the web browser. To test the JMS queue, the development group wrote a Java proxy. It connected to a queue and printed any arriving data to the console. They also wrote a Ruby client that received this data via a pipe, which was then available in the Ruby automated test system.\n\nEmails were automatically sent when alarm conditions were encountered. The alarm emails contained both plain text email and email with attach- ments. The MIME attachments contained data useful for testing, so a Ruby email client that supported attachments was written.\n\nThe End-to-End Tests\n\nQuadrant 3 includes end-to-end functional testing that demonstrates the de- sired behavior of every part of the system. From the beginning, it was apparent that correct operation of the whole Remote Data Monitoring system could only be determined when all components were used. Once the simulator, em- bedded device tests, web services tests, and application tests were written, it was a relatively simple matter to combine them to produce an automated test of the entire system. Once again, Excel spreadsheets were used to hold the test data, and Ruby classes were written to access the data and expected results.\n\nThe end-to-end tests were complicated by the unpredictable response of the satellite transmission path. A predeﬁned timeout value was set, and if the test’s actual value did not match the expected value, the test would cycle until it matched or the timeout was reached. When the timeout expired, the test was deemed to have failed. Most transmission issues were found and eliminated\n\n249",
      "content_length": 2576,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "250\n\nCHAPTER 12\n\n(cid:2)\n\nSUMMARY OF TESTING QUADRANTS\n\nthis way. It would have been highly unlikely that they would have been found with manual testing, because they were sporadic issues.\n\nBecause end-to-end tests such as these can be fragile, they may not be kept as part of the automated regression suite. If all of the components of the system are well covered with automated regression tests, automated end-to-end tests might not be necessary. However, due to the nature of this system, it wasn’t possible to do a full test without automation.\n\nUser Acceptance Testing\n\nUser Acceptance Testing (UAT) is the ﬁnal critique of the product by the cus- tomer, who should have been involved in the project from the start. In this example, the real customer was in France, thousands of miles from the devel- opment team. The team had to be inventive to have a successful UAT. The customer came to work with the team members a couple of times during the year and so was able to interact with the team a little easier than if they’d never met.\n\nAfter the team introduced agile development, Janet went to France to facili- tate the ﬁrst UAT at the customer site. It worked fairly well, and the release was accepted after a few critical issues were ﬁxed. The team learned a lot from that experience.\n\nThe second UAT sign-off was done in-house. To prepare, the team worked with the customer to develop a set of tests the customer could perform to ver- ify new functionality. The customer was able to test the application through- out the development cycle, so UAT didn’t produce any issues. The customer came, ran through the tests, and signed off in a day.\n\nWe cannot stress the importance of working with the customer enough. Even though the product engineer was the proxy for the customer, it was crucial to get face time with the actual customer. The relationship that had been built over time was critical to the success of the project. Janet strongly believes that the UAT succeeded because the customer knew what the team was doing along the way.\n\nReliability\n\nReliability, one of the “ilities” addressed by Quadrant 4 tests, was a critical factor of the system because it was monitoring remote sites that were often",
      "content_length": 2216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "See Chapter 10, “Business-Facing Tests that Critique the Product,” for more about Quad- rant 4 tests such as reliability testing.\n\nChapter 16, “Hit the Ground Run- ning,” gives more examples of ways teams report test results.\n\nChapter 18, “Cod- ing and Testing,” also discusses uses of big visible charts.\n\nDOCUMENTATION\n\ninaccessible, especially in winter. The simulator that was developed for test- ing the embedded system was set up on a separate environment, and was run for weeks at a time measuring stability (yet another “ility”) of the whole sys- tem. Corrections to the system design could be planned and coded as needed. This is a good example of why you shouldn’t wait until the end of the project to do the technology-facing tests that critique the product.\n\nDOCUMENTATION The approach taken to documentation is presented in this section.\n\nDocumenting the Test Code\n\nDuring development, it became clear that a formal documentation system was needed for the test code. The simplest solution was to use RDoc, similar to Javadoc, but for Ruby. RDoc extracted tagged comments from the source code and generated web pages with details of ﬁles, classes, and methods. The documents were generated every night using a batch ﬁle and were available to the complete team. It was easy to ﬁnd what test ﬁxtures were created.\n\nThe documentation of the test code helped to document the tests and make it easier to ﬁnd what we were testing and what the tests did. It was very pow- erful and easy to use.\n\nReporting the Test Results\n\nAlthough comprehensive testing was being performed, there was little evi- dence of this outside of the test team. The logs generated during automated tests provided good information to track down problems but were not suit- able for a wider audience.\n\nTo raise the visibility of the tests being performed, the test team developed a logging and reporting system using Apache, PHP, and mySQL. When a test ran, it logged the result into the database. A web front end allowed project stakeholders to determine what tests were run, the pass/failure rate, and other information.\n\nWe also believed in making our progress visible (good or bad) as much as possible. To this end we created charts and graphs along the way and posted them in common areas. Figure 12-4 shows some of the charts we created.\n\n251",
      "content_length": 2327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "252\n\nCHAPTER 12\n\n(cid:2)\n\nSUMMARY OF TESTING QUADRANTS\n\nFigure 12-4 Big visible charts used by the remote monitoring system project team\n\nUSING THE AGILE TESTING QUADRANTS This example demonstrates how testing practices from all four agile testing quadrants are combined during the life of a complex development project to achieve successful delivery. The experience of this team illustrates many of the principles we have been emphasizing. The whole team, including pro- grammers, testers, customer proxy, and the actual customer, contributed to efforts to solve automation problems. They experimented with different ap- proaches. They combined their homegrown and open source tools in differ- ent ways to perform testing at all levels, from the unit level to end-to-end system testing and UAT. The success of the project demonstrates the success of the testing approach.\n\nAs you plan each epic, release, or iteration, work with your customer team to understand the business priorities and analyze risks. Use the quadrants to help identify all of the different types of testing that will be needed and when they should be performed. Is performance the most important criteria? Is the",
      "content_length": 1184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "SUMMARY\n\nhighest priority the ability to interface with other systems? Is usability per- haps the most important aspect?\n\nInvest in a test architecture that accommodates the complexity of the system under test. Plan to obtain necessary resources and expertise at the right time for specialized tests. For each type of test, your team should work together to choose tools that solve your testing problems. Use retrospectives to continu- ally evaluate whether your team has the resources it needs to succeed, and whether all necessary tests are being speciﬁed in time to serve their purpose, and automated appropriately.\n\nDoes end-to-end testing seem impossible to do? Is your team ﬁnding it hard to write unit tests? As Janet’s team did, get everyone experimenting with dif- ferent approaches and tools. The quadrants provide a framework for produc- tive brainstorming on creative ways to achieve the testing that will let the team deliver value to the business.\n\nSUMMARY In this chapter, we described a real project that used tests from all four agile testing quadrants to overcome difﬁcult testing challenges. We used examples from this project to show how teams can succeed with all types of testing. Some important lessons from the Remote Data Monitoring System project are:\n\n(cid:2) The whole team should choose or create tools that solve each testing\n\nproblem.\n\n(cid:2) Combinations of common business tools such as spreadsheets and custom-written test scripts may be needed to accomplish complex tests.\n\n(cid:2) Invest time in building the right test architecture that works for all\n\nteam members.\n\n(cid:2) Find ways to keep customers involved in all types of testing, even if\n\nthey’re in a remote location.\n\n(cid:2) Report test results in a way that keeps all stakeholders informed about\n\nthe iteration and project progress.\n\n(cid:2) Don’t forget to document . . . but only what is useful. (cid:2) Think about all four quadrants of testing throughout your develop-\n\nment cycles.\n\n(cid:2) Use lessons learned during testing to critique the product in order to\n\ndrive development in subsequent iterations.\n\n253",
      "content_length": 2115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "Part IV AUTOMATION\n\nTest automation is a core agile practice. Agile projects depend on automation. Good-enough automation frees the team to deliver high-quality code fre- quently. It provides a framework that lets the team maximize its velocity while maintaining a high standard. Source code control, automated builds and test suites, deployment, monitoring, and a variety of scripts and tools eliminate tedium, ensure reliability, and allow the team to do its best work at all times.\n\nAutomation is also a vast topic. It includes tasks like writing simple shell scripts, setting up session properties, and creating robust automated tests. The range and number of automated tools seem to grow exponentially as we learn about better ways to produce software. Happily, the number of excel- lent books that teach ways to automate appears to grow just as fast.\n\nThis book is focused on the tester’s role in agile development. Because auto- mation is key to successful agile development, we need to talk about it, but we can’t begin to cover every aspect of the subject. What we do want to explain is why you, as a tester, must embrace automation, and how you and your team can overcome the many obstacles that can hamper your automation efforts. This section describes how you can apply agile values, principles, and prac- tices to grow a practical automation strategy, overcome barriers, and get trac- tion on test automation.",
      "content_length": 1423,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "Culture\n\nWhole-Team Approach\n\nChapter 13\n\nWHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nManual Testing Takes Too Long\n\nReduce Error-Prone Testing Tasks\n\nFree Up Time to Do Best Work\n\nReasons to Automate\n\nSafety Net\n\nProvide Feedback Early and Often\n\nTests & Examples that Drive Coding Can Do More\n\nTests Provide Documentation\n\nROI/Investment\n\nWhy Automate?\n\nBret’s List\n\nAttitude—Why Should We Automate?\n\nCan We Overcome Barriers?\n\nObstacles to Watch For\n\nHump of Pain\n\nInitial Investment\n\nCode in Flux\n\nLegacy Code\n\nFear\n\nOld Habits\n\nWhy do we automate testing, the build process, deployment, and other tasks? Agile teams focus on always having working software, which enables them to release production-ready software as often as needed. Achieving this goal requires constant testing. In this chapter, we look at reasons we want to auto- mate and the challenges that make it hard to get traction on automation.\n\n257",
      "content_length": 925,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "258\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nWHY AUTOMATE? There are multiple reasons to automate besides our saying you need to have automation to be successful using agile. Our list includes the following:\n\n(cid:2) Manual testing takes too long. (cid:2) Manual processes are error prone. (cid:2) Automation frees people to do their best work. (cid:2) Automated regression tests provide a safety net. (cid:2) Automated tests give feedback early and often. (cid:2) Tests and examples that drive coding can do more. (cid:2) Tests provide documentation. (cid:2) Automation can be a good return on investment.\n\nLet’s explore each of these in a little more detail.\n\nManual Testing Takes Too Long\n\nThe most basic reason a team wants to automate is that it simply takes too long to complete all of the necessary testing manually. As your application gets bigger and bigger, the time to test everything grows longer and longer, sometimes exponentially, depending on the complexity of the AUT (Applica- tion under test).\n\nAgile teams are able to deliver production-ready software at the end of each short iteration by having production-ready software every day. Running a full suite of passing regression tests at least daily is an indispensable practice, and you can’t do it with manual regression testing. If you don’t have any au- tomation now, you’ll have to regression test manually, but don’t let that stop you from starting to automate it.\n\nIf you execute your regression testing manually, it takes more and more time testing every day, every iteration. In order for testing to keep pace with cod- ing, either the programmers have to take time to help with manual regres- sion testing, or the team has to hire more testers. Inevitably, both technical debt and frustration will grow.\n\nIf the code doesn’t even have to pass a suite of automated unit level regres- sion tests, the testers will probably spend much of their time researching, try- ing to reproduce and report those simple bugs, and less time ﬁnding potentially serious system level bugs. In addition, because the team isn’t do- ing test-ﬁrst development, code design is more likely to be less testable and may not provide the functionality desired by the business.",
      "content_length": 2259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "WHY AUTOMATE?\n\nManually testing a number of different scenarios can take a lot of time, espe- cially if you’re keying inputs into a user interface. Setting up data for a variety of complex scenarios can be an overwhelming task if you have no automated way to speed it up. As a result, only a limited number of scenarios may be tested, and important defects can be missed.\n\nManual Processes Are Error Prone\n\nManual testing gets repetitive, especially if you’re following scripted tests, and manual tests get boring very quickly. It’s way too easy to make mistakes and overlook even simple bugs. Steps and even entire tests will be skipped. If the team’s facing a tight deadline, there’s a temptation to cut corners, and the result is a missed problem.\n\nBecause manual testing is slow, you might still be testing at midnight on the last day of the iteration. How many bugs will you notice then?\n\nAutomated builds, deployment, version control, and monitoring also go a long way toward mitigating risk and making your development process more consistent. Automating these scripted tests eliminate the possibility of errors, because each test is done exactly the same way every time.\n\nThe adage of “build once, deploy to many” is a tester’s dream come true. Au- tomation of the build and deploy processes allow you to know exactly what you are testing on any given environment.\n\nAutomation Frees People to Do Their Best Work\n\nWriting code test-ﬁrst helps programmers understand requirements and de- sign code accordingly. Having continual builds run all of the unit tests and the functional regression tests means more time to do interesting exploratory testing. Automating the setup for exploratory tests means even more time to probe into potentially weak parts of the system. Because you didn’t spend time executing tedious manual scripts, you have the energy to do a good job, thinking of different scenarios and learning more about how the application works.\n\nIf we’re thinking constantly about how to automate tests for a ﬁx or new fea- ture, we’re more likely to think of testability and a quality design rather than a quick hack that might prove fragile. That means better code and better tests.\n\nAutomating tests can actually help with consistency across the application.\n\n259",
      "content_length": 2280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "260\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nJanet’s Story\n\nSee Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” Chap- ter 12, “Summary of Testing Quad- rants,” and Chap- ter 14, “An Agile Test Automation Strategy,” for more information about Ruby and Watir.\n\nJason (one of my fellow testers) and I were working on some GUI automation scripts using Ruby and Watir, and were adding constants for button names for the tests. We quickly realized that the buttons on each page were not consistently named. We were able to get them changed and resolved those consistency issues very quickly, and had an easy way to enforce the naming conventions.\n\nBooks such as Pragmatic Project Automation [2004] can guide you in auto- mating daily development chores and free your team for important activities such as exploratory testing.\n\nGiving Testers Better Work\n\nChris McMahon described the beneﬁts he’s experienced due to regression test automation in a posting to the agile-testing mailing list in November 2007:\n\nOur UI regression test automation has grown 500% since April [of 2007]. This allows us to focus the attention of real human beings on more inter- esting testing.\n\nChris went on to explain, “Now that we have a lot of automation, we have the leisure to really think about what human tests need doing. For any testing that isn’t trivial, we have just about institutionalized a test- idea brainstorming session before beginning execution.” Usually, Chris and his teammates pair either two testers or one tester and a developer. Sometimes a tester generates ideas and gets them reviewed, via a mind- map, a wiki page, or a list in the release notes. Chris observed, “We almost always come up with good test ideas by pairing that wouldn’t have been found by either individual independently.”\n\nReferring to their frequent releases of signiﬁcant features, Chris says, “Thanks to the good test automation, we have the time to invest in mak- ing certain that the whole product is attractive and functional for real people. Without the automation, testing this product would be both boring and stupid. As it is, we testers have signiﬁcant and interesting work to do for each release.”\n\nWe agree with Chris that the most exciting part of test automation is the way it expands our ability to improve the product through innovative exploratory testing.\n\n—Janet",
      "content_length": 2404,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "Janet’s Story\n\nWHY AUTOMATE?\n\nProjects succeed when good people are free to do their best work. Automat- ing tests appropriately makes that happen. Automated regression tests that detect changes to existing functionality and provide immediate feedback are a primary component of this.\n\nAutomated Regression Tests Provide a Safety Net\n\nMost practitioners who’ve been in the software business for a few years know the feeling of dread when they’re faced with ﬁxing a bug or implementing a new feature in poorly designed code that isn’t covered by automated tests. Squeeze one end of the balloon here and another part of it bulges out. Will it break?\n\nKnowing the code has sufﬁcient coverage by automated regression tests gives a great feeling of conﬁdence. Sure, a change might produce an unexpected ef- fect, but we’ll know about it within a matter of minutes if it’s at the unit level, or hours if at a higher functional level. Making the change test-ﬁrst means thinking through the changed behavior before writing the code and writing a test to verify it, which adds to that conﬁdence.\n\nI recently had a conversation with one of the testers on my team who questioned the value of automated tests. My ﬁrst answer was “It’s a safety net” for the team. However, he challenged that premise. Don’t we just become reliant on the tests rather than ﬁxing the root cause of the problem?\n\nIt made me think a bit more about my answer. He was right in one sense; if we be- come complacent about our testing challenges and depend solely on automated tests to ﬁnd our issues, and then just ﬁx them enough for the test to pass, we do ourselves a disservice.\n\nHowever, if we use the tests to identify problem areas and ﬁx them the right way or refactor as needed, then we are using the safety net of automation in the right way. Automation is critical to the success of an agile project, especially as the ap- plication grows in size.\n\nWhen they don’t have an automated suite of tests acting as a safety net, the programmers may start viewing the testers themselves as a safety net. It’s easy to imagine that Joe Programmer’s thought process goes like this: “I ought to go back and add some automated unit tests for formatEmployeeInfo, but I know Susie Tester is going to check every page where it’s used manually. She’ll see if anything is off, so I’d just be duplicating her effort.”\n\n261\n\n—Janet",
      "content_length": 2383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "262\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nIt’s nice that a programmer would think so highly of the tester’s talents, but Joe is headed down a slippery slope. If he doesn’t automate these unit tests, which other tests might he skip? Susie is going to be awfully busy eyeballing all those pages.\n\nTeams that have good coverage from automated regression tests can make changes to the code fearlessly. They don’t have to wonder, “If I change this formatEmployeeInfo module, will I break something in the user interface?” The tests will tell them right away whether or not they broke anything. They can go lots faster than teams relying exclusively on manual testing.\n\nAutomated Tests Give Feedback, Early and Often\n\nAfter an automated test for a piece of functionality passes, it must continue to pass until the functionality is intentionally changed. When we plan changes in the application, we change the tests to accommodate them. When an auto- mated test fails unexpectedly, a regression defect may have been introduced by a code change. Running an automated suite of tests every time new code is checked in helps ensure that regression bugs will be caught quickly. Quick feedback means the change is still fresh in some programmer’s mind, so trou- bleshooting will go more quickly than if the bug weren’t found until some test- ing phase weeks later. Failing fast means bugs are cheaper to ﬁx.\n\nAutomated tests run regularly and often act as your change detector. They al- low the team an opportunity to know what has changed since the last build. For example, were there any negative side effects with the last build? If your automation suite has sufﬁcient coverage, it can easily tell far-reaching effects that manual testers can never hope to ﬁnd.\n\nMore often than not, if regression tests are not automated, they won’t get run every iteration, let alone every day. The problem arises very quickly during the end game, when the team needs to complete all of the regression tests. Bugs that would have been caught early are found late in the game. Many of the beneﬁts of testing early are lost.\n\nTests and Examples that Drive Coding Can Do More\n\nIn Chapter 7, “Technology-Facing Tests that Support the Team,” we talked about using tests and examples to drive coding. We’ve talked about how im- portant it is to drive coding with both unit and customer tests. We also want to stress that if these tests are automated, they become valuable for a different reason. They become the base for a very strong regression suite.",
      "content_length": 2558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "Lisa’s Story\n\nThe bibliography contains an article by Jennitta Andrea [2008] on team et- iquette for TDD.\n\nWHY AUTOMATE?\n\nAfter my team got a handle on unit tests, refactoring, continuous integration, and other technology-facing practices, we were able to catch regression bugs and in- correctly implemented functionality during development.\n\nOf course, this didn’t mean our problems were completely solved; we still some- times missed or misunderstood requirements. However, having an automation framework in place enabled us to start focusing on doing a better job of capturing requirements in up-front tests. We also had more time for exploratory testing. Over time, our defect rate declined dramatically, while our customers’ delight in the delivered business value went up.\n\nTDD and SDD (story test-driven development) keep teams thinking test- ﬁrst. During planning meetings, they talk about the tests and the best way to do them. They design code to make the tests pass, so testability is never an is- sue. The automated test suite grows along with the code base, providing a safety net for constant refactoring. It’s important that the whole team prac- tices TDD and consistently writes unit tests, or the safety net will have holes.\n\nThe team also doesn’t accrue too much technical debt, and their velocity is bound to be stable or even increase over time. That’s one of the reasons why the business managers should be happy to let software teams take the time to implement good practices correctly.\n\nTests Are Great Documentation\n\nIn Part III, we explained how agile teams use examples and tests to guide de- velopment. When tests that illustrate examples of desired behavior are auto- mated, they become “living” documentation of how the system actually works. It’s good to have narrative documentation about how a piece of func- tionality works, but nobody can argue with an executable test that shows in red and green how the code operates on a given set of inputs.\n\nIt’s hard to keep static documentation up to date, but if we don’t update our automated tests when the system changes, the tests fail. We need to ﬁx them to keep our build process “green.” This means that automated tests are always an accurate picture of how our code works. That’s just one of the ways our investment in automation pays off.\n\n263\n\n—Lisa",
      "content_length": 2333,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "264\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nROI and Payback\n\nAll of the reasons just presented contribute to the bottom line and the pay- back of automation. Automation provides consistency to a project and gives the team opportunity to test differently and push the limits of the applica- tion. Automation means extra time for testers and team members to concen- trate on getting the right product out to market in a timely manner.\n\nAn important component of test automation payback is the way defects are ﬁxed. Teams that rely on manual tests tend to ﬁnd bugs long after the code containing the bug is written. They get into the mode of ﬁxing the “bug of the day,” instead of looking at the root cause of the bug and redesigning the code accordingly. When programmers run the automated test suite in their own sandbox, the automated regression tests ﬁnd the bugs before the code is checked in, so there’s time to correct the design. That’s a much bigger pay- back, and it’s how you reduce technical debt and develop solid code.\n\nBARRIERS TO AUTOMATION—THINGS THAT GET IN THE WAY Back in 2001, Bret Pettichord [2001] listed seven problems that plague auto- mation. They are still applicable, but are intended for teams that do not in- corporate automation as part of their development. And of course, because you are doing agile, you are doing that, right?\n\nWe would like to think that everyone has included automation tasks as part of each story, but the reality is that you probably wouldn’t be reading this section if you had it all under control. We’ve included Bret’s list to show what problems you probably have if you don’t include automation as part of the everyday project deliverables.\n\nBret’s List\n\nBret’s list of automation problems looks like this:\n\n(cid:2) Only using spare time for test automation doesn’t give it the focus it\n\nneeds.\n\n(cid:2) There is a lack of clear goals. (cid:2) There is a lack of experience. (cid:2) There is high turnover, because you lose any experience you may have. (cid:2) A reaction to desperation is often the reason why automation is cho- sen, in which case it can be more of a wish than a realistic proposal.",
      "content_length": 2191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "Lisa’s Story\n\nBARRIERS TO AUTOMATION—THINGS THAT GET IN THE WAY\n\n(cid:2) There can be a reluctance to think about testing; the fun is in the\n\nautomating, not in the testing.\n\n(cid:2) Focusing on solving the technology problem can cause you to lose\n\nsight of whether the result meets the testing need.\n\nWe think there are some other problems that teams run into when trying to automate. Even if we do try to include automation in our project deliverables, there are other barriers to success. In the next section, we present our list of obstacles to successful test automation.\n\nOur List\n\nOur list of barriers to successful test automation is based on the experiences we’ve had with our own agile teams as well as that of the other teams we know.\n\n(cid:2) Programmers’ attitude (cid:2) The “Hump of Pain” (cid:2) Initial investment (cid:2) Code that’s always in ﬂux (cid:2) Legacy systems (cid:2) Fear (cid:2) Old habits\n\nProgrammers’ Attitude—“Why Automate?”\n\nProgrammers who are used to working in a traditional environment, where some separate, unseen QA team does all of the testing, may not even give functional test automation a lot of thought. Some programmers don’t bother to test much because they have the QA team as a safety net to catch bugs be- fore release. Long waterfall development cycles make testing even more re- mote to programmers. By the time the unseen testers are doing their job, the programmers have moved on to the next release. Defects go into a queue to be ﬁxed later at great expense, and nobody is accountable for having pro- duced them. Even programmers who have adopted test-driven development and are used to automating tests at the unit level may not think about how acceptance tests beyond the unit level get done.\n\nI once joined an XP team of skilled programmers practicing test-driven development that had a reasonable suite of unit tests running in an automated build process. They had never automated any business-facing tests, so one day I started a discussion about what tools they might use to automate functional business-facing regression tests. The programmers wanted to know why we needed to automate these tests.\n\n265",
      "content_length": 2165,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "266\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nAt the end of the ﬁrst iteration, when everyone was executing the acceptance tests by hand, I pointed out that there would be all these tests to do again in the next iteration as regression tests, in addition to the tests for all of the new stories. In the third iteration, there would be three times as many tests. To a tester, it seems ridiculously obvious, but sometimes programmers need to do the manual tests before they understand the compulsion to automate them.\n\nEducation is the key to getting programmers and the rest of the team to un- derstand the importance of automation.\n\nThe “Hump of Pain” (The Learning Curve)\n\nIt’s hard to learn test automation, especially to learn how to do it in a way that produces a good return on the resources invested in it. A term we’ve heard Brian Marick use to describe the initial phase of automation that developers (including testers) have to overcome is the “hump of pain” (see Figure 13-1). This phrase refers to the struggle that most teams go through when adopting automation.\n\nNew teams are often expected to adopt practices such as TDD and refactor- ing, which are difﬁcult to learn. Without good coaching, plenty of time to master new skills, and strong management support, they’re easily discour-\n\nFigure 13-1 Hump of pain of the automation learning curve\n\n—Lisa",
      "content_length": 1397,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "Lisa’s Story\n\nBARRIERS TO AUTOMATION—THINGS THAT GET IN THE WAY\n\naged. If they have extra obstacles to learning, such as having to work with poorly designed legacy code, it may seem impossible to ever get traction on test automation.\n\nMy team at ePlan Services originally tried to write unit tests for a legacy system that deﬁnitely wasn’t written with testing in mind. They found this to be a difﬁcult, if not impossible, task, so they decided to code all new stories in a new, testable ar- chitecture. Interestingly, about a year later, they discovered it wasn’t really that hard to write unit tests for the old code. The problem was they didn’t know how to write unit tests at all, and it was easier to learn on a well-designed architecture. Writing unit-level tests became simply a natural part of writing code.\n\nThe hump of pain may occur because you are building your domain-speciﬁc testing framework or learning your new functional test tool. You may want to bring in an expert to help you get it set up right.\n\nYou know your team has overcome the “hump” when automation becomes, if not easy, at least a natural and ingrained process. Lisa has worked on three teams that successfully adopted TDD and functional test automation. Each time, the team needed lots of time, training, commitment, and encourage- ment to get traction on the practices.\n\nInitial Investment\n\nEven with the whole team working on the problem, automation requires a big investment, one that may not pay off right away. It takes time and re- search to decide on what test frameworks to use and whether to build them in-house or use externally produced tools. New hardware and software are probably required. Team members may take a while to ramp up on how to use automated test harnesses.\n\nMany people have experienced test automation efforts that didn’t pay off. Their organization may have purchased a vendor capture-playback tool, given it to the QA team, and expected it to solve all of the automation prob- lems. Such tools often sit on a shelf gathering dust. There may have been thousands of lines of GUI test scripts generated, with no one left who knows what they do, or the test scripts that are impossible to maintain are no longer useful.\n\n267\n\n—Lisa",
      "content_length": 2239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "268\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nJanet’s Story\n\nI walked into an organization as a new QA manager. One of my tasks was to eval- uate the current automated test scripts and increase the test coverage. A vendor tool had been purchased a few years earlier, and the testers who had developed the initial suite were no longer with the organization. One of the new testers hired was trying to learn the tool and was adding tests to the suite.\n\nThe ﬁrst thing I did was ask this tester to do an assessment on the test suite to see what the coverage actually was. She spent a week just trying to understand how the tests were organized. I started poking around as well and found that that the existing tests were very poorly designed and had very little value.\n\nWe stopped adding more tests and instead spent a little bit of time understanding what the goal was for our test automation. As it turned out, the vendor tool could not do what we really needed it to do, so we cancelled the licenses and found an open source tool that met our needs.\n\nWe still had to spend time learning the new open source tool, but that investment would have been made if we’d stayed with the original vendor tool anyhow, be- cause no one on the team knew how to use the original tool.\n\nTest design skills have a huge impact on whether automation pays off right away. Poor practices produce tests that are hard to understand and maintain, and may produce hard-to-interpret results or false failures that take time to research. Teams with inadequate training and skills might decide the return on their automation investment isn’t worth their time.\n\nGood test design practices produce simple, well-designed, continually refac- tored, maintainable tests. Libraries of test modules and objects build up over time and make automating new tests quicker. See Chapter 14 for some hints on and guidelines for test design for automation.\n\nWe know it’s not easy to capture metrics. For example, trying to capture the time it takes to write and maintain automated tests versus the time it takes to run the same regression tests manually is almost impossible. Similarly, trying to capture how much it costs to ﬁx defects within minutes of introducing them versus how much it costs to ﬁnd and ﬁx problems after the end of the iteration is also quite difﬁcult. Many teams don’t make the effort to track this information. Without numbers showing that automating requires less effort and provides more value, it’s harder for teams to convince management that an investment in automation is worthwhile. A lack of metrics that demon- strate automation’s return on investment also makes it harder to change a team’s habits.\n\n—Janet",
      "content_length": 2730,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "In Chapter 14, “An Agile Test Automa- tion Strategy,” we’ll look at ways to organize auto- mated tests.\n\nBARRIERS TO AUTOMATION—THINGS THAT GET IN THE WAY\n\nCode that’s Always in Flux\n\nAutomating tests through the user interface is tricky, because UIs tend to change frequently during development. That’s one reason that simple record and playback techniques are rarely a good choice for an agile project.\n\nIf the team is struggling to produce a good design on the underlying business logic and database access, and major rework is done frequently, it might be hard to keep up even with tests automated behind the GUI at the API level. If little thought is given to testing while designing the system, it might be difﬁ- cult and expensive to ﬁnd a way to automate tests. The programmers and testers need to work together to get a testable application.\n\nAlthough the actual code and implementation, like the GUI, tends to change frequently in agile development, the intent of code rarely changes. Organiz- ing test code by the application’s intent, rather than by its implementation, allows you to keep up with development.\n\nLegacy Code\n\nIn our experience, it’s much easier to get traction on automation if you’re writ- ing brand new code in an architecture designed with testing in mind. Writing tests for existing code that has few or no tests is a daunting task at best. It seems virtually impossible to a team new to agile and new to test automation.\n\nIt is sometimes a Catch-22. You want to automate tests so you can refactor some of the legacy code, but the legacy code isn’t designed for testability, so it is hard to automate tests even at the unit level.\n\nIf your team faces this type of challenge and doesn’t budget plenty of time to brainstorm about how to tackle it, it’ll be tough to start automating tests ef- fectively. Chapter 14 gives strategies to address these issues.\n\nFear\n\nTest automation is scary to those who’ve never mastered it, and even to some who have. Programmers may be good at writing production code, but they might not be very experienced at writing automated tests. Testers may not have a strong programming background, and they don’t trust their potential test automation skills.\n\nNon-programming testers have often gotten the message that they have nothing to offer in the agile world. We believe otherwise. No individual tester\n\n269",
      "content_length": 2368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "270\n\nCHAPTER 13\n\n(cid:2) WHY WE WANT TO AUTOMATE TESTS AND WHAT HOLDS US BACK\n\nshould need to worry about how to do automation. It’s a team problem, and there are usually plenty of programmers on the team who can help. The trick is to embrace learning new ideas. Take one day at a time.\n\nOld Habits\n\nWhen iterations don’t proceed smoothly and the team can’t complete all of the programming and testing tasks by the end of an iteration, team members may panic. We’ve observed that when people go into panic mode, they fall into comfortable old habits, even if those habits never produced good results.\n\nSo we may say, “We are supposed to deliver on February 1. If we want to meet that date, we don’t have time to automate any tests. We’ll have to do whatever manual tests can be done in that amount of time and hope for the best. We can always automate the tests later.”\n\nThis is the road to perdition. Some manual tests can get done, but maybe not the important manual exploratory tests that would have found the bug that cost the company hundreds of thousands of dollars in lost sales. Then, be- cause we didn’t ﬁnish with our test automation tasks, those tasks carry over to the next iteration, reducing the amount of business value we can deliver. As iterations proceed, the situation continues to deteriorate.\n\nSee Chapter 3, “Cultural Chal- lenges,” for some ideas on making changes to the team culture in order to facilitate agile practices.\n\nCAN WE OVERCOME THESE BARRIERS? The agile whole-team approach is the foundation to overcoming automa- tion challenges. Programmers who are new to agile are probably used to be- ing rewarded for delivering code, whether it’s buggy or not, as long as they meet deadlines. Test-driven development is oriented more toward design than testing, so business-facing tests may still not enter their consciousness. It takes leadership and a team commitment to quality to get everyone think- ing about how to write, use, and run both technology-facing and business- facing tests. Getting the whole team involved in test automation may be a cultural challenge.\n\nIn the next chapter, we show how to use agile values and principles to over- come some of the problems we’ve described in this chapter.",
      "content_length": 2234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "SUMMARY\n\nSUMMARY In this chapter, we analyzed some important factors related to test automation:\n\n(cid:2) We need automation to provide a safety net, provide us with essential feedback, keep technical debt to a minimum, and help drive coding. (cid:2) Fear, lack of knowledge, negative past experiences with automation, rapidly changing code, and legacy code are among the common bar- riers to automation.\n\n(cid:2) Automating regression tests, running them in an automated build\n\nprocess, and ﬁxing root causes of defects reduces technical debt and permits growth of solid code.\n\n(cid:2) Automating regression tests and tedious manual tasks frees the team\n\nfor more important work, such as exploratory testing.\n\n(cid:2) Teams with automated tests and automated build processes enjoy a\n\nmore stable velocity.\n\n(cid:2) Without automated regression tests, manual regression testing will continue to grow in scope and eventually may simply be ignored. (cid:2) Team culture and history may make it harder for programmers to pri- oritize automation of business-facing tests than coding new features. Using agile principles and values helps the whole team overcome bar- riers to test automation.\n\n271",
      "content_length": 1192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "Chapter 14\n\nAN AGILE TEST AUTOMATION STRATEGY\n\nAutomation Test Categories\n\nTest Pyramid\n\nQuadrants\n\nOrganizing Tests\n\nOrganizing Test Results\n\nManaging Automated Tests\n\nConinuous Integration, Builds, and Deploys\n\nUnit and Component Tests\n\nImplementing Automation\n\nAPI or Web Services\n\nBehind the GUI\n\nIdentifying Requirements\n\nWhat Can We Automate?\n\nGUI\n\nLoad\n\nOne Tool at a Time\n\nChoosing Tools\n\nEvaluating Automation Tools\n\nComparisons\n\nRepetitive Tasks\n\nAgile-Friendly Tools\n\nData Creation or Setup\n\nDeveloping an Automation Strategy\n\nData Generation Tools\n\nUsability\n\nAvoid Database Access\n\nWhen Database Access Is Unavoidable\n\nSupplying Data for Tests\n\nWhat Shouldn’t We Automate?\n\nExploratory\n\nTests That Will Never Fail\n\nUnderstand Your Needs\n\nOne-Off Tests\n\nKeep It Simple\n\nIterative Feedback\n\nWhat Might Be Hard to Automate?\n\nWhole-Team Approach\n\nTake Time to Do It Right\n\nApply Agile Principles to Automation\n\nWhat Hurts the Most?\n\nLearn by Doing\n\nApplying Agile Coding Practices\n\nDeveloping a Strategy—Where Do We Start?\n\nMulti-Layered Approach\n\nTest Design and Maintenance\n\nChoosing the Right Tools\n\n273",
      "content_length": 1115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "274\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nAs we explored each of the Agile Testing Quadrants in Part III, we gave examples of tools that can help those different testing efforts succeed. Many of those tools are for automating tests. As we described in the previous chapter, teams face plenty of obstacles in their quest for successful test automation. Better tools become available all the time, but the trick is to choose the right tools and learn to use them effec- tively. Test automation requires thoughtful investment and incremental improve- ment. In this chapter, we explain how you can apply agile values and principles to get traction in starting or improving your automation efforts.\n\nAN AGILE APPROACH TO TEST AUTOMATION Here you are, reading this chapter on how to get your test automation strat- egy working, maybe hoping for that silver bullet, or an answer to all your questions. We hate to disappoint you, but we need to tell you right up front, there is no silver bullet. There is no one answer that works for every team. Don’t lose heart though, because we have some ideas to help you get started.\n\nFirst, we suggest approaching your automation problems as you would any problem. Deﬁne the problem you are trying to solve. To help you ﬁgure that out, we ﬁrst talk about some basics of test automation and reintroduce some terms.\n\nAutomation Test Categories\n\nIn Part III, we introduced the Agile Testing Quadrants and talked about each quadrant and the purpose of the tests in each quadrant. In this section, we look at the quadrants in a different light. Let’s look carefully at the quadrants (see Figure 14-1).\n\nYou can see that we’ve labeled both quadrants that support the team (Q1 and Q2) as using automation. In Quadrant 4, the tools used for critiquing the product from a technology point of view also usually require automated tools. In Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” we discussed some of the tools that can be used for automating business-facing tests in the quest for supporting the team. In fact, the only quadrant that is not labeled as using automation is Quadrant 3—the business-facing tests that cri- tique the product. However, as we discussed in Chapter 10, “Business-Facing Tests that Critique the Product,” tools may be useful for some of that testing. For example, automation can help set up test data and user scenarios, and ana- lyze logged activity.",
      "content_length": 2446,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "See Chapter 8, “Business-Facing Tests that Support the Team,” for more about Wiz- ard of Oz testing.\n\nAN AGILE APPROACH TO TEST AUTOMATION\n\nAgile Testing Quadrants\n\nAutomated & Manual\n\nBusiness-Facing\n\nManual\n\nm a e T\n\ne h t\n\nFunctional Tests Examples Story Tests Prototypes Simulations\n\nExploratory Testing Scenarios Usability Testing UAT (User Acceptance Testing) Alpha/Beta\n\nQ2 Q3\n\nC r i t i q u e\n\ng n i t r o p p u S\n\nUnit Tests Component Tests\n\nQ1\n\nQ4\n\nPerformance & Load Testing Security Testing “ility” Testing\n\nP r o d u c t\n\nAutomated\n\nTechnology-Facing\n\nTools\n\nFigure 14-1 Agile Testing Quadrants\n\nUse the quadrants to help you identify the different types of automation tools you might need for each project, even for each iteration. We ﬁnd it helpful to go through each quadrant and make a checklist of what tools might be needed. Let’s say we’re about to redesign a UI. We look at Quadrant 1. How can it be coded test-ﬁrst? Do we know how to unit test our presentation layer? Do we need a new tool to help with that? Now on to Quadrant 2. We’ll need to do some prototyping; should we just use paper, or should we plan a Wizard of Oz type activity? What tool will we use to create executable business-facing tests to guide development? Do we have regression test scripts that will need updating or replacing? We know that one of our Quadrant 3 activities will be usability testing. That takes some advance planning. We might want tools to help track the users’ activities so we can analyze them further. Thinking about Quadrant 4, we realize that we have load test scripts that use the old UI, so we have to budget time to update them for the new one.\n\n275",
      "content_length": 1669,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "276\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nAs we emphasized in Part III, “Using the Agile Testing Quadrants,” the order of quadrants doesn’t relate to the order in which we do the testing. As we make our checklist of tools needed for each type of test, we think about when we want to test so we know when to have our automation tools ready. For ex- ample, a team designing a new architecture would plan to do a spike and run scalability test against it as soon as possible. They will need to spend time during the ﬁrst iteration of the project ﬁnding and implementing a perfor- mance test tool.\n\nThe quadrants help us ﬁgure out what tools we might need, but with so many different automation options at different levels, a strategy for where to do which types of testing and how to organize the tests is essential. To deliver value quickly and often, our automation efforts need a high ROI. The test pyramid helps us optimize our test investment.\n\nTest Automation Pyramid\n\nFigure 14-2 illustrates the “test automation pyramid.” We like the version that Mike Cohn introduced, which shows the foundation layer made up of technology-facing unit and component tests. We recognize that many teams will struggle with this idea, because it seems the opposite of what many teams currently have. Many test teams have been taught the “V” model of testing, where activities such as component, system, and release testing are done in sequence after coding activities. Other teams have an inverted pyra- mid, with the majority of the tests in the functional or presentation layer.\n\nThe agile test automation pyramid shows three different layers of automated tests. The lowest tier is the foundation that supports all of the rest. It’s mainly made up of robust unit tests and component tests, the technology-facing tests that support the team. This layer represents the bulk of the automated tests. They’re generally written in the same language as the system under test, using the xUnit family of tools. After a team has mastered the art of TDD, these tests are by far the quickest and least expensive to write. They provide the quickest feedback, too, making them highly valuable. They have the big- gest ROI by far of any type of test.\n\nSee Chapter 7, “Technology- Facing Tests that Support the Team” for more about unit and compo- nent tests.\n\nIn agile development, we try to push as many tests as possible to this layer. While business-facing tests tend to go in one of the higher levels, we imple- ment them at the unit level when it makes sense. If they’re tests the customers don’t have to be able to read, and they can be coded much more quickly as unit tests, it’s a good option. Other types of technology-facing tests such as performance tests may also be possible at the unit level.",
      "content_length": 2797,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "See Chapter, 8, \"Business-Facing Tests that Support the Team,\" for more about business-facing tests that support the team.\n\nAN AGILE APPROACH TO TEST AUTOMATION\n\nManual Tests\n\nGUI Tests\n\nAcceptance Tests (API Layer)\n\nUnit Tests/Component Tests\n\nFigure 14-2 Test automation pyramid\n\nThe middle tier in the pyramid is the layer that includes most of the auto- mated business-facing tests written to support the team. These are the func- tional tests that verify that we are “building the right thing.” The tests in this layer may include “story” tests, “acceptance” tests, and tests that cover larger sets of functionality than the unit test layer. These tests operate at the API level or “behind the GUI,” testing the functionality directly without going through the GUI. We write test cases that set up inputs and ﬁxtures that feed the inputs into the production code, accept the outputs, and compare them to expected results. Because these tests bypass the presentation layer, they are less expensive to write and maintain than tests that use the interface.\n\nWe try to write them in a domain-speciﬁc language that the customers can understand, so they take more work than unit-level tests. They also generally run more slowly, because each test covers more ground than a unit test and may access the database or other components. The feedback they provide is not as quick as the unit-level tests, but it is still much faster than we could get operating through the user interface. Therefore, their ROI is not as high as the tests that form the base of the pyramid, but it’s higher than the top layer.\n\n277",
      "content_length": 1606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "278\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nWe have more about these tests in Chapter 8, “Business-Facing Tests that Support the Team,” and Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” where we discuss the business-facing tests that support the team and the tools that effec- tively capture these tests.\n\nFit and FitNesse are examples of tools used for the middle layer of the pyra- mid. Home-grown test harnesses that use spreadsheets or other business- friendly means for deﬁning test cases are also common.\n\nThe top tier represents what should be the smallest automation effort, be- cause the tests generally provide the lowest ROI. These tests are the ones done through the GUI, the ones that actually operate and manipulate the presentation layer. They are written after the code is completed, and so are usually written to critique the product and go directly to the regression suite.\n\nThese tests are traditionally more expensive to write, although there are new tools that help reduce the investment needed. Because components of the user interface tend to be changed often, these tests are much more brittle than tests that work at a functional or unit level. For example, just renaming HTML elements could cause a test script to fail. Operating through the user interface also slows these tests down, compared to tests in the lower levels of the pyramid that operate directly on production code. The tests in the top layer do provide important feedback, but a suite of GUI tests may take hours to run rather than the few minutes required for unit-level test suites. We want to minimize the number of tests at this layer, so they should only form the tip of the pyramid.\n\nNo matter how many automated tests they have, most systems also need man- ual testing activities, such as exploratory testing and user acceptance testing. We don’t want to forget these, so we’ve illustrated them with the little cloud at the tip of the pyramid. The bulk of our regression testing must be automated or our manual testing won’t give us a good return on investment either.\n\nPatrick Wilson-Welsh [2008] adds a descriptive dimension to the test auto- mation pyramid with a “three little pigs” metaphor. The bottom foundation layer is made of bricks. The tests are solid, and not vulnerable to the hufﬁng and pufﬁng of the Big Bad Wolf. The middle layer is made of sticks. They need rearranging more often than the brick layer to stay strong. The tests in the top layer are made of straw. It’s hard to get them to stay in place, and the wolf can easily blow them around. If we have too many tests made out of straw, we’re going to spend lots of time putting them back into shape.\n\nMost new agile teams don’t start with this shape pyramid—it’s usually in- verted, a left-over from previous projects. GUI test tools are often easier to learn, so teams start out with a lot of tests in their top “straw” layer. As we mentioned in the previous chapter, the “hump of pain” that most program- mers have to overcome to master unit test automation means that the team",
      "content_length": 3091,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "See the bibliogra- phy for a link to Patrick Wilson- Welsh’s discussion of “ﬂipping the test pyramid” right-side up.\n\nWHAT CAN WE AUTOMATE?\n\nmay start out with only a few bricks. The ﬁxtures that automate functional tests in the middle layer are easy to write if the system is designed with those tests in mind, so the sticks might pile up faster than the bricks. As teams mas- ter TDD and unit test automation, the bottom layer starts to grow. When they get traction, a team using TDD will quickly build out the brick foundation of the test pyramid.\n\nThe testing pyramid is a good place to start looking at how test automation can help an agile team. Programmers tend to focus on the bottom of the pyr- amid, and they need plenty of time and training to get over the “hump of pain” and get to the point where TDD is natural and quick. In traditional teams, testers usually have no choice but to automate tests at the GUI level. The whole-team approach used by agile teams means that testers pair with programmers and help them get better at writing tests, which in turn solidi- ﬁes that brick foundation layer of the pyramid. Because tests drive develop- ment, the whole team is always designing for maximum testability, and the pyramid can grow to the right shape.\n\nProgrammers pair with testers to automate functional-level tests, ﬁlling out the middle layer. For example, a tester and customer may prepare a 400-row spreadsheet of test cases for a web services application. The programmer can help ﬁgure out a way to automate those tests. Different team members may have expertise in areas such as generating test data or using tools such as Ex- cel macros, and all that knowledge spreads around the team. Working to- gether, the team ﬁnds the best combinations of tools, test cases, and test data.\n\nInvolving the programmers in ﬁnding cost-effective ways to automate the top-level GUI tests has multiple beneﬁts. These efforts may give program- mers a better understanding of the system’s “big picture,” and testers can learn how to create more pliable, less straw-like GUI tests.\n\nThe more a team can work together and share knowledge, the stronger the team, the application, and the tests will become. The Big Bad Wolf won’t stand a chance. Let’s start by looking at what kind of tests we can automate and then at what we shouldn’t even try.\n\nWHAT CAN WE AUTOMATE? Most types of testing you can think of beneﬁt from automation. Manual unit tests don’t go far toward preventing regression failures, because performing a suite of manual tests before every check-in just isn’t practical. You can’t de- sign code test-ﬁrst through manual unit tests either. When programmers\n\n279",
      "content_length": 2681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "280\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\ncan’t run tests quickly at the touch of a button, they may not be motivated enough to run tests at all. We could manually test that different units of code work together correctly, but automated component tests are a much more ef- fective safety net.\n\nManual exploratory testing is an effective way to ﬁnd functional defects, but if we don’t have enough automated business-facing regression tests, we prob- ably spend all of our time madly trying to keep up with manual regression testing. Let’s talk about all of the different kinds of testing that can be done well with automation.\n\nTo run automated tests, you need some kind of automated framework that allows programmers to check in code often, run tests on that code, and create deployable ﬁles. Let’s consider this ﬁrst.\n\nContinuous Integration, Builds, and Deploys\n\nSee Chapter 7, “Technology- Facing Tests that Support the Team,” for examples of build automation tools.\n\nAny tedious or repetitive task involved in developing software is a candidate for automation. We’ve talked about the importance of an automated build process. You can’t build your automated test pyramid without this. Your team needs the immediate feedback from the unit-level tests to stay on track. Getting automated build emails listing every change checked in is a big help to testers because they know when a build is ready to test without having to bother the programmers.\n\nPeril: Waiting for Tuesday’s Build\n\nIn a traditional environment, it is normal for testers to wait for a stable build, even if that means waiting until next Tuesday. In an agile environment, if testers don’t keep up with the developers, the stories get tested late in the game. If the developers don’t get the feedback, such as suggestions and bugs, the testers can lose credibility with the developers. Bugs won’t be discovered un- til the developers are already on another story and do not want to be inter- rupted to ﬁx them until later.\n\nBugs pile up, and automation suffers because it can’t be completed. Velocity is affected because a story cannot be marked “done” until it is tested. This makes it harder to plan the next iteration. At the end of the release cycle, your story testing runs into the end game and you may not have a successful release. At the very least, you will have a stressful release.",
      "content_length": 2379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "WHAT CAN WE AUTOMATE?\n\nAn automated deployment process also speeds up testing and reduces errors. In fact, the day Janet was editing this chapter, she messed up the deployment because it was a manual process. It was pretty simple, but she was new to the project and moved the ﬁle to the wrong place. Getting an automated deploy- ment process in place went on Janet’s list of things to get done right away. Lisa’s team implemented its continuous integration and build framework ﬁrst thing, and found it fairly easy and quick to do, although it requires con- tinual care and feeding. Other teams, especially those with large, complex systems, face much bigger hurdles.\n\nWe’ve talked with teams who had build times of two hours or more. This meant that a programmer would have to wait for two hours after checking in code to get validation that his check-in didn’t break any preexisting function- ality. That is a long time to wait.\n\nMost agile teams ﬁnd an ongoing build longer than eight to ten minutes to be unworkable. Even 15 minutes is much too long to wait for feedback, be- cause check-ins will start stacking up, and testers will wait a long time to get the latest, greatest build. Can you imagine how the developers working with a build that takes two hours feel as they approach the end of an iteration or release cycle? If they break any functionality, they’ll have to wait two more hours to learn whether or not they had ﬁxed it.\n\nMany times, long builds are the result of accessing the database or trying to test through the interface. Thousands of tests running against a large code- base can tax the resources of the machine running the build. Do some pro- ﬁling of your tests and see where the bottleneck is. For example, if it is the database access that is causing most of the problems, try mocking out the real database and use an in-memory one instead. Conﬁgure the build pro- cess to distribute tests across several machines. See if different software could help manage resources better. Bring in experts from outside your team to help if needed.\n\nThe key to speeding up a continuous integration and build process is to take one small step at a time. Introduce changes one at a time so that you can measure each success separately and know you are on the right track. To start with, you may want to simply remove the most costly (in terms of time) tests to run nightly instead of on every build.\n\nA fast-running continuous integration and build process gives the greatest ROI of any automation effort. It’s the ﬁrst thing every team needs to automate.\n\n281",
      "content_length": 2575,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "282\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nSee the bibliogra- phy for links to build automation tools and books with more infor- mation about im- proving the build process.\n\nChapter 7, “Technology- Facing Tests that Support the Team,” goes into detail about some of the tools that can be used.\n\nWhen it’s in place, the team has a way to get quick feedback from the auto- mated tests. Next, we look at different types of tests that should be automated.\n\nUnit and Component Tests\n\nWe can’t overemphasize the importance of automating the unit tests. If your programmers are using TDD as a mechanism to write their tests, then they are not only creating a great regression suite, but they are using them to de- sign high-quality, robust code. If your team is not automating unit tests, its chances of long-term success are slim. Make unit-level test automation and continuous integration your ﬁrst priority.\n\nAPI or Web Services Testing\n\nTesting an API or web services application is easiest using some form of auto- mation. Janet has been on teams that have successfully used Ruby to read in a spreadsheet with all of the permutations and combinations of input variables and compare the outputs with the expected results stored in the spread- sheets. These data-driven tests are easy to write and maintain.\n\nOne customer of Janet’s used Ruby’s IRB (Interactive Ruby Shell) feature to test the web services for acceptance tests. The team was willing to share its scripts with the customer team, but the business testers preferred to watch to see what happened if inputs were changed on the ﬂy. Running tests interac- tively in a semiautomated manner allowed that.\n\nTesting behind the GUI\n\nSee Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” for speciﬁc tool examples.\n\nTesting behind the GUI is easier to automate than testing the GUI itself. Be- cause the tests aren’t affected by changes to the presentation layer and work on more stable business logic code, they’re more stable. Tools for this type of testing typically provide for writing tests in a declarative format, using tables or spreadsheets. The ﬁxtures that get the production code to operate on the test inputs and return the results can generally be written quickly. This is a prime area for writing business-facing tests, understandable to both custom- ers and developers that drive development.\n\nTesting the GUI\n\nEven a thin GUI with little or no business logic needs to be tested. The fast pace of agile development, delivering new functionality each iteration, man- dates some automated regression tests at the GUI level for most projects.",
      "content_length": 2643,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "See Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” for ex- amples of GUI test frameworks.\n\nSee Chapter 11, “Critiquing the Product Using Technology- Facing Tests,” for examples of load test automation tools.\n\nRead more about source code man- agement tools and IDEs in Chapter 7, “Technology- Facing Tests that Support the Team.”\n\nWHAT CAN WE AUTOMATE?\n\nTool selection is key for successful GUI automation. The automated scripts need to be ﬂexible and easy to maintain. Janet has used Ruby and Watir very successfully when the framework was developed using good coding practices, just as if it were a production application. Time was put into developing the libraries so that there was not a lot of rework or duplication in the code, and changes needed could be made in one place. Making the code easy to main- tain increased the ROI on these tests.\n\nA point about testability here—make sure the programmers name their ob- jects or assign IDs to them. If they rely on system-generated identiﬁers, then every time a new object is added to the page, the IDs will change, requiring changes to the tests.\n\nKeep the tests to just the actual interface. Check things like making sure the buttons really work and do what they are supposed to. Don’t try to try to test business functionality. Other types of tests that can be automated easily are link checkers. There is no need for someone to manually go through every link on every page to make sure they hit the right page. Look for the low- hanging fruit, automate the things that are simple to automate ﬁrst, and you’ll have more time for the bigger challenges.\n\nLoad Tests\n\nSome types of testing can’t be done without automation. Manual load tests aren’t usually feasible or accurate, although we’ve all tried it at one time or another. Performance testing requires both monitoring tools and a way to drive actions in the system under test. You can’t generate a high-volume at- tack to verify whether a website can be hacked or can handle a large load without some tool framework.\n\nComparisons\n\nVisually checking an ASCII ﬁle output by a system process is much easier if you ﬁrst parse the ﬁle and display it in a human-readable format. A script to compare output ﬁles to make sure no unintentional changes were made is a lot faster and more accurate than trying to compare them manually. File comparison tools abound, ranging from the free diff to proprietary tools such as WinDiff. Source code management tools, and IDEs have their own built-in comparison tools. These are essential items in every tester’s toolbox. Don’t forget about creating scripts for comparing database tables when do- ing testing for your data warehouse or data migration projects.\n\n283",
      "content_length": 2730,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "284\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nRepetitive Tasks\n\nAs we work with our customers to better understand the business and learn what’s valuable to them, we might see opportunities to automate some of their tasks. Lisa’s company needed to mail several forms with a cover letter to all of their clients. The programmers could not only generate the forms but could also concatenate them with the cover letter and greatly speed up the mailing effort. Lisa’s fellow tester, Mike Busse, wrote a spreadsheet macro to do complex calculations for allocating funds that the retirement plan admin- istrators had been doing manually. A lot of manual checklists can be replaced with an automated script. Automation isn’t just for testing.\n\nData Creation or Setup\n\nAnother useful area for automation is data creation or setup. If you are con- stantly setting up your data, automate the process. Often, we need to repeat something multiple times to be able to recreate a bug. If that can be auto- mated, you will be guaranteed to have the same results each time.\n\nLisa’s Story\n\nMany of our test schemas, including the ones used for automated regression suites, use canonical data. This canonical or “seed” data was originally taken from production. Some tables in the database, such as lookup tables, don’t change, so they never need to be refreshed with a new copy. Other tables, such as those containing retirement plan, employee, and transaction information, need to start from Ground Zero whenever a regression suite runs.\n\nOur database developer wrote a stored procedure to refresh each test schema from the “seed” schema. We testers may specify the tables we want refreshed in a special table called REFRESH_TABLE_LIST. We have an ant target for each test schema to run the stored procedure that refreshes the data. The automated builds use this target, but we use it ourselves whenever we want to clean up our test schema and start over.\n\nMany of our regression tests create their own data on top of the “seed” data. Our Watir tests create all of the data they need and include logic that makes them re- runnable no matter what data is present. For example, the script that tests an em- ployee requesting a loan from his or her retirement plan ﬁrst cancels any existing loans so a new one can be taken out.\n\nFitNesse tests that test the database layer also create their own data. We use a special schema where we have removed most constraints, so we don’t have to add every column of every table. The tests only add the data that’s pertinent to the functionality being tested. Each test tears down the data it created, so subse- quent tests aren’t affected, and each test is independent and rerunnable.\n\n—Lisa",
      "content_length": 2725,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "We discuss some logging and moni- toring tools in Chapter 10, “Business-Facing Tests that Critique the Product.”\n\nJanet’s Story\n\nWHAT SHOULDN’T WE AUTOMATE?\n\nCleaning up test data is as important as generating it. Your data creation toolkit should include ways to tear down the test data so it doesn’t affect a different test or prevent rerunning the same test.\n\nWe’ve looked at major areas where automation is required or at least useful. Our opinion is that whenever you need to do a test or some testing-related activity, ﬁrst decide whether it can be aided by automation. In some cases, automation won’t be appropriate. Let’s look at some of those.\n\nWHAT SHOULDN’T WE AUTOMATE? Some testing needs human eyes, ears, and intelligence. Usability and explor- atory testing are two that fall into that category. Other tests that may not jus- tify the automation investment are one-off tests and those that will never fail.\n\nUsability Testing\n\nReal usability testing requires someone to actually use the software. Automa- tion might be helpful in setting up scenarios to subsequently examine for us- ability. Observing users in action, debrieﬁng them on their experiences, and judging the results is a job for a person who understands that usability as- pects of software cannot be automated. Logging user actions is helpful for usability testing.\n\nWe had evaluated several GUI tools but decided to use Ruby with Watir. We kept our tests limited to GUI functions only. One of our tests was checking to make sure that correct validation messages were displaying on the screen. I was running the tests and happened to be watching the screen because I hadn’t seen this particu- lar test that one of the other testers created. My eyes caught something weird, but the test passed, so I replayed it again. One of the programmers had added a “$” to the screen, and the error message was displayed offset because of it. The correct message was displayed, just not in the right place. In this instance, the value in watching the tests run was huge because we were preparing to release fairly soon, and we probably wouldn’t have caught that particular problem.\n\nIt is possible to automate tests that make sure the GUI never changes, but you need to ask yourself whether it’s worth the cost. Do you really care that a but- ton has changed positions by one pixel? Do the results justify the effort? We don’t think you should automate “look and feel” testing, because an automated\n\n285\n\n—Janet",
      "content_length": 2478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "286\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nscript can only look for what you tell it to see. Automation would miss visual problems that would jump out at a human.\n\nExploratory Testing\n\nSee Chapter 10, “Business-Facing Tests that Critique the Product,” for more on explor- atory testing and tools that can fa- cilitate it.\n\nSimilarly, exploratory testing may be speeded up with scripts to create test data and jump through some setup steps, but it requires a skilled tester to de- sign and execute the tests. One major goal of exploratory testing is to learn more about the product by doing, and then use that information to improve future development. Automated scripts won’t do that for you. However, as we’ve said before, you won’t have time for exploratory testing without a lot of other automated tests.\n\nTests that Will Never Fail\n\nWe’ve heard an argument that tests that will never fail don’t need to be auto- mated. If a requirement is so obvious that there’s only one way to implement it, and no programmer will ever look at that code later without knowing ex- actly what it should do, the chances of someone introducing a defect in that code are next to nothing. Let’s say we have a form with address ﬁelds. Do we need an automated regression test to verify that the second street address line is not required? After we’ve veriﬁed it manually, how likely is it that someone will accidentally change it to a required ﬁeld later? Even if someone did, it wouldn’t be a catastrophic event. Someone else would notice it and people could work around it easily until it was ﬁxed.\n\nThen again, a test for it would be easy to include. And programmer tricks such as copy/paste errors happen all the time. If you feel comfortable that one-time manual testing does the job and that the risk of future failures doesn’t justify automating regression tests, don’t automate them. If your de- cision turns out to be wrong, you’ll get another chance to automate them later. If you aren’t sure, and it’s not terribly difﬁcult to automate, go for it.\n\nSee Chapter 18, “Coding and Test- ing,” for more about risk analysis and how it relates to testing.\n\nIf you’re testing a life-critical system, even a very small risk of a regression failure is too much. Use risk analysis to help decide what tests should be automated.\n\nOne-Off Tests\n\nMost times, manually executing a one-off test is sufﬁcient. If automating a test doesn’t have payoff, why do it? Sometimes automation is worth doing for a one-off test.",
      "content_length": 2510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "Lisa’s Story\n\nWHAT MIGHT BE HARD TO AUTOMATE?\n\nWe recently did a story to pop up a warning message dialog when posting a pay- roll, but the message should only come up during the ﬁrst two weeks of January. Automating a test for this functionality would require some way to simulate that the current date was between January 1 and January 15. That’s not terribly hard to do, but the consequences of a failure were fairly trivial, and we had more criti- cal stories to deliver that iteration. Automating that test at that time just didn’t have enough value to justify the cost, and the risk factor was low. We decided to test it manually.\n\nThere are other cases where doing a one-off test seems the most intuitive but automation is a better choice. We host sites for different business partners, and each one has unique content, look, and feel. Values in the database drive the correct behavior and content for each brand. Some of the data, such as fee schedules based on asset values and numbers of participants, are highly com- plex. It’s much easier and much more accurate to verify this data using FitNesse tests. We have a set of ﬁxtures that let us specify keys for the partner “brand” that we want to test. We can easily plug in the appropriate expected results from the spreadsheets that the business development staff creates for each new partner. These tests aren’t part of our regression suite. They’re used one time only to vali- date the new brand.\n\nTedious tasks may be worth automating, even if you don’t do them often. Weigh the automation cost against the amount of valuable time eaten up by manually doing the test. If it’s easy to do manually, and automating wouldn’t be quick, just keep it manual.\n\nWHAT MIGHT BE HARD TO AUTOMATE? When code isn’t written test-ﬁrst, or at least with test automation in mind, it’s much harder to automate. Older systems tend to fall into this category, but no doubt plenty of new code with the same untestable characteristics is still being produced.\n\nIf you’re faced with working on existing code that doesn’t already have auto- mated tests, you’re in for an uphill battle, but a winnable one. Legacy code may have I/O, database access, as well as business logic and presentation code intertwined. It may not be clear where to hook into the code to automate a test. How do you get started automating tests on such a system? You certainly can’t plan on automating everything below the GUI, because much of the logic is in the presentation layer.\n\n287\n\n—Lisa",
      "content_length": 2507,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "288\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nThere are at least a couple of different approaches that work well. The “hump of pain” we talked about in Chapter 13, “Why We Want to Automate Tests and What Holds Us Back,” is intimidating, but it can be overcome, and then test automation will become much easier. Michael Feathers’ Working Effectively With Legacy Code [2004] explains how to build a test harness around existing code bases and refactor them to accommodate automation. Even with legacy code, you can write tests to protect against introducing new problems. This approach can work even on systems that lack structure or aren’t object-oriented.\n\nChapter 7, “Technology- Facing Tests that Support the Team,“ goes into more detail about different agile approaches to legacy code.\n\nLisa’s team decided on a different but equally effective approach. The team members started ”strangling” the legacy code by writing all new features in a new test-friendly architecture. They’re gradually replacing all of the old code with code written test-ﬁrst. When they do work on old code to ﬁx bugs, or in the cases where the old code needs updating, they simply add unit tests for all of the code they change. A GUI smoke test suite covers the critical functions of the rest of the legacy system that has no unit tests.\n\nAs with any automation project, approach the hard-to-automate code one piece at a time, and address the highest risk areas ﬁrst. Solve the testability problem and ﬁnd a way to write unit-level tests. The effort will pay off.\n\nDEVELOPING AN AUTOMATION STRATEGY—WHERE DO WE START? A simple, step-by-step approach sounds incompatible with an automation strategy, but in agile testing we try to understand the problem ﬁrst. Deciding where and how to start with automation requires a bit of thought and discus- sion. As your team looks at testing challenges, you’ll need to consider where automation is appropriate. Before you start searching for a particular auto- mation tool, you’ll want to identify your requirements.\n\nYou need to understand what problem you are trying to solve. What are you trying to automate? For example, if you have no test automation of any kind, and you start by buying an expensive commercial test tool thinking it will au- tomate all your functional tests, you may be starting in the wrong place.\n\nWe suggest you start at the beginning. Look for your biggest gain. The biggest bang for the buck is deﬁnitely the unit tests that the programmers can do. In- stead of starting at the top of the test pyramid, you may want to start at the bottom, making sure that the basics are in place. You also need to consider",
      "content_length": 2666,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "DEVELOPING AN AUTOMATION STRATEGY—WHERE DO WE START?\n\nthe different types of tests you need to automate, and when you’ll need to have tools ready to use.\n\nIn this section, we assume you have automated Quadrant 1 unit and compo- nent tests in place, and are looking to automate your business-facing tests in Quadrants 2 and 3, or your Quadrant 4 technology-facing tests that critique the product. We’ll help you design a good strategy for building your automa- tion resources.\n\nThink about the skills and experience on your team. Who needs the automa- tion, and why? What goals are you trying to achieve? Understanding some of these issues may affect your choice of tools and what effort you expend. There is a section on evaluating tools at the end of this chapter.\n\nAutomation is scary, especially if you’re starting from scratch, so where do we begin?\n\nWhere Does It Hurt the Most?\n\nTo ﬁgure out where to focus your automation efforts next, ask your team, “What’s the greatest area of pain?” or, for some teams, “What’s the greatest area of boredom?” Can you even get code deployed in order to test it? Do team members feel conﬁdent about changing the code, or do they lack any safety net of automated tests? Maybe your team members are more ad- vanced, have mastered TDD, and have a full suite of unit tests. But they don’t have a good framework for specifying business-facing tests, or can’t quite get a handle on automating them. Perhaps you do have some GUI tests, but they’re extremely slow and are costing a lot to maintain.\n\nPeril: Trying to Test Everything Manually\n\nIf you’re spending all your time retesting features that you’ve tested before, not getting to new features, and needing to add more and more testing, you’re suffering from a severe lack of test automation. This peril means that testers don’t have time to participate in design and implementation discus- sions, regression bugs may creep in unnoticed, testing can’t keep up anymore with development, and testers get stuck in a rut. Developers aren’t getting in- volved in the business-facing testing, and testers don’t have time to ﬁgure out a better way to solve the testing problems.\n\nYour team can ﬁx this by developing an automation strategy, as we describe in this chapter. The team starts designing for testability and chooses and imple- ments appropriate automation tools. Testers get an opportunity to develop their technical skills.\n\n289",
      "content_length": 2422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "290\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nChapter 18, “Cod- ing and Testing,” has more informa- tion on a simple approach to risk analysis.\n\nWherever it hurts the most, that’s the place to start your automation efforts. For example, if your team is struggling to even deliver deployable code, you need to implement an automated build process. Nothing’s worse than twid- dling your thumbs while you wait for some code to test.\n\nBut, if performance puts the existence of your organization in danger, per- formance testing has to be the top priority. It’s back to understanding what problem you are trying to solve. Risk analysis is your friend here.\n\nJanet’s Story\n\nI worked on a legacy system that was trying to address some quality issues as well as add new features for our main customer. There were no automated unit or functional tests for the existing application, but we needed to refactor the code to address the quality issues. The team members decided to tackle it one piece at a time. As they chose a chunk of functionality to refactor, the programmers wrote unit tests, made sure they passed, and then rewrote the code until the tests passed again. At the end of the refactoring, they had testable, well-written code and the tests to go with them. The testers wrote the higher-level functional tests at the same time. Within a year, most of the poor-quality legacy code had been re- written, and the team had achieved good test coverage just by tackling one chunk at a time.\n\nTest automation won’t pay off unless other good development practices are in place. Continuous integration running a robust suite of unit tests is a ﬁrst step toward automating other tests. Code that’s continually refactored for main- tainability and good design will help increase the ROI on automation. Refac- toring can’t happen without that good unit test coverage. These development practices also need to be applied to the automated functional test scripts.\n\nMulti-Layered Approach\n\nWhile we recommend mastering one tool at a time, don’t expect too much out of any one tool. Use the right tool for each need. The tool that works best for unit tests may or may not be appropriate to automate functional tests. GUI, load, performance, and security testing may each require a different tool or tools.\n\nMike Cohn’s test pyramid concept (see Figure 14-2) has helped our teams put their automation efforts where they do the most good. We want to maximize the tests that have the best ROI. If the system architecture is designed for test- ability, test automation will be less expensive, especially at the unit level.\n\n—Janet",
      "content_length": 2628,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "Lisa’s Story\n\nDEVELOPING AN AUTOMATION STRATEGY—WHERE DO WE START?\n\nTests that go through the user interface usually have the lowest ROI, because they’re expensive to maintain, but we do need them. They make up the small tip of our pyramid We may choose to automate some of these tests, but the majority of GUI tests are deﬁned in business terms and probably are best left as human interaction tests (i.e., manual tests).\n\nThe middle layer represents the functional tests that work directly with produc- tion code, without a GUI or other layer in between. While they’re not as inex- pensive to automate as unit-level tests, and provide feedback a bit more slowly, the right tools allow them to have a good ROI. The fact that these tests can be written in a language the business experts understand adds to their value.\n\nThere are many different layers in the application that can be tested indepen- dently. In his book xUnit Test Patterns [2007], Gerard Meszaros refers to this as the Layer Test pattern. He cautions that when trying to test all of the layers of the application separately, we still have to verify that the layers are hooked up correctly, and this may require at least one test of the business logic through the presentation layer.\n\nAs my team built our automation framework one step at a time, we gathered an arsenal of tools. After implementing a continuous build framework with Ant and CruiseControl, we mastered JUnit for unit testing. We knew that unit test automa- tion is the quickest and cheapest way to automate, and provides the fastest feed- back to the programmers.\n\nOur legacy system had no automated tests, so we built a GUI regression test suite with Canoo WebTest. This provided good payback because the WebTest scripts were speciﬁed, not programmed. They were quick to write and easy to maintain.\n\nAfter JUnit and WebTest were in place, we experimented with FitNesse and found it worked well for functional testing behind the GUI. We found automating with FitNesse to go relatively quickly. Although FitNesse tests are signiﬁcantly more ex- pensive to produce and maintain than unit tests, their value in driving develop- ment and promoting collaboration among customers, programmers, and testers kept the ROI high.\n\nAll of these tools were easy to learn, implement, and integrate with the build pro- cess, and provided continual feedback about our regression issues. They were im- portant considerations when we were deciding on our test automation strategy.\n\nWhen evaluating the payback of your automation efforts, consider less tangi- ble aspects such as whether the tool promoted collaboration between the\n\n291\n\n—Lisa",
      "content_length": 2655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "292\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\ntechnical and customer teams. A primary reason to write tests is to help guide development. If the process of writing your automated acceptance tests results in a thorough understanding of business requirements, that’s plenty of payback, even if the tests never ﬁnd a single regression bug later on.\n\nThink about Test Design and Maintenance\n\nThink about all of the manual test scripts you’ve written in your life. Don’t you just wish those all would have been automated? Wouldn’t your life have been a lot easier? We believe that all scripted tests should be automated. Let’s get started converting those manual scripted tests.\n\nAfter you get started, it can be quite easy to automate tests. For example, when you have a working FitNesse ﬁxture, adding more test cases requires little ef- fort. This is great when you have a lot of different permutations to test. You’ll probably test more conditions than you would if all your testing was done manually. When Lisa’s team members rewrote their retirement plan’s loan sys- tem, they could test hundreds of different possibilities for loan payment pro- cessing via FitNesse tests. What happens when three loan payments are processed on the same day? If someone doesn’t make any payments for three months, and then sends in a large payment, is the interest calculated and ap- plied correctly? It was easy to write automated tests to ﬁnd out.\n\nThat’s a great advantage, but it has a down side. Now the team has dozens, or even hundreds, of test cases to maintain. What if the rules about calculating the amount of interest for a loan payment change a year from now? This could require updating every test. If your test tool doesn’t easily accommo- date making changes to existing tests, your big suite of automated tests can turn into a headache.\n\nEnd-to-end tests are particularly tricky to automate because they have the most potential to need maintenance as business rules change. How do we balance the need for automation with the cost?\n\nChapter 8, “Business-Facing Tests that Support the Team,” explains more about thin slices.\n\nTest Design Remember to start with the thin slice or steel thread of the feature you’re test- ing. Approach automation just as programmers approach coding. Get one small unit of the steel thread working, and then move on to the next. After you’ve covered the whole thin slice, go back and ﬂesh it out.",
      "content_length": 2441,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "See Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” for more information about effective test design.\n\nDEVELOPING AN AUTOMATION STRATEGY—WHERE DO WE START?\n\nChoose your test pattern thoughtfully. Automate all of the test cases you need, but no more, and automate them at the lowest level that you can. Limit the scope of each test case to one test condition or one business rule. Under- stand the purpose of the test. Avoid dependencies between tests, because they quickly increase complexity and maintenance expense.\n\nConsider Options As we’ve mentioned before, the lower the level at which you automate a test, the better the ROI. Push test automation as far down the pyramid as you can. If you have good coverage in your unit and code integration tests, you don’t need to automate as many functional tests. With solid coverage at the lower levels, it might be enough to do end-to-end tests manually to verify the sys- tem’s behavior. Use risk analysis to help you decide.\n\nUser Interface The user interface does need to be tested. In some situations, test automation at the GUI level is critical. Perhaps your team is using third-party GUI con- trols, and you aren’t sure how they will behave. If your risk and ROI analysis supports a lot of automation at the GUI level, make the investment.\n\nIf you do automate at the higher levels, don’t go overboard and automate ev- ery possible path through the system. You don’t have to keep every auto- mated test created during the development phase in the regression suite; consider tradeoffs of build time and the chance of ﬁnding defects. Focus your efforts on covering every important path through the code at the unit, code integration, and functional levels. You’ll get a much better payback.\n\nStrike a Balance Striking a balance isn’t an agile principle, it’s just common sense. You need a good-enough solution right now, but it doesn’t have to be perfect. Does the tool provide the results you need right now? Does it provide an adequate re- turn on the resources needed to use it for automation? If so, go ahead and use it, and budget time later to look for alternatives. You can improve your auto- mation framework over time. The most important factor is whether your au- tomation tools ﬁt your particular situation right now.\n\nDon’t slide the other way, and think, “OK, we can generate a bunch of scripts with this record tool, get our immediate testing done, and refactor the scripts later to make them maintainable.” While you don’t need to keep searching for the perfectly ideal automation solution, you do need a solution that doesn’t\n\n293",
      "content_length": 2620,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "294\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nadd to your team’s technical debt. Find a balance between “It ﬁnds the bugs we need to know about and doesn’t cost too much to maintain” and “This is the most elegant and cool solution we can ﬁnd.”\n\nChoosing the Right Tools\n\nIt’s cool that we have so many tools available to help us solve our automation problems. Don’t go for more sophistication than you need. Lisa’s coworkers have found that a spreadsheet that retrieves data from the database and per- forms calculations independently of the system is a powerful tool, both for driving development and for verifying the application’s calculations.\n\nWe usually minimize test automation at the GUI layer, but there are situa- tions where more GUI automation is appropriate. If the user makes a change at X, what else changes? Some problems only manifest themselves at the GUI level. Lisa tested a bug ﬁx that addressed a back-end problem when retire- ment plan participants requested a distribution of money from their ac- counts. The change was surrounded by unit tests, but it was a GUI regression test that failed when the distribution form failed to pop up upon request. Nobody anticipated that a back-end change could affect the GUI, so they probably wouldn’t have bothered to test it manually. That’s why you need GUI regression tests, too.\n\nWe’ve talked about some disadvantages of record/playback tools, but they’re appropriate in the right situation. You may be using a record/playback tool for a good reason: Maybe your legacy code already has a suite of automated tests created in that tool, your team has a lot of expertise in the tool, or your management wants you to use it for whatever reason. You can use recorded scripts as a starting point, then break the scripts into modules, replace hard- coded data with parameters where appropriate, and assemble tests using the modules as building blocks. Even if you don’t have much programming ex- perience, it’s not hard to identify the blocks of script that should be in a mod- ule. Login, for example, is an obvious choice.\n\nRecord/playback may also be appropriate for legacy systems that are de- signed in such a way that makes unit testing difﬁcult and hand-scripting tests from scratch too costly. It’s possible to build a record and playback capability into the application, even a legacy application. With the right design, and the use of some human-readable format for the recorded interaction, it’s even possible to build playback tests before the code is built.",
      "content_length": 2543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "DEVELOPING AN AUTOMATION STRATEGY—WHERE DO WE START?\n\nGUI Test Automation: From the Dark Ages to Successful Automation in an Agile Environment\n\nPierre Veragen, SQA Lead at iLevel by Weyerhaeuser, explains how his team used a tool that provided both record/playback and scripting capability pro- ductively in a waterfall environment, and then leveraged it when the com- pany adopted agile development.\n\nBack in our waterfall development days, in 2000, we started doing GUI test automation using a record-playback approach. We quickly accumu- lated tens of thousands of lines of recorded scripts that didn’t meet our testing needs. When I took over 18 months later, I quickly became con- vinced that the record-playback approach was for the dinosaurs.\n\nWhen we had a chance to obtain a new test automation tool at the end of 2003, we carefully evaluated tools with these criteria in mind: record capability to help us understand the scripting language, and the ability to build an object-oriented library to cover most of our needs, including test reporting. At the time, TestPartner from CompuWare fulﬁlled all of our requirements.\n\nWe started using TestPartner on a highly complex, CAD-with-engineering application, built in Visual Basic 6, still using a waterfall process. Before we started automating tests, our releases were quickly followed by one or more patches. We focused our automation efforts toward checking the engineering calculations through the GUI, and later, the actual posi- tion of the CAD details. These tests included hundreds of thousands of individual veriﬁcation points, which could never have been done by hand. Within a year, having added a solid set of manual tests of the user interaction, in addition to our automated tests, we were releasing robust software without the usual follow-up patches. We felt conﬁdent about our combination of manual and automated tests, which didn’t include a single line of recorded scripts.\n\nIn 2004, our group moved to Visual Basic .NET. I spent several months adapting our TestPartner library to activate .NET controls. In 2006, we adopted an Agile methodology. Building on lessons previously learned in the non-Agile world, we achieved astonishing results with test automa- tion. By the end of 2006, team members were able to produce maintain- able GUI test scripts and library components after just a few days of training. At the same time, the team embraced unit testing with NUnit and user acceptance tests with FitNesse.\n\nAs of this writing, issues are caught at all three levels of our automated testing: Unit, FitNesse, and GUI. The issues found by each of the three testing tiers are of a different nature. Because everything is automated and triggered automatically, issues are caught really fast, in true Agile fashion. Each part of our test automation is bringing value.\n\n295",
      "content_length": 2848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "296\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nSome people feel resources would be better spent on architecture and design, so that GUI test automation isn’t needed. In our development group, each team made its own decision about whether to automate GUI tests.\n\nIn case you decide to use GUI test automation, here’s some advice: Stay away from recorded scripts, invest in maintainability, and minimize the required GUI testing with a good architecture of the application. It is my experience that investing in good GUI test automation practices will al- ways pay off.\n\nPierre’s advice reﬂects well how good development practices, especially those followed in agile development projects, apply to automated test develop- ment as well as to production code development.\n\nBuilt-In Record & Playback\n\nGerard Meszaros, agile coach and author of xUnit Test Patterns [2007], de- scribes a situation where the simplest approach turned out to be record/ playback. We’ve mentioned drawbacks to record/playback tools, but if you design your code to support them, they can be the best approach.\n\nI was asked to help a team that was porting a “safety sensitive” applica- tion from OS2 to Windows. The business was very concerned about the amount of time it would take it to retest the ported system and the like- lihood that the team would miss important bugs. The system was de- signed to only offer the user valid choices that would not compromise safety. They considered using a test recording tool to record tests on the old system and play them back on the new system, but there were no test recording tools available for both OS2 and Windows that could deal with windows drawn using ASCII characters. After reviewing the architec- ture of the system, we determined that writing xUnit tests would not be a cost-effective way to test the system because much of the business logic was embedded in the user interface logic, and refactoring the code to separate them would be too risky and time-consuming. Instead, we proposed building a Record & Playback test capability right into the system before we ported it.\n\nEven though the rest of the project was milestone-driven, we developed the built-in test mechanism in a very agile way. Each screen required at least one new hook and sometimes several. We started with the most frequently used screens, adding the necessary hooks to record the user’s actions and the systems responses to them into an XML ﬁle. We also added the hooks to play back the XML and determine the test results. Initially, we focused our efforts on proving the concept by hooking only",
      "content_length": 2609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "See more exam- ples of speciﬁc tools for business- facing tests in Chapter 9, “Tool- kit for Business- Facing Tests that Support the Team.”\n\nDEVELOPING AN AUTOMATION STRATEGY—WHERE DO WE START?\n\nthe screens we needed to record and then playing back a simple but re- alistic test. After everyone was convinced the approach would work, we prioritized the screens with respect to how much beneﬁt it would pro- vide. We implemented the hooks one by one until we could automate a signiﬁcant portion of the tests. We also built an XSLT stylesheet that would format the XML in a Fit-like way, with green cells indicating ac- ceptable results and red cells indicating a failed test step.\n\nIn the meantime, the client was identifying the test scenarios that needed test cases. As we ﬁnished enough screens to record a particular test, the client would “acceptance test” our hooks by recording and playing back (still on OS2) the test(s) that were waiting for those hooks. When all of the hooks were in place, we could go ahead and port the code, including the test hooks, from OS2 to Windows. After verifying successful playback on OS2, the client would move the XML test ﬁles over to Windows and run them against the ported version of the code. The client found this quite easy to do and was able to record a large number of tests in a relatively short period of time. Because the tests were recording actions and responses in business terms, the tests were fairly easy to understand. The client loved the capability, and still raves about how much effort it saved and how much more conﬁdence it has in the product. “Not only did this save tens of man-years of testing effort, but it even uncovered hidden unknown bugs in the legacy system, which we had considered to be the gold standard.”\n\nIn Gerard’s story, the team worked together to retroﬁt testability onto a sys- tem that wasn’t designed for testability. They gave their customers a way to capture their test scenarios on one platform and play them back on both platforms to verify the successful port. This is a stellar example of the whole- team approach. When everyone on the team collaborates on a test automa- tion solution, there’s a much better chance it’s going to succeed.\n\nSome agile teams get value from commercial or open source test tools, while others prefer a completely customized approach. Many testers ﬁnd value writ- ing simple scripts in a scripting language such as Ruby, or a shell, to automate mundane but necessary tasks, generate test data, or drive other tools. Books such as Everyday Scripting with Ruby for Teams, Testers, and You give a road- map for this approach. If you’re a tester without a strong programming back- ground, we encourage you to pick up a book, ﬁnd an online tutorial, or take a class on a scripting language, and see how easy it can be to write useful scripts.\n\nWhat we’re trying to tell you is that you can use many different tools. Look at the problem you are trying to solve and decide as a team the easiest and most effective way to solve it. Every so often, step back and take a look at the tools\n\n297",
      "content_length": 3105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "298\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nyou’re using. Is everyone on the team happy with them? Are you missing prob- lems because you don’t have the right tools? Budget time to explore new tools and see if they might ﬁll gaps or replace a tool that isn’t paying off.\n\nIf your team is new to agile development, or working on a brand-new project, you might be faced with choosing tools and setting up test environments during the early iterations, when you also might be working on high-risk sto- ries. Don’t expect to be able to deliver much business value if you’re still cre- ating your test infrastructure. Plan in lots of time for evaluating tools, setting up build processes, and experimenting with different test approaches.\n\nAPPLYING AGILE PRINCIPLES TO TEST AUTOMATION Every team, every project, and every organization has a unique situation with unique automation challenges. Each has its own culture, history, resources, business pressures, products, and experience. No matter what your team’s situation, you can use the agile principles and values discussed in Chapter 2 to help you ﬁnd solutions. Concepts such as courage, feedback, simplicity, communication, continuous improvement, and responding to change aren’t just agile ideas—they’re qualities that are common to all successful teams.\n\nKeep It Simple\n\nThe agile maxim of “do the simplest thing that could possibly work” applies to tests as well as code. Keep the test design simple, keep the scope minimal, and use the simplest tool that will do the job.\n\nSimplicity is a core agile value for a good reason. The best place to start is the simplest approach you can think of. However, doing the simplest thing doesn’t mean doing the easiest thing. It involves really thinking about what you need now and taking baby steps to get there. By keeping things simple, if you do make a bad choice, you won’t go too far off track before realizing the error of your ways.\n\nIt’s easy to get involved in a task and slip away from the basics into some in- triguing challenge. Weigh the ROI of every automation task before you do it. Automation is fun (when you get past the scary part of getting started). It’s tempting to try something difﬁcult just because you can. Like all other as- pects of testing in an agile development project, the only way to keep up is to do only the minimum required.\n\nUse the simplest tool you can get away with. Remember the test pyramid. If a customer-facing test can be most easily automated at the unit level, do it",
      "content_length": 2524,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "Lisa’s Story\n\nAPPLYING AGILE PRINCIPLES TO TEST AUTOMATION\n\nthere. Lisa sometimes writes test cases in FitNesse, only to learn the program- mers can automate them much faster as JUnit tests. Conversely, sometimes the programmers use FitNesse for TDD instead of JUnit, because the code they’re writing lends itself to testing in one of the FitNesse ﬁxture formats.\n\nIterative Feedback\n\nShort iterations allow us to experiment with various automation approaches, evaluate results, and change course as quickly as needed. Commit to an auto- mation effort, such as developing a test framework in-house or implement- ing an open source tool for at least a couple of iterations. After each iteration, look at what’s working and what’s not working. Think of ideas to overcome problems, and try those in the next iteration. If it’s not the right solution, try something else for a few iterations. Don’t get sucked into a quagmire where you’ve put so many resources into a tool, and have so many tests that use it, that you feel you can’t switch tools. Between the many open source and com- mercial tools, plus programmers’ ability to write home-grown test tools, there’s no reason to settle for less than the optimum tool.\n\nOne of my early XP teams struggled to ﬁnd a good way to automate customer- facing acceptance tests for a Java-based web application. This was back when there were far fewer tool options for agile teams. First, we tried an open source tool that simulated a browser, but it lacked the features we required. It just wasn’t quite robust enough. We discussed this at the next retrospective.\n\nWe decided to try using the unit testing tool for testing behind the GUI for the next two iterations. By committing to two iterations, we felt we were giving our- selves enough time to give the tool a good try, but not so much time that we would have too much invested if it weren’t the right solution. The customers found the unit tests hard to read, and there was logic in the GUI we couldn’t test with this tool.\n\nAfter another discussion during our retrospective, we then committed to two iter- ations of using a vendor GUI test tool I had used extensively on previous projects. The Java programmers found it slow going because the tool used a proprietary scripting language, but it worked well enough to do the minimum automation needed. After two iterations, we decided that it wasn’t ideal, but at the time there weren’t a lot of other options, and it was the best one we had.\n\nIn hindsight, we should have kept looking for a better option. Perhaps we could have developed our own test harness. We were able to automate about 60% of the regression tests above the unit level using the vendor tool, which seemed great at the time. If we had pushed ourselves a little more, we might have done a lot better.\n\n299\n\n—Lisa",
      "content_length": 2826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "300\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nUse iterations to your advantage. They facilitate a step-wise approach. If your idea’s a dud, you’ll know quickly and have a chance to try a different one. Don’t be afraid to keep looking, but don’t keep looking for the perfect solu- tion if one you try performs adequately.\n\nWhole-Team Approach\n\nAgile development can’t work without automation. Fortunately, the whole- team approach, which we explored in Chapter 1, means that a wider range of skills and resources are available to ﬁnd and implement a useful automation strategy. Attacking the problem as a team means it’s more likely that code will be designed for testability. Programmers, testers, and other team members will collaborate to automate tests, bringing multiple viewpoints and skill sets to the effort.\n\nThe whole-team approach helps overcome the fear barrier. Automation tasks can be overwhelming to start with. Knowing there are other people with dif- ferent skills and experience to help gives us courage. Being able to ask for and receive help gives us conﬁdence that we can achieve adequate coverage with our automated tests.\n\nLisa’s Story\n\nMy current team made a commitment to automating regression tests at all levels where it made sense. Here are some examples of where I’ve asked for help to suc- ceed with automation.\n\nEarly on, when we had no automated tests at all and the developers were trying to master test-driven development, we settled on Canoo WebTest for the GUI smoke tests. I needed a bit of help understanding how to conﬁgure WebTest to run in our environment, and I needed a lot of help to run the tests from the auto- mated build process. I asked our system administrator (who was also one of the programmers) to help. We quickly got a suite of tests running in the build.\n\nLater, I really wanted to try FitNesse for functional testing behind the GUI. I had to be patient while the programmers were still getting traction with the automated unit tests. The team agreed to try the tool, but it was hard to ﬁnd time to start us- ing it. I picked a story that seemed suited to FitNesse tests, and asked the pro- grammer working on the story if I could pair with him to try some FitNesse tests. He agreed, and we got some tests automated in FitNesse. The programmer found it easy and worthwhile, and gave a good report to the rest of the team.\n\nAfter that, it wasn’t hard to approach each programmer, suggest writing FitNesse tests for the story he was working on, and let him see the results. The FitNesse tests found test cases the programmer hadn’t thought of, and they saw the bene- ﬁt right away. When everyone on the team had some experience with the tool, they were not only happy to automate the tests, but started designing code in a way that would make writing FitNesse ﬁxtures easier.",
      "content_length": 2843,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "Chapter 11, “Critiquing the Product Using Technology- Facing Tests,” talks about technology- facing tests such as these and differ- ent approaches to handling them.\n\nAPPLYING AGILE PRINCIPLES TO TEST AUTOMATION\n\nWhen our Ruby expert, who designed most of our Watir test suite, left the com- pany, I was quite concerned about maintaining our huge suite of tests as well as being able to code new ones. My Ruby expertise was not as good as his (plus, we were down to just one tester, so time was an issue). Every programmer on the team went out, bought a book on Ruby, and helped when I had problems updat- ing scripts to work when the code changed. One programmer even wrote a new script to test a new story when I didn’t have time for that task. When we hired a new tester, he and I were able to handle the care and feeding of the Watir scripts, so the programmers no longer needed to take on those tasks.\n\nI know I can ask teammates for help with automation issues, and the entire team sees automation as a priority, so the programmers always think about testability when designing the code. This is an example of the whole-team approach at work.\n\nSpecialized technology-facing tests such as security or load testing might re- quire bringing in experts from outside the team. Some companies have spe- cialist teams that are available as shared resources to product teams. Even while taking advantage of these resources, agile teams should still take re- sponsibility for making sure all types of testing are done. They may also be surprised to ﬁnd that team members may have the skills needed if they take a creative approach.\n\nSome organizations have independent test teams that do post-development testing. They may be testing to ensure the software integrates with other sys- tems, or conducting other specialized testing such as large-scale performance testing. Development teams should work closely with these other teams, us- ing feedback from all testing efforts to improve code design and facilitate automation.\n\nTaking the Time to Do It Right\n\nSolving problems and implementing good solutions takes time. We must help our management understand that without enough time to do things the right way, our technical debt will grow, and our velocity will slow. Implementing solutions the “right” way takes time up front but will save time in the long term. Consider the time it takes for brainstorming ideas, solutions, formal training, and for on-the-job learning.\n\nYour organization’s management is understandably interested in producing results as quickly as possible. If management is reluctant to give the team time to implement automation, explain the trade-offs clearly. Delivering some fea- tures in the short term without automated regression tests to make sure they\n\n301\n\n—Lisa",
      "content_length": 2791,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "302\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nkeep working will have a big cost down the line. As your team accumulates technical debt, you’ll be less able to deliver the business value management needs. Work toward a compromise. For example, cut the scope of a feature but keep the essential value, and use automation to deliver and maintain a better product.\n\nWe always have deadlines, and we always feel pressed for time. The tempta- tion to just go back to doing things the way we always have, like executing re- gression tests manually and hoping for the best, is always there even though we know that doesn’t work. There is never enough time to go back and ﬁx things. During your next planning meeting, budget time to make meaningful progress on your automation efforts.\n\nLisa’s Story\n\nOur team focuses on taking time for good design, a strong set of automated tests, and ample time for exploratory testing. Quality, not speed, has always been our goal. Our production problems cost a lot to ﬁx, so the whole company is on board to take the time to prevent them. Sometimes we don’t pick the right de- sign, and we aren’t afraid to rip it out and replace it when we realize it.\n\nNaturally there are business tradeoffs, and the business decides whether to pro- ceed with known risks. We work to explain all of the risks clearly and give exam- ples of potential scenarios.\n\nHere are a couple of recent examples of taking the time to do things right. We started a theme to make major changes to the account statements for the retire- ment plans. One of the programmers, Vince Palumbo, took on the task of collect- ing additional data to be used for the statements. He decided to write robust unit tests for the data collection functionality, even though this meant the story would have to continue on to the next iteration. Writing the unit tests took a great deal of time and effort, and even with the tests, the code was extremely complex and difﬁcult to do. A couple of iterations later, another programmer, Nanda Lankala- palli, picked up another story related to the data collection and was pleasantly surprised to ﬁnd new unit tests. He was able to make his changes quickly, and the testing effort was greatly reduced because the unit tests were in place.\n\nLater, we found we had missed an edge case where some calculations for the change in account value were incorrect. The combination of automated unit tests and a great deal of exploratory testing were not enough to catch all of the scenar- ios. Still, having the tests meant Vince could write his corrected code test-ﬁrst and feel more conﬁdent that the code was now correct.\n\nAnother recent example concerned processing of incoming checks. The business wanted to shorten the two-step process to one step, which meant the money would be invested in the retirement plan accounts two days earlier than was then possible. The existing process was all written in legacy code, without unit tests. We discussed whether to rewrite the processing in the new architecture. Our product owner was concerned about the amount of time this might take. We felt",
      "content_length": 3123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "APPLYING AGILE PRINCIPLES TO TEST AUTOMATION\n\nit would take just as long to change the existing code as to completely rewrite it, because the old code was difﬁcult to understand and had no unit tests at all. We decided on the rewrite, which not only reduced the risk of problems in this criti- cal functionality but also gave us the opportunity to provide a couple of extra fea- tures at little extra cost. So far, this strategy has proven worthwhile.\n\nAllow yourself to succeed. Work at a sustainable pace. Take the time to refac- tor as you go or you’ll end up with a mess eventually. As testers, we always have many different tasks to do. If you’re learning a new tool or trying to au- tomate new tests, don’t multitask. Find a big block of time and focus. This is hard, but switching gears constantly is harder.\n\nIf business stakeholders are impatient for your team to “just get it done,” ana- lyze the problem with them. What are the risks? How much will a production problem cost? What are the beneﬁts of releasing a quick hack? How much technical debt will it add? What’s the long-term return on investment of a solid design supported with automated tests? How will each approach affect company proﬁtability and customer satisfaction? What about the intangible costs, such as the effect that doing poor-quality work has on team morale? Sometimes the business will be right, but we’re betting that you’ll usually ﬁnd that up-front investment pays off.\n\nLearn by Doing\n\nEveryone learns in different ways, but when you’ve decided how you’re going to automate a test, jump in and start doing it. In Everyday Scripting with Ruby for Teams, Testers, and You [2007], Brian Marick advises to learn to program by writing a program. Make mistakes! The more problems you have, the more you’ll learn. Getting someone to pair with you will help speed up learn- ing, even if neither one of you is familiar with the tool or the language.\n\nIf you don’t have anyone to pair with, talk to the “rubber ducky”: Imagine you’re describing the problem to a coworker. The process of explaining can often make the cause of the problem jump into view. Simply reading a test aloud to yourself can help you ﬁnd the weaknesses in it.\n\nApply Agile Coding Practices to Tests\n\nTests are just as valuable as production code. In fact, production code isn’t much without tests to support it. Treat your tests the same way you treat all code. Keep it in the same source code control tool as your production code.\n\n303\n\n—Lisa",
      "content_length": 2495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "304\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nYou should always be able to identify the versions of test scripts that go with a particular version of code.\n\nPairing, refactoring, simple design, modular and object-oriented design, good standards, keeping tests as independent as possible—all of the qualities of good code are also qualities of good automated tests. Agile development is some- times perceived by the uninformed to be chaotic or lax, when in fact it is highly disciplined. Undertake your automation tasks with the greatest discipline, pro- ceeding in small steps, checking in each step that succeeds. If you’re program- ming automated scripts, write them test-ﬁrst, just as any agile programmer would write production code. Keep simplicity in mind, though. Don’t write fancy test scripts with lots of logic unless there’s a good ROI. Those tests need testing and cost more to maintain. Specify tests when you can instead of coding them, and always go with the simplest approach possible.\n\nWe can’t emphasize it enough: Test automation is a team effort. The varying experience, skills, and perspectives of different team members can work to- gether to come up with the best approach to automation. Innovate—be cre- ative. Do what works for your unique situation, no matter what the “common wisdom” says.\n\nAutomation tools are just one piece of the puzzle. Test environments and test data are essential components. Let’s look at test data next.\n\nSUPPLYING DATA FOR TESTS No matter what tool we use to automate tests, the tests need data to process. Ideally, they need realistic data that matches production data. However, pro- duction databases usually contain lots and lots of data, and they can be highly complex. Also, database access slows down tests exponentially. Like so much of agile testing, it’s a balancing act.\n\nData Generation Tools\n\nAs we write this book, there are several cool tools available to generate test data for all kinds of input ﬁelds and boundary conditions. Open source and commercial tools such as Data Generator, databene benerator, testgen, Datatect, and Turbo Data are available to generate ﬂat ﬁles or generate data directly to database tables. These tools can generate huge varieties of differ- ent types of data, such as names and addresses.",
      "content_length": 2301,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "Lisa’s Story\n\nSUPPLYING DATA FOR TESTS\n\nIt’s also fairly easy to generate test data with a home-grown script, using a scripting language such as Ruby or Python, a tool such a Fit or FitNesse, or a shell script.\n\nOur Watir scripts create randomized test data inputs, both to ensure they are re- runnable (they’re unlikely to create an employee with the same SSN twice), and to provide a variety of data and scenarios. The script that creates new retirement plans produces plans with about 200 different combinations of options. The script that tests taking out a loan randomly generates the frequency, reason, and term of the loan, and veriﬁes that the expected payment is correct.\n\nWe have utility scripts to create comma-separated ﬁles for testing uploads. For example, there are several places in the system that upload census ﬁles with new employee information. If I need a test ﬁle with 1,000 new employees with random investment allocations to a retirement plan, I can simply run the script and specify the number of employees, the mutual funds they’re investing in, and the ﬁle name. Each record will have a randomly generated Social Security Number, name, address, beneﬁciaries, salary deferral amounts, and investment fund allocations. Here’s a snippet of the code to generate the investment calculations.\n\n# 33% of the time maximize the number of funds chosen, 33% of the time # select a single fund, and 33% of the time select from 2-4 funds fund_hash = case rand(3) when 0: a.get_random_allocations(@fund_list.clone) when 1: a.get_random_allocations(@fund_list.clone, 1) when 2: a.min_percent = 8; a.get_random_allocations(@fund_list.clone, rand(3) + 2) end emp['fund_allocations'] = fund_hash_to_string(fund_hash)\n\nScripts like these have dual uses, both as regression tests that cover a lot of differ- ent scenarios and exploratory test tools that create test data and build test sce- narios. They aren’t hard to learn to write (see the section “Learning by Doing“ earlier in this chapter).\n\nScripts and tools to generate test data don’t have to be complex. For example, PerlClip simply generates text into the Windows clipboard so it can be pasted in where needed. Any solution that removes enough tedium to let you dis- cover potential issues about the application is worth trying. “The simplest thing that could possibly work” deﬁnitely applies to creating data for tests. You want to keep your tests as simple and fast as possible.\n\n305\n\n—Lisa",
      "content_length": 2460,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "306\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nAvoid Database Access\n\nYour ﬁrst choice for testing should try to have tests that can run completely in-memory. They will still need to set up and tear down test data, but the data won’t store in a database. Each test is independent and runs as quickly as any test could. Database access means I/O and disks are inherently slow. Ev- ery read to the database slows down your test run. If your goal is to give fast feedback to the team, then you want your tests to run as quickly as possible. A fake object such as an in-memory database lets the test do what it needs to do and still give instant feedback.\n\nLisa’s Story\n\nComprehensive explanations and examples of vari- ous types of test doubles can be found in xUnit Test Patterns. See the bibliography for more information on that and tools for working with test stubs and with mock and fake objects.\n\nOne of our build processes runs only unit-level tests, and we try to keep its run- time less than eight minutes, for optimum feedback. The tests substitute fake ob- jects for the real database in most cases. Tests that are actually testing the database layer, such as persisting data to the database, use a small schema with canonical data originally copied from the production database. The data is realis- tic, but the small amount makes access faster.\n\nAt the functional test level, our FitNesse test ﬁxtures build data in-memory wher- ever possible. These tests run quickly, and the results appear almost instanta- neously. When we need to test the database layer, or if we need to test legacy code that’s not accessible independently of the database layer, we usually write FitNesse tests that set up and tear down their own data using a home-grown data ﬁxture. These tests are necessary, but they run slowly and are expensive to main- tain, so we keep them to the absolute minimum needed to give us conﬁdence. We want our build that runs business-facing tests to provide feedback within a couple of hours in order to keep us productive.\n\nTools such as DbFit and NdbUnit can simplify database testing and en- able test-driven database develop- ment; see the bib- liography for more resources.\n\nBecause it’s so difﬁcult to get traction on test automation, it would be easy to say “OK, we’ve got some tests, and they do take hours to run, but it’s better than no tests.” Database access is a major contributor to slow tests. Keep tak- ing small steps to fake the database where you can, and test as much logic as possible without involving the database. If this is difﬁcult, reevaluate your system architecture and see if it can be organized better for testing.\n\nIf you’re testing business logic, algorithms, or calculations in code, you’re in- terested in the behavior of the code itself given certain inputs; you don’t care where the data comes from as long as it accurately represents real data. If this is the case, build test data that is part of the test and can be accessed in mem- ory, and let the production code operate from that. Simulate database access and objects, and focus on the purpose of the test. Not only will the tests run faster, but they’ll be easier to write and maintain.\n\n—Lisa",
      "content_length": 3220,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "Lisa’s Story\n\nSUPPLYING DATA FOR TESTS\n\nWhen generating data for a test, use values that reﬂect the intent of the test, where possible. Unless you’re completely conﬁdent that each test is indepen- dent, generate unique test values for each test. For example, use timestamps as part of the ﬁeld values. Unique data is another safety net to keep tests from infecting each other with stray data. When you need large amounts of data, try generating the data randomly, but always clean it up at the end of the test so that it doesn’t bleed into the next test. We recognize that sometimes you need to test very speciﬁc types of data. In these cases, randomly generated data would defeat the purpose of the test. But you may be able to use enough randomization to ensure that each test has unique inputs.\n\nWhen Database Access Is Unavoidable or Even Desirable\n\nIf the system under test relies heavily on the database, this naturally has to be tested. If the code you’re testing reads from and/or writes to the database, at some point you need to test that, and you’ll probably want at least some re- gression tests that verify the database layer of code.\n\nSetup/Teardown Data for Each Test Our preferred approach is to have every test add the data it needs to a test schema, operate on the data, verify the results in the database, and then de- lete all of that test data so the test can be rerun without impacting other sub- sequent tests. This supports the idea that tests are independent of each other.\n\nWe use a generic data ﬁxture that lets the person writing the test specify the data- base table, columns, and values for the columns in order to add data. Another ge- neric data lookup ﬁxture lets us enter a table name and SQL where clause to verify the actual persisted data. We can also use the generic data ﬁxture to delete data using the table name and a key value. Figure 14-3 shows an example of a table that uses a data ﬁxture to build test data in the database. It populates the table\n\nFigure 14-3 Example of a table using a data ﬁxture to build test data in the database\n\n307",
      "content_length": 2084,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "308\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\n“all fund” with the speciﬁed columns and values. It’s easy for those of us writing test cases to populate the tables with all of the data we need.\n\nNote that the schemas we use for these tests have most of their constraints re- moved, so we only have to populate the tables and columns pertinent to the functionality being tested. This makes maintenance a little easier, too. The down- side is that the test is a bit less realistic, but tests using other tools verify the func- tionality with a realistic environment.\n\nThe downside to creating test data this way is that whenever a change is made in the database, such as a new column with a required value, all of the data ﬁxture tables in the tests that populate that table will have to be changed. These tests can be burdensome to write and maintain, so we only use them when absolutely needed. We try to design the tests to keep maintenance costs down. For exam- ple, the data ﬁxture in Figure 14-3 is in an “include” library and can be included into the tests that need it. Let’s say we add a new column, “fund_category.” We only need to add it to this “include” table, rather than in 20 different tests that use it.\n\nCanonical Data Another alternative is having test schemas that can quickly be refreshed with data from a canonical or seed database. The idea is that this seed data is a repre- sentative sample of real production data. Because it’s a small amount of data, it can be quickly rebuilt each time a suite of regression tests needs to be run.\n\nThis approach also increases the time it takes to run tests, but it’s just a few minutes at the start of the regression suite rather than taking time out of each individual test. The tests will still be slower than tests that don’t access the da- tabase, but they’ll be faster than tests that have to laboriously populate every column in every table.\n\nCanonical data has many uses. Testers and programmers can have their own test schema to refresh at will. They can conduct both manual and automated tests without stepping on anyone else’s testing. If the data is carefully chosen, the data will be more realistic than the limited amount of data each test can build for itself.\n\nOf course, as with practically everything, there’s a downside. Canonical data can be a pain to keep up. When you need new test scenarios, you have to iden- tify production data that will work, or make up the data you need and add it to the seed schema. You have to scrub the data, mask real peoples’ identifying characteristics, making it innocuous for security reasons. Every time you add\n\n—Lisa",
      "content_length": 2646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "SUPPLYING DATA FOR TESTS\n\na table or column to the production database, you must update your test sche- mas accordingly. You might have to roll date-sensitive data forward every year, or do other large-scale maintenance. You have to carefully select which tables should be refreshed and which tables don’t need refreshing, such as lookup ta- bles. If you have to add data to increase test coverage, the refresh will take longer to do, increasing the time of the build process that triggers it. As we’ve been emphasizing, it’s important that your automated builds provide feed- back in a timely manner, so longer and longer database refreshes lengthen your feedback cycle. You also lose the test independence with canonical data, so if one test fails, others may follow suit.\n\nLisa’s team members run their GUI test suites and some of their functional regression tests against schemas refreshed each run with canonical data. On rare occasions, tests fail unexpectedly because of an erroneous update to the seed data. Deciding whether to “roll” data forward, so that, for example, 2008’s rows become 2009’s rows, gets to be a headache. So far, the ROI on using ca- nonical data has been acceptable for the team. Janet’s current team also uses seed data for its “middle layer” testing on local builds. It works well for fast feedback during the development cycle. However, the test environment and the staging environments use a migrated copy of production data. The downside is that the regression tests can only be run on local copies of the build. The risk is low because they practice “build once, deploy to many.”\n\nProduction-Like Data The ability to test a system that is as much like production as possible is essen- tial to most software development teams. However, running a suite of auto- mated regression tests against a copy of a production database would probably run too slowly to be useful feedback. Besides, you couldn’t really depend on any data remaining stable as you bring over new copies to stay up-to-date. Generally, when you’re talking about functional or end-to-end testing, a clone of the production database is most useful for manual exploratory testing.\n\nStress, performance, and load testing, which are automation–intensive, need an environment that closely simulates production in order to provide results that can translate to actual operations. Usability, security, and reliability are other examples of testing that needs a production-like system, although they may not involve much automation.\n\nThere is always a trade-off; your production database might be huge, so it is expensive and slow, but it provides the most accurate test data available. If your organization can afford hardware and software to store multiple copies of production data for testing purposes, this is ideal. Small companies may\n\n309",
      "content_length": 2838,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "310\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nhave resource constraints that might limit the amount of data that can be stored in test and staging environments. In this case, you’ll need to decide how much test data you can support and plan how to copy enough relevant data to make the test representative of what’s used in “real life.” Or you may consider making the investment in hardware, which is getting less expensive every day, to support a real production style environment. Otherwise, your test results might be misleading. As we mentioned with the canonical data, you may need to scrub the data before using it.\n\nData Migration Data migration needs to be tested against a real database. The database up- grade scripts need to be run against real data and against the last known re- lease of the database schema.\n\nTesting a Database Migration\n\nPaul Rogers, an automation test architect, tells this story of testing an eye- opening database migration [2008]:\n\nJust yesterday, I ran a Rails migration against my test database. The developers had written it, tested it, and checked it using their develop- ment databases. My test database was probably 20,000 times larger. The migration for them took seconds. For me, well, I stopped it after three hours, at probably 10% complete. The programmers needed to redo their migration strategy.\n\nI doubt this would have shown up on an in-memory database, so for me, a real database in this instance was deﬁnitely the right choice. In fact, this is likely to feed into things we need to consider before releasing, such as how long does a deployment take, or how long does the data- base update take. We can then use this to estimate how much down time we will need for the actual upgrade.\n\nThis is another example of how we must strike a balance between tests that deliver quick feedback and tests that realistically reﬂect events that might occur in production.\n\nUnderstand Your Needs\n\nIf you understand the purpose of your tests, you can better evaluate your needs. For example, if you don’t need to test stored procedures or SQL que- ries directly for speed, consider tools such as in-memory databases, which work just like real databases but greatly speed up your tests. When you need to simulate the actual production environment, make a copy of the entire",
      "content_length": 2324,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "EVALUATING AUTOMATION TOOLS\n\nproduction database, if necessary. Quick feedback is the goal, so balance test- ing realistic scenarios with ﬁnding defects as efﬁciently as possible.\n\nEVALUATING AUTOMATION TOOLS The ﬁrst step in choosing an automation tool is to make a list of everything the tool needs to do for you. Let’s consider how you can decide on your test tool requirements.\n\nIdentifying Requirements for Your Automation Tool\n\nAfter deciding on the next automation challenge to tackle, think about your tool needs. What tools do you already have? If you need additional ones, you probably want something that integrates well with your existing testing and development infrastructure. Do you need a tool to easily integrate into the continuous build process? Will your existing hardware support the automa- tion you need to do? Setting up a second build process to run functional tests may require additional machinery.\n\nWho’s going to use the test tool you’re hoping to implement? Will non- programmers be writing test cases? Do your programmers want a tool they feel comfortable with as well? Do you have distributed team members who need to collaborate?\n\nWho will be automating and maintaining the tests? The skills already on your team are important. How much time do you have to get a tool installed and learn how to use it? If your application is written in Java, a tool that uses Java for scripting may be the most appropriate. Do team members have experi- ence with particular tools? Is there a separate test team with expertise in a certain tool? If you’re starting the transition to agile development and you al- ready have a team of test automators, it may make sense to leverage their ex- pertise and keep using the tools they know.\n\nYour tool requirements are dependent on your development environment. If you’re testing a web application, and the tool you choose doesn’t support SSL or AJAX, you may have a problem. Not every test tool can test web services applications. Embedded system testing can need different tools again. The case study in Chapter 12, “Summary of Testing Quadrants,” shows one way to use Ruby to test an embedded application.\n\nOf course, the type of testing you’re automating is key. Security testing prob- ably needs highly specialized tools. There are many existing open source and\n\n311",
      "content_length": 2331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "312\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nvendor tools for performance, so the job of selecting one isn’t overwhelming. As you master one challenge, you’ll be better prepared for the next. It took Lisa’s team a couple of years to develop robust regression test suites at the unit, integration, and functional levels. Performance testing was their next area of pain. Lessons learned from the earlier automation efforts helped them do a better job of identifying requirements for a test tool, such as ease of reporting results, compatibility with existing frameworks, and scripting language.\n\nWrite a checklist that captures all your tool requirements. Some of them might conﬂict with or contradict each other—“The tool needs to be easy enough so that customers can specify tests” or “The tests should be easy to automate.” Write them down so you can ﬁnd the right balance. Then start doing your research.\n\nOne Tool at a Time\n\nYou’re going to need different tools to serve different purposes. Implement- ing new tools and learning the best way to use them can get overwhelming pretty quickly. Try one tool at a time, addressing your greatest area of pain. Give it enough time for a fair trial and evaluate the results. If it’s working for you, master that tool before you go on to the next area of pain and the next tool. Multitasking might work for some situations, but new technology de- mands full attention.\n\nWhen you’ve settled on a tool to address a particular need, take a step back and see what else you need. What’s the next automation challenge facing your team? Will the tool you just selected for another purpose work for that need, too, or do you need to start a new selection process?\n\nThe bibliography contains websites that help with tool searches and evaluation.\n\nIf you’ve decided to look outside your own organization for tools, the ﬁrst step is to ﬁnd time to try some out. Start with some basic research: Internet searches, articles and other publications about tools, and mailing lists are good places to get ideas. Compile a list of tools to consider. If your team uses a wiki or online forum tool, post information about tools and start a discus- sion about pros and cons.\n\nBudget time for evaluating tools. Some teams have an “engineering sprint” or “refactoring iteration” every few months where, rather than delivering stories prioritized by the business, they get to work on reducing technical debt, up- grading tool versions, and trying out new tools. If your team doesn’t have these yet, make a case to your management to get them. Reducing your tech-",
      "content_length": 2597,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "Lisa’s Story\n\nChapter 11, “Critiquing the Product Using Technology- Facing Tests, shows an example of the results pro- duced by the per- formance test tool chosen, JMeter.\n\nEVALUATING AUTOMATION TOOLS\n\nnical debt and establishing a good testing infrastructure will improve your velocity in the future and free time for exploratory testing. If you never have time to make code easier to maintain or upgrade tools, technical debt will drag down your velocity until it comes to a halt.\n\nWhen you have a list of tools that may meet your requirements, narrow the possibilities down to one or two, learn how to use each one well enough to try it, and do a spike: Try a simple but representative scenario that you can throw away. Evaluate the results against the requirements. Use retrospectives to consider pros and cons.\n\nWhat resources do you need to implement and use the tool? What impact will the tool have on the team’s productivity and velocity? What risks does it pose? What will it allow you to do in the long term that you can’t do now?\n\nPick your top candidate and commit to trying it for some period of time— long enough to get some competency with it. Make sure you try all your mis- sion-critical functionality. For example, if your application uses a lot of Ajax, make sure you can automate tests using the tool. In retrospectives, look at what worked and what didn’t. Be open to the idea that it might not be right and that you have to throw it out and start over. Don’t feel you have to keep on with the tool because you have so much invested in it already.\n\nWe all know that there’s no “silver bullet” that can solve all your automation problems. Lower your expectations and open your mind. Creative solutions rely on art as much as science.\n\nWhen conducting the performance test tool search, we turned to an agile testing mailing list for suggestions. Many people offered their experiences, and some even offered to help learning and implementing a tool. We searched for a tool that used Java for scripting, had a minimal learning curve, and presented results in a useful graphical format. We listed tools and their pros and cons on the team wiki. We budgeted time for trial runs. Lisa’s coworker, Mike Busse, tried the top two candidates and showed highlights to the rest of the team. A tool was chosen by team consensus and has proven to be a good ﬁt.\n\nChoosing Tools\n\nWe’re lucky to have an already vast range and ever-growing set of tools to choose from: home-grown, open source, vendor tools, or a combination of\n\n313\n\n—Lisa",
      "content_length": 2542,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "314\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nany, are all viable alternatives. With so many choices, the trick is knowing where to look and ﬁnding time to try tools out to see if they ﬁt your require- ments. Because we can’t predict the future, it may be hard to judge the ROI of each potential solution, but an iterative approach to evaluating them helps get to the right one.\n\nShould You Grow Your Own? Does your application present unique testing challenges, such as embedded software or integration with outside systems? Do team members have the skills, time, and inclination to write their own test framework or build one on top of an existing open source tool? If so, home-grown test tools may be the best ﬁt.\n\nA happy result (or perhaps a major success factor) of agile development is that many programmers are “test infected.” Today’s development tools and languages make automation frameworks easier to build. Ruby, Groovy, Rails, and many languages and frameworks lend themselves to automation. Exist- ing open source tools such as Fit and HtmlUnit can be leveraged, with cus- tom frameworks built on top of them.\n\nHome-grown tools have many advantages. They’re deﬁnitely programmer- friendly. If your team is writing its own automation frameworks, they’ll be precisely customized to the needs of your development and customer teams, and integrated with your existing build process and other infrastructure— and you can make them as easy to execute and interpret results as you need.\n\nHome-grown doesn’t mean free, of course. A small team may not have the bandwidth to write and support tools as well as develop production code. A large organization with unique requirements may be able to put together a team of automation specialists who can collaborate with testers, customers, programmers, and others. If your needs are so unique that no existing tool supports them, home-grown may be your only option.\n\nOpen Source Tools Many teams who wrote their own tools have generously made them available to the open source community. Because these tools were written by test- infected programmers whose needs weren’t met by vendor tools, they are usually lightweight and appropriate for agile development. Many of these tools are developed test-ﬁrst, and you can download the test suite along with the source code, making customization easier and safer. These tools have a broad appeal, with features useful to both programmers and testers. The",
      "content_length": 2464,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "See Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” for more about open source test auto- mation tools.\n\nSee the bibliog- raphy for a full discussion by Elisabeth Hendrickson on this subject.\n\nEVALUATING AUTOMATION TOOLS\n\nprice is right, although it’s important to remember that purchase price is only a fraction of any tool’s cost.\n\nNot all open source tools are well documented, and training can be an issue. However, we see seminars and tutorials on using these tools at many confer- ences and user group meetings. Some open source tools have excellent user manuals and even have online tutorials and scheduled classes available.\n\nIf you’re considering an open source solution, look for an active developer and user community. Is there a mailing list with lots of bandwidth? Are new features released often? Is there a way to report bugs, and does anyone ﬁx them? Some of these tools have better support and faster response on bugs than vendor tools. Why? The people writing them are also using them, and they need those features to test their own products.\n\nVendor Tools Commercial tools are perceived as a safe bet. It’s hard to criticize someone for selecting a well-known tool that’s been around for years. They’re likely to come with manuals, support, and training. For testers or other users who lack a technical background, the initial ramp-up might be faster. Some are quite robust and feature-rich. Your company may already own one and have a team of specialists who know how to use it.\n\nAlthough they are changing with the times, vendor tools are historically pro- grammer-unfriendly. They tend to use proprietary scripting languages that programmers don’t want to spend time learning. They also tend to be heavy- weight. The test scripts may be brittle, easily broken by minor changes to the application, and expensive to maintain. Most of these tools are recording scripts for subsequent playback. Record/playback scripts are notoriously costly from a maintenance perspective.\n\nElisabeth Hendrickson [2008] points out that specialized tools such as these may create a need for test automation specialists. Silos such as these can work against agile teams. We need tools that facilitate test-ﬁrst, rather than test-last development. Test tools shouldn’t stand in the way of change.\n\nIf you have people already expert in a vendor tool, and a use for a tool that might be used only by a subset of the development team or a team separate from development, a vendor tool could make lots of sense. Lisa’s ﬁrst two XP teams used a vendor tool with some degree of success.\n\n315",
      "content_length": 2606,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "316\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nAs of this writing, better functional test tools and IDEs are emerging. These fa- cilitate test maintenance tasks with features such as global search/replace. Twist is an example of a tool implemented as a collection of plug-ins to the Eclipse IDE, so it can take advantage of powerful editing and refactoring features.\n\nAgile-Friendly Tools\n\nPart III, “Using Agile Testing Quadrants,” and particularly Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” contain examples of test automation tools that work well on agile projects.\n\nElisabeth Hendrickson [2008] lists some characteristics of effective agile test automation tools. These tools should:\n\n(cid:2) Support starting the test automation effort immediately, using a test-\n\nﬁrst approach\n\n(cid:2) Separate the essence of the test from the implementation details (cid:2) Support and encourage good programming practices for the code\n\nportion of the test automation\n\n(cid:2) Support writing test automation code using real languages, with\n\nreal IDEs\n\n(cid:2) Foster collaboration\n\nIMPLEMENTING AUTOMATION While you’re evaluating tools, think about how quickly your top priority au- tomation need must be addressed. Where will you get the support to help implement it? What training does the team need, and how much time will be available to devote to it? How quickly do you have to ramp up on this tool?\n\nKeep all of these constraints in mind when you’re looking at tools. You might have to settle for a less robust tool than you really want in order to get vital automation going in the short term. Remember that nothing’s permanent. You can build your automation effort step-by-step. Many teams experience unsuccessful attempts before ﬁnding the right combination of tools, skills, and infrastructure.\n\nSelenium at Work\n\nJoe Yakich, a software engineer with test automation experience, describes how a team he worked with implemented a test automation effort with Sele- nium, an open source test automation tool.\n\nThe software company I worked for—let's call it XYZ Corp—had a prob- lem. The product, an enterprise-level web-based application, was a powerful, mature offering. Development projects were managed using",
      "content_length": 2251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "IMPLEMENTING AUTOMATION\n\nAgile and Scrum, and a talented stable of engineers churned out new features quickly. The company was growing steadily.\n\nSo, what was the problem? XYZ was facing a future where software test- ing efforts might not be able to keep pace with the development effort. Software quality issues might slow adoption of the product or—worse yet—cause existing customers to look elsewhere.\n\nTest automation seemed like an obvious way to mitigate these risks, and XYZ was fully aware of it. In fact, they had attempted to create a test au- tomation suite twice before, and failed.\n\nThe third time, XYZ chose to use Selenium RC, driven by the Ruby pro- gramming language. Selenium RC—the RC is for “Remote Control”—is a tool for test automation. Selenium RC consists of a server component and client libraries. The Java server component acts as an HTTP proxy, making the Selenium Core JavaScript appear to originate from the web- server of the application under test (AUT). The server can start and stop browser sessions (supported browsers include nearly all modern brows- ers, including Internet Explorer, Firefox, and Safari) and interpret com- mands to interact with elements such as buttons, links, and input ﬁelds. The client libraries allow test scripts to be written in Java, .NET, Perl, Python, and Ruby.\n\nOur team chose Ruby because it's a purely object-oriented, dynamic, in- terpreted language with a syntax that is elegant, expressive, and tersely powerful. Most importantly, Ruby is an ideal tool for the creation of a Do- main Speciﬁc Language (DSL). Ruby is malleable enough for the program- mer to ﬁrst choose the structure and syntax of the DSL and then craft an implementation, as opposed to a more rigid language that might impose constraints on that freedom. One of our goals was to create an automa- tion framework—a DSL—hiding complex detail. We wanted to be able to say things like\n\neditor.save\n\nin our tests instead of\n\ns.click(\"//table[@class='edit']/tbody/tr[0]//img[@src='save.gif']\")\n\nNot only is the former more readable, it’s also far more maintainable. The XPath expression in the latter can be put in a library method to be called as needed. Using a DSL that employs the nouns and verbs of the applica- tion allows an engineer writing a test to focus on the test, not the under- lying complexity of interacting with on-screen controls.\n\nXYZ created an automation team to build the framework and tests. Cre- ating the framework itself was a time-consuming, technically challenging\n\n317",
      "content_length": 2530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "318\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\ntask. Some of the framework classes themselves were complicated enough to warrant unit tests of their own. After a sufﬁcient amount of test framework was constructed, we began work on actual application tests, using the Ruby RSpec library. RSpec is itself a DSL for test speciﬁca- tions. One of its strengths is the use of simple declarative statements to describe behavior and expectations. One might, for example, write a test using the statement\n\n“A user should be able to save data in an editor by clicking Save”\n\nﬁlling in the body of the test with calls to the Selenium-based test frame- work we had created.\n\nNearly a year later, we had automated nearly two thousand test cases. Although the majority of the application was covered by automation, other portions of the application required manual testing—we had been forced to make choices and prioritize our efforts. Every week the test suite took longer to run than the preceding week; it now took nearly six hours to complete, and we had begun to think about running tests in parallel. We had not yet managed to expand our testing across all of the browsers supported by the application. The enthusiasm that automation generated had waned somewhat, and we found it necessary to carefully manage expectations, both with upper management and with other en- gineers. Despite these issues, Selenium was a clear win, for had we not invested heavily in test automation, testing at XYZ would have required hiring an army of test engineers (which would have been prohibitively expensive even had we been able to ﬁnd enough qualiﬁed applicants).\n\nNot everything can be automated, because of budgetary or technical reasons. In addition, exploratory testing is invaluable and should not be neglected. It should be noted, however, that these drawbacks are shared by every other test automation tool currently available, and most of the other automation tools that can rival Selenium's automation prow- ess are commercial products that cannot match its price: free.\n\nGood development practices are key to any automation effort. Use an ob- ject-oriented approach. As you build your library of test objects, adding new tests becomes easier. A domain speciﬁc language helps make business-facing tests understandable to customers, while lowering the costs of writing and maintaining automated test scripts.\n\nGood object-oriented design isn’t the only key to building a suite of main- tainable automated tests that pay off. You also need to run the tests often enough to get the feedback your team needs.. Whatever tools we choose must be integrated with our build process. Easy-to-interpret results should come to us automatically.",
      "content_length": 2734,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "MANAGING AUTOMATED TESTS\n\nThe tools we choose have to work on our platforms, and must share and play well with our other tools. We have to continually tweak them to help with our current issues. Is the build breaking every day? Maybe we need to hook our results up to an actual trafﬁc light to build team awareness of its status. Did a business-facing test fail? It should be plain exactly what failed, and where. We don’t have extra time to spend isolating problems.\n\nThese concerns are an essential part of the picture, but still only part of the picture. We need tools that help us devise test environments that mimic pro- duction. We need ways to keep these test environments independent, unaf- fected by changes programmers might be making.\n\nBuilding test infrastructure can be a big investment, but it’s one our agile team needs to make to get a jump on test automation. Hardware, software, and tools need to be identiﬁed and implemented. Depending on your company’s re- sources, this might be a long-term project. Brainstorm ways to cope in the short term, while you plan how to put together the infrastructure you really need to minimize risk, maximize velocity, and deliver the best possible product.\n\nMANAGING AUTOMATED TESTS Let’s say we need a way to ﬁnd the test that veriﬁes a particular scenario, to understand what each test does, and to know what part of the application it veriﬁes. Perhaps we need to satisfy an audit requirement for traceability from each requirement to its code and tests. Automated tests need to be main- tained and controlled in the same way as production source code. When you tag your production code for release, the tests that veriﬁed that functionality need to be part of the tag.\n\nHere’s an example where that comes in handy. We just found a problem in the code under development. Is it a new problem, or has it been lurking in the code for a while and somehow missed by the test? We can deploy the tag that’s in production, try to reproduce the problem, and investigate why the tests didn’t catch it. Lisa’s team recently had a situation where the regression suite missed a bug because a database constraint was missing in the test schema. That kind of problem is hard to pinpoint if you aren’t tying your test code versions to your production code versions.\n\nOrganizing Tests\n\nMany tools come with their own means of organization. For example, Fit- Nesse comes with its own wiki, with a hierarchical organization, and built-in\n\n319",
      "content_length": 2478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "320\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nversion control. As of this writing, FitNesse is starting to provide support for source code control tools such as Subversion. Scripts written in other test tools, such as Watir and Canoo WebTest, can and should be maintained within the same source code control system as production code, just as the unit tests are.\n\nOrganizing Tests with the Project under Test\n\nWe asked some agile testing experts how they manage tests. Dierk König, founder and project manager of Canoo WebTest, explained how his teams have managed their automated tests to satisfy the needs of both the devel- opment and customer teams.\n\nWe always organize our tests alongside the project under test. That is, test sources are stored together with the project sources in the exact same repository, using the same mechanisms for revision control, tagging, and sharing the test base.\n\nWebTest comes with a standard layout of how to organize tests and test data in directories. You can adapt this to any structure you fancy, but the \"convention over conﬁguration\" shows its strength here. In large projects, every sub-project maintains its own test base in a “webtest” subdirectory that follows the convention.\n\nWhenever a client did not follow this approach, the experience was very painful for all people involved. We have seen huge databases of test de- scriptions that did not even feature a proper revision control (i.e., where you could, for example, see diffs to old releases or who changed which test for what reason).\n\nKeep in mind that tests are made up from modules so that you can elimi- nate duplication of test code; otherwise the maintenance will kill you. And before changing any module, you need to know where it is used.\n\nIn short: Make sure the master of your tests and your test data is in a text format that is versioned together with your code under test.\n\nNontechnical personnel (for example, management, QA) may require more high-level information about test coverage, latest test results, or even means of triggering a test run. Don't let these valid requirements undermine the engineering approach to test automation. Instead, write little tools, for example, web-based report applications, that address these needs.\n\nThe ability of customers to access information about tests is as important as the ability to keep test and production code coordinated. As Dierk pointed out, you might not be able to do all this with the same tool.",
      "content_length": 2486,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "MANAGING AUTOMATED TESTS\n\nTest management helps your team answer questions such as the following:\n\n(cid:2) Which test cases have been automated? (cid:2) Which still need automating? (cid:2) Which tests are currently running as part of a regression suite? (cid:2) Which tests cover what functional areas? (cid:2) How is feature XYZ designed to work? (cid:2) Who wrote this test case? When? Who changed it last? (cid:2) How long has this test been part of the regression suite?\n\nBecause one of the primary reasons we write tests is to guide development, we need to organize tests so that everyone on the team can ﬁnd the appropri- ate tests for each story and easily identify what functionality the tests cover. Because we use tests as documentation, it’s critical that anyone on either the development or customer team can ﬁnd a particular test quickly when there’s a question about how the system should behave. We might need multiple tools to satisfy different test management goals.\n\nIt’s easy to lose control of test scripts. When a test fails, you need to pinpoint the problem quickly. You may need to know what changes have been made recently to the test script, which is easy with the history available in a source code control system. Your customer team also needs a way to keep track of project progress, to understand how much of the code is covered with tests, and possibly to run tests themselves. Test management systems, like the tests themselves, should promote communication and collaboration among team members and between different teams.\n\nTest Transparency\n\nDeclan Whelan, a software developer and agile coach, uses a test manage- ment approach designed to keep tests visible to testers, developers, manag- ers, and other teams.\n\nWe treat all test artifacts the same as source code from an organizational and revision control perspective. We use Subversion, and anyone who wants to run or edit the tests simply checks them out.\n\nThe latest Fit tests are available on a Conﬂuence Wiki. We did this to sup- port collaboration (team is distributed) and to leverage the strong capa- bilities of Conﬂuence. Having the tests visible on the wiki was also helpful to others such as managers and other teams who did not want to check it out from the repository.\n\n321",
      "content_length": 2275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "322\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nPrior to this, the QA team maintained test cases on a drive that was not ac- cessible to anyone outside of QA. This meant that developers could not easily see what was being tested. Making the tests visible, transparent, and supported by a version control system (Subversion) really helped to break down barriers between developers and testers on the team.\n\nMake sure your tests are managed with solid version control, but augment that with a way for everyone to use the tests in ways that drive the project forward and ensure the right value is delivered.\n\nOrganizing Test Results\n\nEveryone involved with delivering software needs easy access to tests and test results. Another aspect of managing tests is keeping track of what tests are from prior iterations and need to keep passing, versus tests that are driving development in the current iteration and may not be passing yet. A continu- ous integration and build process runs tests for quick feedback on progress and to catch regression failures. Figure 14-4 shows an example of a test result report that’s understandable at a glance. One test failed, and the cause of the failure is clearly stated.\n\nIf you’re driving development with tests, and some of those tests aren’t pass- ing yet, this shouldn’t fail a build. Some teams, such as Lisa’s, simply keep new tests out of the integration and build process until they pass for the ﬁrst time. After that, they always need to pass. Other teams use rules in the build process itself to ignore failures from tests written to cover the code currently being developed.\n\nAs with any test automation tool, you can solve your test management prob- lems with home-grown, open source, or commercial systems. The same cri- teria we described in the section on evaluating test tools can be applied to selecting a test management approach.\n\nTest management is yet another area where agile values and principles, to- gether with the whole-team approach, applies. Start simple. Experiment in small steps until you ﬁnd the right combination of source code control, re- positories, and build management that keeps tests and production code in synch. Evaluate your test management approach often, and make sure it ac- commodates all of the different users of tests. Identify what’s working and what’s missing, and plan tasks or even stories to try another tool or process to ﬁll any gaps. Remember to keep test management lightweight and maintain- able so that everyone will use it.",
      "content_length": 2531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "MANAGING AUTOMATED TESTS\n\nFigure 14-4 Test results from a home-grown test management tool\n\nManaging Tests For Feedback\n\nMegan Sumrell, an agile trainer and coach, describes how her team coordi- nates its build process and tests for optimum feedback.\n\nWe create a FitNesse test suite for each sprint. In that suite, we create a subwiki for each user story that holds its tests. As needed, we create a setup and teardown per test or suite. If for some reason we don't com- plete a user story in the sprint, then we move the tests to the suite for sprint in which we do complete the story.\n\nWe scripted the following rule into our build: If any of the suites from the previous sprint fail, then the build breaks. However, if tests in the current sprint are failing, then do not fail the build.\n\n323",
      "content_length": 795,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "324\n\nCHAPTER 14\n\n(cid:2) AN AGILE TEST AUTOMATION STRATEGY\n\nEach test suite has a lengthy setup process, so when our FitNesse tests started taking longer than 10 minutes to run, our continuous integration build became too slow. We used symbolic links to create a suite of tests that serve as our smoke tests, running as part of our continuous integra- tion build process. We run the complete set of FitNesse tests on a sepa- rate machine. We set it up to check the build server every ﬁve minutes. If a new build existed, then it would pull the build over and run the whole set of FitNesse tests. When it was done, it would then check the build server again every ﬁve minutes and after a new build existed, it would re- peat the process.\n\nMegan’s team took advantage of features built into their tools, such as sym- bolic links to organize FitNesse test suites for different purposes—one for a smoke test, others for complete regression testing. The team members get immediate feedback from the smoke tests, and they’ll know within an hour whether there’s a bug that the smoke tests missed.\n\nGO GET STARTED Don’t be afraid to get something—anything—in place, even if it’s somewhat deﬁcient. The most important factor in success is to just get started. Many, if not most, successful teams have started with a poor process but managed to turn an inadequate process into something truly essential to the team’s suc- cess, one piece at a time. As with so many aspects of agile testing, improving in tiny increments is the key to success.\n\nIf you don’t start somewhere, you’ll never get traction on automation. Get the whole team together and start an experiment. Without the right level of test automation, your team can’t do its best work. You need the right test au- tomation to deliver business value frequently. A year or two from now, you’ll wonder why you thought test automation was so hard.\n\nSUMMARY In this chapter, we considered how to apply agile values, principles, and prac- tices to develop an automation strategy. We discussed the following subjects related to automation:\n\n(cid:2) Use the agile testing quadrants to help identify where you need test\n\nautomation, and when you’ll need it.\n\n(cid:2) The test automation pyramid can help your team make the right in-\n\nvestments in test automation that will pay off the most.",
      "content_length": 2331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "SUMMARY\n\n(cid:2) Apply agile values, principles, and practices to help your team get\n\ntraction on test automation.\n\n(cid:2) Repetitive tasks, continuous integration and build processes, unit\n\ntests, functional tests, load tests, and data creation are all good candi- dates for automation.\n\n(cid:2) Quadrant 3 tests such as usability testing and exploratory testing may beneﬁt from some automation to set up test scenarios and analyze re- sults, but human instincts, critical thinking, and observation can’t be automated.\n\n(cid:2) A simple, whole-team approach, using iterative feedback, and taking\n\nenough time can help you get started on a good solution.\n\n(cid:2) When developing an automation strategy, start with the greatest area of pain, consider a multi-layered approach, and strive for continu- ously revisiting and improving your strategy rather than achieving perfection from the start.\n\n(cid:2) Consider risk and ROI when deciding what to automate. (cid:2) Take time to learn by doing; apply agile coding practices to tests. (cid:2) Decide whether you can simply build inputs in-memory, or whether\n\nyou need production-style data in a database.\n\n(cid:2) Supply test data that will allow tests to be independent, rerunnable,\n\nand as fast as possible.\n\n(cid:2) Take on one tool need at a time, identify your requirements, and de-\n\ncide what type of tool to choose or build that ﬁts your needs.\n\n(cid:2) Use good development practices for test automation, and take time\n\nfor good test design.\n\n(cid:2) Automated tools need to ﬁt into the team’s development infrastructure. (cid:2) Version-control automated tests along with the production code that\n\nthey verify.\n\n(cid:2) Good test management ensures that tests can provide effective docu-\n\nmentation of the system and of development progress.\n\n(cid:2) Get started on test automation today.\n\n325",
      "content_length": 1852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "Part V AN ITERATION IN THE LIFE OF A TESTER\n\nWhenever we do tutorials, webinars, or Q&A sessions with participants who are relatively new to agile development, we’re always asked questions such as “What do testers do during the ﬁrst part of an iteration before anything’s ready to test?” or “Where does user acceptance testing ﬁt into an agile release cycle?” It’s easy to expound on theories of who should do what and when, in an agile process, but we ﬁnd giving concrete examples from our own experi- ence is the best help we can give agile newbies. Through our talking to many different agile teams, we’ve learned that there’s a lot of commonality in what works well for agile development and testing.\n\nIn this part of the book, we’ll follow an agile tester’s life throughout an itera- tion. Actually, we’ll explore more than just an iteration. We’ll start with what testers do during release or theme planning, when the team looks at the work it will do for several upcoming iterations. We’ll give examples of what testers can do to help the team members hit the ground running when they start the iteration. We’ll show how coding and testing are part of one integrated pro- cess of delivering software, and we’ll describe how testers and programmers work closely and incrementally. We’ll explain different ways that testers can help their teams stay on track and gauge progress, including useful ap- proaches to metrics and handling defects. We’ll look at testing-related activi- ties involved in wrapping up an iteration and ﬁnding ways to improve for the next one. Finally, we’ll examine a tester’s role in a successful release, includ- ing the end game, UAT, packaging, documentation, and training.",
      "content_length": 1706,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "328\n\nPART V (cid:2) AN ITERATION IN THE LIFE OF A TESTER\n\nThe activities described in this slice-of-life look at agile testing can be per- formed by anyone on the team, not only testing specialists. On some teams, all team members can, and do, perform any task, be it development, testing, database, infrastructure, or other tasks. For simplicity, in this section we’ll as- sume we’re following someone whose primary role is testing as they help to deliver high-quality software.",
      "content_length": 479,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "Chapter 15\n\nTESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nTracking Test Tasks\n\nPurpose of Release Planning\n\nCommunicating Test Results\n\nPreparing for Visibility\n\nRelease Metrics\n\nSizing Stories\n\nSizing\n\nTester’s Role\n\nAn Example\n\nLIghtweight Test Plan\n\nTest Matrix\n\nSpreadsheet\n\nTest Plan Alternatives\n\nRelease/Theme Planning\n\nPrioritizing\n\nWhy Do We Prioritize?\n\nTesting Considerations\n\nWhiteboard\n\nAutomated Test List\n\nDeadlines and Timelines\n\nFocus on Value\n\nWhat’s In-Scope?\n\nWhere to Start?\n\nSystem-Wide Impact\n\nWhy Test Plan?\n\nThird-Party Involvement\n\nTypes of Testing\n\nInfrastructure\n\nTest Planning\n\nTest Environments\n\nTest Data\n\nTest Results\n\nAgile development teams complete stories and deliver production-ready software in every iteration but plan the big picture or a larger chunk of functionality in advance. A theme, epic, or project may encompass several iterations. In this chapter, we look at what testers do when their team takes time to plan their release. We also consider ways to track whether our development is proceeding as anticipated, or if course corrections are needed.\n\n329",
      "content_length": 1103,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "330\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nTHE PURPOSE OF RELEASE PLANNING One reason software teams try agile development is because they know long- range plans don’t work. Most business environments are volatile, and priori- ties change every week, or even every day. Agile development is supposed to avoid “big design up front.” Most of us have experienced making plans that turned out to be a waste of effort. But we have to have some understanding of what our customer is looking for and how we might deliver it in order to get off to a good start. Fortunately, an agile approach can make planning a useful way to give us a head start on knowing how we will deliver the product.\n\nAgile Planning Applied\n\nJanet’s sister, Carol Vaage, teaches ﬁrst grade when she isn’t directing confer- ences. She relates her ﬁrst experience with using agile practices to organize a conference:\n\nMy table is loaded with binders and to-do lists, and a feeling of being overwhelmed freezes me into inaction. I am Conference Director and the task right now seems onerous. When my sister offers to help me, I agree, because I am desperate to get this planning under control. I welcome Janet to my clutter, show her my pages of hand-written lists of things that need to get done, explain the huge tasks waiting for my attention, and share how my committee works.\n\nJanet showed me in simple language how to separate each task onto a sticky note and use color coordination for different responsibilities and different individuals. She explained about the columns of “To Do,” “In Progress,” “To Review,” and “Done.” I had never heard of the word itera- tion before but fully understood about a timeline. She recommended two-week blocks of time, but I chose one-week iterations. We set up a wall for my planning board, and Janet left me to pull it together and to add the tasks needed.\n\nIn the six days since Janet has been here, ten tasks have been moved from the To-Do column to In-Progress. Three tasks are Done, and speciﬁc time-related tasks have been blocked by the correct time period. The most positive thing is that as I add more tasks in the To-Do column, I am not feeling overwhelmed. I understand that all I need to do is initiate the steps to start it, and then the job becomes easier. The feeling of chaos is gone; I see progress and understand that there is still much work to be done. The timeline is clear, the tasks are discrete and concrete. And the most difﬁcult task of all, ﬁnding a way to coordinate the video confer- ence for our keynote speaker has been tackled. This system works!\n\nAgile planning and tracking practices are useful for more than software devel- opment. A little time carefully invested, and simple tools used in organizing and planning the testing activities and resources for a release, will help the team deliver high-quality software.",
      "content_length": 2887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "THE PURPOSE OF RELEASE PLANNING\n\nXP teams may take a day every few months for release planning. Other agile teams do advance planning when getting ready to start on a theme, epic, or major feature, which we think of as a related group of stories. They work to understand the theme or release at a high level. What is the customer’s vision of what we should be delivering? What’s the purpose of the release? What’s the big picture? What value will it deliver to the business, to the customers? What other teams or projects are involved and require coordination? When will UAT take place? When will code be released to staging, to production? What metrics do we need to know if we’re on track? These general questions are addressed in release planning.\n\nSome teams don’t spend much time doing release planning activities. Priori- ties change quickly, even within a particular theme of features. Nobody wants to do too much work up front that ends up being wasted. Some teams just look at the ﬁrst couple of stories to make sure they can get a running start. At the very least, teams want to know enough to get their system archi- tecture pointed in the right direction and get started on the ﬁrst few stories.\n\nThese planning meetings aren’t intended to plan every iteration of the release in detail. And we know we can’t predict exactly how many stories we can complete each iteration. However, we do have an idea of our average velocity, so we can get a general idea of the possible scope of the release. The team talks about the features and stories, trying to get a 20,000-foot view of what can go into the release and how many iterations it might take to complete. Both of us like Mike Cohn’s approach to release planning in his book Agile Estimating and Planning [2005]. Stories that the business wants to include are sized relative to each other, and then features are prioritized according to the value they deliver. The team may identify “thin slices” through the fea- tures to determine what stories absolutely have to be done, what’s in scope, what “nice-to-haves” could be put off until later. They look at dependencies between stories, relative risk, and other factors that determine the order in which features should be coded. The order in which stories are coded is as important, or sometimes more important, than the size of the stories. Teams want to deliver value the ﬁrst iteration of the release.\n\nRelease planning is a chance for the developers and customers to consider the impact of the planned features on the larger system, clarify assumptions, and look at dependencies that might affect what stories are done ﬁrst. They may think about testing at a high level and whether new resources such as test en- vironments and software will be needed.\n\nLet’s follow our agile tester through release planning activities and see how she contributes value through her unique perspective and focus.\n\n331",
      "content_length": 2915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "332\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nSIZING Agile teams estimate the relative size of each story. Some teams size as they go, delaying the estimation until the iteration where they’ll actually complete the story. Others have meetings to estimate stories even in advance of release planning. Some developer and customer teams sit together to write and esti- mate the size of stories all at one time. The goal of sizing is for the program- mers to give the business an idea of the cost of each story and to help them prioritize and plan the ﬁrst few iterations. High-functioning teams who’ve worked together for years may take a less formal approach. For new agile teams, learning to size stories takes a lot of practice and experience. It’s not important to get each story sized correctly but to be close enough to give cus- tomers some idea of how big the stories are so they can prioritize with better information. Over time, variations on individual story sizing will average out, and we ﬁnd that a theme or related group of stories takes about the amount of time expected.\n\nHow to Size Stories\n\nAs far as how to calculate story size, different teams use different techniques, but again, we like Mike Cohn’s approach to determining story size. We size in story points, ideal days, or simply “small, medium, large.” The relative size of each story to others is the important factor. For example, adding an input ﬁeld to an existing user interface is obviously much smaller than developing a brand new screen from scratch.\n\nIf the business knows the average velocity (the number of story points the team completes each iteration) and has the initial size estimates of each story it wants to get done, it has an idea of how long it might take to implement a given theme. As with any other development methodology, there are no guarantees, because estimates are just that. Still, the business can plan well enough to conduct its usual activities.\n\nOur teams use planning poker (explained in Mike Cohn’s book Agile Estimat- ing and Planning) to estimate story size. In planning poker, each team mem- ber has a deck of cards. Each card has a number of points on it. The process begins with the customer or product owner reading a story and explaining its purpose and the value it will deliver. He might list a few conditions of satisfac- tion or high-level test cases. After a brief discussion, team members each hold up a point card that represents how “big” they think the story is from their perspective. They discuss any big differences in point value and estimate again until they reach consensus. Figure 15-1 shows team members talking about",
      "content_length": 2679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "SIZING\n\nFigure 15-1 Planning poker\n\nthe point values they each just displayed. This needs to be a quick process— long discussions about details don’t result in more accurate size estimates.\n\nSome teams ﬁgure the relative sizes of stories by how many people are needed to complete a given story in a set amount of time. Others estimate how many ideal days one person would need to ﬁnish it. Use a measurement that makes sense to all team members and one that provides consistency among estimates.\n\nThe Tester’s Role in Sizing Stories\n\nOne of our favorite sayings is, “No story is done until it’s tested.” However, we’ve run across teams where testing wasn’t included in estimates of story size. In some cases, testing a piece of functionality might take longer than coding it.\n\nIn our experience, testers usually have a different viewpoint than other team members. They often have a broad understanding of the domain and can quickly identify “ripple effects” that one story might have on the rest of the system. They also tend to think of activities not directly related to develop- ment that might need to be done, such as training users on a new or changed interface.\n\n333",
      "content_length": 1173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "334\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nLisa’s Story\n\nWhat does a tester do during the story sizing process? I think quickly about the story from several viewpoints. What business problem is the story solving, or what business value does it deliver? If this isn’t clear, I ask the product owner questions. How will the end user actually use the feature? If it’s still not clear, I ask the prod- uct owner for a quick example. I might ask, “What’s the worst thing that could go wrong with it?” This negative approach helps gauge the story’s risk. What testing considerations might affect the story’s size? If test data will be hard to obtain or the story involves a third party, testing might take longer than coding. I try to quickly ﬂush out any hidden assumptions. Are there dependencies or special se- curity risks? Will this part of the application need to handle a big load?\n\nMany stories aren’t big enough to warrant that much thought. Usually, we don’t need much detail to get an idea of relative size. However, your team can really get burned if a story is underestimated by a factor of ﬁve or ten. We once gave a rela- tively small estimate to a story that ended up being at least ten times the size. These are the disasters we want to avoid by asking good questions.\n\nTesters need to be part of the sizing process. Some teams think that only pro- grammers should participate, but when testers are active participants, they can help to get a much more accurate story sizing, which is in the best inter- ests of the whole team.\n\nAn Example of Sizing Stories\n\nLet’s imagine we have the story in Figure 15-2 to size up.\n\nStory PA-3\n\nAs a shopper on our site, I want to delete items\n\nout of my shopping cart so I don’t purchase\n\nextra items that I decide I don't want.\n\nFigure 15-2 Story to delete items\n\n—Lisa",
      "content_length": 1848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "SIZING\n\nAfter the product owner reads the story, the following discussion ensues:\n\nProduct Owner: “We just want some easy way for users to delete items, but we don’t have a speciﬁc implementation in mind.” Tester: “Should they be able to delete several items at once?” Product Owner: “Oh, yes, just make it as easy as possible.” Tester: “What if they accidentally delete an item they wanted to buy?” Product Owner: “Is there some way the deleted items can be saved for later retrieval?” Programmer: “Sure, but you should write a new story for that. For now, we should start with the basic delete functionality.” Tester: “Last release we implemented a wish list feature. Do you want users to be able to move items from their shopping basket to their wish list? That would be a new story also.” Product Owner: “Yes, those are two more stories we want to do, for sure. I’ll write those down, we can size them also. But we could deﬁnitely put them off until the next release, if we have to.” Tester: “What’s the worst thing that could happen with this feature?” Product Owner: “If they can’t ﬁgure out how to delete, they might just abandon their whole shopping basket. It has to be really easy and obvious.”\n\nThe ScrumMaster calls for an estimate. The team understands they’re sizing only the basic story for deleting items, not for doing something else with the deleted items. They quickly agree on a point value.\n\nLet’s look at another story. (See Figure 15-3.)\n\nStory PA-4\n\nAs a customer, I want to know how much my\n\norder will cost to ship based on the shipping\n\nspeed I choose so that I can choose a different\n\nshipping speed if I want to.\n\nFigure 15-3 Story on shipping speed\n\n335",
      "content_length": 1683,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "336\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nTester: “What are the shipping speeds the user can choose?”\n\nProduct Owner: “Standard 5-day, 2-day, and next-day.”\n\nProgrammer: “We should probably start by only offering one speed, and calculating that cost. Then we can easily implement the other two speeds.”\n\nProduct Owner: “It’s ﬁne to break it up like that.”\n\nTester: “Will we use BigExpressShipping’s API to calculate cost based on weight and destination?”\n\nProgrammer: “That would be the easiest.”\n\nThe team holds up their point cards. The tester and one of the programmers hold up an 8; the other developers hold up a 5.\n\nScrumMaster: “Why did you two choose 8?”\n\nTester: “We’ve never used BigExpressShipping’s cost API before, and I’m not sure how that will impact our testing. We have to ﬁnd out how to ac- cess their system for testing.”\n\nOther Programmer with 8: “I agree, I think the testing effort is more intense than the coding effort for this story.”\n\nThe team agrees to size the story as eight points.\n\nThis sizing process may occur before the planning meeting, and if the sto- ries were sized or estimated a long time ago, the team might want to make sure they feel comfortable with the story sizes. Teams may have changed or may be more experienced. Either of those factors can make a team change the estimates.\n\nThere are many times when a story will have a large testing component, and the coding effort is small. At other times, the reverse will be true. It’s impor- tant to consider all perspectives.",
      "content_length": 1547,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "Lisa’s Story\n\nSIZING\n\nOur team grew to dread story sizing meetings, because we got into overly long discussions about details, and the meetings always lasted long past the scheduled time. Since then, our ScrumMaster has found ways to keep us on track. She uses an egg timer to time discussions, and stops them each time the sand runs out to see if we think we really need more time to ask questions. Our product owner has also learned what information we need for estimating and usually has what we need. We also learned to only work on stories that were likely to come up in the next few iterations.\n\nWith all of our meetings, little traditions have grown to make the meetings more fun. Someone always brings treats to iteration planning meetings. In stand-up meetings, we pass around a combination penlight and laser pointer, so each of us holds it as we report on what we’re working on. We always end story sizing meet- ings with a competition to see who can throw his or her deck of planning poker cards into the small plastic tub where they live. Figure 15-4 shows this goofy but fun meeting-ending activity. Always remember the agile value of enjoyment and have some fun with your meetings.\n\nFigure 15-4 A meeting-ending tradition\n\n337\n\n—Lisa",
      "content_length": 1248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "338\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nPRIORITIZING The purpose of the release planning meeting is also to get an idea of what stories the team will try to ﬁnish by the release date. The customers prioritize the stories, but there may be dependencies, so it makes sense to do certain stories ﬁrst, even if they aren’t the highest priority. It is important that the team understands the possibility that not all of the stories will get completed by the release date. One of the basic premises of agile is to deliver working software, so it is important to have the highest-value stories completed ﬁrst so that the software we do deliver meets the customer’s needs.\n\nWhy We Prioritize Stories\n\nEveryone’s goal is to deliver real value in each iteration. Testers can help the team pick out the core functionality that has to work. In Chapter 8, we ex- plained the “thin slice” or “steel thread” concept, identifying one path through the functionality to code and test ﬁrst, and adding more features after the ﬁrst critical path works. This concept applies at the release level, too. The order of the stories is critical. Lisa’s team will sometimes break up a story and pull out a core part of a feature to do in the ﬁrst iteration.\n\nSome teams that don’t do full-blown release planning do take time to look at the stories and decide which two or three should be ﬁrst. That way, they de- liver business value in the very ﬁrst iteration of the release.\n\nLet’s look at an example.\n\nIf our theme is providing the ability for an online shopper to choose shipping options and then calculate the shipping cost based on weight, shipping speed, and destination, it may be a good idea to complete sim- ple stories or even subsets of stories so that the checkout process can pro- ceed end-to-end. Start by only allowing standard 5-day shipping, items less than 10 pounds, and destinations in the continental United States. When the user can get the shipping cost for that scenario and check out, the team can decide the next priorities. They may include heavyweight items, faster shipping speeds, shipping to Hawaii and Alaska, and ship- ping to Canada and Mexico.\n\nBy providing this thin slice ﬁrst, the testers have something to start testing immediately. The programmers have also tested their design and code inte- gration steps and so have a solid idea of how things will work when the whole feature is complete.",
      "content_length": 2437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "PRIORITIZING\n\nTesting Considerations While Prioritizing\n\nIt is important that the team understands the big picture or theme. In our ex- ample, team members know the stories for shipping outside the continental United States will come later. This knowledge may affect how they imple- ment the ﬁrst story. This doesn’t mean they have to plan for every eventual- ity, but if they know they need more shipping options, they may implement a drop-down list rather than a basic text ﬁeld. No need to make more work or rework than necessary.\n\nDuring release planning, we also consider the relative risk of the stories. If certain stories have many unknowns, it might be best to include them in an early iteration, so there’s time to recover if a story “blows up” and takes much more time than estimated. The same may apply to a story which, if not com- pleted or implemented incorrectly, would have a costly negative impact. Scheduling it early will leave more time for testing.\n\nIf new technology or software is needed, it might be good to learn it by devel- oping a straightforward story and plan more difﬁcult ones for later itera- tions. This new technology may or may not affect your test automation. You may want more time to check out the impact. If the features are all brand new and the team needs more time to understand how they should work, plan to do less than your average velocity for the ﬁrst iteration. That way, you’ll have more time to write tests that will correctly guide development. Identify risks and decide what approach makes the most sense from a testing perspective as well as a development perspective. This is one of the reasons it is important to include the whole team in the planning sessions.\n\nLooking at the stories from a testing viewpoint is essential. This is where testers add the most value. The team needs to develop in small, testable chunks in order to help decide what stories are tentatively planned for which iteration. The key here is testable. Many new agile teams think small chunks means doing all of the database work ﬁrst, or all of the conﬁguration stuff. Testable doesn’t necessarily mean it needs a GUI either. For example, the al- gorithm that calculates shipping cost is an independent piece of code that can be tested independently of any user interface but requires extensive test- ing. That might be a good story for the ﬁrst iteration. It can be tested as free- standing code and then later tested in combination with the UI and other parts of the system.\n\nThe testers may lobby for getting an end-to-end tracer bullet through the code quickly, so they can build an automation framework, and then ﬂesh it\n\n339",
      "content_length": 2662,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "340\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nout as the story development proceeds. If there are stories that present a big testing challenge, it might be good to do those early on. For example, if the release includes implementing a new third-party tool to create documents from templates and dynamic data, there are many permutations to test. If the team is unfamiliar with the tool, the testers can ask the team to consider do- ing those stories in the ﬁrst iteration of the release.\n\nWHAT’S IN SCOPE? Agile teams continually manage scope in order to meet business deadlines while preserving quality. High-value stories are the ﬁrst priority. Stories that are “nice-to-haves” might be elbowed out of the release.\n\nLisa’s Story\n\nOur team’s customers list their stories in priority order and then draw a line be- tween the stories that must be done before the release can occur, and the ones that could safely be put off. They call the less important stories “below the line,” and those stories may never get done.\n\nFor example, when we undertook the theme to allow retirement plan participants to borrow money from their retirement accounts, there was a “below the line” story to send emails to any participants whose loans are changing status to “pend- ing default” or “default.” When the loan is in “default” status, the borrower must pay taxes and penalties on the balance. The email would be extremely helpful to the borrowers, but it wasn’t as important to our business as the software to re- quest, approve and distribute loans, or process loan payments.\n\nThe email story didn’t make it into the release. It wasn’t done until more than two years later, after enough complaints from people who didn’t know their loans were going into default until it was too late.\n\nJanet worked with a team whose customers were under the misplaced as- sumption that all of the features would get into their release and that when they were prioritizing, they were just picking which stories got done ﬁrst. When the rest of the team realized the misunderstanding, they also imple- mented the idea of stories above and below the line. It helped to track progress as well as make the stories that were dropped below the line very visible.\n\nDeadlines and Timelines\n\nMany domains revolve around ﬁxed dates on the calendar. Retail businesses make most of their proﬁt during the holiday season. An Internet retail site is smart to have all new features implemented by October 1. Implementing a\n\n—Lisa",
      "content_length": 2511,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "Janet’s Story\n\nLisa’s Story\n\nWHAT’S IN SCOPE?\n\nnew feature close to the peak buying period is risky. Lisa’s company’s cus- tomers must complete government-required tasks during certain periods of the year. When it’s too late for a feature to get released this year, it often gets put off for the next year, because more urgent priorities must be addressed. Regulatory changes have speciﬁc timelines, and organizations have no choice about the timeline.\n\nWhile working on this book, I was planning a release with my team at WestJet. We had several possible stories and worked with the customers to decide what the release would look like. We had one regulatory change that was light work for the programmers, but heavy for the testers. It needed to be in production by a certain date, so the other stories we were considering for the release took that into consideration.\n\nWe decided to create a small maintenance release with just that one major fea- ture, along with a few bugs from the backlog so the release of the regulatory change would not be jeopardized. While the testers completed their testing, the rest of the team started some of elaboration stories for the next release.\n\nAn alternative plan could have been that the programmers chip in and help test and ﬁt in more features. However, the whole team decided that this plan would work the best with the least amount of risk.\n\nFocus on Value\n\nIt’s rather easy for a team to start discussing a complex story and lose sight of what value the features actually deliver. Release planning is the time to start asking for examples and use cases of how the features will be used, and what value they’ll provide. Drawing ﬂowcharts or sample calculations on the white- board can help pinpoint the core functionality.\n\nOur product owner wrote a story to provide a warning if an employer overrides the date a participant becomes eligible to contribute to a retirement account after the participant has already made contributions.\n\nThe warning needed to be incorporated into the legacy UI code, which didn’t easily accommodate it. The team discussed how it might be implemented, but every option was fairly costly. Not only would coding be tricky, but a lot of time was needed to test it adequately and update existing automated tests. This fea- ture wouldn’t provide much value to the business, just a bit of help to the end users. The release was already pretty close to the limit on features.\n\nOne of the programmers suggested providing a report of participants who met the criteria so the plan administrators could simply call the employers who may\n\n341\n\n—Janet",
      "content_length": 2614,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "342\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nneed to make corrections. The report story was much smaller than the warning story, could easily ﬁt into the release, and was acceptable to the customer.\n\nThere is no guarantee that these initial “guesstimates” at what will be in a given release will hold up over time. That is why customers needs to under- stand their priorities, take checkpoints at the end of every iteration, and re- evaluate the priorities of remaining stories.\n\nSystem-Wide Impact\n\nWe talked about the “ripple effects” in Chapter 8, “Busi- ness Facing Tests that Support the Team.”\n\nOne of our jobs as testers is to keep the big picture in mind. The agile tester thinks about how each story might affect the system as a whole, or other sys- tems that ours has to work with. For example, if the toy warehouse makes a change to its inventory software, and the new code has a bug that overstates the number of items in stock, the website might sell more of the hot new doll than there are available, disappointing thousands of children and their par- ents at Christmas. When risk is high, listing areas of the system that might be affected for a theme or group of stories might be a worthwhile exercise even during release planning.\n\nContact points between our system and that of partners or vendors always merit consideration. Even a minor change to a csv or xml ﬁle format could have a huge impact if we don’t communicate it correctly to partners who ftp ﬁles to us. Stories that mean changes for third parties need to be done early enough in the release cycle to let the third parties make necessary changes.\n\nFigure 15-5 shows a simpliﬁed diagram of a new system that touches many pieces of the existing system. Different tools might be needed to test the integrations.\n\nTesters who have worked with some of the other systems or understand what testing needs to happen on those systems can offer valuable insight into the impact of a new story. Often, stories will need to be delayed until a future re- lease if the impact has not been explored. This is a good time to recall previ- ous releases that didn’t end so well.\n\nThird-Party Involvement\n\nWorking with vendor tools, partners, or other contractor teams on a big project complicates release planning. If anyone outside your team is respon- sible for some part of the project, that’s one piece that’s out of your control. If\n\n—Lisa",
      "content_length": 2433,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "WHAT’S IN SCOPE?\n\nABC System\n\nProfiles (NS Tool 1)\n\nStandard Reports\n\nSAP\n\nOverhead (NS Tool 2)\n\nLegacy System\n\nGas System\n\nNew System\n\nWeb Reports\n\nOMS\n\nFuel System\n\nConversion (NS Tool 3)\n\nFigure 15-5 System impacts\n\nyou need to coordinate with others, including possible new users of the sys- tem, it’s best to start early.\n\nLisa’s team has written several interfaces to allow users to upload data to their systems. In each case, they had to get the proposed ﬁle format out to the us- ers early to make sure it would work for them. Other projects involved send- ing data to partners or vendors. These required extra planning to arrange testing with their test systems and getting their feedback on whether data was valid and correctly formatted.\n\nIf you’re using a third-party product as part of your solution, you might as- sume it has been tested, but that might be a poor assumption. You will need to budget extra time to test your application in conjunction with the vendor software. If there’s a problem in the other company’s software, it might take a long time to resolve. Lisa’s team uses third-party software for critical tasks such as document creation. If a theme includes modifying or creating new documents, they plan extra time to upgrade the software if needed, and extra\n\n343",
      "content_length": 1294,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "344\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\ntime for testing in case ﬁxes are needed. If possible, bring the third-party software into the project early, and start end-to-end testing. The more you can work with the interface, the better off you’ll be.\n\nOther third-party software that we often forget about until it’s too late is our own testing environments. Sometimes a team will incorporate new code that takes advantages of new features in their chosen language. For example, if team members are using AJAX or JavaScript, they may need to upgrade the software development kit they’re using. This means that a team will have to upgrade its production runtime environment as well, so take that into con- sideration and test early.\n\nClients or partners might have concerns about a release that isn’t within your own team’s scope. Lisa’s team was once prevented at the very last minute from releasing a new feature because a partner didn’t have time to okay the change with their legal advisors. The programmers had to quickly devise a way to turn the functionality off without requiring extensive additional test- ing. Interestingly, partners who aren’t using agile development sometimes have trouble meeting their own deadlines. They might be unprepared when your team meets the deadline.\n\nJanet’s Story\n\nI worked on a project to implement a feature that required a new piece of hard- ware for scanning a new 2D bar code. The team decided to implement in stages because it was not known when the scanners would be available for full testing, but the customer wanted the code ready for when the scanners arrived.\n\nThe initial phase was programmer-intensive because there was a lot of research to be done. After they determined how they would implement the feature, the story was created to add it into the code. However, we knew we couldn’t thoroughly test it until the scanners were available. The code was ready to test, but instead of backing it all out, we only needed to worry about testing that the feature could be turned off for the release. The next release would require more testing, but only if the scanners were available. The testing of the story was kept in the prod- uct backlog so we would not forget to do it.\n\nIf you’ll be working with other teams developing different components of the same system, or related systems, budget time to coordinate with them. It’s a good idea to designate a member from each team to coordinate together.\n\nRelease planning is the time to identify extra roles you need on your team, ad- ditional resources, and time needed for out-of-the-ordinary circumstances.\n\n—Janet",
      "content_length": 2647,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "Chapter 8, \"Business-Facing Tests that Support the Team,\" ex- plains how to identify steel threads or thin slices in a story or theme.\n\nTEST PLANNING\n\nTEST PLANNING We can’t expect to plan the iterations in a release at a detailed level. We can get an idea of the theme’s steel threads, prioritize stories, and make a guess at what stories will be in which iteration. Detailed test planning needs to wait for iteration planning. Still, we need to think about testing at a high level, and try to budget enough time for it. We might even take time separately from the release planning meeting to strategize our testing for the release. In Chapter 8, Business-Facing Tests that Support the Team, we mentioned one of the perils of agile testing: “forgetting the big picture.” Test Planning will help you with that problem.\n\nWhere to Start\n\nDuring release planning, it’s helpful to know the business conditions of satis- faction for each story or high-level user acceptance test case. When stories need clariﬁcation, agile testers ask for examples. At this stage, examples will be high-level, covering just the basics, but enough to be able to size and pri- oritize the story. Drawing ﬂowcharts or writing calculations on the white- board and discussing them helps us identify project-speciﬁc testing issues.\n\nAt a minimum, the team needs to understand the top-priority stories that are scheduled to be performed ﬁrst. Lightweight planning might involve only looking at those core stories with the understanding that more time will be needed for deﬁning additional tests.\n\nAs we get a sense of which stories will probably be included in the release, we can start thinking about the scope of the testing. What assumptions have been made that might affect testing? Use of third-party software, such as the example of using a shipping company’s shipping calculation API, affects test planning. Are there any unusual risks in this release that will impact testing? If we have stories to implement batch jobs, and we’ve never had any batch processing in the system before, there are probably new frameworks that im- pact testing. We need to budget time to learn them.\n\nWhy Write a Test Plan?\n\nIn release planning, we talk about the purpose of the release, what’s in scope, and what assumptions we’re making. We do some quick risk analysis and plan our test approach to address those risks. We consider automation and what we need for test environments and test data. We certainly want to iden- tify milestones and deliverables. Hmmm, this is starting to sound like . . . a test plan!\n\n345",
      "content_length": 2578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "346\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nSee Chapter 5, “Transitioning Typical Processes,” for more about test plans and test strategies.\n\nIf, like ourselves, you spent time working in a traditional waterfall environ- ment, you might have wasted time writing bulky test plans that nobody read and nobody bothered to maintain. In agile development, we want our test plans to serve our needs. Your customer might require a test plan for each re- lease for compliance reasons. Even if it’s not a required deliverable, it can be useful. Keep it concise and lightweight. It only has to serve your purposes during this release. Address the testing issues that are speciﬁc to this release or project. Include your risk analysis and identify assumptions. Outline the critical success factors that your customer has identiﬁed. Think about what people need to know related to testing, and remove anything extraneous.\n\nEven if you don’t create a formal test plan, be sure you have made note of all these different testing factors involved in the release. You’ll want to keep them in mind during every iteration planning session. The biggest beneﬁt in test planning is the planning itself. It allows you to consider and address issues such as test data requirements, infrastructure, or even what test results are re- quired. Test planning is a risk mitigation strategy. Let’s consider some of these issues.\n\nTypes of Testing\n\nIn Part III, we covered the four quadrants of testing and talked about all of the different types of testing you can do during your project. Release plan- ning is a good time to consider these different needs. Do you need to plan to bring in a load test tool, or will there be the need to build some kind of spe- cialty test harness?\n\nIt could be that your next release is just an extension of your last, and you will just carry on creating your examples, automating your story tests, and doing the rest of the testing as you’ve been doing. You are one of the lucky ones. For those of you who are starting a brand new project with no previous processes in place, now is the time to consider what testing you will need. We don’t mean you have to decide how to test each story, but look at the big pic- ture and think about the quadrants. Will you need to plan for a special UAT, or will the iteration demos be enough? It is important to raise these issues early so the team can plan for them.\n\nInfrastructure\n\nWhile you are doing your test planning, you need to consider your infra- structure. Infrastructure can mean your continuous integration setup, test environments, and test database. It can mean how you promote your builds",
      "content_length": 2675,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "Lisa’s Story\n\nTEST PLANNING\n\nto your test environments. It might mean your test lab, if you have one, or having a separate server to run all your automation tests. These are generally pieces of infrastructure that need some lead time to get in place. This is the time to make a plan.\n\nSome types of testing might require extra effort. My team had a tool to do perfor- mance testing and some scripts, but we lacked a production-style environment where we could control all of the variables that might affect performance. For ex- ample, the test database was shared by testers, programmers, and two build pro- cesses. Slower performance might simply mean someone was running database- intensive tests. We used our staging environment to get a baseline, but it was miss- ing some of the components of production. We set a six-month goal to acquire hardware and software for a proper test environment and get it set up. We wrote a task card or two each iteration to establish the environment step by step.\n\nWhatever your needs are, make sure you understand them and can plan for what you need. If you don’t have the right infrastructure, then you will waste time trying to get it together and cause a bottleneck in mid-iteration.\n\nTest Environments\n\nAs we look at the types of features in the next release, we might see the need for a whole new test environment. Think about specialized test environments you may need as well. Will you need more tools? Do you need to expand your test lab so that you can test with different browsers and operating systems? This is the time to think about all testing considerations.\n\nIf you’re planning your ﬁrst release, test environments are a key consider- ation. You might need a story or iteration just to set up the infrastructure you need. We’ve started more than one project where the only place we could test was the development environment. We found that doesn’t work very well, because the environment is never stable enough for effective testing.\n\nJust as programmers have their own sandboxes to work and test in, it works well if each tester has that same availability and control. We recognize that not all applications lend themselves to this, but at the very least, you need to know what build you’re testing. You also need test data that others will not walk over with their tests. If you don’t have a testing sandbox that’s under your own control, take time to plan what you need to establish for your test environments. Brainstorm with your team about how you can obtain the\n\n347\n\n—Lisa",
      "content_length": 2535,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "348\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nnecessary hardware and software. It might take time, so develop a Plan B for getting something done while waiting for the infrastructure you need.\n\nIf you’re working on a large system, you may have to queue up along with other teams to get time on a test or staging environment that includes all of the various pieces of software with which yours must work. This staging en- vironment should mimic your production system as much as possible. If your organization doesn’t have someone responsible for creating environ- ments, your team might need extra roles dedicated to obtaining the test envi- ronments you need. These roles might involve working with other teams as well. Release planning is the time to consider all of these testing infrastruc- ture requirements.\n\nTest Data\n\nRelease or theme planning is also a good time to think about what test data you might need during the project.\n\nUsing test data that closely resembles real data is generally a good practice. Plan for the data you need. We’ve had the opportunity in several organiza- tions to use a copy of production data. Real data provides a good base for different scenarios for exploratory testing. Production data may need to be “scrubbed” before it’s used for testing in order to remove any sensitive in- formation such as identiﬁcation or bank account numbers. The data needs to be altered to hide the original values but remain valid so that it doesn’t violate database restrictions. Because it takes time for database experts to port production data to a test environment, be sure they’re included in your planning.\n\nJanet’s Story\n\nIn one of the organizations I was working with, we used two different baseline test data schemes. For our individual test environments, we used Fit ﬁxtures to load predeﬁned data. We tried to make this data as close to production as possi- ble, but we also seeded it with some very speciﬁc test data. Every time we checked out a new version of code, we were able to reload a base set of data. In this way, we also tested the database schema as well to see if anything had changed.\n\nFor our more stable test environment where we wanted data persisted, we used the data migration scripts that the programmers developed as they made data- base changes. These migration scripts were eventually used for the initial cut over from production and by then we were pretty certain they were correct.\n\n—Janet",
      "content_length": 2474,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "Chapter 14, “An Agile Test Automa- tion Strategy,” ex- plores different approaches to ob- taining test data\n\nTEST PLANNING\n\nEnlist your customers’ support in obtaining meaningful test data. If you’re working on a story that involves sending a ﬁle to a third-party vendor, your business expert can ﬁnd out what data the vendor expects in the ﬁle. Lisa’s team developed features to allow retirement plan brokers to offer their cus- tomers portfolios of mutual funds. They asked the product owner to provide samples of portfolios, including a name, description, and set of funds for each. This helped them test with realistic data.\n\nTest data tends to get stale and out of date over time. Older data, even if it came from production, may no longer accurately reﬂect current production data. A “passing” test using data that’s no longer valid gives a misleading sense of conﬁdence. Continually review your test data needs. Refresh data or create it using a new approach, as needed.\n\nTest data requirements vary according to the type of testing. Regression tests can usually create their own data or run against a small representational set of data that can be refreshed to a known state quickly. Exploratory testing may need a complete replica of production type data.\n\nTest Results\n\nDifferent teams have different requirements for test result reporting. Think about how you are going to report test results at this stage of the game so that you can do so effectively when the time comes to do the actual reporting. Your organization may have audit compliance requirements, or maybe your customer just wants to know how you tested. Understand your needs so that you can choose the approach that is right for your team.\n\nThere are many ways to report test results. There are vendor tools that will record both automated and manual results. Your team may ﬁnd a way to per- sist the results from tools such as Fit, or you may just choose to keep a big visible manual chart.\n\nThe approach that a few teams have taken is to create home-grown test result applications. For example, a simple Ruby application written with Ruby on Rails for the database or a MySQL database with a PHP front end can make a very simple but easy-to-use test management system.\n\nA tool such as this can be very simple or can include added complexity such as the capability to categorize your tests. The important thing is the test re- sults. If your automated tests record their pass or fail result along with the er- ror, you have some history to help determine fragility of the test.\n\n349",
      "content_length": 2558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "350\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nYour team can conﬁgure your automated build process to provide test results from each build, by email, or a feedback utility or web interface that team members can view online. Results over time can be summarized in a variety of formats that make progress visible. One of Lisa’s teams produced a daily graph of tests written, run, and passing that was posted in the team’s work area. An- other produced a daily calendar with the number of unit tests passing every day. Even simple visual results are effective.\n\nWe talk about some of the metrics you can use later in this chapter.\n\nTEST PLAN ALTERNATIVES We’ve talked about why to test plan and what you should consider. Now we talk about some of the alternatives to the heavy test plans you may be used to. Whatever type of test plan your organization uses, make it yours. Use it in a way that beneﬁts your team, and make sure you meet your customer’s needs. As with any document your team produces, it should fulﬁll a purpose.\n\nLightweight Test Plans\n\nIf your organization or customer insists on a test plan for SOX compliance or other regulatory needs, consider a lightweight test plan that covers the neces- sities but not any extras. Do not repeat items that have already been included in the Project Plan or Project Charter. A sample Test Plan might look some- thing like the one shown in Figure 15-6.\n\nA test plan should not cover every eventuality or every story, and it is not meant to address traceability. It should be a tool to help you think about test- ing risks to your project. It should not replace face-to-face conversation with your customer or the rest of your team.\n\nUsing a Test Matrix\n\nJanet uses release planning to work with the testers and customers to develop a high-level test matrix. A test matrix is a simple way to communicate the big picture concerning what functionality you want to test. It gives your team a quick overview of the testing required.\n\nA test matrix is just a list of functionality down the side and test conditions across the top. When thinking about test conditions and functionality, con- sider the whole application and any impact the new or changed functionality",
      "content_length": 2238,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "TEST PLAN ALTERNATIVES\n\nProject ABC Test Plan Prepared by: Janet Gregory and Lisa Crispin\n\nIntroduction The Test Plan is intended as a baseline to identify what is deemed in and out of scope for testing, and what the risks and assumptions are.\n\nResourcing\n\nTester Janet Lisa\n\n% Committed 100% 50%\n\nIn Scope Testing includes all new functionality, identiﬁed high-risk regression suite functionality, UAT, and Load Testing. Localization is part of this project. Manual regression tests deemed low priority will be run if time permits.\n\nOut of Scope Actual translation testing is outsourced, so it is not part of this test plan.\n\nNew Functionality The following functionality is being changed in this release.\n\nFeature Description Adding new toggle for language selection on home page\n\nDepth of Testing Testing all 5 languages (English, Spanish, French, Italian, and German). Testing that we are able to dynamically switch languages.\n\nPerformance & Load Testing Load testing will concentrate on the following areas. Load testing details will be found in the Load Test Plan document [link to Load Test Plan].\n\nUAT (User Acceptance Testing) UAT will be performed and coordinated with the Paris ofﬁce as well as the Calgary ofﬁce. Users will be chosen for their expertise in select areas and transactions as well as being ﬂuent in one of the following languages: German, Italian, Spanish, or French.\n\nInfrastructure Considerations The test lab will need all 5 languages installed and available for testing.\n\nAssumptions Translation has been tested before being delivered to project team.\n\nRisks The following risks have been identiﬁed and the appropriate action identiﬁed to miti- gate their impact on the project. The impact (or severity) of the risk is based on how the project would be affected if the risk was triggered.\n\n# 1\n\nRisk Users aren’t ready for UAT\n\nImpact HIgh\n\nMitigation Plan\n\nFigure 15-6 Sample Test Plan\n\n351",
      "content_length": 1921,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "352\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nmight have on the rest of the application. Testers sitting with customers and thinking about test conditions is what is important.\n\nIt can also be a mechanism to track coverage and can be as detailed as you like. A high-level test matrix can be used by the team to show the customer team or management what has been tested already and what is left. A more detailed test matrix can be used by the team to show what is planned for test- ing and track the progress of the testing. After the matrix has been created, it becomes easy to ﬁll in the squares when testing is done. Keep it simple. Be- cause we like big visible charts that are easy to read, we recommend colors that mean something to your team. For example, green (G) means testing is done and the team is happy with it, while yellow (Y) might mean some test- ing has been done but more exploratory testing is needed if there is time. Red (R) means something is broken. A white square means it hasn’t been tested yet, and a gray (not applicable) square means it doesn’t need to be tested.\n\nLet’s look at an example. We have a small release we want to put out that cal- culates shipping costs. In Figure 15-7, different pieces of functionality are represented on one axis, and properties of the shipment are represented on the other. Individual cells are color-coded to show which cases are tested and which need more attention. All of the cells for “<= 2 lbs” are ﬁnished, the top three cells for > 4 lbs are done but need more exploratory testing, and the “Ship to Alaska”/“>4 lbs” cell denotes a possible issue.\n\nShipping Test Matrix\n\nTest Conditions\n\nFunctionality\n\nn o i t a n\n\ni t s e D e\n\nl\n\ng n\n\ni\n\nS\n\ns n o i t a n\n\ni t s e D e p i t l\n\nl\n\nu M\n\ns s e r d d A\n\nl a c i s y h P\n\ns b\n\nl\n\n2 = <\n\ns b\n\nl\n\n4\n\n–\n\n2\n\ns b\n\nl\n\n4 >\n\ny a D e m a S\n\ny a D t x e N\n\ns y a D s s e n i s u B\n\n5 <\n\nShip within US\n\nG\n\nG\n\nY\n\nShip to Canada\n\nG\n\nY\n\nShip to Hawaii\n\nG\n\nY\n\nShip to Alaska\n\nG\n\nR\n\nShipping estimates\n\nG\n\nFigure 15-7 A sample test matrix\n\ns e t a m i t s E g n p p h S\n\ni\n\ni\n\nn/a",
      "content_length": 2109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "Janet’s Story\n\nTEST PLAN ALTERNATIVES\n\nI had an unexpected side effect from using a test matrix in one project I was on. The customers and testers put the test matrix together, and had thought of all af- fected functionality for the project and the high-level test conditions they would need. As expected, the act of planning brought a lot of issues out that would have been missed until later.\n\nWhen they hung the matrix on the wall in their team area, Dave, the developer team lead, expressed an interest. One of the testers explained the matrix to him, and I was surprised when he said it was very useful for them as well. Dave said “I didn’t know that this functionality would affect this area. We need to make sure our unit tests touch on this as well.”\n\nLooking back on this, I shouldn’t have been surprised, but I had never had that ex- perience with the programmers before.\n\nA test matrix is a very powerful tool and can be used to help address trace- ability issues if your team has those problems. Think about what makes sense for your team and adapt it for your team and what makes sense to you.\n\nTest Spreadsheet\n\nJanet has also seen a spreadsheet format used with some success. For exam- ple, at WestJet, the ﬁrst tab in a workbook was a high-level list of functional- ity that existed in the application. For each row, the team determined if the project affected that piece of functionality. If so, they gave a rating of the ex- pected impact. After the impact of the changes had been determined, deci- sions about test environments, test data, or UAT could then be made.\n\nTabs were used for risks and assumptions but could be used for anything your team may need. A ﬂexible format such as a spreadsheet means you can tailor it to work for you.\n\nThis information can be used in a number of different ways. It can be used to determine where to concentrate your exploratory testing efforts, or maybe to help create a high-level test matrix to make sure you touch on all of the areas during your testing.\n\nA Whiteboard\n\nIf your team is informal and has small releases, any kind of documentation may be too much. Sometimes it’s enough to list the risks and assumptions on a whiteboard or on index cards. Janet has used a whiteboard to manage risks,\n\n353\n\n—Janet",
      "content_length": 2271,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "354\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nand it worked quite well. If a risk actually became an issue, the result was documented and crossed off. It was easy to add new risks and mitigation strategies, and the list was visible to the whole team. This could also be done on a wiki page.\n\nWe cannot stress enough that you need to know your team and its needs.\n\nAutomated Test List\n\nSometimes you may be required to present more information to your cus- tomers, such as a list of test cases. If your team has a tool from which you could extract a list of test case names, you could provide this list easily to any- one who needed it. This would present more of a traditional type detailed test plan but wouldn’t be available until after tests were actually written. We don’t recommend spending any time on this because we don’t see added value, but sometimes this list may be required for risk assessment or auditability.\n\nPREPARING FOR VISIBILITY If your team is just getting started with agile development, make sure you have necessary infrastructure in place for your early iterations. You may change the way you are tracking progress as you go along, and your retrospectives will help you bring these issues to light. If you’re having problems completing the work planned for each iteration, maybe you need more visible charts or visual aids to help you gauge progress and make mid-iteration adjustments. Do your customers have some way to know how the iteration is progressing and which stories are done? Take time before the each iteration to evaluate whether you’re getting the right kind of feedback to keep track of testing.\n\nTracking Test Tasks and Status\n\nThe effective agile teams we know all follow this simple rule: “No story is done until it’s tested.” This rule can be expanded to say that not only must the story be tested, the code must be checked in, it must have automated tests that are run by a continual build process, it must be documented, or whatever your team’s “doneness” criteria are. At any time during an iteration, you need to be able to quickly assess how much testing work remains on each story, and which stories are “done.” Story or task boards are ideal for this purpose, especially if they use color-coding to denote test tasks vs. development and other types of tasks. Cork boards, steel sheets with magnets, poster-sized sticky notes, or whiteboards all work ﬁne. Give each story its own row, and order them by pri- ority. Have columns for “to do,” “work in progress,” “verify,” and “done.”",
      "content_length": 2559,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "Janet’s Story\n\nJanet’s Story\n\nPREPARING FOR VISIBILITY\n\nI started with team members who had been doing agile for a few months with only a couple of programmers and one tester. They had been using XPlanner to track their tasks and stories, and it was working ok for them. At the same time I came on board, a couple of new programmers were added, and the stand-ups became less effective; the team was not completing the stories it had planned. I suggested a storyboard, and although they were skeptical about keeping two sets of “tasks,” they said they would try it.\n\nWe took an open wall and used stickies to create our story board. We started hav- ing stand-ups in front of the story board and our discussion became more spe- ciﬁc. It provided a nice visible way of knowing when the tasks were done and what was left to do. After a couple of months, the team grew again and we had to move the story board into an ofﬁce. We also moved our stand-ups and our test re- sult charts there. However, the constant visibility was lost, and programmers and testers stopped moving their tasks.\n\nWe had to reevaluate what we wanted to do. One size does not ﬁt all teams. Make sure you plan for what is right for your team.\n\nSome teams use different colored index cards for the different types of tasks: green for testing, white for coding, yellow and red for bugs. Other teams use one card per development task, and add different colored stickers to show that testing is in progress or show that there are bugs to resolve. Use any method that lets you see at a quick glance how many stories are “done,” with all cod- ing, database, testing, and other tasks completed. As the iteration progresses, it’s easy to see if the team is on track, or if you need to pull a story out or have programmers pitch in on testing tasks.\n\nOur story board (shown in Figure 15-8) wasn’t very big, and we didn’t have a lot of wall space to expand to have the regular column-type task board. Instead, we decided to use stickers to designate the status.\n\nWhite cards, such as those shown in the ﬁrst row of Figure 15-8, were regular tasks, blue cards designated technical stories such as refactoring or spikes, and pink cards, shown toward the right-hand side of the board as the darkest color, were bugs that need to be addressed. It is easy to see that this picture was taken at the beginning of an iteration because there are no colored circles on each card. In the top right-hand corner, you can see the legend. Blue stickers meant it has been coded, green would indicate done (tested), and red meant the task has been deemed not completed or a bug was rejected as not ﬁxed. As a task or story was completed (i.e., green sticker), it was moved to the right of the board.\n\n355\n\n—Janet\n\n—Janet",
      "content_length": 2761,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 399,
      "content": "356\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nFigure 15-8 Example story board\n\nLisa’s Story\n\nFor more than four years, our story board was a couple of sheets of sheet metal, painted in company colors, using color-coded index cards attached to the board with magnets. Figure 15-9 shows a picture of it early in an iteration. Our task cards were also color-coded: white for development tasks, green for coding tasks, yel- low and red for bugs, and striped for cards not originally planned in the iteration. The board was so effective in indicating our progress that we eventually stopped bothering with a task burndown chart. It let us focus on completing one story at a time. We also used it to post other big visible charts, such as a big red sign show- ing the build had failed. We loved our board.\n\nThen, one of our team members moved overseas. We tried using a spreadsheet along with our physical story board, but our remote teammate found the spread- sheet too hard to use. We tried several software packages designed for Scrum teams, but they were so different from our real story board that we couldn’t ad- just to using them. We ﬁnally found a product (Mingle) that looked and worked enough like our physical board that everyone, including our remote person, could use it. We painted our old story board white, and now we can project the story board on the wall during stand-up meetings.\n\n—Lisa",
      "content_length": 1428,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "PREPARING FOR VISIBILITY\n\nFigure 15-9 Another sample story board\n\nDistributed teams need some kind of online story board. This might be a spreadsheet, or specialized software that mimics a physical story board as Mingle does.\n\nCommunicating Test Results\n\nEarlier, we talked about planning how to track test results. Now we want to talk about effectively communicating them. Test results are one of the most important ways to measure progress, see whether new tests are being written and run for each story, and whether they’re all passing. Some teams post big visible charts of the number of tests written, run, and passed. Others have their build process email automated test results to team members and stake- holders. Some continuous integration tools provide GUI tools to monitor builds and build results.\n\nWe’ve heard of teams that have a projector hooked up to the machine that runs FitNesse tests on a continuous build and displays the test results at all times. Test results are a concrete depiction of the team’s progress. If the number of\n\n357",
      "content_length": 1053,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "358\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\ntests doesn’t go up every day or every iteration, that might indicate a problem. Either the team isn’t writing tests (assuming they’re developing test-ﬁrst), or they aren’t getting much code completed. Of course, it’s possible they are rip- ping out old code and the tests that went with it. It’s important to analyze why trends are going the wrong way. The next section gives you some ideas about the types of metrics you may want to gather and display.\n\nHowever your team decides they want to communicate your progress, make sure you think about it up front and everyone gets value from it.\n\nRelease Metrics\n\nWe include this section here, because it is important to understand what metrics you want to gather from the very beginning of a release. These met- rics should give you continual feedback about how development is proceed- ing, so that you can respond to unexpected events and change your process as needed. Remember, you need to understand what problem you are trying to solve with your metrics so that you can track the right ones. The metrics we talk about here are just some examples that you may choose to track.\n\nNumber of Passing Tests Many agile teams track the number of tests at each level: unit, functional, story tests, GUI, load, and so on. The trend is more important than the num- ber. We get a warm fuzzy feeling seeing the number of tests go up. A number without context is just a number, though. For example, if a team says it has 1000 tests, what does that mean? Do 1000 tests give 10% or 90% coverage? What happens when code that has tests is removed?\n\nTracking the number of tests written, running, and passing at a story level is one way to show a story’s status. The number of tests written shows progress of tests to drive development. Knowing how many tests aren’t passing yet gives you an idea of how much code still needs to be written.\n\nAfter a test passes, it needs to stay “green” as long as the functionality is present in the code. Graphs of the number of tests passing and failing over time show whether there’s a problem with regression failures and also show the growth of the code base. Again, it’s the trend that’s important. Watch for anomalies.\n\nThese types of measurements can be reported simply and still be effective.",
      "content_length": 2343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "PREPARING FOR VISIBILITY\n\nFigure 15-10 Full build result email from Lisa’s team\n\nLisa’s Story\n\nMy team emails a color-coded calendar out every day showing whether the “full build” with the full suite of regression tests passed each day (see Figure 15-10). Two “red” days in a row (the darkest color) are a cause for concern and noticed by management as well as the development team. Seeing the visual test results helps the organization pull together to ﬁx the failing tests or any other problems causing the build to not run, such as hardware or database issues.\n\nThere are different ways to measure the number of tests. Choose one and try to stay consistent across the board with all types of tests, otherwise your met- rics may get confusing. Measuring the number of test scripts or classes is one way, but each one may contain multiple individual test cases or “asserts,” so it may be more accurate to count those.\n\nIf you’re going to count tests, be sure to report the information so that it can be used. Build emails or build status UIs can communicate the number of tests run, passed, and failed at various levels. The customer team may be content\n\n359\n\n—Lisa",
      "content_length": 1166,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": "360\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nto see this information only at the end of each sprint, in the sprint review, or an email.\n\nWhatever metrics you choose to gather, be sure the team buys into them.\n\nJanet’s Story\n\nI started a new contract with a team that had been doing agile for a couple of years, and they had developed a large number of automated functional tests. I started keeping track of the number of tests passing each day. The team didn’t see a problem when the trending showed fewer and fewer tests were passing. The unit tests were maintained and were doing what they were supposed to do, so the team felt conﬁdent in the release. It seemed this happened with every re- lease, and the team would spend the last week before the release to make all of the tests pass. It was costly to maintain the tests, but the team didn’t want to slow down to ﬁx them. Everyone was okay with this except me.\n\nI did not see how ﬁxing the tests at that late date could ensure the right expected results were captured. I felt that we ran the risk of getting false positives.\n\nAt the start of the next release cycle, I got the team to agree to try ﬁxing the tests as they broke. It didn’t take long for the team to realize that it wasn’t so tough to ﬁx the tests as soon as we knew they were broken, and we found a lot of issues early that hadn’t usually been caught until much later. The team soon set a goal of having 95% of the tests passing at all times.\n\nWe also realized how brittle the tests were. The team made a concerted effort to refactor some of the more complex tests and eliminate redundant ones. Over time, the number of high-level tests was reduced, but the quality and coverage was increased.\n\nWe started out measuring passing rates, but we ended up with far more.\n\nDon’t get so caught up in the actual measurements that you don’t recognize other side effects of the trending. Be open to adjusting what you are measur- ing if the need is there.\n\nCode Coverage Code coverage is another traditional metric. How much of our code is exer- cised by our tests? There are excellent commercial and open source code cov- erage tools available, and these can be integrated into your build process so that you know right away if coverage has gone up or down. As with most met- rics, the trend is the thing to watch. Figure 15-11 shows a sample code coverage report.\n\n—Janet",
      "content_length": 2411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 404,
      "content": "PREPARING FOR VISIBILITY\n\n361\n\nGHIDRAH\n\nOverall Coverage Summary\n\nName all classes\n\nClass, % 95% (1727/1809)\n\nMethod, % 77% (13605/17678)\n\nBlock, % 72% (201131/279707)\n\nLine, % 75% (43454.5/58224)\n\nOverall Stats Summary\n\ntotal packages: total executable ﬁles: total classes: total methods: total executable lines:\n\n240 1329 1809 17678 58224\n\nWHITNEY\n\nOverall Coverage Summary\n\nName all classes\n\nClass, % 15% (109/737)\n\nMethod, % 8% (669/8760)\n\nBlock, % 4% (16292/363257)\n\nLine, % 5% (3713.7/80358)\n\nOverall Stats Summary\n\ntotal packages: total executable ﬁles: total classes: total methods: total executable lines:\n\n46 655 737 8760 80358\n\nFigure 15-11 Sample code coverage report from Lisa’s team. “Ghidrah” is the new architecture; “Whitney” is the legacy system.\n\nFigures 15-12 and 15-13 are two examples of trends that work together. Figure 15-12 shows a trend of the total number of methods each iteration. Fig- ure 15-12 is the matching code coverage. These examples show why graphs need to be looked at in context. If you only look at the ﬁrst graph showing the number of methods, you’ll only get half the story. The number of meth- ods is increasing, which looks good, but the coverage is actually decreasing. We do not know the reason for the decreased coverage, but it should be a trigger to ask the team, “Why?”\n\nRemember that these tools can only measure coverage of the code you’ve written. If some functionality was missed, your code coverage report will not",
      "content_length": 1471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "362\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nFigure 15-12 Number of methods trend\n\nFigure 15-13 Test coverage",
      "content_length": 137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "PREPARING FOR VISIBILITY\n\nbring that to light. You might have 80% code coverage with your tests, but you’re missing 10% of the code you should have. Driving development with tests helps avoid this problem, but don’t value code coverage statistics more than they deserve.\n\nKnow What You Are Measuring\n\nAlessandro Collino, a computer science and information engineer with Onion S.p.A. who works on agile projects, told us about an experience where code coverage fell suddenly and disastrously. His agile team developed middle- ware for a real-time operating system on an embedded system. He explained:\n\nA TDD approach was followed to develop a great number of good unit tests oriented to achieve good code coverage. We wrote many effective acceptance tests to check all of the complex functionalities. After that, we instrumented the code with a code coverage tool and reached a statement coverage of 95%.\n\nThe code that couldn’t be tested was veriﬁed by inspection, leading them to declare 100% of statement coverage after ten four-week sprints.\n\nAfter that, the customer required to us to add a small feature before we delivered the software product. We implemented this request and ap- plied the code optimization of the compiler.\n\nThis time, when we ran the acceptance tests, the result was disastrous; 47% of acceptance tests failed, and the statement coverage had fallen down to 62%!\n\nWhat happened? The problem turned out to be due to enabling compiler optimization but with an incorrect setting. Because of this, a key value was read once as the application started up and was stored in a CPU register. Even when the variable was modiﬁed in memory, the value in the CPU register was never replaced. The routine kept reading this same stale value instead of the correct updated value, causing tests to fail.\n\nAlessandro concludes, “The lesson learned from this example is that the en- abling of the compiler optimization options should be planned at the beginning of the project. It’s a mistake to activate them at the ﬁnal stages of the project.”\n\nGood metrics require some good planning. Extra effort can give you more meaningful data. Pierre Veragen’s team members use a break-test baseline technique to learn if their code coverage metric is meaningful. They manu- ally introduce a ﬂaw into each method and then run their tests to make sure the tests catch the problem. Some tests just make sure the code returns some value, any value. Pierre’s team makes sure the tests return the correct value. In this way, they can determine whether their test coverage is good enough.\n\n363",
      "content_length": 2586,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "364\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nCode coverage is just one small part of the puzzle. Use it as such. It doesn’t tell you how good your tests are but only if a certain chunk of code was run during the test. It does not tell you if different paths through the application were run, either. Understand your application and try identifying your highest risk areas, and set a coverage goal that is higher for those areas than for low-risk areas. Don’t forget to include your functional tests in the coverage report as well.\n\nDefect Metrics As your team sets goals related to defects, use appropriate metrics to measure progress toward those goals. There are trends that you will want to monitor for the whole release, and there are ones that are iteration-speciﬁc. For exam- ple, if you’re trying to achieve zero defects, you may want to track the open bugs at the end of each iteration, or how many bugs were found after devel- opment but before release. Most of us are interested in knowing how many defects have been reported after the code is in production, which is some- thing completely different altogether. These issues will tell you after the fact how well your team did on the last release, but not how well you are doing on the current release. They may give you some indication of what processes you need to change to reduce the number of defects. Lisa’s team is more con- cerned with production defects found in the “new” code that was rewritten in the new architecture. They’re working hard to produce this new code with zero defects, so they need to know how well they’re doing. They expect that bugs will be found fairly often in the legacy system, where only the most crit- ical functionality is covered by automated GUI smoke tests, and there are few automated unit and behind-the-GUI tests.\n\nKnowing the defect rate of legacy code might be good justiﬁcation for refac- toring or rewriting it, but the team’s top priority is doing a good job with the new code, so they group bugs by “new” and “old” code, and focus on the “new” bugs.\n\nMore on defect tracking systems can be found in Chapter 5, “Transi- tioning Traditional Processes.”\n\nMake sure your bug database can track what you want to measure. You may have to make some changes in both the database and your process to get the data you need. For example, if you want to measure how many defects were found in production after a release, you have to make sure you have environ- ment and version as mandatory ﬁelds, or make sure that people who enter bugs always ﬁll them in.\n\nBecause defect tracking systems are often used for purposes besides tracking bugs, be sure not to muddle the numbers. A request for a manual update to the database doesn’t necessarily reﬂect an issue with the existing code. Use your defect tracking tool properly to ensure that your metrics are meaningful.",
      "content_length": 2891,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "Lisa’s Story\n\nPREPARING FOR VISIBILITY\n\nPeriodically evaluate the metrics you’re reporting and see if they’re still relevant. Figure 15-14 shows two defect reports that Lisa’s team used for years. When we ﬁrst transitioned to agile, managers and others looked at these reports to see the progress that resulted from the new process. Four years later, our ScrumMaster found that nobody was reading these reports anymore, so we quit producing them. By that time, rates of new defects had reduced dramatically, and nobody really cared about the old defects still hanging about in the legacy code.\n\nFigure 15-14 Sample defect reports used (and no longer used) by Lisa’s team\n\n365\n\n—Lisa",
      "content_length": 682,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "366\n\nCHAPTER 15\n\n(cid:2) TESTER ACTIVITIES IN RELEASE OR THEME PLANNING\n\nRelease planning is a good time to evaluate the ROI of the metrics you’ve been tracking. How much effort are you spending to gather and report the metrics? Do they tell you what you need to know? Does the code you release meet your team’s standards for internal quality? Is the code coverage percent- age going up? Is the team meeting its goals for reducing the number of de- fects that get out to production? If not, was there a good reason?\n\nMetrics are just one piece of the puzzle. Use your release, theme, or project planning meetings to refocus on delivering business value when the business needs it. Take some time to learn about the features you’re about to develop. Don’t get caught up with committing to your plans—the situation is bound to change. Instead, prepare for doing the right activities and getting the right resources in time to meet the customers’ priorities.\n\nSUMMARY As your team puts together its plan for a new theme or release, keep the main points of this chapter in mind.\n\n(cid:2) When sizing a story, consider different viewpoints, including business value, risk, technical implementation, and how the feature will be used. Ask clarifying questions, but don’t get bogged down in details. (cid:2) Testers can help identify the “thin slice” or “critical path” through a\n\nfeature set to help prioritize stories. Schedule high-risk stories early if they might require extra testing early.\n\n(cid:2) The size of testing effort for a story helps determine whether that\n\nstory is in scope for the release.\n\n(cid:2) Testers can help the team think about how new stories will impact the\n\nlarger system.\n\n(cid:2) Plan for extra testing time and resources when features may affect sys-\n\ntems or subsystems developed by outside teams.\n\n(cid:2) As the team identiﬁes the scope of the release, evaluate the scope of\n\ntesting and budget enough time and resources for it.\n\n(cid:2) Spend some time during release planning to address infrastructure,\n\ntest environment, and test data concerns.\n\n(cid:2) A lightweight, agile test plan can help make sure all of the testing con- siderations are addressed during the life of the release or project. (cid:2) Consider alternatives to test plans that might be more appropriate for your team; test matrices, spreadsheets, or even a whiteboard may be sufﬁcient.",
      "content_length": 2387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 410,
      "content": "SUMMARY\n\n(cid:2) Formal release planning may not be appropriate for your situation. In the absence of release planning, consider identifying and discussing at least the ﬁrst few stories that should be done ﬁrst.\n\n(cid:2) Plan for what metrics you want to capture for the life of the release; think about what problem you are trying to solve and capture only those metrics that are meaningful for your team.\n\n367",
      "content_length": 411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "Chapter 16\n\nHIT THE GROUND RUNNING\n\nBenefits\n\nResources\n\nBe Proactive\n\nDo You Really Need This?\n\nPotential Downsides to Advanced Preparation\n\nCustomers Speak with One Voice\n\nPrioritize Defects\n\nBefore the Iteration\n\nAdvance Clarity\n\nStory Size\n\nGeographically Dispersed Customers and Teams\n\nTest Strategies\n\nExamples\n\nIn agile development, we generally like to do tasks “just in time.” We can’t see around the curves in the road ahead, so we focus on the activities at hand. Then again, we want to hit the ground running when we start each new iteration. That may require a little preparation. Baking is a good analogy here. You decide you want to bake cookies because someone is coming over. Before you start, you make sure you have the right ingredients. If you don’t, you either go buy what you need, or you choose a different kind to make.\n\nDon’t go overboard—if a pre-iteration activity doesn’t save time during the iteration, or help you do a better job, don’t do it before the iteration. Do what is appropriate for your team, and keep experimenting. Maybe you’ll do some of these activities after the iteration starts instead. Here are some ideas to think about that might help you “bake quality in” to your product.\n\nBE PROACTIVE In Chapter 2, “Ten Principles of an Agile Tester,” we explained how agile testers have to shift their mind-set. Instead of waiting for work to come to us, we develop a proactive attitude where we get up and go look for ways to\n\n369",
      "content_length": 1469,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "370\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\ncontribute. With the fast and constant pace of agile development, it’s easy to get immersed in the current iteration’s stories. We are so busy making sure we’ve covered the features with up-front tests, performing exploratory test- ing to be sure we’ve understood the business requirements, and automating adequate regression tests, it’s hard to think of anything else. However, it’s sometimes appropriate to take a bit of time to help our customers and our team prepare for the next iteration. When our team is about to break new ground, or work on complex and risky stories, some work before the itera- tion can help maximize our team’s velocity and minimize frustration.\n\nWe sure don’t want to spend all our time in meetings, or planning for stories that might be re-prioritized. However, if we can make our iteration planning go faster, and reduce the risk of the stories we’re about to undertake, it’s worth doing some research and brainstorming before we start the iteration.\n\nBeneﬁts\n\nWorking on stories in advance of the iteration may be especially useful for teams that are split across different geographic locations. By working ahead, there’s time to get information to everyone and give them a chance to give their input.\n\nThe problem is that we’re so busy during each short agile iteration that it’s hard to ﬁnd time to meet about the next iteration’s stories, much less start writing test cases. If your iterations always go smoothly, with stories delivered incrementally and plenty of time to test, and the delivered software matches customer expectations, you may not need to take time to prepare in advance. If your team has trouble ﬁnishing stories, or ends up with big mismatches be- tween actual and desired behavior of features, a little advance planning may save you time during the iteration.\n\nLisa’s Story\n\nOur team used to feel we didn’t have time to plan in advance for the next itera- tion. After many experiences of misunderstanding stories and having them far ex- ceed estimations, and ﬁnding most “bugs” were missed requirements, we decided to budget time in the iteration to start talking about the next one. Now the whole team, including the product owner and other customers as needed, meet for an hour or less the day before the planning meeting for our next sprint.\n\nWe laughingly call this the “pre-planning” meeting. We go over the stories for the next iteration. The product owner explains the purpose of each story. He goes over the business conditions of satisfaction and other items in his story checklists, and gives examples of desired behavior. We brainstorm about potential risks and dependencies, and identify steel threads where appropriate.",
      "content_length": 2737,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 414,
      "content": "BE PROACTIVE\n\nSometimes it’s enough to spend a few minutes listening to the product owner’s explanation of the stories. At other times, we take time to diagram thin slices of a story on the whiteboard. Figure 16-1 shows an example diagram where we got into both the details of the UI ﬂow and the database tables. Note the numbers for the “threads.” Thread #1 is our critical path. Thread #2 is the second layer, and so on. We upload photos of these diagrams to the wiki so our remote developer can see them too.\n\nWe can start thinking about what task cards we might write the next day and what approach we might take to each story. For some reason, being able to rumi- nate about the stories overnight makes us more productive in the actual iteration planning meeting the next day. After doing this for a few iterations, we were spending less time overall in planning the iteration, even though we had two meetings to do it.\n\nSometimes we pull in other customers to discuss stories that affect them directly. If they aren’t available right then, we still have time before our iteration planning meeting to talk to them to clarify the story.\n\nIn one pre-planning meeting, our product owner introduced a story about ob- taining performance data for mutual funds. We would send a ﬁle to the vendor containing a list of mutual funds, and the vendor would provide an XML ﬁle on a website with all the latest performance information for those funds. We would then upload that data into our database.\n\nFigure 16-1 Sample planning whiteboard diagram\n\n371",
      "content_length": 1546,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "372\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\nIn the pre-planning meeting, we asked questions such as, “What’s the format of the ﬁle we send the vendor?” “Is the ‘as-of’ date for each fund always the last day of the month?” “Is there any security on the website that contains the XML?” “Will we ever get a record for the same fund and “as-of” date that has new data, or can we ignore records with a date we already have in our database?”\n\nBy the next day’s iteration planning meeting, the product owner had obtained an- swers to all our questions. Writing task cards went quickly, and coding could pro- ceed with correct assumptions.\n\nOften, we ﬁnd a much simpler solution to a story when we discuss it in the pre- iteration planning discussion. We found that to go fast, we needed to slow down ﬁrst.\n\nDo You Really Need This?\n\nYour team may not need much or any advance preparation. Pierre Veragen and Erika Boyer described to Lisa how their teams at iLevel by Weyerhaeuser write user acceptance tests together at their iteration kickoff meeting.\n\nThese tests, which were written on a wiki page or some similar tool along with the story narrative, are used later when team members write task cards for each story and start writing more tests and code. Examples are turned into executable tests. Because the tests change as the team learns more about the story, the team may opt not to maintain the original ones that were writ- ten at the start. Keep it simple to start with, and dig into details later.\n\nLisa subsequently observed one of their planning sessions and saw ﬁrst-hand how effective this technique was. Even when the product manager provides concrete examples, turning them into tests may ﬂush out missing require- ments. Their team did not need to do this before the iteration planning ses- sion, but it is not the case with all teams.\n\nLisa’s Story\n\nMy team liked the practice of writing tests together, but because we were writing task cards during iteration planning, we decided to write user acceptance tests together during the pre-planning meeting. We found this kept our discussions fo- cused and we understood each story more quickly. We also did a better job of delivering exactly what the customer had in mind. Our customers noticed a differ- ence in quality, and our product owner encouraged us to continue this practice.\n\n—Lisa\n\n—Lisa",
      "content_length": 2363,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "ADVANCE CLARITY\n\nExperiment with short pre-iteration discussions and test-writing sessions. It’ll take you several iterations to ﬁnd your team’s rhythm, and ﬁnd out if ad- vance story discussions make you more productive during the iteration.\n\nPotential Downsides to Advance Preparation\n\nThere’s a risk to “working ahead.” You could spend time learning more de- tails about a feature only to have the business people re-prioritize at the last minute and put that feature off indeﬁnitely. Invest preparation time when it’s appropriate. When you know you have a complex theme or story coming up, and it has a hard deadline such as Lisa’s team had with the statement story, consider spending some time up front checking out different viewpoints. The only reason to discuss stories in advance is to save time during iteration planning and during development. A deeper understanding of the feature be- havior can speed up testing and coding, and can help make sure you deliver the right functionality.\n\nIf your situation is so dynamic that stories might be re-prioritized the day that the iteration starts, it isn’t worth trying to do this planning. Instead, make sure you budget time for these discussions during your planning meeting.\n\nADVANCE CLARITY Lisa’s product owner, Steve Perkins, came up with the term “advance clarity.” Different parts of each organization have different priorities and agendas. For example, Business Development is looking for new features to attract new business, while Operations is prioritizing features that would reduce the number of phone calls from users. The development team tries to understand the range of business needs and get a feel for each individual’s job.\n\nWith many different agendas, someone needs to decide what stories should be implemented in the next iteration. Because there are many ways to imple- ment any given story, someone has to decide the speciﬁc requirements and capture them in the form of examples, conditions of satisfaction, and test cases. Steve gets everyone together to agree on the value they want from each story, and to provide “advance clarity.”\n\nCustomers Speak with One Voice\n\nScrum provides the helpful role of the product owner to help all the custom- ers “Speak with One Voice.” Whether or not you’re on a Scrum team, ﬁnd some way to help your customers agree on the priority of the stories and how\n\n373",
      "content_length": 2378,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "374\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\nthe components of each story ought to be implemented. Management sup- port is crucial, because any person in this role needs time and authority to get everyone on the same page.\n\nOther teams use business analysts to help ﬂesh out the stories before the next iteration. In one organization Janet worked with, the customers were not available full-time to answer questions, but each team had a business analyst that worked with the customers to ﬂesh out the requirements before the iter- ation planning meeting. If there were any questions that she could not an- swer at the meeting, the team either called the customer directly or the analyst followed up immediately after the meeting.\n\nAs a tester, you want to sit in on story writing and prioritization meetings. Ask questions that help the customers focus on the core functionality, the critical business value they need. Help participants stay focused on concrete examples that crystallize the meaning of the stories. In meetings that involve multiple customers, it is critical to have a strong facilitator and a method for determining consensus.\n\nAs with code, stories are best if they have the bare minimum. For example, an Internet shopping cart needs some way to delete unwanted items, but the ability to move items from the cart to a “save for later” list can probably wait. It may be helpful to talk about this before the iteration, so that the team is clear on what tasks need to be planned. Focus on the simplest thing ﬁrst and use an example to make it clear.\n\nGet All Viewpoints Getting requirements from different customers for a story, each of whom has a different agenda, might create chaos. That’s why it’s essential for someone on the customer team to get consensus and coordinate all points of view. This doesn’t mean we shouldn’t get input from different customers. As a tester, you’re considering each story from multiple points of view. It helps to know what the story means to people in different roles.\n\nLisa’s Story\n\nWhen my company decided to redesign the retirement plan participants’ quarterly account statements, different people on the business side wanted changes for different reasons. The plan administrators wanted a clearly understandable layout that would minimize the number of calls from confused participants to customer support.\n\nFor example, they wanted the statement to show the date and amount of the par- ticipant’s most recent contribution. This helps the participant know whether her",
      "content_length": 2528,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "Lisa’s Story\n\nADVANCE CLARITY\n\nemployer is late in posting contributions to the accounts. Business development wanted jazzy new features that they could sell to potential customers, such as graphs of performance by fund category. Our legal person needed some new text and data on the statement to satisfy federal regulations.\n\nWhile the product owner balanced all the different needs and presented the ﬁnal statement layout, it was still important for our team to understand the purpose behind each new piece of information. We needed to talk directly to business ex- perts in the plan administration, business development, and legal areas, and to the product owner. A tester and a programmer met with each group to gather the different viewpoints. By doing this before starting on stories to gather and display data, we understood the requirements much more clearly and even made sugges- tions to produce the information more efﬁciently.\n\nMake sure you are as efﬁcient as possible in collecting this data. Sometimes it is important for the whole team to understand the need, and sometimes it is sufﬁcient for one or two of the team members to do the research.\n\nStory Size\n\nAs you discuss stories for the next iteration with the customer team mem- bers, ask questions to help them make sure each story delivers the value needed. This is a good time to identify new stories they might need to write. Even though the team sized the stories previously, you might ﬁnd a story is bigger than previously thought. You might even discover that a feature can be implemented more simply than planned, and the story can be smaller.\n\nSometimes assumptions are made when the story is sized and on further in- vestigation turn out to be false. Even simple stories deserve a closer look. It’s hard for any one person to remember all the details of an application.\n\nHere are some examples of stories that turned out to be signiﬁcantly bigger or smaller than originally thought.\n\n1. The story was to produce a ﬁle of account statements for all participants in a given company retirement plan, which was to be sent to a vendor who would print and mail the statements. It was originally sized with the assumption that all statements were exactly three pages long. Upon further investigation, we discovered that some participants had four-page statements, but the vendor required that all statements be the same length. Our business experts had to decide whether to have a feature to ﬂag any plans whose participants had four-page statements and deal with those manually, or change the statements to make them all four pages long. That’s a much bigger effort than the original\n\n375\n\n—Lisa",
      "content_length": 2668,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": "376\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\nstory. After we started developing the story, the customers revealed another requirement: If any participant’s address was missing or invalid, the statement should be mailed to the employer instead. It’s reasonable, but we didn’t know about it when we sized the story.\n\n2. Our customers wanted to start displaying the sales phone number in various locations in the UI. There is a different sales phone number for each partner’s site, and at the time there were about 25 different partner sites. This sounded like such a straightforward story that it wasn’t even given to the team to size. The development manager just assigned it a small point value, and it was just “added” to the iteration. He had assumed the phone number was stored in the database, when in fact it was hard-coded in the HTML of each partner’s “con- tact” page. Storing the correct number for each partner in the database, and changing the code to retrieve the value, made the story twice as big, and there wasn’t room for it in that iteration, so it did not get done.\n\n3. We sized a story for the user interface to allow administrators to submit a request for a batch job to rebalance participant accounts that met a certain condition. It included a conﬁrmation page displaying the number of partici- pants affected. Because the request was queued to run as an asynchronous batch job, the code to determine which participants were affected was in the batch job’s code. Refactoring the code to obtain the number of participants at request time was a big job. After we started working on the story, we asked the primary user of the feature whether he really needed that number upon submitting the request, and he decided it wasn’t necessary. The story became much smaller than originally thought. We always ask questions to ﬁnd out the true business value that the customers want and eliminate components that don’t have a good ROI.\n\nThese stories show that a few questions up front might save time during the iteration that could be spent ﬁguring out what to do with new discoveries. However, we recognize that not all discoveries can be found early. For exam- ple, on the ﬁrst story, a simple question about statement size may have pre- vented last-minute confusion about how to handle four-page statements, but the inaccurate address issue may not have been considered until it was being coded or tested.\n\nWe know there will always be discoveries along the way, but if we can catch the big “gotchas” ﬁrst, that will help the team work as effectively as possible.\n\nGeographically Dispersed Teams\n\nSome preparation for the next iteration may be useful for teams that are split across different locations. Teams that are distributed in multiple locations may do their iteration planning by conference call, online meeting, or telecon- ference. One practice, which a team of Lisa’s used, is to assign each team a\n\n—Lisa",
      "content_length": 2936,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 420,
      "content": "ADVANCE CLARITY\n\nsubset of the upcoming stories and have them write task cards in advance. During the planning meeting, everyone can review all the task cards and make changes as needed. The up-front work enhances communication, makes the stories and tasks visible to everyone, and speeds up the planning process.\n\nOf course, this assumes that the team is using an electronic story or task board. Lisa’s team uses Thoughtwork’s Mingle, but there are many other products out there that serve this purpose.\n\nCoping with Geographic Diversity\n\nWe talked to a team we know at a software company that has customers, de- velopers, and testers spread all over the globe. Not only are the customers far away from the technical team but they don’t have bandwidth to be avail- able to answer the development team’s questions. Instead, the team relies on functional analysts who understand both the business side of the applica- tion at a detailed level and the technical implementation of the software. These functional analysts act as liaisons between the business and technical teams.\n\nPatrick Fleisch and Apurva Chandra are consultants who were working with this company and served as functional analysts on a project to develop web- based entitlement software, because they are experts in this domain. They traveled between locations to facilitate communication between stakehold- ers and developers.\n\nThe functional analysts worked in advance of the iteration, sizing and getting stories ready to size, helping the technical team to understand the stories. They entered stories into an online tool and built on them by deﬁning test cases, edge conditions, and other information that helped the technical team understand the story. They documented high-level functionality on a wiki aimed at the business users.\n\nApurva and Patrick played a key role in making the decisions that the techni- cal team needed to get started with the new stories. Their deep business and technical understanding allowed them to provide the team with require- ments they needed to get coding, because the actual customers weren’t available to them. David Reed, a tester and automation engineer, told us how he relied on Apurva and Patrick for the information he needed to perform and automate tests. While agile principles say to collaborate closely with the customer, in some situations you have to be creative and ﬁnd another way to get clear business requirements.\n\nIf customers aren’t readily available to answer questions and make decisions, other domain experts who are accessible at all times should be empowered to guide the team by determining priorities and expressing desired system\n\n377",
      "content_length": 2670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "378\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\nbehavior with examples. Testers and business analysts are often called upon to do these activities.\n\nEXAMPLES You may notice that we talk about examples in just about every chapter of this book. Examples are an effective way to learn about and illustrate desired (and undesired) functionality; it’s worth using them throughout your devel- opment cycle. Our motto was coined by Brian Marick: “An example would be handy right about now.” (See Figure 16-2.) Start your discussions about fea- tures and stories with a realistic example. The idea has taken off, so that at a recent workshop for functional testing we were discussing ideas around call- ing it “Example-Driven Development.”\n\nWhen Lisa’s team members meet with their product owner to talk about the next iteration, they ask him for examples of desired behavior for each story. This keeps the discussion at a concrete level and is a fast way to learn how the new features should work. Have a whiteboard handy while you do this, and start drawing. If some team members are in a distant location, consider using tools that allow everyone to see whiteboard diagrams and participate in the discussion. Go through real examples with your customers or their proxies. As during release planning, consider different points of view: the business, end users, developers, and business partners. Unlike release planning, you are looking at far more detail because these are the stories you are planning for the next iteration.\n\nUsing examples, you can write high-level tests to ﬂesh out each story a bit more. You may not need to do this before the iteration starts, but for com-\n\nFigure 16-2 Brian Marick’s example sticker",
      "content_length": 1718,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "EXAMPLES\n\nplex stories, it can be a good idea to write at least one happy path and one negative path test case in advance. Let’s consider the story in Figure 16-3.\n\nStory PA-3\n\nAs a shopper on our site, I want to delete items\n\nout of my shopping cart, so I don’t purchase extra\n\nitems I don't want.\n\nFigure 16-3 Story for deleting items from shopping cart\n\nThe product owner sketches out the desired UI on the whiteboard. There’s a “delete” checkbox next to each item and an “update cart” button. The user can select one or more items and click the button to remove the items. The high-level tests might be:\n\n(cid:2) When the user clicks the delete checkbox next to the item and clicks the “update cart” button, the page refreshes showing the item is no longer in the cart.\n\n(cid:2) When the user clicks the delete checkboxes next to every item in the cart and clicks the “update cart” button, the page refreshes showing an empty cart. (This will generate questions—should the user be di- rected to another page? Should a “keep shopping” button display?) (cid:2) When the user clicks the “update cart” button without checking an item for delete, the page is refreshed and nothing is removed from the cart.\n\nAsk your customers to write down examples and high-level test cases before the iteration. This can help them think through the stories more and help de- ﬁne their conditions of satisfaction. It also helps them identify which features are critical, and which might be able to wait. It also helps to deﬁne when the story is done and manage expectations among the team.\n\n379",
      "content_length": 1578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 423,
      "content": "380\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\nFigure 16-4 Sample customer mock-up\n\nFigure 16-4 shows a sample mock-up, where the product owner marked changes on the existing page. Be careful about using an existing screenshot from an old system, because you will run the risk of having a new system look exactly like the old one even if that is not what you wanted.\n\nMock-ups are essential for stories involving the UI or a report. Ask your cus- tomers to draw up their ideas about how the page should look. Share these ideas with the team. One idea is to scan them in and upload them on the wiki so everyone has access. Use those as a starting point and do more paper pro- totypes, or draw them on the whiteboard. These can be photographed and uploaded for remote team members to see.\n\nTEST STRATEGIES As you learn about the stories for the next iteration, think about how to ap- proach testing them. Do they present any special automation challenge? Are any new tools needed?\n\nLisa’s Story\n\nRecently, our company needed to replace the voice response unit hardware and interactive voice interface software. A contractor was to provide the software to operate the voice application, but it needed to interact via stored procedures with the database.",
      "content_length": 1252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "Chapter 9, “Tool- kit for Business- Facing Tests that Support the Team,” Chapter 10, “Business-Facing Tests that Critique the Product,” and Chapter 11, “Critiquing the Product Using Technology- Facing Tests,” pro- vide examples of tools for different types of testing.\n\nRESOURCES\n\nThis was a big departure from any software we’d worked on before, so it was helpful to have an extra day to research how other teams have tested this type of application before the ﬁrst iteration planning that involved a story related to this project. During the iteration planning session, we were able to write tasks that were pertinent to the testing needed and give better estimates.\n\nWhen your team embarks on a new type of software, you may decide to do a development spike to see what you can learn about how to develop it. At the same time, try a test spike to help make sure you’ll know how to drive the de- velopment with tests and how to test the resulting software. If a major new epic or feature is coming up, write some cards to research it and hold brain- storming meetings an iteration or two in advance. That helps you know what stories and tasks to plan when you actually start coding. One idea is to have a “scout” team that looks at what technical solutions might work for upcom- ing stories or themes.\n\nPRIORITIZE DEFECTS In our ideal world, we want zero defects at the end of each iteration and deﬁ- nitely at the end of the release. However, we recognize that we don’t live in an ideal world. Sometimes we have legacy system defects to worry about, and sometimes ﬁxing a defect is just not high enough value for the business to ﬁx. What happens to these defects? We’ll talk about strategies in Chapter 18, “Coding and Testing,” but for now, let’s just consider that we have defects to deal with.\n\nBefore the next iteration is an ideal time to review outstanding issues with the customer and triage the value of ﬁxing versus leaving them in the system. Those that are deemed necessary to be ﬁxed should be scheduled into the next iteration.\n\nRESOURCES Another thing to double-check before the iteration is whether your team has all the resources you need to complete any high-risk stories. Do you need any experts who are shared with other projects? For example, you may need a se- curity expert if one of the stories poses a security risk or is for a security fea- ture. If load testing will be done, you may need to have a special tool, or have help from a load testing specialist from another team, or even a vendor who provides load testing services. This is your last chance to plan ahead.\n\n381\n\n—Lisa",
      "content_length": 2609,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "382\n\nCHAPTER 16\n\n(cid:2) HIT THE GROUND RUNNING\n\nSUMMARY Your team may or may not need to do any preparation in advance of an iter- ation. Because priorities change fast in agile development, you don’t want to waste time planning stories that may be postponed. However, if you’re about to implement some new technology, embark on a complex new theme, hope to save time in iteration planning, or your team is divided into different loca- tions, you might ﬁnd some up-front planning and research to be productive. As a tester, you can do the following:\n\n(cid:2) Help the customers achieve “advance clarity”—consensus on the desired behavior of each story—by asking questions and getting examples.\n\n(cid:2) Be proactive, learn about complex stories in advance of the iteration,\n\nand make sure they’re sized correctly.\n\n(cid:2) You don’t always need advance preparation to be able to hit the\n\nground running in the next iteration. Don’t do any preparation that doesn’t save time during the iteration or ensure more success at meet- ing customer requirements.\n\n(cid:2) Coordinate between different locations and facilitate communication.\n\nThere are many tools to help with this.\n\n(cid:2) Obtain examples to help illustrate each story. (cid:2) Develop test strategies in advance of the next iteration for new and\n\nunusual features.\n\n(cid:2) Triage and prioritize existing defects to determine whether any should\n\nbe scheduled for the next iteration.\n\n(cid:2) Determine whether any necessary testing resources not currently at\n\nhand need to be lined up for the next iteration.",
      "content_length": 1569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "Chapter 17\n\nITERATION KICKOFF\n\nReview with Customers\n\nLearning the Details\n\nReview with Programmers\n\nTest Cases as Documentation\n\nHigh-Level Tests and Examples\n\nIteration Planning\n\nConsidering All Viewpoints\n\nWriting Task Cards\n\nDeciding on Workload\n\nIteration Kickoff\n\nCollaborate with Customers\n\nTestable Stories\n\nAgile testers play an essential role during iteration planning, helping to plan testing and development tasks. As the iteration gets under way, testers actively collaborate with customers and developers, writing the high-level tests that help guide development, eliciting and illustrating examples, making sure stories are testable. Let’s take a closer look at the agile tester’s activities at the beginning of each iteration.\n\nITERATION PLANNING Most teams kick off their new iteration with a planning session. This might be preceded by a retrospective, or “lessons learned” session, to look back to see what worked well and what didn’t in the previous iteration. Although the retrospective’s action items or “start, stop, continue” suggestions will affect the iteration that’s about to start, we’ll talk about the retrospective as an end- of-iteration activity in Chapter 19, “Wrap Up the Iteration.”\n\nWhile planning the work for the iteration, the development team discusses one story at a time, writing and estimating all of the tasks needed to implement\n\n383",
      "content_length": 1379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "384\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nFigure 17-1 Iteration planning meeting\n\nthat story. If you’ve done some work ahead of time to prepare for the iteration, this planning session will likely go fairly quickly.\n\nTeams new to agile development often need a lot of time for their iteration planning sessions. Iteration planning often took a whole day when Lisa’s team ﬁrst started out. Now they are done in two or three hours, which in- cludes time for the retrospective. Lisa’s team uses a projector to display user acceptance test cases and conditions of satisfaction from their wiki so that everyone on the team can see them. They also project their online story board tool, where they write the task cards. Another traditional component of their planning meetings is a plate of treats that they take turns providing. Figure 17-1 shows an iteration planning meeting in progress.\n\nLearning the Details\n\nIdeally, the product owner and/or other customer team members participate in the iteration planning, answering questions and providing examples de- scribing requirements of each story. If nobody from the business side can at- tend, team members who work closely with the customers, such as analysts and testers, can serve as proxies. They explain details and make decisions on behalf of the customers, or take note of questions to get answered quickly. If",
      "content_length": 1366,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "ITERATION PLANNING\n\nyour team went over stories with the customers in advance of the iteration, you may think you don’t need them on hand during the iteration planning session. However, we suggest that they be available just in case you do have extra questions.\n\nAs we’ve emphasized throughout the book, use examples to help the team understand each story, and turn these examples into tests that drive coding. Address stories in priority order. If you haven’t previously gone over stories with the customers, the product owner or other person representing the cus- tomer team ﬁrst reads each story to be planned. They explain the purpose of the story, the value it will deliver, and give examples of how it will be used. This might involve passing around examples or writing on a whiteboard. UI and report stories may already have wire frames or mock-ups that the team can study.\n\nA practice that helps some teams is to write user acceptance tests for each story together, during the iteration planning. Along with the product owner and possibly other stakeholders, they write high-level tests that, when pass- ing, will show that the story is done. This could also be done shortly in ad- vance of iteration planning as part of the iteration “prep work.”\n\nStories should be sized so they’ll take no more than a few days to complete. When we get small stories to test on a regular basis, we do not have them all ﬁnished at once and stacked up at the end of the iteration waiting to be tested. If a story has made it past release planning and pre-iteration discus- sions and is still too large, this is the ﬁnal chance to break it up into smaller pieces. Even a small story can be complex. The team may go through an exer- cise to identify the thin slices or critical path through the functionality. Use examples to guide you, and ﬁnd the most basic user scenarios.\n\nAgile testers, along with other team members, are alert to “scope creep.” Don’t be afraid to raise a red ﬂag when a story seems to be growing in all directions. Lisa’s team makes a conscious effort to point out “bling,” or “nice to have” components, which aren’t central to the story’s functionality. Those can be put off until last, or postponed, in case the story takes longer than planned to ﬁnish.\n\nConsidering All Viewpoints\n\nAs a tester, you’ll try to put each story into the context of the larger system and assess the potential of unanticipated impacts on other areas. As you did in the release planning meeting, put yourself in the different mind-sets of\n\n385",
      "content_length": 2534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "386\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nuser, business stakeholder, programmer, technical writer, and everyone in- volved in creating and using the functionality. Now you’re working at a de- tailed level.\n\nIn the release planning chapter, we used this example story:\n\nAs a customer, I want to know how much my order will cost to ship based on the shipping speed I choose, so I can change if I want to.\n\nWe decided to take a “thin slice” and change this story to assume there is only one shipping speed. The other shipping speeds will be later stories. For this story, we need to calculate shipping cost based on item weight and destina- tion, and we decided to use BigExpressShipping’s API for the calculation. Our story is now as shown in Figure 17-2.\n\nStory PA-5\n\nAs a customer, I want to know how much my order\n\nwill cost to ship for standard 5-day delivery based\n\non weight and destination, so I can decide if\n\nthat’s the shipping option I want.\n\nFigure 17-2 Story shipping speed for 5-day delivery\n\nThe team starts discussing the story.\n\nTester: “Does this story apply for all items available on the site? Are any items too heavy or otherwise disqualiﬁed for 5-day delivery?\n\nProduct Owner: “5-day ground is available for all our items. It’s the overnight and 2-day that are restricted to less than 25 lbs.”\n\nTester: “What’s the goal here, from the business perspective? Making it easy to ﬁgure the cost to speed up the checkout? Are you hoping to",
      "content_length": 1457,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": "ITERATION PLANNING\n\nencourage them to check the other shipping methods—are those more proﬁtable?”\n\nProduct Owner: “Ease of use is our main goal, we want the checkout process to be quick, and we want the user to easily determine the total cost of the order so they won’t be afraid to complete the purchase.”\n\nProgrammer: “We could have the 5-day shipping cost display as a de- fault as soon as the user enters the shipping address. When we do the stories for the other shipping options, we can put buttons to pop up those costs quickly.”\n\nProduct Owner: “That’s what we want, get the costs up front. We’re go- ing to market our site as the most customer friendly.”\n\nTester: “Is there any way the user can screw up? What will they do on this page?”\n\nProduct Owner: “When we add the other shipping options, they can opt to change their shipping option. But for now, it’s really straightfor- ward. We already have validation to make sure their postal code matches the city they enter for the shipping address.”\n\nTester: “What if they realize they messed up their shipping address? Maybe they accidentally gave the billing address. How can they get back to change the shipping address?”\n\nProgrammer: “We’ll put buttons to edit billing and shipping addresses, so it will be very easy for the user to correct errors. We’ll show both ad- dresses on this page where the shipping cost displays. We can extend this later when we add the multiple shipping addresses option.”\n\nTester: “That would make the UI easy to use. I know when I shop on- line, it bugs me to not be able to see the shipping cost until the order conﬁrmation. If the shipping is ridiculously expensive and I don’t want to continue, I’ve already wasted time. We want to make sure users can’t get stuck in the checkout process, get frustrated, and just give up. So, the next page they’ll see is the order conﬁrmation page. Is there any chance the shipping cost could be different when the user gets to that page?”\n\nProgrammer: “No, the API that gives us the estimated cost should al- ways match the actual cost, as long as the same items are still in the shop- ping cart.”\n\nProduct Owner: “If they change quantities or delete any items, we need to make sure the shipping cost is immediately changed to reﬂect that.”\n\n387",
      "content_length": 2276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "388\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nAs you can see by the conversation, a lot of clariﬁcation came to light. Every- one on the team now has a common understanding of the story. It’s impor- tant to talk about all aspects of the story. Writing user acceptance tests as a group is a good way to make sure the development team understands the customer requirements. Let’s continue monitoring this conversation.\n\nTester: “Let’s just write up some quick tests to make sure we get it right.”\n\nCustomer: “OK, how’s this example?\n\nI can select two items with a 5-day shipping option and see my costs immediately.\n\nTester: “Great start, but we won’t know where to ship it to at that point. How about a more generic test like:\n\nVerify the 5-day shipping cost displays as the default as soon as the user enters a shipping address.\n\nCustomer: “That works for me.”\n\nConsidering All of the Facets\n\nPaul Rogers recounts a situation during an iteration planning meeting, where a performance issue came up for a story that appeared to be straightforward and quick.\n\nDuring our iteration meeting, one of the stories we were discussing was for adding some new images to part of a web application. This discussion ensued.\n\nProduct Owner: “I’d like to also get in the story for additional images.”\n\nDeveloper 1: “OK, who has ideas on how long it will take?”\n\nDeveloper 2: “It’s fairly quick, maybe half a day.”\n\nDeveloper 3: “But what about the database changes?”\n\nDeveloper 2: “I included those in the estimate.”\n\nDeveloper 1: “OK, let’s go with half a day.”\n\nMe: “Hang on. We looked at some performance issues last iteration. If we add all those images, we will be taking a performance hit.",
      "content_length": 1679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "Lisa’s Story\n\nITERATION PLANNING\n\nDeveloper 1: “OK, we should think about that some more. Maybe there are other ways of implementing it.\n\nDeveloper 2: “Why don’t we do a quick spike, add the mock images, and run another performance test?”\n\nIt was really good that this discussion before even starting on a story gave us some ideas of what problems we may encounter.\n\nAnyone who’s uncertain about either the impact of a story on the rest of the system or the difﬁculty of developing the functionality can, and should, raise an issue during iteration planning. It’s better to address uncertainty early on and then do more research or a spike to get more information.\n\nAsking questions based on different viewpoints will help to clarify the story and allow the team to do a better job.\n\nWriting Task Cards\n\nWhen your team has a good understanding of a story, you can start writing and estimating task cards. Because agile development drives coding with tests, we write both testing and development task cards at the same time.\n\nIf you have done any pre-planning, you may have some task cards already written out. If not, write them during the iteration planning meeting. It doesn’t matter who writes the task cards, but everyone on the team should review them and get a chance to give their input. We recognize that tasks may be added as we begin coding, but recognizing most of the tasks and estimat- ing them during the meeting gives the team a good sense of what is involved.\n\nWhen our team is ready to start writing task cards, programmers usually come up with the coding task cards. The testers write testing task cards at the same time.\n\nI usually start with a card to write high-level test cases. I ask the programmers whether the story can be tested behind the GUI, and write testing task cards accordingly. This usually means a test card to “Write FitNesse test cases” and a developer task card to “Write FitNesse ﬁxture,” unless the ﬁxtures already exist. Sometimes all of the behind-GUI tests can be covered more easily in unit tests, so it is always good to ask whether this is the case.\n\nWe put anything the team needs to remember during the iteration on a task card. “Show UI to Anne” or “Send test ﬁles to Joe” go up on the story board along with all of the other tasks.\n\n389",
      "content_length": 2287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "390\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nI estimate the testing task cards as I go, and ask the team for feedback on the cards and the estimates. Sometimes we divide into groups, and each group takes some stories and writes task cards for them. We always review all cards together, along with the estimated time. If development time is relatively low compared to testing time, or vice versa, that provokes a discussion. We reach a consensus as to whether we think all aspects of the story have been covered with task cards. If there are still some unknowns, we simply postpone writing the task cards until we have the information.\n\nThe testing and development cards all go on the story board in the “to do” col- umn. Anyone on the team can sign up for any card. Some testing task cards move to the “work in progress” or “done” column before coding cards start to move, so that programmers have some tests to guide their coding. As coding task cards are moved to the “done” column, the cards for testing the “done” functionality are moved into “work in progress.”\n\nJanet uses an approach similar to this, but the programmer’s coding card stays in the “To Test” column until the testing task has been completed. Both cards move at the same time to the “Done” column.\n\nThree test cards for Story PA-5 (Figure 17-2), displaying the shipping cost for 5- day delivery based on weight and destination, that Lisa’s team might write are:\n\n(cid:2) Write FitNesse tests for calculating 5-day ship cost based on weight\n\nand destination.\n\n(cid:2) Write WebTest tests for displaying the 5-day ship cost. (cid:2) Manually test displaying the 5-day delivery ship cost.\n\nSome teams prefer to write testing tasks directly on the development task cards. It’s a simple solution, because the task is obviously not “done” until the testing is ﬁnished. You’re trying to avoid a “mini-waterfall” approach where testing is done last, and the programmer feels she is done because she “sent the story to QA.” See what approach works best for your team.\n\nIf the story heavily involves outside parties or shared resources, write task cards to make sure those tasks aren’t forgotten, and make the estimates gen- erous enough to allow for dependencies and events beyond the team’s con- trol. Our hypothetical team working on the shipping cost story has to work with the shipper’s cost calculation API.\n\n—Lisa",
      "content_length": 2381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 434,
      "content": "ITERATION PLANNING\n\nTester: “Does anyone know who we work with at BigExpressShipping to get specs on their API? What do we pass to them, just the weight and postal code? Do we already have access for testing this?”\n\nScrum Master: “Joe at BigExpressShipping is our contact, and he’s al- ready sent this document specifying input and output format. They still need to authorize access from our test system, but that should be done in a couple of days.”\n\nTester: “Oh good, we need that information to write test cases. We’ll write a test card just to verify that we can access their API and get a ship- ping cost back. But how do we know the cost is really correct?”\n\nScrum Master: “Joe has provided us with some test cases for weight and postal code and expected cost, so we can send those inputs and check for the correct output. We also have this spreadsheet showing rates for some different postal codes.”\n\nTester: “We should allow lots of time for just making sure we’re access- ing their API correctly. I’m going to put a high estimate on this card to verify using the API for testing. Maybe the developer card for the inter- face to the API should have a pretty conservative estimate, too.”\n\nWhen writing programmer task cards, make sure that coding task estimates include time for writing unit tests and for all necessary testing by program- mers. A card for “end-to-end” testing helps make sure that programmers working on different, independent tasks verify that all of the pieces work to- gether. Testers should help make sure all necessary cards are written and that they have reasonable estimates. You don’t want second-guess estimates, but if the testing estimates are twice as high as the coding estimates, it might be worth talking about.\n\nSome teams keep testing tasks to a day’s work or less and don’t bother to write estimated hours on the card. If a task card is still around after a day’s work, the team talks about why that happened and writes new cards to go forward. This might cut down on overhead and record-keeping, but if you are entering tasks into your electronic system, it may not. Do what makes sense for your team.\n\nEstimating time for bug ﬁxing is always tricky as well. If existing defects are pulled in as stories, it is pretty simple. But what about the bugs that are found as part of the iteration?\n\n391",
      "content_length": 2339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "392\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nJanet’s Story\n\nWith new agile teams, I found that they always seem to end up with time spent on bugs that wasn’t allotted as part of their estimates for the stories. Over time, pro- grammers learn how much time they typically spend ﬁxing bugs from a story, and can just add half a day or a couple hours to their tasks for that purpose. Retesting bug ﬁxes adds time to tester’s estimates as well.\n\nUntil the team members get a handle on this, it may be appropriate to track the time spent on ﬁxing and testing bugs separately. My current team adds a story in XPlanner with tasks for ﬁxing and testing those bugs that didn’t get caught immedi- ately. We are tracking the time so we can better estimate down the road.\n\nHowever your team chooses to estimate time spent for ﬁxing defects during the iteration, whether it is included in the story estimate or tracked sepa- rately, make sure it is done consistently.\n\nAnother item to consider when estimating testing tasks is test data. The be- ginning of an iteration is almost too late to think about the data you need to test with. As we mentioned in Chapter 15, “Tester Activities in Release or Theme Planning,” think about test data during release planning, and ask the customers to help identify and obtain it. Certainly think about it as you prep for the next iteration. When the iteration starts, whatever test data is missing must be created or obtained, so don’t forget to allow for this in estimates.\n\nLisa’s Story\n\nWe worked on a theme related to quarterly account statements for participants in retirement plans. We were modifying a monthly job that takes a “snapshot” of each participant’s account on the speciﬁed date. The snapshot relies on a huge amount of data in the production database, including thousands of daily transac- tions. We planned ahead.\n\nFor the ﬁrst iteration, we did a few stories in the theme knowing we could only test a few cases using some individual retirement plans for which we had data in the test database. We also knew we needed a larger-scale test, with all of the re- tirement plans in the database and at least an entire month’s worth of data. We wrote a task card to copy enough production data to produce one monthly “snapshot” and made sure the data was scrubbed to protect privacy.\n\nThen we planned the full-blown test in the next iteration. This data enabled the testers to ﬁnd problems that were undetectable earlier when only partial data was available. It was a nice balance of “just enough” data to do most of the cod- ing and the full amount available in time to verify the complete functionality. Be- cause the team planned ahead, the bugs were ﬁxed in time for the critical release.\n\n—Janet\n\n—Lisa",
      "content_length": 2745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 436,
      "content": "TESTABLE STORIES\n\nDeciding on Workload\n\nWe, as the technical team, control our own workload. As we write tasks for each story and post them on our (real or virtual) story board, we add up the estimated hours or visually check the number of cards. How much work can we take on? In XP, we can’t exceed the number of story points we completed in the last iteration. In Scrum, we commit to a set of stories based on the ac- tual time we think we need to complete them.\n\nLisa’s current team has several years of experience in their agile process and ﬁnds they sometimes waste time writing task cards for stories they may not have time to do during the iteration. They start with enough stories to keep everyone busy. As people start to free up, they pull in more stories and plan the related tasks. They might have some stories ready “on deck” to bring in as soon as they ﬁnish the initial ones. This sounds easy, but it is difﬁcult to do until you’ve learned enough to be more conﬁdent about story sizes and team velocity, and know what your team can and cannot do in a given amount of time and in speciﬁc circumstances.\n\nYour job as tester is to make sure enough time is allocated to testing, and to remind the team that testing and quality are the responsibility of the whole team. When the team decides how many stories they can deliver in the itera- tion, the question isn’t “How much coding can we ﬁnish?” but “How much coding and testing can we complete?” There will be times when a story is easy to code but the testing will be very time consuming. As a tester, it is impor- tant that you only accept as many stories into the iteration as can be tested.\n\nIf you have to commit, commit conservatively. It’s always better to bring in another story than to have to drop one. If you have high-risk stories that are hard to estimate, or some tasks are unknown or need more research, write task cards for an extra story or two and have them ready on the sidelines to bring in mid-iteration.\n\nAs a team, we’re always going to do our best. We need to remember that no story is done until it’s tested, so plan accordingly.\n\nTESTABLE STORIES When you are looking at stories, and the programmers start to think about implementation, always think how you can test them. An example goes a long way toward “testing the testability.” What impact will it have on my test- ing? Part III, “The Agile Testing Quadrants,” gives a lot of examples of how to\n\n393",
      "content_length": 2443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 437,
      "content": "394\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\ndesign the application to enable effective testing. This is your last opportu- nity to think about testability of a story before coding begins.\n\nJanet’s Story\n\nOne team I worked with told me about issues they had in the previous release. The team was rewriting the ﬁrst step of a multistep process. What they didn’t an- ticipate was that when the development on the new step started, the rest of the process broke. No testing could be done on any other changes in that iteration until the whole ﬁrst step was ﬁnished.\n\nTestability had not been considered when planning the story. In the next release, when they decided to rewrite the second step, they learned from their previous mistake. The programmers created an extra button on the page that allowed the testers to either call the new page (in ﬂux) or the old page to allow them test other stories.\n\nRemember to ask, “How can we test this?” if it is not obvious to you.\n\nDuring iteration planning, think about what kind of variations you will need to test. That may drive other questions.\n\nJanet’s Story\n\nDuring one iteration planning meeting that I was in, the programmers started talk- ing about implementation and drawing pictures on the whiteboard to show what they were thinking.\n\nI thought about it for a bit and asked the question, “Can it be done more simply? The permutations and combinations for testing your proposed implementation will make testing horrendous.”\n\nThe programmers thought about it for a couple of minutes and suggested an alter- native that not only met the customer’s needs, but was simpler and easier to test. It was a win-win combination for everyone.\n\nWhen testability is an issue, make it the team’s problem to solve. Teams that start their planning by writing test task cards probably have an advantage here, because as they think about their testing tasks, they’ll ask how the story can be tested. Can any functionality be tested behind the GUI? Is it possible to do the business-facing tests at the unit level? Every agile team should be thinking test-ﬁrst. As your team writes developer task cards for a story, think about how to test the story and how to automate testing for it. If the pro- grammers aren’t yet in the habit of coding TDD or automating unit tests, try\n\n—Janet\n\n—Janet",
      "content_length": 2320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "Lisa’s Story\n\nLisa’s Story\n\nTESTABLE STORIES\n\nwriting a “XUnit” task card for each story. Write programming task cards for any test automation ﬁxtures that will be needed. Think about application changes that could help with testing, such as runtime properties and APIs.\n\nThe application that I work on has many time- and date-dependent activities. The programmers added a runtime server property to the web application to set the server date. I can specify a date and time override, and when the server starts up, it behaves accordingly. This allows kicking off monthly or quarterly pro- cesses with a simple override. This property has helped in testing a wide variety of stories.\n\nMarkus Gärtner [2008] told us his team has a similar property, a “DATE_OFFSET” counted in “days to advance.” However, this was only used by the Java compo- nents of the application where the business logic lives. The back-end systems in C and C++ don’t use the date offset, which caused a problem.\n\nIf you have similar issues because other teams are developing parts of the sys- tem, write a task card to discuss the problem with the other team and come up with a coordinated solution. If working with the other team isn’t an op- tion, budget time to brainstorm another solution. At the very least, be mind- ful of the limitations, and adjust testing estimates accordingly and manage the associated risk.\n\nWe started a project to replace our company’s interactive voice response (IVR) system, which allows retirement plan participants to obtain account information and manage accounts by phone. We contracted with another company to write the system in Java, with the intention that our team would maintain it after a cer- tain time period.\n\nWe spent some time brainstorming what testing would be needed and how to do it. Presumably, the contractor would test things like the text-to-speech functional- ity, but we had to supply stored procedures to retrieve appropriate data from the database.\n\nOur ﬁrst step was to negotiate with the contractor to deliver small chunks of fea- tures on an iteration basis, so they could be tested as the project progressed and the work would be spread out evenly over the life of the contract. We decided to test the stored procedures using FitNesse ﬁxtures, and explored the options. We settled on PL/SQL to access the stored procedures. A programmer was tasked with getting up to speed on PL/SQL to tackle the test automation.\n\nThe team aimed for a step-by-step approach. By allocating plenty of time for tasks at the start, we allowed for the steep learning curves involved.\n\n395\n\n—Lisa",
      "content_length": 2608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 439,
      "content": "396\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nInterestingly, the contractor delivered an initial build for the ﬁrst iteration but was not able to deliver the increments of code for the next few iterations. We ended up canceling the contract and postponing the project until we could ﬁnd a better solution. By forcing the contractor to work in increments, we discovered right away that it couldn’t deliver. What if we had let them take six months to write the whole application? It probably wouldn’t have ended well. We put what we learned to good use in researching a better approach.\n\nWhen you’re embarking on something new to the team, such as a new tem- plating framework or reporting library, remember to include it as a risk in your test plan. Hopefully, your team considered the testability before choos- ing a new framework or tool, and selected one that enhanced your ability to test. Be generous with your testing task estimates with everything new, in- cluding new domains, because there are lots of unknowns. Sometimes new domain knowledge or new technology means a steep learning curve.\n\nCOLLABORATE WITH CUSTOMERS Working closely with customers, or customer proxies such as functional ana- lysts, is one of our most important activities as agile testers. As you kick off the iteration, your customer collaboration will also kick into high gear. This is the time to do all those good activities described in Chapter 8, “Business- Facing Tests that Support the Team.” Ask the customers for examples, ask open-ended questions about each story’s functionality and behavior, have discussions around the whiteboard, and then turn those examples into tests to drive coding.\n\nEven if your product owner and/or other customers explained the stories be- fore and during iteration planning, it’s sometimes helpful to go over them brieﬂy one more time as the iteration starts. Not everyone may have heard it before, and the customer may have more information.\n\nLisa’s Story\n\nWe start writing high-level acceptance tests the ﬁrst day of the iteration. Because we go over all stories with the product owner the day before the iteration and write user acceptance tests as a team for the more complex stories, we have a pretty good idea of what’s needed. However, the act of writing more test cases of- ten brings up new questions. We go over the high-level tests and any questions we have with the product owner, who has also been thinking more about the stories.\n\n—Lisa",
      "content_length": 2467,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "HIGH-LEVEL TESTS AND EXAMPLES\n\nOne example of this was a story that involved a ﬁle of monetary distributions to plan participants who withdraw money from their retirement accounts. This ﬁle is sent to a partner who uses the information to cut checks to the participant. The amounts in some of the records were not reconciling correctly in the partner’s sys- tem, and the partner asked for a new column with an amount to allow them to do a reconciliation.\n\nAfter the iteration planning meeting, our product owner became concerned that the new column wasn’t the right solution and brought up his misgivings in the story review meeting. He and a tester studied the problem further and found that instead of adding a new amount, a calculation needed to be changed. This was actually a bigger story, but it addressed a core issue with the distributions. The team discussed the larger story and wrote new task cards. It was worth taking a little time to discuss the story further, because the initial understanding turned out to be wrong.\n\nGood communication usually takes work. If you’re not taking enough op- portunities to ask questions and review test cases, go ahead and schedule reg- ular meetings to do so. If there’s not much to discuss, the meetings will go quickly. Time in a meeting for an insightful discussion can save coding and testing time later, because you’re more certain of the requirements.\n\nHIGH-LEVEL TESTS AND EXAMPLES We want “big picture” tests to help the programmers get started in the right direction on a story. As usual, we recommend starting with examples and turning them into tests. You’ll have to experiment to see how much detail is appropriate at the acceptance test level before coding starts. Lisa’s team has found that high-level tests drawn from examples are what they need to kick off a story.\n\nHigh-level tests should convey the main purpose behind the story. They may include examples of both desired and undesired behavior. For our earlier Story PA-5 (Figure 17-2) that asks to show the shipping cost for 5-day deliv- ery based on the order’s weight and destination, our high-level tests might include:\n\n(cid:2) Verify that the 5-day shipping cost displays as the default as soon as\n\nthe user enters a shipping address.\n\n(cid:2) Verify that the estimated shipping cost matches the shipping cost on\n\nthe ﬁnal invoice.\n\n397\n\n—Lisa",
      "content_length": 2367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 441,
      "content": "398\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\n(cid:2) Verify that the user can click a button to change the shipping address,\n\nand when this is done, the updated shipping cost displays.\n\n(cid:2) Verify that if the user deletes items from the cart or adds items to the\n\ncart, the updated shipping option is displayed.\n\nSee the bibliogra- phy for links to more information on graphical tests and model-driven development.\n\nDon’t conﬁne yourself to words on a wiki page when you write high-level tests. For example, a test matrix such as the one shown in Figure 15-7 might work better. Some people express tests graphically, using workﬂow drawings and pictures. Brian Marick [2007] has a technique to draw graphical tests that can be turned into Ruby test scripts. Model-driven development provides an- other way to express high-level scope for a story. Use cases are another possi- ble avenue for expressing desired behavior at the “big picture” level.\n\nA Picture Is Worth a Thousand Words\n\nThe saying “A picture says a thousand words” can also be applied to test cases and test validations.\n\nPaul Rogers [2008] has been experimenting with some cool ideas around this and explains his team’s approach to its problem in the following sidebar. Fig- ure 17-3 shows the UI model he describes.\n\nThe application I work on is very graphical in its nature. It allows a user to modify a web page by adding “photo enhancements” such as glasses, hats, or speech bubbles to images, or by highlighting the text in the web page with a highlighter pen effect.\n\nThere is a complex set of business rules as to what additions can be ap- plied to images, how and where they are afﬁxed, and how they can be rotated. To explain the tests for these rules, it was much simpler to draw a sketch of a typical web page with the different types of additions and add small notes to each picture.\n\nText highlighting also posed many challenges. Most problematic were the areas where text highlighting covered only part of an HTML tag. To de- scribe what should be expected in many different situations, we created different web pages and printed them out.\n\nUsing real pen highlighters, we highlighted the areas we expected to show as highlighted after starting and ending in certain areas. This way, we had an easy-to-read regression test.\n\nLow-tech tools can take the mystery out of complex application design. Find ways to express business rules as simply as possible, and share those with the entire team.",
      "content_length": 2475,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "See the sample mock-up of UI changes in Chap- ter 16, “Hit the Ground Running.”\n\nSee Chapter 9, “Toolkit for Business-Facing Tests that Support the Team,” for some ideas on tools to gather and communicate requirements.\n\nHIGH-LEVEL TESTS AND EXAMPLES\n\nFigure 17-3 Sample of UI modeling technique\n\nMock-ups can convey requirements for a UI or report quickly and clearly. If an existing report needs modifying, take a screenshot of the report and use highlighters, pen, pencil, or whatever tools are handy. If you want to capture it electronically, try the Windows Paint program or other graphical tool to draw the changes and post it on the wiki page that describes the report’s requirements.\n\nDistributed teams need high-level tests available electronically, while co- located teams might work well from drawings on a whiteboard, or even from having the customer sit with them and tell them the requirements as they code.\n\nWhat’s important as you begin the iteration is that you quickly learn the ba- sic requirements for each story and express them in context in a way that works for the whole team. Most agile teams we’ve talked to say their biggest\n\n399",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 443,
      "content": "400\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nproblem is to understand each story well enough to deliver exactly what the customer wanted. They might produce code that’s technically bug-free but doesn’t quite match the customer’s desired functionality. Or they may end up doing a lot of rework on one story during the iteration as the customer clari- ﬁes requirements, and run out of time to complete another story as a result.\n\nSee Chapter 8, “Business-Facing Tests that Support the Team,” for more about what makes up a requirement.\n\nPut time and effort into experimenting with different ways to capture and express the high-level tests in a way that ﬁts your domain and environment. Janet likes to say that a requirement is a combination of the story + conver- sation + a user scenario or supporting picture if needed + a coaching test or example.\n\nReviewing with Customers\n\nEarlier in this chapter we talked about the importance of constant customer collaboration. Reviewing high-level tests with customers is a good opportu- nity for enforced collaboration and enhanced communication, especially for a new agile team. After your team is in the habit of continually talking about stories, requirements, and test cases, you might not need to sit down and go over every test case.\n\nIf your team is contracting to develop software, requirements and test cases might be formal deliverables that you have to present. Even if they aren’t, it’s a good idea to provide the test cases in a format that the customers can easily read on their own and understand.\n\nReviewing with Programmers\n\nYou can have all of the diagrams and wiki pages in the world, but if nobody looks at them, they won’t help. Direct communication is always best. Sit down with the programmers and go over the high-level tests and requirements. Go over whiteboard diagrams or paper prototypes together. Figure 17-4 shows a tester and a programmer discussing a diagram of thin slices or threads through a user workﬂow. If you’re working with a team member in another location, ﬁnd a way to schedule a phone conversation. If team members have trouble understanding the high-level tests and requirements, you’ll know to try a dif- ferent approach next time.\n\nProgrammers with good domain knowledge may understand a story right away and be able to start coding even before high-level tests are written. Even so, it’s always a good idea to review the stories from the customer and tester perspective with the programmers. Their understanding of the story might",
      "content_length": 2520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "Chapter 2, “Ten Principles for Ag- ile Testers,” intro- duces the “Power of Three” rule.\n\nHIGH-LEVEL TESTS AND EXAMPLES\n\nFigure 17-4 A whiteboard discussion\n\nbe different than yours, and it’s important to look at mismatches. Remember the “Power of Three” rule and grab a customer if there are two opinions you can’t reconcile. The test cases also help put the story in context with the rest of the application. Programmers can use the tests to help them to code the story correctly. This is the main reason you want to get this done as close to the start of the iteration as you can—before programmers start to code.\n\nDon’t forget to ask the programmers what they think you might have missed. What are the high-risk areas of the code? Where do they think the testing should be focused? Getting more technical perspective will help with design- ing detailed test cases. If you’ve created a test matrix, you may want to review the impacted areas again as well.\n\nOne beneﬁcial side effect of reviewing the tests with the programmers is the cross-learning that happens. You as a tester are exposed to what they are thinking, and they learn some techniques for testing that they would not have otherwise encountered. As programmers, they may get a better under- standing of what high-level tests they hadn’t considered.\n\n401",
      "content_length": 1319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "402\n\nCHAPTER 17\n\n(cid:2)\n\nITERATION KICKOFF\n\nTest Cases as Documentation\n\nHigh-level test cases, along with the executable tests you’ll write during the iteration, will form the core of your application’s documentation. Require- ments will change during and after this iteration, so make sure your execut- able test cases are easy to maintain. People unfamiliar with agile development often have the misconception that there’s no documentation. In fact, agile projects produce usable documentation that contains executable tests and thus is always up to date.\n\nThe great advantage of having executable tests as part of your requirements document is that it’s hard to argue with their results.\n\nLisa’s Story\n\nFrequently, product owners, plan administrators, or business development man- agers will come and ask me a question such as, “What’s the system supposed to do if someone submits a loan payment for zero dollars?” or “Why didn’t everyone in this plan get a 3% nonelective contribution?”\n\nShowing them a FitNesse test that replicates the scenario is much more powerful than just showing them narrative requirements. Maybe the system wasn’t designed the way it should have been, but the test illustrates how it actually works, because we can clearly see the results of the inputs and operations. This has saved a lot of arguments on the level of “I thought it worked this way.”\n\nIf they decide the functionality, as implemented, is incorrect, we can change the expected outputs of the test and write a story to implement code to make the test pass again with the new expectations. You can’t do that with a requirements document.\n\nOrganizing the test cases and tests isn’t always straightforward. Many teams document tests and requirements on a wiki. The downside to a wiki’s ﬂexi- bility is that you can end up with a jumble of hierarchies. You might have trouble ﬁnding the particular requirement or example you need.\n\nChapter 14, “An Agile Test Automa- tion Strategy,” has more on test management.\n\nLisa’s team periodically revisits its wiki documentation and FitNesse tests, and refactors the way they’re organized. If you’re having trouble organizing your re- quirements and test cases, budget some time to research new tools that might help. Hiring a skilled technical writer is a good way to get your valuable test cases and examples into a usable repository of easy-to-ﬁnd information.\n\n—Lisa",
      "content_length": 2403,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 446,
      "content": "SUMMARY\n\nSUMMARY The iteration planning session sets the tone for the whole iteration. In this chapter, we looked at what agile testers do to help kick off the iteration to a good start.\n\n(cid:2) During iteration planning, testers help the team learn about the sto-\n\nries by asking questions and considering all viewpoints.\n\n(cid:2) Task cards need to be written along with development task cards and\n\nestimated realistically.\n\n(cid:2) Another way of tackling testing tasks is to write them directly on the\n\ndeveloper task cards.\n\n(cid:2) Teams should commit to the work for which they can complete all of\n\nthe testing tasks, because no story is done until it’s fully tested.\n\n(cid:2) The start of an iteration is the last chance to ensure that the stories are\n\ntestable and that adequate test data is provided.\n\n(cid:2) Testers collaborate with customers to explore stories in detail and write high-level test cases to let programmers kick off coding.\n\n(cid:2) Testers review high-level tests and requirements with programmers to\n\nmake sure they are communicating well.\n\n(cid:2) Tests form the core of the application’s documentation.\n\n403",
      "content_length": 1140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 447,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "Chapter 18\n\nCODING AND TESTING\n\nMeasuring Progress\n\nDefect Metrics\n\nIteration Metrics\n\nStart Simple\n\nResources\n\nAdd Complexity\n\nAssess Risk\n\nKeep the Build “Green”\n\nDriving Development\n\nTesting and Coding Progress Together\n\nKeep the Build Quick\n\nBuilding a Regression Suite\n\nRegression Tests\n\nIdentify Variations\n\nPower of Three\n\nChecking the “Big Picture”\n\nFocus on One Story\n\nTesters Facilitate Communication\n\nDistributed Teams\n\nFacilitate Communication\n\nCoding and Testing\n\nTests that Critique the Product\n\nWhich Bugs to Log?\n\nCollaborate with Programmers\n\nPair Testing\n\nShow Me\n\nWhen to Fix Bugs?\n\nWhat Media Do We Use to Log Bugs?\n\nAlternatives & Suggestions\n\nIt’s All about Choices\n\nTalk to Customers\n\nShow Customers\n\nUnderstand Business Needs\n\nStart Simple\n\nDefect vs. Feature\n\nTechnical Debt\n\nDealing with Bugs\n\nCompleting Testing Tasks\n\nAddress the Testing Crunch\n\nAnyone Can Do Testing Tasks\n\nZero Bug Tolerance\n\nOur agile tester has helped plan the release, size stories appropriately, and make sure they’re testable. She, along with colleagues on the customer and develop- ment team, has turned examples of desired behavior for each story into high- level user acceptance tests. She and her team have lined up the resources and infrastructure needed to deliver business value. Now, team members have picked up task cards and started writing code. What do testers do next, especially before any stories are ready to test?\n\n405",
      "content_length": 1437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "406\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nDRIVING DEVELOPMENT The beginning of coding is a good time to start writing detailed tests. The high-level tests written before the iteration, or in the ﬁrst couple days of it, provide enough information for the programmers to start their own test- driven development. So now we have a bit of breathing room, but if we don’t move quickly, coding could get way ahead of testing and go off in the wrong direction.\n\nNow’s the time to start writing executable tests that illustrate the details about a story in order to keep development moving forward smoothly and help testing keep pace with coding. Like the high-level tests, we base detailed tests on examples provided by the customers.\n\nAt this point, we’re mainly writing tests that will be automated, but we’re also thinking ahead to the important exploratory testing we need to do as coding is completed.\n\nStart Simple\n\nAs testers, we’re easily distracted by interesting code smells and edge cases. However, if we’re using tests to guide coding, we have to start with the basics. Write the simplest happy path test you can in order to show that the core functionality works.\n\nChapter 14, “An Agile Automation Strategy,” gives pointers for select- ing the right tools.\n\nWhy executable tests? We’re working on an extremely tight schedule, and neither the programmers nor the testers have time to stop and run manual tests over and over. They do have time to click a button and run an auto- mated test. That test needs to fail in a way that makes the cause as obvious as possible. Ideally, we would give these tests to the programmers so that they could execute them as they code. That is one reason why picking the right automation framework is so important.\n\nFor some stories, automating the tests might take a long time. By keeping the ﬁrst test simple, you keep the focus on designing the automation solution. When the simple test works, it’s worth putting time into more complex test cases.\n\nWe stress the importance of automation, but Janet has worked with teams that have successfully used manual tests in the form of checklists or spread- sheets to give the programmers the information they need to start. However, to be successful in the long run, these tests do need to be automated.",
      "content_length": 2288,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 450,
      "content": "DRIVING DEVELOPMENT\n\nAdd Complexity\n\nAs soon as the happy path test works, start adding more test cases. Add boundary and edge conditions. The tests may show that the programmers misunderstood a requirement, or they may show that a tester did, or maybe the requirement’s true meaning eluded everyone. The important thing is that everyone talks about it and gets on track.\n\nAs testers think of new scenarios to validate with executable tests, they also think about potential scenarios for manual exploratory testing. Make a note of these for later pursuit.\n\nRemember the purpose of these tests. They should provide examples that tell the programmers what code to write. As the code evolves, your tests can chal- lenge it more, but resist the temptation to immediately follow smells into edge cases. Get the basics working ﬁrst. If you think of more cases based on some risk analysis, you can always add extra tests later.\n\nAssess Risk\n\nTesters have used risk analysis to help prioritize testing for a long time, and consideration for risk is already built into agile development. High-risk sto- ries may get higher size estimates, and teams consider risk as they prioritize stories during release and iteration planning.\n\nSome quick risk analysis can help you decide what testing to do ﬁrst and where to focus your efforts. We never have time to test everything, and we can use risk analysis to ﬁgure out how much testing is just enough.\n\nIf you have a really complex story, you may want to start by listing all of the potential risks related to the story. These aren’t limited to functionality. Con- sider security, performance, usability, and other “ilities.” Next, for each item, rate the impact on the business if it were to occur, using a scale of 1 to 5 (or whatever scale works for you): 1 being a low impact, 5 being a critical nega- tive impact.\n\nNow, consider the likelihood of each item occurring, using the same scale: 1 for not at all likely to happen, and 5 for items that probably will come up. Multiply the two ratings together to get the total risk rating for each item. This makes it easy to pick out the areas where your team should focus its test- ing efforts ﬁrst. Low-risk items can be left for last, or, because their impact is low or they’re highly unlikely to occur, may not be addressed at all.\n\n407",
      "content_length": 2324,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "408\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nYour domain makes a huge difference here. If you’re testing software that runs in heart pacemakers, you probably need to cover all risks with your testing no matter how low or unlikely they are. If you’re testing an internal company web application to be used by a few trained subject matter experts, you may be able to skip over scenarios that are unlikely or have an obvious workaround.\n\nConsider the story in Figure 18-1.\n\nStory PA-5\n\nAs a customer, I want to know how much my order\n\nwill cost to ship for based on the shipping speed\n\nI select so that I can choose a different shipping\n\nspeed if I want to.\n\nFigure 18-1 Story on shipping speeds\n\nFigure 18-2 shows a possible risk assessment for this shipping cost story.\n\n# 1 2 3\n\n4\n\n5\n\n6 7 8\n\nItem Incorrect cost displayed User can’t choose different shipping option Item isn’t eligible for selected shipping option, but selection allowed Estimated cost doesn’t match actual cost at checkout Invalid postal code entered and not caught by validation User can’t understand shipping option rules User can’t change shipping address User changes shipping address, but cost doesn’t change accordingly\n\nImpact 4 5 3\n\n3\n\n4\n\n2 5 5\n\nProbability 2 1 2\n\n4\n\n1\n\n3 2 4\n\nFigure 18-2 Sample risk assessment\n\nItem 8 is the highest-risk item, so we’d want to be sure to test changing ship- ping addresses and verify the updated costs. We might want to automate an\n\nRisk 8 5 6\n\n12\n\n4\n\n6 10 20",
      "content_length": 1471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "DRIVING DEVELOPMENT\n\nend-to-end test with this scenario. We’re not too worried about item 5; maybe we have already tested our postal code validation and feel good about it, so we don’t need to test it more. You may even have a very low-risk item that you chose not to test.\n\nHistory is usually a good teacher. Take note of past issues and make sure they don’t happen again.\n\nCoding and Testing Progress Together\n\nAt this point in the iteration, coding and testing continue hand in hand. Testers, programmers, database experts, and other team members collabo- rate to develop the stories, following the guidelines provided by examples and tests. Different team members may contribute their particular expertise, but all of them feel responsible for making sure each story is ﬁnished. All of them learn about the story and learn from each other as work progresses.\n\nLet’s look at how a team might work on the shipping cost story in Figure 18-1. Patty Programmer picks up a task card to code the estimated shipping cost calculations. She already understands the story pretty well from earlier dis- cussions, but she may look at the wiki pages or back of the story card where the testers wrote down some narrative describing the purpose of the story, some examples of how it should work, and some high-level tests to make sure she has a good idea of where to start. Tammy Tester sees that coding work has begun and starts to write behind-the-GUI test cases for the cost calculations.\n\nThe team had agreed during planning to start by calculating the 5-day ship- ping cost based on the shipping address and item weight. Items can only be shipped within continental North America, but that validation will be done in the presentation layer, so the cost calculation tests can assume only valid destinations are considered for input. They’re using a cost calculation API provided by the shipping partner, and Tammy asks Patty where to ﬁnd the al- gorithms so she can ﬁgure the cost herself in order to write the tests. Tammy writes the simplest test case she can think of in their behind-the-GUI test tool. We show it as a simple table in Figure 18-3.\n\nWeight 5 lbs\n\nDestination Postal Code 80104\n\nCost 7.25\n\nFigure 18-3 Simple happy path test\n\n409",
      "content_length": 2239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "410\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nPatty hasn’t ﬁnished the code that would make this test pass yet, so Tammy starts working on another testing task for the story, setting up the test envi- ronment to work with the shipping partner’s test system.\n\nIdentify Variations\n\nBecause this story and the test are so straightforward, Patty and Tammy don’t discuss the test design and tweak it as they might on more complex stories. They also haven’t needed to ask the product owner more questions yet. Patty calls Tammy over to show her that the simple test is now working. Tammy writes up more test cases, trying different weights and destinations within the United States. Those all work ﬁne. She tries a Canadian postal code, and the test gets an exception. She shows this to Patty, who realizes that the API defaults to U.S. postal codes, and requires a country code for codes in Can- ada and Mexico. She hadn’t written any unit tests yet for other countries. They revise the test inputs, and Patty pairs with Paul Programmer to change the code that calls the API. Now the test looks something like Figure 18-4.\n\nWeight 5 lbs 5 lbs\n\nDestination Postal Code 80104 T2J 2M7\n\nCoutry Code US CA\n\nFigure 18-4 Revised happy path test\n\nThis simple example illustrates the iterative back-and-forth between cod- ing and testing. Different teams take different approaches. Patty and Tammy might pair on both the coding and testing. Tammy might pair with Paul to write the ﬁxture to automate the test. Tammy might be in a remote ofﬁce, using an online collaboration tool to work with Patty. Patty might write the executable story tests herself and then write the code to make them work, practicing true story test-driven development. The point is that testing and coding are part of one development process in which all team members participate.\n\nTammy can continue to identify new test cases, including edge cases and boundary conditions, until she feels all risk areas have been covered by the minimum amount and variety of test cases. She might test with the heaviest item available on the website sent to the most expensive destination. She might test having a large quantity of the same item. Some edge cases may be so unlikely that she doesn’t bother with them, or she decides to run a test but after it passes doesn’t include it in the regression suite. Some tests might be better done manually after a UI is available.\n\nCost 7.25 9.40",
      "content_length": 2435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "DRIVING DEVELOPMENT\n\nPower of Three\n\nPatty has written unit tests with Hawaii as the shipping destination, but Tammy believes that only continental destinations are acceptable. Neither of them is sure whether military post ofﬁce box destinations are acceptable. They go see Polly Product-Owner to ask what she thinks. They’re using the Power of Three. When disagreements or questions arise, having three differ- ent viewpoints is an effective way to make sure you get a good solution and you won’t have to rehash the issue later. If one of the participants in the dis- cussion isn’t familiar with the topic, the others will have to organize their thoughts to explain it clearly, which is always helpful. Involving people in dif- ferent roles helps make sure that changes to requirements don’t ﬂy under the radar and surprise team members later.\n\nWhen unexpected problems arise, as they always do, the Power of Three rule is a great place to start. You may need to pull in more people, or even the whole team, depending on the severity or complexity of the issue. What if the shipping partner’s API proves to be so slow that the response time on the website will be unacceptable? Both the development team and the customer team need to quickly explore alternative solutions.\n\nFocus on One Story\n\nPaul looks for a programming task to work on. Although the UI tasks for the estimated shipping cost story are still in the “to do” column on the task board, he’s more interested in the story to delete items out of the shopping cart, so he picks up one of those cards. Nobody has time to start writing the executable tests for that story, so he plunges ahead on his own.\n\nNow the team has two stories going. They don’t really know how much time it will take to ﬁnish either story. A much better approach would be for Paul to start working on a UI task for the ﬁrst story so that story can be ﬁnished sooner. When a story’s done (meaning all of the code is written and tested), you know exactly how much work is left to do on it: zero. If disaster struck and no other stories got ﬁnished this iteration, there is at least one completed story to release.\n\nCompleting the whole story isn’t a testing concept, but it’s one that testers should promote and follow. If a programmer has started coding on a story, make sure someone has also started working on testing tasks for that story. This is a bal- ancing act. What if nobody has written even high-level tests for the delete items story? Maybe that’s the highest testing priority? Usually, ﬁnishing a story should be the goal before the team can move on to the next story.\n\n411",
      "content_length": 2619,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "412\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nUnless the team is very small, there is always more than one story in progress at any given time. It might be more difﬁcult, but try to focus on ﬁnishing one story at a time. Patty is about to wrap up the shipping cost story, and Paul has moved on to the delete items story. Patty runs into a snag, and she isn’t sure how to solve it. Paul helps her to ﬁnish the code so that Tammy can ﬁnish her exploratory testing and they can mark the story “done.” Now they have a bet- ter idea of how much they have left to ﬁnish this iteration (or at least, how much they don’t still have to work on).\n\nSometimes, several different stories can be done at the same time if a pro- grammer and tester pair up to complete each story together. This works if the stories are small and independent. What you don’t want to see is program- mers starting coding without testing tasks being completed at the same time.\n\nTESTS THAT CRITIQUE THE PRODUCT As soon as testable chunks of code are available, and the automated tests that guided their coding pass, take time to explore the functionality more deeply. Try different scenarios and learn more about the code’s behavior. You should have task cards for tests that critique the product, both business- and technology- facing. The story’s not “done” until all of these types of tests are complete.\n\nThis becomes more important when all tasks except testing are complete for a story. Now you should be able to test from one end of the story’s thread to the other end, with all of the variations in between. Don’t put this testing off. You may ﬁnd requirements that were in the story but were missed with the tests that drove development and are thus missing in the code. Now’s the time to write those missing tests and code. Fill in all of the gaps and add more value while the team is still focused on the story. Doing this later will cost much more.\n\nChapter 10, “Business-Facing Tests that Critique the Product,” and Chapter 11, “Tecnology-Facing Tests that Critique the Product,” will help you make sure you cover all of the necessary tests that critique the product.\n\nBe aware that some of what you learn in testing the ﬁnal story may be consid- ered “nice to have,” perhaps making the functionality easier to use or faster, items that weren’t part of the original story. Consult with your customer. If there’s time to add it in the iteration, and the business can use the extra value, go ahead. These additions are much cheaper to add now. But don’t jeopardize other stories by spending too much time adding “bling” that doesn’t have a big ROI.\n\nIf your exploratory testing leads the team and the customers to realize that signiﬁcant functionality wasn’t covered by the stories, write new stories for future iterations. Keep a tight rein on “scope creep” or your team won’t have time to deliver the value you planned originally.",
      "content_length": 2907,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 456,
      "content": "COLLABORATE WITH PROGRAMMERS\n\nTechnology-facing tests to critique the product are often best done during coding. This is the time to know if the design doesn’t scale, or if there are se- curity holes.\n\nCOLLABORATE WITH PROGRAMMERS Our vignette describing a team writing and using detailed tests to drive cod- ing shows how closely testers and programmers collaborate. This continues as coding and testing proceed. Working together enhances the team’s ability to deliver the right product and provides many opportunities to transfer skills. Programmers learn new ways of testing, and they’ll be better at testing their own code as they write it. Testers learn more about the process of cod- ing and how the right tests might make it easier.\n\nPair Testing\n\nPaul Programmer has completed the user interface for the estimated ship- ping options story, but he hasn’t checked it in yet. He asks Tammy to come sit with him and demonstrates how the end user would enter the shipping ad- dress during the checkout process. The estimated shipping cost displays right away. Tammy changes the shipping address and sees the new cost appear. She enters a postal code that doesn’t match the rest of the address and sees the appropriate error message appear. The UI looks good to both of them, so Paul checks in the code, and Tammy continues with her exploratory manual testing of it.\n\nJanet likes to have the programmer “drive” during these pair testing sessions while she watches what happens. She ﬁnds that it is far more effective than taking control of the keyboard and mouse while the programmer watches.\n\n“Show Me”\n\nTammy is especially concerned with changing the shipping address and having the estimated cost recalculate, because they identiﬁed that as a risky area. She ﬁnds that if she displays the estimated cost, goes ahead to the billing address page, and then comes back to change the shipping address, the estimated costs don’t change properly. She gets Paul to come observe this behavior. He realizes there is a problem with session caching and goes back to ﬁx it.\n\nShowing someone a problem and working through it together is much more effective than ﬁling a bug in a defect tracking system and waiting for someone to have time to look at it. It’s harder to do if the team isn’t co-located. If team\n\n413",
      "content_length": 2305,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "414\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nmembers are working in vastly different time zones, it’s even harder. Stick to the most direct communication available to you. One of Lisa’s teammates is in a time zone 121⁄2 hours ahead. He works late into his nighttime, and when needed, he calls Lisa and they work through test results and examples together.\n\nThe bibliography contains refer- ences for further reading on this subject.\n\nThe simple act of showing the GUI to another person may help Paul realize he’s implemented some erroneous behavior. Similarly, if Tammy is having trouble getting her GUI test script to work, explaining the problem might be enough for her to realize what’s causing it. If there is nobody available to look at what you’ve just coded or help you debug a problem, it sometimes helps to explain it out loud to yourself. “Rubber Ducking” and “Thinking Out Loud” are surprisingly effective ways to solve your own problems. Janet likes to have her own little rubber duck sitting on her desk to remind herself to think be- fore she asks.\n\nTALK TO CUSTOMERS It’s shockingly easy for development team members to get their heads down cranking out stories and forget to keep customers in the loop. In addition to consulting business experts when we have questions, we need to show them what we’ve delivered so far.\n\nHopefully, you were able to review test cases with customers, or with some- one who could represent the customer, before coding began. If not, it’s never too late. For situations where customers need to be more involved with the details of the executable tests, be sure to ﬁnd test tools that work for them as well as for technical team members.\n\nAs we described in the last two chapters, you may have already gone over mock-ups or paper prototypes with your customers. If tasks to mock up a re- port or interface remain in the iteration plan, remember to keep the process simple. For example, don’t code an HTML prototype when drawing on a whiteboard will do just as well. We want to keep the process as simple as pos- sible; simplicity is a core value.\n\nShow Customers\n\nAs soon as a coded user interface or report is ready, even if it’s still rudimentary, lacking all features or displaying hard-coded data, show it to the appropriate customers. Nobody can explain exactly what they want ahead of time. They need to see, feel, and use the application to know if it’s right. You may not be",
      "content_length": 2427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "Lisa’s Story\n\nCOMPLETING TESTING TASKS\n\nable to implement big changes mid-iteration, but if you start early, there may be time for minor tweaks, and your customers will know what to expect.\n\nThe iteration review meeting is a great opportunity to show what the team delivered and get feedback for the next iteration, but don’t wait until then to get input from customers. Keep them involved throughout the iteration.\n\nUnderstand the Business\n\nAlthough we get caught up in the fast pace of iterations, we also need to stop and take time to understand the business better. Spend some time talking to business people about their jobs and what aspects might be enhanced with new software features. The better you understand your customer’s business, the better you can be at providing a good product.\n\nMy team budgets time for each development team member to sit with the retire- ment plan administration team members as they do their daily work. Not only do we understand those jobs better, but we often identify small changes in the appli- cation that will make the administrator’s work easier.\n\nSimple additions such as a bit of extra data provided, an additional search ﬁlter, or changing the order of a display can make a big difference to a tedious and de- tailed process. We also document what we learn with ﬂow charts and wiki pages so that other team members can beneﬁt.\n\nSome teams actually sit with the business people permanently so that they are involved with the actual business on a daily basis.\n\nCOMPLETING TESTING TASKS Agile testers are proactive. We don’t sit and wait for work to come to us. Testers who are accustomed to a waterfall process may feel there’s nothing to do until a story is 100% complete. That’s rarely true during an agile iteration. Work with programmers so that they produce some testable piece of code early on. The shipping cost algorithm presented earlier is a good example. It can be tested completely in isolation, without needing to access the database or the user interface. Alternatively, the user interface could be stubbed out with hard-coded data before the services accessing the real data are complete, and the behavior of the presentation layer can be tested by itself.\n\n415\n\n—Lisa",
      "content_length": 2229,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "416\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nPeril: The Testing Crunch\n\nEven experienced agile teams often experience a testing crunch at the end of an iteration. Maybe a story or two turned out to take much longer than ex- pected, or a production problem took time away from development. What happens when tomorrow is the end of your iteration and your task board (real or virtual) is still full of testing cards?\n\nIf you see this, recognize it as a bad smell. Work with the team to determine what the problem may be. Are the programmers not working closely enough with the testers? Were there too many interruptions?\n\nThe way to address this peril is to involve the whole team. Remember that anyone on the team can sign up for testing tasks. In your daily stand-up, you can evaluate whether the team is on track to ﬁnish all of the stories. If multiple stories are in danger of not being completed, choose a story to drop, or re- duce the scope on one or more stories. Focus on completing one story at a time. As the end of the iteration approaches, programmers may have to stop working on new features and start picking up testing tasks instead. Missing some functionality from a release is better than missing the entire release be- cause testing couldn’t be completed on all or most stories.\n\nThe programmers on Lisa’s team regularly automate behind-the-GUI tests in addition to unit and integration tests. They also often write the functional behind-the-GUI test cases. Sometimes they write the initial happy path execut- able test so they can coordinate test and code design; then a tester adds more test cases. Occasionally, they write all of the functional test cases, because the testers don’t have the bandwidth to cover all of the test-intensive stories.\n\nEveryone on the team also must be willing to take on manual testing tasks. If your team is just starting and hasn’t been able to address automation needs yet, the whole team should plan time to execute manual regression test scripts as well as manually testing new features. As Lisa’s team can attest, this task provides great motivation for learning how to design the application to facilitate test automation. Other teams tell us this worked for them as well.\n\nDEALING WITH BUGS We’ve known many teams that struggle with the question of how to track bugs, or whether to track them at all. As Tom and Mary Poppendieck write in their book Implementing Lean Software Development: From Concept to Cash [2006], defect queues are queues of rework and thus collection points for",
      "content_length": 2541,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "Chapter 5, “Transi- tioning Typical Processes,” talks about why your team may or may not want to use a Defect Tracking System.\n\nDEALING WITH BUGS\n\nwaste. Some teams simply ﬁx bugs as soon as they’re discovered. They write a unit test to reproduce the bug, ﬁx the code so the test passes, check in the test and the bug ﬁx, and go on. If someone breaks that piece of code later, the test will catch the regression.\n\nOther teams ﬁnd value in documenting problems and ﬁxes in a defect track- ing system (DTS), especially problems that weren’t caught until after code was released. They may even look for patterns in the bugs that got to produc- tion and do root cause analysis to learn how to prevent similar issues from recurring. Still, defect systems don’t provide a good forum for face-to-face communication about how to produce higher-quality code.\n\nLisa and her fellow testers prefer to talk to a programmer as soon as a prob- lem is found. If the programmer can ﬁx it immediately, there’s no need to log the bug anywhere. If no programmer is available immediately to work on the problem, and there’s a possibility the bug might be forgotten, they write a card for it or enter it into their DTS.\n\nWe’ve added this section to this chapter because this is when you run into the problem. You have been writing tests ﬁrst, but are ﬁnding problems as you work with the programmer. Do you log a bug? If so, how? You’ve been doing your exploratory testing and found a bug from a story that was marked done. Do you log a bug for that? Let’s discuss more about defects and consider op- tions that are open to you and your team.\n\nIs It a Defect or Is It a Feature?\n\nFirst, let’s talk about defects versus features. The age-old question in software development is, “What is a bug”? Some answers we’ve heard are: It’s a devia- tion from the requirements or it’s behavior that is not what was expected. Of course, there are some really obvious defects such as incorrect output or in- correct error messages. But what really matters is the user’s perception of the quality of the product. If the customer says it is a defect, then it is a defect.\n\nIn agile, we have the opportunity to work with customers to get things ﬁxed to their satisfaction. Customers don’t have to try to think of every possible feature and detail up front. It is okay for them to change their minds when they see something.\n\nIn the end, does it really matter if it is a bug or a feature if it needs to be ﬁxed? The customer chooses priorities and the value proposition. If software quality\n\n417",
      "content_length": 2556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 461,
      "content": "418\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nis a higher priority for the customer than getting all of the new features, then we should try to ﬁx all defects as we ﬁnd them.\n\nCustomers on the team use their knowledge to give the best advice they can to the team on day-to-day development. However, when a product goes to UAT and is exposed to a larger customer base, there will always be requests in the form of bugs or new enhancements.\n\nTechnical Debt\n\nChapter 6, “The Purpose of Test- ing,” explains how tests help manage technical debt.\n\nOne way of thinking about defects is as technical debt. The longer a defect stays in the system and goes undetected, the greater the impact. It also is true that leaving bugs festering in a code base has a negative effect on code quality, system intuitiveness, system ﬂexibility, team morale, and velocity. Fixing one defect in buggy code may reveal more, so maintenance tasks take longer.\n\nZero Bug Tolerance\n\nJanet encourages teams that she works with to strive for “zero tolerance” to- ward bug counts. New agile teams usually have a hard time believing it can be done. In one organization Janet was working with, she challenged each of the ﬁve project teams to see how close they could come to zero bugs out- standing at the end of each iteration, and zero at release time.\n\nZero Bug Iterations\n\nJakub Oleszkiewicz, the QA manager at NT Services [2008], recounts how his team learned how to ﬁnish each iteration with no bugs carried over to the next one.\n\nI think it really comes down to exceptional communication between the testers, the developers, and the business analysts. Discipline was also key, because we set a goal to close off iterations with fully developed, functional, deployable, and defect-free features while striving to avoid falling into a waterfall trap. To us, avoiding waterfall meant we had to maintain alignment with code and test activities; we tried to plan an it- eration's activities so that a given feature's test cases were designed and automated at the same time as that feature's code was written. We quickly found that we were practicing a form of test-driven develop- ment. I don't think it was pure TDD, because we weren't actually exe- cuting the tests until code was checked in, but we were developing the tests as developers wrote code, and developers were asking us how our tests were structured and what our expected results were.",
      "content_length": 2416,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 462,
      "content": "IT’S ALL ABOUT CHOICES\n\nConversely, we regularly asked the developers how they were imple- menting a given feature. This kind of two-way questioning often elevated inconsistencies in how requirements were interpreted and ultimately highlighted defects in our interpretations before code was actually committed.\n\nEvery morning during our Scrum, we further ensured parity between the functional groups within the team through simple dialogue. Communica- tion was ridiculously good—we sat close to each other, often even at the same computer. When a defect was discovered, the developer was right there observing, taking notes, and talking through the require- ments. A business analyst was always nearby to further validate our thinking. Often within minutes a resolution was checked-in, deployed to the test environment, and veriﬁed.\n\nBoth developers and testers had to be committed to this approach or it wouldn't have worked. Without discipline, the developers could have easily moved forward onto more features and let the bugs slide until the end of the project, risking an incomplete iteration. If we were not co- located as we were, communication would have suffered; likely a bug tracking system or email would have become our primary means of com- municating defects, resulting in longer turn-around times and an in- creased probability of rework.\n\nAs part of any development, you will always need to make trade-offs. Your team may decide to release with some outstanding bugs because it is deemed more important to get new functionality out the door than to ﬁx low-level bugs.\n\nIT’S ALL ABOUT CHOICES Teams have solved the problem of how to handle defects in many different ways. Some teams put all of their bugs on task cards. Other teams have cho- sen to write a card, estimate it, and schedule it as a story. Still others suggest adding a test for every bug—that way you don’t have to record the defect, just the test.\n\nIs there one right way? Of course not! But, how do you know what is right for your team? We have some suggestions to help you choose and decide what is right for you. Think about your team and your product and what might work in your situation. First, we’ll talk about what defects we should log, then we’ll talk a bit about when you should ﬁx them, and ﬁnally we’ll look at what media to choose. The right combination will depend on how far along your team is in its agile journey and how mature your product is.\n\n419",
      "content_length": 2449,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 463,
      "content": "420\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nDecide Which Bugs to Log\n\nNot all bugs need to be logged, but teams often struggle with which ones should be recorded and which ones don’t need to be. We recommend that you avoid creating a defect report if possible. Have a conversation with a real person ﬁrst, and only produce a defect report if it is truly a real problem that demands a change to the product or the programmers just can’t get to it right away.\n\nUnit Test Failures Don’t log unit test failures. If you are part of a team that is practicing TDD (test-driven development) and has good coverage with its unit tests, you know that failed tests during the build should not be logged. A failed test during the continuous integration build is a signal for the programmers to address the problem right away. Logging these bugs would be redundant and a waste of time.\n\nFailures in Higher-Level Regression Tests Many teams have builds that run regression tests above the unit level, such as tests behind the GUI and tests through the GUI. When one of these builds fails, should you log the bug in a DTS?\n\nLisa’s Story\n\nWe have two builds, an “ongoing build” that runs only unit tests, and a “full build” that runs the functional tests behind and through the GUI. When the “full build” breaks, if a developer investigates and tackles the problem right away as some- times happens, usually no bug is logged. The problem is ﬁxed quickly. At other times, the failure is not straightforward. One of the testers investigates, narrows down the problem, and ﬁles a bug that either states the name of the failing test or provides manual steps to recreate the problem.\n\nIn either case, tests are written that reproduce the bug, and the code is ﬁxed to make the tests pass. The tests become part of one of the builds.\n\nFailing tests in themselves are a type of recorded bug. But sometimes, as in Lisa’s case, more information needs to be added to allow for an effective and clean ﬁx, so logging the defect is warranted.\n\nStory Bugs within the Current Iteration Don’t log bugs that can be ﬁxed immediately, especially if you would other- wise record them in an electronic DTS. If your team is working closely with\n\n—Lisa",
      "content_length": 2212,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 464,
      "content": "IT’S ALL ABOUT CHOICES\n\nthe programmers and is practicing pair testing as soon as a story is com- pleted, we strongly recommend that you don’t log those bugs as long as the programmer addresses them right away. As you notice issues, talk them over with the programmer and decide whether they are real issues or not. Talk to the customer if you need to, but make a couple of notes so you remember what you saw so you can adjust your tests if needed.\n\nIf you are using index cards to log bugs, you may want to put an index card up on the task board (or a card on your electronic board) just as a reminder.\n\nPost-Iteration Bugs (Or Those that Can’t Be Fixed Immediately) Do log bugs that can’t be ﬁxed right away. We stress testing early in order to catch as many bugs as possible while the programmers are still working on the story. We know it is cheaper to ﬁx them when caught early; however, sometimes we just don’t catch them right away. The programmer has moved on to another story and can’t drop everything to ﬁx it now. Those are the ones that are good candidates for logging. Sometimes a “bug” is really a missed requirement and needs to be handled as a story—estimated and pri- oritized for a future iteration.\n\nFrom the Legacy System Do log bugs that occur in the legacy system. If your product has been around a long time, it likely has a number of bugs that have been lurking in the back- ground just waiting to be discovered. When you ﬁnd them, you have a couple of choices. If your product owner thinks it is worthwhile to ﬁx them, then log the bugs and they can be prioritized as part of the product backlog. However, if they have been around a long time and cause no issues, your product owner may decide it is not worth ﬁxing them. In this case, don’t bother log- ging them. They will never get addressed anyhow, so don’t waste your time.\n\nFound in Production Do log all production bugs. When your application is in production, all bugs found by the customer should be logged. Depending on their severity, these bugs may be ﬁxed immediately, at the time of the next release, or they’ll be estimated, prioritized, and put in your product backlog.\n\nChoose When to Fix Your Bugs\n\nThere are three options. All bugs you ﬁnd need to be triaged to determine if you ﬁx them now, ﬁx them later, or don’t ﬁx them at all. This triage may be as simple as a discussion with the programmer to determine if they are really\n\n421",
      "content_length": 2427,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 465,
      "content": "422\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nbugs in the story he is working on. The triage may be a discussion with the product owner to determine if there should be another story for the next it- eration. The triage may also be a formal process with the customers to prior- itize which bugs to ﬁx.\n\nFix Now The more bugs you can ﬁx immediately, the less technical debt your applica- tion generates and the less “defect” inventory you have. Defects are also cheaper to ﬁx the sooner they are discovered. In an article in iSixSigma Mag- azine, Mukesh Soni [2008] quotes a report from IBM that the cost to ﬁx an error found after product release was four to ﬁve times as much as one un- covered during design, and up to 100 times more than one identiﬁed in the maintenance phase (see Figure 18-5).\n\nFigure 18-5 shows a statistic based on phased methodology, but the statistic still holds true for agile development. It is cheaper to ﬁx bugs that are found during development than after.\n\nIf a defect is found while developing a new feature, or is a side effect from an- other bug ﬁx, it should be automatically ﬁxed. But, as usual, this is to be ap- plied with prudence. For example, if a bug is found that the programmers say will be difﬁcult to ﬁx and may destabilize the product, it should be taken to the customers to prioritize.\n\nPhase/Stage of the S/W Development in which the Defect is Found\n\n120\n\n100\n\n100x\n\n80\n\n60\n\n40\n\n20\n\n15x\n\n1x\n\n6.5x\n\n0\n\nDesign\n\nImplementation\n\nTesting\n\nMaintenance\n\nFigure 18-5 Relative costs to ﬁx software defects (Source: IBM Systems Sciences Institute)",
      "content_length": 1585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 466,
      "content": "IT’S ALL ABOUT CHOICES\n\nIf you ﬁx the bugs during development, you lessen the presence of bugs later in the process. Your team velocity can include time to ﬁx bugs. Over time, your team members will get a good idea of how long they spend on ﬁxing bugs found by the testers for a story. Hopefully, there are few. If your team is a new agile team, there may be quite a few bugs that escape development, but as the team gets more comfortable with the tools and the processes, the num- ber of bugs found will lessen. To start, try making the estimate for a story to include two hours or half a day for ﬁxing associated bugs.\n\nFix Later Different teams have different ways of handling defects. Some teams believe that all defects found should be prioritized by the customers before they get put on the list to ﬁx. They believe it is completely up to the customer to deter- mine whether they really are defects, and if so, whether they should be ﬁxed.\n\nNever Fix Your team has recognized a defect, but know it won’t get ﬁxed. Perhaps that section of code needs a complete rewrite later because the functionality will change, or perhaps it is just such a low-priority issue or so obscure that your customers may never ﬁnd it. There are a multitude of reasons why it won’t get ﬁxed. If your triage determines this is the case, we suggest you just close the bug. Don’t keep it open pretending that you will ﬁx it someday.\n\nChoose the Media You Should Use to Log a Bug\n\nWhen we talk about media, we mean the variety of ways you can log a bug. It could be a defect tracking system or index cards, or maybe you choose to have no physical record at all.\n\nIndex Cards Index cards (whether real or virtual cards in an online planning and tracking system) don’t leave a lot of room for a lot of clerical details, but they do give great visibility to outstanding issues when they are pinned on the story board, especially if they are in another color. Some teams use screen prints and staple them to the back of the card or write the details in a text ﬁle, or even record steps in audio form on a hand-held voice recorder.\n\nThere are lots of options, but we would suggest that you pick one that contains enough information to guide someone to reproduce a problem or to focus a discussion when the programmer is ready to ﬁx it. The card is tangible. Five hundred bugs in a DTS are just a number. A stack of 500 cards is impressive.\n\n423",
      "content_length": 2418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 467,
      "content": "424\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nUse cards in the following circumstances:\n\n(cid:2) You are a disciplined agile team and are ﬁxing all bugs within an\n\niteration.\n\n(cid:2) You want to make bugs visible to the team.\n\nThere is nothing stopping you from having both index cards and a DTS.\n\nDefect Tracking System Use a DTS in the following circumstances:\n\n(cid:2) Your team is distributed. (cid:2) You need to track bugs for audit purposes or to capture them in re-\n\nlease notes.\n\n(cid:2) You have bugs that escape an iteration and you need to remember to\n\nﬁx them later.\n\n(cid:2) You have a legacy system with a large number of defects.\n\nOne way or the other, you will likely want to have some kind of DTS to log some of the bugs. This does not mean you need to log them all. Be smart about which ones you do log.\n\nNone at All Why wouldn’t you log a bug? Most teams that we have worked with have set rules for themselves that no bug is ﬁxed without a unit test. If you also have a functional automation suite, then you can catch the larger bugs with those. The argument is that if there is a test that will catch the bug, you have no need to log the bug. Anything learned from ﬁxing the bug was captured in the test and the code. However, you need to recognize that not all tests are easy to automate.\n\nUse tests to capture bugs in the following circumstance:\n\n(cid:2) Your team is disciplined and writes tests for every bug found.\n\nAlternatives and Suggestions for Dealing with Bugs\n\nAs teams mature, they ﬁnd procedures that work for them. They eliminate redundant tasks. They become more practiced at using story cards, story",
      "content_length": 1637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 468,
      "content": "Janet’s Story\n\nIT’S ALL ABOUT CHOICES\n\nboards, and project backlogs. They use tests effectively, and learn which bugs to log and what metrics make sense to their team. In this section, we’ll share some ideas that other teams have found work for them.\n\nSet Rules Set rules like, “The number of pink cards (bugs) should never get higher than ten at any one time.” Revisit these each time you have a team retrospective. If your defect rate is going down, no worries. If the trend is the opposite, spend time analyzing the root cause of bugs and create new rules to mitigate those.\n\nFix All Bugs Don’t forget to ﬁx low-priority bugs found during the iteration as well, be- cause they have an effect on future development. In our experience, there seems to be a strong correlation between “low priority” and “quick to ﬁx,” al- though we don’t have hard facts to support that. We suggest stopping small, isolated bugs before they become large, tangled bugs.\n\nCombine Bugs If you ﬁnd a lot of bugs in one area, think about combining them into an en- hancement or story.\n\nWhen I ﬁrst started working at WestJet, I found a lot of small issues with the mobile application. The application worked correctly, but I was confused about the ﬂow. I only found these issues because I was new and had no previous perceptions.\n\nThe team decided to group the issues I had raised and look at the whole issue as a new story. After studying the full problem with all of the known details, the ﬁnal outcome was a solid feature. If the bugs had been ﬁxed piecemeal, the effect would not have been so pretty.\n\nTreat It as a Story If a “bug” is really missed functionality, choose to write a card for the bug and schedule it as a story. These stories are estimated and prioritized just like any other story. Be aware that bug stories may not receive as much attention as the new user stories in the product backlog. It also takes time to create the story, prioritize, and schedule it.\n\n425\n\n—Janet",
      "content_length": 1970,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 469,
      "content": "426\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nThe Hidden Backlog\n\nAntony Marcano, author of www.TestingReﬂections.com, points out that while user stories and their acceptance tests describe desired behavior, de- fect reports describe misbehavior. Behind each misbehavior is a desired be- havior, often not previously deﬁned. Thus, behind every defect report may be a hidden user story. He explains his experiences.\n\nIn Chapter 5, “Transitioning Typical Processes,” we mentioned Antony Marcano’s blog post about defect tracking systems being a hidden backlog in agile teams. Antony shares his ideas about how to bring that secret out into the open.\n\nXP publications suggest that if you ﬁnd a bug you should write an auto- mated test reproducing it. Many teams ﬁle a bug report and then write a separate automated test. I’ve found that this results in duplication of ef- fort—and therefore waste. When we write a bug report, we state the steps, what should have happened (expectation), and what actually happened (anti-expectation). An automated test tells you the same things—steps, expectation, and running it for the ﬁrst time should dem- onstrate the anti-expectation. When you are able to write an automated acceptance test as easily as you write a bug-report and the test commu- nicates as much as the bug report does and your backlogs and story boards allow you to manage the work involved in ﬁxing it, then why write a separate bug report?\n\nBug metrics are all that remain. Bug metrics are traditionally used to help predict when software would be ready for release or highlight whether quality is improving or worsening. In test-ﬁrst approaches, rather than telling us if quality is improving or worsening, it tells us how good we were at predicting tests—that is, how big the gaps were in our original thinking. This is useful information for retrospectives and can be achieved simply by tagging each test with details of when it was identi- ﬁed—story elaboration, post-implementation exploration, or in produc- tion. As for predicting when we will be able to release—when we are completing software of “releasable quality” every iteration—this job is handled by burn-down/burn-up charts and the like.\n\nWith one new project I was working on, I suggested that we start using a bug-tracking system when the need for one was compelling. We cap- tured the output of exploratory testing performed inside the iteration as automated tests rather than bug reports. We determined whether the test belonged to the current story, another story, or whether these tests inspired new stories. We managed these stories as we would any other story and used burn-down charts to predict how much scope would be done by the end of the iteration. We never even set up a bug-tracking system in the end.\n\nThere is a difference between typical user stories and bug-inspired user stories, however. Previously our stories and tests only dealt with missing behaviors (i.e., features we know we want to implement in the future).",
      "content_length": 3008,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 470,
      "content": "Janet’s Story\n\nIT’S ALL ABOUT CHOICES\n\nNow, they also started to represent misbehaviors. We found it useful to in- clude summary information about the misbehavior in our proposed user story to help the customer prioritize it better. For example:\n\nAs a registered user, I want to be prevented from accessing the system if my password is entered using the incorrect case, so that I can feel safer that no one else can guess my password, rather than being allowed to access the system.\n\nThe “rather than” was understood by the customer to mean \"that's something that happens currently\"— which is a misbehavior rather than merely a yet-to-be-implemented behavior.\n\nUsing this test-only approach to capturing bugs, I’ve noticed that bug- inspired stories are prioritized more as equals to the new-feature user stories, whereas before they often gave more attention to the “cool new features” in the product backlog than the misbehaviors described in the bug tracking. That's when I realized that bug-tracking systems are essen- tially hidden, or secret backlogs.\n\nOn some teams, however, the opposite is true. Fix-all-bugs policies can give more attention to bugs at the expense of perhaps more important new features in the main backlog.\n\nNow, if I'm coaching a team mid-project, I help them to ﬁnd better and faster ways of writing automated tests. I help them use those improve- ments in writing bug-derived automated tests. I help them ﬁnd the ap- propriate story—new or existing—and help them harness the aggregate information useful to retrospectives. Eventually, they come to the same realization that I did: Traditional bug tracking starts to feel wasteful and redundant. That's when they decide that they no longer want or need a hidden backlog.\n\nIf bugs are simply logged in a DTS, important information might be effectively lost from the project. When we write acceptance tests to drive development, we tend to focus on desired behavior. Learning about undesired behavior from a defect, and turning that into stories is a vital addition to producing the right functionality.\n\nBlue, Green, and Red Stickers Each team needs to determine the process that works for it, and how to make that process easily visible. The following story is about one process that worked for Janet.\n\nA few years ago, I worked on a legacy system with lots of bugs already logged against the system before agile was introduced. One of the developers was adamant that he would not use a defect-tracking system. He ﬁrmly believed they\n\n427",
      "content_length": 2517,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 471,
      "content": "428\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nwere a waste of time. However, the testers needed the defects logged because there were so many.\n\nThe team worked out a compromise that worked for everyone. Bugs that were found during pair testing with the programmers were not recorded, because they were ﬁxed right away. All others were logged in the DTS. Bugs that needed to be ﬁxed in the current iteration were recorded on pink cards with the summary and bug number and then put on the story board. All others became part of the prod- uct backlog.\n\nThe programmers could look at details in the system but also asked testers for more information, if required. Because the issues were on the story board, they became part of the daily stand-ups and discussions. When a bug was ﬁxed, the programmers wrote the ﬁx and any extra information on the back of the card. They put a blue sticker on the card so the testers knew it was ready for testing. A green sticker meant it had been veriﬁed as ﬁxed, and a red sticker meant it wasn’t ﬁxed and needed more work. Of course, there were lots of conversations be- tween the testers and the programmers. James, one of the programmers, and I had a lot of fun with one bug that just wouldn’t stay ﬁxed. By the end, the card looked like it had a caterpillar on it—blue, red, blue, red, blue, and ﬁnally green. We were all quite excited when that bug was squashed.\n\nThe testers closed bugs and did most of the administration, because the DTS was their requirement. After a while, the programmers started entering what they ﬁxed into the defect-tracking system because it was easier than writing on the card. The team still continued to use the cards because of the visibility. It was easy to see at a glance how many outstanding bugs there were in the iteration or on the backlog.\n\nThis approach worked for this team because there was a lot of discipline in the team, and most new bugs were ﬁxed in the iteration if they were part of the new or changed functionality. The only bugs that went into the backlog were legacy bugs that were deemed low risk.\n\nStart Simple\n\nWe suggest using as simple a system as possible and applying complexity as required. Code produced test-ﬁrst is, in our experience, fairly free of bugs by the time it’s checked in. If you’re ﬁnding a lot of bugs in new code, your team needs to ﬁgure out why, and take action. Try to shorten the cycle of coding, integrating and testing so that programmers get immediate feed- back about code quality. Perhaps some buggy section of legacy code needs to be redesigned before it mires your team in technical debt. Maybe you need to work more closely with the business experts to understand the de- sired functionality.\n\n—Janet",
      "content_length": 2725,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 472,
      "content": "More on retro- spectives in Chap- ter 19, “Wrap Up the Iteration.”\n\nFACILITATE COMMUNICATION\n\nAnother idea might be to create an ongoing “start, stop, continue” list so that you can remember some of the issues during the iteration retrospective.\n\nFACILITATE COMMUNICATION The daily stand-up helps teams maintain the close communication they need. Everyone on the team learns the current status of tasks and stories, and can help each other with obstacles. Often, hearing programmers describe tasks they’re working on provides a clue that they may have misunderstood the customer’s requirements. That signals the need for a group discussion af- ter the stand-up. If a tester needs help with a testing issue that’s come up, she might ask the team to stay after the stand-up to talk about it. Missed tasks are often identiﬁed during stand-ups, and new cards can be written on the spot.\n\nThe stand-up is a good time to look at progress. Use big, visible charts such as story boards, burndown charts, and other visual cues to help keep focus and know your status. If the end of the iteration is drawing near, and coding on a story seems “stuck,” raise a red ﬂag and ask the team what can be done about it. Perhaps some pairing or extra help will get things going. Lisa has of- ten noted when there’s a lot of testing left to do and time is running out. She asks for help to pick up the slack. The whole team focuses on what needs to be done to complete each story and talks about the best approach.\n\nWhen teams use an electronic medium for keeping track of stories, there is a tendency to forget the story board. Janet ﬁnds that having both may seem like a duplication of effort, but the visibility of progress to the team far out- weighs the extra overhead of writing up the task cards and moving them as they are completed. Having the story board gives your team focus during the stand-ups or when you are talking to someone outside the team about your progress.\n\nTesters Facilitate Communication\n\nTesters can help keep the iteration progressing smoothly by helping make sure everyone is communicating enough. Talk to programmers when they start working on a story, and make sure they understand it. Lisa ﬁnds that she can write all of the tests and examples she wants on the team wiki, but if nobody bothers to read them, they don’t help. When in doubt, she goes over requirements and tests with the programmer who picks up the task cards.\n\nProgrammers will always have questions as they develop a story, even if they understand the business and the story well. It’s best if a customer is available\n\n429",
      "content_length": 2601,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 473,
      "content": "430\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nto answer questions, because that is the most direct communication. Testers shouldn’t get in the way of that; however, we’ve observed that business ex- perts sometimes have trouble explaining a requirement, or a programmer simply gets the wrong idea and can’t get on the same page with the customer. The Power of Three applies here. Testers can help customers and program- mers ﬁnd a common language.\n\nA Little Friendly Competition\n\nGerard Meszaros, well-known agile coach and author of xUnit Test Patterns [2007], shared this story about a team he was working with and how a game solved a communication issue.\n\nWe were having trouble getting the developers to talk to the business people about their assumptions. When they did talk, the tester often got left out of the loop. The tester would sometimes discuss something with the business but never pass it on to the developer. Our project manager, Janice, decided to try to change the behavior through friendly competition.\n\nAll of the developers were given blue poker chips with a “D” written on them. All of the testers got a red chip with a “T” on them, and the busi- ness people got yellow chips with a “B” on them. Whenever someone met with a counterpart from another area, he or she could exchange one chip with each person. The goal was to get the most complete sets of chips: T-B-D. The winner got a custom-made T-B-D trophy decorated with the three kinds of chips. The end result was that everyone was much keener to meet with each other because they would get more chips!\n\nFind creative ways to get the business experts and programmers to talk and agree upon requirements. If a poker chip game gets them talking, embrace it.\n\nFacilitating communication usually involves drawing on a whiteboard, mock- ing up interfaces, listing other areas that might be affected, or working through real examples. Whenever communication appears to reach a dead end, or con- fusion is rampant, ask for a new example and focus on that.\n\nLisa’s Story\n\nWhen retirement plan participants want to withdraw money from their accounts, many complex vesting rules and government regulations come into play. It gets worse if the participant has withdrawn money in the past. Working on a story to calculate a participant’s vested balance, my team members all had different ideas on the correct algorithm, even though the product owner had worked through",
      "content_length": 2432,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 474,
      "content": "Chapter 9, “Tool- kit for Business- Facing Tests that Support the Team,” talks about some tools that can help distrib- uted teams\n\nLisa’s Story\n\nFACILITATE COMMUNICATION\n\nseveral examples at the beginning of the iteration. My fellow tester, Mike, asked the product owner to work through a new example, and several programmers and testers joined the session. It took a couple of rather tortuous hours of writing numbers and ﬂowcharts on a whiteboard, but eventually they arrived at the cor- rect formula, and everyone was on the same page.\n\nWork through as many examples as you need until the team understands enough different aspects of the system. Try a different format if it’s not work- ing. For example, if pictures drawn on the whiteboard aren’t sufﬁcient to un- derstand the story, try spreadsheets or some other format that’s familiar to the business experts.\n\nDistributed Teams\n\nAs we’ve noted in other chapters, having team members in different locations and different time zones means you have to work harder at communication. Phones, email, and instant messaging form the basics of communication, but better collaboration tools are developed all the time.\n\nOne of the programmers on our team, who is also a manager, moved to India. Nanda works late into the evening there, so he’s available for the Denver team in the mornings. He has a cell phone with a local Denver phone number, so it’s easy to talk to him by phone as well as by instant message and email. We schedule meetings where we discuss stories, such as estimating meetings, brainstorming sessions, and iteration planning, early in the morning so he can participate. Al- though the team can’t be as productive as we were when we were co-located, we’re still able to beneﬁt from Nanda’s domain expertise and deep knowledge of the software.\n\nIf Nanda hires more team members in India, we may have to address more com- plex issues, such as coordinating integration and builds. We may consider more sophisticated technical solutions to communication problems.\n\nYou will need to experiment to see what works for your distributed team. Use retrospectives to evaluate whether collaboration and communication need improving, and brainstorm ways to improve. You, as a tester, may have a lot of experience in helping with process improvement projects. Just think about improving communication as one of those continual improvement needs.\n\n431\n\n—Lisa\n\n—Lisa",
      "content_length": 2418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 475,
      "content": "432\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nA Remote Tester’s Story\n\nSometimes, the testers are the remote team members. Erika Boyer of iLevel by Weyerhaeuser lives on the East Coast and works with a team in Denver. She’s a tester by profession, but on her team all tasks are up for grabs. She might write ﬁxtures to automate a FitNesse test or pair with a programmer to write production code. Being able to get in touch with people when she needs them is an issue. If she doesn’t get a response when she instant-messages a coworker, she phones; every work area in the Denver ofﬁce has a phone. It’s not foolproof, because everyone could be in the break room at a going-away party and forgot to tell her. Teams in different locations have to make a spe- cial effort to keep each other informed.\n\nBecause Erika starts working a few hours before the team’s daily stand-up, she needs work she can do alone during that time. She works with any team members who come in early in Denver and converses with other program- mers late in the day about work she’ll do the next morning.\n\nErika is able to see the team’s tasks using a tool on their intranet that shows each task, its status, and its percentage complete. With a few extra accom- modations, the team (which has other remote members) is able to keep up good communication.\n\nEven from a distance, Erika has been able to transfer testing skills to the pro- grammers but has found they think differently than testers. Her team uses these varying perspectives to their advantage by rotating all types of tasks among all of the team members.\n\nSuccessful teams keep remote members “in the loop” and share skills and ex- pertise. Distributed teams face extra challenges in successfully completing testing activities, but some minor adjustments, thoughtfulness on the part of all team members, and good communication tools help ensure that remote testers can be productive.\n\nWe all need to be able to communicate well with each other for our projects to succeed. When teams are in diverse geographic locations, they might have to work twice as hard to stay in constant touch.\n\nREGRESSION TESTS Unless you’re on a team that’s just starting its automation efforts, you have automated regression tests covering stories from previous iterations. Hope- fully, these are running as part of a continual build process, or at least part of a daily build process. If they aren’t, ask your team to make implementing this critical infrastructure a priority, and brainstorm with them how this might be done. Plan time in the next iteration to start a build process.",
      "content_length": 2596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 476,
      "content": "REGRESSION TESTS\n\nKeep the Build “Green”\n\nProgrammers should run all automated unit tests before checking in new code. However, unit tests may fail in the continual build, either because some- one forgot to run them before check-in, or because of a difference in runtime environment or IDE. We have unit tests for a reason, so whenever one fails, the team’s highest priority (apart from a showstopper production issue) should be to ﬁx it and get the build working again.\n\nTeams take different approaches to make sure their build stays “green.” Lisa’s team has a build process that emails results after every build. If the build fails, the person who checked in the failure usually ﬁxes it right away. If it’s not clear why the build failed, team members will get together to investigate. Their ScrumMaster has a stuffed toy that she puts on the desk of the person who “broke the build,” as a visual reminder that it has to be ﬁxed right away.\n\nSome teams use a trafﬁc light, ambient orb, GUI build monitoring tool, or other electronic visual way to show the build status. When the lights turn red, it’s time to stop new development and ﬁx the build. Another technique is to have a screen pop up in everyone’s IDE showing that the build has failed, and the popup won’t go away until you click “Ok, I’ll ﬁx the build.” Have some fun with it, but keeping the build running is serious business.\n\nIn extreme cases, you may have to temporarily comment out a failing test un- til it can be diagnosed, but this is a dangerous practice, especially for a novice team. Everyone on the team should stop what they’re doing if necessary until the build works again.\n\nKeep the Build Quick\n\nThe build needs to provide immediate feedback, so keep it short. If the build takes longer than the average frequency of code check-ins, builds start to stack up, and testers can’t get the code they need to test. The XP guideline for build time is ten minutes [Fowler, 2006]. Lisa’s team tries to keep the build less than eight minutes, because they check in so often.\n\nTests that take too long, such as tests that update the database, functional tests above the unit level, or GUI test scripts, should run in a separate build process. If the team is limited in hardware, they might have to run the “full” build with the full suite of tests at night and the “ongoing” build that has only unit tests continually during working hours. Having a separate, continual “full” build with all of the regression test suites is worth the investment. Lisa’s team gets feedback every 90 minutes from their “full” build, and this has\n\n433",
      "content_length": 2599,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 477,
      "content": "434\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nproven invaluable in heading off regression issues. This secondary suite of tests does not stop a programmer from checking in their code.\n\nBuilding a Regression Suite\n\nDuring the iteration, you’re automating new tests. As soon as these pass, add them to the regression suite, as appropriate. You may not need every edge case or permutation included in the regression suite, and you want to keep the regression suites fast enough to provide timely feedback. As each story is completed, tests that conﬁrm its functionality should be included in the re- gression suite and be part of the regular build cycle.\n\nThe regression tests themselves must be under some form of version control. It’s best to keep them in the same source code control system as the produc- tion code. That way, when you tag the code for production release, the tag also contains all of the versions of the tests that worked with the code. At minimum, keep a daily backup of the test code.\n\nWhen tests have been added to the regression suite, their purpose changes. They no longer exist to help drive development, and they are not expected to ﬁnd new bugs. There sole purpose in life is to detect unexpected changes or side effects in the system.\n\nChecking the “Big Picture”\n\nHopefully, you wrote task cards to test the story in the context of the larger ap- plication and regression test other parts of the system to ensure the new story hasn’t had a negative effect. You may have automated some of those end-to- end tests like the example in Chapter 12, “Summary of Testing Quadrants.”\n\nBut sometimes, even if you have a large suite of regression tests, manual ex- ploratory testing can be appropriate. The story isn’t “done” until you’ve com- pleted these tasks as well.\n\nRESOURCES As you start the iteration, make sure that test environments, test data, and test tools are in place to accommodate testing this iteration’s stories. Hope-",
      "content_length": 1954,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 478,
      "content": "Chapter 15, “Tester Activities in Release or Theme Planning,” talks about useful metrics to keep.\n\nITERATION METRICS\n\nfully you’ve anticipated these needs, but some requirements might only be- come obvious when you start working on a story. Collaborate with database experts, system administrators, and other team members to set up any addi- tional infrastructure needed.\n\nYou may have brought in outside resources for this iteration to help with per- formance, usability, security, or other forms of testing. Include them in stand-ups and discussions with the customers as needed. Pair with them and help them understand the team’s objectives. This is an opportunity to pick up new skills.\n\nITERATION METRICS In Chapter 5, “Transitioning Typical Processes,” we talked a bit about the purpose of metrics, but because metrics are critical to understanding how your coding and testing activities are progressing, we’ll delve into them more here. Know what problem you are trying to solve before you start measuring data points and going to all the work of analyzing the results. In this section, we’ll cover some of the typical measurements that teams gather through the iteration.\n\nMeasuring Progress\n\nYou need some way to know how much work your team has completed at any point in the iteration and an idea of how much work is left to do. You need to know when it becomes obvious that some stories can’t be completed and the team needs a Plan B. Iteration burndown charts and estimated versus actual time for tasks are examples used to measure team progress. They may or may not provide value for your particular team.\n\nStory or task boards are a good visual way to know the iteration’s status, es- pecially if color coding is used. If too many test task cards are still in the “to do” column or not enough coding task cards have been moved to “Done” or “Tested,” it’s time for the team to think of ways to make sure all of the testing is completed. Maybe some team members need to stop coding and start tak- ing on testing tasks, or maybe one story or a less critical part of a story needs to be put off until the next iteration so that testing for all the other stories can be ﬁnished.\n\n435",
      "content_length": 2192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 479,
      "content": "436\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nThis can be accomplished with virtual story boards as well as physical ones. Get creative with your visual effects so that problems are instantly visible. Re- member that no story is “done” until it’s tested at all appropriate levels. Teams may have other criteria for when a story is “done,” such as whether it has been peer reviewed or the automated regression tests are completed. On the story board shown in Figure 18-6, the “Done” column for each story row is the rightmost column. The column just to the left of it is the “Verify” col- umn. The story isn’t considered “done” until all the cards, including testing task cards, are in that “Done” column. A glance at the board is enough to know which stories are ﬁnished.\n\nEven teams that don’t track burndown at the task level can do so at the story level. Knowing how much work the team can do each iteration (its velocity) helps with the overall release plan, and the reprioritizing for each iteration. It simply may be enough to know the number of stories completed in an it- eration if they tend to average out to the same size. Although plans are ten- tative at best, it’s helpful to get an idea of about how many stories can be\n\nFigure 18-6 Story board showing iteration stories and tasks",
      "content_length": 1294,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 480,
      "content": "Janet’s Story\n\nITERATION METRICS\n\ncompleted by a hard release date or what stories might get done in the up- coming quarter.\n\nDefect Metrics\n\nWe talked about defect metrics in Chapter 15, “Tester Activities in Release or Theme Planning” giving you some high level ideas about what to track. Gathering metrics on defects can be very time consuming so always consider the goal before you start to measure. What is the purpose of the metrics you would like to gather? How long will you need to follow the trend before you know if they are useful?\n\nDefect containment is always a favorite metric to capture. When was the de- fect found? In traditional projects, it is much easier as you have “hard” re- quirements and coding phases. When the whole team is responsible for quality, and everyone is working together throughout, it is much harder to determine “when” the defect was injected into the system.\n\nWe would like to challenge the idea of this type of metric as not necessary in agile development. However, if you ﬁnd a lot of bugs are slipping through, you may want to start tracking what type of bugs they are so you can address the root cause. For example, if the bugs could have been caught with unit tests, then maybe the programmers need more training on writing unit tests. If the bugs are missed or misunderstood requirements, then maybe not enough time is spent in iteration planning, or acceptance tests aren’t detailed enough.\n\nIf you are practicing zero tolerance for defects, then you probably have no need to be tracking defects during coding and testing. A simple card on the story board will give you all the information you need.\n\nWhatever metrics you choose to measure, go for simplicity.\n\nIn one organization I was with, we tracked the number of defects logged in the DTS over several releases. These were defects that escaped the iteration or were found in the legacy system. Figure 18-7 shows the trend over a year and a half.\n\nAt the beginning, the number of issues found right after it was released to QA for ﬁnal testing was high (33 issues found in one month). The customers found even more issues during UAT which lasted over two months because they were not\n\n437",
      "content_length": 2191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 481,
      "content": "438\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nFigure 18-7 Sample Defect Trend (but stopped after a while)\n\nconﬁdent in the quality of the release. In the month that zero defects were re- ported, we were just starting a new release so there was no new functionality to test. Over the next year, fewer and fewer defects were logged and it becomes im- possible to tell where an actual release happened by just looking at the trend.\n\nThis graph was used to show customers that the team was becoming consistent with their testing and their releases. Once the team and customers had faith the numbers were not going up, the metrics were no longer needed and were dropped.\n\nDon’t be afraid to stop using metrics when they are no longer useful. If the problem they were initially gathered for no longer exists, there is no reason to keep gathering them.\n\nYour team may have to provide metrics to upper managers or a Project Man- agement Ofﬁce (PMO), especially if you work for a large organization. Patrick Fleisch, an Accenture Consultant who was working as a functional analyst at a\n\n—Janet",
      "content_length": 1083,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 482,
      "content": "ITERATION METRICS\n\nUseful Iteration Metrics\n\nConi Tartaglia, a software test manager at Primavera Systems, Inc., explains some ways she has found to achieve useful iteration metrics.\n\nCollecting metrics at the end of the iteration is particularly useful when many different teams are working on the same product releases. This helps ensure all teams end the iteration with the same standard for “done.” The teams should agree on what should be measured. What fol- lows are some standards for potentially shippable software [Schwaber 2004], and different ways of judging the state of each one.\n\nSprint deliverables are refactored and coded to standards.\n\nUse a static analysis tool. Focus on data that is useful and actionable. De- cide each sprint if corrective action is needed. For example, use an open source tool like FindBugs, and look for an increase each sprint in the number of priority one issues. Correct these accordingly.\n\nSprint deliverables are unit tested.\n\nFor example, look at the code coverage results each sprint. Count the number of packages with unit test coverage falling into ranges of 0%–30% (low coverage), 31%–55% (average coverage), and 56%–100% (high) coverage. Legacy packages may fall into the low coverage range, while coverage for new packages should fall into the 56%–100% range, if you are practicing test driven development. An increase in the high coverage range is desirable.\n\nSprint deliverables have passing, automated acceptance tests.\n\nMap automated acceptance tests to requirements in a quality manage- ment system. At the end of the iteration, generate a coverage report showing that all requirements selected as goals for the iteration have passing tests. Requirements that do not show passing test coverage are not complete. The same approach is easily executed using story cards on a bulletin board. The intent is simply to show that the agreed-upon tests for each requirement or story are passing at the end of the sprint.\n\nSprint deliverables are successfully integrated.\n\nCheck the continuous integration build test results to ensure they are passing. Run other integration tests during the sprint. Make corrections prior to the beginning of the next iteration. Hesitate to start a new itera- tion if integration tests are failing.\n\nSprint deliverables are free of defects.\n\nRequirements completed during the iteration should be free of defects.\n\nCan the product ship in [30] days?\n\n439",
      "content_length": 2435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 483,
      "content": "440\n\nCHAPTER 18\n\n(cid:2) CODING AND TESTING\n\nSimply ask yourself this question at the end of each iteration, and pro- ceed into the next iteration according to the answer.\n\nMetrics like this are easy to collect and easy to analyze, and can provide valu- able opportunities to help teams correct their course. They can also conﬁrm the engineering standards the teams have put in place to create potentially shippable software in each iteration.\n\nsoftware company during the time we wrote this book, gave us the following examples of metrics his team provides to their PMO.\n\n(cid:2) Test execution numbers by story and functional area (cid:2) Test automation status (number of tests automated vs. manual) (cid:2) Line graph of the number of tests passing/failing over time (cid:2) Summary and status of each story (cid:2) Defect metrics\n\nGathering and reporting metrics such as these may result in signiﬁcant over- head. Look for the simplest ways to satisfy the needs of your organization.\n\nSUMMARY At this point in our example iteration, our agile tester works closely with pro- grammers, customers, and other team members to produce stories in small testing-coding-reviewing-testing increments. Some points to keep in mind are:\n\n(cid:2) Coding and testing are part of one process during the iteration. (cid:2) Write detailed tests for a story as soon as coding begins. (cid:2) Drive development by starting with a simple test; when the simple tests pass, write more complex test cases to further guide coding. (cid:2) Use simple risk assessment techniques to help focus testing efforts. (cid:2) Use the “Power of Three” when requirements aren’t clear or opinions\n\nvary.\n\n(cid:2) Focus on completing one story at a time. (cid:2) Collaborate closely with programmers so that testing and coding are\n\nintegrated.\n\n(cid:2) Tests that critique the product are part of development. (cid:2) Keep customers in the loop throughout the iteration; let them review\n\nearly and often.\n\n(cid:2) Everyone on the team can work on testing tasks.",
      "content_length": 2027,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 484,
      "content": "SUMMARY\n\n(cid:2) Testers can facilitate communication between the customer team and\n\ndevelopment team.\n\n(cid:2) Determine what the best “bug ﬁxing” choice for your team is, but a\n\ngood goal is to aim to have no bugs by release time.\n\n(cid:2) Add new automated tests to the regression suite and schedule it to run\n\noften enough to provide adequate feedback.\n\n(cid:2) Manual exploratory testing helps ﬁnd missing requirements after all\n\nthe application has been coded.\n\n(cid:2) Collaborate with other experts to get the resources and infrastructure\n\nneeded to complete testing.\n\n(cid:2) Consider what metrics you need during the iteration; progress and\n\ndefect metrics are two examples.\n\n441",
      "content_length": 689,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 485,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 486,
      "content": "Chapter 19\n\nWRAP UP THE ITERATION\n\nIteration Demo\n\nWrap Up the Iteration\n\nStart, Stop, Continue\n\nCelebrate Successes\n\nRetrospectives\n\nIdeas for Improvement\n\nWe’ve completed an iteration. What do testers do as the team wraps up this iter- ation and prepares for the next? We like to focus on how we and the rest of our team can improve and deliver a better product next time.\n\nITERATION DEMO One of the pleasures of agile development is the chance to show completed stories to customers at the end of each iteration. Customers get to see a real, live, working application. They get to ask questions and give feedback. Every- one involved in the project, from both the business and technical sides, gets to enjoy a sense of accomplishment.\n\nOn Lisa’s team, the testers conduct the iteration review. Among all the team members, they’ve usually worked on the most stories. They have a natural role as information providers, and they have a good idea what the customers need to know about the new functionality. Having testers show off the deliv- erables is a common practice, although there is no hard and fast rule. The business experts on the team are a good choice for conducting the demo too, because they have the best understanding of how the software meets the business needs and they’ll feel greater ownership of the product. The Scrum- Master, a programmer, or a business analyst could demonstrate the new fea- tures and often does. Janet encourages rotating this honor.\n\n443",
      "content_length": 1480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 487,
      "content": "444\n\nCHAPTER 19\n\n(cid:2) WRAP UP THE ITERATION\n\nListening to the Customers\n\nPierre Veragen explains how his team uses iteration demonstrations.\n\n“We shut up and listen to our customers. It’s all about the chemistry of the group’s presentation. Somehow, sharing the moment brings brains together—we look at things from a different perspective. The event gives birth to ideas and concepts. Some die as the next person speaks; some live on and become that great idea that differentiates the product.”\n\nThe demo is a chance to show off the new stories, but the feedback custom- ers provide is the biggest reason to do them.\n\nAnyone may note the comments made by customers as they participate in the demo, but testers are good candidates. They may notice previously unde- tected inconsistencies as the demo progresses. As questions come up, cus- tomers might decide they want to change something minor, such as help text, or something bigger, such as how a feature behaves. Minor changes can usu- ally be made into tasks and dealt with in the next iteration, but some changes are big enough to turn into stories to plan into future releases.\n\nIteration demos (called sprint reviews in the Scrum world) are a super op- portunity to get everyone talking and thinking about the application. Take advantage of it. Review meetings are usually short and can be under half an hour. If there’s time left over after demonstrating new stories, ask customers if they’ve experienced any problems with the previous release that they haven’t reported. Do they have any general concerns, do they need help understand- ing how to use a feature, or have any new issues arisen? Of course, you can talk to customers anytime, but having most of the stakeholders in the room with the development team can lead to interesting ideas.\n\nRETROSPECTIVES Agile development means continually improving the way you work, and ret- rospectives are an excellent place to start identifying what and how you can do better. We recommend taking time at the end of each iteration and release cycle to look back and talk about what went well, what didn’t, and what you might like to try in the next iteration. There are different approaches for con- ducting retrospective sessions. No matter what approach you use, it’s key that each team member feels safe, everyone is respected, and there’s no ﬁnger- pointing or blame.",
      "content_length": 2377,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 488,
      "content": "Agile Retrospec- tives: Making Good Teams Great [2006] has imagi- native ideas for making retrospec- tives more pro- ductive (see the bibliography).\n\nRETROSPECTIVES\n\nThe whole idea is to make the process better, one baby step at a time.\n\nStart, Stop, Continue\n\nOne common exercise used in iteration retrospectives is “start, stop, continue.” The team asks itself: “What went well during this past iteration? What hap- pened that shouldn’t happen again? What can we start doing to help with things that didn’t go well?” Each team member can suggest things to start do- ing to improve, things to stop doing that weren’t working, and things that are helping that should be continued. A facilitator or ScrumMaster lists them on a whiteboard or big piece of paper. Post them in a location where everyone can read them again during the iteration. Figure 19-1 shows a “stop, start, and con- tinue” retrospective in progress. The ScrumMaster (standing) is writing stop, start, and continue suggestions on the big piece of paper on the story board.\n\nSome teams start this process ahead of time. All team members write “start,” “stop,” and “continue” items on sticky notes, and then during the retrospec- tive meeting they put the stickies on the board and group them by topic. “Start, stop, continue” is just one example of the terms you might use. Some other ideas are: “Things that went well,” “Things to improve,” “Enjoyable,” “Frustrating,” and “To Try.” Use whatever names that work for you. It can be\n\nFigure 19-1 A retrospective in progress\n\n445",
      "content_length": 1543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 489,
      "content": "446\n\nCHAPTER 19\n\n(cid:2) WRAP UP THE ITERATION\n\nhard to remember the past two weeks, much less an entire release, if that’s what your retrospective covers. Research different creative approaches to re- ﬂecting on your team’s experiences.\n\nHere’s a sample “stop, start, continue” list from Lisa’s team:\n\nStart: (cid:2) Sending out next sprint’s stories to us earlier. (cid:2) Don’t do lazy, single-record processing. Think of every service call as\n\na remote call.\n\n(cid:2) Communicate any database changes to everyone.\n\nStop: (cid:2) Accepting stories without complete requirements.\n\nContinue: (cid:2) Running FitNesse tests for the code you’re working on. (cid:2) Documenting what came up in meeting or informal discussions. (cid:2) Communicating better with each other. (cid:2) Showing mock-ups early. (cid:2) Doing FitNesse driven development.\n\nIf the list of “start, stop, continue” items is long, it’s a good idea to choose one or two to focus on for the new iteration. To prioritize the items, give each team member “n” votes they can assign to items. The ten people on Lisa’s team each get three votes, and they can apply them all to one item if they feel that’s most important, or they can vote for two or three different items. The items with the most votes are noted as the focus items. Janet has had success with this way of prioritizing as well.\n\nIn addition to “start, stop, continue” items, the team may simply write task cards for actions to be undertaken the next iteration. For example, if the ongo- ing build is too slow, write a card to “get ongoing build under ten minutes.”\n\nIn the next iteration, take some time to look at the one or two focus items you wanted to improve. At the end of that iteration, take a checkpoint to see if you improved. If not, ask why. Should you try something different? Is it still impor- tant? It could be it has dropped in importance or really wasn’t important in the big picture. If you thought you improved on a problem area and it resurfaces, you’ll have to decide to do something about it or else quit talking about it.",
      "content_length": 2074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 490,
      "content": "RETROSPECTIVES\n\nWe’ve found that retrospectives are a simple and highly effective way for teams to identify and address issues. The retrospective meeting is a perfect opportunity to raise testing-related issues. Bring up the issues in an objective, non-blaming way. The team can discuss each problem, what might be caus- ing it, and write down some ideas to ﬁx it.\n\nIdeas for Improvements\n\nLet’s take a look at some of those items that made it onto the list for improve- ment. Too many times, a team will identify really big issues but never follow up and actually do something about them. For example, maybe a lot of unit- level bugs are discovered after the programmers have claimed coding was complete.\n\nThe team may decide the programmers aren’t covering enough code with unit tests. They might write an action item to run the code coverage tool be- fore they check in new code, or start writing a “unit tests” task card for each story to make sure they’re completed. Perhaps the team didn’t ﬁnish all the test automation tasks before the iteration ended. As they discuss the prob- lem, the team ﬁnds that the initial executable tests were too complex, and they need to focus on writing and automating a simple test ﬁrst, or pair for a better test design. Make sure the action items are concrete.\n\nAgile teams try to solve their own problems and set guidelines to help them- selves improve. Action items aimed at one problem may help with others. When Lisa’s team had trouble ﬁnishing stories and getting them tested dur- ing each iteration, it came up with various rules over the course of a few retrospectives:\n\n(cid:2) Finish high-level test cases for all stories by the fourth day of the\n\niteration.\n\n(cid:2) Deliver one story to test by the fourth day of the iteration. (cid:2) Focus on ﬁnishing one story at a time. (cid:2) 100% of features must be checked in by close of business on the next-\n\nto-last day of the iteration.\n\nThese rules did more than help the team ﬁnish testing tasks. They facilitated a ﬂow and rhythm that helped the team work at a steady, sustainable pace over the course of each iteration.\n\nBegin the next retrospective meeting by reviewing the action items to see what items were beneﬁcial. Lisa’s team puts happy, sad, or neutral faces next to\n\n447",
      "content_length": 2282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 491,
      "content": "448\n\nCHAPTER 19\n\n(cid:2) WRAP UP THE ITERATION\n\nitems to denote whether the team tried them and found them successful. The team should ﬁgure out the reasons behind any sad faces. Were some items simply forgotten? Did time constraints keep the team from trying a new activ- ity? Did it just seem to be less of a good idea later? These discussions might lead to changing the improvement item or evolving it into a new one.\n\nWhen the actions for improvement become a habit to the team, they no longer need to be written on the “stop, start, and continue” list. “Start” items that work well may be moved to the “Continue” column. Some ideas don’t work, or prove to be unnecessary, and those can also be taken off the list for the next iteration.\n\nRefer to your ideas for improvement and action items during the iteration. Post them in a location (on a wall or online) where everyone sees them often. Lisa’s team sometimes goes through the list during a mid-iteration stand-up meeting. If you think of new improvement ideas during the iteration, write them down, possibly even on the existing list, so you won’t forget for the next iteration.\n\nIt’s a good idea to keep track of things that get in your way throughout the it- eration. Keep an impediment backlog on some big visible chart. Talk about the impediments in each iteration, and write task cards or take action to eliminate them.\n\nAn Approach to Process Improvement\n\nRafael Santos, VP of Software Development and Chief ScrumMaster at Ultimate Software, and Jason Holzer, the Chief PSR (Performance, Security, Reliability) Architect, explained to us that their teams found retrospectives that used the “stop, start, and continue” model ineffective. They made “stop, start, and continue” lists, but those didn’t provide enough focus to address issues.\n\nInstead, the ScrumMaster kept an impediment backlog, and the team found that worked better than retrospectives. Impediments may be related to test- ing or tools.\n\nSee the bibliogra- phy for good re- sources for lean development practices.\n\nThey also do value stream mapping to ﬁnd the biggest “wait time,” and use the “ﬁve whys” from Toyota to understand which impediment is the biggest or which constraint needs to be addressed.\n\nOne example shared was that in a team with three programmers and one tester, the biggest problem was a testing bottleneck. Rafael asked the team what the tester does and wrote those items on a whiteboard. Then he asked the programmers which of those things on the board they couldn’t do. There was only one item they felt they couldn’t handle. This helped the programmers",
      "content_length": 2607,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 492,
      "content": "CELEBRATE SUCCESSES\n\nunderstand how everyone on the development team, not only the testers, could be responsible for testing tasks. This was a highly effective exercise.\n\nCreative approaches like this help new agile teams tackle difﬁcult testing chal- lenges. Retrospectives are a good environment for experimenting.\n\nUse retrospectives as an opportunity to raise testing-related issues and get the whole team thinking about possible solutions. We’ve been pleasantly sur- prised with the innovative ideas that come out of an entire team focusing on how to improve the way it works.\n\nCELEBRATE SUCCESSES Agile development practices tend to moderate the highs and lows that exist in more traditional or chaotic processes. If your waterfall team ﬁnally man- ages to push a release out the door after a year-long cycle ending in a two- month stressful ﬁx-and-test cycle, everyone may be ready to celebrate the event with a big party—or they might just collapse for a couple of weeks. Ag- ile teams that release every two weeks tend to stay in their normal coding and testing groove, starting on the next set of stories after drawing just enough breath to hold an iteration review and retrospective. This is nice, but you know what they say about all work and no play.\n\nMake sure your team takes at least a little time to pat itself on the back and recognize its achievements. Even small successes deserve a reward. Enjoy- ment is a vital agile value, and a little motivation helps your team continue on its successful path. For some reason, this can be hard to do. Many agile teams have trouble taking time to celebrate success. Sometimes you’re eager to get going with the next iteration and don’t take time to congratulate your- selves on the previous accomplishments.\n\nLisa’s team ends an iteration every other Thursday and conducts its retro- spective, iteration review, and release the following day. After their meetings conclude, they usually engage in something they call “Friday Fun.” This sometimes consists of playing a silly trivia or board game, going out for a drink, or playing a round of miniature golf. Getting a chance to relax and have a good laugh has a team-building side beneﬁt.\n\nFor bigger milestones, such as a big release or achieving a test coverage goal, the whole company has a party to celebrate, bringing in catered food or going out\n\n449",
      "content_length": 2364,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 493,
      "content": "450\n\nCHAPTER 19\n\n(cid:2) WRAP UP THE ITERATION\n\nto a restaurant on Friday afternoon. This is a nice reward and recognizes for ev- eryone on both the business and technical teams.\n\nIf yours is a new agile team, motivate yourselves by rewarding small accom- plishments. Cheer the rising number of unit tests passing in each build. Oooh and aaah over the chart showing actual burn down matching the pro- jected burn down. Ring a bell when the broken unit tests in the build are ﬁxed (okay, that one might be annoying, but recognize it in some way.)\n\nCelebrate your individual successes, too. Congratulate your coworker for completing the project’s ﬁrst performance test baseline. Give your DBA a gold star for implementing a production back-up system. Give yourself a treat for solving that hard test-automation problem. Bring cookies to your next meeting with the customers. Recognize the programmer who gave you a JavaScript harness that sped up testing of some GUI validations. Use your imagination.\n\nThe Shout-Out Shoebox\n\nWe love the celebration idea we got from Megan Sumrell, an agile trainer and coach. She shared this with an agile testing Open Space session at Agile 2007.\n\nCelebrating accomplishments is something I am pretty passionate about on teams. On a recent project, we implemented the Shout-Out Shoe- box. I took an old shoebox and decorated it. Then, I just cut a slit in the top of the lid so people could put their shout-outs in the box. The box is open to the entire team during the course of the sprint.\n\nAnytime team members want to give a “shout-out” to another team member, they can write it on a card and put it in the box. They can range from someone helping you with a difﬁcult task to someone going above and beyond the call of duty. If you have distributed team members, en- courage them to email their shout-outs to your ScrumMaster who can then put them in the box as well.\n\nAt the end of our demo, someone from the team gets up and reads all of the cards out of the box. This is even better if you have other stakehold- ers at your demo. That way, folks on your team are getting public recog- nition for their work in front of a larger audience. You can also include small give-aways for folks, too.\n\nIt may be a cliché, but little things can mean a lot. The Shout-Out Shoebox is a great way to recognize the value different team members contribute.",
      "content_length": 2381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 494,
      "content": "SUMMARY\n\nTaking time to celebrate successes lets your team take a step back, get a fresh perspective, and renew its energy so it can keep improving your product. Give team members a chance to appreciate each other’s contributions. Don’t fall into a routine where everyone has their head down working all the time.\n\nIn agile development, we get a chance to stop and get a new perspective at the end of each short iteration. We can make minor course corrections, decide to try out a new test tool, think of better ways to elicit examples from custom- ers, or identify the need for a particular type of testing expertise.\n\nSUMMARY\n\n(cid:2) In this chapter, we looked at some activities to wrap up the iteration\n\nor release.\n\n(cid:2) The iteration review is an excellent opportunity to get feedback and\n\ninput from the customer team.\n\n(cid:2) Retrospectives are a critical practice to help your team improve. (cid:2) Look at all areas where the team can improve, but focus on one or two\n\nat a time.\n\n(cid:2) Find a way to keep improvement items in mind during the iteration. (cid:2) Celebrate both big and small successes, and recognize the contribu-\n\ntions from different roles and activities.\n\n(cid:2) Take advantage of the opportunity after each iteration to identify testing-related obstacles, and think of ways to overcome them.\n\n451",
      "content_length": 1334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 495,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 496,
      "content": "Chapter 20\n\nSUCCESSFUL DELIVERY\n\nWhat Makes a Product?\n\nProduction Support\n\nCustomer Expectations\n\nPlanning Enough Time for Testing\n\nRelease Acceptance Criteria\n\nRelease Management\n\nReleasing the Product\n\nTesting the Release Candidate\n\nPackaging\n\nTesting on a Staging Environment\n\nFinal Nonfunctioning Testing\n\nSuccessful Delivery\n\nEnd Game\n\nIntegration with External Applications\n\nData Conversion, Database Updates\n\nInstallation Testing\n\nDeliverables\n\nCommunication\n\nPost-Development Testing Cycles\n\nWhat If It’s Not Ready?\n\nUAT\n\nCustomer Testing\n\nAlpha/Beta Testing\n\nIn this chapter, we share what you as a tester can do to help your team and your organization successfully deliver a high-quality product. The same process and tools can be used for shrink-wrapped products, customized solutions, or internal development products. Agile testers can make unique contributions that help both the customer and developer team deﬁne and produce the value that the business needs.\n\nWHAT MAKES A PRODUCT? Many of the books on agile development talk about the actual development cycle but neglect to talk about what makes a product and what it takes to suc- cessfully deliver that product. It’s not enough to just code, test, and say it’s\n\n453",
      "content_length": 1236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 497,
      "content": "454\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\ndone. It’s like buying something from a store: If there is great service to go with the purchase, how much more likely are you to go back and buy there again?\n\nJanet’s Story\n\nI was talking to my friend, Ron, who buys and sells coins. Over the years he has de- veloped a very good reputation in the industry and has turned away prospective clients because he is so busy.\n\nWhen I asked him his secret, he said, “It’s not a secret. I just work with my custom- ers to make them feel comfortable and establish a trusting relationship with them. In the end, both I and my customer need to be happy with the deal. It only takes one unhappy customer to break my reputation.”\n\nAgile teams can learn from Ron’s experience. If we treat our customers with re- spect and deliver a product they are happy with, we will have a good relationship with them, hopefully for many years.\n\nOur goal is to deliver value to the business in a timely manner. We don’t want just to meet requirements but also to delight our customers. Before we re- lease, we want to make sure all of the deliverables are ready and polished up appropriately. Hopefully, you started planning early to meet not only the code requirements but to plan for training, documentation, and everything that goes into making a high-value product.\n\nFit and Finish\n\nConi Tartaglia, a software test manager with Primavera Systems, Inc., explains “ﬁt and ﬁnish” deliverables.\n\nIt is helpful to have a “Fit and Finish” checklist. Sometimes ﬁt and ﬁnish items aren’t ready to be included in the product until close to the end. It may be necessary to rebuild parts of the product to include items such as new artwork, license or legal agreements, digital signatures for execut- ables, copyright dates, trademarks, and logos.\n\nIt is helpful to assemble these during the last full development iteration and incorporate them into the product while continuous integration build cycles are running so that extra builds are not needed later.\n\nBusiness value is the goal of agile development. This can include lots beyond the production code. Teams need to plan for all aspects of product delivery.\n\n—Janet",
      "content_length": 2184,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 498,
      "content": "PLANNING ENOUGH TIME FOR TESTING\n\nImagine yourself in the middle of getting your release ready for production. You’ve just ﬁnished your last iteration and are wrapping up your last story test. Your automated regression suite has been running on every new build, or at least on every nightly build. What you do now will depend on how dis- ciplined your process has been. If you’ve been keeping to the “zero tolerance” for bugs, you’re probably in pretty good shape.\n\nIf you’re one of those teams that thinks you can leave bugs until the end to ﬁx, you’re probably not in such good shape and may need to introduce an it- eration for “hardening” or bug ﬁxes. We don’t recommend this, but if your team has a lot of outstanding bugs that have been introduced during the de- velopment cycle, you need to get those addressed before you go into the end game. We ﬁnd that new teams tend to fall into this trap.\n\nIn addition, there are lots of varied components to any release, some in the software, some not. You have customers who need to install and learn to use the new features. Think about all those elements that are critical to a success- ful release, because it’s time to wrap up all those loose ends and hone your product.\n\nBob Galen, an agile coach and end-game expert, observes that agile develop- ment may not have seeped into every organizational nook and cranny. He notes, “Agile testers can serve as a conduit or facilitator when it comes to physical delivery of the software.”\n\nPLANNING ENOUGH TIME FOR TESTING Because testing and coding are part of one process in agile development, we’d prefer not to make special plans for extra testing time, but in real life we might need some extra time.\n\nMost teams accumulate some technical debt, despite the best intentions, espe- cially if they’re working with legacy code. To maintain velocity, your team may need to plan a refactoring iteration at regular intervals to add tests, upgrade tools, and reduce technical debt. Lisa’s team conducts a refactoring sprint about every six months. While the business doesn’t usually receive any direct beneﬁts at the end of a refactoring sprint, the business experts understand that these special sprints result in better test coverage, a solid base for future development, reduced technical debt, and a higher overall team velocity.\n\n455",
      "content_length": 2330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 499,
      "content": "456\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nSome teams resort to “hardening” iterations, where they spend time only ﬁnd- ing and ﬁxing bugs, and they don’t introduce any new functionality. This is a last resort for keeping the application and its infrastructure solid. New teams may need an extra iteration to complete testing tasks, and if so, they budget time for that in the release plan.\n\nUse retrospectives and other process improvement practices to learn ways to integrate testing and coding so that the code produced in each iteration is production-ready. When that goal is achieved, work to ensure that a stable build that could be released to production is available every day. Lisa’s team members thought that this was an unattainable goal in the days when they struggled to get any stable build before release, but it was only a couple of years before almost every build was release-worthy.\n\nWhen your build is stable, you are ready to enter the “End Game.”\n\nTHE END GAME What is the end game? We’ve heard people call the time right before delivery many things, but the “end game” seems to ﬁt best. It’s the time when the team applies the ﬁnishing touches to the product. You’re dotting your i’s and crossing your t’s. It’s the last stretch before the delivery ﬁnish line. It’s not meant to be a bug-ﬁx cycle, because you shouldn’t have any outstanding bugs by then, but that doesn’t mean you might not have one or two to ﬁx.\n\nYou might have groups in your organization that you didn’t involve in your earlier planning. Now it’s time to work closely with the folks that administer the staging and production environments, the conﬁguration managers, the database administrators outside of your team, and everyone who plays a role in moving the software from development to staging and production. If you weren’t working with them early this time, consider talking to these folks during your next release planning sessions, and keep in touch with them throughout the development cycle.\n\nBob Galen tells us that the testers on his team have partnered with the opera- tions group that manages the staging and production environments. Because the operations group is remote, it ﬁnds that having guidance from the agile team is particularly valuable.\n\nThere are always system-level tests that can’t be automated, or are not worth automating. More often than not, your staging environment is the only place",
      "content_length": 2413,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 500,
      "content": "Lisa’s Story\n\nTHE END GAME\n\nwhere you can do some system-level integration tests or system-level load and stress testing. We suggest that you allot some time after development for these types of ﬁnishing tasks. Don’t code right up to the end.\n\nPlan as much time for the end game as you need. Janet has found that the length of time needed for the end game varies with the maturity of the team and the size of the application. It may be that only one day is needed to ﬁnish the extra tasks, but it may be one week or sometimes as much as a whole two-week iteration. The team from the example used in Chapter 12, “Sum- mary of Testing Quadrants,” scheduled two weeks, because it was a complex system that required a fair bit of setup and system testing.\n\nWhen I worked on a team developing applications for a client, we had to follow the client’s release schedule. Testing with other parts of the larger system was only possible during certain two-week windows, every six or eight weeks. Our team completed two or three iterations, ﬁnishing all of the stories for each as if they were releasing each iteration.\n\nThen we entered a testing window where we could coordinate system testing with other development teams, assist the client with UAT, and plan the actual release. This constituted our end game.\n\nIf you have a large organization, you might have ten or ﬁfteen teams develop- ing software for individual products or for separate areas of functionality for the same application. These areas or products may all need to release to- gether, so an integrated end game is necessary. This does not mean that you leave the integration until the very end. Coordination with the other teams will be critical all along your development cycle, and if you have a test inte- gration system, we recommend that you be sure that you have tried to inte- grate long before the end game.\n\nYou also may have considerations beyond your team, for example, working with software delivered by external teams at the enterprise level.\n\nUse this end-game time to do some ﬁnal exploratory testing. Step back and look at the whole system and do some end-to-end scenarios. Such testing will conﬁrm that the application is working correctly, give you added conﬁdence in the product, and provide information for the next iteration or release.\n\n457\n\n—Lisa",
      "content_length": 2327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 501,
      "content": "458\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nTesting the Release Candidate\n\nWe recommend that the automated regression testing be done against every release candidate. If you’re following our recommendation to run automated regression tests continually on each new build, or at least daily, you’ve already done this. If some of your regression tests are manual, you’ll need to plan time for those or they might not get done. A risk assessment based on changes made to each build will determine what tests need to be run if there is more than one release candidate.\n\nTest on a Staging Environment\n\nWhether you are using traditional or agile development processes, a staging en- vironment that mimics production is vital for ﬁnal testing before release, as well as for testing the release process itself. As part of the end game, your applica- tion should be deployed to staging just like you would deploy it to production, or as your customers would on their environments. In many organizations that Janet has seen, the staging environment is usually shared among multiple projects, and the deployment must be scheduled as part of the release plan- ning. Consider ahead of time how to handle dependencies, integrating with other teams using the staging environment, and working with external third parties. It might feel like “traditional” test planning, but you might be dealing with teams that haven’t embraced agile development.\n\nAlthough agile promotes continuous integration, it is often difﬁcult to inte- grate with third-party products or other applications outside your project’s control. Staging environments can have better controls so that external appli- cations may connect and have access tothird-party test environments. Stag- ing environments can also be used for load and performance testing, mock deploys, fail-over testing, and manual regression tests and exploratory func- tional testing. There are always conﬁguration differences between environ- ments so your staging environment is a good place to test these.\n\nFinal Nonfunctional Testing\n\nLoad testing should be scheduled throughout the project on speciﬁc pieces of the application that you are developing. If your staging environment is in high demand, you may not be able to do full system load testing until the end game.\n\nBy this time, you should be able to do long-running reliability tests on all product functionality. Check for crashes and degradation of performance with normal load. When done at release time, it should be a ﬁnal conﬁrma- tion only.",
      "content_length": 2534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 502,
      "content": "Janet’s Story\n\nTHE END GAME\n\nFault tolerance and recovery testing is best done on your staging environment as well, because test environments usually don’t have the necessary setup. For these same reasons, you may only be able to test certain aspects of security. One example is https, a secure http connection through encrypted secure sockets. Some organizations may choose to have the necessary certiﬁcates on their staging environment only. Other examples are clustering or data replica- tion. Make sure you involve all parties who need to be included in this testing.\n\nIntegration with External Applications\n\nYour team may be agile, but other product teams in your organization, or third parties your team works with, may not be.\n\nIn one organization that I worked with, the third-party partner that approved credit cards had a test account that could be used, but it was only accessible from the staging environment.\n\nTo test during development, test stubs were created to return speciﬁed results depending on the credit card number used. However, this wasn’t sufﬁcient be- cause the third party sometimes changed functionality on its end that we weren’t aware of. Testing with the actual third party was critical to the success of the project, and it is a key part of the end game.\n\nCoordinate well in advance with other product teams or outside partners that have products that need to integrate with your product. If you have identiﬁed these risks early and done as much up-front testing as possible, the testing done during the end game should be ﬁnal veriﬁcation only. However, there are always last-minute surprises, so you may need to be prepared to make changes to your application.\n\nTools like simulators and mock objects used for testing during development can help alleviate some of the risks, but the sooner you can test with external applications, the lower the risk.\n\nData Conversion and Database Updates\n\nAs we are developing an application, we change ﬁelds, add columns in the da- tabase, or remove obsolete ones. Different teams tackle this in different ways. Some teams re-create the database with each new build. This works for new applications, because there is no existing data. However, after an application exists in production and has associated data, this approach won’t work.\n\n459\n\n—Janet",
      "content_length": 2319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 503,
      "content": "460\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nAn application needs to consider the data that is part of the product. As with so much in agile development, a joint effort by database experts, program- mers, and testers on the team is required to ensure successful release of data- base changes. Janet has seen a couple of different tactics for dealing with data conversion and backward compatibility. Database scripts can be created by the developers or database administrators as the team makes changes. These scripts become part of the build and are continually tested. Another option is for the team to run “diffs” on the database after all of the database changes have been made.\n\nIf you’re a tester, ask your database administrator/developer to help your team ensure that schemas are kept consistent among the production, test- ing, and staging environments. Find a way to guarantee that all changes made in the test environments will be done in the staging and production environments during release. Keep the schemas matching (except for the new changes still under development) in terms of column names, triggers, constraints, indices, and other components. The same discipline applied to coding and testing also should be applied to database development and maintenance.\n\nLisa’s Story\n\nWe recently had a bug released to production because some of the test schemas, including the one used by regression tests, were missing a constraint. Without the constraint in place, the code didn’t fail. This triggered an effort to make sure the exact same update scripts get run against each schema to make changes for a given release.\n\nIt turned out that different test schemas had small differences, such as old col- umns still remaining in some or columns in different order in different schemas, so it wasn’t possible to run the same script in every environment. Our database ad- ministrator led a major effort to re-create all of the test schemas to be perfectly compatible with production. He creates one script in each iteration with all nec- essary database changes and runs that same script in the staging and production environment when we release. This seems simple, but it’s easy to miss subtle dif- ferences when you’re focused on delivering new features.\n\nAutomating data migrations enhances your ability to test them and reduces the chance for human error. Native database tools such as SQL, stored proce- dures, data import tools such as SQL*Loader and bcp, shell scripts, and Win- dows command ﬁles can be used for automation because they can be cloned and altered easily.\n\n—Lisa",
      "content_length": 2593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 504,
      "content": "Lisa’s Story\n\nTHE END GAME\n\nNo matter how the database update and conversion scripts are created or maintained, they need to be tested. One of the best ways to ensure all of the changes have been captured in the update scripts is to use the customer’s data if it is available. Customers have a habit of using the application in weird and wonderful ways, and the data is not always as clean as we would like it. If the development team cleans up the database and puts extra restrictions on a column, the application on the customer’s site might blow up as soon as a query touches a piece of data that does not match the new restrictions. You need to make sure that any changes you’ve made are still compatible with ex- isting data.\n\nMy team uses the staging environment to test the database update scripts. After the scripts are run, we do manual testing to verify that all changes and data con- versions completed correctly. Some of our GUI test scripts cover a subset of re- gression scenarios. This gives us conﬁdence about releasing to production, where our ability to test is more limited.\n\nWhen planning a data conversion, think about data cleanup as part of the mitigation strategy. You have the opportunity to take the data that was en- tered in some of the “weird and wonderful” ways we mentioned before and massage or manipulate it so it conforms to the new constraints. This type of job can take a long time to do but is often very worthwhile in terms of main- taining data integrity.\n\nNot everyone can do a good enough simulation of production data in the staging environment. If a customer’s data is not available, a mitigation strat- egy is to have a UAT at the customer site. Another way to mitigate risk is to try to avoid large-scale updates and release in smaller stages. Develop new functionality in parallel with the old functionality and use a system property to “turn on” one or the other. The old functionality can continue to work in production until the new functionality is complete. Meanwhile, testing can be done on the new code at each iteration. New columns and tables can be added to production tables without affecting the old code so that the data migration or conversion for the ﬁnal release is minimized.\n\nInstallation Testing\n\nOrganizations often have a separate team that deploys to production or cre- ates the product set. These team members should have the opportunity to\n\n461\n\n—Lisa",
      "content_length": 2421,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 505,
      "content": "462\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\npractice the deployment exactly as they would for production. If they use the deployment to staging as their proving ground, they can work out any of the problems long before they release to the customer.\n\nTesting product installations can also mean testing various installations of shrink-wrapped products to different operating systems or hardware. How does the product behave? Does it do what is expected? How long will the sys- tem need to be down for installation? Can we deploy without taking an out- age? Can we make the user experience as pleasant as possible?\n\nJanet’s Story\n\nI had an experience a while ago that was not so pleasant, and it led me to wish that someone had tested and ﬁxed the issue before I found it. I bought a new lap- top and wanted to transfer my license for one of my applications to the new com- puter. It came with a trial version of the same application, so the transfer should have been easy, but the new PC did not recognize the product key—it kept say- ing it was invalid. I called the support desk and after a bit of diagnostics, I was in- formed they were considered different products, so the key wouldn’t work.\n\nTwo more hours of support time, and the issue was ﬁxed. The trial version had to be removed, an old version had to be reinstalled, the key had to be reentered, and all updates since the original purchase had to be uploaded. How much easier would it have been for the development team to test that scenario and offer the customer an informative message saying, “The trial version is not compatible with your product key.” A message such as that would have let me ﬁgure out the prob- lem and solve it myself rather than taking the support person’s time.\n\nTake the time you need to determine what your requirements are for testing installation. It will be worth it in the end if you satisfy your customers.\n\nCommunication\n\nConstant communication between different development team members is always important, but it’s especially critical as we wrap up the release. Have extra stand-up meetings, if needed, to make sure everything is ready for the release. Write cards for release tasks if there’s any chance some step might be forgotten.\n\nLisa’s Story\n\nMy team releases after each iteration. We usually have a quick stand-up on the last afternoon of the sprint to touch base and identify any loose ends. Before the team had a lot of practice with releases, we wrote release task cards such as “run database update script in staging” and “verify database updates in production.”\n\n—Janet",
      "content_length": 2583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 506,
      "content": "THE END GAME\n\nWith more experience at deploying, we no longer need those cards unless we have a new team member who might need an extra reminder. It never hurts to have cards for release tasks, though.\n\nReminders of tasks, whether they are in a full implementation plan or just written on task cards as Lisa’s team does, are often necessary. On simple im- plementations, a whiteboard works well.\n\nWhat If It’s Not Ready?\n\nBy constantly tracking progress in many forms, such as builds, regression test suites, story boards, and burndown charts, a team usually knows well in ad- vance when it’s in trouble on a release. There’s time to drop stories and read- just. Still, last-minute disasters can happen. What if the build machine breaks on the last day of the iteration? What if the test database crashes so that ﬁnal testing can’t be completed? What if a showstopper bug isn’t detected until ﬁ- nal functional testing?\n\nWe strongly advise against adding extra days to an iteration, because it will eat into the next iteration or release development. An experienced team might be ﬂexible enough to do this, but it can derail a new team. Still, des- perate times call for desperate measures. If you release every two weeks, you may simply be able to skip doing the actual release, budget time into the next iteration to correct the problems and ﬁnish up, and release on the next scheduled date. If testing tasks are being put off or ignored and the release goes ahead, bring up this issue with the team. Did the testing needs change, or is the team taking a chance and sacriﬁcing quality to meet a deadline? The team should cut the release scope if the delivery date is ﬁxed and in jeopardy.\n\nIf your release cycle is longer, more like three months, you should know in advance if your release is in jeopardy. You probably have planned an end game of at least two weeks, which will just be for ﬁnal validation. When you have a longer release cycle, you have more time to determine what you should do, whether it’s dropping functionality or changing the schedule.\n\nIf your organization requires certain functionality to be released on a ﬁxed day and last-minute glitches threaten the release, evaluate your alternatives. See if you can continue on your same development cycle but delay the release itself for a day or a week. Maybe the offending piece of code can be backed out temporarily and a patch done later. The customers have the ultimate say in what will work for the business.\n\n463\n\n—Lisa",
      "content_length": 2494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 507,
      "content": "464\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nLisa’s Story\n\nOn the rare occasions when our team has faced the problem of last-minute show- stoppers, we’ve used different approaches according to the situation. If there’s nothing critical that has to be released right now, we sometimes skip the release and release two iterations’ worth on the next release day. If something critical has to go in, we delay the release a day or two. Sometimes we can go ahead and re- lease what we have and do a patch release the next day. On one occasion, we decided to have a special one-week iteration to correct the problems, release, and then go back to the normal two-week iteration schedule.\n\nAfter more than four years of practicing agile development, we have a stable build almost 100% of the time, and we feel conﬁdent about being able to release whenever it’s necessary. We needed a lot of discipline and continual improvement to our process in order to feel that a more ﬂexible approach could work for us. It’s also nice to be able to release a valuable bit of functionality early, if we can. What we’ve worked hard to avoid is falling into a death spiral where we can never re- lease on schedule and we’re always playing catch-up.\n\nDon’t beat yourself up if you can’t release on time. Your team is doing its best. Do spend time analyzing why you got behind schedule, or over-committed, and take action to keep it from happening again.\n\nWork to prevent a “no go” situation with good planning, close collaboration, driving coding with tests, and testing as you code. If your tracking shows the release could be in jeopardy, remove the functionality that can’t be ﬁnished, if possible. If something bad and unexpected happens, don’t panic. Involve the whole team and the customer team, and brainstorm about the best solution.\n\nCUSTOMER TESTING There are a couple of different ways in which to involve your customers to get their approval or feedback. User Acceptance Testing can be fairly formal, with sign-offs from the business. It signiﬁes acceptance of a release. Alpha or beta testing is a way to get feedback on a product you are looking to release but which is not quite ready.\n\nUAT\n\nUser Acceptance Testing (UAT) is important in large customized applications as well as internal applications. It’s performed by all affected business depart- ments to verify usability of the system and to conﬁrm existing and new (em- phasis on new) business functionality of the system. Your customers are the\n\n—Lisa",
      "content_length": 2500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 508,
      "content": "Janet’s Story\n\nCUSTOMER TESTING\n\nones who have to live with the application, so they need to make sure it works on their system and with their data.\n\nIn previous chapters we’ve often talked about getting the customers involved early, but at those times, the testing is done on speciﬁc features under develop- ment. UAT is usually done after the team decides the quality is good enough to release. Sometimes though, the timeline dictates the release cycle. If that is the case, then try moving the UAT cycle up to run parallel with your end game. The application should be stable enough so that your team could deploy to the customer’s test system at the same time as they deploy to staging.\n\nIn one team I joined, the customers were very picky. In fact, the pickiest I had ever seen. They always asked for a full week of UAT just to be sure they had the time to test it all. They had prepared test cases and checked them all, including all the content, both in English and in French. Showstopper bugs included spelling errors such as a missing accent in the French content. Over time, as they gained more conﬁdence in our releases and found fewer and fewer errors, they relaxed their demands but still wanted a week, just in case they couldn’t get to it right away. Their business group was very busy.\n\nOne release came that pushed the timeline. We were being held to the release date but couldn’t get all the functionality in and leave two weeks for the end game. We talked with the business users and we decided to decrease the end game to one week; the business users would perform their UAT while the project team ﬁnished up their system testing and cleanup. The only reason we were able to do this was because of the trust the customer had in our team and the consis- tency of our releases.\n\nThe good news was that, once again, the UAT found no issues that could not wait until the next release.\n\nFigure 20-1 shows an example timeline with a normal UAT at the end of the release cycle. The team starts working on the next release, doing release plan- ning, and starts the ﬁrst iteration with all team members ready to go.\n\nWork with customers so that they understand the process, their role, and what is expected of them. If the UAT is not smooth, then the chances are there will be a high level of support needed. An experienced customer test team may have deﬁned test cases, but most often its testing is ad hoc. Customers may approach their testing as if they were doing their daily job but will probably focus on the new functionality. This is an opportunity to observe how people\n\n465\n\n—Janet",
      "content_length": 2602,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 509,
      "content": "466\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nRelease Timeline Normal Release with UAT\n\nIter 2\n\nIter 3\n\nEnd Game\n\n. . .\n\nRelease 1\n\nCode and Test Code and Test\n\nSystem Test\n\nUAT\n\nIter 1\n\nIter 2\n\n. . .\n\nRelease 2\n\nRelease Planning\n\nCode and Test Code and Test\n\nFigure 20-1 Release timeline with UAT\n\nuse the system and to get feedback from them on what works well and what improvements would help them.\n\nTesters can provide support to the customers who are doing the UAT by re- viewing tests run and defects logged, and by tracking defects to completion. Both of us have found it helpful to provide customers involved in doing UAT with a report of all of the testing done during development, along with the results. That helps them decide where to focus their own testing.\n\nAlpha/Beta Testing\n\nIf you are an organization that distributes software to a large customer base, you may not have a formal UAT. You are much more likely to incorporate al- pha or beta testing. Your team will want to get feedback on new features from your real customers, and this is one mechanism for doing so. Alpha testing is early distribution of new versions of software. Because there are likely to be some major bugs, you need to pick your customers wisely. If you choose this method of customer feedback, make sure your customers understand their role. Alpha testing is to get feedback on the features—not to report bugs.\n\nBeta testing is closer to UAT. It is expected that the release is fairly stable and can actually be used. It may not be “ready for prime time” for most custom- ers, but many customers may feel the new features are worth the risk. Cus-",
      "content_length": 1640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 510,
      "content": "Lisa’s Story\n\nPOST-DEVELOPMENT TESTING CYCLES\n\ntomers should understand that it is not a formal release and that you are asking them to test your product and report bugs.\n\nAs a tester, it is important to understand how customers view the product, because it may affect how you test. Alpha and beta testing may be the only time you get to interact with end users, so take advantage of the chance to learn how well the product meets their needs.\n\nPOST-DEVELOPMENT TESTING CYCLES If you work in a large organization or are developing a component of a large, complex system, you may need to budget time for testing after development is complete. Sometimes the UAT testing, or the test coordination, isn’t as smooth as it could be, so the timeline stretches out. Test environments that include test versions of all production systems may only be available for small, scheduled windows of time. You may need to coordinate test sessions with teams working on other applications that interact with yours. Whatever the reason, you need extra testing time that does not include the whole development team.\n\nI worked on a team developing components of both internal and external appli- cations for a large telecom client. We could only get access to the complete test environment at scheduled intervals. Releases were also tightly scheduled.\n\nThe development team worked in two-week iterations. It could release to the test environment only after every third iteration. At that time, there was a two-week system integration and user acceptance test cycle, followed by the release.\n\nSomeone from my team needed to direct the post-development testing phase. Meanwhile, the developers were starting a new iteration with new features, and they needed a tester to help with that effort.\n\nThe team had to make a special effort to make sure someone in the tester role fol- lowed each release from start to ﬁnish. For example, I worked from start to ﬁnish on release 1. Shauna took over the tester role as the team started work on the ﬁrst iteration of release 2, while I was coordinating system testing and UAT on re- lease 1. Shauna stayed as primary tester for release 2, while I assumed that role for release 3.\n\nFigure 20-2 shows an example timeline where the UAT was extended. This could happen for any number of reasons, and the issue may not always be UAT. Most of the team is ready to start working on the next release, but often\n\n467\n\n—Lisa",
      "content_length": 2431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 511,
      "content": "468\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nRelease Timeline Overlapping Releases with Extended UAT\n\nTester stays with this release until UAT is completed\n\nIter 2\n\nIter 3\n\nEnd Game\n\n. . .\n\nRelease 1\n\nCode and Test Code and Test\n\nSystem Test\n\nUAT\n\nIf no tester available to work on new release, programmers can do refactoring, spikes\n\nIter 1\n\nIter 2\n\n. . .\n\nRelease 2—Alternative 1\n\nRelease Planning\n\nRefactoring, Spikes\n\nCode and Test\n\nOther tester works on next release\n\nIter 0\n\nIter 1\n\n. . .\n\nRelease 2—Alternative 2\n\nRelease Planning\n\nCode and Test Code and Test\n\nFigure 20-2 Release timeline—alternative approach with extended UAT\n\na tester is still working with customers, completing ﬁnal testing. Sometimes a programmer will be involved as well. There are a couple of options. If the team is large enough, you can probably start the next release while a couple of team members work with the existing release (Release 2—Alternative 2 in Figure 20-2). If you have a small team, you may need to consider an Iteration 0 with programmers doing refactoring or spikes (experiments) on new func- tionality so that the tester working with the customer does not get left behind (Release 2—Alternative 1 in Figure 20-2).\n\nBe creative in dealing with circumstances imposed on your team by the reali- ties of your project. While plans rarely work as expected, planning ahead can still help you make sure the right people are in place to deliver the product in a timely manner.\n\nDELIVERABLES In the ﬁrst section of this chapter we talked about what makes a product. The answer to this will actually depend on the audience: Who is accepting the product, and what are their expectations?",
      "content_length": 1680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 512,
      "content": "DELIVERABLES\n\nIf your customers need to meet SOX (Sarbanes-Oxley)compliance require- ments, there will be certain deliverables that are required. For example, one cus- tomer Janet has worked with felt test results should be thoroughly documented, and made test results one of their SOX compliance measurement points, while a different customer didn’t measure test results at all. Work with compliance and audit personnel to identify reporting needs as you begin a project.\n\nHow much documentation is enough? Janet always asks two questions before answering that question: “Who is it for?” and “What are they using it for?” If there are no adequate answers to those questions, then consider whether the documentation is really needed.\n\nDeliverables are not always for the end customer, and they aren’t always in the form of software. There are many internal customers, such as the produc- tion support team members. What will they need to make their job easier? Workﬂow diagrams can help them understand new features. They would probably like to know if there are work-arounds in place so they can help customers through problems.\n\nJanet often gets asked about test coverage of code, usually by management. How much of the application is being tested by the unit tests or regression tests? The problem is that the number by itself is just a number, and there are so many reasons why it might be high or low. Also, code coverage doesn’t tell you about features that might have been missed, for which no code exists yet. The audience for a deliverable such as code coverage should not be manage- ment, but the team itself. It can be used to see what areas of the code are not being tested.\n\nTraining could be considered a deliverable as well. Many applications require customized training sessions for customers. Others may only need online help or a user manual. Training could determine the success of your product, so it’s important to consider. Lisa’s team often writes task cards for either a tester or the product owner to make sure training materials and sessions are arranged. Some people may feel training isn’t the job of testers or anyone else on the development team. However, agile teams aim to work as closely as possible with the business. Testers often have the domain expertise to be able to at least identify training that might be needed for new or updated features. Even if training isn’t the tester’s responsibility, she can raise the issue if the business isn’t planning training sessions.\n\nMany agile teams have technical writers as part of the team that write online help or electronic forms of documentation. One application even included\n\n469",
      "content_length": 2665,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 513,
      "content": "470\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\ntraining videos to help get started, and different members of the team were the trainers. It is the responsibility of the team to create a successful product.\n\nNonsoftware Deliverables\n\nConi Tartaglia, software test manager at Primavera Systems, Inc., reﬂects on what has worked for her team in delivering items that aren’t code but are necessary for a successful release.\n\nAside from the software, what is the team delivering? It is helpful to have a conversation with the people outside of the development team who may be concerned with this question. Groups such as Legal, Product Marketing, Training, and Customer Support will want to contribute to the list of deliverables.\n\nAfter there is agreement on what is being delivered, assembly of the components can begin, and the Release Management function can pro- vide conﬁrmation of the delivery through execution of a release check- list. If the release is an update to an existing product, testers can check the deliverables from previous releases to ensure nothing critical is left out of the update package. Deliverables can include legal notices, docu- mentation, translations, and third-party software that are provided as a courtesy to the customers.\n\nAgile teams are delivering value, not just software. We work together with the customer team to improve all aspects of the product.\n\nThere are no hard and fast rules to what should be delivered with the prod- uct. Think of deliverables as something that adds value to your product. Who should be the recipient of the deliverable, and when does it make the most sense to deliver it?\n\nRELEASING THE PRODUCT When we talk about releasing the product, we mean making it available to the customer in whatever format that may take. Your organization might have a website that gets updated or a custom application that is delivered to a few large customers. Maybe the product is shrink-wrapped and delivered to millions of PCs around the world, or downloaded off the Internet.\n\nRelease Acceptance Criteria\n\nHow do you know when you’re done? Acceptance criteria are a traditional way of deﬁning when to accept the product. Performance criteria may have",
      "content_length": 2202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 514,
      "content": "RELEASING THE PRODUCT\n\nto be met. We capture these for each story at the start of each iteration, and we may also specify them for larger feature sets when we begin a theme or epic. Customers may set quality criteria such as a certain percentage of code covered by automated tests, or that certain tests must pass. Line items such as having zero critical bugs, or zero bugs with serious impact to the system, are often part of the release criteria. The customers need to decide how they’ll know when there’s enough value in the product. Testers can help them deﬁne release criteria that accomplish their goals.\n\nAgile teams work to attain the spirit of the quality goals, not just the letter. They don’t downgrade the severity of bugs to medium so they can say they achieved the criterion of no high-severity bugs. Instead, they frequently look at bug trends and think of ways to ensure that high-severity bugs don’t occur in production.\n\nYour quality level should be negotiated with your customer up front so that there are no unpleasant surprises. The acceptance tests your team and your customers deﬁned, using real examples, should serve as milestones for progress toward release. If your customer has a very low tolerance for bugs, and 100% of those acceptance tests must be passing, your iteration velocity should take that into consideration. If new features are more important than bug ﬁxes, well, maybe you will be shipping with bugs.\n\nA Tale of Multitiered “Doneness”\n\nBob Galen, agile coach and author of Software Endgames, explains how his teams deﬁne release acceptance criteria and evaluate whether they’ve been met.\n\nI’ve joined several new agile teams over the past few years, and I’ve seen a common pattern within those teams. My current team does a wonder- ful job of establishing criteria at a user story or feature level—basically deﬁning acceptance criteria. We’ve worked hard at reﬁning our accep- tance criteria. Initially they were developed from the Product Owners’ perspective, and often they were quite ambiguous and ill-deﬁned. The testers decided they could really assist the customers in reﬁning their tests to be much more relevant, clear, and testable. That collaboration proved to be a signiﬁcant win at the story level, and the Product Owners really valued the engagement and help.\n\nQuite often the testers would also automate the user story acceptance tests, running them during each sprint but also demonstrating overall ac- ceptance during the sprint review.\n\n471",
      "content_length": 2500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 515,
      "content": "472\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nOne problem we had, though, was getting this same level of clarity for “doneness” at a story level to extend beyond the individual stories. We found that often, when we approached the end of a Sprint or the end game of a release, we would have open expectations of what the team was supposed to accomplish within the sprint. For example, we would deliver stories that were thoroughly tested “in the small”; that is, the functionality of those stories was tested but the stories were not inte- grated into our staging environment for broader testing. That wasn’t part of our “understanding,” but external stakeholders had that expectation of the teams’ deliverables.\n\nThe way the teams solved this problem was to look at our criteria as a multitiered set of guiding goals that wrap each phase, if you will, of agile development. An example of this is shown in Table 20-1.\n\nDeﬁning doneness at these individual levels has proven to work for our teams and has signiﬁcantly improved our ability to quantify and meet all of our various customer expectations. Keep in mind that there is a con- nection among all of the criteria, so deﬁning at one level really helps de- ﬁne the others. We often start at the Release Criteria level and work our way “backwards.”\n\nAgile development doesn’t work if stories, iterations, or releases aren’t “done.” “Doneness” includes testing, and testing is often the thing that gets postponed when time is tight. Make sure your success criteria at every level includes all of the necessary testing to guide and validate development.\n\nTable 20-1 Different Levels of Doneness\n\nActivity Basic Team Work Products\n\nUser Story Level\n\nSprint or Iteration Level\n\nRelease Level\n\nCriteria Doneness criteria\n\nAcceptance tests\n\nDoneness criteria\n\nRelease criteria\n\nExample Pairing or pair inspections of code prior to check-in, or to development, execu- tion, and passing of unit tests Development of FitNesse-based accep- tance tests with the customer AND their successful execution and passing Deﬁning a Sprint Goal that clariﬁes the feature development and all external dependencies associated with a sprint Deﬁning a broad set of conditions (artifacts, testing activities or coverage levels, results/metrics, collaboration with other groups, meeting compliance levels, etc.) that, if met, would mean the release could occur",
      "content_length": 2387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 516,
      "content": "RELEASING THE PRODUCT\n\nEach project, each team, each business is unique. Agile teams work with the business experts to decide when they’re ready to deliver software to produc- tion. If the release deadline is set in stone, the business will have to modify scope. If there’s enough ﬂexibility to release when the software has enough value, the teams can decide when the quality criteria have been met and the software can go to production.\n\nChallenging Release Candidate Builds\n\nConi Tartaglia’s team uses a checklist to evaluate each release candidate build. The checklist might specify that the release candidate build:\n\nIncludes all features that provide business value for the release, includ- ing artwork, logos, legal agreements, and documentation\n\nMeets all build acceptance criteria • Has proof that all agreed-upon tests (acceptance, integration, regres- sion, nonfunctional, UAT) have passed\n\nHas no open defect reports\n\nConi’s team challenges the software they might ship with a ﬁnal set of inspec- tions and agreed-upon “release acceptance tests,” or “RATS.” She explains:\n\nThe key phrase is “agreed-upon tests.” By agreeing on the tests in ad- vance, the scope for the release checklist is well deﬁned. Include system- level, end-to-end tests in the RATS, and select from the compatibility ros- ter tests, which will really challenge the release candidate build. Perfor- mance tests can also be included in RATs. Agree in advance on the content of the automation suites as well as a subset of manual tests for each RAT.\n\nAgree in advance which tests will be repeated if a RAT succeeds in caus- ing the failure of a release candidate build. If the software has survived several iterations of continuously run automated regression tests, passing these ﬁnal challenges should be a breeze.\n\nDeﬁning acceptance criteria is ultimately up to the customers. Testers are in a unique position to help the customer and development teams agree on the criteria that optimize product quality.\n\nTraditional software development works in long time frames, with deadlines set far in advance and hurdles to clear from one phase to the next. Agile de- velopment lets us produce quality software in small increments and release as necessary. The development and customer teams can work closely to deﬁne and decide what to release and when. Testers can play a critical role in this goal-setting process.\n\n473",
      "content_length": 2399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 517,
      "content": "474\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nRelease Management\n\nMany organizations have a release management team, but if you don’t, some- one still does the work. Many times in a small organization it is the QA man- ager who fulﬁlls this role. The person leading the release may hold a release readiness meeting with the stakeholders to evaluate readiness.\n\nA release readiness checklist is a great tool to use to walk through what is im- portant to your team. The intention of this checklist is to help the team ob- jectively determine what was completed and identify the risks associated with not completing a task.\n\nFor example, if training is not required because the changes made to the prod- uct were transparent to the end user, then the risk is low. However, if there were signiﬁcant changes to the process for how a new user is created in the system, the risk would be very high to the production support or help teams, and may warrant a delay. The needs of all stakeholders must be considered.\n\nRelease notes are important for any product release. The formality of these depends on the audience. If your product is aimed at developers, then a “read me” text ﬁle is probably ﬁne. In other cases, you may want to make them more formal. Whatever the media, they should address the needs of the audi- ence. Don’t provide a lot of added information that isn’t needed.\n\nWhen Janet gets a new release, one of the ﬁrst things she does is check the version and all of the components. “Did I get what they said they gave me? Are there special instructions I need to consider before installing, such as de- pendencies or upgrade scripts?” Those are good simple questions to answer in release notes. Other things to include are the new features that the cus- tomer should look for.\n\nRelease notes should give special consideration to components that aren’t part of what your development team delivered, such as a help ﬁle or user manuals prepared by a different team. Sometimes old release notes get left on the release media, which may or may not be useful to the end user. Consider what is right for your team and your application.\n\nPackaging\n\nWe’ve talked a lot about continual integration. We tend to take it for granted and forget what good conﬁguration management means. “Build once, deploy multiple times” is part of what gives us conﬁdence when we release. We know that the build we tested in staging is the same build that the customer tested",
      "content_length": 2453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 518,
      "content": "CUSTOMER EXPECTATIONS\n\nin UAT and is the build we release to production. This is critical for a success- ful release.\n\nIf the product is intended for an external customer, the installation should be easy, because the installation may be the ﬁrst look at the product that cus- tomer has. Know your audience and its tolerance level for errors. How will the product be delivered? For example, if it is to be downloaded off the Inter- net, then it should be a simple download and install. If it is a huge enterprise system, then maybe your organization needs to send a support person with the product to help with the install.\n\nCUSTOMER EXPECTATIONS Before we spring new software on our customers, we’d better be certain they are ready for it. We must be sure they know what new functionality to expect and that they have some means to deal with problems that arise.\n\nProduction Support\n\nMany organizations have a production or operations support team that maintains the code and supports customers after it’s in production. If your company has a production support team, that group is your ﬁrst customer. Make it your partner as well. Production support teams receive defect reports and enhancement requests from the customers, and they can work with your team to identify high-risk areas.\n\nVery often the production support team is the team that accepts the release from the development team. If your organization has this type of hand-off, it is important that your development team works closely with the production support team to make it a smooth transition. Make sure the production sup- port team understands how to use the system’s log ﬁles and the messaging and monitoring systems in order to keep track of operations and identify problems quickly.\n\nUnderstand Impact to Business\n\nEvery time a deployment to production requires an outage, the product is unavailable to your customer. If your product is a website, this may be a huge impact. If your product is an independent product to be downloaded onto a PC, the impact is low. Agile teams release frequently to maximize value to the business, and small releases have a lower risk of a large negative impact. It’s common sense to work with the business to time releases for time periods\n\n475",
      "content_length": 2249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 519,
      "content": "476\n\nCHAPTER 20\n\n(cid:2)\n\nSUCCESSFUL DELIVERY\n\nthat minimize disruption. Automate and streamline deployment processes as much as possible to keep downtime windows small. A quick deployment process is also helpful during development in short iterations where we may deploy a dozen times in one day.\n\nInternational Considerations\n\nMarkus Gärtner, an “agile affected” testing group lead, explains his team’s ap- proach to timing its releases:\n\nWe build telecommunications software for mobiles, so we usually install our software at night, when no one is likely to make calls. This might be during our ofﬁce hours, when we're handling a customer in Australia, but usually it is during our nighttime.\n\nMy colleagues who do the actual installation—there are three within our team—are most likely to appear late during next day's ofﬁce hours be- cause we don't have a separate group for these tasks.\n\nAs businesses and development teams become more global, release timing gets more complicated. Fortunately, production conﬁgurations can make re- leases easier. If your production environment has multiple application servers, you may be able to bring them down one at a time for release without dis- rupting users.\n\nNew releases should be as transparent as possible to the customer. The fewer emergency releases or patches required after a release, the more conﬁdence your customer will have in both the product and the development team.\n\nLearn from each release and take actions to make the next one go more smoothly. Get all roles, such as system and database administrators, involved in the planning. Evaluate each release and think of ways to improve the next one.\n\nSUMMARY This chapter covered the following points:\n\n(cid:2) Successful delivery of a product includes more than just the applica- tion you are building. Plan the non-software deliverables such as doc- umentation, legal notices, and training.\n\n(cid:2) The end game is an opportunity to put the spit and polish, the ﬁnal\n\nﬁnishing touches, on your product.",
      "content_length": 2017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 520,
      "content": "SUMMARY\n\n(cid:2) Other groups may be responsible for environments, tools, and other components of the end game and release. Coordinate with them ahead of time.\n\n(cid:2) Be sure to test database update scripts, data conversions, and other\n\nparts of the installation.\n\n(cid:2) UAT is an opportunity for customers to test against their data and to\n\nbuild their conﬁdence in the product.\n\n(cid:2) Budget time for extra cycles as needed, such as post-development\n\ncycles to coordinate testing with outside parties.\n\n(cid:2) Establish release acceptance criteria during release planning so that\n\nyou can know when you’re ready to release.\n\n(cid:2) Testers often are involved in managing releases and testing the\n\npackaging.\n\n(cid:2) When releasing the product, consider the whole package—what the\n\ncustomer needs and expects.\n\n(cid:2) Learn from each release, and adapt to improve your processes.\n\n477",
      "content_length": 895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 521,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 522,
      "content": "Part VI SUMMARY\n\nIn Chapter 21, “Key Success Factors,” we pull things together and summarize the agile approach to testing.",
      "content_length": 123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 523,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 524,
      "content": "Chapter 21\n\nKEY SUCCESS FACTORS\n\nLook at the Big Picture\n\nUse the Whole-Team Approach\n\nCollaborate with Customers\n\nAdopt an Agile Testing Mind-Set\n\nKey Success Factors\n\nContinuous Integration\n\nTest Environments\n\nAutomate Regression Testing\n\nManage Technical Debt\n\nWorking Incrementally\n\nBuild a Foundation of Core Agile Practices\n\nProvide and Obtain Feedback\n\nCoding and Testing Are Part of One Process\n\nSynergy between Practices\n\nHaving traveled through an iteration and beyond, following an agile tester as she engages in many activities, we can now pick out some key factors that help testers succeed on agile teams and help agile teams succeed at delivering a high-quality product. We think agile testers have something special to offer. “Agile-infected” testers learn how to apply agile practices and principles to help their whole team produce a better product. “Test-infected” programmers on agile teams learn how to use testing to produce better work. Lines between roles are blurred, but that’s a good thing. Everyone is focused on quality.\n\nWe have gleaned some critical testing guidelines for agile teams and testers\n\nfrom our own trial and error as well as from teams with which we’ve worked. These guidelines are built on the agile testing matrix, on our experience of learning to overcome cultural and organizational obstacles, our adventures in performing the tester role on agile teams, and our experience of ﬁguring out how best to use test automation. We like lucky numbers, so in this chapter we present seven key factors that help an agile tester succeed.\n\nWe asked a small group of people who were reviewing some of our chapters to suggest the order in which to present these success factors. The results varied quite a bit, although many (but not all) agreed on the top two. Pick the success factor that will give you the biggest return on investment, and start working on it today.\n\n481",
      "content_length": 1909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 525,
      "content": "482\n\nCHAPTER 21\n\n(cid:2) KEY SUCCESS FACTORS\n\nSUCCESS FACTOR 1: USE THE WHOLE-TEAM APPROACH When the whole development team takes responsibility for testing and qual- ity, you have a large variety of skill sets and experience levels taking on what- ever testing issues might arrive. Test automation isn’t a big problem to a group of skilled programmers. When testing is a team priority, and anyone can sign up for testing tasks, the team designs testable code.\n\nMaking testers truly part of the development team means giving them the support and training they need to adapt to the fast pace of agile develop- ment. They have time to acquire new skills in order to collaborate closely with members of both the development and customer teams.\n\nIf you manage an agile team, use the suggestions in Part II, “Organizational Challenges,” to help your team adopt the whole-team approach. Remember that quality, not speed, is the goal of agile development. Your team needs testers to help customers clarify requirements, turn those into tests that guide development, and provide a unique viewpoint that will promote delivery of a solid product. Make sure the testers can transfer their skills and expertise to the rest of the team. Make sure they aren’t pigeonholed in a role such as only doing manual testing. Make sure that when they ask for help (which may re- quire considerable courage on their part), their team members give it. The reverse is true, too; a tester should step up whenever someone needs assis- tance that they can provide.\n\nSee Chapter 2, “Ten Principles for Agile Testers,” for an example of how the “Power of Three” works.\n\nIf you’re a tester on an agile team, and there are planning meetings and de- sign discussions happening that don’t include you, or the business users are struggling to deﬁne their stories and requirements alone, it’s time to get up and go talk to the rest of the team. Sit with the programmers, invite yourself to meetings, and propose trying the “Power of Three” by involving a tester, a programmer, and a business expert. Be useful, giving feedback and helping the customers provide examples. Make your problems the team’s problems, and make their problems yours. Ask your teammates to adopt a whole-team approach.\n\nSUCCESS FACTOR 2: ADOPT AN AGILE TESTING MIND-SET In Chapter 2, “Ten Principles for Agile Testers,” we cautioned agile testers to lose any “Quality Police” mind-set they might have brought with them. You’re on an agile team now, where programmers test and testers do what- ever they can think of to help the team deliver the best possible product. As",
      "content_length": 2607,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 526,
      "content": "See Chapter 2, “Ten Principles for Agile Testers,” for more about the agile testing mindset.\n\nSUCCESS FACTOR 2: ADOPT AN AGILE TESTING MIND-SET\n\nwe emphasized in Chapter 2, an agile testing attitude is proactive, creative, open to new ideas, and willing to take on any task. The agile tester constantly hones her craft, is always ready to collaborate, trusts her instincts, and is pas- sionate about helping the team and the business succeed.\n\nWe don’t mean that you should put on your Super Tester cape and go protect the world from bugs. There’s no room for big egos on agile teams. Your teammates share your passion for quality. Focus on the team’s goals and do what you can to help everyone do their best work.\n\nUse agile principles and values to guide you. Always try the simplest ap- proach to meeting a testing need. Be courageous in seeking help and experi- menting with new ideas. Focus on delivering value. Communicate as directly and as often as possible. Be ﬂexible in responding to change. Remember that agile development is people-centric, and that we should all enjoy our work. When in doubt, go back to the values and principles to decide what to do.\n\nAn important component of the agile testing mind-set is the drive to contin- ually ﬁnd better ways to work. A successful agile tester constantly polishes her craft. Read good books, blogs, and articles to get new ideas and skills. At- tend local user group meetings. Participate in mailing list discussions to get feedback on problems or new ideas. If your company won’t pay for you to at- tend a good conference, put what you’ve learned into an experience report to exchange for a free conference registration. Giving back to your testing and agile development communities will help you, too.\n\nExperiment with new practices, tools, and techniques. Encourage your team to try new approaches. Short iterations are ideally suited to experimentation. You might fail, but it’ll be fast, and you can try something else.\n\nIf you manage agile testers or an agile team, give them time to learn and pro- vide support for the training they need. Remove obstacles so that they can do their best work.\n\nWhen you’re faced with problems that impact testing, bring those problems to the team. Ask the team to brainstorm ways to overcome these obstacles. Retrospectives are one place to talk about issues and how to resolve them. Keep an impediment backlog and address one or two in every iteration. Use big visible charts, or their virtual equivalents, to ensure that everyone is aware of problems that arise and that everyone can track the progress of cod- ing and testing.\n\n483",
      "content_length": 2632,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 527,
      "content": "484\n\nCHAPTER 21\n\n(cid:2) KEY SUCCESS FACTORS\n\nSUCCESS FACTOR 3: AUTOMATE REGRESSION TESTING Can an agile team succeed with no test automation? Maybe, but the success- ful teams that we know rely on automated regression tests. As we’ve said of- ten in this book, if you’re spending all your time doing manual regression testing, you’ll never have time for the important exploratory testing that will ferret out the damaging behaviors lurking in the code.\n\nSee Part II for more on the Agile Test- ing Quadrants.\n\nAgile development uses tests to guide development. In order to write code to make a test pass, you need a quick and easy way to run the test. Without the short feedback cycle and safety net regression that suites provide, your team will soon become mired in technical debt, with a growing defect queue and ever-slowing velocity.\n\nSee Chapter 14, “Automation Strat- egy,” for more on the test automa- tion pyramid.\n\nAutomating regression tests is a team effort. The whole team should choose appropriate tools for each type of test. Thinking about tests up front will let programmers design code for ease of test automation. Use the Agile Testing Quadrants and test automation pyramid to help you automate different types of tests effectively.\n\nRemember to start simply. You’ll be surprised at how much value some basic automated smoke tests or automated unit tests can provide.\n\nSee the bibliogra- phy for resources on promoting change.\n\nTest automation is a team effort. It’s also hard, at least at ﬁrst. There’s often a big “hump of pain” to overcome. If you manage a development or testing team, make sure you’re providing enough support in the form of time, train- ing, and motivation. If you’re a tester on a team with no automation, and the programmers are too frantic trying to write production code to stop and think about testing, you have a big challenge ahead of you. Experiment with different ways of getting support from management and from team members to start some tiny automation effort.\n\nSUCCESS FACTOR 4: PROVIDE AND OBTAIN FEEDBACK Feedback is a core agile value. The short iterations of agile are designed to provide constant feedback in order to keep the team on track. Testers are in a unique position to help provide feedback in the form of automated test re- sults, discoveries made during exploratory testing, and observations of actual users of the system.",
      "content_length": 2393,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 528,
      "content": "SUCCESS FACTOR 4: PROVIDE AND OBTAIN FEEDBACK\n\nAgile Is All about Feedback\n\nBret Pettichord, CTO of WatirCraft and co-author of Lessons Learned in Soft- ware Testing, shared these thoughts on the importance of feedback to agile development.\n\nAgile methods allow your team to get feedback regarding the software you are building. That’s the point. The feedback works on several levels. Pair programming gives developers instant feedback on their code. Sto- ries represent units of work where testers and analysts can give feed- back to developers. Iteration releases facilitate feedback from outside the team. Most agile practices are valuable because they create feed- back loops that allow teams to adapt.\n\nA lot of teams adopt Agile with a grab-bag approach without quite real- izing the point of the practices. They pair-program without discussion or changing drivers. They send code to QA that the testers can’t test be- cause the story boundaries are arbitrary; they can’t tell whether they found a bug or just the end of the story. Iterations become schedule milestones rather than real opportunities to improve alignment and ad- just objectives.\n\nThe reason Agile teams can do with less planning is because feedback al- lows you to make sure that you are on course. If you don’t have mean- ingful feedback, then you’re not agile. You’re just in a new form of chaos.\n\nOn my last project, we deﬁned our stories so that they made sense to everyone on the team. Our analysts, testers, and developers could all understand and review individual stories. But we found that we had to create a larger grouping, which we called features, to facilitate meaning- ful review from outside our team. We made sure all the stories in a fea- ture were complete before soliciting feedback from outside the team.\n\nBeing able to give and receive meaningful feedback is often a challenge for people. Yet it is crucial to success with Agile.\n\nAgile teams get into terrible binds when executives or clients hand them a list of requirements at the start, tell them to use Agile (because it’s faster), and then don’t want to participate in the feedback process.\n\nAgile isn’t faster all by itself. Agile is only a beneﬁt in a world that ac- knowledges the value of adapting. And that adaptability needs to go all the way to whoever is funding the project. It is not enough for the team to be agile. The sponsors need to be agile too. Are all of the require- ments really required? Do we know exactly what the software needs to look like from the start?\n\n485",
      "content_length": 2537,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 529,
      "content": "486\n\nCHAPTER 21\n\n(cid:2) KEY SUCCESS FACTORS\n\nAgile is faster because feedback allows you to ﬁnd and focus on the most valuable features. If you are certain you know what needs to be built, don’t use Agile. If you don’t have time to gather and act on feed- back from customers, then don’t use Agile. If you are sure that everyone understands exactly what needs to be done from the start, then don’t use Agile.\n\nAgile practices build a technical and organizational infrastructure to facil- itate getting and acting on feedback. If you aren’t going to adapt to feedback, then this infrastructure is waste that will only slow you down.\n\nTo us, the value of agile development isn’t that it’s faster but that it delivers enough value quickly enough to help the business grow and succeed. Testers play a key role in providing the feedback that allows that to happen.\n\nTesters need feedback too. How do you know that you have the right exam- ples of desired behavior from the customers? How do you know if the test cases you wrote reﬂected these examples correctly? Can the programmers understand what to code by looking at the examples you’ve captured and the tests you’ve created?\n\nOne of the most valuable skills you can learn is how to ask for feedback on your own work. Ask the programmers if they get enough information to un- derstand requirements and whether that information guides their coding. Ask customers if they feel their quality criteria are being met. Take time in both the iteration planning meetings and retrospectives to talk about these issues and suggest ways to improve.\n\nSUCCESS FACTOR 5: BUILD A FOUNDATION OF CORE PRACTICES An old saying in the testing business is, “You can’t test quality into the prod- uct.” This is, of course, true of agile development as well. We feel you can’t deliver high-quality software without following some fundamental practices. While we think of these as agile practices, they’ve been around longer than the term “agile development,” and they’re simply core practices of successful software development.\n\nContinuous Integration\n\nEvery development team needs source code management and continuous in- tegration to be successful. You can’t test effectively if you don’t know exactly",
      "content_length": 2232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 530,
      "content": "See the bibliog- raphy for more information about continu- ous integration.\n\nSUCCESS FACTOR 5: BUILD A FOUNDATION OF CORE PRACTICES\n\nwhat you’re testing, and you can’t test at all if you have no code you can de- ploy. All team members need to check in their work at least once a day. Every integration must be veriﬁed by an automated build that includes tests to pro- vide rapid feedback about the state of the software.\n\nImplementing a continuous integration process should be one of the ﬁrst priorities of any software development team. If your team doesn’t have at least a daily veriﬁed build, stop what you’re doing and get one started. It’s that important. It doesn’t have to be perfect to start with. If you have a huge system to integrate, it’s deﬁnitely more challenging. In general, though, it’s not that difﬁcult. There’s a plethora of outstanding tools, both open source and commercial, available for this purpose.\n\nTest Environments\n\nYou can’t test productively without a test environment that you control. You need to know what build is deployed, what database schema is being used, whether anyone else is updating that schema, and what other processes are running on the machine.\n\nHardware is getting less expensive all the time, and more open source software is available that can be used for test environments. Your team must make the investment so that you can effectively conduct automated and manual explor- atory tests quickly and efﬁciently. If there’s a problem with the test environ- ment, speak up and let it be a problem for the team to solve creatively.\n\nManage Technical Debt\n\nEven good software development teams, feeling time pressure, neglect refac- toring or resort to quick ﬁxes and hacks to solve a problem quickly. As the code becomes more confusing and hard to maintain, more bugs creep in, and it doesn’t take long before the team’s velocity is consumed by bug ﬁxes and trying to make sense out of the code in order to add new features. Your team must constantly evaluate the amount of technical debt dragging it down and work on reducing and preventing it.\n\nPeople often say, “Our management won’t give us time to do things right, we don’t have time to refactor, and we’re under tight deadlines.” However, it’s not hard to make a clear business case showing what growing technical debt is costing the company. There are many ways to measure code and defect rates that can translate technical debt into its impact on the bottom line. Merely pointing to your decreasing velocity may be enough. Businesses need\n\n487",
      "content_length": 2549,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 531,
      "content": "488\n\nCHAPTER 21\n\n(cid:2) KEY SUCCESS FACTORS\n\ntheir software development teams to remain consistently productive. They may have to reduce the scope of their desired features in order to allow enough time for good, test-guided code design and good practices such as continual small refactoring.\n\nGood coverage from automated regression tests is key to minimizing technical debt. If these are lacking, budget time in each iteration to build up the auto- mated tests, plan a “refactoring iteration” to upgrade or add necessary tools, and write tests and do major refactoring efforts. In every iteration, take the time to guide code with tests, refactor the code you’re touching as needed, and add automated tests where they’re missing. Increase your estimates to account for this work. In the long run, the team will be able to go much faster.\n\nWorking Incrementally\n\nOne reason agile teams are able to create a quality product is that they work on a small scale. Stories represent a few days of work, and each story may be broken into several thin slices or steel threads and built step-by-step. This al- lows testing each small piece and then incrementally testing as the pieces are put together.\n\nRead more about small chunks and thin slices in Chap- ter 8, “Business- Facing Tests that Support the Team.”\n\nIf your team members are tempted to take on a large chunk of functionality at once, encourage them to look at a stepwise approach. Ask questions: “What’s the central business value in this story? What’s the most basic path through this piece of code? What would come next?” Suggest writing task cards to code and test the small pieces, get a proof of concept for your design, and conﬁrm your test and test automation strategy.\n\nCoding and Testing Are Part of One Process\n\nPeople who are new to agile often ask agile testers, “What do you do until all the stories are ﬁnished and you can test?” Experienced agile practitioners say, “Testers must be involved throughout the whole iteration, the whole devel- opment process. Otherwise it doesn’t work.”\n\nTesters write tests, based on examples provided by customers, to help program- mers understand the story and get started. Tests and examples provide a com- mon language that everyone involved in producing the software understands. Testers and programmers collaborate closely as coding proceeds, and they both also collaborate closely with the customers. Programmers show testers the functionality they’ve written, and testers show programmers the unexpected behaviors they’ve found. Testers write more tests as coding proceeds, program-",
      "content_length": 2594,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 532,
      "content": "Read more about coding and test- ing in Chapter 18, “Coding and Testing.”\n\nSUCCESS FACTOR 6: COLLABORATE WITH CUSTOMERS\n\nmers make them pass, and testers do more exploratory testing to learn whether the right value has been delivered. Each agile iteration consists of dozens of constant, quick, incremental test-code-test-code-test iterations.\n\nWhen this collaboration and feedback cycle is disturbed, and testing is sepa- rated from development, bad things happen. If a story is tested in the itera- tion after which it was coded and bugs are found, the programmer has to stop working on the new story, remember how the code worked for the last iteration’s story, ﬁx it, and wait for someone to test the ﬁx. There are few facts in software development, but we know for sure that bugs are cheaper to ﬁx the sooner they’re found.\n\nWhen coding is constantly guided by tests, and testing happens alongside coding, we’re much more likely to achieve the behavior and provide the value that the customer wanted. Testing is a team responsibility. If your team doesn’t share this view, ask everyone to think about their focus on quality, their desire to deliver the best possible product, and what steps they can take to ensure that the team achieves its goals.\n\nSynergy between Practices\n\nA single agile development practice such as continuous integration can make a difference, but the combination of multiple agile practices is greater than the sum of the parts. Test-driven design, collective code ownership, and continu- ous integration together deliver rapid feedback, continually improving code design and the ability to deliver business value quickly. Automating tests is good, but using automated tests to drive development, followed up by explor- atory testing to detect gaps or weaknesses, is many levels of magnitude better.\n\nSome practices don’t work well in isolation. Refactoring is impossible with- out automated tests. It’s possible to do small releases in a mini-waterfall fash- ion and avoid all beneﬁts of agile development. If your on-site customer isn’t empowered to make decisions, her value to the team is limited.\n\nAgile practices were designed to complement each other. Take time to under- stand the purpose of each one, consider what is needed to take full advantage of each practice, and make thoughtful decisions about what works for your team.\n\nSUCCESS FACTOR 6: COLLABORATE WITH CUSTOMERS Some of the greatest value that testers contribute to agile teams is helping customers clarify and prioritize requirements, illustrating the requirements\n\n489",
      "content_length": 2570,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 533,
      "content": "490\n\nCHAPTER 21\n\n(cid:2) KEY SUCCESS FACTORS\n\nwith concrete examples of desired behavior and user scenarios, and turning those examples into executable tests. Testers speak the domain language of the business and the technical language of the development team. We make good facilitators and translators.\n\nNever get in the way of direct communication between programmers and customers. Do encourage as much direct communication as possible. Use the “Power of Three.” When requirements are missed or misunderstood, a cus- tomer, programmer, and tester need to work together to get questions an- swered. Get the customers talking in front of a whiteboard or its virtual equivalent as often as necessary. If customers are scattered around the cam- pus, the country, or the globe, use every tool you can ﬁnd to enhance com- munication and collaboration. Teleconferences, instant messages, and wikis aren’t an ideal replacement for face-to-face conversation, but they beat send- ing emails or not talking at all.\n\nSUCCESS FACTOR 7: LOOK AT THE BIG PICTURE This is a generalization, of course, but we’ve found that testers tend to look at the big picture, and usually from a customer point of view. Programmers usually have to focus on delivering the story they’re working on now, and while they may be using tests to guide them, they have to focus on the techni- cal implementation of the requirements.\n\nThis big-picture viewpoint is a huge contribution to the team. Test-driven development, done well, delivers solid code that may, in isolation, be free of defects. What if that new feature causes some apparently unrelated part of the application to break? Someone has to consider the impact to the larger system and bring that to the team’s attention. What if we’ve overlooked some little detail that will irritate the customers? The new UI may be ﬂawlessly coded, but if the background color makes the text hard to read, that’s what the end user’s going to notice.\n\nPart III explains how to use the Agile Testing Quadrants.\n\nUse the Agile Testing Quadrants as a guide to help you plan testing that will cover all the angles. Use the test pyramid idea to ensure good ROI from your test automation. Guiding development with tests helps make sure you don’t miss something big, but it’s not perfect. Use exploratory testing to learn more about how the application should work, and what direction your test- ing needs to take. Make your test environments as similar as possible to pro- duction, using data that reﬂects the real world. Be diligent about re-creating a production-style situation for activities such as load testing.",
      "content_length": 2623,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 534,
      "content": "SUMMARY\n\nIt’s easy for everyone on the team to narrowly focus only on the task or story at hand. That’s a drawback of working on small chunks of functionality at a time. Help your team take a step back now and then to evaluate how your current stories ﬁt into the grand scheme of the business. Keep asking your- selves how you can do a better job of delivering real value.\n\nSUMMARY Testing and quality are the responsibility of the whole team, but testers bring a special viewpoint and unique skills. As a tester, your passion for delivering a product that delights your customers will carry you through the frustrations you and your team may encounter. Don’t be afraid to be an agent for continual improvement. Let agile principles and values guide you as you work with the customer and development teams, adding value throughout each iteration.\n\nIn this concluding chapter, we looked at seven key factors for successful agile testing:\n\n1. 2. 3. 4. 5. 6. 7.\n\nUse the whole-team approach. Adopt an agile testing mind-set. Automate regression testing. Provide and obtain feedback. Build a foundation of core practices. Collaborate with customers. Look at the big picture.\n\n491",
      "content_length": 1175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 535,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 536,
      "content": "GLOSSARY\n\nThis glossary contains the authors’ deﬁnitions of terms used throughout this book.\n\nAcceptance Test Acceptance tests are tests that deﬁne the business value each story must deliver. They may verify functional requirements or nonfunctional requirements such as performance or reliability. Although they are used to help guide development, it is at a higher level than the unit-level tests used for code design in test-driven development. Acceptance test is a broad term that may include both business-facing and technology-facing tests.\n\nApplication programming interface (API) APIs enable other software to invoke some piece of functionality. The API may consist of functions, proce- dures, or classes that support requests made by other programs.\n\nBuild A build is the process of converting source code into a deployable arti- fact that can be installed to run the application. The term “build” also refers to the deployable artifact.\n\nComponent A component is a larger part of the overall system that may be separately deployable. For example, on the Windows platform, dynamic linked libraries (DLLs) are used as components, Java Archives (JAR ﬁles) are components on the Java platform, and a service-oriented architecture (SOA) uses Web Services as components.\n\nComponent Test A component test veriﬁes a component’s behavior. Compo- nent tests help with component design by testing interactions between objects.\n\nConditions of Satisfaction Conditions of satisfaction, also called satisfac- tion conditions or conditions of business satisfaction, are key assumptions and decisions made by the customer team to deﬁne the desired behavior of\n\n493",
      "content_length": 1656,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 537,
      "content": "494\n\nGLOSSARY\n\nthe code delivered for a given story. Conditions of satisfaction are criteria by which the outcome of a story can be measured. They evolve during conversa- tions with the customer about high-level acceptance criteria for each story. Discussing conditions of satisfaction helps identify risky assumptions and in- creases the team’s conﬁdence in writing and correctly estimating all the tasks to complete the story.\n\nContext-Driven Testing Context-driven testing follows seven principles, the ﬁrst being that the value of any practice depends on its context. Every new project and every new application may require different ways of ap- proaching a project. All seven practices can be found on the website www.context-driven-testing.com/.\n\nCustomer Team The customer team identiﬁes and prioritizes the features needed by the business. In Scrum, these features become epics or themes, which are further broken into stories and comprise the product backlog. Cus- tomer teams include all stakeholders outside of the development team, such as business experts, subject-matter experts, and end users. Testers and developers work closely with the customer team to specify examples of desired behavior for each story and turn those examples into tests to guide development.\n\nCustomer Test A customer test veriﬁes the behavior of a slice or piece of functionality that is visible to the customer and related directly back to a story or feature. The terms “business-facing test” and “customer-facing test” refer to the same type of test as customer test.\n\nDevelopment Team The development team is the technical team that pro- duces the software requested by the customer team. Everyone involved in de- livering software is a developer, including programmers, testers, database experts, system administrators, technical writers, architects, usability experts, and analysts. This development team works together to produce the software and deliver value to the business, whether they are a co-located team or a vir- tual team.\n\nEpic An epic is a piece of functionality, or feature, described by the customer and is an item on the product backlog. An epic is broken up into related sto- ries that are then sized and estimated. Some teams use the term “theme” in- stead of epic.\n\nExploratory Testing Exploratory testing is interactive testing that combines test design with test execution and focuses on learning about the application.",
      "content_length": 2435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 538,
      "content": "GLOSSARY\n\nSee Chapter 10, “Business-Facing Tests that Critique the Product,” for an ex- tensive deﬁnition of exploratory testing.\n\nFake Object A fake object replaces the functionality of the depended-on component with a simpler implementation. It emulates the behavior of the real depended-on component but is easier to use for testing purposes.\n\nFeature A feature is a piece of functionality described by the customer and is an item on the product backlog. A feature is broken up into related stories that are then sized and estimated. In agile development, the terms “epic” or “theme” are often used in place of “feature.\"\n\nFunctional Test Functional tests verify the system’s expected behavior given a set of inputs and/or actions.\n\nGreenﬁeld Greenﬁeld projects are new application development projects starting from scratch with no existing code base. There are no constraints, so development teams have many options open to them.\n\nIntegrated Development Environment (IDE) An Integrated Development Environment, or IDE, is a set of tools that support programming and testing. It usually includes an editor, compiler or intepreter debugger, refactoring ca- pabilities, and build automation tools. IDEs usually enable integration with a source code control system and provide language-speciﬁc support to help with code design.\n\nIteration An iteration is a short development cycle, generally from one to four weeks, at the end of which production-ready code can potentially be delivered. Several iterations, each one the same length, may be needed to deliver an entire theme or epic. Some teams actually release the code to production each iteration, but even if the code isn’t released, it is ready for release.\n\nJava Messaging Service (JMS) The Java Messaging Service (JMS) API is a messaging standard that enables application components based on the Java 2 Platform, Enterprise Edition (J2EE) to create, send, receive, and read messages.\n\nLegacy System A legacy system is one that does not have any (or few) auto- mated regression tests. Introducing changes in legacy code, or refactoring it, might be risky because there are no tests to catch unintended changes in sys- tem behavior.\n\n495",
      "content_length": 2193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 539,
      "content": "496\n\nGLOSSARY\n\nMultipurpose Internet Mail Extensions (MIME) Multipurpose Internet Mail Extensions, or MIME, extend the format of Internet mail to enable non-textual messages, multipart message bodies, and non-US-ASCII textual messages and headers.\n\nMock Object A mock object simulates the responses of an existing object. It helps with designing and testing interactions between objects, replacing a real component so that a test can verify its indirect outputs.\n\nProduct Backlog Product Backlog is a Scrum term for the prioritized mas- ter list of all functionality desired in the product. This backlog grows over time as the organization thinks of new features they may need.\n\nProduct Owner Product Owner is a Scrum term for the person responsi- ble for prioritizing the product backlog, or stories. He or she is typically someone from a marketing role or a key business expert involved with development.\n\nQuality Assurance (QA) Team Quality Assurance, or QA, can be deﬁned as actions taken to ensure compliance with a quality standard. In software de- velopment, the term “QA Team” is often used to refer to the team that does software testing. Test teams (see Test Team) provide stakeholders with infor- mation related to the quality of the software product. They perform activities to learn how the system under test should behave and verify that it behaves as expected. In agile development, these activities are fully integrated with de- velopment activities. Testers are often part of the development team along with everyone else involved in developing the software.\n\nProduction Code Production code is the code for the system that is, or will be, used in production, as distinguished from the code that is written to test it. Test code invokes or operates on production code to verify its behavior.\n\nRefactoring Refactoring is changing code, without changing its functional- ity, to make it more maintainable, easier to read, easier to test, or easier to extend.\n\nRegression Test A regression test veriﬁes that the behavior of the system un- der test hasn’t changed. Regression tests are usually written as unit tests to drive coding or acceptance tests to deﬁne desired system behavior. Once the tests pass, they become part of a regression test suite, to guard against unin- tended changes being introduced. Regression tests should be automated to ensure continual feedback.",
      "content_length": 2386,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 540,
      "content": "GLOSSARY\n\nRelease Candidate A release candidate is a version or build of the product that can potentially be released to production. The release candidate may un- dergo further testing or be augmented with documentation or other materials.\n\nReturn on Investment (ROI) Return on investment, or ROI, is a term bor- rowed from the world of ﬁnancial investments and is a measure of the efﬁ- ciency of an investment. ROI can be calculated in different ways, but it’s basically the difference between the gain from an investment and the cost of that investment, divided by the cost of that investment. In testing, ROI is the beneﬁt gained from a testing activity such as automating a test, weighed against the cost of producing and maintaining that test or activity.\n\nSOAP SOAP is a protocol for exchanging XML-based messages over net- works, normally using HTTP/HTTPS. It forms the foundation layer of the web services protocol stack, providing a basic messaging framework upon which abstract layers can be built. A common SOAP messaging pattern is the Remote Procedure Call (RPC) pattern, in which the client network node sends a request message to the server node, and the server immediately sends a response to the client.\n\nStory A user story is a short description of functionality told from the per- spective of the user that is valuable to either the user or the customer team. Stories are traditionally written on index cards. The card typically contains a one-line description of the feature. For example, “As a shopper, I can put items in my shopping cart so that I can check out with them later” is a story. Cards are only useable in combination with subsequent conversations be- tween the customer team and the development team and some veriﬁcation that the story has been implemented through writing and running tests.\n\nStory Test A story test deﬁnes expected behavior for the code to be delivered by the story. Story tests may be business-facing, specifying the functional re- quirements, or technology-facing, such as security or performance tests. These tests are used to guide development as well as to verify the delivered code. Most agile practitioners use the term “story test” synonymously with “acceptance test,” although the term “acceptance test” might be used for tests that verify behavior at a higher level than one story.\n\nStory Board The story board, also called the task board, is used to track the work the team does during an iteration. Task cards, which may be color-coordinated for the type of task, are written for each story. These cards, along with a visual cue of some kind, provide an easy mechanism for seeing the current status of an iteration’s progress. It may use columns or\n\n497",
      "content_length": 2717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 541,
      "content": "498\n\nGLOSSARY\n\ndifferent colored stickers on cards for different states such as “To do,” “Work in Progress,” “Verify,” and “Done.” The story board might be a physical board on a wall or a virtual online board.\n\nTask Tasks are pieces of work needed to ﬁnish a story. A task might be action needed to implement a small piece of a story, or it might be for building a bit of infrastructure, or testing that encompasses more than one story. Generally it should represent a day or less of work.\n\nTechnical Debt Ward Cunningham ﬁrst introduced this metaphor. When a team produces software without using good practices such as TDD, continu- ous integration, and refactoring, it may incur technical debt. Like ﬁnancial debt, technical debt accrues interest that will cost the team more at a later date. Sometimes this debt may be worthwhile, such as to take advantage of a sudden business opportunity. Usually, though, technical debt compounds and slows the team’s velocity. Less and less business value can be produced in each iteration because the code lacks a safety net of automated regression tests or has become difﬁcult to understand and maintain.\n\nTest Double A test double is any object or component that’s installed in place of the real component for the express purpose of running a test. Test doubles include dummy objects, mock objects, test stubs, and fake objects.\n\nTest-Driven Development (TDD) In test-driven development, the pro- grammer writes and automates a small unit test before writing the small piece of code that will make the test pass. The production code is made to work one test at a time.\n\nTest-First Development In test-ﬁrst development, tests are written in ad- vance of the corresponding production code, but the code is not necessarily made to work one test at a time. Customer or story tests may be used in test- ﬁrst development as well as unit tests.\n\nTest Stub A test stub is an object that replaces a real component needed by the system under test with a test-speciﬁc object that feeds desired indirect in- puts into the system under test. This enables the test to verify logic indepen- dently of the other components.\n\nTest Team A test team performs activities that help deﬁne and subsequently verify the desired behavior of the system under test. The test team provides information to the stakeholders about the external quality of the system, the risks that may be present, and potential risk mitigation strategies. In agile de-",
      "content_length": 2463,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 542,
      "content": "GLOSSARY\n\nvelopment, these activities are fully integrated with development activities. Testers are often part of the development team along with everyone else in- volved in developing the software.\n\nTester A tester provides information to stakeholders about the software be- ing developed. A tester helps customers deﬁne functional and nonfunc- tional requirements and quality criteria, and helps turn these into tests that guide development and verify desired behavior. Testers perform a wide vari- ety of activities related to delivering high-quality software, such as test auto- mation and exploratory testing. In agile development, everyone on the development team performs testing activities. Team members who identify themselves as testers work closely with other members of both the developer and customer teams.\n\nTheme A theme is the same as an epic or feature. It is a piece of functionality described by the customer and placed in the product backlog to be broken up into stories that are sized and estimated.\n\nUnit Test A unit test veriﬁes the behavior of a small part of the overall sys- tem. It may be as small as a single object or method that is a consequence of one or more design decisions.\n\nVelocity A development team’s velocity is the amount of value it delivers in each iteration, measured in story points, ideal days, or hours. Generally, only completed stories are included in the velocity. Velocity is helpful to the busi- ness in planning for future features and releases. Agile teams use their veloc- ity for the previous iteration to help determine the amount of work they can take on in the next iteration.\n\nWeb Service Description Language (WSDL) Web Service Description Lan- guage (WDSL) is an XML format for describing network services as a set of endpoints operating on messages containing either document-oriented or procedure-oriented information.\n\n499",
      "content_length": 1887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 543,
      "content": "This page intentionally left blank",
      "content_length": 34,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 544,
      "content": "BIBLIOGRAPHY\n\nBOOKS, ARTICLES, PAPERS, AND BLOG POSTINGS Agile Alliance. “Principles Behind the Agile Manifesto,” www.agilemanifesto .org/principles.html, 2001.\n\nAlles, Micah, David Crosby, Carl Erickson, Brian Harleton, Michael Marsiglia, Greg Pattison, and Curt Stienstra. “Presenter First: Organizing Complex GUI Applications for Test-Driven Development,” Agile 2006, Minneapolis, MN, July 2006.\n\nAmbler, Scott. Agile Database Techniques: Effective Strategies for the Agile Soft- ware Developer, Wiley, 2003.\n\nAstels, David. Test-Driven Development: A Practical Guide, Prentice Hall, 2003.\n\nBach, James. “Exploratory Testing Explained,” www.satisﬁce.com/articles/ et-article.pdf, 2003.\n\nBach, Jonathan. “Session-Based Test Management,” Software Testing and Quality Engineering Magazine, November, 2000, www.satisﬁce.com/articles/ sbtm.pdf.\n\nBeck, Kent. Extreme Programming Explained: Embrace Change, Addison- Wesley, 2000.\n\nBeck, Kent, and Andres, Cynthia. Extreme Programming Explained: Embrace Change. 2nd Edition, Addison-Wesley, 2004.\n\nBerczuk, Stephen and Brad Appleton. Software Conﬁguration Management Patterns: Effective Teamwork, Practical Integration, Addison-Wesley, 2003.\n\n501",
      "content_length": 1191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 545,
      "content": "502\n\nBIBLIOGRAPHY\n\nBolton, Michael. “Testing Without a Map,” Better Software, January 2005, www.developsense.com/articles/Testing%20Without%20A%20Map.pdf.\n\nBos, Erik and Christ Vriens. “An Agile CMM,” in Extreme Programming and Agile Methods–XP/Agile Universe 2004, 4th Conference on Extreme Pro- gramming and Agile Methods, Calgary, Canada, August 15–18, 2004, Pro- ceedings, ed. Carmen Zannier, Hakan Erdogmus, Lowell Lindstrom, pp. 129–138, Springer, 2004.\n\nBoutelle, Jonathan. “Usability Testing for Agile Development,” www .jonathanboutelle.com/mt/archives/2005/08/usability_testi_1.html, 2005.\n\nBrown, Titus. “The (Lack of) Testing Death Spiral,” http://ivory.idyll.org/ blog/mar-08/software-quality-death-spiral.html, 2008.\n\nBuwalda, Hans. “Soap Opera Testing,” Better Software Magazine, February 2004, www.logigear.com/resources/articles_lg/soap_opera_testing.asp.\n\nClark, Mike. Pragmatic Project Automation: How to Build, Deploy and Moni- tor Java Apps, The Pragmatic Programmers, 2004.\n\nCohn, Mike. User Stories Applied for Agile Software Development, Addison- Wesley, 2004.\n\nCohn, Mike. Agile Estimating and Planning, Prentice Hall, 2005.\n\nCrispin, Lisa and Tip House. Testing Extreme Programming, Addison-Wesley 2002.\n\nCrispin, Lisa. Articles “Hiring an Agile Tester,” “An Agile Tool Selection Strat- egy for Web Testing Tools,” “Driving Software Quality: How Test-Driven De- velopment Impacts Software Quality,” http://lisa.crispin.home.att.net.\n\nDeMarco, Tom and Timothy Lister. Managing Risk on Software Projects, Dor- set House, 2003.\n\nDerby, Esther and Larsen, Diana. Agile Retrospectives: Making Good Teams Great, Pragmatic Bookshelf, 2006.\n\nDerby, Esther and Rothman, Johanna. Behind Closed Doors: Secrets of Great Management, Pragmatic Bookshelf, 2006.",
      "content_length": 1772,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 546,
      "content": "BIBLIOGRAPHY\n\nDe Souza, Ken. “A tester in developer’s clothes” blog, http://kendesouza .blogspot.com.\n\nDustin, Elfriede, Chris Wysopal, Lucas Nelson, and Dino Dia Zovi. The Art of Software Security Testing: Identifying Software Security Flaws, Symantec Press, 2006.\n\nDustin, Elfriede. “Teamwork Tackles the Quality Goal,” Software Test & Per- formance, Volume 2, Issue 200, March 2005.\n\nDuvall, Paul, Steve Matyas, and Andrew Glover. Continuous Integration: Im- proving Software Quality and Reducing Risk, Addison-Wesley, 2007.\n\nEckstein, Jutta. Agile Software Development in the Large: Diving Into the Deep, Dorset House, 2004.\n\nEvans, Eric. Domain-Driven Design: Tackling Complexity in the Heart of Soft- ware, Addison-Wesley, 2003.\n\nFeathers, Michael. Working Effectively with Legacy Code, Prentice Hall, 2004.\n\nFreeman, Steve and Nat Pryce. “Mock Objects,” www.mockobjects.com.\n\nFowler, Martin. “Continuous Integration,” http://martinfowler.com/articles/ continuousIntegration.html, 2006.\n\nFowler, Martin. “StranglerApplication,” www.martinfowler.com/bliki/ StranglerApplication.html, 2004.\n\nFowler, Martin, “TechnicalDebt,” http://martinfowler.com/bliki/ TechnicalDebt.html, 2003.\n\nGårtner, Markus, Blog, http://blog.shino.de.\n\nGalen, Robert. Software Endgames: Eliminating Defects, Controlling Change, and the Countdown to On-Time Delivery, Dorset House, 2005.\n\nGhiorghiu, Grig. “Performance vs. load vs. stress testing,” http:// agiletesting.blogspot.com/2005/02/performance-vs-load-vs-stress- testing.html, 2005.\n\nGhirghiu, Grig. “Agile Testing” blog, http://agiletesting.blogspot.com.\n\n503",
      "content_length": 1598,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 547,
      "content": "504\n\nBIBLIOGRAPHY\n\nHagar, Jon. Software Testing Papers, www.swtesting.com/hagar_papers_ index.html.\n\nHendrickson, Elisabeth. “Tester Developers, Developer Testers,” http:// testobsessed.com/2007/01/17/tester-developers-developer-testers/, 2007.\n\nHendrickson, Elisabeth. “Test Heuristics Cheat Sheet,” http://testobsessed.com/ wordpress/wp-content/uploads/2007/02/testheuristicscheatsheetv1.pdf, 2007.\n\nHendrickson, Elisabeth. “Agile-Friendly Test Automation Tools/Frame- works,” http://testobsessed.com/2008/04/29/agile-friendly-test-automation- toolsframeworks, 2008.\n\nHighsmith, Jim. Agile Project Management: Creating Innovative Products, Addison-Wesley, 2004.\n\nHunt, Andrew and David Thomas. The Pragmatic Programmer: From Jour- neyman to Master, Addison-Wesley, 1999.\n\nKaner, Cem, James Bach, and Bret Pettichord. Lessons Learned in Software Testing, Wiley, 2001.\n\nKerth, Norman. Project Retrospectives: A Handbook for Team Reviews, Dorset House, 2001.\n\nKniberg, Henrik. “How to Catch Up on Test Automation,” http:// blog.crisp.se/henrikkniberg/2008/01/03/1199386980000.html, 2008.\n\nKniberg, Henrik. Scrum and XP from the Trenches, Lulu.com, 2007.\n\nKoenig, Dierk, Andrew Glover, Paul King, Guillaume Laforge, and Jon Skeet. Groovy in Action, Manning Publications, 2007.\n\nKohl, Jonathan.“Man and Machine,” Better Software magazine, December 2007.\n\nKohl, Jonathan. Blog and articles, www.kohl.ca/.\n\nLouvion, Christophe. Blog, www.runningagile.com.",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 548,
      "content": "BIBLIOGRAPHY\n\nManns, Mary Lynn and Linda Rising. Fearless Change: Patterns for Introduc- ing New Ideas, Addison-Wesley, 2004.\n\nMarick, Brian. Everyday Scripting with Ruby: For Teams, Testers and You, Pragmatic Bookshelf, 2007.\n\nMarick, Brian, “My Agile Testing Project,” www.exampler.com/old-blog/ 2003/ 08/21/, 2003.\n\nMarick, Brian. “An Alternative to Business-Facing TDD,” www.exampler .com/ blog/category/aa-ftt, 2008.\n\nMarick, Brian. Blog and articles on agile testing, http://exampler.com.\n\nMarcano, Antony. Blog, www.testingreﬂections.com.\n\nMeszaros, Gerard. XUnit Test Patterns: Refactoring Test Code, Addison- Wesley, 2007.\n\nMeszaros, Gerard and Janice Aston. “Adding Usability Testing to an Agile Project,” Agile 2006, Minneapolis, MN, 2006, http://papers.gerardmeszaros .com/AgileUsabilityPaper.pdf.\n\nMeszaros, Gerard, Ralph Bohnet, and Jennitta Andrea. “Agile Regression Testing Using Record & Playback,” XP/Agile Universe 2003, New Orleans, LA, 2003, http://agileregressiontestpaper.gerardmeszaros.com.\n\nMeszaros, Gerard. “Using Storyotypes to Split Bloated XP Stories,” http:// storyotypespaper.gerardmeszaros.com.\n\nMugridge, Rick and Ward Cunningham. Fit for Developing Software: Frame- work for Integrated Tests, Prentice Hall, 2005.\n\nNewkirk, James and Alexei Vorontsov. Test-Driven Development in Microsoft .NET, Microsoft Professional, 2004.\n\nNielsen, Jakob. “Time Budgets for Usability Sessions,” www.useit.com/ alertbox/usability_sessions.html, 2005.\n\nNorth, Dan. “Introducing BDD,” http://dannorth.net/introducing-bdd, 2006.\n\n505",
      "content_length": 1550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 549,
      "content": "506\n\nBIBLIOGRAPHY\n\nPatterson, Kerry, Joseph Gernny, Ben McMillan, Al Switzler and Stephen R. Covey. Crucial Conversations: Tools for Talking when the Stakes are High, McGraw-Hill, 2002.\n\nPatton, Jeff. “Test Software Before You Code,” StickyMinds.com, August 2006, www.stickyminds.com/sitewide.asp?Function=edetail&ObjectType= COL&ObjectId=11104.\n\nPatton, Jeff. “Holistic Agile Product Design and Development,” www.ag- ileproductdesign.com/blog/agile_product_development.html, 2006.\n\nPols, Andy. “The Perfect Customer,” www.pols.co.uk/archives/category/ testing, 2008.\n\nPettichord, Bret. “Homebrew Test Automation,” www.io.com/~wazmo/ papers/homebrew_test_automation_200409.pdf, 2004.\n\nPettichord, Bret. “Seven Steps to Test Automation Success,” www.io .com/ ~wazmo/papers/seven_steps.html, 2001.\n\nPoppendieck, Mary and Tom Poppendieck. Implementing Lean Software Development: From Concept to Cash, Addison-Wesley, 2006.\n\nPoppendieck, Mary and Tom Poppendieck. Lean Software Development: An Agile Toolkit, Addison-Wesley, 2003.\n\nRainsberger, J. B. JUnit Recipes: Practical Methods for Programmer Testing, Manning Publications, 2004.\n\nRasmusson, Jonathan. “Introducing XP into Greenﬁeld Projects: Lessons Learned,” IEEE Software, 2003, http://rasmusson.ﬁles.wordpress.com/2008/ 01/s3021.pdf.\n\nRobbins, Stephen and Tim Judge. Essentials of Organizational Behavior, 9th Edition, Prentice Hall, 2007.\n\nSchwaber, Ken. Agile Project Management with Scrum, Microsoft Press, 2004.\n\nShore, James and Shane Warden. The Art of Agile Development, O’Reilly Me- dia, 2007.",
      "content_length": 1557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 550,
      "content": "BIBLIOGRAPHY\n\nSoni, Mukesh. “Defect Prevention: Reducing Costs and Enhancing Quality,” iSixSigma, http://software.isixsigma.com/library/content/c060719b.asp.\n\nSumrell, Megan. “’Shout-Out’ Shoebox – Boosting Team Morale,” http:// megansumrell.wordpress.com/2007/08/27/shout-out-shoebox-boosting- team-morale, 2007.\n\nSutherland, Jeff, Carsten Ruseng Jakobsen, and Kent Johnson. “Scrum and CMMI Level 5: The Magic Potion for Code Warriors,” Agile 2007, Washington, DC, 2007, http://jeffsutherland.com/scrum/ Sutherland-ScrumCMMI6pages.pdf.\n\nTabaka, Jean. Collaboration Explained: Facilitation Skills for Software Project Leaders, Addison-Wesley, 2006.\n\nThomas, Mike. “Strangling Legacy Code,” Better Software magazine, October 2005, http://samoht.com/wiki_downloads/StranglingLegacyCodeArticle.pdf.\n\nTholfsen, Mike. “The Rise of the Customer Champions,” STAREAST, May 7–9, 2008.\n\nVoris, John. ADEPT AS400 Displays for External Prototyping and Testing, www.AdeptTesting.org.\n\nWake, Bill. “XP Radar Chart,” http://xp123.com/xplor/xp0012b/index.shtml, 2001.\n\nVriens, Christ. “Certifying for CMM Level 2 and ISO9001 with XP@Scrum,” in ADC 2003: Proceedings of the Agile Development Conference, 25–28 June 2003, Salt Lake City, UT, USA, 120–124, IEEE, 2003.\n\nTOOL REFERENCES Abbot Java GUI Test Framework, http://abbot.sourceforge.net/doc/ overview.shtml.\n\nAdzik, Gojko. DbFit: Test-driven Database Development, http://gojko.net/ ﬁtnesse/dbﬁt/.\n\nFaught, Danny. “Test Tools List,” http://testingfaqs.org, 2008.\n\n507",
      "content_length": 1506,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 551,
      "content": "508\n\nBIBLIOGRAPHY\n\nCanoo WebTest, Open Source Tool for Automated Testing of Web Applica- tions, http://webtest.canoo.com.\n\neasyb, Behavior Driven Development Framework for the Java Platform, www.easyb.org/.\n\nFit, Framework for Integrated Test, http://ﬁt.c2.com.\n\nJUnit, Resources for Test-Driven Development, www.junit.org.\n\nJUnitPerf, JUnit Test Decorators for Performance and Scalability Testing, http://clarkware.com/software/JUnitPerf.html.\n\nFitNesse, Fully Integrated Standalone Wiki and Acceptance Testing Frame- work, www.ﬁtnesse.org.\n\nHower, Rick, Software QA and Testing Tools Info, www.softwareqatest.com/ qattls1.html.\n\nNUnit, Unit-testing Framework for .NET Languages, http://nunit.org/ index.php.\n\nOpen Source Software Testing Tools, News and Discussion. www .opensourcetesting.org/.\n\nRpgUnit, RPG Regression Testing Framework, www.RPGunit.org.\n\nSelenium, Web Application Testing System, http://selenium.openqa.org.\n\nsoapUI, Web Services Testing Tool, www.soapui.org.\n\nSource Conﬁguration Management, http://better-scm.berlios.de.\n\nSubversion, Open Source Version Control System, http://subversion.tigris.org/.\n\nUnit Testing Frameworks. http://en.wikipedia.org/wiki/List_of_unit_ testing_frameworks.\n\nWatir, Web Application Testing in Ruby, http://wtr.rubyforge.org, http:// watircraft.com.",
      "content_length": 1303,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 552,
      "content": "INDEX\n\nA Abbot GUI test tool, 127 Acceptance tests. See also Business-facing tests\n\ndeﬁnition, 501 Remote Data Monitoring system example,\n\n245\n\nUAT (user acceptance testing) compared\n\nwith, 130\n\nAd hoc testing, 198 Adaptability, skills and, 39–40 ADEPT (AS400 Displays for External\n\nPrototyping and Testing), 117–118\n\nAdvance clarity\n\ncustomers speaking with one voice,\n\n373–374\n\ndetermining story size, 375–376 gathering all viewpoints regarding\n\nrequirements, 374–375\n\noverview of, 140–142, 373\n\nAdvance preparation downside of, 373 how much needed, 372–373\n\nAgile development\n\nAgile manifesto and, 3–4 barriers to. See Barriers to adopting agile\n\ndevelopment team orientation of, 6\n\nAgile Estimating and Planning (Cohn), 331, 332 Agile manifesto\n\npeople focus, 30 statement of, 4 value statements in, 21\n\nAgile principles. See Principles, for agile\n\ntesters\n\nAgile testers. See also Testers\n\nagile testing mind-set, 482–483 deﬁnition, 4 giving all team members equal weight, 31 hiring, 67–69 what they are, 19–20\n\nAgile testing\n\ndeﬁnition, 6 as mind-set, 20–21 what we mean, 4–7\n\nAgile values, 3–4 Alcea’s FIT IssueTrack, 84 Alpha tests, 466–467 ant, 284\n\nas build tool, 126 continual builds and, 175, 291\n\nAnthillPro, 126 ANTS Proﬁler Pro, 234 Apache JMeter. See JMeter API-layer functional test tools, 168–170\n\nFit and FitNesse, 168–170 overview of, 168 testing web Services, 170\n\nAPI testing\n\nautomating, 282 overview of, 205–206\n\nAPIs (application programming interfaces),\n\n501\n\nAppleton, Brad, 124 Application under test (AUT), 246\n\n509",
      "content_length": 1544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 553,
      "content": "510\n\nINDEX\n\nApplications\n\nintegration testing with external applications,\n\n459\n\nRemote Data Monitoring system example,\n\n242–243\n\nArchitecture\n\nincremental approach to testing, 114 layered, 116 Quadrant 1 tests and, 99 scalability and, 104, 221 testable, 30, 115, 182, 184, 267\n\nAS400 Displays for External Prototyping and\n\nTesting (ADEPT), 117–118\n\nAssumptions, hidden\n\nagile testers response to, 25 failure to detect, 32 questions that uncover, 136 worst-case scenarios and, 334\n\nAttitude\n\nagile testing mind-set, 482–483 barriers to adopting agile development, 48 vs. skills, 20\n\nAudits, compliance with audit requirements,\n\n89–90\n\nAUT (application under test), 143, 225, 246,\n\n317\n\nAuthorization, security testing and, 224 Automated regression testing key success factors, 484 release candidates and, 458 as a safety net, 261–262\n\nAutomated test lists, test plan alternatives,\n\n353–354 Automation\n\ncode ﬂux and, 269 of deployment, 232 driving development with, 262–263 of exploratory testing, 201 fear of, 269–270 feedback from, 262 freeing people for other work, 259–261 of functional test structure, 245–247 home-brewed test, 175 investment required, 267–268\n\nlearning curve, 266–267 legacy code and, 269 maintainability and, 227–228 manual testing vs., 258–259 obstacles to, 264–265 old habits and, 270 overview of, 255 programmers’ attitude regarding,\n\n265–266\n\nreasons for, 257–258 responding to change and, 29 ROI and, 264 task cards and, 394–395 testability and, 149–150 tests as documentation, 263–264\n\nAutomation strategy\n\nagile coding practices and, 303–304 applying one tool at a time, 312–313 data generation tools, 304–305 database access and, 306–310 design and maintenance and, 292–294 developing, 288–289 identifying tool requirements, 311–312 implementing, 316–319 iterative approach, 299–300 keep it simple, 298–299 learning by doing, 303 managing automated tests, 319 multi-layered approach to, 290–292 organizing test results, 322–324 organizing tests, 319–322 overview of, 273 principles, 298 record/playback tools and, 294, 296–297 starting with area of greatest pain,\n\n289–290\n\ntaking time to do it right, 301–303 test automation pyramid, 276–279 test categories, 274–276 tool selection, 294–298, 313–316 understanding purpose of tests and, 310–311 what can be automated, 279–285 what might be difﬁcult to automate,\n\n287–288",
      "content_length": 2350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 554,
      "content": "what should not be automated, 285–287 whole team approach, 300–301\n\nAutomation tools, 164–177\n\nAPI-layer functional test tools, 168–170 builds and, 126 GUI test tools, 170–176 overview of, 164–165 unit-level test tools, 165–168 web services test tool, 170\n\nB Bach, James, 195, 200, 212 Bach, Jonathan, 201 Back-end testing\n\nbehind the GUI, 282 non-UI testing, 204–205\n\nBamboo, 126 Barriers to adopting agile development, 44–49\n\nconﬂicting or multiple roles, 45 cultural differences among roles, 48–49 lack of training, 45 lack of understanding of agile concepts,\n\n45–48\n\nloss of identity, 44–45 overview of, 44 past experience and attitudes, 48\n\nBaselines\n\nbreak-test baseline technique, 363 performance, 235–237\n\nBatch\n\nﬁles, 251 processing, 345 scheduling process, 182\n\nBDD (Behavior-driven development)\n\neasyb tool, 166–168 tools for Quadrant 1 tests, 127\n\nBeck, Kent, 26, 99 Benander, Mark, 51 Benchmarking, 237 Berczuk, Stephen, 124 Beta testing, 466–467 Big picture\n\nagile testers focus on, 23 high-level tests and examples, 397–402\n\nINDEX\n\nkey success factors, 490–491 peril of forgetting, 148 regression tests and, 434\n\nBolton, Michael, 195 Bos, Erik, 114 Boundary conditions API testing and, 205 automation and, 11 data generation tools and, 304 identifying test variations, 410 writing test cases for, 137 Boyer, Erika, 140, 163, 372, 432 Brainstorming\n\nautomation giving testers better work,\n\n260\n\nprior to iteration, 370, 381 quadrants as framework for, 253 taking time for, 301 testers, 121\n\nBreak-test baseline technique, 363 Browsers, compatibility testing and, 230 Budget limits, 55 Bug tracking. See Defect tracking Bugs. See Defects Build\n\nautomating, 280–282 challenging release candidate builds, 473 deﬁnition, 501 incremental, 178–179 speeding up, 118–119\n\nBuild automation tools, 126, 282 Build/Operate/Check pattern, 180 Build tools, 126 BuildBeat, 126 Business analysts, 374 Business expert role\n\nagreement regarding requirements, 428,\n\n430\n\ncommon language and, 134, 291, 414 on customer team, 6–7 iteration demo and, 443 language of, 291 Power of Three and, 482 tools geared to, 134\n\n511",
      "content_length": 2113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 555,
      "content": "512\n\nINDEX\n\nBusiness-facing tests agile testing as, 6 Quadrants 2 & 3, 97–98 technology-facing tests compared with, 120\n\nBusiness-facing tests, critiquing the product\n\n(Quadrant 3), 189–215\n\nacceptance tests, 245 API testing, 205–206 demonstrations, 191–192 emulator tools, 213–214 end-to-end tests, 249–250 exploratory testing, 195–202, 248–249 generating test data, 212 GUI testing, 204 monitoring tools, 212–213 overview of, 189–191 reports, 208–210 scenario testing, 192–195 session-based testing, 200–201 setting up tests, 211–212 simulator tools, 213 tools for exploratory testing, 210–211 usability testing, 202–204 user acceptance testing, 250 user documentation, 207–208 web services testing, 207\n\nBusiness-facing tests, supporting team\n\n(Quadrant 2), 129–151 advance clarity, 140–142 automating functional tests, 245–247 common language and, 134–135 conditions of satisfaction and, 142–143 doneness, 146–147 driving development with, 129–132 eliciting requirements, 135–140 embedded testing, 248 incremental approach, 144–146 requirements quandary and, 132–134 ripple effects, 143–144 risk mitigation and, 147–149 testability and automation, 149–150 toolkit for. See Toolkit (Quadrant 2) web services testing, 247–248\n\nBusiness impact, 475–476\n\nBusiness value\n\nadding value, 31–33 as goal of agile development, 5–8, 69, 454 metrics and, 75 release cycles and, 3 role, function, business value pattern, 155 team approach and, 16\n\nBusse, Mike, 106, 235, 284, 313 Buwalda, Hans, 193\n\nC Canonical data, automating databases and,\n\n308–309 Canoo WebTest\n\nautomating GUI tests, 184, 186 GUI regression test suite, 291 GUI smoke tests, 300 GUI test tools, 174–175 organizing tests and, 320 scripts and, 320 XML Editor for, 125\n\nCapability Maturity Model Integration\n\n(CMMI), 90–91 Capture-playback tool, 267 Celebrating successes\n\nchange implementation and, 50–52 iteration wrap up and, 449–451\n\nChandra, Apurva, 377 Chang, Tae, 53–54 Change\n\ncelebrating successes, 50–52 giving team ownership, 50 introducing, 49 not coming easy, 56–57 responsiveness to, 28–29 talking about fears, 49–50\n\nChecklists\n\nrelease readiness, 474 tools for eliciting examples and requirements,\n\n156\n\nCI. See Continuous integration (CI) CI Factory, 126 CMMI (Capability Maturity Model\n\nIntegration), 90–91",
      "content_length": 2284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 556,
      "content": "Co-location, team logistics and, 65–66 Coaches\n\nadjusting to agile culture and, 40 learning curve and, 266 providing encouragement, 69 skill development and, 122 training and, 45–46 Cockburn, Alistair, 115 Code\n\nautomation and code ﬂux, 269 automation and legacy code, 269 automation strategy and, 303–304 documentation of, 251 standards, 227 writing testable, 115\n\nCode coverage, release metrics, 360–364 Coding and testing, 405–441 adding complexity, 407 alternatives for dealing with bugs, 424–428 choosing when to ﬁx bugs, 421–423 collaborating with programmers, 413–414 dealing with bugs, 416–419 deciding which bugs to log, 420–421 driving development and, 406 facilitating communication, 429–432 focusing on one story, 411–412 identifying variations, 410 iteration metrics, 435–440 media for logging bugs, 423–424 overview of, 405 Power of Three for resolving differences in\n\nviewpoint, 411\n\nregression testing and, 432–434 resources, 434–435 risk assessment, 407–409 as simultaneous process, 409–410,\n\n488–489\n\nstarting simple, 406, 428–429 talking to customers, 414–415 tests that critique the product, 412–413\n\nCohn, Mike, 50, 155, 276, 296, 331, 332 Collaboration\n\nwith customers, 396–397 key success factors, 489–490\n\nINDEX\n\nwith programmers, 413–414 whole team approach, 15–16\n\nCollino, Alessandro, 103, 363 Communication\n\ncommon language and, 134–135 with customer, 140, 396–397 DTS (Defect Tracking System) and, 83 facilitating, 23–25, 429–432 product delivery and, 462–463 size as challenge to, 42–43 between teams, 69–70 test results, 357–358\n\nComparisons, automating, 283 Compatibility testing, 229–230 Component tests\n\nautomating, 282 deﬁnition, 501 supporting function of, 5\n\nConditions of satisfaction\n\nbusiness-facing tests and, 142–143 deﬁnition, 501–502 Context-driven testing\n\ndeﬁnition, 502 quadrants and, 106–107 Continuous build process\n\nfailure notiﬁcation and, 112 feedback and, 119 FitNesse tests and, 357 implementing, 114 integrating tools with, 175, 311 source code control and, 124 what testers can do, 121\n\nContinuous feedback principle, 22 Continuous improvement principle, 27–28 Continuous integration (CI)\n\nautomating, 280–282 as core practice, 486–487 installability and, 231–232 Remote Data Monitoring system example, 244 running tests and, 111–112\n\nConversion, data migration and, 460–461 Core practices\n\ncoding and testing as one process, 488–489 continuous integration, 486–487\n\n513",
      "content_length": 2426,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 557,
      "content": "514\n\nINDEX\n\nCore practices, continued\n\nincremental approach, 488 overview of, 486 synergy between practices, 489 technical debt management, 487–488 test environments, 487\n\nCourage, principles, 25–26, 71 Credibility, building, 57 Critiquing the product\n\nbusiness facing tests. See Business-facing tests, critiquing the product (Quadrant 3)\n\ntechnology-facing tests. See\n\nTechnology-facing tests, critiquing the product (Quadrant 4) CrossCheck, testing Web Services, 170 CruiseControl, 126, 244, 291 Cultural change, 37. See also Organizations Cunningham, Ward, 106, 168, 506 Customer expectations\n\nbusiness impact and, 475–476 production support, 475\n\nCustomer-facing test. See Business-facing tests Customer support, DTS (Defect Tracking\n\nSystem) and, 82\n\nCustomer team\n\ndeﬁnition, 502 interaction between customer and developer\n\nteams, 8 overview of, 7 Customer testing\n\nAlpha/Beta testing, 466–467 deﬁnition, 502 overview of, 464 UAT (user acceptance testing), 464–466\n\nCustomers\n\ncollaborating with, 396–397, 489–490 considering all viewpoints during iteration\n\nplanning, 388–389 delivering value to, 22–23 importance of communicating with, 140,\n\n414–415, 444\n\niteration demo, 191–192, 443–444 participation in iteration planning,384–385\n\nrelationship with, 41–42 reviewing high-level tests with, 400 speaking with one voice, 373–374\n\nCVS, source code control and, 124\n\nD Data\n\nautomating creation or setup, 284–285 cleanup, 461 conversion, 459–461 release planning and, 348 writing task cards and, 392\n\nData-driven tests, 182–183 Data feeds, testing, 249 Data generation tools, 304–305 Data migration, automating, 310, 460 Databases\n\navoiding access when running tests, 306–310 canonical data and automation, 308–309 maintainability and, 228 product delivery and updates, 459–461 production-like data and automation,\n\n309–310\n\nsetting up/tearing down data for each\n\nautomated test, 307–308\n\ntesting data migration, 310\n\nDe Souza, Ken, 223 Deadlines, scope and, 340–341 Defect metrics\n\noverview of, 437–440 release metrics, 364–366\n\nDefect tracking, 79–86\n\nDTS (Defect Tracking System), 79–83 keeping focus and, 85–86 overview of, 79 reasons for, 79 tools for, 83–85\n\nDefect Tracking System. See DTS (Defect\n\nTracking System)\n\nDefects\n\nalternatives for dealing with bugs,\n\n424–428\n\nchoosing when to ﬁx bugs, 421–423",
      "content_length": 2318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 558,
      "content": "dealing with bugs, 416–419 deciding which bugs to log, 420–421 media for logging bugs, 423–424 metrics and, 79 TDD (test-driven development) and, 490 writing task cards and, 391–392 zero bug tolerance, 79, 418–419\n\nDeliverables\n\n“ﬁt and ﬁnish” deliverables, 454 nonsoftware, 470 overview of, 468–470\n\nDelivering product\n\nAlpha/Beta testing, 466–467 business impact and, 475–476 communication and, 462–463 customer expectations, 475 customer testing, 464 data conversion and database updates, 459–461 deliverables, 468–470 end game, 456–457 installation testing, 461–462 integration with external applications, 459 nonfunctional testing and, 458–459 overview of, 453 packaging, 474–475 planning time for testing, 455–456 post-development testing cycles, 467–468 production support, 475 release acceptance criteria, 470–473 release management, 470, 474 releasing product, 470 staging environment and, 458 testing release candidates, 458 UAT (user acceptance testing), 464–466 what if it is not ready, 463–464 what makes a product, 453–455\n\nDemos/demonstrations\n\nof an iteration, 443–444 value to customers, 191–192 Deployment, automating, 280–282 Design\n\nautomation strategy and, 292–294 designing with testing in mind, 115–118\n\nINDEX\n\nDetailed test cases\n\nart and science of writing, 178 big picture approach and, 148–149 designing with, 401\n\nDeveloper team\n\ninteraction between customer and\n\ndeveloper teams, 8\n\noverview of, 7–8\n\nDevelopment\n\nagile development, 3–4, 6 automated tests driving, 262–263 business-facing tests driving,\n\n129–132 coding driving, 406 post-development testing cycles,\n\n467–468\n\nDevelopment spikes, 381 Development team, 502 diff tool, 283 Distributed teams, 431–432\n\ndefect tracing systems, and, 82 physical logistics, 66 online high level tests for, 399 online story board for, 357 responding to change, 29 software-based tools to elicit examples and\n\nrequirements, and, 163–164\n\nDocumentation\n\nautomated tests as source of, 263–264 problems and ﬁxes, 417 reports, 208–210 of test code, 251 tests as, 402 user documentation, 207–208\n\nDoneness\n\nknowing when a story is done, 104–105 multitiered, 471–472\n\nDriving development with tests. See TDD\n\n(test-driven development) DTS (Defect Tracking System), 80–83\n\nbeneﬁts of, 80–82 choosing media for logging bugs, 424 documenting problems and ﬁxes, 417\n\n515",
      "content_length": 2330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 559,
      "content": "516\n\nINDEX\n\nDTS (Defect Tracking System), continued\n\nlogging bugs and, 420 reason for not using, 82–83\n\nDymond, Robin, xxx Dynamic analysis, security testing tools, 225\n\nE easyb behavior-driven development tool, 165–168 EasyMock, 127 Eclipse, 125, 316 Edge cases\n\nidentifying variations, 410 not having time for, 112 starting simple and then adding complexity,\n\n406–407 test cases for, 137\n\nEmbedded system, Remote Data Monitoring\n\nexample, 248\n\nEmpowerment, of teams, 44 Emulator tools, 213–214 End game\n\nAgile testing, 91 iteration, 14 product delivery and, 456–457 release and, 327\n\nEnd-to-end tests, 249–250 Enjoyment, principle of, 31 Environment, test environment, 347–348 Epic. See also Themes deﬁnition, 502 features becoming, 502 iterations in, 76, 329 planning, 252\n\nePlan Services, Inc., xli, 267 Errors, manual testing and, 259 Estimating story size, 332–338 eValid, 234 Event-based patterns, test design patterns, 181 Everyday Scripting with Ruby for Teams, Testers,\n\nand You (Marick), 297, 303\n\nExample-driven development, 378–380 Examples\n\nfor eliciting requirements, 136–137 tools for eliciting examples and requirements,\n\n155–156\n\nExecutable tests, 406 Exploratory testing (ET)\n\nactivities, characteristics, and skills (Hagar),\n\n198–200\n\nattributes of exploratory tester,\n\n201–202 automation of, 201 deﬁnition, 502–503 end game and, 457 explained (Bolton), 195–198 manual testing and, 280 monitoring tools, 212 overview of, 26, 195 Remote Data Monitoring system example,\n\n248–249\n\nsession-based testing and, 200–201 setup, 211–212 simulators and emulators, 212–213 tests that critique the product, 412–413 tools for, 210–212 tools for generating test data, 212 what should not be automated, 286\n\nExternal quality, business facing tests deﬁning,\n\n99, 131\n\nExternal teams, 43, 457 Extreme Programming. See XP (Extreme\n\nProgramming)\n\nExtreme Programming Explained (Beck),\n\n26\n\nF Face-to-face communication, 23–25 Failover tests, 232 Failure, courage to learn from, 25 Fake objects, 115, 118, 306, 502–503 Fault tolerance, product delivery and, 459 Fear\n\nbarriers to automation, 269–270 change and, 49–50\n\nFearless Change (Manns and Rising), 121 Feathers, Michael, 117, 288 Features\n\ndefects vs., 417–418 deﬁnition, 502–503 focusing on value, 341",
      "content_length": 2259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 560,
      "content": "Feedback\n\nautomated tests providing, 262 continuous feedback principle, 22 iterative approach and, 299–300 key success factors, 484–486 managing tests for, 323–324 Quadrant 1 tests and, 118–119 “Fit and ﬁnish” deliverables, 454 Fit (Framework for Integrated Test),\n\n134–135\n\nAPI-layer functional test tools, 168–169 automation test pyramid and, 278\n\nFIT IssueTrack, Alcea, 83–84 FitNesse\n\nadvantages of, 163 API-layer functional test tools, 169–170 automating functional tests with, 30,\n\n145\n\nbusiness-facing tests with, 154, 178 collaboration and, 164 continual builds and, 119, 357 data veriﬁcation with, 287 doneness and, 472 encouraging use of, 122 examples and, 136, 169 feedback and, 323–324 ﬁle parsing rules illustrated with, 205 functional testing behind the GUI, 291,\n\n300\n\nhome-grown scripts and, 305 JUnit compared with, 299 keywords or actions words for automating\n\ntests, 182–183\n\nmanual vs. automated testing, 210 memory demands of, 306 organizing tests and, 319–320 overview of, 168–170 remote testing and, 432 “start, stop, continue” list, 446 support for source code control tools,\n\n320\n\ntest automation pyramid and, 278 test cards and, 389–390 test cases as documentation, 402 test design and maintenance, 292\n\nINDEX\n\ntesting database layer with, 284 testing stories, 395 traceability requirements and, 88 user acceptance testing, 295 wikis and, 186\n\nFleisch, Patrick, 377, 440 Flow diagrams\n\nscenario testing and, 194–195 tools for eliciting examples and requirements,\n\n160–163 Fowler, Martin, 117 Framework for Integrated Test. See Fit\n\n(Framework for Integrated Test)\n\nFrameworks, 90–93 ftptt, 234 Functional analysts, 386 Functional testing\n\ncompatibility issues and, 230 deﬁnition, 502–503 end-to-end tests, 249–250 layers, 246 nonfunctional tests compared with, 225 Remote Data Monitoring system example,\n\n245–247\n\nG Galen, Bob, 455–456, 471 Gärtner, Markus, 395, 476 Geographically dispersed teams\n\ncoping with, 376–378 facilitating communication and,\n\n431–432\n\nGheorghiu, Grig, 225–226, 234 Glover, Andrew, 166 Greenﬁeld projects\n\ncode testing and, 116 deﬁnition, 502–503\n\nGUI (graphical user interface)\n\nautomation strategy and, 293 code ﬂux and, 269 standards, 227 GUI smoke tests\n\nCanoo WebTest and, 300 continual builds and, 119 defect metrics, 437\n\n517",
      "content_length": 2284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 561,
      "content": "518\n\nINDEX\n\nGUI test tools, 170–176\n\nCanoo Web Test, 174–175 “home-brewed” test automation tools,\n\n175\n\nopen source test tools, 172 overview of, 170–171 record/playback tools, 171–172 Ruby with Watir, 172–174 Selenium, 174\n\nGUI testing\n\nAPI testing, 205–206 automating, 282–283, 295–296 automation test pyramid and, 278 GUI smoke tests, 119, 300, 437 overview of, 204 Web service testing, 207\n\nH Hagar, Jon, 198 Hardware\n\ncompatibility and, 229 cost of test environments, 487 functional testing and, 230 investing in automation and, 267 production environment and, 310 scalability and, 233 test infrastructure, 319 testing product installation, 462 Hendrickson, Elisabeth, 203, 315–316 High-level test cases, 397–402\n\nmockups, 398–399 overview of, 397–398 reviewing with customers, 400 reviewing with programmers,\n\n400–401\n\ntest cases as documentation, 402\n\nHiring a tester, 67–69 Holzer, Jason, 220, 448 Home-grown test tool\n\nautomation tools, 314 GUI test tools, 175 test results, 323\n\nhttperf, 234 Hudson, 126\n\nI IBM Rational ClearCase, 124 IDEs (Integrated Development Environments)\n\ndeﬁnition, 502–503 log analysis tools, 212 tools for Quadrant 1 tests, 124–126\n\n“ility” testing\n\ncompatibility testing, 229–230 installability testing, 231–232 interoperability testing, 228–229 maintainability testing, 227–228 reliability testing, 230–231, 250–251 security testing, 223–227\n\nImpact, system-wide, 342 Implementing Lean Software Development: From Concept to Cash (Poppendieck), 74, 416\n\nImprovement\n\napproach to process improvement, 448–449 continuous improvement principle, 27–28 ideas for improvement from retrospectives,\n\n447–449\n\nIncremental development\n\nbuilding tests incrementally, 178–179 as core practice, 488 “ilities” tests and, 232 thin slices, small chunks, 144–146 traditional vs. agile testing, 12–13\n\nIndex cards, logging bugs on, 423 Infrastructure\n\nQuadrant 1 tests, 111–112 test infrastructure, 319 test plans and, 346–347 Installability testing, 231–232 Installation testing, 461–462 Integrated Development Environments. See\n\nIDEs (Integrated Development Environments) Integration testing\n\ninteroperability and, 229 product and external applications, 459\n\nIntelliJ IDEA, 125 Internal quality\n\nmeasuring internal quality of code, 99 meeting team standards, 366",
      "content_length": 2282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 562,
      "content": "Quadrant 1 tests and, 111 speed and, 112\n\nInteroperability testing, 228–229 Investment, automation requiring,\n\n267–268\n\nIteration\n\nautomation strategy and, 299–300 deﬁnition, 502–503 demo, 443–444 life of a tester and, 327 pre-iteration activities. See Pre-iteration\n\nactivities\n\nprioritizing stories and, 338 review, 415, 435–437 traditional vs. agile testing, 12–13\n\nIteration kickoff, 383–403\n\ncollaboration with customers, 396–397 considering all viewpoints, 385–389 controlling workload, 393 high-level tests and examples, 397–402 iteration planning, 383–384 learning project details, 384–385 overview of, 383 testable stories, 393–396 writing task cards, 389–392\n\nIteration metrics, 435–440 defect metrics, 437–440 measuring progress with, 435–437 overview of, 435 usefulness of, 439–440\n\nIteration planning\n\nconsidering all viewpoints, 385–389 controlling workload, 393 learning project details, 384–385 overview of, 383–384 writing task cards, 389–392 Iteration review meeting, 415 Iteration wrap up, 443–451\n\ncelebrating successes, 449–451 demo of iteration, 443–444 ideas for improvement, 447–449 retrospectives, 444–445 “start, stop, continue” exercise for retrospectives, 445–447\n\nINDEX\n\nITIL (Information Technology Infrastructure\n\nLibrary), 90–91\n\nJ JBehave, 165 JConsole, 234 JMeter\n\nperformance baseline tests, 235 performance testing, 223, 234, 313\n\nJMS (Java Messaging Service)\n\ndeﬁnition, 502–503 integration with external applications and,\n\n243\n\ntesting data feeds and, 249\n\nJProﬁler, 234 JUnit\n\nFitNesse as alternative for TDD, 299 functional testing, 176 load testing tools, 234–235 unit test tools, 126, 165, 291\n\nJUnitPerf, 234 Just in time development, 369. See also\n\nPre-iteration activities\n\nK Key success factors\n\nagile testing mind-set, 482–483 automating regression testing, 484 big picture approach, 490–491 coding and testing as one process, 488–489 collaboration with customers, 489–490 continuous integration (CI), 486–487 feedback, 484–486 foundation of core practices, 486 incremental approach (thin slices, small\n\nchunks), 488 overview of, 481 synergy between practices, 489 technical debt management, 487–488 test environments, 487 whole team approach, 482 Keyword-driven tests, 182–183 King, Joseph, 176 Knowledge base, DTS, 80–81\n\n519",
      "content_length": 2274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 563,
      "content": "520\n\nINDEX\n\nKohl, Jonathan, 201, 204, 211 König, Dierk, 320\n\nL Language, need for common, 134–135 Layered architecture, 116 Lean measurements, metrics, 74–75 Learning\n\nautomation strategy and, 303 continuous improvement principle, 27\n\nLearning curve, automation and, 266–267, 303 Legacy code, 269 Legacy code rescue (Feathers), 117 Legacy systems ccde, 269 deﬁnition, 502–503 logging bugs and, 421 testing, 117\n\nLessons Learned in Software Testing (Pettichord),\n\n485\n\nLessons learned sessions, 383. See also\n\nRetrospectives\n\nLightweight processes, 73–74 Lightweight test plans, 350 Load testing. See Performance and load testing LoadRunner, 234 LoadTest, 234 Logistics, physical, 65–66 LogWatch tool, 212 Loss of identity, QA teams fearing, 44–45 Louvion, Christophe, 63\n\nM Maintainability testing, 227–228 Management, 52–55\n\nadvance clarity and, 373–374 cultural change and, 52–54 overview of, 52 providing metrics to, 440\n\nManagers\n\ncultural changes for, 52–54 how to inﬂuence testing, 122–123 speaking managerís language, 55\n\nManns, Mary Lynn, 121–122\n\nManual testing\n\nautomation vs., 258–259 peril of, 289\n\nMarcano, Antony, 83, 426 Marick, Brian, 5, 24, 97, 134, 170, 203, 303 Martin, Micah, 169 Martin, Robert C., 169 Matrices\n\nhigh-level tests and, 398–399 text matrices, 350–353\n\nMaven, 126 McMahon, Chris, 260 Mean time between failure, reliability testing,\n\n230\n\nMean time to failure, reliability testing, 230 Media, for logging bugs, 423–424 Meetings\n\ndemonstrations, 71, 192 geographically dispersed, 376 iteration kickoff, 372 iteration planning, 23–24, 244, 331, 384, 389 iteration review, 71, 415 pre-planning, 370–372 release planning, 338, 345 retrospective, 447 scheduling, 70 sizing process and, 336–337 standup, 177, 429, 462 team participation and, 32 test planning, 263 Memory leaks, 237–238 Memory management testing, 237–238 Meszaros, Gerald, 99, 111, 113, 138, 146, 182,\n\n204, 291, 296, 430\n\nMetrics, 74–79\n\ncode coverage, 360–364 communication of, 77–78 defect metrics, 364–366, 437–440 iteration metrics, 435–440 justifying investment in automation, 268 lean measurements, 74–75 overview of, 74 passing tests, 358–360 reasons for tracking defects, 52, 75–77, 82",
      "content_length": 2187,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 564,
      "content": "release metrics, 358 ROI and, 78–79 what not to do with, 77 XP radar charts, 47–48\n\nMilestones, celebrating successes, 449–450 MIME (Multipurpose Internet Mail\n\nExtensions) deﬁnition, 504 testing data feeds and, 249\n\nMind maps, 156–158 Mind-set\n\nagile testing as, 20–21 key success factors, 482–483 pro-active, 369–370\n\n“Mini-waterfall” phenomenon, 46–47 Mock objects\n\ndeﬁnition, 504 risk alleviation and, 459 tools for implementing, 127 unit tests and, 114\n\nMock-ups\n\nfacilitating communication and, 430 high-level tests and, 398–399 stories and, 380 tools for eliciting examples and requirements,\n\n160\n\nModel-driven development, 398 Models\n\nquality models, 90–93 UI modeling example, 399 Monitoring tools, 212–213, 235 Multi-layered approach, automation strategy,\n\n290–292\n\nMultipurpose Internet Mail Extensions\n\n(MIME) deﬁnition, 504 testing data feeds and, 249\n\nN Naming conventions, 227 Nant, 126 Navigation, usability testing and, 204 NBehave, 165 NeoLoad, 234\n\nINDEX\n\nNessus, vulnerability scanner, 226 .NET Memory Proﬁler, 234 NetBeans, 125 NetScout, 235 Non-functional testing. See also\n\nTechnology-facing tests, critiquing the product (Quadrant 4)\n\ndelivering product and, 458–459 functional testing compared with, 225 requirements, 218–219 when to perform, 222\n\nNorth, Dan, 165 NSpec, 165 NUnit, 126, 165\n\nO Oleszkiewicz, Jakub, 418 One-off tests, 286–287 Open source tools\n\nagile open source test tools, 172–175 automation and, 314–315 GUI test tools, 172 IDEs, 124–125 OpenWebLoad, 234 Operating systems (OSs), compatibility testing\n\nand, 230\n\nOrganizations, 37–44\n\nchallenges of agile development, 35 conﬂicting cultures, 43 customer relationships and, 41–42 overview of, 37–38 quality philosophy, 38–40 size and, 42–43 sustainable pace of testing and, 40–41 team empowerment, 44\n\nOSs (operating systems), compatibility testing\n\nand, 230\n\nOwnership, giving team ownership, 50\n\nP Packaging, product delivery and, 474–475 Pair programming\n\ncode review and, 227 developers trained in, 61\n\n521",
      "content_length": 2003,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 565,
      "content": "522\n\nINDEX\n\nPair programming, continued\n\nIDEs and, 125 team approach and, 244\n\nPair testing, 413 Passing tests, release metrics, 358–360 PerfMon, 235 Perforce, 124 Performance and load testing\n\nautomating, 283 baselines, 235–237 memory management testing, 237–238 overview of, 234 product delivery and, 458 scalability testing, 233–234 test environment, 237 tools for, 234–235 when to perform, 223 who performs the test, 220–221 Performance, rewards and, 70–71 Perils\n\nforgetting the big picture, 148 quality police mentality, 39 the testing crunch, 416 waiting for Tuesdayís build, 280 youíre not really part of the team, 32\n\nPerkins, Steve, 156, 159, 373 PerlClip\n\ndata generation tools, 305 tools for generating test data, 212\n\nPersona testing, 202–204 Pettichord, Bret, 175, 264, 485 Phased and gated development, 73–74, 129 Physical logistics, 65–66 Planning\n\nadvance, 43 iteration. See Iteration planning release/theme planning. See Release planning testing. See Test planning\n\nPMO (Project Management Ofﬁce), 440 Pols, Andy, 134 Ports and Adapters pattern (Cockburn), 115 Post-development testing, 467–468 Post-iteration bugs, 421 Pounder, 234\n\nPower of Three\n\nbusiness expert and, 482 ﬁnding a common language, 430 good communication and, 33, 490 problem solving and, 24 resolving differences in viewpoint, 401, 411 whole team approach and, 482 Pragmatic Project Automation, 260 Pre-iteration activities, 369–382\n\nadvance clarity, 373 beneﬁts of working on stories in advance,\n\n370–372\n\ncustomers speaking with one voice, 373–374 determining story size, 375–376 evaluating amount of advance preparation\n\nneeded, 372–373\n\nexamples, 378–380 gathering all viewpoints regarding\n\nrequirements, 374–375\n\ngeographically dispersed team and,\n\n376–378 overview of, 369 prioritizing defects, 381 pro-active mindset, 369–370 resources, 381 test strategies and, 380–381 Pre-planning meeting, 370–372 Principles, automation\n\nagile coding practices, 303–304 iterative approach, 299–300 keep it simple, 298–299 learning by doing, 303 overview of, 298 taking time to do it right, 301–303 whole team approach, 300–301\n\nPrinciples, for agile testers continuous feedback, 22 continuous improvement, 27–28 courage, 25–26 delivering value to customer, 22–23 enjoyment, 31 face-to-face communication, 23–25 keeping it simple, 26–27 overview of, 21–22",
      "content_length": 2335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 566,
      "content": "people focus, 30 responsive to change, 28–29 self-organizing, 29–30 Prioritizing defects, 381 Prioritizing stories, 338–340 Pro-active mindset, 369–370 Product\n\nbusiness value, 31–33 delivery. See Delivering product tests that critique (Q3 & Q4), 101–104 what makes a product, 453–455\n\nProduct owner\n\nconsidering all viewpoints during iteration\n\nplanning, 386–389\n\ndeﬁnition, 504 iteration planning and, 384 Scrum roles, 141, 373 tools geared to, 134\n\nProduction\n\nlogging bugs and, 421 support, 475 Production code\n\nautomation test pyramid and, 277–278 deﬁnition, 504 delivering value to, 70 programmers writing, 48 source code control and, 434 synchronization with testing, 322 test-ﬁrst development and, 113 tests supporting, 303–304\n\nProduction-like data, automating databases\n\nand, 309–310\n\nProfessional development, 57 Proﬁling tools, 234 Programmers\n\nattitude regarding automation, 265–266 big picture tests, 397 collaboration with, 413–414 considering all viewpoints during iteration\n\nplanning, 387–389\n\nfacilitating communication and, 429–430 reviewing high-level tests with, 400–401 tester-developer ratio, 66–67 testers compared with, 4, 5\n\nINDEX\n\ntraining, 61 writing task cards and, 391\n\nProject Management Ofﬁce (PMO), 440 Projects, PAS example, 176–177 Prototypes\n\naccessible as common language, 134 mock-ups and, 160 paper, 22, 138–139, 380, 400, 414 paper vs. Wizard of Oz type, 275 UI (user interface), 107\n\nPulse, 126 PyUnit unit test tool for Python, 126\n\nQ QA (quality assurance)\n\ndeﬁnition, 504 in job titles, 31 independent QA team, 60 interchangeable with “test,” 59 whole team approach, 39 working on traditional teams, 9\n\nQuadrant 1. See Technology-facing tests, supporting team (Quadrant 1) Quadrant 2. See Business-facing tests, supporting team (Quadrant 2)\n\nQuadrant 3. See Business-facing tests, critiquing\n\nthe product (Quadrant 3)\n\nQuadrant 4. See Technology-facing tests, critiquing the product (Quadrant 4)\n\nQuadrants\n\nautomation test categories, 274–276 business facing (Q2 & Q3), 97–98 context-driven testing and, 106–108 critiquing the product (Q3 & Q4), 104 managing technical debt, 106 overview of, 97–98 as planning guide, 490 purpose of testing and, 97 Quadrant 1 summary, 99 Quadrant 2 summary, 99–100 Quadrant 3 summary, 101–102 Quadrant 4 summary, 102–104 shared responsibility and, 105–106 story completion and, 104–105\n\n523",
      "content_length": 2368,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 567,
      "content": "524\n\nINDEX\n\nQuadrants, continued\n\nsupporting the team (Q1 & Q2), 100–101 technology facing (Q1 & Q4), 97–98\n\nQuality\n\ncustomer role in setting quality standards, 26 models, 90–93 organizational philosophy regarding,\n\n38–40\n\nQuality assurance. See QA (quality assurance) Quality police mentality, 57 Questions, for eliciting requirements,\n\n135–136\n\nR Radar charts, XP, 47–48 Rasmusson, Jonathan, 11 Record/playback tools\n\nautomation strategy and, 294, 296–297 GUI test tools, 171–172\n\nRecovery testing, 459 Redundancy tests, 232 Reed, David, 171, 377 Refactoring\n\ndeﬁnition, 504 IDEs supporting, 124–126\n\nRegression suite, 434 Regression tests, 432–434\n\nautomated regression tests as a safety net,\n\n261–262\n\nautomating as success factor, 484 checking big picture, 434 deﬁnition, 504 exploratory testing and, 212 keeping the build “green,” 433 keeping the build quick, 433–434 logging bugs and, 420 regression suite and, 434 release candidates and, 458\n\nRelease\n\nacceptance criteria, 470–473 end game, 327, 456–457 management, 474 product delivery, 470 what if it is not ready, 463–464\n\nRelease candidates\n\nchallenging release candidate builds,\n\n473 deﬁnition, 505 testing, 458 Release metrics\n\ncode coverage, 360–364 defect metrics, 364–366 overview of, 358 passing tests, 358–360\n\nRelease notes, 474 Release planning, 329–367\n\noverview of, 329 prioritizing and, 338–340 purpose of, 330–331 scope, 340–344 sizing and, 332–337 test plan alternatives, 350–354 test planning, 345–350 visibility and, 354–366\n\nReliability testing\n\noverview of, 230–231 Remote Data Monitoring system example,\n\n250–251\n\nRemote Data Monitoring system example\n\nacceptance tests, 245 application, 242–243 applying test quadrants, 252–253 automated functional test structure,\n\n245–247\n\ndocumenting test code, 251 embedded testing, 248 end-to-end tests, 249–250 exploratory testing, 248–249 overview of, 242 reliability testing, 250–251 reporting test results, 251 team and process, 243–244 testing data feeds, 249 unit tests, 244–245 user acceptance testing, 250 web services, 247–248\n\nRemote team member. See Geographically\n\ndispersed teams",
      "content_length": 2113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 568,
      "content": "Repetitive tasks, automating, 284 Reports\n\ndocumentation and, 208–210 Remote Data Monitoring system example,\n\n251 Repository, 124 Requirements\n\nbusiness-facing tests addressing, 130 documentation of, 402 gathering all viewpoints regarding\n\nrequirements, 374–375\n\nhow to elicit, 135–140 nonfunctional, 218–219 quandary, 132–134 tools for eliciting examples and requirements,\n\n155–156\n\nResources\n\ncompleting stories and, 381 hiring agile tester, 67–69 overview of, 66 tester-developer ratio, 66–67 testing and, 434–435\n\nResponse time API, 411 load testing and, 234–235 measurable goals and, 76 web services and, 207\n\nRetrospectives\n\ncontinuous improvement and, 28 ideas for improvement, 447–449 iteration planning and, 383 overview of, 444–445 process improvement and, 90 “start, stop, and continue” exercise,\n\n445–447\n\nReturn on investment. See ROI (return on\n\ninvestment)\n\nRewards, performance and, 70–71 Rich-client unit testing tools, 127 Rising, Linda, 121–122 Risk\n\nrisk analysis, 198, 286, 290, 345–346 risk assessment, 407–409 test mitigating, 147–149\n\nINDEX\n\nRogers, Paul, 242, 310, 388, 398 ROI (return on investment)\n\nautomation and, 264 deﬁnition, 505 lean measurement and, 75 metrics and, 78–79 speaking managerís language, 55\n\nRole, function, business value pattern, 155 Roles\n\nconﬂicting or multiple roles, 45 cultural differences among, 48–49 customer team, 7 developer team, 7–8 interaction of, 8\n\nRPGUnit, 118 RSpec, 165, 318 Ruby Test::Unit, 170 Ruby with Watir\n\nfunctional testing, 247 GUI testing, 285 identifying defects with, 212 keywords or actions words for automating\n\ntests, 182\n\noverview of, 172–174 test automation with, 186\n\nRubyMock, 127 Rules, managing bugs and, 425\n\nS Safety tests, 232 Santos, Rafael, 448 Satisfaction conditions. See Conditions of\n\nsatisfaction\n\nScalability testing, 233–234 Scenario testing, 192–193\n\nﬂow diagrams and, 194–195 overview of, 192–195 soap opera tests, 193\n\nScope, 340–344\n\nbusiness-facing tests deﬁning, 134 deadlines and timelines and, 340–341 focusing on value, 341–342 overview of, 340 system-wide impact, 342\n\n525",
      "content_length": 2082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 569,
      "content": "526\n\nINDEX\n\nScope, continued\n\ntest plans and, 345 third-party involvement and, 342–344\n\nScope creep, 385, 412 Scripts\n\nautomating comparisons, 283 as automation tools, 297 conversion scripts, 461 data generation tools, 305 exploratory testing and, 211–212\n\nScrum\n\nproduct owner role, 141, 373 Remote Data Monitoring system example,\n\n244\n\nsprint reviews, 444\n\nScrumMaster\n\napproach to process improvement, 448–449 sizing stories and, 336–337 writing task cards and, 391\n\nSDD (story test-driven development)\n\nidentifying variations, 410 overview of, 262–263 test-ﬁrst development and, 263 testing web services and, 170\n\nSecurity testing\n\noutside-in approach of attackers, 225 overview of, 223–227 specialized knowledge required for, 220\n\nSelenium\n\nGUI test tools, 174–175 implementing automation, 316–318 open source tools, 163 test automation with, 186, 316\n\nSelf-organization\n\nprinciples, 29–30 self-organizing teams, 69 Session-based testing, 200–201 Setup\n\nautomating, 284–285 exploratory testing, 211–212\n\nShared resources access to, 43 specialists as, 301 writing tasks and, 390\n\nShared responsibility, 105–106 Shout-Out Shoebox, 450 “Show me,” collaboration with programmers,\n\n413–414\n\nSimplicity\n\nautomation and, 298–299 coding, 406 logging bugs and, 428–429 principle of “keeping it simple,” 26–27\n\nSimulator tools\n\nembedded testing and, 248 overview of, 213\n\nSize, organizational, 42–43 Sizing stories, 332–337 example of, 334–337 how to, 332–333 overview of, 332 tester’s role in, 333–334\n\nSkills\n\nadaptability and, 39–40 vs. attitude, 20 continuous improvement principle,\n\n27\n\nwho performs tests and, 220–221\n\nSmall chunks, incremental development,\n\n144–146\n\nSOAP\n\ndeﬁnition, 505 performance tests and, 223, 234\n\nSoap opera tests, 193 soapUI\n\ndeﬁnition, 505 performance tests and, 223, 234 testing Web Services, 170–171\n\nSOATest, 234 Software-based tools, 163 Software Conﬁguration Management Patterns: Effective Teamwork, Practical Integrations (Berczuk and Appleton), 124\n\nSoftware Endgames (Galen), 471 Source code control beneﬁts of, 255 overview of, 123–124 tools for, 124, 320",
      "content_length": 2092,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 570,
      "content": "SOX compliance, 469 Speak with one voice, customers, 373–374 Specialization, 220–221 Speed as a goal, 112 Spikes, development and test, 381 Spreadsheets\n\ntest spreadsheets, 353 tools for eliciting examples and requirements,\n\n159\n\nSprint reviews, 444. See also Demos/\n\ndemonstrations\n\nSQL*Loader, 460 Stability testing, 28 Staging environment, 458 Stand-up meetings, 177, 429, 462 Standards\n\nmaintainability and, 227 quality models and, 90–93\n\n“Start, stop, continue” exercise, retrospectives,\n\n445–447\n\nStatic analysis, security testing tools, 225 Steel thread, incremental development, 144,\n\n338, 345\n\nStories. See also Business-facing tests\n\nbeneﬁts of working on in advance of\n\niterations, 370–372\n\nbriefness of, 129–130 business-facing tests as, 130 determining story size, 375–376 focusing on one story when coding,\n\n411–412\n\nidentifying variations, 410 knowing when a story is done,\n\n104–105\n\nlogging bugs and, 420–421 mock-ups and, 380 prioritizing, 338–340 resources and, 381 scope and, 340 sizing. See Sizing stories starting simple, 133, 406 story tests deﬁned, 505 system-wide impact of, 342 test plans and, 345\n\nINDEX\n\ntest strategies and, 380–381 testable, 393–396 treating bugs as, 425\n\nStory boards\n\nburndown charts, 429 deﬁnition, 505–506 examples, 356–357 online, 357, 384 physical, 356 stickers and, 355 tasks, 222, 355, 436 virtual, 357, 384, 393 work in progress, 390\n\nStory cards\n\naudits and, 89 dealing with bugs and, 424–425 iteration planning and, 244 story narrative on, 409\n\nStory test-driven development. See SDD (story test-driven development) Strangler application (Fowler), 116–117 Strategy\n\nautomation. See Automation strategy test planning vs. test strategy, 86–87 test strategies, 380–381 Strategy, for writing tests\n\nbuilding tests incrementally, 178–179 iteration planning and, 372 keep the tests passing, 179 overview of, 177–178 test design patterns, 179–183 testability and, 183–185 Stress testing. See Load testing Subversion (SVN), 124, 320 Success factors. See Key success factors Successes, celebrating\n\nchange implementation and, 50–52 iteration wrap up and, 449–451\n\nSumrell, Megan, 365, 450 Sustainable pace, of testing, 40–41, 303 SVN (Subversion), 124, 320 SWTBot GUI test tool, 127 Synergy, between practices, 489 System, system-wide impact of story, 342\n\n527",
      "content_length": 2307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 571,
      "content": "528\n\nINDEX\n\nT tail-f, 212 Tartaglia, Coni, 439, 454, 470, 473 Task boards. See Story boards Task cards\n\nautomating testing and, 394–395 iteration planning and, 389–392 product delivery and, 462–463\n\nTasks\n\ncompleting testing tasks, 415–416 deﬁnition, 505–506\n\nTDD (test-driven development)\n\nautomated tests driving, 262–263 defects and, 490 deﬁnition, 506 overview of, 5 Test-First Development compared with,\n\n113–114\n\nunit tests and, 111, 244–245\n\nTeam City, 126 Team structure, 59–65\n\nagile project teams, 64–65 independent QA team, 60 integration of testers into agile project, 61–63 overview of, 59 traditional functional structure vs agile\n\nstructure, 64\n\nTeams\n\nautomation as team effort, 484 building, 69–71 celebrating success, 50–52 co-located, 65–66 controlling workload and, 393 customer, 7 developer, 7–8 empowerment of, 44 facilitating communication and, 429–432 geographically dispersed, 376–378, 431–432 giving all team members equal weight, 31 giving ownership to, 50 hiring agile tester for, 67–69 interaction between customer and developer\n\nteams, 8\n\niteration planning and, 384–385\n\nlogistics, 59 problem solving and, 123 Remote Data Monitoring system example,\n\n243–244\n\nshared responsibility and, 105–106 traditional, 9–10 using tests to support Quadrants 1 and 2,\n\n100–101\n\nwhole team approach. See Whole team\n\napproach\n\nworking on agile teams, 10–12\n\nTeardown, for tests, 307–308 Technical debt\n\ndefects as, 418 deﬁnition, 506 managing, 106, 487–488\n\nTechnology-facing tests\n\noverview of, 5 Quadrants 1 & 4, 97–98\n\nTechnology-facing tests, critiquing the product\n\n(Quadrant 4), 217–239\n\nbaselines, 235–237 coding and testing and, 412–413 compatibility testing, 229–230 installability testing, 231–232 interoperability testing, 228–229 maintainability testing, 227–228 memory management testing, 237–238 overview of, 217–219 performance and load testing, 234 performance and load testing tools, 234–235 reliability testing, 230–231, 250–251 scalability testing, 233–234 security testing, 223–227 test environment and, 237 when to use, 222–223 who performs the test, 220–222\n\nTechnology-facing tests, supporting team\n\n(Quadrant 1) build tools, 126 designing with testing in mind, 115–118 ease of accomplishing tasks, 114–115 IDEs for, 124–126 infrastructure supporting, 111–112",
      "content_length": 2297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 572,
      "content": "overview of, 109–110 purpose of, 110–111 source code control, 123–124 speed as beneﬁt of, 112–114 timely feedback, 118–119 toolkit for, 123 unit test tools, 126–127 unit tests, 244–245 what to do if team doesn’t perform these\n\ntests, 121–123\n\nwhere/when to stop, 119–121\n\nTest automation pyramid\n\nmulti-layered approach to automation and,\n\n290–291\n\noverview of, 276–279 three little pigs metaphor, 278\n\nTest behind UI, 282 Test cases\n\nadding complexity, 407 as documentation, 402 example-driven development, 379 identifying variations, 410 starting simple, 406\n\nTest coverage (and/or code coverage),\n\n360–364\n\nTest design patterns, 179–183\n\nBuild/Operate/Check pattern, 180 data-driven and keyword-driven tests,\n\n182–183 overview of, 179 test genesis patterns (Veragen), 179 time-based, activity, and event patterns, 181\n\nTest doubles\n\ndeﬁnition, 506 layered architectures and, 116\n\nTest-driven development. See TDD (test-driven\n\ndevelopment)\n\nTest environments, 237, 487 Test-First Development\n\ndeﬁnition, 506 TDD (test-driven development) compared\n\nwith, 113–114\n\nTest management, 186 Test management toolkit (Quadrant 2), 186\n\nINDEX\n\nTest plan alternatives, 350-354 Test planning, 345–350\n\nautomated test lists, test plan alternatives,\n\n353–354\n\ninfrastructure and, 346–347 overview of, 86, 345 reasons for writing, 345–346 test environment and, 347–348 test plan alternatives, 350–354 test plans, lightweight 350 test plan sample, 351 test strategy vs., 86–88 traceability and, 88 types of tests and, 346 where to start, 345\n\nTest results\n\ncommunicating, 357–358 organizing, 322–324 release planning and, 349–350\n\nTest skills. See Skills Test spikes, 381 Test spreadsheets, 353 Test strategy\n\niterations, pre-iteration activities and,\n\n380–381 test plan vs., 86–88\n\nTest stubs\n\ndeﬁnition, 506 integration with external applications and,\n\n459\n\nunit tests and, 127\n\nTest teams, 506–507. See also Teams Test tools. See also Toolkits\n\nAPI-layer functional, 168–170 exploratory testing, 210–211 generating test data with, 212 GUI tests, 170–176 home-brewed, 175 home-grown, 314 IDEs, 124–126 performance testing, 234–235 security testing, 225 unit-level tests, 126–127, 165–168 web service tests, 170\n\n529",
      "content_length": 2204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 573,
      "content": "530\n\nINDEX\n\nTest types\n\nalpha/beta, 466–467 exploratory. See Exploratory testing (ET) functional. See Functional testing GUI. See GUI testing integration, 229, 459 load. See Load testing performance. See Performance and load\n\ntesting\n\nreliability, 230–231, 250–251 security, 220, 223–227 stress. See Load testing unit. See Unit testing usability. See Usability testing user acceptance testing. See UAT (user\n\nacceptance testing)\n\nTest writing strategy. See Strategy, for writing\n\ntests\n\nTestability, 183–185\n\nautomated vs. manual Quadrant 2 tests, 185 automation and, 149–150 code design and test design and, 184–185 overview of, 183 of stories, 393–396\n\nTesters\n\nadding value, 12 agile testers, 4, 19–20 agile testing mindset, 20–21 automation allowing focus on more\n\nimportant work, 260\n\ncollaboration with customers, 396–397 considering all viewpoints during iteration\n\nplanning, 386–389\n\ncontrolling workload and, 393 deﬁnition, 507 facilitating communication, 429–430 feedback and, 486 hiring agile tester, 67–69 how to inﬂuence testing, 121–122 integration of testers into agile project,\n\n61–63\n\niterations and, 327 making job easier, 114–115 sizing stories, 333–334\n\ntester-developer ratio, 66–67 writing task cards and, 391\n\nTester's bill of rights, 49–50 Testing\n\ncoding and testing simultaneously, 409–410 completing testing tasks, 415–416 identifying variations, 410 managing, 320–322 organizing test results, 322–324 organizing tests, 319–322 planning time for, 455–456 post-development cycles, 467–468 quadrants. See Quadrants release candidates, 458 risk assessment and, 407–409 sustainable pace of, 40–41 traditional vs. agile, 12–15 transparency of tests, 321–322\n\nTesting in context\n\ncontext-driven testing and, 106–108 deﬁnition, 502\n\nTestNG GUI test tool, 127 Tests that never fail, 286 Text matrices, 350–353 The Grinder, 234 Themes. See also Release planning\n\ndeﬁnition, 507 prioritizing stories and, 339 writing task cards and, 392\n\nThin slices, incremental development and, 338 Third parties\n\ncompatibility testing and, 230 release planning and, 342–344 software, 163 Tholfsen, Mike, 203 Thomas, Mike, 116, 194 Three little pigs metaphor, 278 Timelines, scope and, 340–341 Toolkit (Quadrant 1) build tools, 126 IDEs, 124–126 overview of, 123 source code control, 123–124 unit test tools, 126–127",
      "content_length": 2318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 574,
      "content": "Toolkit (Quadrant 2)\n\nAPI-layer functional test tools, 168–170 automation tools, 164–165 building tests incrementally, 178–179 checklists, 156 ﬂow diagrams, 160–163 GUI test tools, 170–176 keep the tests passing, 179 mind maps, 156–158 mock-ups, 160 software-based tools, 163 spreadsheets, 159 strategies for writing tests, 177–178 test design patterns, 179–183 test management, 186 testability and, 183–185 tool strategy, 153–155 tools for eliciting examples and requirements,\n\n155–156\n\nunit-level test tools, 165–168 Web service test tool, 170\n\nToolkit (Quadrant 3)\n\nemulator tools, 213–214 monitoring tools, 212–213 simulator tools, 213 user acceptance testing, 250\n\nToolkit (Quadrant 4) baselines, 235–237 performance and load testing tools, 234–235\n\nTools\n\nAPI-layer functional test tools, 168–170 automation, 164–165 data generation, 304–305 defect tracking, 83–85 eliciting examples and requirements,\n\n155–156, 159–163\n\nemulator tools, 213–214 exploratory testing, 210–211 generating test data, 212 GUI test tools, 170–176 home-brewed, 175 home-grown, 314 IDEs, 124–126 load testing, 234–235\n\nINDEX\n\nmonitoring, 212–213 open source, 172, 314–315 performance testing, 234–235 for product owners and business experts, 134 security testing, 225 simulators, 213 software-based, 163 unit-level tests, 126–127, 165–168 vendor/commercial, 315–316 web service test tool, 170\n\nTools, automation\n\nagile-friendly, 316 applying one tool at a time, 312–313 home-brewed, 175 home-grown, 314 identifying tool requirements, 311–312 open source, 314–315 selecting, 294–298 vendors, 315–316\n\nTraceability\n\nDTS and, 82 matrices, 86 test planning and, 88\n\nTracking, test tasks and status, 354–357 Traditional processes, transitioning. See\n\nTransitioning traditional processes to agile\n\nTraditional teams, 9–10 Traditional vs. agile testing, 12–15 Training\n\nas deliverable, 469 lack of, 45\n\nTransitioning traditional processes to agile, 73–93\n\ndefect tracking. See Defect tracking existing process and, 88–92 lean measurements, 74–75 lightweight processes and, 73–74 metrics and, 74–79 overview of, 73 test planning. See Test planning\n\nU UAT (user acceptance testing)\n\npost-development testing cycles, 467–468 product delivery and, 464–466\n\n531",
      "content_length": 2230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 575,
      "content": "532\n\nINDEX\n\nUAT (user acceptance testing), continued\n\nin Quadrant 3, 102 release planning for, 331, 346 Remote Data Monitoring system example,\n\n250\n\nin test plan, 351 tryng out new features and, 102 writing at iteration kickoff meeting, 372\n\nUI (user interface). See also GUI (graphical user\n\ninterface)\n\nautomation strategy and, 293 modeling and, 399\n\nUnit test tools, 165–168. See also by individual\n\nunit tools\n\nbehavior-driven development tools, 166–168 list of, 126–127 overview of, 165\n\nUnit testing\n\nautomating, 282 BDD (Behavior-driven development), 165–168 deﬁnition, 507 metrics and, 76 supporting function of, 5 TDD (test-driven development) and, 111 technology-facing tests, 120 tools for Quadrant 1 tests, 126–127\n\nUsability testing, 202–204\n\nchecking out applications of competitors, 204 navigation and, 204 overview of, 202 users needs and persona testing, 202–204 what should not be automated, 285–286\n\nUse cases, 398 User acceptance testing. See UAT (user\n\nacceptance testing) User documentation, 207–208 User interface (UI). See also GUI (graphical user\n\ninterface)\n\nautomation strategy and, 293 modeling and, 399 User story. See Story User story card. See Story card\n\nUser Stories Applied for Agile Software\n\nDevelopment (Cohn), 155\n\nV Vaage, Carol, 330 Value\n\nadding, 31–33 delivering to customer, 22–23 focusing on, 341–342 testers adding, 12\n\nValues, agile, 3–4. See also Principles, for agile\n\ntesters\n\nVariations, coding and testing and, 410 Velocity\n\nautomation and, 255, 484 burnout rate and, 79 database impact on, 228 defects and, 487 deﬁnition, 507 maximizing, 370 sustainable pace of testing and, 41 taking time to do it right, 301 technical debt and, 106, 313, 418, 506\n\nVendors\n\nautomation tools, 315–316 capture-playback tool, 267 IDEs, 125 planning and, 342–344 source code control tools, 124 working with, 142, 349\n\nVeragen, Pierre, 76, 163, 179, 295, 363, 372,\n\n444\n\nVersion control, 123–124, 186. See also\n\nSource Code Control Viewpoints. See also Big picture\n\nconsidering all viewpoints during iteration\n\nplanning, 385–389\n\ngathering all viewpoints regarding\n\nrequirements, 374–375\n\nPower of Three and, 411 using multiple viewpoints in eliciting\n\nrequirement, 137–138\n\nVisibility, 354–366\n\ncode coverage, 360–364 communicating test results, 357–358 defect metrics, 364–366 number of passing tests, 358–360",
      "content_length": 2343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 576,
      "content": "overview of, 354 release metrics, 358 tracking test tasks and status, 354–357\n\nVisual Studio, 125 Voris, John, 117\n\nW Waterfall approach, to development\n\nagile development compared with, 12–13 ìmini-waterfallî phenomenon, 46–47 successes of, 112 test plans and, 346\n\nWatir (Web Application Testing in Ruby), 163, 172–174, 320. See also Ruby with Watir Web Services Description Language (WSDL),\n\n507\n\nWeb service testing automating, 282 overview of, 207 Remote Data Monitoring system example,\n\n247–248 tools for, 170–171\n\nWebLoad, 234 Whelan, Declan, 321 Whiteboards\n\nexample-driven development, 379 facilitating communication, 430 modeling, 399 planning diagram, 371 reviewing high-level tests with programmers,\n\n400–401\n\ntest plan alternatives, 353–354\n\nWhole team approach, 325\n\nadvantages of, 26 agile vs. traditional development, 15–16 automation strategy and, 300–301 budget limits and, 55 ﬁnding enjoyment in work and, 31 key success factors, 482, 491 pairing testers with programmers, 279\n\nINDEX\n\nshared responsibility and, 105–106 team building and, 69 team structure and, 59–62 to test automation, 270 test management and, 322 traditional cross-functional team compared\n\nwith, 64\n\nvalue of team members and, 70\n\nWiki\n\nas communication tool, 164 graphical documentation of examples,\n\n398–399 mockups, 160, 380 requirements, 402 story checklists and, 156 test cases, 372 traceability and, 88\n\nWilson-Welsh, Patrick, 278 Wizard of Oz Testing, 138–139 Workﬂow diagrams, 398 Working Effectively With Legacy Code (Feathers),\n\n117, 288\n\nWorkload, 393 Worst-case scenarios, 136, 334 Writing tests, strategy for. See Strategy, for\n\nwriting tests\n\nWSDL (Web Services Description Language),\n\n507\n\nX XP (Extreme Programming)\n\nagile team embracing, 10–11 courage as core value in, 25\n\nxUnit, 126–127\n\nY Yakich, Joe, 316\n\nZ Zero bug tolerance, 79, 418–419\n\n533",
      "content_length": 1855,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 577,
      "content": "FREE Online Edition\n\nYour purchase of Agile Testing includes access to a free online edition for 45 days through the Safari Books Online subscription service. Nearly every Addison-Wesley Professional book is available online through Safari Books Online, along with more than 5,000 other technical books and videos from publishers such as, Cisco Press, Exam Cram, IBM Press, O’Reilly, Prentice Hall, Que, and Sams.\n\nSAFARI BOOKS ONLINE allows you to search for a speciﬁ c answer, cut and paste code, download chapters, and stay current with emerging technologies.\n\nActivate your FREE Online Edition at www.informit.com/safarifree\n\nSTEP 1:\n\nEnter the coupon code: BTTZRBI.\n\nSTEP 2:\n\nNew Safari users, complete the brief registration form. Safari subscribers, just log in.\n\nIf you have difﬁ culty registering on Safari or accessing the online edition, please e-mail customer-service@safaribooksonline.com",
      "content_length": 901,
      "extraction_method": "Unstructured"
    }
  ]
}