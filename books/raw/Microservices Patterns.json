{
  "metadata": {
    "title": "Microservices Patterns",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 522,
    "conversion_date": "2025-12-19T17:36:08.277073",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Microservices Patterns.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-11)",
      "start_page": 1,
      "end_page": 11,
      "detection_method": "topic_boundary",
      "content": "M A N N I N G\nChris Richardson\n\n\nList of Patterns\nApplication architecture patterns\nMonolithic architecture (40)\nMicroservice architecture (40)\nDecomposition patterns\nDecompose by business capability (51)\nDecompose by subdomain (54)\nMessaging style patterns\nMessaging (85)\nRemote procedure invocation (72)\nReliable communications patterns\nCircuit breaker (78)\nService discovery patterns\n3rd party registration (85)\nClient-side discovery (83)\nSelf-registration (82)\nServer-side discovery (85)\nTransactional messaging patterns\nPolling publisher (98)\nTransaction log tailing (99)\nTransactional outbox (98)\nData consistency patterns\nSaga (114)\nBusiness logic design patterns\nAggregate (150)\nDomain event (160)\nDomain model (150)\nEvent sourcing (184)\nTransaction script (149)\nQuerying patterns\nAPI composition (223)\nCommand query responsibility segregation \n(228)\nExternal API patterns\nAPI gateway (259)\nBackends for frontends (265)\nTesting patterns\nConsumer-driven contract test (302)\nConsumer-side contract test (303)\nService component test (335)\nSecurity patterns\nAccess token (354) \nCross-cutting concerns patterns\nExternalized configuration (361)\nMicroservice chassis (379)\nObservability patterns\nApplication metrics (373)\nAudit logging (377)\nDistributed tracing (370)\nException tracking (376)\nHealth check API (366)\nLog aggregation (368)\nDeployment patterns\nDeploy a service as a container (393)\nDeploy a service as a VM (390)\nLanguage-specific packaging format (387)\nService mesh (380)\nServerless deployment (416)\nSidecar (410)\nRefactoring to microservices patterns\nAnti-corruption layer (447)\nStrangler application (432)\n \n\n\nMicroservices Patterns\n \n\n\nMicroservices Patterns\nWITH EXAMPLES IN JAVA\nCHRIS RICHARDSON\nM A N N I N G\nSHELTER ISLAND\n \n\n\nFor online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2019 by Chris Richardson. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning \nPublications was aware of a trademark claim, the designations have been printed in initial caps \nor all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nManning Publications Co.\nDevelopment editor: Marina Michaels\n20 Baldwin Road\nTechnical development editor: Christian Mennerich\nPO Box 761\nReview editor: Aleksandar Dragosavljevic´\nShelter Island, NY 11964\nProject editor: Lori Weidert\nCopy editor: Corbin Collins\nProofreader: Alyson Brener\nTechnical proofreader: Andy Miles\nTypesetter: Dennis Dalinnik\nCover designer: Marija Tudor\nISBN: 9781617294549\nPrinted in the United States of America\n1 2 3 4 5 6 7 8 9 10 – DP – 23 22 21 20 19 18\n \n\n\n Where you see wrong or inequality or injustice, speak out, because this is your country. \nThis is your democracy. Make it. Protect it. Pass it on.\n — Thurgood Marshall, Justice of the Supreme Court\n \n\n\nvii\nbrief contents\n1\n■\nEscaping monolithic hell\n1\n2\n■\nDecomposition strategies\n33\n3\n■\nInterprocess communication in a microservice \narchitecture\n65\n4\n■\nManaging transactions with sagas\n110\n5\n■\nDesigning business logic in a microservice \narchitecture\n146\n6\n■\nDeveloping business logic with event sourcing\n183\n7\n■\nImplementing queries in a microservice architecture\n220\n8\n■\nExternal API patterns\n253\n9\n■\nTesting microservices: Part 1\n292\n10\n■\nTesting microservices: Part 2\n318\n11\n■\nDeveloping production-ready services\n348\n12\n■\nDeploying microservices\n383\n13\n■\nRefactoring to microservices\n428\n \n\n\nix\ncontents\npreface\nxvii\nacknowledgments\nxx\nabout this book\nxxii\nabout the cover illustration\nxxvi\n1 \nEscaping monolithic hell\n1\n1.1\nThe slow march toward monolithic hell\n2\nThe architecture of the FTGO application\n3\n■The benefits of the \nmonolithic architecture\n4\n■Living in monolithic hell\n4\n1.2\nWhy this book is relevant to you\n7\n1.3\nWhat you’ll learn in this book\n7\n1.4\nMicroservice architecture to the rescue\n8\nScale cube and microservices\n8\n■Microservices as a form of \nmodularity\n11\n■Each service has its own database\n12\nThe FTGO microservice architecture\n12\n■Comparing the \nmicroservice architecture and SOA\n13\n1.5\nBenefits and drawbacks of the microservice \narchitecture\n14\nBenefits of the microservice architecture\n14\n■Drawbacks of the \nmicroservice architecture\n17\n \n\n\nCONTENTS\nx\n1.6\nThe Microservice architecture pattern language\n19\nMicroservice architecture is not a silver bullet\n19\n■Patterns and \npattern languages\n20\n■Overview of the Microservice architecture \npattern language\n23\n1.7\nBeyond microservices: Process and organization\n29\nSoftware development and delivery organization\n29\n■Software \ndevelopment and delivery process\n30\n■The human side of \nadopting microservices\n31\n2 \nDecomposition strategies\n33\n2.1\nWhat is the microservice architecture exactly?\n34\nWhat is software architecture and why does it matter?\n34\nOverview of architectural styles\n37\n■The microservice architecture \nis an architectural style\n40\n2.2\nDefining an application’s microservice architecture\n44\nIdentifying the system operations\n45\n■Defining services by \napplying the Decompose by business capability pattern\n51\nDefining services by applying the Decompose by sub-domain \npattern\n54\n■Decomposition guidelines\n56\n■Obstacles to \ndecomposing an application into services\n57\n■Defining service \nAPIs\n61\n3 \nInterprocess communication in a microservice architecture\n65\n3.1\nOverview of interprocess communication in a microservice \narchitecture\n66\nInteraction styles\n67\n■Defining APIs in a microservice \narchitecture\n68\n■Evolving APIs\n69\n■Message formats\n71\n3.2\nCommunicating using the synchronous Remote \nprocedure invocation pattern\n72\nUsing REST\n73\n■Using gRPC\n76\n■Handling partial failure \nusing the Circuit breaker pattern\n77\n■Using service discovery\n80\n3.3\nCommunicating using the Asynchronous messaging \npattern\n85\nOverview of messaging\n86\n■Implementing the interaction styles \nusing messaging\n87\n■Creating an API specification for a \nmessaging-based service API\n89\n■Using a message broker\n90\nCompeting receivers and message ordering\n94\n■Handling \nduplicate messages\n95\n■Transactional messaging\n97\nLibraries and frameworks for messaging\n100\n \n\n\nCONTENTS\nxi\n3.4\nUsing asynchronous messaging to improve \navailability\n103\nSynchronous communication reduces availability\n103\nEliminating synchronous interaction\n104\n4 \nManaging transactions with sagas\n110\n4.1\nTransaction management in a microservice \narchitecture\n111\nThe need for distributed transactions in a microservice \narchitecture\n112\n■The trouble with distributed \ntransactions\n112\n■Using the Saga pattern to maintain \ndata consistency\n114\n4.2\nCoordinating sagas\n117\nChoreography-based sagas\n118\n■Orchestration-based sagas\n121\n4.3\nHandling the lack of isolation\n126\nOverview of anomalies\n127\n■Countermeasures for handling the \nlack of isolation\n128\n4.4\nThe design of the Order Service and \nthe Create Order Saga\n132\nThe OrderService class\n133\n■The implementation of the Create \nOrder Saga\n135\n■The OrderCommandHandlers class\n142\nThe OrderServiceConfiguration class\n143\n5 \nDesigning business logic in a microservice architecture\n146\n5.1\nBusiness logic organization patterns\n147\nDesigning business logic using the Transaction script pattern\n149\nDesigning business logic using the Domain model pattern\n150\nAbout Domain-driven design\n151\n5.2\nDesigning a domain model using the \nDDD aggregate pattern\n152\nThe problem with fuzzy boundaries\n153\n■Aggregates have \nexplicit boundaries\n154\n■Aggregate rules\n155\n■Aggregate \ngranularity\n158\n■Designing business logic with aggregates\n159\n5.3\nPublishing domain events\n160\nWhy publish change events?\n160\n■What is a domain \nevent?\n161\n■Event enrichment\n161\n■Identifying domain \nevents\n162\n■Generating and publishing domain events\n164\nConsuming domain events\n167\n \n\n\nCONTENTS\nxii\n5.4\nKitchen Service business logic\n168\nThe Ticket aggregate\n169\n5.5\nOrder Service business logic\n173\nThe Order Aggregate\n175\n■The OrderService class\n180\n6 \nDeveloping business logic with event sourcing\n183\n6.1\nDeveloping business logic using event sourcing\n184\nThe trouble with traditional persistence\n185\n■Overview of event \nsourcing\n186\n■Handling concurrent updates using optimistic \nlocking\n193\n■Event sourcing and publishing events\n194\nUsing snapshots to improve performance\n195\n■Idempotent \nmessage processing\n197\n■Evolving domain events\n198\nBenefits of event sourcing\n199\n■Drawbacks of event \nsourcing\n200\n6.2\nImplementing an event store\n202\nHow the Eventuate Local event store works\n203\n■The Eventuate \nclient framework for Java\n205\n6.3\nUsing sagas and event sourcing together\n209\nImplementing choreography-based sagas using event sourcing\n210\nCreating an orchestration-based saga\n211\n■Implementing an \nevent sourcing-based saga participant\n213\n■Implementing saga \norchestrators using event sourcing\n216\n7 \nImplementing queries in a microservice architecture\n220\n7.1\nQuerying using the API composition pattern\n221\nThe findOrder() query operation\n221\n■Overview of the API \ncomposition pattern\n222\n■Implementing the findOrder() query \noperation using the API composition pattern\n224\n■API \ncomposition design issues\n225\n■The benefits and drawbacks \nof the API composition pattern\n227\n7.2\nUsing the CQRS pattern\n228\nMotivations for using CQRS\n229\n■Overview of CQRS\n232\nThe benefits of CQRS\n235\n■The drawbacks of CQRS\n236\n7.3\nDesigning CQRS views\n236\nChoosing a view datastore\n237\n■Data access module design\n239\nAdding and updating CQRS views\n241\n7.4\nImplementing a CQRS view with AWS DynamoDB\n242\nThe OrderHistoryEventHandlers module\n243\nData modeling and query design with DynamoDB\n244\nThe OrderHistoryDaoDynamoDb class\n249\n \n",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 12-20)",
      "start_page": 12,
      "end_page": 20,
      "detection_method": "topic_boundary",
      "content": "CONTENTS\nxiii\n8 \nExternal API patterns\n253\n8.1\nExternal API design issues\n254\nAPI design issues for the FTGO mobile client\n255\n■API design \nissues for other kinds of clients\n258\n8.2\nThe API gateway pattern\n259\nOverview of the API gateway pattern\n259\n■Benefits and \ndrawbacks of an API gateway\n267\n■Netflix as an example \nof an API gateway\n267\n■API gateway design issues\n268\n8.3\nImplementing an API gateway\n271\nUsing an off-the-shelf API gateway product/service\n271\nDeveloping your own API gateway\n273\n■Implementing an \nAPI gateway using GraphQL\n279\n9 \nTesting microservices: Part 1\n292\n9.1\nTesting strategies for microservice architectures\n294\nOverview of testing\n294\n■The challenge of testing \nmicroservices\n299\n■The deployment pipeline\n305\n9.2\nWriting unit tests for a service\n307\nDeveloping unit tests for entities\n309\n■Writing unit tests for value \nobjects\n310\n■Developing unit tests for sagas\n310\n■Writing \nunit tests for domain services\n312\n■Developing unit tests for \ncontrollers\n313\n■Writing unit tests for event and message \nhandlers\n315\n10 \nTesting microservices: Part 2\n318\n10.1\nWriting integration tests\n319\nPersistence integration tests\n321\n■Integration testing REST-based \nrequest/response style interactions\n322\n■Integration testing \npublish/subscribe-style interactions\n326\n■Integration contract \ntests for asynchronous request/response interactions\n330\n10.2\nDeveloping component tests\n335\nDefining acceptance tests\n336\n■Writing acceptance tests using \nGherkin\n337\n■Designing component tests\n339\n■Writing \ncomponent tests for the FTGO Order Service\n340\n10.3\nWriting end-to-end tests\n345\nDesigning end-to-end tests\n345\n■Writing end-to-end tests\n346\nRunning end-to-end tests\n346\n \n\n\nCONTENTS\nxiv\n11 \nDeveloping production-ready services\n348\n11.1\nDeveloping secure services\n349\nOverview of security in a traditional monolithic application\n350\nImplementing security in a microservice architecture\n353\n11.2\nDesigning configurable services\n360\nUsing push-based externalized configuration\n362\n■Using pull-\nbased externalized configuration\n363\n11.3\nDesigning observable services\n364\nUsing the Health check API pattern\n366\n■Applying the Log \naggregation pattern\n368\n■Using the Distributed tracing \npattern\n370\n■Applying the Application metrics pattern\n373\nUsing the Exception tracking pattern\n376\n■Applying the Audit \nlogging pattern\n377\n11.4\nDeveloping services using the Microservice chassis \npattern\n378\nUsing a microservice chassis\n379\n■From microservice chassis to \nservice mesh\n380\n12 \nDeploying microservices\n383\n12.1\nDeploying services using the Language-specific packaging \nformat pattern\n386\nBenefits of the Service as a language-specific package pattern\n388\nDrawbacks of the Service as a language-specific package \npattern\n389\n12.2\nDeploying services using the Service as a virtual machine \npattern\n390\nThe benefits of deploying services as VMs\n392\n■The drawbacks of \ndeploying services as VMs\n392\n12.3\nDeploying services using the Service as a container \npattern\n393\nDeploying services using Docker\n395\n■Benefits of deploying \nservices as containers\n398\n■Drawbacks of deploying services \nas containers\n399\n12.4\nDeploying the FTGO application with Kubernetes\n399\nOverview of Kubernetes\n399\n■Deploying the Restaurant service \non Kubernetes\n402\n■Deploying the API gateway\n405\nZero-downtime deployments\n406\n■Using a service mesh \nto separate deployment from release\n407\n \n\n\nCONTENTS\nxv\n12.5\nDeploying services using the Serverless deployment \npattern\n415\nOverview of serverless deployment with AWS Lambda\n416\nDeveloping a lambda function\n417\n■Invoking lambda \nfunctions\n417\n■Benefits of using lambda functions\n418\nDrawbacks of using lambda functions\n419\n12.6\nDeploying a RESTful service using AWS Lambda \nand AWS Gateway\n419\nThe design of the AWS Lambda version of Restaurant Service\n419\nPackaging the service as ZIP file\n424\n■Deploying lambda \nfunctions using the Serverless framework\n425\n13 \nRefactoring to microservices\n428\n13.1\nOverview of refactoring to microservices\n429\nWhy refactor a monolith?\n429\n■Strangling the monolith\n430\n13.2\nStrategies for refactoring a monolith to \nmicroservices\n433\nImplement new features as services\n434\n■Separate presentation \ntier from the backend\n436\n■Extract business capabilities into \nservices\n437\n13.3\nDesigning how the service and the monolith \ncollaborate\n443\nDesigning the integration glue\n444\n■Maintaining data \nconsistency across a service and a monolith\n449\n■Handling \nauthentication and authorization\n453\n13.4\nImplementing a new feature as a service: handling \nmisdelivered orders\n455\nThe design of Delayed Delivery Service\n456\n■Designing the \nintegration glue for Delayed Delivery Service\n457\n13.5\nBreaking apart the monolith: extracting delivery \nmanagement\n459\nOverview of existing delivery management functionality\n460\nOverview of Delivery Service\n462\n■Designing the Delivery Service \ndomain model\n463\n■The design of the Delivery Service integration \nglue\n465\n■Changing the FTGO monolith to interact with Delivery \nService\n467\nindex\n473\n \n\n\nxvii\npreface\nOne of my favorite quotes is\nThe future is already here—it’s just not very evenly distributed.\n—William Gibson, science fiction author\nThe essence of that quote is that new ideas and technology take a while to diffuse\nthrough a community and become widely adopted. A good example of the slow diffu-\nsion of ideas is the story of how I discovered microservices. It began in 2006, when,\nafter being inspired by a talk given by an AWS evangelist, I started down a path that\nultimately led to my creating the original Cloud Foundry. (The only thing in common\nwith today’s Cloud Foundry is the name.) Cloud Foundry was a Platform-as-a-Service\n(PaaS) for automating the deployment of Java applications on EC2. Like every other\nenterprise Java application that I’d built, my Cloud Foundry had a monolith architec-\nture consisting of a single Java Web Application Archive (WAR) file.\n Bundling a diverse and complex set of functions such as provisioning, configura-\ntion, monitoring, and management into a monolith created both development and\noperations challenges. You couldn’t, for example, change the UI without testing and\nredeploying the entire application. And because the monitoring and management\ncomponent relied on a Complex Event Processing (CEP) engine which maintained\nin-memory state we couldn’t run multiple instances of the application! That’s embar-\nrassing to admit, but all I can say is that I am a software developer, and, “let he who is\nwithout sin cast the first stone.”\n \n\n\nPREFACE\nxviii\n Clearly, the application had quickly outgrown its monolith architecture, but what was\nthe alternative? The answer had been out in the software community for some time at\ncompanies such as eBay and Amazon. Amazon had, for example, started to migrate away\nfrom the monolith around 2002 (https://plus.google.com/110981030061712822816/\nposts/AaygmbzVeRq). The new architecture replaced the monolith with a collection\nof loosely coupled services. Services are owned by what Amazon calls two-pizza teams—\nteams small enough to be fed by two pizzas.\n Amazon had adopted this architecture to accelerate the rate of software develop-\nment so that the company could innovate faster and compete more effectively. The\nresults are impressive: Amazon reportedly deploys changes into production every 11.6\nseconds!\n In early 2010, after I’d moved on to other projects, the future of software architec-\nture finally caught up with me. That’s when I read the book The Art of Scalability:\nScalable Web Architecture, Processes, and Organizations for the Modern Enterprise (Addison-\nWesley Professional, 2009) by Michael T. Fisher and Martin L. Abbott. A key idea in\nthat book is the scale cube, which, as described in chapter 2, is a three-dimensional\nmodel for scaling an application. The Y-axis scaling defined by the scale cube func-\ntionally decomposes an application into services. In hindsight, this was quite obvious,\nbut for me at the time, it was an a-ha moment! I could have solved the challenges I was\nfacing two years earlier by architecting Cloud Foundry as a set of services!\n In April 2012, I gave my first talk on this architectural approach, called “Decom-\nposing Applications of Deployability and Scalability” (www.slideshare.net/chris.e\n.richardson/decomposing-applications-for-scalability-and-deployability-april-2012). At\nthe time, there wasn’t a generally accepted term for this kind of architecture. I some-\ntimes called it modular, polyglot architecture, because the services could be written in\ndifferent languages.\n But in another example of how the future is unevenly distributed, the term micro-\nservice was used at a software architecture workshop in 2011 to describe this kind of\narchitecture (https://en.wikipedia.org/wiki/Microservices). I first encountered the\nterm when I heard Fred George give a talk at Oredev 2013, and I liked it!\n In January 2014, I created the https://microservices.io website to document archi-\ntecture and design patterns that I had encountered. Then in March 2014, James Lewis\nand Martin Fowler published a blog post about microservices (https://martinfowler\n.com/articles/microservices.html). By popularizing the term microservices, the blog\npost caused the software community to consolidate around the concept.\n The idea of small, loosely coupled teams, rapidly and reliably developing and deliv-\nering microservices is slowly diffusing through the software community. But it’s likely\nthat this vision of the future is quite different from your daily reality. Today, business-\ncritical enterprise applications are typically large monoliths developed by large teams.\nSoftware releases occur infrequently and are often painful for everyone involved. IT\noften struggles to keep up with the needs of the business. You’re wondering how on\nearth you can adopt the microservice architecture.\n \n\n\nPREFACE\nxix\n The goal of this book is to answer that question. It will give you a good understand-\ning of the microservice architecture, its benefits and drawbacks, and when to use it.\nThe book describes how to solve the numerous design challenges you’ll face, includ-\ning how to manage distributed data. It also covers how to refactor a monolithic appli-\ncation to a microservice architecture. But this book is not a microservices manifesto.\nInstead, it’s organized around a collection of patterns. A pattern is a reusable solution\nto a problem that occurs in a particular context. The beauty of a pattern is that\nbesides describing the benefits of the solution, it also describes the drawbacks and the\nissues you must address in order to successfully implement a solution. In my experi-\nence, this kind of objectivity when thinking about solutions leads to much better deci-\nsion making. I hope you’ll enjoy reading this book and that it teaches you how to\nsuccessfully develop microservices.\n \n\n\nxx\nacknowledgments\nAlthough writing is a solitary activity, it takes a large number of people to turn rough\ndrafts into a finished book.\n First, I want to thank Erin Twohey and Michael Stevens from Manning for their\npersistent encouragement to write another book. I would also like to thank my devel-\nopment editors, Cynthia Kane and Marina Michaels. Cynthia Kane got me started and\nworked with me on the first few chapters. Marina Michaels took over from Cynthia\nand worked with me to the end. I’ll be forever grateful for Marina’s meticulous and\nconstructive critiques of my chapters. And I want to thank the rest of the Manning\nteam who’s been involved in getting this book published.\n I’d like to thank my technical development editor, Christian Mennerich, my tech-\nnical proofreader, Andy Miles, and all my external reviewers: Andy Kirsch, Antonio\nPessolano, Areg Melik-Adamyan, Cage Slagel, Carlos Curotto, Dror Helper, Eros\nPedrini, Hugo Cruz, Irina Romanenko, Jesse Rosalia, Joe Justesen, John Guthrie,\nKeerthi Shetty, Michele Mauro, Paul Grebenc, Pethuru Raj, Potito Coluccelli, Shobha\nIyer, Simeon Leyzerzon, Srihari Sridharan, Tim Moore, Tony Sweets, Trent Whiteley,\nWes Shaddix, William E. Wheeler, and Zoltan Hamori.\n I also want to thank everyone who purchased the MEAP and provided feedback in\nthe forum or to me directly.\n I want to thank the organizers and attendees of all of the conferences and meetups\nat which I’ve spoken for the chance to present and revise my ideas. And I want to\nthank my consulting and training clients around the world for giving me the opportu-\nnity to help them put my ideas into practice.\n \n\n\nACKNOWLEDGMENTS\nxxi\n I want to thank my colleagues Andrew, Valentin, Artem, and Stanislav at Eventuate,\nInc., for their contributions to the Eventuate product and open source projects.\n Finally, I’d like to thank my wife, Laura, and my children, Ellie, Thomas, and Janet\nfor their support and understanding over the last 18 months. While I’ve been glued to\nmy laptop, I’ve missed out on going to Ellie’s soccer games, watching Thomas learn-\ning to fly on his flight simulator, and trying new restaurants with Janet.\n Thank you all!\n \n\n\nxxii\nabout this book\nThe goal of this book is to teach you how to successfully develop applications using\nthe microservice architecture.\n Not only does it discuss the benefits of the microservice architecture, it also\ndescribes the drawbacks. You’ll learn when you should consider using the monolithic\narchitecture and when it makes sense to use microservices.\nWho should read this book\nThe focus of this book is on architecture and development. It’s meant for anyone\nresponsible for developing and delivering software, such as developers, architects,\nCTOs, or VPs of engineering.\n The book focuses on explaining the microservice architecture patterns and other\nconcepts. My goal is for you to find this material accessible, regardless of the technol-\nogy stack you use. You only need to be familiar with the basics of enterprise applica-\ntion architecture and design. In particular, you need to understand concepts like\nthree-tier architecture, web application design, relational databases, interprocess com-\nmunication using messaging and REST, and the basics of application security. The\ncode examples, though, use Java and the Spring framework. In order to get the most\nout of them, you should be familiar with the Spring framework.\n \n \n \n",
      "page_number": 12
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 21-30)",
      "start_page": 21,
      "end_page": 30,
      "detection_method": "topic_boundary",
      "content": "ABOUT THIS BOOK\nxxiii\nRoadmap\nThis book consists of 13 chapters:\n■\nChapter 1 describes the symptoms of monolithic hell, which occurs when a\nmonolithic application outgrows its architecture, and advises on how to escape\nby adopting the microservice architecture. It also provides an overview of the\nmicroservice architecture pattern language, which is the organizing theme for\nmost of the book.\n■\nChapter 2 explains why software architecture is important and describes the\npatterns you can use to decompose an application into a collection of services.\nIt also explains how to overcome the various obstacles you typically encounter\nalong the way.\n■\nChapter 3 describes the different patterns for robust, interprocess communica-\ntion in a microservice architecture. It explains why asynchronous, message-\nbased communication is often the best choice.\n■\nChapter 4 explains how to maintain data consistency across services by using\nthe Saga pattern. A saga is a sequence of local transactions coordinated using\nasynchronous messaging.\n■\nChapter 5 describes how to design the business logic for a service using the\ndomain-driven design (DDD) Aggregate and Domain event patterns.\n■\nChapter 6 builds on chapter 5 and explains how to develop business logic using\nthe Event sourcing pattern, an event-centric way to structure the business logic\nand persist domain objects.\n■\nChapter 7 describes how to implement queries that retrieve data scattered\nacross multiple services by using either the API composition pattern or the\nCommand query responsibility segregation (CQRS) pattern.\n■\nChapter 8 covers the external API patterns for handling requests from a diverse\ncollection of external clients, such as mobile applications, browser-based Java-\nScript applications, and third-party applications.\n■\nChapter 9 is the first of two chapters on automated testing techniques for micro-\nservices. It introduces important testing concepts such as the test pyramid, which\ndescribes the relative proportions of each type of test in your test suite. It also\nshows how to write unit tests, which form the base of the testing pyramid.\n■\nChapter 10 builds on chapter 9 and describes how to write other types of tests in\nthe test pyramid, including integration tests, consumer contract tests, and com-\nponent tests.\n■\nChapter 11 covers various aspects of developing production-ready services,\nincluding security, the Externalized configuration pattern, and the service\nobservability patterns. The service observability patterns include Log aggrega-\ntion, Application metrics, and Distributed tracing.\n■\nChapter 12 describes the various deployment patterns that you can use to\ndeploy services, including virtual machines, containers, and serverless. It also\n \n\n\nABOUT THIS BOOK\nxxiv\ndiscusses the benefits of using a service mesh, a layer of networking software\nthat mediates communication in a microservice architecture.\n■\nChapter 13 explains how to incrementally refactor a monolithic architecture to\na microservice architecture by applying the Strangler application pattern: imple-\nmenting new features as services and extracting modules out of the monolith\nand converting them to services. \nAs you progress through these chapters, you’ll learn about different aspects of the\nmicroservice architecture.\nAbout the code\nThis book contains many examples of source code both in numbered listings and\ninline with normal text. In both cases, source code is formatted in a fixed-width font\nlike this to separate it from ordinary text. Sometimes code is also in bold to high-\nlight code that has changed from previous steps in the chapter, such as when a new\nfeature adds to an existing line of code. In many cases, the original source code has\nbeen reformatted; the publisher has added line breaks and reworked indentation to\naccommodate the available page space in the book. In rare cases, even this was not\nenough, and listings include line-continuation markers (➥). Additionally, comments\nin the source code have often been removed from the listings when the code is\ndescribed in the text. Code annotations accompany many of the listings, highlighting\nimportant concepts.\n Every chapter, except chapters 1, 2, and 13, contains code from the companion\nexample application. You can find the code for this application in a GitHub reposi-\ntory: https://github.com/microservices-patterns/ftgo-application.\nBook forum\nThe purchase of Microservices Patterns includes free access to a private web forum\nrun by Manning Publications where you can make comments about the book, ask\ntechnical questions, share your solutions to exercises, and receive help from the\nauthor and from other users. To access the forum and subscribe to it, point your web\nbrowser to https://forums.manning.com/forums/microservices-patterns. You can\nalso learn more about Manning’s forums and the rules of conduct at https://forums\n.manning.com/forums/about.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the author can take\nplace. It’s not a commitment to any specific amount of participation on the part of the\nauthor, whose contribution to the forum remains voluntary (and unpaid). We suggest\nyou try asking the author some challenging questions lest his interest stray! The forum\nand the archives of previous discussions will be accessible from the publisher’s website\nas long as the book is in print.\n \n\n\nABOUT THIS BOOK\nxxv\nOther online resources\nAnother great resource for learning the microservice architecture is my website http://\nmicroservices.io.\n Not only does it contain the complete pattern language, it also has links to other\nresources such as articles, presentations, and example code.\nAbout the author\nChris Richardson is a developer and architect. He is a Java Champion, a JavaOne rock\nstar, and the author of POJOs in Action (Manning, 2006), which describes how to build\nenterprise Java applications with frameworks such as Spring and Hibernate.\n Chris was also the founder of the original CloudFoundry.com, an early Java PaaS\nfor Amazon EC2.\n Today, he is a recognized thought leader in microservices and speaks regularly at\ninternational conferences. Chris is the creator of Microservices.io, a pattern language\nfor microservices. He provides microservices consulting and training to organizations\naround the world that are adopting the microservice architecture. Chris is working on\nhis third startup: Eventuate.io, an application platform for developing transactional\nmicroservices.\n \n\n\nxxvi\nabout the cover illustration\nJefferys\nThe figure on the cover of Microservices Patterns is captioned “Habit of a Morisco\nSlave in 1568.” The illustration is taken from Thomas Jefferys’ A Collection of the Dresses\nof Different Nations, Ancient and Modern (four volumes), London, published between\n1757 and 1772. The title page states that these are hand-colored copperplate engrav-\nings, heightened with gum arabic. \n Thomas Jefferys (1719–1771) was called “Geographer to King George III.” He was\nan English cartographer who was the leading map supplier of his day. He engraved\nand printed maps for government and other official bodies and produced a wide\nrange of commercial maps and atlases, especially of North America. His work as a map\nmaker sparked an interest in local dress customs of the lands he surveyed and\nmapped, which are brilliantly displayed in this collection. Fascination with faraway\nlands and travel for pleasure were relatively new phenomena in the late 18th century,\nand collections such as this one were popular, introducing both the tourist as well as\nthe armchair traveler to the inhabitants of other countries.\n The diversity of the drawings in Jefferys’ volumes speaks vividly of the uniqueness\nand individuality of the world’s nations some 200 years ago. Dress codes have changed\nsince then, and the diversity by region and country, so rich at the time, has faded away.\nIt’s now often hard to tell the inhabitants of one continent from another. Perhaps, try-\ning to view it optimistically, we’ve traded a cultural and visual diversity for a more var-\nied personal life—or a more varied and interesting intellectual and technical life.\n \n\n\nABOUT THE COVER ILLUSTRATION\nxxvii\n At a time when it’s difficult to tell one computer book from another, Manning cel-\nebrates the inventiveness and initiative of the computer business with book covers\nbased on the rich diversity of regional life of two centuries ago, brought back to life by\nJeffreys’ pictures.\n \n\n\n1\nEscaping monolithic hell\nIt was only Monday lunchtime, but Mary, the CTO of Food to Go, Inc. (FTGO), was\nalready feeling frustrated. Her day had started off really well. She had spent the\nprevious week with other software architects and developers at an excellent confer-\nence learning about the latest software development techniques, including contin-\nuous deployment and the microservice architecture. Mary had also met up with her\nformer computer science classmates from North Carolina A&T State and shared\ntechnology leadership war stories. The conference had left her feeling empowered\nand eager to improve how FTGO develops software.\nThis chapter covers\nThe symptoms of monolithic hell and how to \nescape it by adopting the microservice \narchitecture\nThe essential characteristics of the microservice \narchitecture and its benefits and drawbacks\nHow microservices enable the DevOps style of \ndevelopment of large, complex applications\nThe microservice architecture pattern language \nand why you should use it\n \n\n\n2\nCHAPTER 1\nEscaping monolithic hell\n Unfortunately, that feeling had quickly evaporated. She had just spent the first\nmorning back in the office in yet another painful meeting with senior engineering\nand business people. They had spent two hours discussing why the development team\nwas going to miss another critical release date. Sadly, this kind of meeting had become\nincreasingly common over the past few years. Despite adopting agile, the pace of devel-\nopment was slowing down, making it next to impossible to meet the business’s goals.\nAnd, to make matters worse, there didn’t seem to be a simple solution.\n The conference had made Mary realize that FTGO was suffering from a case of\nmonolithic hell and that the cure was to adopt the microservice architecture. But the\nmicroservice architecture and the associated state-of-the-art software development\npractices described at the conference felt like an elusive dream. It was unclear to Mary\nhow she could fight today’s fires while simultaneously improving the way software was\ndeveloped at FTGO.\n Fortunately, as you will learn in this book, there is a way. But first, let’s look at the\nproblems that FTGO is facing and how they got there.\n1.1\nThe slow march toward monolithic hell\nSince its launch in late 2005, FTGO had grown by leaps and bounds. Today, it’s one of\nthe leading online food delivery companies in the United States. The business even\nplans to expand overseas, although those plans are in jeopardy because of delays in\nimplementing the necessary features.\n At its core, the FTGO application is quite simple. Consumers use the FTGO web-\nsite or mobile application to place food orders at local restaurants. FTGO coordinates\na network of couriers who deliver the orders. It’s also responsible for paying couriers\nand restaurants. Restaurants use the FTGO website to edit their menus and manage\norders. The application uses various web services, including Stripe for payments,\nTwilio for messaging, and Amazon Simple Email Service (SES) for email.\n Like many other aging enterprise applications, the FTGO application is a mono-\nlith, consisting of a single Java Web Application Archive (WAR) file. Over the years, it\nhas become a large, complex application. Despite the best efforts of the FTGO devel-\nopment team, it’s become an example of the Big Ball of Mud pattern (www.laputan\n.org/mud/). To quote Foote and Yoder, the authors of that pattern, it’s a “haphaz-\nardly structured, sprawling, sloppy, duct-tape and bailing wire, spaghetti code jungle.”\nThe pace of software delivery has slowed. To make matters worse, the FTGO applica-\ntion has been written using some increasingly obsolete frameworks. The FTGO appli-\ncation is exhibiting all the symptoms of monolithic hell.\n The next section describes the architecture of the FTGO application. Then it\ntalks about why the monolithic architecture worked well initially. We’ll get into how\nthe FTGO application has outgrown its architecture and how that has resulted in\nmonolithic hell.\n \n\n\n3\nThe slow march toward monolithic hell\n1.1.1\nThe architecture of the FTGO application\nFTGO is a typical enterprise Java application. Figure 1.1 shows its architecture. The\nFTGO application has a hexagonal architecture, which is an architectural style\ndescribed in more detail in chapter 2. In a hexagonal architecture, the core of the\napplication consists of the business logic. Surrounding the business logic are various\nadapters that implement UIs and integrate with external systems.\nThe business logic consists of modules, each of which is a collection of domain\nobjects. Examples of the modules include Order Management, Delivery Management,\nBilling, and Payments. There are several adapters that interface with the external sys-\ntems. Some are inbound adapters, which handle requests by invoking the business\nlogic, including the REST API and Web UI adapters. Others are outbound adapters,\nwhich enable the business logic to access the MySQL database and invoke cloud ser-\nvices such as Twilio and Stripe.\n Despite having a logically modular architecture, the FTGO application is packaged\nas a single WAR file. The application is an example of the widely used monolithic style\nInvoked by mobile applications\nTwilio\nmessaging\nservice\nCloud services\nFTGO application\nAWS SES\nemail\nservice\nStripe\npayment\nservice\nAdapters invoke\ncloud services.\nTwilio\nadapter\nCourier\nREST\nAPI\nWeb\nUI\nMySQL\nadapter\nRestaurant\nmanagement\nPayments\nBilling\nNotiﬁcation\nOrder\nmanagement\nDelivery\nmanagement\nAmazon\nSES\nadapter\nStripe\nadapter\nConsumer\nRestaurant\nMySQL\nFigure 1.1\nThe FTGO application has a hexagonal architecture. It consists of business logic \nsurrounded by adapters that implement UIs and interface with external systems, such as mobile \napplications and cloud services for payments, messaging, and email.\n \n\n\n4\nCHAPTER 1\nEscaping monolithic hell\nof software architecture, which structures a system as a single executable or deploy-\nable component. If the FTGO application were written in the Go language (GoLang),\nit would be a single executable. A Ruby or NodeJS version of the application would be\na single directory hierarchy of source code. The monolithic architecture isn’t inher-\nently bad. The FTGO developers made a good decision when they picked monolithic\narchitecture for their application. \n1.1.2\nThe benefits of the monolithic architecture\nIn the early days of FTGO, when the application was relatively small, the application’s\nmonolithic architecture had lots of benefits:\nSimple to develop—IDEs and other developer tools are focused on building a sin-\ngle application.\nEasy to make radical changes to the application—You can change the code and the\ndatabase schema, build, and deploy.\nStraightforward to test—The developers wrote end-to-end tests that launched the\napplication, invoked the REST API, and tested the UI with Selenium.\nStraightforward to deploy—All a developer had to do was copy the WAR file to a\nserver that had Tomcat installed.\nEasy to scale—FTGO ran multiple instances of the application behind a load\nbalancer.\nOver time, though, development, testing, deployment, and scaling became much more\ndifficult. Let’s look at why. \n1.1.3\nLiving in monolithic hell\nUnfortunately, as the FTGO developers have discovered, the monolithic architecture\nhas a huge limitation. Successful applications like the FTGO application have a habit\nof outgrowing the monolithic architecture. Each sprint, the FTGO development team\nimplemented a few more stories, which made the code base larger. Moreover, as the\ncompany became more successful, the size of the development team steadily grew.\nNot only did this increase the growth rate of the code base, it also increased the man-\nagement overhead.\n As figure 1.2 shows, the once small, simple FTGO application has grown over the\nyears into a monstrous monolith. Similarly, the small development team has now\nbecome multiple Scrum teams, each of which works on a particular functional area.\nAs a result of outgrowing its architecture, FTGO is in monolithic hell. Development is\nslow and painful. Agile development and deployment is impossible. Let’s look at why\nthis has happened.\nCOMPLEXITY INTIMIDATES DEVELOPERS\nA major problem with the FTGO application is that it’s too complex. It’s too large for\nany developer to fully understand. As a result, fixing bugs and correctly implementing\nnew features have become difficult and time consuming. Deadlines are missed.\n \n\n\n5\nThe slow march toward monolithic hell\nTo make matters worse, this overwhelming complexity tends to be a downward spiral.\nIf the code base is difficult to understand, a developer won’t make changes correctly.\nEach change makes the code base incrementally more complex and harder to under-\nstand. The clean, modular architecture shown earlier in figure 1.1 doesn’t reflect real-\nity. FTGO is gradually becoming a monstrous, incomprehensible, big ball of mud.\n Mary remembers recently attending a conference where she met a developer who\nwas writing a tool to analyze the dependencies between the thousands of JARs in their\nmultimillion lines-of-code (LOC) application. At the time, that tool seemed like some-\nthing FTGO could use. Now she’s not so sure. Mary suspects a better approach is to\nmigrate to an architecture that is better suited to a complex application: microservices. \nDEVELOPMENT IS SLOW\nAs well as having to fight overwhelming complexity, FTGO developers find day-to-day\ndevelopment tasks slow. The large application overloads and slows down a developer’s\nIDE. Building the FTGO application takes a long time. Moreover, because it’s so large,\nthe application takes a long time to start up. As a result, the edit-build-run-test loop\ntakes a long time, which badly impacts productivity. \nPATH FROM COMMIT TO DEPLOYMENT IS LONG AND ARDUOUS\nAnother problem with the FTGO application is that deploying changes into produc-\ntion is a long and painful process. The team typically deploys updates to production\nonce a month, usually late on a Friday or Saturday night. Mary keeps reading that the\nstate-of-the-art for Software-as-a-Service (SaaS) applications is continuous deployment:\nLarge\ndevelopment\norganization\nSingle code base creates\ncommunication and\ncoordination overhead.\nLarge, complex\nunreliable, difﬁcult\nto maintain\nThe path from code commit to\nproduction is arduous.\nChanges sit in a queue until\nthey can be manually tested.\nOrder management team\nRestaurant management team\nDelivery management team\nFTGO development\nProduction\nJenkins\nCl\nBacklog\nDeployment pipeline\nSource\ncode\nrepository\nManual\ntesting\nFTGO\napplication\nFigure 1.2\nA case of monolithic hell. The large FTGO developer team commits their changes to a \nsingle source code repository. The path from code commit to production is long and arduous and \ninvolves manual testing. The FTGO application is large, complex, unreliable, and difficult to maintain.\n \n",
      "page_number": 21
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 31-38)",
      "start_page": 31,
      "end_page": 38,
      "detection_method": "topic_boundary",
      "content": "6\nCHAPTER 1\nEscaping monolithic hell\ndeploying changes to production many times a day during business hours. Apparently,\nas of 2011, Amazon.com deployed a change into production every 11.6 seconds with-\nout ever impacting the user! For the FTGO developers, updating production more\nthan once a month seems like a distant dream. And adopting continuous deployment\nseems next to impossible.\n FTGO has partially adopted agile. The engineering team is divided into squads\nand uses two-week sprints. Unfortunately, the journey from code complete to running\nin production is long and arduous. One problem with so many developers committing\nto the same code base is that the build is frequently in an unreleasable state. When the\nFTGO developers tried to solve this problem by using feature branches, their attempt\nresulted in lengthy, painful merges. Consequently, once a team completes its sprint, a\nlong period of testing and code stabilization follows.\n Another reason it takes so long to get changes into production is that testing takes\na long time. Because the code base is so complex and the impact of a change isn’t well\nunderstood, developers and the Continuous Integration (CI) server must run the\nentire test suite. Some parts of the system even require manual testing. It also takes a\nwhile to diagnose and fix the cause of a test failure. As a result, it takes a couple of days\nto complete a testing cycle. \nSCALING IS DIFFICULT\nThe FTGO team also has problems scaling its application. That’s because different\napplication modules have conflicting resource requirements. The restaurant data, for\nexample, is stored in a large, in-memory database, which is ideally deployed on servers\nwith lots of memory. In contrast, the image processing module is CPU intensive and\nbest deployed on servers with lots of CPU. Because these modules are part of the same\napplication, FTGO must compromise on the server configuration. \nDELIVERING A RELIABLE MONOLITH IS CHALLENGING\nAnother problem with the FTGO application is the lack of reliability. As a result, there\nare frequent production outages. One reason it’s unreliable is that testing the applica-\ntion thoroughly is difficult, due to its large size. This lack of testability means bugs\nmake their way into production. To make matters worse, the application lacks fault iso-\nlation, because all modules are running within the same process. Every so often, a bug\nin one module—for example, a memory leak—crashes all instances of the applica-\ntion, one by one. The FTGO developers don’t enjoy being paged in the middle of the\nnight because of a production outage. The business people like the loss of revenue\nand trust even less. \nLOCKED INTO INCREASINGLY OBSOLETE TECHNOLOGY STACK\nThe final aspect of monolithic hell experienced by the FTGO team is that the archi-\ntecture forces them to use a technology stack that’s becoming increasingly obsolete. The\nmonolithic architecture makes it difficult to adopt new frameworks and languages. It\nwould be extremely expensive and risky to rewrite the entire monolithic application so\nthat it would use a new and presumably better technology. Consequently, developers\n \n\n\n7\nWhat you’ll learn in this book\nare stuck with the technology choices they made at the start of the project. Quite\noften, they must maintain an application written using an increasingly obsolete tech-\nnology stack.\n The Spring framework has continued to evolve while being backward compatible,\nso in theory FTGO might have been able to upgrade. Unfortunately, the FTGO applica-\ntion uses versions of frameworks that are incompatible with newer versions of Spring.\nThe development team has never found the time to upgrade those frameworks. As a\nresult, major parts of the application are written using increasingly out-of-date frame-\nworks. What’s more, the FTGO developers would like to experiment with non-JVM\nlanguages such as GoLang and NodeJS. Sadly, that’s not possible with a monolithic\napplication. \n1.2\nWhy this book is relevant to you\nIt’s likely that you’re a developer, architect, CTO, or VP of engineering. You’re responsi-\nble for an application that has outgrown its monolithic architecture. Like Mary at\nFTGO, you’re struggling with software delivery and want to know how to escape\nmonolith hell. Or perhaps you fear that your organization is on the path to mono-\nlithic hell and you want to know how to change direction before it’s too late. If you\nneed to escape or avoid monolithic hell, this is the book for you.\n This book spends a lot of time explaining microservice architecture concepts. My\ngoal is for you to find this material accessible, regardless of the technology stack you\nuse. All you need is to be familiar with the basics of enterprise application architecture\nand design. In particular, you need to know the following:\nThree-tier architecture\nWeb application design\nHow to develop business logic using object-oriented design\nHow to use an RDBMS: SQL and ACID transactions\nHow to use interprocess communication using a message broker and REST APIs\nSecurity, including authentication and authorization\nThe code examples in this book are written using Java and the Spring framework.\nThat means in order to get the most out of the examples, you need to be familiar with\nthe Spring framework too.\n1.3\nWhat you’ll learn in this book\nBy the time you finish reading this book you’ll understand the following:\nThe essential characteristics of the microservice architecture, its benefits and\ndrawbacks, and when to use it\nDistributed data management patterns\nEffective microservice testing strategies\nDeployment options for microservices\nStrategies for refactoring a monolithic application into a microservice architecture\n \n\n\n8\nCHAPTER 1\nEscaping monolithic hell\nYou’ll also be able to do the following:\nArchitect an application using the microservice architecture pattern\nDevelop the business logic for a service\nUse sagas to maintain data consistency across services\nImplement queries that span services\nEffectively test microservices\nDevelop production-ready services that are secure, configurable, and observable\nRefactor an existing monolithic application to services\n1.4\nMicroservice architecture to the rescue\nMary has come to the conclusion that FTGO must migrate to the microservice\narchitecture.\n Interestingly, software architecture has very little to do with functional require-\nments. You can implement a set of use cases—an application’s functional require-\nments—with any architecture. In fact, it’s common for successful applications, such as\nthe FTGO application, to be big balls of mud.\n Architecture matters, however, because of how it affects the so-called quality of ser-\nvice requirements, also called nonfunctional requirements, quality attributes, or ilities. As\nthe FTGO application has grown, various quality attributes have suffered, most nota-\nbly those that impact the velocity of software delivery: maintainability, extensibility,\nand testability.\n On the one hand, a disciplined team can slow down the pace of its descent toward\nmonolithic hell. Team members can work hard to maintain the modularity of their\napplication. They can write comprehensive automated tests. On the other hand, they\ncan’t avoid the issues of a large team working on a single monolithic application. Nor\ncan they solve the problem of an increasingly obsolete technology stack. The best a\nteam can do is delay the inevitable. To escape monolithic hell, they must migrate to a\nnew architecture: the Microservice architecture.\n Today, the growing consensus is that if you’re building a large, complex applica-\ntion, you should consider using the microservice architecture. But what are micro-\nservices exactly? Unfortunately, the name doesn’t help because it overemphasizes size.\nThere are numerous definitions of the microservice architecture. Some take the name\ntoo literally and claim that a service should be tiny—for example, 100 LOC. Others\nclaim that a service should only take two weeks to develop. Adrian Cockcroft, formerly\nof Netflix, defines a microservice architecture as a service-oriented architecture com-\nposed of loosely coupled elements that have bounded contexts. That’s not a bad defi-\nnition, but it is a little dense. Let’s see if we can do better.\n1.4.1\nScale cube and microservices\nMy definition of the microservice architecture is inspired by Martin Abbott and\nMichael Fisher’s excellent book, The Art of Scalability (Addison-Wesley, 2015). This\n \n\n\n9\nMicroservice architecture to the rescue\nbook describes a useful, three-dimensional scalability model: the scale cube, shown in\nfigure 1.3.\nThe model defines three ways to scale an application: X, Y, and Z.\nX-AXIS SCALING LOAD BALANCES REQUESTS ACROSS MULTIPLE INSTANCES\nX-axis scaling is a common way to scale a monolithic application. Figure 1.4 shows\nhow X-axis scaling works. You run multiple instances of the application behind a\nload balancer. The load balancer distributes requests among the N identical instances of\nthe application. This is a great way of improving the capacity and availability of an\napplication. \nZ-AXIS SCALING ROUTES REQUESTS BASED ON AN ATTRIBUTE OF THE REQUEST\nZ-axis scaling also runs multiple instances of the monolith application, but unlike X-axis\nscaling, each instance is responsible for only a subset of the data. Figure 1.5 shows how\nZ-axis scaling works. The router in front of the instances uses a request attribute to\nroute it to the appropriate instance. An application might, for example, route requests\nusing userId.\n In this example, each application instance is responsible for a subset of users. The\nrouter uses the userId specified by the request Authorization header to select one of\nMicroservices\nY-axis scaling,\na.k.a. functional\ndecomposition\nScale by splitting\nthings that are\ndifferent, such as\nby function.\nX-axis scaling,\na.k.a. horizontal duplication\nScale by cloning.\nZ-axis scaling,\na.k.a. data partitioning\nScale by splitting\nsimilar things, such as\nby customer ID.\nOne\ninstance\nMany\ninstances\nOne\npartition\nMany\npartitions\nMonolith\nFigure 1.3\nThe scale cube defines three separate ways to scale an application: X-axis \nscaling load balances requests across multiple, identical instances; Z-axis scaling routes \nrequests based on an attribute of the request; Y-axis functionally decomposes an application \ninto services.\n \n\n\n10\nCHAPTER 1\nEscaping monolithic hell\nthe N identical instances of the application. Z-axis scaling is a great way to scale an\napplication to handle increasing transaction and data volumes. \nY-AXIS SCALING FUNCTIONALLY DECOMPOSES AN APPLICATION INTO SERVICES\nX- and Z-axis scaling improve the application’s capacity and availability. But neither\napproach solves the problem of increasing development and application complexity. To\nsolve those, you need to apply Y-axis scaling, or functional decomposition. Figure 1.6 shows\nhow Y-axis scaling works: by splitting a monolithic application into a set of services.\nApplication\ninstance 1\nN identical application\ninstances\nApplication\ninstance 2\nLoad\nbalancer\nClient\nRequest\nApplication\ninstance 3\nRoute requests using a\nload balancing algorithm.\nFigure 1.4\nX-axis scaling runs multiple, identical instances of the monolithic \napplication behind a load balancer.\nApplication\ninstance 1\nN identical application\ninstances\nApplication\ninstance 2\nClient\nRouter\nRequest:\nGET /...\nAuthorization: userId:password\nApplication\ninstance 3\nUsers: a–h\nUsers: i-p\nUsers: r–z\nUses the userId to decide\nwhere to route requests\nEach instance is responsible\nfor a subset of the users.\nFigure 1.5\nZ-axis scaling runs multiple identical instances of the monolithic application behind \na router, which routes based on a request attribute . Each instance is responsible for a subset \nof the data.\n \n\n\n11\nMicroservice architecture to the rescue\nA service is a mini application that implements narrowly focused functionality, such as\norder management, customer management, and so on. A service is scaled using X-axis\nscaling, though some services may also use Z-axis scaling. For example, the Order ser-\nvice consists of a set of load-balanced service instances.\n The high-level definition of microservice architecture (microservices) is an archi-\ntectural style that functionally decomposes an application into a set of services. Note\nthat this definition doesn’t say anything about size. Instead, what matters is that each\nservice has a focused, cohesive set of responsibilities. Later in the book I discuss what\nthat means.\n Now let’s look at how the microservice architecture is a form of modularity. \n1.4.2\nMicroservices as a form of modularity\nModularity is essential when developing large, complex applications. A modern appli-\ncation like FTGO is too large to be developed by an individual. It’s also too complex\nto be understood by a single person. Applications must be decomposed into modules\nthat are developed and understood by different people. In a monolithic application,\nmodules are defined using a combination of programming language constructs (such\nas Java packages) and build artifacts (such as Java JAR files). However, as the FTGO\ndevelopers have discovered, this approach tends not to work well in practice. Long-\nlived, monolithic applications usually degenerate into big balls of mud.\n The microservice architecture uses services as the unit of modularity. A service has\nan API, which is an impermeable boundary that is difficult to violate. You can’t bypass\nOrder\nService\nApplication\nCustomer\nService\nClient\nReview\nService\nOrder\nrequests\nCustomer\nrequests\nReview\nrequests\nOrder\nService\ninstance 1\nOrder service\nOrder\nService\ninstance 2\nOrder\nService\ninstance 3\nLoad\nbalancer\nRequest\nY-axis scaling functionality decomposes\nan application into services.\nEach service is typically scaled using\nX-axis and possibly Z-axis scaling.\nFigure 1.6\nY-axis scaling splits the application into a set of services. Each service is responsible for \na particular function. A service is scaled using X-axis scaling and, possibly, Z-axis scaling.\n \n\n\n12\nCHAPTER 1\nEscaping monolithic hell\nthe API and access an internal class as you can with a Java package. As a result, it’s\nmuch easier to preserve the modularity of the application over time. There are other\nbenefits of using services as building blocks, including the ability to deploy and scale\nthem independently. \n1.4.3\nEach service has its own database\nA key characteristic of the microservice architecture is that the services are loosely\ncoupled and communicate only via APIs. One way to achieve loose coupling is by each\nservice having its own datastore. In the online store, for example, Order Service has a\ndatabase that includes the ORDERS table, and Customer Service has its database, which\nincludes the CUSTOMERS table. At development time, developers can change a service’s\nschema without having to coordinate with developers working on other services. At\nruntime, the services are isolated from each other—for example, one service will\nnever be blocked because another service holds a database lock.\nNow that we’ve defined the microservice architecture and described some of its essen-\ntial characteristics, let’s look at how this applies to the FTGO application. \n1.4.4\nThe FTGO microservice architecture\nThe rest of this book discusses the FTGO application’s microservice architecture in\ndepth. But first let’s quickly look at what it means to apply Y-axis scaling to this applica-\ntion. If we apply Y-axis decomposition to the FTGO application, we get the architec-\nture shown in figure 1.7. The decomposed application consists of numerous frontend\nand backend services. We would also apply X-axis and, possibly Z-axis scaling, so that\nat runtime there would be multiple instances of each service.\n The frontend services include an API gateway and the Restaurant Web UI. The API\ngateway, which plays the role of a facade and is described in detail in chapter 8, provides\nthe REST APIs that are used by the consumers’ and couriers’ mobile applications. The\nRestaurant Web UI implements the web interface that’s used by the restaurants to man-\nage menus and process orders.\n The FTGO application’s business logic consists of numerous backend services.\nEach backend service has a REST API and its own private datastore. The backend ser-\nvices include the following:\n\nOrder Service—Manages orders\n\nDelivery Service—Manages delivery of orders from restaurants to consumers\nDon’t worry: Loose coupling doesn’t make Larry Ellison richer\nThe requirement for each service to have its own database doesn’t mean it has its\nown database server. You don’t, for example, have to spend 10 times more on Oracle\nRDBMS licenses. Chapter 2 explores this topic in depth.\n \n\n\n13\nMicroservice architecture to the rescue\n\nRestaurant Service—Maintains information about restaurants\n\nKitchen Service—Manages the preparation of orders\n\nAccounting Service—Handles billing and payments\nMany services correspond to the modules described earlier in this chapter. What’s dif-\nferent is that each service and its API are very clearly defined. Each one can be inde-\npendently developed, tested, deployed, and scaled. Also, this architecture does a good\njob of preserving modularity. A developer can’t bypass a service’s API and access its\ninternal components. Chapter 13 describes how to transform an existing monolithic\napplication into microservices. \n1.4.5\nComparing the microservice architecture and SOA\nSome critics of the microservice architecture claim it’s nothing new—it’s service-\noriented architecture (SOA). At a very high level, there are some similarities. SOA\nand the microservice architecture are architectural styles that structure a system as a\nset of services. But as table 1.1 shows, once you dig deep, you encounter significant\ndifferences.\nAmazon\nSES\nAdapter\nTwilio\nAdapter\nStripe\nAdapter\nThe API Gateway routes\nrequests from the mobile\napplications to services.\nServices have APIs.\nA service’s data is private.\nServices corresponding\nto business capabilities/\ndomain-driven design\n(DDD) subdomains\nAPI\nGateway\nRestaurant\nWeb UI\nOrder\nService\nCourier\nREST\nAPI\nREST\nAPI\nREST\nAPI\nConsumer\nRestaurant\nRestaurant\nService\nREST\nAPI\nAccounting\nService\nREST\nAPI\nNotiﬁcation\nService\nREST\nAPI\nKitchen\nService\nREST\nAPI\nDelivery\nService\nREST\nAPI\nFigure 1.7\nSome of the services of the microservice architecture-based version of the FTGO \napplication. An API Gateway routes requests from the mobile applications to services. The services \ncollaborate via APIs.\n \n",
      "page_number": 31
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 39-46)",
      "start_page": 39,
      "end_page": 46,
      "detection_method": "topic_boundary",
      "content": "14\nCHAPTER 1\nEscaping monolithic hell\nSOA and the microservice architecture usually use different technology stacks. SOA\napplications typically use heavyweight technologies such as SOAP and other WS* stan-\ndards. They often use an ESB, a smart pipe that contains business and message-processing\nlogic to integrate the services. Applications built using the microservice architecture\ntend to use lightweight, open source technologies. The services communicate via dumb\npipes, such as message brokers or lightweight protocols like REST or gRPC.\n SOA and the microservice architecture also differ in how they treat data. SOA\napplications typically have a global data model and share databases. In contrast, as\nmentioned earlier, in the microservice architecture each service has its own database.\nMoreover, as described in chapter 2, each service is usually considered to have its own\ndomain model.\n Another key difference between SOA and the microservice architecture is the size\nof the services. SOA is typically used to integrate large, complex, monolithic applica-\ntions. Although services in a microservice architecture aren’t always tiny, they’re\nalmost always much smaller. As a result, a SOA application usually consists of a few\nlarge services, whereas a microservices-based application typically consists of dozens or\nhundreds of smaller services. \n1.5\nBenefits and drawbacks of the microservice \narchitecture\nLet’s first consider the benefits and then we’ll look at the drawbacks.\n1.5.1\nBenefits of the microservice architecture\nThe microservice architecture has the following benefits:\nIt enables the continuous delivery and deployment of large, complex applications.\nServices are small and easily maintained.\nServices are independently deployable.\nServices are independently scalable.\nThe microservice architecture enables teams to be autonomous.\nIt allows easy experimenting and adoption of new technologies.\nIt has better fault isolation.\nTable 1.1\nComparing SOA with microservices\nSOA\nMicroservices\nInter-service \ncommunication\nSmart pipes, such as Enterprise Ser-\nvice Bus, using heavyweight protocols, \nsuch as SOAP and the other WS* \nstandards.\nDumb pipes, such as a message \nbroker, or direct service-to-service \ncommunication, using lightweight \nprotocols such as REST or gRPC\nData\nGlobal data model and shared data-\nbases\nData model and database per service\nTypical service\nLarger monolithic application\nSmaller service\n \n\n\n15\nBenefits and drawbacks of the microservice architecture\nLet’s look at each benefit.\nENABLES THE CONTINUOUS DELIVERY AND DEPLOYMENT OF LARGE, COMPLEX APPLICATIONS\nThe most important benefit of the microservice architecture is that it enables continu-\nous delivery and deployment of large, complex applications. As described later in sec-\ntion 1.7, continuous delivery/deployment is part of DevOps, a set of practices for the\nrapid, frequent, and reliable delivery of software. High-performing DevOps organiza-\ntions typically deploy changes into production with very few production issues.\n There are three ways that the microservice architecture enables continuous deliv-\nery/deployment:\nIt has the testability required by continuous delivery/deployment—Automated testing is\na key practice of continuous delivery/deployment. Because each service in a\nmicroservice architecture is relatively small, automated tests are much easier to\nwrite and faster to execute. As a result, the application will have fewer bugs.\nIt has the deployability required by continuous delivery/deployment—Each service can\nbe deployed independently of other services. If the developers responsible for a\nservice need to deploy a change that’s local to that service, they don’t need to\ncoordinate with other developers. They can deploy their changes. As a result,\nit’s much easier to deploy changes frequently into production.\nIt enables development teams to be autonomous and loosely coupled—You can structure\nthe engineering organization as a collection of small (for example, two-pizza)\nteams. Each team is solely responsible for the development and deployment of\none or more related services. As figure 1.8 shows, each team can develop, deploy,\nand scale their services independently of all the other teams. As a result, the\ndevelopment velocity is much higher.\nThe ability to do continuous delivery and deployment has several business benefits:\nIt reduces the time to market, which enables the business to rapidly react to\nfeedback from customers.\nIt enables the business to provide the kind of reliable service today’s customers\nhave come to expect.\nEmployee satisfaction is higher because more time is spent delivering valuable\nfeatures instead of fighting fires.\nAs a result, the microservice architecture has become the table stakes of any business\nthat depends upon software technology. \nEACH SERVICE IS SMALL AND EASILY MAINTAINED\nAnother benefit of the microservice architecture is that each service is relatively small.\nThe code is easier for a developer to understand. The small code base doesn’t slow\ndown the IDE, making developers more productive. And each service typically starts a\nlot faster than a large monolith does, which also makes developers more productive\nand speeds up deployments. \n \n\n\n16\nCHAPTER 1\nEscaping monolithic hell\nSERVICES ARE INDEPENDENTLY SCALABLE\nEach service in a microservice architecture can be scaled independently of other ser-\nvices using X-axis cloning and Z-axis partitioning. Moreover, each service can be\ndeployed on hardware that’s best suited to its resource requirements. This is quite dif-\nferent than when using a monolithic architecture, where components with wildly dif-\nferent resource requirements—for example, CPU-intensive vs. memory-intensive—\nmust be deployed together. \nBETTER FAULT ISOLATION\nThe microservice architecture has better fault isolation. For example, a memory leak\nin one service only affects that service. Other services will continue to handle requests\nnormally. In comparison, one misbehaving component of a monolithic architecture\nwill bring down the entire system. \nEASILY EXPERIMENT WITH AND ADOPT NEW TECHNOLOGIES\nLast but not least, the microservice architecture eliminates any long-term commit-\nment to a technology stack. In principle, when developing a new service, the develop-\ners are free to pick whatever language and frameworks are best suited for that service.\nSmall, autonomous,\nloosely coupled teams\nEach service has\nits own source\ncode repository.\nEach service has\nits own automated\ndeployment pipeline.\nSmall, simple,\nreliable, easy to\nmaintain services\nOrder management team\nRestaurant management team\nDelivery management team\nFTGO development\nProduction\nJenkins Cl\nDeployment pipeline\nOrder Service\nsource code\nrepository\nOrder Service\nJenkins Cl\nDeployment pipeline\nRestaurant Service\nsource code\nrepository\nRestaurant Service\nJenkins Cl\nDeployment pipeline\nDelivery Service\nsource code\nrepository\nDelivery Service\nFigure 1.8\nThe microservices-based FTGO application consists of a set of loosely coupled services. \nEach team develops, tests, and deploys their services independently.\n \n\n\n17\nBenefits and drawbacks of the microservice architecture\nIn many organizations, it makes sense to restrict the choices, but the key point is that\nyou aren’t constrained by past decisions.\n Moreover, because the services are small, rewriting them using better languages\nand technologies becomes practical. If the trial of a new technology fails, you can\nthrow away that work without risking the entire project. This is quite different than\nwhen using a monolithic architecture, where your initial technology choices severely\nconstrain your ability to use different languages and frameworks in the future. \n1.5.2\nDrawbacks of the microservice architecture\nCertainly, no technology is a silver bullet, and the microservice architecture has a\nnumber of significant drawbacks and issues. Indeed most of this book is about how to\naddress these drawbacks and issues. As you read about the challenges, don’t worry.\nLater in this book I describe ways to address them.\n Here are the major drawbacks and issues of the microservice architecture:\nFinding the right set of services is challenging.\nDistributed systems are complex, which makes development, testing, and deploy-\nment difficult.\nDeploying features that span multiple services requires careful coordination.\nDeciding when to adopt the microservice architecture is difficult.\nLet’s look at each one in turn.\nFINDING THE RIGHT SERVICES IS CHALLENGING\nOne challenge with using the microservice architecture is that there isn’t a concrete,\nwell-defined algorithm for decomposing a system into services. As with much of soft-\nware development, it’s something of an art. To make matters worse, if you decompose\na system incorrectly, you’ll build a distributed monolith, a system consisting of coupled\nservices that must be deployed together. A distributed monolith has the drawbacks of\nboth the monolithic architecture and the microservice architecture. \nDISTRIBUTED SYSTEMS ARE COMPLEX\nAnother issue with using the microservice architecture is that developers must deal\nwith the additional complexity of creating a distributed system. Services must use an\ninterprocess communication mechanism. This is more complex than a simple method\ncall. Moreover, a service must be designed to handle partial failure and deal with the\nremote service either being unavailable or exhibiting high latency.\n Implementing use cases that span multiple services requires the use of unfamiliar\ntechniques. Each service has its own database, which makes it a challenge to implement\ntransactions and queries that span services. As described in chapter 4, a microservices-\nbased application must use what are known as sagas to maintain data consistency\nacross services. Chapter 7 explains that a microservices-based application can’t retrieve\ndata from multiple services using simple queries. Instead, it must implement queries\nusing either API composition or CQRS views.\n \n\n\n18\nCHAPTER 1\nEscaping monolithic hell\n IDEs and other development tools are focused on building monolithic applica-\ntions and don’t provide explicit support for developing distributed applications. Writ-\ning automated tests that involve multiple services is challenging. These are all issues\nthat are specific to the microservice architecture. Consequently, your organization’s\ndevelopers must have sophisticated software development and delivery skills in order\nto successfully use microservices.\n The microservice architecture also introduces significant operational complexity.\nMany more moving parts—multiple instances of different types of service—must be\nmanaged in production. To successfully deploy microservices, you need a high level of\nautomation. You must use technologies such as the following:\nAutomated deployment tooling, like Netflix Spinnaker\nAn off-the-shelf PaaS, like Pivotal Cloud Foundry or Red Hat OpenShift\nA Docker orchestration platform, like Docker Swarm or Kubernetes\nI describe the deployment options in more detail in chapter 12. \nDEPLOYING FEATURES SPANNING MULTIPLE SERVICES NEEDS CAREFUL COORDINATION\nAnother challenge with using the microservice architecture is that deploying features\nthat span multiple services requires careful coordination between the various develop-\nment teams. You have to create a rollout plan that orders service deployments based\non the dependencies between services. That’s quite different than a monolithic archi-\ntecture, where you can easily deploy updates to multiple components atomically. \nDECIDING WHEN TO ADOPT IS DIFFICULT\nAnother issue with using the microservice architecture is deciding at what point during\nthe lifecycle of the application you should use this architecture. When developing the\nfirst version of an application, you often don’t have the problems that this architec-\nture solves. Moreover, using an elaborate, distributed architecture will slow down\ndevelopment. That can be a major dilemma for startups, where the biggest problem is\nusually how to rapidly evolve the business model and accompanying application.\nUsing the microservice architecture makes it much more difficult to iterate rapidly. A\nstartup should almost certainly begin with a monolithic application.\n Later on, though, when the problem is how to handle complexity, that’s when it\nmakes sense to functionally decompose the application into a set of microservices.\nYou may find refactoring difficult because of tangled dependencies. Chapter 13 goes\nover strategies for refactoring a monolithic application into microservices.\n As you can see, the microservice architecture offer many benefits, but also has some\nsignificant drawbacks. Because of these issues, adopting a microservice architecture\nshould not be undertaken lightly. But for complex applications, such as a consumer-\nfacing web application or SaaS application, it’s usually the right choice. Well-known\nsites like eBay (www.slideshare.net/RandyShoup/the-ebay-architecture-striking-a-\nbalance-between-site-stability-feature-velocity-performance-and-cost), Amazon.com,\nGroupon, and Gilt have all evolved from a monolithic architecture to a microservice\narchitecture.\n \n\n\n19\nThe Microservice architecture pattern language\n You must address numerous design and architectural issues when using the micro-\nservice architecture. What’s more, many of these issues have multiple solutions, each\nwith a different set of trade-offs. There is no one single perfect solution. To help guide\nyour decision making, I’ve created the Microservice architecture pattern language. I ref-\nerence this pattern language throughout the rest of the book as I teach you about the\nmicroservice architecture. Let’s look at what a pattern language is and why it’s helpful. \n1.6\nThe Microservice architecture pattern language\nArchitecture and design are all about making decisions. You need to decide whether\nthe monolithic or microservice architecture is the best fit for your application. When\nmaking these decisions you have lots of trade-offs to consider. If you pick the microser-\nvice architecture, you’ll need to address lots of issues.\n A good way to describe the various architectural and design options and improve\ndecision making is to use a pattern language. Let’s first look at why we need patterns\nand a pattern language, and then we’ll take a tour of the Microservice architecture\npattern language.\n1.6.1\nMicroservice architecture is not a silver bullet\nBack in 1986, Fred Brooks, author of The Mythical Man-Month (Addison-Wesley Profes-\nsional, 1995), said that in software engineering, there are no silver bullets. That means\nthere are no techniques or technologies that if adopted would give you a tenfold\nboost in productivity. Yet decades years later, developers are still arguing passionately\nabout their favorite silver bullets, absolutely convinced that their favorite technology\nwill give them a massive boost in productivity.\n A lot of arguments follow the suck/rock dichotomy (http://nealford.com/memeagora/\n2009/08/05/suck-rock-dichotomy.html), a term coined by Neal Ford that describes\nhow everything in the software world either sucks or rocks, with no middle ground.\nThese arguments have this structure: if you do X, then a puppy will die, so therefore\nyou must do Y. For example, synchronous versus reactive programming, object-oriented\nversus functional, Java versus JavaScript, REST versus messaging. Of course, reality is\nmuch more nuanced. Every technology has drawbacks and limitations that are often\noverlooked by its advocates. As a result, the adoption of a technology usually follows\nthe Gartner hype cycle (https://en.wikipedia.org/wiki/Hype_cycle), in which an emerg-\ning technology goes through five phases, including the peak of inflated expectations (it\nrocks), followed by the trough of disillusionment (it sucks), and ending with the plateau\nof productivity (we now understand the trade-offs and when to use it).\n Microservices are not immune to the silver bullet phenomenon. Whether this\narchitecture is appropriate for your application depends on many factors. Conse-\nquently, it’s bad advice to advise always using the microservice architecture, but it’s\nequally bad advice to advise never using it. As with many things, it depends.\n The underlying reason for these polarized and hyped arguments about technology is\nthat humans are primarily driven by their emotions. Jonathan Haidt, in his excellent\n \n\n\n20\nCHAPTER 1\nEscaping monolithic hell\nbook The Righteous Mind: Why Good People Are Divided by Politics and Religion (Vintage,\n2013), uses the metaphor of an elephant and its rider to describe how the human mind\nworks. The elephant represents the emotion part of the human brain. It makes most of\nthe decisions. The rider represents the rational part of the brain. It can sometimes influ-\nence the elephant, but it mostly provides justifications for the elephant’s decisions.\n We—the software development community—need to overcome our emotional\nnature and find a better way of discussing and applying technology. A great way to dis-\ncuss and describe technology is to use the pattern format, because it’s objective. When\ndescribing a technology in the pattern format, you must, for example, describe the\ndrawbacks. Let’s take a look at the pattern format. \n1.6.2\nPatterns and pattern languages\nA pattern is a reusable solution to a problem that occurs in a particular context. It’s an\nidea that has its origins in real-world architecture and that has proven to be useful in\nsoftware architecture and design. The concept of a pattern was created by Christo-\npher Alexander, a real-world architect. He also created the concept of a pattern lan-\nguage, a collection of related patterns that solve problems within a particular domain.\nHis book A Pattern Language: Towns, Buildings, Construction (Oxford University Press,\n1977) describes a pattern language for architecture that consists of 253 patterns. The\npatterns range from solutions to high-level problems, such as where to locate a city\n(“Access to water”), to low-level problems, such as how to design a room (“Light on\ntwo sides of every room”). Each of these patterns solves a problem by arranging physi-\ncal objects that range in scope from cities to windows.\n Christopher Alexander’s writings inspired the software community to adopt the\nconcept of patterns and pattern languages. The book Design Patterns: Elements of Reus-\nable Object-Oriented Software (Addison-Wesley Professional, 1994), by Erich Gamma,\nRichard Helm, Ralph Johnson, and John Vlissides is a collection of object-oriented\ndesign patterns. The book popularized patterns among software developers. Since the\nmid-1990s, software developers have documented numerous software patterns. A soft-\nware pattern solves a software architecture or design problem by defining a set of col-\nlaborating software elements.\n Let’s imagine, for example, that you’re building a banking application that must\nsupport a variety of overdraft policies. Each policy defines limits on the balance of an\naccount and the fees charged for an overdrawn account. You can solve this problem\nusing the Strategy pattern, which is a well-known pattern from the classic Design Pat-\nterns book. The solution defined by the Strategy pattern consists of three parts:\nA strategy interface called Overdraft that encapsulates the overdraft algorithm\nOne or more concrete strategy classes, one for each particular context\nThe Account class that uses the algorithm\nThe Strategy pattern is an object-oriented design pattern, so the elements of the solution\nare classes. Later in this section, I describe high-level design patterns, where the solu-\ntion consists of collaborating services.\n \n\n\n21\nThe Microservice architecture pattern language\n One reason why patterns are valuable is because a pattern must describe the con-\ntext within which it applies. The idea that a solution is specific to a particular context\nand might not work well in other contexts is an improvement over how technology\nused to typically be discussed. For example, a solution that solves the problem at the\nscale of Netflix might not be the best approach for an application with fewer users.\n The value of a pattern, however, goes far beyond requiring you to consider the\ncontext of a problem. It forces you to describe other critical yet frequently overlooked\naspects of a solution. A commonly used pattern structure includes three especially\nvaluable sections:\nForces\nResulting context\nRelated patterns\nLet’s look at each of these, starting with forces.\nFORCES: THE ISSUES THAT YOU MUST ADDRESS WHEN SOLVING A PROBLEM\nThe forces section of a pattern describes the forces (issues) that you must address\nwhen solving a problem in a given context. Forces can conflict, so it might not be\npossible to solve all of them. Which forces are more important depends on the con-\ntext. You have to prioritize solving some forces over others. For example, code must\nbe easy to understand and have good performance. Code written in a reactive style\nhas better performance than synchronous code, yet is often more difficult to under-\nstand. Explicitly listing the forces is useful because it makes clear which issues need\nto be solved. \nRESULTING CONTEXT: THE CONSEQUENCES OF APPLYING A PATTERN\nThe resulting context section of a pattern describes the consequences of applying the\npattern. It consists of three parts:\nBenefits—The benefits of the pattern, including the forces that have been resolved\nDrawbacks—The drawbacks of the pattern, including the unresolved forces\nIssues—The new problems that have been introduced by applying the pattern\nThe resulting context provides a more complete and less biased view of the solution,\nwhich enables better design decisions. \nRELATED PATTERNS: THE FIVE DIFFERENT TYPES OF RELATIONSHIPS\nThe related patterns section of a pattern describes the relationship between the pattern\nand other patterns. There are five types of relationships between patterns:\nPredecessor—A predecessor pattern is a pattern that motivates the need for this\npattern. For example, the Microservice architecture pattern is the predecessor\nto the rest of the patterns in the pattern language, except the monolithic archi-\ntecture pattern.\nSuccessor—A pattern that solves an issue that has been introduced by this pat-\ntern. For example, if you apply the Microservice architecture pattern, you must\n \n",
      "page_number": 39
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 47-57)",
      "start_page": 47,
      "end_page": 57,
      "detection_method": "topic_boundary",
      "content": "22\nCHAPTER 1\nEscaping monolithic hell\nthen apply numerous successor patterns, including service discovery patterns\nand the Circuit breaker pattern.\nAlternative—A pattern that provides an alternative solution to this pattern. For\nexample, the Monolithic architecture pattern and the Microservice architec-\nture pattern are alternative ways of architecting an application. You pick one or\nthe other.\nGeneralization—A pattern that is a general solution to a problem. For example,\nin chapter 12 you’ll learn about the different implementations of the Single ser-\nvice per host pattern.\nSpecialization—A specialized form of a particular pattern. For example, in chap-\nter 12 you’ll learn that the Deploy a service as a container pattern is a specializa-\ntion of Single service per host.\nIn addition, you can organize patterns that tackle issues in a particular problem area\ninto groups. The explicit description of related patterns provides valuable guidance\non how to effectively solve a particular problem. Figure 1.9 shows how the relation-\nships between patterns is visually represented.\nThe different kinds of relationships between patterns shown in figure 1.9 are repre-\nsented as follows:\nRepresents the predecessor-successor relationship\nPatterns that are alternative solutions to the same problem\nIndicates that one pattern is a specialization of another pattern\nPatterns that apply to a particular problem area\nPattern\nProblem area\nDeployment\nMonolithic\narchitecture\nKey\nMicroservice\narchitecture\nSingle service\nper host\nService-per-container\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nFigure 1.9\nThe visual representation of different types of relationships \nbetween the patterns: a successor pattern solves a problem created by applying \nthe predecessor pattern; two or more patterns can be alternative solutions to \nthe same problem; one pattern can be a specialization of another pattern; and \npatterns that solve problems in the same area can be grouped, or generalized.\n \n\n\n23\nThe Microservice architecture pattern language\nA collection of patterns related through these relationships sometimes form what is\nknown as a pattern language. The patterns in a pattern language work together to\nsolve problems in a particular domain. In particular, I’ve created the Microservice\narchitecture pattern language. It’s a collection of interrelated software architecture\nand design patterns for microservices. Let’s take a look at this pattern language. \n1.6.3\nOverview of the Microservice architecture pattern language\nThe Microservice architecture pattern language is a collection of patterns that help\nyou architect an application using the microservice architecture. Figure 1.10 shows\nthe high-level structure of the pattern language. The pattern language first helps\nyou decide whether to use the microservice architecture. It describes the monolithic\narchitecture and the microservice architecture, along with their benefits and draw-\nbacks. Then, if the microservice architecture is a good fit for your application, the\npattern language helps you use it effectively by solving various architecture and\ndesign issues.\n The pattern language consists of several groups of patterns. On the left in figure 1.10\nis the application architecture patterns group, the Monolithic architecture pattern\nand the Microservice architecture pattern. Those are the patterns we’ve been discussing\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nDecomposition\nApplication infrastructure patterns\nCommunication patterns\nInfrastructure patterns\nMicroservice patterns\nApplication\narchitecture\nApplication patterns\nTesting\nObservability\nMaintaining\ndata consistency\nDatabase\narchitecture\nKey\nQuerying\nSecurity\nCross-cutting\nconcerns\nReliability\nExternal\nAPI\nCommunication style\nDiscovery\nTransactional messaging\nProblem area\nDeployment\nMonolithic\narchitecture\nMicroservice\narchitecture\nFigure 1.10\nA high-level view of the Microservice architecture pattern language showing the different problem \nareas that the patterns solve. On the left are the application architecture patterns: Monolithic architecture and \nMicroservice architecture. All the other groups of patterns solve problems that result from choosing the \nMicroservice architecture pattern.\n \n\n\n24\nCHAPTER 1\nEscaping monolithic hell\nin this chapter. The rest of the pattern language consists of groups of patterns that are\nsolutions to issues that are introduced by using the Microservice architecture pattern.\n The patterns are also divided into three layers:\nInfrastructure patterns—These solve problems that are mostly infrastructure issues\noutside of development.\nApplication infrastructure —These are for infrastructure issues that also impact\ndevelopment.\nApplication patterns—These solve problems faced by developers.\nThese patterns are grouped together based on the kind of problem they solve. Let’s\nlook at the main groups of patterns.\nPATTERNS FOR DECOMPOSING AN APPLICATION INTO SERVICES\nDeciding how to decompose a system into a set of services is very much an art, but\nthere are a number of strategies that can help. The two decomposition patterns\nshown in figure 1.11 are different strategies you can use to define your application’s\narchitecture.\nChapter 2 describes these patterns in detail. \nCOMMUNICATION PATTERNS\nAn application built using the microservice architecture is a distributed system. Conse-\nquently, interprocess communication (IPC) is an important part of the microservice\narchitecture. You must make a variety of architectural and design decisions about how\nyour services communicate with one another and the outside world. Figure 1.12 shows\nthe communication patterns, which are organized into five groups:\nCommunication style—What kind of IPC mechanism should you use?\nDiscovery—How does a client of a service determine the IP address of a service\ninstance so that, for example, it makes an HTTP request?\nReliability—How can you ensure that communication between services is reli-\nable even though services can be unavailable?\nTransactional messaging—How should you integrate the sending of messages and\npublishing of events with database transactions that update business data?\nExternal API—How do clients of your application communicate with the services?\nDecompose by\nbusiness capability\nDecompose by\nsubdomain\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nKey\nProblem area\nFigure 1.11\nThere are two \ndecomposition patterns: Decompose \nby business capability, which organizes \nservices around business capabilities, \nand Decompose by subdomain, which \norganizes services around domain-\ndriven design (DDD) subdomains.\n \n\n\n25\nThe Microservice architecture pattern language\nChapter 3 looks at the first four groups of patterns: communication style, discovery,\nreliability, and transaction messaging. Chapter 8 looks at the external API patterns. \nDATA CONSISTENCY PATTERNS FOR IMPLEMENTING TRANSACTION MANAGEMENT\nAs mentioned earlier, in order to ensure loose coupling, each service has its own data-\nbase. Unfortunately, having a database per service introduces some significant issues. I\ndescribe in chapter 4 that the traditional approach of using distributed transactions\n(2PC) isn’t a viable option for a modern application. Instead, an application needs to\nmaintain data consistency by using the Saga pattern. Figure 1.13 shows data-related\npatterns.\n Chapters 4, 5, and 6 describe these patterns in more detail. \nPATTERNS FOR QUERYING DATA IN A MICROSERVICE ARCHITECTURE\nThe other issue with using a database per service is that some queries need to join\ndata that’s owned by multiple services. A service’s data is only accessible via its API, so\nyou can’t use distributed queries against its database. Figure 1.14 shows a couple of\npatterns you can use to implement queries.\nPolling\npublisher\nTransaction\nlog tailing\nTransactional messaging\nTransactional\noutbox\nMessaging\nRemote procedure\ninvocation\nCircuit\nbreaker\nCommunication style\nReliability\nDomain-speciﬁc\nSelf registration\nClient-side\ndiscovery\nDiscovery\nExternal API\n3rd-party\nregistration\nAPI gateway\nBackend for\nfrontend\nServer-side\ndiscovery\nService registry\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nKey\nProblem area\nFigure 1.12\nThe five groups of communication patterns\n \n\n\n26\nCHAPTER 1\nEscaping monolithic hell\nSometimes you can use the API composition pattern, which invokes the APIs of one or\nmore services and aggregates results. Other times, you must use the Command query\nresponsibility segregation (CQRS) pattern, which maintains one or more easily queried\nreplicas of the data. Chapter 7 looks at the different ways of implementing queries. \nSERVICE DEPLOYMENT PATTERNS\nDeploying a monolithic application isn’t always easy, but it is straightforward in the\nsense that there is a single application to deploy. You have to run multiple instances of\nthe application behind a load balancer.\n In comparison, deploying a microservices-based application is much more com-\nplex. There may be tens or hundreds of services that are written in a variety of lan-\nguages and frameworks. There are many more moving parts that need to be managed.\nFigure 1.15 shows the deployment patterns.\n The traditional, and often manual, way of deploying applications in a language-\nspecific packaging format, for example WAR files, doesn’t scale to support a microser-\nvice architecture. You need a highly automated deployment infrastructure. Ideally,\nyou should use a deployment platform that provides the developer with a simple UI\n(command-line or GUI) for deploying and managing their services. The deployment\nplatform will typically be based on virtual machines (VMs), containers, or serverless\ntechnology. Chapter 12 looks at the different deployment options. \nDatabase per\nservice\nSaga\nEvent\nsourcing\nDomain\nevent\nAggregate\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nKey\nProblem area\nFigure 1.13\nBecause each service has its own database, you must use the Saga pattern to maintain \ndata consistency across services.\nCQRS\nAPI\ncomposition\nDatabase\nper service\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nKey\nProblem area\nFigure 1.14\nBecause each service has its own database, you must use one \nof the querying patterns to retrieve data scattered across multiple services.\n \n\n\n27\nThe Microservice architecture pattern language\nOBSERVABILITY PATTERNS PROVIDE INSIGHT INTO APPLICATION BEHAVIOR\nA key part of operating an application is understanding its runtime behavior and trouble-\nshooting problems such as failed requests and high latency. Though understanding and\ntroubleshooting a monolithic application isn’t always easy, it helps that requests are han-\ndled in a simple, straightforward way. Each incoming request is load balanced to a par-\nticular application instance, which makes a few calls to the database and returns a\nresponse. For example, if you need to understand how a particular request was handled,\nyou look at the log file of the application instance that handled the request.\n In contrast, understanding and diagnosing problems in a microservice architec-\nture is much more complicated. A request can bounce around between multiple ser-\nvices before a response is finally returned to a client. Consequently, there isn’t one log\nfile to examine. Similarly, problems with latency are more difficult to diagnose because\nthere are multiple suspects.\n You can use the following patterns to design observable services:\nHealth check API—Expose an endpoint that returns the health of the service.\nLog aggregation—Log service activity and write logs into a centralized logging\nserver, which provides searching and alerting.\nTraditional approach of deploying\nservices using their language-speciﬁc\npackaging, such as WAR ﬁles\nAutomated, self-service\nplatform for deploying\nand managing services\nA modern approach,\nwhich runs your code\nwithout you having to\nworry about managing\nthe infrastructure\nA modern approach, which\nencapsulates a service’s\ntechnology stack\nSingle service\nper host\nMultiple services\nper host\nServerless\ndeployment\nService-per-container\nService-per-VM\nService deployment\nplatform\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nKey\nProblem area\nFigure 1.15\nSeveral patterns for deploying microservices. The traditional approach is to deploy \nservices in a language-specific packaging format. There are two modern approaches to deploying \nservices. The first deploys services as VM or containers. The second is the serverless approach. \nYou simply upload the service’s code and the serverless platform runs it. You should use a service \ndeployment platform, which is an automated, self-service platform for deploying and managing \nservices.\n \n\n\n28\nCHAPTER 1\nEscaping monolithic hell\nDistributed tracing—Assign each external request a unique ID and trace requests\nas they flow between services.\nException tracking—Report exceptions to an exception tracking service, which\ndeduplicates exceptions, alerts developers, and tracks the resolution of each\nexception.\nApplication metrics—Maintain metrics, such as counters and gauges, and expose\nthem to a metrics server.\nAudit logging—Log user actions.\nChapter 11 describes these patterns in more detail. \nPATTERNS FOR THE AUTOMATED TESTING OF SERVICES\nThe microservice architecture makes individual services easier to test because they’re\nmuch smaller than the monolithic application. At the same time, though, it’s import-\nant to test that the different services work together while avoiding using complex,\nslow, and brittle end-to-end tests that test multiple services together. Here are patterns\nfor simplifying testing by testing services in isolation:\nConsumer-driven contract test—Verify that a service meets the expectations of its\nclients.\nConsumer-side contract test—Verify that the client of a service can communicate\nwith the service.\nService component test—Test a service in isolation.\nChapters 9 and 10 describe these testing patterns in more detail. \nPATTERNS FOR HANDLING CROSS-CUTTING CONCERNS\nIn a microservice architecture, there are numerous concerns that every service must\nimplement, including the observability patterns and discovery patterns. It must also\nimplement the Externalized Configuration pattern, which supplies configuration\nparameters such as database credentials to a service at runtime. When developing a\nnew service, it would be too time consuming to reimplement these concerns from\nscratch. A much better approach is to apply the Microservice Chassis pattern and\nbuild services on top of a framework that handles these concerns. Chapter 11\ndescribes these patterns in more detail. \nSECURITY PATTERNS\nIn a microservice architecture, users are typically authenticated by the API gateway. It\nmust then pass information about the user, such as identity and roles, to the services it\ninvokes. A common solution is to apply the Access token pattern. The API gateway\npasses an access token, such as JWT (JSON Web Token), to the services, which can val-\nidate the token and obtain information about the user. Chapter 11 discusses the\nAccess token pattern in more detail.\n Not surprisingly, the patterns in the Microservice architecture pattern language\nare focused on solving architect and design problems. You certainly need the right\n \n\n\n29\nBeyond microservices: Process and organization\narchitecture in order to successfully develop software, but it’s not the only concern.\nYou must also consider process and organization. \n1.7\nBeyond microservices: Process and organization\nFor a large, complex application, the microservice architecture is usually the best\nchoice. But in addition to having the right architecture, successful software develop-\nment requires you to also have organization, and development and delivery processes.\nFigure 1.16 shows the relationships between process, organization, and architecture.\nI’ve already described the microservice architecture. Let’s look at organization and\nprocess.\n1.7.1\nSoftware development and delivery organization\nSuccess inevitably means that the engineering team will grow. On the one hand, that’s\na good thing because more developers can get more done. The trouble with large\nteams is, as Fred Brooks wrote in The Mythical Man-Month, the communication over-\nhead of a team of size N is O(N 2). If the team gets too large, it will become inefficient,\ndue to the communication overhead. Imagine, for example, trying to do a daily standup\nwith 20 people.\n The solution is to refactor a large single team into a team of teams. Each team is\nsmall, consisting of no more than 8–12 people. It has a clearly defined business-oriented\nmission: developing and possibly operating one or more services that implement a\nfeature or a business capability. The team is cross-functional and can develop, test,\nand deploy its services without having to frequently communicate or coordinate with\nother teams.\nEnables\nEnables\nArchitecture:\nMicroservice\narchitecture\nOrganization:\nSmall, autonomous,\ncross-functional teams\nProcess:\nDevOps/continuous delivery/deployment\nEnables\nRapid, frequent,\nand reliable delivery\nof software\nFigure 1.16\nThe rapid, frequent, and reliable delivery of large, \ncomplex applications requires a combination of DevOps, which \nincludes continuous delivery/deployment, small, autonomous \nteams, and the microservice architecture.\n \n\n\n30\nCHAPTER 1\nEscaping monolithic hell\nThe velocity of the team of teams is significantly higher than that of a single large\nteam. As described earlier in section 1.5.1, the microservice architecture plays a key\nrole in enabling the teams to be autonomous. Each team can develop, deploy, and\nscale their services without coordinating with other teams. Moreover, it’s very clear\nwho to contact when a service isn’t meeting its SLA.\n What’s more, the development organization is much more scalable. You grow the\norganization by adding teams. If a single team becomes too large, you split it and its\nassociated service or services. Because the teams are loosely coupled, you avoid the\ncommunication overhead of a large team. As a result, you can add people without\nimpacting productivity. \n1.7.2\nSoftware development and delivery process\nUsing the microservice architecture with a waterfall development process is like driv-\ning a horse-drawn Ferrari—you squander most of the benefit of using microservices. If\nyou want to develop an application with the microservice architecture, it’s essential\nthat you adopt agile development and deployment practices such as Scrum or Kan-\nban. Better yet, you should practice continuous delivery/deployment, which is a part\nof DevOps.\n Jez Humble (https://continuousdelivery.com/) defines continuous delivery as\nfollows:\nContinuous Delivery is the ability to get changes of all types—including new features,\nconfiguration changes, bug fixes and experiments—into production, or into the hands of\nusers, safely and quickly in a sustainable way.\nA key characteristic of continuous delivery is that software is always releasable. It\nrelies on a high level of automation, including automated testing. Continuous\ndeployment takes continuous delivery one step further in the practice of automati-\ncally deploying releasable code into production. High-performing organizations\nThe reverse Conway maneuver\nIn order to effectively deliver software when using the microservice architecture, you\nneed to take into account Conway’s law (https://en.wikipedia.org/wiki/Conway%27s\n_law), which states the following:\nOrganizations which design systems … are constrained to produce designs\nwhich are copies of the communication structures of these organizations.\nMelvin Conway\nIn other words, your application’s architecture mirrors the structure of the organiza-\ntion that developed it. It’s important, therefore, to apply Conway’s law in reverse\n(www.thoughtworks.com/radar/techniques/inverse-conway-maneuver) and design\nyour organization so that its structure mirrors your microservice architecture. By doing\nso, you ensure that your development teams are as loosely coupled as the services.\n \n\n\n31\nBeyond microservices: Process and organization\nthat practice continuous deployment deploy multiple times per day into produc-\ntion, have far fewer production outages, and recover quickly from any that do occur\n(https://puppet.com/ resources/whitepaper/state-of-devops-report). As described ear-\nlier in section 1.5.1, the microservice architecture directly supports continuous\ndelivery/deployment.\n1.7.3\nThe human side of adopting microservices\nAdopting the microservice architecture changes your architecture, your organization,\nand your development processes. Ultimately, though, it changes the working environ-\nment of people, who are, as mentioned earlier, emotional creatures. If ignored, their\nemotions can make the adoption of microservices a bumpy ride. Mary and the other\nFTGO leaders will struggle to change how FTGO develops software.\n The best-selling book Managing Transitions (Da Capo Lifelong Books, 2017,\nhttps://wmbridges.com/books) by William and Susan Bridges introduces the con-\ncept of a transition, which refers to the process of how people respond emotionally to a\nchange. It describes a three-stage Transition Model:\n1\nEnding, Losing, and Letting Go—The period of emotional upheaval and resis-\ntance when people are presented with a change that forces them out of their\ncomfort zone. They often mourn the loss of the old way of doing things. For\nexample, when people reorganize into cross-functional teams, they miss their\nformer teammates. Similarly, a data modeling group that owns the global data\nmodel will be threatened by the idea of each service having its own data\nmodel.\nMove fast without breaking things\nThe goal of continuous delivery/deployment (and, more generally, DevOps) is to rap-\nidly yet reliably deliver software. Four useful metrics for assessing software develop-\nment are as follows:\nDeployment frequency—How often software is deployed into production\nLead time—Time from a developer checking in a change to that change being\ndeployed\nMean time to recover—Time to recover from a production problem\nChange failure rate—Percentage of changes that result in a production problem\nIn a traditional organization, the deployment frequency is low, and the lead time is\nhigh. Stressed-out developers and operations people typically stay up late into the\nnight fixing last-minute issues during the maintenance window. In contrast, a DevOps\norganization releases software frequently, often multiple times per day, with far fewer\nproduction issues. Amazon, for example, deployed changes into production every\n11.6 seconds in 2014 (www.youtube.com/watch?v=dxk8b9rSKOo), and Netflix had\na lead time of 16 minutes for one software component (https://medium.com/netflix-\ntechblog/how-we-build-code-at-netflix-c5d9bd727f15). \n \n\n\n32\nCHAPTER 1\nEscaping monolithic hell\n2\nThe Neutral Zone—The intermediate stage between the old and new ways of doing\nthings, where people are often confused. They are often struggling to learn the\nnew way of doing things.\n3\nThe New Beginning—The final stage where people have enthusiastically embraced\nthe new way of doing things and are starting to experience the benefits.\nThe book describes how best to manage each stage of the transition and increase the\nlikelihood of successfully implementing the change. FTGO is certainly suffering from\nmonolithic hell and needs to migrate to a microservice architecture. It must also\nchange its organization and development processes. In order for FTGO to successfully\naccomplish this, however, it must take into account the transition model and consider\npeople’s emotions.\n In the next chapter, you’ll learn about the goal of software architecture and how to\ndecompose an application into services. \nSummary\nThe Monolithic architecture pattern structures the application as a single deploy-\nable unit.\nThe Microservice architecture pattern decomposes a system into a set of inde-\npendently deployable services, each with its own database.\nThe monolithic architecture is a good choice for simple applications, but micro-\nservice architecture is usually a better choice for large, complex applications.\nThe microservice architecture accelerates the velocity of software development\nby enabling small, autonomous teams to work in parallel.\nThe microservice architecture isn’t a silver bullet—there are significant draw-\nbacks, including complexity.\nThe Microservice architecture pattern language is a collection of patterns that\nhelp you architect an application using the microservice architecture. It helps\nyou decide whether to use the microservice architecture, and if you pick the\nmicroservice architecture, the pattern language helps you apply it effectively.\nYou need more than just the microservice architecture to accelerate software\ndelivery. Successful software development also requires DevOps and small,\nautonomous teams.\nDon’t forget about the human side of adopting microservices. You need to con-\nsider employees’ emotions in order to successfully transition to a microservice\narchitecture. \n \n",
      "page_number": 47
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 58-66)",
      "start_page": 58,
      "end_page": 66,
      "detection_method": "topic_boundary",
      "content": "33\nDecomposition strategies\nSometimes you have to be careful what you wish for. After an intense lobbying\neffort, Mary had finally convinced the business that migrating to a microservice\narchitecture was the right thing to do. Feeling a mixture of excitement and some\ntrepidation, Mary had a morning-long meeting with her architects to discuss where\nto begin. During the discussion, it became apparent that some aspects of the Micro-\nservice architecture pattern language, such as deployment and service discovery,\nwere new and unfamiliar, yet straightforward. The key challenge, which is the\nessence of the microservice architecture, is the functional decomposition of the\napplication into services. The first and most important aspect of the architecture is,\nThis chapter covers\nUnderstanding software architecture and why it’s \nimportant\nDecomposing an application into services by \napplying the decomposition patterns Decompose \nby business capability and Decompose by \nsubdomain\nUsing the bounded context concept from domain-\ndriven design (DDD) to untangle data and make \ndecomposition easier\n \n\n\n34\nCHAPTER 2\nDecomposition strategies\ntherefore, the definition of the services. As they stood around the whiteboard, the\nFTGO team wondered exactly how to do that!\n In this chapter, you’ll learn how to define a microservice architecture for an appli-\ncation. I describe strategies for decomposing an application into services. You’ll learn\nthat services are organized around business concerns rather than technical concerns.\nI also show how to use ideas from domain-driven design (DDD) to eliminate god\nclasses, which are classes that are used throughout an application and cause tangled\ndependencies that prevent decomposition.\n I begin this chapter by defining the microservice architecture in terms of software\narchitecture concepts. After that, I describe a process for defining a microservice\narchitecture for an application starting from its requirements. I discuss strategies for\ndecomposing an application into a collection of services, obstacles to it, and how to\novercome them. Let’s start by examining the concept of software architecture.\n2.1\nWhat is the microservice architecture exactly?\nChapter 1 describes how the key idea of the microservice architecture is functional\ndecomposition. Instead of developing one large application, you structure the appli-\ncation as a set of services. On one hand, describing the microservice architecture as a\nkind of functional decomposition is useful. But on the other hand, it leaves several\nquestions unanswered, including how does the microservice architecture relate to the\nbroader concepts of software architecture? What’s a service? And how important is the\nsize of a service?\n In order to answer those questions, we need to take a step back and look at what is\nmeant by software architecture. The architecture of a software application is its high-level\nstructure, which consists of constituent parts and the dependencies between those\nparts. As you’ll see in this section, an application’s architecture is multidimensional, so\nthere are multiple ways to describe it. The reason architecture is important is because\nit determines the application’s software quality attributes or -ilities. Traditionally, the\ngoal of architecture has been scalability, reliability, and security. But today it’s import-\nant that the architecture also enables the rapid and safe delivery of software. You’ll\nlearn that the microservice architecture is an architecture style that gives an applica-\ntion high maintainability, testability, and deployability.\n I begin this section by describing the concept of software architecture and why it’s\nimportant. Next, I discuss the idea of an architectural style. Then I define the micro-\nservice architecture as a particular architectural style. Let’s start by looking at the con-\ncept of software architecture.\n2.1.1\nWhat is software architecture and why does it matter?\nArchitecture is clearly important. There are at least two conferences dedicated to the\ntopic: O’Reilly Software Architecture Conference (https://conferences.oreilly.com/\nsoftware-architecture) and the SATURN conference (https://resources.sei.cmu.edu/\nnews-events/events/saturn/). Many developers have the goal of becoming an archi-\ntect. But what is architecture and why does it matter?\n \n\n\n35\nWhat is the microservice architecture exactly?\n To answer that question, I first define what is meant by the term software architecture.\nAfter that, I discuss how an application’s architecture is multidimensional and is best\ndescribed using a collection of views or blueprints. I then describe that software archi-\ntecture matters because of its impact on the application’s software quality attributes.\nA DEFINITION OF SOFTWARE ARCHITECTURE\nThere are numerous definitions of software architecture. For example, see https://\nen.wikiquote.org/wiki/Software_architecture to read some of them. My favorite defi-\nnition comes from Len Bass and colleagues at the Software Engineering Institute\n(www.sei.cmu.edu), who played a key role in establishing software architecture as a\ndiscipline. They define software architecture as follows:\nThe software architecture of a computing system is the set of structures needed to reason about\nthe system, which comprise software elements, relations among them, and properties of both.\nDocumenting Software Architectures by Bass et al.\nThat’s obviously a quite abstract definition. But its essence is that an application’s\narchitecture is its decomposition into parts (the elements) and the relationships (the\nrelations) between those parts. Decomposition is important for a couple of reasons:\nIt facilitates the division of labor and knowledge. It enables multiple people (or\nmultiple teams) with possibly specialized knowledge to work productively together\non an application.\nIt defines how the software elements interact.\nIt’s the decomposition into parts and the relationships between those parts that deter-\nmine the application’s -ilities. \nTHE 4+1 VIEW MODEL OF SOFTWARE ARCHITECTURE\nMore concretely, an application’s architecture can be viewed from multiple perspec-\ntives, in the same way that a building’s architecture can be viewed from structural,\nplumbing, electrical, and other perspectives. Phillip Krutchen wrote a classic paper\ndescribing the 4+1 view model of software architecture, “Architectural Blueprints—\nThe ‘4+1’ View Model of Software Architecture” (www.cs.ubc.ca/~gregor/teaching/\npapers/4+1view-architecture.pdf). The 4+1 model, shown in Figure 2.1, defines four\ndifferent views of a software architecture. Each describes a particular aspect of the\narchitecture and consists of a particular set of software elements and relationships\nbetween them.\n The purpose of each view is as follows:\nLogical view—The software elements that are created by developers. In object-\noriented languages, these elements are classes and packages. The relations\nbetween them are the relationships between classes and packages, including\ninheritance, associations, and depends-on.\nImplementation view—The output of the build system. This view consists of mod-\nules, which represent packaged code, and components, which are executable\n \n\n\n36\nCHAPTER 2\nDecomposition strategies\nor deployable units consisting of one or more modules. In Java, a module is a\nJAR file, and a component is typically a WAR file or an executable JAR file. The\nrelations between them include dependency relationships between modules\nand composition relationships between components and modules.\nProcess view—The components at runtime. Each element is a process, and the\nrelations between processes represent interprocess communication.\nDeployment—How the processes are mapped to machines. The elements in this\nview consist of (physical or virtual) machines and the processes. The relations\nbetween machines represent networking. This view also describes the relation-\nship between processes and machines.\nIn addition to these four views, there are the scenarios—the +1 in the 4+1 model—\nthat animate views. Each scenario describes how the various architectural components\nwithin a particular view collaborate in order to handle a request. A scenario in the log-\nical view, for example, shows how the classes collaborate. Similarly, a scenario in the\nprocess view shows how the processes collaborate.\n The 4+1 view model is an excellent way to describe an applications’s architec-\nture. Each view describes an important aspect of the architecture, and the scenarios\nLogical\nview\nImplementation\nview\nProcess\nview\nDeployment\nview\nScenarios\nWhat developers create\nElements: Classes and packages\nRelations: The relationships\nbetween them\nWhat is produced by the build system\nElements: Modules, (JAR ﬁles) and\ncomponents (WAR ﬁles\nor executables)\nRelations: Their dependencies\nRunning components\nElements: Processes\nRelations: Inter-process\ncommunication\nProcesses running on “machines”\nElements: Machines and processes\nRelations: Networking\nAnimate the views.\nFigure 2.1\nThe 4+1 view model describes an application’s architecture using four views, \nalong with scenarios that show how the elements within each view collaborate to handle \nrequests.\n \n\n\n37\nWhat is the microservice architecture exactly?\nillustrate how the elements of a view collaborate. Let’s now look at why architecture\nis important. \nWHY ARCHITECTURE MATTERS\nAn application has two categories of requirements. The first category includes the\nfunctional requirements, which define what the application must do. They’re usually\nin the form of use cases or user stories. Architecture has very little to do with the func-\ntional requirements. You can implement functional requirements with almost any\narchitecture, even a big ball of mud.\n Architecture is important because it enables an application to satisfy the second\ncategory of requirements: its quality of service requirements. These are also known as\nquality attributes and are the so-called -ilities. The quality of service requirements\ndefine the runtime qualities such as scalability and reliability. They also define devel-\nopment time qualities including maintainability, testability, and deployability. The\narchitecture you choose for your application determines how well it meets these\nquality requirements. \n2.1.2\nOverview of architectural styles\nIn the physical world, a building’s architecture often follows a particular style, such as\nVictorian, American Craftsman, or Art Deco. Each style is a package of design deci-\nsions that constrains a building’s features and building materials. The concept of\narchitectural style also applies to software. David Garlan and Mary Shaw (An Introduc-\ntion to Software Architecture, January 1994, https://www.cs.cmu.edu/afs/cs/project/\nable/ftp/intro_softarch/intro_softarch.pdf), pioneers in the discipline of software\narchitecture, define an architectural style as follows:\nAn architectural style, then, defines a family of such systems in terms of a pattern of\nstructural organization. More specifically, an architectural style determines the vocabulary\nof components and connectors that can be used in instances of that style, together with a\nset of constraints on how they can be combined.\nA particular architectural style provides a limited palette of elements (components)\nand relations (connectors) from which you can define a view of your application’s\narchitecture. An application typically uses a combination of architectural styles. For\nexample, later in this section I describe how the monolithic architecture is an archi-\ntectural style that structures the implementation view as a single (executable/deploy-\nable) component. The microservice architecture structures an application as a set of\nloosely coupled services.\nTHE LAYERED ARCHITECTURAL STYLE\nThe classic example of an architectural style is the layered architecture. A layered archi-\ntecture organizes software elements into layers. Each layer has a well-defined set of\nresponsibilities. A layered architecture also constraints the dependencies between the\nlayers. A layer can only depend on either the layer immediately below it (if strict layer-\ning) or any of the layers below it.\n \n\n\n38\nCHAPTER 2\nDecomposition strategies\n You can apply the layered architecture to any of the four views discussed earlier.\nThe popular three-tier architecture is the layered architecture applied to the logical\nview. It organizes the application’s classes into the following tiers or layers:\nPresentation layer—Contains code that implements the user interface or exter-\nnal APIs\nBusiness logic layer—Contains the business logic\nPersistence layer—Implements the logic of interacting with the database\nThe layered architecture is a great example of an architectural style, but it does have\nsome significant drawbacks:\nSingle presentation layer—It doesn’t represent the fact that an application is likely\nto be invoked by more than just a single system.\nSingle persistence layer—It doesn’t represent the fact that an application is likely\nto interact with more than just a single database.\nDefines the business logic layer as depending on the persistence layer—In theory, this\ndependency prevents you from testing the business logic without the database.\nAlso, the layered architecture misrepresents the dependencies in a well-designed\napplication. The business logic typically defines an interface or a repository of inter-\nfaces that define data access methods. The persistence tier defines DAO classes that\nimplement the repository interfaces. In other words, the dependencies are the reverse\nof what’s depicted by a layered architecture.\n Let’s look at an alternative architecture that overcomes these drawbacks: the hex-\nagonal architecture. \nABOUT THE HEXAGONAL ARCHITECTURE STYLE\nHexagonal architecture is an alternative to the layered architectural style. As figure 2.2\nshows, the hexagonal architecture style organizes the logical view in a way that places\nthe business logic at the center. Instead of the presentation layer, the application has\none or more inbound adapters that handle requests from the outside by invoking the\nbusiness logic. Similarly, instead of a data persistence tier, the application has one or\nmore outbound adapters that are invoked by the business logic and invoke external\napplications. A key characteristic and benefit of this architecture is that the business\nlogic doesn’t depend on the adapters. Instead, they depend upon it.\n The business logic has one or more ports. A port defines a set of operations and is\nhow the business logic interacts with what’s outside of it. In Java, for example, a port is\noften a Java interface. There are two kinds of ports: inbound and outbound ports. An\ninbound port is an API exposed by the business logic, which enables it to be invoked\nby external applications. An example of an inbound port is a service interface, which\ndefines a service’s public methods. An outbound port is how the business logic invokes\nexternal systems. An example of an output port is a repository interface, which defines a\ncollection of data access operations.\n \n\n\n39\nWhat is the microservice architecture exactly?\nSurrounding the business logic are adapters. As with ports, there are two types of\nadapters: inbound and outbound. An inbound adapter handles requests from the out-\nside world by invoking an inbound port. An example of an inbound adapter is a\nSpring MVC Controller that implements either a set of REST endpoints or a set of\nweb pages. Another example is a message broker client that subscribes to messages.\nMultiple inbound adapters can invoke the same inbound port.\n An outbound adapter implements an outbound port and handles requests from\nthe business logic by invoking an external application or service. An example of an\noutbound adapter is a data access object (DAO) class that implements operations for\naccessing a database. Another example would be a proxy class that invokes a remote\nservice. Outbound adapters can also publish events.\n An important benefit of the hexagonal architectural style is that it decouples the\nbusiness logic from the presentation and data access logic in the adapters. The busi-\nness logic doesn’t depend on either the presentation logic or the data access logic.\nBusiness logic\nBrowser\nMessage broker\nOutbound adapter\nOutbound port\nOutbound adapter\nInbound port\nInbound adapter\nInbound adapter\nSome\ncontroller\nclass\nMessage\nconsumer\nMessaging\ninterface\nFoo\nservice\nRepository\ninterface\nDAO\nDatabase\nMessage\nproducer\nFigure 2.2\nAn example of a hexagonal architecture, which consists of the business logic and one or \nmore adapters that communicate with external systems. The business logic has one or more ports. \nInbound adapters, which handled requests from external systems, invoke an inbound port. An \noutbound adapter implements an outbound port, and invokes an external system.\n \n\n\n40\nCHAPTER 2\nDecomposition strategies\nBecause of this decoupling, it’s much easier to test the business logic in isolation.\nAnother benefit is that it more accurately reflects the architecture of a modern appli-\ncation. The business logic can be invoked via multiple adapters, each of which imple-\nments a particular API or UI. The business logic can also invoke multiple adapters,\neach one of which invokes a different external system. Hexagonal architecture is a\ngreat way to describe the architecture of each service in a microservice architecture.\n The layered and hexagonal architectures are both examples of architectural styles.\nEach defines the building blocks of an architecture and imposes constraints on the\nrelationships between them. The hexagonal architecture and the layered architec-\nture, in the form of a three-tier architecture, organize the logical view. Let’s now\ndefine the microservice architecture as an architectural style that organizes the imple-\nmentation view. \n2.1.3\nThe microservice architecture is an architectural style\nI’ve discussed the 4+1 view model and architectural styles, so I can now define mono-\nlithic and microservice architecture. They’re both architectural styles. Monolithic\narchitecture is an architectural style that structures the implementation view as a sin-\ngle component: a single executable or WAR file. This definition says nothing about\nthe other views. A monolithic application can, for example, have a logical view that’s\norganized along the lines of a hexagonal architecture.\nThe microservice architecture is also an architectural style. It structures the imple-\nmentation view as a set of multiple components: executables or WAR files. The com-\nponents are services, and the connectors are the communication protocols that\nenable those services to collaborate. Each service has its own logical view architecture,\nwhich is typically a hexagonal architecture. Figure 2.3 shows a possible microservice\narchitecture for the FTGO application. The services in this architecture correspond to\nbusiness capabilities, such as Order management and Restaurant management.\nLater in this chapter, I describe what is meant by business capability . The connectors\nbetween services are implemented using interprocess communication mechanisms\nsuch as REST APIs and asynchronous messaging. Chapter 3 discusses interprocess\ncommunication in more detail.\nPattern: Monolithic architecture\nStructure the application as a single executable/deployable component. See http://\nmicroservices.io/patterns/ monolithic.html.\nPattern: Microservice architecture\nStructure the application as a collection of loosely coupled, independently deployable\nservices. See http://microservices.io/patterns/microservices.html.\n \n\n\n41\nWhat is the microservice architecture exactly?\nA key constraint imposed by the microservice architecture is that the services are\nloosely coupled. Consequently, there are restrictions on how the services collaborate.\nIn order to explain those restrictions, I’ll attempt to define the term service, describe\nwhat it means to be loosely coupled, and tell you why this matters.\nWHAT IS A SERVICE?\nA service is a standalone, independently deployable software component that imple-\nments some useful functionality. Figure 2.4 shows the external view of a service, which in\nthis example is the Order Service. A service has an API that provides its clients access to\nits functionality. There are two types of operations: commands and queries. The API\nconsists of commands, queries, and events. A command, such as createOrder(), per-\nforms actions and updates data. A query, such as findOrderById(), retrieves data. A ser-\nvice also publishes events, such as OrderCreated, which are consumed by its clients.\n A service’s API encapsulates its internal implementation. Unlike in a monolith, a\ndeveloper can’t write code that bypasses its API. As a result, the microservice architec-\nture enforces the application’s modularity.\n Each service in a microservice architecture has its own architecture and, potentially,\ntechnology stack. But a typical service has a hexagonal architecture. Its API is imple-\nmented by adapters that interact with the service’s business logic. The operations\nAmazon\nSES\nAdapter\nTwilio\nAdapter\nStripe\nAdapter\nThe API Gateway routes\nrequests from the mobile\napplications to services.\nServices have APIs.\nA service’s data is private.\nServices corresponding\nto business capabilities/\nDDD subdomains\nAPI\nGateway\nRestaurant\nWeb UI\nOrder\nService\nCourier\nREST\nAPI\nREST\nAPI\nREST\nAPI\nConsumer\nRestaurant\nRestaurant\nService\nREST\nAPI\nAccounting\nService\nREST\nAPI\nNotiﬁcation\nService\nREST\nAPI\nKitchen\nService\nREST\nAPI\nDelivery\nService\nREST\nAPI\nFigure 2.3\nA possible microservice architecture for the FTGO application. It consists of numerous \nservices.\n \n",
      "page_number": 58
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 67-76)",
      "start_page": 67,
      "end_page": 76,
      "detection_method": "topic_boundary",
      "content": "42\nCHAPTER 2\nDecomposition strategies\nadapter invokes the business logic, and the events adapter publishes events emitted by\nthe business logic.\n Later in chapter 12, when I discuss deployment technologies, you’ll see that the\nimplementation view of a service can take many forms. The component might be a\nstandalone process, a web application or OSGI bundle running in a container, or a\nserverless cloud function. An essential requirement, however, is that a service has an\nAPI and is independently deployable. \nWHAT IS LOOSE COUPLING?\nAn important characteristic of the microservice architecture is that the services are\nloosely coupled (https://en.wikipedia.org/wiki/Loose_coupling). All interaction with a\nservice happens via its API, which encapsulates its implementation details. This enables\nthe implementation of the service to change without impacting its clients. Loosely\ncoupled services are key to improving an application’s development time attributes,\nincluding its maintainability and testability. They are much easier to understand, change,\nand test.\n The requirement for services to be loosely coupled and to collaborate only via APIs\nprohibits services from communicating via a database. You must treat a service’s\npersistent data like the fields of a class and keep them private. Keeping the data pri-\nvate enables a developer to change their service’s database schema without having to\nOrder Service\nInvokes\nSubscribes to events\nOrder\nService\nclient\nDeﬁnes operations\nPublishes events when data changes\nCommands:\ncreateOrder()\n...\nQueries:\nﬁndOrderbyId()\n...\nOrder\nevent\npublisher\nService API\nOrder created\nOrder cancelled\nFigure 2.4\nA service has an API that encapsulates the implementation. The API defines \noperations, which are invoked by clients. There are two types of operations: commands update \ndata, and queries retrieve data. When its data changes, a service publishes events that clients \ncan subscribe to.\n \n\n\n43\nWhat is the microservice architecture exactly?\nspend time coordinating with developers working on other services. Not sharing data-\nbase tables also improves runtime isolation. It ensures, for example, that one service\ncan’t hold database locks that block another service. Later on, though, you’ll learn\nthat one downside of not sharing databases is that maintaining data consistency and\nquerying across services are more complex. \nTHE ROLE OF SHARED LIBRARIES\nDevelopers often package functionality in a library (module) so that it can be reused\nby multiple applications without duplicating code. After all, where would we be today\nwithout Maven or npm repositories? You might be tempted to also use shared libraries\nin microservice architecture. On the surface, it looks like a good way to reduce code\nduplication in your services. But you need to ensure that you don’t accidentally intro-\nduce coupling between your services.\n Imagine, for example, that multiple services need to update the Order business\nobject. One approach is to package that functionality as a library that’s used by multi-\nple services. On one hand, using a library eliminates code duplication. On the other\nhand, consider what happens when the requirements change in a way that affects the\nOrder business object. You would need to simultaneously rebuild and redeploy those\nservices. A much better approach would be to implement functionality that’s likely to\nchange, such as Order management, as a service.\n You should strive to use libraries for functionality that’s unlikely to change. For\nexample, in a typical application it makes no sense for every service to implement a\ngeneric Money class. Instead, you should create a library that’s used by the services. \nTHE SIZE OF A SERVICE IS MOSTLY UNIMPORTANT\nOne problem with the term microservice is that the first thing you hear is micro. This\nsuggests that a service should be very small. This is also true of other size-based terms\nsuch as miniservice or nanoservice. In reality, size isn’t a useful metric.\n A much better goal is to define a well-designed service to be a service capable of\nbeing developed by a small team with minimal lead time and with minimal collabora-\ntion with other teams. In theory, a team might only be responsible for a single service,\nso that service is by no means micro. Conversely, if a service requires a large team or\ntakes a long time to test, it probably makes sense to split the team and the service. Or\nif you constantly need to change a service because of changes to other services or if it’s\ntriggering changes in other services, that’s a sign that it’s not loosely coupled. You\nmight even have built a distributed monolith.\n The microservice architecture structures an application as a set of small, loosely\ncoupled services. As a result, it improves the development time attributes—main-\ntainability, testability, deployability, and so on—and enables an organization to\ndevelop better software faster. It also improves an application’s scalability, although\nthat’s not the main goal. To develop a microservice architecture for your application,\nyou need to identify the services and determine how they collaborate. Let’s look at\nhow to do that. \n \n\n\n44\nCHAPTER 2\nDecomposition strategies\n2.2\nDefining an application’s microservice architecture\nHow should we define a microservice architecture? As with any software development\neffort, the starting points are the written requirements, hopefully domain experts, and\nperhaps an existing application. Like much of software development, defining an\narchitecture is more art than science. This section describes a simple, three-step pro-\ncess, shown in figure 2.5, for defining an application’s architecture. It’s important to\nremember, though, that it’s not a process you can follow mechanically. It’s likely to be\niterative and involve a lot of creativity.\nAn application exists to handle requests, so the first step in defining its architecture is\nto distill the application’s requirements into the key requests. But instead of describing\nthe requests in terms of specific IPC technologies such as REST or messaging, I use\nOrder\nService\nFTGO\nFTGO\nRestaurant\nService\nKitchen\nService\n...\nOrder\nService\nIterate\nverifyOrder()\nRestaurant\nService\nKitchen\nService\nFunctional requirements\ncreateOrder()\ncreateTicket()\nacceptOrder()\ncreateOrder()\nacceptOrder()\nFTGO\nAs a consumer\nI want to place an order\nso that I can ...\ncreateOrder()\nacceptOrder()\nAs a restaurant\nI want to accept an order\nso that I can ...\nStep 1: Identify system operations\nStep 2: Identify services\nStep 3: Deﬁne service APIs and collaborations\nThe starting point are the requirements,\nsuch as the user stories.\nA system operation represents\nan external request.\nFigure 2.5\nA three-step process for defining an application’s microservice architecture\n \n\n\n45\nDefining an application’s microservice architecture\nthe more abstract notion of system operation. A system operation is an abstraction of a\nrequest that the application must handle. It’s either a command, which updates data,\nor a query, which retrieves data. The behavior of each command is defined in terms\nof an abstract domain model, which is also derived from the requirements. The sys-\ntem operations become the architectural scenarios that illustrate how the services\ncollaborate.\n The second step in the process is to determine the decomposition into services.\nThere are several strategies to choose from. One strategy, which has its origins in the\ndiscipline of business architecture, is to define services corresponding to business\ncapabilities. Another strategy is to organize services around domain-driven design sub-\ndomains. The end result is services that are organized around business concepts\nrather than technical concepts.\n The third step in defining the application’s architecture is to determine each ser-\nvice’s API. To do that, you assign each system operation identified in the first step to a\nservice. A service might implement an operation entirely by itself. Alternatively, it\nmight need to collaborate with other services. In that case, you determine how the ser-\nvices collaborate, which typically requires services to support additional operations.\nYou’ll also need to decide which of the IPC mechanisms I describe in chapter 3 to\nimplement each service’s API.\n There are several obstacles to decomposition. The first is network latency. You\nmight discover that a particular decomposition would be impractical due to too many\nround-trips between services. Another obstacle to decomposition is that synchronous\ncommunication between services reduces availability. You might need to use the con-\ncept of self-contained services, described in chapter 3. The third obstacle is the\nrequirement to maintain data consistency across services. You’ll typically need to use\nsagas, discussed in chapter 4. The fourth and final obstacle to decomposition is so-\ncalled god classes, which are used throughout an application. Fortunately, you can use\nconcepts from domain-driven design to eliminate god classes.\n This section first describes how to identity an application’s operations. After that,\nwe’ll look at strategies and guidelines for decomposing an application into services,\nand at obstacles to decomposition and how to address them. Finally, I’ll describe how\nto define each service’s API.\n2.2.1\nIdentifying the system operations\nThe first step in defining an application’s architecture is to define the system opera-\ntions. The starting point is the application’s requirements, including user stories and\ntheir associated user scenarios (note that these are different from the architectural\nscenarios). The system operations are identified and defined using the two-step pro-\ncess shown in figure 2.6. This process is inspired by the object-oriented design process\ncovered in Craig Larman’s book Applying UML and Patterns (Prentice Hall, 2004) (see\nwww.craiglarman.com/wiki/index.php?title=Book_Applying_UML_and_Patterns for\ndetails). The first step creates the high-level domain model consisting of the key classes\n \n\n\n46\nCHAPTER 2\nDecomposition strategies\nthat provide a vocabulary with which to describe the system operations. The second\nstep identifies the system operations and describes each one’s behavior in terms of the\ndomain model.\nThe domain model is derived primarily from the nouns of the user stories, and the sys-\ntem operations are derived mostly from the verbs. You could also define the domain\nmodel using a technique called Event Storming, which I talk about in chapter 5.\nThe behavior of each system operation is described in terms of its effect on one or\nmore domain objects and the relationships between them. A system operation can\ncreate, update, or delete domain objects, as well as create or destroy relationships\nbetween them.\n Let’s look at how to define a high-level domain model. After that I’ll define the sys-\ntem operations in terms of the domain model.\nCREATING A HIGH-LEVEL DOMAIN MODEL\nThe first step in the process of defining the system operations is to sketch a high-\nlevel domain model for the application. Note that this domain model is much sim-\npler than what will ultimately be implemented. The application won’t even have a\nsingle domain model because, as you’ll soon learn, each service has its own domain\nmodel. Despite being a drastic simplification, a high-level domain model is useful at\nthis stage because it defines the vocabulary for describing the behavior of the system\noperations.\n A domain model is created using standard techniques such as analyzing the nouns\nin the stories and scenarios and talking to the domain experts. Consider, for example,\nFunctional requirements\nFTGO\nAs a consumer\nI want to place an order\nso that I can ...\ncreateOrder()\nacceptOrder()\nAs a restaurant\nI want to accept an order\nso that I can ...\nStep 2\nHigh-level domain model\nStep 1\nOrder\nMaps to\nSystem operations are deﬁned\nin terms of domain model.\nDomain model\nderived from\nrequirements\nRestaurant\nDelivery\nFigure 2.6\nSystem operations are derived from the application’s requirements using a two-step process. The first \nstep is to create a high-level domain model. The second step is to define the system operations, which are defined \nin terms of the domain model.\n \n\n\n47\nDefining an application’s microservice architecture\nthe Place Order story. We can expand that story into numerous user scenarios includ-\ning this one:\nGiven a consumer\nAnd a restaurant\nAnd a delivery address/time that can be served by that restaurant\nAnd an order total that meets the restaurant's order minimum\nWhen the consumer places an order for the restaurant\nThen consumer's credit card is authorized\nAnd an order is created in the PENDING_ACCEPTANCE state\nAnd the order is associated with the consumer\nAnd the order is associated with the restaurant\nThe nouns in this user scenario hint at the existence of various classes, including\nConsumer, Order, Restaurant, and CreditCard.\n Similarly, the Accept Order story can be expanded into a scenario such as this one:\nGiven an order that is in the PENDING_ACCEPTANCE state\nand a courier that is available to deliver the order\nWhen a restaurant accepts an order with a promise to prepare by a particular\ntime\nThen the state of the order is changed to ACCEPTED\nAnd the order's promiseByTime is updated to the promised time\nAnd the courier is assigned to deliver the order\nThis scenario suggests the existence of Courier and Delivery classes. The end result\nafter a few iterations of analysis will be a domain model that consists, unsurprisingly,\nof those classes and others, such as MenuItem and Address. Figure 2.7 is a class dia-\ngram that shows the key classes.\nConsumer\nOrder\nstate\n...\ncreditcardId\n...\ndeliveryTime\nquantity\nname\nprice\nstreet1\nstreet2\ncity\nstate\nzip\nname\n...\navailable\n...\nlat\nlon\nRestaurant\nCourier\nLocation\nPaymentInfo\nDeliveryInfo\nOrderLineItem\nMenuItem\nAddress\nPlaced by\nFor\nAssigned to\nPaid using\nPays using\nFigure 2.7\nThe key classes in the FTGO domain model\n \n\n\n48\nCHAPTER 2\nDecomposition strategies\nThe responsibilities of each class are as follows:\n\nConsumer—A consumer who places orders.\n\nOrder—An order placed by a consumer. It describes the order and tracks its status.\n\nOrderLineItem—A line item of an Order.\n\nDeliveryInfo—The time and place to deliver an order.\n\nRestaurant—A restaurant that prepares orders for delivery to consumers.\n\nMenuItem—An item on the restaurant’s menu.\n\nCourier—A courier who deliver orders to consumers. It tracks the availability of\nthe courier and their current location.\n\nAddress—The address of a Consumer or a Restaurant.\n\nLocation—The latitude and longitude of a Courier.\nA class diagram such as the one in figure 2.7 illustrates one aspect of an application’s\narchitecture. But it isn’t much more than a pretty picture without the scenarios to ani-\nmate it. The next step is to define the system operations, which correspond to archi-\ntectural scenarios. \nDEFINING SYSTEM OPERATIONS\nOnce you’ve defined a high-level domain model, the next step is to identify the requests\nthat the application must handle. The details of the UI are beyond the scope of this\nbook, but you can imagine that in each user scenario, the UI will make requests to the\nbackend business logic to retrieve and update data. FTGO is primarily a web applica-\ntion, which means that most requests are HTTP-based, but it’s possible that some clients\nmight use messaging. Instead of committing to a specific protocol, therefore, it makes\nsense to use the more abstract notion of a system operation to represent requests.\n There are two types of system operations:\nCommands—System operations that create, update, and delete data\nQueries—System operations that read (query) data\nUltimately, these system operations will correspond to REST, RPC, or messaging\nendpoints, but for now thinking of them abstractly is useful. Let’s first identify some\ncommands.\n A good starting point for identifying system commands is to analyze the verbs in the\nuser stories and scenarios. Consider, for example, the Place Order story. It clearly sug-\ngests that the system must provide a Create Order operation. Many other stories individ-\nually map directly to system commands. Table 2.1 lists some of the key system commands.\nTable 2.1\nKey system commands for the FTGO application\nActor\nStory\nCommand\nDescription\nConsumer\nCreate Order\ncreateOrder()\nCreates an order\nRestaurant\nAccept Order\nacceptOrder()\nIndicates that the restaurant has \naccepted the order and is committed \nto preparing it by the indicated time\n \n\n\n49\nDefining an application’s microservice architecture\nA command has a specification that defines its parameters, return value, and behavior\nin terms of the domain model classes. The behavior specification consists of precondi-\ntions that must be true when the operation is invoked, and post-conditions that are\ntrue after the operation is invoked. Here, for example, is the specification of the\ncreateOrder() system operation:\nThe preconditions mirror the givens in the Place Order user scenario described ear-\nlier. The post-conditions mirror the thens from the scenario. When a system operation\nis invoked it will verify the preconditions and perform the actions required to make\nthe post-conditions true.\n Here’s the specification of the acceptOrder() system operation:\nRestaurant\nOrder Ready \nfor Pickup\nnoteOrderReadyForPickup()\nIndicates that the order is ready for \npickup\nCourier\nUpdate \nLocation\nnoteUpdatedLocation()\nUpdates the current location of the \ncourier\nCourier\nDelivery \npicked up\nnoteDeliveryPickedUp()\nIndicates that the courier has \npicked up the order\nCourier\nDelivery \ndelivered\nnoteDeliveryDelivered()\nIndicates that the courier has deliv-\nered the order\nOperation\ncreateOrder (consumer id, payment method, delivery address, delivery time, \nrestaurant id, order line items)\nReturns\norderId, …\nPreconditions\nThe consumer exists and can place orders.\nThe line items correspond to the restaurant’s menu items.\nThe delivery address and time can be serviced by the restaurant.\nPost-conditions\nThe consumer’s credit card was authorized for the order total.\nAn order was created in the PENDING_ACCEPTANCE state.\nOperation\nacceptOrder(restaurantId, orderId, readyByTime)\nReturns\n—\nPreconditions\nThe order.status is PENDING_ACCEPTANCE.\nA courier is available to deliver the order.\nPost-conditions\nThe order.status was changed to ACCEPTED.\nThe order.readyByTime was changed to the readyByTime.\nThe courier was assigned to deliver the order.\nTable 2.1\nKey system commands for the FTGO application (continued)\nActor\nStory\nCommand\nDescription\n \n\n\n50\nCHAPTER 2\nDecomposition strategies\nIts pre- and post-conditions mirror the user scenario from earlier.\n Most of the architecturally relevant system operations are commands. Sometimes,\nthough, queries, which retrieve data, are also important.\n Besides implementing commands, an application must also implement queries.\nThe queries provide the UI with the information a user needs to make decisions. At\nthis stage, we don’t have a particular UI design for FTGO application in mind, but\nconsider, for example, the flow when a consumer places an order:\n1\nUser enters delivery address and time.\n2\nSystem displays available restaurants.\n3\nUser selects restaurant.\n4\nSystem displays menu.\n5\nUser selects item and checks out.\n6\nSystem creates order.\nThis user scenario suggests the following queries:\n\nfindAvailableRestaurants(deliveryAddress, deliveryTime)—Retrieves the\nrestaurants that can deliver to the specified delivery address at the specified time\n\nfindRestaurantMenu(id)—Retrieves information about a restaurant including\nthe menu items\nOf the two queries, findAvailableRestaurants() is probably the most architecturally\nsignificant. It’s a complex query involving geosearch. The geosearch component of\nthe query consists of finding all points—restaurants—that are near a location—the\ndelivery address. It also filters out those restaurants that are closed when the order\nneeds to be prepared and picked up. Moreover, performance is critical, because this\nquery is executed whenever a consumer wants to place an order.\n The high-level domain model and the system operations capture what the applica-\ntion does. They help drive the definition of the application’s architecture. The behav-\nior of each system operation is described in terms of the domain model. Each\nimportant system operation represents an architecturally significant scenario that’s\npart of the description of the architecture.\n Once the system operations have been defined, the next step is to identify the\napplication’s services. As mentioned earlier, there isn’t a mechanical process to follow.\nThere are, however, various decomposition strategies that you can use. Each one\nattacks the problem from a different perspective and uses its own terminology. But\nwith all strategies, the end result is the same: an architecture consisting of services that\nare primarily organized around business rather than technical concepts.\n Let’s look at the first strategy, which defines services corresponding to business\ncapabilities. \n \n\n\n51\nDefining an application’s microservice architecture\n2.2.2\nDefining services by applying the Decompose by business \ncapability pattern\nOne strategy for creating a microservice architecture is to decompose by business\ncapability. A concept from business architecture modeling, a business capability is some-\nthing that a business does in order to generate value. The set of capabilities for a given\nbusiness depends on the kind of business. For example, the capabilities of an insur-\nance company typically include Underwriting, Claims management, Billing, Compliance,\nand so on. The capabilities of an online store include Order management, Inventory\nmanagement, Shipping, and so on.\nBUSINESS CAPABILITIES DEFINE WHAT AN ORGANIZATION DOES\nAn organization’s business capabilities capture what an organization’s business is.\nThey’re generally stable, as opposed to how an organization conducts its business, which\nchanges over time, sometimes dramatically. That’s especially true today, with the rapidly\ngrowing use of technology to automate many business processes. For example, it wasn’t\nthat long ago that you deposited checks at your bank by handing them to a teller. It then\nbecame possible to deposit checks using an ATM. Today you can conveniently deposit\nmost checks using your smartphone. As you can see, the Deposit check business capabil-\nity has remained stable, but the manner in which it’s done has drastically changed. \nIDENTIFYING BUSINESS CAPABILITIES\nAn organization’s business capabilities are identified by analyzing the organization’s\npurpose, structure, and business processes. Each business capability can be thought of\nas a service, except it’s business-oriented rather than technical. Its specification con-\nsists of various components, including inputs, outputs, and service-level agreements.\nFor example, the input to an Insurance underwriting capability is the consumer’s\napplication, and the outputs include approval and price.\n A business capability is often focused on a particular business object. For example,\nthe Claim business object is the focus of the Claim management capability. A capability\ncan often be decomposed into sub-capabilities. For example, the Claim management\ncapability has several sub-capabilities, including Claim information management, Claim\nreview, and Claim payment management.\n It is not difficult to imagine that the business capabilities for FTGO include the\nfollowing:\nSupplier management\n– Courier management—Managing courier information\n– Restaurant information management—Managing restaurant menus and other\ninformation, including location and open hours\nPattern: Decompose by business capability\nDefine services corresponding to business capabilities. See http://microservices.io/\npatterns/decomposition/decompose-by-business-capability.html.\n \n",
      "page_number": 67
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 77-85)",
      "start_page": 77,
      "end_page": 85,
      "detection_method": "topic_boundary",
      "content": "52\nCHAPTER 2\nDecomposition strategies\nConsumer management—Managing information about consumers\nOrder taking and fulfillment\n– Order management—Enabling consumers to create and manage orders\n– Restaurant order management—Managing the preparation of orders at a\nrestaurant\n– Logistics\n– Courier availability management—Managing the real-time availability of couri-\ners to delivery orders\n– Delivery management—Delivering orders to consumers\nAccounting\n– Consumer accounting—Managing billing of consumers\n– Restaurant accounting—Managing payments to restaurants\n– Courier accounting—Managing payments to couriers\n…\nThe top-level capabilities include Supplier management, Consumer management,\nOrder taking and fulfillment, and Accounting. There will likely be many other top-\nlevel capabilities, including marketing-related capabilities. Most top-level capabilities\nare decomposed into sub-capabilities. For example, Order taking and fulfillment is\ndecomposed into five sub-capabilities.\n On interesting aspect of this capability hierarchy is that there are three restaurant-\nrelated capabilities: Restaurant information management, Restaurant order manage-\nment, and Restaurant accounting. That’s because they represent three very different\naspects of restaurant operations.\n Next we’ll look at how to use business capabilities to define services. \nFROM BUSINESS CAPABILITIES TO SERVICES\nOnce you’ve identified the business capabilities, you then define a service for each\ncapability or group of related capabilities. Figure 2.8 shows the mapping from capabil-\nities to services for the FTGO application. Some top-level capabilities, such as the\nAccounting capability, are mapped to services. In other cases, sub-capabilities are\nmapped to services.\n The decision of which level of the capability hierarchy to map to services, because\nis somewhat subjective. My justification for this particular mapping is as follows:\nI mapped the sub-capabilities of Supplier management to two services, because\nRestaurants and Couriers are very different types of suppliers.\nI mapped the Order taking and fulfillment capability to three services that are\neach responsible for different phases of the process. I combined the Courier\navailability management and Delivery management capabilities and mapped\nthem to a single service because they’re deeply intertwined.\nI mapped the Accounting capability to its own service, because the different\ntypes of accounting seem similar.\n \n\n\n53\nDefining an application’s microservice architecture\nLater on, it may make sense to separate payments (of Restaurants and Couriers) and\nbilling (of Consumers).\n A key benefit of organizing services around capabilities is that because they’re sta-\nble, the resulting architecture will also be relatively stable. The individual components\nof the architecture may evolve as the how aspect of the business changes, but the archi-\ntecture remains unchanged.\n Having said that, it’s important to remember that the services shown in figure 2.8\nare merely the first attempt at defining the architecture. They may evolve over time as\nwe learn more about the application domain. In particular, an important step in the\narchitecture definition process is investigating how the services collaborate in each of\nthe key architectural services. You might, for example, discover that a particular\ndecomposition is inefficient due to excessive interprocess communication and that\nyou must combine services. Conversely, a service might grow in complexity to the\nCourier Service\nCourier management\nConsumer management\nSupplier management\nCapability hierarchy\nServices\nCouriers and restaurants\nare very different\nkinds of suppliers\n=> different services.\nThree different services\nhandling different\nphases of the order\ntaking and fulﬁllment\nTreat payments and\nbilling the same for now.\nRestaurant Service\nRestaurant information\nmanagement\nOrder Service\nOrder management\nOrder taking and fulﬁllment\nAccounting\nKitchen Service\nRestaurant order\nticket management\nConsumer Service\nDelivery Service\nConsumer accounting\nRestaurant accounting\nCourier accounting\nAccounting Service\nLogistics\nDelivery management\nCourier availability\nmanagement\nFigure 2.8\nMapping FTGO business capabilities to services. Capabilities at various levels of the \ncapability hierarchy are mapped to services.\n \n\n\n54\nCHAPTER 2\nDecomposition strategies\npoint where it becomes worthwhile to split it into multiple services. What’s more, in\nsection 2.2.5, I describe several obstacles to decomposition that might cause you to\nrevisit your decision.\n Let’s take a look at another way to decompose an application that is based on\ndomain-driven design. \n2.2.3\nDefining services by applying the Decompose by \nsub-domain pattern\nDDD, as described in the excellent book Domain-driven design by Eric Evans\n(Addison-Wesley Professional, 2003), is an approach for building complex software\napplications that is centered on the development of an object-oriented domain\nmodel. A domain mode captures knowledge about a domain in a form that can be\nused to solve problems within that domain. It defines the vocabulary used by the\nteam, what DDD calls the Ubiquitous Language. The domain model is closely mir-\nrored in the design and implementation of the application. DDD has two concepts\nthat are incredibly useful when applying the microservice architecture: subdomains\nand bounded contexts.\nDDD is quite different than the traditional approach to enterprise modeling, which\ncreates a single model for the entire enterprise. In such a model there would be, for\nexample, a single definition of each business entity, such as customer, order, and so\non. The problem with this kind of modeling is that getting different parts of an orga-\nnization to agree on a single model is a monumental task. Also, it means that from the\nperspective of a given part of the organization, the model is overly complex for their\nneeds. Moreover, the domain model can be confusing because different parts of the\norganization might use either the same term for different concepts or different terms\nfor the same concept. DDD avoids these problems by defining multiple domain mod-\nels, each with an explicit scope.\n DDD defines a separate domain model for each subdomain. A subdomain is a part\nof the domain, DDD’s term for the application’s problem space. Subdomains are iden-\ntified using the same approach as identifying business capabilities: analyze the busi-\nness and identify the different areas of expertise. The end result is very likely to be\nsubdomains that are similar to the business capabilities. The examples of subdomains\nin FTGO include Order taking, Order management, Kitchen management, Delivery,\nand Financials. As you can see, these subdomains are very similar to the business capa-\nbilities described earlier.\nPattern: Decompose by subdomain\nDefine services corresponding to DDD subdomains. See http://microservices.io\n/patterns/decomposition/decompose-by-subdomain.html.\n \n\n\n55\nDefining an application’s microservice architecture\n DDD calls the scope of a domain model a bounded context. A bounded context\nincludes the code artifacts that implement the model. When using the microservice\narchitecture, each bounded context is a service or possibly a set of services. We can\ncreate a microservice architecture by applying DDD and defining a service for each\nsubdomain. Figure 2.9 shows how the subdomains map to services, each with its own\ndomain model.\nDDD and the microservice architecture are in almost perfect alignment. The DDD\nconcept of subdomains and bounded contexts maps nicely to services within a micro-\nservice architecture. Also, the microservice architecture’s concept of autonomous\nteams owning services is completely aligned with the DDD’s concept of each domain\nmodel being owned and developed by a single team. Even better, as I describe later in\nthis section, the concept of a subdomain with its own domain model is a great way to\neliminate god classes and thereby make decomposition easier.\n Decompose by subdomain and Decompose by business capability are the two main\npatterns for defining an application’s microservice architecture. There are, however,\nsome useful guidelines for decomposition that have their roots in object-oriented\ndesign. Let’s take a look at them. \nAccounting Service\nAccounting\ndomain model\nKitchen Service\n.... Service\nOrder taking\nsubdomain\nMaps to\nMaps to\nMaps to\nMaps to\nMaps to\nKitchen\nsubdomain\nAccounting\nsubdomain\nDelivery\nsubdomain\n....\nsubdomain\nKitchen\ndomain model\nDelivery Service\nDelivery\ndomain model\nOrder Service\nFTGO domain\nOrder\ndomain model\nFigure 2.9\nFrom subdomains to services: each subdomain of the FTGO application domain \nis mapped to a service, which has its own domain model.\n \n\n\n56\nCHAPTER 2\nDecomposition strategies\n2.2.4\nDecomposition guidelines\nSo far in this chapter, we’ve looked at the main ways to define a microservice architec-\nture. We can also adapt and use a couple of principles from object-oriented design\nwhen applying the microservice architecture pattern. These principles were created\nby Robert C. Martin and described in his classic book Designing Object Oriented C++\nApplications Using The Booch Method (Prentice Hall, 1995). The first principle is the Sin-\ngle Responsibility Principle (SRP), for defining the responsibilities of a class. The sec-\nond principle is the Common Closure Principle (CCP), for organizing classes into\npackages. Let’s take a look at these principles and see how they can be applied to the\nmicroservice architecture.\nSINGLE RESPONSIBILITY PRINCIPLE\nOne of the main goals of software architecture and design is determining the respon-\nsibilities of each software element. The Single Responsibility Principle is as follows:\nA class should have only one reason to change.\n                                                                      \nRobert C. Martin\nEach responsibility that a class has is a potential reason for that class to change. If a\nclass has multiple responsibilities that change independently, the class won’t be stable.\nBy following the SRP, you define classes that each have a single responsibility and\nhence a single reason for change.\n We can apply SRP when defining a microservice architecture and create small,\ncohesive services that each have a single responsibility. This will reduce the size of the\nservices and increase their stability. The new FTGO architecture is an example of SRP\nin action. Each aspect of getting food to a consumer—order taking, order prepara-\ntion, and delivery—is the responsibility of a separate service. \nCOMMON CLOSURE PRINCIPLE\nThe other useful principle is the Common Closure Principle:\nThe classes in a package should be closed together against the same kinds of changes. A\nchange that affects a package affects all the classes in that package.\nRobert C. Martin\nThe idea is that if two classes change in lockstep because of the same underlying rea-\nson, then they belong in the same package. Perhaps, for example, those classes imple-\nment a different aspect of a particular business rule. The goal is that when that\nbusiness rule changes, developers only need to change code in a small number of\npackages (ideally only one). Adhering to the CCP significantly improves the maintain-\nability of an application.\n We can apply CCP when creating a microservice architecture and package compo-\nnents that change for the same reason into the same service. Doing this will minimize\n \n\n\n57\nDefining an application’s microservice architecture\nthe number of services that need to be changed and deployed when some require-\nment changes. Ideally, a change will only affect a single team and a single service. CCP\nis the antidote to the distributed monolith anti-pattern.\n SRP and CCP are 2 of the 11 principles developed by Bob Martin. They’re particu-\nlarly useful when developing a microservice architecture. The remaining nine princi-\nples are used when designing classes and packages. For more information about SRP,\nCCP, and the other OOD principles, see the article “The Principles of Object Ori-\nented Design” on Bob Martin’s website (http://butunclebob.com/ArticleS.UncleBob\n.PrinciplesOfOod).\n Decomposition by business capability and by subdomain along with SRP and CCP\nare good techniques for decomposing an application into services. In order to apply\nthem and successfully develop a microservice architecture, you must solve some trans-\naction management and interprocess communication issues. \n2.2.5\nObstacles to decomposing an application into services\nOn the surface, the strategy of creating a microservice architecture by defining ser-\nvices corresponding to business capabilities or subdomains looks straightforward. You\nmay, however, encounter several obstacles:\nNetwork latency\nReduced availability due to synchronous communication\nMaintaining data consistency across services\nObtaining a consistent view of the data\nGod classes preventing decomposition\nLet’s take a look at each obstacle, starting with network latency.\nNETWORK LATENCY\nNetwork latency is an ever-present concern in a distributed system. You might discover\nthat a particular decomposition into services results in a large number of round-trips\nbetween two services. Sometimes, you can reduce the latency to an acceptable amount\nby implementing a batch API for fetching multiple objects in a single round trip. But\nin other situations, the solution is to combine services, replacing expensive IPC with\nlanguage-level method or function calls. \nSYNCHRONOUS INTERPROCESS COMMUNICATION REDUCES AVAILABILITY\nAnother problem is how to implement interservice communication in a way that\ndoesn’t reduce availability. For example, the most straightforward way to implement\nthe createOrder() operation is for the Order Service to synchronously invoke the\nother services using REST. The drawback of using a protocol like REST is that it\nreduces the availability of the Order Service. It won’t be able to create an order if any\nof those other services are unavailable. Sometimes this is a worthwhile trade-off, but in\nchapter 3 you’ll learn that using asynchronous messaging, which eliminates tight cou-\npling and improves availability, is often a better choice. \n \n\n\n58\nCHAPTER 2\nDecomposition strategies\nMAINTAINING DATA CONSISTENCY ACROSS SERVICES\nAnother challenge is maintaining data consistency across services. Some system opera-\ntions need to update data in multiple services. For example, when a restaurant accepts\nan order, updates must occur in both the Kitchen Service and the Delivery Service.\nThe Kitchen Service changes the status of the Ticket. The Delivery Service sched-\nules delivery of the order. Both of these updates must be done atomically.\n The traditional solution is to use a two-phase, commit-based, distributed trans-\naction management mechanism. But as you’ll see in chapter 4, this is not a good\nchoice for modern applications, and you must use a very different approach to trans-\naction management, a saga. A saga is a sequence of local transactions that are coordi-\nnated using messaging. Sagas are more complex than traditional ACID transactions\nbut they work well in many situations. One limitation of sagas is that they are eventu-\nally consistent. If you need to update some data atomically, then it must reside within\na single service, which can be an obstacle to decomposition. \nOBTAINING A CONSISTENT VIEW OF THE DATA\nAnother obstacle to decomposition is the inability to obtain a truly consistent view of\ndata across multiple databases. In a monolithic application, the properties of ACID\ntransactions guarantee that a query will return a consistent view of the database. In\ncontrast, in a microservice architecture, even though each service’s database is consis-\ntent, you can’t obtain a globally consistent view of the data. If you need a consistent\nview of some data, then it must reside in a single service, which can prevent decompo-\nsition. Fortunately, in practice this is rarely a problem. \nGOD CLASSES PREVENT DECOMPOSITION\nAnother obstacle to decomposition is the existence of so-called god classes. God classes\nare the bloated classes that are used throughout an application (http://wiki.c2.com/\n?GodClass). A god class typically implements business logic for many different aspects\nof the application. It normally has a large number of fields mapped to a database\ntable with many columns. Most applications have at least one of these classes, each\nrepresenting a concept that’s central to the domain: accounts in banking, orders in\ne-commerce, policies in insurance, and so on. Because a god class bundles together\nstate and behavior for many different aspects of an application, it’s an insurmountable\nobstacle to splitting any business logic that uses it into services.\n The Order class is a great example of a god class in the FTGO application. That’s\nnot surprising—after all, the purpose of FTGO is to deliver food orders to customers.\nMost parts of the system involve orders. If the FTGO application had a single domain\nmodel, the Order class would be a very large class. It would have state and behavior\ncorresponding to many different parts of the application. Figure 2.10 shows the struc-\nture of this class that would be created using traditional modeling techniques.\n As you can see, the Order class has fields and methods corresponding to order pro-\ncessing, restaurant order management, delivery, and payments. This class also has a\ncomplex state model, due to the fact that one model has to describe state transitions\n \n\n\n59\nDefining an application’s microservice architecture\nfrom disparate parts of the application. In its current form, this class makes it extremely\ndifficult to split code into services.\n One solution is to package the Order class into a library and create a central Order\ndatabase. All services that process orders use this library and access the access data-\nbase. The trouble with this approach is that it violates one of the key principles of the\nmicroservice architecture and results in undesirable, tight coupling. For example, any\nchange to the Order schema requires the teams to update their code in lockstep.\n Another solution is to encapsulate the Order database in an Order Service, which\nis invoked by the other services to retrieve and update orders. The problem with that\ndesign is that the Order Service would be a data service with an anemic domain\nmodel containing little or no business logic. Neither of these options is appealing, but\nfortunately, DDD provides a solution.\n A much better approach is to apply DDD and treat each service as a separate sub-\ndomain with its own domain model. This means that each of the services in the FTGO\napplication that has anything to do with orders has its own domain model with its\nversion of the Order class. A great example of the benefit of multiple domain mod-\nels is the Delivery Service. Its view of an Order, shown in figure 2.11, is extremely\nsimple: pickup address, pickup time, delivery address, and delivery time. Moreover,\nrather than call it an Order, the Delivery Service uses the more appropriate name of\nDelivery.\nOrder\nOrderLineItem\nAddress\nCourier\nConsumer\nRestaurant\nPaymentInfo\nOrderTotal\ndeliveryTime\nstatus\n<<delivery>>\npickupTime\n<<billing>>\ntransactionid\n<<orderTaking>>\ncreate()\ncancel()\n<<restaurant>>\naccept()\nreject()\nnoteReadyForPickup()\n<<delivery>>\nassignCourier()\nnotePickedUp()\nnoteDelivered()\nFigure 2.10\nThe Order god class is bloated with numerous responsibilities.\n \n\n\n60\nCHAPTER 2\nDecomposition strategies\nThe Delivery Service isn’t interested in any of the other attributes of an order.\n The Kitchen Service also has a much simpler view of an order. Its version of an\nOrder is called a Ticket. As figure 2.12 shows, a Ticket simply consist of a status, the\nrequestedDeliveryTime, a prepareByTime, and a list of line items that tell the\nrestaurant what to prepare. It’s unconcerned with the consumer, payment, delivery,\nand so on.\nThe Order service has the most complex view of an order, shown in figure 2.13. Even\nthough it has quite a few fields and methods, it’s still much simpler than the original\nversion.\nThe Order class in each domain model represents different aspects of the same Order\nbusiness entity. The FTGO application must maintain consistency between these differ-\nent objects in different services. For example, once the Order Service has authorized\nDelivery\nAddress\nPickup location\nDelivery location\nAssigned to\nCourier\nstatus\nscheduledPickupTime\nScheduledDeliveryTime\nFigure 2.11\nThe Delivery Service domain model\nTicket\nstatus\nrequestedDeliveryTime\npreparedByTime\nTicketLineItem\nquantity\nitem\nFigure 2.12\nThe Kitchen Service domain model\nOrder\nOrderLineItem\nAddress\nConsumer\nRestaurant\nPaymentInfo\nstatus\norderTotal\ndeliveryTime\n...\nFigure 2.13\nThe Order Service domain model\n \n",
      "page_number": 77
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 86-93)",
      "start_page": 86,
      "end_page": 93,
      "detection_method": "topic_boundary",
      "content": "61\nDefining an application’s microservice architecture\nthe consumer’s credit card, it must trigger the creation of the Ticket in the Kitchen\nService. Similarly, if the restaurant rejects the order via the Kitchen Service, it must\nbe cancelled in the Order Service service, and the customer credited in the billing\nservice. In chapter 4, you’ll learn how to maintain consistency between services, using\nthe previously mentioned event-driven mechanism sagas.\n As well as creating technical challenges, having multiple domain models also\nimpacts the implementation of the user experience. An application must translate\nbetween the user experience, which is its own domain model, and the domain models\nof each of the services. In the FTGO application, for example, the Order status dis-\nplayed to a consumer is derived from Order information stored in multiple services.\nThis translation is often handled by the API gateway, discussed in chapter 8. Despite\nthese challenges, it’s essential that you identify and eliminate god classes when defin-\ning a microservice architecture.\n We’ll now look at how to define the service APIs. \n2.2.6\nDefining service APIs\nSo far, we have a list of system operations and a list of a potential services. The next\nstep is to define each service’s API: its operations and events. A service API operation\nexists for one of two reasons: some operations correspond to system operations. They\nare invoked by external clients and perhaps by other services. The other operations\nexist to support collaboration between services. These operations are only invoked by\nother services.\n A service publishes events primarily to enable it to collaborate with other ser-\nvices. Chapter 4 describes how events can be used to implement sagas, which main-\ntain data consistency across services. And chapter 7 discusses how events can be used\nto update CQRS views, which support efficient querying. An application can also use\nevents to notify external clients. For example, it could use WebSockets to deliver\nevents to a browser.\n The starting point for defining the service APIs is to map each system operation to\na service. After that, we decide whether a service needs to collaborate with others to\nimplement a system operation. If collaboration is required, we then determine what\nAPIs those other services must provide in order to support the collaboration. Let’s\nbegin by looking at how to assign system operations to services.\nASSIGNING SYSTEM OPERATIONS TO SERVICES\nThe first step is to decide which service is the initial entry point for a request. Many\nsystem operations neatly map to a service, but sometimes the mapping is less obvious.\nConsider, for example, the noteUpdatedLocation() operation, which updates the\ncourier location. On one hand, because it’s related to couriers, this operation should\nbe assigned to the Courier service. On the other hand, it’s the Delivery Service\nthat needs the courier location. In this case, assigning an operation to a service that\nneeds the information provided by the operation is a better choice. In other situations,\n \n\n\n62\nCHAPTER 2\nDecomposition strategies\nit might make sense to assign an operation to the service that has the information nec-\nessary to handle it.\n Table 2.2 shows which services in the FTGO application are responsible for which\noperations.\nAfter having assigned operations to services, the next step is to decide how the services\ncollaborate in order to handle each system operation. \nDETERMINING THE APIS REQUIRED TO SUPPORT COLLABORATION BETWEEN SERVICES\nSome system operations are handled entirely by a single service. For example, in the\nFTGO application, the Consumer Service handles the createConsumer() operation\nentirely by itself. But other system operations span multiple services. The data needed\nto handle one of these requests might, for instance, be scattered around multiple ser-\nvices. For example, in order to implement the createOrder() operation, the Order\nService must invoke the following services in order to verify its preconditions and\nmake the post-conditions become true:\n\nConsumer Service—Verify that the consumer can place an order and obtain their\npayment information.\n\nRestaurant Service—Validate the order line items, verify that the delivery\naddress/time is within the restaurant’s service area, verify order minimum is\nmet, and obtain prices for the order line items.\n\nKitchen Service—Create the Ticket.\n\nAccounting Service—Authorize the consumer’s credit card.\nSimilarly, in order to implement the acceptOrder() system operation, the Kitchen\nService must invoke the Delivery Service to schedule a courier to deliver the order.\nTable 2.3 shows the services, their revised APIs, and their collaborators. In order to\nfully define the service APIs, you need to analyze each system operation and deter-\nmine what collaboration is required.\nTable 2.2\nMapping system operations to services in the FTGO application\nService\nOperations\nConsumer Service\ncreateConsumer()\nOrder Service\ncreateOrder()\nRestaurant Service\nfindAvailableRestaurants()\nKitchen Service\nacceptOrder()\nnoteOrderReadyForPickup()\nDelivery Service\nnoteUpdatedLocation()\nnoteDeliveryPickedUp()\nnoteDeliveryDelivered()\n \n\n\n63\nDefining an application’s microservice architecture\nSo far, we’ve identified the services and the operations that each service implements.\nBut it’s important to remember that the architecture we’ve sketched out is very\nabstract. We’ve not selected any specific IPC technology. Moreover, even though the\nterm operation suggests some kind of synchronous request/response-based IPC mecha-\nnism, you’ll see that asynchronous messaging plays a significant role. Throughout this\nbook I describe architecture and design concepts that influence how these services\ncollaborate.\n Chapter 3 describes specific IPC technologies, including synchronous communica-\ntion mechanisms such as REST, and asynchronous messaging using a message broker.\nI discuss how synchronous communication can impact availability and introduce the\nconcept of a self-contained service, which doesn’t invoke other services synchronously.\nOne way to implement a self-contained service is to use the CQRS pattern, covered in\nchapter 7. The Order Service could, for example, maintain a replica of the data owned\nby the Restaurant Service in order to eliminate the need for it to synchronously\ninvoke the Restaurant Service to validate an order. It keeps the replica up-to-date by\nsubscribing to events published by the Restaurant Service whenever it updates\nits data.\n Chapter 4 introduces the saga concept and how it uses asynchronous messaging\nfor coordinating the services that participate in the saga. As well as reliably updating\nTable 2.3\nThe services, their revised APIs, and their collaborators\nService\nOperations\nCollaborators\nConsumer Service\nverifyConsumerDetails()\n—\nOrder Service\ncreateOrder()\nConsumer Service\nverifyConsumerDetails()\nRestaurant Service\nverifyOrderDetails()\nKitchen Service\ncreateTicket()\nAccounting Service\nauthorizeCard()\nRestaurant \nService\nfindAvailableRestaurants()\nverifyOrderDetails()\n—\nKitchen Service\ncreateTicket()\nacceptOrder()\nnoteOrderReadyForPickup()\nDelivery Service\nscheduleDelivery()\nDelivery Service\nscheduleDelivery()\nnoteUpdatedLocation()\nnoteDeliveryPickedUp()\nnoteDeliveryDelivered()\n—\nAccounting \nService\nauthorizeCard()\n—\n \n\n\n64\nCHAPTER 2\nDecomposition strategies\ndata scattered across multiple services, a saga is also a way to implement a self-contained\nservice. For example, I describe how the createOrder() operation is implemented\nusing a saga, which invokes services such as the Consumer Service, Kitchen Service,\nand Accounting Service using asynchronous messaging.\n Chapter 8 describes the concept of an API gateway, which exposes an API to exter-\nnal clients. An API gateway might implement a query operation using the API compo-\nsition pattern, described in chapter 7, rather than simply route it to the service. Logic\nin the API gateway gathers the data needed by the query by calling multiple services\nand combining the results. In this situation, the system operation is assigned to the\nAPI gateway rather than a service. The services need to implement the query opera-\ntions needed by the API gateway. \nSummary\nArchitecture determines your application’s -ilities, including maintainability,\ntestability, and deployability, which directly impact development velocity.\nThe microservice architecture is an architecture style that gives an application\nhigh maintainability, testability, and deployability.\nServices in a microservice architecture are organized around business concerns—\nbusiness capabilities or subdomains—rather than technical concerns.\nThere are two patterns for decomposition:\n– Decompose by business capability, which has its origins in business archi-\ntecture\n– Decompose by subdomain, based on concepts from domain-driven design\nYou can eliminate god classes, which cause tangled dependencies that prevent\ndecomposition, by applying DDD and defining a separate domain model for\neach service. \n \n\n\n65\nInterprocess\ncommunication in\na microservice architecture\nMary and her team, like most other developers, had some experience with inter-\nprocess communication (IPC) mechanisms. The FTGO application has a REST API\nthat’s used by mobile applications and browser-side JavaScript. It also uses various\nThis chapter covers\nApplying the communication patterns: Remote \nprocedure invocation, Circuit breaker, Client-side \ndiscovery, Self registration, Server-side discovery, \nThird party registration, Asynchronous messaging, \nTransactional outbox, Transaction log tailing, \nPolling publisher\nThe importance of interprocess communication in \na microservice architecture\nDefining and evolving APIs\nThe various interprocess communication options \nand their trade-offs\nThe benefits of services that communicate using \nasynchronous messaging\nReliably sending messages as part of a database \ntransaction\n \n\n\n66\nCHAPTER 3\nInterprocess communication in a microservice architecture\ncloud services, such as the Twilio messaging service and the Stripe payment service.\nBut within a monolithic application like FTGO, modules invoke one another via\nlanguage-level method or function calls. FTGO developers generally don’t need to\nthink about IPC unless they’re working on the REST API or the modules that inte-\ngrate with cloud services.\n In contrast, as you saw in chapter 2, the microservice architecture structures an\napplication as a set of services. Those services must often collaborate in order to han-\ndle a request. Because service instances are typically processes running on multiple\nmachines, they must interact using IPC. It plays a much more important role in a\nmicroservice architecture than it does in a monolithic application. Consequently, as\nthey migrate their application to microservices, Mary and the rest of the FTGO devel-\nopers will need to spend a lot more time thinking about IPC.\n There’s no shortage of IPC mechanisms to chose from. Today, the fashionable\nchoice is REST (with JSON). It’s important, though, to remember that there are no\nsilver bullets. You must carefully consider the options. This chapter explores various\nIPC options, including REST and messaging, and discusses the trade-offs.\n The choice of IPC mechanism is an important architectural decision. It can impact\napplication availability. What’s more, as I explain in this chapter and the next, IPC\neven intersects with transaction management. I favor an architecture consisting of\nloosely coupled services that communicate with one another using asynchronous mes-\nsaging. Synchronous protocols such as REST are used mostly to communicate with\nother applications.\n I begin this chapter with an overview of interprocess communication in micro-\nservice architecture. Next, I describe remote procedure invocation-based IPC, of which\nREST is the most popular example. I cover important topics including service discov-\nery and how to handle partial failure. After that, I describe asynchronous messaging-\nbased IPC. I also talk about scaling consumers while preserving message ordering,\ncorrectly handling duplicate messages, and transactional messaging. Finally, I go\nthrough the concept of self-contained services that handle synchronous requests with-\nout communicating with other services in order to improve availability.\n3.1\nOverview of interprocess communication in a \nmicroservice architecture\nThere are lots of different IPC technologies to choose from. Services can use\nsynchronous request/response-based communication mechanisms, such as HTTP-\nbased REST or gRPC. Alternatively, they can use asynchronous, message-based com-\nmunication mechanisms such as AMQP or STOMP. There are also a variety of differ-\nent messages formats. Services can use human-readable, text-based formats such as JSON\nor XML. Alternatively, they could use a more efficient binary format such as Avro or\nProtocol Buffers.\n Before getting into the details of specific technologies, I want to bring up several\ndesign issues you should consider. I start this section with a discussion of interaction\n \n\n\n67\nOverview of interprocess communication in a microservice architecture\nstyles, which are a technology-independent way of describing how clients and services\ninteract. Next I discuss the importance of precisely defining APIs in a microservice\narchitecture, including the concept of API-first design. After that, I discuss the\nimportant topic of API evolution. Finally, I discuss different options for message for-\nmats and how they can determine ease of API evolution. Let’s begin by looking at\ninteraction styles.\n3.1.1\nInteraction styles\nIt’s useful to first think about the style of interaction between a service and its clients\nbefore selecting an IPC mechanism for a service’s API. Thinking first about the inter-\naction style will help you focus on the requirements and avoid getting mired in the\ndetails of a particular IPC technology. Also, as described in section 3.4, the choice of\ninteraction style impacts the availability of your application. Furthermore, as you’ll see\nin chapters 9 and 10, it helps you select the appropriate integration testing strategy.\n There are a variety of client-service interaction styles. As table 3.1 shows, they can\nbe categorized in two dimensions. The first dimension is whether the interaction is\none-to-one or one-to-many:\nOne-to-one—Each client request is processed by exactly one service.\nOne-to-many—Each request is processed by multiple services.\nThe second dimension is whether the interaction is synchronous or asynchronous:\nSynchronous—The client expects a timely response from the service and might\neven block while it waits.\nAsynchronous—The client doesn’t block, and the response, if any, isn’t necessar-\nily sent immediately.\nThe following are the different types of one-to-one interactions:\nRequest/response—A service client makes a request to a service and waits for a\nresponse. The client expects the response to arrive in a timely fashion. It might\nevent block while waiting. This is an interaction style that generally results in\nservices being tightly coupled.\nAsynchronous request/response—A service client sends a request to a service, which\nreplies asynchronously. The client doesn’t block while waiting, because the ser-\nvice might not send the response for a long time.\nTable 3.1\nThe various interaction styles can be characterized in two dimensions: one-to-one vs one-to-\nmany and synchronous vs asynchronous.\none-to-one\none-to-many\nSynchronous\nRequest/response\n—\nAsynchronous\nAsynchronous request/response\nOne-way notifications\nPublish/subscribe\nPublish/async responses\n \n\n\n68\nCHAPTER 3\nInterprocess communication in a microservice architecture\nOne-way notifications—A service client sends a request to a service, but no reply\nis expected or sent.\nIt’s important to remember that the synchronous request/response interaction style is\nmostly orthogonal to IPC technologies. A service can, for example, interact with\nanother service using request/response style interaction with either REST or messag-\ning. Even if two services are communicating using a message broker, the client service\nmight be blocked waiting for a response. It doesn’t necessarily mean they’re loosely\ncoupled. That’s something I revisit later in this chapter when discussing the impact of\ninter-service communication on availability.\n The following are the different types of one-to-many interactions:\nPublish/subscribe—A client publishes a notification message, which is consumed\nby zero or more interested services.\nPublish/async responses—A client publishes a request message and then waits for\na certain amount of time for responses from interested services.\nEach service will typically use a combination of these interaction styles. Many of the\nservices in the FTGO application have both synchronous and asynchronous APIs for\noperations, and many also publish events.\n Let’s look at how to define a service’s API. \n3.1.2\nDefining APIs in a microservice architecture\nAPIs or interfaces are central to software development. An application is comprised of\nmodules. Each module has an interface that defines the set of operations that mod-\nule’s clients can invoke. A well-designed interface exposes useful functionality while\nhiding the implementation. It enables the implementation to change without impact-\ning clients.\n In a monolithic application, an interface is typically specified using a program-\nming language construct such as a Java interface. A Java interface specifies a set of\nmethods that a client can invoke. The implementation class is hidden from the client.\nMoreover, because Java is a statically typed language, if the interface changes to be\nincompatible with the client, the application won’t compile.\n APIs and interfaces are equally important in a microservice architecture. A ser-\nvice’s API is a contract between the service and its clients. As described in chapter 2, a\nservice’s API consists of operations, which clients can invoke, and events, which are\npublished by the service. An operation has a name, parameters, and a return type. An\nevent has a type and a set of fields and is, as described in section 3.3, published to a\nmessage channel.\n The challenge is that a service API isn’t defined using a simple programming lan-\nguage construct. By definition, a service and its clients aren’t compiled together. If a\nnew version of a service is deployed with an incompatible API, there’s no compilation\nerror. Instead, there will be runtime failures.\n \n",
      "page_number": 86
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 94-101)",
      "start_page": 94,
      "end_page": 101,
      "detection_method": "topic_boundary",
      "content": "69\nOverview of interprocess communication in a microservice architecture\n Regardless of which IPC mechanism you choose, it’s important to precisely define\na service’s API using some kind of interface definition language (IDL). Moreover, there\nare good arguments for using an API-first approach to defining services (see www\n.programmableweb.com/news/how-to-design-great-apis-api-first-design-and-raml/how-to/\n2015/07/10 for more). First you write the interface definition. Then you review the\ninterface definition with the client developers. Only after iterating on the API defini-\ntion do you then implement the service. Doing this up-front design increases your\nchances of building a service that meets the needs of its clients.\nThe nature of the API definition depends on which IPC mechanism you’re using. For\nexample, if you’re using messaging, the API consists of the message channels, the mes-\nsage types, and the message formats. If you’re using HTTP, the API consists of the\nURLs, the HTTP verbs, and the request and response formats. Later in this chapter,\nI explain how to define APIs.\n A service’s API is rarely set in stone. It will likely evolve over time. Let’s take a look\nat how to do that and consider the issues you’ll face. \n3.1.3\nEvolving APIs\nAPIs invariably change over time as new features are added, existing features are\nchanged, and (perhaps) old features are removed. In a monolithic application, it’s rel-\natively straightforward to change an API and update all the callers. If you’re using a\nstatically typed language, the compiler helps by giving a list of compilation errors. The\nonly challenge may be the scope of the change. It might take a long time to change a\nwidely used API.\n In a microservices-based application, changing a service’s API is a lot more diffi-\ncult. A service’s clients are other services, which are often developed by other teams.\nThe clients may even be other applications outside of the organization. You usually\ncan’t force all clients to upgrade in lockstep with the service. Also, because modern\napplications are usually never down for maintenance, you’ll typically perform a rolling\nupgrade of your service, so both old and new versions of a service will be running\nsimultaneously.\n It’s important to have a strategy for dealing with these challenges. How you handle\na change to an API depends on the nature of the change.\nAPI-first design is essential\nEven in small projects, I’ve seen problems occur because components don’t agree\non an API. For example, on one project the backend Java developer and the AngularJS\nfrontend developer both said they had completed development. The application, how-\never, didn’t work. The REST and WebSocket API used by the frontend application to\ncommunicate with the backend was poorly defined. As a result, the two applications\ncouldn’t communicate!\n \n\n\n70\nCHAPTER 3\nInterprocess communication in a microservice architecture\nUSE SEMANTIC VERSIONING\nThe Semantic Versioning specification (http://semver.org) is a useful guide to ver-\nsioning APIs. It’s a set of rules that specify how version numbers are used and incre-\nmented. Semantic versioning was originally intended to be used for versioning of\nsoftware packages, but you can use it for versioning APIs in a distributed system.\n The Semantic Versioning specification (Semvers) requires a version number to\nconsist of three parts: MAJOR.MINOR.PATCH. You must increment each part of a version\nnumber as follows:\n\nMAJOR—When you make an incompatible change to the API\n\nMINOR—When you make backward-compatible enhancements to the API\n\nPATCH—When you make a backward-compatible bug fix\nThere are a couple of places you can use the version number in an API. If you’re\nimplementing a REST API, you can, as mentioned below, use the major version as\nthe first element of the URL path. Alternatively, if you’re implementing a service\nthat uses messaging, you can include the version number in the messages that it\npublishes. The goal is to properly version APIs and to evolve them in a controlled\nfashion. Let’s look at how to handle minor and major changes. \nMAKING MINOR, BACKWARD-COMPATIBLE CHANGES\nIdeally, you should strive to only make backward-compatible changes. Backward-\ncompatible changes are additive changes to an API:\nAdding optional attributes to request\nAdding attributes to a response\nAdding new operations\nIf you only ever make these kinds of changes, older clients will work with newer services,\nprovided that they observe the Robustness principle (https://en.wikipedia.org/wiki/\nRobustness_principle), which states: “Be conservative in what you do, be liberal in\nwhat you accept from others.” Services should provide default values for missing\nrequest attributes. Similarly, clients should ignore any extra response attributes. In\norder for this to be painless, clients and services must use a request and response for-\nmat that supports the Robustness principle. Later in this section, I describe how text-\nbased formats such as JSON and XML generally make it easier to evolve APIs. \nMAKING MAJOR, BREAKING CHANGES\nSometimes you must make major, incompatible changes to an API. Because you can’t\nforce clients to upgrade immediately, a service must simultaneously support old and\nnew versions of an API for some period of time. If you’re using an HTTP-based IPC\nmechanism, such as REST, one approach is to embed the major version number in the\nURL. For example, version 1 paths are prefixed with '/v1/…', and version 2 paths\nwith '/v2/…'.\n \n\n\n71\nOverview of interprocess communication in a microservice architecture\n Another option is to use HTTP’s content negotiation mechanism and include the\nversion number in the MIME type. For example, a client would request version 1.x of\nan Order using a request like this:\nGET /orders/xyz HTTP/1.1\nAccept: application/vnd.example.resource+json; version=1\n...\nThis request tells the Order Service that the client expects a version 1.x response.\n In order to support multiple versions of an API, the service’s adapters that imple-\nment the APIs will contain logic that translates between the old and new versions.\nAlso, as described in chapter 8, the API gateway will almost certainly use versioned\nAPIs. It may even have to support numerous older versions of an API.\n Now we’ll look at the issue of message formats, the choice of which can impact how\neasy evolving an API will be. \n3.1.4\nMessage formats\nThe essence of IPC is the exchange of messages. Messages usually contain data, and so\nan important design decision is the format of that data. The choice of message format\ncan impact the efficiency of IPC, the usability of the API, and its evolvability. If you’re\nusing a messaging system or protocols such as HTTP, you get to pick your message for-\nmat. Some IPC mechanisms—such as gRPC, which you’ll learn about shortly—might\ndictate the message format. In either case, it’s essential to use a cross-language mes-\nsage format. Even if you’re writing your microservices in a single language today, it’s\nlikely that you’ll use other languages in the future. You shouldn’t, for example, use\nJava serialization.\n There are two main categories of message formats: text and binary. Let’s look at\neach one.\nTEXT-BASED MESSAGE FORMATS\nThe first category is text-based formats such as JSON and XML. An advantage of these\nformats is that not only are they human readable, they’re self describing. A JSON mes-\nsage is a collection of named properties. Similarly, an XML message is effectively a col-\nlection of named elements and values. This format enables a consumer of a message\nto pick out the values of interest and ignore the rest. Consequently, many changes to\nthe message schema can easily be backward-compatible.\n The structure of XML documents is specified by an XML schema (www.w3.org/\nXML/Schema). Over time, the developer community has come to realize that JSON also\nneeds a similar mechanism. One popular option is to use the JSON Schema standard\n(http://json-schema.org). A JSON schema defines the names and types of a message’s\nproperties and whether they’re optional or required. As well as being useful documenta-\ntion, a JSON schema can be used by an application to validate incoming messages.\n A downside of using a text-based messages format is that the messages tend to be\nverbose, especially XML. Every message has the overhead of containing the names of\n \n\n\n72\nCHAPTER 3\nInterprocess communication in a microservice architecture\nthe attributes in addition to their values. Another drawback is the overhead of parsing\ntext, especially when messages are large. Consequently, if efficiency and performance\nare important, you may want to consider using a binary format. \nBINARY MESSAGE FORMATS\nThere are several different binary formats to choose from. Popular formats include\nProtocol Buffers (https://developers.google.com/protocol-buffers/docs/overview)\nand Avro (https://avro.apache.org). Both formats provide a typed IDL for defining\nthe structure of your messages. A compiler then generates the code that serializes and\ndeserializes the messages. You’re forced to take an API-first approach to service\ndesign! Moreover, if you write your client in a statically typed language, the compiler\nchecks that it uses the API correctly.\n One difference between these two binary formats is that Protocol Buffers uses\ntagged fields, whereas an Avro consumer needs to know the schema in order to inter-\npret messages. As a result, handling API evolution is easier with Protocol Buffers\nthan with Avro. This blog post (http://martin.kleppmann.com/2012/12/05/schema-\nevolution-in-avro-protocol-buffers-thrift.html) is an excellent comparison of Thrift,\nProtocol Buffers, and Avro.\n Now that we’ve looked at message formats, let’s look at specific IPC mechanisms\nthat transport the messages, starting with the Remote procedure invocation (RPI)\npattern. \n3.2\nCommunicating using the synchronous Remote \nprocedure invocation pattern\nWhen using a remote procedure invocation-based IPC mechanism, a client sends a\nrequest to a service, and the service processes the request and sends back a response.\nSome clients may block waiting for a response, and others might have a reactive, non-\nblocking architecture. But unlike when using messaging, the client assumes that the\nresponse will arrive in a timely fashion.\n Figure 3.1 shows how RPI works. The business logic in the client invokes a proxy\ninterface , implemented by an RPI proxy adapter class. The RPI proxy makes a request to\nthe service. The request is handled by an RPI server adapter class, which invokes the\nservice’s business logic via an interface. It then sends back a reply to the RPI proxy,\nwhich returns the result to the client’s business logic.\nThe proxy interface usually encapsulates the underlying communication protocol.\nThere are numerous protocols to choose from. In this section, I describe REST and\nPattern: Remote procedure invocation\nA client invokes a service using a synchronous, remote procedure invocation-based\nprotocol, such as REST (http://microservices.io/patterns/communication-style/\nmessaging.html).\n \n\n\n73\nCommunicating using the synchronous Remote procedure invocation pattern\ngRPC. I cover how to improve the availability of your services by properly handling\npartial failure and explain why a microservices-based application that uses RPI must\nuse a service discovery mechanism.\n Let’s first take a look at REST.\n3.2.1\nUsing REST\nToday, it’s fashionable to develop APIs in the RESTful style (https://en.wikipedia\n.org/wiki/Representational_state_transfer). REST is an IPC mechanism that (almost\nalways) uses HTTP. Roy Fielding, the creator of REST, defines REST as follows:\nREST provides a set of architectural constraints that, when applied as a whole, emphasizes\nscalability of component interactions, generality of interfaces, independent deployment of\ncomponents, and intermediary components to reduce interaction latency, enforce security,\nand encapsulate legacy systems.\nwww.ics.uci.edu/~fielding/pubs/dissertation/top.htm\nA key concept in REST is a resource, which typically represents a single business\nobject, such as a Customer or Product, or a collection of business objects. REST\nuses the HTTP verbs for manipulating resources, which are referenced using a\nURL. For example, a GET request returns the representation of a resource, which is\noften in the form of an XML document or JSON object, although other formats\nsuch as binary can be used. A POST request creates a new resource, and a PUT\nrequest updates a resource. The Order Service, for example, has a POST /orders\nendpoint for creating an Order and a GET /orders/{orderId} endpoint for retriev-\ning an Order.\nBusiness logic\ninvokes\nBusiness logic\nProxy interface\nService interface\nClient\nService\nRPI\nproxy\nRequest\nReply\nRPI\nserver\nFigure 3.1\nThe client’s business logic invokes an interface that is implemented by an RPI proxy \nadapter class. The RPI proxy class makes a request to the service. The RPI server adapter class \nhandles the request by invoking the service’s business logic.\n \n\n\n74\nCHAPTER 3\nInterprocess communication in a microservice architecture\n Many developers claim their HTTP-based APIs are RESTful. But as Roy Fielding\ndescribes in a blog post, not all of them actually are (http://roy.gbiv.com/untangled/\n2008/rest-apis-must-be-hypertext-driven). To understand why, let’s take a look at the\nREST maturity model.\nTHE REST MATURITY MODEL\nLeonard Richardson (no relation to your author) defines a very useful maturity model\nfor REST (http://martinfowler.com/articles/richardsonMaturityModel.html) that con-\nsists of the following levels:\nLevel 0—Clients of a level 0 service invoke the service by making HTTP POST\nrequests to its sole URL endpoint. Each request specifies the action to perform,\nthe target of the action (for example, the business object), and any parameters.\nLevel 1—A level 1 service supports the idea of resources. To perform an action\non a resource, a client makes a POST request that specifies the action to per-\nform and any parameters.\nLevel 2—A level 2 service uses HTTP verbs to perform actions: GET to retrieve,\nPOST to create, and PUT to update. The request query parameters and body, if\nany, specify the actions' parameters. This enables services to use web infrastruc-\nture such as caching for GET requests.\nLevel 3—The design of a level 3 service is based on the terribly named\nHATEOAS (Hypertext As The Engine Of Application State) principle. The\nbasic idea is that the representation of a resource returned by a GET request\ncontains links for performing actions on that resource. For example, a client\ncan cancel an order using a link in the representation returned by the GET\nrequest that retrieved the order. The benefits of HATEOAS include no longer\nhaving to hard-wire URLs into client code (www.infoq.com/news/2009/04/\nhateoas-restful-api-advantages).\nI encourage you to review the REST APIs at your organization to see which level they\ncorrespond to. \nSPECIFYING REST APIS\nAs mentioned earlier in section 3.1, you must define your APIs using an interface defi-\nnition language (IDL). Unlike older communication protocols like CORBA and\nSOAP, REST did not originally have an IDL. Fortunately, the developer community\nhas rediscovered the value of an IDL for RESTful APIs. The most popular REST IDL is\nthe Open API Specification (www.openapis.org), which evolved from the Swagger\nopen source project. The Swagger project is a set of tools for developing and docu-\nmenting REST APIs. It includes tools that generate client stubs and server skeletons\nfrom an interface definition. \nTHE CHALLENGE OF FETCHING MULTIPLE RESOURCES IN A SINGLE REQUEST\nREST resources are usually oriented around business objects, such as Consumer and\nOrder. Consequently, a common problem when designing a REST API is how to\n \n\n\n75\nCommunicating using the synchronous Remote procedure invocation pattern\nenable the client to retrieve multiple related objects in a single request. For example,\nimagine that a REST client wanted to retrieve an Order and the Order's Consumer. A\npure REST API would require the client to make at least two requests, one for the\nOrder and another for its Consumer. A more complex scenario would require even\nmore round-trips and suffer from excessive latency.\n One solution to this problem is for an API to allow the client to retrieve related\nresources when it gets a resource. For example, a client could retrieve an Order and its\nConsumer using GET /orders/order-id-1345?expand=consumer. The query parame-\nter specifies the related resources to return with the Order. This approach works well\nin many scenarios but it’s often insufficient for more complex scenarios. It’s also\npotentially time consuming to implement. This has led to the increasing popularity of\nalternative API technologies such as GraphQL (http://graphql.org) and Netflix Falcor\n(http://netflix.github.io/falcor/), which are designed to support efficient data fetching. \nTHE CHALLENGE OF MAPPING OPERATIONS TO HTTP VERBS\nAnother common REST API design problem is how to map the operations you want\nto perform on a business object to an HTTP verb. A REST API should use PUT for\nupdates, but there may be multiple ways to update an order, including cancelling it,\nrevising the order, and so on. Also, an update might not be idempotent, which is a\nrequirement for using PUT. One solution is to define a sub-resource for updating a\nparticular aspect of a resource. The Order Service, for example, has a POST /orders/\n{orderId}/cancel endpoint for cancelling orders, and a POST /orders/{orderId}/\nrevise endpoint for revising orders. Another solution is to specify a verb as a URL\nquery parameter. Sadly, neither solution is particularly RESTful.\n This problem with mapping operations to HTTP verbs has led to the growing pop-\nularity of alternatives to REST, such as gPRC, discussed shortly in section 3.2.2. But\nfirst let’s look at the benefits and drawbacks of REST. \nBENEFITS AND DRAWBACKS OF REST\nThere are numerous benefits to using REST:\nIt’s simple and familiar.\nYou can test an HTTP API from within a browser using, for example, the Post-\nman plugin, or from the command line using curl (assuming JSON or some\nother text format is used).\nIt directly supports request/response style communication.\nHTTP is, of course, firewall friendly.\nIt doesn’t require an intermediate broker, which simplifies the system’s archi-\ntecture.\nThere are some drawbacks to using REST:\nIt only supports the request/response style of communication.\nReduced availability. Because the client and service communicate directly with-\nout an intermediary to buffer messages, they must both be running for the\nduration of the exchange.\n \n\n\n76\nCHAPTER 3\nInterprocess communication in a microservice architecture\nClients must know the locations (URLs) of the service instances(s). As described\nin section 3.2.4, this is a nontrivial problem in a modern application. Clients must\nuse what is known as a service discovery mechanism to locate service instances.\nFetching multiple resources in a single request is challenging.\nIt’s sometimes difficult to map multiple update operations to HTTP verbs.\nDespite these drawbacks, REST seems to be the de facto standard for APIs, though\nthere are a couple of interesting alternatives. GraphQL, for example, implements\nflexible, efficient data fetching. Chapter 8 discusses GraphQL and covers the API\ngateway pattern.\n gRPC is another alternative to REST. Let’s take a look at how it works. \n3.2.2\nUsing gRPC\nAs mentioned in the preceding section, one challenge with using REST is that\nbecause HTTP only provides a limited number of verbs, it’s not always straightforward\nto design a REST API that supports multiple update operations. An IPC technology\nthat avoids this issue is gRPC (www.grpc.io), a framework for writing cross-language\nclients and servers (see https://en.wikipedia.org/wiki/Remote_procedure_call for\nmore). gRPC is a binary message-based protocol, and this means—as mentioned ear-\nlier in the discussion of binary message formats—you’re forced to take an API-first\napproach to service design. You define your gRPC APIs using a Protocol Buffers-based\nIDL, which is Google’s language-neutral mechanism for serializing structured data.\nYou use the Protocol Buffer compiler to generate client-side stubs and server-side skel-\netons. The compiler can generate code for a variety of languages, including Java, C#,\nNodeJS, and GoLang. Clients and servers exchange binary messages in the Protocol\nBuffers format using HTTP/2.\n A gRPC API consists of one or more services and request/response message defini-\ntions. A service definition is analogous to a Java interface and is a collection of strongly\ntyped methods. As well as supporting simple request/response RPC, gRPC support\nstreaming RPC. A server can reply with a stream of messages to the client. Alterna-\ntively, a client can send a stream of messages to the server.\n gRPC uses Protocol Buffers as the message format. Protocol Buffers is, as men-\ntioned earlier, an efficient, compact, binary format. It’s a tagged format. Each field of\na Protocol Buffers message is numbered and has a type code. A message recipient can\nextract the fields that it needs and skip over the fields that it doesn’t recognize. As a\nresult, gRPC enables APIs to evolve while remaining backward-compatible.\n Listing 3.1 shows an excerpt of the gRPC API for the Order Service. It defines sev-\neral methods, including createOrder(). This method takes a CreateOrderRequest as\na parameter and returns a CreateOrderReply.\nservice OrderService {\nrpc createOrder(CreateOrderRequest) returns (CreateOrderReply) {}\nListing 3.1\nAn excerpt of the gRPC API for the Order Service\n \n",
      "page_number": 94
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 102-109)",
      "start_page": 102,
      "end_page": 109,
      "detection_method": "topic_boundary",
      "content": "77\nCommunicating using the synchronous Remote procedure invocation pattern\nrpc cancelOrder(CancelOrderRequest) returns (CancelOrderReply) {}\nrpc reviseOrder(ReviseOrderRequest) returns (ReviseOrderReply) {}\n...\n}\nmessage CreateOrderRequest {\nint64 restaurantId = 1;\nint64 consumerId = 2;\nrepeated LineItem lineItems = 3;\n...\n}\nmessage LineItem {\nstring menuItemId = 1;\nint32 quantity = 2;\n}\nmessage CreateOrderReply {\nint64 orderId = 1;\n}\n...\nCreateOrderRequest and CreateOrderReply are typed messages. For example, Create-\nOrderRequest message has a restaurantId field of type int64. The field’s tag value is 1.\n gRPC has several benefits:\nIt’s straightforward to design an API that has a rich set of update operations.\nIt has an efficient, compact IPC mechanism, especially when exchanging large\nmessages.\nBidirectional streaming enables both RPI and messaging styles of communication.\nIt enables interoperability between clients and services written in a wide range\nof languages.\ngRPC also has several drawbacks:\nIt takes more work for JavaScript clients to consume gRPC-based API than\nREST/JSON-based APIs.\nOlder firewalls might not support HTTP/2.\ngRPC is a compelling alternative to REST, but like REST, it’s a synchronous communi-\ncation mechanism, so it also suffers from the problem of partial failure. Let’s take a\nlook at what that is and how to handle it. \n3.2.3\nHandling partial failure using the Circuit breaker pattern\nIn a distributed system, whenever a service makes a synchronous request to another\nservice, there is an ever-present risk of partial failure. Because the client and the ser-\nvice are separate processes, a service may not be able to respond in a timely way to a\nclient’s request. The service could be down because of a failure or for maintenance.\nOr the service might be overloaded and responding extremely slowly to requests.\n \n\n\n78\nCHAPTER 3\nInterprocess communication in a microservice architecture\nBecause the client is blocked waiting for a response, the danger is that the failure\ncould cascade to the client’s clients and so on and cause an outage.\nConsider, for example, the scenario shown in figure 3.2, where the Order Service is\nunresponsive. A mobile client makes a REST request to an API gateway, which, as dis-\ncussed in chapter 8, is the entry point into the application for API clients. The API\ngateway proxies the request to the unresponsive Order Service.\nA naive implementation of the OrderServiceProxy would block indefinitely, waiting\nfor a response. Not only would that result in a poor user experience, but in many\napplications it would consume a precious resource, such as a thread. Eventually the\nAPI gateway would run out of resources and become unable to handle requests. The\nentire API would be unavailable.\n It’s essential that you design your services to prevent partial failures from cascading\nthroughout the application. There are two parts to the solution:\nYou must use design RPI proxies, such as OrderServiceProxy, to handle unre-\nsponsive remote services.\nYou need to decide how to recover from a failed remote service.\nFirst we’ll look at how to write robust RPI proxies.\nPattern: Circuit breaker\nAn RPI proxy that immediately rejects invocations for a timeout period after the num-\nber of consecutive failures exceeds a specified threshold. See http://microservices\n.io/patterns/reliability/circuit-breaker.html.\nAPI\ngateway\nUnresponsive remote service\nMobile\napp\nOrder\nService\nOrder\nService\nproxy\nCreate\norder\nendpoint\nPOST/orders\nPOST/orders\nFigure 3.2\nAn API gateway must protect itself from unresponsive services, such as the Order \nService.\n \n\n\n79\nCommunicating using the synchronous Remote procedure invocation pattern\nDEVELOPING ROBUST RPI PROXIES\nWhenever one service synchronously invokes another service, it should protect itself\nusing the approach described by Netflix (http://techblog.netflix.com/2012/02/fault-\ntolerance-in-high-volume.html). This approach consists of a combination of the fol-\nlowing mechanisms:\nNetwork timeouts—Never block indefinitely and always use timeouts when wait-\ning for a response. Using timeouts ensures that resources are never tied up\nindefinitely.\nLimiting the number of outstanding requests from a client to a service—Impose an upper\nbound on the number of outstanding requests that a client can make to a par-\nticular service. If the limit has been reached, it’s probably pointless to make\nadditional requests, and those attempts should fail immediately.\nCircuit breaker pattern—Track the number of successful and failed requests,\nand if the error rate exceeds some threshold, trip the circuit breaker so that\nfurther attempts fail immediately. A large number of requests failing suggests\nthat the service is unavailable and that sending more requests is pointless.\nAfter a timeout period, the client should try again, and, if successful, close the\ncircuit breaker.\nNetflix Hystrix (https://github.com/Netflix/Hystrix) is an open source library that\nimplements these and other patterns. If you’re using the JVM, you should definitely\nconsider using Hystrix when implementing RPI proxies. And if you’re running in a\nnon-JVM environment, you should use an equivalent library. For example, the Polly\nlibrary is popular in the .NET community (https://github.com/App-vNext/Polly). \nRECOVERING FROM AN UNAVAILABLE SERVICE\nUsing a library such as Hystrix is only part of the solution. You must also decide on a\ncase-by-case basis how your services should recover from an unresponsive remote ser-\nvice. One option is for a service to simply return an error to its client. For example,\nthis approach makes sense for the scenario shown in figure 3.2, where the request to\ncreate an Order fails. The only option is for the API gateway to return an error to the\nmobile client.\n In other scenarios, returning a fallback value, such as either a default value or a\ncached response, may make sense. For example, chapter 7 describes how the API gate-\nway could implement the findOrder() query operation by using the API composition\npattern. As figure 3.3 shows, its implementation of the GET /orders/{orderId} end-\npoint invokes several services, including the Order Service, Kitchen Service, and\nDelivery Service, and combines the results.\n It’s likely that each service’s data isn’t equally important to the client. The data\nfrom the Order Service is essential. If this service is unavailable, the API gateway\nshould return either a cached version of its data or an error. The data from the other\nservices is less critical. A client can, for example, display useful information to the user\neven if the delivery status was unavailable. If the Delivery Service is unavailable,\n \n\n\n80\nCHAPTER 3\nInterprocess communication in a microservice architecture\nthe API gateway should return either a cached version of its data or omit it from the\nresponse.\n It’s essential that you design your services to handle partial failure, but that’s not\nthe only problem you need to solve when using RPI. Another problem is that in order\nfor one service to invoke another service using RPI, it needs to know the network\nlocation of a service instance. On the surface this sounds simple, but in practice it’s\na challenging problem. You must use a service discovery mechanism. Let’s look at\nhow that works. \n3.2.4\nUsing service discovery\nSay you’re writing some code that invokes a service that has a REST API. In order to\nmake a request, your code needs to know the network location (IP address and port)\nof a service instance. In a traditional application running on physical hardware, the\nnetwork locations of service instances are usually static. For example, your code could\nread the network locations from a configuration file that’s occasionally updated. But\nin a modern, cloud-based microservices application, it’s usually not that simple. As is\nshown in figure 3.4, a modern application is much more dynamic.\n Service instances have dynamically assigned network locations. Moreover, the set of\nservice instances changes dynamically because of autoscaling, failures, and upgrades.\nConsequently, your client code must use a service discovery.\nAPI\ngateway\nHow to handle each\nunresponsive service?\nUnresponsive\nservice\nMobile\napp\nGet\norder\nendpoint\nGet/orders/xyz\nOrder\nService\nOrder\nService\nproxy\nGET/orders/xyz\nKitchen\nService\nKitchen\nService\nproxy\nGET/tickets?orderId=xyz\nDelivery\nService\nDelivery\nService\nproxy\nGET/deliveries?orderId-xyz\n...\nService\n...\nService\nproxy\nFigure 3.3\nThe API gateway implements the GET /orders/{orderId} endpoint using API \ncomposition. It calls several services, aggregates their responses, and sends a response to the \nmobile app. The code that implements the endpoint must have a strategy for handling the failure \nof each service that it calls.\n \n\n\n81\nCommunicating using the synchronous Remote procedure invocation pattern\nOVERVIEW OF SERVICE DISCOVERY\nAs you’ve just seen, you can’t statically configure a client with the IP addresses of the\nservices. Instead, an application must use a dynamic service discovery mechanism. Ser-\nvice discovery is conceptually quite simple: its key component is a service registry,\nwhich is a database of the network locations of an application’s service instances.\n The service discovery mechanism updates the service registry when service instances\nstart and stop. When a client invokes a service, the service discovery mechanism que-\nries the service registry to obtain a list of available service instances and routes the\nrequest to one of them.\n There are two main ways to implement service discovery:\nThe services and their clients interact directly with the service registry.\nThe deployment infrastructure handles service discovery. (I talk more about\nthat in chapter 12.)\nLet’s look at each option. \nAPPLYING THE APPLICATION-LEVEL SERVICE DISCOVERY PATTERNS\nOne way to implement service discovery is for the application’s services and their cli-\nents to interact with the service registry. Figure 3.5 shows how this works. A service\ninstance registers its network location with the service registry. A service client invokes\na service by first querying the service registry to obtain a list of service instances. It\nthen sends a request to one of those instances.\nService\ninstance 1\nOrder service\n10.232.23.1\n10.232.23.2\n10.232.23.3\nService\ninstance 2\nService\nclient\nService\ninstance 3\n?\nDynamically\nassigned IP\nDynamically created\nand destroyed\nFigure 3.4\nService instances have dynamically assigned IP addresses.\n \n\n\n82\nCHAPTER 3\nInterprocess communication in a microservice architecture\nThis approach to service discovery is a combination of two patterns. The first pat-\ntern is the Self registration pattern. A service instance invokes the service registry’s\nregistration API to register its network location. It may also supply a health check\nURL, described in more detail in chapter 11. The health check URL is an API end-\npoint that the service registry invokes periodically to verify that the service instance\nis healthy and available to handle requests. A service registry may require a service\ninstance to periodically invoke a “heartbeat” API in order to prevent its registration\nfrom expiring.\nThe second pattern is the Client-side discovery pattern. When a service client wants to\ninvoke a service, it queries the service registry to obtain a list of the service’s instances.\nTo improve performance, a client might cache the service instances. The service client\nPattern: Self registration\nA service instance registers itself with the service registry. See http://microser-\nvices.io/patterns/self-registration.html.\nService\ninstance 1\nOrder service\n10.232.23.1\nLoad balance request\n10.232.23.1\n10.232.23.2\n10.232.23.3\n10.232.23.2\nRegister(\"order-service\", \"10.232.23.1\")\nQuery(\"order-service\")\nQuery API\nRegistration API\n10.232.23.3\nService\ninstance 2\nService\ninstance 3\nService\ndiscovery library\nService\nclient\nService\norder-service\norder-service\norder-service\n...\nService registry\nIP address\n10.232.23.1\n10.232.23.2\n10.232.23.3\n...\nRPC/rest\nclient\nService\ndiscovery library\nService\ndiscovery library\nService\ndiscovery library\nClient-side discovery\nSelf registration pattern\nFigure 3.5\nThe service registry keeps track of the service instances. Clients query the service \nregistry to find network locations of available service instances.\n \n\n\n83\nCommunicating using the synchronous Remote procedure invocation pattern\nthen uses a load-balancing algorithm, such as a round-robin or random, to select a ser-\nvice instance. It then makes a request to a select service instance.\nApplication-level service discovery has been popularized by Netflix and Pivotal. Netflix\ndeveloped and open sourced several components: Eureka, a highly available service\nregistry, the Eureka Java client, and Ribbon, a sophisticated HTTP client that supports\nthe Eureka client. Pivotal developed Spring Cloud, a Spring-based framework that\nmakes it remarkably easy to use the Netflix components. Spring Cloud-based services\nautomatically register with Eureka, and Spring Cloud-based clients automatically use\nEureka for service discovery.\n One benefit of application-level service discovery is that it handles the scenario\nwhen services are deployed on multiple deployment platforms. Imagine, for example,\nyou’ve deployed only some of services on Kubernetes, discussed in chapter 12, and the\nrest is running in a legacy environment. Application-level service discovery using\nEureka, for example, works across both environments, whereas Kubernetes-based ser-\nvice discovery only works within Kubernetes.\n One drawback of application-level service discovery is that you need a service dis-\ncovery library for every language—and possibly framework—that you use. Spring\nCloud only helps Spring developers. If you’re using some other Java framework or a\nnon-JVM language such as NodeJS or GoLang, you must find some other service dis-\ncovery framework. Another drawback of application-level service discovery is that\nyou’re responsible for setting up and managing the service registry, which is a distrac-\ntion. As a result, it’s usually better to use a service discovery mechanism that’s pro-\nvided by the deployment infrastructure. \nAPPLYING THE PLATFORM-PROVIDED SERVICE DISCOVERY PATTERNS\nLater in chapter 12 you’ll learn that many modern deployment platforms such as\nDocker and Kubernetes have a built-in service registry and service discovery mecha-\nnism. The deployment platform gives each service a DNS name, a virtual IP (VIP)\naddress, and a DNS name that resolves to the VIP address. A service client makes a\nrequest to the DNS name/VIP, and the deployment platform automatically routes the\nrequest to one of the available service instances. As a result, service registration, ser-\nvice discovery, and request routing are entirely handled by the deployment platform.\nFigure 3.6 shows how this works.\n The deployment platform includes a service registry that tracks the IP addresses of\nthe deployed services. In this example, a client accesses the Order Service using the\nPattern: Client-side discovery\nA service client retrieves the list of available service instances from the service reg-\nistry and load balances across them. See http://microservices.io/patterns/client-\nside-discovery.html.\n \n\n\n84\nCHAPTER 3\nInterprocess communication in a microservice architecture\nDNS name order-service, which resolves to the virtual IP address 10.1.3.4. The\ndeployment platform automatically load balances requests across the three instances\nof the Order Service.\n This approach is a combination of two patterns:\n3rd party registration pattern—Instead of a service registering itself with the ser-\nvice registry, a third party called the registrar, which is typically part of the\ndeployment platform, handles the registration.\nServer-side discovery pattern—Instead of a client querying the service registry, it\nmakes a request to a DNS name, which resolves to a request router that queries\nthe service registry and load balances requests.\nService\norder-service\norder-service\norder-service\n...\nService registry\nIP address\n10.232.23.1\n10.232.23.2\n10.232.23.3\n...\nService\nclient\nGET http://order-service/...\nDeployment platform\nRPC/rest\nclient\nService\ninstance 1\nOrder service\nObserves\n10.232.23.1\n10.232.24.99\nService\ninstance 2\nService\ninstance 3\nPlatform\nrouter\nQueries\nUpdates\n10.232.23.2\n10.232.23.3\nRegistrar\n3rd party registration\nServer-side discovery\nService DNS name\nresolves to service VIP\nService virtual IP address (VIP)\nFigure 3.6\nThe platform is responsible for service registration, discovery, and request routing. Service \ninstances are registered with the service registry by the registrar. Each service has a network location, \na DNS name/virtual IP address. A client makes a request to the service’s network location. The router \nqueries the service registry and load balances requests across the available service instances.\n \n",
      "page_number": 102
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 110-129)",
      "start_page": 110,
      "end_page": 129,
      "detection_method": "topic_boundary",
      "content": "85\nCommunicating using the Asynchronous messaging pattern\nThe key benefit of platform-provided service discovery is that all aspects of service dis-\ncovery are entirely handled by the deployment platform. Neither the services nor the\nclients contain any service discovery code. Consequently, the service discovery mecha-\nnism is readily available to all services and clients regardless of which language or\nframework they’re written in.\n One drawback of platform-provided service discovery is that it only supports the\ndiscovery of services that have been deployed using the platform. For example, as\nmentioned earlier when describing application-level discovery, Kubernetes-based dis-\ncovery only works for services running on Kubernetes. Despite this limitation, I rec-\nommend using platform-provided service discovery whenever possible.\n Now that we’ve looked at synchronous IPC using REST or gRPC, let’s take a look at\nthe alternative: asynchronous, message-based communication. \n3.3\nCommunicating using the Asynchronous messaging \npattern\nWhen using messaging, services communicate by asynchronously exchanging mes-\nsages. A messaging-based application typically uses a message broker, which acts as an\nintermediary between the services, although another option is to use a brokerless\narchitecture, where the services communicate directly with each other. A service client\nmakes a request to a service by sending it a message. If the service instance is expected\nto reply, it will do so by sending a separate message back to the client. Because the\ncommunication is asynchronous, the client doesn’t block waiting for a reply. Instead,\nthe client is written assuming that the reply won’t be received immediately.\nI start this section with an overview of messaging. I show how to describe a messaging\narchitecture independently of messaging technology. Next I compare and contrast\nPattern: 3rd party registration\nService instances are automatically registered with the service registry by a third party.\nSee http://microservices.io/patterns/3rd-party-registration.html.\nPattern: Server-side discovery\nA client makes a request to a router, which is responsible for service discovery. See\nhttp://microservices.io/patterns/server-side-discovery.html.\nPattern: Messaging\nA client invokes a service using asynchronous messaging. See http://microservices\n.io/patterns/communication-style/messaging.html.\n \n\n\n86\nCHAPTER 3\nInterprocess communication in a microservice architecture\nbrokerless and broker-based architectures and describe the criteria for selecting a\nmessage broker. I then discuss several important topics, including scaling consum-\ners while preserving message ordering, detecting and discarding duplicate messages,\nand sending and receiving messages as part of a database transaction. Let’s begin by\nlooking at how messaging works.\n3.3.1\nOverview of messaging\nA useful model of messaging is defined in the book Enterprise Integration Patterns\n(Addison-Wesley Professional, 2003) by Gregor Hohpe and Bobby Woolf. In this\nmodel, messages are exchanged over message channels. A sender (an application or\nservice) writes a message to a channel, and a receiver (an application or service) reads\nmessages from a channel. Let’s look at messages and then look at channels.\nABOUT MESSAGES\nA message consists of a header and a message body (www.enterpriseintegrationpatterns\n.com/Message.html). The header is a collection of name-value pairs, metadata that\ndescribes the data being sent. In addition to name-value pairs provided by the mes-\nsage’s sender, the message header contains name-value pairs, such as a unique message\nid generated by either the sender or the messaging infrastructure, and an optional\nreturn address, which specifies the message channel that a reply should be written to.\nThe message body is the data being sent, in either text or binary format.\n There are several different kinds of messages:\nDocument—A generic message that contains only data. The receiver decides how\nto interpret it. The reply to a command is an example of a document message.\nCommand—A message that’s the equivalent of an RPC request. It specifies the\noperation to invoke and its parameters.\nEvent—A message indicating that something notable has occurred in the sender.\nAn event is often a domain event, which represents a state change of a domain\nobject such as an Order, or a Customer.\nThe approach to the microservice architecture described in this book uses commands\nand events extensively.\n Let’s now look at channels, the mechanism by which services communicate. \nABOUT MESSAGE CHANNELS\nAs figure 3.7 shows, messages are exchanged over channels (www.enterpriseintegra-\ntionpatterns.com/MessageChannel.html). The business logic in the sender invokes a\nsending port interface, which encapsulates the underlying communication mechanism.\nThe sending port is implemented by a message sender adapter class, which sends a mes-\nsage to a receiver via a message channel. A message channel is an abstraction of the\nmessaging infrastructure. A message handler adapter class in the receiver is invoked to\nhandle the message. It invokes a receiving port interface implemented by the consumer’s\n \n\n\n87\nCommunicating using the Asynchronous messaging pattern\nbusiness logic. Any number of senders can send messages to a channel. Similarly, any\nnumber of receivers can receive messages from a channel.\n There are two kinds of channels: point-to-point (www.enterpriseintegrationpatterns\n.com/PointToPointChannel.html) and publish-subscribe (www.enterpriseintegration-\npatterns.com/PublishSubscribeChannel.html):\nA point-to-point channel delivers a message to exactly one of the consumers that\nis reading from the channel. Services use point-to-point channels for the one-\nto-one interaction styles described earlier. For example, a command message is\noften sent over a point-to-point channel.\nA publish-subscribe channel delivers each message to all of the attached consum-\ners. Services use publish-subscribe channels for the one-to-many interaction\nstyles described earlier. For example, an event message is usually sent over a\npublish-subscribe channel. \n3.3.2\nImplementing the interaction styles using messaging\nOne of the valuable features of messaging is that it’s flexible enough to support all the\ninteraction styles described in section 3.1.1. Some interaction styles are directly imple-\nmented by messaging. Others must be implemented on top of messaging.\n Let’s look at how to implement each interaction style, starting with request/response\nand asynchronous request/response.\nIMPLEMENTING REQUEST/RESPONSE AND ASYNCHRONOUS REQUEST/RESPONSE\nWhen a client and service interact using either request/response or asynchronous\nrequest/response, the client sends a request and the service sends back a reply. The\nBusiness\nlogic\ninvokes\ninvokes\nBusiness logic\nSending port\nReceiving port\nSender\nReceiver\nMessage\nsender\nMessage\nMessage\nchannel\nReceives\nSends\nHeader\nBody\nMessaging\ninfrastructure\nMessage\nhandler\nService\nFigure 3.7\nThe business logic in the sender invokes a sending port interface, which is implemented by a  message \nsender adapter. The message sender sends a message to a receiver via a message channel. The message channel \nis an abstraction of messaging infrastructure. A message handler adapter in the receiver is invoked to handle the \nmessage. It invokes the receiving port interface implemented by the receiver’s business logic.\n \n\n\n88\nCHAPTER 3\nInterprocess communication in a microservice architecture\ndifference between the two interaction styles is that with request/response the client\nexpects the service to respond immediately, whereas with asynchronous request/\nresponse there is no such expectation. Messaging is inherently asynchronous, so only\nprovides asynchronous request/response. But a client could block until a reply is\nreceived.\n The client and service implement the asynchronous request/response style inter-\naction by exchanging a pair of messages. As figure 3.8 shows, the client sends a com-\nmand message, which specifies the operation to perform, and parameters, to a point-\nto-point messaging channel owned by a service. The service processes the requests\nand sends a reply message, which contains the outcome, to a point-to-point channel\nowned by the client.\nThe client must tell the service where to send a reply message and must match reply mes-\nsages to requests. Fortunately, solving these two problems isn’t that difficult. The client\nsends a command message that has a reply channel header. The server writes the reply mes-\nsage, which contains a correlation id that has the same value as message identifier, to the reply\nchannel. The client uses the correlation id to match the reply message with the request.\n Because the client and service communicate using messaging, the interaction is\ninherently asynchronous. In theory, a messaging client could block until it receives a\nreply, but in practice the client will process replies asynchronously. What’s more,\nreplies are typically processed by any one of the client’s instances. \nRequest\nSends\nReads\nReads\nSends\nMessageId: msgId\nReturnAddress: ReplyChannel\nBody\nCorrelationId:msgId\nBody\nRequest channel\nReply channel\nReply\nSpeciﬁes\nClient\nService\nClient sends message containing\nmsgId and a reply channel.\nService sends reply to the speciﬁed reply\nchannel. The reply contains a correlationId,\nwhich is the request’s msgId.\nFigure 3.8\nImplementing asynchronous request/response by including a reply channel and message \nidentifier in the request message. The receiver processes the message and sends the reply to the \nspecified reply channel.\n \n\n\n89\nCommunicating using the Asynchronous messaging pattern\nIMPLEMENTING ONE-WAY NOTIFICATIONS\nImplementing one-way notifications is straightforward using asynchronous messaging.\nThe client sends a message, typically a command message, to a point-to-point channel\nowned by the service. The service subscribes to the channel and processes the mes-\nsage. It doesn’t send back a reply. \nIMPLEMENTING PUBLISH/SUBSCRIBE\nMessaging has built-in support for the publish/subscribe style of interaction. A client\npublishes a message to a publish-subscribe channel that is read by multiple consum-\ners. As described in chapters 4 and 5, services use publish/subscribe to publish\ndomain events, which represent changes to domain objects. The service that publishes\nthe domain events owns a publish-subscribe channel, whose name is derived from the\ndomain class. For example, the Order Service publishes Order events to an Order\nchannel, and the Delivery Service publishes Delivery events to a Delivery chan-\nnel. A service that’s interested in a particular domain object’s events only has to sub-\nscribe to the appropriate channel. \nIMPLEMENTING PUBLISH/ASYNC RESPONSES\nThe publish/async responses interaction style is a higher-level style of interaction that’s\nimplemented by combining elements of publish/subscribe and request/response. A cli-\nent publishes a message that specifies a reply channel header to a publish-subscribe\nchannel. A consumer writes a reply message containing a correlation id to the reply\nchannel. The client gathers the responses by using the correlation id to match the reply\nmessages with the request.\n Each service in your application that has an asynchronous API will use one or\nmore of these implementation techniques. A service that has an asynchronous API for\ninvoking operations will have a message channel for requests. Similarly, a service that\npublishes events will publish them to an event message channel.\n As described in section 3.1.2, it’s important to write an API specification for a ser-\nvice. Let’s look at how to do that for an asynchronous API. \n3.3.3\nCreating an API specification for a messaging-based service API\nThe specification for a service’s asynchronous API must, as figure 3.9 shows, specify\nthe names of the message channels, the message types that are exchanged over each\nchannel, and their formats. You must also describe the format of the messages using a\nstandard such as JSON, XML, or Protobuf. But unlike with REST and Open API, there\nisn’t a widely adopted standard for documenting the channels and the message types.\nInstead, you need to write an informal document.\n A service’s asynchronous API consists of operations, invoked by clients, and events,\npublished by the services. They’re documented in different ways. Let’s take a look at\neach one, starting with operations.\n \n\n\n90\nCHAPTER 3\nInterprocess communication in a microservice architecture\nDOCUMENTING ASYNCHRONOUS OPERATIONS\nA service’s operations can be invoked using one of two different interaction styles:\nRequest/async response-style API—This consists of the service’s command message\nchannel, the types and formats of the command message types that the service\naccepts, and the types and formats of the reply messages sent by the service.\nOne-way notification-style API—This consists of the service’s command message\nchannel and the types and format of the command message types that the ser-\nvice accepts.\nA service may use the same request channel for both asynchronous request/response\nand one-way notification. \nDOCUMENTING PUBLISHED EVENTS\nA service can also publish events using a publish/subscribe interaction style. The spec-\nification of this style of API consists of the event channel and the types and formats of\nthe event messages that are published by the service to the channel.\n The messages and channels model of messaging is a great abstraction and a good\nway to design a service’s asynchronous API. But in order to implement a service you\nneed to choose a messaging technology and determine how to implement your design\nusing its capabilities. Let’s take a look at what’s involved. \n3.3.4\nUsing a message broker\nA messaging-based application typically uses a message broker, an infrastructure service\nthrough which the service communicates. But a broker-based architecture isn’t the\nonly messaging architecture. You can also use a brokerless-based messaging architec-\nture, in which the services communicate with one another directly. The two approaches,\nshown in figure 3.10, have different trade-offs, but usually a broker-based architecture\nis a better approach.\nService\nCommand\nquery\nAPI\nService API\nReplies\nR\nR\nEvents\nR\nEvent\npublisher\n«Command channel»\n«Event channel»\n«Reply channel»\nCommands\nC\nC\nC\nFigure 3.9\nA service’s asynchronous API consists of message channels and command, reply, and \nevent message types.\n \n\n\n91\nCommunicating using the Asynchronous messaging pattern\nThis book focuses on broker-based architecture, but it’s worthwhile to take a quick look\nat the brokerless architecture, because there may be scenarios where you find it useful.\nBROKERLESS MESSAGING\nIn a brokerless architecture, services can exchange messages directly. ZeroMQ (http://\nzeromq.org) is a popular brokerless messaging technology. It’s both a specification\nand a set of libraries for different languages. It supports a variety of transports, includ-\ning TCP, UNIX-style domain sockets, and multicast.\n The brokerless architecture has some benefits:\nAllows lighter network traffic and better latency, because messages go directly\nfrom the sender to the receiver, instead of having to go from the sender to the\nmessage broker and from there to the receiver\nEliminates the possibility of the message broker being a performance bottle-\nneck or a single point of failure\nFeatures less operational complexity, because there is no message broker to set\nup and maintain\nAs appealing as these benefits may seem, brokerless messaging has significant drawbacks:\nServices need to know about each other’s locations and must therefore use one\nof the discovery mechanisms describer earlier in section 3.2.4.\nIt offers reduced availability, because both the sender and receiver of a message\nmust be available while the message is being exchanged.\nImplementing mechanisms, such as guaranteed delivery, is more challenging.\nService\nService\nService\nService\nService\nService\nMessage broker\nVs.\nBrokerless architecture\nBroker-based architecture\nFigure 3.10\nThe services in brokerless architecture communicate directly, whereas the services \nin a broker-based architecture communicate via a message broker.\n \n\n\n92\nCHAPTER 3\nInterprocess communication in a microservice architecture\nIn fact, some of these drawbacks, such as reduced availability and the need for service\ndiscovery, are the same as when using synchronous, response/response.\n Because of these limitations, most enterprise applications use a message broker-\nbased architecture. Let’s look at how that works. \nOVERVIEW OF BROKER-BASED MESSAGING\nA message broker is an intermediary through which all messages flow. A sender writes\nthe message to the message broker, and the message broker delivers it to the receiver.\nAn important benefit of using a message broker is that the sender doesn’t need to\nknow the network location of the consumer. Another benefit is that a message broker\nbuffers messages until the consumer is able to process them.\n There are many message brokers to chose from. Examples of popular open source\nmessage brokers include the following:\nActiveMQ (http://activemq.apache.org)\nRabbitMQ (https://www.rabbitmq.com)\nApache Kafka (http://kafka.apache.org)\nThere are also cloud-based messaging services, such as AWS Kinesis (https://aws.amazon\n.com/kinesis/) and AWS SQS (https://aws.amazon.com/sqs/).\n When selecting a message broker, you have various factors to consider, including\nthe following:\nSupported programming languages—You probably should pick one that supports a\nvariety of programming languages.\nSupported messaging standards—Does the message broker support any standards,\nsuch as AMQP and STOMP, or is it proprietary?\nMessaging ordering—Does the message broker preserve ordering of messages?\nDelivery guarantees—What kind of delivery guarantees does the broker make?\nPersistence—Are messages persisted to disk and able to survive broker crashes?\nDurability—If a consumer reconnects to the message broker, will it receive the\nmessages that were sent while it was disconnected?\nScalability—How scalable is the message broker?\nLatency—What is the end-to-end latency?\nCompeting consumers—Does the message broker support competing consumers?\nEach broker makes different trade-offs. For example, a very low-latency broker might\nnot preserve ordering, make no guarantees to deliver messages, and only store mes-\nsages in memory. A messaging broker that guarantees delivery and reliably stores\nmessages on disk will probably have higher latency. Which kind of message broker is\nthe best fit depends on your application’s requirements. It’s even possible that differ-\nent parts of your application will have different messaging requirements.\n It’s likely, though, that messaging ordering and scalability are essential. Let’s now\nlook at how to implement message channels using a message broker. \n \n\n\n93\nCommunicating using the Asynchronous messaging pattern\nIMPLEMENTING MESSAGE CHANNELS USING A MESSAGE BROKER\nEach message broker implements the message channel concept in a different way. As\ntable 3.2 shows, JMS message brokers such as ActiveMQ have queues and topics.\nAMQP-based message brokers such as RabbitMQ have exchanges and queues. Apache\nKafka has topics, AWS Kinesis has streams, and AWS SQS has queues. What’s more,\nsome message brokers offer more flexible messaging than the message and channels\nabstraction described in this chapter.\nAlmost all the message brokers described here support both point-to-point and publish-\nsubscribe channels. The one exception is AWS SQS, which only supports point-to-point\nchannels.\n Now let’s look at the benefits and drawbacks of broker-based messaging. \nBENEFITS AND DRAWBACKS OF BROKER-BASED MESSAGING\nThere are many advantages to using broker-based messaging:\nLoose coupling—A client makes a request by simply sending a message to the\nappropriate channel. The client is completely unaware of the service instances.\nIt doesn’t need to use a discovery mechanism to determine the location of a ser-\nvice instance.\nMessage buffering—The message broker buffers messages until they can be pro-\ncessed. With a synchronous request/response protocol such as HTTP, both the\nclient and service must be available for the duration of the exchange. With mes-\nsaging, though, messages will queue up until they can be processed by the con-\nsumer. This means, for example, that an online store can accept orders from\ncustomers even when the order-fulfillment system is slow or unavailable. The\nmessages will simply queue up until they can be processed.\nFlexible communication—Messaging supports all the interaction styles described\nearlier.\nExplicit interprocess communication—RPC-based mechanism attempts to make invok-\ning a remote service look the same as calling a local service. But due to the laws\nof physics and the possibility of partial failure, they’re in fact quite different.\nTable 3.2\nEach message broker implements the message channel concept in a different way.\nMessage broker\nPoint-to-point channel\nPublish-subscribe channel\nJMS\nQueue\nTopic\nApache Kafka\nTopic\nTopic\nAMQP-based brokers, such as \nRabbitMQ\nExchange + Queue\nFanout exchange and a queue per \nconsumer\nAWS Kinesis\nStream\nStream\nAWS SQS\nQueue\n—\n \n\n\n94\nCHAPTER 3\nInterprocess communication in a microservice architecture\nMessaging makes these differences very explicit, so developers aren’t lulled into\na false sense of security.\nThere are some downsides to using messaging:\nPotential performance bottleneck—There is a risk that the message broker could be\na performance bottleneck. Fortunately, many modern message brokers are\ndesigned to be highly scalable.\nPotential single point of failure—It’s essential that the message broker is highly\navailable—otherwise, system reliability will be impacted. Fortunately, most mod-\nern brokers have been designed to be highly available.\nAdditional operational complexity—The messaging system is yet another system\ncomponent that must be installed, configured, and operated.\nLet’s look at some design issues you might face. \n3.3.5\nCompeting receivers and message ordering\nOne challenge is how to scale out message receivers while preserving message order-\ning. It’s a common requirement to have multiple instances of a service in order to pro-\ncess messages concurrently. Moreover, even a single service instance will probably use\nthreads to concurrently process multiple messages. Using multiple threads and service\ninstances to concurrently process messages increases the throughput of the applica-\ntion. But the challenge with processing messages concurrently is ensuring that each\nmessage is processed once and in order.\n For example, imagine that there are three instances of a service reading from the\nsame point-to-point channel and that a sender publishes Order Created, Order Updated,\nand Order Cancelled event messages sequentially. A simplistic messaging implementa-\ntion could concurrently deliver each message to a different receiver. Because of delays\ndue to network issues or garbage collections, messages might be processed out of order,\nwhich would result in strange behavior. In theory, a service instance might process the\nOrder Cancelled message before another service processes the Order Created message!\n A common solution, used by modern message brokers like Apache Kafka and AWS\nKinesis, is to use sharded (partitioned) channels. Figure 3.11 shows how this works.\nThere are three parts to the solution:\n1\nA sharded channel consists of two or more shards, each of which behaves like\na channel.\n2\nThe sender specifies a shard key in the message’s header, which is typically an\narbitrary string or sequence of bytes. The message broker uses a shard key to\nassign the message to a particular shard/partition. It might, for example, select\nthe shard by computing the hash of the shard key modulo the number of shards.\n3\nThe messaging broker groups together multiple instances of a receiver and\ntreats them as the same logical receiver. Apache Kafka, for example, uses the\nterm consumer group. The message broker assigns each shard to a single receiver.\nIt reassigns shards when receivers start up and shut down.\n \n\n\n95\nCommunicating using the Asynchronous messaging pattern\nIn this example, each Order event message has the orderId as its shard key. Each event\nfor a particular order is published to the same shard, which is read by a single consumer\ninstance. As a result, these messages are guaranteed to be processed in order. \n3.3.6\nHandling duplicate messages\nAnother challenge you must tackle when using messaging is dealing with duplicate\nmessages. A message broker should ideally deliver each message only once, but guar-\nanteeing exactly-once messaging is usually too costly. Instead, most message brokers\npromise to deliver a message at least once.\n When the system is working normally, a message broker that guarantees at-least-\nonce delivery will deliver each message only once. But a failure of a client, network, or\nmessage broker can result in a message being delivered multiple times. Say a client\ncrashes after processing a message and updating its database—but before acknowledg-\ning the message. The message broker will deliver the unacknowledged message again,\neither to that client when it restarts or to another replica of the client.\n Ideally, you should use a message broker that preserves ordering when redeliver-\ning messages. Imagine that the client processes an Order Created event followed by\nan Order Cancelled event for the same Order, and that somehow the Order Created\nevent wasn’t acknowledged. The message broker should redeliver both the Order Cre-\nated and Order Cancelled events. If it only redelivers the Order Created, the client\nmay undo the cancelling of the Order.\n There are a couple of different ways to handle duplicate messages:\nWrite idempotent message handlers.\nTrack messages and discard duplicates.\nLet’s look at each option.\nRoutes based on a\nhash of the shard-key\nReceiver A\ninstance 1\nReceiver A\ninstance 2\nReceiver\nShard\nassignment\nReceiver\n...\nRouter\nShard 0\nChannel\nLogical receiver A\nShard 1\nShard ...\nCreate order\nrequest\nShard-key:orderId\nSender\nFigure 3.11\nScaling consumers while preserving message ordering by using a sharded (partitioned) message \nchannel. The sender includes the shard key in the message. The message broker writes the message to a shard \ndetermined by the shard key. The message broker assigns each partition to an instance of the replicated receiver.\n \n\n\n96\nCHAPTER 3\nInterprocess communication in a microservice architecture\nWRITING IDEMPOTENT MESSAGE HANDLERS\nIf the application logic that processes messages is idempotent, then duplicate mes-\nsages are harmless. Application logic is idempotent if calling it multiple times with the\nsame input values has no additional effect. For instance, cancelling an already-cancelled\norder is an idempotent operation. So is creating an order with a client-supplied ID.\nAn idempotent message handler can be safely executed multiple times, provided that\nthe message broker preserves ordering when redelivering messages.\n Unfortunately, application logic is often not idempotent. Or you may be using a\nmessage broker that doesn’t preserve ordering when redelivering messages. Duplicate\nor out-of-order messages can cause bugs. In this situation, you must write message\nhandlers that track messages and discard duplicate messages. \nTRACKING MESSAGES AND DISCARDING DUPLICATES\nConsider, for example, a message handler that authorizes a consumer credit card. It\nmust authorize the card exactly once for each order. This example of application logic\nhas a different effect each time it’s invoked. If duplicate messages caused the message\nhandler to execute this logic multiple times, the application would behave incorrectly.\nThe message handler that executes this kind of application logic must become idem-\npotent by detecting and discarding duplicate messages.\n A simple solution is for a message consumer to track the messages that it has pro-\ncessed using the message id and discard any duplicates. It could, for example, store\nthe message id of each message that it consumed in a database table. Figure 3.12\nshows how to do this using a dedicated table.\nWhen a consumer handles a message, it records the message id in the database table as\npart of the transaction that creates and updates business entities. In this example, the\nconsumer inserts a row containing the message id into a PROCESSED_MESSAGES table. If a\nmessage is a duplicate, the INSERT will fail and the consumer can discard the message.\nMSG_ID\nPROCESSED_MESSAGE table\nINSERT\nINSERT will fail for\nduplicate messages.\nUPDATE\n...\n...\nApplication table\nxyz\nTransaction\nMessage\nid: xyz\nConsumer\nFigure 3.12\nA consumer detects and discards duplicate messages by recording the IDs of \nprocessed messages in a database table. If a message has been processed before, the INSERT \ninto the PROCESSED_MESSAGES table will fail.\n \n\n\n97\nCommunicating using the Asynchronous messaging pattern\n Another option is for a  message handler to record message ids in an application\ntable instead of a dedicated table. This approach is particularly useful when using a\nNoSQL database that has a limited transaction model, so it doesn’t support updat-\ning two tables as part of a database transaction. Chapter 7 shows an example of this\napproach. \n3.3.7\nTransactional messaging\nA service often needs to publish messages as part of a transaction that updates the\ndatabase. For instance, throughout this book you see examples of services that publish\ndomain events whenever they create or update business entities. Both the database\nupdate and the sending of the message must happen within a transaction. Otherwise,\na service might update the database and then crash, for example, before sending the\nmessage. If the service doesn’t perform these two operations atomically, a failure\ncould leave the system in an inconsistent state.\n The traditional solution is to use a distributed transaction that spans the database\nand the message broker. But as you’ll learn in chapter 4, distributed transactions\naren’t a good choice for modern applications. Moreover, many modern brokers such\nas Apache Kafka don’t support distributed transactions.\n As a result, an application must use a different mechanism to reliably publish mes-\nsages. Let’s look at how that works.\nUSING A DATABASE TABLE AS A MESSAGE QUEUE\nLet’s imagine that your application is using a relational database. A straightforward\nway to reliably publish messages is to apply the Transactional outbox pattern. This\npattern uses a database table as a temporary message queue. As figure 3.13 shows, a\nservice that sends messages has an OUTBOX database table. As part of the database\nOrder\nService\nRead\nOUTBOX\ntable\nPublish\n...\n...\nORDER table\nINSERT,\nUPDATE,DELETE\nINSERT\nDatabase\nMessage\nrelay\nTransaction\nOUTBOX table\nMessage\nbroker\nFigure 3.13\nA service reliably publishes a message by inserting it into an OUTBOX table as part of the transaction \nthat updates the database. The Message Relay reads the OUTBOX table and publishes the messages to a \nmessage broker.\n \n\n\n98\nCHAPTER 3\nInterprocess communication in a microservice architecture\ntransaction that creates, updates, and deletes business objects, the service sends mes-\nsages by inserting them into the OUTBOX table. Atomicity is guaranteed because this is a\nlocal ACID transaction.\n The OUTBOX table acts a temporary message queue. The MessageRelay is a compo-\nnent that reads the OUTBOX table and publishes the messages to a message broker.\nYou can use a similar approach with some NoSQL databases. Each business entity\nstored as a record in the database has an attribute that is a list of messages that need\nto be published. When a service updates an entity in the database, it appends a mes-\nsage to that list. This is atomic because it’s done with a single database operation. The\nchallenge, though, is efficiently finding those business entities that have events and\npublishing them.\n There are a couple of different ways to move messages from the database to the\nmessage broker. We’ll look at each one. \nPUBLISHING EVENTS BY USING THE POLLING PUBLISHER PATTERN\nIf the application uses a relational database, a very simple way to publish the messages\ninserted into the OUTBOX table is for the MessageRelay to poll the table for unpub-\nlished messages. It periodically queries the table:\nSELECT * FROM OUTBOX ORDERED BY ... ASC\nNext, the MessageRelay publishes those messages to the message broker, sending one\nto its destination message channel. Finally, it deletes those messages from the OUTBOX\ntable:\nBEGIN\nDELETE FROM OUTBOX WHERE ID in (....)\nCOMMIT\nPolling the database is a simple approach that works reasonably well at low scale. The\ndownside is that frequently polling the database can be expensive. Also, whether you\ncan use this approach with a NoSQL database depends on its querying capabilities.\nThat’s because rather than querying an OUTBOX table, the application must query the\nPattern: Transactional outbox\nPublish an event or message as part of a database transaction by saving it in an OUT-\nBOX in the database. See http://microservices.io/patterns/data/transactional-out-\nbox.html.\nPattern: Polling publisher\nPublish messages by polling the outbox in the database. See http://microser-\nvices.io/patterns/data/polling-publisher.html.\n \n\n\n99\nCommunicating using the Asynchronous messaging pattern\nbusiness entities, and that may or may not be possible to do efficiently. Because of\nthese drawbacks and limitations, it’s often better—and in some cases, necessary—to\nuse the more sophisticated and performant approach of tailing the database transac-\ntion log. \nPUBLISHING EVENTS BY APPLYING THE TRANSACTION LOG TAILING PATTERN\nA sophisticated solution is for MessageRelay to tail the database transaction log (also\ncalled the commit log). Every committed update made by an application is repre-\nsented as an entry in the database’s transaction log. A transaction log miner can read\nthe transaction log and publish each change as a message to the message broker. Fig-\nure 3.14 shows how this approach works.\nThe Transaction Log Miner reads the transaction log entries. It converts each relevant\nlog entry corresponding to an inserted message into a message and publishes that mes-\nsage to the message broker. This approach can be used to publish messages written to\nan OUTBOX table in an RDBMS or messages appended to records in a NoSQL database.\nPattern: Transaction log tailing\nPublish changes made to the database by tailing the transaction log. See http://micro-\nservices.io/patterns/data/transaction-log-tailing.html.\nDatabase\nOUTBOX table\nTransaction log\nTransaction log\nminer\nINSERT INTO OUTBOX ...\nMessage\nbroker\nChanges\nPublish\nOrder\nService\nCommitted inserts into\nthe OUTBOX table are\nrecorded in the database’s\ntransaction log.\nReads the transaction log\nFigure 3.14\nA service publishes messages inserted into the OUTBOX table by mining \nthe database’s transaction log.\n \n\n\n100\nCHAPTER 3\nInterprocess communication in a microservice architecture\nThere are a few examples of this approach in use:\nDebezium (http://debezium.io)—An open source project that publishes data-\nbase changes to the Apache Kafka message broker.\nLinkedIn Databus (https://github.com/linkedin/databus)—An open source proj-\nect that mines the Oracle transaction log and publishes the changes as events.\nLinkedIn uses Databus to synchronize various derived data stores with the sys-\ntem of record.\nDynamoDB streams (http://docs.aws.amazon.com/amazondynamodb/latest/\ndeveloperguide/Streams.html)—DynamoDB streams contain the time-ordered\nsequence of changes (creates, updates, and deletes) made to the items in a\nDynamoDB table in the last 24 hours. An application can read those changes\nfrom the stream and, for example, publish them as events.\nEventuate Tram (https://github.com/eventuate-tram/eventuate-tram-core)—Your\nauthor’s very own open source transaction messaging library that uses MySQL\nbinlog protocol, Postgres WAL, or polling to read changes made to an OUTBOX\ntable and publish them to Apache Kafka.\nAlthough this approach is obscure, it works remarkably well. The challenge is that\nimplementing it requires some development effort. You could, for example, write low-\nlevel code that calls database-specific APIs. Alternatively, you could use an open source\nframework such as Debezium that publishes changes made by an application to MySQL,\nPostgres, or MongoDB to Apache Kafka. The drawback of using Debezium is that its\nfocus is capturing changes at the database level and that APIs for sending and receiving\nmessages are outside of its scope. That’s why I created the Eventuate Tram framework,\nwhich provides the messaging APIs as well as transaction tailing and polling. \n3.3.8\nLibraries and frameworks for messaging\nA service needs to use a library to send and receive messages. One approach is to use\nthe message broker’s client library, although there are several problems with using\nsuch a library directly:\nThe client library couples business logic that publishes messages to the message\nbroker APIs.\nA message broker’s client library is typically low level and requires many lines of\ncode to send or receive a message. As a developer, you don’t want to repeatedly\nwrite boilerplate code. Also, as the author of this book I don’t want the example\ncode cluttered with low-level boilerplate.\nThe client library usually provides only the basic mechanism to send and\nreceive messages and doesn’t support the higher-level interaction styles.\nA better approach is to use a higher-level library or framework that hides the low-level\ndetails and directly supports the higher-level interaction styles. For simplicity, the\nexamples in this book use my Eventuate Tram framework. It has a simple, easy-to-\nunderstand API that hides the complexity of using the message broker. Besides an API\n \n\n\n101\nCommunicating using the Asynchronous messaging pattern\nfor sending and receiving messages, Eventuate Tram also supports higher-level inter-\naction styles such as asynchronous request/response and domain event publishing.\nEventuate Tram also implements two important mechanisms:\nTransactional messaging—It publishes messages as part of a database transaction.\nDuplicate message detection—The Eventuate Tram message consumer detects and\ndiscards duplicate messages, which is essential for ensuring that a consumer\nprocesses messages exactly once, as discussed in section 3.3.6.\nLet’s take a look at the Eventuate Tram APIs.\nBASIC MESSAGING\nThe basic messaging API consists of two Java interfaces: MessageProducer and Message-\nConsumer. A producer service uses the MessageProducer interface to publish messages\nto message channels. Here’s an example of using this interface:\nMessageProducer messageProducer = ...;\nString channel = ...;\nString payload = ...;\nmessageProducer.send(destination, MessageBuilder.withPayload(payload).build())\nA consumer service uses the MessageConsumer interface to subscribe to messages:\nMessageConsumer messageConsumer;\nmessageConsumer.subscribe(subscriberId, Collections.singleton(destination), \nmessage -> { ... })\nMessageProducer and MessageConsumer are the foundation of the higher-level APIs\nfor asynchronous request/response and domain event publishing.\n Let’s talk about how to publish and subscribe to events. \nWhat!? Why the Eventuate frameworks?\nThe code samples in this book use the open source Eventuate frameworks I’ve devel-\noped for transactional messaging, event sourcing, and sagas. I chose to use my\nframeworks because, unlike with, say, dependency injection and the Spring frame-\nwork, there are no widely adopted frameworks for many of the features the microser-\nvice architecture requires. Without the Eventuate Tram framework, many examples\nwould have to use the low-level messaging APIs directly, making them much more\ncomplicated and obscuring important concepts. Or they would use a framework that\nisn’t widely adopted, which would also provoke criticism.\nInstead, the examples use the Eventuate Tram frameworks, which have a simple,\neasy-to-understand API that hides the implementation details. You can use these\nframeworks in your applications. Alternatively, you can study the Eventuate Tram\nframeworks and reimplement the concepts yourself.\n \n\n\n102\nCHAPTER 3\nInterprocess communication in a microservice architecture\nDOMAIN EVENT PUBLISHING\nEventuate Tram has APIs for publishing and consuming domain events. Chapter 5\nexplains that domain events are events that are emitted by an aggregate (business\nobject) when it’s created, updated, or deleted. A service publishes a domain event\nusing the DomainEventPublisher interface. Here is an example:\nDomainEventPublisher domainEventPublisher;\nString accountId = ...;\nDomainEvent domainEvent = new AccountDebited(...);\ndomainEventPublisher.publish(\"Account\", accountId, Collections.singletonList(\ndomainEvent));\nA service consumes domain events using the DomainEventDispatcher. An example\nfollows:\nDomainEventHandlers domainEventHandlers = DomainEventHandlersBuilder\n.forAggregateType(\"Order\")\n.onEvent(AccountDebited.class, domainEvent -> { ... })\n.build();\nnew DomainEventDispatcher(\"eventDispatcherId\",\ndomainEventHandlers,\nmessageConsumer);\nEvents aren’t the only high-level messaging pattern supported by Eventuate Tram. It\nalso supports command/reply-based messaging. \nCOMMAND/REPLY-BASED MESSAGING\nA client can send a command message to a service using the CommandProducer inter-\nface. For example\nCommandProducer commandProducer = ...;\nMap<String, String> extraMessageHeaders = Collections.emptyMap();\nString commandId = commandProducer.send(\"CustomerCommandChannel\",\nnew DoSomethingCommand(),\n\"ReplyToChannel\",\nextraMessageHeaders);\nA service consumes command messages using the CommandDispatcher class. Command-\nDispatcher uses the MessageConsumer interface to subscribe to specified events. It dis-\npatches each command message to the appropriate handler method. Here’s an example:\nCommandHandlers commandHandlers =CommandHandlersBuilder\n.fromChannel(commandChannel)\n.onMessage(DoSomethingCommand.class, (command) -\n> { ... ; return withSuccess(); })\n.build();\n \n\n\n103\nUsing asynchronous messaging to improve availability\nCommandDispatcher dispatcher = new CommandDispatcher(\"subscribeId\", \ncommandHandlers, messageConsumer, messageProducer);\nThroughout this book, you’ll see code examples that use these APIs for sending and\nreceiving messages.\n As you’ve seen, the Eventuate Tram framework implements transactional messag-\ning for Java applications. It provides a low-level API for sending and receiving messages\ntransactionally. It also provides the higher-level APIs for publishing and consuming\ndomain events and for sending and processing commands.\n Let’s now look at a service design approach that uses asynchronous messaging to\nimprove availability. \n3.4\nUsing asynchronous messaging to improve availability\nAs you’ve seen, a variety of IPC mechanisms have different trade-offs. One particular\ntrade-off is how your choice of IPC mechanism impacts availability. In this section,\nyou’ll learn that synchronous communication with other services as part of request\nhandling reduces application availability. As a result, you should design your services\nto use asynchronous messaging whenever possible.\n Let’s first look at the problem with synchronous communication and how it\nimpacts availability.\n3.4.1\nSynchronous communication reduces availability\nREST is an extremely popular IPC mechanism. You may be tempted to use it for inter-\nservice communication. The problem with REST, though, is that it’s a synchronous\nprotocol: an HTTP client must wait for the service to send a response. Whenever\nservices communicate using a synchronous protocol, the availability of the applica-\ntion is reduced.\n To see why, consider the scenario shown in figure 3.15. The Order Service has a\nREST API for creating an Order. It invokes the Consumer Service and the Restaurant\nService to validate the Order. Both of those services also have REST APIs.\nClient\nOrder\nService\nConsumer\nService\nRestaurant\nService\nPOST/orders\nGET/consumers/id\nGET/restaurant/id\nFigure 3.15\nThe Order Service invokes other services using REST. It’s straightforward, but it \nrequires all the services to be simultaneously available, which reduces the availability of the API.\n \n\n\n104\nCHAPTER 3\nInterprocess communication in a microservice architecture\nThe sequence of steps for creating an order is as follows:\n1\nClient makes an HTTP POST /orders request to the Order Service.\n2\nOrder Service retrieves consumer information by making an HTTP GET\n/consumers/id request to the Consumer Service.\n3\nOrder Service retrieves restaurant information by making an HTTP GET\n/restaurant/id request to the Restaurant Service.\n4\nOrder Taking validates the request using the consumer and restaurant infor-\nmation.\n5\nOrder Taking creates an Order.\n6\nOrder Taking sends an HTTP response to the client.\nBecause these services use HTTP, they must all be simultaneously available in order\nfor the FTGO application to process the CreateOrder request. The FTGO application\ncouldn’t create orders if any one of these three services is down. Mathematically\nspeaking, the availability of a system operation is the product of the availability of the\nservices that are invoked by that operation. If the Order Service and the two services\nthat it invokes are 99.5% available, the overall availability is 99.5%3 = 98.5%, which is\nsignificantly less. Each additional service that participates in handling a request fur-\nther reduces availability.\n This problem isn’t specific to REST-based communication. Availability is reduced\nwhenever a service can only respond to its client after receiving a response from\nanother service. This problem exists even if services communicate using request/\nresponse style interaction over asynchronous messaging. For example, the availability\nof the Order Service would be reduced if it sent a message to the Consumer Service\nvia a message broker and then waited for a response.\n If you want to maximize availability, you must minimize the amount of synchro-\nnous communication. Let’s look at how to do that. \n3.4.2\nEliminating synchronous interaction\nThere are a few different ways to reduce the amount of synchronous communication\nwith other services while handling synchronous requests. One solution is to avoid the\nproblem entirely by defining services that only have asynchronous APIs. That’s not\nalways possible, though. For example, public APIs are commonly RESTful. Services\nare therefore sometimes required to have synchronous APIs.\n Fortunately, there are ways to handle synchronous requests without making syn-\nchronous requests. Let’s talk about the options.\nUSE ASYNCHRONOUS INTERACTION STYLES\nIdeally, all interactions should be done using the asynchronous interaction styles\ndescribed earlier in this chapter. For example, say a client of the FTGO application\nused an asynchronous request/asynchronous response style of interaction to create\norders. A client creates an order by sending a request message to the Order Service.\n \n",
      "page_number": 110
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 130-138)",
      "start_page": 130,
      "end_page": 138,
      "detection_method": "topic_boundary",
      "content": "105\nUsing asynchronous messaging to improve availability\nThis service then asynchronously exchanges messages with other services and eventu-\nally sends a reply message to the client. Figure 3.16 shows the design.\nThe client and the services communicate asynchronously by sending messages via\nmessaging channels. No participant in this interaction is ever blocked waiting for a\nresponse.\n Such an architecture would be extremely resilient, because the message broker\nbuffers messages until they can be consumed. The problem, however, is that services\noften have an external API that uses a synchronous protocol such as REST, so it must\nrespond to requests immediately.\n If a service has a synchronous API, one way to improve availability is to replicate\ndata. Let’s see how that works. \nREPLICATE DATA\nOne way to minimize synchronous requests during request processing is to replicate\ndata. A service maintains a replica of the data that it needs when processing requests.\nIt keeps the replica up-to-date by subscribing to events published by the services that\nown the data. For example, Order Service could maintain a replica of data owned by\nConsumer Service and Restaurant Service. This would enable Order Service to\nhandle a request to create an order without having to interact with those services.\nFigure 3.17 shows the design.\n Consumer Service and Restaurant Service publish events whenever their data\nchanges. Order Service subscribes to those events and updates its replica.\n In some situations, replicating data is a useful approach. For example, chapter 5\ndescribes how Order Service replicates data from Restaurant Service so that it can\nvalidate and price menu items. One drawback of replication is that it can sometimes\nrequire the replication of large amounts of data, which is inefficient. For example, it\nmay not be practical for Order Service to maintain a replica of the data owned by\nConsumer Service, due to the large number of consumers. Another drawback of\nClient\nConsumer\nService\nRestaurant\nService\nOrder request\nchannel\nConsumer request\nchannel\nOrder Service\nreply channel\nRestaurant request\nchannel\nCreate order\nrequest\nCreate order\nresponse\nOrder\nService\nClient reply\nchannel\nFigure 3.16\nThe FTGO application has higher availability if its services communicate using asynchronous \nmessaging instead of synchronous calls.\n \n\n\n106\nCHAPTER 3\nInterprocess communication in a microservice architecture\nreplication is that it doesn’t solve the problem of how a service updates data owned by\nother services.\n One way to solve that problem is for a service to delay interacting with other ser-\nvices until after it responds to its client. We’ll next look at how that works.\nFINISH PROCESSING AFTER RETURNING A RESPONSE\nAnother way to eliminate synchronous communication during request processing is\nfor a service to handle a request as follows:\n1\nValidate the request using only the data available locally.\n2\nUpdate its database, including inserting messages into the OUTBOX table.\n3\nReturn a response to its client.\nWhile handling a request, the service doesn’t synchronously interact with any other\nservices. Instead, it asynchronously sends messages to other services. This approach\nensures that the services are loosely coupled. As you’ll learn in the next chapter, this is\noften implemented using a saga.\n For example, if Order Service uses this approach, it creates an order in a PENDING\nstate and then validates the order asynchronously by exchanging messages with other\nservices. Figure 3.18 shows what happens when the createOrder() operation is\ninvoked. The sequence of events is as follows:\n1\nOrder Service creates an Order in a PENDING state.\n2\nOrder Service returns a response to its client containing the order ID.\n3\nOrder Service sends a ValidateConsumerInfo message to Consumer Service.\nServices publish events\nwhen their data changes.\nReplicated data enables Order Service to\nhandle the createOrder() request without\nsynchronously invoking services.\nRestaurant\nService\nConsumer event\nchannel\nRestaurant event\nchannel\nOrder\nService\nConsumer\nService\nConsumer Service database\n«table»\nCONSUMERS\ncreateOrder()\nRestaurant Service database\n«table»\nRESTAURANTS\nOrder Service database\n«table»\nORDERS\n«table»\nCONSUMERS\n«table»\nRESTAURANTS\nFigure 3.17\nOrder Service is self-contained because it has replicas of the consumer and restaurant data.\n \n\n\n107\nUsing asynchronous messaging to improve availability\n4\nOrder Service sends a ValidateOrderDetails message to Restaurant Service.\n5\nConsumer Service receives a ValidateConsumerInfo message, verifies the con-\nsumer can place an order, and sends a ConsumerValidated message to Order\nService.\n6\nRestaurant Service receives a ValidateOrderDetails message, verifies the\nmenu item are valid and that the restaurant can deliver to the order’s delivery\naddress, and sends an OrderDetailsValidated message to Order Service.\n7\nOrder Service receives ConsumerValidated and OrderDetailsValidated and\nchanges the state of the order to VALIDATED.\n8\n…\nOrder Service can receive the ConsumerValidated and OrderDetailsValidated mes-\nsages in either order. It keeps track of which message it receives first by changing the\nstate of the order. If it receives the ConsumerValidated first, it changes the state of the\norder to CONSUMER_VALIDATED, whereas if it receives the OrderDetailsValidated mes-\nsage first, it changes its state to ORDER_DETAILS_VALIDATED. Order Service changes\nthe state of the Order to VALIDATED when it receives the other message.\nSynchronous\nKey\nAsynchronous\nOrder Service\nClient\nConsumer Service\nRestaurant Service\n...\ncreateOrder\nAsynchronous\nSynchronous\ncreate order\nupdate order\nupdate order\ncreateOrder\nValidateConsumerInfo\nValidateOrderDetails\nConsumerValidated\nOrderDetailsValidated\n...\nFigure 3.18\nOrder Service creates an order without invoking any other service. It then asynchronously \nvalidates the newly created Order by exchanging messages with other services, including Consumer Service \nand Restaurant Service.\n \n\n\n108\nCHAPTER 3\nInterprocess communication in a microservice architecture\n After the Order has been validated, Order Service completes the rest of the order-\ncreation process, discussed in the next chapter. What’s nice about this approach is\nthat even if Consumer Service is down, for example, Order Service still creates orders\nand responds to its clients. Eventually, Consumer Service will come back up and pro-\ncess any queued messages, and orders will be validated.\n A drawback of a service responding before fully processing a request is that it\nmakes the client more complex. For example, Order Service makes minimal guaran-\ntees about the state of a newly created order when it returns a response. It creates the\norder and returns immediately before validating the order and authorizing the con-\nsumer’s credit card. Consequently, in order for the client to know whether the order\nwas successfully created, either it must periodically poll or Order Service must send it\na notification message. As complex as it sounds, in many situations this is the pre-\nferred approach—especially because it also addresses the distributed transaction man-\nagement issues I discuss in the next chapter. In chapters 4 and 5, for example, I\ndescribe how Order Service uses this approach. \nSummary\nThe microservice architecture is a distributed architecture, so interprocess\ncommunication plays a key role.\nIt’s essential to carefully manage the evolution of a service’s API. Backward-\ncompatible changes are the easiest to make because they don’t impact clients. If\nyou make a breaking change to a service’s API, it will typically need to support\nboth the old and new versions until its clients have been upgraded.\nThere are numerous IPC technologies, each with different trade-offs. One key\ndesign decision is to choose either a synchronous remote procedure invocation\npattern or the asynchronous Messaging pattern. Synchronous remote proce-\ndure invocation-based protocols, such as REST, are the easiest to use. But ser-\nvices should ideally communicate using asynchronous messaging in order to\nincrease availability.\nIn order to prevent failures from cascading through a system, a service client\nthat uses a synchronous protocol must be designed to handle partial failures,\nwhich are when the invoked service is either down or exhibiting high latency. In\nparticular, it must use timeouts when making requests, limit the number of out-\nstanding requests, and use the Circuit breaker pattern to avoid making calls to a\nfailing service.\nAn architecture that uses synchronous protocols must include a service discov-\nery mechanism in order for clients to determine the network location of a ser-\nvice instance. The simplest approach is to use the service discovery mechanism\nimplemented by the deployment platform: the Server-side discovery and 3rd\nparty registration patterns. But an alternative approach is to implement service\ndiscovery at the application level: the Client-side discovery and Self registration\n \n\n\n109\nSummary\npatterns. It’s more work, but it does handle the scenario where services are run-\nning on multiple deployment platforms.\nA good way to design a messaging-based architecture is to use the messages and\nchannels model, which abstracts the details of the underlying messaging system.\nYou can then map that design to a specific messaging infrastructure, which is\ntypically message broker–based.\nOne key challenge when using messaging is atomically updating the database\nand publishing a message. A good solution is to use the Transactional outbox\npattern and first write the message to the database as part of the database trans-\naction. A separate process then retrieves the message from the database using\neither the Polling publisher pattern or the Transaction log tailing pattern and\npublishes it to the message broker. \n \n\n\n110\nManaging transactions\nwith sagas\nWhen Mary started investigating the microservice architecture, one of her biggest\nconcerns was how to implement transactions that span multiple services. Transac-\ntions are an essential ingredient of every enterprise application. Without transac-\ntions it would be impossible to maintain data consistency.\n ACID (Atomicity, Consistency, Isolation, Durability) transactions greatly simplify\nthe job of the developer by providing the illusion that each transaction has exclu-\nsive access to the data. In a microservice architecture, transactions that are within a\nsingle service can still use ACID transactions. The challenge, however, lies in imple-\nmenting transactions for operations that update data owned by multiple services.\nThis chapter covers\nWhy distributed transactions aren’t a good fit for \nmodern applications\nUsing the Saga pattern to maintain data \nconsistency in a microservice architecture\nCoordinating sagas using choreography and \norchestration\nUsing countermeasures to deal with the lack of \nisolation\n \n\n\n111\nTransaction management in a microservice architecture\nFor example, as described in chapter 2, the createOrder() operation spans numer-\nous services, including Order Service, Kitchen Service, and Accounting Service.\nOperations such as these need a transaction management mechanism that works\nacross services.\n Mary discovered that, as mentioned in chapter 2, the traditional approach to dis-\ntributed transaction management isn’t a good choice for modern applications.\nInstead of an ACID transactions, an operation that spans services must use what’s\nknown as a saga, a message-driven sequence of local transactions, to maintain data\nconsistency. One challenge with sagas is that they are ACD (Atomicity, Consistency,\nDurability). They lack the isolation feature of traditional ACID transactions. As a\nresult, an application must use what are known as countermeasures, design techniques\nthat prevent or reduce the impact of concurrency anomalies caused by the lack of\nisolation.\n In many ways, the biggest obstacle that Mary and the FTGO developers will face\nwhen adopting microservices is moving from a single database with ACID transactions\nto a multi-database architecture with ACD sagas. They’re used to the simplicity of the\nACID transaction model. But in reality, even monolithic applications such as the FTGO\napplication typically don’t use textbook ACID transactions. For example, many appli-\ncations use a lower transaction isolation level in order to improve performance. Also,\nmany important business processes, such as transferring money between accounts at\ndifferent banks, are eventually consistent. Not even Starbucks uses two-phase commit\n(www.enterpriseintegrationpatterns.com/ramblings/18_starbucks.html).\n I begin this chapter by looking at the challenges of transaction management in the\nmicroservice architecture and explain why the traditional approach to distributed\ntransaction management isn’t an option. Next I explain how to maintain data consis-\ntency using sagas. After that I look at the two different ways of coordinating sagas:\nchoreography, where participants exchange events without a centralized point of con-\ntrol, and orchestration, where a centralized controller tells the saga participants what\noperation to perform. I discuss how to use countermeasures to prevent or reduce the\nimpact of concurrency anomalies caused by the lack of isolation between sagas. Finally, I\ndescribe the implementation of an example saga.\n Let’s start by taking a look at the challenge of managing transactions in a micro-\nservice architecture.\n4.1\nTransaction management in a microservice \narchitecture\nAlmost every request handled by an enterprise application is executed within a data-\nbase transaction. Enterprise application developers use frameworks and libraries that\nsimplify transaction management. Some frameworks and libraries provide a program-\nmatic API for explicitly beginning, committing, and rolling back transactions. Other\nframeworks, such as the Spring framework, provide a declarative mechanism. Spring\nprovides an @Transactional annotation that arranges for method invocations to be\n \n\n\n112\nCHAPTER 4\nManaging transactions with sagas\nautomatically executed within a transaction. As a result, it’s straightforward to write\ntransactional business logic.\n Or, to be more precise, transaction management is straightforward in a monolithic\napplication that accesses a single database. Transaction management is more chal-\nlenging in a complex monolithic application that uses multiple databases and mes-\nsage brokers. And in a microservice architecture, transactions span multiple services,\neach of which has its own database. In this situation, the application must use a more\nelaborate mechanism to manage transactions. As you’ll learn, the traditional approach\nof using distributed transactions isn’t a viable option for modern applications. Instead, a\nmicroservices-based application must use sagas.\n Before I explain sagas, let’s first look at why transaction management is challeng-\ning in a microservice architecture.\n4.1.1\nThe need for distributed transactions in a microservice \narchitecture\nImagine that you’re the FTGO developer responsible for implementing the create-\nOrder() system operation. As described in chapter 2, this operation must verify that\nthe consumer can place an order, verify the order details, authorize the consumer’s\ncredit card, and create an Order in the database. It’s relatively straightforward to\nimplement this operation in the monolithic FTGO application. All the data required\nto validate the order is readily accessible. What’s more, you can use an ACID transac-\ntion to ensure data consistency. You might use Spring’s @Transactional annotation\non the createOrder() service method.\n In contrast, implementing the same operation in a microservice architecture is\nmuch more complicated. As figure 4.1 shows, the needed data is scattered around\nmultiple services. The createOrder() operation accesses data in numerous services.\nIt reads data from Consumer Service and updates data in Order Service, Kitchen\nService, and Accounting Service.\n Because each service has its own database, you need to use a mechanism to main-\ntain data consistency across those databases. \n4.1.2\nThe trouble with distributed transactions\nThe traditional approach to maintaining data consistency across multiple services,\ndatabases, or message brokers is to use distributed transactions. The de facto standard\nfor distributed transaction management is the X/Open Distributed Transaction Pro-\ncessing (DTP) Model (X/Open XA—see https://en.wikipedia.org/wiki/X/Open_XA).\nXA uses two-phase commit (2PC) to ensure that all participants in a transaction either\ncommit or rollback. An XA-compliant technology stack consists of XA-compliant data-\nbases and message brokers, database drivers, and messaging APIs, and an interprocess\ncommunication mechanism that propagates the XA global transaction ID. Most SQL\ndatabases are XA compliant, as are some message brokers. Java EE applications can,\nfor example, use JTA to perform distributed transactions.\n \n\n\n113\nTransaction management in a microservice architecture\nAs simple as this sounds, there are a variety of problems with distributed transac-\ntions. One problem is that many modern technologies, including NoSQL databases\nsuch as MongoDB and Cassandra, don’t support them. Also, distributed transactions\naren’t supported by modern message brokers such as RabbitMQ and Apache Kafka.\nAs a result, if you insist on using distributed transactions, you can’t use many mod-\nern technologies.\n Another problem with distributed transactions is that they are a form of synchro-\nnous IPC, which reduces availability. In order for a distributed transaction to commit,\nall the participating services must be available. As described in chapter 3, the availabil-\nity is the product of the availability of all of the participants in the transaction. If a dis-\ntributed transaction involves two services that are 99.5% available, then the overall\navailability is 99%, which is significantly less. Each additional service involved in a dis-\ntributed transaction further reduces availability. There is even Eric Brewer’s CAP theo-\nrem, which states that a system can only have two of the following three properties:\nAccount\nTicket\nConsumer\nData consistency required\nWrites\nWrites\ncreateOrder()\nReads\nAccounting Service\nKitchen Service\nOrder\nOrder Service\nConsumer Service\nThe createOrder() operation reads from\nConsumer Service and updates data\nin Order Service, Kitchen Service,\nand Accounting Service.\nOrder\ncontroller\nFigure 4.1\nThe createOrder() operation updates data in several services. It must use a \nmechanism to maintain data consistency across those services.\n \n",
      "page_number": 130
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 139-154)",
      "start_page": 139,
      "end_page": 154,
      "detection_method": "topic_boundary",
      "content": "114\nCHAPTER 4\nManaging transactions with sagas\nconsistency, availability, and partition tolerance (https://en.wikipedia.org/wiki/CAP\n_theorem). Today, architects prefer to have a system that’s available rather than one\nthat’s consistent.\n On the surface, distributed transactions are appealing. From a developer’s per-\nspective, they have the same programming model as local transactions. But because of\nthe problems mentioned so far, distributed transactions aren’t a viable technology for\nmodern applications. Chapter 3 described how to send messages as part of a database\ntransaction without using distributed transactions. To solve the more complex prob-\nlem of maintaining data consistency in a microservice architecture, an application\nmust use a different mechanism that builds on the concept of loosely coupled, asyn-\nchronous services. This is where sagas come in. \n4.1.3\nUsing the Saga pattern to maintain data consistency\nSagas are mechanisms to maintain data consistency in a microservice architecture\nwithout having to use distributed transactions. You define a saga for each system com-\nmand that needs to update data in multiple services. A saga is a sequence of local\ntransactions. Each local transaction updates data within a single service using the\nfamiliar ACID transaction frameworks and libraries mentioned earlier.\nThe system operation initiates the first step of the saga. The completion of a local\ntransaction triggers the execution of the next local transaction. Later, in section 4.2,\nyou’ll see how coordination of the steps is implemented using asynchronous messag-\ning. An important benefit of asynchronous messaging is that it ensures that all the\nsteps of a saga are executed, even if one or more of the saga’s participants is temporar-\nily unavailable.\n Sagas differ from ACID transactions in a couple of important ways. As I describe in\ndetail in section 4.3, they lack the isolation property of ACID transactions. Also, because\neach local transaction commits its changes, a saga must be rolled back using compensat-\ning transactions. I talk more about compensating transactions later in this section. Let’s\ntake a look at an example saga.\nAN EXAMPLE SAGA: THE CREATE ORDER SAGA\nThe example saga used throughout this chapter is the Create Order Saga, which is\nshown in figure 4.2. The Order Service implements the createOrder() operation\nusing this saga. The saga’s first local transaction is initiated by the external request to\ncreate an order. The other five local transactions are each triggered by completion of\nthe previous one.\nPattern: Saga\nMaintain data consistency across services using a sequence of local transactions\nthat are coordinated using asynchronous messaging. See http://microservices.io/\npatterns/data/saga.html.\n \n\n\n115\nTransaction management in a microservice architecture\nThis saga consists of the following local transactions:\n1\nOrder Service—Create an Order in an APPROVAL_PENDING state.\n2\nConsumer Service—Verify that the consumer can place an order.\n3\nKitchen Service—Validate order details and create a Ticket in the CREATE\n_PENDING.\n4\nAccounting Service—Authorize consumer’s credit card.\n5\nKitchen Service—Change the state of the Ticket to AWAITING_ACCEPTANCE.\n6\nOrder Service—Change the state of the Order to APPROVED.\nLater, in section 4.2, I describe how the services that participate in a saga communi-\ncate using asynchronous messaging. A service publishes a message when a local trans-\naction completes. This message then triggers the next step in the saga. Not only does\nusing messaging ensure the saga participants are loosely coupled, it also guarantees\nthat a saga completes. That’s because if the recipient of a message is temporarily\nunavailable, the message broker buffers the message until it can be delivered.\n On the surface, sagas seem straightforward, but there are a few challenges to using\nthem. One challenge is the lack of isolation between sagas. Section 4.3 describes how\nto handle this problem. Another challenge is rolling back changes when an error\noccurs. Let’s take a look at how to do that. \nSAGAS USE COMPENSATING TRANSACTIONS TO ROLL BACK CHANGES\nA great feature of traditional ACID transactions is that the business logic can easily\nroll back a transaction if it detects the violation of a business rule. It executes a ROLL-\nBACK statement, and the database undoes all the changes made so far. Unfortunately,\nsagas can’t be automatically rolled back, because each step commits its changes to the\nlocal database. This means, for example, that if the authorization of the credit card\nfails in the fourth step of the Create Order Saga, the FTGO application must explicitly\nOrder Service\nSaga\nCreate order\nTxn:1\nApprove order\nTxn:6\nConsumer Service\nCreate order\nTxn:1\nVerify consumer\nTxn:2\nKitchen Service\nCreate ticket\nTxn:3\nApprove ticket\nTxn:5\nAccounting Service\nAuthorize card\nTxn:4\nFigure 4.2\nCreating an Order using a saga. The createOrder() operation is implemented by a \nsaga that consists of local transactions in several services.\n \n\n\n116\nCHAPTER 4\nManaging transactions with sagas\nundo the changes made by the first three steps. You must write what are known as com-\npensating transactions.\n Suppose that the (n + 1)th transaction of a saga fails. The effects of the previous n\ntransactions must be undone. Conceptually, each of those steps, Ti, has a correspond-\ning compensating transaction, Ci, which undoes the effects of the Ti. To undo the\neffects of those first n steps, the saga must execute each Ci in reverse order. The\nsequence of steps is T1 … Tn, Cn … C1, as shown in figure 4.3. In this example, Tn+1\nfails, which requires steps T1 … Tn to be undone.\nThe saga executes the compensation transactions in reverse order of the forward\ntransactions: Cn … C1. The mechanics of sequencing the Cis aren’t any different than\nsequencing the Tis. The completion of Ci must trigger the execution of Ci-1.\n Consider, for example, the Create Order Saga. This saga can fail for a variety of\nreasons:\nThe consumer information is invalid or the consumer isn’t allowed to create\norders.\nThe restaurant information is invalid or the restaurant is unable to accept orders.\nThe authorization of the consumer’s credit card fails.\nIf a local transaction fails, the saga’s coordination mechanism must execute compen-\nsating transactions that reject the Order and possibly the Ticket. Table 4.1 shows the\ncompensating transactions for each step of the Create Order Saga. It’s important to\nnote that not all steps need compensating transactions. Read-only steps, such as verify-\nConsumerDetails(), don’t need compensating transactions. Nor do steps such as\nauthorizeCreditCard() that are followed by steps that always succeed.\n Section 4.3 discusses how the first three steps of the Create Order Saga are termed\ncompensatable transactions because they’re followed by steps that can fail, how the\nfourth step is termed the saga’s pivot transaction because it’s followed by steps that\nSaga\nT1\n...\nTn\nTn+1\nFAILS\nCn\n...\nC\nThe changes made by T1...Tn\nhave been committed.\nThe compensating transactions undo\nthe changes made by T1...Tn.\n1\nFigure 4.3\nWhen a step of a saga fails because of a business rule violation, the saga must explicitly \nundo the updates made by previous steps by executing compensating transactions.\n \n\n\n117\nCoordinating sagas\nnever fail, and how the last two steps are termed retriable transactions because they\nalways succeed.\n To see how compensating transactions are used, imagine a scenario where the\nauthorization of the consumer’s credit card fails. In this scenario, the saga executes\nthe following local transactions:\n1\nOrder Service—Create an Order in an APPROVAL_PENDING state.\n2\nConsumer Service—Verify that the consumer can place an order.\n3\nKitchen Service—Validate order details and create a Ticket in the CREATE\n_PENDING state.\n4\nAccounting Service—Authorize consumer’s credit card, which fails.\n5\nKitchen Service—Change the state of the Ticket to CREATE_REJECTED.\n6\nOrder Service—Change the state of the Order to REJECTED.\nThe fifth and sixth steps are compensating transactions that undo the updates made\nby Kitchen Service and Order Service, respectively. A saga’s coordination logic is\nresponsible for sequencing the execution of forward and compensating transactions.\nLet’s look at how that works. \n4.2\nCoordinating sagas\nA saga’s implementation consists of logic that coordinates the steps of the saga.\nWhen a saga is initiated by system command, the coordination logic must select and\ntell the first saga participant to execute a local transaction. Once that transaction\ncompletes, the saga’s sequencing coordination selects and invokes the next saga\nparticipant. This process continues until the saga has executed all the steps. If any\nlocal transaction fails, the saga must execute the compensating transactions in\nreverse order. There are a couple of different ways to structure a saga’s coordina-\ntion logic:\nChoreography—Distribute the decision making and sequencing among the saga\nparticipants. They primarily communicate by exchanging events.\nTable 4.1\nThe compensating transactions for the Create Order Saga\nStep\nService\nTransaction\nCompensating transaction\n1\nOrder Service\ncreateOrder()\nrejectOrder()\n2\nConsumer Service\nverifyConsumerDetails()\n—\n3\nKitchen Service\ncreateTicket()\nrejectTicket()\n4\nAccounting Service\nauthorizeCreditCard()\n—\n5\nKitchen Service\napproveTicket()\n—\n6\nOrder Service\napproveOrder()\n—\n \n\n\n118\nCHAPTER 4\nManaging transactions with sagas\nOrchestration—Centralize a saga’s coordination logic in a saga orchestrator class.\nA saga orchestrator sends command messages to saga participants telling them\nwhich operations to perform.\nLet’s look at each option, starting with choreography.\n4.2.1\nChoreography-based sagas\nOne way you can implement a saga is by using choreography. When using choreogra-\nphy, there’s no central coordinator telling the saga participants what to do. Instead,\nthe saga participants subscribe to each other’s events and respond accordingly. To\nshow how choreography-based sagas work, I’ll first describe an example. After that, I’ll\ndiscuss a couple of design issues that you must address. Then I’ll discuss the benefits\nand drawbacks of using choreography.\nIMPLEMENTING THE CREATE ORDER SAGA USING CHOREOGRAPHY\nFigure 4.4 shows the design of the choreography-based version of the Create Order\nSaga. The participants communicate by exchanging events. Each participant, starting\nwith the Order Service, updates its database and publishes an event that triggers the\nnext participant.\nAccounting Service\n4. createPendingAuthorization()\n6. authorizeCard()\nKitchen Service\n3. createTicket()\n6. approveTicket()\nOrder\nService\n1. createOrder()\n7. approveOrder()\nConsumer Service\n2. verifyConsumerDetails()\nOrder events\nMessage broker\nConsumer veriﬁed\nPublish\nKey\nSubscribe\nConsumer events\nTicket events\nCredit card events\n2\nOrder created\n1\nTicket created\n3\nCredit card authorized\n5\n6\n4\n5a\n7\n5b\nFigure 4.4\nImplementing the Create Order Saga using choreography. The saga participants communicate by \nexchanging events.\n \n\n\n119\nCoordinating sagas\nThe happy path through this saga is as follows:\n1\nOrder Service creates an Order in the APPROVAL_PENDING state and publishes\nan OrderCreated event.\n2\nConsumer Service consumes the OrderCreated event, verifies that the con-\nsumer can place the order, and publishes a ConsumerVerified event.\n3\nKitchen Service consumes the OrderCreated event, validates the Order, cre-\nates a Ticket in a CREATE_PENDING state, and publishes the TicketCreated event.\n4\nAccounting Service consumes the OrderCreated event and creates a Credit-\nCardAuthorization in a PENDING state.\n5\nAccounting Service consumes the TicketCreated and ConsumerVerified\nevents, charges the consumer’s credit card, and publishes the CreditCard-\nAuthorized event.\n6\nKitchen Service consumes the CreditCardAuthorized event and changes the\nstate of the Ticket to AWAITING_ACCEPTANCE.\n7\nOrder Service receives the CreditCardAuthorized events, changes the state of\nthe Order to APPROVED, and publishes an OrderApproved event.\nThe Create Order Saga must also handle the scenario where a saga participant rejects\nthe Order and publishes some kind of failure event. For example, the authorization of\nthe consumer’s credit card might fail. The saga must execute the compensating trans-\nactions to undo what’s already been done. Figure 4.5 shows the flow of events when\nthe AccountingService can’t authorize the consumer’s credit card.\n The sequence of events is as follows:\n1\nOrder Service creates an Order in the APPROVAL_PENDING state and publishes\nan OrderCreated event.\n2\nConsumer Service consumes the OrderCreated event, verifies that the con-\nsumer can place the order, and publishes a ConsumerVerified event.\n3\nKitchen Service consumes the OrderCreated event, validates the Order, creates\na Ticket in a CREATE_PENDING state, and publishes the TicketCreated event.\n4\nAccounting Service consumes the OrderCreated event and creates a Credit-\nCardAuthorization in a PENDING state.\n5\nAccounting Service consumes the TicketCreated and ConsumerVerified\nevents, charges the consumer’s credit card, and publishes a Credit Card\nAuthorization Failed event.\n6\nKitchen Service consumes the Credit Card Authorization Failed event and\nchanges the state of the Ticket to REJECTED.\n7\nOrder Service consumes the Credit Card Authorization Failed event and\nchanges the state of the Order to REJECTED.\nAs you can see, the participants of choreography-based sagas interact using publish/\nsubscribe. Let’s take a closer look at some issues you’ll need to consider when imple-\nmenting publish/subscribe-based communication for your sagas. \n \n\n\n120\nCHAPTER 4\nManaging transactions with sagas\nRELIABLE EVENT-BASED COMMUNICATION\nThere are a couple of interservice communication-related issues that you must con-\nsider when implementing choreography-based sagas. The first issue is ensuring that a\nsaga participant updates its database and publishes an event as part of a database\ntransaction. Each step of a choreography-based saga updates the database and pub-\nlishes an event. For example, in the Create Order Saga, Kitchen Service receives a\nConsumer Verified event, creates a Ticket, and publishes a Ticket Created event.\nIt’s essential that the database update and the publishing of the event happen atomi-\ncally. Consequently, to communicate reliably, the saga participants must use transac-\ntional messaging, described in chapter 3.\n The second issue you need to consider is ensuring that a saga participant must\nbe able to map each event that it receives to its own data. For example, when Order\nService receives a Credit Card Authorized event, it must be able to look up the\ncorresponding Order. The solution is for a saga participant to publish events con-\ntaining a correlation id, which is data that enables other participants to perform the\nmapping.\nAccounting Service\n4. createPendingAuthorization()\n6. authorizeCard()\nKitchen Service\n3. createTicket()\n6. rejectTicket()\nOrder\nService\n1. createOrder()\n7. rejectOrder()\nConsumer Service\n2. verifyConsumerDetails()\nOrder events\nMessage broker\nConsumer veriﬁed\nPublish\nKey\nSubscribe\nConsumer events\nTicket events\nCredit card events\n2\nOrder created\n1\nTicket created\n3\nCredit card authorization failed\n5\n6\n4\n5a\n7\n5b\nFigure 4.5\nThe sequence of events in the Create Order Saga when the authorization of the consumer’s credit \ncard fails. Accounting Service publishes the Credit Card Authorization Failed event, which causes \nKitchen Service to reject the Ticket, and Order Service to reject the Order.\n \n\n\n121\nCoordinating sagas\n For example, the participants of the Create Order Saga can use the orderId as a\ncorrelation ID that’s passed from one participant to the next. Accounting Service pub-\nlishes a Credit Card Authorized event containing the orderId from the Ticket-\nCreated event. When Order Service receives a Credit Card Authorized event, it uses\nthe orderId to retrieve the corresponding Order. Similarly, Kitchen Service uses the\norderId from that event to retrieve the corresponding Ticket. \nBENEFITS AND DRAWBACKS OF CHOREOGRAPHY-BASED SAGAS\nChoreography-based sagas have several benefits:\nSimplicity—Services publish events when they create, update, or delete business\nobjects.\nLoose coupling —The participants subscribe to events and don’t have direct knowl-\nedge of each other.\nAnd there are some drawbacks:\nMore difficult to understand—Unlike with orchestration, there isn’t a single place\nin the code that defines the saga. Instead, choreography distributes the imple-\nmentation of the saga among the services. Consequently, it’s sometimes difficult\nfor a developer to understand how a given saga works.\nCyclic dependencies between the services—The saga participants subscribe to each\nother’s events, which often creates cyclic dependencies. For example, if you\ncarefully examine figure 4.4, you’ll see that there are cyclic dependencies, such\nas Order Service  Accounting Service  Order Service. Although this isn’t\nnecessarily a problem, cyclic dependencies are considered a design smell.\nRisk of tight coupling—Each saga participant needs to subscribe to all events that\naffect them. For example, Accounting Service must subscribe to all events that\ncause the consumer’s credit card to be charged or refunded. As a result, there’s\na risk that it would need to be updated in lockstep with the order lifecycle\nimplemented by Order Service.\nChoreography can work well for simple sagas, but because of these drawbacks it’s\noften better for more complex sagas to use orchestration. Let’s look at how orches-\ntration works. \n4.2.2\nOrchestration-based sagas\nOrchestration is another way to implement sagas. When using orchestration, you\ndefine an orchestrator class whose sole responsibility is to tell the saga participants\nwhat to do. The saga orchestrator communicates with the participants using command/\nasync reply-style interaction. To execute a saga step, it sends a command message to a\nparticipant telling it what operation to perform. After the saga participant has per-\nformed the operation, it sends a reply message to the orchestrator. The orchestrator\nthen processes the message and determines which saga step to perform next.\n \n\n\n122\nCHAPTER 4\nManaging transactions with sagas\n To show how orchestration-based sagas work, I’ll first describe an example. Then\nI’ll describe how to model orchestration-based sagas as state machines. I’ll discuss how\nto make use of transactional messaging to ensure reliable communication between\nthe saga orchestrator and the saga participants. I’ll then describe the benefits and\ndrawbacks of using orchestration-based sagas.\nIMPLEMENTING THE CREATE ORDER SAGA USING ORCHESTRATION\nFigure 4.6 shows the design of the orchestration-based version of the Create Order\nSaga. The saga is orchestrated by the CreateOrderSaga class, which invokes the saga\nparticipants using asynchronous request/response. This class keeps track of the pro-\ncess and sends command messages to saga participants, such as Kitchen Service and\nConsumer Service. The CreateOrderSaga class reads reply messages from its reply\nchannel and then determines the next step, if any, in the saga.\nAccounting\nService\nKitchen\nService\nConsumer\nService\nConsumer Service\nrequest channel\nConsumer veriﬁed\n2\n4\n6\nVerify consumer\n1\nApprove\nrestaurant\norder\n7\nApprove\norder\n8\nCreate\nticket\n3\nAuthorize\ncard\n5\nCard\nauthorized\nMessage broker\nOrder Service\nCommand message\nKey\nReply message\nCreate order\nsaga reply channel\nKitchen Service\nrequest channel\nAccounting Service\nrequest channel\nOrder Service\nrequest channel\nCreate\norder saga\norchestrator\nTicket created\nFigure 4.6\nImplementing the Create Order Saga using orchestration. Order Service \nimplements a saga orchestrator, which invokes the saga participants using asynchronous request/\nresponse.\n \n\n\n123\nCoordinating sagas\nOrder Service first creates an Order and a Create Order Saga orchestrator. After that,\nthe flow for the happy path is as follows:\n1\nThe saga orchestrator sends a Verify Consumer command to Consumer Service.\n2\nConsumer Service replies with a Consumer Verified message.\n3\nThe saga orchestrator sends a Create Ticket command to Kitchen Service.\n4\nKitchen Service replies with a Ticket Created message.\n5\nThe saga orchestrator sends an Authorize Card message to Accounting Service.\n6\nAccounting Service replies with a Card Authorized message.\n7\nThe saga orchestrator sends an Approve Ticket command to Kitchen Service.\n8\nThe saga orchestrator sends an Approve Order command to Order Service.\nNote that in final step, the saga orchestrator sends a command message to Order\nService, even though it’s a component of Order Service. In principle, the Create\nOrder Saga could approve the Order by updating it directly. But in order to be consis-\ntent, the saga treats Order Service as just another participant.\n Diagrams such as figure 4.6 each depict one scenario for a saga, but a saga is likely\nto have numerous scenarios. For example, the Create Order Saga has four scenarios.\nIn addition to the happy path, the saga can fail due to a failure in either Consumer\nService, Kitchen Service, or Accounting Service. It’s useful, therefore, to model a\nsaga as a state machine, because it describes all possible scenarios. \nMODELING SAGA ORCHESTRATORS AS STATE MACHINES\nA good way to model a saga orchestrator is as a state machine. A state machine con-\nsists of a set of states and a set of transitions between states that are triggered by\nevents. Each transition can have an action, which for a saga is the invocation of a\nsaga participant. The transitions between states are triggered by the completion of a\nlocal transaction performed by a saga participant. The current state and the specific\noutcome of the local transaction determine the state transition and what action, if\nany, to perform. There are also effective testing strategies for state machines. As a\nresult, using a state machine model makes designing, implementing, and testing\nsagas easier.\n Figure 4.7 shows the state machine model for the Create Order Saga. This state\nmachine consists of numerous states, including the following:\n\nVerifying Consumer—The initial state. When in this state, the saga is waiting\nfor the Consumer Service to verify that the consumer can place the order.\n\nCreating Ticket—The saga is waiting for a reply to the Create Ticket command.\n\nAuthorizing Card—Waiting for Accounting Service to authorize the con-\nsumer’s credit card.\n\nOrder Approved—A final state indicating that the saga completed successfully.\n\nOrder Rejected—A final state indicating that the Order was rejected by one of\nthe participants.\n \n\n\n124\nCHAPTER 4\nManaging transactions with sagas\nThe state machine also defines numerous state transitions. For example, the state\nmachine transitions from the Creating Ticket state to either the Authorizing Card\nor the Rejected Order state. It transitions to the Authorizing Card state when it\nreceives a successful reply to the Create Ticket command. Alternatively, if Kitchen\nService couldn’t create the Ticket, the state machine transitions to the Rejected\nOrder state.\n The state machine’s initial action is to send the VerifyConsumer command to\nConsumer Service. The response from Consumer Service triggers the next state tran-\nsition. If the consumer was successfully verified, the saga creates the Ticket and tran-\nsitions to the Creating Ticket state. But if the consumer verification failed, the saga\nrejects the Order and transitions to the Rejecting Order state. The state machine\nundergoes numerous other state transitions, driven by the responses from saga partici-\npants, until it reaches a final state of either Order Approved or Order Rejected. \nVeriﬁng\nconsumer\nRejecting\norder\nCreating\nticket\nAuthorizing\ncard\nRejecting\nticket\nApproving\nticket\nApproving\norder\nOrder\napproved\nOrder\nrejected\n/Send VerifyConsumer\nConsumerVeriﬁcationFailed/\nsend RejectOrder\nTicket creation failed/\nsend RejectOrder\nConsumerVeriﬁed/\nsend CreateRestaurantOrder\nTicket created/\nsend AuthorizeCard\nCard authorized/\nsend ApproveTicket\nTicket approved/\nsend ApproveOrder\nOrder approved\nCard authorization failed/\nsend RejectTicket\nFigure 4.7\nThe state machine model for the Create Order Saga\n \n\n\n125\nCoordinating sagas\nSAGA ORCHESTRATION AND TRANSACTIONAL MESSAGING\nEach step of an orchestration-based saga consists of a service updating a database\nand publishing a message. For example, Order Service persists an Order and a\nCreate Order Saga orchestrator and sends a message to the first saga participant. A\nsaga participant, such as Kitchen Service, handles a command message by updat-\ning its database and sending a reply message. Order Service processes the partici-\npant’s reply message by updating the state of the saga orchestrator and sending a\ncommand message to the next saga participant. As described in chapter 3, a service\nmust use transactional messaging in order to atomically update the database and\npublish messages. Later on in section 4.4, I’ll describe the implementation of the\nCreate Order Saga orchestrator in more detail, including how it uses transaction\nmessaging.\n Let’s take a look at the benefits and drawbacks of using saga orchestration. \nBENEFITS AND DRAWBACKS OF ORCHESTRATION-BASED SAGAS\nOrchestration-based sagas have several benefits:\nSimpler dependencies—One benefit of orchestration is that it doesn’t introduce\ncyclic dependencies. The saga orchestrator invokes the saga participants, but\nthe participants don’t invoke the orchestrator. As a result, the orchestrator\ndepends on the participants but not vice versa, and so there are no cyclic\ndependencies.\nLess coupling—Each service implements an API that is invoked by the orches-\ntrator, so it does not need to know about the events published by the saga\nparticipants.\nImproves separation of concerns and simplifies the business logic—The saga coordina-\ntion logic is localized in the saga orchestrator. The domain objects are simpler\nand have no knowledge of the sagas that they participate in. For example, when\nusing orchestration, the Order class has no knowledge of any of the sagas, so it\nhas a simpler state machine model. During the execution of the Create Order\nSaga, it transitions directly from the APPROVAL_PENDING state to the APPROVED\nstate. The Order class doesn’t have any intermediate states corresponding to the\nsteps of the saga. As a result, the business is much simpler.\nOrchestration also has a drawback: the risk of centralizing too much business logic in\nthe orchestrator. This results in a design where the smart orchestrator tells the dumb\nservices what operations to do. Fortunately, you can avoid this problem by designing\norchestrators that are solely responsible for sequencing and don’t contain any other\nbusiness logic.\n I recommend using orchestration for all but the simplest sagas. Implementing the\ncoordination logic for your sagas is just one of the design problems you need to solve.\nAnother, which is perhaps the biggest challenge that you’ll face when using sagas, is\nhandling the lack of isolation. Let’s take a look at that problem and how to solve it. \n \n\n\n126\nCHAPTER 4\nManaging transactions with sagas\n4.3\nHandling the lack of isolation\nThe I in ACID stands for isolation. The isolation property of ACID transactions ensures\nthat the outcome of executing multiple transactions concurrently is the same as if they\nwere executed in some serial order. The database provides the illusion that each ACID\ntransaction has exclusive access to the data. Isolation makes it a lot easier to write busi-\nness logic that executes concurrently.\n The challenge with using sagas is that they lack the isolation property of ACID\ntransactions. That’s because the updates made by each of a saga’s local transactions\nare immediately visible to other sagas once that transaction commits. This behavior\ncan cause two problems. First, other sagas can change the data accessed by the saga\nwhile it’s executing. And other sagas can read its data before the saga has completed\nits updates, and consequently can be exposed to inconsistent data. You can, in fact,\nconsider a saga to be ACD:\nAtomicity—The saga implementation ensures that all transactions are executed\nor all changes are undone.\nConsistency—Referential integrity within a service is handled by local databases.\nReferential integrity across services is handled by the services.\nDurability—Handled by local databases.\nThis lack of isolation potentially causes what the database literature calls anomalies. An\nanomaly is when a transaction reads or writes data in a way that it wouldn’t if transac-\ntions were executed one at time. When an anomaly occurs, the outcome of executing\nsagas concurrently is different than if they were executed serially.\n On the surface, the lack of isolation sounds unworkable. But in practice, it’s com-\nmon for developers to accept reduced isolation in return for higher performance. An\nRDBMS lets you specify the isolation level for each transaction (https://dev.mysql\n.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html). The default iso-\nlation level is usually an isolation level that’s weaker than full isolation, also known as\nserializable transactions. Real-world database transactions are often different from\ntextbook definitions of ACID transactions.\n The next section discusses a set of saga design strategies that deal with the lack of\nisolation. These strategies are known as countermeasures. Some countermeasures imple-\nment isolation at the application level. Other countermeasures reduce the business\nrisk of the lack of isolation. By using countermeasures, you can write saga-based busi-\nness logic that works correctly.\n I’ll begin the section by describing the anomalies that are caused by the lack of iso-\nlation. After that, I’ll talk about countermeasures that either eliminate those anoma-\nlies or reduce their business risk.\n \n \n \n\n\n127\nHandling the lack of isolation\n4.3.1\nOverview of anomalies\nThe lack of isolation can cause the following three anomalies:\nLost updates—One saga overwrites without reading changes made by another saga.\nDirty reads—A transaction or a saga reads the updates made by a saga that has\nnot yet completed those updates.\nFuzzy/nonrepeatable reads—Two different steps of a saga read the same data and\nget different results because another saga has made updates.\nAll three anomalies can occur, but the first two are the most common and the most\nchallenging. Let’s take a look at those two types of anomaly, starting with lost updates.\nLOST UPDATES\nA lost update anomaly occurs when one saga overwrites an update made by another\nsaga. Consider, for example, the following scenario:\n1\nThe first step of the Create Order Saga creates an Order.\n2\nWhile that saga is executing, the Cancel Order Saga cancels the Order.\n3\nThe final step of the Create Order Saga approves the Order.\nIn this scenario, the Create Order Saga ignores the update made by the Cancel Order\nSaga and overwrites it. As a result, the FTGO application will ship an order that the\ncustomer had cancelled. Later in this section, I’ll show how to prevent lost updates. \nDIRTY READS\nA dirty read occurs when one saga reads data that’s in the middle of being updated by\nanother saga. Consider, for example, a version of the FTGO application store where\nconsumers have a credit limit. In this application, a saga that cancels an order consists\nof the following transactions:\n\nConsumer Service—Increase the available credit.\n\nOrder Service—Change the state of the Order to cancelled.\n\nDelivery Service—Cancel the delivery.\nLet’s imagine a scenario that interleaves the execution of the Cancel Order and Create\nOrder Sagas, and the Cancel Order Saga is rolled back because it’s too late to cancel\nthe delivery. It’s possible that the sequence of transactions that invoke the Consumer\nService is as follows:\n1\nCancel Order Saga—Increase the available credit.\n2\nCreate Order Saga—Reduce the available credit.\n3\nCancel Order Saga—A compensating transaction that reduces the available credit.\nIn this scenario, the Create Order Saga does a dirty read of the available credit that\nenables the consumer to place an order that exceeds their credit limit. It’s likely that\nthis is an unacceptable risk to the business.\n Let’s look at how to prevent this and other kinds of anomalies from impacting an\napplication. \n \n\n\n128\nCHAPTER 4\nManaging transactions with sagas\n4.3.2\nCountermeasures for handling the lack of isolation\nThe saga transaction model is ACD, and its lack of isolation can result in anomalies\nthat cause applications to misbehave. It’s the responsibility of the developer to write\nsagas in a way that either prevents the anomalies or minimizes their impact on the\nbusiness. This may sound like a daunting task, but you’ve already seen an example of a\nstrategy that prevents anomalies. An Order’s use of *_PENDING states, such as APPROVAL\n_PENDING, is an example of one such strategy. Sagas that update Orders, such as the\nCreate Order Saga, begin by setting the state of an Order to *_PENDING. The *_PENDING\nstate tells other transactions that the Order is being updated by a saga and to act\naccordingly.\n An Order’s use of *_PENDING states is an example of what the 1998 paper “Seman-\ntic ACID properties in multidatabases using remote procedure calls and update prop-\nagations” by Lars Frank and Torben U. Zahle calls a semantic lock countermeasure\n(https://dl.acm.org/citation.cfm?id=284472.284478). The paper describes how to deal\nwith the lack of transaction isolation in multi-database architectures that don’t use dis-\ntributed transactions. Many of its ideas are useful when designing sagas. It describes a\nset of countermeasures for handling anomalies caused by lack of isolation that either\nprevent one or more anomalies or minimize their impact on the business. The counter-\nmeasures described by this paper are as follows:\nSemantic lock—An application-level lock.\nCommutative updates—Design update operations to be executable in any order.\nPessimistic view—Reorder the steps of a saga to minimize business risk.\nReread value—Prevent dirty writes by rereading data to verify that it’s unchanged\nbefore overwriting it.\nVersion file—Record the updates to a record so that they can be reordered.\nBy value—Use each request’s business risk to dynamically select the concur-\nrency mechanism.\nLater in this section, I describe each of these countermeasures, but first I want to\nintroduce some terminology for describing the structure of a saga that’s useful when\ndiscussing countermeasures.\nTHE STRUCTURE OF A SAGA\nThe countermeasures paper mentioned in the last section defines a useful model for\nthe structure of a saga. In this model, shown in figure 4.8, a saga consists of three types\nof transactions:\nCompensatable transactions—Transactions that can potentially be rolled back using\na compensating transaction.\nPivot transaction—The go/no-go point in a saga. If the pivot transaction com-\nmits, the saga will run until completion. A pivot transaction can be a transaction\nthat’s neither compensatable nor retriable. Alternatively, it can be the last com-\npensatable transaction or the first retriable transaction.\n \n\n\n129\nHandling the lack of isolation\nRetriable transactions—Transactions that follow the pivot transaction and are guar-\nanteed to succeed.\nIn the Create Order Saga, the createOrder(), verifyConsumerDetails(), and create-\nTicket() steps are compensatable transactions. The createOrder() and create-\nTicket() transactions have compensating transactions that undo their updates. The\nverifyConsumerDetails() transaction is read-only, so doesn’t need a compensating\ntransaction. The authorizeCreditCard() transaction is this saga’s pivot transaction. If\nthe consumer’s credit card can be authorized, this saga is guaranteed to complete. The\napproveTicket() and approveOrder() steps are retriable transactions that follow the\npivot transaction.\n The distinction between compensatable transactions and retriable transactions is\nespecially important. As you’ll see, each type of transaction plays a different role in the\ncountermeasures. Chapter 13 states that when migrating to microservices, the mono-\nlith must sometimes participate in sagas and that it’s significantly simpler if the mono-\nlith only ever needs to execute retriable transactions.\n Let’s now look at each countermeasure, starting with the semantic lock counter-\nmeasure.\nCOUNTERMEASURE: SEMANTIC LOCK\nWhen using the semantic lock countermeasure, a saga’s compensatable transaction\nsets a flag in any record that it creates or updates. The flag indicates that the record\nStep\n1\n2\n3\n4\n5\n6\nService\nOrder Service\nConsumer Service\nKitchen Service\nAccounting Service\nRestaurant Order Service\nOrder Service\nTransaction\ncreateOrder()\nverifyConsumerDetails()\ncreateTicket()\nauthorizeCreditCard()\napproveRestaurantOrder()\napproveOrder()\nCompensation Transaction\nrejectOrder()\n-\nrejectTicket()\n-\n-\n-\nCompensatable transactions:\nMust support roll back\nPivot transactions:\nThe saga’s go/no-go transaction.\nIf it succeeds, then the saga runs\nto completion.\nRetriable transactions:\nGuaranteed to complete\nFigure 4.8\nA saga consists of three different types of transactions: compensatable transactions, \nwhich can be rolled back, so have a compensating transaction, a pivot transaction, which is the \nsaga’s go/no-go point, and retriable transactions, which are transactions that don’t need to be \nrolled back and are guaranteed to complete.\n \n",
      "page_number": 139
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 155-168)",
      "start_page": 155,
      "end_page": 168,
      "detection_method": "topic_boundary",
      "content": "130\nCHAPTER 4\nManaging transactions with sagas\nisn’t committed and could potentially change. The flag can either be a lock that prevents\nother transactions from accessing the record or a warning that indicates that other\ntransactions should treat that record with suspicion. It’s cleared by either a retriable\ntransaction—saga is completing successfully—or by a compensating transaction: the\nsaga is rolling back.\n The Order.state field is a great example of a semantic lock. The *_PENDING states,\nsuch as APPROVAL_PENDING and REVISION_PENDING, implement a semantic lock. They\ntell other sagas that access an Order that a saga is in the process of updating the Order.\nFor instance, the first step of the Create Order Saga, which is a compensatable trans-\naction, creates an Order in an APPROVAL_PENDING state. The final step of the Create\nOrder Saga, which is a retriable transaction, changes the field to APPROVED. A compen-\nsating transaction changes the field to REJECTED.\n Managing the lock is only half the problem. You also need to decide on a case-by-\ncase basis how a saga should deal with a record that has been locked. Consider, for\nexample, the cancelOrder() system command. A client might invoke this operation\nto cancel an Order that’s in the APPROVAL_PENDING state.\n There are a few different ways to handle this scenario. One option is for the cancel-\nOrder() system command to fail and tell the client to try again later. The main benefit\nof this approach is that it’s simple to implement. The drawback, however, is that it\nmakes the client more complex because it has to implement retry logic.\n Another option is for cancelOrder() to block until the lock is released. A benefit\nof using semantic locks is that they essentially recreate the isolation provided by ACID\ntransactions. Sagas that update the same record are serialized, which significantly\nreduces the programming effort. Another benefit is that they remove the burden of\nretries from the client. The drawback is that the application must manage locks. It\nmust also implement a deadlock detection algorithm that performs a rollback of a\nsaga to break a deadlock and re-execute it. \nCOUNTERMEASURE: COMMUTATIVE UPDATES\nOne straightforward countermeasure is to design the update operations to be com-\nmutative. Operations are commutative if they can be executed in any order. An\nAccount’s debit() and credit() operations are commutative (if you ignore overdraft\nchecks). This countermeasure is useful because it eliminates lost updates.\n Consider, for example, a scenario where a saga needs to be rolled back after a com-\npensatable transaction has debited (or credited) an account. The compensating trans-\naction can simply credit (or debit) the account to undo the update. There’s no\npossibility of overwriting updates made by other sagas. \nCOUNTERMEASURE: PESSIMISTIC VIEW\nAnother way to deal with the lack of isolation is the pessimistic view countermeasure. It\nreorders the steps of a saga to minimize business risk due to a dirty read. Consider, for\nexample, the scenario earlier used to describe the dirty read anomaly. In that scenario,\nthe Create Order Saga performed a dirty read of the available credit and created an\n \n\n\n131\nHandling the lack of isolation\norder that exceeded the consumer credit limit. To reduce the risk of that happening,\nthis countermeasure would reorder the Cancel Order Saga:\n1\nOrder Service—Change the state of the Order to cancelled.\n2\nDelivery Service—Cancel the delivery.\n3\nCustomer Service—Increase the available credit.\nIn this reordered version of the saga, the available credit is increased in a retriable\ntransaction, which eliminates the possibility of a dirty read. \nCOUNTERMEASURE: REREAD VALUE\nThe reread value countermeasure prevents lost updates. A saga that uses this counter-\nmeasure rereads a record before updating it, verifies that it’s unchanged, and then\nupdates the record. If the record has changed, the saga aborts and possibly restarts. This\ncountermeasure is a form of the Optimistic Offline Lock pattern (https://martinfowler\n.com/eaaCatalog/optimisticOfflineLock.html).\n The Create Order Saga could use this countermeasure to handle the scenario\nwhere the Order is cancelled while it’s in the process of being approved. The transac-\ntion that approves the Order verifies that the Order is unchanged since it was created\nearlier in the saga. If it’s unchanged, the transaction approves the Order. But if the\nOrder has been cancelled, the transaction aborts the saga, which causes its compensat-\ning transactions to be executed. \nCOUNTERMEASURE: VERSION FILE\nThe version file countermeasure is so named because it records the operations that are\nperformed on a record so that it can reorder them. It’s a way to turn noncommutative\noperations into commutative operations. To see how this countermeasure works, con-\nsider a scenario where the Create Order Saga executes concurrently with a Cancel\nOrder Saga. Unless the sagas use the semantic lock countermeasure, it’s possible that\nthe Cancel Order Saga cancels the authorization of the consumer’s credit card before\nthe Create Order Saga authorizes the card.\n One way for the Accounting Service to handle these out-of-order requests is for it\nto record the operations as they arrive and then execute them in the correct order. In\nthis scenario, it would first record the Cancel Authorization request. Then, when the\nAccounting Service receives the subsequent Authorize Card request, it would notice\nthat it had already received the Cancel Authorization request and skip authorizing\nthe credit card. \nCOUNTERMEASURE: BY VALUE\nThe final countermeasure is the by value countermeasure. It’s a strategy for selecting\nconcurrency mechanisms based on business risk. An application that uses this\ncountermeasure uses the properties of each request to decide between using sagas\nand distributed transactions. It executes low-risk requests using sagas, perhaps apply-\ning the countermeasures described in the preceding section. But it executes high-risk\nrequests involving, for example, large amounts of money, using distributed transactions.\n \n\n\n132\nCHAPTER 4\nManaging transactions with sagas\nThis strategy enables an application to dynamically make trade-offs about business\nrisk, availability, and scalability.\n It’s likely that you’ll need to use one or more of these countermeasures when\nimplementing sagas in your application. Let’s look at the detailed design and imple-\nmentation of the Create Order Saga, which uses the semantic lock countermeasure. \n4.4\nThe design of the Order Service and \nthe Create Order Saga\nNow that we’ve looked at various saga design and implementation issues, let’s see an\nexample. Figure 4.9 shows the design of Order Service. The service’s business logic\nconsists of traditional business logic classes, such as Order Service and the Order\nDeﬁnes the Restaurant Order\nService’s messaging API\nSends commands to\nsaga participants\nOrchestrator for the\nCreate Order Saga\nProcesses replies from\nsaga participants\nHandles commands sent by the the\nCreate Order Saga to the Order Service\nOrderServiceRequests\nAccountingServiceRequests\nConsumerServiceRequests\nKitchenServiceRequests\ncreateOrder()\ncancelOrder()\n...\nOrder Service\ncontroller\nOrder\ncommand\nhandlers\nCommand\nmessage\npublisher\nCreateOrderSagaReplies\nReply\nconsumer\nOrderService\nCreateOrder\nSaga\nOrderService\nProxy\nKitchenService\nProxy\nOrder\ncreateOrder()\ncancelOrder()\napproveOrder()\nrejectOrder()\n...\nOrderRepository\nsave()\nﬁndById()\n...\nFigure 4.9\nThe design of the Order Service and its sagas\n \n\n\n133\nThe design of the Order Service and the Create Order Saga\nentity. There are also saga orchestrator classes, including the CreateOrderSaga class,\nwhich orchestrates Create Order Saga. Also, because Order Service participates in its\nown sagas, it has an OrderCommandHandlers adapter class that handles command mes-\nsages by invoking OrderService.\n Some parts of Order Service should look familiar. As in a traditional application,\nthe core of the business logic is implemented by the OrderService, Order, and Order-\nRepository classes. In this chapter, I’ll briefly describe these classes. I describe them\nin more detail in chapter 5.\n What’s less familiar about Order Service are the saga-related classes. This service is\nboth a saga orchestrator and a saga participant. Order Service has several saga orches-\ntrators, such as CreateOrderSaga. The saga orchestrators send command messages to a\nsaga participant using a saga participant proxy class, such as KitchenServiceProxy and\nOrderServiceProxy. A saga participant proxy defines a saga participant’s messaging\nAPI. Order Service also has an OrderCommandHandlers class, which handles the com-\nmand messages sent by sagas to Order Service.\n Let’s look in more detail at the design, starting with the OrderService class.\n4.4.1\nThe OrderService class\nThe OrderService class is a domain service called by the service’s API layer. It’s\nresponsible for creating and managing orders. Figure 4.10 shows OrderService and\nsome of its collaborators. OrderService creates and updates Orders, invokes the\nOrderRepository to persist Orders, and creates sagas, such as the CreateOrderSaga,\nusing the SagaManager. The SagaManager class is one of the classes provided by the\nEventuate Tram Saga framework, which is a framework for writing saga orchestrators\nand participants, and is discussed a little later in this section.\ncreateOrder()\n...\nOrderService\nsave()\nﬁndOne()\n...\nOrderRepository\ncreate(..)\nSagaManager\nOrder\nCreateOrder\nSaga\nFigure 4.10\nOrderService creates and updates Orders, invokes the \nOrderRepository to persist Orders, and creates sagas, including the \nCreateOrderSaga.\n \n\n\n134\nCHAPTER 4\nManaging transactions with sagas\nI’ll discuss this class in more detail in chapter 5. For now, let’s focus on the create-\nOrder() method. The following listing shows OrderService’s createOrder() method.\nThis method first creates an Order and then creates an CreateOrderSaga to validate\nthe order.\n@Transactional\npublic class OrderService {\n@Autowired\nprivate SagaManager<CreateOrderSagaState> createOrderSagaManager;\n@Autowired\nprivate OrderRepository orderRepository;\n@Autowired\nprivate DomainEventPublisher eventPublisher;\npublic Order createOrder(OrderDetails orderDetails) {\n...\nResultWithEvents<Order> orderAndEvents = Order.createOrder(...);  \nOrder order = orderAndEvents.result;\norderRepository.save(order);\neventPublisher.publish(Order.class,\nLong.toString(order.getId()),\norderAndEvents.events);\nCreateOrderSagaState data =\nnew CreateOrderSagaState(order.getId(), orderDetails);\n  \ncreateOrderSagaManager.create(data, Order.class, order.getId());\nreturn order;\n}\n...\n}\nThe createOrder() method creates an Order by calling the factory method Order\n.createOrder(). It then persists the Order using the OrderRepository, which is a JPA-\nbased repository. It creates the CreateOrderSaga by calling SagaManager.create(),\npassing a CreateOrderSagaState containing the ID of the newly saved Order and the\nOrderDetails. The SagaManager instantiates the saga orchestrator, which causes it to\nsend a command message to the first saga participant, and persists the saga orchestra-\ntor in the database.\n Let’s look at the CreateOrderSaga and its associated classes. \nListing 4.1\nThe OrderService class and its createOrder() method\nEnsure that service \nmethods are transactional.\nCreate the\nOrder.\nPersist the Order \nin the database.\nPublish domain \nevents.\nCreate a\nCreateOrderSaga.\n \n\n\n135\nThe design of the Order Service and the Create Order Saga\n4.4.2\nThe implementation of the Create Order Saga\nFigure 4.11 shows the classes that implement the Create Order Saga. The responsibil-\nities of each class are as follows:\n\nCreateOrderSaga—A singleton class that defines the saga’s state machine. It\ninvokes the CreateOrderSagaState to create command messages and sends\nthem to participants using message channels specified by the saga participant\nproxy classes, such as KitchenServiceProxy.\nEventuate tram sagas\ncreate()\n...\nSagaManager\n«interface»\nSimpleSaga\nSagaDeﬁnition\nCommandEndpoint\nSagaDeﬁnition\ngetSagaDeﬁnition()\n«table»\nSAGA_INSTANCE\nEventuate tram\nUses\nImplements\nCreates\nManages\nInvokes\nOrder database\nCreateOrderSaga\nreplies\nOrderService\nrequests\nStores the state\nof saga instances\nThe state of a saga\nDescribes a\nmessage channel\nDescribes the\nsteps of a saga\nAbstract superclass\nfor saga\norchestrators\norderId\norderDetails\n...\nCreateOrder\nSagaState\nCreateOrder\nSaga\nKitchen\nServiceProxy\nthis.sagaDeﬁnition=\nstep()\n.withCompensation(...)\n.step()\n.invokeParticipant(...)\n.step()\n.invokeParticipant(...)\n.onReply(...)\n.withCompensation(...)\nOrderService\nProxy\nOrderService\nThe SagaManager handles persisting a saga,\nsending the command messages that it\ngenerates, subscribing to reply messages,\nand invoking the saga to handle replies.\nFigure 4.11\nThe OrderService's sagas, such as Create Order Saga, are implemented using \nthe Eventuate Tram Saga framework.\n \n\n\n136\nCHAPTER 4\nManaging transactions with sagas\n\nCreateOrderSagaState—A saga’s persistent state, which creates command\nmessages.\nSaga participant proxy classes, such as KitchenServiceProxy—Each proxy class\ndefines a saga participant’s messaging API, which consists of the command\nchannel, the command message types, and the reply types.\nThese classes are written using the Eventuate Tram Saga framework.\n The Eventuate Tram Saga framework provides a domain-specific language (DSL) for\ndefining a saga’s state machine. It executes the saga’s state machine and exchanges mes-\nsages with saga participants using the Eventuate Tram framework. The framework also\npersists the saga’s state in the database.\n Let’s take a closer look at the implementation of Create Order Saga, starting with\nthe CreateOrderSaga class.\nTHE CREATEORDERSAGA ORCHESTRATOR\nThe CreateOrderSaga class implements the state machine shown earlier in figure 4.7.\nThis class implements SimpleSaga, a base interface for sagas. The heart of the Create-\nOrderSaga class is the saga definition shown in the following listing. It uses the DSL\nprovided by the Eventuate Tram Saga framework to define the steps of the Create\nOrder Saga.\npublic class CreateOrderSaga implements SimpleSaga<CreateOrderSagaState> {\nprivate SagaDefinition<CreateOrderSagaState> sagaDefinition;\npublic CreateOrderSaga(OrderServiceProxy orderService,\nConsumerServiceProxy consumerService,\nKitchenServiceProxy kitchenService,\nAccountingServiceProxy accountingService) {\nthis.sagaDefinition =\nstep()\n.withCompensation(orderService.reject,\nCreateOrderSagaState::makeRejectOrderCommand)\n.step()\n.invokeParticipant(consumerService.validateOrder,\nCreateOrderSagaState::makeValidateOrderByConsumerCommand)\n.step()\n.invokeParticipant(kitchenService.create,\nCreateOrderSagaState::makeCreateTicketCommand)\n.onReply(CreateTicketReply.class,\nCreateOrderSagaState::handleCreateTicketReply)\n.withCompensation(kitchenService.cancel,\nCreateOrderSagaState::makeCancelCreateTicketCommand)\n.step()\n.invokeParticipant(accountingService.authorize,\nCreateOrderSagaState::makeAuthorizeCommand)\n.step()\n.invokeParticipant(kitchenService.confirmCreate,\nCreateOrderSagaState::makeConfirmCreateTicketCommand)\nListing 4.2\nThe definition of the CreateOrderSaga\n \n\n\n137\nThe design of the Order Service and the Create Order Saga\n.step()\n.invokeParticipant(orderService.approve,\nCreateOrderSagaState::makeApproveOrderCommand)\n.build();\n}\n@Override\npublic SagaDefinition<CreateOrderSagaState> getSagaDefinition() {\nreturn sagaDefinition;\n}\nThe CreateOrderSaga’s constructor creates the saga definition and stores it in the\nsagaDefinition field. The getSagaDefinition() method returns the saga definition.\n To see how CreateOrderSaga works, let’s look at the definition of the third step of\nthe saga, shown in the following listing. This step of the saga invokes the Kitchen Ser-\nvice to create a Ticket. Its compensating transaction cancels that Ticket. The\nstep(), invokeParticipant(), onReply(), and withCompensation() methods are\npart of the DSL provided by Eventuate Tram Saga.\npublic class CreateOrderSaga ...\npublic CreateOrderSaga(..., KitchenServiceProxy kitchenService,\n...) {\n...\n.step()\n.invokeParticipant(kitchenService.create,\nCreateOrderSagaState::makeCreateTicketCommand)\n.onReply(CreateTicketReply.class,\nCreateOrderSagaState::handleCreateTicketReply)\n.withCompensation(kitchenService.cancel,\n  \nCreateOrderSagaState::makeCancelCreateTicketCommand)\n...\n;\nThe call to invokeParticipant() defines the forward transaction. It creates the Create-\nTicket command message by calling CreateOrderSagaState.makeCreateTicket-\nCommand() and sends it to the channel specified by kitchenService.create. The call\nto onReply() specifies that CreateOrderSagaState.handleCreateTicketReply()\nshould be called when a successful reply is received from Kitchen Service. This\nmethod stores the returned ticketId in the CreateOrderSagaState. The call to\nwithCompensation() defines the compensating transaction. It creates a RejectTicket-\nCommand command message by calling CreateOrderSagaState.makeCancelCreate-\nTicket() and sends it to the channel specified by kitchenService.create.\n The other steps of the saga are defined in a similar fashion. The CreateOrder-\nSagaState creates each message, which is sent by the saga to the messaging endpoint\nListing 4.3\nThe definition of the third step of the saga\nDefine the forward \ntransaction.\nCall handleCreateTicketReply() when\na successful reply is received.\nDefine the compensating\ntransaction.\n \n\n\n138\nCHAPTER 4\nManaging transactions with sagas\ndefined by a KitchenServiceProxy. Let’s take a look at each of those classes, starting\nwith CreateOrderSagaState. \nTHE CREATEORDERSAGASTATE CLASS\nThe CreateOrderSagaState class, shown in the following listing, represents the state\nof a saga instance. An instance of this class is created by OrderService and is persisted\nin the database by the Eventuate Tram Saga framework. Its primary responsibility is to\ncreate the messages that are sent to saga participants.\npublic class CreateOrderSagaState {\nprivate Long orderId;\nprivate OrderDetails orderDetails;\nprivate long ticketId;\npublic Long getOrderId() {\nreturn orderId;\n}\nprivate CreateOrderSagaState() {\n}\npublic CreateOrderSagaState(Long orderId, OrderDetails orderDetails) {  \nthis.orderId = orderId;\nthis.orderDetails = orderDetails;\n}\nCreateTicket makeCreateTicketCommand() {\nreturn new CreateTicket(getOrderDetails().getRestaurantId(),\ngetOrderId(), makeTicketDetails(getOrderDetails()));\n}\nvoid handleCreateTicketReply(CreateTicketReply reply) {   \nlogger.debug(\"getTicketId {}\", reply.getTicketId());\nsetTicketId(reply.getTicketId());\n}\nCancelCreateTicket makeCancelCreateTicketCommand() {\n  \nreturn new CancelCreateTicket(getOrderId());\n}\n...\nThe CreateOrderSaga invokes the CreateOrderSagaState to create the command\nmessages. It sends those command messages to the endpoints defined by the Saga-\nParticipantProxy classes. Let’s take a look at one of those classes: KitchenService-\nProxy. \nListing 4.4\nCreateOrderSagaState stores the state of a saga instance\nInvoked by the\nOrderService to\ninstantiate a\nCreateOrderSagaState\nCreates a CreateTicket \ncommand message\nSaves the ID \nof the newly \ncreated Ticket\nCreates \nCancelCreateTicket \ncommand message\n \n\n\n139\nThe design of the Order Service and the Create Order Saga\nTHE KITCHENSERVICEPROXY CLASS\nThe KitchenServiceProxy class, shown in listing 4.5, defines the command message\nendpoints for Kitchen Service. There are three endpoints:\n\ncreate—Creates a Ticket\n\nconfirmCreate—Confirms the creation\n\ncancel—Cancels a Ticket\nEach CommandEndpoint specifies the command type, the command message’s destina-\ntion channel, and the expected reply types.\npublic class KitchenServiceProxy {\npublic final CommandEndpoint<CreateTicket> create =\nCommandEndpointBuilder\n.forCommand(CreateTicket.class)\n.withChannel(\nKitchenServiceChannels.kitchenServiceChannel)\n.withReply(CreateTicketReply.class)\n.build();\npublic final CommandEndpoint<ConfirmCreateTicket> confirmCreate =\nCommandEndpointBuilder\n.forCommand(ConfirmCreateTicket.class)\n.withChannel(\nKitchenServiceChannels.kitchenServiceChannel)\n.withReply(Success.class)\n.build();\npublic final CommandEndpoint<CancelCreateTicket> cancel =\nCommandEndpointBuilder\n.forCommand(CancelCreateTicket.class)\n.withChannel(\nKitchenServiceChannels.kitchenServiceChannel)\n.withReply(Success.class)\n.build();\n}\nProxy classes, such as KitchenServiceProxy, aren’t strictly necessary. A saga could sim-\nply send command messages directly to participants. But proxy classes have two import-\nant benefits. First, a proxy class defines static typed endpoints, which reduces the chance\nof a saga sending the wrong message to a service. Second, a proxy class is a well-defined\nAPI for invoking a service that makes the code easier to understand and test. For exam-\nple, chapter 10 describes how to write tests for KitchenServiceProxy that verify that\nOrder Service correctly invokes Kitchen Service. Without KitchenServiceProxy, it\nwould be impossible to write such a narrowly scoped test. \nListing 4.5\nKitchenServiceProxy defines the command message endpoints for \n Kitchen Service\n \n\n\n140\nCHAPTER 4\nManaging transactions with sagas\nTHE EVENTUATE TRAM SAGA FRAMEWORK\nThe Eventuate Tram Saga, shown in figure 4.12, is a framework for writing both saga\norchestrators and saga participants. It uses transactional messaging capabilities of Even-\ntuate Tram, discussed in chapter 3.\nThe saga orchestration package is the most complex part of the framework. It pro-\nvides SimpleSaga, a base interface for sagas, and a SagaManager class, which creates\nand manages saga instances. The SagaManager handles persisting a saga, sending the\ncommand messages that it generates, subscribing to reply messages, and invoking\nthe saga to handle replies. Figure 4.13 shows the sequence of events when OrderService\ncreates a saga. The sequence of events is as follows:\n1\nOrderService creates the CreateOrderSagaState.\n2\nIt creates an instance of a saga by invoking the SagaManager.\n3\nThe SagaManager executes the first step of the saga definition.\n4\nThe CreateOrderSagaState is invoked to generate a command message.\nParticipant\nOrchestration\ncreate(sagaState)\n...\nSagaManager\nSimpleSaga\nSagaDeﬁnition\nCommandEndpoint\nSagaCommand\nDispatcher\nSagaCommand\nHandlersBuilder\nSagaDeﬁnition\ngetSagaDeﬁnition()\n«table»\nSAGA_INSTANCE\nEventuate tram\nEventuate tram saga framework\nUses\nSends\nand receives\nOrder database\nChannels\nThe SagaManager handles persisting a\nsaga, sending the command messages\nthat it generates, subscribing to reply\nmessages, and invoking the saga to\nhandle replies.\nAbstract superclass\nfor saga orchestrators\nDescribes a\nmessage channel\nRoutes command\nmessages to\nmessage handlers\nDescribes the\nsteps of a saga\nStores the state of\nsaga instances\nFigure 4.12\nEventuate Tram Saga is a framework for writing both saga orchestrators and saga \nparticipants.\n \n\n\n141\nThe design of the Order Service and the Create Order Saga\n5\nThe SagaManager sends the command message to the saga participant (the\nConsumer Service).\n6\nThe SagaManager saves the saga instance in the database.\nFigure 4.14 shows the sequence of events when SagaManager receives a reply from\nConsumer Service.\nThe sequence of events is as follows:\n1\nEventuate Tram invokes SagaManager with the reply from Consumer Service.\n2\nSagaManager retrieves the saga instance from the database.\n3\nSagaManager executes the next step of the saga definition.\nOrderService\nCreateOrderSagaState\nSagaManager\nCreateOrderSaga\nSagaDeﬁnition\nEventuateTram\nDatabase\nnew()\ncreate(sagaState)\ngetSagaDeﬁnition()\nexecuteFirstStep(sagaState)\nmakeValidateOrderByConsumerCommand()\nsendMessage(command)\nsaveSagaInstance(sagaState)\nFigure 4.13\nThe sequence of events when OrderService creates an instance of Create Order Saga\nCreateOrderSagaState\nSagaManager\nCreateOrderSaga\nSagaDeﬁnition\nEventuateTram\nDatabase\nhandleMessage()\nloadSagaInstance()\ngetSagaDeﬁnition()\nexecuteFirstStep(sagaState)\nmakeValidateOrderByConsumerCommand()\nsendMessage\n(command)\nsaveSagaInstance\n(sageState)\nFigure 4.14\nThe sequence of events when the SagaManager receives a reply message from a saga participant\n \n\n\n142\nCHAPTER 4\nManaging transactions with sagas\n4\nCreateOrderSagaState is invoked to generate a command message.\n5\nSagaManager sends the command message to the specified saga participant\n(Kitchen Service).\n6\nSagaManager saves the update saga instance in the database.\nIf a saga participant fails, SagaManager executes the compensating transactions in\nreverse order.\n The other part of the Eventuate Tram Saga framework is the saga participant\npackage. It provides the SagaCommandHandlersBuilder and SagaCommandDispatcher\nclasses for writing saga participants. These classes route command messages to han-\ndler methods, which invoke the saga participants’ business logic and generate reply\nmessages. Let’s take a look at how these classes are used by Order Service. \n4.4.3\nThe OrderCommandHandlers class\nOrder Service participates in its own sagas. For example, CreateOrderSaga invokes\nOrder Service to either approve or reject an Order. The OrderCommandHandlers class,\nshown in figure 4.15, defines the handler methods for the command messages sent by\nthese sagas.\n Each handler method invokes OrderService to update an Order and makes a\nreply message. The SagaCommandDispatcher class routes the command messages to\nthe appropriate handler method and sends the reply.\napproveOrder()\nrejectOrder()\n...\nOrderCommandHandlers\nEventuate\nTram Sagas\napproveOrder()\nrejectOrder()\n...\nOrderService\nInvokes\nInvokes\nUses\nReads\nSends\nSagaCommand\nDispatcher\nEventuate tram\nOrderService\nrequests\nCreateOrderSaga\nreplies\nRoutes command messages to\nhandlers and sends replies\nFigure 4.15\nOrderCommandHandlers implements command handlers for the commands that are \nsent by the various Order Service sagas.\n \n\n\n143\nThe design of the Order Service and the Create Order Saga\nThe following listing shows the OrderCommandHandlers class. Its commandHandlers()\nmethod maps command message types to handler methods. Each handler method\ntakes a command message as a parameter, invokes OrderService, and returns a reply\nmessage.\npublic class OrderCommandHandlers {\n@Autowired\nprivate OrderService orderService;\npublic CommandHandlers commandHandlers() {\n  \nreturn SagaCommandHandlersBuilder\n.fromChannel(\"orderService\")\n.onMessage(ApproveOrderCommand.class, this::approveOrder)\n.onMessage(RejectOrderCommand.class, this::rejectOrder)\n...\n.build();\n}\npublic Message approveOrder(CommandMessage<ApproveOrderCommand> cm) {\nlong orderId = cm.getCommand().getOrderId();\norderService.approveOrder(orderId);\nreturn withSuccess();\n}\npublic Message rejectOrder(CommandMessage<RejectOrderCommand> cm) {\nlong orderId = cm.getCommand().getOrderId();\norderService.rejectOrder(orderId);\nreturn withSuccess();\n}\nThe approveOrder() and rejectOrder() methods update the specified Order by\ninvoking OrderService. The other services that participate in sagas have similar com-\nmand handler classes that update their domain objects. \n4.4.4\nThe OrderServiceConfiguration class\nThe Order Service uses the Spring framework. The following listing is an excerpt of\nthe OrderServiceConfiguration class, which is an @Configuration class that instanti-\nates and wires together the Spring @Beans.\n@Configuration\npublic class OrderServiceConfiguration {\n@Bean\npublic OrderService orderService(RestaurantRepository restaurantRepository,\nListing 4.6\nThe command handlers for Order Service\nListing 4.7\nThe OrderServiceConfiguration is a Spring @Configuration\n class that defines the Spring @Beans for the Order Service.\nRoute each command \nmessage to the appropriate \nhandler method.\nChange the state \nof the Order to \nauthorized.\nReturn a generic \nsuccess message.\nChange the state of \nthe Order to rejected.\n \n",
      "page_number": 155
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 169-189)",
      "start_page": 169,
      "end_page": 189,
      "detection_method": "topic_boundary",
      "content": "144\nCHAPTER 4\nManaging transactions with sagas\n...\nSagaManager<CreateOrderSagaState>\ncreateOrderSagaManager,\n...) {\nreturn new OrderService(restaurantRepository,\n...\ncreateOrderSagaManager\n...);\n}\n@Bean\npublic SagaManager<CreateOrderSagaState> createOrderSagaManager(CreateOrderS\naga saga) {\nreturn new SagaManagerImpl<>(saga);\n}\n@Bean\npublic CreateOrderSaga createOrderSaga(OrderServiceProxy orderService,\nConsumerServiceProxy consumerService,\n...) {\nreturn new CreateOrderSaga(orderService, consumerService, ...);\n}\n@Bean\npublic OrderCommandHandlers orderCommandHandlers() {\nreturn new OrderCommandHandlers();\n}\n@Bean\npublic SagaCommandDispatcher\norderCommandHandlersDispatcher(OrderCommandHan\ndlers orderCommandHandlers) {\nreturn new SagaCommandDispatcher(\"orderService\", orderCommandHandlers.comma\nndHandlers());\n}\n@Bean\npublic KitchenServiceProxy kitchenServiceProxy() {\nreturn new KitchenServiceProxy();\n}\n@Bean\npublic OrderServiceProxy orderServiceProxy() {\nreturn new OrderServiceProxy();\n}\n...\n}\nThis class defines several Spring @Beans including orderService, createOrder-\nSagaManager, createOrderSaga, orderCommandHandlers, and orderCommandHandlers-\nDispatcher. It also defines Spring @Beans for the various proxy classes, including\nkitchenServiceProxy and orderServiceProxy.\n \n\n\n145\nSummary\n CreateOrderSaga is only one of Order Service’s many sagas. Many of its other sys-\ntem operations also use sagas. For example, the cancelOrder() operation uses a Can-\ncel Order Saga, and the reviseOrder() operation uses a Revise Order Saga. As a\nresult, even though many services have an external API that uses a synchronous proto-\ncol, such as REST or gRPC, a large amount of interservice communication will use\nasynchronous messaging.\n As you can see, transaction management and some aspects of business logic design\nare quite different in a microservice architecture. Fortunately, saga orchestrators are\nusually quite simple state machines, and you can use a saga framework to simplify your\ncode. Nevertheless, transaction management is certainly more complicated than in a\nmonolithic architecture. But that’s usually a small price to pay for the tremendous\nbenefits of microservices. \nSummary\nSome system operations need to update data scattered across multiple services.\nTraditional, XA/2PC-based distributed transactions aren’t a good fit for mod-\nern applications. A better approach is to use the Saga pattern. A saga is sequence\nof local transactions that are coordinated using messaging. Each local transac-\ntion updates data in a single service. Because each local transaction commits its\nchanges, if a saga must roll back due to the violation of a business rule, it must\nexecute compensating transactions to explicitly undo changes.\nYou can use either choreography or orchestration to coordinate the steps of a\nsaga. In a choreography-based saga, a local transaction publishes events that trig-\nger other participants to execute local transactions. In an orchestration-based\nsaga, a centralized saga orchestrator sends command messages to participants\ntelling them to execute local transactions. You can simplify development and test-\ning by modeling saga orchestrators as state machines. Simple sagas can use chore-\nography, but orchestration is usually a better approach for complex sagas.\nDesigning saga-based business logic can be challenging because, unlike ACID\ntransactions, sagas aren’t isolated from one another. You must often use counter-\nmeasures, which are design strategies that prevent concurrency anomalies\ncaused by the ACD transaction model. An application may even need to use\nlocking in order to simplify the business logic, even though that risks deadlocks. \n \n\n\n146\nDesigning\nbusiness logic in\na microservice architecture\nThe heart of an enterprise application is the business logic, which implements the\nbusiness rules. Developing complex business logic is always challenging. The FTGO\napplication’s business logic implements some quite complex business logic, espe-\ncially for order management and delivery management. Mary had encouraged her\nteam to apply object-oriented design principles, because in her experience this was\nthe best way to implement complex business logic. Some of the business logic used\nthe procedural Transcription script pattern. But the majority of the FTGO applica-\ntion’s business logic is implemented in an object-oriented domain model that’s\nmapped to the database using JPA.\n Developing complex business logic is even more challenging in a microservice\narchitecture where the business logic is spread over multiple services. You need to\nThis chapter covers\nApplying the business logic organization patterns: \nTransaction script pattern and Domain model \npattern\nDesigning business logic with the Domain-driven \ndesign (DDD) aggregate pattern\nApplying the Domain event pattern in a \nmicroservice architecture\n \n\n\n147\nBusiness logic organization patterns\naddress two key challenges. First, a typical domain model is a tangled web of intercon-\nnected classes. Although this isn’t a problem in a monolithic application, in a micro-\nservice architecture, where classes are scattered around different services, you need to\neliminate object references that would otherwise span service boundaries. The second\nchallenge is designing business logic that works within the transaction management\nconstraints of a microservice architecture. Your business logic can use ACID transac-\ntions within services, but as described in chapter 4, it must use the Saga pattern to\nmaintain data consistency across services.\n Fortunately, we can address these issues by using the Aggregate pattern from\nDDD. The Aggregate pattern structures a service’s business logic as a collection of\naggregates. An aggregate is a cluster of objects that can be treated as a unit. There are\ntwo reasons why aggregates are useful when developing business logic in a micro-\nservice architecture:\nAggregates avoid any possibility of object references spanning service boundar-\nies, because an inter-aggregate reference is a primary key value rather than an\nobject reference.\nBecause a transaction can only create or update a single aggregate, aggregates\nfit the constraints of the microservices transaction model.\nAs a result, an ACID transaction is guaranteed to be within a single service.\n I begin this chapter by describing the different ways of organizing business logic:\nthe Transcription script pattern and the Domain model pattern. Next I introduce the\nconcept of a DDD aggregate and explain why it’s a good building block for a service’s\nbusiness logic. After that, I describe the Domain event pattern events and explain why\nit’s useful for a service to publish events. I end this chapter with a couple of examples\nof business logic from Kitchen Service and Order Service.\n Let’s now look at business logic organization patterns.\n5.1\nBusiness logic organization patterns\nFigure 5.1 shows the architecture of a typical service. As described in chapter 2, the\nbusiness logic is the core of a hexagonal architecture. Surrounding the business logic\nare the inbound and outbound adapters. An inbound adapter handles requests from cli-\nents and invokes the business logic. An outbound adapter, which is invoked by the busi-\nness logic, invokes other services and applications.\n This service consists of the business logic and the following adapters:\n\nREST API adapter—An inbound adapter that implements a REST API which\ninvokes the business logic\n\nOrderCommandHandlers—An inbound adapter that consumes command mes-\nsages from a message channel and invokes the business logic\n\nDatabase Adapter—An outbound adapter that’s invoked by the business logic\nto access the database\n\nDomain Event Publishing Adapter—An outbound adapter that publishes events\nto a message broker\n \n\n\n148\nCHAPTER 5\nDesigning business logic in a microservice architecture\nThe business logic is typically the most complex part of the service. When develop-\ning business logic, you should consciously organize your business logic in the way\nthat’s most appropriate for your application. After all, I’m sure you’ve experienced\nthe frustration of having to maintain someone else’s badly structured code. Most\nenterprise applications are written in an object-oriented language such as Java, so\nthey consist of classes and methods. But using an object-oriented language doesn’t\nguarantee that the business logic has an object-oriented design. The key decision you\nmust make when developing business logic is whether to use an object-oriented\napproach or a procedural approach. There are two main patterns for organizing\nOutbound adapters\nInbound adapters\nOrder\nService requests\nPOST/orders\nGET/order/Id\nREST API\nOrder database\nOrder Service\nbusiness logic\nOrder\ncommand\nhandlers\nOrder events\nDomain event\npublisher adapter\nDatabase\nadapter\nFigure 5.1\nThe Order Service has a hexagonal architecture. It consists of the business logic \nand one or more adapters that interface with external applications and other services.\n \n\n\n149\nBusiness logic organization patterns\nbusiness logic: the procedural Transaction script pattern, and the object-oriented\nDomain model pattern.\n5.1.1\nDesigning business logic using the Transaction script pattern\nAlthough I’m a strong advocate of the object-oriented approach, there are some situa-\ntions where it is overkill, such as when you are developing simple business logic. In such\na situation, a better approach is to write procedural code and use what the book Patterns\nof Enterprise Application Architecture by Martin Fowler (Addison-Wesley Professional, 2002)\ncalls the Transaction script pattern. Rather than doing any object-oriented design, you\nwrite a method called a transaction script to handle each request from the presentation\ntier. As figure 5.2 shows, an important characteristic of this approach is that the classes\nthat implement behavior are separate from those that store state.\nWhen using the Transaction script pattern, the scripts are usually located in service\nclasses, which in this example is the OrderService class. A service class has one\nmethod for each request/system operation. The method implements the business\nlogic for that request. It accesses the database using data access objects (DAOs), such\nas the OrderDao. The data objects, which in this example is the Order class, are pure\ndata with little or no behavior.\nThis style of design is highly procedural and relies on few of the capabilities of object-\noriented programming (OOP) languages. This what you would create if you were writ-\ning the application in C or another non-OOP language. Nevertheless, you shouldn’t be\nPattern: Transaction script\nOrganize the business logic as a collection of procedural transaction scripts, one for\neach type of request.\ncreateOrder()\nreviseOrder()\ncancelOrder()\n...\nOrderService\nClasses with\nbehavior\nClasses\nwith state\nsave(Order)\nﬁndOrderById()\n...\nOrderDao\norderId\norderLineItems\n...\nOrder\nFigure 5.2\nOrganizing business logic \nas transaction scripts. In a typical \ntransaction script–based design, one \nset of classes implements behavior \nand another set stores state. The \ntransaction scripts are organized into \nclasses that typically have no state. \nThe scripts use data classes, which \ntypically have no behavior.\n \n\n\n150\nCHAPTER 5\nDesigning business logic in a microservice architecture\nashamed to use a procedural design when it’s appropriate. This approach works well\nfor simple business logic. The drawback is that this tends not to be a good way to\nimplement complex business logic. \n5.1.2\nDesigning business logic using the Domain model pattern\nThe simplicity of the procedural approach can be quite seductive. You can write code with-\nout having to carefully consider how to organize the classes. The problem is that if your\nbusiness logic becomes complex, you can end up with code that’s a nightmare to main-\ntain. In fact, in the same way that a monolithic application has a habit of continually grow-\ning, transaction scripts have the same problem. Consequently, unless you’re writing an\nextremely simple application, you should resist the temptation to write procedural code\nand instead apply the Domain model pattern and develop an object-oriented design.\nIn an object-oriented design, the business logic consists of an object model, a network\nof relatively small classes. These classes typically correspond directly to concepts from\nthe problem domain. In such a design some classes have only either state or behavior,\nbut many contain both, which is the hallmark of a well-designed class. Figure 5.3\nshows an example of the Domain model pattern.\nPattern: Domain model\nOrganize the business logic as an object model consisting of classes that have state\nand behavior.\ncreateOrder()\nreviseOrder()\ncancelOrder()\n...\nOrderService\ndeliveryTime\ndeliveryAddress\nDeliveryInformation\nﬁndOrderById()\n...\nOrderRepository\nSome classes have only state.\nMany classes have\nstate and behavior.\nUses\nSome classes have only behavior.\n«private»\norderId\norderLineItems\n...\nrevise()\ncancel()\n«static»\ncreate()\nOrder\nFigure 5.3\nOrganizing business logic as a domain model. The majority of \nthe business logic consists of classes that have state and behavior.\n \n\n\n151\nBusiness logic organization patterns\nAs with the Transaction script pattern, an OrderService class has a method for each\nrequest/system operation. But when using the Domain model pattern, the service\nmethods are usually simple. That’s because a service method almost always delegates\nto persistent domain objects, which contain the bulk of the business logic. A service\nmethod might, for example, load a domain object from the database and invoke one\nof its methods. In this example, the Order class has both state and behavior. Moreover,\nits state is private and can only be accessed indirectly via its methods.\n Using an object-oriented design has a number of benefits. First, the design is\neasy to understand and maintain. Instead of consisting of one big class that does\neverything, it consists of a number of small classes that each have a small number of\nresponsibilities. In addition, classes such as Account, BankingTransaction, and\nOverdraftPolicy closely mirror the real world, which makes their role in the design\neasier to understand. Second, our object-oriented design is easier to test: each class\ncan and should be tested independently. Finally, an object-oriented design is easier to\nextend because it can use well-known design patterns, such as the Strategy pattern and\nthe Template method pattern, that define ways of extending a component without\nmodifying the code.\n The Domain model pattern works well, but there are a number of problems with\nthis approach, especially in a microservice architecture. To address those problems,\nyou need to use a refinement of OOD known as DDD. \n5.1.3\nAbout Domain-driven design\nDDD, which is described in the book Domain-Driven Design by Eric Evans (Addison-\nWesley Professional, 2003), is a refinement of OOD and is an approach for developing\ncomplex business logic. I introduced DDD in chapter 2 when discussing the useful-\nness of DDD subdomains when decomposing an application into services. When using\nDDD, each service has its own domain model, which avoids the problems of a single,\napplication-wide domain model. Subdomains and the associated concept of Bounded\nContext are two of the strategic DDD patterns.\n DDD also has some tactical patterns that are building blocks for domain models.\nEach pattern is a role that a class plays in a domain model and defines the characteris-\ntics of the class. The building blocks that have been widely adopted by developers\ninclude the following:\nEntity—An object that has a persistent identity. Two entities whose attributes\nhave the same values are still different objects. In a Java EE application, classes\nthat are persisted using JPA @Entity are usually DDD entities.\nValue object—An object that is a collection of values. Two value objects whose\nattributes have the same values can be used interchangeably. An example of a\nvalue object is a Money class, which consists of a currency and an amount.\nFactory—An object or method that implements object creation logic that’s too\ncomplex to be done directly by a constructor. It can also hide the concrete\n \n\n\n152\nCHAPTER 5\nDesigning business logic in a microservice architecture\nclasses that are instantiated. A factory might be implemented as a static method\nof a class.\nRepository—An object that provides access to persistent entities and encapsu-\nlates the mechanism for accessing the database.\nService—An object that implements business logic that doesn’t belong in an\nentity or a value object.\nThese building blocks are used by many developers. Some are supported by frame-\nworks such as JPA and the Spring framework. There is one more building block that\nhas been generally ignored (myself included!) except by DDD purists: aggregates. As\nit turns out, aggregates are an extremely useful concept when developing microser-\nvices. Let’s first look at some subtle problems with classic OOD that are solved by\nusing aggregates. \n5.2\nDesigning a domain model using the \nDDD aggregate pattern\nIn traditional object-oriented design, a domain model is a collection of classes and\nrelationships between classes. The classes are usually organized into packages. For\nexample, figure 5.4 shows part of a domain model for the FTGO application. It’s a typ-\nical domain model consisting of a web of interconnected classes.\nThis example has several classes corresponding to business objects: Consumer, Order,\nRestaurant, and Courier. But interestingly, the explicit boundaries of each business\nobject are missing from this kind of traditional domain model. It doesn’t specify, for\nConsumer\nOrder\nstate\n...\ncreditcardId\n...\ndeliveryTime\nquantity\nname\nprice\nstreet1\nstreet2\ncity\nstate\nzip\nname\n...\navailable\n...\nlat\nlon\nRestaurant\nCourier\nLocation\nPaymentInfo\nDeliveryInfo\nOrderLineItem\nMenuItem\nAddress\nPlaced by\nFor\nAssigned to\nPaid using\nPays using\nFigure 5.4\nA traditional domain model is a web of interconnected classes. It doesn’t explicitly specify the \nboundaries of business objects, such as Consumer and Order.\n \n\n\n153\nDesigning a domain model using the DDD aggregate pattern\nexample, which classes are part of the Order business object. This lack of boundaries\ncan sometimes cause problems, especially in microservice architecture.\n I begin this section with an example problem caused by the lack of explicit bound-\naries. Next I describe the concept of an aggregate and how it has explicit boundaries.\nAfter that, I describe the rules that aggregates must obey and how they make aggre-\ngates a good fit for the microservice architecture. I then describe how to carefully\nchoose the boundaries of your aggregates and why it matters. Finally, I discuss how to\ndesign business logic using aggregates. Let’s first take a look at the problems caused\nby fuzzy boundaries.\n5.2.1\nThe problem with fuzzy boundaries\nImagine, for example, that you want to perform an operation, such as a load or delete,\non an Order business object. What exactly does that mean? What is the scope an oper-\nation? You would certainly load or delete the Order object. But in reality there’s more\nto an Order than simply the Order object. There are also the order line items, the pay-\nment information, and so on. Figure 5.4 leaves the boundaries of a domain object to\nthe developer’s intuition.\n Besides a conceptual fuzziness, the lack of explicit boundaries causes problems\nwhen updating a business object. A typical business object has invariants, business\nrules that must be enforced at all times. An Order has a minimum order amount, for\nexample. The FTGO application must ensure that any attempt to update an order\ndoesn’t violate an invariant such as the minimum order amount. The challenge is that\nin order to enforce invariants, you must design your business logic carefully.\n For example, let’s look at how to ensure the order minimum is met when multiple\nconsumers work together to create an order. Two consumers—Sam and Mary—are\nworking together on an order and simultaneously decide that the order exceeds their\nbudget. Sam reduces the quantity of samosas, and Mary reduces the quantity of naan\nbread. From the application’s perspective, both consumers retrieve the order and its\nline items from the database. Both consumers then update a line item to reduce the\ncost of the order. From each consumer’s perspective the order minimum is preserved.\nHere’s the sequence of database transactions.\nConsumer - Mary\nConsumer - Sam\nBEGIN TXN\nSELECT ORDER_TOTAL FROM ORDER\nWHERE ORDER ID = X\nSELECT * FROM ORDER_LINE_ITEM\nWHERE ORDER_ID = X\n...\nEND TXN\nBEGIN TXN\nSELECT ORDER_TOTAL FROM ORDER\nWHERE ORDER ID = X\nSELECT * FROM ORDER_LINE_ITEM\nWHERE ORDER_ID = X\n...\nEND TXN\nVerify minimum is met\n \n\n\n154\nCHAPTER 5\nDesigning business logic in a microservice architecture\nEach consumer changes a line item using a sequence of two transactions. The first\ntransaction loads the order and its line items. The UI verifies that the order minimum\nis satisfied before executing the second transaction. The second transaction updates\nthe line item quantity using an optimistic offline locking check that verifies that the\norder line is unchanged since it was loaded by the first transaction.\n In this scenario, Sam reduces the order total by $X and Mary reduces it by $Y. As a\nresult, the Order is no longer valid, even though the application verified that the order\nstill satisfied the order minimum after each consumer’s update. As you can see, directly\nupdating part of a business object can result in the violation of the business rules. DDD\naggregates are intended to solve this problem. \n5.2.2\nAggregates have explicit boundaries\nAn aggregate is a cluster of domain objects within a boundary that can be treated as a\nunit. It consists of a root entity and possibly one or more other entities and value\nobjects. Many business objects are modeled as aggregates. For example, in chapter 2\nwe created a rough domain model by analyzing the nouns used in the requirements\nand by domain experts. Many of these nouns, such as Order, Consumer, and Restau-\nrant, are aggregates.\nFigure 5.5 shows the Order aggregate and its boundary. An Order aggregate consists of\nan Order entity, one or more OrderLineItem value objects, and other value objects\nsuch as a delivery Address and PaymentInformation.\nBEGIN TXN\nUPDATE ORDER_LINE_ITEM\nSET VERSION=..., QUANTITY=...\nWHERE VERSION = <loaded version>\nAND ID = ...\nEND TXN\nVerify minimum is met\nBEGIN TXN\nUPDATE ORDER_LINE_ITEM\nSET VERSION=..., QUANTITY=...\nWHERE VERSION = <loaded version>\nAND ID = ...\nEND TXN\nPattern: Aggregate\nOrganize a domain model as a collection of aggregates, each of which is a graph of\nobjects that can be treated as a unit.\n \n\n\n155\nDesigning a domain model using the DDD aggregate pattern\nAggregates decompose a domain model into chunks, which are individually easier to\nunderstand. They also clarify the scope of operations such as load, update, and delete.\nThese operations act on the entire aggregate rather than on parts of it. An aggregate\nis often loaded in its entirety from the database, thereby avoiding any complications of\nlazy loading. Deleting an aggregate removes all of its objects from a database.\nAGGREGATES ARE CONSISTENCY BOUNDARIES\nUpdating an entire aggregate rather than its parts solves the consistency issues, such\nas the example described earlier. Update operations are invoked on the aggregate\nroot, which enforces invariants. Also, concurrency is handled by locking the aggregate\nroot using, for example, a version number or a database-level lock. For example,\ninstead of updating line items’ quantities directly, a client must invoke a method on\nthe root of the Order aggregate, which enforces invariants such as the minimum order\namount. Note, though, that this approach doesn’t require the entire aggregate to be\nupdated in the database. An application might, for example, only update the rows cor-\nresponding to the Order object and the updated OrderLineItem. \nIDENTIFYING AGGREGATES IS KEY\nIn DDD, a key part of designing a domain model is identifying aggregates, their\nboundaries, and their roots. The details of the aggregates’ internal structure is sec-\nondary. The benefit of aggregates, however, goes far beyond modularizing a domain\nmodel. That’s because aggregates must obey certain rules. \n5.2.3\nAggregate rules\nDDD requires aggregates to obey a set of rules. These rules ensure that an aggregate is\na self-contained unit that can enforce its invariants. Let’s look at each of the rules.\n«\n»\naggregate root\nOrder\nquantity\n«\n»\naggregate root\nConsumer\nOrder aggregate\nConsumer aggregate\n...\n«\n»\naggregate root\nRestaurant\n«\n»\nvalue object\nOrderLineItem\nRestaurant aggregate\n«\n»\nvalue object\nDeliveryInfo\n«\n»\nvalue object\nPaymentInfo\n«\n»\nvalue object\nDeliveryInfo\n«\n»\nvalue object\nPaymentInfo\nFigure 5.5\nStructuring a domain model as a set of aggregates makes the boundaries explicit.\n \n\n\n156\nCHAPTER 5\nDesigning business logic in a microservice architecture\nRULE #1: REFERENCE ONLY THE AGGREGATE ROOT\nThe previous example illustrated the perils of updating OrderLineItems directly. The\ngoal of the first aggregate rule is to eliminate this problem. It requires that the root\nentity be the only part of an aggregate that can be referenced by classes outside of the\naggregate. A client can only update an aggregate by invoking a method on the aggre-\ngate root.\n A service, for example, uses a repository to load an aggregate from the database\nand obtain a reference to the aggregate root. It updates an aggregate by invoking a\nmethod on the aggregate root. This rule ensures that the aggregate can enforce its\ninvariant.\nRULE #2: INTER-AGGREGATE REFERENCES MUST USE PRIMARY KEYS\nAnother rule is that aggregates reference each other by identity (for example, primary\nkey) instead of object references. For example, as figure 5.6 shows, an Order refer-\nences its Consumer using a consumerId rather than a reference to the Consumer object.\nSimilarly, an Order references a Restaurant using a restaurantId.\nThis approach is quite different from traditional object modeling, which considers\nforeign keys in the domain model to be a design smell. It has a number of benefits.\nThe use of identity rather than object references means that the aggregates are loosely\ncoupled. It ensures that the aggregate boundaries between aggregates are well\ndefined and avoids accidentally updating a different aggregate. Also, if an aggregate is\npart of another service, there isn’t a problem of object references that span services.\n This approach also simplifies persistence since the aggregate is the unit of storage.\nIt makes it easier to store aggregates in a NoSQL database such as MongoDB. It also\nconsumerId\nrestaurantId\n...\n«\n»\naggregate root\nOrder\nquantity\nOrderLineItem\nDeliveryInfo\n...\n«\n»\naggregate root\nConsumer\nOrder aggregate\nConsumer aggregate\n...\n«\n»\naggregate root\nRestaurant\nRestaurant aggregate\nPaymentInfo\nDeliveryInfo\nPaymentInfo\nFigure 5.6\nReferences between aggregates are by primary key rather than by object reference. The \nOrder aggregate has the IDs of the Consumer and Restaurant aggregates. Within an aggregate, \nobjects have references to one another.\n \n\n\n157\nDesigning a domain model using the DDD aggregate pattern\neliminates the need for transparent lazy loading and its associated problems. Scaling\nthe database by sharding aggregates is relatively straightforward.\nRULE #3: ONE TRANSACTION CREATES OR UPDATES ONE AGGREGATE\nAnother rule that aggregates must obey is that a transaction can only create or update\na single aggregate. When I first read about it many years ago, this rule made no sense! At\nthe time, I was developing traditional monolithic applications that used an RDBMS, so\ntransactions could update multiple aggregates. Today, this constraint is perfect for the\nmicroservice architecture. It ensures that a transaction is contained within a service.\nThis constraint also matches the limited transaction model of most NoSQL databases.\n This rule makes it more complicated to implement operations that need to create\nor update multiple aggregates. But this is exactly the problem that sagas (described in\nchapter 4) are designed to solve. Each step of the saga creates or updates exactly one\naggregate. Figure 5.7 shows how this works.\nIn this example, the saga consists of three transactions. The first transaction updates\naggregate X in service A. The other two transactions are both in service B. One transac-\ntion updates aggregate X, and the other updates aggregate Y.\n An alternative approach to maintaining consistency across multiple aggregates\nwithin a single service is to cheat and update multiple aggregates within a transaction.\nFor example, service B could update aggregates Y and Z in a single transaction. This is\nonly possible when using a database, such as an RDBMS, that supports a rich transac-\ntion model. If you’re using a NoSQL database that only has simple transactions,\nthere’s no other option except to use sagas.\n Or is there? It turns out that aggregate boundaries are not set in stone. When\ndeveloping a domain model, you get to choose where the boundaries lie. But like a\n20th century colonial power drawing national boundaries, you need to be careful. \nService A\nSaga\nLocal transaction 1\nCreate/update\nAggregate X\nService B\nLocal transaction 2\nAggregate Y\nLocal transaction 3\nAggregate Z\nCreate/update\nCreate/update\nFigure 5.7\nA transaction can only create or update a single aggregate, so an application uses a saga \nto update multiple aggregates. Each step of the saga creates or updates one aggregate.\n \n\n\n158\nCHAPTER 5\nDesigning business logic in a microservice architecture\n5.2.4\nAggregate granularity\nWhen developing a domain model, a key decision you must make is how large to\nmake each aggregate. On one hand, aggregates should ideally be small. Because\nupdates to each aggregate are serialized, more fine-grained aggregates will increase\nthe number of simultaneous requests that the application can handle, improving scal-\nability. It will also improve the user experience because it reduces the chance of two\nusers attempting conflicting updates of the same aggregate. On the other hand, because\nan aggregate is the scope of transaction, you may need to define a larger aggregate in\norder to make a particular update atomic.\n For example, earlier I mentioned how in the FTGO application’s domain model\nOrder and Consumer are separate aggregates. An alternative design is to make Order\npart of the Consumer aggregate. Figure 5.8 shows this alternative design.\nA benefit of this larger Consumer aggregate is that the application can atomically\nupdate a Consumer and one or more of its Orders. A drawback of this approach is that\nit reduces scalability. Transactions that update different orders for the same customer\nwould be serialized. Similarly, two users would conflict if they attempted to edit differ-\nent orders for the same customer.\n Another drawback of this approach in a microservice architecture is that it is an\nobstacle to decomposition. The business logic for Orders and Consumers, for exam-\nple, must be collocated in the same service, which makes the service larger. Because of\nthese issues, making aggregates as fine-grained as possible is best. \nrestaurantId\n...\nOrder\nquantity\nOrderLineItem\nDeliveryInfo\n...\n<<aggregate root>>\nConsumer\nConsumer aggregate\n...\n<<aggregate root>>\nRestaurant\nRestaurant aggregate\nPaymentInfo\nDeliveryInfo\nPaymentInfo\nFigure 5.8\nAn alternative design defines a Customer aggregate that contains the Customer and \nOrder classes. This design enables an application to atomically update a Consumer and one or more \nof its Orders.\n \n\n\n159\nDesigning a domain model using the DDD aggregate pattern\n5.2.5\nDesigning business logic with aggregates\nIn a typical (micro)service, the bulk of the business logic consists of aggregates. The\nrest of the business logic resides in the domain services and the sagas. The sagas orches-\ntrate sequences of local transactions in order to enforce data consistency. The services\nare the entry points into the business logic and are invoked by inbound adapters. A\nservice uses a repository to retrieve aggregates from the database or save aggregates to\nthe database. Each repository is implemented by an outbound adapter that accesses\nthe database. Figure 5.9 shows the aggregate-based design of the business logic for the\nOrder Service.\nThe business logic consists of the Order aggregate, the OrderService service class, the\nOrderRepository, and one or more sagas. The OrderService invokes the Order-\nRepository to save and load Orders. For simple requests that are local to the service,\nREST API\nDomain\nevent\npublisher\n«service»\nOrderService\n«saga»\nCreateOrder\nSaga\n«saga»\nReviseOrder\nSaga\ncreateOrder()\nreviseOrder()\ncancelOrder()\n«value object»\nOrderLineItem\nquantity\nmenuItem\nname\nOrder\ncommand\nhandlers\nDatabase\nadapter\n«aggregate»\nOrder\nid\n...\n«repository»\nOrderRepository\nvoidSave(Order)\nOrer ﬁndOne(id)\n...\nFigure 5.9\nAn aggregate-based design for the Order Service business logic\n \n\n\n160\nCHAPTER 5\nDesigning business logic in a microservice architecture\nthe service updates an Order aggregate. If an update request spans multiple services,\nthe OrderService will also create a saga, as described in chapter 4.\n We’ll take a look at the code—but first, let’s examine a concept that’s closely\nrelated to aggregates: domain events. \n5.3\nPublishing domain events\nMerriam-Webster (https://www.merriam-webster.com/dictionary/event) lists several\ndefinitions of the word event, including these:\n1\nSomething that happens\n2\nA noteworthy happening\n3\nA social occasion or activity\n4\nAn adverse or damaging medical occurrence, a heart attack or other cardiac event\nIn the context of DDD, a domain event is something that has happened to an aggre-\ngate. It’s represented by a class in the domain model. An event usually represents a\nstate change. Consider, for example, an Order aggregate in the FTGO application. Its\nstate-changing events include Order Created, Order Cancelled, Order Shipped, and\nso forth. An Order aggregate might, if there are interested consumers, publish one of\nthe events each time it undergoes a state transition.\n5.3.1\nWhy publish change events?\nDomain events are useful because other parties—users, other applications, or other\ncomponents within the same application—are often interested in knowing about an\naggregate’s state changes. Here are some example scenarios:\nMaintaining data consistency across services using choreography-based sagas,\ndescribed in chapter 4.\nNotifying a service that maintains a replica that the source data has changed.\nThis approach is known as Command Query Responsibility Segregation (CQRS),\nand it’s described in chapter 7.\nNotifying a different application via a registered webhook or via a message bro-\nker in order to trigger the next step in a business process.\nNotifying a different component of the same application in order, for example,\nto send a WebSocket message to a user’s browser or update a text database such\nas ElasticSearch.\nSending notifications—text messages or emails—to users informing them that\ntheir order has shipped, their Rx prescription is ready for pick up, or their\nflight is delayed.\nPattern: Domain event\nAn aggregate publishes a domain event when it’s created or undergoes some other\nsignificant change.\n \n\n\n161\nPublishing domain events\nMonitoring domain events to verify that the application is behaving correctly.\nAnalyzing events to model user behavior.\nThe trigger for the notification in all these scenarios is the state change of an aggre-\ngate in an application’s database. \n5.3.2\nWhat is a domain event?\nA domain event is a class with a name formed using a past-participle verb. It has proper-\nties that meaningfully convey the event. Each property is either a primitive value or a\nvalue object. For example, an OrderCreated event class has an orderId property.\n A domain event typically also has metadata, such as the event ID, and a timestamp.\nIt might also have the identity of the user who made the change, because that’s useful\nfor auditing. The metadata can be part of the event object, perhaps defined in a\nsuperclass. Alternatively, the event metadata can be in an envelope object that wraps\nthe event object. The ID of the aggregate that emitted the event might also be part of\nthe envelope rather than an explicit event property.\n The OrderCreated event is an example of a domain event. It doesn’t have any\nfields, because the Order’s ID is part of the event envelope. The following listing\nshows the OrderCreated event class and the DomainEventEnvelope class.\ninterface DomainEvent {}\ninterface OrderDomainEvent extends DomainEvent {}\nclass OrderCreated implements OrderDomainEvent {}\nclass DomainEventEnvelope<T extends DomainEvent> {\nprivate String aggregateType;      \nprivate Object aggregateId;\nprivate T event;\n...\n}\nThe DomainEvent interface is a marker interface that identifies a class as a domain\nevent. OrderDomainEvent is a marker interface for events, such as OrderCreated, which\nare published by the Order aggregate. The DomainEventEnvelope is a class that con-\ntains event metadata and the event object. It’s a generic class that’s parameterized by\nthe domain event type. \n5.3.3\nEvent enrichment\nLet’s imagine, for example, that you’re writing an event consumer that processes Order\nevents. The OrderCreated event class shown previously captures the essence of what has\nhappened. But your event consumer may need the order details when processing an\nListing 5.1\nThe OrderCreated event and the DomainEventEnvelope class\nThe event’s \nmetadata\n \n\n\n162\nCHAPTER 5\nDesigning business logic in a microservice architecture\nOrderCreated event. One option is for it to retrieve that information from the Order-\nService. The drawback of an event consumer querying the service for the aggregate is\nthat it incurs the overhead of a service request.\n An alternative approach known as event enrichment is for events to contain informa-\ntion that consumers need. It simplifies event consumers because they no longer need\nto request that data from the service that published the event. In the OrderCreated\nevent, the Order aggregate can enrich the event by including the order details. The\nfollowing listing shows the enriched event.\nclass OrderCreated implements OrderEvent {\nprivate List<OrderLineItem> lineItems;\nprivate DeliveryInformation deliveryInformation;       \nprivate PaymentInformation paymentInformation;\nprivate long restaurantId;\nprivate String restaurantName;\n...\n}\nBecause this version of the OrderCreated event contains the order details, an event\nconsumer, such as the Order History Service (discussed in chapter 7) no longer\nneeds to fetch that data when processing an OrderCreated event.\n Although event enrichment simplifies consumers, the drawback is that it risks mak-\ning the event classes less stable. An event class potentially needs to change whenever\nthe requirements of its consumers change. This can reduce maintainability because\nthis kind of change can impact multiple parts of the application. Satisfying every con-\nsumer can also be a futile effort. Fortunately, in many situations it’s fairly obvious\nwhich properties to include in an event.\n Now that we’ve covered the basics of domain events, let’s look at how to discover\nthem. \n5.3.4\nIdentifying domain events\nThere are a few different strategies for identifying domain events. Often the require-\nments will describe scenarios where notifications are required. The requirements\nmight include language such as “When X happens do Y.” For example, one require-\nment in the FTGO application is “When an Order is placed send the consumer an\nemail.” A requirement for a notification suggests the existence of a domain event.\n Another approach, which is increasing in popularity, is to use event storming. Event\nstorming is an event-centric workshop format for understanding a complex domain. It\ninvolves gathering domain experts in a room, lots of sticky notes, and a very large sur-\nface—a whiteboard or paper roll—to stick the notes on. The result of event storming\nis an event-centric domain model consisting of aggregates and events.\n \nListing 5.2\nThe enriched OrderCreated event \nData that its \nconsumers \ntypically need\n \n\n\n163\nPublishing domain events\n Event storming consist of three main steps:\n1\nBrainstorm events—Ask the domain experts to brainstorm the domain events.\nDomain events are represented by orange sticky notes that are laid out in a\nrough timeline on the modeling surface.\n2\nIdentify event triggers—Ask the domain experts to identify the trigger of each\nevent, which is one of the following:\n– User actions, represented as a command using a blue sticky note\n– External system, represented by a purple sticky note\n– Another domain event\n– Passing of time\n3\nIdentify aggregates—Ask the domain experts to identify the aggregate that con-\nsumes each command and emits the corresponding event. Aggregates are rep-\nresented by yellow sticky notes.\nFigure 5.10 shows the result of an event-storming workshop. In just a couple of hours,\nthe participants identified numerous domain events, commands, and aggregates. It\nwas a good first step in the process of creating a domain model.\nEvent storming is a useful technique for quickly creating a domain model.\n Now that we’ve covered the basics of domain events, let’s look at the mechanics of\ngenerating and publishing them. \nEvent\nCommand\nAggregate\nPolicy\nFigure 5.10\nThe result of an event-storming workshop that lasted a couple of hours. The sticky notes \nare events, which are laid out along a timeline; commands, which represent user actions; and \naggregates, which emit events in response to a command.\n \n\n\n164\nCHAPTER 5\nDesigning business logic in a microservice architecture\n5.3.5\nGenerating and publishing domain events\nCommunicating using domain events is a form of asynchronous messaging, discussed\nin chapter 3. But before the business logic can publish them to a message broker, it\nmust first create them. Let’s look at how to do that.\nGENERATING DOMAIN EVENTS\nConceptually, domain events are published by aggregates. An aggregate knows when\nits state changes and hence what event to publish. An aggregate could invoke a mes-\nsaging API directly. The drawback of this approach is that because aggregates can’t\nuse dependency injection, the messaging API would need to be passed around as a\nmethod argument. That would intertwine infrastructure concerns and business logic,\nwhich is extremely undesirable.\n A better approach is to split responsibility between the aggregate and the service\n(or equivalent class) that invokes it. Services can use dependency injection to obtain a\nreference to the messaging API, easily publishing events. The aggregate generates the\nevents whenever its state changes and returns them to the service. There are a couple\nof different ways an aggregate can return events back to the service. One option is for\nthe return value of an aggregate method to include a list of events. For example, the\nfollowing listing shows how a Ticket aggregate’s accept() method can return a Ticket-\nAcceptedEvent to its caller.\npublic class Ticket {\npublic List<DomainEvent> accept(ZonedDateTime readyBy) {\n...\nthis.acceptTime = ZonedDateTime.now();\n  \nthis.readyBy = readyBy;\nreturn singletonList(new TicketAcceptedEvent(readyBy));      \n}\n}\nThe service invokes the aggregate root’s method, and then publishes the events. For\nexample, the following listing shows how KitchenService invokes Ticket.accept() and\npublishes the events.\npublic class KitchenService {\n@Autowired\nprivate TicketRepository ticketRepository;\n@Autowired\nprivate DomainEventPublisher domainEventPublisher;\nListing 5.3\nThe Ticket aggregate’s accept() method\nListing 5.4\nKitchenService calls Ticket.accept() \nUpdates \nthe Ticket\nReturns \nan event\n \n",
      "page_number": 169
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 190-205)",
      "start_page": 190,
      "end_page": 205,
      "detection_method": "topic_boundary",
      "content": "165\nPublishing domain events\npublic void accept(long ticketId, ZonedDateTime readyBy) {\nTicket ticket =\nticketRepository.findById(ticketId)\n.orElseThrow(() ->\nnew TicketNotFoundException(ticketId));\nList<DomainEvent> events = ticket.accept(readyBy);\ndomainEventPublisher.publish(Ticket.class, orderId, events);\n}\nThe accept() method first invokes the TicketRepository to load the Ticket from the\ndatabase. It then updates the Ticket by calling accept(). KitchenService then pub-\nlishes events returned by Ticket by calling DomainEventPublisher.publish(),\ndescribed shortly.\n This approach is quite simple. Methods that would otherwise have a void return\ntype now return List<Event>. The only potential drawback is that the return type of\nnon-void methods is now more complex. They must return an object containing the\noriginal return value and List<Event>. You’ll see an example of such a method soon.\n Another option is for the aggregate root to accumulate events in a field. The ser-\nvice then retrieves the events and publishes them. For example, the following listing\nshows a variant of the Ticket class that works this way.\npublic class Ticket extends AbstractAggregateRoot {\npublic void accept(ZonedDateTime readyBy) {\n...\nthis.acceptTime = ZonedDateTime.now();\nthis.readyBy = readyBy;\nregisterDomainEvent(new TicketAcceptedEvent(readyBy));\n}\n}\nTicket extends AbstractAggregateRoot, which defines a registerDomainEvent()\nmethod that records the event. A service would call AbstractAggregateRoot.get-\nDomainEvents() to retrieve those events.\n My preference is for the first option: the method returning events to the service.\nBut accumulating events in the aggregate root is also a viable option. In fact, the\nSpring Data Ingalls release train (https://spring.io/blog/2017/01/30/what-s-new-in-\nspring-data-release-ingalls) implements a mechanism that automatically publishes\nevents to the Spring ApplicationContext. The main drawback is that to reduce code\nduplication, aggregate roots should extend a superclass such as AbstractAggregate-\nRoot, which might conflict with a requirement to extend some other superclass. Another\nissue is that although it’s easy for the aggregate root’s methods to call register-\nDomainEvent(), methods in other classes in the aggregate would find it challenging.\nThey would mostly likely need to somehow pass the events to the aggregate root. \nListing 5.5\nThe Ticket extends a superclass, which records domain events\nPublishes \ndomain \nevents\n \n\n\n166\nCHAPTER 5\nDesigning business logic in a microservice architecture\nHOW TO RELIABLY PUBLISH DOMAIN EVENTS?\nChapter 3 talks about how to reliably send messages as part of a local database transac-\ntion. Domain events are no different. A service must use transactional messaging to\npublish events to ensure that they’re published as part of the transaction that updates\nthe aggregate in the database. The Eventuate Tram framework, described in chapter 3,\nimplements such a mechanism. It insert events into an OUTBOX table as part of the\nACID transaction that updates the database. After the transaction commits, the events\nthat were inserted into the OUTBOX table are then published to the message broker.\n The Tram framework provides a DomainEventPublisher interface, shown in the\nfollowing listing. It defines several overloaded publish() methods that take the aggre-\ngate type and ID as parameters, along with a list of domain events.\npublic interface DomainEventPublisher {\nvoid publish(String aggregateType, Object aggregateId, \nList<DomainEvent> domainEvents);\nIt uses the Eventuate Tram framework’s MessageProducer interface to publish those\nevents transactionally.\n A service could call the DomainEventPublisher publisher directly. But one draw-\nback of doing so is that it doesn’t ensure that a service only publishes valid events.\nKitchenService, for example, should only publish events that implement Ticket-\nDomainEvent, which is the marker interface for the Ticket aggregate’s events. A better\noption is for services to implement a subclass of AbstractAggregateDomainEvent-\nPublisher, which is shown in listing 5.7. AbstractAggregateDomainEventPublisher\nis an abstract class that provides a type-safe interface for publishing domain events.\nIt’s a generic class that has two type parameters, A, the aggregate type, and E, the\nmarker interface type for the domain events. A service publishes events by calling\nthe publish() method, which has two parameters: an aggregate of type A and a list of\nevents of type E.\npublic abstract class AbstractAggregateDomainEventPublisher<A, E extends Doma\ninEvent> {\nprivate Function<A, Object> idSupplier;\nprivate DomainEventPublisher eventPublisher;\nprivate Class<A> aggregateType;\nprotected AbstractAggregateDomainEventPublisher(\nDomainEventPublisher eventPublisher,\nClass<A> aggregateType,\nFunction<A, Object> idSupplier) {\nthis.eventPublisher = eventPublisher;\nthis.aggregateType = aggregateType;\nListing 5.6\nThe Eventuate Tram framework’s DomainEventPublisher interface\nListing 5.7\nThe abstract superclass of type-safe domain event publishers\n \n\n\n167\nPublishing domain events\nthis.idSupplier = idSupplier;\n}\npublic void publish(A aggregate, List<E> events) {\neventPublisher.publish(aggregateType, idSupplier.apply(aggregate), \n(List<DomainEvent>) events);\n}\n}\nThe publish() method retrieves the aggregate’s ID and invokes DomainEventPublisher\n.publish(). The following listing shows the TicketDomainEventPublisher, which\npublishes domain events for the Ticket aggregate.\npublic class TicketDomainEventPublisher extends \nAbstractAggregateDomainEventPublisher<Ticket, TicketDomainEvent> {\npublic TicketDomainEventPublisher(DomainEventPublisher eventPublisher) {\nsuper(eventPublisher, Ticket.class, Ticket::getId);\n}\n}\nThis class only publishes events that are a subclass of TicketDomainEvent.\n Now that we’ve looked at how to publish domain events, let’s see how to con-\nsume them. \n5.3.6\nConsuming domain events\nDomain events are ultimately published as messages to a message broker, such as\nApache Kafka. A consumer could use the broker’s client API directly. But it’s more\nconvenient to use a higher-level API such as the Eventuate Tram framework’s Domain-\nEventDispatcher, described in chapter 3. A DomainEventDispatcher dispatches\ndomain events to the appropriate handle method. Listing 5.9 shows an example event\nhandler class. KitchenServiceEventConsumer subscribes to events published by\nRestaurant Service whenever a restaurant’s menu is updated. It’s responsible for\nkeeping Kitchen Service’s replica of the data up-to-date.\npublic class KitchenServiceEventConsumer {\n@Autowired\nprivate RestaurantService restaurantService;\npublic DomainEventHandlers domainEventHandlers() {       \nreturn DomainEventHandlersBuilder\n.forAggregateType(\"net.chrisrichardson.ftgo.restaurantservice.Restaurant\")\n.onEvent(RestaurantMenuRevised.class, this::reviseMenu)\nListing 5.8\nA type-safe interface for publishing Ticket aggregates' domain events\nListing 5.9\nDispatching events to event handler methods\nMaps events to \nevent handlers\n \n\n\n168\nCHAPTER 5\nDesigning business logic in a microservice architecture\n.build();\n}\npublic void reviseMenu(DomainEventEnvelope<RestaurantMenuRevised> de) {  \nlong id = Long.parseLong(de.getAggregateId());\nRestaurantMenu revisedMenu = de.getEvent().getRevisedMenu();\nrestaurantService.reviseMenu(id, revisedMenu);\n}\n}\nThe reviseMenu() method handles RestaurantMenuRevised events. It calls restaurant-\nService.reviseMenu(), which updates the restaurant’s menu. That method returns a\nlist of domain events, which are published by the event handler.\n Now that we’ve looked at aggregates and domain events, it’s time to consider some\nexample business logic that’s implemented using aggregates. \n5.4\nKitchen Service business logic\nThe first example is Kitchen Service, which enables a restaurant to manage their\norders. The two main aggregates in this service are the Restaurant and Ticket aggre-\ngates. The Restaurant aggregate knows the restaurant’s menu and opening hours\nand can validate orders. A Ticket represents an order that a restaurant must prepare\nfor pickup by a courier. Figure 5.11 shows these aggregates and other key parts of the\nservice’s business logic, as well as the service’s adapters.\n In addition to the aggregates, the other main parts of Kitchen Service’s business\nlogic are KitchenService, TicketRepository, and RestaurantRepository. Kitchen-\nService is the business logic’s entry. It defines methods for creating and updating\nthe Restaurant and Ticket aggregates. TicketRepository and RestaurantRepository\ndefine methods for persisting Tickets and Restaurants respectively.\n The Kitchen Service service has three inbound adapters:\n\nREST API—The REST API invoked by the user interface used by workers at the\nrestaurant. It invokes KitchenService to create and update Tickets.\n\nKitchenServiceCommandHandler—The asynchronous request/response-based\nAPI that’s invoked by sagas. It invokes KitchenService to create and update\nTickets.\n\nKitchenServiceEventConsumer—Subscribes to events published by Restaurant\nService. It invokes KitchenService to create and update Restaurants.\nThe service also has two outbound adapters:\n\nDB adapter—Implements the TicketRepository and the RestaurantRepository\ninterfaces and accesses the database.\n\nDomainEventPublishingAdapter—Implements the DomainEventPublisher inter-\nface and publishes Ticket domain events.\nAn event handler for the\nRestaurantMenuRevised\nevent\n \n\n\n169\nKitchen Service business logic\nLet’s take a closer look at the design of KitchenService, starting with the Ticket\naggregate.\n5.4.1\nThe Ticket aggregate\nTicket is one of the aggregates of Kitchen Service. As described in chapter 2, when\ntalking about the concept of a Bounded Context, this aggregate represents the restau-\nrant kitchen’s view of an order. It doesn’t contain information about the consumer,\nsuch as their identity, the delivery information, or payment details. It’s focused on\nenabling a restaurant’s kitchen to prepare the Order for pickup. Moreover, Kitchen-\nService doesn’t generate a unique ID for this aggregate. Instead, it uses the ID sup-\nplied by OrderService.\n Let’s first look at the structure of this class and then we’ll examine its methods.\nKitchen Service\ncommand channel\nRestaurant Events\nchannel\nTicket events\nchannel\nKitchen Service\ndatabase\nCreate ticket\nConﬁrm create ticket\nRestaurant created\nRestaurant menu revised\naccept\nreject\npreparing\nreadyForPickup\npickedUp\nREST API\nRestaurant\nKitchen\nService\nDomain event\npublisher\n«aggregate»\nTicket\n«aggregate»\nrestaurant\n«repository»\nTicket\nRepository\n«repository»\nRestaurant\nRepository\nKitchenService\nCommandHandler\nKitchenService\nEventConsumer\nDatabase\nadapter\nDomain event\npublishing adapter\nFigure 5.11\nThe design of Kitchen Service\n \n\n\n170\nCHAPTER 5\nDesigning business logic in a microservice architecture\nSTRUCTURE OF THE TICKET CLASS\nThe following listing shows an excerpt of the code for this class. The Ticket class is\nsimilar to a traditional domain class. The main difference is that references to other\naggregates are by primary key.\n@Entity(table=\"tickets\")\npublic class Ticket {\n@Id\nprivate Long id;\nprivate TicketState state;\nprivate Long restaurantId;\n@ElementCollection\n@CollectionTable(name=\"ticket_line_items\")\nprivate List<TicketLineItem> lineItems;\nprivate ZonedDateTime readyBy;\nprivate ZonedDateTime acceptTime;\nprivate ZonedDateTime preparingTime;\nprivate ZonedDateTime pickedUpTime;\nprivate ZonedDateTime readyForPickupTime;\n...\nThis class is persisted with JPA and is mapped to the TICKETS table. The restaurantId\nfield is a Long rather than an object reference to a Restaurant. The readyBy field\nstores the estimate of when the order will be ready for pickup. The Ticket class has\nseveral fields that track the history of the order, including acceptTime, preparing-\nTime, and pickupTime. Let’s look at this class’s methods. \nBEHAVIOR OF THE TICKET AGGREGATE\nThe Ticket aggregate defines several methods. As you saw earlier, it has a static create()\nmethod, which is a factory method that creates a Ticket. There are also some meth-\nods that are invoked when the restaurant updates the state of the order:\n\naccept()—The restaurant has accepted the order.\n\npreparing()—The restaurant has started preparing the order, which means the\norder can no longer be changed or cancelled.\n\nreadyForPickup()—The order can now be picked up.\nThe following listing shows some of its methods.\n \n \n \n \nListing 5.10\nPart of the Ticket class, which is a JPA entity\n \n\n\n171\nKitchen Service business logic\npublic class Ticket {\npublic static ResultWithAggregateEvents<Ticket, TicketDomainEvent> \ncreate(Long id, TicketDetails details) {\nreturn new ResultWithAggregateEvents<>(new Ticket(id, details), new \nTicketCreatedEvent(id, details));\n}\npublic List<TicketPreparationStartedEvent> preparing() {\nswitch (state) {\ncase ACCEPTED:\nthis.state = TicketState.PREPARING;\nthis.preparingTime = ZonedDateTime.now();\nreturn singletonList(new TicketPreparationStartedEvent());\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\npublic List<TicketDomainEvent> cancel() {\nswitch (state) {\ncase CREATED:\ncase ACCEPTED:\nthis.state = TicketState.CANCELLED;\nreturn singletonList(new TicketCancelled());\ncase READY_FOR_PICKUP:\nthrow new TicketCannotBeCancelledException();\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\nThe create() method creates a Ticket. The preparing() method is called when the\nrestaurant starts preparing the order. It changes the state of the order to PREPARING,\nrecords the time, and publishes an event. The cancel() method is called when a user\nattempts to cancel an order. If the cancellation is allowed, this method changes the\nstate of the order and returns an event. Otherwise, it throws an exception. These\nmethods are invoked in response to REST API requests as well as events and com-\nmand messages. Let’s look at the classes that invoke the aggregate’s method. \nTHE KITCHENSERVICE DOMAIN SERVICE\nKitchenService is invoked by the service’s inbound adapters. It defines various meth-\nods for changing the state of an order, including accept(), reject(), preparing(), and\nothers. Each method loads the specifies aggregate, calls the corresponding method on\nthe aggregate root, and publishes any domain events. The following listing shows its\naccept() method.\nListing 5.11\nSome of the Ticket's methods\n \n\n\n172\nCHAPTER 5\nDesigning business logic in a microservice architecture\npublic class KitchenService {\n@Autowired\nprivate TicketRepository ticketRepository;\n@Autowired\nprivate TicketDomainEventPublisher domainEventPublisher;\npublic void accept(long ticketId, ZonedDateTime readyBy) {\nTicket ticket =\nticketRepository.findById(ticketId)\n.orElseThrow(() ->\nnew TicketNotFoundException(ticketId));\nList<TicketDomainEvent> events = ticket.accept(readyBy);\ndomainEventPublisher.publish(ticket, events);          \n}\n}\nThe accept() method is invoked when the restaurant accepts a new order. It has two\nparameters:\n\norderId—ID of the order to accept\n\nreadyBy—Estimated time when the order will be ready for pickup\nThis method retrieves the Ticket aggregate and calls its accept() method. It pub-\nlishes any generated events.\n Now let’s look at the class that handles asynchronous commands. \nTHE KITCHENSERVICECOMMANDHANDLER CLASS\nThe KitchenServiceCommandHandler class is an adapter that’s responsible for handling\ncommand messages sent by the various sagas implemented by Order Service. This class\ndefines a handler method for each command, which invokes KitchenService to create\nor update a Ticket. The following listing shows an excerpt of this class.\npublic class KitchenServiceCommandHandler {\n@Autowired\nprivate KitchenService kitchenService;\n public CommandHandlers commandHandlers() {\n  \nreturn CommandHandlersBuilder\n.fromChannel(\"orderService\")\n.onMessage(CreateTicket.class, this::createTicket)\n.onMessage(ConfirmCreateTicket.class,\nthis::confirmCreateTicket)\nListing 5.12\nThe service’s accept() method updates Ticket\nListing 5.13\nHandling command messages sent by sagas\nPublish \ndomain \nevents\nMaps  command messages \nto message handlers\n \n\n\n173\nOrder Service business logic\n.onMessage(CancelCreateTicket.class,\nthis::cancelCreateTicket)\n.build();\n}\nprivate Message createTicket(CommandMessage<CreateTicket>\ncm) {\nCreateTicket command = cm.getCommand();\nlong restaurantId = command.getRestaurantId();\nLong ticketId = command.getOrderId();\nTicketDetails ticketDetails =\ncommand.getTicketDetails();\ntry {\nTicket ticket =\n   \nkitchenService.createTicket(restaurantId,\nticketId, ticketDetails);\nCreateTicketReply reply =\nnew CreateTicketReply(ticket.getId());\nreturn withSuccess(reply);\n  \n} catch (RestaurantDetailsVerificationException e) {\nreturn withFailure();\n}\n}\nprivate Message confirmCreateTicket\n(CommandMessage<ConfirmCreateTicket> cm) {\n  \nLong ticketId = cm.getCommand().getTicketId();\nkitchenService.confirmCreateTicket(ticketId);\nreturn withSuccess();\n}\n...\nAll the command handler methods invoke KitchenService and reply with either a\nsuccess or a failure reply.\n Now that you’ve seen the business logic for a relatively simple service, we’ll look at\na more complex example: Order Service. \n5.5\nOrder Service business logic\nAs mentioned in earlier chapters, Order Service provides an API for creating, updat-\ning, and canceling orders. This API is primarily invoked by the consumer. Figure 5.12\nshows the high-level design of the service. The Order aggregate is the central aggre-\ngate of Order Service. But there’s also a Restaurant aggregate, which is a partial\nreplica of data owned by Restaurant Service. It enables Order Service to validate\nand price an Order’s line items.\n In addition to the Order and Restaurant aggregates, the business logic consists of\nOrderService, OrderRepository, RestaurantRepository, and various sagas such as\nthe CreateOrderSaga described in chapter 4. OrderService is the primary entry\npoint into the business logic and defines methods for creating and updated Orders\nInvokes KitchenService \nto create the Ticket\nSends back a \nsuccessful reply\nSends back a \nfailure reply\nConfirms \nthe order\n \n\n\n174\nCHAPTER 5\nDesigning business logic in a microservice architecture\nand Restaurants. OrderRepository defines methods for persisting Orders, and\nRestaurantRepository has methods for persisting Restaurants. Order Service has\nseveral inbound adapters:\n\nREST API—The REST API invoked by the user interface used by consumers. It\ninvokes OrderService to create and update Orders.\nRestaurant Events\nchannel\nOrder Service\ncommand channel\nConsumer Service\ncommand channel\nKitchen Service\ncommand channel\nAccounting Service\ncommand channel\nCreate order saga\nreply channel\nCancel order saga\nreply channel\nRevise order saga\nreply channel\nTicket events\nchannel\ncreateOrder()\ncancelOrder()\nreviseOrder()\nREST API\nConsumer\nOrderService\nDomain event\npublisher\nCommand\nproducer\n«aggregate»\nRestaurant\n«aggregate»\nOrder\n«repository»\nOrderRepository\n«repository»\nRestaurant\nRepository\nOrderEvent\nconsumer\nOrder\ncommand\nhandlers\nSagaReply\nmessage\nadapter\nDatabase\nadapter\nOutbound\ncommand\nmessage\nadapter\n«saga»\n*OrderSaga\nOrder Service\ndatabase\nDomain event\npublishing\nadapter\nFigure 5.12\nThe design of the Order Service. It has a REST API for managing orders. It exchanges messages \nand events with other services via several message channels.\n \n\n\n175\nOrder Service business logic\n\nOrderEventConsumer—Subscribes to events published by Restaurant Service. It\ninvokes OrderService to create and update its replica of Restaurants.\n\nOrderCommandHandlers—The asynchronous request/response-based API that’s\ninvoked by sagas. It invokes OrderService to update Orders.\n\nSagaReplyAdapter—Subscribes to the saga reply channels and invokes the sagas.\nThe service also has some outbound adapters:\n\nDB adapter—Implements the OrderRepository interface and accesses the Order\nService database\n\nDomainEventPublishingAdapter—Implements the DomainEventPublisher inter-\nface and publishes Order domain events\n\nOutboundCommandMessageAdapter—Implements the CommandPublisher inter-\nface and sends command messages to saga participants\nLet’s first take a closer look at the Order aggregate and then examine OrderService.\n5.5.1\nThe Order Aggregate\nThe Order aggregate represents an order placed by a consumer. We’ll first look at the\nstructure of the Order aggregate and then check out its methods.\nTHE STRUCTURE OF THE ORDER AGGREGATE\nFigure 5.13 shows the structure of the Order aggregate. The Order class is the root of\nthe Order aggregate. The Order aggregate also consists of value objects such as Order-\nLineItem, DeliveryInfo, and PaymentInfo.\n«value object»\nAddress\nstreet1\nstreet2\ncity\nstate\nzip\n«aggregate»\nOrder\nstate\nconsumerId\nrestaurantId\n...\nPrice\nOrder minimum\n«value object»\nOrderLineItem\nquantity\nmenuItem\nname\n«value object»\nDeliveryInfo\ndeliveryTime\n«value object»\nMoney\namount\n«value object»\nPaymentInfo\npaymentMethodId\nFigure 5.13\nThe design of the Order aggregate, which consists of the Order aggregate root \nand various value objects.\n \n\n\n176\nCHAPTER 5\nDesigning business logic in a microservice architecture\nThe Order class has a collection of OrderLineItems. Because the Order’s Consumer\nand Restaurant are other aggregates, it references them by primary key value. The\nOrder class has a DeliveryInfo class, which stores the delivery address and the\ndesired delivery time, and a PaymentInfo, which stores the payment info. The follow-\ning listing shows the code.\n@Entity\n@Table(name=\"orders\")\n@Access(AccessType.FIELD)\npublic class Order {\n@Id\n@GeneratedValue\nprivate Long id;\n@Version\nprivate Long version;\nprivate OrderState state;\nprivate Long consumerId;\nprivate Long restaurantId;\n@Embedded\nprivate OrderLineItems orderLineItems;\n@Embedded\nprivate DeliveryInformation deliveryInformation;\n@Embedded\nprivate PaymentInformation paymentInformation;\n@Embedded\nprivate Money orderMinimum;\nThis class is persisted with JPA and is mapped to the ORDERS table. The id field is the\nprimary key. The version field is used for optimistic locking. The state of an Order is\nrepresented by the OrderState enumeration. The DeliveryInformation and Payment-\nInformation fields are mapped using the @Embedded annotation and are stored as col-\numns of the ORDERS table. The orderLineItems field is an embedded object that\ncontains the order line items. The Order aggregate consists of more than just fields. It\nalso implements business logic, which can be described by a state machine. Let’s take\na look at the state machine. \nTHE ORDER AGGREGATE STATE MACHINE\nIn order to create or update an order, Order Service must collaborate with other ser-\nvices using sagas. Either OrderService or the first step of the saga invokes an Order\nmethod that verifies that the operation can be performed and changes the state of the\nOrder to a pending state. A pending state, as explained in chapter 4, is an example of\nListing 5.14\nThe Order class and its fields\n \n\n\n177\nOrder Service business logic\na semantic lock countermeasure, which helps ensure that sagas are isolated from one\nanother. Eventually, once the saga has invoked the participating services, it then\nupdates the Order to reflect the outcome. For example, as described in chapter 4, the\nCreate Order Saga has multiple participant services, including Consumer Service,\nAccounting Service, and Kitchen Service. OrderService first creates an Order in an\nAPPROVAL_PENDING state, and then later changes its state to either APPROVED or\nREJECTED. The behavior of an Order can be modeled as the state machine shown in\nfigure 5.14.\nSimilarly, other Order Service operations such as revise() and cancel() first change\nthe Order to a pending state and use a saga to verify that the operation can be per-\nformed. Then, once the saga has verified that the operation can be performed, it\nchanges the Order transitions to some other state that reflects the successful outcome\nof the operation. If the verification of the operation fails, the Order reverts to the pre-\nvious state. For example, the cancel() operation first transitions the Order to the\nCANCEL_PENDING state. If the order can be cancelled, the Cancel Order Saga changes\nthe state of the Order to the CANCELLED state. Otherwise, if a cancel() operation is\nrejected because, for example, it’s too late to cancel the order, then the Order transi-\ntions back to the APPROVED state.\n Let’s now look at the how the Order aggregate implements this state machine. \nTHE ORDER AGGREGATE’S METHODS\nThe Order class has several groups of methods, each of which corresponds to a saga.\nIn each group, one method is invoked at the start of the saga, and the other methods\nare invoked at the end. I’ll first discuss the business logic that creates an Order. After\nthat we’ll look at how an Order is updated. The following listing shows the Order’s\nmethods that are invoked during the process of creating an Order.\nAPPROVAL_PENDING\nCANCEL_PENDING\ncancelRejected\ncancelConﬁrmed\nrevise\nrejected\nauthorized\ncancel\nreviseConﬁrmed\nreviseRejected\nREVISION_PENDING\nAPPROVED\nCANCELLED\nREJECTED\n...\nInitial state\nFigure 5.14\nPart of the state machine model of the Order aggregate\n \n\n\n178\nCHAPTER 5\nDesigning business logic in a microservice architecture\npublic class Order { ...\npublic static ResultWithDomainEvents<Order, OrderDomainEvent>\n createOrder(long consumerId, Restaurant restaurant,\nList<OrderLineItem> orderLineItems) {\nOrder order = new Order(consumerId, restaurant.getId(), orderLineItems);\nList<OrderDomainEvent> events = singletonList(new OrderCreatedEvent(\nnew OrderDetails(consumerId, restaurant.getId(), orderLineItems,\norder.getOrderTotal()),\nrestaurant.getName()));\nreturn new ResultWithDomainEvents<>(order, events);\n}\npublic Order(OrderDetails orderDetails) {\nthis.orderLineItems = new OrderLineItems(orderDetails.getLineItems());\nthis.orderMinimum = orderDetails.getOrderMinimum();\nthis.state = APPROVAL_PENDING;\n}\n...\npublic List<DomainEvent> noteApproved() {\nswitch (state) {\ncase APPROVAL_PENDING:\nthis.state = APPROVED;\nreturn singletonList(new OrderAuthorized());\n...\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\npublic List<DomainEvent> noteRejected() {\nswitch (state) {\ncase APPROVAL_PENDING:\nthis.state = REJECTED;\nreturn singletonList(new OrderRejected());\n...\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\nThe createOrder() method is a static factory method that creates an Order and pub-\nlishes an OrderCreatedEvent. The OrderCreatedEvent is enriched with the details of\nthe Order, including the line items, the total amount, the restaurant ID, and the\nrestaurant name. Chapter 7 discusses how Order History Service uses Order events,\nincluding OrderCreatedEvent, to maintain an easily queried replica of Orders.\nListing 5.15\nThe methods that are invoked during order creation\n \n\n\n179\nOrder Service business logic\n The initial state of the Order is APPROVAL_PENDING. When the CreateOrderSaga\ncompletes, it will invoke either noteApproved() or noteRejected(). The note-\nApproved() method is invoked when the consumer’s credit card has been successfully\nauthorized. The noteRejected() method is called when one of the services rejects\nthe order or authorization fails. As you can see, the state of the Order aggregate\ndetermines the behavior of most of its methods. Like the Ticket aggregate, it also\nemits events.\n In addition to createOrder(), the Order class defines several update methods. For\nexample, the Revise Order Saga revises an order by first invoking the revise() method\nand then, once it’s verified that the revision can be made, it invokes the confirm-\nRevised() method. The following listing shows these methods.\nclass Order ...\npublic List<OrderDomainEvent> revise(OrderRevision orderRevision) {\nswitch (state) {\ncase APPROVED:\nLineItemQuantityChange change =\norderLineItems.lineItemQuantityChange(orderRevision);\nif (change.newOrderTotal.isGreaterThanOrEqual(orderMinimum)) {\nthrow new OrderMinimumNotMetException();\n}\nthis.state = REVISION_PENDING;\nreturn singletonList(new OrderRevisionProposed(orderRevision,\nchange.currentOrderTotal, change.newOrderTotal));\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\npublic List<OrderDomainEvent> confirmRevision(OrderRevision orderRevision) {\nswitch (state) {\ncase REVISION_PENDING:\nLineItemQuantityChange licd =\norderLineItems.lineItemQuantityChange(orderRevision);\norderRevision\n.getDeliveryInformation()\n.ifPresent(newDi -> this.deliveryInformation = newDi);\nif (!orderRevision.getRevisedLineItemQuantities().isEmpty()) {\norderLineItems.updateLineItems(orderRevision);\n}\nthis.state = APPROVED;\nreturn singletonList(new OrderRevised(orderRevision,\nlicd.currentOrderTotal, licd.newOrderTotal));\nListing 5.16\nThe Order method for revising an Order\n \n\n\n180\nCHAPTER 5\nDesigning business logic in a microservice architecture\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\n}\nThe revise() method is called to initiate the revision of an order. Among other\nthings, it verifies that the revised order won’t violate the order minimum and changes\nthe state of the order to REVISION_PENDING. Once Revise Order Saga has successfully\nupdated Kitchen Service and Accounting Service, it then calls confirmRevision()\nto complete the revision.\n These methods are invoked by OrderService. Let’s take a look at that class. \n5.5.2\nThe OrderService class\nThe OrderService class defines methods for creating and updating Orders. It’s the\nmain entry point into the business logic and is invoked by various inbound adapters,\nsuch as the REST API. Most of its methods create a saga to orchestrate the creation and\nupdating of Order aggregates. As a result, this service is more complicated than the\nKitchenService class discussed earlier. The following listing shows an excerpt of this\nclass. OrderService is injected with various dependencies, including OrderRepository,\nOrderDomainEventPublisher, and several saga managers. It defines several methods,\nincluding createOrder() and reviseOrder().\n@Transactional\npublic class OrderService {\n@Autowired\nprivate OrderRepository orderRepository;\n@Autowired\nprivate SagaManager<CreateOrderSagaState, CreateOrderSagaState>\ncreateOrderSagaManager;\n@Autowired\nprivate SagaManager<ReviseOrderSagaState, ReviseOrderSagaData>\nreviseOrderSagaManagement;\n@Autowired\nprivate OrderDomainEventPublisher orderAggregateEventPublisher;\npublic Order createOrder(OrderDetails orderDetails) {\nRestaurant restaurant = restaurantRepository.findById(restaurantId)\n.orElseThrow(() -\n> new RestaurantNotFoundException(restaurantId));\nListing 5.17\nThe OrderService class has methods for creating and managing orders\n \n",
      "page_number": 190
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 206-214)",
      "start_page": 206,
      "end_page": 214,
      "detection_method": "topic_boundary",
      "content": "181\nOrder Service business logic\nList<OrderLineItem> orderLineItems =\n  \nmakeOrderLineItems(lineItems, restaurant);\nResultWithDomainEvents<Order, OrderDomainEvent> orderAndEvents =\nOrder.createOrder(consumerId, restaurant, orderLineItems);\nOrder order = orderAndEvents.result;\norderRepository.save(order);\norderAggregateEventPublisher.publish(order, orderAndEvents.events);  \nOrderDetails orderDetails =\nnew OrderDetails(consumerId, restaurantId, orderLineItems,\norder.getOrderTotal());\nCreateOrderSagaState data = new CreateOrderSagaState(order.getId(),\norderDetails);\ncreateOrderSagaManager.create(data, Order.class, order.getId());  \nreturn order;\n}\npublic Order reviseOrder(Long orderId, Long expectedVersion,\nOrderRevision orderRevision)\n{\npublic Order reviseOrder(long orderId, OrderRevision orderRevision) {\nOrder order = orderRepository.findById(orderId)\n  \n.orElseThrow(() -> new OrderNotFoundException(orderId));\nReviseOrderSagaData sagaData =\nnew ReviseOrderSagaData(order.getConsumerId(), orderId,\nnull, orderRevision);\nreviseOrderSagaManager.create(sagaData);   \nreturn order;\n}\n}\nThe createOrder() method first creates and persists an Order aggregate. It then pub-\nlishes the domain events emitted by the aggregate. Finally, it creates a CreateOrder-\nSaga. The reviseOrder() retrieves the Order and then creates a ReviseOrderSaga.\n In many ways, the business logic for a microservices-based application is not that\ndifferent from that of a monolithic application. It’s comprised of classes such as ser-\nvices, JPA-backed entities, and repositories. There are some differences, though. A\ndomain model is organized as a set of DDD aggregates that impose various design\nconstraints. Unlike in a traditional object model, references between classes in differ-\nent aggregates are in terms of primary key value rather than object references. Also, a\ntransaction can only create or update a single aggregate. It’s also useful for aggregates\nto publish domain events when their state changes.\n Another major difference is that services often use sagas to maintain data consis-\ntency across multiple services. For example, Kitchen Service merely participates in\nsagas, it doesn’t initiate them. In contrast, Order Service relies heavily on sagas when\nCreates the Order \naggregate\nPersists the Order \nin the database\nPublishes\ndomain\nevents\nCreates the Create\nOrder Saga\nRetrieves\nthe Order\nCreates the \nRevise Order \nSaga\n \n\n\n182\nCHAPTER 5\nDesigning business logic in a microservice architecture\ncreating and updating orders. That’s because Orders must be transactionally consis-\ntent with data owned by other services. As a result, most OrderService methods create\na saga rather than update an Order directly.\n This chapter has covered how to implement business logic using a traditional\napproach to persistence. That has involved integrating messaging and event publish-\ning with database transaction management. The event publishing code is intertwined\nwith the business logic. The next chapter looks at event sourcing, an event-centric\napproach to writing business logic where event generation is integral to the business\nlogic rather than being bolted on. \nSummary\nThe procedural Transaction script pattern is often a good way to implement\nsimple business logic. But when implementing complex business logic you should\nconsider using the object-oriented Domain model pattern.\nA good way to organize a service’s business logic is as a collection of DDD aggre-\ngates. DDD aggregates are useful because they modularize the domain model,\neliminate the possibility of object reference between services, and ensure that\neach ACID transaction is within a service.\nAn aggregate should publish domain events when it’s created or updated.\nDomain events have a wide variety of uses. Chapter 4 discusses how they can\nimplement choreography-based sagas. And, in chapter 7, I talk about how to\nuse domain events to update replicated data. Domain event subscribers can also\nnotify users and other applications, and publish WebSocket messages to a user’s\nbrowser. \n \n\n\n183\nDeveloping business\nlogic with event sourcing\nMary liked the idea, described in chapter 5, of structuring business logic as a collec-\ntion of DDD aggregates that publish domain events. She could imagine the use of\nthose events being extremely useful in a microservice architecture. Mary planned\nto use events to implement choreography-based sagas, which maintain data consis-\ntency across services and are described in chapter 4. She also expected to use CQRS\nviews, replicas that support efficient querying that are described in chapter 7.\n She was, however, worried that the event publishing logic might be error prone.\nOn one hand, the event publishing logic is reasonably straightforward. Each of an\naggregate’s methods that initializes or changes the state of the aggregate returns a\nlist of events. The domain service then publishes those events. But on the other\nThis chapter covers\nUsing the Event sourcing pattern to develop \nbusiness logic\nImplementing an event store\nIntegrating sagas and event sourcing-based \nbusiness logic\nImplementing saga orchestrators using event \nsourcing\n \n\n\n184\nCHAPTER 6\nDeveloping business logic with event sourcing\nhand, the event publishing logic is bolted on to the business logic. The business logic\ncontinues to work even when the developer forgets to publish an event. Mary was con-\ncerned that this way of publishing events might be a source of bugs.\n Many years ago, Mary had learned about event sourcing, an event-centric way of writ-\ning business logic and persisting domain objects. At the time she was intrigued by its\nnumerous benefits, including how it preserves the complete history of the changes to\nan aggregate, but it remained a curiosity. Given the importance of domain events\nin microservice architecture, she now wonders whether it would be worthwhile to\nexplore using event sourcing in the FTGO application. After all, event sourcing elimi-\nnates a source of programming errors by guaranteeing that an event will be published\nwhenever an aggregate is created or updated.\n I begin this chapter by describing how event sourcing works and how you can use it\nto write business logic. I describe how event sourcing persists each aggregate as a\nsequence of events in what is known as an event store. I discuss the benefits and draw-\nbacks of event sourcing and cover how to implement an event store. I describe a sim-\nple framework for writing event sourcing-based business logic. After that, I discuss\nhow event sourcing is a good foundation for implementing sagas. Let’s start by look-\ning at how to develop business logic with event sourcing.\n6.1\nDeveloping business logic using event sourcing\nEvent sourcing is a different way of structuring the business logic and persisting aggre-\ngates. It persists an aggregate as a sequence of events. Each event represents a state\nchange of the aggregate. An application recreates the current state of an aggregate by\nreplaying the events.\nEvent sourcing has several important benefits. For example, it preserves the history of\naggregates, which is valuable for auditing and regulatory purposes. And it reliably\npublishes domain events, which is particularly useful in a microservice architecture.\nEvent sourcing also has drawbacks. It involves a learning curve, because it’s a different\nway to write your business logic. Also, querying the event store is often difficult, which\nrequires you to use the CQRS pattern, described in chapter 7.\n I begin this section by describing the limitations of traditional persistence. I then\ndescribe event sourcing in detail and talk about how it overcomes those limitations.\nAfter that, I show how to implement the Order aggregate using event sourcing. Finally,\nI describe the benefits and drawbacks of event sourcing.\n Let’s first look at the limitations of the traditional approach to persistence.\nPattern: Event sourcing\nPersist an aggregate as a sequence of domain events that represent state changes.\nSee http://microservices.io/patterns/data/event-sourcing.html.\n \n\n\n185\nDeveloping business logic using event sourcing\n6.1.1\nThe trouble with traditional persistence\nThe traditional approach to persistence maps classes to database tables, fields of those\nclasses to table columns, and instances of those classes to rows in those tables. For\nexample, figure 6.1 shows how the Order aggregate, described in chapter 5, is mapped\nto the ORDER table. Its OrderLineItems are mapped to the ORDER_LINE_ITEM table.\nThe application persists an order instance as rows in the ORDER and ORDER_LINE_ITEM\ntables. It might do that using an ORM framework such as JPA or a lower-level frame-\nwork such as MyBATIS.\n This approach clearly works well because most enterprise applications store data\nthis way. But it has several drawbacks and limitations:\nObject-Relational impedance mismatch.\nLack of aggregate history.\nImplementing audit logging is tedious and error prone.\nEvent publishing is bolted on to the business logic.\nLet’s look at each of these problems, starting with the Object-Relational impedance\nmismatch problem.\nOBJECT-RELATIONAL IMPEDANCE MISMATCH\nOne age-old problem is the so-called Object-Relational impedance mismatch problem.\nThere’s a fundamental conceptual mismatch between the tabular relational schema\nand the graph structure of a rich domain model with its complex relationships.\nSome aspects of this problem are reflected in polarized debates over the suitability of\nObject/Relational mapping (ORM) frameworks. For example, Ted Neward has said\nthat “Object-Relational mapping is the Vietnam of Computer Science” (http://blogs\n.tedneward.com/post/the-vietnam-of-computer-science/). To be fair, I’ve used\n«class»\nOrder\nID\n1234\nCUSTOMER_ID\ncustomer-abc\nORDER_TOTAL\n1234.56\n...\n...\n«class»\nOrderLineItem\nID\n567\nORDER_ID\n1234\nORDER table\nORDER_LINE_ITEM table\nQUANTITY\n2\n...\n...\nFigure 6.1\nThe traditional approach to persistence maps classes to tables and objects to rows in \nthose tables.\n \n\n\n186\nCHAPTER 6\nDeveloping business logic with event sourcing\nHibernate successfully to develop applications where the database schema has been\nderived from the object model. But the problems are deeper than the limitations of\nany particular ORM framework. \nLACK OF AGGREGATE HISTORY\nAnother limitation of traditional persistence is that it only stores the current state of\nan aggregate. Once an aggregate has been updated, its previous state is lost. If an\napplication must preserve the history of an aggregate, perhaps for regulatory pur-\nposes, then developers must implement this mechanism themselves. It is time con-\nsuming to implement an aggregate history mechanism and involves duplicating code\nthat must be synchronized with the business logic. \nIMPLEMENTING AUDIT LOGGING IS TEDIOUS AND ERROR PRONE\nAnother issue is audit logging. Many applications must maintain an audit log that\ntracks which users have changed an aggregate. Some applications require auditing for\nsecurity or regulatory purposes. In other applications, the history of user actions is an\nimportant feature. For example, issue trackers and task-management applications\nsuch as Asana and JIRA display the history of changes to tasks and issues. The chal-\nlenge of implementing auditing is that besides being a time-consuming chore, the\nauditing logging code and the business logic can diverge, resulting in bugs. \nEVENT PUBLISHING IS BOLTED ON TO THE BUSINESS LOGIC\nAnother limitation of traditional persistence is that it usually doesn’t support publishing\ndomain events. Domain events, discussed in chapter 5, are events that are published by\nan aggregate when its state changes. They’re a useful mechanism for synchronizing data\nand sending notifications in microservice architecture. Some ORM frameworks, such\nas Hibernate, can invoke application-provided callbacks when data objects change.\nBut there’s no support for automatically publishing messages as part of the transac-\ntion that updates the data. Consequently, as with history and auditing, developers\nmust bolt on event-generation logic, which risks not being synchronized with the busi-\nness logic. Fortunately, there’s a solution to these issues: event sourcing. \n6.1.2\nOverview of event sourcing\nEvent sourcing is an event-centric technique for implementing business logic and per-\nsisting aggregates. An aggregate is stored in the database as a series of events. Each\nevent represents a state change of the aggregate. An aggregate’s business logic is struc-\ntured around the requirement to produce and consume these events. Let’s see how\nthat works.\nEVENT SOURCING PERSISTS AGGREGATES USING EVENTS\nEarlier, in section 6.1.1, I discussed how traditional persistence maps aggregates to\ntables, their fields to columns, and their instances to rows. Event sourcing is a very\ndifferent approach to persisting aggregates that builds on the concept of domain\nevents. It persists each aggregate as a sequence of events in the database, known as\nan event store.\n \n\n\n187\nDeveloping business logic using event sourcing\n Consider, for example, the Order aggregate. As figure 6.2 shows, rather than store\neach Order as a row in an ORDER table, event sourcing persists each Order aggregate as\none or more rows in an EVENTS table. Each row is a domain event, such as Order\nCreated, Order Approved, Order Shipped, and so on.\nWhen an application creates or updates an aggregate, it inserts the events emitted by\nthe aggregate into the EVENTS table. An application loads an aggregate from the event\nstore by retrieving its events and replaying them. Specifically, loading an aggregate\nconsists of the following three steps:\n1\nLoad the events for the aggregate.\n2\nCreate an aggregate instance by using its default constructor.\n3\nIterate through the events, calling apply().\nFor example, the Eventuate Client framework, covered later in section 6.2.2, uses code\nsimilar to the following to reconstruct an aggregate:\nClass aggregateClass = ...;\nAggregate aggregate = aggregateClass.newInstance();\nfor (Event event : events) {\naggregate = aggregate.applyEvent(event);\n}\n// use aggregate...\nIt creates an instance of the class and iterates through the events, calling the aggre-\ngate’s applyEvent() method. If you’re familiar with functional programming, you\nmay recognize this as a fold or reduce operation.\nevent_id\n102\n103\n104\n105\n...\nEVENTS table\nevent_type\nOrder\nCreated\nOrder\nApproved\nOrder\nShipped\nOrder\nDelivered\n...\nentity_type\nOrder\nOrder\nOrder\nOrder\n...\nentity_id\n101\n101\n101\n101\n...\nevent_data\n{...}\n{...}\n{...}\n{...}\n...\nUnique event ID\nThe type of the event\nIdentiﬁes the aggregate\nThe serialized event,\nsuch as JSON\nFigure 6.2\nEvent sourcing persists each aggregate as a sequence of events. A RDBMS-based \napplication can, for example, store the events in an EVENTS table.\n \n\n\n188\nCHAPTER 6\nDeveloping business logic with event sourcing\n It may be strange and unfamiliar to reconstruct the in-memory state of an aggre-\ngate by loading the events and replaying events. But in some ways, it’s not all that dif-\nferent from how an ORM framework such as JPA or Hibernate loads an entity. An\nORM framework loads an object by executing one or more SELECT statements to\nretrieve the current persisted state, instantiating objects using their default construc-\ntors. It uses reflection to initialize those objects. What’s different about event sourcing\nis that the reconstruction of the in-memory state is accomplished using events.\n Let’s now look at the requirements event sourcing places on domain events. \nEVENTS REPRESENT STATE CHANGES\nChapter 5 defines domain events as a mechanism for notifying subscribers of changes\nto aggregates. Events can either contain minimal data, such as just the aggregate ID,\nor can be enriched to contain data that’s useful to a typical consumer. For example,\nthe Order Service can publish an OrderCreated event when an order is created. An\nOrderCreated event may only contain the orderId. Alternatively, the event could con-\ntain the complete order so consumers of that event don’t have to fetch the data from\nthe Order Service. Whether events are published and what those events contain are\ndriven by the needs of the consumers. With event sourcing, though, it’s primarily the\naggregate that determines the events and their structure.\n Events aren’t optional when using event sourcing. Every state change of an aggre-\ngate, including its creation, is represented by a domain event. Whenever the aggregate’s\nstate changes, it must emit an event. For example, an Order aggregate must emit an\nOrderCreated event when it’s created, and an Order* event whenever it is updated.\nThis is a much more stringent requirement than before, when an aggregate only emit-\nted events that were of interest to consumers.\n What’s more, an event must contain the data that the aggregate needs to perform\nthe state transition. The state of an aggregate consists of the values of the fields of the\nobjects that comprise the aggregate. A state change might be as simple as changing\nthe value of the field of an object, such as Order.state. Alternatively, a state change\ncan involve adding or removing objects, such as revising an Order’s line items.\n Suppose, as figure 6.3 shows, that the current state of the aggregate is S and the\nnew state is S'. An event E that represents the state change must contain the data such\nthat when an Order is in state S, calling order.apply(E) will update the Order to state\nS'. In the next section you’ll see that apply() is a method that performs the state\nchange represented by an event.\n Some events, such as the Order Shipped event, contain little or no data and just\nrepresent the state transition. The apply() method handles an Order Shipped event\nby changing the Order’s status field to SHIPPED. Other events, however, contain a lot\nof data. An OrderCreated event, for example, must contain all the data needed by the\napply() method to initialize an Order, including its line items, payment information,\ndelivery information, and so on. Because events are used to persist an aggregate, you\nno longer have the option of using a minimal OrderCreated event that contains the\norderId. \n \n\n\n189\nDeveloping business logic using event sourcing\nAGGREGATE METHODS ARE ALL ABOUT EVENTS\nThe business logic handles a request to update an aggregate by calling a command\nmethod on the aggregate root. In a traditional application, a command method typi-\ncally validates its arguments and then updates one or more of the aggregate’s fields.\nCommand methods in an event sourcing-based application work because they must\ngenerate events. As figure 6.4 shows, the outcome of invoking an aggregate’s com-\nmand method is a sequence of events that represent the state changes that must be\nmade. These events are persisted in the database and applied to the aggregate to\nupdate its state.\nThe requirement to generate events and apply them requires a restructuring—albeit\nmechanical—of the business logic. Event sourcing refactors a command method into\ntwo or more methods. The first method takes a command object parameter, which\nrepresents the request, and determines what state changes need to be performed. It\nvalidates its arguments, and without changing the state of the aggregate, returns a list\nof events representing the state changes. This method typically throws an exception if\nthe command cannot be performed.\nObjects and ﬁeld values\nUpdated objects\nand ﬁeld values\n«aggregate»\nOrder\nS\nEvent\napply()\n«aggregate»\nOrder\nS’\nFigure 6.3\nApplying event E \nwhen the Order is in state S \nmust change the Order state to \nS'. The event must contain the \ndata necessary to perform the \nstate change.\n«aggregate»\nOrder\nS\nEvent\napply()\nProcess(command)\n«aggregate»\nOrder\nS’\n«aggregate»\nOrder\nS\nEvent\nFigure 6.4\nProcessing a command \ngenerates events without changing \nthe state of the aggregate. An \naggregate is updated by applying \nan event.\n \n",
      "page_number": 206
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 215-223)",
      "start_page": 215,
      "end_page": 223,
      "detection_method": "topic_boundary",
      "content": "190\nCHAPTER 6\nDeveloping business logic with event sourcing\n The other methods each take a particular event type as a parameter and update\nthe aggregate. There’s one of these methods for each event. It’s important to note\nthat these methods can’t fail, because an event represents a state change that has hap-\npened. Each method updates the aggregate based on the event.\n The Eventuate Client framework, an event-sourcing framework described in more\ndetail in section 6.2.2, names these methods process() and apply(). A process()\nmethod takes a command object, which contains the arguments of the update\nrequest, as a parameter and returns a list of events. An apply() method takes an event\nas a parameter and returns void. An aggregate will define multiple overloaded ver-\nsions of these methods: one process() method for each command class and one\napply() method for each event type emitted by the aggregate. Figure 6.5 shows an\nexample.\nReturns events without updating the Order\nApplies events to update the Order\npublic class Order {\npublic List<Event> process(ReviseOrder command) {\nOrderRevision orderRevision = command.getOrderRevision();\nswitch (state) {\ncase AUTHORIZED:\nLineItemQuantityChange change =\norderLineItems.lineItemQuantityChange(orderRevision);\nif (change.newOrderTotal.isGreaterThanOrEqual(orderMinimum)) {\nthrow new OrderMinimumNotMetException();\n}\nreturn singletonList(\nnew OrderRevisionProposed(\norderRevision, change.currentOrderTotal,\nchange.newOrderTotal));\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\npublic class Order {\npublic void apply(OrderRevisionProposed event) {\nthis.state = REVISION_PENDING;\n}\npublic class Order {\npublic List<DomainEvent> revise(OrderRevision orderRevision) {\nswitch (state) {\ncase AUTHORIZED:\nLineItemQuantityChange change =\norderLineItems.lineItemQuantityChange(orderRevision);\nif (change.newOrderTotal.isGreaterThanOrEqual(orderMinimum)) {\nthrow new OrderMinimumNotMetException();\n}\nthis.state = REVISION_PENDING;\nreturn …;\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\nFigure 6.5\nEvent sourcing splits a method that updates an aggregate into a process() method, which takes \na command and returns events, and one or more apply() methods, which take an event and update the \naggregate.\n \n\n\n191\nDeveloping business logic using event sourcing\nIn this example, the reviseOrder() method is replaced by a process() method and\nan apply() method. The process() method takes a ReviseOrder command as a\nparameter. This command class is defined by applying Introduce Parameter Object refactor-\ning (https://refactoring.com/catalog/introduceParameterObject.html) to the revise-\nOrder() method. The process() method either returns an OrderRevisionProposed\nevent, or throws an exception if it’s too late to revise the Order or if the proposed revi-\nsion doesn’t meet the order minimum. The apply() method for the OrderRevision-\nProposed event changes the state of the Order to REVISION_PENDING.\n An aggregate is created using the following steps:\n1\nInstantiate aggregate root using its default constructor.\n2\nInvoke process() to generate the new events.\n3\nUpdate the aggregate by iterating through the new events, calling its apply().\n4\nSave the new events in the event store.\nAn aggregate is updated using the following steps:\n1\nLoad aggregate’s events from the event store.\n2\nInstantiate the aggregate root using its default constructor.\n3\nIterate through the loaded events, calling apply() on the aggregate root.\n4\nInvoke its process() method to generate new events.\n5\nUpdate the aggregate by iterating through the new events, calling apply().\n6\nSave the new events in the event store.\nTo see this in action, let’s now look at the event sourcing version of the Order aggregate. \nEVENT SOURCING-BASED ORDER AGGREGATE\nListing 6.1 shows the Order aggregate’s fields and the methods responsible for creat-\ning it. The event sourcing version of the Order aggregate has some similarities to the\nJPA-based version shown in chapter 5. Its fields are almost identical, and it emits simi-\nlar events. What’s different is that its business logic is implemented in terms of pro-\ncessing commands that emit events and applying those events, which updates its state.\nEach method that creates or updates the JPA-based aggregate, such as createOrder()\nand reviseOrder(), is replaced in the event sourcing version by process() and\napply() methods.\npublic class Order {\nprivate OrderState state;\nprivate Long consumerId;\nprivate Long restaurantId;\nprivate OrderLineItems orderLineItems;\nprivate DeliveryInformation deliveryInformation;\nprivate PaymentInformation paymentInformation;\nprivate Money orderMinimum;\nListing 6.1\nThe Order aggregate’s fields and its methods that initialize an instance\n \n\n\n192\nCHAPTER 6\nDeveloping business logic with event sourcing\npublic Order() {\n}\npublic List<Event> process(CreateOrderCommand command) {   \n... validate command ...\nreturn events(new OrderCreatedEvent(command.getOrderDetails()));\n}\npublic void apply(OrderCreatedEvent event) {\n  \nOrderDetails orderDetails = event.getOrderDetails();\nthis.orderLineItems = new OrderLineItems(orderDetails.getLineItems());\nthis.orderMinimum = orderDetails.getOrderMinimum();\nthis.state = APPROVAL_PENDING;\n}\nThis class’s fields are similar to those of the JPA-based Order. The only difference is\nthat the aggregate’s id isn’t stored in the aggregate. The Order’s methods are quite\ndifferent. The createOrder() factory method has been replaced by process() and\napply() methods. The process() method takes a CreateOrder command and emits\nan OrderCreated event. The apply() method takes the OrderCreated and initializes\nthe fields of the Order.\n We’ll now look at the slightly more complex business logic for revising an order.\nPreviously this business logic consisted of three methods: reviseOrder(), confirm-\nRevision(), and rejectRevision(). The event sourcing version replaces these three\nmethods with three process() methods and some apply() methods. The following list-\ning shows the event sourcing version of reviseOrder() and confirmRevision().\npublic class Order {\npublic List<Event> process(ReviseOrder command) {\n  \nOrderRevision orderRevision = command.getOrderRevision();\nswitch (state) {\ncase APPROVED:\nLineItemQuantityChange change =\norderLineItems.lineItemQuantityChange(orderRevision);\nif (change.newOrderTotal.isGreaterThanOrEqual(orderMinimum)) {\nthrow new OrderMinimumNotMetException();\n}\nreturn singletonList(new OrderRevisionProposed(orderRevision,\nchange.currentOrderTotal, change.newOrderTotal));\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\npublic void apply(OrderRevisionProposed event) {\n  \nthis.state = REVISION_PENDING;\n}\nListing 6.2\nThe process() and apply() methods that revise an Order aggregate\nValidates the command and\nreturns an OrderCreatedEvent\nApply the OrderCreatedEvent by\ninitializing the fields of the Order.\nVerify that the Order \ncan be revised and \nthat the revised \norder meets the \norder minimum.\nChange the state of the Order \nto REVISION_PENDING.\n \n\n\n193\nDeveloping business logic using event sourcing\npublic List<Event> process(ConfirmReviseOrder command) {\n  \nOrderRevision orderRevision = command.getOrderRevision();\nswitch (state) {\ncase REVISION_PENDING:\nLineItemQuantityChange licd =\norderLineItems.lineItemQuantityChange(orderRevision);\nreturn singletonList(new OrderRevised(orderRevision,\nlicd.currentOrderTotal, licd.newOrderTotal));\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\npublic void apply(OrderRevised event) {\nOrderRevision orderRevision = event.getOrderRevision();\nif (!orderRevision.getRevisedLineItemQuantities().isEmpty()) {\norderLineItems.updateLineItems(orderRevision);\n}\nthis.state = APPROVED;\n}\nAs you can see, each method has been replaced by a process() method and one or\nmore apply() methods. The reviseOrder() method has been replaced by process\n(ReviseOrder) and apply(OrderRevisionProposed). Similarly, confirmRevision()\nhas been replaced by process(ConfirmReviseOrder) and apply(OrderRevised). \n6.1.3\nHandling concurrent updates using optimistic locking\nIt’s not uncommon for two or more requests to simultaneously update the same\naggregate. An application that uses traditional persistence often uses optimistic lock-\ning to prevent one transaction from overwriting another’s changes. Optimistic locking\ntypically uses a version column to detect whether an aggregate has changed since it\nwas read. The application maps the aggregate root to a table that has a VERSION col-\numn, which is incremented whenever the aggregate is updated. The application\nupdates the aggregate using an UPDATE statement like this:\nUPDATE AGGREGATE_ROOT_TABLE\nSET VERSION = VERSION + 1 ...\nWHERE VERSION = <original version>\nThis UPDATE statement will only succeed if the version is unchanged from when the\napplication read the aggregate. If two transactions read the same aggregate, the first\none that updates the aggregate will succeed. The second one will fail because the ver-\nsion number has changed, so it won’t accidentally overwrite the first transaction’s\nchanges.\n An event store can also use optimistic locking to handle concurrent updates. Each\naggregate instance has a version that’s read along with the events. When the applica-\ntion inserts events, the event store verifies that the version is unchanged. A simple\nVerify that the \nrevision can be \nconfirmed and \nreturn an Order-\nRevised event.\nRevise the \nOrder.\n \n\n\n194\nCHAPTER 6\nDeveloping business logic with event sourcing\napproach is to use the number of events as the version number. Alternatively, as you’ll\nsee below in section 6.2, an event store could maintain an explicit version number. \n6.1.4\nEvent sourcing and publishing events\nStrictly speaking, event sourcing persists aggregates as events and reconstructs the cur-\nrent state of an aggregate from those events. You can also use event sourcing as a reli-\nable event publishing mechanism. Saving an event in the event store is an inherently\natomic operation. We need to implement a mechanism to deliver all persisted events\nto interested consumers.\n Chapter 3 describes a couple of different mechanisms—polling and transaction log\ntailing—for publishing messages that are inserted into the database as part of a transac-\ntion. An event sourcing-based application can publish events using one of these mecha-\nnisms. The main difference is that it permanently stores events in an EVENTS table rather\nthan temporarily saving events in an OUTBOX table and then deleting them. Let’s take a\nlook at each approach, starting with polling.\nUSING POLLING TO PUBLISH EVENTS\nIf events are stored in the EVENTS table shown in figure 6.6, an event publisher can\npoll the table for new events by executing a SELECT statement and publish the events\nto a message broker. The challenge is determining which events are new. For exam-\nple, imagine that eventIds are monotonically increasing. The superficially appealing\napproach is for the event publisher to record the last eventId that it has processed. It\nwould then retrieve new events using a query like this: SELECT * FROM EVENTS where\nevent_id > ? ORDER BY event_id ASC.\n The problem with this approach is that transactions can commit in an order that’s\ndifferent from the order in which they generate events. As a result, the event pub-\nlisher can accidentally skip over an event. Figure 6.6 shows such as a scenario.\nTransaction A\nTransaction B\nCOMMIT\nBEGIN\nBEGIN\nCOMMIT\nINSERT event with\nEVENT_ID = 1020\nSELECT * FROM EVENTS\nWHERE EVENT_ID > ....\nSELECT * FROM EVENTS\nWHERE EVENT_ID > 1020...\nINSERT event with\nEVENT_ID = 1010\nRetrieves event 1020\nCommits last\nSkips event 1010 because\n1010 <= event 1020\nFigure 6.6\nA scenario where an event is skipped because its transaction A commits after \ntransaction B. Polling sees eventId=1020 and then later skips eventId=1010.\n \n\n\n195\nDeveloping business logic using event sourcing\nIn this scenario, Transaction A inserts an event with an EVENT_ID of 1010. Next, trans-\naction B inserts an event with an EVENT_ID of 1020 and then commits. If the event\npublisher were now to query the EVENTS table, it would find event 1020. Later on, after\ntransaction A committed and event 1010 became visible, the event publisher would\nignore it.\n One solution to this problem is to add an extra column to the EVENTS table that\ntracks whether an event has been published. The event publisher would then use the\nfollowing process:\n1\nFind unpublished events by executing this SELECT statement: SELECT * FROM\nEVENTS where PUBLISHED = 0 ORDER BY event_id ASC.\n2\nPublish events to the message broker.\n3\nMark the events as having been published: UPDATE EVENTS SET PUBLISHED = 1\nWHERE EVENT_ID in.\nThis approach prevents the event publisher from skipping events. \nUSING TRANSACTION LOG TAILING TO RELIABLY PUBLISH EVENTS\nMore sophisticated event stores use transaction log tailing, which, as chapter 3 describes,\nguarantees that events will be published and is also more performant and scalable.\nFor example, Eventuate Local, an open source event store, uses this approach. It reads\nevents inserted into an EVENTS table from the database transaction log and pub-\nlishes them to the message broker. Section 6.2 discusses how Eventuate Local works\nin more detail. \n6.1.5\nUsing snapshots to improve performance\nAn Order aggregate has relatively few state transitions, so it only has a small number of\nevents. It’s efficient to query the event store for those events and reconstruct an Order\naggregate. Long-lived aggregates, though, can have a large number of events. For\nexample, an Account aggregate potentially has a large number of events. Over time, it\nwould become increasingly inefficient to load and fold those events.\n A common solution is to periodically persist a snapshot of the aggregate’s state.\nFigure 6.7 shows an example of using a snapshot. The application restores the state of\nThe application only needs\nto retrieve the snapshot and\nevents that occur after it.\nEvent 1\nEvent 2\nEvent ...\nEvent N\nEvent\n+1\nN\nSnapshot\nversion N\nEvent\n+2\nN\nFigure 6.7\nUsing a snapshot improves performance by eliminating the need to load all \nevents. An application only needs to load the snapshot and the events that occur after it.\n \n\n\n196\nCHAPTER 6\nDeveloping business logic with event sourcing\nan aggregate by loading the most recent snapshot and only those events that have\noccurred since the snapshot was created.\n In this example, the snapshot version is N. The application only needs to load the\nsnapshot and the two events that follow it in order to restore the state of the aggre-\ngate. The previous N events are not loaded from the event store.\n When restoring the state of an aggregate from a snapshot, an application first creates\nan aggregate instance from the snapshot and then iterates through the events, applying\nthem. For example, the Eventuate Client framework, described in section 6.2.2, uses\ncode similar to the following to reconstruct an aggregate:\nClass aggregateClass = ...;\nSnapshot snapshot = ...;\nAggregate aggregate = recreateFromSnapshot(aggregateClass, snapshot);\nfor (Event event : events) {\naggregate = aggregate.applyEvent(event);\n}\n// use aggregate...\nWhen using snapshots, the aggregate instance is recreated from the snapshot instead\nof being created using its default constructor. If an aggregate has a simple, easily seri-\nalizable structure, the snapshot can be, for example, its JSON serialization. More com-\nplex aggregates can be snapshotted using the Memento pattern (https://en.wikipedia\n.org/wiki/Memento_pattern).\n The Customer aggregate in the online store example has a very simple structure:\nthe customer’s information, their credit limit, and their credit reservations. A snap-\nshot of a Customer is the JSON serialization of its state. Figure 6.8 shows how to recre-\nate a Customer from a snapshot corresponding to the state of a Customer as of event\n#103. The Customer Service needs to load the snapshot and the events that have\noccurred after event #103.\nThe Customer Service recreates the Customer by deserializing the snapshot’s JSON\nand then loading and applying events #104 through #106. \nevent_id\n...\n103\n104\n105\n106\nEVENTS\nevent_type\n...\n...\nCredit\nReserved\nAddress\nChanged\nCredit\nReserved\nentity_type\n...\nCustomer\nCustomer\nCustomer\nCustomer\nentity_id\n...\n101\n101\n101\n101\nevent_data\n...\n{...}\n{...}\n{...}\n{...}\nevent_id\n...\n103\n...\n...\nSNAPSHOTS\nentity_type\n...\nCustomer\n...\n...\nsnapshot_data\n...\n{name: “...” , ...}\n...\n...\nevent_id\n...\n101\n...\n...\nFigure 6.8\nThe Customer Service recreates the Customer by deserializing the snapshot’s JSON and then \nloading and applying events #104 through #106.\n \n\n\n197\nDeveloping business logic using event sourcing\n6.1.6\nIdempotent message processing\nServices often consume messages from other applications or other services. A service\nmight, for example, consume domain events published by aggregates or command\nmessages sent by a saga orchestrator. As described in chapter 3, an important issue\nwhen developing a message consumer is ensuring that it’s idempotent, because a mes-\nsage broker might deliver the same message multiple times.\n A message consumer is idempotent if it can safely be invoked with the same mes-\nsage multiple times. The Eventuate Tram framework, for example, implements idem-\npotent message handling by detecting and discarding duplicate messages. It records\nthe ids of processed messages in a PROCESSED_MESSAGES table as part of the local\nACID transaction used by the business logic to create or update aggregates. If the ID\nof a message is in the PROCESSED_MESSAGES table, it’s a duplicate and can be dis-\ncarded. Event sourcing-based business logic must implement an equivalent mecha-\nnism. How this is done depends on whether the event store uses an RDBMS or a\nNoSQL database.\nIDEMPOTENT MESSAGE PROCESSING WITH AN RDBMS-BASED EVENT STORE\nIf an application uses an RDBMS-based event store, it can use an identical approach to\ndetect and discard duplicates messages. It inserts the message ID into the PROCESSED\n_MESSAGES table as part of the transaction that inserts events into the EVENTS table. \nIDEMPOTENT MESSAGE PROCESSING WHEN USING A NOSQL-BASED EVENT STORE\nA NoSQL-based event store, which has a limited transaction model, must use a different\nmechanism to implement idempotent message handling. A message consumer must\nsomehow atomically persist events and record the message ID. Fortunately, there’s a\nsimple solution. A message consumer stores the message’s ID in the events that are\ngenerated while processing it. It detects duplicates by verifying that none of an aggre-\ngate’s events contains the message ID.\n One challenge with using this approach is that processing a message might not\ngenerate any events. The lack of events means there’s no record of a message having\nbeen processed. A subsequent redelivery and reprocessing of the same message might\nresult in incorrect behavior. For example, consider the following scenario:\n1\nMessage A is processed but doesn’t update an aggregate.\n2\nMessage B is processed, and the message consumer updates the aggregate.\n3\nMessage A is redelivered, and because there’s no record of it having been pro-\ncessed, the message consumer updates the aggregate.\n4\nMessage B is processed again….\nIn this scenario, the redelivery of events results in a different and possibly erroneous\noutcome.\n One way to avoid this problem is to always publish an event. If an aggregate doesn’t\nemit an event, an application saves a pseudo event solely to record the message ID.\nEvent consumers must ignore these pseudo events. \n \n\n\n198\nCHAPTER 6\nDeveloping business logic with event sourcing\n6.1.7\nEvolving domain events\nEvent sourcing, at least conceptually, stores events forever—which is a double-edged\nsword. On one hand, it provides the application with an audit log of changes that’s\nguaranteed to be accurate. It also enables an application to reconstruct the historical\nstate of an aggregate. On the other hand, it creates a challenge, because the structure\nof events often changes over time.\n An application must potentially deal with multiple versions of events. For example,\na service that loads an Order aggregate could potentially need to fold multiple ver-\nsions of events. Similarly, an event subscriber might potentially see multiple versions.\n Let’s first look at the different ways that events can change, and then I’ll describe a\ncommonly used approach for handling changes.\nEVENT SCHEMA EVOLUTION\nConceptually, an event sourcing application has a schema that’s organized into\nthree levels:\nConsists of one or more aggregates\nDefines the events that each aggregate emits\nDefines the structure of the events\nTable 6.1 shows the different types of changes that can occur at each level.\nThese changes occur naturally as a service’s domain model evolves over time—for\nexample, when a service’s requirements change or as its developers gain deeper insight\ninto a domain and improve the domain model. At the schema level, developers add,\nremove, and rename aggregate classes. At the aggregate level, the types of events\nTable 6.1\nThe different ways that an application’s events can evolve\nLevel\nChange\nBackward compatible\nSchema\nDefine a new aggregate type\nYes\nRemove aggregate\nRemove an existing aggregate\nNo\nRename aggregate\nChange the name of an aggregate type\nNo\nAggregate\nAdd a new event type\nYes\nRemove event\nRemove an event type\nNo\nRename event\nChange the name of an event type\nNo\nEvent\nAdd a new field\nYes\nDelete field\nDelete a field\nNo\nRename field\nRename a field\nNo\nChange type of field\nChange the type of a field\nNo\n \n",
      "page_number": 215
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 224-234)",
      "start_page": 224,
      "end_page": 234,
      "detection_method": "topic_boundary",
      "content": "199\nDeveloping business logic using event sourcing\nemitted by a particular aggregate can change. Developers can change the structure of\nan event type by adding, removing, and changing the name or type of a field.\n Fortunately, many of these types of changes are backward-compatible changes. For\nexample, adding a field to an event is unlikely to impact consumers. A consumer\nignores unknown fields. Other changes, though, aren’t backward compatible. For\nexample, changing the name of an event or the name of a field requires consumers of\nthat event type to be changed. \nMANAGING SCHEMA CHANGES THROUGH UPCASTING\nIn the SQL database world, changes to a database schema are commonly handled\nusing schema migrations. Each schema change is represented by a migration, a SQL\nscript that changes the schema and migrates the data to a new schema. The schema\nmigrations are stored in a version control system and applied to a database using a\ntool such as Flyway.\n An event sourcing application can use a similar approach to handle non-backward-\ncompatible changes. But instead of migrating events to the new schema version in\nsitu, event sourcing frameworks transform events when they’re loaded from the event\nstore. A component commonly called an upcaster updates individual events from an\nold version to a newer version. As a result, the application code only ever deals with\nthe current event schema.\n Now that we’ve looked at how event sourcing works, let’s consider its benefits and\ndrawbacks. \n6.1.8\nBenefits of event sourcing\nEvent sourcing has both benefits and drawbacks. The benefits include the following:\nReliably publishes domain events\nPreserves the history of aggregates\nMostly avoids the O/R impedance mismatch problem\nProvides developers with a time machine\nLet’s examine each benefit in more detail.\nRELIABLY PUBLISHES DOMAIN EVENTS\nA major benefit of event sourcing is that it reliably publishes events whenever the state\nof an aggregate changes. That’s a good foundation for an event-driven microservice\narchitecture. Also, because each event can store the identity of the user who made the\nchange, event sourcing provides an audit log that’s guaranteed to be accurate. The\nstream of events can be used for a variety of other purposes, including notifying users,\napplication integration, analytics, and monitoring. \nPRESERVES THE HISTORY OF AGGREGATES\nAnother benefit of event sourcing is that it stores the entire history of each aggregate.\nYou can easily implement temporal queries that retrieve the past state of an aggregate.\nTo determine the state of an aggregate at a given point in time, you fold the events\n \n\n\n200\nCHAPTER 6\nDeveloping business logic with event sourcing\nthat occurred up until that point. It’s straightforward, for example, to calculate the\navailable credit of a customer at some point in the past. \nMOSTLY AVOIDS THE O/R IMPEDANCE MISMATCH PROBLEM\nEvent sourcing persists events rather than aggregating them. Events typically have a\nsimple, easily serializable structure. As mentioned earlier, a service can snapshot a\ncomplex aggregate by serializing a memento of its state, which adds a level of indirec-\ntion between an aggregate and its serialized representation. \nPROVIDES DEVELOPERS WITH A TIME MACHINE\nEvent sourcing stores a history of everything that’s happened in the lifetime of an\napplication. Imagine that the FTGO developers need to implement a new require-\nment to customers who added an item to their shopping cart and then removed it. A\ntraditional application wouldn’t preserve this information, so could only market to\ncustomers who add and remove items after the feature is implemented. In contrast, an\nevent sourcing-based application can immediately market to customers who have done\nthis in the past. It’s as if event sourcing provides developers with a time machine for\ntraveling to the past and implementing unanticipated requirements. \n6.1.9\nDrawbacks of event sourcing\nEvent sourcing isn’t a silver bullet. It has the following drawbacks:\nIt has a different programming model that has a learning curve.\nIt has the complexity of a messaging-based application.\nEvolving events can be tricky.\nDeleting data is tricky.\nQuerying the event store is challenging.\nLet’s look at each drawback.\nDIFFERENT PROGRAMMING MODEL THAT HAS A LEARNING CURVE\nIt’s a different and unfamiliar programming model, and that means a learning curve.\nIn order for an existing application to use event sourcing, you must rewrite its busi-\nness logic. Fortunately, that’s a fairly mechanical transformation that you can do when\nyou migrate your application to microservices. \nCOMPLEXITY OF A MESSAGING-BASED APPLICATION\nAnother drawback of event sourcing is that message brokers usually guarantee at-least-\nonce delivery. Event handlers that aren’t idempotent must detect and discard dupli-\ncate events. The event sourcing framework can help by assigning each event a mono-\ntonically increasing ID. An event handler can then detect duplicate events by tracking\nthe highest-seen event ID. This even happens automatically when event handlers\nupdate aggregates. \n \n\n\n201\nDeveloping business logic using event sourcing\nEVOLVING EVENTS CAN BE TRICKY\nWith event sourcing, the schema of events (and snapshots!) will evolve over time.\nBecause events are stored forever, aggregates potentially need to fold events corre-\nsponding to multiple schema versions. There’s a real risk that aggregates may become\nbloated with code to deal with all the different versions. As mentioned in section 6.1.7,\na good solution to this problem is to upgrade events to the latest version when they’re\nloaded from the event store. This approach separates the code that upgrades events\nfrom the aggregate, which simplifies the aggregates because they only need to apply\nthe latest version of the events. \nDELETING DATA IS TRICKY\nBecause one of the goals of event sourcing is to preserve the history of aggregates, it\nintentionally stores data forever. The traditional way to delete data when using event\nsourcing is to do a soft delete. An application deletes an aggregate by setting a\ndeleted flag. The aggregate will typically emit a Deleted event, which notifies any\ninterested consumers. Any code that accesses that aggregate can check the flag and\nact accordingly.\n Using a soft delete works well for many kinds of data. One challenge, however, is\ncomplying with the General Data Protection Regulation (GDPR), a European data\nprotection and privacy regulation that grants individuals the right to erasure (https://\ngdpr-info.eu/art-17-gdpr/). An application must have the ability to forget a user’s per-\nsonal information, such as their email address. The issue with an event sourcing-based\napplication is that the email address might either be stored in an AccountCreated\nevent or used as the primary key of an aggregate. The application somehow must for-\nget about the user without deleting the events.\n Encryption is one mechanism you can use to solve this problem. Each user has an\nencryption key, which is stored in a separate database table. The application uses that\nencryption key to encrypt any events containing the user’s personal information\nbefore storing them in an event store. When a user requests to be erased, the applica-\ntion deletes the encryption key record from the database table. The user’s personal\ninformation is effectively deleted, because the events can no longer be decrypted.\n Encrypting events solves most problems with erasing a user’s personal information.\nBut if some aspect of a user’s personal information, such as email address, is used as\nan aggregate ID, throwing away the encryption key may not be sufficient. For exam-\nple, section 6.2 describes an event store that has an entities table whose primary key\nis the aggregate ID. One solution to this problem is to use the technique of pseud-\nonymization, replacing the email address with a UUID token and using that as the\naggregate ID. The application stores the association between the UUID token and the\nemail address in a database table. When a user requests to be erased, the application\ndeletes the row for their email address from that table. This prevents the application\nfrom mapping the UUID back to the email address. \n \n\n\n202\nCHAPTER 6\nDeveloping business logic with event sourcing\nQUERYING THE EVENT STORE IS CHALLENGING\nImagine you need to find customers who have exhausted their credit limit. Because\nthere isn’t a column containing the credit, you can’t write SELECT * FROM CUSTOMER\nWHERE CREDIT_LIMIT = 0. Instead, you must use a more complex and potentially ineffi-\ncient query that has a nested SELECT to compute the credit limit by folding events that\nset the initial credit and adjusting it. To make matters worse, a NoSQL-based event\nstore will typically only support primary key-based lookup. Consequently, you must\nimplement queries using the CQRS approach described in chapter 7. \n6.2\nImplementing an event store\nAn application that uses event sourcing stores its events in an event store. An event store\nis a hybrid of a database and a message broker. It behaves as a database because it has\nan API for inserting and retrieving an aggregate’s events by primary key. And it\nbehaves as a message broker because it has an API for subscribing to events.\n There are a few different ways to implement an event store. One option is to imple-\nment your own event store and event sourcing framework. You can, for example, per-\nsist events in an RDBMS. A simple, albeit low-performance, way to publish events is for\nsubscribers to poll the EVENTS table for events. But, as noted in section 6.1.4, one chal-\nlenge is ensuring that a subscriber processes all events in order.\n Another option is to use a special-purpose event store, which typically provides a\nrich set of features and better performance and scalability. There are several of these\nto chose from:\nEvent Store—A .NET-based open source event store developed by Greg Young,\nan event sourcing pioneer (https://eventstore.org).\nLagom—A microservices framework developed by Lightbend, the company for-\nmerly known as Typesafe (www.lightbend.com/lagom-framework).\nAxon—An open source Java framework for developing event-driven applications\nthat use event sourcing and CQRS (www.axonframework.org).\nEventuate—Developed by my startup, Eventuate (http://eventuate.io). There are\ntwo versions of Eventuate: Eventuate SaaS, a cloud service, and Eventuate Local,\nan Apache Kafka/RDBMS-based open source project.\nAlthough these frameworks differ in the details, the core concepts remain the same.\nBecause Eventuate is the framework I’m most familiar with, that’s the one I cover\nhere. It has a straightforward, easy-to-understand architecture that illustrates event\nsourcing concepts. You can use it in your applications, reimplement the concepts\nyourself, or apply what you learn here to build applications with one of the other\nevent sourcing frameworks.\n I begin the following sections by describing how the Eventuate Local event store\nworks. Then I describe the Eventuate Client framework for Java, an easy-to-use frame-\nwork for writing event sourcing-based business logic that uses the Eventuate Local\nevent store.\n \n\n\n203\nImplementing an event store\n6.2.1\nHow the Eventuate Local event store works\nEventuate Local is an open source event store. Figure 6.9 shows the architecture.\nEvents are stored in a database, such as MySQL. Applications insert and retrieve\naggregate events by primary key. Applications consume events from a message broker,\nsuch as Apache Kafka. A transaction log tailing mechanism propagates events from\nthe database to the message broker.\nLet’s look at the different Eventuate Local components, starting with the database\nschema.\nTHE SCHEMA OF EVENTUATE LOCAL’S EVENT DATABASE\nThe event database consists of three tables:\n\nevents—Stores the events\n\nentities—One row per entity\n\nsnapshots—Stores snapshots\nThe central table is the events table. The structure of this table is very similar to the\ntable shown in figure 6.2. Here’s its definition:\nevent_id\n102\n103\n...\nEVENTS\nEvent database\nEvent broker\nOrder topic\nEvent relay\nEvent relay\nApplication\nCustomer topic\nevent_type\nOrder\nCreated\nOrder\nApproved\n...\nentity_type\nOrder\nOrder\n...\nentity_id\n101\n101\n...\nevent_data\n{...}\n{...}\n...\nENTITIES\nentity_type\n...\nentity_version\n...\nentity_id\n...\n...\n...\nSNAPSHOTS\nentity_type\n...\nentity_version\n...\nentity_id\n...\n...\n...\nStores the events\nPublishes events stored\nin the database to\nthe message broker\nFigure 6.9\nThe architecture of Eventuate Local. It consists of an event database (such as MySQL) \nthat stores the events, an event broker (like Apache Kafka) that delivers events to subscribers, and an \nevent relay that publishes events stored in the event database to the event broker.\n \n\n\n204\nCHAPTER 6\nDeveloping business logic with event sourcing\ncreate table events (\nevent_id varchar(1000) PRIMARY KEY,\nevent_type varchar(1000),\nevent_data varchar(1000) NOT NULL,\nentity_type VARCHAR(1000) NOT NULL,\nentity_id VARCHAR(1000) NOT NULL,\ntriggering_event VARCHAR(1000)\n);\nThe triggering_event column is used to detect duplicate events/messages. It stores\nthe ID of the message/event whose processing generated this event.\n The entities table stores the current version of each entity. It’s used to imple-\nment optimistic locking. Here’s the definition of this table:\ncreate table entities (\nentity_type VARCHAR(1000),\nentity_id VARCHAR(1000),\nentity_version VARCHAR(1000) NOT NULL,\nPRIMARY KEY(entity_type, entity_id)\n);\nWhen an entity is created, a row is inserted into this table. Each time an entity is\nupdated, the entity_version column is updated.\n The snapshots table stores the snapshots of each entity. Here’s the definition of\nthis table:\ncreate table snapshots (\nentity_type VARCHAR(1000),\nentity_id VARCHAR(1000),\nentity_version VARCHAR(1000),\nsnapshot_type VARCHAR(1000) NOT NULL,\nsnapshot_json VARCHAR(1000) NOT NULL,\ntriggering_events VARCHAR(1000),\nPRIMARY KEY(entity_type, entity_id, entity_version)\n)\nThe entity_type and entity_id columns specify the snapshot’s entity. The snapshot\n_json column is the serialized representation of the snapshot, and the snapshot_type\nis its type. The entity_version specifies the version of the entity that this is a snap-\nshot of.\n The three operations supported by this schema are find(), create(), and\nupdate(). The find() operation queries the snapshots table to retrieve the latest\nsnapshot, if any. If a snapshot exists, the find() operation queries the events table to\nfind all events whose event_id is greater than the snapshot’s entity_version. Other-\nwise, find() retrieves all events for the specified entity. The find() operation also\nqueries the entity table to retrieve the entity’s current version.\n The create() operation inserts a row into the entity table and inserts the events\ninto the events table. The update() operation inserts events into the events table. It\n \n\n\n205\nImplementing an event store\nalso performs an optimistic locking check by updating the entity version in the\nentities table using this UPDATE statement:\nUPDATE entities SET entity_version = ?\nWHERE entity_type = ? and entity_id = ? and entity_version = ?\nThis statement verifies that the version is unchanged since it was retrieved by the find()\noperation. It also updates the entity_version to the new version. The update() opera-\ntion performs these updates within a transaction in order to ensure atomicity.\n Now that we’ve looked at how Eventuate Local stores an aggregate’s events and snap-\nshots, let’s see how a client subscribes to events using Eventuate Local’s event broker. \nCONSUMING EVENTS BY SUBSCRIBING TO EVENTUATE LOCAL’S EVENT BROKER\nServices consume events by subscribing to the event broker, which is implemented\nusing Apache Kafka. The event broker has a topic for each aggregate type. As described\nin chapter 3, a topic is a partitioned message channel. This enables consumers to scale\nhorizontally while preserving message ordering. The aggregate ID is used as the parti-\ntion key, which preserves the ordering of events published by a given aggregate. To\nconsume an aggregate’s events, a service subscribes to the aggregate’s topic.\n Let’s now look at the event relay—the glue between the event database and the\nevent broker. \nTHE EVENTUATE LOCAL EVENT RELAY PROPAGATES EVENTS FROM THE DATABASE TO \nTHE MESSAGE BROKER\nThe event relay propagates events inserted into the event database to the event bro-\nker. It uses transaction log tailing whenever possible and polling for other databases.\nFor example, the MySQL version of the event relay uses the MySQL master/slave rep-\nlication protocol. The event relay connects to the MySQL server as if it were a slave\nand reads the MySQL binlog, a record of updates made to the database. Inserts into\nthe EVENTS table, which correspond to events, are published to the appropriate\nApache Kafka topic. The event relay ignores any other kinds of changes.\n The event relay is deployed as a standalone process. In order to restart correctly,\nit periodically saves the current position in the binlog—filename and offset—in a\nspecial Apache Kafka topic. On startup, it first retrieves the last recorded position\nfrom the topic. The event relay then starts reading the MySQL binlog from that\nposition.\n The event database, message broker, and event relay comprise the event store.\nLet’s now look at the framework a Java application uses to access the event store. \n6.2.2\nThe Eventuate client framework for Java\nThe Eventuate client framework enables developers to write event sourcing-based\napplications that use the Eventuate Local event store. The framework, shown in fig-\nure 6.10, provides the foundation for developing event sourcing-based aggregates, ser-\nvices, and event handlers.\n \n\n\n206\nCHAPTER 6\nDeveloping business logic with event sourcing\nThe framework provides base classes for aggregates, commands, and events. There’s\nalso an AggregateRepository class that provides CRUD functionality. And the frame-\nwork has an API for subscribing to events.\n Let’s briefly look at each of the types shown in figure 6.10.\nDEFINING AGGREGATES WITH THE REFLECTIVEMUTABLECOMMANDPROCESSINGAGGREGATE CLASS\nReflectiveMutableCommandProcessingAggregate is the base class for aggregates. It’s\na generic class that has two type parameters: the first is the concrete aggregate class,\nand the second is the superclass of the aggregate’s command classes. As its rather\nlong name suggests, it uses reflection to dispatch command and events to the appro-\npriate method. Commands are dispatched to a process() method, and events to an\napply() method.\n The Order class you saw earlier extends ReflectiveMutableCommandProcessing-\nAggregate. The following listing shows the Order class.\npublic class Order extends ReflectiveMutableCommandProcessingAggregate<Order,\nOrderCommand> {\npublic List<Event> process(CreateOrderCommand command) { ... }\npublic void apply(OrderCreatedEvent event) { ... }\nListing 6.3\nThe Eventuate version of the Order class\nOrderService\nEventHandlers\ncreditReserved()\n«interface»\nOrderEvent\n«interface»\nOrderCommand\n«event»\nOrderCreated\n«command»\nCreateOrder\nOrder\nprocess()\napply()\nOrder\nService\ncreateOrder()\n«annotation»\nEvent\nSubscriber\n«interface»\nEvent\n«interface»\nCommand\n«abstract»\nReﬂectiveMutableCommand\nProcessingAggregate\nAggregate\nRepository\nEventuate client framework\nOrder Service\nsave()\nﬁnd()\nupdate()\nAbstract classes and interfaces that\napplication classes extend or implement\nFigure 6.10\nThe main classes and interfaces provided by the Eventuate client framework for Java\n \n\n\n207\nImplementing an event store\n...\n}\nThe two type parameters passed to ReflectiveMutableCommandProcessingAggregate\nare Order and OrderCommand, which is the base interface for Order’s commands. \nDEFINING AGGREGATE COMMANDS\nAn aggregate’s command classes must extend an aggregate-specific base interface,\nwhich itself must extend the Command interface. For example, the Order aggregate’s\ncommands extend OrderCommand:\npublic interface OrderCommand extends Command {\n}\npublic class CreateOrderCommand implements OrderCommand { ... }\nThe OrderCommand interface extends Command, and the CreateOrderCommand com-\nmand class extends OrderCommand. \nDEFINING DOMAIN EVENTS\nAn aggregate’s event classes must extend the Event interface, which is a marker inter-\nface with no methods. It’s also useful to define a common base interface, which\nextends Event for all of an aggregate’s event classes. For example, here’s the defini-\ntion of the OrderCreated event:\ninterface OrderEvent extends Event {\n}\npublic class OrderCreated extends OrderEvent { ... }\nThe OrderCreated event class extends OrderEvent, which is the base interface for the\nOrder aggregate’s event classes. The OrderEvent interface extends Event. \nCREATING, FINDING, AND UPDATING AGGREGATES WITH THE AGGREGATEREPOSITORY CLASS\nThe framework provides several ways to create, find, and update aggregates. The sim-\nplest approach, which I describe here, is to use an AggregateRepository. Aggregate-\nRepository is a generic class that’s parameterized by the aggregate class and the\naggregate’s base command class. It provides three overloaded methods:\n\nsave()—Creates an aggregate\n\nfind()—Finds an aggregate\n\nupdate()—Updates an aggregate\nThe save () and update() methods are particularly convenient because they encapsu-\nlate the boilerplate code required for creating and updating aggregates. For instance,\nsave() takes a command object as a parameter and performs the following steps:\n1\nInstantiates the aggregate using its default constructor\n2\nInvokes process() to process the command\n \n\n\n208\nCHAPTER 6\nDeveloping business logic with event sourcing\n3\nApplies the generated events by calling apply()\n4\nSaves the generated events in the event store\nThe update() method is similar. It has two parameters, an aggregate ID and a com-\nmand, and performs the following steps:\n1\nRetrieves the aggregate from the event store\n2\nInvokes process() to process the command\n3\nApplies the generated events by calling apply()\n4\nSaves the generated events in the event store\nThe AggregateRepository class is primarily used by services, which create and update\naggregates in response to external requests. For example, the following listing shows\nhow OrderService uses an AggregateRepository to create an Order.\npublic class OrderService {\nprivate AggregateRepository<Order, OrderCommand> orderRepository;\npublic OrderService(AggregateRepository<Order, OrderCommand> orderRepository)\n{\nthis.orderRepository = orderRepository;\n}\npublic EntityWithIdAndVersion<Order> createOrder(OrderDetails orderDetails) {\nreturn orderRepository.save(new CreateOrder(orderDetails));\n}\n}\nOrderService is injected with an AggregateRepository for Orders. Its create()\nmethod invokes AggregateRepository.save() with a CreateOrder command. \nSUBSCRIBING TO DOMAIN EVENTS\nThe Eventuate Client framework also provides an API for writing event handlers. List-\ning 6.5 shows an event handler for CreditReserved events. The @EventSubscriber\nannotation specifies the ID of the durable subscription. Events that are published when\nthe subscriber isn’t running will be delivered when it starts up. The @EventHandler-\nMethod annotation identifies the creditReserved() method as an event handler.\n@EventSubscriber(id=\"orderServiceEventHandlers\")\npublic class OrderServiceEventHandlers {\n@EventHandlerMethod\npublic void creditReserved(EventHandlerContext<CreditReserved> ctx) {\nCreditReserved event = ctx.getEvent();\n...\n}\nListing 6.4\nOrderService uses an AggregateRepository\nListing 6.5\nAn event handler for OrderCreatedEvent\n \n\n\n209\nUsing sagas and event sourcing together\nAn event handler has a parameter of type EventHandlerContext, which contains the\nevent and its metadata.\n Now that we’ve looked at how to write event sourcing-based business logic using\nthe Eventuate client framework, let’s look at how to use event sourcing-based business\nlogic with sagas. \n6.3\nUsing sagas and event sourcing together\nImagine you’ve implemented one or more services using event sourcing. You’ve prob-\nably written services similar to the one shown in listing 6.4. But if you’ve read chapter 4,\nyou know that services often need to initiate and participate in sagas, sequences of\nlocal transactions used to maintain data consistency across services. For example,\nOrder Service uses a saga to validate an Order. Kitchen Service, Consumer Service,\nand Accounting Service participate in that saga. Consequently, you must integrate\nsagas and event sourcing-based business logic.\n Event sourcing makes it easy to use choreography-based sagas. The participants\nexchange the domain events emitted by their aggregates. Each participant’s aggre-\ngates handle events by processing commands and emitting new events. You need to\nwrite the aggregates and the event handler classes, which update the aggregates.\n But integrating event sourcing-based business logic with orchestration-based sagas\ncan be more challenging. That’s because the event store’s concept of a transaction\nmight be quite limited. When using some event stores, an application can only create\nor update a single aggregate and publish the resulting event(s). But each step of a\nsaga consists of several actions that must be performed atomically:\nSaga creation—A service that initiates a saga must atomically create or update an\naggregate and create the saga orchestrator. For example, Order Service’s\ncreateOrder() method must create an Order aggregate and a CreateOrderSaga.\nSaga orchestration—A saga orchestrator must atomically consume replies, update\nits state, and send command messages.\nSaga participants—Saga participants, such as Kitchen Service and Order Service,\nmust atomically consume messages, detect and discard duplicates, create or\nupdate aggregates, and send reply messages.\nBecause of this mismatch between these requirements and the transactional capabili-\nties of an event store, integrating orchestration-based sagas and event sourcing poten-\ntially creates some interesting challenges.\n A key factor in determining the ease of integrating event sourcing and orchestration-\nbased sagas is whether the event store uses an RDBMS or a NoSQL database. The\nEventuate Tram saga framework described in chapter 4 and the underlying Tram mes-\nsaging framework described in chapter 3 rely on flexible ACID transactions provided\nby the RDBMS. The saga orchestrator and the saga participants use ACID transactions\nto atomically update their databases and exchange messages. If the application uses\nan RDBMS-based event store, such as Eventuate Local, then it can cheat and invoke the\n \n",
      "page_number": 224
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 235-245)",
      "start_page": 235,
      "end_page": 245,
      "detection_method": "topic_boundary",
      "content": "210\nCHAPTER 6\nDeveloping business logic with event sourcing\nEventuate Tram saga framework and update the event store within an ACID transac-\ntion. But if the event store uses a NoSQL database, which can’t participate in the same\ntransaction as the Eventuate Tram saga framework, it will have to take a different\napproach.\n Let’s take a closer look at some of the different scenarios and issues you’ll need to\naddress:\nImplementing choreography-based sagas\nCreating an orchestration-based saga\nImplementing an event sourcing-based saga participant\nImplementing saga orchestrators using event sourcing\nWe’ll begin by looking at how to implement choreography-based sagas using event\nsourcing.\n6.3.1\nImplementing choreography-based sagas using event sourcing\nThe event-driven nature of event sourcing makes it quite straightforward to imple-\nment choreography-based sagas. When an aggregate is updated, it emits an event.\nAn event handler for a different aggregate can consume that event and update its\naggregate. The event sourcing framework automatically makes each event handler\nidempotent.\n For example, chapter 4 discusses how to implement Create Order Saga using cho-\nreography. ConsumerService, KitchenService, and AccountingService subscribe to\nthe OrderService’s events and vice versa. Each service has an event handler similar\nto the one shown in listing 6.5. The event handler updates the corresponding aggre-\ngate, which emits another event.\n Event sourcing and choreography-based sagas work very well together. Event sourc-\ning provides the mechanisms that sagas need, including messaging-based IPC, mes-\nsage de-duplication, and atomic updating of state and message sending. Despite its\nsimplicity, choreography-based sagas have several drawbacks. I talk about some draw-\nbacks in chapter 4, but there’s a drawback that’s specific to event sourcing.\n The problem with using events for saga choreography is that events now have a\ndual purpose. Event sourcing uses events to represent state changes, but using events\nfor saga choreography requires an aggregate to emit an event even if there is no state\nchange. For example, if updating an aggregate would violate a business rule, then the\naggregate must emit an event to report the error. An even worse problem is when a\nsaga participant can’t create an aggregate. There’s no aggregate that can emit an\nerror event.\n Because of these kinds of issues, it’s best to implement more complex sagas using\norchestration. The following sections explain how to integrate orchestration-based\nsagas and event sourcing. As you’ll see, it involves solving some interesting problems.\n Let’s first look at how a service method such as OrderService.createOrder() cre-\nates a saga orchestrator. \n \n\n\n211\nUsing sagas and event sourcing together\n6.3.2\nCreating an orchestration-based saga\nSaga orchestrators are created by some service methods. Other service methods, such\nas OrderService.createOrder(), do two things: create or update an aggregate and\ncreate a saga orchestrator. The service must perform both actions in a way that guar-\nantees that if it does the first action, then the second action will be done eventually.\nHow the service ensures that both of these actions are performed depends on the\nkind of event store it uses.\nCREATING A SAGA ORCHESTRATOR WHEN USING AN RDBMS-BASED EVENT STORE\nIf a service uses an RDBMS-based event store, it can update the event store and create\na saga orchestrator within the same ACID transaction. For example, imagine that the\nOrderService uses Eventuate Local and the Eventuate Tram saga framework. Its\ncreateOrder() method would look like this:\nclass OrderService\n@Autowired\nprivate SagaManager<CreateOrderSagaState> createOrderSagaManager;\n@Transactional\n      \npublic EntityWithIdAndVersion<Order> createOrder(OrderDetails orderDetails) {\nEntityWithIdAndVersion<Order> order =\norderRepository.save(new CreateOrder(orderDetails));  \nCreateOrderSagaState data =\nnew CreateOrderSagaState(order.getId(), orderDetails);\n  \ncreateOrderSagaManager.create(data, Order.class, order.getId());\nreturn order;\n}\n...\nIt’s a combination of the OrderService in listing 6.4 and the OrderService described\nin chapter 4. Because Eventuate Local uses an RDBMS, it can participate in the same\nACID transaction as the Eventuate Tram saga framework. But if a service uses a\nNoSQL-based event store, creating a saga orchestrator isn’t as straightforward. \nCREATING A SAGA ORCHESTRATOR WHEN USING A NOSQL-BASED EVENT STORE\nA service that uses a NoSQL-based event store will most likely be unable to atomically\nupdate the event store and create a saga orchestrator. The saga orchestration frame-\nwork might use an entirely different database. Even if it uses the same NoSQL data-\nbase, the application won’t be able to create or update two different objects atomically\nbecause of the NoSQL database’s limited transaction model. Instead, a service must\nhave an event handler that creates the saga orchestrator in response to a domain\nevent emitted by the aggregate.\n For example, figure 6.11 shows how Order Service creates a CreateOrderSaga\nusing an event handler for the OrderCreated event. Order Service first creates an\nEnsure the createOrder() executes\nwithin a database transaction.\nCreate the Order \naggregate.\nCreate the \nCreateOrderSaga.\n \n\n\n212\nCHAPTER 6\nDeveloping business logic with event sourcing\nOrder aggregate and persists it in the event store. The event store publishes the Order-\nCreated event, which is consumed by the event handler. The event handler invokes\nthe Eventuate Tram saga framework to create a CreateOrderSaga.\nOne issue to keep in mind when writing an event handler that creates a saga orches-\ntrator is that it must handle duplicate events. At-least-once message delivery means\nthat the event handler that creates the saga might be invoked multiple times. It’s\nimportant to ensure that only one saga instance is created.\n A straightforward approach is to derive the ID of the saga from a unique attribute\nof the event. There are a couple of different options. One is to use the ID of the aggre-\ngate that emits the event as the ID of the saga. This works well for sagas that are cre-\nated in response to aggregate creation events.\n Another option is to use the event ID as the saga ID. Because event IDs are unique,\nthis will guarantee that the saga ID is unique. If an event is a duplicate, the event han-\ndler’s attempt to create the saga will fail because the ID already exists. This option is\nuseful when multiple instances of the same saga can exist for a given aggregate\ninstance.\n A service that uses an RDBMS-based event store can also use the same event-driven\napproach to create sagas. A benefit of this approach is that it promotes loose coupling\nbecause services such as OrderService no longer explicitly instantiate sagas.\n Now that we’ve looked at how to reliably create a saga orchestrator, let’s see how\nevent sourcing-based services can participate in orchestration-based sagas. \nCreate a CreateOrderSaga\nin response to an\nOrderCreated event.\nPersist an\nOrderCreated\nevent.\nOrderCreated\nOrderCreated\nOrder\nOrderCreated\nEventHandler\nCreateOrderSaga\nEvent store\nPersisted as\nOrder Service\nFigure 6.11\nUsing an event handler to reliably create a saga after a service creates an event \nsourcing-based aggregate\n \n\n\n213\nUsing sagas and event sourcing together\n6.3.3\nImplementing an event sourcing-based saga participant\nImagine that you used event sourcing to implement a service that needs to participate\nin an orchestration-based saga. Not surprisingly, if your service uses an RDBMS-based\nevent store such as Eventuate Local, you can easily ensure that it atomically processes\nsaga command messages and sends replies. It can update the event store as part of the\nACID transaction initiated by the Eventuate Tram framework. But you must use an\nentirely different approach if your service uses an event store that can’t participate in\nthe same transaction as the Eventuate Tram framework.\n You must address a couple of different issues:\nIdempotent command message handling\nAtomically sending a reply message\nLet’s first look at how to implement idempotent command message handlers.\nIDEMPOTENT COMMAND MESSAGE HANDLING\nThe first problem to solve is how an event sourcing-based saga participant can detect\nand discard duplicate messages in order to implement idempotent command message\nhandling. Fortunately, this is an easy problem to address using the idempotent mes-\nsage handling mechanism described earlier. A saga participant records the message\nID in the events that are generated when processing the message. Before updating an\naggregate, the saga participant verifies that it hasn’t processed the message before by\nlooking for the message ID in the events. \nATOMICALLY SENDING REPLY MESSAGES\nThe second problem to solve is how an event sourcing-based saga participant can\natomically send replies. In principle, a saga orchestrator could subscribe to the events\nemitted by an aggregate, but there are two problems with this approach. The first is\nthat a saga command might not actually change the state of an aggregate. In this sce-\nnario, the aggregate won’t emit an event, so no reply will be sent to the saga orchestra-\ntor. The second problem is that this approach requires the saga orchestrator to treat\nsaga participants that use event sourcing differently from those that don’t. That’s\nbecause in order to receive domain events, the saga orchestrator must subscribe to the\naggregate’s event channel in addition to its own reply channel.\n A better approach is for the saga participant to continue to send a reply message to\nthe saga orchestrator’s reply channel. But rather than send the reply message directly,\na saga participant uses a two-step process:\n1\nWhen a saga command handler creates or updates an aggregate, it arranges for\na SagaReplyRequested pseudo event to be saved in the event store along with\nthe real events emitted by the aggregate.\n2\nAn event handler for the SagaReplyRequested pseudo event uses the data con-\ntained in the event to construct the reply message, which it then writes to the\nsaga orchestrator’s reply channel.\nLet’s look at an example to see how this works.\n \n\n\n214\nCHAPTER 6\nDeveloping business logic with event sourcing\nEXAMPLE EVENT SOURCING-BASED SAGA PARTICIPANT\nThis example looks at Accounting Service, one of the participants of Create Order\nSaga. Figure 6.12 shows how Accounting Service handles the Authorize Command\nsent by the saga. Accounting Service is implemented using the Eventuate Saga frame-\nwork. The Eventuate Saga framework is an open source framework for writing sagas\nthat use event sourcing. It’s built on the Eventuate Client framework.\nThis figure shows how Create Order Saga and AccountingService interact. The\nsequence of events is as follows:\nAccountCreated\n....\nAccountAuthorized\nAccountAuthorized\nSagaReplyRequested\nEvent store\nEvent dispatcher\nEventuate API\nAccounting Service\nSagaReplyRequested\nOrder Service\nAggregate\nrepository\nSagaReply\nrequested\nEventHandler\nEventuate saga framework\nSaga command\ndispatcher\nAuthorize\ncommand\nAuthorize\nreply\nAccount\ncommand channel\nCreate order saga\nreply channel\nCreate\norder\nsaga\nAccount\nauthorize()\nAuthorize account\ncommand handler\nAuthorize\nthe account.\nSend command to\naccounting service.\nHandle SagaReply\nrequested event\nand send reply.\nEmit\nSagaReply\nrequested\nevent.\nFigure 6.12\nHow the event sourcing-based Accounting Service participates in Create \nOrder Saga\n \n\n\n215\nUsing sagas and event sourcing together\n1\nCreate Order Saga sends an AuthorizeAccount command to Accounting-\nService via a messaging channel. The Eventuate Saga framework’s SagaCommand-\nDispatcher invokes AccountingServiceCommandHandler to handle the command\nmessage.\n2\nAccountingServiceCommandHandler sends the command to the specified\nAccount aggregate.\n3\nThe aggregate emits two events, AccountAuthorized and SagaReplyRequested-\nEvent.\n4\nSagaReplyRequestedEventHandler handles SagaReplyRequestedEvent by send-\ning a reply message to CreateOrderSaga.\nThe AccountingServiceCommandHandler shown in the following listing handles the\nAuthorizeAccount command message by calling AggregateRepository.update() to\nupdate the Account aggregate.\npublic class AccountingServiceCommandHandler {\n@Autowired\nprivate AggregateRepository<Account, AccountCommand> accountRepository;\npublic void authorize(CommandMessage<AuthorizeCommand> cm) {\nAuthorizeCommand command = cm.getCommand();\naccountRepository.update(command.getOrderId(),\ncommand,\nreplyingTo(cm)\n.catching(AccountDisabledException.class,\n() -> withFailure(new AccountDisabledReply()))\n.build());\n}\n...\nThe authorize() method invokes an AggregateRepository to update the Account\naggregate. The third argument to update(), which is the UpdateOptions, is computed\nby this expression:\nreplyingTo(cm)\n.catching(AccountDisabledException.class,\n() -> withFailure(new AccountDisabledReply()))\n.build()\nThese UpdateOptions configure the update() method to do the following:\n1\nUse the message id as an idempotency key to ensure that the message is pro-\ncessed exactly once. As mentioned earlier, the Eventuate framework stores the\nidempotency key in all generated events, enabling it to detect and ignore dupli-\ncate attempts to update an aggregate.\nListing 6.6\nHandles command messages sent by sagas\n \n\n\n216\nCHAPTER 6\nDeveloping business logic with event sourcing\n2\nAdd a SagaReplyRequestedEvent pseudo event to the list of events saved in the\nevent store. When SagaReplyRequestedEventHandler receives the SagaReply-\nRequestedEvent pseudo event, it sends a reply to the CreateOrderSaga’s reply\nchannel.\n3\nSend an AccountDisabledReply instead of the default error reply when the\naggregate throws an AccountDisabledException.\nNow that we’ve looked at how to implement saga participants using event sourcing,\nlet’s find out how to implement saga orchestrators. \n6.3.4\nImplementing saga orchestrators using event sourcing\nSo far in this section, I’ve described how event sourcing-based services can initiate\nand participate in sagas. You can also use event sourcing to implement saga orches-\ntrators. This will enable you to develop applications that are entirely based on an\nevent store.\n There are three key design problems you must solve when implementing a saga\norchestrator:\n1\nHow can you persist a saga orchestrator?\n2\nHow can you atomically change the state of the orchestrator and send com-\nmand messages?\n3\nHow can you ensure that a saga orchestrator processes reply messages exactly\nonce?\nChapter 4 discusses how to implement an RDBMS-based saga orchestrator. Let’s look\nat how to solve these problems when using event sourcing.\nPERSISTING A SAGA ORCHESTRATOR USING EVENT SOURCING\nA saga orchestrator has a very simple lifecycle. First, it’s created. Then it’s updated in\nresponse to replies from saga participants. We can, therefore, persist a saga using the\nfollowing events:\n\nSagaOrchestratorCreated—The saga orchestrator has been created.\n\nSagaOrchestratorUpdated—The saga orchestrator has been updated.\nA saga orchestrator emits a SagaOrchestratorCreated event when it’s created and a\nSagaOrchestratorUpdated event when it has been updated. These events contain the\ndata necessary to re-create the state of the saga orchestrator. For example, the events\nfor CreateOrderSaga, described in chapter 4, would contain a serialized (for example,\nJSON) CreateOrderSagaState. \nSENDING COMMAND MESSAGES RELIABLY\nAnother key design issue is how to atomically update the state of the saga and send a\ncommand. As described in chapter 4, the Eventuate Tram-based saga implementa-\ntion does this by updating the orchestrator and inserting the command message\ninto a message table as part of the same transaction. An application that uses an\n \n\n\n217\nUsing sagas and event sourcing together\nRDBMS-based event store, such as Eventuate Local, can use the same approach. An\napplication that uses a NoSQL-based event store, such as Eventuate SaaS, can use an\nanalogous approach, despite having a very limited transaction model.\n The trick is to persist a SagaCommandEvent, which represents a command to send.\nAn event handler then subscribes to SagaCommandEvents and sends each command\nmessage to the appropriate channel. Figure 6.13 shows how this works.\nThe saga orchestrator uses a two-step process to send commands:\n1\nA saga orchestrator emits a SagaCommandEvent for each command that it wants\nto send. SagaCommandEvent contains all the data needed to send the command,\nsuch as the destination channel and the command object. These events are per-\nsisted in the event store.\n2\nAn event handler processes these SagaCommandEvents and sends command\nmessages to the destination message channel.\nThis two-step approach guarantees that the command will be sent at least once.\n Because the event store provides at-least-once delivery, an event handler might be\ninvoked multiple times with the same event. That will cause the event handler for\nSagaCommandEvents to send duplicate command messages. Fortunately, though, a\nsaga participant can easily detect and discard duplicate commands using the following\n2. Handle SagaCommandEvent\nby sending a command.\n1. Emit a SagaCommandEvent\nfor each command to send.\nSagaCommandEvent\nSagaCreatedEvent\nSagaCommandEvent\nSagaUpdatedEvent\nSagaCommandEvent\n«saga»\nCreateOrderSaga\nSagaCommand\nEventHandler\nEvent store\nPersisted as\nService\nService Command\nChannel\nSends\ncommand\nMessage broker\nFigure 6.13\nHow an event sourcing-based saga orchestrator sends commands to saga participants\n \n\n\n218\nCHAPTER 6\nDeveloping business logic with event sourcing\nmechanism. The ID of SagaCommandEvent, which is guaranteed to be unique, is used\nas the ID of the command message. As a result, the duplicate messages will have the\nsame ID. A saga participant that receives a duplicate command message will discard it\nusing the mechanism described earlier. \nPROCESSING REPLIES EXACTLY ONCE\nA saga orchestrator also needs to detect and discard duplicate reply messages, which it\ncan do using the mechanism described earlier. The orchestrator stores the reply mes-\nsage’s ID in the events that it emits when processing the reply. It can then easily deter-\nmine whether a message is a duplicate.\n As you can see, event sourcing is a good foundation for implementing sagas. This\nis in addition to the other benefits of event sourcing, including the inherently reli-\nable generation of events whenever data changes, reliable audit logging, and the\nability to do temporal queries. Event sourcing isn’t a silver bullet, though. It involves\na significant learning curve. Evolving the event schema isn’t always straightforward.\nBut despite these drawbacks, event sourcing has a major role to play in a micro-\nservice architecture. In the next chapter, we’ll switch gears and look at how to tackle\na different distributed data management challenge in a microservice architecture:\nqueries. I’ll describe how to implement queries that retrieve data scattered across\nmultiple services. \nSummary\nEvent sourcing persists an aggregate as a sequence of events. Each event rep-\nresents either the creation of the aggregate or a state change. An application\nrecreates the state of an aggregate by replaying events. Event sourcing preserves\nthe history of a domain object, provides an accurate audit log, and reliably pub-\nlishes domain events.\nSnapshots improve performance by reducing the number of events that must\nbe replayed.\nEvents are stored in an event store, a hybrid of a database and a message broker.\nWhen a service saves an event in an event store, it delivers the event to subscribers.\nEventuate Local is an open source event store based on MySQL and Apache\nKafka. Developers use the Eventuate client framework to write aggregates and\nevent handlers.\nOne challenge with using event sourcing is handling the evolution of events. An\napplication potentially must handle multiple event versions when replaying\nevents. A good solution is to use upcasting, which upgrades events to the latest\nversion when they’re loaded from the event store.\nDeleting data in an event sourcing application is tricky. An application must use\ntechniques such as encryption and pseudonymization in order to comply with\nregulations like the European Union’s GDPR that requires an application to\nerase an individual’s data.\n \n\n\n219\nSummary\nEvent sourcing is a simple way to implement choreography-based sagas. Ser-\nvices have event handlers that listen to the events published by event sourcing-\nbased aggregates.\nEvent sourcing is a good way to implement saga orchestrators. As a result, you\ncan write applications that exclusively use an event store. \n \n\n\n220\nImplementing queries in a\nmicroservice architecture\nMary and her team were just starting to get comfortable with the idea of using sagas\nto maintain data consistency. Then they discovered that transaction management\nwasn’t the only distributed data-related challenge they had to worry about when\nmigrating the FTGO application to microservices. They also had to figure out how\nto implement queries.\n In order to support the UI, the FTGO application implements a variety of\nquery operations. Implementing these queries in the existing monolithic applica-\ntion is relatively straightforward, because it has a single database. For the most\npart, all the FTGO developers needed to do was write SQL SELECT statements\nand define the necessary indexes. As Mary discovered, writing queries in a micro-\nservice architecture is challenging. Queries often need to retrieve data that’s scattered\nThis chapter covers\nThe challenges of querying data in a microservice \narchitecture\nWhen and how to implement queries using the \nAPI composition pattern\nWhen and how to implement queries using the \nCommand query responsibility segregation \n(CQRS) pattern\n \n",
      "page_number": 235
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 246-260)",
      "start_page": 246,
      "end_page": 260,
      "detection_method": "topic_boundary",
      "content": "221\nQuerying using the API composition pattern\namong the databases owned by multiple services. You can’t, however, use a traditional\ndistributed query mechanism, because even if it were technically possible, it violates\nencapsulation.\n Consider, for example, the query operations for the FTGO application described\nin chapter 2. Some queries retrieve data that’s owned by just one service. The find-\nConsumerProfile() query, for example, returns data from Consumer Service. But\nother FTGO query operations, such as findOrder() and findOrderHistory(), return\ndata owned by multiple services. Implementing these query operations is not as\nstraightforward.\n There are two different patterns for implementing query operations in a microser-\nvice architecture:\nThe API composition pattern—This is the simplest approach and should be used\nwhenever possible. It works by making clients of the services that own the data\nresponsible for invoking the services and combining the results.\nThe Command query responsibility segregation (CQRS) pattern—This is more power-\nful than the API composition pattern, but it’s also more complex. It maintains\none or more view databases whose sole purpose is to support queries.\nAfter discussing these two patterns, I will talk about how to design CQRS views, fol-\nlowed by the implementation of an example view. Let’s start by taking a look at the\nAPI composition pattern.\n7.1\nQuerying using the API composition pattern\nThe FTGO application implements numerous query operations. Some queries, as\nmentioned earlier, retrieve data from a single service. Implementing these queries is\nusually straightforward—although later in this chapter, when I cover the CQRS pat-\ntern, you’ll see examples of single service queries that are challenging to implement.\n There are also queries that retrieve data from multiple services. In this section, I\ndescribe the findOrder() query operation, which is an example of a query that\nretrieves data from multiple services. I explain the challenges that often crop up when\nimplementing this type of query in a microservice architecture. I then describe the\nAPI composition pattern and show how you can use it to implement queries such as\nfindOrder().\n7.1.1\nThe findOrder() query operation\nThe findOrder() operation retrieves an order by its primary key. It takes an orderId\nas a parameter and returns an OrderDetails object, which contains information\nabout the order. As shown in figure 7.1, this operation is called by a frontend module,\nsuch as a mobile device or a web application, that implements the Order Status view.\n The information displayed by the Order Status view includes basic information\nabout the order, including its status, payment status, status of the order from the\n \n\n\n222\nCHAPTER 7\nImplementing queries in a microservice architecture\nrestaurant’s perspective, and delivery status, including its location and estimated deliv-\nery time if in transit.\n Because its data resides in a single database, the monolithic FTGO application can\neasily retrieve the order details by executing a single SELECT statement that joins the\nvarious tables. In contrast, in the microservices-based version of the FTGO applica-\ntion, the data is scattered around the following services:\n\nOrder Service—Basic order information, including the details and status\n\nKitchen Service—Status of the order from the restaurant’s perspective and the\nestimated time it will be ready for pickup\n\nDelivery Service—The order’s delivery status, estimated delivery information,\nand its current location\n\nAccounting Service—The order’s payment status\nAny client that needs the order details must ask all of these services. \n7.1.2\nOverview of the API composition pattern\nOne way to implement query operations, such as findOrder(), that retrieve data owned\nby multiple services is to use the API composition pattern. This pattern implements a\nOrder\nOrder Service\nid:3492-2323\nrestaurant:Ajanta\nTicket\nKitchen Service\nFTGO application\nOrderDetails ﬁndOrder(orderId)\nFTGO frontend\nOrder status view\nOrder id:\nRestaurant:\nStatus:\nETA:\nPayment:\n3492-2323\nAjanta\nEn route\n6:25 pm\nPaid\nid:3492-2323\nstatus:PREPARED\nDelivery\nDelivery Service\nid:45-4545\norderId:3492-2323\nstatus:ENROUTE\neta:6:25 pm\nBill\nAccounting Service\nid:343-45611\norderId:3492-2323\nstatus:PAID\nOrder status\nData from multiple services\nMobile device or web application\nFigure 7.1\nThe findOrder() operation is invoked by a FTGO frontend module and returns the \ndetails of an Order.\n \n\n\n223\nQuerying using the API composition pattern\nquery operation by invoking the services that own the data and combining the results.\nFigure 7.2 shows the structure of this pattern. It has two types of participants:\nAn API composer—This implements the query operation by querying the pro-\nvider services.\nA provider service—This is a service that owns some of the data that the query\nreturns.\nFigure 7.2 shows three provider services. The API composer implements the query by\nretrieving data from the provider services and combining the results. An API com-\nposer might be a client, such as a web application, that needs the data to render a web\npage. Alternatively, it might be a service, such as an API gateway and its Backends for\nfrontends variant described in chapter 8, which exposes the query operation as an API\nendpoint.\nWhether you can use this pattern to implement a particular query operation depends\non several factors, including how the data is partitioned, the capabilities of the APIs\nexposed by the services that own the data, and the capabilities of the databases used\nby the services. For instance, even if the Provider services have APIs for retrieving the\nPattern: API composition\nImplement a query that retrieves data from several services by querying each service\nvia its API and combining the results. See http://microservices.io/patterns/data/api-\ncomposition.html.\nquery()\nAPI composer\nProvider Service A\nDatabase A\nqueryA()\nProvider Service B\nDatabase B\nqueryB()\nProvider Service C\nDatabase C\nqueryC()\nImplements the query operation\nby invoking the providers and\ncombining the results.\nServices that own data\nFigure 7.2\nThe API composition pattern consists of an API composer and two or more provider \nservices. The API composer implements a query by querying the providers and combining the results.\n \n\n\n224\nCHAPTER 7\nImplementing queries in a microservice architecture\nrequired data, the aggregator might need to perform an inefficient, in-memory join\nof large datasets. Later on, you’ll see examples of query operations that can’t be\nimplemented using this pattern. Fortunately, though, there are many scenarios where\nthis pattern is applicable. To see it in action, we’ll look at an example. \n7.1.3\nImplementing the findOrder() query operation using the API \ncomposition pattern\nThe findOrder() query operation corresponds to a simple primary key-based equi-\njoin query. It’s reasonable to expect that each of the Provider services has an API end-\npoint for retrieving the required data by orderId. Consequently, the findOrder()\nquery operation is an excellent candidate to be implemented by the API composition\npattern. The API composer invokes the four services and combines the results together.\nFigure 7.3 shows the design of the Find Order Composer.\nIn this example, the API composer is a service that exposes the query as a REST endpoint.\nThe Provider services also implement REST APIs. But the concept is the same if the ser-\nvices used some other interprocess communication protocol, such as gRPC, instead of\nHTTP. The Find Order Composer implements a REST endpoint GET /order/{orderId}.\nIt invokes the four services and joins the responses using the orderId. Each Provider ser-\nvice implements a REST endpoint that returns a response corresponding to a single\naggregate. The OrderService retrieves its version of an Order by primary key and the\nother services use the orderId as a foreign key to retrieve their aggregates.\n As you can see, the API composition pattern is quite simple. Let’s look at a couple\nof design issues you must address when applying this pattern. \nFind Order\nComposer\nOrder Service\n«aggregate»\nOrder\nGET/orders/\n{orderId}\nGET/charges?\norderId=\n{orderId}\nGET/tickets?\norderId=\n{orderId}\nGET/deliveries?\norderId=\n{orderId}\nKitchen Service\n«aggregate»\nRestaurantOrder\nDelivery Service\n«aggregate»\nDelivery\nAccounting Service\n«aggregate»\nCharge\nGET/order/{orderId}\nFigure 7.3\nImplementing findOrder() using the API composition pattern\n \n\n\n225\nQuerying using the API composition pattern\n7.1.4\nAPI composition design issues\nWhen using this pattern, you have to address a couple of design issues:\nDeciding which component in your architecture is the query operation’s API\ncomposer\nHow to write efficient aggregation logic\nLet’s look at each issue.\nWHO PLAYS THE ROLE OF THE API COMPOSER?\nOne decision that you must make is who plays the role of the query operation’s API\ncomposer. You have three options. The first option, shown in figure 7.4, is for a client of\nthe services to be the API composer.\nA frontend client such as a web application, that implements the Order Status view\nand is running on the same LAN, could efficiently retrieve the order details using this\npattern. But as you’ll learn in chapter 8, this option is probably not practical for cli-\nents that are outside of the firewall and access services via a slower network.\n The second option, shown in figure 7.5, is for an API gateway, which implements the\napplication’s external API, to play the role of an API composer for a query operation.\n This option makes sense if the query operation is part of the application’s external\nAPI. Instead of routing a request to another service, the API gateway implements the\nAPI composition logic. This approach enables a client, such as a mobile device, that’s\nrunning outside of the firewall to efficiently retrieve data from numerous services with\na single API call. I discuss the API gateway in chapter 8.\n The third option, shown in figure 7.6, is to implement an API composer as a stand-\nalone service.\nClient, such as web application\nOrder\nService\nDelivery\nService\nKitchen\nService\nAccounting\nService\nAPI composer\nFigure 7.4\nImplementing API \ncomposition in a client. The \nclient queries the provider \nservices to retrieve the data.\n \n\n\n226\nCHAPTER 7\nImplementing queries in a microservice architecture\nAPI gateway\nExternal client, such as\nmobile application\nOrder\nService\nDelivery\nService\nKitchen\nService\nAccounting\nService\nﬁndOrder()\nAPI composer\nFigure 7.5\nImplementing \nAPI composition in the API \ngateway. The API queries the \nprovider services to retrieve \nthe data, combines the \nresults, and returns a \nresponse to the client.\nOrder\nService\nDelivery\nService\nKitchen\nService\nAccounting\nService\nFind Order Service\nClients\nﬁndOrder()\nAPI composer\nFigure 7.6\nImplement a query \noperation used by multiple \nclients and services as a \nstandalone service.\n \n\n\n227\nQuerying using the API composition pattern\nYou should use this option for a query operation that’s used internally by multiple ser-\nvices. This operation can also be used for externally accessible query operations whose\naggregation logic is too complex to be part of an API gateway. \nAPI COMPOSERS SHOULD USE A REACTIVE PROGRAMMING MODEL\nWhen developing a distributed system, minimizing latency is an ever-present concern.\nWhenever possible, an API composer should call provider services in parallel in order to\nminimize the response time for a query operation. The Find Order Aggregator\nshould, for example, invoke the four services concurrently because there are no\ndependencies between the calls. Sometimes, though, an API composer needs the result\nof one Provider service in order to invoke another service. In this case, it will need to\ninvoke some—but hopefully not all—of the provider services sequentially.\n The logic to efficiently execute a mixture of sequential and parallel service invo-\ncations can be complex. In order for an API composer to be maintainable as well as\nperformant and scalable, it should use a reactive design based on Java Completable-\nFuture’s, RxJava observables, or some other equivalent abstraction. I discuss this topic\nfurther in chapter 8 when I cover the API gateway pattern. \n7.1.5\nThe benefits and drawbacks of the API composition pattern\nThis pattern is a simple and intuitive way to implement query operations in a micro-\nservice architecture. But it has some drawbacks:\nIncreased overhead\nRisk of reduced availability\nLack of transactional data consistency\nLet’s take a look at them.\nINCREASED OVERHEAD\nOne drawback of this pattern is the overhead of invoking multiple services and query-\ning multiple databases. In a monolithic application, a client can retrieve data with a\nsingle request, which will often execute a single database query. In comparison, using\nthe API composition pattern involves multiple requests and database queries. As a\nresult, more computing and network resources are required, increasing the cost of\nrunning the application. \nRISK OF REDUCED AVAILABILITY\nAnother drawback of this pattern is reduced availability. As described in chapter 3, the\navailability of an operation declines with the number of services that are involved.\nBecause the implementation of a query operation involves at least three services—the\nAPI composer and at least two provider services—its availability will be significantly less\nthan that of a single service. For example, if the availability of an individual service is\n99.5%, then the availability of the findOrder() endpoint, which invokes four provider\nservices, is 99.5%(4+1) = 97.5%!\n There are couple of strategies you can use to improve availability. The first strat-\negy is for the API composer to return previously cached data when a Provider service is\n \n\n\n228\nCHAPTER 7\nImplementing queries in a microservice architecture\nunavailable. An API composer sometimes caches the data returned by a Provider service in\norder to improve performance. It can also use this cache to improve availability. If a\nprovider is unavailable, the API composer can return data from the cache, though it\nmay be potentially stale.\n Another strategy for improving availability is for the API composer to return incom-\nplete data. For example, imagine that Kitchen Service is temporarily unavailable.\nThe API Composer for the findOrder() query operation could omit that service’s data\nfrom the response, because the UI can still display useful information. You’ll see more\ndetails on API design, caching, and reliability in chapter 8. \nLACK OF TRANSACTIONAL DATA CONSISTENCY\nAnother drawback of the API composition pattern is the lack of data consistency. A\nmonolithic application typically executes a query operation using a single database\ntransaction. ACID transactions—subject to the fine print about isolation levels—ensure\nthat an application has a consistent view of the data, even if it executes multiple data-\nbase queries. In contrast, the API composition pattern executes multiple database que-\nries against multiple databases. There’s a risk, therefore, that a query operation will\nreturn inconsistent data.\n For example, an Order retrieved from Order Service might be in the CANCELLED\nstate, whereas the corresponding Ticket retrieved from Kitchen Service might not\nyet have been cancelled. The API composer must resolve this discrepancy, which increases\nthe code complexity. To make matters worse, an API composer might not always be able\nto detect inconsistent data, and will return it to the client.\n Despite these drawbacks, the API composition pattern is extremely useful. You can\nuse it to implement many query operations. But there are some query operations that\ncan’t be efficiently implemented using this pattern. A query operation might, for\nexample, require the API composer to perform an in-memory join of large datasets.\n It’s usually better to implement these types of query operations using the CQRS\npattern. Let’s take a look at how this pattern works. \n7.2\nUsing the CQRS pattern\nMany enterprise applications use an RDBMS as the transactional system of record and\na text search database, such as Elasticsearch or Solr, for text search queries. Some\napplications keep the databases synchronized by writing to both simultaneously. Oth-\ners periodically copy data from the RDBMS to the text search engine. Applications\nwith this architecture leverage the strengths of multiple databases: the transactional\nproperties of the RDBMS and the querying capabilities of the text database.\nPattern: Command query responsibility segregation\nImplement a query that needs data from several services by using events to maintain\na read-only view that replicates data from the services. See http://microservices\n.io/patterns/data/cqrs.html.\n \n\n\n229\nUsing the CQRS pattern\nCQRS is a generalization of this kind of architecture. It maintains one or more view\ndatabases—not just text search databases—that implement one or more of the appli-\ncation’s queries. To understand why this is useful, we’ll look at some queries that can’t\nbe efficiently implemented using the API composition pattern. I’ll explain how CQRS\nworks and then talk about the benefits and drawbacks of CQRS. Let’s take a look at\nwhen you need to use CQRS.\n7.2.1\nMotivations for using CQRS\nThe API composition pattern is a good way to implement many queries that must\nretrieve data from multiple services. Unfortunately, it’s only a partial solution to the\nproblem of querying in a microservice architecture. That’s because there are multiple\nservice queries the API composition pattern can’t implement efficiently.\n What’s more, there are also single service queries that are challenging to imple-\nment. Perhaps the service’s database doesn’t efficiently support the query. Alterna-\ntively, it sometimes makes sense for a service to implement a query that retrieves data\nowned by a different service. Let’s take a look at these problems, starting with a multi-\nservice query that can’t be efficiently implemented using API composition.\nIMPLEMENTING THE FINDORDERHISTORY() QUERY OPERATION\nThe findOrderHistory() operation retrieves a consumer’s order history. It has sev-\neral parameters:\n\nconsumerId—Identifies the consumer\n\npagination—Page of results to return\n\nfilter—Filter criteria, including the max age of the orders to return, an\noptional order status, and optional keywords that match the restaurant name and\nmenu items\nThis query operation returns an OrderHistory object that contains a summary of the\nmatching orders sorted by increasing age. It’s called by the module that implements\nthe Order History view. This view displays a summary of each order, which includes\nthe order number, order status, order total, and estimated delivery time.\n On the surface, this operation is similar to the findOrder() query operation. The\nonly difference is that it returns multiple orders instead of just one. It may appear that\nthe API composer only has to execute the same query against each Provider service and\ncombine the results. Unfortunately, it’s not that simple.\n That’s because not all services store the attributes that are used for filtering or\nsorting. For example, one of the findOrderHistory() operation’s filter criteria is a\nkeyword that matches against a menu item. Only two of the services, Order Service\nand Kitchen Service, store an Order’s menu items. Neither Delivery Service nor\nAccounting Service stores the menu items, so can’t filter their data using this key-\nword. Similarly, neither Kitchen Service nor Delivery Service can sort by the\norderCreationDate attribute.\n \n\n\n230\nCHAPTER 7\nImplementing queries in a microservice architecture\n There are two ways an API composer could solve this problem. One solution is for the\nAPI composer to do an in-memory join, as shown in figure 7.7. It retrieves all orders for\nthe consumer from Delivery Service and Accounting Service and performs a join\nwith the orders retrieved from Order Service and Kitchen Service.\nThe drawback of this approach is that it potentially requires the API composer to retrieve\nand join large datasets, which is inefficient.\n The other solution is for the API composer to retrieve matching orders from Order\nService and Kitchen Service and then request orders from the other services by ID.\nBut this is only practical if those services have a bulk fetch API. Requesting orders\nindividually will likely be inefficient because of excessive network traffic.\n Queries such as findOrderHistory() require the API composer to duplicate the\nfunctionality of an RDBMS’s query execution engine. On one hand, this potentially\nmoves work from the less scalable database to the more scalable application. On the\nother hand, it’s less efficient. Also, developers should be writing business functionality,\nnot a query execution engine.\n Next I show you how to apply the CQRS pattern and use a separate datastore,\nwhich is designed to efficiently implement the findOrderHistory() query operation.\nFigure 7.7\nAPI composition can’t efficiently retrieve a consumer’s orders, because some providers, \nsuch as Delivery Service, don’t store the attributes used for filtering.\nFind orders\ncomposer\nOrder Service\n«aggregate»\nOrder\nGET/orders?\nconsumerId=\n&keyword=\nGET/charges?\nconsumerId=\nGET/tickets?\nconsumerId=\n&keyword=\nGET/deliveries?\nconsumerId=\nKitchen Service\n«aggregate»\nRestaurantOrder\nDelivery Service\n«aggregate»\nDelivery\nAccounting Service\n«aggregate»\nCharge\nGET/order?consumerId=&keyword=\nThese services don’t store the data needed for a keyword\nsearch, so will return all of a consumer’s orders.\n \n\n\n231\nUsing the CQRS pattern\nBut first, let’s look at an example of a query operation that’s challenging to imple-\nment, despite being local to a single service. \nA CHALLENGING SINGLE SERVICE QUERY: FINDAVAILABLERESTAURANTS()\nAs you’ve just seen, implementing queries that retrieve data from multiple services\ncan be challenging. But even queries that are local to a single service can be difficult\nto implement. There are a couple of reasons why this might be the case. One is\nbecause, as discussed shortly, sometimes it’s not appropriate for the service that owns\nthe data to implement the query. The other reason is that sometimes a service’s data-\nbase (or data model) doesn’t efficiently support the query.\n Consider, for example, the findAvailableRestaurants() query operation. This\nquery finds the restaurants that are available to deliver to a given address at a given\ntime. The heart of this query is a geospatial (location-based) search for restaurants\nthat are within a certain distance of the delivery address. It’s a critical part of the order\nprocess and is invoked by the UI module that displays the available restaurants.\n The key challenge when implementing this query operation is performing an effi-\ncient geospatial query. How you implement the findAvailableRestaurants() query\ndepends on the capabilities of the database that stores the restaurants. For example,\nit’s straightforward to implement the findAvailableRestaurants() query using\neither MongoDB or the Postgres and MySQL geospatial extensions. These databases\nsupport geospatial datatypes, indexes, and queries. When using one of these databases,\nRestaurant Service persists a Restaurant as a database record that has a location\nattribute. It finds the available restaurants using a geospatial query that’s optimized by\na geospatial index on the location attribute.\n If the FTGO application stores restaurants in some other kind of database, imple-\nmenting the findAvailableRestaurant() query is more challenging. It must main-\ntain a replica of the restaurant data in a form that’s designed to support the geospatial\nquery. The application could, for example, use the Geospatial Indexing Library for\nDynamoDB (https://github.com/awslabs/dynamodb-geo) that uses a table as a geo-\nspatial index. Alternatively, the application could store a replica of the restaurant data\nin an entirely different type of database, a situation very similar to using a text search\ndatabase for text queries.\n The challenge with using replicas is keeping them up-to-date whenever the origi-\nnal data changes. As you’ll learn below, CQRS solves the problem of synchronizing\nreplicas. \nTHE NEED TO SEPARATE CONCERNS\nAnother reason why single service queries are challenging to implement is that some-\ntimes the service that owns the data shouldn’t be the one that implements the query.\nThe findAvailableRestaurants() query operation retrieves data that is owned by\nRestaurant Service. This service enables restaurant owners to manage their restau-\nrant’s profile and menu items. It stores various attributes of a restaurant, including its\nname, address, cuisines, menu, and opening hours. Given that this service owns the\n \n\n\n232\nCHAPTER 7\nImplementing queries in a microservice architecture\ndata, it makes sense, at least on the surface, for it to implement this query operation.\nBut data ownership isn’t the only factor to consider.\n You must also take into account the need to separate concerns and avoid overload-\ning services with too many responsibilities. For example, the primary responsibility\nof the team that develops Restaurant Service is enabling restaurant managers to\nmaintain their restaurants. That’s quite different from implementing a high-\nvolume, critical query. What’s more, if they were responsible for the findAvailable-\nRestaurants() query operation, the team would constantly live in fear of deploying a\nchange that prevented consumers from placing orders.\n It makes sense for Restaurant Service to merely provide the restaurant data to\nanother service that implements the findAvailableRestaurants() query operation\nand is most likely owned by the Order Service team. As with the findOrderHistory()\nquery operation, and when needing to maintain geospatial index, there’s a require-\nment to maintain an eventually consistent replica of some data in order to implement\na query. Let’s look at how to accomplish that using CQRS. \n7.2.2\nOverview of CQRS\nThe examples described in section 7.2.1 highlighted three problems that are commonly\nencountered when implementing queries in a microservice architecture:\nUsing the API composition pattern to retrieve data scattered across multiple\nservices results in expensive, inefficient in-memory joins.\nThe service that owns the data stores the data in a form or in a database that\ndoesn’t efficiently support the required query.\nThe need to separate concerns means that the service that owns the data isn’t\nthe service that should implement the query operation.\nThe solution to all three of these problems is to use the CQRS pattern.\nCQRS SEPARATES COMMANDS FROM QUERIES\nCommand Query Responsibility Segregation, as the name suggests, is all about segrega-\ntion, or the separation of concerns. As figure 7.8 shows, it splits a persistent data model\nand the modules that use it into two parts: the command side and the query side. The\ncommand side modules and data model implement create, update, and delete opera-\ntions (abbreviated CUD—for example, HTTP POSTs, PUTs, and DELETEs). The\nquery-side modules and data model implement queries (such as HTTP GETs). The\nquery side keeps its data model synchronized with the command-side data model by\nsubscribing to the events published by the command side.\n Both the non-CQRS and CQRS versions of the service have an API consisting of\nvarious CRUD operations. In a non-CQRS-based service, those operations are typically\nimplemented by a domain model that’s mapped to a database. For performance, a few\nqueries might bypass the domain model and access the database directly. A single per-\nsistent data model supports both commands and queries.\n \n\n\n233\nUsing the CQRS pattern\nIn a CQRS-based service, the command-side domain model handles CRUD operations\nand is mapped to its own database. It may also handle simple queries, such as non-\njoin, primary key-based queries. The command side publishes domain events when-\never its data changes. These events might be published using a framework such as\nEventuate Tram or using event sourcing.\n A separate query model handles the nontrivial queries. It’s much simpler than the\ncommand side because it’s not responsible for implementing the business rules. The\nquery side uses whatever kind of database makes sense for the queries that it must sup-\nport. The query side has event handlers that subscribe to domain events and update\nthe database or databases. There may even be multiple query models, one for each\ntype of query. \nCQRS AND QUERY-ONLY SERVICES\nNot only can CQRS be applied within a service, but you can also use this pattern to\ndefine query services. A query service has an API consisting of only query opera-\ntions—no command operations. It implements the query operations by querying a\ndatabase that it keeps up-to-date by subscribing to events published by one or more\nother services. A query-side service is a good way to implement a view that’s built by\nService\nCRUD\nCRUD operations\nR\nDomain model\nAggregate\nQuery\nbypass\nAggregate\nDatabase\nOne database for creates, updates, and deletes. A\nseparate database for queries. It is kept up-to-date\nby using events that are published whenever the\ncommand-side database changes.\nSingle database for all CRUD\nService\nCUD\nCRUD operations\nR\nCommand/domain model\nEvents\nCQRS\nNon-CQRS\nAggregate\nEvent\nhandler\nAggregate\nCommand-side\ndatabase\nQuery database\nQuery model\nFigure 7.8\nOn the left is the non-CQRS version of the service, and on the right is the CQRS version. \nCQRS restructures a service into command-side and query-side modules, which have separate \ndatabases.\n \n\n\n234\nCHAPTER 7\nImplementing queries in a microservice architecture\nsubscribing to events published by multiple services. This kind of view doesn’t belong\nto any particular service, so it makes sense to implement it as a standalone service. A\ngood example of such a service is Order History Service, which is a query service\nthat implements the findOrderHistory() query operation. As figure 7.9 shows, this\nservice subscribes to events published by several services, including Order Service,\nDelivery Service, and so on.\nOrder History Service has event handlers that subscribe to events published by sev-\neral services and update the Order History View Database. I describe the implemen-\ntation of this service in more detail in section 7.4.\n A query service is also a good way to implement a view that replicates data owned\nby a single service yet because of the need to separate concerns isn’t part of that service.\nFor example, the FTGO developers can define an Available Restaurants Service,\nwhich implements the findAvailableRestaurants() query operation described ear-\nlier. It subscribes to events published by Restaurant Service and updates a database\ndesigned for efficient geospatial queries.\n In many ways, CQRS is an event-based generalization of the popular approach of\nusing RDBMS as the system of record and a text search engine, such as Elasticsearch,\nto handle text queries. What’s different is that CQRS uses a broader range of database\nOrder Service\nKitchen Service\nOrder History\nService\nﬁndOrderHistory()\nﬁndOrder()\nDelivery Service\nAccounting Service\nOrder history\nview database\nEvent\nhandlers\nOrder\nevents\nTicket\nevents\nDelivery\nevents\nAccounting\nevents\nFigure 7.9\nThe design of Order History Service, which is a query-side service. It \nimplements the findOrderHistory() query operation by querying a database, which \nit maintains by subscribing to events published by multiple other services.\n \n\n\n235\nUsing the CQRS pattern\ntypes—not just a text search engine. Also, CQRS query-side views are updated in near\nreal time by subscribing to events.\n Let’s now look at the benefits and drawbacks of CQRS. \n7.2.3\nThe benefits of CQRS\nCQRS has both benefits and drawbacks. The benefits are as follows:\nEnables the efficient implementation of queries in a microservice architecture\nEnables the efficient implementation of diverse queries\nMakes querying possible in an event sourcing-based application\nImproves separation of concerns\nENABLES THE EFFICIENT IMPLEMENTATION OF QUERIES IN A MICROSERVICE ARCHITECTURE\nOne benefit of the CQRS pattern is that it efficiently implements queries that retrieve\ndata owned by multiple services. As described earlier, using the API composition pat-\ntern to implement queries sometimes results in expensive, inefficient in-memory joins\nof large datasets. For those queries, it’s more efficient to use an easily queried CQRS\nview that pre-joins the data from two or more services.\nENABLES THE EFFICIENT IMPLEMENTATION OF DIVERSE QUERIES\nAnother benefit of CQRS is that it enables an application or service to efficiently\nimplement a diverse set of queries. Attempting to support all queries using a single\npersistent data model is often challenging and in some cases impossible. Some\nNoSQL databases have very limited querying capabilities. Even when a database has\nextensions to support a particular kind of query, using a specialized database is often\nmore efficient. The CQRS pattern avoids the limitations of a single datastore by defin-\ning one or more views, each of which efficiently implements specific queries. \nENABLES QUERYING IN AN EVENT SOURCING-BASED APPLICATION\nCQRS also overcomes a major limitation of event sourcing. An event store only sup-\nports primary key-based queries. The CQRS pattern addresses this limitation by defin-\ning one or more views of the aggregates, which are kept up-to-date, by subscribing to\nthe streams of events that are published by the event sourcing-based aggregates. As a\nresult, an event sourcing-based application invariably uses CQRS. \nIMPROVES SEPARATION OF CONCERNS\nAnother benefit of CQRS is that it separates concerns. A domain model and its corre-\nsponding persistent data model don’t handle both commands and queries. The CQRS\npattern defines separate code modules and database schemas for the command and\nquery sides of a service. By separating concerns, the command side and query side are\nlikely to be simpler and easier to maintain.\n Moreover, CQRS enables the service that implements a query to be different than\nthe service that owns the data. For example, earlier I described how even though\nRestaurant Service owns the data that’s queried by the findAvailableRestaurants\nquery operation, it makes sense for another service to implement such a critical,\n \n",
      "page_number": 246
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 261-268)",
      "start_page": 261,
      "end_page": 268,
      "detection_method": "topic_boundary",
      "content": "236\nCHAPTER 7\nImplementing queries in a microservice architecture\nhigh-volume query. A CQRS query service maintains a view by subscribing to the events\npublished by the service or services that own the data. \n7.2.4\nThe drawbacks of CQRS\nEven though CQRS has several benefits, it also has significant drawbacks:\nMore complex architecture\nDealing with the replication lag\nLet’s look at these drawbacks, starting with the increased complexity.\nMORE COMPLEX ARCHITECTURE\nOne drawback of CQRS is that it adds complexity. Developers must write the query-\nside services that update and query the views. There is also the extra operational com-\nplexity of managing and operating the extra datastores. What’s more, an application\nmight use different types of databases, which adds further complexity for both devel-\nopers and operations. \nDEALING WITH THE REPLICATION LAG\nAnother drawback of CQRS is dealing with the “lag” between the command-side and\nthe query-side views. As you might expect, there’s delay between when the command\nside publishes an event and when that event is processed by the query side and the\nview updated. A client application that updates an aggregate and then immediately\nqueries a view may see the previous version of the aggregate. It must often be written\nin a way that avoids exposing these potential inconsistencies to the user.\n One solution is for the command-side and query-side APIs to supply the client with\nversion information that enables it to tell that the query side is out-of-date. A client\ncan poll the query-side view until it’s up-to-date. Shortly I’ll discuss how the service\nAPIs can enable a client to do this.\n A UI application such as a native mobile application or single page JavaScript\napplication can handle replication lag by updating its local model once the command\nis successful without issuing a query. It can, for example, update its model using data\nreturned by the command. Hopefully, when a user action triggers a query, the view\nwill be up-to-date. One drawback of this approach is that the UI code may need to\nduplicate server-side code in order to update its model.\n As you can see, CQRS has both benefits and drawbacks. As mentioned earlier, you\nshould use the API composition whenever possible and use CQRS only when you must.\n Now that you’ve seen the benefits and drawbacks of CQRS, let’s now look at how to\ndesign CQRS views. \n7.3\nDesigning CQRS views\nA CQRS view module has an API consisting of one more query operations. It imple-\nments these query operations by querying a database that it maintains by subscribing\nto events published by one or more services. As figure 7.10 shows, a view module con-\nsists of a view database and three submodules.\n \n\n\n237\nDesigning CQRS views\nThe data access module implements the database access logic. The event handlers\nand query API modules use the data access module to update and query the database.\nThe event handlers module subscribes to events and updates the database. The query\nAPI module implements the query API.\n You must make some important design decisions when developing a view module:\nYou must choose a database and design the schema.\nWhen designing the data access module, you must address various issues, includ-\ning ensuring that updates are idempotent and handling concurrent updates.\nWhen implementing a new view in an existing application or changing the\nschema of an existing application, you must implement a mechanism to effi-\nciently build or rebuild the view.\nYou must decide how to enable a client of the view to cope with the replication\nlag, described earlier.\nLet’s look at each of these issues.\n7.3.1\nChoosing a view datastore\nA key design decision is the choice of database and the design of the schema. The pri-\nmary purpose of the database and the data model is to efficiently implement the view\nmodule’s query operations. It’s the characteristics of those queries that are the pri-\nmary consideration when selecting a database. But the database must also efficiently\nimplement the update operations performed by the event handlers.\nSQL VS. NOSQL DATABASES\nNot that long ago, there was one type of database to rule them all: the SQL-based\nRDBMS. As the Web grew in popularity, though, various companies discovered that\nan RDBMS couldn’t satisfy their web scale requirements. That led to the creation of\nCQRS view module\nEvent\nhandlers\nquery()\nupdate()\nQuery API\nData access\nView database\nEvents\nﬁnd...()\n...\nImplements data\naccess logic\nFigure 7.10\nThe design of a CQRS \nview module. Event handlers update \nthe view database, which is queried \nby the Query API module.\n \n\n\n238\nCHAPTER 7\nImplementing queries in a microservice architecture\nthe so-called NoSQL databases. A NoSQL database typically has a limited form of trans-\nactions and less general querying capabilities. For certain use cases, these databases\nhave certain advantages over SQL databases, including a more flexible data model\nand better performance and scalability.\n A NoSQL database is often a good choice for a CQRS view, which can leverage its\nstrengths and ignore its weaknesses. A CQRS view benefits from the richer data model,\nand performance of a NoSQL database. It’s unaffected by the limitations of a NoSQL\ndatabase, because it only uses simple transactions and executes a fixed set of queries.\n Having said that, sometimes it makes sense to implement a CQRS view using a SQL\ndatabase. A modern RDBMS running on modern hardware has excellent perfor-\nmance. Developers, database administrators, and IT operations are, in general, much\nmore familiar with SQL databases than they are with NoSQL databases. As mentioned\nearlier, SQL databases often have extensions for non-relational features, such as geo-\nspatial datatypes and queries. Also, a CQRS view might need to use a SQL database in\norder to support a reporting engine.\n As you can see in table 7.1, there are lots of different options to choose from. And\nto make the choice even more complicated, the differences between the different\ntypes of database are starting to blur. For example, MySQL, which is an RDBMS, has\nexcellent support for JSON, which is one of the strengths of MongoDB, a JSON-style\ndocument-oriented database.\nNow that I’ve discussed the different kinds of databases you can use to implement a\nCQRS view, let’s look at the problem of how to efficiently update a view. \nSUPPORTING UPDATE OPERATIONS\nBesides efficiently implementing queries, the view data model must also efficiently\nimplement the update operations executed by the event handlers. Usually, an event\nTable 7.1\nQuery-side view stores\nIf you need\nUse\nExample\nPK-based lookup of JSON \nobjects\nA document store such as MongoDB \nor DynamoDB, or a key value store \nsuch as Redis\nImplement order history by main-\ntaining a MongoDB document \ncontaining the per-customer.\nQuery-based lookup of JSON \nobjects\nA document store such as MongoDB \nor DynamoDB\nImplement customer view using \nMongoDB or DynamoDB.\nText queries\nA text search engine such as Elastic-\nsearch\nImplement text search for orders \nby maintaining a per-order Elas-\nticsearch document.\nGraph queries\nA graph database such as Neo4j\nImplement fraud detection by \nmaintaining a graph of custom-\ners, orders, and other data.\nTraditional SQL reporting/BI\nAn RDBMS\nStandard business reports and \nanalytics.\n \n\n\n239\nDesigning CQRS views\nhandler will update or delete a record in the view database using its primary key. For\nexample, soon I’ll describe the design of a CQRS view for the findOrderHistory()\nquery. It stores each Order as a database record using the orderId as the primary key.\nWhen this view receives an event from Order Service, it can straightforwardly update\nthe corresponding record.\n Sometimes, though, it will need to update or delete a record using the equiva-\nlent of a foreign key. Consider, for instance, the event handlers for Delivery*\nevents. If there is a one-to-one correspondence between a Delivery and an Order,\nthen Delivery.id might be the same as Order.id. If it is, then Delivery* event han-\ndlers can easily update the order’s database record.\n But suppose a Delivery has its own primary key or there is a one-to-many relation-\nship between an Order and a Delivery. Some Delivery* events, such as the Delivery-\nCreated event, will contain the orderId. But other events, such as a DeliveryPickedUp\nevent, might not. In this scenario, an event handler for DeliveryPickedUp will need\nto update the order’s record using the deliveryId as the equivalent of a foreign key.\n Some types of database efficiently support foreign-key-based update operations.\nFor example, if you’re using an RDBMS or MongoDB, you create an index on the nec-\nessary columns. However, non-primary key-based updates are not straightforward\nwhen using other NOSQL databases. The application will need to maintain some kind\nof database-specific mapping from a foreign key to a primary key in order to deter-\nmine which record to update. For example, an application that uses DynamoDB,\nwhich only supports primary key-based updates and deletes, must first query a Dyna-\nmoDB secondary index (discussed shortly) to determine the primary keys of the items\nto update or delete. \n7.3.2\nData access module design\nThe event handlers and the query API module don’t access the datastore directly.\nInstead they use the data access module, which consists of a data access object (DAO)\nand its helper classes. The DAO has several responsibilities. It implements the update\noperations invoked by the event handlers and the query operations invoked by the\nquery module. The DAO maps between the data types used by the higher-level code\nand the database API. It also must handle concurrent updates and ensure that updates\nare idempotent.\n Let’s look at these issues, starting with how to handle concurrent updates.\nHANDLING CONCURRENCY\nSometimes a DAO must handle the possibility of multiple concurrent updates to the\nsame database record. If a view subscribes to events published by a single aggregate\ntype, there won’t be any concurrency issues. That’s because events published by a par-\nticular aggregate instance are processed sequentially. As a result, a record correspond-\ning to an aggregate instance won’t be updated concurrently. But if a view subscribes to\nevents published by multiple aggregate types, then it’s possible that multiple events\nhandlers update the same record simultaneously.\n \n\n\n240\nCHAPTER 7\nImplementing queries in a microservice architecture\n For example, an event handler for an Order* event might be invoked at the same\ntime as an event handler for a Delivery* event for the same order. Both event han-\ndlers then simultaneously invoke the DAO to update the database record for that\nOrder. A DAO must be written in a way that ensures that this situation is handled cor-\nrectly. It must not allow one update to overwrite another. If a DAO implements updates\nby reading a record and then writing the updated record, it must use either pessimistic\nor optimistic locking. In the next section you’ll see an example of a DAO that handles\nconcurrent updates by updating database records without reading them first.\nIDEMPOTENT EVENT HANDLERS\nAs mentioned in chapter 3, an event handler may be invoked with the same event\nmore than once. This is generally not a problem if a query-side event handler is idem-\npotent. An event handler is idempotent if handling duplicate events results in the cor-\nrect outcome. In the worst case, the view datastore will temporarily be out-of-date. For\nexample, an event handler that maintains the Order History view might be invoked\nwith the (admittedly improbable) sequence of events shown in figure 7.11: Delivery-\nPickedUp, DeliveryDelivered, DeliveryPickedUp, and DeliveryDelivered. After\ndelivering the DeliveryPickedUp and DeliveryDelivered events the first time, the\nmessage broker, perhaps because of a network error, starts delivering the events from\nan earlier point in time, and so redelivers DeliveryPickedUp and DeliveryDelivered.\nAfter the event handler processes the second DeliveryPickedUp event, the Order\nHistory view temporarily contains the out-of-date state of the Order until the Delivery-\nDelivered is processed. If this behavior is undesirable, then the event handler should\ndetect and discard duplicate events, like a non-idempotent event handler.\n An event handler isn’t idempotent if duplicate events result in an incorrect out-\ncome. For example, an event handler that increments the balance of a bank account\nisn’t idempotent. A non-idempotent event handler must, as explained in chapter 3,\ndetect and discard duplicate events by recording the IDs of events that it has pro-\ncessed in the view datastore.\nDelivery picked up\nOrder History View\nOrderId: 123\nState: PICKED_UP\nTemporarily out of date\nDelivery delivered\nOrderId: 123\nState: DELIVERED\nDelivery picked up\nOrderId: 123\nState: PICKED_UP\nDelivery delivered\nOrderId: 123\nState: DELIVERED\nTime\nFigure 7.11\nThe DeliveryPickedUp and DeliveryDelivered events are delivered \ntwice, which causes the order state in view to be temporarily out-of-date.\n \n\n\n241\nDesigning CQRS views\n In order to be reliable, the event handler must record the event ID and update the\ndatastore atomically. How to do this depends on the type of database. If the view data-\nbase store is a SQL database, the event handler could insert processed events into a\nPROCESSED_EVENTS table as part of the transaction that updates the view. But if the\nview datastore is a NoSQL database that has a limited transaction model, the event\nhandler must save the event in the datastore “record” (for example, a MongoDB doc-\nument or DynamoDB table item) that it updates.\n It’s important to note that the event handler doesn’t need to record the ID of\nevery event. If, as is the case with Eventuate, events have a monotonically increasing\nID, then each record only needs to store the max(eventId) that’s received from a\ngiven aggregate instance. Furthermore, if the record corresponds to a single aggre-\ngate instance, then the event handler only needs to record max(eventId). Only\nrecords that represent joins of events from multiple aggregates must contain a map\nfrom [aggregate type, aggregate id] to max(eventId).\n For example, you’ll soon see that the DynamoDB implementation of the Order\nHistory view contains items that have attributes for tracking events that look like this:\n{...\n\"Order3949384394-039434903\" : \"0000015e0c6fc18f-0242ac1100e50002\",\n\"Delivery3949384394-039434903\" : \"0000015e0c6fc264-0242ac1100e50002\",\n}\nThis view is a join of events published by various services. The name of each of these\nevent-tracking attributes is «aggregateType»«aggregateId», and the value is the\neventId. Later on, I describe how this works in more detail. \nENABLING A CLIENT APPLICATION TO USE AN EVENTUALLY CONSISTENT VIEW\nAs I said earlier, one issue with using CQRS is that a client that updates the command\nside and then immediately executes a query might not see its own update. The view is\neventually consistent because of the unavoidable latency of the messaging infrastructure.\n The command and query module APIs can enable the client to detect an inconsis-\ntency using the following approach. A command-side operation returns a token con-\ntaining the ID of the published event to the client. The client then passes the token to\na query operation, which returns an error if the view hasn’t been updated by that\nevent. A view module can implement this mechanism using the duplicate event-\ndetection mechanism. \n7.3.3\nAdding and updating CQRS views\nCQRS views will be added and updated throughout the lifetime of an application.\nSometimes you need to add a new view to support a new query. At other times you\nmight need to re-create a view because the schema has changed or you need to fix a\nbug in code that updates the view.\n Adding and updating views is conceptually quite simple. To create a new view, you\ndevelop the query-side module, set up the datastore, and deploy the service. The query\n \n\n\n242\nCHAPTER 7\nImplementing queries in a microservice architecture\nside module’s event handlers process all the events, and eventually the view will be\nup-to-date. Similarly, updating an existing view is also conceptually simple: you change\nthe event handlers and rebuild the view from scratch. The problem, however, is that\nthis approach is unlikely to work in practice. Let’s look at the issues.\nBUILD CQRS VIEWS USING ARCHIVED EVENTS\nOne problem is that message brokers can’t store messages indefinitely. Traditional\nmessage brokers such as RabbitMQ delete a message once it’s been processed by a\nconsumer. Even more modern brokers such as Apache Kafka, that retain messages for\na configurable retention period, aren’t intended to store events indefinitely. As a\nresult, a view can’t be built by only reading all the needed events from the message\nbroker. Instead, an application must also read older events that have been archived in,\nfor example, AWS S3. You can do this by using a scalable big data technology such as\nApache Spark. \nBUILD CQRS VIEWS INCREMENTALLY\nAnother problem with view creation is that the time and resources required to process\nall events keep growing over time. Eventually, view creation will become too slow and\nexpensive. The solution is to use a two-step incremental algorithm. The first step peri-\nodically computes a snapshot of each aggregate instance based on its previous snap-\nshot and events that have occurred since that snapshot was created. The second step\ncreates a view using the snapshots and any subsequent events. \n7.4\nImplementing a CQRS view with AWS DynamoDB\nNow that we’ve looked at the various design issues you must address when using\nCQRS, let’s consider an example. This section describes how to implement a CQRS\nview for the findOrderHistory() operation using DynamoDB. AWS DynamoDB is\na scalable, NoSQL database that’s available as a service on the Amazon cloud. The\nDynamoDB data model consists of tables that contain items that, like JSON objects,\nare collections of hierarchical name-value pairs. AWS DynamoDB is a fully man-\naged database, and you can scale the throughput capacity of a table up and down\ndynamically.\n The CQRS view for the findOrderHistory() consumes events from multiple ser-\nvices, so it’s implemented as a standalone Order View Service. The service has an API\nthat implements two operations: findOrderHistory() and findOrder(). Even though\nfindOrder() can be implemented using API composition, this view provides this oper-\nation for free. Figure 7.12 shows the design of the service. Order History Service is\nstructured as a set of modules, each of which implements a particular responsibility\nin order to simplify development and testing. The responsibility of each module is\nas follows:\n\nOrderHistoryEventHandlers—Subscribes to events published by the various\nservices and invokes the OrderHistoryDAO\n\nOrderHistoryQuery APImodule—Implements the REST endpoints described earlier\n \n\n\n243\nImplementing a CQRS view with AWS DynamoDB\n\nOrderHistoryDataAccess—Contains the OrderHistoryDAO, which defines the\nmethods that update and query the ftgo-order-history DynamoDB table and\nits helper classes\n\nftgo-order-history DynamoDB table—The table that stores the orders\nLet’s look at the design of the event handlers, the DAO, and the DynamoDB table in\nmore detail.\n7.4.1\nThe OrderHistoryEventHandlers module\nThis module consists of the event handlers that consume events and update the\nDynamoDB table. As the following listing shows, the event handlers are simple meth-\nods. Each method is a one-liner that invokes an OrderHistoryDao method with argu-\nments that are derived from the event.\n \n \n \n \n \nOrder History Service\nOrderHistory\nEvent\nHandlers\nQuery\nUpdate\nOrderHistory\nQuery\nOrderHistoryDataAccess\n<DynamoDB table>\nftgo-order-history\nOrder\ndelivery\n...\nevents\nﬁndOrderHistory()\nﬁndOrder\nOrderHistoryDAO\nFigure 7.12\nThe design of OrderHistoryService. OrderHistory-\nEventHandlers updates the database in response to events. The \nOrderHistoryQuery module implements the query operations by query-\ning the database. These two modules use the OrderHistory-\nDataAccess module to access the database.\n \n",
      "page_number": 261
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 269-277)",
      "start_page": 269,
      "end_page": 277,
      "detection_method": "topic_boundary",
      "content": "244\nCHAPTER 7\nImplementing queries in a microservice architecture\npublic class OrderHistoryEventHandlers {\nprivate OrderHistoryDao orderHistoryDao;\npublic OrderHistoryEventHandlers(OrderHistoryDao orderHistoryDao) {\nthis.orderHistoryDao = orderHistoryDao;\n}\npublic void handleOrderCreated(DomainEventEnvelope<OrderCreated> dee) {\norderHistoryDao.addOrder(makeOrder(dee.getAggregateId(), dee.getEvent()),\nmakeSourceEvent(dee));\n}\nprivate Order makeOrder(String orderId, OrderCreatedEvent event) {\n...\n}\npublic void handleDeliveryPickedUp(DomainEventEnvelope<DeliveryPickedUp>\ndee) {\norderHistoryDao.notePickedUp(dee.getEvent().getOrderId(),\nmakeSourceEvent(dee));\n}\n...\nEach event handler has a single parameter of type DomainEventEnvelope, which\ncontains the event and some metadata describing the event. For example, the\nhandleOrderCreated() method is invoked to handle an OrderCreated event. It calls\norderHistoryDao.addOrder() to create an Order in the database. Similarly, the\nhandleDeliveryPickedUp() method is invoked to handle a DeliveryPickedUp event.\nIt calls orderHistoryDao.notePickedUp() to update the status of the Order in the\ndatabase.\n Both methods call the helper method makeSourceEvent(), which constructs a\nSourceEvent containing the type and ID of the aggregate that emitted the event and\nthe event ID. In the next section you’ll see that OrderHistoryDao uses SourceEvent to\nensure that update operations are idempotent.\n Let’s now look at the design of the DynamoDB table and after that examine\nOrderHistoryDao. \n7.4.2\nData modeling and query design with DynamoDB\nLike many NoSQL databases, DynamoDB has data access operations that are much\nless powerful than those that are provided by an RDBMS. Consequently, you must\ncarefully design how the data is stored. In particular, the queries often dictate the\ndesign of the schema. We need to address several design issues:\nDesigning the ftgo-order-history table\nDefining an index for the findOrderHistory query\nListing 7.1\nEvent handlers that call the OrderHistoryDao\n \n\n\n245\nImplementing a CQRS view with AWS DynamoDB\nImplementing the findOrderHistory query\nPaginating the query results\nUpdating orders\nDetecting duplicate events\nWe’ll look at each one in turn.\nDESIGNING THE FTGO-ORDER-HISTORY TABLE\nThe DynamoDB storage model consists of tables, which contain items, and indexes,\nwhich provide alternative ways to access a table’s items (discussed shortly). An item is a\ncollection of named attributes. An attribute value is either a scalar value such as a string,\na multivalued collection of strings, or a collection of named attributes. Although an item\nis the equivalent to a row in an RDBMS, it’s a lot more flexible and can store an entire\naggregate.\n This flexibility enables the OrderHistoryDataAccess module to store each Order\nas a single item in a DynamoDB table called ftgo-order-history. Each field of the\nOrder class is mapped to an item attribute, as shown in figure 7.13. Simple fields such\nas orderCreationTime and status are mapped to single-value item attributes. The\nlineItems field is mapped to an attribute that is a list of maps, one map per time line.\nIt can be considered to be a JSON array of objects.\nAn important part of the definition of a table is its primary key. A DynamoDB applica-\ntion inserts, updates, and retrieves a table’s items by primary key. It would seem to\nmake sense for the primary key to be orderId. This enables Order History Service\nto insert, update, and retrieve an order by orderId. But before finalizing this decision,\nlet’s first explore how a table’s primary key impacts the kinds of data access operations\nit supports. \nDEFINING AN INDEX FOR THE FINDORDERHISTORY QUERY\nThis table definition supports primary key-based reads and writes of Orders. But it\ndoesn’t support a query such as findOrderHistory() that returns multiple matching\norders sorted by increasing age. That’s because, as you will see later in this section,\nthis query uses the DynamoDB query() operation, which requires a table to have a\norderId\n...\n...\nPrimary key\nftgo-order-history table\nconsumerId\nxyz-abc\n...\norderCreationTime\n22939283232\n...\nstatus\nCREATED\n...\nlineItems\n[{...}.\n{...},\n....]\n....\n...\n...\n...\nFigure 7.13\nPreliminary structure of the DynamoDB OrderHistory table\n \n\n\n246\nCHAPTER 7\nImplementing queries in a microservice architecture\ncomposite primary key consisting of two scalar attributes. The first attribute is a parti-\ntion key. The partition key is so called because DynamoDB’s Z-axis scaling (described in\nchapter 1) uses it to select an item’s storage partition. The second attribute is the sort\nkey. A query() operation returns those items that have the specified partition key,\nhave a sort key in the specified range, and match the optional filter expression. It\nreturns items in the order specified by the sort key.\n The findOrderHistory() query operation returns a consumer’s orders sorted by\nincreasing age. It therefore requires a primary key that has the consumerId as the par-\ntition key and the orderCreationDate as the sort key. But it doesn’t make sense for\n(consumerId, orderCreationDate) to be the primary key of the ftgo-order-history\ntable, because it’s not unique.\n The solution is for findOrderHistory() to query what DynamoDB calls a secondary\nindex on the ftgo-order-history table. This index has (consumerId, orderCreation-\nDate) as its non-unique key. Like an RDBMS index, a DynamoDB index is automati-\ncally updated whenever its table is updated. But unlike a typical RDBMS index, a\nDynamoDB index can have non-key attributes. Non-key attributes improve performance\nbecause they’re returned by the query, so the application doesn’t have to fetch them\nfrom the table. Also, as you’ll soon see, they can be used for filtering. Figure 7.14\nshows the structure of the table and this index.\n The index is part of the definition of the ftgo-order-history table and is called\nftgo-order-history-by-consumer-id-and-creation-time. The index’s attributes\norderId\ncde-fgh\n...\nPrimary key\nftgo-order-history table\nconsumerId\nxyz-abc\n...\norderCreationTime\n22939283232\n...\nstatus\nCREATED\n...\nlineItems\n[{...}.\n{...},\n....]\n....\n...\n...\n...\nPrimary key\nftgo-order-history-by-consumer-id-and-creation-time\nglobal secondary index\nconsumerId\nxyz-abc\n...\norderCreationTime\n22939283232\n...\norderId\ncde-fgh\n...\n...\n...\n...\nstatus\nCREATED\n...\nFigure 7.14\nThe design of the OrderHistory table and index\n \n\n\n247\nImplementing a CQRS view with AWS DynamoDB\ninclude the primary key attributes, consumerId and orderCreationTime, and non-key\nattributes, including orderId and status.\n The ftgo-order-history-by-consumer-id-and-creation-time index enables\nthe OrderHistoryDaoDynamoDb to efficiently retrieve a consumer’s orders sorted by\nincreasing age.\n Let’s now look at how to retrieve only those orders that match the filter criteria. \nIMPLEMENTING THE FINDORDERHISTORY QUERY\nThe findOrderHistory() query operation has a filter parameter that specifies the\nsearch criteria. One filter criterion is the maximum age of the orders to return. This is\neasy to implement because the DynamoDB Query operation’s key condition expression\nsupports a range restriction on the sort key. The other filter criteria correspond to\nnon-key attributes and can be implemented using a filter expression , which is a Boolean\nexpression. A DynamoDB Query operation returns only those items that satisfy the filter\nexpression. For example, to find Orders that are CANCELLED, the OrderHistoryDao-\nDynamoDb uses a query expression orderStatus = :orderStatus, where :orderStatus\nis a placeholder parameter.\n The keyword filter criteria is more challenging to implement. It selects orders\nwhose restaurant name or menu items match one of the specified keywords. The\nOrderHistoryDaoDynamoDb enables the keyword search by tokenizing the restaurant\nname and menu items and storing the set of keywords in a set-valued attribute called\nkeywords. It finds the orders that match the keywords by using a filter expression\nthat uses the contains() function, for example contains(keywords, :keyword1)\nOR contains(keywords, :keyword2), where :keyword1 and :keyword2 are placehold-\ners for the specified keywords. \nPAGINATING THE QUERY RESULTS\nSome consumers will have a large number of orders. It makes sense, therefore, for the\nfindOrderHistory() query operation to use pagination. The DynamoDB Query oper-\nation has an operation pageSize parameter, which specifies the maximum number of\nitems to return. If there are more items, the result of the query has a non-null Last-\nEvaluatedKey attribute. A DAO can retrieve the next page of items by invoking the\nquery with the exclusiveStartKey parameter set to LastEvaluatedKey.\n As you can see, DynamoDB doesn’t support position-based pagination. Conse-\nquently, Order History Service returns an opaque pagination token to its client. The\nclient uses this pagination token to request the next page of results.\n Now that I’ve described how to query DynamoDB for orders, let’s look at how to\ninsert and update them. \nUPDATING ORDERS\nDynamoDB supports two operations for adding and updating items: PutItem() and\nUpdateItem(). The PutItem() operation creates or replaces an entire item by its\nprimary key. In theory, OrderHistoryDaoDynamoDb could use this operation to insert\n \n\n\n248\nCHAPTER 7\nImplementing queries in a microservice architecture\nand update orders. One challenge, however, with using PutItem() is ensuring that\nsimultaneous updates to the same item are handled correctly.\n Consider, for example, the scenario where two event handlers simultaneously\nattempt to update the same item. Each event handler calls OrderHistoryDaoDynamoDb\nto load the item from DynamoDB, change it in memory, and update it in DynamoDB\nusing PutItem(). One event handler could potentially overwrite the change made by\nthe other event handler. OrderHistoryDaoDynamoDb can prevent lost updates by using\nDynamoDB’s optimistic locking mechanism. But an even simpler and more efficient\napproach is to use the UpdateItem() operation.\n The UpdateItem() operation updates individual attributes of the item, creating\nthe item if necessary. Since different event handlers update different attributes of the\nOrder item, using UpdateItem makes sense. This operation is also more efficient\nbecause there’s no need to first retrieve the order from the table.\n One challenge with updating the database in response to events is, as mentioned\nearlier, detecting and discarding duplicate events. Let’s look at how to do that when\nusing DynamoDB. \nDETECTING DUPLICATE EVENTS\nAll of Order History Service’s event handlers are idempotent. Each one sets one\nor more attributes of the Order item. Order History Service could, therefore, sim-\nply ignore the issue of duplicate events. The downside of ignoring the issue, though,\nis that Order item will sometimes be temporarily out-of-date. That’s because an\nevent handler that receives a duplicate event will set an Order item’s attributes to\nprevious values. The Order item won’t have the correct values until later events are\nredelivered.\n As described earlier, one way to prevent data from becoming out-of-date is to\ndetect and discard duplicate events. OrderHistoryDaoDynamoDb can detect duplicate\nevents by recording in each item the events that have caused it to be updated. It can\nthen use the UpdateItem() operation’s conditional update mechanism to only update\nan item if an event isn’t a duplicate.\n A conditional update is only performed if a condition expression is true. A condition\nexpression tests whether an attribute exists or has a particular value. The Order-\nHistoryDaoDynamoDb DAO can track events received from each aggregate instance\nusing an attribute called «aggregateType»«aggregateId» whose value is the highest\nreceived event ID. An event is a duplicate if the attribute exists and its value is less\nthan or equal to the event ID. The OrderHistoryDaoDynamoDb DAO uses this condi-\ntion expression:\nattribute_not_exists(«aggregateType»«aggregateId») \nOR «aggregateType»«aggregateId» < :eventId\nThe condition expression only allows the update if the attribute doesn’t exist or the\neventId is greater than the last processed event ID.\n \n\n\n249\nImplementing a CQRS view with AWS DynamoDB\n For example, suppose an event handler receives a DeliveryPickup event whose ID\nis 123323-343434 from a Delivery aggregate whose ID is 3949384394-039434903.\nThe name of the tracking attribute is Delivery3949384394-039434903. The event\nhandler should consider the event to be a duplicate if the value of this attribute is\ngreater than or equal to 123323-343434. The query() operation invoked by the event\nhandler updates the Order item using this condition expression:\nattribute_not_exists(Delivery3949384394-039434903) \nOR Delivery3949384394-039434903 < :eventId\nNow that I’ve described the DynamoDB data model and query design, let’s take a look\nat OrderHistoryDaoDynamoDb, which defines the methods that update and query the\nftgo-order-history table. \n7.4.3\nThe OrderHistoryDaoDynamoDb class\nThe OrderHistoryDaoDynamoDb class implements methods that read and write items\nin the ftgo-order-history table. Its update methods are invoked by OrderHistory-\nEventHandlers, and its query methods are invoked by OrderHistoryQuery API. Let’s\ntake a look at some example methods, starting with the addOrder() method.\nTHE ADDORDER() METHOD\nThe addOrder() method, which is shown in listing 7.2, adds an order to the ftgo-\norder-history table. It has two parameters: order and sourceEvent. The order\nparameter is the Order to add, which is obtained from the OrderCreated event. The\nsourceEvent parameter contains the eventId and the type and ID of the aggregate\nthat emitted the event. It’s used to implement the conditional update.\npublic class OrderHistoryDaoDynamoDb ...\n@Override\npublic boolean addOrder(Order order, Optional<SourceEvent> eventSource) {\nUpdateItemSpec spec = new UpdateItemSpec()\n.withPrimaryKey(\"orderId\", order.getOrderId())\n  \n.withUpdateExpression(\"SET orderStatus = :orderStatus, \" +  \n\"creationDate = :cd, consumerId = :consumerId, lineItems =\" +\n\" :lineItems, keywords = :keywords, restaurantName = \" +\n\":restaurantName\")\n.withValueMap(new Maps()\n                \n.add(\":orderStatus\", order.getStatus().toString())\n.add(\":cd\", order.getCreationDate().getMillis())\n.add(\":consumerId\", order.getConsumerId())\n.add(\":lineItems\", mapLineItems(order.getLineItems()))\n.add(\":keywords\", mapKeywords(order))\n.add(\":restaurantName\", order.getRestaurantName())\n.map())\n.withReturnValues(ReturnValue.NONE);\nreturn idempotentUpdate(spec, eventSource);\n}\nListing 7.2\nThe addOrder() method adds or updates an Order\nThe primary key of the\nOrder item to update\nThe update\nexpression that\nupdates the\nattributes\nThe values of the\nplaceholders in\nthe update\nexpression\n \n\n\n250\nCHAPTER 7\nImplementing queries in a microservice architecture\nThe addOrder() method creates an UpdateSpec, which is part of the AWS SDK and\ndescribes the update operation. After creating the UpdateSpec, it calls idempotent-\nUpdate(), a helper method that performs the update after adding a condition expres-\nsion that guards against duplicate updates. \nTHE NOTEPICKEDUP() METHOD\nThe notePickedUp() method, shown in listing 7.3, is called by the event handler for\nthe DeliveryPickedUp event. It changes the deliveryStatus of the Order item to\nPICKED_UP.\npublic class OrderHistoryDaoDynamoDb ...\n@Override\npublic void notePickedUp(String orderId, Optional<SourceEvent> eventSource) {\nUpdateItemSpec spec = new UpdateItemSpec()\n.withPrimaryKey(\"orderId\", orderId)\n.withUpdateExpression(\"SET #deliveryStatus = :deliveryStatus\")\n.withNameMap(Collections.singletonMap(\"#deliveryStatus\",\nDELIVERY_STATUS_FIELD))\n.withValueMap(Collections.singletonMap(\":deliveryStatus\",\nDeliveryStatus.PICKED_UP.toString()))\n.withReturnValues(ReturnValue.NONE);\nidempotentUpdate(spec, eventSource);\n}\nThis method is similar to addOrder(). It creates an UpdateItemSpec and invokes\nidempotentUpdate(). Let’s look at the idempotentUpdate() method. \nTHE IDEMPOTENTUPDATE() METHOD\nThe following listing shows the idempotentUpdate() method, which updates the item\nafter possibly adding a condition expression to the UpdateItemSpec that guards against\nduplicate updates.\npublic class OrderHistoryDaoDynamoDb ...\nprivate boolean idempotentUpdate(UpdateItemSpec spec, Optional<SourceEvent>\neventSource) {\ntry {\ntable.updateItem(eventSource.map(es -> es.addDuplicateDetection(spec))\n.orElse(spec));\nreturn true;\n} catch (ConditionalCheckFailedException e) {\n// Do nothing\nreturn false;\n}\n}\nListing 7.3\nThe notePickedUp() method changes the order status to PICKED_UP\nListing 7.4\nThe idempotentUpdate() method ignores duplicate events\n \n\n\n251\nImplementing a CQRS view with AWS DynamoDB\nIf the sourceEvent is supplied, idempotentUpdate() invokes SourceEvent.add-\nDuplicateDetection() to add to UpdateItemSpec the condition expression that was\ndescribed earlier. The idempotentUpdate() method catches and ignores the\nConditionalCheckFailedException, which is thrown by updateItem() if the event\nwas a duplicate.\n Now that we’ve seen the code that updates the table, let’s look at the query method. \nTHE FINDORDERHISTORY() METHOD\nThe findOrderHistory() method, shown in listing 7.5, retrieves the consumer’s orders by\nquerying the ftgo-order-history table using the ftgo-order-history-by-consumer-\nid-and-creation-time secondary index. It has two parameters: consumerId specifies\nthe consumer, and filter specifies the search criteria. This method creates Query-\nSpec—which, like UpdateSpec, is part of the AWS SDK—from its parameters, queries\nthe index, and transforms the returned items into an OrderHistory object.\npublic class OrderHistoryDaoDynamoDb ...\n@Override\npublic OrderHistory findOrderHistory(String consumerId, OrderHistoryFilter\nfilter) {\nQuerySpec spec = new QuerySpec()\n.withScanIndexForward(false)\n   \n.withHashKey(\"consumerId\", consumerId)\n.withRangeKeyCondition(new RangeKeyCondition(\"creationDate\")  \n.gt(filter.getSince().getMillis()));\nfilter.getStartKeyToken().ifPresent(token ->\nspec.withExclusiveStartKey(toStartingPrimaryKey(token)));\nMap<String, Object> valuesMap = new HashMap<>();\nString filterExpression = Expressions.and(\n      \nkeywordFilterExpression(valuesMap, filter.getKeywords()),\nstatusFilterExpression(valuesMap, filter.getStatus()));\nif (!valuesMap.isEmpty())\nspec.withValueMap(valuesMap);\nif (StringUtils.isNotBlank(filterExpression)) {\nspec.withFilterExpression(filterExpression);\n}\nfilter.getPageSize().ifPresent(spec::withMaxResultSize);  \nItemCollection<QueryOutcome> result = index.query(spec);\nreturn new OrderHistory(\nStreamSupport.stream(result.spliterator(), false)\nListing 7.5\nThe findOrderHistory() method retrieves a consumer’s matching orders\nSpecifies that query must \nreturn the orders in order \nof increasing age\nThe maximum\nage of the\norders to\nreturn\nConstruct a filter expression\nand placeholder value map\nfrom the OrderHistoryFilter.\nLimit the number \nof results if the \ncaller has specified \na page size.\n \n\n\n252\nCHAPTER 7\nImplementing queries in a microservice architecture\n.map(this::toOrder)\n      \n.collect(toList()),\nOptional.ofNullable(result\n.getLastLowLevelResult()\n.getQueryResult().getLastEvaluatedKey())\n.map(this::toStartKeyToken));\n}\nAfter building a QuerySpec, this method then executes a query and builds an Order-\nHistory, which contains the list of Orders, from the returned items.\n The findOrderHistory() method implements pagination by serializing the value\nreturned by getLastEvaluatedKey() into a JSON token. If a client specifies a start\ntoken in OrderHistoryFilter, then findOrderHistory() serializes it and invokes\nwithExclusiveStartKey() to set the start key.\n As you can see, you must address numerous issues when implementing a CQRS\nview, including picking a database, designing the data model that efficiently imple-\nments updates and queries, handling concurrent updates, and dealing with duplicate\nevents. The only complex part of the code is the DAO, because it must properly han-\ndle concurrency and ensure that updates are idempotent. \nSummary\nImplementing queries that retrieve data from multiple services is challenging\nbecause each service’s data is private.\nThere are two ways to implement these kinds of query: the API composition\npattern and the Command query responsibility segregation (CQRS) pattern.\nThe API composition pattern, which gathers data from multiple services, is the\nsimplest way to implement queries and should be used whenever possible.\nA limitation of the API composition pattern is that some complex queries require\ninefficient in-memory joins of large datasets.\nThe CQRS pattern, which implements queries using view databases, is more\npowerful but more complex to implement.\nA CQRS view module must handle concurrent updates as well as detect and dis-\ncard duplicate events.\nCQRS improves separation of concerns by enabling a service to implement a\nquery that returns data owned by a different service.\nClients must handle the eventual consistency of CQRS views. \nCreate an Order from \nan item returned by \nthe query.\n \n",
      "page_number": 269
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 278-286)",
      "start_page": 278,
      "end_page": 286,
      "detection_method": "topic_boundary",
      "content": "253\nExternal API patterns\nThe FTGO application, like many other applications, has a REST API. Its clients\ninclude the FTGO mobile applications, JavaScript running in the browser, and\napplications developed by partners. In such a monolithic architecture, the API\nthat’s exposed to clients is the monolith’s API. But when once the FTGO team\nstarts deploying microservices, there’s no longer one API, because each service has\nits own API. Mary and her team must decide what kind of API the FTGO applica-\ntion should now expose to its clients. For example, should clients be aware of the\nexistence of services and make requests to them directly?\nThis chapter covers\nThe challenge of designing APIs that support a \ndiverse set of clients\nApplying API gateway and Backends for frontends \npatterns\nDesigning and implementing an API gateway\nUsing reactive programming to simplify API \ncomposition\nImplementing an API gateway using GraphQL\n \n\n\n254\nCHAPTER 8\nExternal API patterns\n The task of designing an application’s external API is made even more challenging\nby the diversity of its clients. Different clients typically require different data. A desktop\nbrowser-based UI usually displays far more information than a mobile application. Also,\ndifferent clients access the services over different kinds of networks. The clients within\nthe firewall use a high-performance LAN, and the clients outside of the firewall use the\ninternet or mobile network, which will have lower performance. Consequently, as you’ll\nlearn, it often doesn’t make sense to have a single, one-size-fits-all API.\n This chapter begins by describing various external API design issues. I then\ndescribe the external API patterns. I cover the API gateway pattern and then the Back-\nends for frontends pattern. After that, I discuss how to design and implement an API\ngateway. I review the various options that are available, which include off-the-shelf API\ngateway products and frameworks for developing your own. I describe the design and\nimplementation of an API gateway that’s built using the Spring Cloud Gateway frame-\nwork. I also describe how to build an API gateway using GraphQL, a framework that\nprovides graph-based query language.\n8.1\nExternal API design issues\nIn order to explore the various API-related issues, let’s consider the FTGO application.\nAs figure 8.1 shows, this application’s services are consumed by a variety of clients. Four\nkinds of clients consume the services’ APIs:\nWeb applications, such as Consumer web application, which implements the\nbrowser-based UI for consumers, Restaurant web application, which imple-\nments the browser-based UI for restaurants, and Admin web application, which\nimplements the internal administrator UI\nJavaScript applications running in the browser\nMobile applications, one for consumers and the other for couriers\nApplications written by third-party developers\nThe web applications run inside the firewall, so they access the services over a high-\nbandwidth, low-latency LAN. The other clients run outside the firewall, so they access\nthe services over the lower-bandwidth, higher-latency internet or mobile network.\n One approach to API design is for clients to invoke the services directly. On the\nsurface, this sounds quite straightforward—after all, that’s how clients invoke the API\nof a monolithic application. But this approach is rarely used in a microservice archi-\ntecture because of the following drawbacks:\nThe fine-grained service APIs require clients to make multiple requests to\nretrieve the data they need, which is inefficient and can result in a poor user\nexperience.\nThe lack of encapsulation caused by clients knowing about each service and its\nAPI makes it difficult to change the architecture and the APIs.\nServices might use IPC mechanisms that aren’t convenient or practical for cli-\nents to use, especially those clients outside the firewall.\n \n\n\n255\nExternal API design issues\nTo learn more about these drawbacks, let’s take a look at how the FTGO mobile appli-\ncation for consumers retrieves data from the services.\n8.1.1\nAPI design issues for the FTGO mobile client\nConsumers use the FTGO mobile client to place and manage their orders. Imagine\nyou’re developing the mobile client’s View Order view, which displays an order. As\ndescribed in chapter 7, the information displayed by this view includes basic order\ninformation, including its status, payment status, status of the order from the restau-\nrant’s perspective, and delivery status, including its location and estimated delivery\ntime if in transit.\n The monolithic version of the FTGO application has an API endpoint that returns\nthe order details. The mobile client retrieves the information it needs by making a sin-\ngle request. In contrast, in the microservices version of the FTGO application, the\norder details are, as described previously, scattered across several services, including\nthe following:\nLower-performance\ninternet\nHigher-performance\nLAN\nBackend services\nOrder Service\nFirewall\nAPI\nrequests\nAPI\nrequests\nAPI\nrequests\nWeb page\nrequests\nWeb\napplication\nConsumer\nService\nDelivery\nService\nKitchen\nService\nBrowser\niPhone/\nAndroid\napplication\n3rd-party\napplication\nHTML\nJavaScript\napplication\nFigure 8.1\nThe FTGO application’s services and their clients. There are several \ndifferent types of clients. Some are inside the firewall, and others are outside. \nThose outside the firewall access the services over the lower-performance \ninternet/mobile network. Those clients inside the firewall use a higher-\nperformance LAN.\n \n\n\n256\nCHAPTER 8\nExternal API patterns\n\nOrder Service—Basic order information, including the details and status\n\nKitchen Service—The status of the order from the restaurant’s perspective\nand the estimated time it will be ready for pickup\n\nDelivery Service—The order’s delivery status, its estimated delivery time, and\nits current location\n\nAccounting Service—The order’s payment status\nIf the mobile client invokes the services directly, then it must, as figure 8.2 shows, make\nmultiple calls to retrieve this data.\nFTGO backend services\nOrder Service\nFirewall\nMonolithic FTGO\napplication\nFirewall\nInternet\nInternet\ngetOrder()\ngetDelivery()\ngetOrderDetails()\ngetBill()\ngetTicket()\nDelivery\nService\nAccounting\nService\nKitchen\nService\niPhone/\nAndroid\nconsumer\napplication\niPhone/\nAndroid\nconsumer\napplication\nOne API required\nMany API calls required\nFigure 8.2\nA client can retrieve the order details from the monolithic FTGO application with a \nsingle request. But the client must make multiple requests to retrieve the same information in a \nmicroservice architecture.\n \n\n\n257\nExternal API design issues\nIn this design, the mobile application is playing the role of API composer. It invokes\nmultiple services and combines the results. Although this approach seems reasonable,\nit has several serious problems.\nPOOR USER EXPERIENCE DUE TO THE CLIENT MAKING MULTIPLE REQUESTS\nThe first problem is that the mobile application must sometimes make multiple\nrequests to retrieve the data it wants to display to the user. The chatty interaction\nbetween the application and the services can make the application seem unrespon-\nsive, especially when it uses the internet or a mobile network. The internet has much\nlower bandwidth and higher latency than a LAN, and mobile networks are even worse.\nThe latency of a mobile network (and internet) is typically 100x greater than a LAN.\n The higher latency might not be a problem when retrieving the order details,\nbecause the mobile application minimizes the delay by executing the requests concur-\nrently. The overall response time is no greater than that of a single request. But in\nother scenarios, a client may need to execute requests sequentially, which will result in\na poor user experience.\n What’s more, poor user experience due to network latency is not the only issue\nwith a chatty API. It requires the mobile developer to write potentially complex API\ncomposition code. This work is a distraction from their primary task of creating a\ngreat user experience. Also, because each network request consumes power, a chatty\nAPI drains the mobile device’s battery faster.\nLACK OF ENCAPSULATION REQUIRES FRONTEND DEVELOPERS TO CHANGE THEIR CODE IN LOCKSTEP \nWITH THE BACKEND\nAnother drawback of a mobile application directly accessing the services is the lack of\nencapsulation. As an application evolves, the developers of a service sometimes\nchange an API in a way that breaks existing clients. They might even change how the\nsystem is decomposed into services. Developers may add new services and split or\nmerge existing services. But if knowledge about the services is baked into a mobile\napplication, it can be difficult to change the services’ APIs.\n Unlike when updating a server-side application, it takes hours or perhaps even\ndays to roll out a new version of a mobile application. Apple or Google must approve\nthe upgrade and make it available for download. Users might not download the\nupgrade immediately—if ever. And you may not want to force reluctant users to\nupgrade. The strategy of exposing service APIs to mobile creates a significant obstacle\nto evolving those APIs.\nSERVICES MIGHT USE CLIENT-UNFRIENDLY IPC MECHANISMS\nAnother challenge with a mobile application directly calling services is that some ser-\nvices could use protocols that aren’t easily consumed by a client. Client applications\nthat run outside the firewall typically use protocols such as HTTP and WebSockets.\nBut as described in chapter 3, service developers have many protocols to choose\nfrom—not just HTTP. Some of an application’s services might use gRPC, whereas\nothers could use the AMQP messaging protocol. These kinds of protocols work well\n \n\n\n258\nCHAPTER 8\nExternal API patterns\ninternally, but might not be easily consumed by a mobile client. Some aren’t even fire-\nwall friendly. \n8.1.2\nAPI design issues for other kinds of clients\nI picked the mobile client because it’s a great way to demonstrate the drawbacks of cli-\nents accessing services directly. But the problems created by exposing services to cli-\nents aren’t specific to just mobile clients. Other kinds of clients, especially those\noutside the firewall, also encounter these problems. As described earlier, the FTGO\napplication’s services are consumed by web applications, browser-based JavaScript\napplications, and third-party applications. Let’s take a look at the API design issues\nwith these clients.\nAPI DESIGN ISSUES FOR WEB APPLICATIONS\nTraditional server-side web applications, which handle HTTP requests from browsers\nand return HTML pages, run within the firewall and access the services over a LAN.\nNetwork bandwidth and latency aren’t obstacles to implementing API composition in\na web application. Also, web applications can use non-web-friendly protocols to access\nthe services. The teams that develop web applications are part of the same organiza-\ntion and often work in close collaboration with the teams writing the backend ser-\nvices, so a web application can easily be updated whenever the backend services are\nchanged. Consequently, it’s feasible for a web application to access the backend ser-\nvices directly. \nAPI DESIGN ISSUES FOR BROWSER-BASED JAVASCRIPT APPLICATIONS\nModern browser applications use some amount of JavaScript. Even if the HTML is pri-\nmarily generated by a server-side web application, it’s common for JavaScript running\nin the browser to invoke services. For example, all of the FTGO application web appli-\ncations—Consumer, Restaurant, and Admin—contain JavaScript that invokes the back-\nend services. The Consumer web application, for instance, dynamically refreshes the\nOrder Details page using JavaScript that invokes the service APIs.\n On one hand, browser-based JavaScript applications are easy to update when ser-\nvice APIs change. On the other hand, JavaScript applications that access the services\nover the internet have the same problems with network latency as mobile applications.\nTo make matters worse, browser-based UIs, especially those for the desktop, are usu-\nally more sophisticated and need to compose more services than mobile applications.\nIt’s likely that the Consumer and Restaurant applications, which access services over\nthe internet, won’t be able to compose service APIs efficiently. \nDESIGNING APIS FOR THIRD-PARTY APPLICATIONS\nFTGO, like many other organizations, exposes an API to third-party developers. The\ndevelopers can use the FTGO API to write applications that place and manage\norders. These third-party applications access the APIs over the internet, so API com-\nposition is likely to be inefficient. But the inefficiency of API composition is a rela-\ntively minor problem compared to the much larger challenge of designing an API\n \n\n\n259\nThe API gateway pattern\nthat’s used by third-party applications. That’s because third-party developers need\nan API that’s stable.\n Very few organizations can force third-party developers to upgrade to a new API.\nOrganizations that have an unstable API risk losing developers to a competitor.\nConsequently, you must carefully manage the evolution of an API that’s used by third-\nparty developers. You typically have to maintain older versions for a long time—pos-\nsibly forever.\n This requirement is a huge burden for an organization. It’s impractical to make\nthe developers of the backend services responsible for maintaining long-term back-\nward compatibility. Rather than expose services directly to third-party developers,\norganizations should have a separate public API that’s developed by a separate team.\nAs you’ll learn later, the public API is implemented by an architectural component\nknown as an API gateway. Let’s look at how an API gateway works. \n8.2\nThe API gateway pattern\nAs you’ve just seen, there are numerous drawbacks with services accessing services\ndirectly. It’s often not practical for a client to perform API composition over the inter-\nnet. The lack of encapsulation makes it difficult for developers to change service\ndecomposition and APIs. Services sometimes use communication protocols that\naren’t suitable outside the firewall. Consequently, a much better approach is to use an\nAPI gateway.\nAn API gateway is a service that’s the entry point into the application from the outside\nworld. It’s responsible for request routing, API composition, and other functions,\nsuch as authentication. This section covers the API gateway pattern. I discuss its bene-\nfits and drawbacks and describe various design issues you must address when develop-\ning an API gateway.\n8.2.1\nOverview of the API gateway pattern\nSection 8.1.1 described the drawbacks of clients, such as the FTGO mobile applica-\ntion, making multiple requests in order to display information to the user. A much\nbetter approach is for a client to make a single request to an API gateway, a service\nthat serves as the single entry point for API requests into an application from outside\nthe firewall. It’s similar to the Facade pattern from object-oriented design. Like a facade,\nan API gateway encapsulates the application’s internal architecture and provides an API\nto its clients. It may also have other responsibilities, such as authentication, monitoring,\nPattern: API gateway\nImplement a service that’s the entry point into the microservices-based application\nfrom external API clients. See http://microservices.io/patterns/apigateway.html.\n \n\n\n260\nCHAPTER 8\nExternal API patterns\nand rate limiting. Figure 8.3 shows the relationship between the clients, the API gate-\nway, and the services.\nThe API gateway is responsible for request routing, API composition, and protocol\ntranslation. All API requests from external clients first go to the API gateway, which\nroutes some requests to the appropriate service. The API gateway handles other\nrequests using the API composition pattern and by invoking multiple services and\naggregating the results. It may also translate between client-friendly protocols such as\nHTTP and WebSockets and client-unfriendly protocols used by the services.\nREQUEST ROUTING\nOne of the key functions of an API gateway is request routing. An API gateway imple-\nments some API operations by routing requests to the corresponding service. When it\nreceives a request, the API gateway consults a routing map that specifies which service\nto route the request to. A routing map might, for example, map an HTTP method\nand path to the HTTP URL of a service. This function is identical to the reverse proxy-\ning features provided by web servers such as NGINX. \nLower-performance\ninternet\nHigher-performance\nLAN\nBackend services\nOrder Service\nFirewall\nAPI\nrequests\nAPI\nrequests\nAPI\nrequests\nWeb page\nrequests\nWeb\napplication\nConsumer\nService\nDelivery\nService\nBrowser\niPhone/\nAndroid\napplication\n3rd-party\napplication\nHTML\nJavaScript\napplication\nAPI\ngateway\nFigure 8.3\nThe API gateway is the single entry point into the application for API calls from outside \nthe firewall.\n \n\n\n261\nThe API gateway pattern\nAPI COMPOSITION\nAn API gateway typically does more than simply reverse proxying. It might also imple-\nment some API operations using API composition. The FTGO API gateway, for exam-\nple, implements the Get Order Details API operation using API composition. As\nfigure 8.4 shows, the mobile application makes one request to the API gateway, which\nfetches the order details from multiple services.\n The FTGO API gateway provides a coarse-grained API that enables mobile clients\nto retrieve the data they need with a single request. For example, the mobile client\nmakes a single getOrderDetails() request to the API gateway. \nFTGO backend services\nOrder Service\nFirewall\nInternet\ngetOrder()\nLAN\ngetDelivery()\ngetOrderDetails()\ngetBill()\ngetTicket()\nDelivery\nService\nAccounting\nService\nKitchen\nService\niPhone/\nAndroid\nconsumer\napplication\nAPI\ngateway\nFTGO backend services\nOrder Service\nFirewall\nInternet\ngetOrder()\ngetDelivery()\ngetBill()\ngetTicket()\nDelivery\nService\nAccounting\nService\nKitchen\nService\niPhone/\nAndroid\nconsumer\napplication\nMany API calls required\nOne API call required\nLower-performance\nnetwork\nHigher-performance\nnetwork\nFigure 8.4\nAn API gateway often does API composition, which enables a client such as a mobile \ndevice to efficiently retrieve data using a single API request.\n \n",
      "page_number": 278
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 287-298)",
      "start_page": 287,
      "end_page": 298,
      "detection_method": "topic_boundary",
      "content": "262\nCHAPTER 8\nExternal API patterns\nPROTOCOL TRANSLATION\nAn API gateway might also perform protocol translation. It might provide a RESTful\nAPI to external clients, even though the application services use a mixture of protocols\ninternally, including REST and gRPC. When needed, the implementation of some\nAPI operations translates between the RESTful external API and the internal gRPC-\nbased APIs. \nTHE API GATEWAY PROVIDES EACH CLIENT WITH CLIENT-SPECIFIC API\nAn API gateway could provide a single one-size-fits-all (OSFA) API. The problem with\na single API is that different clients often have different requirements. For instance, a\nthird-party application might require the Get Order Details API operation to return\nthe complete Order details, whereas a mobile client only needs a subset of the data.\nOne way to solve this problem is to give clients the option of specifying in a request\nwhich fields and related objects the server should return. This approach is adequate\nfor a public API that must serve a broad range of third-party applications, but it often\ndoesn’t give clients the control they need.\n A better approach is for the API gateway to provide each client with its own API.\nFor example, the FTGO API gateway can provide the FTGO mobile client with an API\nthat’s specifically designed to meet its requirements. It may even have different APIs\nfor the Android and iPhone mobile applications. The API gateway will also implement\na public API for third-party developers to use. Later on, I’ll describe the Backends for\nfrontends pattern that takes this concept of an API-per-client even further by defining\na separate API gateway for each client. \nIMPLEMENTING EDGE FUNCTIONS\nAlthough an API gateway’s primary responsibilities are API routing and composition,\nit may also implement what are known as edge functions. An edge function is, as the\nname suggests, a request-processing function implemented at the edge of an applica-\ntion. Examples of edge functions that an application might implement include the\nfollowing:\nAuthentication—Verifying the identity of the client making the request.\nAuthorization—Verifying that the client is authorized to perform that particular\noperation.\nRate limiting —Limiting how many requests per second from either a specific cli-\nent and/or from all clients.\nCaching—Cache responses to reduce the number of requests made to the services.\nMetrics collection—Collect metrics on API usage for billing analytics purposes.\nRequest logging—Log requests.\nThere are three different places in your application where you could implement these\nedge functions. First, you can implement them in the backend services. This might\nmake sense for some functions, such as caching, metrics collection, and possibly autho-\nrization. But it’s generally more secure if the application authenticates requests on the\nedge before they reach the services.\n \n\n\n263\nThe API gateway pattern\n The second option is to implement these edge functions in an edge service that’s\nupstream from the API gateway. The edge service is the first point of contact for an\nexternal client. It authenticates the request and performs other edge processing\nbefore passing it to the API gateway.\n An important benefit of using a dedicated edge service is that it separates con-\ncerns. The API gateway focuses on API routing and composition. Another benefit is\nthat it centralizes responsibility for critical edge functions such as authentication.\nThat’s particularly valuable when an application has multiple API gateways that are\npossibly written using a variety of languages and frameworks. I’ll talk more about that\nlater. The drawback of this approach is that it increases network latency because of the\nextra hop. It also adds to the complexity of the application.\n As a result, it’s often convenient to use the third option and implement these edge\nfunctions, especially authorization, in the API gateway itself. There’s one less network\nhop, which improves latency. There are also fewer moving parts, which reduces com-\nplexity. Chapter 11 describes how the API gateway and the services collaborate to\nimplement security. \nAPI GATEWAY ARCHITECTURE\nAn API gateway has a layered, modular architecture. Its architecture, shown in figure 8.5,\nconsists of two layers: the API layer and a common layer. The API layer consists of\none or more independent API modules. Each API module implements an API for a\nAPI gateway\nMobile client\nMobile API\nAPI layer\nBrowser JavaScript\napplication\nBrowser API\nCommon layer\n3rd-party application\nPublic API\nFigure 8.5\nAn API gateway has a layered modular architecture. The API for each client is \nimplemented by a separate module. The common layer implements functionality common to all \nAPIs, such as authentication.\n \n\n\n264\nCHAPTER 8\nExternal API patterns\nparticular client. The common layer implements shared functionality, including edge\nfunctions such as authentication.\n In this example, the API gateway has three API modules:\nMobile API—Implements the API for the FTGO mobile client\nBrowser API—Implements the API for the JavaScript application running in the\nbrowser\nPublic API—Implements the API for third-party developers\nAn API module implements each API operation in one of two ways. Some API opera-\ntions map directly to single service API operation. An API module implements these\noperations by routing requests to the corresponding service API operation. It might\nroute requests using a generic routing module that reads a configuration file describ-\ning the routing rules.\n An API module implements other, more complex API operations using API com-\nposition. The implementation of this API operation consists of custom code. Each API\noperation implementation handles requests by invoking multiple services and com-\nbining the results. \nAPI GATEWAY OWNERSHIP MODEL\nAn important question that you must answer is who is responsible for the develop-\nment of the API gateway and its operation? There are a few different options. One is\nfor a separate team to be responsible for the API gateway. The drawback to that is that\nit’s similar to SOA, where an Enterprise Service Bus (ESB) team was responsible for all\nESB development. If a developer working on the mobile application needs access to a\nparticular service, they must submit a request to the API gateway team and wait for\nthem to expose the API. This kind of centralized bottleneck in the organization is very\nmuch counter to the philosophy of the microservice architecture, which promotes\nloosely coupled autonomous teams.\n A better approach, which has been promoted by Netflix, is for the client teams—\nthe mobile, web, and public API teams—to own the API module that exposes their\nAPI. An API gateway team is responsible for developing the Common module and for\nthe operational aspects of the gateway. This ownership model, shown in figure 8.6,\ngives the teams control over their APIs.\n When a team needs to change their API, they check in the changes to the source\nrepository for the API gateway. To work well, the API gateway’s deployment pipeline\nmust be fully automated. Otherwise, the client teams will often be blocked waiting for\nthe API gateway team to deploy the new version. \nUSING THE BACKENDS FOR FRONTENDS PATTERN\nOne concern with an API gateway is that responsibility for it is blurred. Multiple teams\ncontribute to the same code base. An API gateway team is responsible for its opera-\ntion. Though not as bad as a SOA ESB, this blurring of responsibilities is counter to\nthe microservice architecture philosophy of “if you build it, you own it.”\n \n\n\n265\nThe API gateway pattern\nThe solution is to have an API gateway for each client, the so-called Backends for front-\nends (BFF) pattern, which was pioneered by Phil Calçado (http://philcalcado.com/)\nand his colleagues at SoundCloud. As figure 8.7 shows, each API module becomes its\nown standalone API gateway that’s developed and operated by a single client team.\nPattern: Backends for frontends\nImplement a separate API gateway for each type of client. See http://microservices\n.io/patterns/apigateway.html.\nAPI gateway\nMobile client\nMobile API\nAPI layer\nBrowser JavaScript\napplication\nBrowser API\nCommon layer\n3rd-party application\nPublic API\nMobile client team\nAPI gateway team\nBrowser client team\nOwns\nOwns\nOwns\nOwns\nPublic API team\nFigure 8.6\nA client team owns their API module. As they change the client, they can change the API \nmodule and not ask the API gateway team to make the changes.\n \n\n\n266\nCHAPTER 8\nExternal API patterns\nThe public API team owns and operates their API gateway, the mobile team owns and\noperates theirs, and so on. In theory, different API gateways could be developed using\ndifferent technology stacks. But that risks duplicating code for common functionality,\nsuch as the code that implements edge functions. Ideally, all API gateways use the\nsame technology stack. The common functionality is a shared library implemented by\nthe API gateway team.\n Besides clearly defining responsibilities, the BFF pattern has other benefits. The\nAPI modules are isolated from one another, which improves reliability. One misbehav-\ning API can’t easily impact other APIs. It also improves observability, because different\nAPI modules are different processes. Another benefit of the BFF pattern is that each\nAPI is independently scalable. The BFF pattern also reduces startup time because\neach API gateway is a smaller, simpler application. \n \n \nMobile API\ngateway\nMobile client\nMobile API\nAPI layer\nCommon layer\nBrowser API\ngateway\nBrowser API\nAPI layer\nCommon layer\nPublic API\ngateway\nPublic API\nAPI layer\nCommon layer\nMobile client team\nBrowser client team\nOwns\nOwns\nOwns\nPublic API team\nBrowser JavaScript\napplication\n3rd-party application\nFigure 8.7\nThe Backends for frontends pattern defines a separate API gateway for each client. Each \nclient team owns their API gateway. An API gateway team owns the common layer.\n \n\n\n267\nThe API gateway pattern\n8.2.2\nBenefits and drawbacks of an API gateway\nAs you might expect, the API gateway pattern has both benefits and drawbacks.\nBENEFITS OF AN API GATEWAY\nA major benefit of using an API gateway is that it encapsulates internal structure of the\napplication. Rather than having to invoke specific services, clients talk to the gateway.\nThe API gateway provides each client with a client-specific API, which reduces the\nnumber of round-trips between the client and application. It also simplifies the client\ncode. \nDRAWBACKS OF AN API GATEWAY\nThe API gateway pattern also has some drawbacks. It is yet another highly available\ncomponent that must be developed, deployed, and managed. There’s also a risk that\nthe API gateway becomes a development bottleneck. Developers must update the API\ngateway in order to expose their services’s API. It’s important that the process for\nupdating the API gateway be as lightweight as possible. Otherwise, developers will be\nforced to wait in line in order to update the gateway. Despite these drawbacks, though,\nfor most real-world applications, it makes sense to use an API gateway. If necessary,\nyou can use the Backends for frontends pattern to enable the teams to develop and\ndeploy their APIs independently. \n8.2.3\nNetflix as an example of an API gateway\nA great example of an API gateway is the Netflix API. The Netflix streaming service is\navailable on hundreds of different kinds of devices including televisions, Blu-ray\nplayers, smartphones, and many more gadgets. Initially, Netflix attempted to have a\none-size-fits-all style API for its streaming service (www.programmableweb.com/news/\nwhy-rest-keeps-me-night/2012/05/15). But the company soon discovered that didn’t\nwork well because of the diverse range of devices and their different needs. Today,\nNetflix uses an API gateway that implements a separate API for each device. The client\ndevice team develops and owns the API implementation.\n In the first version of the API gateway, each client team implemented their API\nusing Groovy scripts that perform routing and API composition. Each script invoked\none or more service APIs using Java client libraries provided by the service teams. On\none hand, this works well, and client developers have written thousands of scripts. The\nNetflix API gateway handles billions of requests per day, and on average each API call\nfans out to six or seven backend services. On the other hand, Netflix has found this\nmonolithic architecture to be somewhat cumbersome.\n As a result, Netflix is now moving to an API gateway architecture similar to the\nBackends for frontends pattern. In this new architecture, client teams write API mod-\nules using NodeJS. Each API module runs its own Docker container, but the scripts\ndon’t invoke the services directly. Rather, they invoke a second “API gateway,” which\nexposes the service APIs using Netflix Falcor. Netflix Falcor is an API technology that\ndoes declarative, dynamic API composition and enables a client to invoke multiple\n \n\n\n268\nCHAPTER 8\nExternal API patterns\nservices using a single request. This new architecture has a number of benefits. The\nAPI modules are isolated from one another, which improves reliability and observabil-\nity, and the client API module is independently scalable. \n8.2.4\nAPI gateway design issues\nNow that we’ve looked at the API gateway pattern and its benefits and drawbacks, let’s\nexamine various API gateway design issues. There are several issues to consider when\ndesigning an API gateway:\nPerformance and scalability\nWriting maintainable code by using reactive programming abstractions\nHandling partial failure\nBeing a good citizen in the application’s architecture\nWe’ll look at each one.\nPERFORMANCE AND SCALABILITY\nAn API gateway is the application’s front door. All external requests must first pass\nthrough the gateway. Although most companies don’t operate at the scale of Netflix,\nwhich handles billions of requests per day, the performance and scalability of the API\ngateway is usually very important. A key design decision that affects performance and\nscalability is whether the API gateway should use synchronous or asynchronous I/O.\n In the synchronous I/O model , each network connection is handled by a dedicated\nthread. This is a simple programming model and works reasonably well. For example,\nit’s the basis of the widely used Java EE servlet framework, although this framework\nprovides the option of completing a request asynchronously. One limitation of syn-\nchronous I/O, however, is that operating system threads are heavyweight, so there is a\nlimit on the number of threads, and hence concurrent connections, that an API gate-\nway can have.\n The other approach is to use the asynchronous (nonblocking) I/O model . In this\nmodel, a single event loop thread dispatches I/O requests to event handlers. You have\na variety of asynchronous I/O technologies to choose from. On the JVM you can use\none of the NIO-based frameworks such as Netty, Vertx, Spring Reactor, or JBoss\nUndertow. One popular non-JVM option is NodeJS, a platform built on Chrome’s\nJavaScript engine.\n Nonblocking I/O is much more scalable because it doesn’t have the overhead of\nusing multiple threads. The drawback, though, is that the asynchronous, callback-\nbased programming model is much more complex. The code is more difficult to\nwrite, understand, and debug. Event handlers must return quickly to avoid blocking\nthe event loop thread.\n Also, whether using nonblocking I/O has a meaningful overall benefit depends on\nthe characteristics of the API gateway’s request-processing logic. Netflix had mixed results\nwhen it rewrote Zuul, its edge server, to use NIO (see https://medium.com/netflix-\ntechblog/zuul-2-the-netflix-journey-to-asynchronous-non-blocking-systems-45947377fb5c).\n \n\n\n269\nThe API gateway pattern\nOn one hand, as you would expect, using NIO reduced the cost of each network con-\nnection, due to the fact that there’s no longer a dedicated thread for each one. Also, a\nZuul cluster that ran I/O-intensive logic—such as request routing—had a 25% increase\nin throughput and a 25% reduction in CPU utilization. On the other hand, a Zuul clus-\nter that ran CPU-intensive logic—such as decryption and compression—showed no\nimprovement. \nUSE REACTIVE PROGRAMMING ABSTRACTIONS\nAs mentioned earlier, API composition consists of invoking multiple backend services.\nSome backend service requests depend entirely on the client request’s parameters.\nOthers might depend on the results of other service requests. One approach is for an\nAPI endpoint handler method to call the services in the order determined by the depen-\ndencies. For example, the following listing shows the handler for the findOrder()\nrequest that’s written this way. It calls each of the four services, one after the other.\n@RestController\npublic class OrderDetailsController {\n@RequestMapping(\"/order/{orderId}\")\npublic OrderDetails getOrderDetails(@PathVariable String orderId) {\nOrderInfo orderInfo = orderService.findOrderById(orderId);\nTicketInfo ticketInfo = kitchenService\n.findTicketByOrderId(orderId);\nDeliveryInfo deliveryInfo = deliveryService\n.findDeliveryByOrderId(orderId);\nBillInfo billInfo = accountingService\n.findBillByOrderId(orderId);\nOrderDetails orderDetails =\nOrderDetails.makeOrderDetails(orderInfo, ticketInfo,\ndeliveryInfo, billInfo);\nreturn orderDetails;\n}\n...\nThe drawback of calling the services sequentially is that the response time is the sum\nof the service response times. In order to minimize response time, the composition\nlogic should, whenever possible, invoke services concurrently. In this example, there\nare no dependencies between the service calls. All services should be invoked concur-\nrently, which significantly reduces response time. The challenge is to write concurrent\ncode that’s maintainable.\n This is because the traditional way to write scalable, concurrent code is to use\ncallbacks. Asynchronous, event-driven I/O is inherently callback-based. Even a Servlet\nListing 8.1\nFetching the order details by calling the backend services sequentially\n \n\n\n270\nCHAPTER 8\nExternal API patterns\nAPI-based API composer that invokes services concurrently typically uses callbacks. It\ncould execute requests concurrently by calling ExecutorService.submitCallable().\nThe problem there is that this method returns a Future, which has a blocking API. A\nmore scalable approach is for an API composer to call ExecutorService.submit\n(Runnable) and for each Runnable to invoke a callback with the outcome of the\nrequest. The callback accumulates results, and once all of them have been received it\nsends back the response to the client.\n Writing API composition code using the traditional asynchronous callback approach\nquickly leads you to callback hell. The code will be tangled, difficult to understand,\nand error prone, especially when composition requires a mixture of parallel and\nsequential requests. A much better approach is to write API composition code in a\ndeclarative style using a reactive approach. Examples of reactive abstractions for the\nJVM include the following:\nJava 8 CompletableFutures\nProject Reactor Monos\nRxJava (Reactive Extensions for Java) Observables, created by Netflix specifi-\ncally to solve this problem in its API gateway\nScala Futures\nA NodeJS-based API gateway would use JavaScript promises or RxJS, which is reactive\nextensions for JavaScript. Using one of these reactive abstractions will enable you to\nwrite concurrent code that’s simple and easy to understand. Later in this chapter, I\nshow an example of this style of coding using Project Reactor Monos and version 5 of\nthe Spring Framework. \nHANDLING PARTIAL FAILURES\nAs well as being scalable, an API gateway must also be reliable. One way to achieve reli-\nability is to run multiple instances of the gateway behind a load balancer. If one\ninstance fails, the load balancer will route requests to the other instances.\n Another way to ensure that an API gateway is reliable is to properly handle failed\nrequests and requests that have unacceptably high latency. When an API gateway\ninvokes a service, there’s always a chance that the service is slow or unavailable. An API\ngateway may wait a very long time, perhaps indefinitely, for a response, which con-\nsumes resources and prevents it from sending a response to its client. An outstanding\nrequest to a failed service might even consume a limited, precious resource such as a\nthread and ultimately result in the API gateway being unable to handle any other\nrequests. The solution, as described in chapter 3, is for an API gateway to use the Cir-\ncuit breaker pattern when invoking services. \nBEING A GOOD CITIZEN IN THE ARCHITECTURE\nIn chapter 3 I described patterns for service discovery, and in chapter 11, I cover\npatterns for observability. The service discovery patterns enable a service client,\nsuch as an API gateway, to determine the network location of a service instance so\nthat it can invoke it. The observability patterns enable developers to monitor the\n \n\n\n271\nImplementing an API gateway\nbehavior of an application and troubleshoot problems. An API gateway, like other ser-\nvices in the architecture, must implement the patterns that have been selected for the\narchitecture. \n8.3\nImplementing an API gateway\nLet’s now look at how to implement an API gateway. As mentioned earlier, the respon-\nsibilities of an API gateway are as follows:\nRequest routing—Routes requests to services using criteria such as HTTP request\nmethod and path. The API gateway must route using the HTTP request method\nwhen the application has one or more CQRS query services. As discussed in\nchapter 7, in such an architecture commands and queries are handled by sepa-\nrate services.\nAPI composition—Implements a GET REST endpoint using the API composition\npattern, described in chapter 7. The request handler combines the results of\ninvoking multiple services.\nEdge functions—Most notable among these is authentication.\nProtocol translation—Translates between client-friendly protocols and the client-\nunfriendly protocols used by services.\nBeing a good citizen in the application’s architecture.\nThere are a couple of different ways to implement an API gateway:\nUsing an off-the-shelf API gateway product/service—This option requires little or no\ndevelopment but is the least flexible. For example, an off-the-shelf API gateway\ntypically does not support API composition\nDeveloping your own API gateway using either an API gateway framework or a web frame-\nwork as the starting point—This is the most flexible approach, though it requires\nsome development effort.\nLet’s look at these options, starting with using an off-the-shelf API gateway product or\nservice.\n8.3.1\nUsing an off-the-shelf API gateway product/service\nSeveral off-the-self services and products implement API gateway features. Let’s first\nlook at a couple of services that are provided by AWS. After that, I’ll discuss some\nproducts that you can download, configure, and run yourself.\nAWS API GATEWAY\nThe AWS API gateway, one of the many services provided by Amazon Web Services, is\na service for deploying and managing APIs. An AWS API gateway API is a set of REST\nresources, each of which supports one or more HTTP methods. You configure the API\ngateway to route each (Method, Resource) to a backend service. A backend service is\neither an AWS Lambda Function, described later in chapter 12, an application-\ndefined HTTP service, or an AWS service. If necessary, you can configure the API\n \n\n\n272\nCHAPTER 8\nExternal API patterns\ngateway to transform request and response using a template-based mechanism. The\nAWS API gateway can also authenticate requests.\n The AWS API gateway fulfills some of the requirements for an API gateway that I\nlisted earlier. The API gateway is provided by AWS, so you’re not responsible for instal-\nlation and operations. You configure the API gateway, and AWS handles everything\nelse, including scaling.\n Unfortunately, the AWS API gateway has several drawbacks and limitations that\ncause it to not fulfill other requirements. It doesn’t support API composition, so you’d\nneed to implement API composition in the backend services. The AWS API gateway\nonly supports HTTP(S) with a heavy emphasis on JSON. It only supports the Server-\nside discovery pattern, described in chapter 3. An application will typically use an AWS\nElastic Load Balancer to load balance requests across a set of EC2 instances or ECS\ncontainers. Despite these limitations, unless you need API composition, the AWS API\ngateway is a good implementation of the API gateway pattern. \nAWS APPLICATION LOAD BALANCER\nAnother AWS service that provides API gateway-like functionality is the AWS Applica-\ntion Load Balancer, which is a load balancer for HTTP, HTTPS, WebSocket, and\nHTTP/2 (https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/).\nWhen configuring an Application Load Balancer, you define routing rules that route\nrequests to backend services, which must be running on AWS EC2 instances.\n Like the AWS API gateway, the AWS Application Load Balancer meets some of the\nrequirements for an API gateway. It implements basic routing functionality. It’s hosted,\nso you’re not responsible for installation or operations. Unfortunately, it’s quite lim-\nited. It doesn’t implement HTTP method-based routing. Nor does it implement API\ncomposition or authentication. As a result, the AWS Application Load Balancer\ndoesn’t meet the requirements for an API gateway. \nUSING AN API GATEWAY PRODUCT\nAnother option is to use an API gateway product such as Kong or Traefik . These are\nopen source packages that you install and operate yourself. Kong is based on the\nNGINX HTTP server, and Traefik is written in GoLang. Both products let you config-\nure flexible routing rules that use the HTTP method, headers, and path to select the\nbackend service. Kong lets you configure plugins that implement edge functions such\nas authentication. Traefik can even integrate with some service registries, described in\nchapter 3.\n Although these products implement edge functions and powerful routing capabil-\nities, they have some drawbacks. You must install, configure, and operate them your-\nself. They don’t support API composition. And if you want the API gateway to perform\nAPI composition, you must develop your own API gateway. \n \n\n\n273\nImplementing an API gateway\n8.3.2\nDeveloping your own API gateway\nDeveloping an API gateway isn’t particularly difficult. It’s basically a web application\nthat proxies requests to other services. You can build one using your favorite web\nframework. There are, however, two key design problems that you’ll need to solve:\nImplementing a mechanism for defining routing rules in order to minimize the\ncomplex coding\nCorrectly implementing the HTTP proxying behavior, including how HTTP\nheaders are handled\nConsequently, a better starting point for developing an API gateway is to use a frame-\nwork designed for that purpose. Its built-in functionality significantly reduces the\namount of code you need to write.\n We’ll take a look at Netflix Zuul, an open source project by Netflix, and then con-\nsider the Spring Cloud Gateway, an open source project from Pivotal.\nUSING NETFLIX ZUUL\nNetflix developed the Zuul framework to implement edge functions such as routing,\nrate limiting, and authentication (https://github.com/Netflix/zuul). The Zuul frame-\nwork uses the concept of filters, reusable request interceptors that are similar to servlet\nfilters or NodeJS Express middleware. Zuul handles an HTTP request by assembling a\nchain of applicable filters that then transform the request, invoke backend services,\nand transform the response before it’s sent back to the client. Although you can use\nZuul directly, using Spring Cloud Zuul, an open source project from Pivotal, is far eas-\nier. Spring Cloud Zuul builds on Zuul and through convention-over-configuration\nmakes developing a Zuul-based server remarkably easy.\n Zuul handles the routing and edge functionality. You can extend Zuul by defining\nSpring MVC controllers that implement API composition. But a major limitation of\nZuul is that it can only implement path-based routing. For example, it’s incapable of\nrouting GET /orders to one service and POST /orders to a different service. Conse-\nquently, Zuul doesn’t support the query architecture described in chapter 7. \nABOUT SPRING CLOUD GATEWAY\nNone of the options I’ve described so far meet all the requirements. In fact, I had\ngiven up in my search for an API gateway framework and had started developing an\nAPI gateway based on Spring MVC. But then I discovered the Spring Cloud Gate-\nway project (https://cloud.spring.io/spring-cloud-gateway/). It’s an API gateway\nframework built on top of several frameworks, including Spring Framework 5,\nSpring Boot 2, and Spring Webflux, which is a reactive web framework that's part of\nSpring Framework 5 and built on Project Reactor. Project Reactor is an NIO-based\nreactive framework for the JVM that provides the Mono abstraction used a little\nlater in this chapter.\n \n \n \n",
      "page_number": 287
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 299-306)",
      "start_page": 299,
      "end_page": 306,
      "detection_method": "topic_boundary",
      "content": "274\nCHAPTER 8\nExternal API patterns\n Spring Cloud Gateway provides a simple yet comprehensive way to do the following:\nRoute requests to backend services.\nImplement request handlers that perform API composition.\nHandle edge functions such as authentication.\nFigure 8.8 shows the key parts of an API gateway built using this framework.\nThe API gateway consists of the following packages:\n\nApiGatewayMain package—Defines the Main program for the API gateway.\nOne or more API packages—An API package implements a set of API endpoints.\nFor example, the Orders package implements the Order-related API endpoints.\nProxy package—Consists of proxy classes that are used by the API packages to\ninvoke the services.\n«@SpringBootApplication»\nApiGatewayApplication\n«@Bean»\norderProxyRouting\n«@Bean»\norderHandlerRouting\nGET/orders/{orderId}\n=>\nOrderHandlers::getOrderDetails\norders*\n=>\nhttp://orderservice\nmono<ServerResponse>\ngetOrderDetails(ServerRequest){\n...\n}\nmono<OrderInfo>\nﬁndOrderById()(orderId){\n...WebClient\n.get()\n.url(\"http://order-service/...\"}\n}\nstatic void main(String[]args){\n...\n}\nRemote proxies«package»\nOrders«API package»\n«proxy»\n....\n«proxy»\nDeliveryService\nﬁndDeliveryByOrder()\n«proxy»\nOrderService\nﬁndOrderById()\nOrder handlers\nSpring Cloud Gateway\nSpring 5\ngetOrderDetails()\nSpring webFlux\nProject reactor\n«Spring Conﬁguration»OrderConﬁguration\nFigure 8.8\nThe architecture of an API gateway built using Spring Cloud Gateway\n \n\n\n275\nImplementing an API gateway\nThe OrderConfiguration class defines the Spring beans responsible for routing\nOrder-related requests. A routing rule can match against some combination of the\nHTTP method, the headers, and the path. The orderProxyRoutes @Bean defines rules\nthat map API operations to backend service URLs. For example, it routes paths begin-\nning with /orders to the Order Service.\n The orderHandlers @Bean defines rules that override those defined by order-\nProxyRoutes. These rules map API operations to handler methods, which are the\nSpring WebFlux equivalent of Spring MVC controller methods. For example, order-\nHandlers maps the operation GET /orders/{orderId} to the OrderHandlers::get-\nOrderDetails() method.\n The OrderHandlers class implements various request handler methods, such as\nOrderHandlers::getOrderDetails(). This method uses API composition to fetch the\norder details (described earlier). The handle methods invoke backend services using\nremote proxy classes, such as OrderService. This class defines methods for invoking\nthe OrderService.\n Let’s take a look at the code, starting with the OrderConfiguration class. \nTHE ORDERCONFIGURATION CLASS\nThe OrderConfiguration class, shown in listing 8.2, is a Spring @Configuration class.\nIt defines the Spring @Beans that implement the /orders endpoints. The order-\nProxyRouting and orderHandlerRouting @Beans use the Spring WebFlux routing\nDSL to define the request routing. The orderHandlers @Bean implements the request\nhandlers that perform API composition.\n@Configuration\n@EnableConfigurationProperties(OrderDestinations.class)\npublic class OrderConfiguration {\n@Bean\npublic RouteLocator orderProxyRouting(OrderDestinations orderDestinations) {\nreturn Routes.locator()\n.route(\"orders\")\n.uri(orderDestinations.orderServiceUrl)\n.predicate(path(\"/orders\").or(path(\"/orders/*\")))\n  \n.and()\n...\n.build();\n}\n@Bean\npublic RouterFunction<ServerResponse>\norderHandlerRouting(OrderHandlers orderHandlers) {\nreturn RouterFunctions.route(GET(\"/orders/{orderId}\"),\n  \norderHandlers::getOrderDetails);\n}\nListing 8.2\nThe Spring @Beans that implement the /orders endpoints\nBy default, route all requests whose\npath begins with /orders to the URL\norderDestinations.orderServiceUrl.\nRoute a GET \n/orders/{orderId} \nto orderHandlers::\ngetOrderDetails.\n \n\n\n276\nCHAPTER 8\nExternal API patterns\n@Bean\npublic OrderHandlers orderHandlers(OrderService orderService,\nKitchenService kitchenService,\nDeliveryService deliveryService,\nAccountingService accountingService) {\nreturn new OrderHandlers(orderService, kitchenService,\n      \ndeliveryService, accountingService);\n}\n}\nOrderDestinations, shown in the following listing, is a Spring @Configuration-\nProperties class that enables the externalized configuration of backend service URLs.\n@ConfigurationProperties(prefix = \"order.destinations\")\npublic class OrderDestinations {\n@NotNull\npublic String orderServiceUrl;\npublic String getOrderServiceUrl() {\nreturn orderServiceUrl;\n}\npublic void setOrderServiceUrl(String orderServiceUrl) {\nthis.orderServiceUrl = orderServiceUrl;\n}\n...\n}\nYou can, for example, specify the URL of the Order Service either as the order\n.destinations.orderServiceUrl property in a properties file or as an operating sys-\ntem environment variable, ORDER_DESTINATIONS_ORDER_SERVICE_URL. \nTHE ORDERHANDLERS CLASS\nThe OrderHandlers class, shown in the following listing, defines the request handler\nmethods that implement custom behavior, including API composition. The getOrder-\nDetails() method, for example, performs API composition to retrieve information\nabout an order. This class is injected with several proxy classes that make requests to\nbackend services.\npublic class OrderHandlers {\nprivate OrderService orderService;\nprivate KitchenService kitchenService;\nprivate DeliveryService deliveryService;\nprivate AccountingService accountingService;\nListing 8.3\nThe externalized configuration of backend service URLs\nListing 8.4\nThe OrderHandlers class implements custom request-handling logic.\nThe @Bean, which implements the\ncustom request-handling logic\n \n\n\n277\nImplementing an API gateway\npublic OrderHandlers(OrderService orderService,\nKitchenService kitchenService,\nDeliveryService deliveryService,\nAccountingService accountingService) {\nthis.orderService = orderService;\nthis.kitchenService = kitchenService;\nthis.deliveryService = deliveryService;\nthis.accountingService = accountingService;\n}\npublic Mono<ServerResponse> getOrderDetails(ServerRequest serverRequest) {\nString orderId = serverRequest.pathVariable(\"orderId\");\nMono<OrderInfo> orderInfo = orderService.findOrderById(orderId);\nMono<Optional<TicketInfo>> ticketInfo =\nkitchenService\n.findTicketByOrderId(orderId)\n.map(Optional::of)\n      \n.onErrorReturn(Optional.empty());\n  \nMono<Optional<DeliveryInfo>> deliveryInfo =\ndeliveryService\n.findDeliveryByOrderId(orderId)\n.map(Optional::of)\n.onErrorReturn(Optional.empty());\nMono<Optional<BillInfo>> billInfo = accountingService\n.findBillByOrderId(orderId)\n.map(Optional::of)\n.onErrorReturn(Optional.empty());\nMono<Tuple4<OrderInfo, Optional<TicketInfo>,\n  \nOptional<DeliveryInfo>, Optional<BillInfo>>> combined =\nMono.when(orderInfo, ticketInfo, deliveryInfo, billInfo);\nMono<OrderDetails> orderDetails =\n               \ncombined.map(OrderDetails::makeOrderDetails);\nreturn orderDetails.flatMap(person -> ServerResponse.ok()   \n.contentType(MediaType.APPLICATION_JSON)\n.body(fromObject(person)));\n}\n}\nThe getOrderDetails() method implements API composition to fetch the order\ndetails. It’s written in a scalable, reactive style using the Mono abstraction , which is pro-\nvided by Project Reactor. A Mono, which is a richer kind of Java 8 CompletableFuture,\ncontains the outcome of an asynchronous operation that’s either a value or an\nexception. It has a rich API for transforming and combining the values returned by\nasynchronous operations. You can use Monos to write concurrent code in a style that’s\nTransform a TicketInfo into \nan Optional<TicketInfo>.\nIf the service invocation failed, \nreturn Optional.empty().\nCombine the four \nvalues into a single \nvalue, a Tuple4.\nTransform the Tuple4 \ninto an OrderDetails.\nTransform the\nOrderDetails into\na ServerResponse.\n \n\n\n278\nCHAPTER 8\nExternal API patterns\nsimple and easy to understand. In this example, the getOrderDetails() method\ninvokes the four services in parallel and combines the results to create an Order-\nDetails object.\n The getOrderDetails() method takes a ServerRequest, which is the Spring Web-\nFlux representation of an HTTP request, as a parameter and does the following:\n1\nIt extracts the orderId from the path.\n2\nIt invokes the four services asynchronously via their proxies, which return Monos.\nIn order to improve availability, getOrderDetails() treats the results of all ser-\nvices except the OrderService as optional. If a Mono returned by an optional\nservice contains an exception, the call to onErrorReturn() transforms it into a\nMono containing an empty Optional.\n3\nIt combines the results asynchronously using Mono.when(), which returns a\nMono<Tuple4> containing the four values.\n4\nIt transforms the Mono<Tuple4> into a Mono<OrderDetails> by calling Order-\nDetails::makeOrderDetails.\n5\nIt transforms the OrderDetails into a ServerResponse, which is the Spring\nWebFlux representation of the JSON/HTTP response.\nAs you can see, because getOrderDetails() uses Monos, it concurrently invokes the\nservices and combines the results without using messy, difficult-to-read callbacks. Let’s\ntake a look at one of the service proxies that return the results of a service API call\nwrapped in a Mono. \nTHE ORDERSERVICE CLASS\nThe OrderService class, shown in the following listing, is a remote proxy for the Order\nService. It invokes the Order Service using a WebClient, which is the Spring Web-\nFlux reactive HTTP client.\n@Service\npublic class OrderService {\nprivate OrderDestinations orderDestinations;\nprivate WebClient client;\npublic OrderService(OrderDestinations orderDestinations, WebClient client)\n{\nthis.orderDestinations = orderDestinations;\nthis.client = client;\n}\npublic Mono<OrderInfo> findOrderById(String orderId) {\nMono<ClientResponse> response = client\n.get()\nListing 8.5\nOrderService class—a remote proxy for Order Service\n \n\n\n279\nImplementing an API gateway\n.uri(orderDestinations.orderServiceUrl + \"/orders/{orderId}\",\norderId)\n.exchange();\n    \nreturn response.flatMap(resp -> resp.bodyToMono(OrderInfo.class));  \n}\n}\nThe findOrder() method retrieves the OrderInfo for an order. It uses the WebClient\nto make the HTTP request to the Order Service and deserializes the JSON response\nto an OrderInfo. WebClient has a reactive API, and the response is wrapped in a Mono.\nThe findOrder() method uses flatMap() to transform the Mono<ClientResponse>\ninto a Mono<OrderInfo>. As the name suggests, the bodyToMono() method returns the\nresponse body as a Mono. \nTHE APIGATEWAYAPPLICATION CLASS\nThe ApiGatewayApplication class, shown in the following listing, implements the API\ngateway’s main() method. It’s a standard Spring Boot main class.\n@SpringBootConfiguration\n@EnableAutoConfiguration\n@EnableGateway\n@Import(OrdersConfiguration.class)\npublic class ApiGatewayApplication {\npublic static void main(String[] args) {\nSpringApplication.run(ApiGatewayApplication.class, args);\n}\n}\nThe @EnableGateway annotation imports the Spring configuration for the Spring\nCloud Gateway framework.\n Spring Cloud Gateway is an excellent framework for implementing an API gateway.\nIt enables you to configure basic proxying using a simple, concise routing rules DSL.\nIt’s also straightforward to route requests to handler methods that perform API com-\nposition and protocol translation. Spring Cloud Gateway is built using the scalable,\nreactive Spring Framework 5 and Project Reactor frameworks. But there’s another\nappealing option for developing your own API gateway: GraphQL, a framework that\nprovides graph-based query language. Let’s look at how that works. \n8.3.3\nImplementing an API gateway using GraphQL\nImagine that you’re responsible for implementing the FTGO’s API Gateway’s GET\n/orders/{orderId} endpoint, which returns the order details. On the surface, imple-\nmenting this endpoint might appear to be simple. But as described in section 8.1, this\nendpoint retrieves data from multiple services. Consequently, you need to use the\nListing 8.6\nThe main() method for the API gateway\nInvoke the\nservice.\nConvert the response\nbody to an OrderInfo.\n \n\n\n280\nCHAPTER 8\nExternal API patterns\nAPI composition pattern and write code that invokes the services and combines\nthe results.\n Another challenge, mentioned earlier, is that different clients need slightly differ-\nent data. For example, unlike the mobile application, the desktop SPA application dis-\nplays your rating for the order. One way to tailor the data returned by the endpoint, as\ndescribed in chapter 3, is to give the client the ability to specify the data they need. An\nendpoint can, for example, support query parameters such as the expand parameter,\nwhich specifies the related resources to return, and the field parameter, which speci-\nfies the fields of each resource to return. The other option is to define multiple ver-\nsions of this endpoint as part of applying the Backends for frontends pattern. This is a\nlot of work for just one of the many API endpoints that the FTGO’s API Gateway\nneeds to implement.\n Implementing an API gateway with a REST API that supports a diverse set of cli-\nents well is time consuming. Consequently, you may want to consider using a graph-\nbased API framework, such as GraphQL, that’s designed to support efficient data\nfetching. The key idea with graph-based API frameworks is that, as figure 8.9 shows,\nthe server’s API consists of a graph-based schema. The graph-based schema defines a\nset of nodes (types), which have properties (fields) and relationships with other nodes.\nThe client retrieves data by executing a query that specifies the required data in terms\nof the graph’s nodes and their properties and relationships. As a result, a client can\nretrieve the data it needs in a single round-trip to the API gateway.\nGraph-based API technology has a couple of important benefits. It gives clients con-\ntrol over what data is returned. Consequently, developing a single API that’s flexible\nConsumer\nConsumer\nRestaurant\nDelivery\nConsumer Service\nAPI gateway\nGraph-based API framework\nGraph schema\nOrder\nOrder Service\nRestaurant\nRestaurant Service\nDelivery\nDelivery Service\nOrder\nSchema\n=>\nService\nmapping\nClient\nQuery\nQuery\nQuery\nQuery\nQuery\nFigure 8.9\nThe API gateway’s API consists of a graph-based schema that’s mapped to the services. A client \nissues a query that retrieves multiple graph nodes. The graph-based API framework executes the query by \nretrieving data from one or more services.\n \n\n\n281\nImplementing an API gateway\nenough to support diverse clients becomes feasible. Another benefit is that even though\nthe API is much more flexible, this approach significantly reduces the development\neffort. That’s because you write the server-side code using a query execution frame-\nwork that’s designed to support API composition and projections. It’s as if, rather than\nforce clients to retrieve data via stored procedures that you need to write and main-\ntain, you let them execute queries against the underlying database.\nThis section talks about how to develop an API gateway using Apollo GraphQL. I’m\nonly going to cover a few of the key features of GraphQL and Apollo GraphQL. For\nmore information, you should consult the GraphQL and Apollo GraphQL docu-\nmentation.\n The GraphQL-based API gateway, shown in figure 8.10, is written in JavaScript\nusing the NodeJS Express web framework and the Apollo GraphQL server. The key\nparts of the design are as follows:\nGraphQL schema—The GraphQL schema defines the server-side data model and\nthe queries it supports.\nResolver functions—The resolve functions map elements of the schema to the\nvarious backend services.\nProxy classes—The proxy classes invoke the FTGO application’s services.\nThere’s also a small amount of glue code that integrates the GraphQL server with the\nExpress web framework. Let’s look at each part, starting with the GraphQL schema.\nSchema-driven API technologies\nThe two most popular graph-based API technologies are GraphQL (http://graphql.org)\nand Netflix Falcor (http://netflix.github.io/falcor/). Netflix Falcor models server-side\ndata as a virtual JSON object graph. The Falcor client retrieves data from a Falcor\nserver by executing a query that retrieves properties of that JSON object. The client\ncan also update properties. In the Falcor server, the properties of the object graph\nare mapped to backend data sources, such as services with REST APIs. The server\nhandles a request to set or get properties by invoking one or more backend data\nsources.\nGraphQL, developed by Facebook and released in 2015, is another popular graph-\nbased API technology. It models the server-side data as a graph of objects that have\nfields and references to other objects. The object graph is mapped to backend data\nsources. GraphQL clients can execute queries that retrieve data and mutations that\ncreate and update data. Unlike Netflix Falcor, which is an implementation, GraphQL\nis a standard, with clients and servers available for a variety of languages, including\nNodeJS, Java, and Scala.\nApollo GraphQL is a popular JavaScript/NodeJS implementation (www.apollographql\n.com). It’s a platform that includes a GraphQL server and client. Apollo GraphQL\nimplements some powerful extensions to the GraphQL specification, such as sub-\nscriptions that push changed data to the client.\n \n",
      "page_number": 299
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 307-317)",
      "start_page": 307,
      "end_page": 317,
      "detection_method": "topic_boundary",
      "content": "282\nCHAPTER 8\nExternal API patterns\nDEFINING A GRAPHQL SCHEMA\nA GraphQL API is centered around a schema, which consists of a collection of types\nthat define the structure of the server-side data model and the operations, such as\nqueries, that a client can perform. GraphQL has several different kinds of types. The\nexample code in this section uses just two kinds of types: object types, which are the\ntype Query{\norders(consumerId:Int!): [Order]\norder(orderId : int!): Order\nconsumer(consumerId : int!): Consumer\n}\ntype Order {\norderId: ID,\nconsumerId: Int,\nconsumer: Consumer\nrestaurant: Restaurant\ndeliveryInfo : DeliveryInfo\n...\nconst resolvers = {\nQuery:{\norders: resolveOrders,\norder: resolveOrder,\n...\n},\nOrder:{\nconsumer: resolveOrderConsumer,\nrestaurant: resolveOrderRestaurant,\ndeliveryInfo: resolveOrderDeliveryInfo\n},\n...\nfunction resolveOrder(_. {orderId}, context){\nreturn context.orderServiceProxy.ﬁndOrder(orderI\nd);\n}\nfunction resolveOrderDeliveryInfo({orderId}, args,\ncontext) {\nreturn context.deliveryServiceProxy.ﬁndDeliveryF\norOrder(orderId);\n}\nApollo graphQL engine\nConsumerServiceProxy\nOrderServiceProxy\nRestaurantServiceProxy\nDeliveryServiceProxy\nConsumer Service\ninvokes\ninvokes\ninvokes\ninvokes\nOrder Service\nRestaurant Service\nDelivery Service\nExpress web framework\nApollo\ngraphQL\nclient\nFTGO API gateway\nhttp://.../graphql?query={orders(consumerId:1){orde\nrId,restaurant{id}}}\nFigure 8.10\nThe design of the GraphQL-based FTGO API Gateway\n \n\n\n283\nImplementing an API gateway\nprimary way of defining the data model, and enums, which are similar to Java enums.\nAn object type has a name and a collection of typed, named fields. A field can be a sca-\nlar type, such as a number, string, or enum; a list of scalar types; a reference to another\nobject type; or a collection of references to another object type. Despite resembling a\nfield of a traditional object-oriented class, a GraphQL field is conceptually a function\nthat returns a value. It can have arguments, which enable a GraphQL client to tailor\nthe data the function returns.\n GraphQL also uses fields to define the queries supported by the schema. You\ndefine the schema’s queries by declaring an object type, which by convention is called\nQuery. Each field of the Query object is a named query, which has an optional set of\nparameters, and a return type. I found this way of defining queries a little confusing\nwhen I first encountered it, but it helps to keep in mind that a GraphQL field is a\nfunction. It will become even clearer when we look at how fields are connected to the\nbackend data sources.\n The following listing shows part of the schema for the GraphQL-based FTGO API\ngateway. It defines several object types. Most of the object types correspond to the\nFTGO application’s Consumer, Order, and Restaurant entities. It also has a Query object\ntype that defines the schema’s queries.\ntype Query {\n   \norders(consumerId : Int!): [Order]\norder(orderId : Int!): Order\nconsumer(consumerId : Int!): Consumer\n}\ntype Consumer {\nid: ID\n  \nfirstName: String\nlastName: String\norders: [Order]\n     \n}\ntype Order {\norderId: ID,\nconsumerId : Int,\nconsumer: Consumer\nrestaurant: Restaurant\ndeliveryInfo : DeliveryInfo\n...\n}\ntype Restaurant {\nid: ID\nname: String\n...\n}\nListing 8.7\nThe GraphQL schema for the FTGO API gateway\nDefines the queries \nthat a client can \nexecute\nThe unique ID \nfor a Consumer\nA consumer has \na list of orders.\n \n\n\n284\nCHAPTER 8\nExternal API patterns\ntype DeliveryInfo {\nstatus : DeliveryStatus\nestimatedDeliveryTime : Int\nassignedCourier :String\n}\nenum DeliveryStatus {\nPREPARING\nREADY_FOR_PICKUP\nPICKED_UP\nDELIVERED\n}\nDespite having a different syntax, the Consumer, Order, Restaurant, and Delivery-\nInfo object types are structurally similar to the corresponding Java classes. One differ-\nence is the ID type, which represents a unique identifier.\n This schema defines three queries:\n\norders()—Returns the Orders for the specified Consumer\n\norder()—Returns the specified Order\n\nconsumer()—Returns the specified Consumer\nThese queries may seem not different from the equivalent REST endpoints, but\nGraphQL gives the client tremendous control over the data that’s returned. To under-\nstand why, let’s look at how a client executes GraphQL queries. \nEXECUTING GRAPHQL QUERIES\nThe principal benefit of using GraphQL is that its query language gives the client\nincredible control over the returned data. A client executes a query by making a\nrequest containing a query document to the server. In the simple case, a query docu-\nment specifies the name of the query, the argument values, and the fields of the result\nobject to return. Here’s a simple query that retrieves firstName and lastName of the\nconsumer with a particular ID:\nquery {\nconsumer(consumerId:1)\n  \n{\n  \nfirstName\nlastName\n}\n}\nThis query returns those fields of the specified Consumer.\n Here’s a more elaborate query that returns a consumer, their orders, and the ID\nand name of each order’s restaurant:\nquery {\nconsumer(consumerId:1)\n{\nid\nfirstName\nlastName\nSpecifies the query called consumer, \nwhich fetches a consumer\nThe fields of the \nConsumer to return\n \n\n\n285\nImplementing an API gateway\norders {\norderId\nrestaurant {\nid\nname\n}\ndeliveryInfo {\nestimatedDeliveryTime\nname\n}\n}\n}\n}\nThis query tells the server to return more than just the fields of the Consumer. It\nretrieves the consumer’s Orders and each Order’s restaurant. As you can see, a\nGraphQL client can specify exactly the data to return, including the fields of transi-\ntively related objects.\n The query language is more flexible than it might first appear. That’s because a\nquery is a field of the Query object, and a query document specifies which of those fields\nthe server should return. These simple examples retrieve a single field, but a query doc-\nument can execute multiple queries by specifying multiple fields. For each field, the\nquery document supplies the field’s arguments and specifies what fields of the result\nobject it’s interested in. Here’s a query that retrieves two different consumers:\nquery {\nc1: consumer (consumerId:1)\n{ id, firstName, lastName}\nc2: consumer (consumerId:2)\n{ id, firstName, lastName}\n}\nIn this query document, c1 and c2 are what GraphQL calls aliases. They’re used to dis-\ntinguish between the two Consumers in the result, which would otherwise both be\ncalled consumer. This example retrieves two objects of the same type, but a client\ncould retrieve several objects of different types.\n A GraphQL schema defines the shape of the data and the supported queries. To\nbe useful, it has to be connected to the source of the data. Let’s look at how to do that. \nCONNECTING THE SCHEMA TO THE DATA\nWhen the GraphQL server executes a query, it must retrieve the requested data from\none or more data stores. In the case of the FTGO application, the GraphQL server\nmust invoke the APIs of the services that own the data. You associate a GraphQL\nschema with the data sources by attaching resolver functions to the fields of the object\ntypes defined by the schema. The GraphQL server implements the API composition\npattern by invoking resolver functions to retrieve the data, first for the top-level query,\nand then recursively for the fields of the result object or objects.\n The details of how resolver functions are associated with the schema depend on\nwhich GraphQL server you are using. Listing 8.8 shows how to define the resolvers\n \n\n\n286\nCHAPTER 8\nExternal API patterns\nwhen using the Apollo GraphQL server. You create a doubly nested JavaScript object.\nEach top-level property corresponds to an object type, such as Query and Order. Each\nsecond-level property, such as Order.consumer, defines a field’s resolver function.\nconst resolvers = {\nQuery: {\norders: resolveOrders,\nconsumer: resolveConsumer,\norder: resolveOrder\n},\nOrder: {\nconsumer: resolveOrderConsumer,\n  \nrestaurant: resolveOrderRestaurant,\ndeliveryInfo: resolveOrderDeliveryInfo\n...\n};\nA resolver function has three parameters:\nObject—For a top-level query field, such as resolveOrders, object is a root\nobject that’s usually ignored by the resolver function. Otherwise, object is the\nvalue returned by the resolver for the parent object. For example, the resolver\nfunction for the Order.consumer field is passed the value returned by the Order’s\nresolver function.\nQuery arguments—These are supplied by the query document.\nContext—Global state of the query execution that’s accessible by all resolvers. It’s\nused, for example, to pass user information and dependencies to the resolvers.\nA resolver function might invoke a single service or it might implement the API com-\nposition pattern and retrieve data from multiple services. An Apollo GraphQL server\nresolver function returns a Promise, which is JavaScript’s version of Java’s Completable-\nFuture. The promise contains the object (or a list of objects) that the resolver func-\ntion retrieved from the data store. GraphQL engine includes the return value in the\nresult object.\n Let’s look at a couple of examples. Here’s the resolveOrders() function, which is\nthe resolver for the orders query:\nfunction resolveOrders(_, { consumerId }, context) {\nreturn context.orderServiceProxy.findOrders(consumerId);\n}\nThis function obtains the OrderServiceProxy from the context and invokes it to\nfetch a consumer’s orders. It ignores its first parameter. It passes the consumerId argu-\nment, provided by the query document, to OrderServiceProxy.findOrders(). The\nfindOrders() method retrieves the consumer’s orders from OrderHistoryService.\nListing 8.8\nAttaching the resolver functions to fields of the GraphQL schema\nThe resolver for \nthe orders query\nThe resolver for \nthe consumer field \nof an Order\n \n\n\n287\nImplementing an API gateway\n Here’s the resolveOrderRestaurant() function, which is the resolver for the\nOrder.restaurant field that retrieves an order’s restaurant:\nfunction resolveOrderRestaurant({restaurantId}, args, context) {\nreturn context.restaurantServiceProxy.findRestaurant(restaurantId);\n}\nIts first parameter is Order. It invokes RestaurantServiceProxy.findRestaurant()\nwith the Order’s restaurantId, which was provided by resolveOrders().\n GraphQL uses a recursive algorithm to execute the resolver functions. First, it exe-\ncutes the resolver function for the top-level query specified by the Query document.\nNext, for each object returned by the query, it iterates through the fields specified in\nthe Query document. If a field has a resolver, it invokes the resolver with the object\nand the arguments from the Query document. It then recurses on the object or\nobjects returned by that resolver.\n Figure 8.11 shows how this algorithm executes the query that retrieves a consumer’s\norders and each order’s delivery information and restaurant. First, the GraphQL engine\ninvokes resolveConsumer(), which retrieves Consumer. Next, it invokes resolve-\nConsumerOrders(), which is the resolver for the Consumer.orders field that returns\nthe consumer’s orders. The GraphQL engine then iterates through Orders, invok-\ning the resolvers for the Order.restaurant and Order.deliveryInfo fields.\nThe result of executing the resolvers is a Consumer object populated with data retrieved\nfrom multiple services.\n Let’s now look at how to optimize the executing of resolvers by using batching and\ncaching. \nResolver functions\nSchema\nQuery document\ntype Query{\nconsumer(consumerId:int!): Consumer\n}\ntype Order {\n...\nrestaurant: Restaurant\ndeliveryInfo : DeliveryInfo\n...\nquery{\nconsumer(consumerId:1){\nid\nﬁrstName\nlastName\norders{\norderId\nrestaurant{\nid\nname\n}\ndeliveryInfo{\nestimatedDeliveryTime\nname\n}\n}\n}\n}\nconsumer = resolveConsumer(..., 1)\norders = resolveConsumerOrders(consumer)\nresolveOrderRestaurant(order, ...)\nresolveOrderDeliveryInfo(order)\nQuery arguments passed to resolver\nFigure 8.11\nGraphQL executes a query by recursively invoking the resolver functions for the fields specified in \nthe Query document. First, it executes the resolver for the query, and then it recursively invokes the resolvers for \nthe fields in the result object hierarchy.\n \n\n\n288\nCHAPTER 8\nExternal API patterns\nOPTIMIZING LOADING USING BATCHING AND CACHING\nGraphQL can potentially execute a large number of resolvers when executing a query.\nBecause the GraphQL server executes each resolver independently, there’s a risk of\npoor performance due to excessive round-trips to the services. Consider, for example,\na query that retrieves a consumer, their orders, and the orders’ restaurants. If there\nare N orders, then a simplistic implementation would make one call to Consumer\nService, one call to Order History Service, and then N calls to Restaurant Service.\nEven though the GraphQL engine will typically make the calls to Restaurant Service\nin parallel, there’s a risk of poor performance. Fortunately, you can use a few tech-\nniques to improve performance.\n One important optimization is to use a combination of server-side batching and\ncaching. Batching turns N calls to a service, such as Restaurant Service, into a sin-\ngle call that retrieves a batch of N objects. Caching reuses the result of a previous\nfetch of the same object to avoid making an unnecessary duplicate call. The combi-\nnation of batching and caching significantly reduces the number of round-trips to\nbackend services.\n A NodeJS-based GraphQL server can use the DataLoader module to implement\nbatching and caching (https://github.com/facebook/dataloader). It coalesces loads\nthat occur within a single execution of the event loop and calls a batch loading func-\ntion that you provide. It also caches calls to eliminate duplicate loads. The following list-\ning shows how RestaurantServiceProxy can use DataLoader. The findRestaurant()\nmethod loads a Restaurant via DataLoader.\nconst DataLoader = require('dataloader');\nclass RestaurantServiceProxy {\nconstructor() {\nthis.dataLoader =\n      \nnew DataLoader(restaurantIds =>\nthis.batchFindRestaurants(restaurantIds));\n}\nfindRestaurant(restaurantId) {\n         \nreturn this.dataLoader.load(restaurantId);\n}\nbatchFindRestaurants(restaurantIds) {\n     \n...\n}\n}\nRestaurantServiceProxy and, hence, DataLoader are created for each request, so\nthere’s no possibility of DataLoader mixing together different users’ data.\n Let’s now look at how to integrate the GraphQL engine with a web framework so\nthat it can be invoked by clients. \nListing 8.9\nUsing a DataLoader to optimize calls to Restaurant Service\nCreate a DataLoader, which uses \nbatchFindRestaurants() as the \nbatch loading functions.\nLoad the specified Restaurant \nvia the DataLoader.\nLoad a batch of \nRestaurants.\n \n\n\n289\nImplementing an API gateway\nINTEGRATING THE APOLLO GRAPHQL SERVER WITH EXPRESS\nThe Apollo GraphQL server executes GraphQL queries. In order for clients to invoke\nit, you need to integrate it with a web framework. Apollo GraphQL server supports\nseveral web frameworks, including Express, a popular NodeJS web framework.\n Listing 8.10 shows how to use the Apollo GraphQL server in an Express applica-\ntion. The key function is graphqlExpress, which is provided by the apollo-server-\nexpress module. It builds an Express request handler that executes GraphQL queries\nagainst a schema. This example configures Express to route requests to the GET\n/graphql and POST /graphql endpoints of this GraphQL request handler. It also creates\na GraphQL context containing the proxies, which makes them available to the resolvers.\nconst {graphqlExpress} = require(\"apollo-server-express\");\nconst typeDefs = gql`\n   \ntype Query {\norders: resolveOrders,\n...\n}\ntype Consumer {\n...\nconst resolvers = {     \nQuery: {\n...\n}\n}\nconst schema = makeExecutableSchema({ typeDefs, resolvers });    \nconst app = express();\nfunction makeContextWithDependencies(req) {      \nconst orderServiceProxy = new OrderServiceProxy();\nconst consumerServiceProxy = new ConsumerServiceProxy();\nconst restaurantServiceProxy = new RestaurantServiceProxy();\n...\nreturn {orderServiceProxy, consumerServiceProxy,\nrestaurantServiceProxy, ...};\n}\nfunction makeGraphQLHandler() {\n   \nreturn graphqlExpress(req => {\nreturn {schema: schema, context: makeContextWithDependencies(req)}\n});\n}\napp.post('/graphql', bodyParser.json(), makeGraphQLHandler());\n  \napp.get('/graphql', makeGraphQLHandler());\napp.listen(PORT);\nListing 8.10\nIntegrating the GraphQL server with the Express web framework\nDefine the GraphQL \nschema.\nDefine the \nresolvers.\nCombine the \nschema with the \nresolvers to create \nan executable \nschema.\nInject repositories into \nthe context so they’re \navailable to resolvers.\nMake an express request handler \nthat executes GraphQL queries \nagainst the executable schema.\nRoute POST /graphql and GET\n/graphql endpoints to the\nGraphQL server.\n \n\n\n290\nCHAPTER 8\nExternal API patterns\nThis example doesn’t handle concerns such as security, but those would be straight-\nforward to implement. The API gateway could, for example, authenticate users using\nPassport, a NodeJS security framework described in chapter 11. The makeContext-\nWithDependencies() function would pass the user information to each repository’s\nconstructor so that they can propagate the user information to the services.\n Let’s now look at how a client can invoke this server to execute GraphQL queries. \nWRITING A GRAPHQL CLIENT\nThere are a couple of different ways a client application can invoke the GraphQL\nserver. Because the GraphQL server has an HTTP-based API, a client application\ncould use an HTTP library to make requests, such as GET http://localhost:3000/\ngraphql?query={orders(consumerId:1){orderId,restaurant{id}}}'. It’s easier,\nthough, to use a GraphQL client library, which takes care of properly formatting\nrequests and typically provides features such as client-side caching.\n The following listing shows the FtgoGraphQLClient class, which is a simple\nGraphQL-based client for the FTGO application. Its constructor instantiates Apollo-\nClient, which is provided by the Apollo GraphQL client library. The FtgoGraphQL-\nClient class defines a findConsumer() method that uses the client to retrieve the\nname of a consumer.\nclass FtgoGraphQLClient {\nconstructor(...) {\nthis.client = new ApolloClient({ ... });\n}\nfindConsumer(consumerId) {\nreturn this.client.query({\nvariables: { cid: consumerId},\n  \nquery: gql`\nquery foo($cid : Int!) {\n  \nconsumer(consumerId: $cid)\n{\n  \nid\nfirstName\nlastName\n}\n} `,\n})\n}\n}\nThe FtgoGraphQLClient class can define a variety of query methods, such as find-\nConsumer(). Each one executes a query that retrieves exactly the data needed by the\nclient.\nListing 8.11\nUsing the Apollo GraphQL client to execute queries\nSupply the value \nof the $cid.\nDefine $cid as a \nvariable of type Int.\nSet the value of \nquery parameter \nconsumerid to $cid.\n \n\n\n291\nSummary\n This section has barely scratched the surface of GraphQL’s capabilities. I hope I’ve\ndemonstrated that GraphQL is a very appealing alternative to a more traditional,\nREST-based API gateway. It lets you implement an API that’s flexible enough to sup-\nport a diverse set of clients. Consequently, you should consider using GraphQL to\nimplement your API gateway. \nSummary\nYour application’s external clients usually access the application’s services via an\nAPI gateway. An API gateway provides each client with a custom API. It’s respon-\nsible for request routing, API composition, protocol translation, and implemen-\ntation of edge functions such as authentication.\nYour application can have a single API gateway or it can use the Backends for\nfrontends pattern, which defines an API gateway for each type of client. The\nmain advantage of the Backends for frontends pattern is that it gives the client\nteams greater autonomy, because they develop, deploy, and operate their own\nAPI gateway.\nThere are numerous technologies you can use to implement an API gateway,\nincluding off-the-shelf API gateway products. Alternatively, you can develop\nyour own API gateway using a framework.\nSpring Cloud Gateway is a good, easy-to-use framework for developing an API\ngateway. It routes requests using any request attribute, including the method\nand the path. Spring Cloud Gateway can route a request either directly to a\nbackend service or to a custom handler method. It’s built using the scalable,\nreactive Spring Framework 5 and Project Reactor frameworks. You can write\nyour custom request handlers in a reactive style using, for example, Project\nReactor’s Mono abstraction.\nGraphQL, a framework that provides graph-based query language, is another\nexcellent foundation for developing an API Gateway. You write a graph-oriented\nschema to describe the server-side data model and its supported queries. You\nthen map that schema to your services by writing resolvers, which retrieve data.\nGraphQL-based clients execute queries against the schema that specify exactly\nthe data that the server should return. As a result, a GraphQL-based API gate-\nway can support diverse clients. \n \n\n\n292\nTesting microservices:\nPart 1\nFTGO, like many organizations, had adopted a traditional approach to testing. Test-\ning is primarily an activity that happens after development. The FTGO developers\nthrow their code over a wall to the QA team, who verify that the software works as\nexpected. What’s more, most of their testing is done manually. Sadly, this approach\nto testing is broken—for two reasons:\nManual testing is extremely inefficient—You should never ask a human to do\nwhat a machine can do better. Compared to machines, humans are slow and\ncan’t work 24/7. You won’t be able to deliver software rapidly and safely if\nyou rely on manual testing. It’s essential that you write automated tests.\nTesting is done far too late in the delivery process—There certainly is a role for tests\nthat critique an application after it’s been written, but experience has shown\nthat those tests are insufficient. A much better approach is for developers to\nThis chapter covers\nEffective testing strategies for microservices\nUsing mocks and stubs to test a software \nelement in isolation\nUsing the test pyramid to determine where to \nfocus testing efforts\nUnit testing the classes inside a service\n \n",
      "page_number": 307
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 318-329)",
      "start_page": 318,
      "end_page": 329,
      "detection_method": "topic_boundary",
      "content": "293\nwrite automated tests as part of development. It improves their productivity\nbecause, for example, they’ll have tests that provide immediate feedback while\nediting code.\nIn this regard, FTGO is a fairly typical organization. The Sauce Labs Testing Trends in\n2018 report paints a fairly gloomy picture of the state of test automation (https://\nsaucelabs.com/resources/white-papers/testing-trends-for-2018). It describes how only\n26% of organizations are mostly automated, and a minuscule 3% are fully automated!\n The reliance on manual testing isn’t because of a lack of tooling and frameworks.\nFor example, JUnit, a popular Java testing framework, was first released in 1998. The\nreason for the lack of automated tests is mostly cultural: “Testing is QA’s job,” “It’s not\nthe best use of a developers’s time,” and so on. It also doesn’t help that developing a\nfast-running, yet effective, maintainable test suite is challenging. And, a typical large,\nmonolithic application is extremely difficult to test.\n One key motivation for using the microservice architecture is, as described in\nchapter 2, improving testability. Yet at the same time, the complexity of the microser-\nvice architecture demands that you write automated tests. Furthermore, some aspects\nof testing microservices are challenging. That’s because we need to verify that services\ncan interact correctly while minimizing the number of slow, complex, and unreliable\nend-to-end-tests that launch many services.\n This chapter is the first of two chapters on testing. It’s an introduction to testing.\nChapter 10 covers more advanced testing concepts. The two chapters are long, but\ntogether they cover testing ideas and techniques that are essential to modern software\ndevelopment in general, and to the microservice architecture in particular.\n I begin this chapter by describing effective testing strategies for a microservices-\nbased application. These strategies enable you to be confident that your software\nworks, while minimizing test complexity and execution time. After that, I describe\nhow to write one particular kind of test for your services: unit tests. Chapter 10 covers\nthe other kinds of tests: integration, component, and end-to-end.\n Let’s start by taking a look at testing strategies for microservices.\nWhy an introduction to testing?\nYou may be wondering why this chapter includes an introduction to basic testing con-\ncepts. If you’re already familiar with concepts such as the test pyramid and the different\ntypes of tests, feel free to speed-read this chapter and move onto the next one, which\nfocuses on microservices-specific testing topics. But based on my experiences consult-\ning for and training clients all over the world, a fundamental weakness of many software\ndevelopment organizations is the lack of automated testing. That’s because if you want\nto deliver software quickly and reliably, it’s absolutely essential to do automated testing.\nIt’s the only way to have a short lead time, which is the time it takes to get committed\ncode into production. Perhaps even more importantly, automated testing is essential\nbecause it forces you to develop a testable application. It’s typically very difficult to\nintroduce automating testing into an already large, complex application. In other words,\nthe fast track to monolithic hell is to not write automated tests.\n \n\n\n294\nCHAPTER 9\nTesting microservices: Part 1\n9.1\nTesting strategies for microservice architectures\nLet’s say you’ve made a change to FTGO application’s Order Service. Naturally, the\nnext step is for you to run your code and verify that the change works correctly. One\noption is to test the change manually. First, you run Order Service and all its depen-\ndencies, which include infrastructure services such as a database and other applica-\ntion services. Then you “test” the service by either invoking its API or using the FTGO\napplication’s UI. The downside of this approach is that it’s a slow, manual way to test\nyour code.\n A much better option is to have automated tests that you can run during develop-\nment. Your development workflow should be: edit code, run tests (ideally with a single\nkeystroke), repeat. The fast-running tests quickly tell you whether your changes work\nwithin a few seconds. But how do you write fast-running tests? And are they sufficient\nor do you need more comprehensive tests? These are the kind of questions I answer in\nthis and other sections in this chapter.\n I start this section with an overview of important automated testing concepts. We’ll\nlook at the purpose of testing and the structure of a typical test. I cover the different\ntypes of tests that you’ll need to write. I also describe the test pyramid, which provides\nvaluable guidance about where you should focus your testing efforts. After covering\ntesting concepts, I discuss strategies for testing microservices. We’ll look at the distinct\nchallenges of testing applications that have a microservice architecture. I describe\ntechniques you can use to write simpler and faster, yet still-effective, tests for your\nmicroservices.\n Let’s take a look at testing concepts.\n9.1.1\nOverview of testing\nIn this chapter, my focus is on automated testing, and I use the term test as shorthand\nfor automated test. Wikipedia defines a test case, or test, as follows:\nA test case is a set of test inputs, execution conditions, and expected results developed for\na particular objective, such as to exercise a particular program path or to verify compliance\nwith a specific requirement.\nhttps://en.wikipedia.org/wiki/Test_case\nIn other words, the purpose of a test is, as figure 9.1 shows, to verify the behavior of\nthe System Under Test (SUT). In this definition, system is a fancy term that means the\nsoftware element being tested. It might be something as small as a class, as large as the\nentire application, or something in between, such as a cluster of classes or an individ-\nual service. A collection of related tests form a test suite.\n Let’s first look at the concept of an automated test. Then I discuss the different\nkinds of tests that you’ll need to write. After that, I discuss the test pyramid, which\ndescribes the relative proportions of the different types of tests that you should write.\n \n\n\n295\nTesting strategies for microservice architectures\nWRITING AUTOMATED TESTS\nAutomated tests are usually written using a testing framework. JUnit, for example, is a\npopular Java testing framework. Figure 9.2 shows the structure of an automated test.\nEach test is implemented by a test method, which belongs to a test class.\nAn automated test typically consists of four phases (http://xunitpatterns.com/\nFour%20Phase%20Test.html):\n1\nSetup—Initialize the test fixture, which consists of the SUT and its dependen-\ncies, to the desired initial state. For example, create the class under test and ini-\ntialize it to the state required for it to exhibit the desired behavior.\n2\nExercise—Invoke the SUT—for example, invoke a method on the class under test.\n3\nVerify—Make assertions about the invocation’s outcome and the state of the\nSUT. For example, verify the method’s return value and the new state of the class\nunder test.\nSystem Under\nTest (SUT)\nTest\nTest suite\nVeriﬁes behavior of\nFigure 9.1\nThe goal of a test is to \nverify the behavior of the system \nunder test. An SUT might be as \nsmall as a class or as large as an \nentire application.\nTest class\nSetup\nExecute\nVerify\nTeardown\nTest method\nTest method\nTest runner\nSUT\nFixture\nExecutes\nTest method\nFigure 9.2\nEach automated test is implemented by a test method, which belongs to a test class. A \ntest consists of four phases: setup, which initializes the test fixture, which is everything required to \nrun the test; execute, which invokes the SUT; verify, which verifies the outcome of the test; and \nteardown, which cleans up the test fixture.\n \n\n\n296\nCHAPTER 9\nTesting microservices: Part 1\n4\nTeardown—Clean up the test fixture, if necessary. Many tests omit this phase,\nbut some types of database test will, for example, roll back a transaction initi-\nated by the setup phase.\nIn order to reduce code duplication and simplify tests, a test class might have setup\nmethods that are run before a test method, and teardown methods that are run after-\nwards. A test suite is a set of test classes. The tests are executed by a test runner. \nTESTING USING MOCKS AND STUBS\nAn SUT often has dependencies. The trouble with dependencies is that they can com-\nplicate and slow down tests. For example, the OrderController class invokes Order-\nService, which ultimately depends on numerous other application services and\ninfrastructure services. It wouldn’t be practical to test the OrderController class by\nrunning a large portion of the system. We need a way to test an SUT in isolation.\n The solution, as figure 9.3 shows, is to replace the SUT’s dependencies with test\ndoubles. A test double is an object that simulates the behavior of the dependency.\nThere are two types of test doubles: stubs and mocks. The terms stubs  and mocks  are\noften used interchangeably, although they have slightly different behavior. A stub is a\ntest double that returns values to the SUT. A mock is a test double that a test uses to ver-\nify that the SUT correctly invokes a dependency. Also, a mock is often a stub.\n Later on in this chapter, you’ll see examples of test doubles in action. For example,\nsection 9.2.5 shows how to test the OrderController class in isolation by using a test\ndouble for the OrderService class. In that example, the OrderService test double is\nimplemented using Mockito, a popular mock object framework for Java. Chapter 10\nshows how to test Order Service using test doubles for the other services that it invokes.\nThose test doubles respond to command messages sent by Order Service.\n Let’s now look at the different types of tests. \nSlow, complex\ntest\nTests\nReplaced with\nTests\nSystem Under\nTest (SUT)\nDependency\nFaster, simpler\ntest\nSystem Under\nTest (SUT)\nTest double\nFigure 9.3\nReplacing a dependency with a test double enables the SUT to \nbe tested in isolation. The test is simpler and faster.\n \n\n\n297\nTesting strategies for microservice architectures\nTHE DIFFERENT TYPES OF TESTS\nThere are many different types of tests. Some tests, such as performance tests and\nusability tests, verify that the application satisfies its quality of service requirements. In\nthis chapter, I focus on automated tests that verify the functional aspects of the appli-\ncation or service. I describe how to write four different types of tests:\nUnit tests—Test a small part of a service, such as a class.\nIntegration tests—Verify that a service can interact with infrastructure services\nsuch as databases and other application services.\nComponent tests—Acceptance tests for an individual service.\nEnd-to-end tests—Acceptance tests for the entire application.\nThey differ primarily in scope. At one end of the spectrum are unit tests, which verify\nbehavior of the smallest meaningful program element. For an object-oriented lan-\nguage such as Java, that’s a class. At the other end of the spectrum are end-to-end\ntests, which verify the behavior of an entire application. In the middle are component\ntests, which test individual services. Integration tests, as you’ll see in the next chapter,\nhave a relatively small scope, but they’re more complex than pure unit tests. Scope is\nonly one way of characterizing tests. Another way is to use the test quadrant.\nUSING THE TEST QUADRANT TO CATEGORIZE TESTS\nA good way to categorize tests is Brian Marick’s test quadrant (www.exampler.com/old-\nblog/2003/08/21/#agile-testing-project-1). The test quadrant, shown in figure 9.4,\ncategorizes tests along two dimensions:\nWhether the test is business facing or technology facing—A business-facing test is\ndescribed using the terminology of a domain expert, whereas a technology-facing\ntest is described using the terminology of developers and the implementation.\nWhether the goal of the test is to support programming or critique the application—Devel-\nopers use tests that support programming as part of their daily work. Tests that\ncritique the application aim to identify areas that need improvement.\nCompile-time unit tests\nTesting is an integral part of development. The modern development workflow is to\nedit code, then run tests. Moreover, if you’re a Test-Driven Development (TDD) prac-\ntitioner, you develop a new feature or fix a bug by first writing a failing test and then\nwriting the code to make it pass. Even if you’re not a TDD adherent, an excellent way\nto fix a bug is to write a test that reproduces the bug and then write the code that\nfixes it.\nThe tests that you run as part of this workflow are known as compile-time tests. In a\nmodern IDE, such as IntelliJ IDEA or Eclipse, you typically don’t compile your code as\na separate step. Rather, you use a single keystroke to compile the code and run the\ntests. In order to stay in the flow, these tests need to execute quickly—ideally, no\nmore than a few seconds. \n \n\n\n298\nCHAPTER 9\nTesting microservices: Part 1\nThe test quadrant defines four different categories of tests:\nQ1—Support programming/technology facing: unit and integration tests\nQ2—Support programming/business facing: component and end-to-end test\nQ3—Critique application/business facing: usability and exploratory testing\nQ4—Critique application/technology facing: nonfunctional acceptance tests such\nas performance tests\nThe test quadrant isn’t the only way of organizing tests. There’s also the test pyramid,\nwhich provides guidance on how many tests of each type to write. \nUSING THE TEST PYRAMID AS A GUIDE TO FOCUSING YOUR TESTING EFFORTS\nWe must write different kinds of tests in order to be confident that our application\nworks. The challenge, though, is that the execution time and complexity of a test\nincrease with its scope. Also, the larger the scope of a test and the more moving parts\nit has, the less reliable it becomes. Unreliable tests are almost as bad as no tests,\nbecause if you can’t trust a test, you’re likely to ignore failures.\n On one end of the spectrum are unit tests for individual classes. They’re fast to\nexecute, easy to write, and reliable. At the other end of the spectrum are end-to-end\ntests for the entire application. These tend to be slow, difficult to write, and often\nunreliable because of their complexity. Because we don’t have unlimited budget for\ndevelopment and testing, we want to focus on writing tests that have small scope with-\nout compromising the effectiveness of the test suite.\n The test pyramid, shown in figure 9.5, is a good guide (https://martinfowler.com/\nbliki/TestPyramid.html). At the base of the pyramid are the fast, simple, and reliable\nunit tests. At the top of the pyramid are the slow, complex, and brittle end-to-end tests.\nLike the USDA food pyramid, although more useful and less controversial (https://en\n.wikipedia.org/wiki/History_of_USDA_nutrition_guides), the test pyramid describes\nthe relative proportions of each type of test.\n The key idea of the test pyramid is that as we move up the pyramid we should write\nfewer and fewer tests. We should write lots of unit tests and very few end-to-end tests.\nQ2 AUTOMATED\nQ3 MANUAL\nBusiness facing\nTechnology facing\nSupport programming\nCritique project\nQ1 AUTOMATED\nQ4 MANUAL/\nAUTOMATED\nFunctional/\nacceptance tests\nExploratory\ntesting, usability\ntesting\nUnit,\nintegration,\ncomponent\nNon-functional\nacceptance tests:\nperformance\nand more\nFigure 9.4\nThe test quadrant categorizes tests along \ntwo dimensions. The first dimension is whether a test \nis business facing or technology facing. The second is \nwhether the purpose of the test is to support \nprogramming or critique the application.\n \n\n\n299\nTesting strategies for microservice architectures\nAs you’ll see in this chapter, I describe a strategy that emphasizes testing the pieces of\na service. It even minimizes the number of component tests, which test an entire service.\n It’s clear how to test individual microservices such as Consumer Service, which\ndon’t depend on any other services. But what about services such as Order Service,\nthat do depend on numerous other services? And how can we be confident that the\napplication as a whole works? This is the key challenge of testing applications that\nhave a microservice architecture. The complexity of testing has moved from the\nindividual services to the interactions between them. Let’s look at how to tackle this\nproblem. \n9.1.2\nThe challenge of testing microservices\nInterprocess communication plays a much more important role in a microservices-\nbased application than in a monolithic application. A monolithic application might\ncommunicate with a few external clients and services. For example, the monolithic\nversion of the FTGO application uses a few third-party web services, such as Stripe\nfor payments, Twilio for messaging, and Amazon SES for email, which have stable\nAPIs. Any interaction between the modules of the application is through program-\nming language-based APIs. Interprocess communication is very much on the edge\nof the application.\n In contrast, interprocess communication is central to microservice architecture. A\nmicroservices-based application is a distributed system. Teams are constantly develop-\ning their services and evolving their APIs. It’s essential that developers of a service\nwrite tests that verify that their service interacts with its dependencies and clients.\n As described in chapter 3, services communicate with each other using a variety\nof interaction styles and IPC mechanisms. Some services use request/response-style\ninteraction that’s implemented using a synchronous protocol, such as REST or gRPC.\nEnd-to-end\nSlow, brittle, costly\nFast, reliable, cheap\nComponent\nIntegration\nUnit\nAcceptance tests for\nan application\nAcceptance tests\nfor a service\nVerify that a service\ncommunicates with\nits dependencies\nTest the business logic\nFigure 9.5\nThe test pyramid describes the relative proportions of each type of test that \nyou need to write. As you move up the pyramid, you should write fewer and fewer tests.\n \n\n\n300\nCHAPTER 9\nTesting microservices: Part 1\nOther services interact through request/asynchronous reply or publish/subscribe\nusing asynchronous messaging. For instance, figure 9.6 shows how some of the ser-\nvices in the FTGO application communicate. Each arrow points from a consumer ser-\nvice to a producer service.\nThe arrow points in the direction of the dependency, from the consumer of the API\nto the provider of the API. The assumptions that a consumer makes about an API\ndepend on the nature of the interaction:\nREST client  service—The API gateway routes requests to services and imple-\nments API composition.\nDomain event consumer  publisher—Order History Service consumes events pub-\nlished by Order Service.\nCommand message requestor  replier—Order Service sends command messages\nto various services and consumes the replies.\nREST client\nREST service\nKey\nSubscriber\nDomain event\npublisher\n(Command message)\nrequestor\nReplier\nE\nE\nE\nE\nE\nC\nC\nC\nC\nRestaurant\nService\nConsumer\nService\nOrder History\nService\nDelivery\nService\nAPI\ngateway\nInvokes services\nusing HTTP\nSubscribes to\norder* events\nOrder Service saga\nsends commands\nto various services.\nAccounting\nService\nOrder\nService\nKitchen\nService\nFigure 9.6\nSome of the interservice communication in the FTGO application. Each arrow points \nfrom a consumer service to a producer service.\n \n\n\n301\nTesting strategies for microservice architectures\nEach interaction between a pair of services represents an agreement or contract\nbetween the two services. Order History Service and Order Service must, for exam-\nple, agree on the event message structure and the channel that they’re published to.\nSimilarly, the API gateway and the services must agree on the REST API endpoints.\nAnd Order Service and each service that it invokes using asynchronous request/\nresponse must agree on the command channel and the format of the command and\nreply messages.\n As a developer of a service, you need to be confident that the services you consume\nhave stable APIs. Similarly, you don’t want to unintentionally make breaking changes\nto your service’s API. For example, if you’re working on Order Service, you want to be\nsure that the developers of your service’s dependencies, such as Consumer Service and\nKitchen Service, don’t change their APIs in ways that are incompatible with your ser-\nvice. Similarly, you must ensure that you don’t change the Order Services’s API in a\nway that breaks the API Gateway or Order History Service.\n One way to verify that two services can interact is to run both services, invoke an API\nthat triggers the communication, and verify that it has the expected outcome. This will\ncertainly catch integration problems, but it’s basically an end-to-end. The test likely\nwould need to run numerous other transitive dependencies of those services. A test\nmight also need to invoke complex, high-level functionality such as business logic, even\nif its goal is to test relatively low-level IPC. It’s best to avoid writing end-to-end tests like\nthese. Somehow, we need to write faster, simpler, and more reliable tests that ideally test\nservices in isolation. The solution is to use what’s known as consumer-driven contract testing.\nCONSUMER-DRIVEN CONTRACT TESTING\nImagine that you’re a member of the team developing API Gateway, described in chap-\nter 8. The API Gateway’s OrderServiceProxy invokes various REST endpoints, includ-\ning the GET /orders/{orderId} endpoint. It’s essential that we write tests that verify that\nAPI Gateway and Order Service agree on an API. In the terminology of consumer con-\ntract testing, the two services participate in a consumer-provider relationship. API Gateway is\na consumer, and Order Service is a provider. A consumer contract test is an integration\ntest for a provider, such as Order Service, that verifies that its API matches the expecta-\ntions of a consumer, such as API Gateway.\n A consumer contract test focuses on verifying that the “shape” of a provider’s API\nmeets the consumer’s expectations. For a REST endpoint, a contract test verifies that\nthe provider implements an endpoint that\nHas the expected HTTP method and path\nAccepts the expected headers, if any\nAccepts a request body, if any\nReturns a response with the expected status code, headers, and body\nIt’s important to remember that contract tests don’t thoroughly test the provider’s\nbusiness logic. That’s the job of unit tests. Later on, you’ll see that consumer contract\ntests for a REST API are in fact mock controller tests.\n \n\n\n302\nCHAPTER 9\nTesting microservices: Part 1\n The team that develops the consumer writes a contract test suite and adds it (for\nexample, via a pull request) to the provider’s test suite. The developers of other ser-\nvices that invoke Order Service also contribute a test suite, as shown in figure 9.7.\nEach test suite will test those aspects of Order Service’s API that are relevant to each\nconsumer. The test suite for Order History Service, for example, verifies that Order\nService publishes the expected events.\nThese test suites are executed by the deployment pipeline for Order Service. If a con-\nsumer contract test fails, that failure tells the producer team that they’ve made a break-\ning change to the API. They must either fix the API or talk to the consumer team.\nConsumer-driven contract tests typically use testing by example. The interaction\nbetween a consumer and provider is defined by a set of examples, known as contracts.\nEach contract consists of example messages that are exchanged during one interaction.\nPattern: Consumer-driven contract test\nVerify that a service meets the expectations of its clients See http://microser-\nvices.io/patterns/testing/service-integration-contract-test.html.\nAPI gateway team\nWrites\nOrder Service deployment pipeline\nOrder\nService\nAPI gateway -\nOrder Service\ncontract test\nsuite\nOrder History Service team\nWrites\nTests\nTests\nTests\nOrder History\nService - Order\nService contract\ntest suite\n... Service team\nWrites\n... Service -\nOrder Service\ncontract test\nsuite\nFigure 9.7\nEach team that develops a service that consumes Order Service’s API contributes \na contract test suite. The test suite verifies that the API matches the consumer’s expectations. \nThis test suite, along with those contributed by other teams, is run by Order Service’s \ndeployment pipeline.\n \n\n\n303\nTesting strategies for microservice architectures\nFor instance, a contract for a REST API consists of an example HTTP request and\nresponse. On the surface, it may seem better to define the interaction using schemas\nwritten using, for example, OpenAPI or JSON schema. But it turns out schemas aren’t\nthat useful when writing tests. A test can validate the response using the schema but it\nstill needs to invoke the provider with an example request.\n What’s more, consumer tests also need example responses. That’s because even\nthough the focus of consumer-driven contract testing is to test a provider, contracts\nare also used to verify that the consumer conforms to the contract. For instance, a\nconsumer-side contract test for a REST client uses the contract to configure an HTTP\nstub service that verifies that the HTTP request matches the contract’s request and\nsends back the contract’s HTTP response. Testing both sides of interaction ensures\nthat the consumer and provider agree on the API. Later on we’ll look at examples of\nhow to write this kind of testing, but first let’s see how to write consumer contract tests\nusing Spring Cloud Contract.\nTESTING SERVICES USING SPRING CLOUD CONTRACT\nTwo popular contract testing frameworks are Spring Cloud Contract (https://cloud\n.spring.io/spring-cloud-contract/), which is a consumer contract testing framework\nfor Spring applications, and the Pact family of frameworks (https://github.com/pact-\nfoundation), which support a variety of languages. The FTGO application is a Spring\nframework-based application, so in this chapter I’m going to describe how to use\nSpring Cloud Contract. It provides a Groovy domain-specific language (DSL) for writ-\ning contracts. Each contract is a concrete example of an interaction between a con-\nsumer and a provider, such as an HTTP request and response. Spring Cloud Contract\ncode generates contract tests for the provider. It also configures mocks, such as a\nmock HTTP server, for consumer integration tests.\n Say, for example, you’re working on API Gateway and want to write a consumer\ncontract test for Order Service. Figure 9.8 shows the process, which requires you to col-\nlaborate with Order Service teams. You write contracts that define how API Gateway\ninteracts with Order Service. The Order Service team uses these contracts to test Order\nService, and you use them to test API Gateway. The sequence of steps is as follows:\n1\nYou write one or more contracts, such as the one shown in listing 9.1. Each con-\ntract consists of an HTTP request that API Gateway might send to Order Service\nand an expected HTTP response. You give the contracts, perhaps via a Git pull\nrequest, to the Order Service team.\n2\nThe Order Service team tests Order Service using consumer contract tests,\nwhich Spring Cloud Contract code generates from contracts.\nPattern: Consumer-side contract test\nVerify that the client of a service can communicate with the service. See https://\nmicroservices.io/patterns/testing/consumer-side-contract-test.html. \n \n\n\n304\nCHAPTER 9\nTesting microservices: Part 1\n3\nThe Order Service team publishes the contracts that tested Order Service to a\nMaven repository.\n4\nYou use the published contracts to write tests for API Gateway.\nBecause you test API Gateway using the published contracts, you can be confident that\nit works with the deployed Order Service.\n The contracts are the key part of this testing strategy. The following listing shows an\nexample Spring Cloud Contract. It consists of an HTTP request and an HTTP response.\norg.springframework.cloud.contract.spec.Contract.make {\nrequest {\n                   \nmethod 'GET'\nurl '/orders/1223232'\n}\nresponse {\n       \nstatus 200\nheaders {\nheader('Content-Type': 'application/json;charset=UTF-8')\n}\nbody(\"{ ... }\")\n}\n}\nListing 9.1\nA contract that describes how API Gateway invokes Order Service\nWrites\nCode generated\nfrom\nContract.make {\nrequest {..}\nresponse {...}\n}\n}\nOrder Service\nconsumer\ncontract tests\nAPI gateway\nAPI gateway team\nReads\nTests\nDevelops\nTests\nDevelops\nPublishes\nPublished\ncontract\nMaven repository\nOrder\nService\nOrder Service team\nAPI gateway\nintegration test\nFigure 9.8\nThe API Gateway team writes the contracts. The Order Service team \nuses those contracts to test Order Service and publishes them to a repository. The \nAPI Gateway team uses the published contracts to test API Gateway.\nThe HTTP request’s \nmethod and path\nThe HTTP response’s status \ncode, headers, and body\n \n",
      "page_number": 318
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 330-340)",
      "start_page": 330,
      "end_page": 340,
      "detection_method": "topic_boundary",
      "content": "305\nTesting strategies for microservice architectures\nThe request element is an HTTP request for the REST endpoint GET /orders/\n{orderId}. The response element is an HTTP response that describes an Order\nexpected by API Gateway. The Groovy contracts are part of the provider’s code base.\nEach consumer team writes contracts that describe how their service interacts with\nthe provider and gives them, perhaps via a Git pull request, to the provider team.\nThe provider team is responsible for packaging the contracts as a JAR and publish-\ning them to a Maven repository. The consumer-side tests download the JAR from the\nrepository.\n Each contract’s request and response play dual roles of test data and the specifi-\ncation of expected behavior. In a consumer-side test, the contract is used to config-\nure a stub, which is similar to a Mockito mock object and simulates the behavior of\nOrder Service. It enables API Gateway to be tested without running Order Service.\nIn the provider-side test, the generated test class invokes the provider with the con-\ntract’s request and verifies that it returns a response that matches the contract’s\nresponse. The next chapter discusses the details of how to use Spring Cloud Con-\ntract, but now we’re going to look at how to use consumer contract testing for mes-\nsaging APIs. \nCONSUMER CONTRACT TESTS FOR MESSAGING APIS\nA REST client isn’t the only kind of consumer that has expectations of a provider’s API.\nServices that subscribe to domain events and use asynchronous request/response-based\ncommunication are also consumers. They consume some other service’s messaging\nAPI, and make assumptions about the nature of that API. We must also write con-\nsumer contract tests for these services.\n Spring Cloud Contract also provides support for testing messaging-based interac-\ntions. The structure of a contract and how it’s used by the tests depend on the type of\ninteraction. A contract for domain event publishing consists of an example domain\nevent. A provider test causes the provider to emit an event and verifies that it matches\nthe contract’s event. A consumer test verifies that the consumer can handle that event.\nIn the next chapter, I describe an example test.\n A contract for an asynchronous request/response interaction is similar to an\nHTTP contract. It consists of a request message and a response message. A provider\ntest invokes the API with the contract’s request message and verifies that the response\nmatches the contract’s response. A consumer test uses the contract to configure a stub\nsubscriber, which listens for the contract’s request message and replies with the speci-\nfied response. The next chapter discusses an example test. But first we’ll take a look at\nthe deployment pipeline, which runs these and other tests. \n9.1.3\nThe deployment pipeline\nEvery service has a deployment pipeline. Jez Humble’s book, Continuous Delivery\n(Addison-Wesley, 2010) describes a deployment pipeline as the automated process of get-\nting code from the developer’s desktop into production. As figure 9.9 shows, it consists\n \n\n\n306\nCHAPTER 9\nTesting microservices: Part 1\nof a series of stages that execute test suites, followed by a stage that releases or deploys\nthe service. Ideally, it’s fully automated, but it might contain manual steps. A deploy-\nment pipeline is often implemented using a Continuous Integration (CI) server, such\nas Jenkins.\nAs code flows through the pipeline, the test suites subject it to increasingly more thor-\nough testing in environments that are more production like. At the same time, the\nexecution time of each test suite typically grows. The idea is to provide feedback about\ntest failures as rapidly as possible.\n The example deployment pipeline shown in figure 9.9 consists of the following\nstages:\nPre-commit tests stage—Runs the unit tests. This is executed by the developer\nbefore committing their changes.\nCommit tests stage—Compiles the service, runs the unit tests, and performs static\ncode analysis.\nIntegration tests stage—Runs the integration tests.\nComponent tests stage—Runs the component tests for the service.\nDeploy stage—Deploys the service into production.\nThe CI server runs the commit stage when a developer commits a change. It executes\nextremely quickly, so it provides rapid feedback about the commit. The later stages\ntake longer to run, providing less immediate feedback. If all the tests pass, the final\nstage is when this pipeline deploys it into production.\n In this example, the deployment pipeline is fully automated all the way from com-\nmit to deployment. There are, however, situations that require manual steps. For\nexample, you might need a manual testing stage, such as a staging environment. In\nsuch a scenario, the code progresses to the next stage when a tester clicks a button to\nindicate that it was successful. Alternatively, a deployment pipeline for an on-premise\nPre-commit\ntests\nSlow feedback\nFast feedback\nProduction\nready\nNot production\nready\nCommit\ntests\nstage\nDeployment pipeline\nIntegration\ntests\nstage\nComponent\ntests\nstage\nProduction\nenvironment\nDeploy\nstage\nFigure 9.9\nAn example deployment pipeline for Order Service. It consists of a series of stages. \nThe pre-commit tests are run by the developer prior to committing their code. The remaining stages \nare executed by an automated tool, such as the Jenkins CI server.\n \n\n\n307\nWriting unit tests for a service\nproduct would release the new version of the service. Later on, the released services\nwould be packaged into a product release and shipped to customers.\n Now that we’ve looked at the organization of the deployment pipeline and when it\nexecutes the different types of tests, let’s head to the bottom of the test pyramid and\nlook at how to write unit tests for a service. \n9.2\nWriting unit tests for a service\nImagine that you want to write a test that verifies that the FTGO application’s Order\nService correctly calculates the subtotal of an Order. You could write tests that run\nOrder Service, invoke its REST API to create an Order, and check that the HTTP\nresponse contains the expected values. The drawback of this approach is that not only\nis the test complex, it’s also slow. If these tests were the compile-time tests for the\nOrder class, you’d waste a lot of time waiting for it to finish. A much more productive\napproach is to write unit tests for the Order class.\n As figure 9.10 shows, unit tests are the lowest level of the test pyramid. They’re\ntechnology-facing tests that support development. A unit test verifies that a unit, which\nis a very small part of a service, works correctly. A unit is typically a class, so the goal of\nunit testing is to verify that it behaves as expected.\nEnd-to-end\nComponent\nIntegration\nUnit\nStub/mock\ndependency 1\nStub/mock\ndependency 2\nStub/mock\ndependency\n...\nDependency 1\nDependency 2\nDependency\n...\nSolitary\nunit test\nTests\nClass\nSocial\nunit test\nTests\nClass\nFigure 9.10\nUnit tests are the base of the pyramid. They’re fast running, easy to write, and reliable. \nA solitary unit test tests a class in isolation, using mocks or stubs for its dependencies. A sociable \nunit test tests a class and its dependencies.\n \n\n\n308\nCHAPTER 9\nTesting microservices: Part 1\nThere are two types of unit tests (https://martinfowler.com/bliki/UnitTest.html):\nSolitary unit test—Tests a class in isolation using mock objects for the class’s\ndependencies\nSociable unit test—Tests a class and its dependencies\nThe responsibilities of the class and its role in the architecture determine which type\nof test to use. Figure 9.11 shows the hexagonal architecture of a typical service and the\ntype of unit test that you’ll typically use for each kind of class. Controller and service\nclasses are often tested using solitary unit tests. Domain objects, such as entities and\nvalue objects, are typically tested using sociable unit tests.\n«Message Channel»\n«Message Channel»\nPOST/something\nGET/something/id\nDomain logic\nService\nEntity\nSolitary\nunit test\nSociable\nunit test\nValue\nobject\nSaga\nInbound\nmessage\nadapter\nOutbound\nmessage\nadapter\nDatabase\nadapter\nDatabase\nRepository\nController\nSolitary\nunit test\nFigure 9.11\nThe responsibilities of a class determine whether to use a solitary or sociable unit test.\n \n\n\n309\nWriting unit tests for a service\nThe typical testing strategy for each class is as follows:\nEntities, such as Order, which as described in chapter 5 are objects with per-\nsistent identity, are tested using sociable unit tests.\nValue objects, such as Money, which as described in chapter 5 are objects that are\ncollections of values, are tested using sociable unit tests.\nSagas, such as CreateOrderSaga, which as described in chapter 4 maintain data\nconsistency across services, are tested using sociable unit tests.\nDomain services, such as OrderService, which as described in chapter 5 are\nclasses that implement business logic that doesn’t belong in entities or value\nobjects, are tested using solitary unit tests.\nControllers, such as OrderController, which handle HTTP requests, are tested\nusing solitary unit tests.\nInbound and outbound messaging gateways are tested using solitary unit tests.\nLet’s begin by looking at how to test entities.\n9.2.1\nDeveloping unit tests for entities\nThe following listing shows an excerpt of OrderTest class, which implements the unit\ntests for the Order entity. The class has an @Before setUp() method that creates an Order\nbefore running each test. Its @Test methods might further initialize Order, invoke one of\nits methods, and then make assertions about the return value and the state of Order.\npublic class OrderTest {\nprivate ResultWithEvents<Order> createResult;\nprivate Order order;\n@Before\npublic void setUp() throws Exception {\ncreateResult = Order.createOrder(CONSUMER_ID, AJANTA_ID, CHICKEN_VINDALOO\n_LINE_ITEMS);\norder = createResult.result;\n}\n@Test\npublic void shouldCalculateTotal() {\nassertEquals(CHICKEN_VINDALOO_PRICE.multiply(CHICKEN_VINDALOO_QUANTITY),\norder.getOrderTotal());\n}\n...\n}\nThe @Test shouldCalculateTotal() method verifies that Order.getOrderTotal()\nreturns the expected value. Unit tests thoroughly test the business logic. They are\nListing 9.2\nA simple, fast-running unit test for the Order entity\n \n\n\n310\nCHAPTER 9\nTesting microservices: Part 1\nsociable unit tests for the Order class and its dependencies. You can use them as\ncompile-time tests because they execute extremely quickly. The Order class relies on\nthe Money value object, so it’s important to test that class as well. Let’s see how to do that. \n9.2.2\nWriting unit tests for value objects\nValue objects are immutable, so they tend to be easy to test. You don’t have to worry\nabout side effects. A test for a value object typically creates a value object in a particu-\nlar state, invokes one of its methods, and makes assertions about the return value. List-\ning 9.3 shows the tests for the Money value object, which is a simple class that\nrepresents a money value. These tests verify the behavior of the Money class’s methods,\nincluding add(), which adds two Money objects, and multiply(), which multiplies a\nMoney object by an integer. They are solitary tests because the Money class doesn’t\ndepend on any other application classes.\npublic class MoneyTest {\nprivate final int M1_AMOUNT = 10;\nprivate final int M2_AMOUNT = 15;\nprivate Money m1 = new Money(M1_AMOUNT);\nprivate Money m2 = new Money(M2_AMOUNT);\n@Test\npublic void shouldAdd() {\n         \nassertEquals(new Money(M1_AMOUNT + M2_AMOUNT), m1.add(m2));\n}\n@Test\npublic void shouldMultiply() {     \nint multiplier = 12;\nassertEquals(new Money(M2_AMOUNT * multiplier), m2.multiply(multiplier));\n}\n...\n}\nEntities and value objects are the building blocks of a service’s business logic. But\nsome business logic also resides in the service’s sagas and services. Let’s look at how to\ntest those. \n9.2.3\nDeveloping unit tests for sagas\nA saga, such as the CreateOrderSaga class, implements important business logic, so\nneeds to be tested. It’s a persistent object that sends command messages to saga partic-\nipants and processes their replies. As described in chapter 4, CreateOrderSaga\nexchanges command/reply messages with several services, such as Consumer Service\nand Kitchen Service. A test for this class creates a saga and verifies that it sends the\nListing 9.3\nA simple, fast-running test for the Money value object\nVerify that two \nMoney objects can \nbe added together.\nVerify that a Money \nobject can be multiplied \nby an integer.\n \n\n\n311\nWriting unit tests for a service\nexpected sequence of messages to the saga participants. One test you need to write is\nfor the happy path. You must also write tests for the various scenarios where the saga\nrolls back because a saga participant sent back a failure message.\n One approach would be to write tests that use a real database and message broker\nalong with stubs to simulate the various saga participants. For example, a stub for\nConsumer Service would subscribe to the consumerService command channel and\nsend back the desired reply message. But tests written using this approach would be\nquite slow. A much more effective approach is to write tests that mock those classes\nthat interact with the database and message broker. That way, we can focus on testing\nthe saga’s core responsibility.\n Listing 9.4 shows a test for CreateOrderSaga. It’s a sociable unit test that tests the\nsaga class and its dependencies. It’s written using the Eventuate Tram Saga testing\nframework (https://github.com/eventuate-tram/eventuate-tram-sagas). This frame-\nwork provides an easy-to-use DSL that abstracts away the details of interacting with\nsagas. With this DSL, you can create a saga and verify that it sends the correct com-\nmand messages. Under the covers, the Saga testing framework configures the Saga\nframework with mocks for the database and messaging infrastructure.\npublic class CreateOrderSagaTest {\n@Test\npublic void shouldCreateOrder() {\ngiven()\n.saga(new CreateOrderSaga(kitchenServiceProxy),     \nnew CreateOrderSagaState(ORDER_ID,\nCHICKEN_VINDALOO_ORDER_DETAILS)).\nexpect().\n              \ncommand(new ValidateOrderByConsumer(CONSUMER_ID, ORDER_ID,\nCHICKEN_VINDALOO_ORDER_TOTAL)).\nto(ConsumerServiceChannels.consumerServiceChannel).\nandGiven().\nsuccessReply().\n               \nexpect().\ncommand(new CreateTicket(AJANTA_ID, ORDER_ID, null)).   \nto(KitchenServiceChannels.kitchenServiceChannel);\n}\n@Test\npublic void shouldRejectOrderDueToConsumerVerificationFailed() {\ngiven()\n.saga(new CreateOrderSaga(kitchenServiceProxy),\nnew CreateOrderSagaState(ORDER_ID,\nCHICKEN_VINDALOO_ORDER_DETAILS)).\nexpect().\ncommand(new ValidateOrderByConsumer(CONSUMER_ID, ORDER_ID,\nCHICKEN_VINDALOO_ORDER_TOTAL)).\nto(ConsumerServiceChannels.consumerServiceChannel).\nandGiven().\nListing 9.4\nA simple, fast-running unit test for CreateOrderSaga\nCreate\nthe saga.\nVerify that it sends \na ValidateOrderBy-\nConsumer message \nto Consumer Service.\nSend a Success reply \nto that message.\nVerify that it sends \na CreateTicket \nmessage to \nKitchen Service.\n \n\n\n312\nCHAPTER 9\nTesting microservices: Part 1\nfailureReply().\n            \nexpect().\ncommand(new RejectOrderCommand(ORDER_ID)).\nto(OrderServiceChannels.orderServiceChannel);\n      \n}\n}\nThe @Test shouldCreateOrder() method tests the happy path. The @Test should-\nRejectOrderDueToConsumerVerificationFailed() method tests the scenario where\nConsumer Service rejects the order. It verifies that CreateOrderSaga sends a Reject-\nOrderCommand to compensate for the consumer being rejected. The CreateOrder-\nSagaTest class has methods that test other failure scenarios.\n Let’s now look at how to test domain services. \n9.2.4\nWriting unit tests for domain services\nThe majority of a service’s business logic is implemented by the entities, value objects,\nand sagas. Domain service classes, such as the OrderService class, implement the\nremainder. This class is a typical domain service class. Its methods invoke entities and\nrepositories and publish domain events. An effective way to test this kind of class is to\nuse a mostly solitary unit test, which mocks dependencies such as repositories and\nmessaging classes.\n Listing 9.5 shows the OrderServiceTest class, which tests OrderService. It defines\nsolitary unit tests, which use Mockito mocks for the service’s dependencies. Each test\nimplements the test phases as follows:\n1\nSetup—Configures the mock objects for the service’s dependencies\n2\nExecute—Invokes a service method\n3\nVerify—Verifies that the value returned by the service method is correct and that\nthe dependencies have been invoked correctly\npublic class OrderServiceTest {\nprivate OrderService orderService;\nprivate OrderRepository orderRepository;\nprivate DomainEventPublisher eventPublisher;\nprivate RestaurantRepository restaurantRepository;\nprivate SagaManager<CreateOrderSagaState> createOrderSagaManager;\nprivate SagaManager<CancelOrderSagaData> cancelOrderSagaManager;\nprivate SagaManager<ReviseOrderSagaData> reviseOrderSagaManager;\n@Before\npublic void setup() {\norderRepository = mock(OrderRepository.class);         \neventPublisher = mock(DomainEventPublisher.class);\nrestaurantRepository = mock(RestaurantRepository.class);\nListing 9.5\nA simple, fast-running unit test for the OrderService class\nSend a failure \nreply indicating \nthat Consumer \nService rejected \nOrder.\nVerify that the saga sends\na RejectOrderCommand\nmessage to Order Service.\nCreate Mockito \nmocks for \nOrderService’s \ndependencies.\n \n\n\n313\nWriting unit tests for a service\ncreateOrderSagaManager = mock(SagaManager.class);\ncancelOrderSagaManager = mock(SagaManager.class);\nreviseOrderSagaManager = mock(SagaManager.class);\norderService = new OrderService(orderRepository, eventPublisher,  \nrestaurantRepository, createOrderSagaManager,\ncancelOrderSagaManager, reviseOrderSagaManager);\n}\n@Test\npublic void shouldCreateOrder() {\nwhen(restaurantRepository\n       \n.findById(AJANTA_ID)).thenReturn(Optional.of(AJANTA_RESTAURANT_);\nwhen(orderRepository.save(any(Order.class))).then(invocation -> {  \nOrder order = (Order) invocation.getArguments()[0];\norder.setId(ORDER_ID);\nreturn order;\n});\nOrder order = orderService.createOrder(CONSUMER_ID,\n   \nAJANTA_ID, CHICKEN_VINDALOO_MENU_ITEMS_AND_QUANTITIES);\nverify(orderRepository).save(same(order));        \nverify(eventPublisher).publish(Order.class, ORDER_ID,     \nsingletonList(\nnew OrderCreatedEvent(CHICKEN_VINDALOO_ORDER_DETAILS)));\nverify(createOrderSagaManager)\n                  \n.create(new CreateOrderSagaState(ORDER_ID,\nCHICKEN_VINDALOO_ORDER_DETAILS),\nOrder.class, ORDER_ID);\n}\n}\nThe setUp() method creates an OrderService injected with mock dependencies.\nThe @Test shouldCreateOrder() method verifies that OrderService.createOrder()\ninvokes OrderRepository to save the newly created Order, publishes an OrderCreated-\nEvent, and creates a CreateOrderSaga.\n Now that we’ve seen how to unit test the domain logic classes, let’s look at how to\nunit test the adapters that interact with external systems. \n9.2.5\nDeveloping unit tests for controllers\nServices, such as Order Service, typically have one or more controllers that handle\nHTTP requests from other services and the API gateway. A controller class consists of\na set of request handler methods. Each method implements a REST API endpoint. A\nmethod’s parameters represent values from the HTTP request, such as path variables.\nIt typically invokes a domain service or a repository and returns a response object.\nCreate an OrderService injected\nwith mock dependencies.\nConfigure RestaurantRepository.findById() \nto return the Ajanta restaurant.\nConfigure OrderRepository.save()\nto set Order’s ID.\n Invoke\nOrderService\n.create().\nVerify that \nOrderService saved \nthe newly created \nOrder in the database.\nVerify that\nOrderService\npublished\nan Order-\nCreatedEvent.\nVerify that Order-\nService created a \nCreateOrderSaga.\n \n\n\n314\nCHAPTER 9\nTesting microservices: Part 1\nOrderController, for instance, invokes OrderService and OrderRepository. An\neffective testing strategy for controllers is solitary unit tests that mock the services\nand repositories.\n You could write a test class similar to the OrderServiceTest class to instantiate a\ncontroller class and invoke its methods. But this approach doesn’t test some import-\nant functionality, such as request routing. It’s much more effective to use a mock MVC\ntesting framework, such as Spring Mock Mvc, which is part of the Spring Framework,\nor Rest Assured Mock MVC, which builds on Spring Mock Mvc. Tests written using\none of these frameworks make what appear to be HTTP requests and make assertions\nabout HTTP responses. These frameworks enable you to test HTTP request routing\nand conversion of Java objects to and from JSON without having to make real network\ncalls. Under the covers, Spring Mock Mvc instantiates just enough of the Spring MVC\nclasses to make this possible.\nListing 9.6 shows the OrderControllerTest class, which tests Order Service’s Order-\nController. It defines solitary unit tests that use mocks for OrderController’s depen-\ndencies. It’s written using Rest Assured Mock MVC , which provides a simple DSL that\nabstracts away the details of interacting with controllers. Rest Assured makes it easy to\nsend a mock HTTP request to a controller and verify the response. OrderController-\nTest creates a controller that’s injected with Mockito mocks for OrderService and\nOrderRepository. Each test configures the mocks, makes an HTTP request, verifies that\nthe response is correct, and possibly verifies that the controller invoked the mocks.\npublic class OrderControllerTest {\nprivate OrderService orderService;\nprivate OrderRepository orderRepository;\n@Before\npublic void setUp() throws Exception {\norderService = mock(OrderService.class);\n      \norderRepository = mock(OrderRepository.class);\nAre these really unit tests?\nBecause these tests use the Spring Framework, you might argue that they’re not unit\ntests. They’re certainly more heavyweight than the unit tests I’ve described so far.\nThe Spring Mock Mvc documentation refers to these as out-of-servlet-container inte-\ngration tests (https://docs.spring.io/spring/docs/current/spring-framework-reference/\ntesting.html#spring-mvc-test-vs-end-to-end-integration-tests). Yet Rest Assured Mock\nMVC describes these tests as unit tests (https://github.com/rest-assured/rest-\nassured/wiki/Usage#spring-mock-mvc-module). Regardless of the debate over termi-\nnology, these are important tests to write.\nListing 9.6\nA simple, fast-running unit test for the OrderController class\nCreate mocks for \nOrderController’s \ndependencies.\n \n\n\n315\nWriting unit tests for a service\norderController = new OrderController(orderService, orderRepository);\n}\n@Test\npublic void shouldFindOrder() {\nwhen(orderRepository.findById(1L))\n.thenReturn(Optional.of(CHICKEN_VINDALOO_ORDER_);    \ngiven().\nstandaloneSetup(configureControllers(\n      \nnew OrderController(orderService, orderRepository))).\nwhen().\nget(\"/orders/1\").\n    \nthen().\nstatusCode(200).\n   \nbody(\"orderId\",\n            \nequalTo(new Long(OrderDetailsMother.ORDER_ID).intValue())).\nbody(\"state\",\nequalTo(OrderDetailsMother.CHICKEN_VINDALOO_ORDER_STATE.name())).\nbody(\"orderTotal\",\nequalTo(CHICKEN_VINDALOO_ORDER_TOTAL.asString()))\n;\n}\n@Test\npublic void shouldFindNotOrder() { ... }\nprivate StandaloneMockMvcBuilder controllers(Object... controllers) { ... }\n}\nThe shouldFindOrder() test method first configures the OrderRepository mock to\nreturn an Order. It then makes an HTTP request to retrieve the order. Finally, it\nchecks that the request was successful and that the response body contains the\nexpected data.\n Controllers aren’t the only adapters that handle requests from external systems.\nThere are also event/message handlers, so let’s talk about how to unit test those. \n9.2.6\nWriting unit tests for event and message handlers\nServices often process messages sent by external systems. Order Service, for example,\nhas OrderEventConsumer, which is a message adapter that handles domain events pub-\nlished by other services. Like controllers, message adapters tend to be simple classes\nthat invoke domain services. Each of a message adapter’s methods typically invokes a\nservice method with data from the message or event.\n We can unit test message adapters using an approach similar to the one we used\nfor unit testing controllers. Each test instances the message adapter, sends a message\nto a channel, and verifies that the service mock was invoked correctly. Behind the\nConfigure the mock \nOrderRepository to \nreturn an Order.\nConfigure \nOrderController.\nMake an\nHTTP\nrequest.\nVerify the response \nstatus code.\nVerify\nelements\nof the JSON\nresponse\nbody.\n \n",
      "page_number": 330
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 341-352)",
      "start_page": 341,
      "end_page": 352,
      "detection_method": "topic_boundary",
      "content": "316\nCHAPTER 9\nTesting microservices: Part 1\nscenes, though, the messaging infrastructure is stubbed, so no message broker is\ninvolved. Let’s look at how to test the OrderEventConsumer class.\n Listing 9.7 shows part of the OrderEventConsumerTest class, which tests Order-\nEventConsumer. It verifies that OrderEventConsumer routes each event to the appro-\npriate handler method and correctly invokes OrderService. The test uses the\nEventuate Tram Mock Messaging framework, which provides an easy-to-use DSL for\nwriting mock messaging tests that uses the same given-when-then format as Rest\nAssured. Each test instantiates OrderEventConsumer injected with a mock Order-\nService, publishes a domain event, and verifies that OrderEventConsumer correctly\ninvokes the service mock.\npublic class OrderEventConsumerTest {\nprivate OrderService orderService;\nprivate OrderEventConsumer orderEventConsumer;\n@Before\npublic void setUp() throws Exception {\norderService = mock(OrderService.class);\norderEventConsumer = new OrderEventConsumer(orderService);\n  \n}\n@Test\npublic void shouldCreateMenu() {\ngiven().\neventHandlers(orderEventConsumer.domainEventHandlers()).  \nwhen().\naggregate(\"net.chrisrichardson.ftgo.restaurantservice.domain.Restaurant\",\nAJANTA_ID).\npublishes(new RestaurantCreated(AJANTA_RESTAURANT_NAME,   \nRestaurantMother.AJANTA_RESTAURANT_MENU))\nthen().\nverify(() -> {\n    \nverify(orderService)\n.createMenu(AJANTA_ID,\nnew RestaurantMenu(RestaurantMother.AJANTA_RESTAURANT_MENU_ITEMS));\n})\n;\n}\n}\nThe setUp() method creates an OrderEventConsumer injected with a mock Order-\nService. The shouldCreateMenu() method publishes a RestaurantCreated event\nand verifies that OrderEventConsumer invoked OrderService.createMenu(). The\nOrderEventConsumerTest class and the other unit test classes execute extremely quickly.\nThe unit tests run in just a few seconds.\nListing 9.7\nA fast-running unit test for the OrderEventConsumer class\nInstantiate\nOrderEventConsumer with\nmocked dependencies.\nConfigure\nOrderEventConsumer\ndomain handlers.\nPublish a\nRestaurant-\nCreated\nevent.\nVerify that OrderEventConsumer \ninvoked OrderService.createMenu().\n \n\n\n317\nSummary\n But the unit tests don’t verify that a service, such as Order Service, properly inter-\nacts with other services. For example, the unit tests don’t verify that an Order can be\npersisted in MySQL. Nor do they verify that CreateOrderSaga sends command mes-\nsages in the right format to the right message channel. And they don’t verify that the\nRestaurantCreated event processed by OrderEventConsumer has the same structure\nas the event published by Restaurant Service. In order to verify that a service prop-\nerly interacts with other services, we must write integration tests. We also need to write\ncomponent tests that test an entire service in isolation. The next chapter discusses\nhow to conduct those types of tests, as well as end-to-end tests. \nSummary\nAutomated testing is the key foundation of rapid, safe delivery of software.\nWhat’s more, because of its inherent complexity, to fully benefit from the\nmicroservice architecture you must automate your tests.\nThe purpose of a test is to verify the behavior of the system under test (SUT). In\nthis definition, system is a fancy term that means the software element being\ntested. It might be something as small as a class, as large as the entire applica-\ntion, or something in between, such as a cluster of classes or an individual ser-\nvice. A collection of related tests form a test suite.\nA good way to simplify and speed up a test is to use test doubles. A test double is\nan object that simulates the behavior of a SUT’s dependency. There are two\ntypes of test doubles: stubs and mocks. A stub is a test double that returns values\nto the SUT. A mock is a test double that a test uses to verify that the SUT cor-\nrectly invokes a dependency.\nUse the test pyramid to determine where to focus your testing efforts for your\nservices. The majority of your tests should be fast, reliable, and easy-to-write unit\ntests. You must minimize the number of end-to-end tests, because they’re slow,\nbrittle, and time consuming to write.\n \n\n\n318\nTesting microservices:\nPart 2\nThis chapter builds on the previous chapter, which introduced testing concepts,\nincluding the test pyramid. The test pyramid describes the relative proportions of\nthe different types of tests that you should write. The previous chapter described\nhow to write unit tests, which are at the base of the testing pyramid. In this chapter,\nwe continue our ascent of the testing pyramid.\n This chapter begins with how to write integration tests, which are the level\nabove unit tests in the testing pyramid. Integration tests verify that a service can prop-\nerly interact with infrastructure services, such as databases, and other application\nservices. Next, I cover component tests, which are acceptance tests for services. A com-\nponent test tests a service in isolation by using stubs for its dependencies. After\nthat, I describe how to write end-to-end tests, which test a group of services or the\nThis chapter covers\nTechniques for testing services in isolation\nUsing consumer-driven contract testing to write \ntests that quickly yet reliably verify interservice \ncommunication\nWhen and how to do end-to-end testing of \napplications\n \n\n\n319\nWriting integration tests\nentire application. End-to-end tests are at the top of the test pyramid and should,\ntherefore, be used sparingly.\n Let’s start by taking a look at how to write integration tests.\n10.1\nWriting integration tests\nServices typically interact with other services. For example, Order Service, as fig-\nure 10.1 shows, interacts with several services. Its REST API is consumed by API Gateway,\nand its domain events are consumed by services, including Order History Service.\nOrder Service uses several other services. It persists Orders in MySQL. It also sends\ncommands to and consumes replies from several other services, such as Kitchen\nService.\nIn order to be confident that a service such as Order Service works as expected, we\nmust write tests that verify that the service can properly interact with infrastructure\nservices and other application services. One approach is to launch all the services and\ntest them through their APIs. This, however, is what’s known as end-to-end testing,\nwhich is slow, brittle, and costly. As explained in section 10.3, there’s a role for end-to-end\nOrder History\nService\nAPI\ngateway\nOrder\nhistory\nevent\nhandlers\nKitchen\nService\nOrder\nService\nKitchen\nService\ncommand\nhandler\nClass under test\nLegend\nTest\nTest\nTest\nTest\nDatabase\nOrder\naggregate\nevent\npublisher\nEvent\nchannel\nCommand\nchannel\nReply\nchannel\nProvider\nProvider\nConsumer\nConsumer\nProvider\nProvider\nOrder\ncontroller\nOrder\nRepository\nKitchen\nService\nproxy\nOrder\nService\nproxy\nFigure 10.1\nIntegration tests must verify that a service can communicate with its clients and \ndependencies. But rather than testing whole services, the strategy is to test the individual adapter \nclasses that implement the communication.\n \n\n\n320\nCHAPTER 10\nTesting microservices: Part 2\ntesting sometimes, but it’s at the top of the test pyramid, so you want to minimize the\nnumber of end-to-end tests.\n A much more effective strategy is to write what are known as integration tests. As fig-\nure 10.2 shows, integration tests are the layer above unit tests in the testing pyramid.\nThey verify that a service can properly interact with infrastructure services and other ser-\nvices. But unlike end-to-end tests, they don’t launch services. Instead, we use a couple of\nstrategies that significantly simplify the tests without impacting their effectiveness.\nThe first strategy is to test each of the service’s adapters, along with, perhaps, the\nadapter’s supporting classes. For example, in section 10.1.1 you’ll see a JPA per-\nsistence test that verifies that Orders are persisted correctly. Rather than test persistence\nthrough Order Service’s API, it directly tests the OrderRepository class. Similarly, in\nsection 10.1.3 you’ll see a test that verifies that Order Service publishes correctly\nstructured domain events by testing the OrderDomainEventPublisher class. The bene-\nfit of testing only a small number of classes rather than the entire service is that the\ntests are significantly simpler and faster.\n The second strategy for simplifying integration tests that verify interactions\nbetween application services is to use contracts, discussed in chapter 9. A contract is a\nconcrete example of an interaction between a pair of services. As table 10.1 shows, the\nstructure of a contract depends on the type of interaction between the services.\nTable 10.1\nThe structure of a contract depends on the type of interaction between the services.\nInteraction style\nConsumer\nProvider\nContract\nREST-based, \nrequest/response\nAPI Gateway\nOrder Service\nHTTP request and \nresponse\nPublish/subscribe\nOrder History Service\nOrder Service\nDomain event\nAsynchronous \nrequest/response\nOrder Service\nKitchen Service\nCommand message \nand reply message\nEnd-to-end\nComponent\nIntegration\nUnit\nFigure 10.2\nIntegration tests are the layer \nabove unit tests. They verify that a service \ncan communicate with its dependencies, \nwhich includes infrastructure services, such \nas the database, and application services.\n \n\n\n321\nWriting integration tests\nA contract consists of either one message, in the case of publish/subscribe style inter-\nactions, or two messages, in the case of request/response and asynchronous request/\nresponse style interactions.\n The contracts are used to test both the consumer and the provider, which ensures\nthat they agree on the API. They’re used in slightly different ways depending on\nwhether you’re testing the consumer or the provider:\nConsumer-side tests—These are tests for the consumer’s adapter. They use the\ncontracts to configure stubs that simulate the provider, enabling you to write\nintegration tests for a consumer that don’t require a running provider.\nProvider-side tests—These are tests for the provider’s adapter. They use the con-\ntracts to test the adapters using mocks for the adapters’s dependencies.\nLater in this section, I describe examples of these types of tests—but first let’s look at\nhow to write persistence tests.\n10.1.1 Persistence integration tests\nServices typically store data in a database. For instance, Order Service persists aggre-\ngates, such as Order, in MySQL using JPA. Similarly, Order History Service maintains\na CQRS view in AWS DynamoDB. The unit tests we wrote earlier only test in-memory\nobjects. In order to be confident that a service works correctly, we must write per-\nsistence integration tests, which verify that a service’s database access logic works as\nexpected. In the case of Order Service, this means testing the JPA repositories, such\nas OrderRepository.\n Each phase of a persistence integration test behaves as follows:\nSetup—Set up the database by creating the database schema and initializing it to\na known state. It might also begin a database transaction.\nExecute—Perform a database operation.\nVerify—Make assertions about the state of the database and objects retrieved\nfrom the database.\nTeardown—An optional phase that might undo the changes made to the database\nby, for example, rolling back the transaction that was started by the setup phase.\nListing 10.1 shows a persistent integration test for the Order aggregate and Order-\nRepository. Apart from relying on JPA to create the database schema, the persistence\nintegration tests don’t make any assumption about the state of the database. Conse-\nquently, tests don’t need to roll back the changes they make to the database, which\navoids problems with the ORM caching data changes in memory.\n@RunWith(SpringRunner.class)\n@SpringBootTest(classes = OrderJpaTestConfiguration.class)\npublic class OrderJpaTest {\nListing 10.1\nAn integration test that verifies that an Order can be persisted\n \n\n\n322\nCHAPTER 10\nTesting microservices: Part 2\n@Autowired\nprivate OrderRepository orderRepository;\n@Autowired\nprivate TransactionTemplate transactionTemplate;\n@Test\npublic void shouldSaveAndLoadOrder() {\nLong orderId = transactionTemplate.execute((ts) -> {\nOrder order =\nnew Order(CONSUMER_ID, AJANTA_ID, CHICKEN_VINDALOO_LINE_ITEMS);\norderRepository.save(order);\nreturn order.getId();\n});\ntransactionTemplate.execute((ts) -> {\nOrder order = orderRepository.findById(orderId).get();\nassertEquals(OrderState.APPROVAL_PENDING, order.getState());\nassertEquals(AJANTA_ID, order.getRestaurantId());\nassertEquals(CONSUMER_ID, order.getConsumerId().longValue());\nassertEquals(CHICKEN_VINDALOO_LINE_ITEMS, order.getLineItems());\nreturn null;\n});\n}\n}\nThe shouldSaveAndLoadOrder() test method executes two transactions. The first\nsaves a newly created Order in the database. The second transaction loads the Order\nand verifies that its fields are properly initialized.\n One problem you need to solve is how to provision the database that’s used in per-\nsistence integration tests. An effective solution to run an instance of the database during\ntesting is to use Docker. Section 10.2 describes how to use the Docker Compose Gradle\nplugin to automatically run services during component testing. You can use a similar\napproach to run MySQL, for example, during persistence integration testing.\n The database is only one of the external services a service interacts with. Let’s now\nlook at how to write integration tests for interservice communication between applica-\ntion services, starting with REST. \n10.1.2 Integration testing REST-based request/response style \ninteractions\nREST is a widely used interservice communication mechanism. The REST client and\nREST service must agree on the REST API, which includes the REST endpoints and\nthe structure of the request and response bodies. The client must send an HTTP\nrequest to the correct endpoint, and the service must send back the response that the\nclient expects.\n \n\n\n323\nWriting integration tests\n For example, chapter 8 describes how the FTGO application’s API Gateway makes\nREST API calls to numerous services, including ConsumerService, Order Service,\nand Delivery Service. The OrderService’s GET /orders/{orderId} endpoint is one\nof the endpoints invoked by the API Gateway. In order to be confident that API Gateway\nand Order Service can communicate without using an end-to-end test, we need to\nwrite integration tests.\n As stated in the preceding chapter, a good integration testing strategy is to use\nconsumer-driven contract tests. The interaction between API Gateway and GET\n/orders/{orderId} can be described using a set of HTTP-based contracts. Each con-\ntract consists of an HTTP request and an HTTP reply. The contracts are used to test\nAPI Gateway and Order Service.\n Figure 10.3 shows how to use Spring Cloud Contract to test REST-based interac-\ntions. The consumer-side API Gateway integration tests use the contracts to configure\nan HTTP stub server that simulates the behavior of Order Service. A contract’s\nrequest specifies an HTTP request from the API gateway, and the contract’s response\nspecifies the response that the stub sends back to the API gateway. Spring Cloud Con-\ntract uses the contracts to code-generate the provider-side Order Service integration\ntests, which test the controllers using Spring Mock MVC or Rest Assured Mock MVC.\nThe contract’s request specifies the HTTP request to make to the controller, and the\ncontract’s response specifies the controller’s expected response.\n The consumer-side OrderServiceProxyTest invokes OrderServiceProxy, which\nhas been configured to make HTTP requests to WireMock. WireMock is a tool for effi-\nciently mocking HTTP servers—in this test it simulates Order Service. Spring Cloud\nWiremock-\nbased HTTP\nstub server\nOrder\ncontroller\nProvider-side integration\ntest for Order Service\nConsumer-side integration\ntest forAPI gateway\nSpring Cloud\nContract\nTests\nTests\nUses\nConﬁgures\nConﬁgures\nGenerates\nReads\nHTTP\nOrderService\nProxyTest\nclass HttpTest\nextends BaseHttp {\n}\nabstract class BaseHttp {\n@Before\npublic void setup() {\nRestAssuredMockMvc\n.standaloneSetup(...);\n}\n}\nContract.make {\nrequest {..}\nresponse {...}\n}\n}\nOrderService\nProxy\nFigure 10.3\nThe contracts are used to verify that the adapter classes on both sides of the \nREST-based communication between API Gateway and Order Service conform to the contract. \nThe consumer-side tests verify that OrderServiceProxy invokes Order Service correctly. The \nprovider-side tests verify that OrderController implements the REST API endpoints correctly.\n \n\n\n324\nCHAPTER 10\nTesting microservices: Part 2\nContract manages WireMock and configures it to respond to the HTTP requests\ndefined by the contracts.\n On the provider side, Spring Cloud Contract generates a test class called HttpTest,\nwhich uses Rest Assured Mock MVC to test Order Service’s controllers. Test classes\nsuch as HttpTest must extend a handwritten base class. In this example, the base class\nBaseHttp instantiates OrderController injected with mock dependencies and calls\nRestAssuredMockMvc.standaloneSetup() to configure Spring MVC.\n Let’s take a closer look at how this works, starting with an example contract.\nAN EXAMPLE CONTRACT FOR A REST API\nA REST contract, such as the one shown in listing 10.2, specifies an HTTP request,\nwhich is sent by the REST client, and the HTTP response, which the client expects to\nget back from the REST server. A contract’s request specifies the HTTP method, the\npath, and optional headers. A contract’s response specifies the HTTP status code,\noptional headers, and, when appropriate, the expected body.\norg.springframework.cloud.contract.spec.Contract.make {\nrequest {\nmethod 'GET'\nurl '/orders/1223232'\n}\nresponse {\nstatus 200\nheaders {\nheader('Content-Type': 'application/json;charset=UTF-8')\n}\nbody('''{\"orderId\" : \"1223232\", \"state\" : \"APPROVAL_PENDING\"}''')\n}\n}\nThis particular contract describes a successful attempt by API Gateway to retrieve an\nOrder from Order Service. Let’s now look at how to use this contract to write integra-\ntion tests, starting with the tests for Order Service. \nCONSUMER-DRIVEN CONTRACT INTEGRATION TESTS FOR ORDER SERVICE\nThe consumer-driven contract integration tests for Order Service verify that its API\nmeets its clients’ expectations. Listing 10.3 shows HttpBase, which is the base class\nfor the test class code-generated by Spring Cloud Contract. It’s responsible for the\nsetup phase of the test. It creates the controllers injected with mock dependencies\nand configures those mocks to return values that cause the controller to generate the\nexpected response.\npublic abstract class HttpBase {\nprivate StandaloneMockMvcBuilder controllers(Object... controllers) {\n...\nListing 10.2\nA contract that describes an HTTP-based request/response style interaction\nListing 10.3\nThe abstract base class for the tests code-generated by Spring Cloud Contract\n \n\n\n325\nWriting integration tests\nreturn MockMvcBuilders.standaloneSetup(controllers)\n.setMessageConverters(...);\n}\n@Before\npublic void setup() {\nOrderService orderService = mock(OrderService.class);\n  \nOrderRepository orderRepository = mock(OrderRepository.class);\nOrderController orderController =\nnew OrderController(orderService, orderRepository);\nwhen(orderRepository.findById(1223232L))\n            \n.thenReturn(Optional.of(OrderDetailsMother.CHICKEN_VINDALOO_ORDER));\n...\nRestAssuredMockMvc.standaloneSetup(controllers(orderController));  \n}\n}\nThe argument 1223232L that’s passed to the mock OrderRepository’s findById()\nmethod matches the orderId specified in the contract shown in listing 10.3. This test\nverifies that Order Service has a GET /orders/{orderId} endpoint that matches its\nclient’s expectations.\n Let’s take a look at the corresponding client test. \nCONSUMER-SIDE INTEGRATION TEST FOR API GATEWAY’S ORDERSERVICEPROXY\nAPI Gateway’s OrderServiceProxy invokes the GET /orders/{orderId} endpoint. List-\ning 10.4 shows the OrderServiceProxyIntegrationTest test class, which verifies that\nit conforms to the contracts. This class is annotated with @AutoConfigureStubRunner,\nprovided by Spring Cloud Contract. It tells Spring Cloud Contract to run the Wire-\nMock server on a random port and configure it using the specified contracts. Order-\nServiceProxyIntegrationTest configures OrderServiceProxy to make requests to\nthe WireMock port.\n@RunWith(SpringRunner.class)\n@SpringBootTest(classes=TestConfiguration.class,\nwebEnvironment= SpringBootTest.WebEnvironment.NONE)\n@AutoConfigureStubRunner(ids =\n        \n{\"net.chrisrichardson.ftgo.contracts:ftgo-order-service-contracts\"},\nworkOffline = false)\n@DirtiesContext\npublic class OrderServiceProxyIntegrationTest {\n@Value(\"${stubrunner.runningstubs.ftgo-order-service-contracts.port}\")  \nListing 10.4\nA consumer-side integration test for API Gateway's \nOrderServiceProxy\nCreate\nOrderRepository\ninjected with mocks.\nConfigure OrderResponse to return an Order when findById()\nis invoked with the orderId specified in the contract.\nConfigure Spring MVC with\nOrderController.\nTell Spring Cloud Contract\nto configure WireMock with\nOrder Service’s contracts.\nObtain the randomly assigned port \nthat WireMock is running on.\n \n\n\n326\nCHAPTER 10\nTesting microservices: Part 2\nprivate int port;\nprivate OrderDestinations orderDestinations;\nprivate OrderServiceProxy orderService;\n@Before\npublic void setUp() throws Exception {\norderDestinations = new OrderDestinations();\nString orderServiceUrl = \"http://localhost:\" + port;\norderDestinations.setOrderServiceUrl(orderServiceUrl);\norderService = new OrderServiceProxy(orderDestinations,\n  \nWebClient.create());\n}\n@Test\npublic void shouldVerifyExistingCustomer() {\nOrderInfo result = orderService.findOrderById(\"1223232\").block();\nassertEquals(\"1223232\", result.getOrderId());\nassertEquals(\"APPROVAL_PENDING\", result.getState());\n}\n@Test(expected = OrderNotFoundException.class)\npublic void shouldFailToFindMissingOrder() {\norderService.findOrderById(\"555\").block();\n}\n}\nEach test method invokes OrderServiceProxy and verifies that either it returns the\ncorrect values or throws the expected exception. The shouldVerifyExisting-\nCustomer() test method verifies that findOrderById() returns values equal to those\nspecified in the contract’s response. The shouldFailToFindMissingOrder() attempts\nto retrieve a nonexistent Order and verifies that OrderServiceProxy throws an Order-\nNotFoundException. Testing both the REST client and the REST service using the\nsame contracts ensures that they agree on the API.\n Let’s now look at how to do the same kind of testing for services that interact using\nmessaging. \n10.1.3 Integration testing publish/subscribe-style interactions\nServices often publish domain events that are consumed by one or more other ser-\nvices. Integration testing must verify that the publisher and its consumers agree on the\nmessage channel and the structure of the domain events. Order Service, for example,\npublishes Order* events whenever it creates or updates an Order aggregate. Order\nHistory Service is one of the consumers of those events. We must, therefore, write\ntests that verify that these services can interact.\n Figure 10.4 shows the approach to integration testing publish/subscribe interac-\ntions. Its quite similar to the approach used for testing REST interactions. As before,\nthe interactions are defined by a set of contracts. What’s different is that each contract\nspecifies a domain event.\nCreate an OrderServiceProxy\nconfigured to make requests\nto WireMock.\n \n\n\n327\nWriting integration tests\nEach consumer-side test publishes the event specified by the contract and verifies that\nOrderHistoryEventHandlers invokes its mocked dependencies correctly.\n On the provider side, Spring Cloud Contract code-generates test classes that\nextend MessagingBase, which is a hand-written abstract superclass. Each test method\ninvokes a hook method defined by MessagingBase, which is expected to trigger the\npublication of an event by the service. In this example, each hook method invokes\nOrderDomainEventPublisher, which is responsible for publishing Order aggregate\nevents. The test method then verifies that OrderDomainEventPublisher published\nthe expected event. Let’s look at the details of how these tests work, starting with the\ncontract.\nTHE CONTRACT FOR PUBLISHING AN ORDERCREATED EVENT\nListing 10.5 shows the contract for an OrderCreated event. It specifies the event’s\nchannel, along with the expected body and message headers.\nProvider-side integration\ntest for Order Service\nConsumer-side\nintegration test for\nOrder History Service\nSpring cloud\ncontract\nTests\nTests\nReads from\nPublishes to\nConﬁgures\nCode\ngenerates\nPublishes to\nReads from\nInvokes\nUses\ncontract.make{\nlabel 'orderCreatedEvent'\ninput{\ntriggeredBy('orderCreated()')\n}\noutputMessage{...}\n}\nclass MessageTest extends MessagingBase{\n@Test\npublic void validate_orderCreatedEvent(){\norderCreated();\n...\n}\n}\nclass MessagingBase{\nvoid orderCreated(){\n}\nOrderHistory\nEventHandlers\nTest\nOrderHistory\nEventHandlers\nChannel\nOrder domain\nEventPublisher\nChannel\nMessaging stub\nClass under test\nClass under test\nTriggers\n'orderCreatedEvent'\nInvokes trigger function\nthat veriﬁes that the output\nmessage is published to the\nexpected channel\nFigure 10.4\nThe contracts are used to test both sides of the publish/subscribe interaction. The provider-side \ntests verify that OrderDomainEventPublisher publishes events that confirm to the contract. The \nconsumer-side tests verify that OrderHistoryEventHandlers consume the example events from \nthe contract.\n \n",
      "page_number": 341
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 353-368)",
      "start_page": 353,
      "end_page": 368,
      "detection_method": "topic_boundary",
      "content": "328\nCHAPTER 10\nTesting microservices: Part 2\npackage contracts;\norg.springframework.cloud.contract.spec.Contract.make {\nlabel 'orderCreatedEvent'\n  \ninput {\ntriggeredBy('orderCreated()')       \n}\noutputMessage {\nsentTo('net.chrisrichardson.ftgo.orderservice.domain.Order')\nbody('''{\"orderDetails\":{\"lineItems\":[{\"quantity\":5,\"menuItemId\":\"1\",\n                 \"name\":\"Chicken Vindaloo\",\"price\":\"12.34\",\"total\":\"61.70\"}],\n                 \"orderTotal\":\"61.70\",\"restaurantId\":1, \n        \"consumerId\":1511300065921},\"orderState\":\"APPROVAL_PENDING\"}''')\n        headers {\n            header('event-aggregate-type', \n                        'net.chrisrichardson.ftgo.orderservice.domain.Order')\n            header('event-aggregate-id', '1')\n        }\n}\n}\nThe contract also has two other important elements:\n\nlabel—is used by a consumer test to trigger publication of the event by Spring\nContact\n\ntriggeredBy—the name of the superclass method invoked by the generated\ntest method to trigger the publishing of the event\nLet’s look at how the contract is used, starting with the provider-side test for Order-\nService.\nCONSUMER-DRIVEN CONTRACT TESTS FOR ORDER SERVICE\nThe provider-side test for Order Service is another consumer-driven contract inte-\ngration test. It verifies that OrderDomainEventPublisher, which is responsible for\npublishing Order aggregate domain events, publishes events that match its clients’\nexpectations. Listing 10.6 shows MessagingBase, which is the base class for the test\nclasses code-generated by Spring Cloud Contract. It’s responsible for configuring the\nOrderDomainEventPublisher class to use in-memory messaging stubs. It also defines\nthe methods, such as orderCreated(), which are invoked by the generated tests to\ntrigger the publishing of the event.\n@RunWith(SpringRunner.class)\n@SpringBootTest(classes = MessagingBase.TestConfiguration.class,\nwebEnvironment = SpringBootTest.WebEnvironment.NONE)\n@AutoConfigureMessageVerifier\npublic abstract class MessagingBase {\nListing 10.5\nA contract for a publish/subscribe interaction style\nListing 10.6\nThe abstract base class for the Spring Cloud Contract provider-side tests\nUsed by the \nconsumer test to \ntrigger the event \nto be published\nInvoked by the code-\ngenerated provider test\nAn Order-\nCreated\ndomain\nevent\n \n\n\n329\nWriting integration tests\n@Configuration\n@EnableAutoConfiguration\n@Import({EventuateContractVerifierConfiguration.class,\nTramEventsPublisherConfiguration.class,\nTramInMemoryConfiguration.class})\npublic static class TestConfiguration {\n@Bean\npublic OrderDomainEventPublisher\nOrderDomainEventPublisher(DomainEventPublisher eventPublisher) {\nreturn new OrderDomainEventPublisher(eventPublisher);\n}\n}\n@Autowired\nprivate OrderDomainEventPublisher OrderDomainEventPublisher;\nprotected void orderCreated() {\n    \nOrderDomainEventPublisher.publish(CHICKEN_VINDALOO_ORDER,\nsingletonList(new OrderCreatedEvent(CHICKEN_VINDALOO_ORDER_DETAILS)\n));\n}\n}\nThis test class configures OrderDomainEventPublisher with in-memory messaging\nstubs. orderCreated() is invoked by the test method generated from the contract\nshown earlier in listing 10.5. It invokes OrderDomainEventPublisher to publish an\nOrderCreated event. The test method attempts to receive this event and then verifies\nthat it matches the event specified in the contract. Let’s now look at the correspond-\ning consumer-side tests. \nCONSUMER-SIDE CONTRACT TEST FOR THE ORDER HISTORY SERVICE\nOrder History Service consumes events published by Order Service. As I described\nin chapter 7, the adapter class that handles these events is the OrderHistoryEvent-\nHandlers class. Its event handlers invoke OrderHistoryDao to update the CQRS view.\nListing 10.7 shows the consumer-side integration test. It creates an OrderHistoryEvent-\nHandlers injected with a mock OrderHistoryDao. Each test method first invokes Spring\nCloud to publish the event defined in the contract and then verifies that OrderHistory-\nEventHandlers invokes OrderHistoryDao correctly.\n@RunWith(SpringRunner.class)\n@SpringBootTest(classes= OrderHistoryEventHandlersTest.TestConfiguration.class,\nwebEnvironment= SpringBootTest.WebEnvironment.NONE)\n@AutoConfigureStubRunner(ids =\n{\"net.chrisrichardson.ftgo.contracts:ftgo-order-service-contracts\"},\nworkOffline = false)\nListing 10.7\nThe consumer-side integration test for the OrderHistoryEventHandlers \nclass\norderCreated() is invoked by a\ncode-generated test subclass\nto publish the event.\n \n\n\n330\nCHAPTER 10\nTesting microservices: Part 2\n@DirtiesContext\npublic class OrderHistoryEventHandlersTest {\n@Configuration\n@EnableAutoConfiguration\n@Import({OrderHistoryServiceMessagingConfiguration.class,\nTramCommandProducerConfiguration.class,\nTramInMemoryConfiguration.class,\nEventuateContractVerifierConfiguration.class})\npublic static class TestConfiguration {\n@Bean\npublic OrderHistoryDao orderHistoryDao() {\nreturn mock(OrderHistoryDao.class);\n  \n}\n}\n@Test\npublic void shouldHandleOrderCreatedEvent() throws ... {\nstubFinder.trigger(\"orderCreatedEvent\");\n         \neventually(() -> {\n      \nverify(orderHistoryDao).addOrder(any(Order.class), any(Optional.class));\n});\n}\nThe shouldHandleOrderCreatedEvent() test method tells Spring Cloud Contract to\npublish the OrderCreated event. It then verifies that OrderHistoryEventHandlers\ninvoked orderHistoryDao.addOrder(). Testing both the domain event’s publisher and\nconsumer using the same contracts ensures that they agree on the API. Let’s now look at\nhow to do integration test services that interact using asynchronous request/response. \n10.1.4 Integration contract tests for asynchronous request/response \ninteractions\nPublish/subscribe isn’t the only kind of messaging-based interaction style. Services\nalso interact using asynchronous request/response. For example, in chapter 4 we saw\nthat Order Service implements sagas that send command messages to various ser-\nvices, such as Kitchen Service, and processes the reply messages.\n The two parties in an asynchronous request/response interaction are the requestor,\nwhich is the service that sends the command, and the replier, which is the service that\nprocesses the command and sends back a reply. They must agree on the name of com-\nmand message channel and the structure of the command and reply messages. Let’s\nlook at how to write integration tests for asynchronous request/response interactions.\n Figure 10.5 shows how to test the interaction between Order Service and Kitchen\nService. The approach to integration testing asynchronous request/response interac-\ntions is quite similar to the approach used for testing REST interactions. The interac-\ntions between the services are defined by a set of contracts. What’s different is that a\ncontract specifies an input message and an output message instead of an HTTP request\nand reply.\nCreate a mock OrderHistoryDao \nto inject into OrderHistory-\nEventHandlers.\nTrigger the \norderCreatedEvent \nstub, which emits an \nOrderCreated event.\nVerify that OrderHistoryEventHandlers\ninvoked orderHistoryDao.addOrder().\n \n\n\n331\nWriting integration tests\nThe consumer-side test verifies that the command message proxy class sends correctly\nstructured command messages and correctly processes reply messages. In this exam-\nple, KitchenServiceProxyTest tests KitchenServiceProxy. It uses Spring Cloud Con-\ntract to configure messaging stubs that verify that the command message matches a\ncontract’s input message and replies with the corresponding output message.\n The provider-side tests are code-generated by Spring Cloud Contract. Each test\nmethod corresponds to a contract. It sends the contract’s input message as a com-\nmand message and verifies that the reply message matches the contract’s output mes-\nsage. Let’s look at the details, starting with the contract.\nEXAMPLE ASYNCHRONOUS REQUEST/RESPONSE CONTRACT\nListing 10.8 shows the contract for one interaction. It consists of an input message and\nan output message. Both messages specify a message channel, message body, and mes-\nsage headers. The naming convention is from the provider’s perspective. The input\nmessage’s messageFrom element specifies the channel that the message is read from.\nProvider-side\nintegration test for\nKitchen Service\nConsumer-side\nintegration test for\nKitchen Service\nSpring cloud\ncontract\nReads\nTests\nSends to\nSends to\nReceives from\nConﬁgures\nCode\ngenerates\nReceives\nfrom\nExtends\nConﬁgures\nInvokes\nReads\ncommand\nReads reply\nSends\nreply\ncommand\nSends\ncommand\nMessage\nContract.make {\ninputMessage{...}\nOutputMessage{...}\n}\nabstract class BaseMessaging{\nvoid setUp(){...}\nclass MessageTest extends BaseMessaging{\n}\nKitchenService\nProxy\nIntegrationTest\nKitchenService\nProxy\nKitchenService\nCommandHandler\nReply\nchannel\nCommand\nchannel\nCommand\nchannel\nReply\nchannel\n«mock»\nKitchenService\nMessaging stub\nSends input message and\nveriﬁes that reply matches\ncontract’s output message\nFigure 10.5\nThe contracts are used to test the adapter classes that implement each side of the asynchronous \nrequest/response interaction. The provider-side tests verify that KitchenServiceCommandHandler handles \ncommands and sends back replies. The consumer-side tests verify KitchenServiceProxy sends commands \nthat conform to the contract, and that it handles the example replies from the contract.\n \n\n\n332\nCHAPTER 10\nTesting microservices: Part 2\nSimilarly, the output message’s sentTo element specifies the channel that the reply\nshould be sent to.\npackage contracts;\norg.springframework.cloud.contract.spec.Contract.make {\nlabel 'createTicket'\ninput {\n    \nmessageFrom('kitchenService')\nmessageBody('''{\"orderId\":1,\"restaurantId\":1,\"ticketDetails\":{...}}''')\nmessageHeaders {\nheader('command_type','net.chrisrichardson...CreateTicket')\nheader('command_saga_type','net.chrisrichardson...CreateOrderSaga')\nheader('command_saga_id',$(consumer(regex('[0-9a-f]{16}-[0-9a-f]\n{16}'))))\nheader('command_reply_to','net.chrisrichardson...CreateOrderSaga-Reply')\n}\n}\noutputMessage {\n     \nsentTo('net.chrisrichardson...CreateOrderSaga-reply')\nbody([\nticketId: 1\n])\nheaders {\nheader('reply_type', 'net.chrisrichardson...CreateTicketReply')\nheader('reply_outcome-type', 'SUCCESS')\n}\n}\n}\nIn this example contract, the input message is a CreateTicket command that’s sent to\nthe kitchenService channel. The output message is a successful reply that’s sent to the\nCreateOrderSaga’s reply channel. Let’s look at how to use this contract in tests, start-\ning with the consumer-side tests for Order Service. \nCONSUMER-SIDE CONTRACT INTEGRATION TEST FOR AN ASYNCHRONOUS REQUEST/RESPONSE \nINTERACTION\nThe strategy for writing a consumer-side integration test for an asynchronous request/\nresponse interaction is similar to testing a REST client. The test invokes the service’s\nmessaging proxy and verifies two aspects of its behavior. First, it verifies that the mes-\nsaging proxy sends a command message that conforms to the contract. Second, it ver-\nifies that the proxy properly handles the reply message.\n Listing 10.9 shows the consumer-side integration test for KitchenServiceProxy,\nwhich is the messaging proxy used by Order Service to invoke Kitchen Service. Each\ntest sends a command message using KitchenServiceProxy and verifies that it returns\nthe expected result. It uses Spring Cloud Contract to configure messaging stubs for\nListing 10.8\nContract describing how Order Service asynchronously invokes \nKitchen Service\nThe command message \nsent by Order Service \nto the kitchenService \nchannel\nThe reply message sent \nby Kitchen Service\n \n\n\n333\nWriting integration tests\nKitchen Service that find the contract whose input message matches the command\nmessage and sends its output message as the reply. The tests use in-memory messaging\nfor simplicity and speed.\n@RunWith(SpringRunner.class)\n@SpringBootTest(classes= \nKitchenServiceProxyIntegrationTest.TestConfiguration.class,\nwebEnvironment= SpringBootTest.WebEnvironment.NONE)\n@AutoConfigureStubRunner(ids =\n      \n{\"net.chrisrichardson.ftgo.contracts:ftgo-kitchen-service-contracts\"},\nworkOffline = false)\n@DirtiesContext\npublic class KitchenServiceProxyIntegrationTest {\n@Configuration\n@EnableAutoConfiguration\n@Import({TramCommandProducerConfiguration.class,\nTramInMemoryConfiguration.class,\nEventuateContractVerifierConfiguration.class})\npublic static class TestConfiguration { ... }\n@Autowired\nprivate SagaMessagingTestHelper sagaMessagingTestHelper;\n@Autowired\nprivate\nKitchenServiceProxy kitchenServiceProxy;\n@Test\npublic void shouldSuccessfullyCreateTicket() {\nCreateTicket command = new CreateTicket(AJANTA_ID,\nOrderDetailsMother.ORDER_ID,\nnew TicketDetails(Collections.singletonList(\nnew TicketLineItem(CHICKEN_VINDALOO_MENU_ITEM_ID,\nCHICKEN_VINDALOO,\nCHICKEN_VINDALOO_QUANTITY))));\nString sagaType = CreateOrderSaga.class.getName();\nCreateTicketReply reply =\nsagaMessagingTestHelper\n      \n.sendAndReceiveCommand(kitchenServiceProxy.create,\ncommand,\nCreateTicketReply.class, sagaType);\nassertEquals(new CreateTicketReply(OrderDetailsMother.ORDER_ID), reply);  \n}\n}\nListing 10.9\nThe consumer-side contract integration test for Order Service\nConfigure the stub\nKitchen Service to\nrespond to messages.\nSend the \ncommand and \nwait for a reply.\nVerify the\nreply.\n \n\n\n334\nCHAPTER 10\nTesting microservices: Part 2\nThe shouldSuccessfullyCreateTicket() test method sends a CreateTicket com-\nmand message and verifies that the reply contains the expected data. It uses Saga-\nMessagingTestHelper, which is a test helper class that synchronously sends and receives\nmessages.\n Let’s now look at how to write provider-side integration tests.\nWRITING PROVIDER-SIDE, CONSUMER-DRIVEN CONTRACT TESTS FOR ASYNCHRONOUS \nREQUEST/RESPONSE INTERACTIONS\nA provider-side integration test must verify that the provider handles a command mes-\nsage by sending the correct reply. Spring Cloud Contract generates test classes that\nhave a test method for each contract. Each test method sends the contract’s input\nmessage and verifies that the reply matches the contract’s output message.\n The provider-side integration tests for Kitchen Service test KitchenService-\nCommandHandler. The KitchenServiceCommandHandler class handles a message by\ninvoking KitchenService. The following listing shows the AbstractKitchenService-\nConsumerContractTest class, which is the base class for the Spring Cloud Contract-\ngenerated tests. It creates a KitchenServiceCommandHandler injected with a mock\nKitchenService.\n@RunWith(SpringRunner.class)\n@SpringBootTest(classes = \nAbstractKitchenServiceConsumerContractTest.TestConfiguration.class,\nwebEnvironment = SpringBootTest.WebEnvironment.NONE)\n@AutoConfigureMessageVerifier\npublic abstract class AbstractKitchenServiceConsumerContractTest {\n@Configuration\n@Import(RestaurantMessageHandlersConfiguration.class)\npublic static class TestConfiguration {\n...\n@Bean\npublic KitchenService kitchenService() {\n   \nreturn mock(KitchenService.class);\n}\n}\n@Autowired\nprivate KitchenService kitchenService;\n@Before\npublic void setup() {\nreset(kitchenService);\nwhen(kitchenService\n.createTicket(eq(1L), eq(1L),\n    \nany(TicketDetails.class)))\n.thenReturn(new Ticket(1L, 1L,\nListing 10.10\nSuperclass of provider-side, consumer-driven contract tests for Kitchen\nService\nOverrides the definition \nof the kitchenService \n@Bean with a mock\nConfigures the mock to \nreturn the values that match \na contract’s output message\n \n\n\n335\nDeveloping component tests\nnew TicketDetails(Collections.emptyList())));\n}\n}\nKitchenServiceCommandHandler invokes KitchenService with arguments that are\nderived from a contract’s input message and creates a reply message that’s derived\nfrom the return value. The test class’s setup() method configures the mock Kitchen-\nService to return the values that match the contract’s output message\n Integration tests and unit tests verify the behavior of individual parts of a service.\nThe integration tests verify that services can communicate with their clients and\ndependencies. The unit tests verify that a service’s logic is correct. Neither type of test\nruns the entire service. In order to verify that a service as a whole works, we’ll move up\nthe pyramid and look at how to write component tests. \n10.2\nDeveloping component tests\nSo far, we’ve looked at how to test individual classes and clusters of classes. But imag-\nine that we now want to verify that Order Service works as expected. In other words,\nwe want to write the service’s acceptance tests, which treat it as a black box and verify\nits behavior through its API. One approach is to write what are essentially end-to-end\ntests and deploy Order Service and all of its transitive dependencies. As you should\nknow by now, that’s a slow, brittle, and expensive way to test a service.\nA much better way to write acceptance tests for a service is to use component testing.\nAs figure 10.6 shows, component tests are sandwiched between integration tests and\nend-to-end tests. Component testing verifies the behavior of a service in isolation. It\nreplaces a service’s dependencies with stubs that simulate their behavior. It might even\nuse in-memory versions of infrastructure services such as databases. As a result, com-\nponent tests are much easier to write and faster to run.\n I begin by briefly describing how to use a testing DSL called Gherkin to write\nacceptance tests for services, such as Order Service. After that I discuss various com-\nponent testing design issues. I then show how to write acceptance tests for Order\nService.\n Let’s look at writing acceptance tests using Gherkin.\n \n \nPattern: Service component test\nTest a service in isolation. See http://microservices.io/patterns/testing/service-\ncomponent-test.html.\n \n\n\n336\nCHAPTER 10\nTesting microservices: Part 2\n10.2.1 Defining acceptance tests\nAcceptance tests are business-facing tests for a software component. They describe the\ndesired externally visible behavior from the perspective of the component’s clients\nrather than in terms of the internal implementation. These tests are derived from user\nstories or use cases. For example, one of the key stories for Order Service is the Place\nOrder story:\nAs a consumer of the Order Service\nI should be able to place an order\nWe can expand this story into scenarios such as the following:\nGiven a valid consumer\nGiven using a valid credit card\nGiven the restaurant is accepting orders\nWhen I place an order for Chicken Vindaloo at Ajanta\nThen the order should be APPROVED\nAnd an OrderAuthorized event should be published\nThis scenario describes the desired behavior of Order Service in terms of its API.\n Each scenario defines an acceptance test. The givens correspond to the test’s setup\nphase, the when maps to the execute phase, and the then and the and to the verifica-\ntion phase. Later, you see a test for this scenario that does the following:\n1\nCreates an Order by invoking the POST /orders endpoint\n2\nVerifies the state of the Order by invoking the GET /orders/{orderId} endpoint\n3\nVerifies that the Order Service published an OrderAuthorized event by sub-\nscribing to the appropriate message channel\nWe could translate each scenario into Java code. An easier option, though, is to write\nthe acceptance tests using a DSL such as Gherkin. \nEnd-to-end\nComponent\nIntegration\nUnit\nComponent\ntest\nTests\nService\nStub\ndependency 1\nStub\ndependency 2\nStub\ndependency\n...\nFigure 10.6\nA component test tests a service in isolation. It typically uses stubs for the service’s \ndependencies.\n \n\n\n337\nDeveloping component tests\n10.2.2 Writing acceptance tests using Gherkin\nWriting acceptance tests in Java is challenging. There’s a risk that the scenarios and\nthe Java tests diverge. There’s also a disconnect between the high-level scenarios and\nthe Java tests, which consist of low-level implementation details. Also, there’s a risk\nthat a scenario lacks precision or is ambiguous and can’t be translated into Java code.\nA much better approach is to eliminate the manual translation step and write execut-\nable scenarios.\n Gherkin is a DSL for writing executable specifications. When using Gherkin, you\ndefine your acceptance tests using English-like scenarios, such as the one shown ear-\nlier. You then execute the specifications using Cucumber, a test automation frame-\nwork for Gherkin. Gherkin and Cucumber eliminate the need to manually translate\nscenarios into runnable code.\n The Gherkin specification for a service such as Order Service consists of a set of fea-\ntures. Each feature is described by a set of scenarios such as the one you saw earlier. A sce-\nnario has the given-when-then structure. The givens are the preconditions, the when is\nthe action or event that occurs, and the then/and are the expected outcome.\n For example, the desired behavior of Order Service is defined by several features,\nincluding Place Order, Cancel Order, and Revise Order. Listing 10.11 is an excerpt of\nthe Place Order feature. This feature consists of several elements:\nName—For this feature, the name is Place Order.\nSpecification brief—This describes why the feature exists. For this feature, the\nspecification brief is the user story.\nScenarios—Order authorized and Order rejected due to expired credit card.\nFeature: Place Order\nAs a consumer of the Order Service\nI should be able to place an order\nScenario: Order authorized\nGiven a valid consumer\nGiven using a valid credit card\nGiven the restaurant is accepting orders\nWhen I place an order for Chicken Vindaloo at Ajanta\nThen the order should be APPROVED\nAnd an OrderAuthorized event should be published\nScenario: Order rejected due to expired credit card\nGiven a valid consumer\nGiven using an expired credit card\nGiven the restaurant is accepting orders\nWhen I place an order for Chicken Vindaloo at Ajanta\nThen the order should be REJECTED\nAnd an OrderRejected event should be published\n...\nListing 10.11\nThe Gherkin definition of the Place Order feature and some of its scenarios\n \n\n\n338\nCHAPTER 10\nTesting microservices: Part 2\nIn both scenarios, a consumer attempts to place an order. In the first scenario, they\nsucceed. In the second scenario, the order is rejected because the consumer’s credit\ncard has expired. For more information on Gherkin, see the book Writing Great Specifi-\ncations: Using Specification by Example and Gherkin by Kamil Nicieja (Manning, 2017).\nEXECUTING GHERKIN SPECIFICATIONS USING CUCUMBER\nCucumber is an automated testing framework that executes tests written in Gherkin.\nIt’s available in a variety of languages, including Java. When using Cucumber for Java,\nyou write a step definition class, such as the one shown in listing 10.12. A step definition\nclass consists of methods that define the meaning of each given-then-when step. Each\nstep definition method is annotated with either @Given, @When, @Then, or @And. Each\nof these annotations has a value element that’s a regular expression, which Cucum-\nber matches against the steps.\npublic class StepDefinitions ...\n{\n...\n@Given(\"A valid consumer\")\npublic void useConsumer() { ... }\n@Given(\"using a(.?) (.*) credit card\")\npublic void useCreditCard(String ignore, String creditCard) { ... }\n@When(\"I place an order for Chicken Vindaloo at Ajanta\")\npublic void placeOrder() { ... }\n@Then(\"the order should be (.*)\")\npublic void theOrderShouldBe(String desiredOrderState) { ... }\n@And(\"an (.*) event should be published\")\npublic void verifyEventPublished(String expectedEventClass)\n{ ... }\n}\nEach type of method is part of a particular phase of the test:\n\n@Given—The setup phase\n\n@When—The execute phase\n\n@Then and @And—The verification phase\nLater in section 10.2.4, when I describe this class in more detail, you’ll see that many\nof these methods make REST calls to Order Service. For example, the placeOrder()\nmethod creates Order by invoking the POST /orders REST endpoint. The the-\nOrderShouldBe() method verifies the status of the order by invoking GET /orders/\n{orderId}.\n But before getting into the details of how to write step classes, let’s explore some\ndesign issues with component tests. \nListing 10.12\nThe Java step definitions class makes the Gherkin scenarios executable.\n \n\n\n339\nDeveloping component tests\n10.2.3 Designing component tests\nImagine you’re implementing the component tests for Order Service. Section 10.2.2\nshows how to specify the desired behavior using Gherkin and execute it using Cucum-\nber. But before a component test can execute the Gherkin scenarios, it must first run\nOrder Service and set up the service’s dependencies. You need to test Order Service\nin isolation, so the component test must configure stubs for several services, including\nKitchen Service. It also needs to set up a database and the messaging infrastructure.\nThere are a few different options that trade off realism with speed and simplicity.\nIN-PROCESS COMPONENT TESTS\nOne option is to write in-process component tests. An in-process component test runs the\nservice with in-memory stubs and mocks for its dependencies. For example, you can\nwrite a component test for a Spring Boot-based service using the Spring Boot testing\nframework. A test class, which is annotated with @SpringBootTest, runs the service in\nthe same JVM as the test. It uses dependency injection to configure the service to use\nmocks and stubs. For instance, a test for Order Service would configure it to use an\nin-memory JDBC database, such as H2, HSQLDB, or Derby, and in-memory stubs for\nEventuate Tram. In-process tests are simpler to write and faster, but have the downside\nof not testing the deployable service. \nOUT-OF-PROCESS COMPONENT TESTING\nA more realistic approach is to package the service in a production-ready format and\nrun it as a separate process. For example, chapter 12 explains that it’s increasingly\ncommon to package services as Docker container images. An out-of-process component\ntest uses real infrastructure services, such as databases and message brokers, but uses\nstubs for any dependencies that are application services. For example, an out-of-process\ncomponent test for FTGO Order Service would use MySQL and Apache Kafka, and\nstubs for services including Consumer Service and Accounting Service. Because Order\nService interacts with those services using messaging, these stubs would consume\nmessages from Apache Kafka and send back reply messages.\n A key benefit of out-of-process component testing is that it improves test coverage,\nbecause what’s being tested is much closer to what’s being deployed. The drawback is\nthat this type of test is more complex to write, slower to execute, and potentially more\nbrittle than an in-process component test. You also have to figure out how to stub the\napplication services. Let’s look at how to do that.\nHOW TO STUB SERVICES IN OUT-OF-PROCESS COMPONENT TESTS\nThe service under test often invokes dependencies using interaction styles that involve\nsending back a response. Order Service, for example, uses asynchronous request/\nresponse and sends command messages to various services. API Gateway uses HTTP,\nwhich is a request/response interaction style. An out-of-process test must configure\nstubs for these kinds of dependencies, which handle requests and send back replies.\n One option is to use Spring Cloud Contract, which we looked at earlier in sec-\ntion 10.1 when discussing integration tests. We could write contracts that configure\n \n\n\n340\nCHAPTER 10\nTesting microservices: Part 2\nstubs for component tests. One thing to consider, though, is that it’s likely that these\ncontracts, unlike those used for integration, would only be used by the component tests.\n Another drawback of using Spring Cloud Contract for component testing is that\nbecause its focus is consumer contract testing, it takes a somewhat heavyweight\napproach. The JAR files containing the contracts must be deployed in a Maven\nrepository rather than merely being on the classpath. Handling interactions involving\ndynamically generated values is also challenging. Consequently, a simpler option is to\nconfigure stubs from within the test itself.\n A test can, for example, configure an HTTP stub using the WireMock stubbing\nDSL. Similarly, a test for a service that uses Eventuate Tram messaging can configure\nmessaging stubs. Later in this section I show an easy-to-use Java library that does this.\n Now that we’ve looked at how to design component tests, let’s consider how to\nwrite component tests for the FTGO Order Service. \n10.2.4 Writing component tests for the FTGO Order Service\nAs you saw earlier in this section, there are a few different ways to implement compo-\nnent tests. This section describes the component tests for Order Service that use the\nout-of-process strategy to test the service running as a Docker container. You’ll see\nhow the tests use a Gradle plugin to start and stop the Docker container. I discuss how\nto use Cucumber to execute the Gherkin-based scenarios that define the desired\nbehavior for Order Service.\n Figure 10.7 shows the design of the component tests for Order Service. Order-\nServiceComponentTest is the test class that runs Cucumber:\n@RunWith(Cucumber.class)\n@CucumberOptions(features = \"src/component-test/resources/features\")\npublic class OrderServiceComponentTest {\n}\nIt has an @CucumberOptions annotation that specifies where to find the Gherkin\nfeature files. It’s also annotated with @RunWith(Cucumber.class), which tells JUNIT\nto use the Cucumber test runner. But unlike a typical JUNIT-based test class, it\ndoesn’t have any test methods. Instead, it defines the tests by reading the Gherkin\nfeatures and uses the OrderServiceComponentTestStepDefinitions class to make\nthem executable.\n Using Cucumber with the Spring Boot testing framework requires a slightly unusual\nstructure. Despite not being a test class, OrderServiceComponentTestStepDefinitions\nis still annotated with @ContextConfiguration, which is part of the Spring Testing\nframework. It creates Spring ApplicationContext, which defines the various Spring\ncomponents, including messaging stubs. Let’s look at the details of the step definitions.\n \n \n \n\n\n341\nDeveloping component tests\nTHE ORDERSERVICECOMPONENTTESTSTEPDEFINITIONS CLASS\nThe OrderServiceComponentTestStepDefinitions class is the heart of the tests. This\nclass defines the meaning of each step in Order Service’s component tests. The fol-\nlowing listing shows the usingCreditCard() method, which defines the meaning of\nthe Given using … credit card step.\n@ContextConfiguration(classes = \nOrderServiceComponentTestStepDefinitions.TestConfiguration.class)\npublic class OrderServiceComponentTestStepDefinitions {\n...\n@Autowired\nprotected SagaParticipantStubManager sagaParticipantStubManager;\n@Given(\"using a(.?) (.*) credit card\")\npublic void useCreditCard(String ignore, String creditCard) {\nif (creditCard.equals(\"valid\"))\n  sagaParticipantStubManager\n   \n.forChannel(\"accountingService\")\n.when(AuthorizeCommand.class).replyWithSuccess();\nelse if (creditCard.equals(\"invalid\"))\nsagaParticipantStubManager\n    \nListing 10.13\nThe @GivenuseCreditCard() method defines the meaning of the \nGiven using … credit card step.\nAs a consumer of the Order Service\nI should be able to create an order\nScenario: Order authorized\nGiven a valid consumer\nGiven using a valid credit card\ndockerCompose {\n...\n}\nftgo-order-service:\nbuild: .\nports:\n- \"8082:8080\"\nOrderService\nComponent\nTest\nOrder Service\ndocker\ncontainer\nsrc/component-test/resources/\ncreateorder.feature\nDocker-compose.yml\nbuild.gradle\nWritten using the\nCucumber testing framework\nKafka\nMySQL\nInvokes\nREST API\nRuns\nRuns\nRuns\nReads command\nand sends\nreplies\nReads events\nReads\nUses\nUses\nOrderService\nComponent\nStep\nDeﬁnitions\nFigure 10.7\nThe component tests for Order Service use the Cucumber testing framework to \nexecute tests scenarios written using Gherkin acceptance testing DSL. The tests use Docker to run \nOrder Service along with its infrastructure services, such as Apache Kafka and MySQL.\nSend a \nsuccess reply.\nSend a failure \nreply.\n \n\n\n342\nCHAPTER 10\nTesting microservices: Part 2\n.forChannel(\"accountingService\")\n.when(AuthorizeCommand.class).replyWithFailure();\nelse\nfail(\"Don't know what to do with this credit card\");\n}\nThis method uses the SagaParticipantStubManager class, a test helper class that con-\nfigures stubs for saga participants. The useCreditCard() method uses it to configure\nthe Accounting Service stub to reply with either a success or a failure message,\ndepending on the specified credit card.\n The following listing shows the placeOrder() method, which defines the When I\nplace an order for Chicken Vindaloo at Ajanta step. It invokes the Order Service\nREST API to create Order and saves the response for validation in a later step.\n@ContextConfiguration(classes = \nOrderServiceComponentTestStepDefinitions.TestConfiguration.class)\npublic class OrderServiceComponentTestStepDefinitions {\nprivate int port = 8082;\nprivate String host = System.getenv(\"DOCKER_HOST_IP\");\nprotected String baseUrl(String path) {\nreturn String.format(\"http://%s:%s%s\", host, port, path);\n}\nprivate Response response;\n@When(\"I place an order for Chicken Vindaloo at Ajanta\")\npublic void placeOrder() {\n    response = given().                                               \n            body(new CreateOrderRequest(consumerId,\n                    RestaurantMother.AJANTA_ID, Collections.singletonList(\n                        new CreateOrderRequest.LineItem(\n                           RestaurantMother.CHICKEN_VINDALOO_MENU_ITEM_ID,\n                          OrderDetailsMother.CHICKEN_VINDALOO_QUANTITY)))).\n            contentType(\"application/json\").\n            when().\n            post(baseUrl(\"/orders\"));\n}\nThe baseUrl() help method returns the URL of the order service.\n Listing 10.15 shows the theOrderShouldBe() method, which defines the meaning\nof the Then the order should be … step. It verifies that Order was successfully created\nand that it’s in the expected state.\n \n \nListing 10.14\nThe placeOrder() method defines the When I place an order for \nChicken Vindaloo at Ajanta step.\nInvokes the Order \nService REST API \nto create Order\n \n\n\n343\nDeveloping component tests\n@ContextConfiguration(classes = \nOrderServiceComponentTestStepDefinitions.TestConfiguration.class)\npublic class OrderServiceComponentTestStepDefinitions {\n@Then(\"the order should be (.*)\")\npublic void theOrderShouldBe(String desiredOrderState) {\nInteger orderId =\n  \nthis.response. then(). statusCode(200).\nextract(). path(\"orderId\");\nassertNotNull(orderId);\neventually(() -> {\nString state = given().\nwhen().\nget(baseUrl(\"/orders/\" + orderId)).\nthen().\nstatusCode(200)\n.extract().\npath(\"state\");\nassertEquals(desiredOrderState, state);\n  \n});\n}\n]\nThe assertion of the expected state is wrapped in a call to eventually(), which\nrepeatedly executes the assertion.\n The following listing shows the verifyEventPublished() method, which defines\nthe And an … event should be published step. It verifies that the expected domain\nevent was published.\n@ContextConfiguration(classes = \nOrderServiceComponentTestStepDefinitions.TestConfiguration.class)\npublic class OrderServiceComponentTestStepDefinitions {\n@Autowired\nprotected MessageTracker messageTracker;\n@And(\"an (.*) event should be published\")\npublic void verifyEventPublished(String expectedEventClass) throws ClassNot\nFoundException {\nmessageTracker.assertDomainEventPublished(\"net.chrisrichardson.ftgo.order\nservice.domain.Order\",\nListing 10.15\nThe @ThentheOrderShouldBe() method verifies HTTP request was \nsuccessful.\nListing 10.16\nThe Cucumber step definitions class for the Order Service component \ntests\nVerify that Order \nwas created \nsuccessfully.\nVerify the \nstate of \nOrder.\n \n",
      "page_number": 353
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 369-377)",
      "start_page": 369,
      "end_page": 377,
      "detection_method": "topic_boundary",
      "content": "344\nCHAPTER 10\nTesting microservices: Part 2\n(Class<DomainEvent>)Class.forName(\"net.chrisrichardson.ftgo.order\nservice.domain.\" + expectedEventClass));\n}\n....\n}\nThe verifyEventPublished() method uses the MessageTracker class, a test helper\nclass that records the events that have been published during the test. This class\nand SagaParticipantStubManager are instantiated by the TestConfiguration\n@Configuration class.\n Now that we’ve looked at the step definitions, let’s look at how to run the compo-\nnent tests. \nRUNNING THE COMPONENT TESTS\nBecause these tests are relatively slow, we don’t want to run them as part of ./gradlew\ntest. Instead, we’ll put the test code in a separate src/component-test/java directory\nand run them using ./gradlew componentTest. Take a look at the ftgo-order-service/\nbuild.gradle file to see the Gradle configuration.\n The tests use Docker to run Order Service and its dependencies. As described in\nchapter 12, a Docker container is a lightweight operating system virtualization\nmechanism that lets you deploy a service instance in an isolated sandbox. Docker\nCompose is an extremely useful tool with which you can define a set of containers\nand start and stop them as a unit. The FTGO application has a docker-compose file\nin the root directory that defines containers for all the services, and the infrastruc-\nture service.\n We can use the Gradle Docker Compose plugin to run the containers before exe-\ncuting the tests and stop the containers once the tests complete:\napply plugin: 'docker-compose'\ndockerCompose.isRequiredBy(componentTest)\ncomponentTest.dependsOn(assemble)\ndockerCompose {\nstartedServices = [ 'ftgo-order-service']\n}\nThe preceding snippet of Gradle configuration does two things. First, it configures\nthe Gradle Docker Compose plugin to run before the component tests and start\nOrder Service along with the infrastructure services that it’s configured to depend\non. Second, it configures componentTest to depend on assemble so that the JAR file\nrequired by the Docker image is built first. With that in place, we can run these com-\nponent tests with the following commands:\n./gradlew\n:ftgo-order-service:componentTest\n \n\n\n345\nWriting end-to-end tests\nThose commands, which take a couple of minutes, perform the following actions:\n1\nBuild Order Service.\n2\nRun the service and its infrastructure services.\n3\nRun the tests.\n4\nStop the running services.\nNow that we’ve looked at how to test a service in isolation, we’ll see how to test the\nentire application. \n10.3\nWriting end-to-end tests\nComponent testing tests each service separately. End-to-end testing, though, tests the\nentire application. As figure 10.8 shows, end-to-end testing is the top of the test pyra-\nmid. That’s because these kinds of tests are—say it with me now—slow, brittle, and\ntime consuming to develop.\nEnd-to-end tests have a large number of moving parts. You must deploy multiple ser-\nvices and their supporting infrastructure services. As a result, end-to-end tests are slow.\nAlso, if your test needs to deploy a large number of services, there’s a good chance\none of them will fail to deploy, making the tests unreliable. Consequently, you should\nminimize the number of end-to-end tests.\n10.3.1 Designing end-to-end tests\nAs I’ve explained, it’s best to write as few of these as possible. A good strategy is to\nwrite user journey tests. A user journey test corresponds to a user’s journey through the\nsystem. For example, rather than test create order, revise order, and cancel order sep-\narately, you can write a single test that does all three. This approach significantly\nreduces the number of tests you must write and shortens the test execution time. \nEnd-to-end\nComponent\nIntegration\nUnit\nEnd-to-end\ntest\nTests\nService\nService 1\nService 2\nService ...\nFigure 10.8\nEnd-to-end tests are at the top of the test pyramid. They are slow, brittle, and time \nconsuming to develop. You should minimize the number of end-to-end tests.\n \n\n\n346\nCHAPTER 10\nTesting microservices: Part 2\n10.3.2 Writing end-to-end tests\nEnd-to-end tests are, like the acceptance tests covered in section 10.2, business-facing\ntests. It makes sense to write them in a high-level DSL that’s understood by the busi-\nness people. You can, for example, write the end-to-end tests using Gherkin and exe-\ncute them using Cucumber. The following listing shows an example of such a test. It’s\nsimilar to the acceptance tests we looked at earlier. The main difference is that rather\nthan a single Then, this test has multiple actions.\nFeature: Place Revise and Cancel\nAs a consumer of the Order Service\nI should be able to place, revise, and cancel an order\nScenario: Order created, revised, and cancelled\nGiven a valid consumer\nGiven using a valid credit card\nGiven the restaurant is accepting orders\nWhen I place an order for Chicken Vindaloo at Ajanta\n  \nThen the order should be APPROVED\nThen the order total should be 16.33\nAnd when I revise the order by adding 2 vegetable samosas   \nThen the order total should be 20.97\nAnd when I cancel the order\nThen the order should be CANCELLED\n      \nThis scenario places an order, revises it, and then cancels it. Let’s look at how to run it. \n10.3.3 Running end-to-end tests\nEnd-to-end tests must run the entire application, including any required infrastruc-\nture services. As you saw in earlier in section 10.2, the Gradle Docker Compose plugin\nprovides a convenient way to do this. Instead of running a single application service,\nthough, the Docker Compose file runs all the application’s services.\n Now that we’ve looked at different aspects of designing and writing end-to-end\ntests, let’s see an example end-to-end test.\n The ftgo-end-to-end-test module implements the end-to-end tests for the FTGO\napplication. The implementation of the end-to-end test is quite similar to the imple-\nmentation of the component tests discussed earlier in section 10.2. These tests are\nwritten using Gherkin and executed using Cucumber. The Gradle Docker Compose\nplugin runs the containers before the tests run. It takes around four to five minutes to\nstart the containers and run the tests.\n That may not seem like a long time, but this is a relatively simple application with\njust a handful of containers and tests. Imagine if there were hundreds of containers\nand many more tests. The tests could take quite a long time. Consequently, it’s best to\nfocus on writing tests that are lower down the pyramid. \nListing 10.17\nA Gherkin-based specification of a user journey\nCreate \nOrder.\nRevise \nOrder.\nCancel \nOrder.\n \n\n\n347\nSummary\nSummary\nUse contracts, which are example messages, to drive the testing of interactions\nbetween services. Rather than write slow-running tests that run both services\nand their transitive dependencies, write tests that verify that the adapters of\nboth services conform to the contracts.\nWrite component tests to verify the behavior of a service via its API. You should\nsimplify and speed up component tests by testing a service in isolation, using\nstubs for its dependencies.\nWrite user journey tests to minimize the number of end-to-end tests, which are\nslow, brittle, and time consuming. A user journey test simulates a user’s journey\nthrough the application and verifies high-level behavior of a relatively large\nslice of the application’s functionality. Because there are few tests, the amount\nof per-test overhead, such as test setup, is minimized, which speeds up the tests. \n \n\n\n348\nDeveloping\nproduction-ready services\nMary and her team felt that they had mastered service decomposition, interservice\ncommunication, transaction management, querying and business logic design, and\ntesting. They were confident that they could develop services that met their func-\ntional requirements. But in order for a service to be ready to be deployed into\nproduction, they needed to ensure that it would also satisfy three critically import-\nant quality attributes: security, configurability, and observability.\nThis chapter covers:\nDeveloping secure services\nApplying the Externalized configuration pattern\nApplying the observability patterns:\n– Health check API\n– Log aggregation\n– Distributed tracing\n– Exception tracking\n– Application metrics\n– Audit logging\nSimplifying the development of services by \napplying the Microservice chassis pattern\n \n\n\n349\nDeveloping secure services\n The first quality attribute is application security. It’s essential to develop secure appli-\ncations, unless you want your company to be in the headlines for a data breach. Fortu-\nnately, most aspects of security in a microservice architecture are not any different\nthan in a monolithic application. The FTGO team knew that much of what they had\nlearned over the years developing the monolith also applied to microservices. But the\nmicroservice architecture forces you to implement some aspects of application-level\nsecurity differently. For example, you need to implement a mechanism to pass the\nidentity of the user from one service to another.\n The second quality attribute you must address is service configurability. A service typ-\nically uses one or more external services, such as message brokers and databases. The\nnetwork location and credentials of each external service often depend on the envi-\nronment that the service is running in. You can’t hard-wire the configuration proper-\nties into the service. Instead, you must use an externalized configuration mechanism\nthat provides a service with configuration properties at runtime.\n The third quality attribute is observability. The FTGO team had implemented\nmonitoring and logging for the existing application. But a microservice architecture\nis a distributed system, and that presents some additional challenges. Every request\nis handled by the API gateway and at least one service. Imagine, for example, that\nyou’re trying to determine which of six services is causing a latency issue. Or imag-\nine trying to understand how a request is handled when the log entries are scattered\nacross five different services. In order to make it easier to understand the behavior\nof your application and troubleshoot problems, you must implement several observ-\nability patterns.\n I begin this chapter by describing how to implement security in a microservice\narchitecture. Next, I discuss how to design services that are configurable. I cover a\ncouple of different service configuration mechanisms. After that I talk about how to\nmake your services easier to understand and troubleshoot by using the observability\npatterns. I end the chapter by showing how to simplify the implementation of these\nand other concerns by developing your services on top of a microservice chassis\nframework.\n Let’s first look at security.\n11.1\nDeveloping secure services\nCybersecurity has become a critical issue for every organization. Almost every day\nthere are headlines about how hackers have stolen a company’s data. In order to\ndevelop secure software and stay out of the headlines, an organization needs to\ntackle a diverse range of security issues, including physical security of the hardware,\nencryption of data in transit and at rest, authentication and authorization, and pol-\nicies for patching software vulnerabilities. Most of these issues are the same regard-\nless of whether you’re using a monolithic or microservice architecture. This section\nfocuses on how the microservice architecture impacts security at the application\nlevel.\n \n\n\n350\nCHAPTER 11\nDeveloping production-ready services\n An application developer is primarily responsible for implementing four different\naspects of security:\nAuthentication—Verifying the identity of the application or human (a.k.a. the\nprincipal) that’s attempting to access the application. For example, an applica-\ntion typically verifies a principal’s credentials, such as a user ID and password or\nan application’s API key and secret.\nAuthorization—Verifying that the principal is allowed to perform the requested\noperation on the specified data. Applications often use a combination of role-\nbased security and access control lists (ACLs). Role-based security assigns each\nuser one or more roles that grant them permission to invoke particular opera-\ntions. ACLs grant users or roles permission to perform an operation on a partic-\nular business object, or aggregate.\nAuditing—Tracking the operations that a principal performs in order to detect\nsecurity issues, help customer support, and enforce compliance.\nSecure interprocess communication—Ideally, all communication in and out of ser-\nvices should be over Transport Layer Security (TLS). Interservice communica-\ntion may even need to use authentication.\nI describe auditing in detail in section 11.3 and touch on securing interservice com-\nmunication when discussing service meshes in section 11.4.1. This section focuses on\nimplementing authentication and authorization.\n I begin by first describing how security is implemented in the FTGO monolith\napplication. I then describe the challenges with implementing security in a microser-\nvice architecture and how techniques that work well in a monolithic architecture can’t\nbe used in a microservice architecture. After that I cover how to implement security in\na microservice architecture.\n Let’s start by reviewing how the monolithic FTGO application handles security.\n11.1.1 Overview of security in a traditional monolithic application\nThe FTGO application has several kinds of human users, including consumers, cou-\nriers, and restaurant staff. They access the application using browser-based web\napplications and mobile applications. All FTGO users must log in to access the appli-\ncation. Figure 11.1 shows how the clients of the monolithic FTGO application authen-\nticate and make requests.\n When a user logs in with their user ID and password, the client makes a POST\nrequest containing the user’s credentials to the FTGO application. The FTGO appli-\ncation verifies the credentials and returns a session token to the client. The client\nincludes the session token in each subsequent request to the FTGO application.\n Figure 11.2 shows a high-level view of how the FTGO application implements secu-\nrity. The FTGO application is written in Java and uses the Spring Security framework,\nbut I’ll describe the design using generic terms that are applicable to other frame-\nworks, such as Passport for NodeJS.\n \n\n\n351\nDeveloping secure services\nOne key part of the security architecture is the session, which stores the principal’s ID\nand roles. The FTGO application is a traditional Java EE application, so the session is\nan HttpSession in-memory session. A session is identified by a session token, which the\nclient includes in each request. It’s usually an opaque token such as a cryptographi-\ncally strong random number. The FTGO application’s session token is an HTTP\ncookie called JSESSIONID.\n The other key part of the security implementation is the security context, which\nstores information about the user making the current request. The Spring Security\nUsing a security framework\nImplementing authentication and authorization correctly is challenging. It’s best to\nuse a proven security framework. Which framework to use depends on your applica-\ntion’s technology stack. Some popular frameworks include the following:\nSpring Security (https://projects.spring.io/spring-security/)—A popular frame-\nwork for Java applications. It’s a sophisticated framework that handles authen-\ntication and authorization.\nApache Shiro (https://shiro.apache.org)—Another Java framework.\nPassport (http://www.passportjs.org)—A popular security framework for NodeJS\napplications that’s focused on authentication.\nLog in to obtain session\ntoken, which is a cookie.\nInclude session token cookie,\nwhich identiﬁes the user, in\nsubsequent requests.\nConsumer\nrestaurant\ncourier\nBrowser\nor mobile\napplication\nPOST /login\nid=...\npassword=...\nHTTP/1.1 200 OK\nSet-cookie: JSESSIONID=...\n...\nGET /orders/order-xyz\nCookie: JSESSIONID=...\nFTGO\napplication\nFigure 11.1\nA client of the FTGO application first logs in to obtain a session token, which is often a \ncookie. The client includes the session token in each subsequent request it makes to the application.\n \n\n\n352\nCHAPTER 11\nDeveloping production-ready services\nframework uses the standard Java EE approach of storing the security context in a\nstatic, thread-local variable, which is readily accessible to any code that’s invoked to han-\ndle the request. A request handler can call SecurityContextHolder.getContext()\n.getAuthentication() to obtain information about the current user, such as their\nidentity and roles. In contrast, the Passport framework stores the security context as\nthe user attribute of the request.\nThe sequence of events shown in Figure 11.2 is as follows:\n1\nThe client makes a login request to the FTGO application.\n2\nThe login request is handled by LoginHandler, which verifies the credentials, cre-\nates the session, and stores information about the principal in the session.\n3\nLogin Handler returns a session token to the client.\n4\nThe client includes the session token in requests that invoke operations.\n5\nThese requests are first processed by SessionBasedSecurityInterceptor. The\ninterceptor authenticates each request by verifying the session token and estab-\nlishes a security context. The security context describes the principal and its roles.\nUser\ndatabase\nLog in with user ID\nand password.\nInitializes\nProvides session cookie\nEstablishes\nReads\nReturn session cookie.\nJane\nLogin-based\nclient\nSessionBased\nSecurity\nInterceptor\nOrderDetails\nRequestHandler\nUserId: jane\nrules: [CONSUMER]\n...\nUserId: jane\nrules: [CONSUMER]\n...\nLogin\nhandler\nPOST /login\nuserId-Jane&password=..\nHTTP/1.1 200 OK\nSet-cookie: JSESSIONID=...\n...\nGET /orders/order-xyz\nCookie: JSESSIONID=...\nFTGO application\nRetrieves user information\nfrom database\nReads\nEstablishes\nSecurity context\nSession\nFigure 11.2\nWhen a client of the FTGO application makes a login request, Login Handler authenticates the \nuser, initializes the session user information, and returns a session token cookie, which securely identifies the \nsession. Next, when the client makes a request containing the session token, SessionBasedSecurity-\nInterceptor retrieves the user information from the specified session and establishes the security context. \nRequest handlers, such as OrderDetailsRequestHandler, retrieve the user information from the security \ncontext.\n \n",
      "page_number": 369
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 378-394)",
      "start_page": 378,
      "end_page": 394,
      "detection_method": "topic_boundary",
      "content": "353\nDeveloping secure services\n6\nA request handler uses the security context to determine whether to allow a\nuser to perform the requested operation and obtain their identity.\nThe FTGO application uses role-based authorization. It defines several roles corre-\nsponding to the different kinds of users, including CONSUMER, RESTAURANT, COURIER,\nand ADMIN. It uses Spring Security’s declarative security mechanism to restrict access to\nURLs and service methods to specific roles. Roles are also interwoven into the busi-\nness logic. For example, a consumer can only access their orders, whereas an adminis-\ntrator can access all orders.\n The security design used by the monolithic FTGO application is only one possible\nway to implement security. For example, one drawback of using an in-memory session\nis that it requires all requests for a particular session to be routed to the same applica-\ntion instance. This requirement complicates load balancing and operations. You must,\nfor example, implement a session draining mechanism that waits for all sessions to\nexpire before shutting down an application instance. An alternative approach, which\navoids these problems, is to store the session in a database.\n You can sometimes eliminate the server-side session entirely. For example, many\napplications have API clients that provide their credentials, such as an API key and\nsecret, in every request. As a result, there’s no need to maintain a server-side session.\nAlternatively, the application can store session state in the session token. Later in\nthis section, I describe one way to use a session token to store the session state. But\nlet’s begin by looking at the challenges of implementing security in a microservice\narchitecture. \n11.1.2 Implementing security in a microservice architecture\nA microservice architecture is a distributed architecture. Each external request is han-\ndled by the API gateway and at least one service. Consider, for example, the get-\nOrderDetails() query, discussed in chapter 8. The API gateway handles this query by\ninvoking several services, including Order Service, Kitchen Service, and Accounting\nService. Each service must implement some aspects of security. For instance, Order\nService must only allow a consumer to see their orders, which requires a combina-\ntion of authentication and authorization. In order to implement security in a micros-\nervice architecture we need to determine who is responsible for authenticating the\nuser and who is responsible for authorization.\n One challenge with implementing security in a microservices application is that we\ncan’t just copy the design from a monolithic application. That’s because two aspects of\nthe monolithic application’s security architecture are nonstarters for a microservice\narchitecture:\nIn-memory security context—Using an in-memory security context, such as a thread-\nlocal, to pass around user identity. Services can’t share memory, so they can’t\nuse an in-memory security context, such as a thread-local, to pass around the\n \n\n\n354\nCHAPTER 11\nDeveloping production-ready services\nuser identity. In a microservice architecture, we need a different mechanism for\npassing user identity from one service to another.\nCentralized session —Because an in-memory security context doesn’t make sense,\nneither does an in-memory session. In theory, multiple services could access a\ndatabase-based session, except that it would violate the principle of loose cou-\npling. We need a different session mechanism in a microservice architecture.\nLet’s begin our exploration of security in a microservice architecture by looking at\nhow to handle authentication.\nHANDLING AUTHENTICATION IN THE API GATEWAY\nThere are a couple of different ways to handle authentication. One option is for the\nindividual services to authenticate the user. The problem with this approach is that it\npermits unauthenticated requests to enter the internal network. It relies on every\ndevelopment team correctly implementing security in all of their services. As a result,\nthere’s a significant risk of an application containing security vulnerabilities.\n Another problem with implementing authentication in the services is that differ-\nent clients authenticate in different ways. Pure API clients supply credentials with\neach request using, for example, basic authentication. Other clients might first log in\nand then supply a session token with each request. We want to avoid requiring services\nto handle a diverse set of authentication mechanisms.\n A better approach is for the API gateway to authenticate a request before forward-\ning it to the services. Centralizing API authentication in the API gateway has the\nadvantage that there’s only one place to get right. As a result, there’s a much smaller\nchance of a security vulnerability. Another benefit is that only the API gateway has to\ndeal with the various different authentication mechanisms. It hides this complexity\nfrom the services.\n Figure 11.3 shows how this approach works. Clients authenticate with the API gate-\nway. API clients include credentials in each request. Login-based clients POST the\nuser’s credentials to the API gateway’s authentication and receive a session token.\nOnce the API gateway has authenticated a request, it invokes one or more services.\nA service invoked by the API gateway needs to know the principal making the request.\nIt must also verify that the request has been authenticated. The solution is for the API\ngateway to include a token in each service request. The service uses the token to vali-\ndate the request and obtain information about the principal. The API gateway might\nalso give the same token to session-oriented clients to use as the session token.\nPattern: Access token\nThe API gateway passes a token containing information about the user, such as their\nidentity and their roles, to the services that it invokes. See http://microservices.io/\npatterns/security/access-token.html.\n \n\n\n355\nDeveloping secure services\nThe sequence of events for API clients is as follows:\n1\nA client makes a request containing credentials.\n2\nThe API gateway authenticates the credentials, creates a security token, and\npasses that to the service or services.\nThe sequence of events for login-based clients is as follows:\n1\nA client makes a login request containing credentials.\n2\nThe API gateway returns a security token.\n3\nThe client includes the security token in requests that invoke operations.\n4\nThe API gateway validates the security token and forwards it to the service or\nservices.\nA little later in this chapter, I describe how to implement tokens, but let’s first look at\nthe other main aspect of security: authorization. \nOrder\nService\nAPI clients supply credentials\nin the Authorization header.\nPass token to services so\nthat they can identify and\nauthorize the user.\nInclude the security token\nin each request.\nLogin clients ﬁrst obtain\na security token.\nAuthentication\nInterceptor\nAPI gateway\nLogin-based\nclient\nGET /orders/1\nAuthorization: ...CREDENTIALS...\n...\nGET /orders/1\n...SECURITY_TOKEN...\nHTTP/1.1 200 OK\n...SECURITY_TOKEN...\nGET /orders/1\n...SECURITY_TOKEN...\nPOST /login\nid=...\npassword=...\nAPI client\nFigure 11.3\nThe API gateway authenticates requests from clients and includes a security token in the requests \nit makes to services. The services use the token to obtain information about the principal. The API gateway can \nalso use the security token as a session token.\n \n\n\n356\nCHAPTER 11\nDeveloping production-ready services\nHANDLING AUTHORIZATION\nAuthenticating a client’s credentials is important but insufficient. An application\nmust also implement an authorization mechanism that verifies that the client is\nallowed to perform the requested operation. For example, in the FTGO application\nthe getOrderDetails() query can only be invoked by the consumer who placed the\nOrder (an example of instance-based security) and a customer service agent who is\nhelping the consumer.\n One place to implement authorization is the API gateway. It can, for example,\nrestrict access to GET /orders/{orderId} to only users who are consumers and cus-\ntomer service agents. If a user isn’t allowed to access a particular path, the API gateway\ncan reject the request before forwarding it on to the service. As with authentication,\ncentralizing authorization within the API gateway reduces the risk of security vulnera-\nbilities. You can implement authorization in the API gateway using a security frame-\nwork, such as Spring Security.\n One drawback of implementing authorization in the API gateway is that it risks\ncoupling the API gateway to the services, requiring them to be updated in lockstep.\nWhat’s more, the API gateway can typically only implement role-based access to URL\npaths. It’s generally not practical for the API gateway to implement ACLs that control\naccess to individual domain objects, because that requires detailed knowledge of a ser-\nvice’s domain logic.\n The other place to implement authorization is in the services. A service can imple-\nment role-based authorization for URLs and for service methods. It can also implement\nACLs to manage access to aggregates. Order Service can, for example, implement the\nrole-based and ACL-based authorization mechanism for controlling access to orders.\nOther services in the FTGO application implement similar authorization logic. \nUSING JWTS TO PASS USER IDENTITY AND ROLES\nWhen implementing security in a microservice architecture, you need to decide which\ntype of token an API gateway should use to pass user information to the services.\nThere are two types of tokens to choose from. One option is to use opaque tokens,\nwhich are typically UUIDs. The downside of opaque tokens is that they reduce perfor-\nmance and availability and increase latency. That’s because the recipient of such a\ntoken must make a synchronous RPC call to a security service to validate the token\nand retrieve the user information.\n An alternative approach, which eliminates the call to the security service, is to use a\ntransparent token containing information about the user. One such popular standard\nfor transparent tokens is the JSON Web Token (JWT). JWT is standard way to securely\nrepresent claims, such as user identity and roles, between two parties. A JWT has a pay-\nload, which is a JSON object that contains information about the user, such as their\nidentity and roles, and other metadata, such as an expiration date. It’s signed with a\nsecret that’s only known to the creator of the JWT, such as the API gateway and the\nrecipient of the JWT, such as a service. The secret ensures that a malicious third party\ncan’t forge or tamper with a JWT.\n \n\n\n357\nDeveloping secure services\n One issue with JWT is that because a token is self-contained, it’s irrevocable. By\ndesign, a service will perform the request operation after verifying the JWT’s signature\nand expiration date. As a result, there’s no practical way to revoke an individual JWT\nthat has fallen into the hands of a malicious third party. The solution is to issue JWTs\nwith short expiration times, because that limits what a malicious party could do. One\ndrawback of short-lived JWTs, though, is that the application must somehow continually\nreissue JWTs to keep the session active. Fortunately, this is one of the many protocols\nthat are solved by a security standard calling OAuth 2.0. Let’s look at how that works. \nUSING OAUTH 2.0 IN A MICROSERVICE ARCHITECTURE\nLet’s say you want to implement a User Service for the FTGO application that man-\nages a user database containing user information, such as credentials and roles. The\nAPI gateway calls the User Service to authenticate a client request and obtain a JWT.\nYou could design a User Service API and implement it using your favorite web frame-\nwork. But that’s generic functionality that isn’t specific to the FTGO application—\ndeveloping such a service wouldn’t be an efficient use of development resources.\n Fortunately, you don’t need to develop this kind of security infrastructure. You can\nuse an off-the-shelf service or framework that implements a standard called OAuth 2.0.\nOAuth 2.0 is an authorization protocol that was originally designed to enable a user of\na public cloud service, such as GitHub or Google, to grant a third-party application\naccess to its information without revealing its password. For example, OAuth 2.0 is the\nmechanism that enables you to securely grant a third party cloud-based Continuous\nIntegration (CI) service access to your GitHub repository.\n Although the original focus of OAuth 2.0 was authorizing access to public cloud\nservices, you can also use it for authentication and authorization in your application.\nLet’s take a quick look at how a microservice architecture might use OAuth 2.0.\nThe key concepts in OAuth 2.0 are the following:\n\nAuthorization Server—Provides an API for authenticating users and obtain-\ning an access token and a refresh token. Spring OAuth is a great example of a\nframework for building an OAuth 2.0 authorization server.\n\nAccess Token—A token that grants access to a Resource Server. The format of\nthe access token is implementation dependent. But some implementations,\nsuch as Spring OAuth, use JWTs.\nAbout OAuth 2.0\nOAuth 2.0 is a complex topic. In this chapter, I can only provide a brief overview and\ndescribe how it can be used in a microservice architecture. For more information\non OAuth 2.0, check out the online book OAuth 2.0 Servers by Aaron Parecki\n(www.oauth.com). Chapter 7 of Spring Microservices in Action (Manning, 2017) also\ncovers this topic (https://livebook.manning.com/#!/book/spring-microservices-in-\naction/chapter-7/).\n \n\n\n358\nCHAPTER 11\nDeveloping production-ready services\n\nRefresh Token—A long-lived yet revocable token that a Client uses to obtain a\nnew AccessToken.\n\nResource Server—A service that uses an access token to authorize access. In a\nmicroservice architecture, the services are resource servers.\n\nClient—A client that wants to access a Resource Server. In a microservice\narchitecture, API Gateway is the OAuth 2.0 client.\nLater in this section, I describe how to support login-based clients. But first, let’s talk\nabout how to authenticate API clients.\n Figure 11.4 shows how the API gateway authenticates a request from an API client.\nThe API gateway authenticate the API client by making a request to the OAuth 2.0\nauthorization server, which returns an access token. The API gateway then makes one\nor more requests containing the access token to the services.\n The sequence of events shown in figure 11.4 is as follows:\n1\nThe client makes a request, supplying its credentials using basic authentication.\n2\nThe API gateway makes an OAuth 2.0 Password Grant request (www.oauth.com/\noauth2-servers/access-tokens/password-grant/) to the OAuth 2.0 authentication\nserver.\nOrder\nService\nUser\ndatabase\nContains the user\nID and their roles\nPassword grant request\nAPI gateway\nSpring OAuth2\nAuthentication\nServer\nGET /orders/1\nAuthorization: Basic...\n....\nPOST/oauth/token\nuserid=...&password=...\nGET /orders/1\nAuthorization: Bearer AccessToken\nHTTP/1.1 200 OK\n...\n{\n\"access_token\": \"AccessToken\"\n...\n}\nAPI client\nFigure 11.4\nAn API gateway authenticates an API client by making a Password Grant request to the OAuth 2.0 \nauthentication server. The server returns an access token, which the API gateway passes to the services. A service \nverifies the token’s signature and extracts information about the user, including their identity and roles.\n \n\n\n359\nDeveloping secure services\n3\nThe authentication server validates the API client’s credentials and returns an\naccess token and a refresh token.\n4\nThe API gateway includes the access token in the requests it makes to the ser-\nvices. A service validates the access token and uses it to authorize the request.\nAn OAuth 2.0-based API gateway can authenticate session-oriented clients by using an\nOAuth 2.0 access token as a session token. What’s more, when the access token\nexpires, it can obtain a new access token using the refresh token. Figure 11.5 shows\nhow an API gateway can use OAuth 2.0 to handle session-oriented clients. An API cli-\nent initiates a session by POSTing its credentials to the API gateway’s /login end-\npoint. The API gateway returns an access token and a refresh token to the client. The\nAPI client then supplies both tokens when it makes requests to the API gateway.\nThe sequence of events is as follows:\n1\nThe login-based client POSTs its credentials to the API gateway.\n2\nThe API gateway’s Login Handler makes an OAuth 2.0 Password Grant request\n(www.oauth.com/oauth2-servers/access-tokens/password-grant/) to the OAuth\n2.0 authentication server.\nOrder\nService\nUser\ndatabase\nPassword grant request\nAPI gateway\nSpring OAuth2\nAuthentication\nServer\nPOST/login\nuserId=...&password=...\nGET/orders/1\nCookie: access_token=...;refresh_token...\nHTTP/1.1 200 OK\nSet-Cookie: access_token=...\nSet-Cookie:refresh_token=...\nPOST/oauth/token\nuserid=...&password=...\nGET /orders/1\nAuthorization: Bearer AccessToken\nHTTP/1.1 200 OK\n...\n{\n\"access_token\": \"AccessToken\"\n...\n}\nLogin-based\nclient\nLogin\nhandler\nSession\nauthentication\ninterceptor\nFigure 11.5\nA client logs in by POSTing its credentials to the API gateway. The API gateway authenticates the \ncredentials using the OAuth 2.0 authentication server and returns the access token and refresh token as cookies. \nA client includes these tokens in the requests it makes to the API gateway.\n \n\n\n360\nCHAPTER 11\nDeveloping production-ready services\n3\nThe authentication server validates the client’s credentials and returns an access\ntoken and a refresh token.\n4\nThe API gateway returns the access and refresh tokens to the client—as cookies,\nfor example.\n5\nThe client includes the access and refresh tokens in requests it makes to the API\ngateway.\n6\nThe API gateway’s Session Authentication Interceptor validates the access\ntoken and includes it in requests it makes to the services.\nIf the access token has expired or is about to expire, the API gateway obtains a new\naccess token by making an OAuth 2.0 Refresh Grant request (www.oauth.com/\noauth2-servers/access-tokens/refreshing-access-tokens/), which contains the refresh\ntoken, to the authorization server. If the refresh token hasn’t expired or been revoked,\nthe authorization server returns a new access token. API Gateway passes the new\naccess token to the services and returns it to the client.\n An important benefit of using OAuth 2.0 is that it’s a proven security standard.\nUsing an off-the-shelf OAuth 2.0 Authentication Server means you don’t have to\nwaste time reinventing the wheel or risk developing an insecure design. But OAuth\n2.0 isn’t the only way to implement security in a microservice architecture. Regardless\nof which approach you use, the three key ideas are as follows:\nThe API gateway is responsible for authenticating clients.\nThe API gateway and the services use a transparent token, such as a JWT, to pass\naround information about the principal.\nA service uses the token to obtain the principal’s identity and roles.\nNow that we’ve looked at how to make services secure, let’s see how to make them\nconfigurable. \n11.2\nDesigning configurable services\nImagine that you’re responsible for Order History Service. As figure 11.6 shows, the\nservice consumes events from Apache Kafka and reads and writes AWS DynamoDB\ntable items. In order for this service to run, it needs various configuration properties,\nincluding the network location of Apache Kafka and the credentials and network loca-\ntion for AWS DynamoDB.\n The values of these configuration properties depend on which environment the\nservice is running in. For example, the developer and production environments will\nuse different Apache Kafka brokers and different AWS credentials. It doesn’t make\nsense to hard-wire a particular environment’s configuration property values into the\ndeployable service because that would require it to be rebuilt for each environment.\nInstead, a service should be built once by the deployment pipeline and deployed into\nmultiple environments.\n Nor does it make sense to hard-wire different sets of configuration properties into\nthe source code and use, for example, the Spring Framework’s profile mechanism to\n \n\n\n361\nDesigning configurable services\nselect the appropriate set at runtime. That’s because doing so would introduce a secu-\nrity vulnerability and limit where it can be deployed. Additionally, sensitive data such\nas credentials should be stored securely using a secrets storage mechanism, such as\nHashicorp Vault (www.vaultproject.io) or AWS Parameter Store (https://docs.aws\n.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html).\nInstead, you should supply the appropriate configuration properties to the service at\nruntime by using the Externalized configuration pattern.\nAn externalized configuration mechanism provides the configuration property values\nto a service instance at runtime. There are two main approaches:\nPush model—The deployment infrastructure passes the configuration properties\nto the service instance using, for example, operating system environment vari-\nables or a configuration file.\nPull model—The service instance reads its configuration properties from a con-\nfiguration server.\nWe’ll look at each approach, starting with the push model.\nPattern: Externalized configuration\nSupply configuration property values, such as database credentials and network\nlocation, to a service at runtime. See http://microservices.io/patterns/externalized-\nconfiguration.html.\nOrder\nHistory\nService\nEnvironment-speciﬁc conﬁguration\nEnvironment-speciﬁc conﬁguration\nApache\nKafka\nconsumer\nApache Kafka\nbootstrap.servers=kafka1:9092\n..\naws.access.key.id=...\naws.secret.access.key=...\naws.region=...\n«Order event channel»\nDynamoDB\nadapter\nAWS DynamoDB\n«Delivery event channel»\nFigure 11.6\nOrder History Service uses Apache Kafka and AWS DynamoDB. It needs to be \nconfigured with each service’s network location, credentials, and so on.\n \n\n\n362\nCHAPTER 11\nDeveloping production-ready services\n11.2.1 Using push-based externalized configuration\nThe push model relies on the collaboration of the deployment environment and the\nservice. The deployment environment supplies the configuration properties when it\ncreates a service instance. It might, as figure 11.7 shows, pass the configuration prop-\nerties as environment variables. Alternatively, the deployment environment may sup-\nply the configuration properties using a configuration file. The service instance then\nreads the configuration properties when it starts up.\nThe deployment environment and the service must agree on how the configuration\nproperties are supplied. The precise mechanism depends on the specific deployment\nenvironment. For example, chapter 12 describes how you can specify the environment\nvariables of a Docker container.\n Let’s imagine that you’ve decided to supply externalized configuration property\nvalues using environment variables. Your application could call System.getenv() to\nobtain their values. But if you’re a Java developer, it’s likely that you’re using a frame-\nwork that provides a more convenient mechanism. The FTGO services are built using\nSpring Boot, which has an extremely flexible externalized configuration mechanism\nthat retrieves configuration properties from a variety of sources with well-defined pre-\ncedence rules (https://docs.spring.io/spring-boot/docs/current/reference/html/boot-\nfeatures-external-config.html). Let’s look at how it works.\n Spring Boot reads properties from a variety of sources. I find the following sources\nuseful in a microservice architecture:\n \nOrder\nHistory Service\ninstance\nProcess\nEnvironment variables\nDeployment\ninfrastructure\nConﬁgures\nCreates\nReads\nBOOTSTRAP_SERVERS=kafka1:9092\nAWS_ACCESS_KEY_ID=\nAWS_SECRET_ACCESS_KEY=...\nAWS_REGION=...\n....\nFigure 11.7\nWhen the deployment infrastructure creates an instance of Order History \nService, it sets the environment variables containing the externalized configuration. Order \nHistory Service reads those environment variables.\n \n\n\n363\nDesigning configurable services\n1\nCommand-line arguments\n2\nSPRING_APPLICATION_JSON, an operating system environment variable or JVM\nsystem property that contains JSON\n3\nJVM System properties\n4\nOperating system environment variables\n5\nA configuration file in the current directory\nA particular property value from a source earlier in this list overrides the same prop-\nerty from a source later in this list. For example, operating system environment vari-\nables override properties read from a configuration file.\n Spring Boot makes these properties available to the Spring Framework’s\nApplicationContext. A service can, for example, obtain the value of a property using\nthe @Value annotation:\npublic class OrderHistoryDynamoDBConfiguration {\n@Value(\"${aws.region}\")\nprivate String awsRegion;\nThe Spring Framework initializes the awsRegion field to the value of the aws.region\nproperty. This property is read from one of the sources listed earlier, such as a config-\nuration file or from the AWS_REGION environment variable.\n The push model is an effective and widely used mechanism for configuring a ser-\nvice. One limitation, however, is that reconfiguring a running service might be chal-\nlenging, if not impossible. The deployment infrastructure might not allow you to\nchange the externalized configuration of a running service without restarting it. You\ncan’t, for example, change the environment variables of a running process. Another\nlimitation is that there’s a risk of the configuration property values being scattered\nthroughout the definition of numerous services. As a result, you may want to consider\nusing a pull-based model. Let’s look at how it works. \n11.2.2 Using pull-based externalized configuration\nIn the pull model, a service instance reads its configuration properties from a configura-\ntion server. Figure 11.8 shows how it works. On startup, a service instance queries the\nconfiguration service for its configuration. The configuration properties for accessing\nthe configuration server, such as its network location, are provided to the service\ninstance via a push-based configuration mechanism, such as environment variables.\n There are a variety of ways to implement a configuration server, including the\nfollowing:\nVersion control system such as Git\nSQL and NoSQL databases\nSpecialized configuration servers, such as Spring Cloud Config Server, Hashicorp\nVault, which is a store for sensitive data such as credentials, and AWS Parameter\nStore\n \n\n\n364\nCHAPTER 11\nDeveloping production-ready services\nThe Spring Cloud Config project is a good example of a configuration server-based\nframework. It consists of a server and a client. The server supports a variety of backends\nfor storing configuration properties, including version control systems, databases, and\nHashicorp Vault. The client retrieves configuration properties from the server and\ninjects them into the Spring ApplicationContext.\n Using a configuration server has several benefits:\nCentralized configuration—All the configuration properties are stored in one\nplace, which makes them easier to manage. What’s more, in order to eliminate\nduplicate configuration properties, some implementations let you define global\ndefaults, which can be overridden on a per-service basis.\nTransparent decryption of sensitive data—Encrypting sensitive data such as database\ncredentials is a security best practice. One challenge of using encryption, though,\nis that usually the service instance needs to decrypt them, which means it needs\nthe encryption keys. Some configuration server implementations automatically\ndecrypt properties before returning them to the service.\nDynamic reconfiguration—A service could potentially detect updated property val-\nues by, for example, polling, and reconfigure itself.\nThe primary drawback of using a configuration server is that unless it’s provided by\nthe infrastructure, it’s yet another piece of infrastructure that needs to be set up and\nmaintained. Fortunately, there are various open source frameworks, such as Spring\nCloud Config, which make it easier to run a configuration server.\n Now that we’ve looked at how to design configurable services, let’s talk about how\nto design observable services. \n11.3\nDesigning observable services\nLet’s say you’ve deployed the FTGO application into production. You probably want\nto know what the application is doing: requests per second, resource utilization, and\nOrder\nHistory Service\ninstance\nProcess\nConﬁgures\nCreates\nCONFIG_SERVER_URL=...\ngetConﬁguration(“orderHistoryService”)\nBOOTSTRAP_SERVERS=kafka1:9092\nAWS_ACCESS_KEY_ID=\nAWS_SECRET_ACCESS_KEY=...\nAWS_REGION=...\n....\nEnvironment variables\nDeployment\ninfrastructure\nConﬁguration\nserver\nFigure 11.8\nOn startup, a service instance retrieves its configuration properties from a configuration server. The \ndeployment infrastructure provides the configuration properties for accessing the configuration server.\n \n\n\n365\nDesigning observable services\nso on. You also need to be alerted if there’s a problem, such as a failed service instance\nor a disk filling up—ideally before it impacts a user. And, if there’s a problem, you\nneed to be able to troubleshoot and identify the root cause.\n Many aspects of managing an application in production are outside the scope of\nthe developer, such as monitoring hardware availability and utilization. These are\nclearly the responsibility of operations. But there are several patterns that you, as a ser-\nvice developer, must implement to make your service easier to manage and trouble-\nshoot. These patterns, shown in figure 11.9, expose a service instance’s behavior and\nhealth. They enable a monitoring system to track and visualize the state of a service\nand generate alerts when there’s a problem. These patterns also make troubleshoot-\ning problems easier.\nYou can use the following patterns to design observable services:\nHealth check API—Expose an endpoint that returns the health of the service.\nLog aggregation—Log service activity and write logs into a centralized logging\nserver, which provides searching and alerting.\nPattern\nparticipant\nKey\nOperations\nresponsibility\nDistributed\ntracing\nserver\nException\nTracking\nService\nLogging\nServer\nLogging\naggregation\npipeline\nLog ﬁle\nMetrics\nService\nDeveloper\nresponsibility\nPattern\nObservable\nService\nDistributed\ntracing\nexporter\nException\nreporter\nMetrics\nexporter\nHealth\ncheck\nAPI\nHealth check\ninvoker, such as\nmonitoring service\nInvokes\nAudit\ndatabase\nadapter\nAuditing\ndatabase\nLogging\nadapter\nDistributed\ntracing pattern\nApplication\nmetrics pattern\nAudit\nlogging pattern\nHealth check\nAPI pattern\nException\ntracking pattern\nLog aggregation\npattern\nFigure 11.9\nThe observability patterns enable developers and operations to understand the behavior of an \napplication and troubleshoot problems. Developers are responsible for ensuring that their services are observable. \nOperations are responsible for the infrastructure that collects the information exposed by the services.\n \n\n\n366\nCHAPTER 11\nDeveloping production-ready services\nDistributed tracing—Assign each external request a unique ID and trace requests\nas they flow between services.\nException tracking—Report exceptions to an exception tracking service, which\nde-duplicates exceptions, alerts developers, and tracks the resolution of each\nexception.\nApplication metrics—Services maintain metrics, such as counters and gauges, and\nexpose them to a metrics server.\nAudit logging—Log user actions.\nA distinctive feature of most of these patterns is that each pattern has a developer\ncomponent and an operations component. Consider, for example, the Health check\nAPI pattern. The developer is responsible for ensuring that their service implements a\nhealth check endpoint. Operations is responsible for the monitoring system that peri-\nodically invokes the health check API. Similarly, for the Log aggregation pattern, a\ndeveloper is responsible for ensuring that their services log useful information,\nwhereas operations is responsible for log aggregation.\n Let’s take a look at each of these patterns, starting with the Health check API pattern.\n11.3.1 Using the Health check API pattern\nSometimes a service may be running but unable to handle requests. For instance, a\nnewly started service instance may not be ready to accept requests. The FTGO Con-\nsumer Service, for example, takes around 10 seconds to initialize the messaging and\ndatabase adapters. It would be pointless for the deployment infrastructure to route\nHTTP requests to a service instance until it’s ready to process them.\n Also, a service instance can fail without terminating. For example, a bug might\ncause an instance of Consumer Service to run out of database connections and\nbe unable to access the database. The deployment infrastructure shouldn’t route\nrequests to a service instance that has failed yet is still running. And, if the service\ninstance does not recover, the deployment infrastructure must terminate it and create\na new instance.\nA service instance needs to be able to tell the deployment infrastructure whether or\nnot it’s able to handle requests. A good solution is for a service to implement a health\ncheck endpoint, which is shown in figure 11.10. The Spring Boot Actuator Java library,\nfor example, implements a GET /actuator/health endpoint, which returns 200 if and\nonly if the service is healthy, and 503 otherwise. Similarly, the HealthChecks .NET\nPattern: Health check API\nA service exposes a health check API endpoint, such as GET /health, which returns\nthe health of the service. See http://microservices.io/patterns/observability/health-\ncheck-api.html.\n \n\n\n367\nDesigning observable services\nlibrary implements a GET /hc endpoint (https://docs.microsoft.com/en-us/dotnet/\nstandard/microservices-architecture/implement-resilient-applications/monitor-app-\nhealth). The deployment infrastructure periodically invokes this endpoint to determine\nthe health of the service instance and takes the appropriate action if it’s unhealthy.\nA Health Check Request Handler typically tests the service instance’s connections to\nexternal services. It might, for example, execute a test query against a database. If all\nthe tests succeed, Health Check Request Handler returns a healthy response, such as\nan HTTP 200 status code. If any of them fails, it returns an unhealthy response, such\nas an HTTP 500 status code.\n Health Check Request Handler might simply return an empty HTTP response with\nthe appropriate status code. Or it might return a detailed description of the health of\neach of the adapters. The detailed information is useful for troubleshooting. But\nbecause it may contain sensitive information, some frameworks, such as Spring Boot\nActuator, let you configure the level of detail in the health endpoint response.\n There are two issues you need to consider when using health checks. The first is\nthe implementation of the endpoint, which must report back on the health of the ser-\nvice instance. The second issue is how to configure the deployment infrastructure to\ninvoke the health check endpoint. Let’s first look at how to implement the endpoint.\nIMPLEMENTING THE HEALTH CHECK ENDPOINT\nThe code that implements the health check endpoint must somehow determine the\nhealth of the service instance. One simple approach is to verify that the service\ninstance can access its external infrastructure services. How to do this depends on the\nService\nChecks\nChecks\nHealth check\ninvoker\nInvokes\nHealth check\nendpoint\nHealth check\nrequest\nhandler\nMessaging\nadapter\nMessage\nbroker\nDatabase\nadapter\nMySQL\nTests the service’s connections\nto infrastructure services\nFor example: monitoring\nsystem, Service registry, and others\nFigure 11.10\nA service implements a health check endpoint, which is periodically invoked by the \ndeployment infrastructure to determine the health of the service instance.\n \n\n\n368\nCHAPTER 11\nDeveloping production-ready services\ninfrastructure service. The health check code can, for example, verify that it’s con-\nnected to an RDBMS by obtaining a database connection and executing a test query.\nA more elaborate approach is to execute a synthetic transaction that simulates the\ninvocation of the service’s API by a client. This kind of health check is more thorough,\nbut it’s likely to be more time consuming to implement and take longer to execute.\n A great example of a health check library is Spring Boot Actuator. As mentioned\nearlier, it implements a /actuator/health endpoint. The code that implements this\nendpoint returns the result of executing a set of health checks. By using convention\nover configuration, Spring Boot Actuator implements a sensible set of health checks\nbased on the infrastructure services used by the service. If, for example, a service uses\na JDBC DataSource, Spring Boot Actuator configures a health check that executes a\ntest query. Similarly, if the service uses the RabbitMQ message broker, it automatically\nconfigures a health check that verifies that the RabbitMQ server is up.\n You can also customize this behavior by implementing additional health checks for\nyour service. You implement a custom health check by defining a class that imple-\nments the HealthIndicator interface. This interface defines a health() method,\nwhich is called by the implementation of the /actuator/health endpoint. It returns\nthe outcome of the health check. \nINVOKING THE HEALTH CHECK ENDPOINT\nA health check endpoint isn’t much use if nobody calls it. When you deploy your ser-\nvice, you must configure the deployment infrastructure to invoke the endpoint. How\nyou do that depends on the specific details of your deployment infrastructure. For\nexample, as described in chapter 3, you can configure some service registries, such as\nNetflix Eureka, to invoke the health check endpoint in order to determine whether\ntraffic should be routed to the service instance. Chapter 12 discusses how to configure\nDocker and Kubernetes to invoke a health check endpoint. \n11.3.2 Applying the Log aggregation pattern\nLogs are a valuable troubleshooting tool. If you want to know what’s wrong with your\napplication, a good place to start is the log files. But using logs in a microservice archi-\ntecture is challenging. For example, imagine you’re debugging a problem with the\ngetOrderDetails() query. As described in chapter 8, the FTGO application imple-\nments this query using API composition. As a result, the log entries you need are scat-\ntered across the log files of the API gateway and several services, including Order\nService and Kitchen Service.\nPattern: Log aggregation\nAggregate the logs of all services in a centralized database that supports searching\nand alerting. See http://microservices.io/patterns/observability/application-logging\n.html.\n \n\n\n369\nDesigning observable services\nThe solution is to use log aggregation. As figure 11.11 shows, the log aggregation pipe-\nline sends the logs of all of the service instances to a centralized logging server. Once\nthe logs are stored by the logging server, you can view, search, and analyze them. You\ncan also configure alerts that are triggered when certain messages appear in the logs.\nThe logging pipeline and server are usually the responsibility of operations. But ser-\nvice developers are responsible for writing services that generate useful logs. Let’s first\nlook at how a service generates a log.\nHOW A SERVICE GENERATES A LOG\nAs a service developer, there are a couple of issues you need to consider. First you\nneed to decide which logging library to use. The second issue is where to write the log\nentries. Let’s first look at the logging library.\n Most programming languages have one or more logging libraries that make it easy\nto generate correctly structured log entries. For example, three popular Java logging\nlibraries are Logback, log4j, and JUL (java.util.logging). There’s also SLF4J, which is a\nlogging facade API for the various logging frameworks. Similarly, Log4JS is a popular\nlogging framework for NodeJS. One reasonable way to use logging is to sprinkle calls\nto one of these logging libraries in your service’s code. But if you have strict logging\nrequirements that can’t be enforced by the logging library, you may need to define\nyour own logging API that wraps a logging library.\n You also need to decide where to log. Traditionally, you would configure the log-\nging framework to write to a log file in a well-known location in the filesystem. But\nwith the more modern deployment technologies, such as containers and serverless,\nService A\ninstance 1\nLogging\nlibrary\nService B\ninstance 1\nLogging\nlibrary\nService A\ninstance 2\nLogging\nlibrary\nLog\nView\nNotify\nLog\nLog\nLog\npipeline\nLogging\nserver\nUser\nFigure 11.11\nThe log aggregation infrastructure ships the logs of each service instance to a \ncentralized logging server. Users can view and search the logs. They can also set up alerts, which are \ntriggered when log entries match search criteria.\n \n",
      "page_number": 378
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 395-405)",
      "start_page": 395,
      "end_page": 405,
      "detection_method": "topic_boundary",
      "content": "370\nCHAPTER 11\nDeveloping production-ready services\ndescribed in chapter 12, this is often not the best approach. In some environments,\nsuch as AWS Lambda, there isn’t even a “permanent” filesystem to write the logs to!\nInstead, your service should log to stdout. The deployment infrastructure will then\ndecide what to do with the output of your service. \nTHE LOG AGGREGATION INFRASTRUCTURE\nThe logging infrastructure is responsible for aggregating the logs, storing them, and\nenabling the user to search them. One popular logging infrastructure is the ELK\nstack. ELK consists of three open source products:\nElasticsearch—A text search-oriented NoSQL database that’s used as the logging\nserver\nLogstash—A log pipeline that aggregates the service logs and writes them to\nElasticsearch\nKibana—A visualization tool for Elasticsearch\nOther open source log pipelines include Fluentd and Apache Flume. Examples of log-\nging servers include cloud services, such as AWS CloudWatch Logs, as well as numerous\ncommercial offerings. Log aggregation is a useful debugging tool in a microservice\narchitecture.\n Let’s now look at distributed tracing, which is another way of understanding the\nbehavior of a microservices-based application. \n11.3.3 Using the Distributed tracing pattern\nImagine you’re a FTGO developer who is investigating why the getOrderDetails()\nquery has slowed down. You’ve ruled out the problem being an external networking\nissue. The increased latency must be caused by either the API gateway or one of the\nservices it has invoked. One option is to look at each service’s average response time.\nThe trouble with this option is that it’s an average across requests rather than the tim-\ning breakdown for an individual request. Plus more complex scenarios might involve\nmany nested service invocations. You may not even be familiar with all services. As a\nresult, it can be challenging to troubleshoot and diagnose these kinds of performance\nproblems in a microservice architecture.\nA good way to get insight into what your application is doing is to use distributed trac-\ning. Distributed tracing is analogous to a performance profiler in a monolithic applica-\ntion. It records information (for example, start time and end time) about the tree of\nservice calls that are made when handling a request. You can then see how the services\nPattern: Distributed tracing\nAssign each external request a unique ID and record how it flows through the system\nfrom one service to the next in a centralized server that provides visualization and\nanalysis. See http://microservices.io/patterns/observability/distributed-tracing.html.\n \n\n\n371\nDesigning observable services\ninteract during the handling of external requests, including a breakdown of where\nthe time is spent.\n Figure 11.12 shows an example of how a distributed tracing server displays what\nhappens when the API gateway handles a request. It shows the inbound request to the\nAPI gateway and the request that the gateway makes to Order Service. For each\nrequest, the distributed tracing server shows the operation that’s performed and the\ntiming of the request.\nFigure 11.12 shows what in distributed tracing terminology is called a trace. A trace\nrepresents an external request and consists of one or more spans. A span represents\nan operation, and its key attributes are an operation name, start timestamp, and end\ntime. A span can have one or more child spans, which represent nested operations.\nFor example, a top-level span might represent the invocation of the API gateway, as\nis the case in figure 11.12. Its child spans represent the invocations of services by the\nAPI gateway.\n A valuable side effect of distributed tracing is that it assigns a unique ID to each\nexternal request. A service can include the request ID in its log entries. When com-\nbined with log aggregation, the request ID enables you to easily find all log entries\nfor a particular external request. For example, here’s an example log entry from\nOrder Service:\n2018-03-04 17:38:12.032 DEBUG [ftgo-order-\nservice,8d8fdc37be104cc6,8d8fdc37be104cc6,false]\n7 --- [nio-8080-exec-6] org.hibernate.SQL\n:\nselect order0_.id as id1_3_0_, order0_.consumer_id as consumer2_3_0_, order\n0_.city as city3_3_0_,\norder0_.delivery_state as delivery4_3_0_, order0_.street1 as street5_3_0_,\norder0_.street2 as street6_3_0_, order0_.zip as zip7_3_0_,\norder0_.delivery_time as delivery8_3_0_, order0_.a\nParent span\nChild span\nTrace\nFigure 11.12\nThe Zipkin server shows how the FTGO application handles a request that’s routed \nby the API gateway to Order Service. Each request is represented by a trace. A trace is a set of \nspans. Each span, which can contain child spans, is the invocation of a service. Depending on the \nlevel of detail collected, a span can also represent the invocation of an operation inside a service.\n \n\n\n372\nCHAPTER 11\nDeveloping production-ready services\nThe [ftgo-order-service,8d8fdc37be104cc6,8d8fdc37be104cc6,false] part of the\nlog entry (the SLF4J Mapped Diagnostic Context—see www.slf4j.org/manual.html)\ncontains information from the distributed tracing infrastructure. It consists of four\nvalues:\n\nftgo-order-service—The name of the application\n\n8d8fdc37be104cc6—The traceId\n\n8d8fdc37be104cc6—The spanId\n\nfalse—Indicates that this span wasn’t exported to the distributed tracing server\nIf you search the logs for 8d8fdc37be104cc6, you’ll find all log entries for that request.\n Figure 11.13 shows how distributed tracing works. There are two parts to distrib-\nuted tracing: an instrumentation library, which is used by each service, and a distributed\ntracing server. The instrumentation library manages the traces and spans. It also adds\nSpan ABC: API gateway\nTrace XYZ\nAPI\ngateway\nGET/orders/1 HTTP/1.1\n....\nGET/orders/1 HTTP/1.1\nX-B3-TraceId: XYZ\nX-B3-ParentSpanId: ABC\nService: API gateway\nTraceId: XYZ\nParentSpan: NONE\nSpan: ABC\nViews traces\nService: Order Service\nTraceId: XYZ\nParentSpan: ABC\nSpan: DEF\nSpan DEF: Order Service\nTransport\nDistributed tracing server\nOrder\nService\nInstrumentation\nlibrary\nInstrumentation\nlibrary\nUser\nTrace\ndatabase\nFigure 11.13\nEach service (including the API gateway) uses an instrumentation library. The \ninstrumentation library assigns an ID to each external request, propagates tracing state between \nservices, and reports spans to the distributed tracing server.\n \n\n\n373\nDesigning observable services\ntracing information, such as the current trace ID and the parent span ID, to outbound\nrequests. For example, one common standard for propagating trace information is\nthe B3 standard (https://github.com/openzipkin/b3-propagation), which uses head-\ners such as X-B3-TraceId and X-B3-ParentSpanId. The instrumentation library also\nreports traces to the distributed tracing server. The distributed tracing server stores\nthe traces and provides a UI for visualizing them.\n Let’s take a look at the instrumentation library and the distribution tracing server,\nbeginning with the library.\nUSING AN INSTRUMENTATION LIBRARY\nThe instrumentation library builds the tree of spans and sends them to the distributed\ntracing server. The service code could call the instrumentation library directly, but that\nwould intertwine the instrumentation logic with business and other logic. A cleaner\napproach is to use interceptors or aspect-oriented programming (AOP).\n A great example of an AOP-based framework is Spring Cloud Sleuth. It uses the\nSpring Framework’s AOP mechanism to automagically integrate distributed tracing\ninto the service. As a result, you have to add Spring Cloud Sleuth as a project depen-\ndency. Your service doesn’t need to call a distributed tracing API except in those cases\nthat aren’t handled by Spring Cloud Sleuth. \nABOUT THE DISTRIBUTED TRACING SERVER\nThe instrumentation library sends the spans to a distributed tracing server. The dis-\ntributed tracing server stitches the spans together to form complete traces and stores\nthem in a database. One popular distributed tracing server is Open Zipkin. Zipkin was\noriginally developed by Twitter. Services can deliver spans to Zipkin using either\nHTTP or a message broker. Zipkin stores the traces in a storage backend, which is\neither a SQL or NoSQL database. It has a UI that displays traces, as shown earlier in\nfigure 11.12. AWS X-ray is another example of a distributed tracing server. \n11.3.4 Applying the Application metrics pattern\nA key part of the production environment is monitoring and alerting. As figure 11.14\nshows, the monitoring system gathers metrics, which provide critical information\nabout the health of an application, from every part of the technology stack. Metrics\nrange from infrastructure-level metrics, such as CPU, memory, and disk utilization, to\napplication-level metrics, such as service request latency and number of requests exe-\ncuted. Order Service, for example, gathers metrics about the number of placed,\napproved, and rejected orders. The metrics are collected by a metrics service, which\nprovides visualization and alerting.\nPattern: Application metrics\nServices report metrics to a central server that provides aggregation, visualization,\nand alerting.\n \n\n\n374\nCHAPTER 11\nDeveloping production-ready services\nMetrics are sampled periodically. A metric sample has the following three properties:\nName—The name of the metric, such as jvm_memory_max_bytes or placed_orders\nValue—A numeric value\nTimestamp—The time of the sample\nIn addition, some monitoring systems support the concept of dimensions, which are\narbitrary name-value pairs. For example, jvm_memory_max_bytes is reported with dimen-\nsions such as area=\"heap\",id=\"PS Eden Space\" and area=\"heap\",id=\"PS Old Gen\".\nDimensions are often used to provide additional information, such as the machine\nname or service name, or a service instance identifier. A monitoring system typically\naggregates (sums or averages) metric samples along one or more dimensions.\n Many aspects of monitoring are the responsibility of operations. But a service\ndeveloper is responsible for two aspects of metrics. First, they must instrument their\nservice so that it collects metrics about its behavior. Second, they must expose those\nservice metrics, along with metrics from the JVM and the application framework, to\nthe metrics server.\n Let’s first look at how a service collects metrics.\nCOLLECTING SERVICE-LEVEL METRICS\nHow much work you need to do to collect metrics depends on the frameworks that\nyour application uses and the metrics you want to collect. A Spring Boot-based service\ncan, for example, gather (and expose) basic metrics, such as JVM metrics, by including\nView\nNotify\nMetrics\nService\nUser\nService instance\nDeployment infrastructure\nMetrics sample:\nname=cpu_percent\nvalue=68\ntimestamp=34938934893\ndimensions:\nmachine=node1\n...\nApplication framework\nLanguage runtime\nApplication code\nMetrics library\nVisualization\nMetrics\ningestion\nAlerts\nMetrics\ndatabase\nFigure 11.14\nMetrics at every level of the stack are collected and stored in a metrics service, which \nprovides visualization and alerting.\n \n\n\n375\nDesigning observable services\nthe Micrometer Metrics library as a dependency and using a few lines of configura-\ntion. Spring Boot’s autoconfiguration takes care of configuring the metrics library and\nexposing the metrics. A service only needs to use the Micrometer Metrics API directly\nif it gathers application-specific metrics.\n The following listing shows how OrderService can collect metrics about the number\nof orders placed, approved, and rejected. It uses MeterRegistry, which is the interface-\nprovided Micrometer Metrics, to gather custom metrics. Each method increments an\nappropriately named counter.\npublic class OrderService {\n@Autowired\nprivate MeterRegistry meterRegistry;\n    \npublic Order createOrder(...) {\n...\nmeterRegistry.counter(\"placed_orders\").increment();    \nreturn order;\n}\npublic void approveOrder(long orderId) {\n...\nmeterRegistry.counter(\"approved_orders\").increment();\n}\npublic void rejectOrder(long orderId) {\n...\nmeterRegistry.counter(\"rejected_orders\").increment();      \n}\nDELIVERING METRICS TO THE METRICS SERVICE\nA service delivers metrics to the Metrics Service in one of two ways: push or pull. With\nthe push model, a service instance sends the metrics to the Metrics Service by invoking\nan API. AWS Cloudwatch metrics, for example, implements the push model.\n With the pull model, the Metrics Service (or its agent running locally) invokes a\nservice API to retrieve the metrics from the service instance. Prometheus, a popular\nopen source monitoring and alerting system, uses the pull model.\n The FTGO application’s Order Service uses the micrometer-registry-prometheus\nlibrary to integrate with Prometheus. Because this library is on the classpath, Spring\nBoot exposes a GET /actuator/prometheus endpoint, which returns metrics in the\nformat that Prometheus expects. The custom metrics from OrderService are reported\nas follows:\n$ curl -v http://localhost:8080/actuator/prometheus | grep _orders\n# HELP placed_orders_total\n# TYPE placed_orders_total counter\nListing 11.1\nOrderService tracks the number of orders placed, approved, and \nrejected.\nThe Micrometer Metrics \nlibrary API for managing \napplication-specific meters\nIncrements the \nplacedOrders counter \nwhen an order has \nsuccessfully been \nplaced\nIncrements the \napprovedOrders \ncounter when an \norder has been \napproved\nIncrements the \nrejectedOrders \ncounter when an \norder has been \nrejected\n \n\n\n376\nCHAPTER 11\nDeveloping production-ready services\nplaced_orders_total{service=\"ftgo-order-service\",} 1.0\n# HELP approved_orders_total\n# TYPE approved_orders_total counter\napproved_orders_total{service=\"ftgo-order-service\",} 1.0\nThe placed_orders counter is, for example, reported as a metric of type counter.\n The Prometheus server periodically polls this endpoint to retrieve metrics. Once\nthe metrics are in Prometheus, you can view them using Grafana, a data visualization\ntool (https://grafana.com). You can also set up alerts for these metrics, such as when\nthe rate of change for placed_orders_total falls below some threshold.\n Application metrics provide valuable insights into your application’s behavior.\nAlerts triggered by metrics enable you to quickly respond to a production issue, per-\nhaps before it impacts users. Let’s now look at how to observe and respond to another\nsource of alerts: exceptions. \n11.3.5 Using the Exception tracking pattern\nA service should rarely log an exception, and when it does, it’s important that you\nidentify the root cause. The exception might be a symptom of a failure or a program-\nming bug. The traditional way to view exceptions is to look in the logs. You might even\nconfigure the logging server to alert you if an exception appears in the log file. There\nare, however, several problems with this approach:\nLog files are oriented around single-line log entries, whereas exceptions consist\nof multiple lines.\nThere’s no mechanism to track the resolution of exceptions that occur in log\nfiles. You would have to manually copy/paste the exception into an issue tracker.\nThere are likely to be duplicate exceptions, but there’s no automatic mecha-\nnism to treat them as one.\nA better approach is to use an exception tracking service. As figure 11.15 shows, you\nconfigure your service to report exceptions to an exception tracking service via, for\nexample, a REST API. The exception tracking service de-duplicates exceptions, gener-\nates alerts, and manages the resolution of exceptions.\n There are a couple of ways to integrate the exception tracking service into your\napplication. Your service could invoke the exception tracking service’s API directly. A\nbetter approach is to use a client library provided by the exception tracking service.\nFor example, HoneyBadger’s client library provides several easy-to-use integration\nmechanisms, including a Servlet Filter that catches and reports exceptions.\nPattern: Exception tracking\nServices report exceptions to a central service that de-duplicates exceptions, gener-\nates alerts, and manages the resolution of exceptions. See http://microservices.io/\npatterns/observability/audit-logging.html.\n \n\n\n377\nDesigning observable services\nThe Exception tracking pattern is a useful way to quickly identify and respond to pro-\nduction issues.\n It’s also important to track user behavior. Let’s look at how to do that. \n11.3.6 Applying the Audit logging pattern\nThe purpose of audit logging is to record each user’s actions. An audit log is typically\nused to help customer support, ensure compliance, and detect suspicious behavior.\nEach audit log entry records the identity of the user, the action they performed, and\nthe business object(s). An application usually stores the audit log in a database table.\nException tracking services\nThere are several exception tracking services. Some, such as Honeybadger (www\n.honeybadger.io), are purely cloud-based. Others, such as Sentry.io (https://sentry.io/\nwelcome/), also have an open source version that you can deploy on your own infra-\nstructure. These services receive exceptions from your application and generate alerts.\nThey provide a console for viewing exceptions and managing their resolution. An excep-\ntion tracking service typically provides client libraries in a variety of languages.\nPattern: Audit logging\nRecord user actions in a database in order to help customer support, ensure com-\npliance, and detect suspicious behavior. See http://microservices.io/patterns/\nobservability/audit-logging.html.\nView & manage\nNotify\nUser\nPOST/exceptions\njava.lang.NullPointerException\nat net.chrisrichardson.ftgo...\nat net.chrisrichardson.ftgo...\nat net.chrisrichardson.ftgo...\nOrder Service\nException tracking\nclient library\nException database\nException tracking service\nReport exception\nFigure 11.15\nA service reports exceptions to an exception tracking service, which de-duplicates \nexceptions and alerts developers. It has a UI for viewing and managing exceptions.\n \n\n\n378\nCHAPTER 11\nDeveloping production-ready services\nThere are a few different ways to implement audit logging:\nAdd audit logging code to the business logic.\nUse aspect-oriented programming (AOP).\nUse event sourcing.\nLet’s look at each option.\nADD AUDIT LOGGING CODE TO THE BUSINESS LOGIC\nThe first and most straightforward option is to sprinkle audit logging code through-\nout your service’s business logic. Each service method, for example, can create an\naudit log entry and save it in the database. The drawback with this approach is that it\nintertwines auditing logging code and business logic, which reduces maintainability.\nThe other drawback is that it’s potentially error prone, because it relies on the devel-\noper writing audit logging code. \nUSE ASPECT-ORIENTED PROGRAMMING\nThe second option is to use AOP. You can use an AOP framework, such as Spring\nAOP, to define advice that automatically intercepts each service method call and per-\nsists an audit log entry. This is a much more reliable approach, because it automati-\ncally records every service method invocation. The main drawback of using AOP is\nthat the advice only has access to the method name and its arguments, so it might be\nchallenging to determine the business object being acted upon and generate a business-\noriented audit log entry. \nUSE EVENT SOURCING\nThe third and final option is to implement your business logic using event sourcing.\nAs mentioned in chapter 6, event sourcing automatically provides an audit log for cre-\nate and update operations. You need to record the identity of the user in each event.\nOne limitation with using event sourcing, though, is that it doesn’t record queries. If\nyour service must create log entries for queries, then you’ll have to use one of the\nother options as well. \n11.4\nDeveloping services using the Microservice chassis \npattern\nThis chapter has described numerous concerns that a service must implement, includ-\ning metrics, reporting exceptions to an exception tracker, logging and health checks,\nexternalized configuration, and security. Moreover, as described in chapter 3, a ser-\nvice may also need to handle service discovery and implement circuit breakers. That’s\nnot something you’d want to set up from scratch each time you implement a new ser-\nvice. If you did, it would potentially be days, if not weeks, before you wrote your first\nline of business logic.\n \n \n \n\n\n379\nDeveloping services using the Microservice chassis pattern\nA much faster way to develop services is to build your services upon a microservices\nchassis. As figure 11.16 shows, a microservice chassis is a framework or set of frameworks\nthat handle these concerns. When using a microservice chassis, you write little, if any,\ncode to handle these concerns.\nIn this section, I first describe the concept of a microservice chassis and suggest some\nexcellent microservice chassis frameworks. After that I introduce the concept of a ser-\nvice mesh, which at the time of writing is emerging as an intriguing alternative to\nusing frameworks and libraries.\n Let’s first look at the idea of a microservice chassis.\n11.4.1 Using a microservice chassis\nA microservices chassis is a framework or set of frameworks that handle numerous\nconcerns including the following:\nExternalized configuration\nHealth checks\nApplication metrics\nService discovery\nPattern: Microservice chassis\nBuild services on a framework or collection of frameworks that handle cross-cutting\nconcerns, such as exception tracking, logging, health checks, externalized configu-\nration, and distributed tracing. See http://microservices.io/patterns/microservice-\nchassis.html.\nService\nService code\nCircuit breaker\nMicroservice chassis\nService discovery\nDistributed tracing\nApplication metrics\nLogging\nHealth check\nExternalized conﬁg.\n...\nFigure 11.16\nA microservice chassis \nis a framework that handles numerous \nconcerns, such as exception tracking, \nlogging, health checks, externalized \nconfiguration, and distributed tracing.\n \n\n\n380\nCHAPTER 11\nDeveloping production-ready services\nCircuit breakers\nDistributed tracing\nIt significantly reduces the amount of code you need to write. You may not even need\nto write any code. Instead, you configure the microservice chassis to fit your require-\nments. A microservice chassis enables you to focus on developing your service’s busi-\nness logic.\n The FTGO application uses Spring Boot and Spring Cloud as the microservice\nchassis. Spring Boot provides functions such as externalized configuration. Spring\nCloud provides functions such as circuit breakers. It also implements client-side ser-\nvice discovery, although the FTGO application relies on the infrastructure for service\ndiscovery. Spring Boot and Spring Cloud aren’t the only microservice chassis frame-\nworks. If, for example, you’re writing services in GoLang, you could use either Go Kit\n(https://github.com/go-kit/kit) or Micro (https://github.com/micro/micro).\n One drawback of using a microservice chassis is that you need one for every lan-\nguage/platform combination that you use to develop services. Fortunately, it’s likely\nthat many of the functions implemented by a microservice chassis will instead be\nimplemented by the infrastructure. For example, as described in chapter 3, many\ndeployment environments handle service discovery. What’s more, many of the network-\nrelated functions of a microservice chassis will be handled by what’s known as a service\nmesh, an infrastructure layer running outside of the services. \n11.4.2 From microservice chassis to service mesh\nA microservice chassis is a good way to implement various cross-cutting concerns, such\nas circuit breakers. But one obstacle to using a microservice chassis is that you need\none for each programming language you use. For example, Spring Boot and Spring\nCloud are useful if you’re a Java/Spring developer, but they aren’t any help if you\nwant to write a NodeJS-based service.\nAn emerging alternative that avoids this problem is to implement some of this func-\ntionality outside of the service in what’s known as a service mesh. A service mesh is net-\nworking infrastructure that mediates the communication between a service and other\nservices and external applications. As figure 11.17 shows, all network traffic in and out\nof a service goes through the service mesh. It implements various concerns including\ncircuit breakers, distributed tracing, service discovery, load balancing, and rule-based\ntraffic routing. A service mesh can also secure interprocess communication by using\nPattern: Service mesh\nRoute all network traffic in and out of services through a networking layer that imple-\nments various concerns, including circuit breakers, distributed tracing, service dis-\ncovery, load balancing, and rule-based traffic routing. See http://microservices.io/\npatterns/deployment/service-mesh.html.\n \n",
      "page_number": 395
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 406-413)",
      "start_page": 406,
      "end_page": 413,
      "detection_method": "topic_boundary",
      "content": "381\nDeveloping services using the Microservice chassis pattern\nTLS-based IPC between services. As a result, you no longer need to implement these\nparticular concerns in the services.\n When using a service mesh, the microservice chassis is much simpler. It only needs\nto implement concerns that are tightly integrated with the application code, such as\nexternalized configuration and health checks. The microservice chassis must support\ndistributed tracing by propagating distributed tracing information, such as the B3\nstandard headers I discussed earlier in section 11.3.3.\nThe service mesh concept is an extremely promising idea. It frees the developer from\nhaving to deal with various cross-cutting concerns. Also, the ability of a service mesh to\nThe current state of service mesh implementations\nThere are various service mesh implementations, including the following:\nIstio (https://istio.io)\nLinkerd (https://linkerd.io)\nConduit (https://conduit.io)\nAs of the time of writing, Linkerd is the most mature, with Istio and Conduit still under\nactive development. For more information about this exciting new technology, take a\nlook at each product’s documentation.\nAPI\ngateway\nMicroservice\nchassis\nOrder\nService\nService\nmesh\nMicroservice\nchassis\nRestaurant\nService\nMicroservice\nchassis\nDeployment infrastructure\nCircuit breaker\nService discovery\nDistributed tracing\nSmart trafﬁc routing\nLoad balancing\nLogging\nMicroservice chassis\nFunctionality moved from\nmicroservice chassis to\nservice mesh\nFewer functions\nExternalized conﬁg.\nDistributed tracing\nApplication metrics\nHealth check\n...\nSecure communications\nFigure 11.17\nAll network traffic in and out of a service flows through the service mesh. The service \nmesh implements various functions including circuit breakers, distributed tracing, service discovery, \nand load balancing. Fewer functions are implemented by the microservice chassis. It also secures \ninterprocess communication by using TLS-based IPC between services.\n \n\n\n382\nCHAPTER 11\nDeveloping production-ready services\nroute traffic enables you to separate deployment from release. It gives you the ability\nto deploy a new version of a service into production but only release it to certain users,\nsuch as internal test users. Chapter 12 discusses this concept further when describing\nhow to deploy services using Kubernetes. \nSummary\nIt’s essential that a service implements its functional requirements, but it must\nalso be secure, configurable, and observable.\nMany aspects of security in a microservice architecture are no different than in\na monolithic architecture. But there are some aspects of application security\nthat are necessarily different, including how user identity is passed between the\nAPI gateway and the services and who is responsible for authentication and autho-\nrization. A commonly used approach is for the API gateway to authenticate clients.\nThe API gateway includes a transparent token, such as a JWT, in each request to a\nservice. The token contains the identity of the principal and their roles. The ser-\nvices use the information in the token to authorize access to resources. OAuth 2.0\nis a good foundation for security in a microservice architecture.\nA service typically uses one or more external services, such as message brokers\nand databases. The network location and credentials of each external service\noften depend on the environment that the service is running in. You must apply\nthe Externalized configuration pattern and implement a mechanism that pro-\nvides a service with configuration properties at runtime. One commonly used\napproach is for the deployment infrastructure to supply those properties via\noperating system environment variables or a properties file when it creates a\nservice instance. Another option is for a service instance to retrieve its configu-\nration from a configuration properties server.\nOperations and developers share responsibility for implementing the observ-\nability patterns. Operations is responsible for the observability infrastructure,\nsuch as servers that handle log aggregation, metrics, exception tracking, and\ndistributed tracing. Developers are responsible for ensuring that their services\nare observable. Services must have health check API endpoints, generate log\nentries, collect and expose metrics, report exceptions to an exception tracking\nservice, and implement distributed tracing.\nIn order to simplify and accelerate development, you should develop services\non top of a microservices chassis. A microservices chassis is framework or set of\nframeworks that handle various cross-cutting concerns, including those described\nin this chapter. Over time, though, it’s likely that many of the networking-\nrelated functions of a microservice chassis will migrate into a service mesh, a\nlayer of infrastructure software through which all of a service’s network traffic\nflows. \n \n\n\n383\nDeploying microservices\nMary and her team at FTGO are almost finished writing their first service. Although\nit’s not yet feature complete, it’s running on developer laptops and the Jenkins CI\nserver. But that’s not good enough. Software has no value to FTGO until it’s run-\nning in production and available to users. FTGO needs to deploy their service into\nproduction.\nThis chapter covers\nThe four key deployment patterns, how they work, \nand their benefits and drawbacks:\n– Language-specific packaging format\n– Deploying a service as a VM\n– Deploying a service as a container\n– Serverless deployment\nDeploying services with Kubernetes\nUsing a service mesh to separate deployment \nfrom release\nDeploying services with AWS Lambda\nPicking a deployment pattern\n \n\n\n384\nCHAPTER 12\nDeploying microservices\n Deployment is a combination of two interrelated concepts: process and architecture.\nThe deployment process consists of the steps that must be performed by people—\ndevelopers and operations—in order to get software into production. The deploy-\nment architecture defines the structure of the environment in which that software\nruns. Both aspects of deployment have changed radically since I first started develop-\ning Enterprise Java applications in the late 1990s. The manual process of developers\nthrowing code over the wall to production has become highly automated. As figure 12.1\nshows, physical production environments have been replaced by increasingly light-\nweight and ephemeral computing infrastructure.\nBack in the 1990s, if you wanted to deploy an application into production, the first\nstep was to throw your application along with a set of operating instructions over the\nwall to operations. You might, for example, file a trouble ticket asking operations to\ndeploy the application. Whatever happened next was entirely the responsibility of\noperations, unless they encountered a problem they needed your help to fix. Typi-\ncally, operations bought and installed expensive and heavyweight application servers\nsuch as WebLogic or WebSphere. Then they would log in to the application server\nconsole and deploy your applications. They would lovingly care for those machines, as\nif they were pets, installing patches and updating the software.\n In the mid 2000s, the expensive application servers were replaced with open\nsource, lightweight web containers such as Apache Tomcat and Jetty. You could still\nrun multiple applications on each web container, but having one application per web\ncontainer became feasible. Also, virtual machines started to replace physical machines.\nPhysical\nmachine\nApplication\nPhysical\nmachine\nVirtual\nmachine\nApplication\nPhysical\nmachine\nVirtual\nmachine\nContainer\nruntime\nApplication\nPhysical\nmachine\n1990s\n2006\n2013\n2014\nAWS EC2\nreleased\nInitial Docker\nrelease\nAWS Lambda\nintroduced\nHidden\ninfrastructure\nServerless\nruntime\nApplication\nLightweight,\nephemeral,\nautomated\nHeavyweight,\npermanent,\nmanual\nTime\nFigure 12.1\nHeavyweight and long-lived physical machines have been abstracted away \nby increasingly lightweight and ephemeral technologies.\n \n\n\n385\nBut machines were still treated as beloved pets, and deployment was still a fundamen-\ntally manual process.\n Today, the deployment process is radically different. Instead of handing off code to\na separate production team, the adoption of DevOps means that the development\nteam is also responsible for deploying their application or services. In some organiza-\ntions, operations provides developers with a console for deploying their code. Or, bet-\nter yet, once the tests pass, the deployment pipeline automatically deploys the code\ninto production.\n The computing resources used in a production environment have also changed rad-\nically with physical machines being abstracted away. Virtual machines running on a\nhighly automated cloud, such as AWS, have replaced the long-lived, pet-like physical and\nvirtual machines. Today’s virtual machines are immutable. They’re treated as disposable\ncattle instead of pets and are discarded and recreated rather than being reconfigured.\nContainers, an even more lightweight abstraction layer of top of virtual machines, are an\nincreasingly popular way of deploying applications. You can also use an even more light-\nweight serverless deployment platform, such as AWS Lambda, for many use cases.\n It’s no coincidence that the evolution of deployment processes and architectures has\ncoincided with the growing adoption of the microservice architecture. An application\nmight have tens or hundreds of services written in a variety of languages and frame-\nworks. Because each service is a small application, that means you have tens or hundreds\nof applications in production. It’s no longer practical, for example, for system adminis-\ntrators to hand configure servers and services. If you want to deploy microservices at\nscale, you need a highly automated deployment process and infrastructure.\n Figure 12.2 shows a high-level view of a production environment. The production\nenvironment enables developers to configure and manage their services, the deployment\nService\nA\nConsumes\nservices\nService\nC\nService\nB\nService\nD\nUser\nObserve and\ntroubleshoot\nservices\nUpdate\nservices\nConﬁgure\nand manage\nservices\nDeveloper\nRouting\nDash-\nboards\nMonitoring\nService\nmanagement\ninterface\nRuntime\nService\nmanagement\nAlerting\nDeployment\npipeline\nFigure 12.2\nA simplified view of the production environment. It provides four main capabilities: \nservice management enables developers to deploy and manage their services, runtime management \nensures that the services are running, monitoring visualizes service behavior and generates alerts, \nand request routing routes requests from users to the services.\n \n\n\n386\nCHAPTER 12\nDeploying microservices\npipeline to deploy new versions of services, and users to access functionality imple-\nmented by those services.\n A production environment must implement four key capabilities:\nService management interface—Enables developers to create, update, and config-\nure services. Ideally, this interface is a REST API invoked by command-line and\nGUI deployment tools.\nRuntime service management—Attempts to ensure that the desired number of ser-\nvice instances is running at all times. If a service instance crashes or is somehow\nunable to handle requests, the production environment must restart it. If a\nmachine crashes, the production environment must restart those service instances\non a different machine.\nMonitoring—Provides developers with insight into what their services are doing,\nincluding log files and metrics. If there are problems, the production environ-\nment must alert the developers. Chapter 11 describes monitoring, also called\nobservability.\nRequest routing—Routes requests from users to the services.\nIn this chapter I discuss the four main deployment options:\nDeploying services as language-specific packages, such as Java JAR or WAR files.\nIt’s worthwhile exploring this option, because even though I recommend using\none of the other options, its drawbacks motivate the other options.\nDeploying services as virtual machines, which simplifies deployment by packag-\ning a service as a virtual machine image that encapsulate the service’s technol-\nogy stack.\nDeploying services as containers, which are more lightweight than virtual\nmachines. I show how to deploy the FTGO application’s Restaurant Service\nusing Kubernetes, a popular Docker orchestration framework.\nDeploying services using serverless deployment, which is even more modern than\ncontainers. We’ll look at how to deploy Restaurant Service using AWS Lambda,\na popular serverless platform.\nLet’s first look at how to deploy services as language-specific packages.\n12.1\nDeploying services using the Language-specific \npackaging format pattern\nLet’s imagine that you want to deploy the FTGO application’s Restaurant Service,\nwhich is a Spring Boot-based Java application. One way to deploy this service is by\nusing the Service as a language-specific package pattern. When using this pattern,\nwhat’s deployed in production and what’s managed by the service runtime is a service\nin its language-specific package. In the case of Restaurant Service, that’s either the\nexecutable JAR file or a WAR file. For other languages, such as NodeJS, a service is a\ndirectory of source code and modules. For some languages, such as GoLang, a service\nis an operating system-specific executable.\n \n\n\n387\nDeploying services using the Language-specific packaging format pattern\nTo deploy Restaurant Service on a machine, you would first install the necessary\nruntime, which in this case is the JDK. If it’s a WAR file, you also need to install a\nweb container such as Apache Tomcat. Once you’ve configured the machine, you\ncopy the package to the machine and start the service. Each service instance runs as\na JVM process.\n Ideally, you’ve set up your deployment pipeline to automatically deploy the service\nto production, as shown in figure 12.3. The deployment pipeline builds an executable\nJAR file or WAR file. It then invokes the production environment’s service manage-\nment interface to deploy the new version.\nA service instance is typically a single process but sometimes may be a group of pro-\ncesses. A Java service instance, for example, is a process running the JVM. A NodeJS\nservice might spawn multiple worker processes in order to process requests concur-\nrently. Some languages support deploying multiple service instances within the same\nprocess.\n Sometimes you might deploy a single service instance on a machine, while retain-\ning the option to deploy multiple service instances on the same machine. For exam-\nple, as figure 12.4 shows, you could run multiple JVMs on a single machine. Each JVM\nruns a single service instance.\nPattern: Language-specific packaging format\nDeploy a language-specific package into production. See http://microservices.io/\npatterns/deployment/language-specific-packaging.html.\nJVM\nprocess\nJVM\nprocess\nJVM\nprocess\nService instance\nBuild time\nRuntime\nService runtime management\nMachine\nProduction\nJDK/JRE\nMachine\nJDK/JRE\nService\ncode\nExecutable\nJAR/WAR ﬁle\nDeployment\npipeline\nFigure 12.3\nThe deployment pipeline builds an executable JAR file and deploys it into production. \nIn production, each service instance is a JVM running on a machine that has the JDK or JRE installed.\n \n\n\n388\nCHAPTER 12\nDeploying microservices\nSome languages also let you run multiple services instances in a single process. For\nexample, as figure 12.5 shows, you can run multiple Java services on a single Apache\nTomcat.\nThis approach is commonly used when deploying applications on traditional expen-\nsive and heavyweight application servers, such as WebLogic and WebSphere. You can\nalso package services as OSGI bundles and run multiple service instances in each\nOSGI container.\n The Service as a language-specific package pattern has both benefits and draw-\nbacks. Let’s first look at the benefits.\n12.1.1 Benefits of the Service as a language-specific package pattern\nThe Service as a language-specific package pattern has a few benefits:\nFast deployment\nEfficient resource utilization, especially when running multiple instances on\nthe same machine or within the same process\nLet’s look at each one.\nJVM\nProcess\nPhysical or virtual machine\nTomcat\nService\ninstance A\nJVM\nProcess\nTomcat\nService\ninstance B\nJVM\nProcess\nTomcat\nService\ninstance ...\nFigure 12.4\nDeploying multiple service \ninstances on the same machine. They \nmight be instances of the same service \nor instances of different services. The \noverhead of the OS is shared among the \nservice instances. Each service instance \nis a separate process, so there’s some \nisolation between them.\nProcess\nPhysical or virtual machine\nService\ninstance A\nJVM\nTomcat\nService\ninstance B\nService\ninstance ...\nFigure 12.5\nDeploying multiple \nservices instances on the same web \ncontainer or application server. They \nmight be instances of the same service \nor instances of different services. The \noverhead of the OS and runtime is shared \namong all the service instances. But \nbecause the service instances are in the \nsame process, there’s no isolation \nbetween them.\n \n",
      "page_number": 406
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 414-438)",
      "start_page": 414,
      "end_page": 438,
      "detection_method": "topic_boundary",
      "content": "389\nDeploying services using the Language-specific packaging format pattern\nFAST DEPLOYMENT\nOne major benefit of this pattern is that deploying a service instance is relatively\nfast: you copy the service to a host and start it. If the service is written in Java, you\ncopy a JAR or WAR file. For other languages, such as NodeJS or Ruby, you copy the\nsource code. In either case, the number of bytes copied over the network is rela-\ntively small.\n Also, starting a service is rarely time consuming. If the service is its own process,\nyou start it. Otherwise, if the service is one of several instances running in the same\ncontainer process, you either dynamically deploy it into the container or restart the\ncontainer. Because of the lack of overhead, starting a service is usually fast. \nEFFICIENT RESOURCE UTILIZATION\nAnother major benefit of this pattern is that it uses resources relatively efficiently. Mul-\ntiple service instances share the machine and its operating system. It’s even more effi-\ncient if multiple service instances run within the same process. For example, multiple\nweb applications could share the same Apache Tomcat server and JVM. \n12.1.2 Drawbacks of the Service as a language-specific package pattern\nDespite its appeal, the Service as a language-specific package pattern has several signif-\nicant drawbacks:\nLack of encapsulation of the technology stack.\nNo ability to constrain the resources consumed by a service instance.\nLack of isolation when running multiple service instances on the same machine.\nAutomatically determining where to place service instances is challenging.\nLet’s look at each drawback.\nLACK OF ENCAPSULATION OF THE TECHNOLOGY STACK\nThe operation team must know the specific details of how to deploy each and every\nservice. Each service needs a particular version of the runtime. A Java web application,\nfor example, needs particular versions of Apache Tomcat and the JDK. Operations\nmust install the correct version of each required software package.\n To make matters worse, services can be written in a variety of languages and frame-\nworks. They might also be written in multiple versions of those languages and frame-\nworks. Consequently, the development team must share lots of details with operations.\nThis complexity increases the risk of errors during deployment. A machine might, for\nexample, have the wrong version of the language runtime. \nNO ABILITY TO CONSTRAIN THE RESOURCES CONSUMED BY A SERVICE INSTANCE\nAnother drawback is that you can’t constrain the resources consumed by a service\ninstance. A process can potentially consume all of a machine’s CPU or memory, starv-\ning other service instances and operating systems of resources. This might happen, for\nexample, because of a bug. \n \n\n\n390\nCHAPTER 12\nDeploying microservices\nLACK OF ISOLATION WHEN RUNNING MULTIPLE SERVICE INSTANCES ON THE SAME MACHINE\nThe problem is even worse when running multiple instances on the same machine.\nThe lack of isolation means that a misbehaving service instance can impact other ser-\nvice instances. As a result, the application risks being unreliable, especially when run-\nning multiple service instances on the same machine. \nAUTOMATICALLY DETERMINING WHERE TO PLACE SERVICE INSTANCES IS CHALLENGING\nAnother challenge with running multiple service instances on the same machine is\ndetermining the placement of service instances. Each machine has a fixed set of\nresources, CPU, memory, and so on, and each service instance needs some amount of\nresources. It’s important to assign service instances to machines in a way that uses the\nmachines efficiently without overloading them. As I explain shortly, VM-based clouds\nand container orchestration frameworks handle this automatically. When deploying\nservices natively, it’s likely that you’ll need to manually decide the placement.\n As you can see, despite its familiarity, the Service as a language-specific package\npattern has some significant drawbacks. You should rarely use this approach, except\nperhaps when efficiency outweighs all other concerns.\n Let’s now look at modern ways of deploying services that avoid these problems. \n12.2\nDeploying services using the Service as a virtual \nmachine pattern\nOnce again, imagine you want to deploy the FTGO Restaurant Service, except this\ntime it’s on AWS EC2. One option would be to create and configure an EC2 instance\nand copy onto it the executable or WAR file. Although you would get some benefit\nfrom using the cloud, this approach suffers from the drawbacks described in the pre-\nceding section. A better, more modern approach is to package the service as an Ama-\nzon Machine Image (AMI), as shown in figure 12.6. Each service instance is an EC2\ninstance created from that AMI. The EC2 instances would typically be managed by an\nAWS Auto Scaling group, which attempts to ensure that the desired number of\nhealthy instances is always running.\nThe virtual machine image is built by the service’s deployment pipeline. The deploy-\nment pipeline, as figure 12.6 shows, runs a VM image builder to create a VM image\nthat contains the service’s code and whatever software is required to run it. For\nexample, the VM builder for a FTGO service installs the JDK and the service’s exe-\ncutable JAR. The VM image builder configures the VM image machine to run the\napplication when the VM boots, using Linux’s init system, such as upstart.\nPattern: Deploy a service as a VM\nDeploy services packaged as VM images into production. Each service instance is a\nVM. See http://microservices.io/patterns/deployment/service-per-vm.html.\n \n\n\n391\nDeploying services using the Service as a virtual machine pattern\nThere are a variety of tools that your deployment pipeline can use to build VM\nimages. One early tool for creating EC2 AMIs is Aminator, created by Netflix, which\nused it to deploy its video-streaming service on AWS (https://github.com/Netflix/\naminator). A more modern VM image builder is Packer, which unlike Aminator sup-\nports a variety of virtualization technologies, including EC2, Digital Ocean, Virtual\nBox, and VMware (www.packer.io). To use Packer to create an AMI, you write a config-\nuration file that specifies the base image and a set of provisioners that install software\nand configure the AMI.\nAbout Elastic Beanstalk\nElastic Beanstalk, which is provided by AWS, is an easy way to deploy your services\nusing VMs. You upload your code, such as a WAR file, and Elastic Beanstalk deploys\nit as one or more load-balanced and managed EC2 instances. Elastic Beanstalk is\nperhaps not quite as fashionable as, say, Kubernetes, but it’s an easy way to deploy\na microservices-based application on EC2.\nInterestingly, Elastic Beanstalk combines elements of the three deployment patterns\ndescribed in this chapter. It supports several packaging formats for several lan-\nguages, including Java, Ruby, and .NET. It deploys the application as VMs, but rather\nthan building an AMI, it uses a base image that installs the application on startup.\nBuild time\nRuntime\nRequests\nDeployed as\nService\nEC2 instance\nAutoscaling group\nService\nEC2 instance\nService\nEC2 instance\nService\ncode\nDeployment pipeline\nCreates\nVM image\nbuilder\nElastic load\nbalancer\nAMI\n(VM\nimage)\nFigure 12.6\nThe deployment pipeline packages a service as a virtual machine image, such as an EC2 \nAMI, containing everything required to run the service, including the language runtime. At runtime, \neach service instance is a VM, such as an EC2 instance, instantiated from that image. An EC2 Elastic \nLoad Balancer routes requests to the instances.\n \n\n\n392\nCHAPTER 12\nDeploying microservices\nLet’s look at the benefits and drawbacks of using this approach.\n12.2.1 The benefits of deploying services as VMs\nThe Service as a virtual machine pattern has a number of benefits:\nThe VM image encapsulates the technology stack.\nIsolated service instances.\nUses mature cloud infrastructure.\nLet’s look at each one.\nTHE VM IMAGE ENCAPSULATES THE TECHNOLOGY STACK\nAn important benefit of this pattern is that the VM image contains the service and all\nof its dependencies. It eliminates the error-prone requirement to correctly install and\nset up the software that a service needs in order to run. Once a service has been pack-\naged as a virtual machine, it becomes a black box that encapsulates your service’s tech-\nnology stack. The VM image can be deployed anywhere without modification. The API\nfor deploying the service becomes the VM management API. Deployment becomes\nmuch simpler and more reliable. \nSERVICE INSTANCES ARE ISOLATED\nA major benefit of virtual machines is that each service instance runs in complete iso-\nlation. That, after all, is one of the main goals of virtual machine technology. Each vir-\ntual machine has a fixed amount of CPU and memory and can’t steal resources from\nother services. \nUSES MATURE CLOUD INFRASTRUCTURE\nAnother benefit of deploying your microservices as virtual machines is that you can\nleverage mature, highly automated cloud infrastructure. Public clouds such as AWS\nattempt to schedule VMs on physical machines in a way that avoids overloading the\nmachine. They also provide valuable features such as load balancing of traffic across\nVMs and autoscaling. \n12.2.2 The drawbacks of deploying services as VMs\nThe Service as a VM pattern also has some drawbacks:\nLess-efficient resource utilization\nRelatively slow deployments\nSystem administration overhead\nLet’s look at each drawback in turn.\n(continued)\nElastic Beanstalk can also deploy Docker containers. Each EC2 instance runs a col-\nlection of one or more containers. Unlike a Docker orchestration framework, covered\nlater in the chapter, the unit of scaling is the EC2 instance rather than a container.\n \n\n\n393\nDeploying services using the Service as a container pattern\nLESS-EFFICIENT RESOURCE UTILIZATION\nEach service instance has the overhead of an entire virtual machine, including its\noperating system. Moreover, a typical public IaaS virtual machine offers a limited set\nof VM sizes, so the VM will probably be underutilized. This is less likely to be a prob-\nlem for Java-based services because they’re relatively heavyweight. But this pattern\nmight be an inefficient way of deploying lightweight NodeJS and GoLang services. \nRELATIVELY SLOW DEPLOYMENTS\nBuilding a VM image typically takes some number of minutes because of the size of\nthe VM. There are lots of bits to be moved over the network. Also, instantiating a VM\nfrom a VM image is time consuming because of, once again, the amount of data that\nmust be moved over the network. The operating system running inside the VM also\ntakes some time to boot, though slow is a relative term. This process, which perhaps\ntakes minutes, is much faster than the traditional deployment process. But it’s much\nslower than the more lightweight deployment patterns you’ll read about soon. \nSYSTEM ADMINISTRATION OVERHEAD\nYou’re responsible for patching the operation system and runtime. System administra-\ntion may seem inevitable when deploying software, but later in section 12.5, I describe\nserverless deployment, which eliminates this kind of system administration.\n Let’s now look at an alternative way to deploy microservices that’s more light-\nweight, yet still has many of the benefits of virtual machines. \n12.3\nDeploying services using the Service as \na container pattern\nContainers are a more modern and lightweight deployment mechanism. They’re an\noperating-system-level virtualization mechanism. A container, as figure 12.7 shows,\nconsists of usually one but sometimes multiple processes running in a sandbox, which\nisolates it from other containers. A container running a Java service, for example,\nwould typically consist of the JVM process.\n From the perspective of a process running in a container, it’s as if it’s running on\nits own machine. It typically has its own IP address, which eliminates port conflicts. All\nJava processes can, for example, listen on port 8080. Each container also has its own\nroot filesystem. The container runtime uses operating system mechanisms to isolate\nthe containers from each other. The most popular example of a container runtime is\nDocker, although there are others, such as Solaris Zones.\nPattern: Deploy a service as a container\nDeploy services packaged as container images into production. Each service instance\nis a container. See http://microservices.io/patterns/deployment/service-per-container\n.html.\n \n\n\n394\nCHAPTER 12\nDeploying microservices\nWhen you create a container, you can specify its CPU, memory resources, and, depend-\ning on the container implementation, perhaps the I/O resources. The container run-\ntime enforces these limits and prevents a container from hogging the resources of its\nmachine. When using a Docker orchestration framework such as Kubernetes, it’s espe-\ncially important to specify a container’s resources. That’s because the orchestration\nframework uses a container’s requested resources to select the machine to run the\ncontainer and thereby ensure that machines aren’t overloaded.\n Figure 12.8 shows the process of deploying a service as a container. At build-time,\nthe deployment pipeline uses a container image-building tool, which reads the ser-\nvice’s code and a description of the image, to create the container image and stores it\nin a registry. At runtime, the container image is pulled from the registry and used to\ncreate containers.\n Let’s take a look at build-time and runtime steps in more detail.\n \n \nContainer\nMachine\nService\nprocess\nContainer\nContainer runtime, such as Docker\nService\nprocess\nContainer\nService\nprocess\nOperating System\nEach container is a sandbox\nthat isolates the processes.\nShared by all of the containers\nFigure 12.7\nA container consists of one or more processes \nrunning in an isolated sandbox. Multiple containers usually run \non a single machine. The containers share the operating system.\n \n\n\n395\nDeploying services using the Service as a container pattern\n12.3.1 Deploying services using Docker\nTo deploy a service as a container, you must package it as a container image. A container\nimage is a filesystem image consisting of the application and any software required to\nrun the service. It’s often a complete Linux root filesystem, although more lightweight\nimages are also used. For example, to deploy a Spring Boot-based service, you build a\ncontainer image containing the service’s executable JAR and the correct version of\nthe JDK. Similarly, to deploy a Java web application, you would build a container\nimage containing the WAR file, Apache Tomcat, and the JDK.\nBUILDING A DOCKER IMAGE\nThe first step in building an image is to create a Dockerfile. A Dockerfile describes how\nto build a Docker container image. It specifies the base container image, a series of\ninstructions for installing software and configuring the container, and the shell com-\nmand to run when the container is created. Listing 12.1 shows the Dockerfile used to\nbuild an image for Restaurant Service. It builds a container image containing the\nservice’s executable JAR file. It configures the container to run the java -jar com-\nmand on startup.\nBuild time\nRuntime\n$ docker build ...\nDeployed\nas\nDeployed\nas\nService\ninstance\nContainer\nVM\nVM\nContainer\nimage registry\nService\ninstance\nContainer\nService\ninstance\nContainer\nService\ncode\nContainer runtime\nContainer runtime\nDeployment pipeline\nCreates\nContainer\nbuilder tool\nDocker\nﬁle\nService\ncontainer\nimage\nFigure 12.8\nA service is packaged as a container image, which is stored in a registry. At runtime \nthe service consists of multiple containers instantiated from that image. Containers typically run on \nvirtual machines. A single VM will usually run multiple containers.\n \n\n\n396\nCHAPTER 12\nDeploying microservices\nFROM openjdk:8u171-jre-alpine\nRUN apk --no-cache add curl\n  \nCMD java ${JAVA_OPTS} -jar ftgo-restaurant-service.jar\n  \nHEALTHCHECK --start-period=30s --\ninterval=5s CMD curl http://localhost:8080/actuator/health || exit 1      \nCOPY build/libs/ftgo-restaurant-service.jar .\n  \nThe base image openjdk:8u171-jre-alpine is a minimal footprint Linux image con-\ntaining the JRE. The Dockerfile copies the service’s JAR into the image and config-\nures the image to execute the JAR on startup. It also configures Docker to periodically\ninvoke the health check endpoint, described in chapter 11. The HEALTHCHECK direc-\ntive says to invoke the health check endpoint API, described in chapter 11, every 5 sec-\nonds after an initial 30-second delay, which gives the service time to start.\n Once you’ve written the Dockerfile, you can then build the image. The following\nlisting shows the shell commands to build the image for Restaurant Service. The\nscript builds the service’s JAR file and executes the docker build command to create\nthe image.\ncd ftgo-restaurant-service\n../gradlew assemble\n  \ndocker build -t ftgo-restaurant-service .\n  \nThe docker build command has two arguments: the -t argument specifies the name\nof the image, and the . specifies what Docker calls the context. The context, which in\nthis example is the current directory, consists of Dockerfile and the files used to\nbuild the image. The docker build command uploads the context to the Docker dae-\nmon, which builds the image. \nPUSHING A DOCKER IMAGE TO A REGISTRY\nThe final step of the build process is to push the newly built Docker image to what is\nknown as a registry. A Docker registry is the equivalent of a Java Maven repository for\nJava libraries, or a NodeJS npm registry for NodeJS packages. Docker hub is an exam-\nple of a public Docker registry and is equivalent to Maven Central or NpmJS.org. But\nfor your applications you’ll probably want to use a private registry provided by ser-\nvices, such as Docker Cloud registry or AWS EC2 Container Registry.\n You must use two Docker commands to push an image to a registry. First, you use\nthe docker tag command to give the image a name that’s prefixed with the hostname\nListing 12.1\nThe Dockerfile used to build Restaurant Service\nListing 12.2\nThe shell commands used to build the container image for \nRestaurant Service\nThe base image\nInstall curl for \nuse by the \nhealth check.\nConfigure Docker \nto run java -jar .. \nwhen the container \nis started.\nConfigure Docker to\ninvoke the health\ncheck endpoint.\nCopies the JAR in Gradle’s build\ndirectory into the image\nChange to the \nservice’s directory.\nBuild the \nservice’s JAR.\nBuild the image.\n \n\n\n397\nDeploying services using the Service as a container pattern\nand optional port of the registry. The image name is also suffixed with the version,\nwhich will be important when you make a new release of the service. For example, if\nthe hostname of the registry is registry.acme.com, you would use this command to\ntag the image:\ndocker tag ftgo-restaurant-service registry.acme.com/ftgo-restaurant-\nservice:1.0.0.RELEASE\nNext you use the docker push command to upload that tagged image to the registry:\ndocker push registry.acme.com/ftgo-restaurant-service:1.0.0.RELEASE\nThis command often takes much less time than you might expect. That’s because a\nDocker image has what’s known as a layered file system, which enables Docker to only\ntransfer part of the image over the network. An image’s operating system, Java run-\ntime, and the application are in separate layers. Docker only needs to transfer those\nlayers that don’t exist in the destination. As a result, transferring an image over a net-\nwork is quite fast when Docker only has to move the application’s layers, which are a\nsmall fraction of the image.\n Now that we’ve pushed the image to a registry, let’s look at how to create a\ncontainer. \nRUNNING A DOCKER CONTAINER\nOnce you’ve packaged your service as a container image, you can then create one or\nmore containers. The container infrastructure will pull the image from the registry\nonto a production server. It will then create one or more containers from that image.\nEach container is an instance of your service.\n As you might expect, Docker provides a docker run command that creates and\nstarts a container. Listing 12.3 shows how to use this command to run Restaurant\nService. The docker run command has several arguments, including the container\nimage and a specification of environment variables to set in the runtime container.\nThese are used to pass an externalized configuration, such as the database’s network\nlocation and more.\ndocker run \\\n-d\n\\\n  \n--name ftgo-restaurant-service\n\\\n   \n-p 8082:8080\n\\\n   \n-e SPRING_DATASOURCE_URL=... -e SPRING_DATASOURCE_USERNAME=...\n\\  \n-e SPRING_DATASOURCE_PASSWORD=... \\\nregistry.acme.com/ftgo-restaurant-service:1.0.0.RELEASE\n  \nListing 12.3\nUsing docker run to run a containerized service\nRuns it as a \nbackground daemon\nThe name of \nthe container\nBinds port 8080 of the \ncontainer to port 8082 \nof the host machine\nEnvironment \nvariables\nImage to run\n \n\n\n398\nCHAPTER 12\nDeploying microservices\nThe docker run command pulls the image from the registry if necessary. It then cre-\nates and starts the container, which runs the java -jar command specified in the\nDockerfile.\n Using the docker run command may seem simple, but there are a couple of prob-\nlems. One is that docker run isn’t a reliable way to deploy a service, because it creates\na container running on a single machine. The Docker engine provides some basic\nmanagement features, such as automatically restarting containers if they crash or if\nthe machine is rebooted. But it doesn’t handle machine crashes.\n Another problem is that services typically don’t exist in isolation. They depend on\nother services, such as databases and message brokers. It would be nice to deploy or\nundeploy a service and its dependencies as a unit.\n A better approach that’s especially useful during development is to use Docker\nCompose. Docker Compose is a tool that lets you declaratively define a set of contain-\ners using a YAML file, and then start and stop those containers as a group. What’s\nmore, the YAML file is a convenient way to specify numerous externalized configura-\ntion properties. To learn more about Docker Compose, I recommend reading Docker\nin Action by Jeff Nickoloff (Manning, 2016) and looking at the docker-compose.yml\nfile in the example code.\n The problem with Docker Compose, though, is that it’s limited to a single machine.\nTo deploy services reliably, you must use a Docker orchestration framework, such as\nKubernetes, which turns a set of machines into a pool of resources. I describe how to\nuse Kubernetes later, in section 12.4. First, let’s review the benefits and drawbacks of\nusing containers. \n12.3.2 Benefits of deploying services as containers\nDeploying services as containers has several benefits. First, containers have many of\nthe benefits of virtual machines:\nEncapsulation of the technology stack in which the API for managing your ser-\nvices becomes the container API.\nService instances are isolated.\nService instances’s resources are constrained.\nBut unlike virtual machines, containers are a lightweight technology. Container\nimages are typically fast to build. For example, on my laptop it takes as little as five sec-\nonds to package a Spring Boot application as a container image. Moving a container\nimage over the network, such as to and from the container registry, is also relatively\nfast, primarily because only a subset of an image’s layers need to be transferred. Con-\ntainers also start very quickly, because there’s no lengthy OS boot process. When a\ncontainer starts, all that runs is the service. \n \n\n\n399\nDeploying the FTGO application with Kubernetes\n12.3.3 Drawbacks of deploying services as containers\nOne significant drawback of containers is that you’re responsible for the undifferenti-\nated heavy lifting of administering the container images. You must patch the operat-\ning system and runtime. Also, unless you’re using a hosted container solution such as\nGoogle Container Engine or AWS ECS, you must administer the container infrastruc-\nture and possibly the VM infrastructure it runs on. \n12.4\nDeploying the FTGO application with Kubernetes\nNow that we’ve looked at containers and their trade-offs, let’s look at how to deploy\nthe FTGO application’s Restaurant Service using Kubernetes. Docker Compose,\ndescribed in section 12.3.1, is great for development and testing. But to reliably run\ncontainerized services in production, you need to use a much more sophisticated con-\ntainer runtime, such as Kubernetes. Kubernetes is a Docker orchestration framework,\na layer of software on top of Docker that turns a set of machines into a single pool of\nresources for running services. It endeavors to keep the desired number of instances\nof each service running at all times, even when service instances or machines crash.\nThe agility of containers combined with the sophistication of Kubernetes is a compel-\nling way to deploy services.\n In this section, I first give an overview of Kubernetes, its functionality, and its archi-\ntecture. After that, I show how to deploy a service using Kubernetes. Kubernetes is a\ncomplex topic, and covering it exhaustively is beyond the scope of this book, so I only\nshow how to use Kubernetes from the perspective of a developer. For more informa-\ntion, I recommend Kubernetes in Action by Marko Luksa (Manning, 2018).\n12.4.1 Overview of Kubernetes\nKubernetes is a Docker orchestration framework. A Docker orchestration framework treats\na set of machines running Docker as a pool of resources. You tell the Docker orches-\ntration framework to run N instances of your service, and it handles the rest. Figure 12.9\nshows the architecture of a Docker orchestration framework.\n A Docker orchestration framework, such as Kubernetes , has three main functions:\nResource management—Treats a cluster of machines as a pool of CPU, memory,\nand storage volumes, turning the collection of machines into a single machine.\nScheduling—Selects the machine to run your container. By default, scheduling\nconsiders the resource requirements of the container and each node’s available\nresources. It might also implement affinity, which colocates containers on the\nsame node, and anti-affinity, which places containers on different nodes.\nService management—Implements the concept of named and versioned services\nthat map directly to services in the microservice architecture. The orchestration\nframework ensures that the desired number of healthy instances is running at\nall times. It load balances requests across them. The orchestration framework\nperforms rolling upgrades of services and lets you roll back to an old version.\n \n\n\n400\nCHAPTER 12\nDeploying microservices\nDocker orchestration frameworks are an increasingly popular way to deploy applica-\ntions. Docker Swarm is part of the Docker engine, so is easy to set up and use. Kuber-\nnetes is much more complex to set up and administer, but it’s much more sophisticated.\nAt the time of writing, Kubernetes has tremendous momentum, with a massive open\nsource community. Let’s take a closer look at how it works.\nKUBERNETES ARCHITECTURE\nKubernetes runs on a cluster of machines. Figure 12.10 shows the architecture of a\nKubernetes cluster. Each machine in a Kubernetes cluster is either a master or a node.\nA typical cluster has a small number of masters—perhaps just one—and many nodes.\nA master machine is responsible for managing the cluster. A node is a worker than runs\none or more pods. A pod is Kubernetes’s unit of deployment and consists of a set of\ncontainers.\n A master runs several components, including the following:\nAPI server—The REST API for deploying and managing services, used by the\nkubectl command-line interface, for example.\nEtcd—A key-value NoSQL database that stores the cluster data.\nSVC\nA\nSVC\nB\nSVC\nC\nContainer\nDocker orchestration framework\nContainer\nContainer\nDocker\nOperating\nsystem\nMachine\nDocker\nOperating\nsystem\nMachine\nDocker\nOperating\nsystem\nMachine\nService management\nScheduling\nResource management\nFigure 12.9\nA Docker orchestration \nframework turns a set of machines running \nDocker into a cluster of resources. It assigns \ncontainers to machines. The framework \nattempts to keep the desired number of \nhealthy containers running at all times.\n \n\n\n401\nDeploying the FTGO application with Kubernetes\nScheduler—Selects a node to run a pod.\nController manager—Runs the controllers, which ensure that the state of the clus-\nter matches the intended state. For example, one type of controller known as a\nreplication controller ensures that the desired number of instances of a service\nare running by starting and terminating instances.\nA node runs several components, including the following:\nKubelet—Creates and manages the pods running on the node\nKube-proxy—Manages networking, including load balancing across pods\nPods—The application services\nSVC\nPod\nKubernetes master\netcd\nKubelet\nKube-proxy\nKubernetes node\nSVC\nPod\nKubelet\nKube-proxy\nKubernetes node\nApplication\nrequests\nConﬁguration\ncommands\nDeveloper\nAplication\nuser\nDeployment\npipeline\nKubecti\nCLI\nAPI Server\nController\nmanagement\nScheduler\nFigure 12.10\nA Kubernetes cluster consists of a master, which manages the cluster, and nodes, \nwhich run the services. Developers and the deployment pipeline interact with Kubernetes through the \nAPI server, which along with other cluster-management software runs on the master. Application \ncontainers run on nodes. Each node runs a Kubelet, which manages the application container, and a \nkube-proxy, which routes application requests to the pods, either directly as a proxy or indirectly by \nconfiguring iptables routing rules built into the Linux kernel.\n \n\n\n402\nCHAPTER 12\nDeploying microservices\nLet’s now look at key Kubernetes concepts you’ll need to master to deploy services on\nKubernetes. \nKEY KUBERNETES CONCEPTS\nAs mentioned in the introduction to this section, Kubernetes is quite complex. But it’s\npossible to use Kubernetes productively once you master a few key concepts, called\nobjects. Kubernetes defines many types of objects. From a developer’s perspective, the\nmost important objects are the following:\nPod—A pod is the basic unit of deployment in Kubernetes. It consists of one or\nmore containers that share an IP address and storage volumes. The pod for a\nservice instance often consists of a single container, such as a container running\nthe JVM. But in some scenarios a pod contains one or more sidecar containers,\nwhich implement supporting functions. For example, an NGINX server could\nhave a sidecar that periodically does a git pull to download the latest version\nof the website. A pod is ephemeral, because either the pod’s containers or the\nnode it’s running on might crash.\nDeployment—A declarative specification of a pod. A deployment is a controller\nthat ensures that the desired number of instances of the pod (service instances)\nare running at all times. It supports versioning with rolling upgrades and roll-\nbacks. Later in section 12.4.2, you’ll see that each service in a microservice\narchitecture is a Kubernetes deployment.\nService—Provides clients of an application service with a static/stable network\nlocation. It’s a form of infrastructure-provided service discovery, described in\nchapter 3. A service has an IP address and a DNS name that resolves to that IP\naddress and load balances TCP and UDP traffic across one or more pods. The\nIP address and a DNS name are only accessible within the Kubernetes. Later, I\ndescribe how to configure services that are accessible from outside the cluster.\nConfigMap—A named collection of name-value pairs that defines the external-\nized configuration for one or more application services (see chapter 11 for an\noverview of externalized configuration). The definition of a pod’s container\ncan reference a ConfigMap to define the container’s environment variables. It\ncan also use a ConfigMap to create configuration files inside the container. You\ncan store sensitive information, such as passwords, in a form of ConfigMap\ncalled a Secret.\nNow that we’ve reviewed the key Kubernetes concepts, let’s see them in action by look-\ning at how to deploy an application service on Kubernetes. \n12.4.2 Deploying the Restaurant service on Kubernetes\nAs mentioned earlier, to deploy a service on Kubernetes, you need to define a deploy-\nment. The easiest way to create a Kubernetes object such as a deployment is by writing\na YAML file. Listing 12.4 is a YAML file defining a deployment for Restaurant Service.\nThis deployment specifies running two replicas of a pod. The pod has just one container.\n \n\n\n403\nDeploying the FTGO application with Kubernetes\nThe container definition specifies the Docker image running along with other attri-\nbutes, such as the values of environment variables. The container’s environment vari-\nables are the service’s externalized configuration. They are read by Spring Boot and\nmade available as properties in the application context.\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\nname: ftgo-restaurant-service\nspec:\nreplicas: 2\ntemplate:\nmetadata:\nlabels:\napp: ftgo-restaurant-service\n   \nspec:\n  \n   containers:\n- name: ftgo-restaurant-service\n  image: msapatterns/ftgo-restaurant-service:latest\n  imagePullPolicy: Always\n  ports:\n  - containerPort: 8080          \n    name: httpport\n  env:                              \n    - name: JAVA_OPTS\n      value: \"-Dsun.net.inetaddr.ttl=30\"\n    - name: SPRING_DATASOURCE_URL\n      value: jdbc:mysql://ftgo-mysql/eventuate\n    - name: SPRING_DATASOURCE_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: ftgo-db-secret\n          key: username\n    - name: SPRING_DATASOURCE_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: ftgo-db-secret            \n          key: password\n    - name: SPRING_DATASOURCE_DRIVER_CLASS_NAME\n      value: com.mysql.jdbc.Driver\n    - name: EVENTUATELOCAL_KAFKA_BOOTSTRAP_SERVERS\n      value: ftgo-kafka:9092\n    - name: EVENTUATELOCAL_ZOOKEEPER_CONNECTION_STRING\n      value: ftgo-zookeeper:2181\n  livenessProbe:\n    httpGet:\n      path: /actuator/health\n      port: 8080\n    initialDelaySeconds: 60\n    periodSeconds: 20\n  readinessProbe:\nListing 12.4\nKubernetes Deployment for ftgo-restaurant-service\nSpecifies that this is an \nobject of type Deployment\nThe name of the deployment\nNumber of pod replicas\nGives each pod a label \ncalled app whose value is \nftgo-restaurant-service\nThe specification of \nthe pod, which defines \njust one container\n The container’s port\nThe container’s environment \nvariables, which are read by \nSpring Boot\nSensitive values that \nare retrieved from the \nKubernetes Secret \ncalled ftgo-db-secret\nConfigure Kubernetes \nto invoke the health \ncheck endpoint.\n \n\n\n404\nCHAPTER 12\nDeploying microservices\n    httpGet:\n      path: /actuator/health\n      port: 8080\n    initialDelaySeconds: 60\n    periodSeconds: 20\nThis deployment definition configures Kubernetes to invoke Restaurant Service’s\nhealth check endpoint. As described in chapter 11, a health check endpoint enables\nKubernetes to determine the health of the service instance. Kubernetes implements\ntwo different checks. The first check is readinessProbe, which it uses to determine\nwhether it should route traffic to a service instance. In this example, Kubernetes\ninvokes the /actuator/health HTTP endpoint every 20 seconds after an initial 30-\nsecond delay, which gives it a chance to initialize. If some number (default is 1) of\nconsecutive readinessProbes succeeds, Kubernetes considers the service to be ready,\nwhereas if some number (default, 3) of consecutive readinessProbes fail, it’s consid-\nered not to be ready. Kubernetes will only route traffic to the service instance when\nthe readinessProbe indicates that it’s ready.\n The second health check is the livenessProbe. It’s configured the same way as the\nreadinessProbe. But rather than determine whether traffic should be routed to a ser-\nvice instance, the livenessProbe determines whether Kubernetes should terminate\nand restart the service instance. If some number (default, 3) of consecutive liveness-\nProbes fail in a row, Kubernetes will terminate and restart the service.\n Once you’ve written the YAML file, you can create or update the deployment by\nusing the kubectl apply command:\nkubectl apply -f ftgo-restaurant-service/src/deployment/kubernetes/ftgo-\nrestaurant-service.yml\nThis command makes a request to the Kubernetes API server that results in the cre-\nation of the deployment and the pods.\n To create this deployment, you must first create the Kubernetes Secret called\nftgo-db-secret. One quick and insecure way to do that is as follows:\nkubectl create secret generic ftgo-db-secret \\\n--from-literal=username=mysqluser --from-literal=password=mysqlpw\nThis command creates a secret containing the database user ID and password speci-\nfied on the command line. See the Kubernetes documentation (https://kubernetes\n.io/docs/concepts/configuration/secret/#creating-your-own-secrets) for more secure\nways to create secrets.\nCREATING A KUBERNETES SERVICE\nAt this point the pods are running, and the Kubernetes deployment will do its best to\nkeep them running. The problem is that the pods have dynamically assigned IP\naddresses and, as such, aren’t that useful to a client that wants to make an HTTP\nrequest. As described in chapter 3, the solution is to use a service discovery mechanism.\n \n\n\n405\nDeploying the FTGO application with Kubernetes\nOne approach is to use a client-side discovery mechanism and install a service registry,\nsuch as Netflix OSS Eureka. Fortunately, we can avoid doing that by using the service\ndiscovery mechanism built in to Kubernetes and define a Kubernetes service.\n A service is a Kubernetes object that provides the clients of one or more pods with a\nstable endpoint. It has an IP address and a DNS name that resolves that IP address.\nThe service load balances traffic to that IP address across the pods. Listing 12.5\nshows the Kubernetes service for Restaurant Service. This service routes traffic\nfrom http://ftgo-restaurant-service:8080 to the pods defined by the deploy-\nment shown in the listing.\napiVersion: v1\nkind: Service\nmetadata:\nname: ftgo-restaurant-service\n \nspec:\nports:\n- port: 8080\ntargetPort: 8080\nselector:\n app: ftgo-restaurant-service\n  \n---\nThe key part of the service definition is selector, which selects the target pods. It selects\nthose pods that have a label named app with the value ftgo-restaurant-service. If\nyou look closely, you’ll see that the container defined in listing 12.4 has such a label.\n Once you’ve written the YAML file, you can create the service using this command:\nkubectl apply -f ftgo-restaurant-service-service.yml\nNow that we’ve created the Kubernetes service, any clients of Restaurant Service\nthat are running inside the Kubernetes cluster can access its REST API via http://\nftgo-restaurant-service:8080. Later, I discuss how to upgrade running services,\nbut first let’s take a look at how to make the services accessible from outside the\nKubernetes cluster. \n12.4.3 Deploying the API gateway\nThe Kubernetes service for Restaurant Service, shown in listing 12.5, is only accessi-\nble from within the cluster. That’s not a problem for Restaurant Service, but what\nabout API Gateway? Its role is to route traffic from the outside world to the service. It\ntherefore needs to be accessible from outside the cluster. Fortunately, a Kubernetes\nservice supports this use case as well. The service we looked at earlier is a ClusterIP\nservice, which is the default, but there are, however, two other types of services: Node-\nPort and LoadBalancer.\nListing 12.5\nThe YAML definition of the Kubernetes service for \nftgo-restaurant-service\nThe name of the service, \nalso the DNS name\nThe exposed \nport\nThe container port \nto route traffic to\nSelects the containers \nto route traffic to\n \n\n\n406\nCHAPTER 12\nDeploying microservices\n A NodePort service is accessible via a cluster-wide port on all the nodes in the clus-\nter. Any traffic to that port on any cluster node is load balanced to the backend pods.\nYou must select an available port in the range of 30000–32767. For example, listing 12.6\nshows a service that routes traffic to port 30000 of Consumer Service.\napiVersion: v1\nkind: Service\nmetadata:\nname: ftgo-api-gateway\nspec:\ntype: NodePort\n  \nports:\n- nodePort: 30000\n  \nport: 80\ntargetPort: 8080\nselector:\napp: ftgo-api-gateway\n---\nAPI Gateway is within the cluster using the URL http://ftgo-api-gateway and out-\nside the URL http://<node-ip-address>:3000/, where node-ip-address is the IP\naddress of one of the nodes. After configuring a NodePort service you can, for exam-\nple, configure an AWS Elastic Load Balancer (ELB) to load balance requests from the\ninternet across the nodes. A key benefit of this approach is that the ELB is entirely\nunder your control. You have complete flexibility when configuring it.\n A NodePort type service isn’t the only option, though. You can also use a Load-\nBalancer service, which automatically configures a cloud-specific load balancer. The\nload balancer will be an ELB if Kubernetes is running on AWS. One benefit of this\ntype of service is that you no longer have to configure your own load balancer. The\ndrawback, however, is that although Kubernetes does give a few options for configur-\ning the ELB, such the SSL certificate, you have a lot less control over its configuration. \n12.4.4 Zero-downtime deployments\nImagine you’ve updated Restaurant Service and want to deploy those changes into\nproduction. Updating a running service is a simple three-step process when using\nKubernetes:\n1\nBuild a new container image and push it to the registry using the same process\ndescribed earlier. The only difference is that the image will be tagged with a dif-\nferent version tag—for example, ftgo-restaurant-service:1.1.0.RELEASE.\n2\nEdit the YAML file for the service’s deployment so that it references the new image.\n3\nUpdate the deployment using the kubectl apply -f command.\nKubernetes will then perform a rolling upgrade of the pods. It will incrementally cre-\nate pods running version 1.1.0.RELEASE and terminate the pods running version\nListing 12.6\nThe YAML definition of a NodePort service that routes traffic to port \n8082 of Consumer Service\nSpecifies a type \nof NodePort\nThe cluster-\nwide port\n \n\n\n407\nDeploying the FTGO application with Kubernetes\n1.0.0.RELEASE. What’s great about how Kubernetes does this is that it doesn’t ter-\nminate old pods until their replacements are ready to handle requests. It uses the\nreadinessProbe mechanism, a health check mechanism described earlier in this\nsection, to determine whether a pod is ready. As a result, there will always be pods\navailable to handle requests. Eventually, assuming the new pods start successfully, all\nthe deployment’s pods will be running the new version.\n But what if there’s a problem and the version 1.1.0.RELEASE pods don’t start?\nPerhaps there’s a bug, such as a misspelled container image name or a missing envi-\nronment variable for a new configuration property. If the pods fail to start, the deploy-\nment will become stuck. At that point, you have two options. One option is to fix the\nYAML file and rerun kubectl apply -f to update the deployment. The other option is\nto roll back the deployment.\n A deployment maintains the history of what are termed rollouts. Each time you\nupdate the deployment, it creates a new rollout. As a result, you can easily roll back a\ndeployment to a previous version by executing the following command:\nkubectl rollout undo deployment ftgo-restaurant-service\nKubernetes will then replace the pods running version 1.1.0.RELEASE with pods run-\nning the older version, 1.0.0.RELEASE.\n A Kubernetes deployment is a good way to deploy a service without downtime. But\nwhat if a bug only appears after the pod is ready and receiving production traffic? In\nthat situation, Kubernetes will continue to roll out new versions, so a growing number\nof users will be impacted. Though your monitoring system will hopefully detect the issue\nand quickly roll back the deployment, you won’t avoid impacting at least some users. To\naddress this issue and make rolling out a new version of a service more reliable, we need\nto separate deploying, which means getting the service running in production, from\nreleasing the service, which means making it available to handle production traffic.\nLet’s look at how to accomplish that using a service mesh. \n12.4.5 Using a service mesh to separate deployment from release\nThe traditional way to roll out a new version of a service is to first test it in a staging\nenvironment. Then, once it’s passed the test in staging, you deploy in production by\ndoing a rolling upgrade that replaces old instances of the service with new service\ninstances. On one hand, as you just saw, Kubernetes deployments make doing a roll-\ning upgrade very straightforward. On the other hand, this approach assumes that\nonce a service version has passed the tests in the staging environment, it will work in\nproduction. Sadly, this is not always the case.\n One reason is because staging is unlikely to be an exact clone, if for no other reason\nthan the production environment is likely to be much larger and handle much more\ntraffic. It’s also time consuming to keep the two environments synchronized. As a result\nof discrepancies, it’s likely that some bugs will only show up in production. And even it\nwere an exact clone, you can’t guarantee that testing will catch all bugs.\n \n\n\n408\nCHAPTER 12\nDeploying microservices\n A much more reliable way to roll out a new version is to separate deployment from\nrelease:\nDeployment—Running in the production environment\nReleasing a service—Making it available to end users\nYou then deploy a service into production using the following steps:\n1\nDeploy the new version into production without routing any end-user requests\nto it.\n2\nTest it in production.\n3\nRelease it to a small number of end users.\n4\nIncrementally release it to an increasingly larger number of users until it’s han-\ndling all the production traffic.\n5\nIf at any point there’s an issue, revert back to the old version—otherwise, once\nyou’re confident the new version is working correctly, delete the old version.\nIdeally, those steps will be performed by a fully automated deployment pipeline that\ncarefully monitors the newly deployed service for errors.\n Traditionally, separating deployments and releases in this way has been challeng-\ning because it requires a lot of work to implement it. But one of the benefits of using a\nservice mesh is that using this style of deployment is a lot easier. A service mesh is, as\ndescribed in chapter 11, networking infrastructure that mediates all communication\nbetween a service and other services and external applications. In addition to taking\non some of the responsibilities of the microservice chassis framework, a service mesh\nprovides rule-based load balancing and traffic routing that lets you safely run multiple\nversions of your services simultaneously. Later in this section, you’ll see that you can\nroute test users to one version of a service and end-users to a different version, for\nexample.\n As described in chapter 11, there are several service meshes to choose from. In this\nsection, I show you how to use Istio, a popular, open source service mesh originally\ndeveloped by Google, IBM, and Lyft. I begin by providing a brief overview of Istio and\na few of its many features. Next I describe how to deploy an application using Istio.\nAfter that, I show how to use its traffic-routing capabilities to deploy and release an\nupgrade to a service.\nOVERVIEW OF THE ISTIO SERVICE MESH\nThe Istio website describes Istio as an “An open platform to connect, manage, and\nsecure microservices” (https://istio.io). It’s a networking layer through which all of\nyour services’ network traffic flows. Istio has a rich set of features organized into four\nmain categories:\nTraffic management—Includes service discovery, load balancing, routing rules,\nand circuit breakers\nSecurity—Secures interservice communication using Transport Layer Security\n(TLS)\n \n\n\n409\nDeploying the FTGO application with Kubernetes\nTelemetry—Captures metrics about network traffic and implements distributed\ntracing\nPolicy enforcement—Enforces quotas and rate limits\nThis section focuses on Istio’s traffic-management capabilities.\n Figure 12.11 shows Istio’s architecture. It consists of a control plane and a data\nplane. The control plane implements management functions, including configuring\nthe data plane to route traffic. The data plane consists of Envoy proxies, one per ser-\nvice instance.\n The two main components of the control plane are the Pilot and the Mixer. The Pilot\nextracts information about deployed services from the underlying infrastructure. When\nrunning on Kubernetes, for example, the Pilot retrieves the services and healthy pods. It\nconfigures the Envoy proxies to route traffic according to the defined routing rules. The\nMixer collects telemetry from the Envoy proxies and enforces policies.\nAPI Gateway\ncontainer\nGET/consumers/1\nGET/consumers/1\nGET/consumers/1\nHost: ftgo-consumer-service\nGET/consumers/1\nHost: ftgo-consumer-service\nPod\nService registry\nConsumer\nService\ncontainer\nIstio Envoy\ncontainer\nLogging\nServer\nService\nPod\nMetrics\nServer\nIstio Envoy\ncontainer\nMixer\nPilot\nIstio control plane\nConﬁgures\nChecks\nTelemetry\nKubernetes\nPod\nKey\nConﬁguration\nRequests\nPolicy check\nTelemetry\nMonitoring infrastructure\nIstio data plane\nQueries for deployed services\nFigure 12.11\nIstio consists of a control plane, whose components include the Pilot and the Mixer, and a data \nplane, which consists of Envoy proxy servers. The Pilot extracts information about deployed services from the \nunderlying infrastructure and configures the data plane. The Mixer enforces policies such as quotas and gathers \ntelemetry, reporting it to the monitoring infrastructure servers. The Envoy proxy servers route traffic in and out of \nservices. There’s one Envoy proxy server per service instance.\n \n\n\n410\nCHAPTER 12\nDeploying microservices\nThe Istio Envoy proxy is a modified version of Envoy (www.envoyproxy.io). It’s a high-\nperformance proxy that supports a variety of protocols, including TCP, low-level pro-\ntocols such as HTTP and HTTPS, and higher-level protocols. It also understands\nMongoDB, Redis, and DynamoDB protocols. Envoy also supports robust interservice\ncommunication with features such as circuit breakers, rate limiting, and automatic\nretries. It can secure communication within the application by using TLS for inter-\nEnvoy communication.\n Istio uses Envoy as a sidecar, a process or container that runs alongside the service\ninstance and implements cross-cutting concerns. When running on Kubernetes, the\nEnvoy proxy is a container within the service’s pod. In other environments that don’t\nhave the pod concept, Envoy runs in the same container as the service. All traffic to\nand from a service flows through its Envoy proxy, which routes traffic according to the\nrouting rules given to it by the control plane. For example, direct Service  Service\ncommunication becomes Service  Source Envoy  Destination Envoy  Service.\nIstio is configured using Kubernetes-style YAML configuration files. It has a command-\nline tool called istioctl that’s similar to kubectl. You use istioctl for creating,\nupdating, and deleting rules and policies. When using Istio on Kubernetes, you can\nalso use kubectl.\n Let’s look at how to deploy a service with Istio. \nDEPLOYING A SERVICE WITH ISTIO\nDeploying a service on Istio is quite straightforward. You define a Kubernetes Service\nand a Deployment for each of your application’s services. Listing 12.7 shows the defini-\ntion of Service and Deployment for Consumer Service. Although it’s almost identical\nto the definitions I showed earlier, there are a few differences. That’s because Istio has\na few requirements for the Kubernetes services and pods:\nA Kubernetes service port must use the Istio naming convention of <proto-\ncol>[-<suffix>], where protocol is http, http2, grpc, mongo, or redis. If the\nport is unnamed, Istio will treat the port as a TCP port and won’t apply rule-\nbased routing.\nA pod should have an app label such as app: ftgo-consumer-service, which\nidentifies the service, in order to support Istio distributed tracing.\nIn order to run multiple versions of a service simultaneously, the name of a\nKubernetes deployment must include the version, such as ftgo-consumer-\nservice-v1, ftgo-consumer-service-v2, and so on. A deployment’s pods should\nhave a version label, such as version: v1, which specifies the version, so that\nIstio can route to a specific version.\nPattern: Sidecar\nImplement cross-cutting concerns in a sidecar process or container that runs alongside\nthe service instance. See http://microservices.io/patterns/deployment/sidecar.html.\n \n\n\n411\nDeploying the FTGO application with Kubernetes\napiVersion: v1\nkind: Service\nmetadata:\nname: ftgo-consumer-service\nspec:\nports:\n- name: http\n  \nport: 8080\ntargetPort: 8080\nselector:\napp: ftgo-consumer-service\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\nname: ftgo-consumer-service-v2\n  \nspec:\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\napp: ftgo-consumer-service\n  \nversion: v2\nspec:\ncontainers:\n- image: image: ftgo-consumer-service:v2\n  \n...\nBy now, you may be wondering how to run the Envoy proxy container in the service’s\npod. Fortunately, Istio makes that remarkably easy by automating modifying the pod\ndefinition to include the Envoy proxy. There are two ways to do that. The first is to use\nmanual sidecar injection  and run the istioctl kube-inject command:\nistioctl kube-inject -f ftgo-consumer-service/src/deployment/kubernetes/ftgo-\nconsumer-service.yml | kubectl apply -f -\nThis command reads a Kubernetes YAML file and outputs the modified configura-\ntion containing the Envoy proxy. The modified configuration is then piped into\nkubectl apply.\n The second way to add the Envoy sidecar to the pod is to use automatic sidecar injec-\ntion. When this feature is enabled, you deploy a service using kubectl apply. Kubernetes\nautomatically invokes Istio to modify the pod definition to include the Envoy proxy.\n If you describe your service’s pod, you’ll see that it consists of more than your ser-\nvice’s container:\n$ kubectl describe po ftgo-consumer-service-7db65b6f97-q9jpr\nName:\nftgo-consumer-service-7db65b6f97-q9jpr\nNamespace:\ndefault\n...\nListing 12.7\nDeploying Consumer Service with Istio\nNamed port\nVersioned \ndeployment\nRecommended \nlabels\nImage \nversion\n \n\n\n412\nCHAPTER 12\nDeploying microservices\nInit Containers:\nistio-init:\n \nImage:\ndocker.io/istio/proxy_init:0.8.0\n....\nContainers:\nftgo-consumer-service:\n \nImage:\nmsapatterns/ftgo-consumer-service:latest\n...\nistio-proxy:\nImage:\ndocker.io/istio/proxyv2:0.8.0\n  \n...\nNow that we’ve deployed the service, let’s look at how to define routing rules. \nCREATE ROUTING RULES TO ROUTE TO THE V1 VERSION\nLet’s imagine that you deployed the ftgo-consumer-service-v2 deployment. In the\nabsence of routing rules, Istio load balances requests across all versions of a service. It\nwould, therefore, load balance across versions 1 and 2 of ftgo-consumer-service,\nwhich defeats the purpose of using Istio. In order to safely roll out a new version, you\nmust define a routing rule that routes all traffic to the current v1 version.\nInitializes the pod\nThe service \ncontainer\nThe Envoy \ncontainer\nAPI gateway\npod\nVirtualService\nDestinationRule\nConsumer\nService\nv1 pod\nmetadata:\nlabels:\napp: ftgo-consumer-service\nversion: v1\nConsumer\nService\nv2 pod\nRoutes to the v\nsubset\n1\nRouting rule for the\nConsumer Service\nDeﬁnes subsets of\npods of a service\nNo trafﬁc routed to v2.\nDeﬁnes subsets\nv\nand v2\n1\nAll trafﬁc routed to v1\nmetadata:\nlabels:\napp: ftgo-consumer-service\nversion: v2\nkind: DestinationRule\nmetadata:\nname:ftgo-consumer-service\nspec:\nhost: ftgo-consumer-service\nsubsets:\n-name: v1\nlabels:\nversion: v1\n-name: v2\nlabels:\nversion: v2\nkind: VirtualService\nmetadata:\nname:ftgo-consumer-service\nspec:\nhosts:\n-ftgo-consumer-service\nhttp:\n-route:\n-destination:\nhost: ftgo-consumer-service\nsubset: v1\nweight: 100\nGET/consumers/1\nhost:ftgo-consumer-\nservice\nFigure 12.12\nThe routing rule for Consumer Service, which routes all traffic to the v1 pods. It consists of a \nVirtualService, which routes its traffic to the v1 subset, and a DestinationRule, which defines the v1 \nsubset as the pods labeled with version: v1. Once you’ve defined this rule, you can safely deploy a new version \nwithout routing any traffic to it initially.\n \n\n\n413\nDeploying the FTGO application with Kubernetes\nFigure 12.12 shows the routing rule for Consumer Service that routes all traffic to v1.\nIt consists of two Istio objects: a VirtualService and a DestinationRule.\n A VirtualService defines how to route requests for one or more hostnames. In this\nexample, VirtualService defines the routes for a single hostname: ftgo-consumer-\nservice. Here’s the definition of VirtualService for Consumer Service:\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: ftgo-consumer-service\nspec:\nhosts:\n- ftgo-consumer-service\n  \nhttp:\n- route:\n- destination:\nhost: ftgo-consumer-service\n  \nsubset: v1\nIt routes all requests for the v1 subset of the pods of Consumer Service. Later, I show\nmore complex examples that route based on HTTP requests and load balance across\nmultiple weighted destinations.\n In addition to VirtualService, you must also define a DestinationRule, which\ndefines one or more subsets of pods for a service. A subset of pods is typically a service\nversion. A DestinationRule can also define traffic policies, such as the load-balancing\nalgorithm. Here’s the DestinationRule for Consumer Service:\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\nname: ftgo-consumer-service\nspec:\nhost: ftgo-consumer-service\nsubsets:\n- name: v1\n  \nlabels:\nversion: v1\n  \n- name: v2\nlabels:\nversion: v2\nThis DestinationRule defines two subsets of pods: v1 and v2. The v1 subset selects\npods with the label version: v1. The v2 subset selects pods with the label version: v2.\n Once you’ve defined these rules, Istio will only route traffic pods labeled version:\nv1. It’s now safe to deploy v2. \n \n \nApplies to the \nConsumer Service\nRoutes to \nConsumer Service\nThe v1 subset\nThe name of \nthe subset\nThe pod selector \nfor the subset\n \n",
      "page_number": 414
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 439-454)",
      "start_page": 439,
      "end_page": 454,
      "detection_method": "topic_boundary",
      "content": "414\nCHAPTER 12\nDeploying microservices\nDEPLOYING VERSION 2 OF CONSUMER SERVICE\nHere’s an excerpt of the version 2 Deployment for Consumer Service:\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\nname: ftgo-consumer-service-v2\n  \nspec:\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\napp: ftgo-consumer-service\nversion: v2\n  \n...\nThis deployment is called ftgo-consumer-service-v2. It labels its pods with version:\nv2. After creating this deployment, both versions of the ftgo-consumer-service will be\nrunning. But because of the routing rules, Istio won’t route any traffic to v2. You’re\nnow ready to route some test traffic to v2. \nROUTING TEST TRAFFIC TO VERSION 2\nOnce you’ve deployed a new version of a service, the next step is to test it. Let’s sup-\npose that requests from test users have a testuser header . We can enhance the ftgo-\nconsumer-service VirtualService to route requests with this header to v2 instances\nby making the following change:\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: ftgo-consumer-service\nspec:\nhosts:\n- ftgo-consumer-service\nhttp:\n- match:\n- headers:\ntestuser:\nregex: \"^.+$\"\n  \nroute:\n- destination:\nhost: ftgo-consumer-service\nsubset: v2\n  \n- route:\n- destination:\nhost: ftgo-consumer-service\nsubset: v1\n  \nIn addition to the original default route, VirtualService has a routing rule that\nroutes requests with the testuser header to the v2 subset. After you’ve updated the\nrules, you can now test Consumer Service. Then, once you feel confident that the v2 is\nworking, you can route some production traffic to it. Let’s look at how to do that. \nVersion 2\nPod is labeled \nwith the version\nMatches a nonblank \ntestuser header\nRoutes test \nusers to v2\nRoutes everyone \nelse to v1\n \n\n\n415\nDeploying services using the Serverless deployment pattern\nROUTING PRODUCTION TRAFFIC TO VERSION 2\nAfter you’ve tested a newly deployed service, the next step is to start routing produc-\ntion traffic to it. A good strategy is to initially only route a small amount of traffic.\nHere, for example, is a rule that routes 95% of traffic to v1 and 5% to v2:\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: ftgo-consumer-service\nspec:\nhosts:\n- ftgo-consumer-service\nhttp:\n- route:\n- destination:\nhost: ftgo-consumer-service\nsubset: v1\nweight: 95\n- destination:\nhost: ftgo-consumer-service\nsubset: v2\nweight: 5\nAs you gain confidence that the service can handle production traffic, you can incre-\nmentally increase the amount of traffic going to the version 2 pods until it reaches\n100%. At that point, Istio isn’t routing any traffic to the v1 pods. You could leave them\nrunning for a little while longer before deleting the version 1 Deployment.\n By letting you easily separate deployment from release, Istio makes rolling out a\nnew version of a service much more reliable. Yet I’ve barely scratched the surface of\nIstio’s capabilities. As of the time of writing, the current version of Istio is 0.8. I’m\nexcited to watch it and the other service meshes mature and become a standard part\nof a production environment. \n12.5\nDeploying services using the Serverless deployment \npattern\nThe Language-specific packaging (section 12.1), Service as a VM (section 12.2), and\nService as a container (section 12.3) patterns are all quite different, but they share\nsome common characteristics. The first is that with all three patterns you must prepro-\nvision some computing resources—either physical machines, virtual machines, or con-\ntainers. Some deployment platforms implement autoscaling, which dynamically adjusts\nthe number of VMs or containers based on the load. But you’ll always need to pay for\nsome VMs or containers, even if they’re idle.\n Another common characteristic is that you’re responsible for system administra-\ntion. If you’re running any kind of machine, you must patch the operating system. In\nthe case of physical machines, this also includes racking and stacking. You’re also\nresponsible for administering the language runtime. This is an example of what Ama-\nzon called “undifferentiated heavy lifting.” Since the early days of computing, system\n \n\n\n416\nCHAPTER 12\nDeploying microservices\nadministration has been one of those things you need to do. As it turns out, though,\nthere’s a solution: serverless.\n12.5.1 Overview of serverless deployment with AWS Lambda\nAt AWS Re:Invent 2014, Werner Vogels, the CTO of Amazon, introduced AWS\nLambda with the amazing phrase “magic happens at the intersection of functions,\nevents, and data.” As this phrase suggests, AWS Lambda was initially for deploying\nevent-driven services. It’s “magic” because, as you’ll see, AWS Lambda is an example of\nserverless deployment technology.\nAWS Lambda supports Java, NodeJS, C#, GoLang, and Python. A lambda function is a\nstateless service. It typically handles requests by invoking AWS services. For example, a\nlambda function that’s invoked when an image is uploaded to an S3 bucket could\ninsert an item into a DynamoDB IMAGES table and publish a message to Kinesis to\ntrigger image processing. A lambda function can also invoke third-party web services.\n To deploy a service, you package your application as a ZIP file or JAR file, upload it\nto AWS Lambda, and specify the name of the function to invoke to handle a request\n(also called an event). AWS Lambda automatically runs enough instances of your\nmicroservice to handle incoming requests. You’re billed for each request based on the\ntime taken and the memory consumed. Of course, the devil is in the details, and later\nyou’ll see that AWS Lambda has limitations. But the notion that neither you as a devel-\noper nor anyone in your organization need worry about any aspect of servers, virtual\nmachines, or containers is incredibly powerful.\nServerless deployment technologies\nThe main public clouds all provide a serverless deployment option, although AWS\nLambda is the most advanced. Google Cloud has Google Cloud functions, which as\nof the time writing is in beta (https://cloud.google.com/functions/). Microsoft Azure\nhas Azure functions (https://azure.microsoft.com/en-us/services/functions).\nThere are also open source serverless frameworks, such as Apache Openwhisk\n(https://openwhisk.apache.org) and Fission for Kubernetes (https://fission.io), that\nyou can run on your own infrastructure. But I’m not entirely convinced of their value.\nYou need to manage the infrastructure that runs the serverless framework—which\ndoesn’t exactly sound like serverless. Moreover, as you’ll see later in this section,\nserverless provides a constrained programming model in exchange for minimal sys-\ntem administration. If you need to manage infrastructure, then you have the con-\nstraints without the benefit.\nPattern: Serverless deployment\nDeploy services using a serverless deployment mechanism provided by a public cloud.\nSee http://microservices.io/patterns/deployment/serverless-deployment.html. \n \n\n\n417\nDeploying services using the Serverless deployment pattern\n12.5.2 Developing a lambda function\nUnlike when using the other three patterns, you must use a different programming\nmodel for your lambda functions. A lambda function’s code and the packaging\ndepend on the programming language. A Java lambda function is a class that imple-\nments the generic interface RequestHandler, which is defined by the AWS Lambda\nJava core library and shown in the following listing. This interface takes two type\nparameters: I, which is the input type, and O, which is the output type. The type of I\nand O depend on the specific kind of request that the lambda handles.\npublic interface RequestHandler<I, O> {\npublic O handleRequest(I input, Context context);\n}\nThe RequestHandler interface defines a single handleRequest() method. This method\nhas two parameters, an input object and a context, which provide access to the lambda\nexecution environment, such as the request ID. The handleRequest() method\nreturns an output object. For lambda functions that handle HTTP requests that are\nproxied by an AWS API Gateway, I and O are APIGatewayProxyRequestEvent and\nAPIGatewayProxyResponseEvent, respectively. As you’ll soon see, the handler func-\ntions are quite similar to old-style Java EE servlets.\n A Java lambda is packaged as either a ZIP file or a JAR file. A JAR file is an uber JAR\n(or fat JAR) created by, for example, the Maven Shade plugin. A ZIP file has the\nclasses in the root directory and JAR dependencies in the lib directory. Later, I show\nhow a Gradle project can create a ZIP file. But first, let’s look at the different ways of\ninvoking lambda function. \n12.5.3 Invoking lambda functions\nThere are four ways to invoke a lambda function:\nHTTP requests\nEvents generated by AWS services\nScheduled invocations\nDirectly using an API call\nLet’s look at each one.\nHANDLING HTTP REQUESTS\nOne way to invoke a lambda function is to configure an AWS API Gateway to route\nHTTP requests to your lambda. The API gateway exposes your lambda function as\nan HTTPS endpoint. It functions as an HTTP proxy, invokes the lambda function\nwith an HTTP request object, and expects the lambda function to return an HTTP\nresponse object. By using the API gateway with AWS Lambda you can, for example,\ndeploy RESTful services as lambda functions. \nListing 12.8\nA Java lambda function is a class that implements the RequestHandler \ninterface.\n \n\n\n418\nCHAPTER 12\nDeploying microservices\nHANDLING EVENTS GENERATED BY AWS SERVICES\nThe second way to invoke a lambda function is to configure your lambda function to\nhandle events generated by an AWS service. Examples of events that can trigger a\nlambda function include the following:\nAn object is created in an S3 bucket.\nAn item is created, updated, or deleted in a DynamoDB table.\nA message is available to read from a Kinesis stream.\nAn email is received via the Simple email service.\nBecause of this integration with other AWS services, AWS Lambda is useful for a wide\nrange of tasks. \nDEFINING SCHEDULED LAMBDA FUNCTIONS\nAnother way to invoke a lambda function is to use a Linux cron-like schedule. You can\nconfigure your lambda function to be invoked periodically—for example, every minute,\n3 hours, or 7 days. Alternatively, you can use a cron expression to specify when AWS\nshould invoke your lambda. cron expressions give you tremendous flexibility. For exam-\nple, you can configure a lambda to be invoked at 2:15 p.m. Monday through Friday. \nINVOKING A LAMBDA FUNCTION USING A WEB SERVICE REQUEST\nThe fourth way to invoke a lambda function is for your application to invoke it using a\nweb service request. The web service request specifies the name of the lambda function\nand the input event data. Your application can invoke a lambda function synchronously\nor asynchronously. If your application invokes the lambda function synchronously, the\nweb service’s HTTP response contains the response of the lambda function. Other-\nwise, if it invokes the lambda function asynchronously, the web service response indi-\ncates whether the execution of the lambda was successfully initiated. \n12.5.4 Benefits of using lambda functions\nDeploying services using lambda functions has several benefits:\nIntegrated with many AWS services—It’s remarkably straightforward to write lamb-\ndas that consume events generated by AWS services, such as DynamoDB and\nKinesis, and handle HTTP requests via the AWS API Gateway.\nEliminates many system administration tasks—You’re no longer responsible for low-\nlevel system administration. There are no operating systems or runtimes to\npatch. As a result, you can focus on developing your application.\nElasticity—AWS Lambda runs as many instances of your application as are needed\nto handle the load. You don’t have the challenge of predicting needed capacity or\nrun the risk of underprovisioning or overprovisioning VMs or containers.\nUsage-based pricing—Unlike a typical IaaS cloud, which charges by the minute or\nhour for a VM or container even when it’s idle, AWS Lambda only charges you\nfor the resources that are consumed while processing each request. \n \n\n\n419\nDeploying a RESTful service using AWS Lambda and AWS Gateway\n12.5.5 Drawbacks of using lambda functions\nAs you can see, AWS Lambda is an extremely convenient way to deploy services, but\nthere are some significant drawbacks and limitations:\nLong-tail latency—Because AWS Lambda dynamically runs your code, some\nrequests have high latency because of the time it takes for AWS to provision an\ninstance of your application and for the application to start. This is particularly\nchallenging when running Java-based services because they typically take at least\nseveral seconds to start. For instance, the example lambda function described in\nthe next section takes a while to start up. Consequently, AWS Lambda may not\nbe suited for latency-sensitive services.\nLimited event/request-based programming model—AWS Lambda isn’t intended to be\nused to deploy long-running services, such as a service that consumes messages\nfrom a third-party message broker.\nBecause of these drawbacks and limitations, AWS Lambda isn’t a good fit for all\nservices. But when choosing a deployment pattern, I recommend first evaluating\nwhether serverless deployment supports your service’s requirements before consid-\nering alternatives. \n12.6\nDeploying a RESTful service using AWS Lambda \nand AWS Gateway\nLet’s take a look at how to deploy Restaurant Service using AWS Lambda. It’s a ser-\nvice that has a REST API for creating and managing restaurants. It doesn’t have long-\nlived connections to Apache Kafka, for example, so it’s a good fit for AWS lambda. Fig-\nure 12.13 shows the deployment architecture for this service. The service consists of\nseveral lambda functions, one for each REST endpoint. An AWS API Gateway is\nresponsible for routing HTTP requests to the lambda functions.\n Each lambda function has a request handler class. The ftgo-create-restaurant\nlambda function invokes the CreateRestaurantRequestHandler class, and the ftgo-\nfind-restaurant lambda function invokes FindRestaurantRequestHandler. Because\nthese request handler classes implement closely related aspects of the same service,\nthey’re packaged together in the same ZIP file, restaurant-service-aws-lambda\n.zip. Let’s look at the design of the service, including those handler classes.\n12.6.1 The design of the AWS Lambda version of Restaurant Service\nThe architecture of the service, shown in figure 12.14, is quite similar to that of a tra-\nditional service. The main difference is that Spring MVC controllers have been\nreplaced by AWS Lambda request handler classes. The rest of the business logic is\nunchanged.\n The service consists of a presentation tier consisting of the request handlers, which\nare invoked by AWS Lambda to handle the HTTP requests, and a traditional business\n \n\n\n420\nCHAPTER 12\nDeploying microservices\nAPI gateway\nAWS Lambda\nfunctions\nftgo-create-restaurant\nftgo-ﬁnd-restaurant\nftgo-restaurant-service-aws-lambda.zip\nftgo-...\nPOST/restaurant\nGET/restaurant/\n{restaurantId}\n«class»\nCreate\nRestaurant\nRequest\nHandler\n«class»\nFindRestaurant\nRequest\nHandler\n«class»\n...\nRequest\nHandler\nFigure 12.13\nDeploying Restaurant Service as AWS Lambda functions. The \nAWS API Gateway routes HTTP requests to the AWS Lambda functions, which are \nimplemented by request handler classes defined by Restaurant Service.\nCreate\nRestaurant\nRequest\nHandler\nPresentation layer\nPOST/restaurant\nGET/restaurant/{restaurantId}\nBusiness and\ndata access layer\nFind\nRestaurant\nRequest\nHandler\nRestaurantService\nRestaurantRepository\nRestaurant\n...\nRequest\nHandler\nFigure 12.14\nThe design of the AWS Lambda-based Restaurant Service. The \npresentation layer consists of request handler classes, which implement the lambda \nfunctions. They invoke the business tier, which is written in a traditional style \nconsisting of a service class, an entity, and a repository.\n \n\n\n421\nDeploying a RESTful service using AWS Lambda and AWS Gateway\ntier. The business tier consists of RestaurantService, the Restaurant JPA entity, and\nRestaurantRepository, which encapsulates the database.\n Let’s take a look at the FindRestaurantRequestHandler class.\nTHE FINDRESTAURANTREQUESTHANDLER CLASS\nThe FindRestaurantRequestHandler class implements the GET /restaurant/\n{restaurantId} endpoint. This class along with the other request handler classes are\nthe leaves of the class hierarchy shown in figure 12.15. The root of the hierarchy is\nRequestHandler, which is part of the AWS SDK. Its abstract subclasses handle errors\nand inject dependencies.\nThe AbstractHttpHandler class is the abstract base class for HTTP request handlers.\nIt catches unhandled exceptions thrown during request handling and returns a 500 -\ninternal server error response. The AbstractAutowiringHttpRequestHandler class\nimplements dependency injection for request handlers. I’ll describe these abstract\nsuperclasses shortly, but first let’s look at the code for FindRestaurantRequestHandler.\n Listing 12.9 shows the code for the FindRestaurantRequestHandler class. The\nFindRestaurantRequestHandler class has a handleHttpRequest() method, which\ntakes an APIGatewayProxyRequestEvent representing an HTTP request as a parame-\nter. It invokes RestaurantService to find the restaurant and returns an APIGateway-\nProxyResponseEvent describing the HTTP response.\nRequest\nHandler\nAbstract\nHttpHandler\nAbstract\nAutowiring\nHttpRequest\nHandler\nCreate\nRestaurant\nRequest\nHandler\nFind\nRestaurant\nRequest\nHandler\n...\nRequest\nHandler\nFigure 12.15\nThe design of the request handler \nclasses. The abstract superclasses implement \ndependency injection and error handling.\n \n\n\n422\nCHAPTER 12\nDeploying microservices\npublic class FindRestaurantRequestHandler \nextends AbstractAutowiringHttpRequestHandler {\n@Autowired\nprivate RestaurantService restaurantService;\n@Override\nprotected Class<?> getApplicationContextClass() {\nreturn CreateRestaurantRequestHandler.class;\n  \n}\n@Override\nprotected APIGatewayProxyResponseEvent\nhandleHttpRequest(APIGatewayProxyRequestEvent request, Context context) {\nlong restaurantId;\ntry {\nrestaurantId = Long.parseLong(request.getPathParameters()\n          .get(\"restaurantId\"));\n} catch (NumberFormatException e) {\nreturn makeBadRequestResponse(context);\n  \n}\nOptional<Restaurant> possibleRestaurant = restaurantService.findById(restaur\nantId);\nreturn possibleRestaurant\n  \n.map(this::makeGetRestaurantResponse)\n.orElseGet(() -> makeRestaurantNotFoundResponse(context,\nrestaurantId));\n}\nprivate APIGatewayProxyResponseEvent makeBadRequestResponse(Context context) {\n...\n}\nprivate APIGatewayProxyResponseEvent\nmakeRestaurantNotFoundResponse(Context context, long restaurantId) { ... }\nprivate\nAPIGatewayProxyResponseEvent\nmakeGetRestaurantResponse(Restaurant restaurant) { ... }\n}\nAs you can see, it’s quite similar to a servlet, except that instead of a service()\nmethod, which takes an HttpServletRequest and returns HttpServletResponse, it\nhas a handleHttpRequest(), which takes an APIGatewayProxyRequestEvent and returns\nAPIGatewayProxyResponseEvent.\n Let’s now take a look at its superclass, which implements dependency injection. \nListing 12.9\nThe handler class for GET /restaurant/{restaurantId}\nThe Spring Java \nconfiguration class to use \nfor the application context\nReturns a 400 - bad request \nresponse if the restaurantId \nis missing or invalid\nReturns either the \nrestaurant or a 404 - \nnot found response\n \n\n\n423\nDeploying a RESTful service using AWS Lambda and AWS Gateway\nDEPENDENCY INJECTION USING THE ABSTRACTAUTOWIRINGHTTPREQUESTHANDLER CLASS\nAn AWS Lambda function is neither a web application nor an application with a\nmain() method. But it would be a shame to not be able to use the features of Spring\nBoot that we’ve been accustomed to. The AbstractAutowiringHttpRequestHandler\nclass, shown in the following listing, implements dependency injection for request han-\ndlers. It creates an ApplicationContext using SpringApplication.run() and autowires\ndependencies prior to handling the first request. Subclasses such as FindRestaurant-\nRequestHandler must implement the getApplicationContextClass() method.\npublic abstract class AbstractAutowiringHttpRequestHandler \nextends AbstractHttpHandler {\nprivate static ConfigurableApplicationContext ctx;\nprivate ReentrantReadWriteLock ctxLock = new ReentrantReadWriteLock();\nprivate boolean autowired = false;\nprotected synchronized ApplicationContext getAppCtx() {   \nctxLock.writeLock().lock();\ntry {\nif (ctx == null) {\nctx =\nSpringApplication.run(getApplicationContextClass());\n}\nreturn ctx;\n} finally {\nctxLock.writeLock().unlock();\n}\n}\n@Override\nprotected void\nbeforeHandling(APIGatewayProxyRequestEvent request, Context context) {\nsuper.beforeHandling(request, context);\nif (!autowired) {\ngetAppCtx().getAutowireCapableBeanFactory().autowireBean(this);  \nautowired = true;\n}\n}\nprotected abstract Class<?> getApplicationContextClass();\n  \n}\nThis class overrides the beforeHandling() method defined by AbstractHttpHandler.\nIts beforeHandling() method injects dependencies using autowiring before handling\nthe first request. \nTHE ABSTRACTHTTPHANDLER CLASS\nThe request handlers for Restaurant Service ultimately extend AbstractHttpHandler,\nshown in listing 12.11. This class implements RequestHandler<APIGatewayProxy-\nRequestEvent and APIGatewayProxyResponseEvent>. Its key responsibility is to catch\nexceptions thrown when handling a request and throw a 500 error code. \nListing 12.10\nAn abstract RequestHandler that implements dependency injection\nCreates the Spring \nBoot application \ncontext just once\nInjects dependencies into\nthe request handler using\nautowiring before handling\nthe first request\nReturns the @Configuration\nclass used to create\nApplicationContext\n \n\n\n424\nCHAPTER 12\nDeploying microservices\npublic abstract class AbstractHttpHandler implements\nRequestHandler<APIGatewayProxyRequestEvent, APIGatewayProxyResponseEvent> {\nprivate Logger log = LoggerFactory.getLogger(this.getClass());\n@Override\npublic APIGatewayProxyResponseEvent handleRequest(\nAPIGatewayProxyRequestEvent input, Context context) {\nlog.debug(\"Got request: {}\", input);\ntry {\nbeforeHandling(input, context);\nreturn handleHttpRequest(input, context);\n} catch (Exception e) {\nlog.error(\"Error handling request id: {}\", context.getAwsRequestId(), e);\nreturn buildErrorResponse(new AwsLambdaError(\n\"Internal Server Error\",\n\"500\",\ncontext.getAwsRequestId(),\n\"Error handling request: \" + context.getAwsRequestId() + \" \" \n+ input.toString()));\n}\n}\nprotected void beforeHandling(APIGatewayProxyRequestEvent request, \nContext context) {\n// do nothing\n}\nprotected abstract APIGatewayProxyResponseEvent handleHttpRequest(\nAPIGatewayProxyRequestEvent request, Context context);\n}\n12.6.2 Packaging the service as ZIP file\nBefore the service can be deployed, we must package it as a ZIP file. We can easily\nbuild the ZIP file using the following Gradle task:\ntask buildZip(type: Zip) {\nfrom compileJava\nfrom processResources\ninto('lib') {\nfrom configurations.runtime\n}\n}\nThis task builds a ZIP with the classes and resources at the top level and the JAR\ndependencies in the lib directory.\n Now that we’ve built the ZIP file, let’s look at how to deploy the lambda function. \nListing 12.11\nAn abstract RequestHandler that catches exceptions and returns \na 500 HTTP response\n \n\n\n425\nDeploying a RESTful service using AWS Lambda and AWS Gateway\n12.6.3 Deploying lambda functions using the Serverless framework\nUsing the tools provided by AWS to deploy lambda functions and configure the API\ngateway is quite tedious. Fortunately, the Serverless open source project makes using\nlambda functions a lot easier. When using Serverless, you write a simple server-\nless.yml file that defines your lambda functions and their RESTful endpoints.\nServerless then deploys the lambda functions and creates and configures an API gate-\nway that routes requests to them.\n The following listing is an excerpt of the serverless.yml that deploys Restaurant\nService as a lambda.\nservice: ftgo-application-lambda\nprovider:\nname: aws\n  \nruntime: java8\ntimeout: 35\nregion: ${env:AWS_REGION}\nstage: dev\nenvironment:\n  \nSPRING_DATASOURCE_DRIVER_CLASS_NAME: com.mysql.jdbc.Driver\nSPRING_DATASOURCE_URL: ...\nSPRING_DATASOURCE_USERNAME: ...\nSPRING_DATASOURCE_PASSWORD: ...\npackage:\n  \nartifact: ftgo-restaurant-service-aws-lambda/build/distributions/\nftgo-restaurant-service-aws-lambda.zip\nfunctions:\n  \ncreate-restaurant:\nhandler: net.chrisrichardson.ftgo.restaurantservice.lambda\n.CreateRestaurantRequestHandler\nevents:\n- http:\npath: restaurants\nmethod: post\nfind-restaurant:\nhandler: net.chrisrichardson.ftgo.restaurantservice.lambda\n.FindRestaurantRequestHandler\nevents:\n- http:\npath: restaurants/{restaurantId}\nmethod: get\nYou can then use the serverless deploy command, which reads the serverless.yml\nfile, deploys the lambda functions, and configures the AWS API Gateway. After a short\nListing 12.12\nThe serverless.yml deploys Restaurant Service.\nTells serverless to \ndeploy on AWS\nSupplies the service’s \nexternalized configuration \nvia environment variables\nThe ZIP file \ncontaining the \nlambda functions\nLambda function definitions \nconsisting of the handler \nfunction and HTTP endpoint\n \n\n\n426\nCHAPTER 12\nDeploying microservices\nwait, your service will be accessible via the API gateway’s endpoint URL. AWS Lambda\nwill provision as many instances of each Restaurant Service lambda function that are\nneeded to support the load. If you change the code, you can easily update the lambda\nby rebuilding the ZIP file and rerunning serverless deploy. No servers involved!\n The evolution of infrastructure is remarkable. Not that long ago, we manually\ndeployed applications on physical machines. Today, highly automated public clouds\nprovide a range of virtual deployment options. One option is to deploy services as vir-\ntual machines. Or better yet, we can package services as containers and deploy them\nusing sophisticated Docker orchestration frameworks such as Kubernetes. Sometimes\nwe even avoid thinking about infrastructure entirely and deploy services as light-\nweight, ephemeral lambda functions. \nSummary\nYou should choose the most lightweight deployment pattern that supports your\nservice’s requirements. Evaluate the options in the following order: serverless,\ncontainers, virtual machines, and language-specific packages.\nA serverless deployment isn’t a good fit for every service, because of long-tail\nlatencies and the requirement to use an event/request-based programming\nmodel. When it is a good fit, though, serverless deployment is an extremely\ncompelling option because it eliminates the need to administer operating sys-\ntems and runtimes and provides automated elastic provisioning and request-\nbased pricing.\nDocker containers, which are a lightweight, OS-level virtualization technol-\nogy, are more flexible than serverless deployment and have more predictable\nlatency. It’s best to use a Docker orchestration framework such as Kuberne-\ntes, which manages containers on a cluster of machines. The drawback of\nusing containers is that you must administer the operating systems and run-\ntimes and most likely the Docker orchestration framework and the VMs that\nit runs on.\nThe third deployment option is to deploy your service as a virtual machine. On\none hand, virtual machines are a heavyweight deployment option, so deploy-\nment is slower and it will most likely use more resources than the second\noption. On the other hand, modern clouds such as Amazon EC2 are highly\nautomated and provide a rich set of features. Consequently, it may sometimes\nbe easier to deploy a small, simple application using virtual machines than to\nset up a Docker orchestration framework.\nDeploying your services as language-specific packages is generally best avoided\nunless you only have a small number of services. For example, as described in\nchapter 13, when starting on your journey to microservices you’ll probably\ndeploy the services using the same mechanism you use for your monolithic\napplication, which is most likely this option. You should only consider setting\n \n\n\n427\nSummary\nup a sophisticated deployment infrastructure such as Kubernetes once you’ve\ndeveloped some services.\nOne of the many benefits of using a service mesh—a networking layer that\nmediates all network traffic in and out of services—is that it enables you to\ndeploy a service in production, test it, and only then route production traffic to\nit. Separating deployment from release improves the reliability of rolling out\nnew versions of services. \n \n\n\n428\nRefactoring to\nmicroservices\nI hope that this book has given you a good understanding of the microservice\narchitecture, its benefits and drawbacks, and when to use it. There is, however, a\nfairly good chance you’re working on a large, complex monolithic application.\nYour daily experience of developing and deploying your application is slow and\npainful. Microservices, which appear like a good fit for your application, seem like\ndistant nirvana. Like Mary and the rest of the FTGO development team, you’re\nwondering how on earth you can adopt the microservice architecture?\n Fortunately, there are strategies you can use to escape from monolithic hell\nwithout having to rewrite your application from scratch. You incrementally convert\nThis chapter covers\nWhen to migrate a monolithic application to a \nmicroservice architecture\nWhy using an incremental approach is essential \nwhen refactoring a monolithic application to \nmicroservices\nImplementing new features as services\nExtracting services from the monolith\nIntegrating a service and the monolith\n \n\n\n429\nOverview of refactoring to microservices\nyour monolith into microservices by developing what’s known as a strangler applica-\ntion. The idea of a strangler application comes from strangler vines, which grow in\nrain forests by enveloping and sometimes killing trees. A strangler application is a new\napplication consisting of microservices that you develop by implementing new func-\ntionality as services and extracting services from the monolith. Over time, as the stran-\ngler application implements more and more functionality, it shrinks and ultimately\nkills the monolith. An important benefit of developing a strangler application is that,\nunlike a big bang rewrite, it delivers value to the business early and often.\n I begin this chapter by describing the motivations for refactoring a monolith to a\nmicroservice architecture. I then describe how to develop the strangler application\nby implementing new functionality as services and extracting services from the\nmonolith. Next, I cover various design topics, including how to integrate the mono-\nlith and services, how to maintain database consistency across the monolith and\nservices, and how to handle security. I end the chapter by describing a couple of\nexample services. One service is Delayed Order Service, which implements brand\nnew functionality. The other service is Delivery Service, which is extracted from\nthe monolith. Let’s start by taking a look at the concept of refactoring to a micro-\nservice architecture.\n13.1\nOverview of refactoring to microservices\nPut yourself in Mary’s shoes. You’re responsible for the FTGO application, a large and\nold monolithic application. The business is extremely frustrated with engineering’s\ninability to deliver features rapidly and reliably. FTGO appears to be suffering from a\nclassic case of monolithic hell. Microservices seem, at least on the surface, to be the\nanswer. Should you propose diverting development resources away from feature devel-\nopment to migrating to a microservice architecture?\n I start this section by discussing why you should consider refactoring to microser-\nvices. I also discuss why it’s important to be sure that your software development prob-\nlems are because you’re in monolithic hell rather than in, for example, a poor\nsoftware development process. I then describe strategies for incrementally refactoring\nyour monolith to a microservice architecture. Next, I discuss the importance of deliv-\nering improvements earlier and often in order to maintain the support of the busi-\nness. I then describe why you should avoid investing in a sophisticated deployment\ninfrastructure until you’ve developed a few services. Finally, I describe the various\nstrategies you can use to introduce services into your architecture, including imple-\nmenting new features as services and extracting services from the monolith.\n13.1.1 Why refactor a monolith?\nThe microservice architecture has, as described in chapter 1, numerous benefits. It\nhas much better maintainability, testability, and deployability, so it accelerates devel-\nopment. The microservice architecture is more scalable and improves fault isolation.\nIt’s also much easier to evolve your technology stack. But refactoring a monolith to\n \n",
      "page_number": 439
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 455-465)",
      "start_page": 455,
      "end_page": 465,
      "detection_method": "topic_boundary",
      "content": "430\nCHAPTER 13\nRefactoring to microservices\nmicroservices is a significant undertaking. It will divert resources away from new fea-\nture development. As a result, it’s likely that the business will only support the adop-\ntion of microservices if it solves a significant business problem.\n If you’re in monolithic hell, it’s likely that you already have at least one business\nproblem. Here are some examples of business problems caused by monolithic hell:\nSlow delivery—The application is difficult to understand, maintain, and test, so\ndeveloper productivity is low. As a result, the organization is unable to compete\neffectively and risks being overtaken by competitors.\nBuggy software releases—The lack of testability means that software releases are\noften buggy. This makes customers unhappy, which results in losing customers\nand reduced revenue.\nPoor scalability—Scaling a monolithic application is difficult because it combines\nmodules with very different resource requirements into one executable compo-\nnent. The lack of scalability means that it’s either impossible or prohibitively\nexpensive to scale the application beyond a certain point. As a result, the appli-\ncation can’t support the current or predicted needs of the business.\nIt’s important to be sure that these problems are there because you’ve outgrown your\narchitecture. A common reason for slow delivery and buggy releases is a poor software\ndevelopment process. For example, if you’re still relying on manual testing, then\nadopting automated testing alone can significantly increase development velocity.\nSimilarly, you can sometimes solve scalability problems without changing your archi-\ntecture. You should first try simpler solutions. If, and only if, you still have software\ndelivery problems should you then migrate to the microservice architecture. Let’s\nlook at how to do that. \n13.1.2 Strangling the monolith\nThe process of transforming a monolithic application into microservices is a form of\napplication modernization (https://en.wikipedia.org/wiki/Software_modernization).\nApplication modernization is the process of converting a legacy application to one having\na modern architecture and technology stack. Developers have been modernizing appli-\ncations for decades. As a result, there is wisdom accumulated through experience we\ncan use when refactoring an application into a microservice architecture. The most\nimportant lesson learned over the years is to not do a big bang rewrite.\n A big bang rewrite is when you develop a new application—in this case, a microservices-\nbased application—from scratch. Although starting from scratch and leaving the legacy\ncode base behind sounds appealing, it’s extremely risky and will likely end in failure.\nYou will spend months, possibly years, duplicating the existing functionality, and only\nthen can you implement the features that the business needs today! Also, you’ll\nneed to develop the legacy application anyway, which diverts effort away from the\nrewrite and means that you have a constantly moving target. What’s more, it’s possible\n \n\n\n431\nOverview of refactoring to microservices\nthat you’ll waste time reimplementing features that are no longer needed. As Martin\nFowler reportedly said, “the only thing a Big Bang rewrite guarantees is a Big Bang!”\n(www.randyshoup.com/evolutionary-architecture).\n Instead of doing a big bang rewrite, you should, as figure 13.1 shows, incrementally\nrefactor your monolithic application. You gradually build a new application, which is\ncalled a strangler application. It consists of microservices that runs in conjunction\nwith your monolithic application. Over time, the amount of functionality imple-\nmented by the monolithic application shrinks until either it disappears entirely or it\nbecomes just another microservice. This strategy is akin to servicing your car while\ndriving down the highway at 70 mph. It’s challenging, but is far less risky that attempt-\ning a big bang rewrite.\nMartin Fowler refers to this application modernization strategy as the Strangler appli-\ncation pattern (www.martinfowler.com/bliki/StranglerApplication.html). The name\ncomes from the strangler vine (or strangler fig—see https://en.wikipedia.org/wiki/\nStrangler_fig) that is found in rain forests. A strangler vine grows around a tree in\nThe monolith shrinks over time.\nThe strangler application\ngrows larger over time.\nMonolith\nMonolith\nService\nStrangler application\nMonolith\nService\nMonolith\nService\n...\n...\nMonolith\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nTime\nFigure 13.1\nThe monolith is incrementally replaced by a strangler application comprised of services. \nEventually, the monolith is replaced entirely by the strangler application or becomes another \nmicroservice.\n \n\n\n432\nCHAPTER 13\nRefactoring to microservices\norder to reach the sunlight above the forest canopy. Often the tree dies, because\neither it’s killed by the vine or it dies of old age, leaving a tree-shaped vine.\nThe refactoring process typically takes months, or years. For example, according to\nSteve Yegge (https://plus.google.com/+RipRowan/posts/eVeouesvaVX) it took Ama-\nzon.com a couple of years to refactor its monolith. In the case of a very large system,\nyou may never complete the process. You could, for example, get to a point where you\nhave tasks that are more important than breaking up the monolith, such as imple-\nmenting revenue-generating features. If the monolith isn’t an obstacle to ongoing\ndevelopment, you may as well leave it alone. \nDEMONSTRATE VALUE EARLY AND OFTEN\nAn important benefit of incrementally refactoring to a microservice architecture is\nthat you get an immediate return on your investment. That’s very different than a big\nbang rewrite, which doesn’t deliver any benefit until it’s complete. When incremen-\ntally refactoring the monolith, you can develop each new service using a new technology\nstack and a modern, high-velocity, DevOps-style development and delivery process. As a\nresult, your team’s delivery velocity steadily increases over time.\n What’s more, you can migrate the high-value areas of your application to microser-\nvices first. For instance, imagine you’re working on the FTGO application. The business\nmight, for example, decide that the delivery scheduling algorithm is a key competitive\nadvantage. It’s likely that delivery management will be an area of constant, ongoing\ndevelopment. By extracting delivery management into a standalone service, the delivery\nmanagement team will be able to work independently of the rest of the FTGO develop-\ners and significantly increase their development velocity. They’ll be able to frequently\ndeploy new versions of the algorithm and evaluate their effectiveness.\n Another benefit of being able to deliver value earlier is that it helps maintain the\nbusiness’s support for the migration effort. Their ongoing support is essential, because\nthe refactoring effort will mean that less time is spent on developing features. Some\norganizations have difficulty eliminating technical debt because past attempts were\ntoo ambitious and didn’t provide much benefit. As a result, the business becomes\nreluctant to invest in further cleanup efforts. The incremental nature of refactoring to\nmicroservices means that the development team is able to demonstrate value early\nand often. \nMINIMIZE CHANGES TO THE MONOLITH\nA recurring theme in this chapter is that you should avoid making widespread\nchanges to the monolith when migrating to a microservice architecture. It’s inevitable\nPattern: Strangler application\nModernize an application by incrementally developing a new (strangler) application\naround the legacy application. See http://microservices.io/patterns/refactoring/\nstrangler-application.html.\n \n\n\n433\nStrategies for refactoring a monolith to microservices\nthat you’ll need to make some changes in order to support migration to services. Sec-\ntion 13.3.2 talks about how the monolith often needs to be modified so that it can par-\nticipate in sagas that maintain data consistency across the monolith and services. The\nproblem with making widespread changes to the monolith is that it’s time consuming,\ncostly, and risky. After all, that’s probably why you want to migrate to microservices in\nthe first place.\n Fortunately, there are strategies you can use for reducing the scope of the changes\nyou need to make. For example, in section 13.2.3, I describe the strategy of replicating\ndata from an extracted service back to the monolith’s database. And in section 13.3.2,\nI show how you can carefully sequence the extraction of services to reduce the impact\non the monolith. By applying these strategies, you can reduce the amount of work\nrequired to refactor the monolith. \nTECHNICAL DEPLOYMENT INFRASTRUCTURE: YOU DON’T NEED ALL OF IT YET\nThroughout this book I’ve discussed a lot of shiny new technology, including deploy-\nment platforms such as Kubernetes and AWS Lambda and service discovery mecha-\nnisms. You might be tempted to begin your migrating to microservices by selecting\ntechnologies and building out that infrastructure. You might even feel pressure from\nthe business people and from your friendly PaaS vendor to start spending money on\nthis kind of infrastructure.\n As tempting as it seems to build out this infrastructure up front, I recommend only\nmaking a minimal up-front investment in developing it. The only thing you can’t live\nwithout is a deployment pipeline that performs automating testing. For example, if\nyou only have a handful of services, you don’t need a sophisticated deployment and\nobservability infrastructure. Initially, you can even get away with just using a hard-\ncoded configuration file for service discovery. I suggest deferring any decisions about\ntechnical infrastructure that involve significant investment until you’ve gained real\nexperience with the microservice architecture. It’s only once you have a few services\nrunning that you’ll have the experience to pick technologies.\n Let’s now look at the strategies you can use for migrating to a microservice\narchitecture. \n13.2\nStrategies for refactoring a monolith to microservices\nThere are three main strategies for strangling the monolith and incrementally replac-\ning it with microservices:\n1\nImplement new features as services.\n2\nSeparate the presentation tier and backend.\n3\nBreak up the monolith by extracting functionality into services.\nThe first strategy stops the monolith from growing. It’s typically a quick way to\ndemonstrate the value of microservices, helping build support for the migration\neffort. The other two strategies break apart the monolith. When refactoring your\nmonolith, you might sometimes use the second strategy, but you’ll definitely use the\n \n\n\n434\nCHAPTER 13\nRefactoring to microservices\nthird strategy, because it’s how functionality is migrated from the monolith into the\nstrangler application.\n Let’s take a look at each of these strategies, starting with implementing new fea-\ntures as services.\n13.2.1 Implement new features as services\nThe Law of Holes states that “if you find yourself in a hole, stop digging” (https://\nen.m.wikipedia.org/wiki/Law_of_holes). This is great advice to follow when your mono-\nlithic application has become unmanageable. In other words, if you have a large, com-\nplex monolithic application, don’t implement new features by adding code to the\nmonolith. That will make your monolith even larger and more unmanageable. Instead,\nyou should implement new features as services.\n This is a great way to begin migrating your monolithic application to a microser-\nvice architecture. It reduces the growth rate of the monolith. It accelerates the devel-\nopment of the new features, because you’re doing development in a brand new code\nbase. It also quickly demonstrates the value of adopting the microservice architecture.\nINTEGRATING THE NEW SERVICE WITH THE MONOLITH\nFigure 13.2 shows the application’s architecture after implementing a new feature as a\nservice. Besides the new service and monolith, the architecture includes two other ele-\nments that integrate the service into the application:\nAPI gateway—Routes requests for new functionality to the new service and\nroutes legacy requests to the monolith.\nIntegration glue code—Integrates the service with the monolith. It enables the ser-\nvice to access data owned by the monolith and to invoke functionality imple-\nmented by the monolith.\nThe integration glue code isn’t a standalone component. Instead, it consists of adapt-\ners in the monolith and the service that use one or more interprocess communication\nmechanisms. For example, integration glue for Delayed Delivery Service, described\nin section 13.4.1, uses both REST and domain events. The service retrieves customer\ncontract information from the monolith by invoking a REST API. The monolith pub-\nlishes Order domain events so that Delayed Delivery Service can track the state of\nOrders and respond to orders that won’t be delivered on time. Section 13.3.1 describes\nthe integration glue code in more detail.\nWHEN TO IMPLEMENT A NEW FEATURE AS A SERVICE\nIdeally, you should implement every new feature in the strangler application rather\nthan in the monolith. You’ll implement a new feature as either a new service or as part\nof an existing service. This way you’ll avoid ever having to touch the monolith code\nbase. Unfortunately, though, not every new feature can be implemented as a service.\n That’s because the essence of a microservice architecture is a set of loosely coupled\nservices that are organized around business capabilities. A feature might, for instance,\nbe too small to be a meaningful service. You might, for example, just need to add a\n \n\n\n435\nStrategies for refactoring a monolith to microservices\nfew fields and methods to an existing class. Or the new feature might be too tightly\ncoupled to the code in the monolith. If you attempted to implement this kind of fea-\nture as a service you would typically find that performance would suffer because of\nexcessive interprocess communication. You might also have problems maintaining\ndata consistency. If a new feature can’t be implemented as a service, the solution is\noften to initially implement the new feature in the monolith. Later on, you can then\nextract that feature along with other related features into their own service.\n Implementing new features as services accelerates the development of those fea-\ntures. It’s a good way to quickly demonstrate the value of the microservice architec-\nture. It also reduces the monolith’s growth rate. But ultimately, you need to break\napart the monolith using the two other strategies. You need to migrate functionality to\nthe strangler application by extracting functionality from the monolith into services.\nYou might also be able to improve development velocity by splitting the monolith hor-\nizontally. Let’s look at how to do that. \nMonolith\nOutbound\nadapter\nAPI gateway\nOld features\nNew features\nIntegration\nglue\nInbound\nadapter\nInbound\nadapter\nDatabase\nadapter\nDatabase\nadapter\nInbound\nadapter\nEvent\nsubscriber\nadapter\nEvent\npublisher\nadapter\nService\ndatabase\nMonolith\ndatabase\n«aggregate»\nDelayedDelivery\nService\n«aggregate»\nOrder\n«aggregate»\nNotification\nService\nimplementing\nnew feature\nFigure 13.2\nA new feature is implemented as a service that’s part of the strangler application. The \nintegration glue integrates the service with the monolith and consists of adapters that implement \nsynchronous and asynchronous APIs. An API gateway routes requests that invoke new functionality \nto the service.\n \n\n\n436\nCHAPTER 13\nRefactoring to microservices\n13.2.2 Separate presentation tier from the backend\nOne strategy for shrinking a monolithic application is to split the presentation layer\nfrom the business logic and data access layers. A typical enterprise application consists\nof the following layers:\nPresentation logic—This consists of modules that handle HTTP requests and gener-\nate HTML pages that implement a web UI. In an application that has a sophisti-\ncated user interface, the presentation tier is often a substantial body of code.\nBusiness logic—This consists of modules that implement the business rules, which\ncan be complex in an enterprise application.\nData access logic—This consists of modules that access infrastructure services\nsuch as databases and message brokers.\nThere is usually a clean separation between the presentation logic and the business\nand data access logic. The business tier has a coarse-grained API consisting of one or\nmore facades that encapsulate the business logic. This API is a natural seam along\nwhich you can split the monolith into two smaller applications, as shown in figure 13.3.\nBusiness logic\nBusiness logic\nREST\nAPI\nREST\nclient\nWeb\napp\nBrowser\nBrowser\nHTML pages\nHTML pages\nMonolith containing\npresentation logic and\nbackend business logic\nSmaller, independently\ndeployable presentation\nlogic monolith\nMySQL\nDatabase\nadapter\nMySQL\nDatabase\nadapter\nWeb\napplication\nSplit\nSmaller, independently\ndeployable backend\nmonolith\nAn API that is callable\nby any future services\nFigure 13.3\nSplitting the frontend from the backend enables each to be deployed independently. It also exposes \nan API for services to invoke.\n \n\n\n437\nStrategies for refactoring a monolith to microservices\nOne application contains the presentation layer, and the other contains the business\nand data access logic. After the split, the presentation logic application makes remote\ncalls to the business logic application.\n Splitting the monolith in this way has two main benefits. It enables you to develop,\ndeploy, and scale the two applications independently of one another. In particular, it\nallows the presentation layer developers to rapidly iterate on the user interface and\neasily perform A/B testing, for example, without having to deploy the backend.\nAnother benefit of this approach is that it exposes a remote API that can be called by\nthe microservices you develop later.\n But this strategy is only a partial solution. It’s very likely that at least one or both of\nthe resulting applications will still be an unmanageable monolith. You need to use the\nthird strategy to replace the monolith with services. \n13.2.3 Extract business capabilities into services\nImplementing new features as services and splitting the frontend web application\nfrom the backend will only get you so far. You’ll still end up doing a lot of develop-\nment in the monolithic code base. If you want to significantly improve your applica-\ntion’s architecture and increase your development velocity, you need to break apart\nthe monolith by incrementally migrating business capabilities from the monolith to\nservices. For example, section 13.5 describes how to extract delivery management\nfrom the FTGO monolith into a new Delivery Service. When you use this strategy,\nover time the number of business capabilities implemented by the services grows, and\nthe monolith gradually shrinks.\n The functionality you want extract into a service is a vertical slice through the\nmonolith. The slice consists of the following:\nInbound adapters that implement API endpoints\nDomain logic\nOutbound adapters such as database access logic\nThe monolith’s database schema\nAs figure 13.4 shows, this code is extracted from the monolith and moved into a stand-\nalone service. An API gateway routes requests that invoke the extracted business capa-\nbility to the service and routes the other requests to the monolith. The monolith and\nthe service collaborate via the integration glue code. As described in section 13.3.1,\nthe integration glue consists of adapters in the service and monolith that use one or\nmore interprocess communication (IPC) mechanisms.\n Extracting services is challenging. You need to determine how to split the mono-\nlith’s domain model into two separate domain models, one of which becomes the ser-\nvice’s domain model. You need to break dependencies such as object references. You\nmight even need to split classes in order to move functionality into the service. You\nalso need to refactor the database.\n Extracting a service is often time consuming, especially because the monolith’s\ncode base is likely to be messy. Consequently, you need to carefully think about which\n \n\n\n438\nCHAPTER 13\nRefactoring to microservices\nservices to extract. It’s important to focus on refactoring those parts of the application\nthat provide a lot of value. Before extracting a service, ask yourself what the benefit is\nof doing that.\n For example, it’s worthwhile to extract a service that implements functionality\nthat’s critical to the business and constantly evolving. It’s not valuable to invest effort\nin extracting services when there’s not much benefit from doing so. Later in this sec-\ntion I describe some strategies for determining what to extract and when. But first,\nlet’s look in more detail at some of the challenges you’ll face when extracting a service\nand how to address them.\n You’ll encounter a couple of challenges when extracting a service:\nSplitting the domain model\nRefactoring the database\nLet’s look at each one, starting with splitting the domain model.\nOutbound\nadapter\nAPI gateway\nService containing\nextracted code\nIntegration\nglue\nInbound\nadapter\nInbound\nadapter\nDatabase\nadapter\nDatabase\nadapter\nInbound\nadapter\nOutbound\nadapter\nInbound\nadapter\nService\ndatabase\nMonolith\ndatabase\nInbound\nadapter\nDatabase\nadapter\nMonolith\ndatabase\n«\n»\nservice\nOrder Service\n«aggregate»\nCourier\n«aggregate»\nPlan\n«service»\nOrder Service\n«aggregate»\nCourier\n«aggregate»\nPlan\nCode to\nextract into\na service\nMonolith\n«\n»\nservice\nOrder Service\n«aggregate»\nOrder\n«\n»\naggregate\nOrder\nGlue code integrating\nservice with monolith\nFigure 13.4\nBreak apart the monolith by extracting services. You identify a slice of functionality, which consists \nof business logic and adapters, to extract into a service. You move that code into the service. The newly extracted \nservice and the monolith collaborate via the APIs provided by the integration glue.\n \n\n\n439\nStrategies for refactoring a monolith to microservices\nSPLITTING THE DOMAIN MODEL\nIn order to extract a service, you need to extract its domain model out of the monolith’s\ndomain model. You’ll need to perform major surgery to split the domain models. One\nchallenge you’ll encounter is eliminating object references that would otherwise span\nservice boundaries. It’s possible that classes that remain in the monolith will reference\nclasses that have been moved to the service or vice versa. For example, imagine that, as\nfigure 13.5 shows, you extract Order Service, and as a result its Order class references\nthe monolith’s Restaurant class. Because a service instance is typically a process, it\ndoesn’t make sense to have object references that cross service boundaries. Somehow\nyou need to eliminate these types of object reference.\nOne good way to solve this problem is to think in terms of DDD aggregates, described\nin chapter 5. Aggregates reference each other using primary keys rather than object ref-\nerences. You would, therefore, think of the Order and Restaurant classes as aggre-\ngates and, as figure 13.6 shows, replace the reference to Restaurant in the Order class\nwith a restaurantId field that stores the primary key value.\nFTGO monolith\nExtracted service\n«Entity»\nRestaurant\nObject reference that spans\nservice boundaries\n«Entity»\nOrder\nDelivery Service\nFTGO monolith\n?\n«Entity»\nRestaurant\n«Entity»\nOrder\nFigure 13.5\nThe Order domain class has a reference to a Restaurant class. If we extract \nOrder into a separate service, we need to do something about its reference to Restaurant, \nbecause object references between processes don’t make sense.\nReplace with primary key.\nDelivery Service\nFTGO monolith\n«Entity»\nRestaurant\n«Entity»\nOrder\nrestaurantId\nObject reference that spans\nservice boundaries\nDelivery Service\nFTGO monolith\n?\n«Entity»\nRestaurant\n«Entity»\nOrder\nFigure 13.6\nThe Order class’s reference to Restaurant is replaced with the Restaurant's \nprimary key in order to eliminate an object that would span process boundaries.\n \n\n\n440\nCHAPTER 13\nRefactoring to microservices\nOne issue with replacing object references with primary keys is that although this is a\nminor change to the class, it can potentially have a large impact on the clients of the\nclass, which expect an object reference. Later in this section, I describe how to reduce\nthe scope of the change by replicating data between the service and monolith. Delivery\nService, for example, could define a Restaurant class that’s a replica of the mono-\nlith’s Restaurant class.\n Extracting a service is often much more involved than moving entire classes into a\nservice. An even greater challenge with splitting a domain model is extracting func-\ntionality that’s embedded in a class that has other responsibilities. This problem often\noccurs in god classes, described in chapter 2, that have an excessive number of responsi-\nbilities. For example, the Order class is one of the god classes in the FTGO applica-\ntion. It implements multiple business capabilities, including order management,\ndelivery management, and so on. Later in section 13.5, I discuss how extracting the\ndelivery management into a service involves extracting a Delivery class from the\nOrder class. The Delivery entity implements the delivery management functionality\nthat was previously bundled with other functionality in the Order class. \nREFACTORING THE DATABASE\nSplitting a domain model involves more than just changing code. Many classes in a\ndomain model are persistent. Their fields are mapped to a database schema. Conse-\nquently, when you extract a service from the monolith, you’re also moving data. You\nneed to move tables from the monolith’s database to the service’s database.\n Also, when you split an entity you need to split the corresponding database table\nand move the new table to the service. For example, when extracting delivery manage-\nment into a service, you split the Order entity and extract a Delivery entity. At the\ndatabase level, you split the ORDERS table and define a new DELIVERY table. You then\nmove the DELIVERY table to the service.\n The book Refactoring Databases by Scott W. Ambler and Pramod J. Sadalage (Addison-\nWesley, 2011) describes a set of refactorings for a database schema. For example, it\ndescribes the Split Table refactoring, which splits a table into two or more tables.\nMany of the technique in that book are useful when extracting services from the\nmonolith. One such technique is the idea of replicating data in order to allow you to\nincrementally update clients of the database to use the new schema. We can adapt\nthat idea to reduce the scope of the changes you must make to the monolith when\nextracting a service.\nREPLICATE DATA TO AVOID WIDESPREAD CHANGES\nAs mentioned, extracting a service requires you to change to the monolith’s domain\nmodel. For example, you replace object references with primary keys and split classes.\nThese types of changes can ripple through the code base and require you to make\nwidespread changes to the monolith. For example, if you split the Order entity and\nextract a Delivery entity, you’ll have to change every place in the code that references\nthe fields that have been moved. Making these kinds of changes can be extremely\ntime consuming and can become a huge barrier to breaking up the monolith.\n \n",
      "page_number": 455
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 466-483)",
      "start_page": 466,
      "end_page": 483,
      "detection_method": "topic_boundary",
      "content": "441\nStrategies for refactoring a monolith to microservices\n A great way to delay and possibly avoid making these kinds of expensive changes is\nto use an approach that’s similar to the one described in Refactoring Databases. A major\nobstacle to refactoring a database is changing all the clients of that database to use the\nnew schema. The solution proposed in the book is to preserve the original schema for\na transition period and use triggers to synchronize the original and new schemas. You\nthen migrate clients from the old schema to the new schema over time.\n We can use a similar approach when extracting services from the monolith. For\nexample, when extracting the Delivery entity, we leave the Order entity mostly\nunchanged for a transition period. As figure 13.7 shows, we make the delivery-related\nfields read-only and keep them up-to-date by replicating data from Delivery Service\nback to the monolith. As a result, we only need to find the places in the monolith’s code\nthat update those fields and change them to invoke the new Delivery Service.\n Preserving the structure of the Order entity by replicating data from Delivery\nService significantly reduces the amount of work we need to do immediately. Over\ntime, we can migrate code that uses the delivery-related Order entity fields or ORDERS\ntable columns to Delivery Service. What’s more, it’s possible that we never need to\nRead-only\ndelivery-related\nﬁelds\nORDER_ID\n...\nORDER table\nRESTAURANT_ID\n...\nSCHEDULED_PICKUP_TIME\n...\nSCHEDULED_DELIVERY_TIME\n...\n...\n...\n«Entity»\nOrder\nFTGO monolith\n...\nconsumerId\nscheduledPickupTime\nscheduledDeliveryTime\n...\nORDER_ID\n...\nORDER table\nRESTAURANT_ID\n...\nSCHEDULED_PICKUP_TIME\n...\nSCHEDULED_DELIVERY_TIME\n...\n...\n...\n«Entity»\nOrder\nFTGO monolith\n...\nconsumerId\nscheduledPickupTime\nscheduledDeliveryTime\n...\nORDER_ID\n...\nDELIVERY table\nSCHEDULED_PICKUP_TIME\n...\nSCHEDULED_DELIVERY_TIME\n...\n...\n...\n«Entity»\nDelivery\nDelivery Service\nExtract Order Service and move columns from\nORDER\nDELIVERY\ntable to a new\ntable.\nReplicate data from Delivery Service to FTGO monolith.\n...\norderId\nscheduledPickupTime\nscheduledDeliveryTime\n...\nFigure 13.7\nMinimize the scope of the changes to the FTGO monolith by replicating delivery-related data from the \nnewly extracted Delivery Service back to the monolith’s database.\n \n\n\n442\nCHAPTER 13\nRefactoring to microservices\nmake that change in the monolith. If that code is subsequently extracted into a ser-\nvice, then the service can access Delivery Service.\nWHAT SERVICES TO EXTRACT AND WHEN\nAs I mentioned, breaking apart the monolith is time consuming. It diverts effort away\nfrom implementing features. As a result, you must carefully decide the sequence in\nwhich you extract services. You need to focus on extracting services that give the larg-\nest benefit. What’s more, you want to continually demonstrate to the business that\nthere’s value in migrating to a microservice architecture.\n On any journey, it’s essential to know where you’re going. A good way to start the\nmigration to microservices is with a time-boxed architecture definition effort. You\nshould spend a short amount of time, such as a couple of weeks, brainstorming your\nideal architecture and defining a set of services. This gives you a destination to aim\nfor. It’s important, though, to remember that this architecture isn’t set in stone. As\nyou break apart the monolith and gain experience, you should revise the architecture\nto take into account what you’ve learned.\n Once you’ve determined the approximate destination, the next step is to start\nbreaking apart the monolith. There are a couple of different strategies you can use to\ndetermine the sequence in which you extract services.\n One strategy is to effectively freeze development of the monolith and extract ser-\nvices on demand. Instead of implementing features or fixing bugs in the monolith,\nyou extract the necessary service or service(s) and change those. One benefit of this\napproach is that it forces you to break up the monolith. One drawback is that the\nextraction of services is driven by short-term requirements rather than long-term\nneeds. For instance, it requires you to extract services even if you’re making a small\nchange to a relatively stable part of the system. As a result, you risk doing a lot of work\nfor minimal benefit.\n An alternative strategy is a more planned approach, where you rank the modules\nof an application by the benefit you anticipate getting from extracting them. There\nare a few reasons why extracting a service is beneficial:\nAccelerates development—If your application’s roadmap suggests that a particular\npart of your application will undergo a lot of development over the next year,\nthen converting it to a service accelerates development.\nSolves a performance, scaling, or reliability problem—If a particular part of your appli-\ncation has a performance or scalability problem or is unreliable, then it’s valu-\nable to convert it to a service.\nEnables the extraction of some other services—Sometimes extracting one service sim-\nplifies the extraction of another service, due to dependencies between modules.\nYou can use these criteria to add refactoring tasks to your application’s backlog,\nranked by expected benefit. The benefit of this approach is that it’s more strategic\nand much more closely aligned with the needs of the business. During sprint plan-\nning, you decide whether it’s more valuable to implement features or extract services. \n \n\n\n443\nDesigning how the service and the monolith collaborate\n13.3\nDesigning how the service and the monolith \ncollaborate\nA service is rarely standalone. It usually needs to collaborate with the monolith. Some-\ntimes a service needs to access data owned by the monolith or invoke its operations.\nFor example, Delayed Delivery Service, described in detail in section 13.4.1, requires\naccess to the monolith’s orders and customer contact info. The monolith might also\nneed to access data owned by the service or invoke its operations. For example, later\nin section 13.5, when discussing how to extract delivery management into a service, I\ndescribe how the monolith needs to invoke Delivery Service.\n One important concern is maintaining data consistency between the service and\nmonolith. In particular, when you extract a service from the monolith, you invariably\nsplit what were originally ACID transactions. You must be careful to ensure that data\nconsistency is still maintained. As described later in this section, sometimes you use\nsagas to maintain data consistency.\n The interaction between a service and the monolith is, as described earlier, facili-\ntated by integration glue code. Figure 13.8 shows the structure of the integration glue.\nIt consists of adapters in the service and monolith that communicate using some kind\nof IPC mechanism. Depending on the requirements, the service and monolith might\ninteract over REST or they might use messaging. They might even communicate using\nmultiple IPC mechanisms.\nFor example, Delayed Delivery Service uses both REST and domain events. It\nretrieves customer contact info from the monolith using REST. It tracks the state of\nOrders by subscribing to domain events published by the monolith.\nMonolith\nService\nInbound\nadapter\nIntegration\nglue\nAPI\nadapter\nAPI\nadapter\nOutbound\nadapter\nOutbound\nadapter\nInbound\nadapter\nFigure 13.8\nWhen migrating a monolith to microservices, the services and monolith often need to access each \nother’s data. This interaction is facilitated by the integration glue, which consists of adapters that implement APIs. \nSome APIs are messaging based. Other APIs are RPI based.\n \n\n\n444\nCHAPTER 13\nRefactoring to microservices\n In this section, I first describe the design of the integration glue. I talk about the prob-\nlems it solves and the different implementation options. After that I describe transaction\nmanagement strategies, including the use of sagas. I discuss how sometimes the require-\nment to maintain data consistency changes the order in which you extract services.\n Let’s first look at the design of the integration glue.\n13.3.1 Designing the integration glue\nWhen implementing a feature as a service or extracting a service from the monolith,\nyou must develop the integration glue that enables a service to collaborate with the\nmonolith. It consists of code in both the service and monolith that uses some kind of\nIPC mechanism. The structure of the integration glue depends on the type of IPC\nmechanism that is used. If, for example, the service invokes the monolith using REST,\nthen the integration glue consists of a REST client in the service and web controllers\nin the monolith. Alternatively, if the monolith subscribes to domain events published\nby the service, then the integration glue consists of an event-publishing adapter in the\nservice and event handlers in the monolith.\nDESIGNING THE INTEGRATION GLUE API\nThe first step in designing the integration glue is to decide what APIs it provides to\nthe domain logic. There are a couple of different styles of interface to choose from,\ndepending on whether you’re querying data or updating data. Let’s say you’re work-\ning on Delayed Delivery Service, which needs to retrieve customer contact info\nfrom the monolith. The service’s business logic doesn’t need to know the IPC mecha-\nnism that the integration glue uses to retrieve the information. Therefore, that mecha-\nnism should be encapsulated by an interface. Because Delayed Delivery Service is\nquerying data, it makes sense to define a CustomerContactInfoRepository:\ninterface CustomerContactInfoRepository {\nCustomerContactInfo findCustomerContactInfo(long customerId)\n}\nThe service’s business logic can invoke this API without knowing how the integration\nglue retrieves the data.\n Let’s consider a different service. Imagine that you’re extracting delivery manage-\nment from the FTGO monolith. The monolith needs to invoke Delivery Service to\nschedule, reschedule, and cancel deliveries. Once again, the details of the underlying\nIPC mechanism aren’t important to the business logic and should be encapsulated by\nan interface. In this scenario, the monolith must invoke a service operation, so using a\nrepository doesn’t make sense. A better approach is to define a service interface, such\nas the following:\ninterface DeliveryService {\nvoid scheduleDelivery(...);\nvoid rescheduleDelivery(...);\nvoid cancelDelivery(...);\n}\n \n\n\n445\nDesigning how the service and the monolith collaborate\nThe monolith’s business logic invokes this API without knowing how it’s implemented\nby the integration glue.\n Now that we’ve seen interface design, let’s look at interaction styles and IPC\nmechanisms. \nPICKING AN INTERACTION STYLE AND IPC MECHANISM\nAn important design decision you must make when designing the integration glue is\nselecting the interaction styles and IPC mechanisms that enable the service and the\nmonolith to collaborate. As described in chapter 3, there are several interaction\nstyles and IPC mechanisms to choose from. Which one you should use depends on\nwhat one party—the service or monolith—needs in order to query or update the\nother party.\n If one party needs to query data owned by the other party, there are several\noptions. One option is, as figure 13.9 shows, for the adapter that implements the\nrepository interface to invoke an API of the data provider. This API will typically use a\nrequest/response interaction style, such as REST or gRPC. For example, Delayed\nDelivery Service might retrieve the customer contact info by invoking a REST API\nimplemented by the FTGO monolith.\nIn this example, the Delayed Delivery Service’s domain logic retrieves the customer\ncontact info by invoking the CustomerContactInfoRepository interface. The imple-\nmentation of this interface invokes the monolith’s REST API.\n An important benefit of querying data by invoking a query API is its simplicity. The\nmain drawback is that it’s potentially inefficient. A consumer might need to make a\nlarge number of requests. A provider might return a large amount of data. Another\ndrawback is that it reduces availability because it’s synchronous IPC. As a result, it\nmight not be practical to use a query API.\nDelayed\nDelivery Service\nCustomer\nContactInfo\nRepository\nGET/customers/{customerId}\nFTGO\nmonolith\nMonolith\ndatabase\nREST\nAPI\nREST\nclient\nFigure 13.9\nThe adapter that implements the CustomerContactInfoRepository interface invokes the \nmonolith’s REST API to retrieve the customer information.\n \n\n\n446\nCHAPTER 13\nRefactoring to microservices\n An alternative approach is for the data consumer to maintain a replica of the data,\nas shown in figure 13.10. The replica is essentially a CQRS view. The data consumer\nkeeps the replica up-to-date by subscribing to domain events published by the data\nprovider.\nUsing a replica has several benefits. It avoids the overhead of repeatedly querying the\ndata provider. Instead, as discussed when describing CQRS in chapter 7, you can\ndesign the replica to support efficient queries. One drawback of using a replica,\nthough, is the complexity of maintaining it. A potential challenge, as described later\nin this section, is the need to modify the monolith to publish domain events.\n Now that we’ve discussed how to do queries, let’s consider how to do updates. One\nchallenge with performing updates is the need to maintain data consistency across the\nservice and monolith. The party making the update request (the requestor) has\nupdated or needs to update its database. So it’s essential that both updates happen.\nThe solution is for the service and monolith to communicate using transactional mes-\nsaging implemented by a framework, such as Eventuate Tram. In simple scenarios, the\nrequestor can send a notification message or publish an event to trigger an update. In\nmore complex scenarios, the requestor must use a saga to maintain data consistency.\nSection 13.3.2 discusses the implications of using sagas. \nIMPLEMENTING AN ANTI-CORRUPTION LAYER\nImagine you’re implementing a new feature as a brand new service. You’re not con-\nstrained by the monolith’s code base, so you can use modern development techniques\nDelayed\nDelivery Service\nFTGO\nmonolith\nMonolith\ndatabase\nService\ndatabase\nEvent\npublisher\nCustomer event channel\nCustomer\ndomain\nevent\nEvent\nsubscriber\nDatabase\nadapter\nCustomer\nContactInfo\nRepository\nquery()\nupdate()\nFigure 13.10\nThe integration glue replicates data from the monolith to the service. The monolith publishes \ndomain events, and an event handler implemented by the service updates the service’s database.\n \n\n\n447\nDesigning how the service and the monolith collaborate\nsuch as DDD and develop a pristine new domain model. Also, because the FTGO\nmonolith’s domain is poorly defined and somewhat out-of-date, you’ll probably model\nconcepts differently. As a result, your service’s domain model will have different class\nnames, field names, and field values. For example, Delayed Delivery Service has a\nDelivery entity with narrowly focused responsibilities, whereas the FTGO monolith\nhas an Order entity with an excessive number of responsibilities. Because the two\ndomain models are different, you must implement what DDD calls an anti-corruption\nlayer (ACL) in order for the service to communicate with the monolith.\nThe goal of an ACL is to prevent a legacy monolith’s domain model from polluting a\nservice’s domain model. It’s a layer of code that translates between the different\ndomain models. For example, as figure 13.11 shows, Delayed Delivery Service has a\nCustomerContactInfoRepository interface, which defines a findCustomerContact-\nInfo() method that returns CustomerContactInfo. The class that implements the\nCustomerContactInfoRepository interface must translate between the ubiquitous\nlanguage of Delayed Delivery Service and that of the FTGO monolith.\nThe implementation of findCustomerContactInfo() invokes the FTGO monolith to\nretrieve the customer information and translates the response to CustomerContact-\nInfo. In this example, the translation is quite simple, but in other scenarios it could\nbe quite complex and involve, for example, mapping values such as status codes.\nPattern: Anti-corruption layer\nA software layer that translates between two different domain models in order to\nprevent concepts from one model polluting another. See https://microservices.io/\npatterns/refactoring/anti-corruption-layer.html.\nDelayed\nDelivery Service\nFTGO\nmonolith\nREST\nAPI\nAPI\nTranslation layer\nGET/user/{userId}\nMonolith layer\nREST client\nCustomer\nContactInfo\nRepository\nUbiquitous language of service\nUbiquitous language of monolith\nAnti-corruption layer\nFigure 13.11\nA service adapter that invokes the monolith must translate between the service’s domain model \nand the monolith’s domain model.\n \n\n\n448\nCHAPTER 13\nRefactoring to microservices\n An event subscriber, which consumes domain events, also has an ACL. Domain\nevents are part of the publisher’s domain model. An event handler must translate\ndomain events to the subscriber’s domain model. For example, as figure 13.12 shows,\nthe FTGO monolith publishes Order domain events. Delivery Service has an event\nhandler that subscribes to those events.\nThe event handler must translate domain events from the monolith’s domain lan-\nguage to that of Delivery Service. It might need to map class and attribute names\nand potentially attribute values.\n It’s not just services that use an anti-corruption layer. A monolith also uses an ACL\nwhen invoking the service and when subscribing to domain events published by a ser-\nvice. For example, the FTGO monolith schedules a delivery by sending a notification\nmessage to Delivery Service. It sends the notification by invoking a method on the\nDeliveryService interface. The implementation class translates its parameters into a\nmessage that Delivery Service understands. \nHOW THE MONOLITH PUBLISHES AND SUBSCRIBES TO DOMAIN EVENTS\nDomain events are an important collaboration mechanism. It’s straightforward for a\nnewly developed service to publish and consume events. It can use one of the mech-\nanisms described in chapter 3, such as the Eventuate Tram framework. A service\nmight even publish events using event sourcing, described in chapter 6. It’s poten-\ntially challenging, though, to change the monolith to publish and consume events.\nLet’s look at why.\n There are a couple of different ways that a monolith can publish domain events.\nOne approach is to use the same domain event publishing mechanism used by the\nDelayed\nDelivery\nService\nFTGO\nmonolith\nEvent handler\nTranslation layer\nMessaging client\nUbiquitous language of service\nUbiquitous language of monolith\nAnti-corruption layer\nEvent channel\nOrder\nevent\nEvent\npublisher\nFigure 13.12\nAn event handler must translate from the event publisher’s domain model to the subscriber’s domain \nmodel.\n \n\n\n449\nDesigning how the service and the monolith collaborate\nservices. You find all the places in the code that change a particular entity and insert a\ncall to an event publishing API. The problem with this approach is that changing a\nmonolith isn’t always easy. It might be time consuming and error prone to locate all\nthe places and insert calls to publish events. To make matters worse, some of the\nmonolith’s business logic might consist of stored procedures that can’t easily publish\ndomain events.\n Another approach is to publish domain events at the database level. You can, for\nexample, use either transaction logic tailing or polling, described in chapter 3. A key\nbenefit of using transaction tailing is that you don’t have to change the monolith. The\nmain drawback of publishing events at the database level is that it’s often difficult to\nidentify the reason for the update and publish the appropriate high-level business\nevent. As a result, the service will typically publish events representing changes to\ntables rather than business entities.\n Fortunately, it’s usually easier for the monolith to subscribe to domain events pub-\nlished as services. Quite often, you can write event handlers using a framework, such\nas Eventuate Tram. But sometimes it’s even challenging for the monolith to subscribe\nto events. For example, the monolith might be written in a language that doesn’t have\na message broker client. In that situation, you need to write a small “helper” applica-\ntion that subscribes to events and updates the monolith’s database directly.\n Now that we’ve looked at how to design the integration glue that enables a ser-\nvice and the monolith to collaborate, let’s look at another challenge you might face\nwhen migrating to microservices: maintaining data consistency across a service and\na monolith. \n13.3.2 Maintaining data consistency across a service and a monolith\nWhen you develop a service, you might find it challenging to maintain data consis-\ntency across the service and the monolith. A service operation might need to update\ndata in the monolith, or a monolith operation might need to update data in the ser-\nvice. For example, imagine you extracted Kitchen Service from the monolith. You\nwould need to change the monolith’s order-management operations, such as create-\nOrder() and cancelOrder(), to use sagas in order to keep the Ticket consistent with\nthe Order.\n The problem with using sagas, however, is that the monolith might not be a will-\ning participant. As described in chapter 4, sagas must use compensating transactions\nto undo changes. Create Order Saga, for example, includes a compensating transac-\ntion that marks an Order as rejected if it’s rejected by Kitchen Service. The prob-\nlem with compensating transactions in the monolith is that you might need to make\nnumerous and time-consuming changes to the monolith in order to support them.\nThe monolith might also need to implement countermeasures to handle the lack of\nisolation between sagas. The cost of these code changes can be a huge obstacle to\nextracting a service.\n \n\n\n450\nCHAPTER 13\nRefactoring to microservices\nFortunately, many sagas are straightforward to implement. As covered in chapter 4, if\nthe monolith’s transactions are either pivot transactions or retriable transactions, then\nimplementing sagas should be straightforward. You may even be able to simplify\nimplementation by carefully ordering the sequence of service extractions so that the\nmonolith’s transactions never need to be compensatable. Or it may be relatively diffi-\ncult to change the monolith to support compensating transactions. To understand\nwhy implementing compensating transactions in the monolith is sometimes challeng-\ning, let’s look at some examples, beginning with a particularly troublesome one.\nTHE CHALLENGE OF CHANGING THE MONOLITH TO SUPPORT COMPENSATABLE TRANSACTIONS\nLet’s dig into the problem of compensating transactions that you’ll need to solve when\nextracting Kitchen Service from the monolith. This refactoring involves splitting the\nOrder entity and creating a Ticket entity in Kitchen Service. It impacts numerous\ncommands implemented by the monolith, including createOrder().\n The monolith implements the createOrder() command as a single ACID transac-\ntion consisting of the following steps:\n1\nValidate order details.\n2\nVerify that the consumer can place an order.\n3\nAuthorize consumer’s credit card.\n4\nCreate an Order.\nYou need to replace this ACID transaction with a saga consisting of the following steps:\n1\nIn the monolith\n– Create an Order in an APPROVAL_PENDING state.\n– Verify that the consumer can place an order.\nKey saga terminology\nI cover sagas in chapter 4. Here are some key terms:\nSaga—A sequence of local transactions coordinated through asynchronous\nmessaging.\nCompensating transaction—A transaction that undoes the updates made by a\nlocal transaction.\nCountermeasure—A design technique used to handle the lack of isolation\nbetween sagas.\nSemantic lock—A countermeasure that sets a flag in a record that is being\nupdated by a saga.\nCompensatable transaction—A transaction that needs a compensating trans-\naction because one of the transactions that follows it in the saga can fail.\nPivot transaction—A transaction that is the saga’s go/no-go point. If it suc-\nceeds, then the saga will run to completion.\nRetriable transaction—A transaction that follows the pivot transaction and is\nguaranteed to succeed.\n \n\n\n451\nDesigning how the service and the monolith collaborate\n2\nIn the Kitchen Service\n– Validate order details.\n– Create a Ticket in the CREATE_PENDING state.\n3\nIn the monolith\n– Authorize consumer’s credit card.\n– Change state of Order to APPROVED.\n4\nIn Kitchen Service\n– Change the state of the Ticket to AWAITING_ACCEPTANCE.\nThis saga is similar to CreateOrderSaga described in chapter 4. It consists of four local\ntransactions, two in the monolith and two in Kitchen Service. The first transaction\ncreates an Order in the APPROVAL_PENDING state. The second transaction creates a\nTicket in the CREATE_PENDING state. The third transaction authorizes the Consumer\ncredit card and changes the state of the order to APPROVED. The fourth and final trans-\naction changes the state of the Ticket to AWAITING_ACCEPTANCE.\n The challenge with implementing this saga is that the first step, which creates the\nOrder, must be compensatable. That’s because the second local transaction, which\noccurs in Kitchen Service, might fail and require the monolith to undo the updates\nperformed by the first local transaction. As a result, the Order entity needs to have an\nAPPROVAL_PENDING, a semantic lock countermeasure, described in chapter 4, that\nindicates an Order is in the process of being created.\n The problem with introducing a new Order entity state is that it potentially requires\nwidespread changes to the monolith. You might need to change every place in the\ncode that touches an Order entity. Making these kinds of widespread changes to the\nmonolith is time consuming and not the best investment of development resources.\nIt’s also potentially risky, because the monolith is often difficult to test. \nSAGAS DON’T ALWAYS REQUIRE THE MONOLITH TO SUPPORT COMPENSATABLE TRANSACTIONS\nSagas are highly domain-specific. Some, such as the one we just looked at, require the\nmonolith to support compensating transactions. But it’s quite possible that when you\nextract a service, you may be able to design sagas that don’t require the monolith to\nimplement compensating transactions. That’s because a monolith only needs to sup-\nport compensating transactions if the transactions that follow the monolith’s transac-\ntion can fail. If each of the monolith’s transactions is either a pivot transaction or a\nretriable transaction, then the monolith never needs to execute a compensating trans-\naction. As a result, you only need to make minimal changes to the monolith to sup-\nport sagas.\n For example, imagine that instead of extracting Kitchen Service, you extract Order\nService. This refactoring involves splitting the Order entity and creating a slimmed-\ndown Order entity in Order Service. It also impacts numerous commands, including\ncreateOrder(), which is moved from the monolith to Order Service. In order to\nextract Order Service, you need to change the createOrder() command to use a\nsaga, using the following steps:\n \n\n\n452\nCHAPTER 13\nRefactoring to microservices\n1\nOrder Service\n– Create an Order in an APPROVAL_PENDING state.\n2\nMonolith\n– Verify that the consumer can place an order.\n– Validate order details and create a Ticket.\n– Authorize consumer’s credit card.\n3\nOrder Service\n– Change state of Order to APPROVED.\nThis saga consists of three local transactions, one in the monolith and two in Order\nService. The first transaction, which is in Order Service, creates an Order in the\nAPPROVAL_PENDING state. The second transaction, which is in the monolith, verifies\nthat the consumer can place orders, authorizes their credit card, and creates a\nTicket. The third transaction, which is in Order Service, changes the state of the\nOrder to APPROVED.\n The monolith’s transaction is the saga’s pivot transaction—the point of no return\nfor the saga. If the monolith’s transaction completes, then the saga will run until com-\npletion. Only the first and second steps of this saga can fail. The third transaction\ncan’t fail, so the second transaction in the monolith never needs to be rolled back. As\na result, all the complexity of supporting compensatable transactions is in Order\nService, which is much more testable than the monolith.\n If all the sagas that you need to write when extracting a service have this struc-\nture, you’ll need to make far fewer changes to the monolith. What’s more, it’s possi-\nble to carefully sequence the extraction of services to ensure that the monolith’s\ntransactions are either pivot transactions or retriable transactions. Let’s look at how\nto do that. \nSEQUENCING THE EXTRACTION OF SERVICES TO AVOID IMPLEMENTING COMPENSATING TRANSACTIONS \nIN THE MONOLITH\nAs we just saw, extracting Kitchen Service requires the monolith to implement com-\npensating transactions, whereas extracting Order Service doesn’t. This suggests that\nthe order in which you extract services matters. By carefully ordering the extraction of\nservices, you can potentially avoid having to make widespread modifications to the\nmonolith to support compensatable transactions. We can ensure that the monolith’s\ntransactions are either pivot transactions or retriable transactions. For example, if we\nfirst extract Order Service from the FTGO monolith and then extract Consumer\nService, extracting Kitchen Service will be straightforward. Let’s take a closer look\nat how to do that.\n Once we have extracted Consumer Service, the createOrder() command uses the\nfollowing saga:\n1\nOrder Service: create an Order in an APPROVAL_PENDING state.\n2\nConsumer Service: verify that the consumer can place an order.\n \n\n\n453\nDesigning how the service and the monolith collaborate\n3\nMonolith\n– Validate order details and create a Ticket.\n– Authorize consumer’s credit card.\n4\nOrder Service: change state of Order to APPROVED.\nIn this saga, the monolith’s transaction is the pivot transaction. Order Service imple-\nments the compensatable transaction.\n Now that we’ve extracted Consumer Service, we can extract Kitchen Service. If we\nextract this service, the createOrder() command uses the following saga:\n1\nOrder Service: create an Order in an APPROVAL_PENDING state.\n2\nConsumer Service: verify that the consumer can place an order.\n3\nKitchen Service: validate order details and create a PENDING Ticket.\n4\nMonolith: authorize consumer’s credit card.\n5\nKitchen Service: change state of Ticket to APPROVED.\n6\nOrder Service: change state of Order to APPROVED.\nIn this saga, the monolith’s transaction is still the pivot transaction. Order Service and\nKitchen Service implement the compensatable transactions.\n We can even continue to refactor the monolith by extracting Accounting Service. If\nwe extract this service, the createOrder() command uses the following saga:\n1\nOrder Service: create an Order in an APPROVAL_PENDING state.\n2\nConsumer Service: verify that the consumer can place an order.\n3\nKitchen Service: validate order details and create a PENDING Ticket.\n4\nAccounting Service: authorize consumer’s credit card.\n5\nKitchen Service: change state of Ticket to APPROVED.\n6\nOrder Service: change state of Order to APPROVED.\nAs you can see, by carefully sequencing the extractions, you can avoid using sagas that\nrequire making complex changes to the monolith. Let’s now look at how to handle\nsecurity when migrating to a microservice architecture. \n13.3.3 Handling authentication and authorization\nAnother design issue you need to tackle when refactoring a monolithic application to\na microservice architecture is adapting the monolith’s security mechanism to support\nthe services. Chapter 11 describes how to handle security in a microservice architec-\nture. A microservices-based application uses tokens, such as JSON Web tokens (JWT),\nto pass around user identity. That’s quite different than a typical traditional, mono-\nlithic application that uses in-memory session state and passes around the user iden-\ntity using a thread local. The challenge when transforming a monolithic application\nto a microservice architecture is that you need to support both the monolithic and\nJWT-based security mechanisms simultaneously.\n Fortunately, there’s a straightforward way to solve this problem that only requires\nyou to make one small change to the monolith’s login request handler. Figure 13.13\n \n\n\n454\nCHAPTER 13\nRefactoring to microservices\nshows how this works. The login handler returns an additional cookie, which in this\nexample I call USERINFO, that contains user information, such as the user ID and roles.\nThe browser includes that cookie in every request. The API gateway extracts the infor-\nmation from the cookie and includes it in the HTTP requests that it makes to a ser-\nvice. As a result, each service has access to the needed user information.\nThe sequence of events is as follows:\n1\nThe client makes a login request containing the user’s credentials.\n2\nAPI Gateway routes the login request to the FTGO monolith.\n3\nThe monolith returns a response containing the JSESSIONID session cookie\nand the USERINFO cookie, which contains the user information, such as ID\nand roles.\n4\nThe client makes a request, which includes the USERINFO cookie, in order to\ninvoke an operation.\n5\nAPI Gateway validates the USERINFO cookie and includes it in the Authoriza-\ntion header of the request that it makes to the service. The service validates the\nUSERINFO token and extracts the user information.\nLet’s look at LoginHandler and API Gateway in more detail.\nTHE MONOLITH’S LOGINHANDLER SETS THE USERINFO COOKIE\nLoginHandler processes the POST of the user’s credentials. It authenticates the user\nand stores information about the user in the session. It’s often implemented by a\nFTGO Monolith\nOrder History Service\nPOST/login\nGET/orders\nAuthorization: TOKEN\n...\nHTTP/1.1 200 OK\nSet-cookie: JSESSIONID=...\nSet-cookie: USERINFO=TOKEN\n...\nGET/orders\nCookie: JSESSIONID=...\nCookie: USERINFO=TOKEN\n...\nBrowser-based\nSPA application\nLog in with user\nID and password.\nUser\ndatabase\nAPI\ngateway\nuserId: xxx\nroles:[a, b, c]\n...\nOrderHistory\nRequestHandler\nLogin\nhandler\nInitializes\nQuery\nPOST/login\nLog in with user\nID and password.\nReturn session cookie.\nProvide JWT.\nProvide session cookie.\nContains user information,\nsuch as ID and roles\nIn-memory\nsession\nFigure 13.13\nThe login handler is enhanced to set a USERINFO cookie, which is a JWT containing user \ninformation. API Gateway transfers the USERINFO cookie to an authorization header when it invokes a \nservice.\n \n\n\n455\nImplementing a new feature as a service: handling misdelivered orders\nsecurity framework, such as Spring Security or Passport for NodeJS. If the applica-\ntion is configured to use the default in-memory session, the HTTP response sets a ses-\nsion cookie, such as JSESSIONID. In order to support the migration to microservices,\nLoginHandler must also set the USERINFO cookie containing the JWT that describes\nthe user. \nTHE API GATEWAY MAPS THE USERINFO COOKIE TO THE AUTHORIZATION HEADER\nThe API gateway, as described in chapter 8, is responsible for request routing and API\ncomposition. It handles each request by making one or more requests to the monolith\nand the services. When the API gateway invokes a service, it validates the USERINFO\ncookie and passes it to the service in the HTTP request’s Authorization header. By\nmapping the cookie to the Authorization header, the API gateway ensures that it\npasses the user identity to the service in a standard way that’s independent of the type\nof client.\n Eventually, we’ll most likely extract login and user management into services. But\nas you can see, by only making one small change to the monolith’s login handler, it’s\nnow possible for services to access user information. This enables you focus on devel-\noping services that provide the greatest value to the business and delay extracting less\nvaluable services, such as user management.\n Now that we’ve looked at how to handle security when refactoring to microser-\nvices, let’s see an example of implementing a new feature as a service. \n13.4\nImplementing a new feature as a service: handling \nmisdelivered orders\nLet’s say you’ve been tasked with improving how FTGO handles misdelivered orders.\nA growing number of customers have been complaining about how customer ser-\nvice handles orders not being delivered. The majority of orders are delivered on\ntime, but from time to time orders are either delivered late or not at all. For exam-\nple, the courier gets delayed by unexpectedly bad traffic, so the order is picked up\nand delivered late. Or perhaps by the time the courier arrives at the restaurant, it’s\nclosed, and the delivery can’t be made. To make matters worse, the first time cus-\ntomer service hears about the misdelivery is when they receive an angry email from\nan unhappy customer.\nA true story: My missing ice cream\nOne Saturday night I was feeling lazy and placed an order using a well-known food\ndelivery app to have ice cream delivered from Smitten. It never showed up. The only\ncommunication from the company was an email the next morning saying my order had\nbeen canceled. I also got a voicemail from a very confused customer service agent\nwho clearly didn’t know what she was calling about. Perhaps the call was prompted\nby one of my tweets describing what happened. Clearly, the delivery company had not\nestablished any mechanisms for properly handling inevitable mistakes.\n \n\n\n456\nCHAPTER 13\nRefactoring to microservices\nThe root cause for many of these delivery problems is the primitive delivery schedul-\ning algorithm used by the FTGO application. A more sophisticated scheduler is under\ndevelopment but won’t be finished for a few months. The interim solution is for\nFTGO to proactively handle delayed or canceled orders by apologizing to the cus-\ntomer, and in some cases offering compensation before the customer complains.\n Your job is to implement a new feature that will do the following:\n1\nNotify the customer when their order won’t be delivered on time.\n2\nNotify the customer when their order can’t be delivered because it can’t be\npicked up before the restaurant closes.\n3\nNotify customer service when an order can’t be delivered on time so that they\ncan proactively rectify the situation by compensating the customer.\n4\nTrack delivery statistics.\nThis new feature is fairly simple. The new code must track the state of each Order, and\nif an Order can’t be delivered as promised, the code must notify the customer and cus-\ntomer support, by, for example, sending an email.\n But how—or perhaps more precisely, where—should you implement this new fea-\nture? One approach is to implement a new module in the monolith. The problem\nthere is that developing and testing this code will be difficult. What’s more, this\napproach increases the size of the monolith and thereby makes monolith hell even\nworse. Remember the Law of Holes from earlier: when you’re in a hole, it’s best to stop\ndigging. Rather than make the monolith larger, a much better approach is to imple-\nment these new features as a service.\n13.4.1 The design of Delayed Delivery Service\nWe’ll implement this feature as a service called Delayed Order Service. Figure 13.14\nshows the FTGO application’s architecture after implementing this service. The appli-\ncation consists of the FTGO monolith, the new Delayed Delivery Service, and an\nAPI Gateway. Delayed Delivery Service has an API that defines a single query opera-\ntion called getDelayedOrders(), which returns the currently delayed or undeliver-\nable orders. API Gateway routes the getDelayedOrders() request to the service and all\nother requests to the monolith. The integration glue provides Delayed Order Service\nwith access to the monolith’s data.\n The Delayed Order Service’s domain model consists of various entities, including\nDelayedOrderNotification, Order, and Restaurant. The core logic is implemented\nby the DelayedOrderService class. It’s periodically invoked by a timer to find orders\nthat won’t be delivered on time. It does that by querying Orders and Restaurants. If\nan Order can’t be delivered on time, DelayedOrderService notifies the consumer and\ncustomer service.\n Delayed Order Service doesn’t own the Order and Restaurant entities. Instead,\nthis data is replicated from the FTGO monolith. What’s more, the service doesn’t\nstore the customer contact information, but instead retrieves it from the monolith.\n \n\n\n457\nImplementing a new feature as a service: handling misdelivered orders\nLet’s look at the design of the integration glue that provides Delayed Order Service\naccess to the monolith’s data. \n13.4.2 Designing the integration glue for Delayed Delivery Service\nEven though a service that implements a new feature defines its own entity classes, it\nusually accesses data that’s owned by the monolith. Delayed Delivery Service is no\nexception. It has a DelayedOrderNotification entity, which represents a notification\nthat it has sent to the consumer. But as I just mentioned, its Order and Restaurant enti-\nties replicate data from the FTGO monolith. It also needs to query user contact infor-\nmation in order to notify the user. Consequently, we need to implement integration\nglue that enables Delivery Service to access the monolith’s data.\n Figure 13.15 shows the design of the integration glue. The FTGO monolith pub-\nlishes Order and Restaurant domain events. Delivery Service consumes these events\nand updates its replicas of those entities. The FTGO monolith implements a REST\nMonolith\n???\nAPI gateway\nREST\nAPI\nIntegration\nglue\nDelayed\nOrder\nService\nGetDelayedOrders()\nREST\nAPI\nNotiﬁcation\nService\nCRM system\nCreate case.\nSend apology\nnotiﬁcation.\nNeed to design.\n???\n«Service»\nDelayedDelivery\nService\n«stereotype»\nOrder\n«entity»\nNotiﬁcation\n«entity»\nRestaurant\n«repository»\nCustomer\nContactInfo\nRepository\n«entity»\nOpeningHours\nFigure 13.14\nThe design of Delayed Delivery Service. The integration glue provides Delayed Delivery \nService access to data owned by the monolith, such as the Order and Restaurant entities, and the customer \ncontact information.\n \n\n\n458\nCHAPTER 13\nRefactoring to microservices\nendpoint for querying the customer contact information. Delivery Service calls this\nendpoint when it needs to notify a user that their order cannot be delivered on time.\nLet’s look at the design of each part of the integration, starting with the REST API for\nretrieving customer contact information.\nQUERYING CUSTOMER CONTACT INFORMATION USING CUSTOMERCONTACTINFOREPOSITORY\nAs described in section 13.3.1, there are a couple of different ways that a service such\nas Delayed Delivery Service could read the monolith’s data. The simplest option is\nfor Delayed Order Service to retrieve data using the monolith’s query API. This\napproach makes sense when retrieving the User contact information. There aren’t\nany latency or performance, issues because Delayed Delivery Service rarely needs to\nretrieve a user’s contact information, and the amount of data is quite small.\n CustomerContactInfoRepository is an interface that enables Delayed Delivery\nService to retrieve a consumer’s contact info. It’s implemented by a Customer-\nContactInfoProxy, which retrieves the user information by invoking the monolith’s\ngetCustomerContactInfo() REST endpoint. \nPUBLISHING AND CONSUMING ORDER AND RESTAURANT DOMAIN EVENTS\nUnfortunately, it isn’t practical for Delayed Delivery Service to query the mono-\nlith for the state of all open Orders and Restaurant hours. That’s because it’s ineffi-\ncient to repeatedly transfer a large amount of data over the network. Consequently,\nDelayed Delivery Service must use the second, more complex option and main-\ntain a replica of Orders and Restaurants by subscribing to events published by the\nmonolith. It’s important to remember that the replica isn’t a complete copy of the\ndata from the monolith—it just stores a small subset of the attributes of Order and\nRestaurant entities.\nMonolith\nEvent\nsubscriber\nDelayed Order Service\nDomain\nevent\npublisher\nREST\nendpoint\nCustomer\nContactInfo\nProxy\n<Repository>\nCustomer\nContactInfo\nRepository\nRestaurant events\ngetCustomerContactInfo()\nOrder events\nRestaurant\nevents\nOrder\nevents\nFigure 13.15\nThe integration glue provides Delayed Delivery Service with access to the data owned by \nthe monolith.\n \n",
      "page_number": 466
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 484-499)",
      "start_page": 484,
      "end_page": 499,
      "detection_method": "topic_boundary",
      "content": "459\nBreaking apart the monolith: extracting delivery management\n As described earlier in section 13.3.1, there are a couple of different ways that we\ncan change the FTGO monolith so that it publishes Order and Restaurant domain\nevents. One option is to modify all the places in the monolith that update Orders and\nRestaurants to publish high-level domain events. The second option is to tail the\ntransaction log to replicate the changes as events. In this particular scenario, we need\nto synchronize the two databases. We don’t require the FTGO monolith to publish\nhigh-level domain events, so either approach is fine.\n Delayed Order Service implements event handlers that subscribe to events from\nthe monolith and update its Order and Restaurant entities. The details of the event\nhandlers depend on whether the monolith publishes specific high-level events or low-\nlevel change events. In either case, you can think of an event handler as translating an\nevent in the monolith’s bounded context to the update of an entity in the service’s\nbounded context.\n An important benefit of using a replica is that it enables Delayed Order Service\nto efficiently query the orders and the restaurant opening hours. One drawback,\nhowever, is that it’s more complex. Another drawback is that it requires the mono-\nlith to publish the necessary Order and Restaurant events. Fortunately, because\nDelayed Delivery Service only needs what’s essentially a subset of the columns of\nthe ORDERS and RESTAURANT tables, we shouldn’t encounter the problems described\nin section 13.3.1.\n Implementing a new feature such as delayed order management as a standalone\nservice accelerates its development, testing, and deployment. What’s more, it enables\nyou to implement the feature using a brand new technology stack instead of the\nmonolith’s older one. It also stops the monolith from growing. Delayed order man-\nagement is just one of many new features planned for the FTGO application. The\nFTGO team can implement many of these features as separate services.\n Unfortunately, you can’t implement all changes as new services. Quite often you\nmust make extensive changes to the monolith to implement new features or change\nexisting features. Any development involving the monolith will mostly likely be slow\nand painful. If you want to accelerate the delivery of these features, you must break up\nthe monolith by migrating functionality from the monolith into services. Let’s look at\nhow to do that. \n13.5\nBreaking apart the monolith: extracting delivery \nmanagement\nTo accelerate the delivery of features that are implemented by a monolith, you need\nto break up the monolith into services. For example, let’s imagine that you want to\nenhance FTGO delivery management by implementing a new routing algorithm. A\nmajor obstacle to developing delivery management is that it’s entangled with order\nmanagement and is part of the monolithic code base. Developing, testing, and deploy-\ning delivery management is likely to be slow. In order to accelerate its development,\nyou need to extract delivery management into a Delivery Service.\n \n\n\n460\nCHAPTER 13\nRefactoring to microservices\n I start this section by describing delivery management and how it’s currently\nembedded within the monolith. Next I discuss the design of the new, standalone\nDelivery Service and its API. I then describe how Delivery Service and the FTGO\nmonolith collaborate. Finally I talk about some of the changes we need to make to the\nmonolith to support Delivery Service.\n Let’s begin by reviewing the existing design.\n13.5.1 Overview of existing delivery management functionality\nDelivery management is responsible for scheduling the couriers that pick up orders at\nrestaurants and deliver them to consumers. Each courier has a plan that is a schedule\nof pickup and deliver actions. A pickup action tells the Courier to pick up an order\nfrom a restaurant at a particular time. A deliver action tells the Courier to deliver an\norder to a consumer. The plans are revised whenever orders are placed, canceled, or\nrevised, and as the location and availability of couriers changes.\n Delivery management is one of the oldest parts of the FTGO application. As fig-\nure 13.16 shows, it’s embedded within order management. Much of the code for man-\naging deliveries is in OrderService. What’s more, there’s no explicit representation of\na Delivery. It’s embedded within the Order entity, which has various delivery-related\nfields, such as scheduledPickupTime and scheduledDeliveryTime.\n Numerous commands implemented by the monolith invoke delivery manage-\nment, including the following:\n\nacceptOrder()—Invoked when a restaurant accepts an order and commits to\npreparing it by a certain time. This operation invokes delivery management to\nschedule a delivery.\n\ncancelOrder()—Invoked when a consumer cancels an order. If necessary, it\ncancels the delivery.\n\nnoteCourierLocationUpdated()—Invoked by the courier’s mobile application\nto update the courier’s location. It triggers the rescheduling of deliveries.\n\nnoteCourierAvailabilityChanged()—Invoked by the courier’s mobile applica-\ntion to update the courier’s availability. It triggers the rescheduling of deliveries.\nAlso, various queries retrieve data maintained by delivery management, including the\nfollowing:\n\ngetCourierPlan()—Invoked by the courier’s mobile application and returns\nthe courier’s plan\n\ngetOrderStatus()—Returns the order’s status, which includes delivery-related\ninformation such as the assigned courier and the ETA\n\ngetOrderHistory()—Returns similar information as getOrderStatus() except\nabout multiple orders\nQuite often what’s extracted into a service is, as mentioned in section 13.2.3, an entire\nvertical slice, with controllers at the top and database tables at the bottom. We could\n \n\n\n461\nBreaking apart the monolith: extracting delivery management\nconsider the Courier-related commands and queries to be part of delivery manage-\nment. After all, delivery management creates the courier plans and is the primary con-\nsumer of the Courier location and availability information. But in order to minimize\nthe development effort, we’ll leave those operations in the monolith and just extract\nthe core of the algorithm. Consequently, the first iteration of Delivery Service won’t\nexpose a publicly accessible API. Instead, it will only be invoked by the monolith.\nNext, let’s explore the design of Delivery Service. \nAPI\nFTGO monolith\n«Service»\nOrderService\n«Service»\nCourierService\n...\n«delivery management»\nscheduleDelivery()\nrescheduleDelivery()\ncancelDelivery()\nreviseSchedule()\n...\nacceptOrder()\ncancelOrder()\ngetOrderStatus()\ngetOrderHistory()\nupdateCourierLocation()\nupdateCourierAvailability()\ngetCourierPlan()\nOrder operations:\nCourier operations:\n«entity»\nCourier\n«value object»\nPlan\n«entity»\nOrder\n«entity»\nRestaurant\n«value object»\nAction\n«value object»\nDropoff\n«value object»\nPickup\nFigure 13.16\nDelivery management is entangled with order management within the FTGO monolith.\n \n\n\n462\nCHAPTER 13\nRefactoring to microservices\n13.5.2 Overview of Delivery Service\nThe proposed new Delivery Service is responsible for scheduling, rescheduling, and\ncanceling deliveries. Figure 13.17 shows a high-level view of the architecture of the\nFTGO application after extracting Delivery Service. The architecture consists of\nthe FTGO monolith and Delivery Service. They collaborate using the integration\nglue, which consists of APIs in both the service and monolith. Delivery Service has\nits own domain model and database.\nIn order to flesh out this architecture and determine the service’s domain model, we\nneed to answer the following questions:\nWhich behavior and data are moved to Delivery Service?\nWhat API does Delivery Service expose to the monolith?\nWhat API does the monolith expose to Delivery Service?\nThese issues are interrelated because the distribution of responsibilities between the\nmonolith and the service affects the APIs. For instance, Delivery Service will need to\ninvoke an API provided by the monolith to access the data in the monolith’s data-\nbase and vice versa. Later, I’ll describe the design of the integration glue that enables\nMonolith\ndomain model\nIntegration glue\nWhat API does the Delivery Service\nexpose to the monolith?\nDelivery Service\ndomain model\nFTGO Monolith\nDelivery Service\nDelivery\nService\ndatabase\nMonolith\ndatabase\nAdapter\nAdapter\nWhat API does the monolith\nexpose to the Delivery Service?\nWhich behavior and\ndata is moved to the\nDelivery Service?\nFigure 13.17\nThe high-level view of the FTGO application after extracting Delivery Service. The FTGO \nmonolith and Delivery Service collaborate using the integration glue, which consists of APIs in each of them. \nThe two key decisions that need to be made are which functionality and data are moved to Delivery Service \nand how do the monolith and Delivery Service collaborate via APIs?\n \n\n\n463\nBreaking apart the monolith: extracting delivery management\nDelivery Service and the FTGO monolith to collaborate. But first, let’s look at the\ndesign of Delivery Service’s domain model. \n13.5.3 Designing the Delivery Service domain model\nTo be able to extract delivery management, we first need to identify the classes that\nimplement it. Once we’ve done that, we can decide which classes to move to Delivery\nService to form its domain logic. In some cases, we’ll need to split classes. We’ll\nalso need to decide which data to replicate between the service and the monolith.\n Let’s start by identifying the classes that implement delivery management.\nIDENTIFYING WHICH ENTITIES AND THEIR FIELDS ARE PART OF DELIVERY MANAGEMENT\nThe first step in the process of designing Delivery Service is to carefully review the\ndelivery management code and identify the participating entities and their fields. Fig-\nure 13.18 shows the entities and fields that are part of delivery management. Some\nfields are inputs to the delivery-scheduling algorithm, and others are the outputs. The\nfigure shows which of those fields are also used by other functionality implemented by\nthe monolith.\nThe delivery scheduling algorithm reads various attributes including the Order’s\nrestaurant, promisedDeliveryTime, and deliveryAddress, and the Courier’s location,\navailability, and current plans. It updates the Courier’s plans, the Order’s scheduled-\nPickupTime, and scheduledDeliveryTime. As you can see, the fields used by delivery\nmanagement are also used by the monolith. \nOrder\n«Monolith Read/Write»\n«Service Read»\nstate\ndeliveryAddress\npromisedDeliveryTime\npreparedByTime\n«Service Read/Write»\n«Monolith Read»\nscheduledPickupTime\nscheduledDeliveryTime\nRestaurant\n«Read»\naddress\nCourier\n«Monolith Read/Write»\n«Service Read»\nLocation\navailability\n«Service Read/Write»\n«Monolith Read»\nPlan\nTask\nFigure 13.18\nThe entities and fields that are accessed by delivery management \nand other functionality implemented by the monolith. A field can be read or written \nor both. It can be accessed by delivery management, the monolith, or both.\n \n\n\n464\nCHAPTER 13\nRefactoring to microservices\nDECIDING WHICH DATA TO MIGRATE TO DELIVERY SERVICE\nNow that we’ve identified which entities and fields participate in delivery manage-\nment, the next step is to decide which of them we should move to the service. In an\nideal scenario, the data accessed by the service is used exclusively by the service, so we\ncould simply move that data to the service and be done. Sadly, it’s rarely that simple,\nand this situation is no exception. All the entities and fields used by the delivery man-\nagement are also used by other functionality implemented by the monolith.\n As a result, when determining which data to move to the service, we need to keep\nin mind two issues. The first is: how does the service access the data that remains in\nthe monolith? The second is: how does the monolith access data that’s moved to the\nservice? Also, as described earlier in section 13.3, we need to carefully consider how to\nmaintain data consistency between the service and the monolith.\n The essential responsibility of Delivery Service is managing courier plans and\nupdating the Order’s scheduledPickupTime and scheduledDeliveryTime fields. It\nmakes sense, therefore, for it to own those fields. We could also move the Cou-\nrier.location and Courier.availability fields to Delivery Service. But because\nwe’re trying to make the smallest possible change, we’ll leave those fields in the mono-\nlith for now. \nTHE DESIGN OF THE DELIVERY SERVICE DOMAIN LOGIC\nFigure 13.19 shows the design of the Delivery Service’s domain model. The core of\nthe service consists of domain classes such as Delivery and Courier. The Delivery-\nServiceImpl class is the entry point into the delivery management business logic. It\nimplements the DeliveryService and CourierService interfaces, which are invoked\nby DeliveryServiceEventsHandler and DeliveryServiceNotificationsHandlers,\ndescribed later in this section.\n The delivery management business logic is mostly code copied from the monolith.\nFor example, we’ll copy the Order entity from the monolith to Delivery Service,\nrename it to Delivery, and delete all fields except those used by delivery manage-\nment. We’ll also copy the Courier entity and delete most of its fields. In order to\ndevelop the domain logic for Delivery Service, we will need to untangle the code\nfrom the monolith. We’ll need to break numerous dependencies, which is likely to be\ntime consuming. Once again, it’s a lot easier to refactor code when using a statically\ntyped language, because the compiler will be your friend.\n Delivery Service is not a standalone service. Let’s look at the design of the inte-\ngration glue that enables Delivery Service and the FTGO monolith to collaborate. \n \n \n \n \n \n\n\n465\nBreaking apart the monolith: extracting delivery management\n13.5.4 The design of the Delivery Service integration glue\nThe FTGO monolith needs to invoke Delivery Service to manage deliveries. The\nmonolith also needs to exchange data with Delivery Service. This collaboration is\nenabled by the integration glue. Figure 13.20 shows the design of the Delivery Ser-\nvice integration glue. Delivery Service has a delivery management API. It also pub-\nlishes Delivery and Courier domain events. The FTGO monolith publishes Courier\ndomain events.\n Let’s look at the design of each part of the integration glue, starting with Delivery\nService’s API for managing deliveries.\nTHE DESIGN OF THE DELIVERY SERVICE API\nDelivery Service must provide an API that enables the monolith to schedule, revise,\nand cancel deliveries. As you’ve seen throughout this book, the preferred approach is\nto use asynchronous messaging, because it promotes loose coupling and increases\navailability. One approach is for Delivery Service to subscribe to Order domain\nevents published by the monolith. Depending on the type of the event, it creates,\nDelivery Service\nDeliveryServiceImpl\n«interface»\nDeliveryService\nvoid schedule(...)\nvoid reschedule(...)\nvoid cancel(...)\n«interface»\nCourierService\nnoteCourierLocationUpdated(...)\nnoteCourierAvailabilityUpdated(...)\n«entity»\nCourier\n«entity»\nDelivery\n«value object»\nPlan\nDeliveryService\nEventsHandlers\nDeliveryService\nNotiﬁcationHandlers\nFigure 13.19\nThe design of the Delivery Service's domain model\n \n\n\n466\nCHAPTER 13\nRefactoring to microservices\nrevises, and cancels a Delivery. A benefit of this approach is that the monolith doesn’t\nneed to explicitly invoke Delivery Service. The drawback of relying on domain events\nis that it requires Delivery Service to know how each Order event impacts the corre-\nsponding Delivery.\n A better approach is for Delivery Service to implement a notification-based API\nthat enables the monolith to explicitly tell Delivery Service to create, revise, and\ncancel deliveries. Delivery Service’s API consists of a message notification channel\nand three message types: ScheduleDelivery, ReviseDelivery, or CancelDelivery. A\nnotification message contains Order information needed by Delivery Service. For\nexample, a ScheduleDelivery notification contains the pickup time and location and\nthe delivery time and location. An important benefit of this approach is that Delivery\nService doesn’t have detailed knowledge of the Order lifecycle. It’s entirely focused\non managing deliveries and has no knowledge of orders.\n This API isn’t the only way that Delivery Service and the FTGO monolith collab-\norate. They also need to exchange data. \nHOW THE DELIVERY SERVICE ACCESSES THE FTGO MONOLITH’S DATA\nDelivery Service needs to access the Courier location and availability data, which is\nowned by the monolith. Because that’s potentially a large amount of data, it’s not practi-\ncal for the service to repeatedly query the monolith. Instead, a better approach is for the\nmonolith to replicate the data to Delivery Service by publishing Courier domain\nevents, CourierLocationUpdated and CourierAvailabilityUpdated. Delivery Service\nhas a CourierEventSubscriber that subscribes to the domain events and updates its\nversion of the Courier. It might also trigger the rescheduling of deliveries. \nDelivery\nService\nFTGO\nmonolith\nCourier events\nCourier events\nDelivery events\nDelivery Service\nnotiﬁcations\nDelivery events\nCourier events\nCourier\nevent\nsubscriber\nDelivery\nevent\nsubscriber\nDelivery\nService\nproxy\nMessaging\nadapter\nMessaging\nadapter\nDelivery\nService\nnotiﬁcations\nhandlers\n«interface»\nDeliveryService\n«interface»\nDeliveryService\n«interface»\nCourierService\nFigure 13.20\nThe design of the Delivery Service integration glue. Delivery Service has a delivery \nmanagement API. The service and the FTGO monolith synchronize data by exchanging domain events.\n \n\n\n467\nBreaking apart the monolith: extracting delivery management\nHOW THE FTGO MONOLITH ACCESSES THE DELIVERY SERVICE DATA\nThe FTGO monolith needs to read the data that’s been moved to Delivery Service,\nsuch as the Courier plans. In theory, the monolith could query the service, but that\nrequires extensive changes to the monolith. For the time being, it’s easier to leave the\nmonolith’s domain model and database schema unchanged and replicate data from\nthe service back to the monolith.\n The easiest way to accomplish that is for Delivery Service to publish Courier and\nDelivery domain events. The service publishes a CourierPlanUpdated event when it\nupdates a Courier’s plan, and a DeliveryScheduleUpdate event when it updates a\nDelivery. The monolith consumes these domain events and updates its database.\n Now that we’ve looked at how the FTGO monolith and Delivery Service interact,\nlet’s see how to change the monolith. \n13.5.5 Changing the FTGO monolith to interact with Delivery Service\nIn many ways, implementing Delivery Service is the easier part of the extraction\nprocess. Modifying the FTGO monolith is much more difficult. Fortunately, replicat-\ning data from the service back to the monolith reduces the size of the change. But we\nstill need to change the monolith to manage deliveries by invoking Delivery Service.\nLet’s look at how to do that.\nDEFINING A DELIVERYSERVICE INTERFACE\nThe first step is to encapsulate the delivery management code with a Java interface\ncorresponding to the messaging-based API defined earlier. This interface, shown in\nfigure 13.21, defines methods for scheduling, rescheduling, and canceling deliveries.\n«interface»\nDeliveryService\nDeliveryServiceImpl\nvoid schedule(...)\nvoid reschedule(...)\nvoid cancel(...)\nDelivery\nmanagement\nDelivery\nmanagement\nclient\nFigure 13.21\nThe first step is to define DeliveryService, which \nis a coarse-grained, remotable API for invoking the delivery \nmanagement logic.\n \n\n\n468\nCHAPTER 13\nRefactoring to microservices\nEventually, we’ll implement this interface with a proxy that sends messages to the\ndelivery service. But initially, we’ll implement this API with a class that calls the deliv-\nery management code.\n The DeliveryService interface is a coarse-grained interface that’s well suited to\nbeing implemented by an IPC mechanism. It defines schedule(), reschedule(), and\ncancel() methods, which correspond to the notification message types defined earlier. \nREFACTORING THE MONOLITH TO CALL THE DELIVERYSERVICE INTERFACE\nNext, as figure 13.22 shows, we need to identify all the places in the FTGO monolith\nthat invoke delivery management and change them to use the DeliveryService inter-\nface. This may take some time and is one of the most challenging aspects of extracting\na service from the monolith.\nIt certainly helps if the monolith is written in a statically typed language, such as Java,\nbecause the tools do a better job of identifying dependencies. If not, then hopefully\nyou have some automated tests with sufficient coverage of the parts of the code that\nneed to be changed. \nIMPLEMENTING THE DELIVERYSERVICE INTERFACE\nThe final step is to replace the DeliveryServiceImpl class with a proxy that sends\nnotification messages to the standalone Delivery Service. But rather than discard\nthe existing implementation right away, we’ll use a design, shown in figure 13.23, that\nenables the monolith to dynamically switch between the existing implementation and\nDelivery Service. We’ll implement the DeliveryService interface with a class that\nuses a dynamic feature toggle to determine whether to invoke the existing implemen-\ntation or Delivery Service.\n«interface»\nDeliveryService\nDeliveryServiceImpl\nvoid schedule(...)\nvoid reschedule(...)\nvoid cancel(...)\nDelivery\nmanagement\nDelivery\nmanagement\nclient\nFigure 13.22\nThe second step is to change the FTGO monolith to \ninvoke delivery management via the DeliveryService interface.\n \n\n\n469\nBreaking apart the monolith: extracting delivery management\nUsing a feature toggle significantly reduces the risk of rolling out Delivery Service. We\ncan deploy Delivery Service and test it. And then, once we’re sure it works, we can flip\nthe toggle to route traffic to it. If we then discover that Delivery Service isn’t working\nas expected, we can switch back to the old implementation.\nOnce we’re sure that Delivery Service is working as expected, we can then remove\nthe delivery management code from the monolith.\n Delivery Service and Delayed Order Service are examples of the services that\nthe FTGO team will develop during their journey to the microservice architecture.\nWhere they go next after implementing these services depends on the priorities of the\nbusiness. One possible path is to extract Order History Service, described in chap-\nter 7. Extracting this service partially eliminates the need for Delivery Service to\nreplicate data back to the monolith.\nAbout feature toggles\nFeature toggles, or feature flags, let you deploy code changes without necessarily\nreleasing them to users. They also enable you to dynamically change the behavior\nof the application by deploying new code. This article by Martin Fowler provides an\nexcellent overview of the topic: https://martinfowler.com/articles/feature-toggles\n.html.\n«interface»\nDeliveryService\nvoid schedule(...)\nvoid reschedule(...)\nvoid cancel(...)\nFeatureToggleBased\nDeliveryServiceImpl\nDeliveryServiceImpl\nDeliveryServiceProxy\nDelivery\nmanagement\nInvokes\nInvokes\nSends\nmessage\nDelivery\nmanagement\nclient\nDelivery notiﬁcations\nFigure 13.23\nThe final step is to implement DeliveryService with a proxy class that sends \nmessages Delivery Service. A feature toggle controls whether the FTGO monolith uses the old \nimplementation or the new Delivery Service.\n \n\n\n470\nCHAPTER 13\nRefactoring to microservices\n After implementing Order History Service, the FTGO team can then extract the\nservices in the order described in section 13.3.2: Order Service, Consumer Service,\nKitchen Service, and so on. As the FTGO team extracts each service, the maintain-\nability and testability of their application gradually improves, and their development\nvelocity increases. \nSummary\nBefore migrating to a microservice architecture, it’s important to be sure that\nyour software delivery problems are a result of having outgrown your mono-\nlithic architecture. You might be able to accelerate delivery by improving your\nsoftware development process.\nIt’s important to migrate to microservices by incrementally developing a stran-\ngler application. A strangler application is a new application consisting of\nmicroservices that you build around the existing monolithic application. You\nshould demonstrate value early and often in order to ensure that the business\nsupports the migration effort.\nA great way to introduce microservices into your architecture is to implement\nnew features as services. Doing so enables you to quickly and easily develop a\nfeature using a modern technology and development process. It’s a good way to\nquickly demonstrate the value of migrating to microservices.\nOne way to break up the monolith is to separate the presentation tier from the\nbackend, which results in two smaller monoliths. Although it’s not a huge\nimprovement, it does mean that you can deploy each monolith independently.\nThis allows, for example, the UI team to iterate more easily on the UI design\nwithout impacting the backend.\nThe main way to break up the monolith is by incrementally migrating function-\nality from the monolith into services. It’s important to focus on extracting the\nservices that provide the most benefit. For example, you’ll accelerate develop-\nment if you extract a service that implements functionality that’s being actively\ndeveloped.\nNewly developed services almost always have to interact with the monolith. A\nservice often needs to access a monolith’s data and invoke its functionality. The\nmonolith sometimes needs to access a service’s data and invoke its functionality.\nTo implement this collaboration, develop integration glue, which consists of\ninbound and outbound adapters in the monolith.\nTo prevent the monolith’s domain model from polluting the service’s domain\nmodel, the integration glue should use an anti-corruption layer, which is a layer\nof software that translates between domain models.\nOne way to minimize the impact on the monolith of extracting a service is to\nreplicate the data that was moved to the service back to the monolith’s data-\nbase. Because the monolith’s schema is left unchanged, this eliminates the\nneed to make potentially widespread changes to the monolith code base.\n \n\n\n471\nSummary\nDeveloping a service often requires you to implement sagas that involve the\nmonolith. But it can be challenging to implement a compensatable transaction\nthat requires making widespread changes to the monolith. Consequently, you\nsometimes need to carefully sequence the extraction of services to avoid imple-\nmenting compensatable transactions in the monolith.\nWhen refactoring to a microservice architecture, you need to simultaneously\nsupport the monolithic application’s existing security mechanism, which is often\nbased on an in-memory session, and the token-based security mechanism used\nby the services. Fortunately, a simple solution is to modify the monolith’s login\nhandler to generate a cookie containing a security token, which is then for-\nwarded to the services by the API gateway. \n \n\n\n473\nindex\nNumerics\n2PC (two-phase commit) 112\n3rd party registration pattern 84–85, 108\n4+1 view model of software architecture 35–37\n500 status code, HTTP 367\nA\nAbstractAutowiringHttpRequestHandler class 423\nAbstractHttpHandler class 423\naccept() method 165, 172\nacceptance tests 335–338\ndefining 336\nexecuting specifications using Cucumber 338\nwriting using Gherkin 337–338\nacceptOrder() method 460\nAccess Token 28, 354, 357\nACD (Atomicity, Consistency, Durability) 111\nACID (Atomicity, Consistency, Isolation, Dur-\nability) transactions 98, 110\nACLs (access control lists) 350\nActiveMQ message broker 92\nadd() method 310\naddOrder() method 249–250\nAggregateRepository class 206–208\naggregates 147, 374, 439\nconsistency boundaries 155\ncreating, finding, and updating 207–208\ndefining aggregate commands 207\ndefining with ReflectiveMutableCommand-\nProcessingAggregate class 206–207\ndesigning business logic with 159–160\nevent sourcing\naggregate history 186, 199–200\naggregate methods and events 189–191\nevent sourcing-based Order aggregate\n191–193\npersisting aggregates using events 186–188\nevent sourcing and aggregate history 199–200\nexplicit boundaries 154–155\ngranularity 158\nidentifying 155\nOrder aggregate 175–180\nmethods 177–180\nstate machine 176–177\nstructure of 175–176\nrules for 155–157\nTicket aggregate 169–173\nbehavior of 170–171\nKitchenService domain service 171–172\nKitchenServiceCommandHandler class\n172–173\nstructure of Ticket class 170\ntraditional persistence and aggregate \nhistory 186\naliases 285\nAlternative pattern 22\nAMI (Amazon Machine Image) 390\nanomalies 126\nAnti-corruption layer pattern 447\nAOP (aspect-oriented programming) 373, 378\nApache Flume 370\nApache Kafka 92\nApache Openwhisk 416\nApache Shiro 351\nAPI composition pattern 221–228\nbenefits and drawbacks of 227–228\nincreased overhead 227\nlack of transactional data consistency\n228\nrisk of reduced availability 227–228\n \n\n\nINDEX\n474\nAPI composition pattern (continued)\ndesign issues 225–227\nreactive programming model 227\nrole of API composer 225–227\nfindOrder() query operation 221–222, 224\noverview of 222–224\nAPI gateway 259–291\nauthentication 354–355\nbenefits of 267\ndesign issues 268–271\nbeing good citizen in architecture 270–271\nhandling partial failures 270\nperformance and scalability 268–269\nreactive programming abstractions 269–270\ndrawbacks of 267\nimplementation using GraphQL 279–291\nconnecting schema to data 285–287\ndefining schema 282–284\nexecuting queries 284–285\nintegrating Apollo GraphQL server with \nExpress 289–290\noptimizing loading using batching and \ncaching 288\nwriting client 290–291\nimplementation using Netflix Zuul 273\nimplementation using off-the-shelf products/\nservices 271–272\nAPI gateway products 272\nAWS API gateway service 271–272\nAWS Application Load Balancer service 272\nimplementation using Spring Cloud \nGateway 273–275\nApiGatewayApplication class 279\nOrderConfiguration class 275–276\nOrderHandlers class 276–278\nOrderService class 278–279\nmapping USERINFO cookie to Authorization \nheader 455\nNetflix example 267–268\noverview of 259–266\nAPI composition 261\narchitecture 263–264\nBackends for frontends pattern 264–266\nclient-specific API 262\nedge functions 262–263\nownership model 264\nprotocol translation 262\nrequest routing 260\nApiGatewayApplication class 279\nApiGatewayMain package 274\nAPIGatewayProxyRequestEvent 417, 421–422\nAPIGatewayProxyResponseEvent 417, 422\nAPIs\ndefining in microservice architecture 68–69\ninterprocess communication 69–71\ncreating specification for messaging-based \nservice API 89–90\nmajor, breaking changes 70–71\nminor, backward-compatible changes 70\nsemantic versioning 70\nspecifying REST APIs 74\nrefactoring to microservices 444–445, 465–466\ntesting microservices\nconsumer contract tests for messaging \nAPIs 305\nconsumer-side integration test for API gate-\nway’s OrderServiceProxy 325–326\nexample contract for REST API 324\nSee also API gateways\nApplication architecture patterns\nMicroservice architecture 8–18, 40\nMonolithic architecture 2–7, 22–34, 40\napplication infrastructure 24\napplication metrics 28, 366, 373–376\ncollecting service-level metrics 374–375\ndelivering metrics to metrics service 375–376\napplication modernization 23–24, 430–432\napplication security 349\napply() method 188, 193\narchitectural styles 37–40\nhexagonal 38–40\nlayered 37–38\nmicroservice architecture 40–43\nloose coupling, defined 42–43\nrelative unimportance of size of service 43\nrole of shared libraries 43\nservices, defined 41–42\naspect-oriented programming (AOP) 373, 378\nasynchronous (nonblocking) I/O model 268\nasynchronous interactions 67\nAsynchronous messaging pattern 85–103\ncompeting receivers and message ordering\n94–95\ncreating API specification 89–90\ndocumenting asynchronous operations 90\ndocumenting published events 90\nduplicate messages 95–97\ntracking messages and discarding \nduplicates 96–97\nwriting idempotent message handlers 96\nimproving availability 103–108\neliminating synchronous interaction\n104–108\nsynchronous communication and \navailability 103–104\ninteraction styles 87–89\none-way notifications 89\npublish/subscribe 89\nrequest/response and asynchronous request/\nresponse 87–88\n \n\n\nINDEX\n475\nAsynchronous messaging pattern (continued)\nlibraries and frameworks for 100–103\nbasic messaging 101\ncommand/reply-based messaging 102–103\ndomain event publishing 102\nmessage brokers 90–94\nbenefits and drawbacks of 93–94\nbrokerless messaging 91–92\nimplementing message channels using 93\noverview of 92\noverview of 86–87\ntransactional messaging 97–100\npublishing events using Polling publisher \npattern 98–99\npublishing events using Transaction log tail-\ning pattern 99–100\nusing database table as message queue\n97–98\nasynchronous request/response interactions\nimplementing 87–88\nintegration tests for\nconsumer-side contract tests 332–335\ncontract tests 330–335\nexample contract 331–332\nAtomicity, Consistency, Durability (ACD) 111\nAtomicity, Consistency, Isolation, Durability \n(ACID) transactions 98, 110\nattribute value 245\naudit logging 28, 186, 366, 377–378\nadding code to business logic 378\naspect-oriented programming 378\nevent sourcing 378\nauditing 350\nauthentication and authorization\nrefactoring to microservices 453–455\nAPI gateway maps USERINFO cookie to \nAuthorization header 455\nLoginHandler sets USERINFO cookie\n454–455\nsecurity in microservice architecture\nhandling authentication 354–355\nhandling authorization 356\nAuthorization Server concept 357\nautomated testing 28, 293, 295–296\nautomatic sidecar injection 411\nAvro 72\nAWS API gateway service 271–272\nAWS Application Load Balancer service 272\nAWS DynamoDB 242–252\ndata modeling and query design 244–249\ndetecting duplicate events 248–249\nfindOrderHistory query 245–247\nFTGO-order-history table 245\npaginating query results 247\nupdating orders 247–248\nOrderHistoryDaoDynamoDb class 249–252\naddOrder() method 249–250\nfindOrderHistory() method 251–252\nidempotentUpdate() method 250–251\nnotePickedUp() method 250\nOrderHistoryEventHandlers module 243–244\nAWS Gateway, deploying RESTful services \nusing 419–426\ndeploying lambda functions using Serverless \nframework 425–426\ndesign of Restaurant Service 419–423\npackaging service as ZIP file 424\nAWS Lambda\nbenefits of lambda functions 418\ndeveloping lambda functions 417\ndrawbacks of lambda functions 419\ninvoking lambda functions 417–418\ndefining scheduled lambda functions 418\nhandling events 418\nhandling HTTP requests 417\ninvoking lambda functions using web service \nrequests 418\noverview of 416\nRESTful services 419–426\ndeploying lambda functions using Serverless \nframework 425–426\ndesign of Restaurant Service 419–423\npackaging service as ZIP file 424\naws.region property 363\nAxon 202\nAzure functions, Microsoft 416\nB\nBackends for frontends (BFF) pattern 264–266\nbatching 288\n@Before setUp() method 309\nbeforeHandling() method 423\nBig Ball of Mud pattern 2\nbig bang rewrite 430\nbinary message formats 72\nbounded context 55\nbroker-based messaging 90–94\nbenefits and drawbacks of 93–94\nimplementing message channels using 93\noverview of 92\nbrokerless messaging 91–92\nBrowser API module 264\nbusiness capability 40\nbusiness logic 146–219\nadding audit logging code to 378\ndomain events 160–168\nconsuming 167–168\ndefined 161\nevent enrichment 161–162\n \n",
      "page_number": 484
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 500-508)",
      "start_page": 500,
      "end_page": 508,
      "detection_method": "topic_boundary",
      "content": "INDEX\n476\nbusiness logic (continued)\ngenerating 164–165\nidentifying 162–163\npublishing 166–167\nreasons to publish 160–161\ndomain model design 152–160\naggregates 154–160\nproblem with fuzzy boundaries 153–154\nevent sourcing 184–202\nbenefits of 199–200\ndrawbacks of 200–202\nevent publishing 194–195\nevolving domain events 198–199\nhandling concurrent updates using optimis-\ntic locking 193–194\nidempotent message processing 197\noverview of 186–193\nsnapshots, improving performance with\n195–196\ntraditional persistence 185–186\nevent store implementation 202–209\nEventuate client framework for Java 205–209\nEventuate Local event store 203–205\nKitchen Service business logic 168–173\nOrder Service business logic 173–182\nOrder aggregate 175–180\nOrderService class 180–182\norganization patterns 147–152\nDomain model pattern 150–151\ndomain-driven design 151–152\nTransaction script pattern 149–150\nsagas and event sourcing together 209–218\ncreating orchestration-based saga 211–212\nimplementing choreography-based sagas \nusing event sourcing 210\nimplementing event sourcing-based saga \nparticipant 213–216\nimplementing saga orchestrators using event \nsourcing 216–218\nBusiness logic design patterns\nAggregate 147, 152–160\nDomain event 160\nDomain model 150–151\nEvent sourcing 184\nTransaction script 149–150\nbusiness logic layer 38, 436\nby value countermeasure 131–132\nC\ncaching 262, 288\ncancel() operation 177\ncancelOrder() method 460\nCAP theorem 113\nCCP (Common Closure Principle) 56\ncentralized sessions 354\nchange failure rate 31\nchoreography 111\nchoreography-based sagas 118–121\nbenefits and drawbacks of 121\nimplementing Create Order saga 118–119\nimplementing using event sourcing 210\nreliable event-based communication 120–121\nCI (Continuous Integration) 6, 306, 357\nCircuit breaker pattern 77–80\ndeveloping robust RPI proxies 79\nrecovering from unavailable services 79–80\nClient concept 358\nClient-side discovery pattern 82–83\ncommand message 86\nCommand query responsibility segregation. See \nCQRS pattern\ncommand/reply-based messaging 102–103\ncommands 41\ncommit tests stage 306\ncommitted records 130\nCommon Closure Principle (CCP) 56–57\ncommunication\nflexible 93\nsecure interprocess 350\ncommunication patterns 23–25\ncommutative update countermeasure 130\ncompensatable transactions 116, 128, 450\ncompensating transaction 450\ncompile-time tests 297\ncomponent tests 306, 339–340\nfor FTGO Order Service\nOrderServiceComponentTestStepDefinitions \nclass 341–344\nrunning 344–345\nwriting 340–345\nin-process component tests 339\nout-of-process component tests 339–340\ncondition expression 248\nConduit 381\nConfigMap 402\nconfigurable services 360–364\npull-based externalized configuration 363–364\npush-based externalized configuration 362–363\n@ConfigurationProperties class 276\nconsumer contract testing 301–303\nfor asynchronous request/response \ninteraction 332–335\nfor messaging APIs 305\nfor publish/subscribe-style interactions\n328–330\nfor REST-based request/response style \ninteractions 324–326\nconsumer group 94\nconsumer-driven contract test 28, 302\n \n\n\nINDEX\n477\nconsumerId parameter 229\nconsumer-provider relationship 301\nconsumer-side contract test 28, 302\ncontainers\ncontainer image 395\nDeploy a service as a container 22, 393\nDocker 395–398\ncontinuous deployment 5\ndeployment pipeline 305–307\nContinuous Integration (CI) 6, 306, 357\ncontrollers, unit tests for 313–315\nConway, Melvin 30\nConway’s law 30\ncorrelation ID 88–89, 120\ncountermeasures 111, 126, 450\nCQRS (Command query responsibility \nsegregation) 26, 63, 160, 228–236\nbenefits of 235–236\nefficient implementation 235\nimproved separation of concerns 235–236\nquerying in event sourcing-based \napplication 235\ndrawbacks of 236\nmore complex architecture 236\nreplication lag 236\nmotivations for using 229–232\nfindAvailableRestaurants() query \noperation 231\nfindOrderHistory() query operation 229–231\nneed to separate concerns 231–232\noverview of 232–235\nquery-only services 233–235\nseparating commands from queries 232–233\nviews\nadding and updating 241–242\ndesigning 236–242\nimplementing with AWS DynamoDB\n242–252\nCreate Order saga 114–115, 135–142\nCreateOrderSaga orchestrator 136–138\nCreateOrderSagaState class 138\nEventuate Tram Saga framework 140–142\nimplementing using choreography 118–119\nimplementing using orchestration 122–123\nKitchenServiceProxy class 139\ncreate, update, and delete (CRUD) \noperations 232\ncreate() method 171, 204\ncreateOrder() operation 114\nCreateOrderSaga orchestrator 136–138\nCreateOrderSagaState class 138\nCreateOrderSagaTest class 312\nCross-cutting concerns patterns\nExternalized configuration 28, 361\nMicroservice chassis 28, 378–382\nCRUD (create, update, and delete) \noperations 232\nCucumber framework 338\nCustomerContactInfoRepository interface 445, \n458\nD\nDAO (data access object) 39, 149, 239\ndata access logic layer 436\ndata consistency 449–453\nAPI composition pattern and 228\nmaintaining across services 58\nrefactoring to microservices\nsagas and compensatable transactions\n451–452\nsequencing extraction of services 452–453\nsupporting compensatable transactions\n450–451\nSaga pattern 25–26, 114–117\ndata consistency patterns 25\nSaga pattern 25–26, 114–117\nDataLoader module 288\nDDD (domain-driven design) 24, 34\nDDD aggregate pattern 152–160\nDebezium 100\nDecompose by business capability pattern 51–54\ndecomposition 52–54\nidentifying business capabilities 51–52\npurpose of business capabilities 51\ndecomposition 33–64\nDecompose by subdomain 54\ndefining application’s microservice \narchitecture 44–64\ndefining service APIs 61–64\nguidelines for decomposition 56–57\nidentifying system operations 45–50\nobstacles to decomposition 57–61\nservice definition with Decompose by business \ncapability pattern 51–54\nservice definition with Decompose by sub-\ndomain pattern 54–55\nguidelines for 56–57\nCommon Closure Principle 56–57\nSingle Responsibility Principle 56\nobstacles to 57–61\ngod classes 58–61\nmaintaining data consistency across \nservices 58\nnetwork latency 57\nobtaining consistent view of data 58\nsynchronous interprocess communication 57\npatterns\nDecompose by business capability 24, 51–54\nDecompose by subdomain 24, 54\n \n\n\nINDEX\n478\nDelayed Delivery Service\nchanging FTGO monolith to interact with\n467–470\ndefining interface 467–468\nimplementing interface 468–470\nrefactoring monolith to call interface\n468\ndesign for 456–457\ndomain model 463–464\ndeciding which data to migrate 464\ndesign of domain logic 464\nidentifying which entities and fields are \npart of delivery management 463\nexisting delivery functionality 460–461\nintegration glue for 457–459, 465–467\nCustomerContactInfoRepository \ninterface 458\ndesign of API 465–466\nhow Delivery Service accesses FTGO \ndata 466\nhow FTGO accesses data 467\npublishing and consuming Order and Restau-\nrant domain events 458–459\noverview of 462–463\ndeleted flag 201\ndeliver action 460\nDeliveryServiceImpl class 468\ndependencies 125\ndeploy stage 306\ndeployment 383–427\nLanguage-specific packaging format \npattern 386–390\nbenefits of 388–389\ndrawbacks of 389–390\nRESTful services using AWS Lambda and AWS \nGateway 419–426\ndeploying lambda functions using Serverless \nframework 425–426\ndesign of Restaurant Service 419–423\npackaging service as ZIP file 424\nServerless deployment pattern 415–419\nbenefits of lambda functions 418\ndeveloping lambda functions 417\ndrawbacks of lambda functions 419\ninvoking lambda functions 417–418\noverview of 416\nService as container pattern 393–399\nbenefits of 398\nDocker 395–398\ndrawbacks of 399\nService as virtual machine pattern 390–393\nbenefits of 392\ndrawbacks of 392–393\nService mesh pattern 380\nSidecar pattern 410\nwith Kubernetes 399–415\ndeploying API gateway 405–406\ndeploying Restaurant Service 402–405\noverview of 399–402\nservice meshes 407–415\nzero-downtime deployments 406–407\ndeployment frequency 31\nDeployment patterns\nDeploy a service as a container 22, 393\nDeploy a service as a VM 390, 392\nLanguage-specific packaging format 386, 390\nServerless deployment 415–419\nService mesh 380\nSidecar 410\ndeployment pipeline 305–307\nDeployment view 36\nDestinationRule 413\ndirty reads 127\nDistributed tracing pattern 28, 366, 370–373\ndistributed tracing server 373\ninstrumentation libraries 373\nDistributed Transaction Processing (DTP) 112\nDocker 395–398\nbuilding images 395–396\npushing images to registry 396–397\nrunning containers 397–398\ndocker build command 396\nDocker containers 267\ndocker push command 397\ndocker run command 397\ndocker tag command 396\ndocument message 86\ndomain event publishing 102\ndomain events 160–168, 198–199\nconsuming 167–168, 458–459\ndefined 161\ndefining 207\nevent enrichment 161–162\nevent schema evolution 198–199\ngenerating 164–165\nidentifying 162–163\nmanaging schema changes through \nupcasting 199\npublishing 102, 166–167, 448–449, 458–459\nreasons to publish 160–161\nsubscribing to 208–209, 448–449\ndomain model 54, 150–160\naggregates\nconsistency boundaries 155\ndesigning business logic with 159–160\nexplicit boundaries 154–155\ngranularity 158\nidentifying aggregates 155\nrules for 155–157\ncreating high-level domain model 46–48\n \n\n\nINDEX\n479\ndomain model (continued)\nDelivery Service 463–464\ndeciding which data to migrate 464\ndesign of domain logic 464\nidentifying which entities and fields are part \nof delivery management 463\nproblem with fuzzy boundaries 153–154\nsplitting 439–440\ndomain services\nKitchenService 171–172\nunit tests for 312–313\ndomain-driven design (DDD) 24, 34\nDSL (domain-specific language) 303\nDTP (Distributed Transaction Processing)\n112\ndumb pipes 14\nduplicate messages 95–97\ntracking messages and discarding \nduplicates 96–97\nwriting idempotent message handlers 96\nDynamoDB streams 100\nE\nedge functions 271\nElastic Beanstalk 391\nElasticsearch 370\n@EnableGateway annotation 279\nend-to-end tests 345–346\ndesigning 345\nrunning 346\nwriting 346\nEnterprise Service Bus (ESB) 264\nentities, unit tests for 309–310\nEntity object, DDD 151\nenums 283\nESB (Enterprise Service Bus) 264\nevent. See Domain events\nevent handlers\nevents generated by AWS services 418\nidempotent 240–241\nunit tests for 315–317\nevent message 86\nevent publishing 194–195\nAsynchronous messaging pattern 89–90, \n98–100, 102\ndomain events 160–168\nconsuming 167–168\ndefined 161\nevent enrichment 161–162\ngenerating and publishing 164–167\nidentifying 162–163\nreasons for 160–161\nevent sourcing 194–195, 199\ntraditional persistence and 186\nusing polling 194–195\nusing transaction log tailing 195\nevent sourcing 184–202\naudit logging 378\nbenefits of 199–200\navoids O/R impedance mismatch \nproblem 200\npreserves aggregate history 199–200\nreliable domain event publishing 199\ntime machine for developers 200\nconcurrent updates and optimistic locking\n193–194\ndrawbacks of 200–202\ncomplexity 200\ndeleting data 201\nevolving events 201\nlearning curve 200\nquerying event store 202\nevent publishing 194–195\nusing polling 194–195\nusing transaction log tailing 195\nevolving domain events 198–199\nevent schema evolution 198–199\nmanaging schema changes through \nupcasting 199\nidempotent message processing 197\nwith NoSQL-based event store 197\nwith RDBMS-based event store 197\noverview of 186–193\naggregate methods required to generate \nevents 189–191\nevent sourcing-based Order aggregate 191–193\nevents representing state changes 188\npersisting aggregates using events 186–188\nsagas and 209–218\ncreating orchestration-based saga 211–212\nimplementing choreography-based sagas \nusing event sourcing 210\nimplementing event sourcing-based saga \nparticipant 213–216\nimplementing saga orchestrators using event \nsourcing 216–218\nsnapshots and performance improvement\n195–196\ntrouble with traditional persistence 185–186\naudit logging 186\nevent publishing bolted to business logic 186\nlack of aggregate history 186\nObject-Relational impedance mismatch\n185–186\nEvent Store 202\nevent store implementation 202–209\nEventuate client framework for Java 205–209\nAggregateRepository class 207–208\ndefining aggregate commands 207\n \n\n\nINDEX\n480\nevent store implementation (continued)\ndefining aggregates with ReflectiveMutable-\nCommandProcessingAggregate \nclass 206–207\ndefining domain events 207\nsubscribing to domain events 208–209\nEventuate Local event store 203–205\nconsuming events by subscribing to event \nbroker 205\nevent relay propagates events from database \nto message broker 205\nschema 203–205\nevent storming 162\nevent-driven I/O 269\n@EventHandlerMethod annotation 208\nevents. See Domain events\n@EventSubscriber annotation 208\nEventuate framework 101, 202, 205–209\nand updating aggregates with the Aggregate-\nRepository class 207–208\ndefining aggregate commands 207\ndefining aggregates with ReflectiveMutable-\nCommandProcessingAggregate class\n206–207\ndefining domain events 207\nsubscribing to domain events 208–209\nEventuate Local event store 203–205\nconsuming events by subscribing to event \nbroker 205\nevent relay propagates events from database to \nmessage broker 205\nschema 203–205\nEventuate Tram 100, 166\nEventuate Tram Saga framework 140–142\nException tracking pattern 28, 366, 376–377\nExpress framework 289–290\nexternal API patterns 253–291\nAPI gateway 76, 227, 254, 259–272\nAPI gateway implementation 271–291\nusing GraphQL 279–291\nusing Netflix Zuul 273\nusing off-the-shelf products/services 271–272\nusing Spring Cloud Gateway 273–275\nAPI gateway pattern 76, 227, 254, 259–271\nbenefits of 267\ndesign issues 268–271\ndrawbacks of 267\nNetflix example 267–268\noverview of 259–266\nBackends for frontends 254, 262, 264–266\ndesign issues 254–259\nbrowser-based JavaScript applications 258\nFTGO mobile client 255–258\nthird-party applications 258–259\nweb applications 258\nexternalized configuration 361\npull-based 363–364\npush-based 262–263\nExternalized Configuration pattern 28, 361\nF\nFactory object, DDD 151\nfault isolation 6\nfeature flags 469\nfeature toggles 469\nfilter expression 247\nfilter parameter 229\nfind() operation 204\nfindAvailableRestaurants() query operation 231\nfindCustomerContactInfo() method 447\nfindOrder() operation 221–222, 224\nfindOrderHistory() query operation 229–231, \n251–252\ndefining index for 245–247\nimplementing 247\nFindRestaurantRequestHandler class 421–422\nFission framework 416\nFluentd 370\nFlume 370\nfold operation 187\nFTGO application\nAPI design issues for mobile client 255–258\nchanging monolith to interact with Delivery \nService 467–470\ncomponent tests for Order Service 340–345\ndeploying with Kubernetes 399–415\nAPI gateway 405–406\nRestaurant Service 402–405\nservice meshes 407–415\nzero-downtime deployments 406–407\nmicroservice architecture of 12–13\nmonolithic architecture of 3–4\nftgo-db-secret 404\nFtgoGraphQLClient class 290\nfunctional decomposition 10\nfuzzy boundaries 153–154\nG\nGDPR (General Data Protection Regulation) 201\ngeneralization pattern 22\nGET REST endpoint 271\ngetDelayedOrders() method 456\ngetOrderDetails() query 368\nGherkin\nexecuting specifications using Cucumber 338\nwriting acceptance tests 337–338\nGo Kit 380\ngod classes 58–61\n \n\n\nINDEX\n481\nGoLang (Go language) 4, 380\nGoogle Cloud functions 416\ngraph-based schema 280\nGraphQL 279, 281–291\nconnecting schema to data 285–287\ndefining schema 282–284\nexecuting queries 284–285\nintegrating Apollo GraphQL server with \nExpress 289–290\nload optimization using batching and caching 288\nwriting client 290–291\ngRPC 76–77\nH\nhandleHttpRequest() method 421\nhandleRequest() method 417\nhealth check 82, 365\nHealth check API pattern 27, 366–368\nimplementing endpoint 367–368\ninvoking endpoint 368\nhexagonal architecture 3, 38–40\nhigh-level design patterns 20\nHoneybadger 377\nHttpServletResponse 422\nHumble, Jez 30\nI\nidempotent message processing 96, 197\nCQRS views 240–241\nevent sourcing-based saga participant 213\nwith NoSQL-based event store 197\nwith RDBMS-based event store 197\nidempotentUpdate() method 250–251\nIDL (interface definition language) 69\n-ilities 8, 34, 37\nImplementation view 35\ninbound adapters 3, 38\ninfrastructure patterns 23–24\ninit system, Linux 390\nin-memory security context 353\ninstrumentation libraries 373\nintegration glue 444–449\ndesigning API for 444–445\nfor Delayed Delivery Service 457–459, 465–467\nCustomerContactInfoRepository \ninterface 458\ndesign of API 465–466\nhow Delivery Service accesses FTGO data 466\nhow FTGO accesses data 467\npublishing and consuming Order and \nRestaurant domain events 458–459\nhow monolith publishes and subscribes to \ndomain events 448–449\nimplementing anti-corruption layer 446–448\npicking interaction style and IPC \nmechanism 445–446\nintegration tests 319–335\nasynchronous request/response \ninteractions 330–335\nexample contract 331–332\ntests for asynchronous request/response \ninteraction 332–335\npersistence integration tests 321–322\npublish/subscribe-style interactions 326–330\ncontract for publishing OrderCreated \nevent 327–328\ntests for Order History Service 329–330\ntests for Order Service 328–329\nREST-based request/response style \ninteractions 322–326\nexample contract 324\ntests for API gateway OrderServiceProxy\n325–326\ntests for Order Service 324–325\ninteraction styles 67–68, 87–89\nasynchronous 104–105\none-way notifications 89\npublish/async responses 89\npublish/subscribe 89\nrequest/response and asynchronous request/\nresponse 87–88\nselecting 445–446\ninterface definition language (IDL) 69\ninvariants 153\nIPC (interprocess communication) 24, 65, \n93–109\noverview of 66–72\ndefining APIs 68–69\nevolving APIs 69–71\ninteraction styles 67–68\nmessage formats 71–72\nusing asynchronous Messaging pattern 85–103\ncompeting receivers and message \nordering 94–95\ncreating API specification 89–90\nduplicate messages 95–97\nimproving availability 103–108\ninteraction styles 87–89\nlibraries and frameworks for 100–103\nmessage brokers 90–94\noverview of 86–87\ntransactional messaging 97–100\nusing synchronous Remote procedure invoca-\ntion pattern 72–85\nCircuit breaker pattern 77–80\ngRPC 76–77\nREST 73–76\nservice discovery 80–85\n \n\n\nINDEX\n482\nIstio 381\ndeploying services 410–412\nEnvoy proxy 410\nservice meshes 408–410\nJ\njava -jar command 395\nJenkins 306\nJSESSIONID cookie 351\nJSON message 71\nJUL (java.util.logging) 369\nJWT (JSON Web Token) 28, 356–357\nK\nKafka 92\nkey condition expression 247\nKibana 370\nKitchen Service\nbusiness logic 168–173\nTicket aggregate 169–173\nKitchenServiceCommandHandler class 172–173\nKitchenServiceProxy class 139\nKong package 272\nkubectl apply command 404\nkubectl apply -f command 406\nKubernetes 399–415\ndeploying API gateway 405–406\ndeploying Restaurant Service 402–405\noverview of 399–402\narchitecture 400–402\nkey concepts 402\nservice meshes 407–415\ndeploying services 410–412\ndeploying v2 of Consumer Service 414\nIstio 408–412\nrouting production traffic to v2 415\nrouting rules to route to v1 version\n412–413\nrouting test traffic to v2 414\nzero-downtime deployments 406–407\nL\nLagom 202\nlambda functions 271, 416\nbenefits of 418\ndeploying using Serverless framework 425–426\ndeveloping 417\ndrawbacks of 419\ninvoking 417–418\ndefining scheduled lambda functions 418\nhandling events generated by AWS \nservices 418\nhandling HTTP requests 417\nusing web service request 418\nLanguage-specific packaging format pattern\n386–390\nbenefits of 388–389\nefficient resource utilization 389\nfast deployment 389\ndrawbacks of 389–390\nautomatically determining where to place ser-\nvice instances 390\nlack of encapsulation of technology stack 389\nlack of isolation 390\nno ability to constrain resources \nconsumed 389\nlatency 419\nlayered architectural style 37–38\nlayered file system 397\nlead time 31, 293\nlines of code (LOC) application 5\nLinkedIn Databus 100\nLinkerd 381\nlivenessProbe 404\nLoadBalancer service 405\nLOC (lines of code) application 5\nLog aggregation pattern 27, 365, 368–370\nlog aggregation infrastructure 370\nlog generation 369–370\nlog4j 369\nLogback 369\nLogical view 35\nLoginHandler 352, 454–455\nLogstash 370\nloose coupling 93, 121\nlost updates 127\nM\nMAJOR part, Semvers 70\nmakeContextWithDependencies() function 290\nmanual sidecar injection 411\nMartin, Robert C. 57\nmaster machine 400\nmean time to recover 31\nMemento pattern 196\nmessage brokers 85, 90–94\nbenefits and drawbacks of 93–94\nimplementing message channels using 93\noverview of 92\nmessage buffering 93\nmessage channels 86–87, 93\nmessage handler adapter class 86\nmessage handlers, unit tests for 315–317\nmessage identifier 88\nmessage ordering 94–95\nmessage sender adapter class 86\n \n\n\nINDEX\n483\nmessaging. See Asynchronous messaging pattern\nMessaging style patterns. See Asynchronous messag-\ning pattern\nmetrics collection 262\nMicro framework 380\nmicrometer-registry-prometheus library 375\nmicroservice architecture 8–14, 34, 43\nas form of modularity 11–12\nbenefits of 14–17\ncontinuous delivery and deployment of large, \ncomplex applications 15\nfault isolation improvement 16\nindependently scalable services 16\nnew technology experimentation and \nadoption 16–17\nsmall, easily maintained services 15\ndefining 44–64\ndecomposition guidelines 56–57\ndefining service APIs 61–64\nidentifying system operations 45–50\nobstacles to decomposing an application into \nservices 57–61\nservice definition with Decompose by business \ncapability pattern 51–54\nservice definition with Decompose by sub-\ndomain pattern 54–55\ndrawbacks of 17–19\nadoption timing 18–19\nchallenge of finding right services 17\ncomplex distributed systems 17–18\ndeployment coordination 18\neach service has own database 12\nFTGO application 12–13\nloose coupling, defined 42–43\nnot silver bullet 19–20\nrelationships between process, organization, \nand 29–32\nhuman side of adopting microservices\n31–32\nsoftware development and delivery \norganization 29–30\nsoftware development and delivery \nprocess 30–31\nrelative unimportance of size of service 43\nrole of shared libraries 43\nscale cube 8–11\nX-axis scaling 9\nY-axis scaling 10–11\nZ-axis scaling 9–10\nservice-oriented architecture versus 13–14\nservices, defined 41–42\nsoftware architecture 34–37\n4+1 view model of 35–37\ndefinition of 35\nrelevance of 37\ntransaction management 111–117\nmaintaining data consistency 114–117\nneed for distributed transactions 112\ntrouble with distributed transactions 112–114\nMicroservice chassis pattern 28, 378–382\nservice meshes 380–382\nusing 379–380\nMINOR part, Semvers 70\nMixer 409\nMobile API module 264\nMockito 305\nmocks 296\nmodularity, microservice architecture as form \nof 11–12\nMono abstraction 277\nmonolithic architecture 1–32, 40\nbenefits of 4\ncauses of monolithic hell 4–7\nintimidation due to complexity 4–5\nlong and arduous path from commit to \ndeployment 5–6\nreliability challenges 6\nscaling challenges 6\nslow development 5\ntechnology stack obsolescence 6–7\nFTGO monolithic architecture 3–4\nmultiply() method 310\nMyBATIS 185\nN\nNetflix Falcor 281\nNetflix Hystrix 79\nNetflix Zuul 273\nNetflix, as API gateway 267–268\nnetwork latency 57\nnetwork timeouts 79\nNodePort service 406\nnodes 280, 400\nnonblocking I/O 268\nnonfunctional requirements 8\nnon-key attributes 246\nNoSQL-based event store\ncreating saga orchestrator when using 211–212\nidempotent message processing when using 197\nSQL versus 237–238\nnotePickedUp() method 250\nO\nO/R (Object-Relational) impedance \nmismatch 185–186, 200\nOAuth 2.0 protocol 357–360\nobject-oriented design pattern 20\nobject-oriented programming (OOP) 149\n \n\n\nINDEX\n484\nObject-Relational (O/R) impedance \nmismatch 185–186, 200\nobservability 349\nobservability patterns 27–28\nApplication metrics 373–376\nAudit logging 377–378\nDistributed tracing 370–373\nException tracking 376–377\nHealth check API 366–368\nLog aggregation 366, 368–370\nobservable services 364–378\nApplication metrics pattern 373–376\ncollecting service-level metrics 374–375\ndelivering metrics to metrics service 375–376\nAudit logging pattern 377–378\nadding code to business logic 378\naspect-oriented programming 378\nevent sourcing 378\nDistributed tracing pattern 370–373\ndistributed tracing server 373\ninstrumentation libraries 373\nException tracking pattern 376–377\nHealth check API pattern 366–368\nimplementing endpoint 367–368\ninvoking endpoint 368\nLog aggregation pattern 368–370\nlog generation 369–370\nlogging aggregation infrastructure 370\nole-based authorization 353\none-size-fits-all (OSFA) 262\none-to-many interaction 67\none-to-one interaction 67\none-way notifications 68, 89\none-way notification-style API 90\nOOP (object-oriented programming) 149\nopaque tokens 356\nOpenwhisk 416\noptimistic locking 193–194\nOptimistic Offline Lock pattern 131\norchestration 111, 399\norchestration-based sagas 121–125\nbenefits and drawbacks of 125\ncreating 211–212\nimplementing Create Order saga 122–123\nimplementing using event sourcing 216–218\nmodeling saga orchestrators as state \nmachines 123–124\ntransactional messaging and 125\nOrder aggregate 175–180\nevent sourcing-based 191–193\nmethods 177–180\nstate machine 176–177\nstructure of 175–176\nOrder domain events, publishing and \nconsuming 458–459\nOrder History Service 329–330\nOrder Service\nbusiness logic 173–182\nOrder aggregate 175–180\nOrderService class 180–182\nconsumer-driven contract integration tests \nfor 324–325\nconsumer-driven contract tests for 328–329\nOrderCommandHandlers class 142–143\nOrderService class 133–134\nOrderServiceConfiguration class 143–145\nOrderCommandHandlers class 142–143\nOrderConfiguration class 275–276\nOrderCreated event 327–328\nOrderDetailsRequestHandler 352\nOrderHandlers class 276–278\nOrderHistoryDaoDynamoDb class 249–252\naddOrder() method 249–250\nfindOrderHistory() method 251–252\nidempotentUpdate() method 250–251\nnotePickedUp() method 250\nOrderHistoryEventHandlers module 243–244\nOrderService class 133–134, 180–182, 278–279\nOrderServiceComponentTestStepDefinitions \nclass 341–344\nOrderServiceConfiguration class 143–145\nOrderServiceProxy 325–326\nOSFA (one-size-fits-all) 262\noutbound adapters 3, 38, 147\noutstanding requests 79\nP\npagination parameter 229\npartition key 246\nPassport framework 351\nPATCH part, Semvers 70\npatterns and pattern language 20–23\nby name\n3rd party registration 85\nAccess token 354\nAggregate 150\nAnti-corruption layer 447\nAPI composition 223\nAPI gateway 259\nApplication metrics 373\nAudit logging 377\nBackends for frontends 265\nCircuit breaker 78\nClient-side discovery 83\nCommand query responsibility \nsegregation 228\nConsumer-driven contract test 302\nConsumer-side contract test 303\nDecompose by business capability 51\n \n",
      "page_number": 500
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 509-516)",
      "start_page": 509,
      "end_page": 516,
      "detection_method": "topic_boundary",
      "content": "INDEX\n485\npatterns and pattern language (continued)\nDecompose by subdomain 54\nDeploy a service as a container 393\nDeploy a service as a VM 390\nDistributed tracing 370\nDomain event 160\nDomain model 150\nEvent sourcing 184\nException tracking 376\nExternalized configuration 361\nHealth check API 366\nLanguage-specific packaging format 387\nLog aggregation 368\nMessaging 85\nMicroservice architecture 40\nMicroservice chassis 379\nMonolithic architecture 40\nPolling publisher 98\nRemote procedure invocation 72\nSaga 114\nSelf registration 82\nServerless deployment 416\nServer-side discovery 85\nService component test 335\nService mesh 380\nSidecar 410\nStrangler application 432\nTransaction log tailing 99\nTransaction script 149\nTransactional outbox 98\ngroups of patterns 23–29\ncommunication patterns 24–25\ndata consistency patterns 25\nfor automated testing of services 28\nfor decomposing applications into \nservices 24\nfor handling cross-cutting concerns 28\nfor querying data 25–26\nobservability patterns 27–28\nsecurity patterns 28–29\nservice deployment patterns 26\nsections of patterns\nforces 21\nrelated patterns 21–23\nresulting context 21\npending state 176\npersistence\npersisting aggregates using events 186–188\ntraditional approach to 185–186\naudit logging 186\nevent publishing bolted to business \nlogic 186\nlack of aggregate history 186\nobject-relational impedance mismatch\n185–186\npersistence integration tests 321–322\nPersistence layer 38\npessimistic view countermeasure 130–131\npickup action 460\nPilot 409\npivot transaction 128, 450\npods 402\npoint-to-point channel 87\npolicy enforcement 409\npolling 194–195\nPolling publisher pattern 98–99\nports 38\npre-commit tests stage 306\npredecessor pattern 21\nPresentation layer 38\npresentation logic 436\nprimary key-based queries 235\nProcess view 36\nprocess() method 190, 193\nproduction-ready service development\n348–382\nconfigurable services 360–364\npull-based externalized configuration\n363–364\npush-based externalized configuration\n362–363\nMicroservice chassis pattern 378–382\nservice meshes 380–382\nusing 379–380\nobservable services 364–378\nApplication metrics pattern 373–376\nAudit logging pattern 377–378\nDistributed tracing pattern 370–373\nException tracking pattern 376–377\nHealth check API pattern 366–368\nLog aggregation pattern 368–370\nsecure services 349–360\nhandling authentication in API gateway\n354–355\nhandling authorization 356\nin traditional monolithic application\n350–353\nusing JWTs to pass user identity and \nroles 356–357\nusing OAuth 2.0 357–360\nPrometheus 375\nproperties, graph-based schema 280\nProtocol Buffers 72\nprovider service 223\nproxy classes 274\nproxy interface 72\npseudonymization 201\nPublic API module 264\npublish() method 166\npublish/async responses 89\n \n\n\nINDEX\n486\npublish/subscribe-style interaction\nimplementing 89\nintegration tests for 326–330\ncontract for publishing OrderCreated \nevent 327–328\ntests for Order History Service 329–330\ntests for Order Service 328–329\npublish-subscribe channel 87\npull model of externalized configuration 361, 375\npush model of externalized configuration 361, \n375\nQ\nquality attributes 8, 34, 37\nquality of service 8, 37\nqueries 41\nquery arguments 286\nquery() operation 246, 249\nquerying patterns 220–252\nAPI composition pattern 26, 64, 79, 221–228\nbenefits and drawbacks of 227–228\ndesign issues 225–227\nfindOrder() query operation 221–222, 224\noverview of 222–224\nCQRS pattern 26, 63, 160, 184, 221, 228–236\nbenefits of 235–236\ndrawbacks of 236\nmotivations for using 229–232\noverview of 232–235\nR\nRabbitMQ 92\nrate limiting 262\nRDBMS-based event store\ncreating saga orchestrator when using 211\nidempotent message processing with 197\nreactive programming model 227\nreadinessProbe 404, 407\nreceiving port interface 86\nreduce operation 187\nrefactoring 428–471\napplication modernization 430–432\ndemonstrating value 432\ndesigning how service and monolith \ncollaborate 443–455\nauthentication and authorization 453–455\ndata consistency 449–453\nintegration glue 444–449\nextracting delivery management 459–470\nchanging FTGO monolith to interact with \nDelivery Service 467–470\ndesigning Delivery Service domain \nmodel 463–464\ndesigning Delivery Service integration \nglue 465–467\nexisting delivery functionality 460–461\noverview of Delivery Service 462–463\nimplementing new features as services 455–459\ndesign for Delayed Delivery Service 456–457\nintegration glue for Delayed Delivery \nService 457–459\nminimizing changes 432–433\noverview of 429–433\nreasons for 429–430\nstrategies for 433–442\nextracting business capabilities into \nservices 437–442\nimplementing new features as services\n434–435\nseparating presentation tier from \nbackend 436–437\ntechnical deployment infrastructure 433\nRefactoring to microservices patterns\nAnti-corruption layer 446–447\nStrangler application 431–432\nReflectiveMutableCommandProcessingAggregate \nclass 206–207\nRefresh Token concept 358\nReleasing services 408\nReliable communications pattern\nCircuit breaker 77–80, 108\nRemote procedure invocation (RPI) pattern\n72–85\nCircuit breaker pattern 77–80\ndeveloping robust RPI proxies 79\nrecovering from unavailable services 79–80\ngRPC 76–77\nREST 73–76\nbenefits and drawbacks of 75–76\nfetching multiple resources in single \nrequest 74–75\nmapping operations to HTTP verbs 75\nREST maturity model 74\nspecifying REST APIs 74\nservice discovery 80–85\noverview of 81\nusing application-level service discovery \npatterns 81–83\nusing platform-provided service discovery \npatterns 83–85\nreply channel header 88–89\nRepository object, DDD 152\nrequest attribute 10\nrequest logging 262\nrequest/async response-style API 90\nrequest/response interactions 87–89\nasynchronous 87–88\nintegration tests for REST-based 322–326\n \n\n\nINDEX\n487\nRequestHandler interface 417\nreread value countermeasure 131\nResource Server concept 358\nREST 73–76\nbenefits and drawbacks of 75–76\nfetching multiple resources in single \nrequest 74–75\nmapping operations to HTTP verbs 75\nREST maturity model 74\nspecifying REST APIs 74\nRest Assured Mock MVC 314\nRestaurant domain events 458–459\nRestaurant Service\ncreating services 404–405\ndeploying 402–405\ndesign of 419–423\nAbstractAutowiringHttpRequestHandler \nclass 423\nAbstractHttpHandler class 423\nFindRestaurantRequestHandler class\n421–422\nREST-based request/response style interactions, \nintegration tests for 322–326\nexample contract 324\ntests for API gateway OrderServiceProxy\n325–326\ntests for Order Service 324–325\nRESTful services 419–426\ndeploying lambda functions using Serverless \nframework 425–426\ndesign of Restaurant Service 419–423\npackaging service as ZIP file 424\nretriable transactions 117, 129, 450\nrevise() method 179\nS\nSaas (Software-as-a-Service) 5\nsaga orchestration package 140\nSaga pattern 26\nSagaOrchestratorCreated event 216\nSagaOrchestratorUpdated event 216\nSagaReplyRequested pseudo event 213\nsagas 17, 58, 106, 110–145, 209–218, 450\ncoordinating 117–125\nchoreography-based sagas 118–121\norchestration-based sagas 121–125\nCreate Order saga 135–142\nCreateOrderSaga orchestrator 136–138\nCreateOrderSagaState class 138\nEventuate Tram Saga framework 140–142\nKitchenServiceProxy class 139\ncreating orchestration-based saga 211–212\nwith a NoSQL-based event store 211–212\nwith RDBMS-based event store 211\nimplementing choreography-based sagas using \nevent sourcing 210\nimplementing event sourcing-based saga \nparticipant 213–216\nimplementing saga orchestrators using event \nsourcing 216–218\npersisting using event sourcing 216\nprocessing replies exactly once 218\nsending command messages reliably\n216–218\nlack of isolation 126–132\nanomalies caused by 127\ncountermeasures for handling 128–132\nOrder Service\nOrderCommandHandlers class 142–143\nOrderService class 133–134\nOrderServiceConfiguration class 143–145\ntransaction management 111–117\nmaintaining data consistency 114–117\nneed for distributed transactions 112\ntrouble with distributed transactions 112–114\nunit tests for 310–312\nSATURN conference 34\nsave() method 207\nscalability 430\nscale cube 8–11\nX-axis scaling 9\nY-axis scaling 10–11\nZ-axis scaling 9–10\nsecure services 349–360\nauthentication in API gateway 354–355\nauthorization 356\nin traditional monolithic application 350–353\nusing JWTs to pass user identity and roles\n356–357\nusing OAuth 2.0 357–360\nsecurity patterns 28–29\nAccess token 28, 38, 354\nSELECT statements 188\nSelf registration pattern 82\nsemantic lock 450\nsemantic lock countermeasure 129–130\nsending port interface 86\nServerless deployment with lambda 415–419\nbenefits of lambda functions 418\ndeveloping lambda functions 417\ndrawbacks of lambda functions 419\ninvoking lambda functions 417–418\ndefining scheduled lambda functions\n418\nhandling events generated by AWS \nservices 418\nhandling HTTP requests 417\nusing web service request 418\noverview of 416\n \n\n\nINDEX\n488\nServerless framework 425–426\nserver-side discovery pattern 84–85\nservice API definition 61–64\nassigning system operations to services\n61–62\ndetermining APIs required to support \ncollaboration between services\n62–64\nService as a container pattern 393–399\nbenefits of 398\nDocker 395–398\nbuilding Docker images 395–396\npushing Docker images to registry\n396–397\nrunning Docker containers 397–398\ndrawbacks of 399\nService as a virtual machine pattern\n390–393\nbenefits of 392\nmature cloud infrastructure 392\nservice instances are isolated 392\nVM image encapsulates technology \nstack 392\ndrawbacks of 392–393\nless-efficient resource utilization 393\nrelatively slow deployments 393\nsystem administration overhead 393\nservice component test 28, 335\nservice configurability 349\nservice definition 76\nDecompose by business capability pattern\n51–54\ndecomposition 52–54\nidentifying business capabilities 51–52\npurpose of business capabilities 51\nDecompose by sub-domain pattern\n54–55\nservice deployment patterns 26\nservice discovery 80–85\n3rd party registration 84–85, 108\nClient-side discovery 82–83\noverview of 81\nSelf registration 82\nServer-side discovery 84–85\nservice meshes 380–382, 407–415\ndeploying v2 of Consumer Service 414\nIstio 408–412\nrouting production traffic to v2 415\nrouting rules to route to v1 version\n412–413\nrouting test traffic to v2 414\nService object, DDD 152\nservice() method 422\nservice-oriented architecture (SOA)\n13–14\nSES (Simple Email Service) 2\nSessionBasedSecurityInterceptor 352\nsessions 351\nsetUp() method 313\nsharded channel 94\nShiro 351\nSidecar pattern 410\nSimple Email Service (SES) 2\nSingle persistence layer 38\nSingle presentation layer 38\nSingle Responsibility Principle (SRP) 56\nsmart pipes 14\nsnapshots 195–196, 201\nSOA (service-oriented architecture) 13\nsociable unit test 308\nsoftware architecture 34–37\n4+1 view model of 35–37\ndefinition of 35\nrelevance of 37\nsoftware pattern 20\nSoftware-as-a-Service (SaaS) 5\nsolitary unit test 308\nSoundCloud 265\nspecialization pattern 22\nSpring Cloud Contract 303–305\nSpring Cloud Gateway 273–275\nApiGatewayApplication class 279\nOrderConfiguration class 275–276\nOrderHandlers class 276–278\nOrderService class 278–279\nSpring Mock MVC 314\nSpring Security 351\nSPRING_APPLICATION_JSON variable\n363\nSQL 237–238\nSRP (Single Responsibility Principle) 56\nstate machines\nmodeling saga orchestrators as\n123–124\nOrder aggregate 176–177\nStrangler Application pattern 431–432\nStrategy pattern 20\nstubs 296, 339–340\nsuccessor pattern 21\nSUT (system under test) 294\nsynchronous I/O model 268\nsynchronous interactions 67\nsystem operations 45\nassigning to services 61–62\ncreating high-level domain model 46–48\ndefining 48–50\nidentifying 45–50\nsystem under test (SUT) 294\nSystem.getenv() method 362\n \n\n\nINDEX\n489\nT\ntelemetry 409\ntest cases 294\ntest double 296\ntest pyramid 298–299\ntest quadrant 297–298\n@Test shouldCalculateTotal() method 309\n@Test shouldCreateOrder() method 312\ntest suites 294\ntesting 292–347\nacceptance tests 335–338\ndefining 336\nwriting using Gherkin 337–338\nchallenge of 299–305\nconsumer contract testing 301–303\nconsumer contract testing for messaging \nAPIs 305\nSpring Cloud Contract 303–305\ncomponent tests 339–340\nfor FTGO Order Service 340–345\nin-process component tests 339\nout-of-process component tests\n339–340\nConsumer-driven contract test 28, \n301–302\nConsumer-side contract test 28, 303\ndeployment pipeline 305–307\nend-to-end tests 345–346\ndesigning 345\nrunning 346\nwriting 346\nintegration tests 319–335\ncontract tests for asynchronous request/\nresponse interactions 330–335\npersistence integration tests 321–322\npublish/subscribe-style interactions\n326–330\nREST-based request/response style \ninteractions 322–326\noverview of 294–299\nautomated tests 295–296\ndifferent types of tests 297\nmocks and stubs 296\ntest pyramid 298–299\ntest quadrant 297–298\nService component test 28, 335\nunit tests 307–317\nfor controllers 313–315\nfor domain services 312–313\nfor entities 309–310\nfor event and message handlers 315–317\nfor sagas 310–312\nfor value objects 310\ntestuser header 414\ntext-based message formats 71–72\nTicket aggregate 169–173\nbehavior of 170–171\nKitchenService domain service 171–172\nKitchenServiceCommandHandler class\n172–173\nstructure of Ticket class 170\ntight coupling 121\ntimeouts 79\nTLS (Transport Layer Security) 350\ntokens 356\nTraefik 272\ntraffic management 408\ntransaction log tailing 99–100, 195\ntransaction management 111–117\nmaintaining data consistency 114–117\nneed for distributed transactions 112\ntrouble with distributed transactions\n112–114\nSee also sagas\nTransaction script pattern 149–150\n@Transactional annotation 111\ntransactional messaging 97–100\nPolling publisher pattern 98–99\nTransaction log tailing pattern 99–100\nTransactional outbox pattern 97–98, 109\nusing database table as message queue\n97–98\ntransparent tokens 356\nTransport Layer Security (TLS) 350\ntwo-phase commit (2PC) 112\nU\nUbiquitous Language 54\nunit tests 307–317\nfor controllers 313–315\nfor domain services 312–313\nfor entities 309–310\nfor event and message handlers 315–317\nfor sagas 310–312\nfor value objects 310\nupcasting 199\nUPDATE statement 193\nupdate() method 204, 207, 215\nUpdateItem() operation 248\nUSERINFO cookie\nLoginHandler and 454–455\nmapping to Authorization header 455\nV\nValue object, DDD 151\nvalue objects, unit tests for 310\nversion file countermeasure 131\n \n\n\nINDEX\n490\nVIP (virtual IP) address 83\nVirtualService 413\nVMs (virtual machines) 26\nW\nWAR (Web Application Archive) file 2\nWebSockets 257\nX\nXML message 71\nZ\nZeroMQ 91\nZipkin 373\n \n\n\nEnables\nEnables\nArchitecture:\nMicroservice\narchitecture\nOrganization:\nSmall, autonomous,\ncross-functional teams\nProcess:\nDevOps/continuous delivery/deployment\nEnables\nRapid, frequent,\nand reliable delivery\nof software\nThe rapid, frequent, and reliable delivery of large, complex applications requires \na combination of DevOps, which includes continuous delivery/deployment, small, \nautonomous teams, and the microservice architecture.\nSmall, autonomous,\nloosely coupled teams\nEach service has\nits own source\ncode repository.\nEach service has\nits own automated\ndeployment pipeline.\nSmall, simple,\nreliable, easy to\nmaintain services\nOrder management team\nRestaurant management team\nDelivery management team\nFTGO development\nProduction\nJenkins Cl\nDeployment pipeline\nOrder Service\nsource code\nrepository\nOrder Service\nJenkins Cl\nDeployment pipeline\nRestaurant Service\nsource code\nrepository\nRestaurant Service\nJenkins Cl\nDeployment pipeline\nDelivery Service\nsource code\nrepository\nDelivery Service\nThe microservice architecture structures an application as a set of loosely coupled services that are \norganized around business capabilities. Each team develops, tests, and deploys their services \nindependently.\n \n\n\nChris Richardson\nS\nuccessfully developing microservices-based applications \nrequires mastering a new set of architectural insights and \npractices. In this unique book, microservice architecture \npioneer and Java Champion Chris Richardson collects, cata-\nlogues, and explains 44 patterns that solve problems such as \nservice decomposition, transaction management, querying, \nand inter-service communication.\nMicroservices Patterns teaches you how to develop and deploy \nproduction-quality microservices-based applications. This \ninvaluable set of design patterns builds on decades of dis-\ntributed system experience, adding new patterns for writing \nservices and composing them into systems that scale and \nperform reliably under real-world conditions. More than just \na patterns catalog, this practical guide offers experience-driven \nadvice to help you design, implement, test, and deploy your \nmicroservices-based application. \nWhat’s Inside\n● How (and why!) to use the microservice architecture\n● Service decomposition strategies\n● Transaction management and querying patterns\n● Effective testing strategies\n● Deployment patterns including containers and serverless\nWritten for enterprise developers familiar with standard enter-\nprise application architecture. Examples are in Java.\nChris Richardson is a Java Champion, a JavaOne rock star, \nauthor of Manning’s POJOs in Action, and the creator of the \noriginal CloudFoundry.com.\nTo download their free eBook in PDF, ePub, and Kindle formats, owners \nof this book should visit manning.com/books/microservices-patterns\n$49.99 / Can $65.99  [INCLUDING eBOOK]\nMicroservices Patterns\nSOFTWARE DEVELOPMENT\nM A N N I N G\n“\nA comprehensive overview \nof the challenges teams face \nwhen moving to microservices, \nwith industry-tested solutions \nto these problems.”\n \n—Tim Moore, Lightbend\n“\nPragmatic treatment of \nan important new \n architectural landscape.”\n \n—Simeon Leyzerzon\nExcelsior Software\n“\nA solid compendium of \ninformation that will quicken \nyour migration to this modern \ncloud-based architecture.”\n—John Guthrie, Dell/EMC \n“\nHow to understand the \nmicroservices approach, and \nhow to use it in real life.”\n \n—Potito Coluccelli\nBizmatica Econocom\nSee first page\n",
      "page_number": 509
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 517-522)",
      "start_page": 517,
      "end_page": 522,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 517
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "M A N N I N G\nChris Richardson\n",
      "content_length": 31,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 2,
      "content": "List of Patterns\nApplication architecture patterns\nMonolithic architecture (40)\nMicroservice architecture (40)\nDecomposition patterns\nDecompose by business capability (51)\nDecompose by subdomain (54)\nMessaging style patterns\nMessaging (85)\nRemote procedure invocation (72)\nReliable communications patterns\nCircuit breaker (78)\nService discovery patterns\n3rd party registration (85)\nClient-side discovery (83)\nSelf-registration (82)\nServer-side discovery (85)\nTransactional messaging patterns\nPolling publisher (98)\nTransaction log tailing (99)\nTransactional outbox (98)\nData consistency patterns\nSaga (114)\nBusiness logic design patterns\nAggregate (150)\nDomain event (160)\nDomain model (150)\nEvent sourcing (184)\nTransaction script (149)\nQuerying patterns\nAPI composition (223)\nCommand query responsibility segregation \n(228)\nExternal API patterns\nAPI gateway (259)\nBackends for frontends (265)\nTesting patterns\nConsumer-driven contract test (302)\nConsumer-side contract test (303)\nService component test (335)\nSecurity patterns\nAccess token (354) \nCross-cutting concerns patterns\nExternalized configuration (361)\nMicroservice chassis (379)\nObservability patterns\nApplication metrics (373)\nAudit logging (377)\nDistributed tracing (370)\nException tracking (376)\nHealth check API (366)\nLog aggregation (368)\nDeployment patterns\nDeploy a service as a container (393)\nDeploy a service as a VM (390)\nLanguage-specific packaging format (387)\nService mesh (380)\nServerless deployment (416)\nSidecar (410)\nRefactoring to microservices patterns\nAnti-corruption layer (447)\nStrangler application (432)\n \n",
      "content_length": 1593,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "Microservices Patterns\n \n",
      "content_length": 25,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "Microservices Patterns\nWITH EXAMPLES IN JAVA\nCHRIS RICHARDSON\nM A N N I N G\nSHELTER ISLAND\n \n",
      "content_length": 93,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": "For online information and ordering of this and other Manning books, please visit\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2019 by Chris Richardson. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning \nPublications was aware of a trademark claim, the designations have been printed in initial caps \nor all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nManning Publications Co.\nDevelopment editor: Marina Michaels\n20 Baldwin Road\nTechnical development editor: Christian Mennerich\nPO Box 761\nReview editor: Aleksandar Dragosavljevic´\nShelter Island, NY 11964\nProject editor: Lori Weidert\nCopy editor: Corbin Collins\nProofreader: Alyson Brener\nTechnical proofreader: Andy Miles\nTypesetter: Dennis Dalinnik\nCover designer: Marija Tudor\nISBN: 9781617294549\nPrinted in the United States of America\n1 2 3 4 5 6 7 8 9 10 – DP – 23 22 21 20 19 18\n \n",
      "content_length": 1770,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": " Where you see wrong or inequality or injustice, speak out, because this is your country. \nThis is your democracy. Make it. Protect it. Pass it on.\n — Thurgood Marshall, Justice of the Supreme Court\n \n",
      "content_length": 201,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "vii\nbrief contents\n1\n■\nEscaping monolithic hell\n1\n2\n■\nDecomposition strategies\n33\n3\n■\nInterprocess communication in a microservice \narchitecture\n65\n4\n■\nManaging transactions with sagas\n110\n5\n■\nDesigning business logic in a microservice \narchitecture\n146\n6\n■\nDeveloping business logic with event sourcing\n183\n7\n■\nImplementing queries in a microservice architecture\n220\n8\n■\nExternal API patterns\n253\n9\n■\nTesting microservices: Part 1\n292\n10\n■\nTesting microservices: Part 2\n318\n11\n■\nDeveloping production-ready services\n348\n12\n■\nDeploying microservices\n383\n13\n■\nRefactoring to microservices\n428\n \n",
      "content_length": 594,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "ix\ncontents\npreface\nxvii\nacknowledgments\nxx\nabout this book\nxxii\nabout the cover illustration\nxxvi\n1 \nEscaping monolithic hell\n1\n1.1\nThe slow march toward monolithic hell\n2\nThe architecture of the FTGO application\n3\n■The benefits of the \nmonolithic architecture\n4\n■Living in monolithic hell\n4\n1.2\nWhy this book is relevant to you\n7\n1.3\nWhat you’ll learn in this book\n7\n1.4\nMicroservice architecture to the rescue\n8\nScale cube and microservices\n8\n■Microservices as a form of \nmodularity\n11\n■Each service has its own database\n12\nThe FTGO microservice architecture\n12\n■Comparing the \nmicroservice architecture and SOA\n13\n1.5\nBenefits and drawbacks of the microservice \narchitecture\n14\nBenefits of the microservice architecture\n14\n■Drawbacks of the \nmicroservice architecture\n17\n \n",
      "content_length": 777,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "CONTENTS\nx\n1.6\nThe Microservice architecture pattern language\n19\nMicroservice architecture is not a silver bullet\n19\n■Patterns and \npattern languages\n20\n■Overview of the Microservice architecture \npattern language\n23\n1.7\nBeyond microservices: Process and organization\n29\nSoftware development and delivery organization\n29\n■Software \ndevelopment and delivery process\n30\n■The human side of \nadopting microservices\n31\n2 \nDecomposition strategies\n33\n2.1\nWhat is the microservice architecture exactly?\n34\nWhat is software architecture and why does it matter?\n34\nOverview of architectural styles\n37\n■The microservice architecture \nis an architectural style\n40\n2.2\nDefining an application’s microservice architecture\n44\nIdentifying the system operations\n45\n■Defining services by \napplying the Decompose by business capability pattern\n51\nDefining services by applying the Decompose by sub-domain \npattern\n54\n■Decomposition guidelines\n56\n■Obstacles to \ndecomposing an application into services\n57\n■Defining service \nAPIs\n61\n3 \nInterprocess communication in a microservice architecture\n65\n3.1\nOverview of interprocess communication in a microservice \narchitecture\n66\nInteraction styles\n67\n■Defining APIs in a microservice \narchitecture\n68\n■Evolving APIs\n69\n■Message formats\n71\n3.2\nCommunicating using the synchronous Remote \nprocedure invocation pattern\n72\nUsing REST\n73\n■Using gRPC\n76\n■Handling partial failure \nusing the Circuit breaker pattern\n77\n■Using service discovery\n80\n3.3\nCommunicating using the Asynchronous messaging \npattern\n85\nOverview of messaging\n86\n■Implementing the interaction styles \nusing messaging\n87\n■Creating an API specification for a \nmessaging-based service API\n89\n■Using a message broker\n90\nCompeting receivers and message ordering\n94\n■Handling \nduplicate messages\n95\n■Transactional messaging\n97\nLibraries and frameworks for messaging\n100\n \n",
      "content_length": 1858,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "CONTENTS\nxi\n3.4\nUsing asynchronous messaging to improve \navailability\n103\nSynchronous communication reduces availability\n103\nEliminating synchronous interaction\n104\n4 \nManaging transactions with sagas\n110\n4.1\nTransaction management in a microservice \narchitecture\n111\nThe need for distributed transactions in a microservice \narchitecture\n112\n■The trouble with distributed \ntransactions\n112\n■Using the Saga pattern to maintain \ndata consistency\n114\n4.2\nCoordinating sagas\n117\nChoreography-based sagas\n118\n■Orchestration-based sagas\n121\n4.3\nHandling the lack of isolation\n126\nOverview of anomalies\n127\n■Countermeasures for handling the \nlack of isolation\n128\n4.4\nThe design of the Order Service and \nthe Create Order Saga\n132\nThe OrderService class\n133\n■The implementation of the Create \nOrder Saga\n135\n■The OrderCommandHandlers class\n142\nThe OrderServiceConfiguration class\n143\n5 \nDesigning business logic in a microservice architecture\n146\n5.1\nBusiness logic organization patterns\n147\nDesigning business logic using the Transaction script pattern\n149\nDesigning business logic using the Domain model pattern\n150\nAbout Domain-driven design\n151\n5.2\nDesigning a domain model using the \nDDD aggregate pattern\n152\nThe problem with fuzzy boundaries\n153\n■Aggregates have \nexplicit boundaries\n154\n■Aggregate rules\n155\n■Aggregate \ngranularity\n158\n■Designing business logic with aggregates\n159\n5.3\nPublishing domain events\n160\nWhy publish change events?\n160\n■What is a domain \nevent?\n161\n■Event enrichment\n161\n■Identifying domain \nevents\n162\n■Generating and publishing domain events\n164\nConsuming domain events\n167\n \n",
      "content_length": 1606,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "CONTENTS\nxii\n5.4\nKitchen Service business logic\n168\nThe Ticket aggregate\n169\n5.5\nOrder Service business logic\n173\nThe Order Aggregate\n175\n■The OrderService class\n180\n6 \nDeveloping business logic with event sourcing\n183\n6.1\nDeveloping business logic using event sourcing\n184\nThe trouble with traditional persistence\n185\n■Overview of event \nsourcing\n186\n■Handling concurrent updates using optimistic \nlocking\n193\n■Event sourcing and publishing events\n194\nUsing snapshots to improve performance\n195\n■Idempotent \nmessage processing\n197\n■Evolving domain events\n198\nBenefits of event sourcing\n199\n■Drawbacks of event \nsourcing\n200\n6.2\nImplementing an event store\n202\nHow the Eventuate Local event store works\n203\n■The Eventuate \nclient framework for Java\n205\n6.3\nUsing sagas and event sourcing together\n209\nImplementing choreography-based sagas using event sourcing\n210\nCreating an orchestration-based saga\n211\n■Implementing an \nevent sourcing-based saga participant\n213\n■Implementing saga \norchestrators using event sourcing\n216\n7 \nImplementing queries in a microservice architecture\n220\n7.1\nQuerying using the API composition pattern\n221\nThe findOrder() query operation\n221\n■Overview of the API \ncomposition pattern\n222\n■Implementing the findOrder() query \noperation using the API composition pattern\n224\n■API \ncomposition design issues\n225\n■The benefits and drawbacks \nof the API composition pattern\n227\n7.2\nUsing the CQRS pattern\n228\nMotivations for using CQRS\n229\n■Overview of CQRS\n232\nThe benefits of CQRS\n235\n■The drawbacks of CQRS\n236\n7.3\nDesigning CQRS views\n236\nChoosing a view datastore\n237\n■Data access module design\n239\nAdding and updating CQRS views\n241\n7.4\nImplementing a CQRS view with AWS DynamoDB\n242\nThe OrderHistoryEventHandlers module\n243\nData modeling and query design with DynamoDB\n244\nThe OrderHistoryDaoDynamoDb class\n249\n \n",
      "content_length": 1843,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "CONTENTS\nxiii\n8 \nExternal API patterns\n253\n8.1\nExternal API design issues\n254\nAPI design issues for the FTGO mobile client\n255\n■API design \nissues for other kinds of clients\n258\n8.2\nThe API gateway pattern\n259\nOverview of the API gateway pattern\n259\n■Benefits and \ndrawbacks of an API gateway\n267\n■Netflix as an example \nof an API gateway\n267\n■API gateway design issues\n268\n8.3\nImplementing an API gateway\n271\nUsing an off-the-shelf API gateway product/service\n271\nDeveloping your own API gateway\n273\n■Implementing an \nAPI gateway using GraphQL\n279\n9 \nTesting microservices: Part 1\n292\n9.1\nTesting strategies for microservice architectures\n294\nOverview of testing\n294\n■The challenge of testing \nmicroservices\n299\n■The deployment pipeline\n305\n9.2\nWriting unit tests for a service\n307\nDeveloping unit tests for entities\n309\n■Writing unit tests for value \nobjects\n310\n■Developing unit tests for sagas\n310\n■Writing \nunit tests for domain services\n312\n■Developing unit tests for \ncontrollers\n313\n■Writing unit tests for event and message \nhandlers\n315\n10 \nTesting microservices: Part 2\n318\n10.1\nWriting integration tests\n319\nPersistence integration tests\n321\n■Integration testing REST-based \nrequest/response style interactions\n322\n■Integration testing \npublish/subscribe-style interactions\n326\n■Integration contract \ntests for asynchronous request/response interactions\n330\n10.2\nDeveloping component tests\n335\nDefining acceptance tests\n336\n■Writing acceptance tests using \nGherkin\n337\n■Designing component tests\n339\n■Writing \ncomponent tests for the FTGO Order Service\n340\n10.3\nWriting end-to-end tests\n345\nDesigning end-to-end tests\n345\n■Writing end-to-end tests\n346\nRunning end-to-end tests\n346\n \n",
      "content_length": 1695,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "CONTENTS\nxiv\n11 \nDeveloping production-ready services\n348\n11.1\nDeveloping secure services\n349\nOverview of security in a traditional monolithic application\n350\nImplementing security in a microservice architecture\n353\n11.2\nDesigning configurable services\n360\nUsing push-based externalized configuration\n362\n■Using pull-\nbased externalized configuration\n363\n11.3\nDesigning observable services\n364\nUsing the Health check API pattern\n366\n■Applying the Log \naggregation pattern\n368\n■Using the Distributed tracing \npattern\n370\n■Applying the Application metrics pattern\n373\nUsing the Exception tracking pattern\n376\n■Applying the Audit \nlogging pattern\n377\n11.4\nDeveloping services using the Microservice chassis \npattern\n378\nUsing a microservice chassis\n379\n■From microservice chassis to \nservice mesh\n380\n12 \nDeploying microservices\n383\n12.1\nDeploying services using the Language-specific packaging \nformat pattern\n386\nBenefits of the Service as a language-specific package pattern\n388\nDrawbacks of the Service as a language-specific package \npattern\n389\n12.2\nDeploying services using the Service as a virtual machine \npattern\n390\nThe benefits of deploying services as VMs\n392\n■The drawbacks of \ndeploying services as VMs\n392\n12.3\nDeploying services using the Service as a container \npattern\n393\nDeploying services using Docker\n395\n■Benefits of deploying \nservices as containers\n398\n■Drawbacks of deploying services \nas containers\n399\n12.4\nDeploying the FTGO application with Kubernetes\n399\nOverview of Kubernetes\n399\n■Deploying the Restaurant service \non Kubernetes\n402\n■Deploying the API gateway\n405\nZero-downtime deployments\n406\n■Using a service mesh \nto separate deployment from release\n407\n \n",
      "content_length": 1690,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "CONTENTS\nxv\n12.5\nDeploying services using the Serverless deployment \npattern\n415\nOverview of serverless deployment with AWS Lambda\n416\nDeveloping a lambda function\n417\n■Invoking lambda \nfunctions\n417\n■Benefits of using lambda functions\n418\nDrawbacks of using lambda functions\n419\n12.6\nDeploying a RESTful service using AWS Lambda \nand AWS Gateway\n419\nThe design of the AWS Lambda version of Restaurant Service\n419\nPackaging the service as ZIP file\n424\n■Deploying lambda \nfunctions using the Serverless framework\n425\n13 \nRefactoring to microservices\n428\n13.1\nOverview of refactoring to microservices\n429\nWhy refactor a monolith?\n429\n■Strangling the monolith\n430\n13.2\nStrategies for refactoring a monolith to \nmicroservices\n433\nImplement new features as services\n434\n■Separate presentation \ntier from the backend\n436\n■Extract business capabilities into \nservices\n437\n13.3\nDesigning how the service and the monolith \ncollaborate\n443\nDesigning the integration glue\n444\n■Maintaining data \nconsistency across a service and a monolith\n449\n■Handling \nauthentication and authorization\n453\n13.4\nImplementing a new feature as a service: handling \nmisdelivered orders\n455\nThe design of Delayed Delivery Service\n456\n■Designing the \nintegration glue for Delayed Delivery Service\n457\n13.5\nBreaking apart the monolith: extracting delivery \nmanagement\n459\nOverview of existing delivery management functionality\n460\nOverview of Delivery Service\n462\n■Designing the Delivery Service \ndomain model\n463\n■The design of the Delivery Service integration \nglue\n465\n■Changing the FTGO monolith to interact with Delivery \nService\n467\nindex\n473\n \n",
      "content_length": 1618,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "xvii\npreface\nOne of my favorite quotes is\nThe future is already here—it’s just not very evenly distributed.\n—William Gibson, science fiction author\nThe essence of that quote is that new ideas and technology take a while to diffuse\nthrough a community and become widely adopted. A good example of the slow diffu-\nsion of ideas is the story of how I discovered microservices. It began in 2006, when,\nafter being inspired by a talk given by an AWS evangelist, I started down a path that\nultimately led to my creating the original Cloud Foundry. (The only thing in common\nwith today’s Cloud Foundry is the name.) Cloud Foundry was a Platform-as-a-Service\n(PaaS) for automating the deployment of Java applications on EC2. Like every other\nenterprise Java application that I’d built, my Cloud Foundry had a monolith architec-\nture consisting of a single Java Web Application Archive (WAR) file.\n Bundling a diverse and complex set of functions such as provisioning, configura-\ntion, monitoring, and management into a monolith created both development and\noperations challenges. You couldn’t, for example, change the UI without testing and\nredeploying the entire application. And because the monitoring and management\ncomponent relied on a Complex Event Processing (CEP) engine which maintained\nin-memory state we couldn’t run multiple instances of the application! That’s embar-\nrassing to admit, but all I can say is that I am a software developer, and, “let he who is\nwithout sin cast the first stone.”\n \n",
      "content_length": 1501,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "PREFACE\nxviii\n Clearly, the application had quickly outgrown its monolith architecture, but what was\nthe alternative? The answer had been out in the software community for some time at\ncompanies such as eBay and Amazon. Amazon had, for example, started to migrate away\nfrom the monolith around 2002 (https://plus.google.com/110981030061712822816/\nposts/AaygmbzVeRq). The new architecture replaced the monolith with a collection\nof loosely coupled services. Services are owned by what Amazon calls two-pizza teams—\nteams small enough to be fed by two pizzas.\n Amazon had adopted this architecture to accelerate the rate of software develop-\nment so that the company could innovate faster and compete more effectively. The\nresults are impressive: Amazon reportedly deploys changes into production every 11.6\nseconds!\n In early 2010, after I’d moved on to other projects, the future of software architec-\nture finally caught up with me. That’s when I read the book The Art of Scalability:\nScalable Web Architecture, Processes, and Organizations for the Modern Enterprise (Addison-\nWesley Professional, 2009) by Michael T. Fisher and Martin L. Abbott. A key idea in\nthat book is the scale cube, which, as described in chapter 2, is a three-dimensional\nmodel for scaling an application. The Y-axis scaling defined by the scale cube func-\ntionally decomposes an application into services. In hindsight, this was quite obvious,\nbut for me at the time, it was an a-ha moment! I could have solved the challenges I was\nfacing two years earlier by architecting Cloud Foundry as a set of services!\n In April 2012, I gave my first talk on this architectural approach, called “Decom-\nposing Applications of Deployability and Scalability” (www.slideshare.net/chris.e\n.richardson/decomposing-applications-for-scalability-and-deployability-april-2012). At\nthe time, there wasn’t a generally accepted term for this kind of architecture. I some-\ntimes called it modular, polyglot architecture, because the services could be written in\ndifferent languages.\n But in another example of how the future is unevenly distributed, the term micro-\nservice was used at a software architecture workshop in 2011 to describe this kind of\narchitecture (https://en.wikipedia.org/wiki/Microservices). I first encountered the\nterm when I heard Fred George give a talk at Oredev 2013, and I liked it!\n In January 2014, I created the https://microservices.io website to document archi-\ntecture and design patterns that I had encountered. Then in March 2014, James Lewis\nand Martin Fowler published a blog post about microservices (https://martinfowler\n.com/articles/microservices.html). By popularizing the term microservices, the blog\npost caused the software community to consolidate around the concept.\n The idea of small, loosely coupled teams, rapidly and reliably developing and deliv-\nering microservices is slowly diffusing through the software community. But it’s likely\nthat this vision of the future is quite different from your daily reality. Today, business-\ncritical enterprise applications are typically large monoliths developed by large teams.\nSoftware releases occur infrequently and are often painful for everyone involved. IT\noften struggles to keep up with the needs of the business. You’re wondering how on\nearth you can adopt the microservice architecture.\n \n",
      "content_length": 3344,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "PREFACE\nxix\n The goal of this book is to answer that question. It will give you a good understand-\ning of the microservice architecture, its benefits and drawbacks, and when to use it.\nThe book describes how to solve the numerous design challenges you’ll face, includ-\ning how to manage distributed data. It also covers how to refactor a monolithic appli-\ncation to a microservice architecture. But this book is not a microservices manifesto.\nInstead, it’s organized around a collection of patterns. A pattern is a reusable solution\nto a problem that occurs in a particular context. The beauty of a pattern is that\nbesides describing the benefits of the solution, it also describes the drawbacks and the\nissues you must address in order to successfully implement a solution. In my experi-\nence, this kind of objectivity when thinking about solutions leads to much better deci-\nsion making. I hope you’ll enjoy reading this book and that it teaches you how to\nsuccessfully develop microservices.\n \n",
      "content_length": 997,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "xx\nacknowledgments\nAlthough writing is a solitary activity, it takes a large number of people to turn rough\ndrafts into a finished book.\n First, I want to thank Erin Twohey and Michael Stevens from Manning for their\npersistent encouragement to write another book. I would also like to thank my devel-\nopment editors, Cynthia Kane and Marina Michaels. Cynthia Kane got me started and\nworked with me on the first few chapters. Marina Michaels took over from Cynthia\nand worked with me to the end. I’ll be forever grateful for Marina’s meticulous and\nconstructive critiques of my chapters. And I want to thank the rest of the Manning\nteam who’s been involved in getting this book published.\n I’d like to thank my technical development editor, Christian Mennerich, my tech-\nnical proofreader, Andy Miles, and all my external reviewers: Andy Kirsch, Antonio\nPessolano, Areg Melik-Adamyan, Cage Slagel, Carlos Curotto, Dror Helper, Eros\nPedrini, Hugo Cruz, Irina Romanenko, Jesse Rosalia, Joe Justesen, John Guthrie,\nKeerthi Shetty, Michele Mauro, Paul Grebenc, Pethuru Raj, Potito Coluccelli, Shobha\nIyer, Simeon Leyzerzon, Srihari Sridharan, Tim Moore, Tony Sweets, Trent Whiteley,\nWes Shaddix, William E. Wheeler, and Zoltan Hamori.\n I also want to thank everyone who purchased the MEAP and provided feedback in\nthe forum or to me directly.\n I want to thank the organizers and attendees of all of the conferences and meetups\nat which I’ve spoken for the chance to present and revise my ideas. And I want to\nthank my consulting and training clients around the world for giving me the opportu-\nnity to help them put my ideas into practice.\n \n",
      "content_length": 1637,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "ACKNOWLEDGMENTS\nxxi\n I want to thank my colleagues Andrew, Valentin, Artem, and Stanislav at Eventuate,\nInc., for their contributions to the Eventuate product and open source projects.\n Finally, I’d like to thank my wife, Laura, and my children, Ellie, Thomas, and Janet\nfor their support and understanding over the last 18 months. While I’ve been glued to\nmy laptop, I’ve missed out on going to Ellie’s soccer games, watching Thomas learn-\ning to fly on his flight simulator, and trying new restaurants with Janet.\n Thank you all!\n \n",
      "content_length": 534,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "xxii\nabout this book\nThe goal of this book is to teach you how to successfully develop applications using\nthe microservice architecture.\n Not only does it discuss the benefits of the microservice architecture, it also\ndescribes the drawbacks. You’ll learn when you should consider using the monolithic\narchitecture and when it makes sense to use microservices.\nWho should read this book\nThe focus of this book is on architecture and development. It’s meant for anyone\nresponsible for developing and delivering software, such as developers, architects,\nCTOs, or VPs of engineering.\n The book focuses on explaining the microservice architecture patterns and other\nconcepts. My goal is for you to find this material accessible, regardless of the technol-\nogy stack you use. You only need to be familiar with the basics of enterprise applica-\ntion architecture and design. In particular, you need to understand concepts like\nthree-tier architecture, web application design, relational databases, interprocess com-\nmunication using messaging and REST, and the basics of application security. The\ncode examples, though, use Java and the Spring framework. In order to get the most\nout of them, you should be familiar with the Spring framework.\n \n \n \n",
      "content_length": 1243,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "ABOUT THIS BOOK\nxxiii\nRoadmap\nThis book consists of 13 chapters:\n■\nChapter 1 describes the symptoms of monolithic hell, which occurs when a\nmonolithic application outgrows its architecture, and advises on how to escape\nby adopting the microservice architecture. It also provides an overview of the\nmicroservice architecture pattern language, which is the organizing theme for\nmost of the book.\n■\nChapter 2 explains why software architecture is important and describes the\npatterns you can use to decompose an application into a collection of services.\nIt also explains how to overcome the various obstacles you typically encounter\nalong the way.\n■\nChapter 3 describes the different patterns for robust, interprocess communica-\ntion in a microservice architecture. It explains why asynchronous, message-\nbased communication is often the best choice.\n■\nChapter 4 explains how to maintain data consistency across services by using\nthe Saga pattern. A saga is a sequence of local transactions coordinated using\nasynchronous messaging.\n■\nChapter 5 describes how to design the business logic for a service using the\ndomain-driven design (DDD) Aggregate and Domain event patterns.\n■\nChapter 6 builds on chapter 5 and explains how to develop business logic using\nthe Event sourcing pattern, an event-centric way to structure the business logic\nand persist domain objects.\n■\nChapter 7 describes how to implement queries that retrieve data scattered\nacross multiple services by using either the API composition pattern or the\nCommand query responsibility segregation (CQRS) pattern.\n■\nChapter 8 covers the external API patterns for handling requests from a diverse\ncollection of external clients, such as mobile applications, browser-based Java-\nScript applications, and third-party applications.\n■\nChapter 9 is the first of two chapters on automated testing techniques for micro-\nservices. It introduces important testing concepts such as the test pyramid, which\ndescribes the relative proportions of each type of test in your test suite. It also\nshows how to write unit tests, which form the base of the testing pyramid.\n■\nChapter 10 builds on chapter 9 and describes how to write other types of tests in\nthe test pyramid, including integration tests, consumer contract tests, and com-\nponent tests.\n■\nChapter 11 covers various aspects of developing production-ready services,\nincluding security, the Externalized configuration pattern, and the service\nobservability patterns. The service observability patterns include Log aggrega-\ntion, Application metrics, and Distributed tracing.\n■\nChapter 12 describes the various deployment patterns that you can use to\ndeploy services, including virtual machines, containers, and serverless. It also\n \n",
      "content_length": 2735,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "ABOUT THIS BOOK\nxxiv\ndiscusses the benefits of using a service mesh, a layer of networking software\nthat mediates communication in a microservice architecture.\n■\nChapter 13 explains how to incrementally refactor a monolithic architecture to\na microservice architecture by applying the Strangler application pattern: imple-\nmenting new features as services and extracting modules out of the monolith\nand converting them to services. \nAs you progress through these chapters, you’ll learn about different aspects of the\nmicroservice architecture.\nAbout the code\nThis book contains many examples of source code both in numbered listings and\ninline with normal text. In both cases, source code is formatted in a fixed-width font\nlike this to separate it from ordinary text. Sometimes code is also in bold to high-\nlight code that has changed from previous steps in the chapter, such as when a new\nfeature adds to an existing line of code. In many cases, the original source code has\nbeen reformatted; the publisher has added line breaks and reworked indentation to\naccommodate the available page space in the book. In rare cases, even this was not\nenough, and listings include line-continuation markers (➥). Additionally, comments\nin the source code have often been removed from the listings when the code is\ndescribed in the text. Code annotations accompany many of the listings, highlighting\nimportant concepts.\n Every chapter, except chapters 1, 2, and 13, contains code from the companion\nexample application. You can find the code for this application in a GitHub reposi-\ntory: https://github.com/microservices-patterns/ftgo-application.\nBook forum\nThe purchase of Microservices Patterns includes free access to a private web forum\nrun by Manning Publications where you can make comments about the book, ask\ntechnical questions, share your solutions to exercises, and receive help from the\nauthor and from other users. To access the forum and subscribe to it, point your web\nbrowser to https://forums.manning.com/forums/microservices-patterns. You can\nalso learn more about Manning’s forums and the rules of conduct at https://forums\n.manning.com/forums/about.\n Manning’s commitment to our readers is to provide a venue where a meaningful\ndialogue between individual readers and between readers and the author can take\nplace. It’s not a commitment to any specific amount of participation on the part of the\nauthor, whose contribution to the forum remains voluntary (and unpaid). We suggest\nyou try asking the author some challenging questions lest his interest stray! The forum\nand the archives of previous discussions will be accessible from the publisher’s website\nas long as the book is in print.\n \n",
      "content_length": 2702,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "ABOUT THIS BOOK\nxxv\nOther online resources\nAnother great resource for learning the microservice architecture is my website http://\nmicroservices.io.\n Not only does it contain the complete pattern language, it also has links to other\nresources such as articles, presentations, and example code.\nAbout the author\nChris Richardson is a developer and architect. He is a Java Champion, a JavaOne rock\nstar, and the author of POJOs in Action (Manning, 2006), which describes how to build\nenterprise Java applications with frameworks such as Spring and Hibernate.\n Chris was also the founder of the original CloudFoundry.com, an early Java PaaS\nfor Amazon EC2.\n Today, he is a recognized thought leader in microservices and speaks regularly at\ninternational conferences. Chris is the creator of Microservices.io, a pattern language\nfor microservices. He provides microservices consulting and training to organizations\naround the world that are adopting the microservice architecture. Chris is working on\nhis third startup: Eventuate.io, an application platform for developing transactional\nmicroservices.\n \n",
      "content_length": 1100,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "xxvi\nabout the cover illustration\nJefferys\nThe figure on the cover of Microservices Patterns is captioned “Habit of a Morisco\nSlave in 1568.” The illustration is taken from Thomas Jefferys’ A Collection of the Dresses\nof Different Nations, Ancient and Modern (four volumes), London, published between\n1757 and 1772. The title page states that these are hand-colored copperplate engrav-\nings, heightened with gum arabic. \n Thomas Jefferys (1719–1771) was called “Geographer to King George III.” He was\nan English cartographer who was the leading map supplier of his day. He engraved\nand printed maps for government and other official bodies and produced a wide\nrange of commercial maps and atlases, especially of North America. His work as a map\nmaker sparked an interest in local dress customs of the lands he surveyed and\nmapped, which are brilliantly displayed in this collection. Fascination with faraway\nlands and travel for pleasure were relatively new phenomena in the late 18th century,\nand collections such as this one were popular, introducing both the tourist as well as\nthe armchair traveler to the inhabitants of other countries.\n The diversity of the drawings in Jefferys’ volumes speaks vividly of the uniqueness\nand individuality of the world’s nations some 200 years ago. Dress codes have changed\nsince then, and the diversity by region and country, so rich at the time, has faded away.\nIt’s now often hard to tell the inhabitants of one continent from another. Perhaps, try-\ning to view it optimistically, we’ve traded a cultural and visual diversity for a more var-\nied personal life—or a more varied and interesting intellectual and technical life.\n \n",
      "content_length": 1670,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "ABOUT THE COVER ILLUSTRATION\nxxvii\n At a time when it’s difficult to tell one computer book from another, Manning cel-\nebrates the inventiveness and initiative of the computer business with book covers\nbased on the rich diversity of regional life of two centuries ago, brought back to life by\nJeffreys’ pictures.\n \n",
      "content_length": 315,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "1\nEscaping monolithic hell\nIt was only Monday lunchtime, but Mary, the CTO of Food to Go, Inc. (FTGO), was\nalready feeling frustrated. Her day had started off really well. She had spent the\nprevious week with other software architects and developers at an excellent confer-\nence learning about the latest software development techniques, including contin-\nuous deployment and the microservice architecture. Mary had also met up with her\nformer computer science classmates from North Carolina A&T State and shared\ntechnology leadership war stories. The conference had left her feeling empowered\nand eager to improve how FTGO develops software.\nThis chapter covers\nThe symptoms of monolithic hell and how to \nescape it by adopting the microservice \narchitecture\nThe essential characteristics of the microservice \narchitecture and its benefits and drawbacks\nHow microservices enable the DevOps style of \ndevelopment of large, complex applications\nThe microservice architecture pattern language \nand why you should use it\n \n",
      "content_length": 1024,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "2\nCHAPTER 1\nEscaping monolithic hell\n Unfortunately, that feeling had quickly evaporated. She had just spent the first\nmorning back in the office in yet another painful meeting with senior engineering\nand business people. They had spent two hours discussing why the development team\nwas going to miss another critical release date. Sadly, this kind of meeting had become\nincreasingly common over the past few years. Despite adopting agile, the pace of devel-\nopment was slowing down, making it next to impossible to meet the business’s goals.\nAnd, to make matters worse, there didn’t seem to be a simple solution.\n The conference had made Mary realize that FTGO was suffering from a case of\nmonolithic hell and that the cure was to adopt the microservice architecture. But the\nmicroservice architecture and the associated state-of-the-art software development\npractices described at the conference felt like an elusive dream. It was unclear to Mary\nhow she could fight today’s fires while simultaneously improving the way software was\ndeveloped at FTGO.\n Fortunately, as you will learn in this book, there is a way. But first, let’s look at the\nproblems that FTGO is facing and how they got there.\n1.1\nThe slow march toward monolithic hell\nSince its launch in late 2005, FTGO had grown by leaps and bounds. Today, it’s one of\nthe leading online food delivery companies in the United States. The business even\nplans to expand overseas, although those plans are in jeopardy because of delays in\nimplementing the necessary features.\n At its core, the FTGO application is quite simple. Consumers use the FTGO web-\nsite or mobile application to place food orders at local restaurants. FTGO coordinates\na network of couriers who deliver the orders. It’s also responsible for paying couriers\nand restaurants. Restaurants use the FTGO website to edit their menus and manage\norders. The application uses various web services, including Stripe for payments,\nTwilio for messaging, and Amazon Simple Email Service (SES) for email.\n Like many other aging enterprise applications, the FTGO application is a mono-\nlith, consisting of a single Java Web Application Archive (WAR) file. Over the years, it\nhas become a large, complex application. Despite the best efforts of the FTGO devel-\nopment team, it’s become an example of the Big Ball of Mud pattern (www.laputan\n.org/mud/). To quote Foote and Yoder, the authors of that pattern, it’s a “haphaz-\nardly structured, sprawling, sloppy, duct-tape and bailing wire, spaghetti code jungle.”\nThe pace of software delivery has slowed. To make matters worse, the FTGO applica-\ntion has been written using some increasingly obsolete frameworks. The FTGO appli-\ncation is exhibiting all the symptoms of monolithic hell.\n The next section describes the architecture of the FTGO application. Then it\ntalks about why the monolithic architecture worked well initially. We’ll get into how\nthe FTGO application has outgrown its architecture and how that has resulted in\nmonolithic hell.\n \n",
      "content_length": 3012,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "3\nThe slow march toward monolithic hell\n1.1.1\nThe architecture of the FTGO application\nFTGO is a typical enterprise Java application. Figure 1.1 shows its architecture. The\nFTGO application has a hexagonal architecture, which is an architectural style\ndescribed in more detail in chapter 2. In a hexagonal architecture, the core of the\napplication consists of the business logic. Surrounding the business logic are various\nadapters that implement UIs and integrate with external systems.\nThe business logic consists of modules, each of which is a collection of domain\nobjects. Examples of the modules include Order Management, Delivery Management,\nBilling, and Payments. There are several adapters that interface with the external sys-\ntems. Some are inbound adapters, which handle requests by invoking the business\nlogic, including the REST API and Web UI adapters. Others are outbound adapters,\nwhich enable the business logic to access the MySQL database and invoke cloud ser-\nvices such as Twilio and Stripe.\n Despite having a logically modular architecture, the FTGO application is packaged\nas a single WAR file. The application is an example of the widely used monolithic style\nInvoked by mobile applications\nTwilio\nmessaging\nservice\nCloud services\nFTGO application\nAWS SES\nemail\nservice\nStripe\npayment\nservice\nAdapters invoke\ncloud services.\nTwilio\nadapter\nCourier\nREST\nAPI\nWeb\nUI\nMySQL\nadapter\nRestaurant\nmanagement\nPayments\nBilling\nNotiﬁcation\nOrder\nmanagement\nDelivery\nmanagement\nAmazon\nSES\nadapter\nStripe\nadapter\nConsumer\nRestaurant\nMySQL\nFigure 1.1\nThe FTGO application has a hexagonal architecture. It consists of business logic \nsurrounded by adapters that implement UIs and interface with external systems, such as mobile \napplications and cloud services for payments, messaging, and email.\n \n",
      "content_length": 1808,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "4\nCHAPTER 1\nEscaping monolithic hell\nof software architecture, which structures a system as a single executable or deploy-\nable component. If the FTGO application were written in the Go language (GoLang),\nit would be a single executable. A Ruby or NodeJS version of the application would be\na single directory hierarchy of source code. The monolithic architecture isn’t inher-\nently bad. The FTGO developers made a good decision when they picked monolithic\narchitecture for their application. \n1.1.2\nThe benefits of the monolithic architecture\nIn the early days of FTGO, when the application was relatively small, the application’s\nmonolithic architecture had lots of benefits:\nSimple to develop—IDEs and other developer tools are focused on building a sin-\ngle application.\nEasy to make radical changes to the application—You can change the code and the\ndatabase schema, build, and deploy.\nStraightforward to test—The developers wrote end-to-end tests that launched the\napplication, invoked the REST API, and tested the UI with Selenium.\nStraightforward to deploy—All a developer had to do was copy the WAR file to a\nserver that had Tomcat installed.\nEasy to scale—FTGO ran multiple instances of the application behind a load\nbalancer.\nOver time, though, development, testing, deployment, and scaling became much more\ndifficult. Let’s look at why. \n1.1.3\nLiving in monolithic hell\nUnfortunately, as the FTGO developers have discovered, the monolithic architecture\nhas a huge limitation. Successful applications like the FTGO application have a habit\nof outgrowing the monolithic architecture. Each sprint, the FTGO development team\nimplemented a few more stories, which made the code base larger. Moreover, as the\ncompany became more successful, the size of the development team steadily grew.\nNot only did this increase the growth rate of the code base, it also increased the man-\nagement overhead.\n As figure 1.2 shows, the once small, simple FTGO application has grown over the\nyears into a monstrous monolith. Similarly, the small development team has now\nbecome multiple Scrum teams, each of which works on a particular functional area.\nAs a result of outgrowing its architecture, FTGO is in monolithic hell. Development is\nslow and painful. Agile development and deployment is impossible. Let’s look at why\nthis has happened.\nCOMPLEXITY INTIMIDATES DEVELOPERS\nA major problem with the FTGO application is that it’s too complex. It’s too large for\nany developer to fully understand. As a result, fixing bugs and correctly implementing\nnew features have become difficult and time consuming. Deadlines are missed.\n \n",
      "content_length": 2625,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "5\nThe slow march toward monolithic hell\nTo make matters worse, this overwhelming complexity tends to be a downward spiral.\nIf the code base is difficult to understand, a developer won’t make changes correctly.\nEach change makes the code base incrementally more complex and harder to under-\nstand. The clean, modular architecture shown earlier in figure 1.1 doesn’t reflect real-\nity. FTGO is gradually becoming a monstrous, incomprehensible, big ball of mud.\n Mary remembers recently attending a conference where she met a developer who\nwas writing a tool to analyze the dependencies between the thousands of JARs in their\nmultimillion lines-of-code (LOC) application. At the time, that tool seemed like some-\nthing FTGO could use. Now she’s not so sure. Mary suspects a better approach is to\nmigrate to an architecture that is better suited to a complex application: microservices. \nDEVELOPMENT IS SLOW\nAs well as having to fight overwhelming complexity, FTGO developers find day-to-day\ndevelopment tasks slow. The large application overloads and slows down a developer’s\nIDE. Building the FTGO application takes a long time. Moreover, because it’s so large,\nthe application takes a long time to start up. As a result, the edit-build-run-test loop\ntakes a long time, which badly impacts productivity. \nPATH FROM COMMIT TO DEPLOYMENT IS LONG AND ARDUOUS\nAnother problem with the FTGO application is that deploying changes into produc-\ntion is a long and painful process. The team typically deploys updates to production\nonce a month, usually late on a Friday or Saturday night. Mary keeps reading that the\nstate-of-the-art for Software-as-a-Service (SaaS) applications is continuous deployment:\nLarge\ndevelopment\norganization\nSingle code base creates\ncommunication and\ncoordination overhead.\nLarge, complex\nunreliable, difﬁcult\nto maintain\nThe path from code commit to\nproduction is arduous.\nChanges sit in a queue until\nthey can be manually tested.\nOrder management team\nRestaurant management team\nDelivery management team\nFTGO development\nProduction\nJenkins\nCl\nBacklog\nDeployment pipeline\nSource\ncode\nrepository\nManual\ntesting\nFTGO\napplication\nFigure 1.2\nA case of monolithic hell. The large FTGO developer team commits their changes to a \nsingle source code repository. The path from code commit to production is long and arduous and \ninvolves manual testing. The FTGO application is large, complex, unreliable, and difficult to maintain.\n \n",
      "content_length": 2444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "6\nCHAPTER 1\nEscaping monolithic hell\ndeploying changes to production many times a day during business hours. Apparently,\nas of 2011, Amazon.com deployed a change into production every 11.6 seconds with-\nout ever impacting the user! For the FTGO developers, updating production more\nthan once a month seems like a distant dream. And adopting continuous deployment\nseems next to impossible.\n FTGO has partially adopted agile. The engineering team is divided into squads\nand uses two-week sprints. Unfortunately, the journey from code complete to running\nin production is long and arduous. One problem with so many developers committing\nto the same code base is that the build is frequently in an unreleasable state. When the\nFTGO developers tried to solve this problem by using feature branches, their attempt\nresulted in lengthy, painful merges. Consequently, once a team completes its sprint, a\nlong period of testing and code stabilization follows.\n Another reason it takes so long to get changes into production is that testing takes\na long time. Because the code base is so complex and the impact of a change isn’t well\nunderstood, developers and the Continuous Integration (CI) server must run the\nentire test suite. Some parts of the system even require manual testing. It also takes a\nwhile to diagnose and fix the cause of a test failure. As a result, it takes a couple of days\nto complete a testing cycle. \nSCALING IS DIFFICULT\nThe FTGO team also has problems scaling its application. That’s because different\napplication modules have conflicting resource requirements. The restaurant data, for\nexample, is stored in a large, in-memory database, which is ideally deployed on servers\nwith lots of memory. In contrast, the image processing module is CPU intensive and\nbest deployed on servers with lots of CPU. Because these modules are part of the same\napplication, FTGO must compromise on the server configuration. \nDELIVERING A RELIABLE MONOLITH IS CHALLENGING\nAnother problem with the FTGO application is the lack of reliability. As a result, there\nare frequent production outages. One reason it’s unreliable is that testing the applica-\ntion thoroughly is difficult, due to its large size. This lack of testability means bugs\nmake their way into production. To make matters worse, the application lacks fault iso-\nlation, because all modules are running within the same process. Every so often, a bug\nin one module—for example, a memory leak—crashes all instances of the applica-\ntion, one by one. The FTGO developers don’t enjoy being paged in the middle of the\nnight because of a production outage. The business people like the loss of revenue\nand trust even less. \nLOCKED INTO INCREASINGLY OBSOLETE TECHNOLOGY STACK\nThe final aspect of monolithic hell experienced by the FTGO team is that the archi-\ntecture forces them to use a technology stack that’s becoming increasingly obsolete. The\nmonolithic architecture makes it difficult to adopt new frameworks and languages. It\nwould be extremely expensive and risky to rewrite the entire monolithic application so\nthat it would use a new and presumably better technology. Consequently, developers\n \n",
      "content_length": 3160,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "7\nWhat you’ll learn in this book\nare stuck with the technology choices they made at the start of the project. Quite\noften, they must maintain an application written using an increasingly obsolete tech-\nnology stack.\n The Spring framework has continued to evolve while being backward compatible,\nso in theory FTGO might have been able to upgrade. Unfortunately, the FTGO applica-\ntion uses versions of frameworks that are incompatible with newer versions of Spring.\nThe development team has never found the time to upgrade those frameworks. As a\nresult, major parts of the application are written using increasingly out-of-date frame-\nworks. What’s more, the FTGO developers would like to experiment with non-JVM\nlanguages such as GoLang and NodeJS. Sadly, that’s not possible with a monolithic\napplication. \n1.2\nWhy this book is relevant to you\nIt’s likely that you’re a developer, architect, CTO, or VP of engineering. You’re responsi-\nble for an application that has outgrown its monolithic architecture. Like Mary at\nFTGO, you’re struggling with software delivery and want to know how to escape\nmonolith hell. Or perhaps you fear that your organization is on the path to mono-\nlithic hell and you want to know how to change direction before it’s too late. If you\nneed to escape or avoid monolithic hell, this is the book for you.\n This book spends a lot of time explaining microservice architecture concepts. My\ngoal is for you to find this material accessible, regardless of the technology stack you\nuse. All you need is to be familiar with the basics of enterprise application architecture\nand design. In particular, you need to know the following:\nThree-tier architecture\nWeb application design\nHow to develop business logic using object-oriented design\nHow to use an RDBMS: SQL and ACID transactions\nHow to use interprocess communication using a message broker and REST APIs\nSecurity, including authentication and authorization\nThe code examples in this book are written using Java and the Spring framework.\nThat means in order to get the most out of the examples, you need to be familiar with\nthe Spring framework too.\n1.3\nWhat you’ll learn in this book\nBy the time you finish reading this book you’ll understand the following:\nThe essential characteristics of the microservice architecture, its benefits and\ndrawbacks, and when to use it\nDistributed data management patterns\nEffective microservice testing strategies\nDeployment options for microservices\nStrategies for refactoring a monolithic application into a microservice architecture\n \n",
      "content_length": 2561,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "8\nCHAPTER 1\nEscaping monolithic hell\nYou’ll also be able to do the following:\nArchitect an application using the microservice architecture pattern\nDevelop the business logic for a service\nUse sagas to maintain data consistency across services\nImplement queries that span services\nEffectively test microservices\nDevelop production-ready services that are secure, configurable, and observable\nRefactor an existing monolithic application to services\n1.4\nMicroservice architecture to the rescue\nMary has come to the conclusion that FTGO must migrate to the microservice\narchitecture.\n Interestingly, software architecture has very little to do with functional require-\nments. You can implement a set of use cases—an application’s functional require-\nments—with any architecture. In fact, it’s common for successful applications, such as\nthe FTGO application, to be big balls of mud.\n Architecture matters, however, because of how it affects the so-called quality of ser-\nvice requirements, also called nonfunctional requirements, quality attributes, or ilities. As\nthe FTGO application has grown, various quality attributes have suffered, most nota-\nbly those that impact the velocity of software delivery: maintainability, extensibility,\nand testability.\n On the one hand, a disciplined team can slow down the pace of its descent toward\nmonolithic hell. Team members can work hard to maintain the modularity of their\napplication. They can write comprehensive automated tests. On the other hand, they\ncan’t avoid the issues of a large team working on a single monolithic application. Nor\ncan they solve the problem of an increasingly obsolete technology stack. The best a\nteam can do is delay the inevitable. To escape monolithic hell, they must migrate to a\nnew architecture: the Microservice architecture.\n Today, the growing consensus is that if you’re building a large, complex applica-\ntion, you should consider using the microservice architecture. But what are micro-\nservices exactly? Unfortunately, the name doesn’t help because it overemphasizes size.\nThere are numerous definitions of the microservice architecture. Some take the name\ntoo literally and claim that a service should be tiny—for example, 100 LOC. Others\nclaim that a service should only take two weeks to develop. Adrian Cockcroft, formerly\nof Netflix, defines a microservice architecture as a service-oriented architecture com-\nposed of loosely coupled elements that have bounded contexts. That’s not a bad defi-\nnition, but it is a little dense. Let’s see if we can do better.\n1.4.1\nScale cube and microservices\nMy definition of the microservice architecture is inspired by Martin Abbott and\nMichael Fisher’s excellent book, The Art of Scalability (Addison-Wesley, 2015). This\n \n",
      "content_length": 2758,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "9\nMicroservice architecture to the rescue\nbook describes a useful, three-dimensional scalability model: the scale cube, shown in\nfigure 1.3.\nThe model defines three ways to scale an application: X, Y, and Z.\nX-AXIS SCALING LOAD BALANCES REQUESTS ACROSS MULTIPLE INSTANCES\nX-axis scaling is a common way to scale a monolithic application. Figure 1.4 shows\nhow X-axis scaling works. You run multiple instances of the application behind a\nload balancer. The load balancer distributes requests among the N identical instances of\nthe application. This is a great way of improving the capacity and availability of an\napplication. \nZ-AXIS SCALING ROUTES REQUESTS BASED ON AN ATTRIBUTE OF THE REQUEST\nZ-axis scaling also runs multiple instances of the monolith application, but unlike X-axis\nscaling, each instance is responsible for only a subset of the data. Figure 1.5 shows how\nZ-axis scaling works. The router in front of the instances uses a request attribute to\nroute it to the appropriate instance. An application might, for example, route requests\nusing userId.\n In this example, each application instance is responsible for a subset of users. The\nrouter uses the userId specified by the request Authorization header to select one of\nMicroservices\nY-axis scaling,\na.k.a. functional\ndecomposition\nScale by splitting\nthings that are\ndifferent, such as\nby function.\nX-axis scaling,\na.k.a. horizontal duplication\nScale by cloning.\nZ-axis scaling,\na.k.a. data partitioning\nScale by splitting\nsimilar things, such as\nby customer ID.\nOne\ninstance\nMany\ninstances\nOne\npartition\nMany\npartitions\nMonolith\nFigure 1.3\nThe scale cube defines three separate ways to scale an application: X-axis \nscaling load balances requests across multiple, identical instances; Z-axis scaling routes \nrequests based on an attribute of the request; Y-axis functionally decomposes an application \ninto services.\n \n",
      "content_length": 1885,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "10\nCHAPTER 1\nEscaping monolithic hell\nthe N identical instances of the application. Z-axis scaling is a great way to scale an\napplication to handle increasing transaction and data volumes. \nY-AXIS SCALING FUNCTIONALLY DECOMPOSES AN APPLICATION INTO SERVICES\nX- and Z-axis scaling improve the application’s capacity and availability. But neither\napproach solves the problem of increasing development and application complexity. To\nsolve those, you need to apply Y-axis scaling, or functional decomposition. Figure 1.6 shows\nhow Y-axis scaling works: by splitting a monolithic application into a set of services.\nApplication\ninstance 1\nN identical application\ninstances\nApplication\ninstance 2\nLoad\nbalancer\nClient\nRequest\nApplication\ninstance 3\nRoute requests using a\nload balancing algorithm.\nFigure 1.4\nX-axis scaling runs multiple, identical instances of the monolithic \napplication behind a load balancer.\nApplication\ninstance 1\nN identical application\ninstances\nApplication\ninstance 2\nClient\nRouter\nRequest:\nGET /...\nAuthorization: userId:password\nApplication\ninstance 3\nUsers: a–h\nUsers: i-p\nUsers: r–z\nUses the userId to decide\nwhere to route requests\nEach instance is responsible\nfor a subset of the users.\nFigure 1.5\nZ-axis scaling runs multiple identical instances of the monolithic application behind \na router, which routes based on a request attribute . Each instance is responsible for a subset \nof the data.\n \n",
      "content_length": 1423,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "11\nMicroservice architecture to the rescue\nA service is a mini application that implements narrowly focused functionality, such as\norder management, customer management, and so on. A service is scaled using X-axis\nscaling, though some services may also use Z-axis scaling. For example, the Order ser-\nvice consists of a set of load-balanced service instances.\n The high-level definition of microservice architecture (microservices) is an archi-\ntectural style that functionally decomposes an application into a set of services. Note\nthat this definition doesn’t say anything about size. Instead, what matters is that each\nservice has a focused, cohesive set of responsibilities. Later in the book I discuss what\nthat means.\n Now let’s look at how the microservice architecture is a form of modularity. \n1.4.2\nMicroservices as a form of modularity\nModularity is essential when developing large, complex applications. A modern appli-\ncation like FTGO is too large to be developed by an individual. It’s also too complex\nto be understood by a single person. Applications must be decomposed into modules\nthat are developed and understood by different people. In a monolithic application,\nmodules are defined using a combination of programming language constructs (such\nas Java packages) and build artifacts (such as Java JAR files). However, as the FTGO\ndevelopers have discovered, this approach tends not to work well in practice. Long-\nlived, monolithic applications usually degenerate into big balls of mud.\n The microservice architecture uses services as the unit of modularity. A service has\nan API, which is an impermeable boundary that is difficult to violate. You can’t bypass\nOrder\nService\nApplication\nCustomer\nService\nClient\nReview\nService\nOrder\nrequests\nCustomer\nrequests\nReview\nrequests\nOrder\nService\ninstance 1\nOrder service\nOrder\nService\ninstance 2\nOrder\nService\ninstance 3\nLoad\nbalancer\nRequest\nY-axis scaling functionality decomposes\nan application into services.\nEach service is typically scaled using\nX-axis and possibly Z-axis scaling.\nFigure 1.6\nY-axis scaling splits the application into a set of services. Each service is responsible for \na particular function. A service is scaled using X-axis scaling and, possibly, Z-axis scaling.\n \n",
      "content_length": 2254,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "12\nCHAPTER 1\nEscaping monolithic hell\nthe API and access an internal class as you can with a Java package. As a result, it’s\nmuch easier to preserve the modularity of the application over time. There are other\nbenefits of using services as building blocks, including the ability to deploy and scale\nthem independently. \n1.4.3\nEach service has its own database\nA key characteristic of the microservice architecture is that the services are loosely\ncoupled and communicate only via APIs. One way to achieve loose coupling is by each\nservice having its own datastore. In the online store, for example, Order Service has a\ndatabase that includes the ORDERS table, and Customer Service has its database, which\nincludes the CUSTOMERS table. At development time, developers can change a service’s\nschema without having to coordinate with developers working on other services. At\nruntime, the services are isolated from each other—for example, one service will\nnever be blocked because another service holds a database lock.\nNow that we’ve defined the microservice architecture and described some of its essen-\ntial characteristics, let’s look at how this applies to the FTGO application. \n1.4.4\nThe FTGO microservice architecture\nThe rest of this book discusses the FTGO application’s microservice architecture in\ndepth. But first let’s quickly look at what it means to apply Y-axis scaling to this applica-\ntion. If we apply Y-axis decomposition to the FTGO application, we get the architec-\nture shown in figure 1.7. The decomposed application consists of numerous frontend\nand backend services. We would also apply X-axis and, possibly Z-axis scaling, so that\nat runtime there would be multiple instances of each service.\n The frontend services include an API gateway and the Restaurant Web UI. The API\ngateway, which plays the role of a facade and is described in detail in chapter 8, provides\nthe REST APIs that are used by the consumers’ and couriers’ mobile applications. The\nRestaurant Web UI implements the web interface that’s used by the restaurants to man-\nage menus and process orders.\n The FTGO application’s business logic consists of numerous backend services.\nEach backend service has a REST API and its own private datastore. The backend ser-\nvices include the following:\n\nOrder Service—Manages orders\n\nDelivery Service—Manages delivery of orders from restaurants to consumers\nDon’t worry: Loose coupling doesn’t make Larry Ellison richer\nThe requirement for each service to have its own database doesn’t mean it has its\nown database server. You don’t, for example, have to spend 10 times more on Oracle\nRDBMS licenses. Chapter 2 explores this topic in depth.\n \n",
      "content_length": 2675,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "13\nMicroservice architecture to the rescue\n\nRestaurant Service—Maintains information about restaurants\n\nKitchen Service—Manages the preparation of orders\n\nAccounting Service—Handles billing and payments\nMany services correspond to the modules described earlier in this chapter. What’s dif-\nferent is that each service and its API are very clearly defined. Each one can be inde-\npendently developed, tested, deployed, and scaled. Also, this architecture does a good\njob of preserving modularity. A developer can’t bypass a service’s API and access its\ninternal components. Chapter 13 describes how to transform an existing monolithic\napplication into microservices. \n1.4.5\nComparing the microservice architecture and SOA\nSome critics of the microservice architecture claim it’s nothing new—it’s service-\noriented architecture (SOA). At a very high level, there are some similarities. SOA\nand the microservice architecture are architectural styles that structure a system as a\nset of services. But as table 1.1 shows, once you dig deep, you encounter significant\ndifferences.\nAmazon\nSES\nAdapter\nTwilio\nAdapter\nStripe\nAdapter\nThe API Gateway routes\nrequests from the mobile\napplications to services.\nServices have APIs.\nA service’s data is private.\nServices corresponding\nto business capabilities/\ndomain-driven design\n(DDD) subdomains\nAPI\nGateway\nRestaurant\nWeb UI\nOrder\nService\nCourier\nREST\nAPI\nREST\nAPI\nREST\nAPI\nConsumer\nRestaurant\nRestaurant\nService\nREST\nAPI\nAccounting\nService\nREST\nAPI\nNotiﬁcation\nService\nREST\nAPI\nKitchen\nService\nREST\nAPI\nDelivery\nService\nREST\nAPI\nFigure 1.7\nSome of the services of the microservice architecture-based version of the FTGO \napplication. An API Gateway routes requests from the mobile applications to services. The services \ncollaborate via APIs.\n \n",
      "content_length": 1787,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "14\nCHAPTER 1\nEscaping monolithic hell\nSOA and the microservice architecture usually use different technology stacks. SOA\napplications typically use heavyweight technologies such as SOAP and other WS* stan-\ndards. They often use an ESB, a smart pipe that contains business and message-processing\nlogic to integrate the services. Applications built using the microservice architecture\ntend to use lightweight, open source technologies. The services communicate via dumb\npipes, such as message brokers or lightweight protocols like REST or gRPC.\n SOA and the microservice architecture also differ in how they treat data. SOA\napplications typically have a global data model and share databases. In contrast, as\nmentioned earlier, in the microservice architecture each service has its own database.\nMoreover, as described in chapter 2, each service is usually considered to have its own\ndomain model.\n Another key difference between SOA and the microservice architecture is the size\nof the services. SOA is typically used to integrate large, complex, monolithic applica-\ntions. Although services in a microservice architecture aren’t always tiny, they’re\nalmost always much smaller. As a result, a SOA application usually consists of a few\nlarge services, whereas a microservices-based application typically consists of dozens or\nhundreds of smaller services. \n1.5\nBenefits and drawbacks of the microservice \narchitecture\nLet’s first consider the benefits and then we’ll look at the drawbacks.\n1.5.1\nBenefits of the microservice architecture\nThe microservice architecture has the following benefits:\nIt enables the continuous delivery and deployment of large, complex applications.\nServices are small and easily maintained.\nServices are independently deployable.\nServices are independently scalable.\nThe microservice architecture enables teams to be autonomous.\nIt allows easy experimenting and adoption of new technologies.\nIt has better fault isolation.\nTable 1.1\nComparing SOA with microservices\nSOA\nMicroservices\nInter-service \ncommunication\nSmart pipes, such as Enterprise Ser-\nvice Bus, using heavyweight protocols, \nsuch as SOAP and the other WS* \nstandards.\nDumb pipes, such as a message \nbroker, or direct service-to-service \ncommunication, using lightweight \nprotocols such as REST or gRPC\nData\nGlobal data model and shared data-\nbases\nData model and database per service\nTypical service\nLarger monolithic application\nSmaller service\n \n",
      "content_length": 2448,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "15\nBenefits and drawbacks of the microservice architecture\nLet’s look at each benefit.\nENABLES THE CONTINUOUS DELIVERY AND DEPLOYMENT OF LARGE, COMPLEX APPLICATIONS\nThe most important benefit of the microservice architecture is that it enables continu-\nous delivery and deployment of large, complex applications. As described later in sec-\ntion 1.7, continuous delivery/deployment is part of DevOps, a set of practices for the\nrapid, frequent, and reliable delivery of software. High-performing DevOps organiza-\ntions typically deploy changes into production with very few production issues.\n There are three ways that the microservice architecture enables continuous deliv-\nery/deployment:\nIt has the testability required by continuous delivery/deployment—Automated testing is\na key practice of continuous delivery/deployment. Because each service in a\nmicroservice architecture is relatively small, automated tests are much easier to\nwrite and faster to execute. As a result, the application will have fewer bugs.\nIt has the deployability required by continuous delivery/deployment—Each service can\nbe deployed independently of other services. If the developers responsible for a\nservice need to deploy a change that’s local to that service, they don’t need to\ncoordinate with other developers. They can deploy their changes. As a result,\nit’s much easier to deploy changes frequently into production.\nIt enables development teams to be autonomous and loosely coupled—You can structure\nthe engineering organization as a collection of small (for example, two-pizza)\nteams. Each team is solely responsible for the development and deployment of\none or more related services. As figure 1.8 shows, each team can develop, deploy,\nand scale their services independently of all the other teams. As a result, the\ndevelopment velocity is much higher.\nThe ability to do continuous delivery and deployment has several business benefits:\nIt reduces the time to market, which enables the business to rapidly react to\nfeedback from customers.\nIt enables the business to provide the kind of reliable service today’s customers\nhave come to expect.\nEmployee satisfaction is higher because more time is spent delivering valuable\nfeatures instead of fighting fires.\nAs a result, the microservice architecture has become the table stakes of any business\nthat depends upon software technology. \nEACH SERVICE IS SMALL AND EASILY MAINTAINED\nAnother benefit of the microservice architecture is that each service is relatively small.\nThe code is easier for a developer to understand. The small code base doesn’t slow\ndown the IDE, making developers more productive. And each service typically starts a\nlot faster than a large monolith does, which also makes developers more productive\nand speeds up deployments. \n \n",
      "content_length": 2797,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "16\nCHAPTER 1\nEscaping monolithic hell\nSERVICES ARE INDEPENDENTLY SCALABLE\nEach service in a microservice architecture can be scaled independently of other ser-\nvices using X-axis cloning and Z-axis partitioning. Moreover, each service can be\ndeployed on hardware that’s best suited to its resource requirements. This is quite dif-\nferent than when using a monolithic architecture, where components with wildly dif-\nferent resource requirements—for example, CPU-intensive vs. memory-intensive—\nmust be deployed together. \nBETTER FAULT ISOLATION\nThe microservice architecture has better fault isolation. For example, a memory leak\nin one service only affects that service. Other services will continue to handle requests\nnormally. In comparison, one misbehaving component of a monolithic architecture\nwill bring down the entire system. \nEASILY EXPERIMENT WITH AND ADOPT NEW TECHNOLOGIES\nLast but not least, the microservice architecture eliminates any long-term commit-\nment to a technology stack. In principle, when developing a new service, the develop-\ners are free to pick whatever language and frameworks are best suited for that service.\nSmall, autonomous,\nloosely coupled teams\nEach service has\nits own source\ncode repository.\nEach service has\nits own automated\ndeployment pipeline.\nSmall, simple,\nreliable, easy to\nmaintain services\nOrder management team\nRestaurant management team\nDelivery management team\nFTGO development\nProduction\nJenkins Cl\nDeployment pipeline\nOrder Service\nsource code\nrepository\nOrder Service\nJenkins Cl\nDeployment pipeline\nRestaurant Service\nsource code\nrepository\nRestaurant Service\nJenkins Cl\nDeployment pipeline\nDelivery Service\nsource code\nrepository\nDelivery Service\nFigure 1.8\nThe microservices-based FTGO application consists of a set of loosely coupled services. \nEach team develops, tests, and deploys their services independently.\n \n",
      "content_length": 1874,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "17\nBenefits and drawbacks of the microservice architecture\nIn many organizations, it makes sense to restrict the choices, but the key point is that\nyou aren’t constrained by past decisions.\n Moreover, because the services are small, rewriting them using better languages\nand technologies becomes practical. If the trial of a new technology fails, you can\nthrow away that work without risking the entire project. This is quite different than\nwhen using a monolithic architecture, where your initial technology choices severely\nconstrain your ability to use different languages and frameworks in the future. \n1.5.2\nDrawbacks of the microservice architecture\nCertainly, no technology is a silver bullet, and the microservice architecture has a\nnumber of significant drawbacks and issues. Indeed most of this book is about how to\naddress these drawbacks and issues. As you read about the challenges, don’t worry.\nLater in this book I describe ways to address them.\n Here are the major drawbacks and issues of the microservice architecture:\nFinding the right set of services is challenging.\nDistributed systems are complex, which makes development, testing, and deploy-\nment difficult.\nDeploying features that span multiple services requires careful coordination.\nDeciding when to adopt the microservice architecture is difficult.\nLet’s look at each one in turn.\nFINDING THE RIGHT SERVICES IS CHALLENGING\nOne challenge with using the microservice architecture is that there isn’t a concrete,\nwell-defined algorithm for decomposing a system into services. As with much of soft-\nware development, it’s something of an art. To make matters worse, if you decompose\na system incorrectly, you’ll build a distributed monolith, a system consisting of coupled\nservices that must be deployed together. A distributed monolith has the drawbacks of\nboth the monolithic architecture and the microservice architecture. \nDISTRIBUTED SYSTEMS ARE COMPLEX\nAnother issue with using the microservice architecture is that developers must deal\nwith the additional complexity of creating a distributed system. Services must use an\ninterprocess communication mechanism. This is more complex than a simple method\ncall. Moreover, a service must be designed to handle partial failure and deal with the\nremote service either being unavailable or exhibiting high latency.\n Implementing use cases that span multiple services requires the use of unfamiliar\ntechniques. Each service has its own database, which makes it a challenge to implement\ntransactions and queries that span services. As described in chapter 4, a microservices-\nbased application must use what are known as sagas to maintain data consistency\nacross services. Chapter 7 explains that a microservices-based application can’t retrieve\ndata from multiple services using simple queries. Instead, it must implement queries\nusing either API composition or CQRS views.\n \n",
      "content_length": 2901,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "18\nCHAPTER 1\nEscaping monolithic hell\n IDEs and other development tools are focused on building monolithic applica-\ntions and don’t provide explicit support for developing distributed applications. Writ-\ning automated tests that involve multiple services is challenging. These are all issues\nthat are specific to the microservice architecture. Consequently, your organization’s\ndevelopers must have sophisticated software development and delivery skills in order\nto successfully use microservices.\n The microservice architecture also introduces significant operational complexity.\nMany more moving parts—multiple instances of different types of service—must be\nmanaged in production. To successfully deploy microservices, you need a high level of\nautomation. You must use technologies such as the following:\nAutomated deployment tooling, like Netflix Spinnaker\nAn off-the-shelf PaaS, like Pivotal Cloud Foundry or Red Hat OpenShift\nA Docker orchestration platform, like Docker Swarm or Kubernetes\nI describe the deployment options in more detail in chapter 12. \nDEPLOYING FEATURES SPANNING MULTIPLE SERVICES NEEDS CAREFUL COORDINATION\nAnother challenge with using the microservice architecture is that deploying features\nthat span multiple services requires careful coordination between the various develop-\nment teams. You have to create a rollout plan that orders service deployments based\non the dependencies between services. That’s quite different than a monolithic archi-\ntecture, where you can easily deploy updates to multiple components atomically. \nDECIDING WHEN TO ADOPT IS DIFFICULT\nAnother issue with using the microservice architecture is deciding at what point during\nthe lifecycle of the application you should use this architecture. When developing the\nfirst version of an application, you often don’t have the problems that this architec-\nture solves. Moreover, using an elaborate, distributed architecture will slow down\ndevelopment. That can be a major dilemma for startups, where the biggest problem is\nusually how to rapidly evolve the business model and accompanying application.\nUsing the microservice architecture makes it much more difficult to iterate rapidly. A\nstartup should almost certainly begin with a monolithic application.\n Later on, though, when the problem is how to handle complexity, that’s when it\nmakes sense to functionally decompose the application into a set of microservices.\nYou may find refactoring difficult because of tangled dependencies. Chapter 13 goes\nover strategies for refactoring a monolithic application into microservices.\n As you can see, the microservice architecture offer many benefits, but also has some\nsignificant drawbacks. Because of these issues, adopting a microservice architecture\nshould not be undertaken lightly. But for complex applications, such as a consumer-\nfacing web application or SaaS application, it’s usually the right choice. Well-known\nsites like eBay (www.slideshare.net/RandyShoup/the-ebay-architecture-striking-a-\nbalance-between-site-stability-feature-velocity-performance-and-cost), Amazon.com,\nGroupon, and Gilt have all evolved from a monolithic architecture to a microservice\narchitecture.\n \n",
      "content_length": 3191,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "19\nThe Microservice architecture pattern language\n You must address numerous design and architectural issues when using the micro-\nservice architecture. What’s more, many of these issues have multiple solutions, each\nwith a different set of trade-offs. There is no one single perfect solution. To help guide\nyour decision making, I’ve created the Microservice architecture pattern language. I ref-\nerence this pattern language throughout the rest of the book as I teach you about the\nmicroservice architecture. Let’s look at what a pattern language is and why it’s helpful. \n1.6\nThe Microservice architecture pattern language\nArchitecture and design are all about making decisions. You need to decide whether\nthe monolithic or microservice architecture is the best fit for your application. When\nmaking these decisions you have lots of trade-offs to consider. If you pick the microser-\nvice architecture, you’ll need to address lots of issues.\n A good way to describe the various architectural and design options and improve\ndecision making is to use a pattern language. Let’s first look at why we need patterns\nand a pattern language, and then we’ll take a tour of the Microservice architecture\npattern language.\n1.6.1\nMicroservice architecture is not a silver bullet\nBack in 1986, Fred Brooks, author of The Mythical Man-Month (Addison-Wesley Profes-\nsional, 1995), said that in software engineering, there are no silver bullets. That means\nthere are no techniques or technologies that if adopted would give you a tenfold\nboost in productivity. Yet decades years later, developers are still arguing passionately\nabout their favorite silver bullets, absolutely convinced that their favorite technology\nwill give them a massive boost in productivity.\n A lot of arguments follow the suck/rock dichotomy (http://nealford.com/memeagora/\n2009/08/05/suck-rock-dichotomy.html), a term coined by Neal Ford that describes\nhow everything in the software world either sucks or rocks, with no middle ground.\nThese arguments have this structure: if you do X, then a puppy will die, so therefore\nyou must do Y. For example, synchronous versus reactive programming, object-oriented\nversus functional, Java versus JavaScript, REST versus messaging. Of course, reality is\nmuch more nuanced. Every technology has drawbacks and limitations that are often\noverlooked by its advocates. As a result, the adoption of a technology usually follows\nthe Gartner hype cycle (https://en.wikipedia.org/wiki/Hype_cycle), in which an emerg-\ning technology goes through five phases, including the peak of inflated expectations (it\nrocks), followed by the trough of disillusionment (it sucks), and ending with the plateau\nof productivity (we now understand the trade-offs and when to use it).\n Microservices are not immune to the silver bullet phenomenon. Whether this\narchitecture is appropriate for your application depends on many factors. Conse-\nquently, it’s bad advice to advise always using the microservice architecture, but it’s\nequally bad advice to advise never using it. As with many things, it depends.\n The underlying reason for these polarized and hyped arguments about technology is\nthat humans are primarily driven by their emotions. Jonathan Haidt, in his excellent\n \n",
      "content_length": 3253,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "20\nCHAPTER 1\nEscaping monolithic hell\nbook The Righteous Mind: Why Good People Are Divided by Politics and Religion (Vintage,\n2013), uses the metaphor of an elephant and its rider to describe how the human mind\nworks. The elephant represents the emotion part of the human brain. It makes most of\nthe decisions. The rider represents the rational part of the brain. It can sometimes influ-\nence the elephant, but it mostly provides justifications for the elephant’s decisions.\n We—the software development community—need to overcome our emotional\nnature and find a better way of discussing and applying technology. A great way to dis-\ncuss and describe technology is to use the pattern format, because it’s objective. When\ndescribing a technology in the pattern format, you must, for example, describe the\ndrawbacks. Let’s take a look at the pattern format. \n1.6.2\nPatterns and pattern languages\nA pattern is a reusable solution to a problem that occurs in a particular context. It’s an\nidea that has its origins in real-world architecture and that has proven to be useful in\nsoftware architecture and design. The concept of a pattern was created by Christo-\npher Alexander, a real-world architect. He also created the concept of a pattern lan-\nguage, a collection of related patterns that solve problems within a particular domain.\nHis book A Pattern Language: Towns, Buildings, Construction (Oxford University Press,\n1977) describes a pattern language for architecture that consists of 253 patterns. The\npatterns range from solutions to high-level problems, such as where to locate a city\n(“Access to water”), to low-level problems, such as how to design a room (“Light on\ntwo sides of every room”). Each of these patterns solves a problem by arranging physi-\ncal objects that range in scope from cities to windows.\n Christopher Alexander’s writings inspired the software community to adopt the\nconcept of patterns and pattern languages. The book Design Patterns: Elements of Reus-\nable Object-Oriented Software (Addison-Wesley Professional, 1994), by Erich Gamma,\nRichard Helm, Ralph Johnson, and John Vlissides is a collection of object-oriented\ndesign patterns. The book popularized patterns among software developers. Since the\nmid-1990s, software developers have documented numerous software patterns. A soft-\nware pattern solves a software architecture or design problem by defining a set of col-\nlaborating software elements.\n Let’s imagine, for example, that you’re building a banking application that must\nsupport a variety of overdraft policies. Each policy defines limits on the balance of an\naccount and the fees charged for an overdrawn account. You can solve this problem\nusing the Strategy pattern, which is a well-known pattern from the classic Design Pat-\nterns book. The solution defined by the Strategy pattern consists of three parts:\nA strategy interface called Overdraft that encapsulates the overdraft algorithm\nOne or more concrete strategy classes, one for each particular context\nThe Account class that uses the algorithm\nThe Strategy pattern is an object-oriented design pattern, so the elements of the solution\nare classes. Later in this section, I describe high-level design patterns, where the solu-\ntion consists of collaborating services.\n \n",
      "content_length": 3276,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "21\nThe Microservice architecture pattern language\n One reason why patterns are valuable is because a pattern must describe the con-\ntext within which it applies. The idea that a solution is specific to a particular context\nand might not work well in other contexts is an improvement over how technology\nused to typically be discussed. For example, a solution that solves the problem at the\nscale of Netflix might not be the best approach for an application with fewer users.\n The value of a pattern, however, goes far beyond requiring you to consider the\ncontext of a problem. It forces you to describe other critical yet frequently overlooked\naspects of a solution. A commonly used pattern structure includes three especially\nvaluable sections:\nForces\nResulting context\nRelated patterns\nLet’s look at each of these, starting with forces.\nFORCES: THE ISSUES THAT YOU MUST ADDRESS WHEN SOLVING A PROBLEM\nThe forces section of a pattern describes the forces (issues) that you must address\nwhen solving a problem in a given context. Forces can conflict, so it might not be\npossible to solve all of them. Which forces are more important depends on the con-\ntext. You have to prioritize solving some forces over others. For example, code must\nbe easy to understand and have good performance. Code written in a reactive style\nhas better performance than synchronous code, yet is often more difficult to under-\nstand. Explicitly listing the forces is useful because it makes clear which issues need\nto be solved. \nRESULTING CONTEXT: THE CONSEQUENCES OF APPLYING A PATTERN\nThe resulting context section of a pattern describes the consequences of applying the\npattern. It consists of three parts:\nBenefits—The benefits of the pattern, including the forces that have been resolved\nDrawbacks—The drawbacks of the pattern, including the unresolved forces\nIssues—The new problems that have been introduced by applying the pattern\nThe resulting context provides a more complete and less biased view of the solution,\nwhich enables better design decisions. \nRELATED PATTERNS: THE FIVE DIFFERENT TYPES OF RELATIONSHIPS\nThe related patterns section of a pattern describes the relationship between the pattern\nand other patterns. There are five types of relationships between patterns:\nPredecessor—A predecessor pattern is a pattern that motivates the need for this\npattern. For example, the Microservice architecture pattern is the predecessor\nto the rest of the patterns in the pattern language, except the monolithic archi-\ntecture pattern.\nSuccessor—A pattern that solves an issue that has been introduced by this pat-\ntern. For example, if you apply the Microservice architecture pattern, you must\n \n",
      "content_length": 2694,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "22\nCHAPTER 1\nEscaping monolithic hell\nthen apply numerous successor patterns, including service discovery patterns\nand the Circuit breaker pattern.\nAlternative—A pattern that provides an alternative solution to this pattern. For\nexample, the Monolithic architecture pattern and the Microservice architec-\nture pattern are alternative ways of architecting an application. You pick one or\nthe other.\nGeneralization—A pattern that is a general solution to a problem. For example,\nin chapter 12 you’ll learn about the different implementations of the Single ser-\nvice per host pattern.\nSpecialization—A specialized form of a particular pattern. For example, in chap-\nter 12 you’ll learn that the Deploy a service as a container pattern is a specializa-\ntion of Single service per host.\nIn addition, you can organize patterns that tackle issues in a particular problem area\ninto groups. The explicit description of related patterns provides valuable guidance\non how to effectively solve a particular problem. Figure 1.9 shows how the relation-\nships between patterns is visually represented.\nThe different kinds of relationships between patterns shown in figure 1.9 are repre-\nsented as follows:\nRepresents the predecessor-successor relationship\nPatterns that are alternative solutions to the same problem\nIndicates that one pattern is a specialization of another pattern\nPatterns that apply to a particular problem area\nPattern\nProblem area\nDeployment\nMonolithic\narchitecture\nKey\nMicroservice\narchitecture\nSingle service\nper host\nService-per-container\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nFigure 1.9\nThe visual representation of different types of relationships \nbetween the patterns: a successor pattern solves a problem created by applying \nthe predecessor pattern; two or more patterns can be alternative solutions to \nthe same problem; one pattern can be a specialization of another pattern; and \npatterns that solve problems in the same area can be grouped, or generalized.\n \n",
      "content_length": 2013,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "23\nThe Microservice architecture pattern language\nA collection of patterns related through these relationships sometimes form what is\nknown as a pattern language. The patterns in a pattern language work together to\nsolve problems in a particular domain. In particular, I’ve created the Microservice\narchitecture pattern language. It’s a collection of interrelated software architecture\nand design patterns for microservices. Let’s take a look at this pattern language. \n1.6.3\nOverview of the Microservice architecture pattern language\nThe Microservice architecture pattern language is a collection of patterns that help\nyou architect an application using the microservice architecture. Figure 1.10 shows\nthe high-level structure of the pattern language. The pattern language first helps\nyou decide whether to use the microservice architecture. It describes the monolithic\narchitecture and the microservice architecture, along with their benefits and draw-\nbacks. Then, if the microservice architecture is a good fit for your application, the\npattern language helps you use it effectively by solving various architecture and\ndesign issues.\n The pattern language consists of several groups of patterns. On the left in figure 1.10\nis the application architecture patterns group, the Monolithic architecture pattern\nand the Microservice architecture pattern. Those are the patterns we’ve been discussing\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nDecomposition\nApplication infrastructure patterns\nCommunication patterns\nInfrastructure patterns\nMicroservice patterns\nApplication\narchitecture\nApplication patterns\nTesting\nObservability\nMaintaining\ndata consistency\nDatabase\narchitecture\nKey\nQuerying\nSecurity\nCross-cutting\nconcerns\nReliability\nExternal\nAPI\nCommunication style\nDiscovery\nTransactional messaging\nProblem area\nDeployment\nMonolithic\narchitecture\nMicroservice\narchitecture\nFigure 1.10\nA high-level view of the Microservice architecture pattern language showing the different problem \nareas that the patterns solve. On the left are the application architecture patterns: Monolithic architecture and \nMicroservice architecture. All the other groups of patterns solve problems that result from choosing the \nMicroservice architecture pattern.\n \n",
      "content_length": 2271,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "24\nCHAPTER 1\nEscaping monolithic hell\nin this chapter. The rest of the pattern language consists of groups of patterns that are\nsolutions to issues that are introduced by using the Microservice architecture pattern.\n The patterns are also divided into three layers:\nInfrastructure patterns—These solve problems that are mostly infrastructure issues\noutside of development.\nApplication infrastructure —These are for infrastructure issues that also impact\ndevelopment.\nApplication patterns—These solve problems faced by developers.\nThese patterns are grouped together based on the kind of problem they solve. Let’s\nlook at the main groups of patterns.\nPATTERNS FOR DECOMPOSING AN APPLICATION INTO SERVICES\nDeciding how to decompose a system into a set of services is very much an art, but\nthere are a number of strategies that can help. The two decomposition patterns\nshown in figure 1.11 are different strategies you can use to define your application’s\narchitecture.\nChapter 2 describes these patterns in detail. \nCOMMUNICATION PATTERNS\nAn application built using the microservice architecture is a distributed system. Conse-\nquently, interprocess communication (IPC) is an important part of the microservice\narchitecture. You must make a variety of architectural and design decisions about how\nyour services communicate with one another and the outside world. Figure 1.12 shows\nthe communication patterns, which are organized into five groups:\nCommunication style—What kind of IPC mechanism should you use?\nDiscovery—How does a client of a service determine the IP address of a service\ninstance so that, for example, it makes an HTTP request?\nReliability—How can you ensure that communication between services is reli-\nable even though services can be unavailable?\nTransactional messaging—How should you integrate the sending of messages and\npublishing of events with database transactions that update business data?\nExternal API—How do clients of your application communicate with the services?\nDecompose by\nbusiness capability\nDecompose by\nsubdomain\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nKey\nProblem area\nFigure 1.11\nThere are two \ndecomposition patterns: Decompose \nby business capability, which organizes \nservices around business capabilities, \nand Decompose by subdomain, which \norganizes services around domain-\ndriven design (DDD) subdomains.\n \n",
      "content_length": 2390,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "25\nThe Microservice architecture pattern language\nChapter 3 looks at the first four groups of patterns: communication style, discovery,\nreliability, and transaction messaging. Chapter 8 looks at the external API patterns. \nDATA CONSISTENCY PATTERNS FOR IMPLEMENTING TRANSACTION MANAGEMENT\nAs mentioned earlier, in order to ensure loose coupling, each service has its own data-\nbase. Unfortunately, having a database per service introduces some significant issues. I\ndescribe in chapter 4 that the traditional approach of using distributed transactions\n(2PC) isn’t a viable option for a modern application. Instead, an application needs to\nmaintain data consistency by using the Saga pattern. Figure 1.13 shows data-related\npatterns.\n Chapters 4, 5, and 6 describe these patterns in more detail. \nPATTERNS FOR QUERYING DATA IN A MICROSERVICE ARCHITECTURE\nThe other issue with using a database per service is that some queries need to join\ndata that’s owned by multiple services. A service’s data is only accessible via its API, so\nyou can’t use distributed queries against its database. Figure 1.14 shows a couple of\npatterns you can use to implement queries.\nPolling\npublisher\nTransaction\nlog tailing\nTransactional messaging\nTransactional\noutbox\nMessaging\nRemote procedure\ninvocation\nCircuit\nbreaker\nCommunication style\nReliability\nDomain-speciﬁc\nSelf registration\nClient-side\ndiscovery\nDiscovery\nExternal API\n3rd-party\nregistration\nAPI gateway\nBackend for\nfrontend\nServer-side\ndiscovery\nService registry\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nKey\nProblem area\nFigure 1.12\nThe five groups of communication patterns\n \n",
      "content_length": 1644,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "26\nCHAPTER 1\nEscaping monolithic hell\nSometimes you can use the API composition pattern, which invokes the APIs of one or\nmore services and aggregates results. Other times, you must use the Command query\nresponsibility segregation (CQRS) pattern, which maintains one or more easily queried\nreplicas of the data. Chapter 7 looks at the different ways of implementing queries. \nSERVICE DEPLOYMENT PATTERNS\nDeploying a monolithic application isn’t always easy, but it is straightforward in the\nsense that there is a single application to deploy. You have to run multiple instances of\nthe application behind a load balancer.\n In comparison, deploying a microservices-based application is much more com-\nplex. There may be tens or hundreds of services that are written in a variety of lan-\nguages and frameworks. There are many more moving parts that need to be managed.\nFigure 1.15 shows the deployment patterns.\n The traditional, and often manual, way of deploying applications in a language-\nspecific packaging format, for example WAR files, doesn’t scale to support a microser-\nvice architecture. You need a highly automated deployment infrastructure. Ideally,\nyou should use a deployment platform that provides the developer with a simple UI\n(command-line or GUI) for deploying and managing their services. The deployment\nplatform will typically be based on virtual machines (VMs), containers, or serverless\ntechnology. Chapter 12 looks at the different deployment options. \nDatabase per\nservice\nSaga\nEvent\nsourcing\nDomain\nevent\nAggregate\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nKey\nProblem area\nFigure 1.13\nBecause each service has its own database, you must use the Saga pattern to maintain \ndata consistency across services.\nCQRS\nAPI\ncomposition\nDatabase\nper service\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nKey\nProblem area\nFigure 1.14\nBecause each service has its own database, you must use one \nof the querying patterns to retrieve data scattered across multiple services.\n \n",
      "content_length": 2032,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "27\nThe Microservice architecture pattern language\nOBSERVABILITY PATTERNS PROVIDE INSIGHT INTO APPLICATION BEHAVIOR\nA key part of operating an application is understanding its runtime behavior and trouble-\nshooting problems such as failed requests and high latency. Though understanding and\ntroubleshooting a monolithic application isn’t always easy, it helps that requests are han-\ndled in a simple, straightforward way. Each incoming request is load balanced to a par-\nticular application instance, which makes a few calls to the database and returns a\nresponse. For example, if you need to understand how a particular request was handled,\nyou look at the log file of the application instance that handled the request.\n In contrast, understanding and diagnosing problems in a microservice architec-\nture is much more complicated. A request can bounce around between multiple ser-\nvices before a response is finally returned to a client. Consequently, there isn’t one log\nfile to examine. Similarly, problems with latency are more difficult to diagnose because\nthere are multiple suspects.\n You can use the following patterns to design observable services:\nHealth check API—Expose an endpoint that returns the health of the service.\nLog aggregation—Log service activity and write logs into a centralized logging\nserver, which provides searching and alerting.\nTraditional approach of deploying\nservices using their language-speciﬁc\npackaging, such as WAR ﬁles\nAutomated, self-service\nplatform for deploying\nand managing services\nA modern approach,\nwhich runs your code\nwithout you having to\nworry about managing\nthe infrastructure\nA modern approach, which\nencapsulates a service’s\ntechnology stack\nSingle service\nper host\nMultiple services\nper host\nServerless\ndeployment\nService-per-container\nService-per-VM\nService deployment\nplatform\nGeneral\nAlternative A\nPredecessor\nSpeciﬁc\nAlternative B\nSuccessor\nKey\nProblem area\nFigure 1.15\nSeveral patterns for deploying microservices. The traditional approach is to deploy \nservices in a language-specific packaging format. There are two modern approaches to deploying \nservices. The first deploys services as VM or containers. The second is the serverless approach. \nYou simply upload the service’s code and the serverless platform runs it. You should use a service \ndeployment platform, which is an automated, self-service platform for deploying and managing \nservices.\n \n",
      "content_length": 2417,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "28\nCHAPTER 1\nEscaping monolithic hell\nDistributed tracing—Assign each external request a unique ID and trace requests\nas they flow between services.\nException tracking—Report exceptions to an exception tracking service, which\ndeduplicates exceptions, alerts developers, and tracks the resolution of each\nexception.\nApplication metrics—Maintain metrics, such as counters and gauges, and expose\nthem to a metrics server.\nAudit logging—Log user actions.\nChapter 11 describes these patterns in more detail. \nPATTERNS FOR THE AUTOMATED TESTING OF SERVICES\nThe microservice architecture makes individual services easier to test because they’re\nmuch smaller than the monolithic application. At the same time, though, it’s import-\nant to test that the different services work together while avoiding using complex,\nslow, and brittle end-to-end tests that test multiple services together. Here are patterns\nfor simplifying testing by testing services in isolation:\nConsumer-driven contract test—Verify that a service meets the expectations of its\nclients.\nConsumer-side contract test—Verify that the client of a service can communicate\nwith the service.\nService component test—Test a service in isolation.\nChapters 9 and 10 describe these testing patterns in more detail. \nPATTERNS FOR HANDLING CROSS-CUTTING CONCERNS\nIn a microservice architecture, there are numerous concerns that every service must\nimplement, including the observability patterns and discovery patterns. It must also\nimplement the Externalized Configuration pattern, which supplies configuration\nparameters such as database credentials to a service at runtime. When developing a\nnew service, it would be too time consuming to reimplement these concerns from\nscratch. A much better approach is to apply the Microservice Chassis pattern and\nbuild services on top of a framework that handles these concerns. Chapter 11\ndescribes these patterns in more detail. \nSECURITY PATTERNS\nIn a microservice architecture, users are typically authenticated by the API gateway. It\nmust then pass information about the user, such as identity and roles, to the services it\ninvokes. A common solution is to apply the Access token pattern. The API gateway\npasses an access token, such as JWT (JSON Web Token), to the services, which can val-\nidate the token and obtain information about the user. Chapter 11 discusses the\nAccess token pattern in more detail.\n Not surprisingly, the patterns in the Microservice architecture pattern language\nare focused on solving architect and design problems. You certainly need the right\n \n",
      "content_length": 2574,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "29\nBeyond microservices: Process and organization\narchitecture in order to successfully develop software, but it’s not the only concern.\nYou must also consider process and organization. \n1.7\nBeyond microservices: Process and organization\nFor a large, complex application, the microservice architecture is usually the best\nchoice. But in addition to having the right architecture, successful software develop-\nment requires you to also have organization, and development and delivery processes.\nFigure 1.16 shows the relationships between process, organization, and architecture.\nI’ve already described the microservice architecture. Let’s look at organization and\nprocess.\n1.7.1\nSoftware development and delivery organization\nSuccess inevitably means that the engineering team will grow. On the one hand, that’s\na good thing because more developers can get more done. The trouble with large\nteams is, as Fred Brooks wrote in The Mythical Man-Month, the communication over-\nhead of a team of size N is O(N 2). If the team gets too large, it will become inefficient,\ndue to the communication overhead. Imagine, for example, trying to do a daily standup\nwith 20 people.\n The solution is to refactor a large single team into a team of teams. Each team is\nsmall, consisting of no more than 8–12 people. It has a clearly defined business-oriented\nmission: developing and possibly operating one or more services that implement a\nfeature or a business capability. The team is cross-functional and can develop, test,\nand deploy its services without having to frequently communicate or coordinate with\nother teams.\nEnables\nEnables\nArchitecture:\nMicroservice\narchitecture\nOrganization:\nSmall, autonomous,\ncross-functional teams\nProcess:\nDevOps/continuous delivery/deployment\nEnables\nRapid, frequent,\nand reliable delivery\nof software\nFigure 1.16\nThe rapid, frequent, and reliable delivery of large, \ncomplex applications requires a combination of DevOps, which \nincludes continuous delivery/deployment, small, autonomous \nteams, and the microservice architecture.\n \n",
      "content_length": 2055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "30\nCHAPTER 1\nEscaping monolithic hell\nThe velocity of the team of teams is significantly higher than that of a single large\nteam. As described earlier in section 1.5.1, the microservice architecture plays a key\nrole in enabling the teams to be autonomous. Each team can develop, deploy, and\nscale their services without coordinating with other teams. Moreover, it’s very clear\nwho to contact when a service isn’t meeting its SLA.\n What’s more, the development organization is much more scalable. You grow the\norganization by adding teams. If a single team becomes too large, you split it and its\nassociated service or services. Because the teams are loosely coupled, you avoid the\ncommunication overhead of a large team. As a result, you can add people without\nimpacting productivity. \n1.7.2\nSoftware development and delivery process\nUsing the microservice architecture with a waterfall development process is like driv-\ning a horse-drawn Ferrari—you squander most of the benefit of using microservices. If\nyou want to develop an application with the microservice architecture, it’s essential\nthat you adopt agile development and deployment practices such as Scrum or Kan-\nban. Better yet, you should practice continuous delivery/deployment, which is a part\nof DevOps.\n Jez Humble (https://continuousdelivery.com/) defines continuous delivery as\nfollows:\nContinuous Delivery is the ability to get changes of all types—including new features,\nconfiguration changes, bug fixes and experiments—into production, or into the hands of\nusers, safely and quickly in a sustainable way.\nA key characteristic of continuous delivery is that software is always releasable. It\nrelies on a high level of automation, including automated testing. Continuous\ndeployment takes continuous delivery one step further in the practice of automati-\ncally deploying releasable code into production. High-performing organizations\nThe reverse Conway maneuver\nIn order to effectively deliver software when using the microservice architecture, you\nneed to take into account Conway’s law (https://en.wikipedia.org/wiki/Conway%27s\n_law), which states the following:\nOrganizations which design systems … are constrained to produce designs\nwhich are copies of the communication structures of these organizations.\nMelvin Conway\nIn other words, your application’s architecture mirrors the structure of the organiza-\ntion that developed it. It’s important, therefore, to apply Conway’s law in reverse\n(www.thoughtworks.com/radar/techniques/inverse-conway-maneuver) and design\nyour organization so that its structure mirrors your microservice architecture. By doing\nso, you ensure that your development teams are as loosely coupled as the services.\n \n",
      "content_length": 2713,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "31\nBeyond microservices: Process and organization\nthat practice continuous deployment deploy multiple times per day into produc-\ntion, have far fewer production outages, and recover quickly from any that do occur\n(https://puppet.com/ resources/whitepaper/state-of-devops-report). As described ear-\nlier in section 1.5.1, the microservice architecture directly supports continuous\ndelivery/deployment.\n1.7.3\nThe human side of adopting microservices\nAdopting the microservice architecture changes your architecture, your organization,\nand your development processes. Ultimately, though, it changes the working environ-\nment of people, who are, as mentioned earlier, emotional creatures. If ignored, their\nemotions can make the adoption of microservices a bumpy ride. Mary and the other\nFTGO leaders will struggle to change how FTGO develops software.\n The best-selling book Managing Transitions (Da Capo Lifelong Books, 2017,\nhttps://wmbridges.com/books) by William and Susan Bridges introduces the con-\ncept of a transition, which refers to the process of how people respond emotionally to a\nchange. It describes a three-stage Transition Model:\n1\nEnding, Losing, and Letting Go—The period of emotional upheaval and resis-\ntance when people are presented with a change that forces them out of their\ncomfort zone. They often mourn the loss of the old way of doing things. For\nexample, when people reorganize into cross-functional teams, they miss their\nformer teammates. Similarly, a data modeling group that owns the global data\nmodel will be threatened by the idea of each service having its own data\nmodel.\nMove fast without breaking things\nThe goal of continuous delivery/deployment (and, more generally, DevOps) is to rap-\nidly yet reliably deliver software. Four useful metrics for assessing software develop-\nment are as follows:\nDeployment frequency—How often software is deployed into production\nLead time—Time from a developer checking in a change to that change being\ndeployed\nMean time to recover—Time to recover from a production problem\nChange failure rate—Percentage of changes that result in a production problem\nIn a traditional organization, the deployment frequency is low, and the lead time is\nhigh. Stressed-out developers and operations people typically stay up late into the\nnight fixing last-minute issues during the maintenance window. In contrast, a DevOps\norganization releases software frequently, often multiple times per day, with far fewer\nproduction issues. Amazon, for example, deployed changes into production every\n11.6 seconds in 2014 (www.youtube.com/watch?v=dxk8b9rSKOo), and Netflix had\na lead time of 16 minutes for one software component (https://medium.com/netflix-\ntechblog/how-we-build-code-at-netflix-c5d9bd727f15). \n \n",
      "content_length": 2765,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "32\nCHAPTER 1\nEscaping monolithic hell\n2\nThe Neutral Zone—The intermediate stage between the old and new ways of doing\nthings, where people are often confused. They are often struggling to learn the\nnew way of doing things.\n3\nThe New Beginning—The final stage where people have enthusiastically embraced\nthe new way of doing things and are starting to experience the benefits.\nThe book describes how best to manage each stage of the transition and increase the\nlikelihood of successfully implementing the change. FTGO is certainly suffering from\nmonolithic hell and needs to migrate to a microservice architecture. It must also\nchange its organization and development processes. In order for FTGO to successfully\naccomplish this, however, it must take into account the transition model and consider\npeople’s emotions.\n In the next chapter, you’ll learn about the goal of software architecture and how to\ndecompose an application into services. \nSummary\nThe Monolithic architecture pattern structures the application as a single deploy-\nable unit.\nThe Microservice architecture pattern decomposes a system into a set of inde-\npendently deployable services, each with its own database.\nThe monolithic architecture is a good choice for simple applications, but micro-\nservice architecture is usually a better choice for large, complex applications.\nThe microservice architecture accelerates the velocity of software development\nby enabling small, autonomous teams to work in parallel.\nThe microservice architecture isn’t a silver bullet—there are significant draw-\nbacks, including complexity.\nThe Microservice architecture pattern language is a collection of patterns that\nhelp you architect an application using the microservice architecture. It helps\nyou decide whether to use the microservice architecture, and if you pick the\nmicroservice architecture, the pattern language helps you apply it effectively.\nYou need more than just the microservice architecture to accelerate software\ndelivery. Successful software development also requires DevOps and small,\nautonomous teams.\nDon’t forget about the human side of adopting microservices. You need to con-\nsider employees’ emotions in order to successfully transition to a microservice\narchitecture. \n \n",
      "content_length": 2259,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "33\nDecomposition strategies\nSometimes you have to be careful what you wish for. After an intense lobbying\neffort, Mary had finally convinced the business that migrating to a microservice\narchitecture was the right thing to do. Feeling a mixture of excitement and some\ntrepidation, Mary had a morning-long meeting with her architects to discuss where\nto begin. During the discussion, it became apparent that some aspects of the Micro-\nservice architecture pattern language, such as deployment and service discovery,\nwere new and unfamiliar, yet straightforward. The key challenge, which is the\nessence of the microservice architecture, is the functional decomposition of the\napplication into services. The first and most important aspect of the architecture is,\nThis chapter covers\nUnderstanding software architecture and why it’s \nimportant\nDecomposing an application into services by \napplying the decomposition patterns Decompose \nby business capability and Decompose by \nsubdomain\nUsing the bounded context concept from domain-\ndriven design (DDD) to untangle data and make \ndecomposition easier\n \n",
      "content_length": 1104,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "34\nCHAPTER 2\nDecomposition strategies\ntherefore, the definition of the services. As they stood around the whiteboard, the\nFTGO team wondered exactly how to do that!\n In this chapter, you’ll learn how to define a microservice architecture for an appli-\ncation. I describe strategies for decomposing an application into services. You’ll learn\nthat services are organized around business concerns rather than technical concerns.\nI also show how to use ideas from domain-driven design (DDD) to eliminate god\nclasses, which are classes that are used throughout an application and cause tangled\ndependencies that prevent decomposition.\n I begin this chapter by defining the microservice architecture in terms of software\narchitecture concepts. After that, I describe a process for defining a microservice\narchitecture for an application starting from its requirements. I discuss strategies for\ndecomposing an application into a collection of services, obstacles to it, and how to\novercome them. Let’s start by examining the concept of software architecture.\n2.1\nWhat is the microservice architecture exactly?\nChapter 1 describes how the key idea of the microservice architecture is functional\ndecomposition. Instead of developing one large application, you structure the appli-\ncation as a set of services. On one hand, describing the microservice architecture as a\nkind of functional decomposition is useful. But on the other hand, it leaves several\nquestions unanswered, including how does the microservice architecture relate to the\nbroader concepts of software architecture? What’s a service? And how important is the\nsize of a service?\n In order to answer those questions, we need to take a step back and look at what is\nmeant by software architecture. The architecture of a software application is its high-level\nstructure, which consists of constituent parts and the dependencies between those\nparts. As you’ll see in this section, an application’s architecture is multidimensional, so\nthere are multiple ways to describe it. The reason architecture is important is because\nit determines the application’s software quality attributes or -ilities. Traditionally, the\ngoal of architecture has been scalability, reliability, and security. But today it’s import-\nant that the architecture also enables the rapid and safe delivery of software. You’ll\nlearn that the microservice architecture is an architecture style that gives an applica-\ntion high maintainability, testability, and deployability.\n I begin this section by describing the concept of software architecture and why it’s\nimportant. Next, I discuss the idea of an architectural style. Then I define the micro-\nservice architecture as a particular architectural style. Let’s start by looking at the con-\ncept of software architecture.\n2.1.1\nWhat is software architecture and why does it matter?\nArchitecture is clearly important. There are at least two conferences dedicated to the\ntopic: O’Reilly Software Architecture Conference (https://conferences.oreilly.com/\nsoftware-architecture) and the SATURN conference (https://resources.sei.cmu.edu/\nnews-events/events/saturn/). Many developers have the goal of becoming an archi-\ntect. But what is architecture and why does it matter?\n \n",
      "content_length": 3242,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "35\nWhat is the microservice architecture exactly?\n To answer that question, I first define what is meant by the term software architecture.\nAfter that, I discuss how an application’s architecture is multidimensional and is best\ndescribed using a collection of views or blueprints. I then describe that software archi-\ntecture matters because of its impact on the application’s software quality attributes.\nA DEFINITION OF SOFTWARE ARCHITECTURE\nThere are numerous definitions of software architecture. For example, see https://\nen.wikiquote.org/wiki/Software_architecture to read some of them. My favorite defi-\nnition comes from Len Bass and colleagues at the Software Engineering Institute\n(www.sei.cmu.edu), who played a key role in establishing software architecture as a\ndiscipline. They define software architecture as follows:\nThe software architecture of a computing system is the set of structures needed to reason about\nthe system, which comprise software elements, relations among them, and properties of both.\nDocumenting Software Architectures by Bass et al.\nThat’s obviously a quite abstract definition. But its essence is that an application’s\narchitecture is its decomposition into parts (the elements) and the relationships (the\nrelations) between those parts. Decomposition is important for a couple of reasons:\nIt facilitates the division of labor and knowledge. It enables multiple people (or\nmultiple teams) with possibly specialized knowledge to work productively together\non an application.\nIt defines how the software elements interact.\nIt’s the decomposition into parts and the relationships between those parts that deter-\nmine the application’s -ilities. \nTHE 4+1 VIEW MODEL OF SOFTWARE ARCHITECTURE\nMore concretely, an application’s architecture can be viewed from multiple perspec-\ntives, in the same way that a building’s architecture can be viewed from structural,\nplumbing, electrical, and other perspectives. Phillip Krutchen wrote a classic paper\ndescribing the 4+1 view model of software architecture, “Architectural Blueprints—\nThe ‘4+1’ View Model of Software Architecture” (www.cs.ubc.ca/~gregor/teaching/\npapers/4+1view-architecture.pdf). The 4+1 model, shown in Figure 2.1, defines four\ndifferent views of a software architecture. Each describes a particular aspect of the\narchitecture and consists of a particular set of software elements and relationships\nbetween them.\n The purpose of each view is as follows:\nLogical view—The software elements that are created by developers. In object-\noriented languages, these elements are classes and packages. The relations\nbetween them are the relationships between classes and packages, including\ninheritance, associations, and depends-on.\nImplementation view—The output of the build system. This view consists of mod-\nules, which represent packaged code, and components, which are executable\n \n",
      "content_length": 2882,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "36\nCHAPTER 2\nDecomposition strategies\nor deployable units consisting of one or more modules. In Java, a module is a\nJAR file, and a component is typically a WAR file or an executable JAR file. The\nrelations between them include dependency relationships between modules\nand composition relationships between components and modules.\nProcess view—The components at runtime. Each element is a process, and the\nrelations between processes represent interprocess communication.\nDeployment—How the processes are mapped to machines. The elements in this\nview consist of (physical or virtual) machines and the processes. The relations\nbetween machines represent networking. This view also describes the relation-\nship between processes and machines.\nIn addition to these four views, there are the scenarios—the +1 in the 4+1 model—\nthat animate views. Each scenario describes how the various architectural components\nwithin a particular view collaborate in order to handle a request. A scenario in the log-\nical view, for example, shows how the classes collaborate. Similarly, a scenario in the\nprocess view shows how the processes collaborate.\n The 4+1 view model is an excellent way to describe an applications’s architec-\nture. Each view describes an important aspect of the architecture, and the scenarios\nLogical\nview\nImplementation\nview\nProcess\nview\nDeployment\nview\nScenarios\nWhat developers create\nElements: Classes and packages\nRelations: The relationships\nbetween them\nWhat is produced by the build system\nElements: Modules, (JAR ﬁles) and\ncomponents (WAR ﬁles\nor executables)\nRelations: Their dependencies\nRunning components\nElements: Processes\nRelations: Inter-process\ncommunication\nProcesses running on “machines”\nElements: Machines and processes\nRelations: Networking\nAnimate the views.\nFigure 2.1\nThe 4+1 view model describes an application’s architecture using four views, \nalong with scenarios that show how the elements within each view collaborate to handle \nrequests.\n \n",
      "content_length": 1982,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "37\nWhat is the microservice architecture exactly?\nillustrate how the elements of a view collaborate. Let’s now look at why architecture\nis important. \nWHY ARCHITECTURE MATTERS\nAn application has two categories of requirements. The first category includes the\nfunctional requirements, which define what the application must do. They’re usually\nin the form of use cases or user stories. Architecture has very little to do with the func-\ntional requirements. You can implement functional requirements with almost any\narchitecture, even a big ball of mud.\n Architecture is important because it enables an application to satisfy the second\ncategory of requirements: its quality of service requirements. These are also known as\nquality attributes and are the so-called -ilities. The quality of service requirements\ndefine the runtime qualities such as scalability and reliability. They also define devel-\nopment time qualities including maintainability, testability, and deployability. The\narchitecture you choose for your application determines how well it meets these\nquality requirements. \n2.1.2\nOverview of architectural styles\nIn the physical world, a building’s architecture often follows a particular style, such as\nVictorian, American Craftsman, or Art Deco. Each style is a package of design deci-\nsions that constrains a building’s features and building materials. The concept of\narchitectural style also applies to software. David Garlan and Mary Shaw (An Introduc-\ntion to Software Architecture, January 1994, https://www.cs.cmu.edu/afs/cs/project/\nable/ftp/intro_softarch/intro_softarch.pdf), pioneers in the discipline of software\narchitecture, define an architectural style as follows:\nAn architectural style, then, defines a family of such systems in terms of a pattern of\nstructural organization. More specifically, an architectural style determines the vocabulary\nof components and connectors that can be used in instances of that style, together with a\nset of constraints on how they can be combined.\nA particular architectural style provides a limited palette of elements (components)\nand relations (connectors) from which you can define a view of your application’s\narchitecture. An application typically uses a combination of architectural styles. For\nexample, later in this section I describe how the monolithic architecture is an archi-\ntectural style that structures the implementation view as a single (executable/deploy-\nable) component. The microservice architecture structures an application as a set of\nloosely coupled services.\nTHE LAYERED ARCHITECTURAL STYLE\nThe classic example of an architectural style is the layered architecture. A layered archi-\ntecture organizes software elements into layers. Each layer has a well-defined set of\nresponsibilities. A layered architecture also constraints the dependencies between the\nlayers. A layer can only depend on either the layer immediately below it (if strict layer-\ning) or any of the layers below it.\n \n",
      "content_length": 2979,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "38\nCHAPTER 2\nDecomposition strategies\n You can apply the layered architecture to any of the four views discussed earlier.\nThe popular three-tier architecture is the layered architecture applied to the logical\nview. It organizes the application’s classes into the following tiers or layers:\nPresentation layer—Contains code that implements the user interface or exter-\nnal APIs\nBusiness logic layer—Contains the business logic\nPersistence layer—Implements the logic of interacting with the database\nThe layered architecture is a great example of an architectural style, but it does have\nsome significant drawbacks:\nSingle presentation layer—It doesn’t represent the fact that an application is likely\nto be invoked by more than just a single system.\nSingle persistence layer—It doesn’t represent the fact that an application is likely\nto interact with more than just a single database.\nDefines the business logic layer as depending on the persistence layer—In theory, this\ndependency prevents you from testing the business logic without the database.\nAlso, the layered architecture misrepresents the dependencies in a well-designed\napplication. The business logic typically defines an interface or a repository of inter-\nfaces that define data access methods. The persistence tier defines DAO classes that\nimplement the repository interfaces. In other words, the dependencies are the reverse\nof what’s depicted by a layered architecture.\n Let’s look at an alternative architecture that overcomes these drawbacks: the hex-\nagonal architecture. \nABOUT THE HEXAGONAL ARCHITECTURE STYLE\nHexagonal architecture is an alternative to the layered architectural style. As figure 2.2\nshows, the hexagonal architecture style organizes the logical view in a way that places\nthe business logic at the center. Instead of the presentation layer, the application has\none or more inbound adapters that handle requests from the outside by invoking the\nbusiness logic. Similarly, instead of a data persistence tier, the application has one or\nmore outbound adapters that are invoked by the business logic and invoke external\napplications. A key characteristic and benefit of this architecture is that the business\nlogic doesn’t depend on the adapters. Instead, they depend upon it.\n The business logic has one or more ports. A port defines a set of operations and is\nhow the business logic interacts with what’s outside of it. In Java, for example, a port is\noften a Java interface. There are two kinds of ports: inbound and outbound ports. An\ninbound port is an API exposed by the business logic, which enables it to be invoked\nby external applications. An example of an inbound port is a service interface, which\ndefines a service’s public methods. An outbound port is how the business logic invokes\nexternal systems. An example of an output port is a repository interface, which defines a\ncollection of data access operations.\n \n",
      "content_length": 2918,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "39\nWhat is the microservice architecture exactly?\nSurrounding the business logic are adapters. As with ports, there are two types of\nadapters: inbound and outbound. An inbound adapter handles requests from the out-\nside world by invoking an inbound port. An example of an inbound adapter is a\nSpring MVC Controller that implements either a set of REST endpoints or a set of\nweb pages. Another example is a message broker client that subscribes to messages.\nMultiple inbound adapters can invoke the same inbound port.\n An outbound adapter implements an outbound port and handles requests from\nthe business logic by invoking an external application or service. An example of an\noutbound adapter is a data access object (DAO) class that implements operations for\naccessing a database. Another example would be a proxy class that invokes a remote\nservice. Outbound adapters can also publish events.\n An important benefit of the hexagonal architectural style is that it decouples the\nbusiness logic from the presentation and data access logic in the adapters. The busi-\nness logic doesn’t depend on either the presentation logic or the data access logic.\nBusiness logic\nBrowser\nMessage broker\nOutbound adapter\nOutbound port\nOutbound adapter\nInbound port\nInbound adapter\nInbound adapter\nSome\ncontroller\nclass\nMessage\nconsumer\nMessaging\ninterface\nFoo\nservice\nRepository\ninterface\nDAO\nDatabase\nMessage\nproducer\nFigure 2.2\nAn example of a hexagonal architecture, which consists of the business logic and one or \nmore adapters that communicate with external systems. The business logic has one or more ports. \nInbound adapters, which handled requests from external systems, invoke an inbound port. An \noutbound adapter implements an outbound port, and invokes an external system.\n \n",
      "content_length": 1772,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "40\nCHAPTER 2\nDecomposition strategies\nBecause of this decoupling, it’s much easier to test the business logic in isolation.\nAnother benefit is that it more accurately reflects the architecture of a modern appli-\ncation. The business logic can be invoked via multiple adapters, each of which imple-\nments a particular API or UI. The business logic can also invoke multiple adapters,\neach one of which invokes a different external system. Hexagonal architecture is a\ngreat way to describe the architecture of each service in a microservice architecture.\n The layered and hexagonal architectures are both examples of architectural styles.\nEach defines the building blocks of an architecture and imposes constraints on the\nrelationships between them. The hexagonal architecture and the layered architec-\nture, in the form of a three-tier architecture, organize the logical view. Let’s now\ndefine the microservice architecture as an architectural style that organizes the imple-\nmentation view. \n2.1.3\nThe microservice architecture is an architectural style\nI’ve discussed the 4+1 view model and architectural styles, so I can now define mono-\nlithic and microservice architecture. They’re both architectural styles. Monolithic\narchitecture is an architectural style that structures the implementation view as a sin-\ngle component: a single executable or WAR file. This definition says nothing about\nthe other views. A monolithic application can, for example, have a logical view that’s\norganized along the lines of a hexagonal architecture.\nThe microservice architecture is also an architectural style. It structures the imple-\nmentation view as a set of multiple components: executables or WAR files. The com-\nponents are services, and the connectors are the communication protocols that\nenable those services to collaborate. Each service has its own logical view architecture,\nwhich is typically a hexagonal architecture. Figure 2.3 shows a possible microservice\narchitecture for the FTGO application. The services in this architecture correspond to\nbusiness capabilities, such as Order management and Restaurant management.\nLater in this chapter, I describe what is meant by business capability . The connectors\nbetween services are implemented using interprocess communication mechanisms\nsuch as REST APIs and asynchronous messaging. Chapter 3 discusses interprocess\ncommunication in more detail.\nPattern: Monolithic architecture\nStructure the application as a single executable/deployable component. See http://\nmicroservices.io/patterns/ monolithic.html.\nPattern: Microservice architecture\nStructure the application as a collection of loosely coupled, independently deployable\nservices. See http://microservices.io/patterns/microservices.html.\n \n",
      "content_length": 2748,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "41\nWhat is the microservice architecture exactly?\nA key constraint imposed by the microservice architecture is that the services are\nloosely coupled. Consequently, there are restrictions on how the services collaborate.\nIn order to explain those restrictions, I’ll attempt to define the term service, describe\nwhat it means to be loosely coupled, and tell you why this matters.\nWHAT IS A SERVICE?\nA service is a standalone, independently deployable software component that imple-\nments some useful functionality. Figure 2.4 shows the external view of a service, which in\nthis example is the Order Service. A service has an API that provides its clients access to\nits functionality. There are two types of operations: commands and queries. The API\nconsists of commands, queries, and events. A command, such as createOrder(), per-\nforms actions and updates data. A query, such as findOrderById(), retrieves data. A ser-\nvice also publishes events, such as OrderCreated, which are consumed by its clients.\n A service’s API encapsulates its internal implementation. Unlike in a monolith, a\ndeveloper can’t write code that bypasses its API. As a result, the microservice architec-\nture enforces the application’s modularity.\n Each service in a microservice architecture has its own architecture and, potentially,\ntechnology stack. But a typical service has a hexagonal architecture. Its API is imple-\nmented by adapters that interact with the service’s business logic. The operations\nAmazon\nSES\nAdapter\nTwilio\nAdapter\nStripe\nAdapter\nThe API Gateway routes\nrequests from the mobile\napplications to services.\nServices have APIs.\nA service’s data is private.\nServices corresponding\nto business capabilities/\nDDD subdomains\nAPI\nGateway\nRestaurant\nWeb UI\nOrder\nService\nCourier\nREST\nAPI\nREST\nAPI\nREST\nAPI\nConsumer\nRestaurant\nRestaurant\nService\nREST\nAPI\nAccounting\nService\nREST\nAPI\nNotiﬁcation\nService\nREST\nAPI\nKitchen\nService\nREST\nAPI\nDelivery\nService\nREST\nAPI\nFigure 2.3\nA possible microservice architecture for the FTGO application. It consists of numerous \nservices.\n \n",
      "content_length": 2061,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "42\nCHAPTER 2\nDecomposition strategies\nadapter invokes the business logic, and the events adapter publishes events emitted by\nthe business logic.\n Later in chapter 12, when I discuss deployment technologies, you’ll see that the\nimplementation view of a service can take many forms. The component might be a\nstandalone process, a web application or OSGI bundle running in a container, or a\nserverless cloud function. An essential requirement, however, is that a service has an\nAPI and is independently deployable. \nWHAT IS LOOSE COUPLING?\nAn important characteristic of the microservice architecture is that the services are\nloosely coupled (https://en.wikipedia.org/wiki/Loose_coupling). All interaction with a\nservice happens via its API, which encapsulates its implementation details. This enables\nthe implementation of the service to change without impacting its clients. Loosely\ncoupled services are key to improving an application’s development time attributes,\nincluding its maintainability and testability. They are much easier to understand, change,\nand test.\n The requirement for services to be loosely coupled and to collaborate only via APIs\nprohibits services from communicating via a database. You must treat a service’s\npersistent data like the fields of a class and keep them private. Keeping the data pri-\nvate enables a developer to change their service’s database schema without having to\nOrder Service\nInvokes\nSubscribes to events\nOrder\nService\nclient\nDeﬁnes operations\nPublishes events when data changes\nCommands:\ncreateOrder()\n...\nQueries:\nﬁndOrderbyId()\n...\nOrder\nevent\npublisher\nService API\nOrder created\nOrder cancelled\nFigure 2.4\nA service has an API that encapsulates the implementation. The API defines \noperations, which are invoked by clients. There are two types of operations: commands update \ndata, and queries retrieve data. When its data changes, a service publishes events that clients \ncan subscribe to.\n \n",
      "content_length": 1941,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "43\nWhat is the microservice architecture exactly?\nspend time coordinating with developers working on other services. Not sharing data-\nbase tables also improves runtime isolation. It ensures, for example, that one service\ncan’t hold database locks that block another service. Later on, though, you’ll learn\nthat one downside of not sharing databases is that maintaining data consistency and\nquerying across services are more complex. \nTHE ROLE OF SHARED LIBRARIES\nDevelopers often package functionality in a library (module) so that it can be reused\nby multiple applications without duplicating code. After all, where would we be today\nwithout Maven or npm repositories? You might be tempted to also use shared libraries\nin microservice architecture. On the surface, it looks like a good way to reduce code\nduplication in your services. But you need to ensure that you don’t accidentally intro-\nduce coupling between your services.\n Imagine, for example, that multiple services need to update the Order business\nobject. One approach is to package that functionality as a library that’s used by multi-\nple services. On one hand, using a library eliminates code duplication. On the other\nhand, consider what happens when the requirements change in a way that affects the\nOrder business object. You would need to simultaneously rebuild and redeploy those\nservices. A much better approach would be to implement functionality that’s likely to\nchange, such as Order management, as a service.\n You should strive to use libraries for functionality that’s unlikely to change. For\nexample, in a typical application it makes no sense for every service to implement a\ngeneric Money class. Instead, you should create a library that’s used by the services. \nTHE SIZE OF A SERVICE IS MOSTLY UNIMPORTANT\nOne problem with the term microservice is that the first thing you hear is micro. This\nsuggests that a service should be very small. This is also true of other size-based terms\nsuch as miniservice or nanoservice. In reality, size isn’t a useful metric.\n A much better goal is to define a well-designed service to be a service capable of\nbeing developed by a small team with minimal lead time and with minimal collabora-\ntion with other teams. In theory, a team might only be responsible for a single service,\nso that service is by no means micro. Conversely, if a service requires a large team or\ntakes a long time to test, it probably makes sense to split the team and the service. Or\nif you constantly need to change a service because of changes to other services or if it’s\ntriggering changes in other services, that’s a sign that it’s not loosely coupled. You\nmight even have built a distributed monolith.\n The microservice architecture structures an application as a set of small, loosely\ncoupled services. As a result, it improves the development time attributes—main-\ntainability, testability, deployability, and so on—and enables an organization to\ndevelop better software faster. It also improves an application’s scalability, although\nthat’s not the main goal. To develop a microservice architecture for your application,\nyou need to identify the services and determine how they collaborate. Let’s look at\nhow to do that. \n \n",
      "content_length": 3223,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "44\nCHAPTER 2\nDecomposition strategies\n2.2\nDefining an application’s microservice architecture\nHow should we define a microservice architecture? As with any software development\neffort, the starting points are the written requirements, hopefully domain experts, and\nperhaps an existing application. Like much of software development, defining an\narchitecture is more art than science. This section describes a simple, three-step pro-\ncess, shown in figure 2.5, for defining an application’s architecture. It’s important to\nremember, though, that it’s not a process you can follow mechanically. It’s likely to be\niterative and involve a lot of creativity.\nAn application exists to handle requests, so the first step in defining its architecture is\nto distill the application’s requirements into the key requests. But instead of describing\nthe requests in terms of specific IPC technologies such as REST or messaging, I use\nOrder\nService\nFTGO\nFTGO\nRestaurant\nService\nKitchen\nService\n...\nOrder\nService\nIterate\nverifyOrder()\nRestaurant\nService\nKitchen\nService\nFunctional requirements\ncreateOrder()\ncreateTicket()\nacceptOrder()\ncreateOrder()\nacceptOrder()\nFTGO\nAs a consumer\nI want to place an order\nso that I can ...\ncreateOrder()\nacceptOrder()\nAs a restaurant\nI want to accept an order\nso that I can ...\nStep 1: Identify system operations\nStep 2: Identify services\nStep 3: Deﬁne service APIs and collaborations\nThe starting point are the requirements,\nsuch as the user stories.\nA system operation represents\nan external request.\nFigure 2.5\nA three-step process for defining an application’s microservice architecture\n \n",
      "content_length": 1615,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "45\nDefining an application’s microservice architecture\nthe more abstract notion of system operation. A system operation is an abstraction of a\nrequest that the application must handle. It’s either a command, which updates data,\nor a query, which retrieves data. The behavior of each command is defined in terms\nof an abstract domain model, which is also derived from the requirements. The sys-\ntem operations become the architectural scenarios that illustrate how the services\ncollaborate.\n The second step in the process is to determine the decomposition into services.\nThere are several strategies to choose from. One strategy, which has its origins in the\ndiscipline of business architecture, is to define services corresponding to business\ncapabilities. Another strategy is to organize services around domain-driven design sub-\ndomains. The end result is services that are organized around business concepts\nrather than technical concepts.\n The third step in defining the application’s architecture is to determine each ser-\nvice’s API. To do that, you assign each system operation identified in the first step to a\nservice. A service might implement an operation entirely by itself. Alternatively, it\nmight need to collaborate with other services. In that case, you determine how the ser-\nvices collaborate, which typically requires services to support additional operations.\nYou’ll also need to decide which of the IPC mechanisms I describe in chapter 3 to\nimplement each service’s API.\n There are several obstacles to decomposition. The first is network latency. You\nmight discover that a particular decomposition would be impractical due to too many\nround-trips between services. Another obstacle to decomposition is that synchronous\ncommunication between services reduces availability. You might need to use the con-\ncept of self-contained services, described in chapter 3. The third obstacle is the\nrequirement to maintain data consistency across services. You’ll typically need to use\nsagas, discussed in chapter 4. The fourth and final obstacle to decomposition is so-\ncalled god classes, which are used throughout an application. Fortunately, you can use\nconcepts from domain-driven design to eliminate god classes.\n This section first describes how to identity an application’s operations. After that,\nwe’ll look at strategies and guidelines for decomposing an application into services,\nand at obstacles to decomposition and how to address them. Finally, I’ll describe how\nto define each service’s API.\n2.2.1\nIdentifying the system operations\nThe first step in defining an application’s architecture is to define the system opera-\ntions. The starting point is the application’s requirements, including user stories and\ntheir associated user scenarios (note that these are different from the architectural\nscenarios). The system operations are identified and defined using the two-step pro-\ncess shown in figure 2.6. This process is inspired by the object-oriented design process\ncovered in Craig Larman’s book Applying UML and Patterns (Prentice Hall, 2004) (see\nwww.craiglarman.com/wiki/index.php?title=Book_Applying_UML_and_Patterns for\ndetails). The first step creates the high-level domain model consisting of the key classes\n \n",
      "content_length": 3247,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "46\nCHAPTER 2\nDecomposition strategies\nthat provide a vocabulary with which to describe the system operations. The second\nstep identifies the system operations and describes each one’s behavior in terms of the\ndomain model.\nThe domain model is derived primarily from the nouns of the user stories, and the sys-\ntem operations are derived mostly from the verbs. You could also define the domain\nmodel using a technique called Event Storming, which I talk about in chapter 5.\nThe behavior of each system operation is described in terms of its effect on one or\nmore domain objects and the relationships between them. A system operation can\ncreate, update, or delete domain objects, as well as create or destroy relationships\nbetween them.\n Let’s look at how to define a high-level domain model. After that I’ll define the sys-\ntem operations in terms of the domain model.\nCREATING A HIGH-LEVEL DOMAIN MODEL\nThe first step in the process of defining the system operations is to sketch a high-\nlevel domain model for the application. Note that this domain model is much sim-\npler than what will ultimately be implemented. The application won’t even have a\nsingle domain model because, as you’ll soon learn, each service has its own domain\nmodel. Despite being a drastic simplification, a high-level domain model is useful at\nthis stage because it defines the vocabulary for describing the behavior of the system\noperations.\n A domain model is created using standard techniques such as analyzing the nouns\nin the stories and scenarios and talking to the domain experts. Consider, for example,\nFunctional requirements\nFTGO\nAs a consumer\nI want to place an order\nso that I can ...\ncreateOrder()\nacceptOrder()\nAs a restaurant\nI want to accept an order\nso that I can ...\nStep 2\nHigh-level domain model\nStep 1\nOrder\nMaps to\nSystem operations are deﬁned\nin terms of domain model.\nDomain model\nderived from\nrequirements\nRestaurant\nDelivery\nFigure 2.6\nSystem operations are derived from the application’s requirements using a two-step process. The first \nstep is to create a high-level domain model. The second step is to define the system operations, which are defined \nin terms of the domain model.\n \n",
      "content_length": 2188,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "47\nDefining an application’s microservice architecture\nthe Place Order story. We can expand that story into numerous user scenarios includ-\ning this one:\nGiven a consumer\nAnd a restaurant\nAnd a delivery address/time that can be served by that restaurant\nAnd an order total that meets the restaurant's order minimum\nWhen the consumer places an order for the restaurant\nThen consumer's credit card is authorized\nAnd an order is created in the PENDING_ACCEPTANCE state\nAnd the order is associated with the consumer\nAnd the order is associated with the restaurant\nThe nouns in this user scenario hint at the existence of various classes, including\nConsumer, Order, Restaurant, and CreditCard.\n Similarly, the Accept Order story can be expanded into a scenario such as this one:\nGiven an order that is in the PENDING_ACCEPTANCE state\nand a courier that is available to deliver the order\nWhen a restaurant accepts an order with a promise to prepare by a particular\ntime\nThen the state of the order is changed to ACCEPTED\nAnd the order's promiseByTime is updated to the promised time\nAnd the courier is assigned to deliver the order\nThis scenario suggests the existence of Courier and Delivery classes. The end result\nafter a few iterations of analysis will be a domain model that consists, unsurprisingly,\nof those classes and others, such as MenuItem and Address. Figure 2.7 is a class dia-\ngram that shows the key classes.\nConsumer\nOrder\nstate\n...\ncreditcardId\n...\ndeliveryTime\nquantity\nname\nprice\nstreet1\nstreet2\ncity\nstate\nzip\nname\n...\navailable\n...\nlat\nlon\nRestaurant\nCourier\nLocation\nPaymentInfo\nDeliveryInfo\nOrderLineItem\nMenuItem\nAddress\nPlaced by\nFor\nAssigned to\nPaid using\nPays using\nFigure 2.7\nThe key classes in the FTGO domain model\n \n",
      "content_length": 1742,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "48\nCHAPTER 2\nDecomposition strategies\nThe responsibilities of each class are as follows:\n\nConsumer—A consumer who places orders.\n\nOrder—An order placed by a consumer. It describes the order and tracks its status.\n\nOrderLineItem—A line item of an Order.\n\nDeliveryInfo—The time and place to deliver an order.\n\nRestaurant—A restaurant that prepares orders for delivery to consumers.\n\nMenuItem—An item on the restaurant’s menu.\n\nCourier—A courier who deliver orders to consumers. It tracks the availability of\nthe courier and their current location.\n\nAddress—The address of a Consumer or a Restaurant.\n\nLocation—The latitude and longitude of a Courier.\nA class diagram such as the one in figure 2.7 illustrates one aspect of an application’s\narchitecture. But it isn’t much more than a pretty picture without the scenarios to ani-\nmate it. The next step is to define the system operations, which correspond to archi-\ntectural scenarios. \nDEFINING SYSTEM OPERATIONS\nOnce you’ve defined a high-level domain model, the next step is to identify the requests\nthat the application must handle. The details of the UI are beyond the scope of this\nbook, but you can imagine that in each user scenario, the UI will make requests to the\nbackend business logic to retrieve and update data. FTGO is primarily a web applica-\ntion, which means that most requests are HTTP-based, but it’s possible that some clients\nmight use messaging. Instead of committing to a specific protocol, therefore, it makes\nsense to use the more abstract notion of a system operation to represent requests.\n There are two types of system operations:\nCommands—System operations that create, update, and delete data\nQueries—System operations that read (query) data\nUltimately, these system operations will correspond to REST, RPC, or messaging\nendpoints, but for now thinking of them abstractly is useful. Let’s first identify some\ncommands.\n A good starting point for identifying system commands is to analyze the verbs in the\nuser stories and scenarios. Consider, for example, the Place Order story. It clearly sug-\ngests that the system must provide a Create Order operation. Many other stories individ-\nually map directly to system commands. Table 2.1 lists some of the key system commands.\nTable 2.1\nKey system commands for the FTGO application\nActor\nStory\nCommand\nDescription\nConsumer\nCreate Order\ncreateOrder()\nCreates an order\nRestaurant\nAccept Order\nacceptOrder()\nIndicates that the restaurant has \naccepted the order and is committed \nto preparing it by the indicated time\n \n",
      "content_length": 2553,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "49\nDefining an application’s microservice architecture\nA command has a specification that defines its parameters, return value, and behavior\nin terms of the domain model classes. The behavior specification consists of precondi-\ntions that must be true when the operation is invoked, and post-conditions that are\ntrue after the operation is invoked. Here, for example, is the specification of the\ncreateOrder() system operation:\nThe preconditions mirror the givens in the Place Order user scenario described ear-\nlier. The post-conditions mirror the thens from the scenario. When a system operation\nis invoked it will verify the preconditions and perform the actions required to make\nthe post-conditions true.\n Here’s the specification of the acceptOrder() system operation:\nRestaurant\nOrder Ready \nfor Pickup\nnoteOrderReadyForPickup()\nIndicates that the order is ready for \npickup\nCourier\nUpdate \nLocation\nnoteUpdatedLocation()\nUpdates the current location of the \ncourier\nCourier\nDelivery \npicked up\nnoteDeliveryPickedUp()\nIndicates that the courier has \npicked up the order\nCourier\nDelivery \ndelivered\nnoteDeliveryDelivered()\nIndicates that the courier has deliv-\nered the order\nOperation\ncreateOrder (consumer id, payment method, delivery address, delivery time, \nrestaurant id, order line items)\nReturns\norderId, …\nPreconditions\nThe consumer exists and can place orders.\nThe line items correspond to the restaurant’s menu items.\nThe delivery address and time can be serviced by the restaurant.\nPost-conditions\nThe consumer’s credit card was authorized for the order total.\nAn order was created in the PENDING_ACCEPTANCE state.\nOperation\nacceptOrder(restaurantId, orderId, readyByTime)\nReturns\n—\nPreconditions\nThe order.status is PENDING_ACCEPTANCE.\nA courier is available to deliver the order.\nPost-conditions\nThe order.status was changed to ACCEPTED.\nThe order.readyByTime was changed to the readyByTime.\nThe courier was assigned to deliver the order.\nTable 2.1\nKey system commands for the FTGO application (continued)\nActor\nStory\nCommand\nDescription\n \n",
      "content_length": 2068,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "50\nCHAPTER 2\nDecomposition strategies\nIts pre- and post-conditions mirror the user scenario from earlier.\n Most of the architecturally relevant system operations are commands. Sometimes,\nthough, queries, which retrieve data, are also important.\n Besides implementing commands, an application must also implement queries.\nThe queries provide the UI with the information a user needs to make decisions. At\nthis stage, we don’t have a particular UI design for FTGO application in mind, but\nconsider, for example, the flow when a consumer places an order:\n1\nUser enters delivery address and time.\n2\nSystem displays available restaurants.\n3\nUser selects restaurant.\n4\nSystem displays menu.\n5\nUser selects item and checks out.\n6\nSystem creates order.\nThis user scenario suggests the following queries:\n\nfindAvailableRestaurants(deliveryAddress, deliveryTime)—Retrieves the\nrestaurants that can deliver to the specified delivery address at the specified time\n\nfindRestaurantMenu(id)—Retrieves information about a restaurant including\nthe menu items\nOf the two queries, findAvailableRestaurants() is probably the most architecturally\nsignificant. It’s a complex query involving geosearch. The geosearch component of\nthe query consists of finding all points—restaurants—that are near a location—the\ndelivery address. It also filters out those restaurants that are closed when the order\nneeds to be prepared and picked up. Moreover, performance is critical, because this\nquery is executed whenever a consumer wants to place an order.\n The high-level domain model and the system operations capture what the applica-\ntion does. They help drive the definition of the application’s architecture. The behav-\nior of each system operation is described in terms of the domain model. Each\nimportant system operation represents an architecturally significant scenario that’s\npart of the description of the architecture.\n Once the system operations have been defined, the next step is to identify the\napplication’s services. As mentioned earlier, there isn’t a mechanical process to follow.\nThere are, however, various decomposition strategies that you can use. Each one\nattacks the problem from a different perspective and uses its own terminology. But\nwith all strategies, the end result is the same: an architecture consisting of services that\nare primarily organized around business rather than technical concepts.\n Let’s look at the first strategy, which defines services corresponding to business\ncapabilities. \n \n",
      "content_length": 2501,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "51\nDefining an application’s microservice architecture\n2.2.2\nDefining services by applying the Decompose by business \ncapability pattern\nOne strategy for creating a microservice architecture is to decompose by business\ncapability. A concept from business architecture modeling, a business capability is some-\nthing that a business does in order to generate value. The set of capabilities for a given\nbusiness depends on the kind of business. For example, the capabilities of an insur-\nance company typically include Underwriting, Claims management, Billing, Compliance,\nand so on. The capabilities of an online store include Order management, Inventory\nmanagement, Shipping, and so on.\nBUSINESS CAPABILITIES DEFINE WHAT AN ORGANIZATION DOES\nAn organization’s business capabilities capture what an organization’s business is.\nThey’re generally stable, as opposed to how an organization conducts its business, which\nchanges over time, sometimes dramatically. That’s especially true today, with the rapidly\ngrowing use of technology to automate many business processes. For example, it wasn’t\nthat long ago that you deposited checks at your bank by handing them to a teller. It then\nbecame possible to deposit checks using an ATM. Today you can conveniently deposit\nmost checks using your smartphone. As you can see, the Deposit check business capabil-\nity has remained stable, but the manner in which it’s done has drastically changed. \nIDENTIFYING BUSINESS CAPABILITIES\nAn organization’s business capabilities are identified by analyzing the organization’s\npurpose, structure, and business processes. Each business capability can be thought of\nas a service, except it’s business-oriented rather than technical. Its specification con-\nsists of various components, including inputs, outputs, and service-level agreements.\nFor example, the input to an Insurance underwriting capability is the consumer’s\napplication, and the outputs include approval and price.\n A business capability is often focused on a particular business object. For example,\nthe Claim business object is the focus of the Claim management capability. A capability\ncan often be decomposed into sub-capabilities. For example, the Claim management\ncapability has several sub-capabilities, including Claim information management, Claim\nreview, and Claim payment management.\n It is not difficult to imagine that the business capabilities for FTGO include the\nfollowing:\nSupplier management\n– Courier management—Managing courier information\n– Restaurant information management—Managing restaurant menus and other\ninformation, including location and open hours\nPattern: Decompose by business capability\nDefine services corresponding to business capabilities. See http://microservices.io/\npatterns/decomposition/decompose-by-business-capability.html.\n \n",
      "content_length": 2813,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "52\nCHAPTER 2\nDecomposition strategies\nConsumer management—Managing information about consumers\nOrder taking and fulfillment\n– Order management—Enabling consumers to create and manage orders\n– Restaurant order management—Managing the preparation of orders at a\nrestaurant\n– Logistics\n– Courier availability management—Managing the real-time availability of couri-\ners to delivery orders\n– Delivery management—Delivering orders to consumers\nAccounting\n– Consumer accounting—Managing billing of consumers\n– Restaurant accounting—Managing payments to restaurants\n– Courier accounting—Managing payments to couriers\n…\nThe top-level capabilities include Supplier management, Consumer management,\nOrder taking and fulfillment, and Accounting. There will likely be many other top-\nlevel capabilities, including marketing-related capabilities. Most top-level capabilities\nare decomposed into sub-capabilities. For example, Order taking and fulfillment is\ndecomposed into five sub-capabilities.\n On interesting aspect of this capability hierarchy is that there are three restaurant-\nrelated capabilities: Restaurant information management, Restaurant order manage-\nment, and Restaurant accounting. That’s because they represent three very different\naspects of restaurant operations.\n Next we’ll look at how to use business capabilities to define services. \nFROM BUSINESS CAPABILITIES TO SERVICES\nOnce you’ve identified the business capabilities, you then define a service for each\ncapability or group of related capabilities. Figure 2.8 shows the mapping from capabil-\nities to services for the FTGO application. Some top-level capabilities, such as the\nAccounting capability, are mapped to services. In other cases, sub-capabilities are\nmapped to services.\n The decision of which level of the capability hierarchy to map to services, because\nis somewhat subjective. My justification for this particular mapping is as follows:\nI mapped the sub-capabilities of Supplier management to two services, because\nRestaurants and Couriers are very different types of suppliers.\nI mapped the Order taking and fulfillment capability to three services that are\neach responsible for different phases of the process. I combined the Courier\navailability management and Delivery management capabilities and mapped\nthem to a single service because they’re deeply intertwined.\nI mapped the Accounting capability to its own service, because the different\ntypes of accounting seem similar.\n \n",
      "content_length": 2468,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "53\nDefining an application’s microservice architecture\nLater on, it may make sense to separate payments (of Restaurants and Couriers) and\nbilling (of Consumers).\n A key benefit of organizing services around capabilities is that because they’re sta-\nble, the resulting architecture will also be relatively stable. The individual components\nof the architecture may evolve as the how aspect of the business changes, but the archi-\ntecture remains unchanged.\n Having said that, it’s important to remember that the services shown in figure 2.8\nare merely the first attempt at defining the architecture. They may evolve over time as\nwe learn more about the application domain. In particular, an important step in the\narchitecture definition process is investigating how the services collaborate in each of\nthe key architectural services. You might, for example, discover that a particular\ndecomposition is inefficient due to excessive interprocess communication and that\nyou must combine services. Conversely, a service might grow in complexity to the\nCourier Service\nCourier management\nConsumer management\nSupplier management\nCapability hierarchy\nServices\nCouriers and restaurants\nare very different\nkinds of suppliers\n=> different services.\nThree different services\nhandling different\nphases of the order\ntaking and fulﬁllment\nTreat payments and\nbilling the same for now.\nRestaurant Service\nRestaurant information\nmanagement\nOrder Service\nOrder management\nOrder taking and fulﬁllment\nAccounting\nKitchen Service\nRestaurant order\nticket management\nConsumer Service\nDelivery Service\nConsumer accounting\nRestaurant accounting\nCourier accounting\nAccounting Service\nLogistics\nDelivery management\nCourier availability\nmanagement\nFigure 2.8\nMapping FTGO business capabilities to services. Capabilities at various levels of the \ncapability hierarchy are mapped to services.\n \n",
      "content_length": 1863,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "54\nCHAPTER 2\nDecomposition strategies\npoint where it becomes worthwhile to split it into multiple services. What’s more, in\nsection 2.2.5, I describe several obstacles to decomposition that might cause you to\nrevisit your decision.\n Let’s take a look at another way to decompose an application that is based on\ndomain-driven design. \n2.2.3\nDefining services by applying the Decompose by \nsub-domain pattern\nDDD, as described in the excellent book Domain-driven design by Eric Evans\n(Addison-Wesley Professional, 2003), is an approach for building complex software\napplications that is centered on the development of an object-oriented domain\nmodel. A domain mode captures knowledge about a domain in a form that can be\nused to solve problems within that domain. It defines the vocabulary used by the\nteam, what DDD calls the Ubiquitous Language. The domain model is closely mir-\nrored in the design and implementation of the application. DDD has two concepts\nthat are incredibly useful when applying the microservice architecture: subdomains\nand bounded contexts.\nDDD is quite different than the traditional approach to enterprise modeling, which\ncreates a single model for the entire enterprise. In such a model there would be, for\nexample, a single definition of each business entity, such as customer, order, and so\non. The problem with this kind of modeling is that getting different parts of an orga-\nnization to agree on a single model is a monumental task. Also, it means that from the\nperspective of a given part of the organization, the model is overly complex for their\nneeds. Moreover, the domain model can be confusing because different parts of the\norganization might use either the same term for different concepts or different terms\nfor the same concept. DDD avoids these problems by defining multiple domain mod-\nels, each with an explicit scope.\n DDD defines a separate domain model for each subdomain. A subdomain is a part\nof the domain, DDD’s term for the application’s problem space. Subdomains are iden-\ntified using the same approach as identifying business capabilities: analyze the busi-\nness and identify the different areas of expertise. The end result is very likely to be\nsubdomains that are similar to the business capabilities. The examples of subdomains\nin FTGO include Order taking, Order management, Kitchen management, Delivery,\nand Financials. As you can see, these subdomains are very similar to the business capa-\nbilities described earlier.\nPattern: Decompose by subdomain\nDefine services corresponding to DDD subdomains. See http://microservices.io\n/patterns/decomposition/decompose-by-subdomain.html.\n \n",
      "content_length": 2644,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "55\nDefining an application’s microservice architecture\n DDD calls the scope of a domain model a bounded context. A bounded context\nincludes the code artifacts that implement the model. When using the microservice\narchitecture, each bounded context is a service or possibly a set of services. We can\ncreate a microservice architecture by applying DDD and defining a service for each\nsubdomain. Figure 2.9 shows how the subdomains map to services, each with its own\ndomain model.\nDDD and the microservice architecture are in almost perfect alignment. The DDD\nconcept of subdomains and bounded contexts maps nicely to services within a micro-\nservice architecture. Also, the microservice architecture’s concept of autonomous\nteams owning services is completely aligned with the DDD’s concept of each domain\nmodel being owned and developed by a single team. Even better, as I describe later in\nthis section, the concept of a subdomain with its own domain model is a great way to\neliminate god classes and thereby make decomposition easier.\n Decompose by subdomain and Decompose by business capability are the two main\npatterns for defining an application’s microservice architecture. There are, however,\nsome useful guidelines for decomposition that have their roots in object-oriented\ndesign. Let’s take a look at them. \nAccounting Service\nAccounting\ndomain model\nKitchen Service\n.... Service\nOrder taking\nsubdomain\nMaps to\nMaps to\nMaps to\nMaps to\nMaps to\nKitchen\nsubdomain\nAccounting\nsubdomain\nDelivery\nsubdomain\n....\nsubdomain\nKitchen\ndomain model\nDelivery Service\nDelivery\ndomain model\nOrder Service\nFTGO domain\nOrder\ndomain model\nFigure 2.9\nFrom subdomains to services: each subdomain of the FTGO application domain \nis mapped to a service, which has its own domain model.\n \n",
      "content_length": 1776,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "56\nCHAPTER 2\nDecomposition strategies\n2.2.4\nDecomposition guidelines\nSo far in this chapter, we’ve looked at the main ways to define a microservice architec-\nture. We can also adapt and use a couple of principles from object-oriented design\nwhen applying the microservice architecture pattern. These principles were created\nby Robert C. Martin and described in his classic book Designing Object Oriented C++\nApplications Using The Booch Method (Prentice Hall, 1995). The first principle is the Sin-\ngle Responsibility Principle (SRP), for defining the responsibilities of a class. The sec-\nond principle is the Common Closure Principle (CCP), for organizing classes into\npackages. Let’s take a look at these principles and see how they can be applied to the\nmicroservice architecture.\nSINGLE RESPONSIBILITY PRINCIPLE\nOne of the main goals of software architecture and design is determining the respon-\nsibilities of each software element. The Single Responsibility Principle is as follows:\nA class should have only one reason to change.\n                                                                      \nRobert C. Martin\nEach responsibility that a class has is a potential reason for that class to change. If a\nclass has multiple responsibilities that change independently, the class won’t be stable.\nBy following the SRP, you define classes that each have a single responsibility and\nhence a single reason for change.\n We can apply SRP when defining a microservice architecture and create small,\ncohesive services that each have a single responsibility. This will reduce the size of the\nservices and increase their stability. The new FTGO architecture is an example of SRP\nin action. Each aspect of getting food to a consumer—order taking, order prepara-\ntion, and delivery—is the responsibility of a separate service. \nCOMMON CLOSURE PRINCIPLE\nThe other useful principle is the Common Closure Principle:\nThe classes in a package should be closed together against the same kinds of changes. A\nchange that affects a package affects all the classes in that package.\nRobert C. Martin\nThe idea is that if two classes change in lockstep because of the same underlying rea-\nson, then they belong in the same package. Perhaps, for example, those classes imple-\nment a different aspect of a particular business rule. The goal is that when that\nbusiness rule changes, developers only need to change code in a small number of\npackages (ideally only one). Adhering to the CCP significantly improves the maintain-\nability of an application.\n We can apply CCP when creating a microservice architecture and package compo-\nnents that change for the same reason into the same service. Doing this will minimize\n \n",
      "content_length": 2701,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "57\nDefining an application’s microservice architecture\nthe number of services that need to be changed and deployed when some require-\nment changes. Ideally, a change will only affect a single team and a single service. CCP\nis the antidote to the distributed monolith anti-pattern.\n SRP and CCP are 2 of the 11 principles developed by Bob Martin. They’re particu-\nlarly useful when developing a microservice architecture. The remaining nine princi-\nples are used when designing classes and packages. For more information about SRP,\nCCP, and the other OOD principles, see the article “The Principles of Object Ori-\nented Design” on Bob Martin’s website (http://butunclebob.com/ArticleS.UncleBob\n.PrinciplesOfOod).\n Decomposition by business capability and by subdomain along with SRP and CCP\nare good techniques for decomposing an application into services. In order to apply\nthem and successfully develop a microservice architecture, you must solve some trans-\naction management and interprocess communication issues. \n2.2.5\nObstacles to decomposing an application into services\nOn the surface, the strategy of creating a microservice architecture by defining ser-\nvices corresponding to business capabilities or subdomains looks straightforward. You\nmay, however, encounter several obstacles:\nNetwork latency\nReduced availability due to synchronous communication\nMaintaining data consistency across services\nObtaining a consistent view of the data\nGod classes preventing decomposition\nLet’s take a look at each obstacle, starting with network latency.\nNETWORK LATENCY\nNetwork latency is an ever-present concern in a distributed system. You might discover\nthat a particular decomposition into services results in a large number of round-trips\nbetween two services. Sometimes, you can reduce the latency to an acceptable amount\nby implementing a batch API for fetching multiple objects in a single round trip. But\nin other situations, the solution is to combine services, replacing expensive IPC with\nlanguage-level method or function calls. \nSYNCHRONOUS INTERPROCESS COMMUNICATION REDUCES AVAILABILITY\nAnother problem is how to implement interservice communication in a way that\ndoesn’t reduce availability. For example, the most straightforward way to implement\nthe createOrder() operation is for the Order Service to synchronously invoke the\nother services using REST. The drawback of using a protocol like REST is that it\nreduces the availability of the Order Service. It won’t be able to create an order if any\nof those other services are unavailable. Sometimes this is a worthwhile trade-off, but in\nchapter 3 you’ll learn that using asynchronous messaging, which eliminates tight cou-\npling and improves availability, is often a better choice. \n \n",
      "content_length": 2757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "58\nCHAPTER 2\nDecomposition strategies\nMAINTAINING DATA CONSISTENCY ACROSS SERVICES\nAnother challenge is maintaining data consistency across services. Some system opera-\ntions need to update data in multiple services. For example, when a restaurant accepts\nan order, updates must occur in both the Kitchen Service and the Delivery Service.\nThe Kitchen Service changes the status of the Ticket. The Delivery Service sched-\nules delivery of the order. Both of these updates must be done atomically.\n The traditional solution is to use a two-phase, commit-based, distributed trans-\naction management mechanism. But as you’ll see in chapter 4, this is not a good\nchoice for modern applications, and you must use a very different approach to trans-\naction management, a saga. A saga is a sequence of local transactions that are coordi-\nnated using messaging. Sagas are more complex than traditional ACID transactions\nbut they work well in many situations. One limitation of sagas is that they are eventu-\nally consistent. If you need to update some data atomically, then it must reside within\na single service, which can be an obstacle to decomposition. \nOBTAINING A CONSISTENT VIEW OF THE DATA\nAnother obstacle to decomposition is the inability to obtain a truly consistent view of\ndata across multiple databases. In a monolithic application, the properties of ACID\ntransactions guarantee that a query will return a consistent view of the database. In\ncontrast, in a microservice architecture, even though each service’s database is consis-\ntent, you can’t obtain a globally consistent view of the data. If you need a consistent\nview of some data, then it must reside in a single service, which can prevent decompo-\nsition. Fortunately, in practice this is rarely a problem. \nGOD CLASSES PREVENT DECOMPOSITION\nAnother obstacle to decomposition is the existence of so-called god classes. God classes\nare the bloated classes that are used throughout an application (http://wiki.c2.com/\n?GodClass). A god class typically implements business logic for many different aspects\nof the application. It normally has a large number of fields mapped to a database\ntable with many columns. Most applications have at least one of these classes, each\nrepresenting a concept that’s central to the domain: accounts in banking, orders in\ne-commerce, policies in insurance, and so on. Because a god class bundles together\nstate and behavior for many different aspects of an application, it’s an insurmountable\nobstacle to splitting any business logic that uses it into services.\n The Order class is a great example of a god class in the FTGO application. That’s\nnot surprising—after all, the purpose of FTGO is to deliver food orders to customers.\nMost parts of the system involve orders. If the FTGO application had a single domain\nmodel, the Order class would be a very large class. It would have state and behavior\ncorresponding to many different parts of the application. Figure 2.10 shows the struc-\nture of this class that would be created using traditional modeling techniques.\n As you can see, the Order class has fields and methods corresponding to order pro-\ncessing, restaurant order management, delivery, and payments. This class also has a\ncomplex state model, due to the fact that one model has to describe state transitions\n \n",
      "content_length": 3318,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "59\nDefining an application’s microservice architecture\nfrom disparate parts of the application. In its current form, this class makes it extremely\ndifficult to split code into services.\n One solution is to package the Order class into a library and create a central Order\ndatabase. All services that process orders use this library and access the access data-\nbase. The trouble with this approach is that it violates one of the key principles of the\nmicroservice architecture and results in undesirable, tight coupling. For example, any\nchange to the Order schema requires the teams to update their code in lockstep.\n Another solution is to encapsulate the Order database in an Order Service, which\nis invoked by the other services to retrieve and update orders. The problem with that\ndesign is that the Order Service would be a data service with an anemic domain\nmodel containing little or no business logic. Neither of these options is appealing, but\nfortunately, DDD provides a solution.\n A much better approach is to apply DDD and treat each service as a separate sub-\ndomain with its own domain model. This means that each of the services in the FTGO\napplication that has anything to do with orders has its own domain model with its\nversion of the Order class. A great example of the benefit of multiple domain mod-\nels is the Delivery Service. Its view of an Order, shown in figure 2.11, is extremely\nsimple: pickup address, pickup time, delivery address, and delivery time. Moreover,\nrather than call it an Order, the Delivery Service uses the more appropriate name of\nDelivery.\nOrder\nOrderLineItem\nAddress\nCourier\nConsumer\nRestaurant\nPaymentInfo\nOrderTotal\ndeliveryTime\nstatus\n<<delivery>>\npickupTime\n<<billing>>\ntransactionid\n<<orderTaking>>\ncreate()\ncancel()\n<<restaurant>>\naccept()\nreject()\nnoteReadyForPickup()\n<<delivery>>\nassignCourier()\nnotePickedUp()\nnoteDelivered()\nFigure 2.10\nThe Order god class is bloated with numerous responsibilities.\n \n",
      "content_length": 1960,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "60\nCHAPTER 2\nDecomposition strategies\nThe Delivery Service isn’t interested in any of the other attributes of an order.\n The Kitchen Service also has a much simpler view of an order. Its version of an\nOrder is called a Ticket. As figure 2.12 shows, a Ticket simply consist of a status, the\nrequestedDeliveryTime, a prepareByTime, and a list of line items that tell the\nrestaurant what to prepare. It’s unconcerned with the consumer, payment, delivery,\nand so on.\nThe Order service has the most complex view of an order, shown in figure 2.13. Even\nthough it has quite a few fields and methods, it’s still much simpler than the original\nversion.\nThe Order class in each domain model represents different aspects of the same Order\nbusiness entity. The FTGO application must maintain consistency between these differ-\nent objects in different services. For example, once the Order Service has authorized\nDelivery\nAddress\nPickup location\nDelivery location\nAssigned to\nCourier\nstatus\nscheduledPickupTime\nScheduledDeliveryTime\nFigure 2.11\nThe Delivery Service domain model\nTicket\nstatus\nrequestedDeliveryTime\npreparedByTime\nTicketLineItem\nquantity\nitem\nFigure 2.12\nThe Kitchen Service domain model\nOrder\nOrderLineItem\nAddress\nConsumer\nRestaurant\nPaymentInfo\nstatus\norderTotal\ndeliveryTime\n...\nFigure 2.13\nThe Order Service domain model\n \n",
      "content_length": 1331,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "61\nDefining an application’s microservice architecture\nthe consumer’s credit card, it must trigger the creation of the Ticket in the Kitchen\nService. Similarly, if the restaurant rejects the order via the Kitchen Service, it must\nbe cancelled in the Order Service service, and the customer credited in the billing\nservice. In chapter 4, you’ll learn how to maintain consistency between services, using\nthe previously mentioned event-driven mechanism sagas.\n As well as creating technical challenges, having multiple domain models also\nimpacts the implementation of the user experience. An application must translate\nbetween the user experience, which is its own domain model, and the domain models\nof each of the services. In the FTGO application, for example, the Order status dis-\nplayed to a consumer is derived from Order information stored in multiple services.\nThis translation is often handled by the API gateway, discussed in chapter 8. Despite\nthese challenges, it’s essential that you identify and eliminate god classes when defin-\ning a microservice architecture.\n We’ll now look at how to define the service APIs. \n2.2.6\nDefining service APIs\nSo far, we have a list of system operations and a list of a potential services. The next\nstep is to define each service’s API: its operations and events. A service API operation\nexists for one of two reasons: some operations correspond to system operations. They\nare invoked by external clients and perhaps by other services. The other operations\nexist to support collaboration between services. These operations are only invoked by\nother services.\n A service publishes events primarily to enable it to collaborate with other ser-\nvices. Chapter 4 describes how events can be used to implement sagas, which main-\ntain data consistency across services. And chapter 7 discusses how events can be used\nto update CQRS views, which support efficient querying. An application can also use\nevents to notify external clients. For example, it could use WebSockets to deliver\nevents to a browser.\n The starting point for defining the service APIs is to map each system operation to\na service. After that, we decide whether a service needs to collaborate with others to\nimplement a system operation. If collaboration is required, we then determine what\nAPIs those other services must provide in order to support the collaboration. Let’s\nbegin by looking at how to assign system operations to services.\nASSIGNING SYSTEM OPERATIONS TO SERVICES\nThe first step is to decide which service is the initial entry point for a request. Many\nsystem operations neatly map to a service, but sometimes the mapping is less obvious.\nConsider, for example, the noteUpdatedLocation() operation, which updates the\ncourier location. On one hand, because it’s related to couriers, this operation should\nbe assigned to the Courier service. On the other hand, it’s the Delivery Service\nthat needs the courier location. In this case, assigning an operation to a service that\nneeds the information provided by the operation is a better choice. In other situations,\n \n",
      "content_length": 3086,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "62\nCHAPTER 2\nDecomposition strategies\nit might make sense to assign an operation to the service that has the information nec-\nessary to handle it.\n Table 2.2 shows which services in the FTGO application are responsible for which\noperations.\nAfter having assigned operations to services, the next step is to decide how the services\ncollaborate in order to handle each system operation. \nDETERMINING THE APIS REQUIRED TO SUPPORT COLLABORATION BETWEEN SERVICES\nSome system operations are handled entirely by a single service. For example, in the\nFTGO application, the Consumer Service handles the createConsumer() operation\nentirely by itself. But other system operations span multiple services. The data needed\nto handle one of these requests might, for instance, be scattered around multiple ser-\nvices. For example, in order to implement the createOrder() operation, the Order\nService must invoke the following services in order to verify its preconditions and\nmake the post-conditions become true:\n\nConsumer Service—Verify that the consumer can place an order and obtain their\npayment information.\n\nRestaurant Service—Validate the order line items, verify that the delivery\naddress/time is within the restaurant’s service area, verify order minimum is\nmet, and obtain prices for the order line items.\n\nKitchen Service—Create the Ticket.\n\nAccounting Service—Authorize the consumer’s credit card.\nSimilarly, in order to implement the acceptOrder() system operation, the Kitchen\nService must invoke the Delivery Service to schedule a courier to deliver the order.\nTable 2.3 shows the services, their revised APIs, and their collaborators. In order to\nfully define the service APIs, you need to analyze each system operation and deter-\nmine what collaboration is required.\nTable 2.2\nMapping system operations to services in the FTGO application\nService\nOperations\nConsumer Service\ncreateConsumer()\nOrder Service\ncreateOrder()\nRestaurant Service\nfindAvailableRestaurants()\nKitchen Service\nacceptOrder()\nnoteOrderReadyForPickup()\nDelivery Service\nnoteUpdatedLocation()\nnoteDeliveryPickedUp()\nnoteDeliveryDelivered()\n \n",
      "content_length": 2122,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "63\nDefining an application’s microservice architecture\nSo far, we’ve identified the services and the operations that each service implements.\nBut it’s important to remember that the architecture we’ve sketched out is very\nabstract. We’ve not selected any specific IPC technology. Moreover, even though the\nterm operation suggests some kind of synchronous request/response-based IPC mecha-\nnism, you’ll see that asynchronous messaging plays a significant role. Throughout this\nbook I describe architecture and design concepts that influence how these services\ncollaborate.\n Chapter 3 describes specific IPC technologies, including synchronous communica-\ntion mechanisms such as REST, and asynchronous messaging using a message broker.\nI discuss how synchronous communication can impact availability and introduce the\nconcept of a self-contained service, which doesn’t invoke other services synchronously.\nOne way to implement a self-contained service is to use the CQRS pattern, covered in\nchapter 7. The Order Service could, for example, maintain a replica of the data owned\nby the Restaurant Service in order to eliminate the need for it to synchronously\ninvoke the Restaurant Service to validate an order. It keeps the replica up-to-date by\nsubscribing to events published by the Restaurant Service whenever it updates\nits data.\n Chapter 4 introduces the saga concept and how it uses asynchronous messaging\nfor coordinating the services that participate in the saga. As well as reliably updating\nTable 2.3\nThe services, their revised APIs, and their collaborators\nService\nOperations\nCollaborators\nConsumer Service\nverifyConsumerDetails()\n—\nOrder Service\ncreateOrder()\nConsumer Service\nverifyConsumerDetails()\nRestaurant Service\nverifyOrderDetails()\nKitchen Service\ncreateTicket()\nAccounting Service\nauthorizeCard()\nRestaurant \nService\nfindAvailableRestaurants()\nverifyOrderDetails()\n—\nKitchen Service\ncreateTicket()\nacceptOrder()\nnoteOrderReadyForPickup()\nDelivery Service\nscheduleDelivery()\nDelivery Service\nscheduleDelivery()\nnoteUpdatedLocation()\nnoteDeliveryPickedUp()\nnoteDeliveryDelivered()\n—\nAccounting \nService\nauthorizeCard()\n—\n \n",
      "content_length": 2156,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "64\nCHAPTER 2\nDecomposition strategies\ndata scattered across multiple services, a saga is also a way to implement a self-contained\nservice. For example, I describe how the createOrder() operation is implemented\nusing a saga, which invokes services such as the Consumer Service, Kitchen Service,\nand Accounting Service using asynchronous messaging.\n Chapter 8 describes the concept of an API gateway, which exposes an API to exter-\nnal clients. An API gateway might implement a query operation using the API compo-\nsition pattern, described in chapter 7, rather than simply route it to the service. Logic\nin the API gateway gathers the data needed by the query by calling multiple services\nand combining the results. In this situation, the system operation is assigned to the\nAPI gateway rather than a service. The services need to implement the query opera-\ntions needed by the API gateway. \nSummary\nArchitecture determines your application’s -ilities, including maintainability,\ntestability, and deployability, which directly impact development velocity.\nThe microservice architecture is an architecture style that gives an application\nhigh maintainability, testability, and deployability.\nServices in a microservice architecture are organized around business concerns—\nbusiness capabilities or subdomains—rather than technical concerns.\nThere are two patterns for decomposition:\n– Decompose by business capability, which has its origins in business archi-\ntecture\n– Decompose by subdomain, based on concepts from domain-driven design\nYou can eliminate god classes, which cause tangled dependencies that prevent\ndecomposition, by applying DDD and defining a separate domain model for\neach service. \n \n",
      "content_length": 1706,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "65\nInterprocess\ncommunication in\na microservice architecture\nMary and her team, like most other developers, had some experience with inter-\nprocess communication (IPC) mechanisms. The FTGO application has a REST API\nthat’s used by mobile applications and browser-side JavaScript. It also uses various\nThis chapter covers\nApplying the communication patterns: Remote \nprocedure invocation, Circuit breaker, Client-side \ndiscovery, Self registration, Server-side discovery, \nThird party registration, Asynchronous messaging, \nTransactional outbox, Transaction log tailing, \nPolling publisher\nThe importance of interprocess communication in \na microservice architecture\nDefining and evolving APIs\nThe various interprocess communication options \nand their trade-offs\nThe benefits of services that communicate using \nasynchronous messaging\nReliably sending messages as part of a database \ntransaction\n \n",
      "content_length": 903,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "66\nCHAPTER 3\nInterprocess communication in a microservice architecture\ncloud services, such as the Twilio messaging service and the Stripe payment service.\nBut within a monolithic application like FTGO, modules invoke one another via\nlanguage-level method or function calls. FTGO developers generally don’t need to\nthink about IPC unless they’re working on the REST API or the modules that inte-\ngrate with cloud services.\n In contrast, as you saw in chapter 2, the microservice architecture structures an\napplication as a set of services. Those services must often collaborate in order to han-\ndle a request. Because service instances are typically processes running on multiple\nmachines, they must interact using IPC. It plays a much more important role in a\nmicroservice architecture than it does in a monolithic application. Consequently, as\nthey migrate their application to microservices, Mary and the rest of the FTGO devel-\nopers will need to spend a lot more time thinking about IPC.\n There’s no shortage of IPC mechanisms to chose from. Today, the fashionable\nchoice is REST (with JSON). It’s important, though, to remember that there are no\nsilver bullets. You must carefully consider the options. This chapter explores various\nIPC options, including REST and messaging, and discusses the trade-offs.\n The choice of IPC mechanism is an important architectural decision. It can impact\napplication availability. What’s more, as I explain in this chapter and the next, IPC\neven intersects with transaction management. I favor an architecture consisting of\nloosely coupled services that communicate with one another using asynchronous mes-\nsaging. Synchronous protocols such as REST are used mostly to communicate with\nother applications.\n I begin this chapter with an overview of interprocess communication in micro-\nservice architecture. Next, I describe remote procedure invocation-based IPC, of which\nREST is the most popular example. I cover important topics including service discov-\nery and how to handle partial failure. After that, I describe asynchronous messaging-\nbased IPC. I also talk about scaling consumers while preserving message ordering,\ncorrectly handling duplicate messages, and transactional messaging. Finally, I go\nthrough the concept of self-contained services that handle synchronous requests with-\nout communicating with other services in order to improve availability.\n3.1\nOverview of interprocess communication in a \nmicroservice architecture\nThere are lots of different IPC technologies to choose from. Services can use\nsynchronous request/response-based communication mechanisms, such as HTTP-\nbased REST or gRPC. Alternatively, they can use asynchronous, message-based com-\nmunication mechanisms such as AMQP or STOMP. There are also a variety of differ-\nent messages formats. Services can use human-readable, text-based formats such as JSON\nor XML. Alternatively, they could use a more efficient binary format such as Avro or\nProtocol Buffers.\n Before getting into the details of specific technologies, I want to bring up several\ndesign issues you should consider. I start this section with a discussion of interaction\n \n",
      "content_length": 3162,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "67\nOverview of interprocess communication in a microservice architecture\nstyles, which are a technology-independent way of describing how clients and services\ninteract. Next I discuss the importance of precisely defining APIs in a microservice\narchitecture, including the concept of API-first design. After that, I discuss the\nimportant topic of API evolution. Finally, I discuss different options for message for-\nmats and how they can determine ease of API evolution. Let’s begin by looking at\ninteraction styles.\n3.1.1\nInteraction styles\nIt’s useful to first think about the style of interaction between a service and its clients\nbefore selecting an IPC mechanism for a service’s API. Thinking first about the inter-\naction style will help you focus on the requirements and avoid getting mired in the\ndetails of a particular IPC technology. Also, as described in section 3.4, the choice of\ninteraction style impacts the availability of your application. Furthermore, as you’ll see\nin chapters 9 and 10, it helps you select the appropriate integration testing strategy.\n There are a variety of client-service interaction styles. As table 3.1 shows, they can\nbe categorized in two dimensions. The first dimension is whether the interaction is\none-to-one or one-to-many:\nOne-to-one—Each client request is processed by exactly one service.\nOne-to-many—Each request is processed by multiple services.\nThe second dimension is whether the interaction is synchronous or asynchronous:\nSynchronous—The client expects a timely response from the service and might\neven block while it waits.\nAsynchronous—The client doesn’t block, and the response, if any, isn’t necessar-\nily sent immediately.\nThe following are the different types of one-to-one interactions:\nRequest/response—A service client makes a request to a service and waits for a\nresponse. The client expects the response to arrive in a timely fashion. It might\nevent block while waiting. This is an interaction style that generally results in\nservices being tightly coupled.\nAsynchronous request/response—A service client sends a request to a service, which\nreplies asynchronously. The client doesn’t block while waiting, because the ser-\nvice might not send the response for a long time.\nTable 3.1\nThe various interaction styles can be characterized in two dimensions: one-to-one vs one-to-\nmany and synchronous vs asynchronous.\none-to-one\none-to-many\nSynchronous\nRequest/response\n—\nAsynchronous\nAsynchronous request/response\nOne-way notifications\nPublish/subscribe\nPublish/async responses\n \n",
      "content_length": 2550,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "68\nCHAPTER 3\nInterprocess communication in a microservice architecture\nOne-way notifications—A service client sends a request to a service, but no reply\nis expected or sent.\nIt’s important to remember that the synchronous request/response interaction style is\nmostly orthogonal to IPC technologies. A service can, for example, interact with\nanother service using request/response style interaction with either REST or messag-\ning. Even if two services are communicating using a message broker, the client service\nmight be blocked waiting for a response. It doesn’t necessarily mean they’re loosely\ncoupled. That’s something I revisit later in this chapter when discussing the impact of\ninter-service communication on availability.\n The following are the different types of one-to-many interactions:\nPublish/subscribe—A client publishes a notification message, which is consumed\nby zero or more interested services.\nPublish/async responses—A client publishes a request message and then waits for\na certain amount of time for responses from interested services.\nEach service will typically use a combination of these interaction styles. Many of the\nservices in the FTGO application have both synchronous and asynchronous APIs for\noperations, and many also publish events.\n Let’s look at how to define a service’s API. \n3.1.2\nDefining APIs in a microservice architecture\nAPIs or interfaces are central to software development. An application is comprised of\nmodules. Each module has an interface that defines the set of operations that mod-\nule’s clients can invoke. A well-designed interface exposes useful functionality while\nhiding the implementation. It enables the implementation to change without impact-\ning clients.\n In a monolithic application, an interface is typically specified using a program-\nming language construct such as a Java interface. A Java interface specifies a set of\nmethods that a client can invoke. The implementation class is hidden from the client.\nMoreover, because Java is a statically typed language, if the interface changes to be\nincompatible with the client, the application won’t compile.\n APIs and interfaces are equally important in a microservice architecture. A ser-\nvice’s API is a contract between the service and its clients. As described in chapter 2, a\nservice’s API consists of operations, which clients can invoke, and events, which are\npublished by the service. An operation has a name, parameters, and a return type. An\nevent has a type and a set of fields and is, as described in section 3.3, published to a\nmessage channel.\n The challenge is that a service API isn’t defined using a simple programming lan-\nguage construct. By definition, a service and its clients aren’t compiled together. If a\nnew version of a service is deployed with an incompatible API, there’s no compilation\nerror. Instead, there will be runtime failures.\n \n",
      "content_length": 2884,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "69\nOverview of interprocess communication in a microservice architecture\n Regardless of which IPC mechanism you choose, it’s important to precisely define\na service’s API using some kind of interface definition language (IDL). Moreover, there\nare good arguments for using an API-first approach to defining services (see www\n.programmableweb.com/news/how-to-design-great-apis-api-first-design-and-raml/how-to/\n2015/07/10 for more). First you write the interface definition. Then you review the\ninterface definition with the client developers. Only after iterating on the API defini-\ntion do you then implement the service. Doing this up-front design increases your\nchances of building a service that meets the needs of its clients.\nThe nature of the API definition depends on which IPC mechanism you’re using. For\nexample, if you’re using messaging, the API consists of the message channels, the mes-\nsage types, and the message formats. If you’re using HTTP, the API consists of the\nURLs, the HTTP verbs, and the request and response formats. Later in this chapter,\nI explain how to define APIs.\n A service’s API is rarely set in stone. It will likely evolve over time. Let’s take a look\nat how to do that and consider the issues you’ll face. \n3.1.3\nEvolving APIs\nAPIs invariably change over time as new features are added, existing features are\nchanged, and (perhaps) old features are removed. In a monolithic application, it’s rel-\natively straightforward to change an API and update all the callers. If you’re using a\nstatically typed language, the compiler helps by giving a list of compilation errors. The\nonly challenge may be the scope of the change. It might take a long time to change a\nwidely used API.\n In a microservices-based application, changing a service’s API is a lot more diffi-\ncult. A service’s clients are other services, which are often developed by other teams.\nThe clients may even be other applications outside of the organization. You usually\ncan’t force all clients to upgrade in lockstep with the service. Also, because modern\napplications are usually never down for maintenance, you’ll typically perform a rolling\nupgrade of your service, so both old and new versions of a service will be running\nsimultaneously.\n It’s important to have a strategy for dealing with these challenges. How you handle\na change to an API depends on the nature of the change.\nAPI-first design is essential\nEven in small projects, I’ve seen problems occur because components don’t agree\non an API. For example, on one project the backend Java developer and the AngularJS\nfrontend developer both said they had completed development. The application, how-\never, didn’t work. The REST and WebSocket API used by the frontend application to\ncommunicate with the backend was poorly defined. As a result, the two applications\ncouldn’t communicate!\n \n",
      "content_length": 2850,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "70\nCHAPTER 3\nInterprocess communication in a microservice architecture\nUSE SEMANTIC VERSIONING\nThe Semantic Versioning specification (http://semver.org) is a useful guide to ver-\nsioning APIs. It’s a set of rules that specify how version numbers are used and incre-\nmented. Semantic versioning was originally intended to be used for versioning of\nsoftware packages, but you can use it for versioning APIs in a distributed system.\n The Semantic Versioning specification (Semvers) requires a version number to\nconsist of three parts: MAJOR.MINOR.PATCH. You must increment each part of a version\nnumber as follows:\n\nMAJOR—When you make an incompatible change to the API\n\nMINOR—When you make backward-compatible enhancements to the API\n\nPATCH—When you make a backward-compatible bug fix\nThere are a couple of places you can use the version number in an API. If you’re\nimplementing a REST API, you can, as mentioned below, use the major version as\nthe first element of the URL path. Alternatively, if you’re implementing a service\nthat uses messaging, you can include the version number in the messages that it\npublishes. The goal is to properly version APIs and to evolve them in a controlled\nfashion. Let’s look at how to handle minor and major changes. \nMAKING MINOR, BACKWARD-COMPATIBLE CHANGES\nIdeally, you should strive to only make backward-compatible changes. Backward-\ncompatible changes are additive changes to an API:\nAdding optional attributes to request\nAdding attributes to a response\nAdding new operations\nIf you only ever make these kinds of changes, older clients will work with newer services,\nprovided that they observe the Robustness principle (https://en.wikipedia.org/wiki/\nRobustness_principle), which states: “Be conservative in what you do, be liberal in\nwhat you accept from others.” Services should provide default values for missing\nrequest attributes. Similarly, clients should ignore any extra response attributes. In\norder for this to be painless, clients and services must use a request and response for-\nmat that supports the Robustness principle. Later in this section, I describe how text-\nbased formats such as JSON and XML generally make it easier to evolve APIs. \nMAKING MAJOR, BREAKING CHANGES\nSometimes you must make major, incompatible changes to an API. Because you can’t\nforce clients to upgrade immediately, a service must simultaneously support old and\nnew versions of an API for some period of time. If you’re using an HTTP-based IPC\nmechanism, such as REST, one approach is to embed the major version number in the\nURL. For example, version 1 paths are prefixed with '/v1/…', and version 2 paths\nwith '/v2/…'.\n \n",
      "content_length": 2660,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "71\nOverview of interprocess communication in a microservice architecture\n Another option is to use HTTP’s content negotiation mechanism and include the\nversion number in the MIME type. For example, a client would request version 1.x of\nan Order using a request like this:\nGET /orders/xyz HTTP/1.1\nAccept: application/vnd.example.resource+json; version=1\n...\nThis request tells the Order Service that the client expects a version 1.x response.\n In order to support multiple versions of an API, the service’s adapters that imple-\nment the APIs will contain logic that translates between the old and new versions.\nAlso, as described in chapter 8, the API gateway will almost certainly use versioned\nAPIs. It may even have to support numerous older versions of an API.\n Now we’ll look at the issue of message formats, the choice of which can impact how\neasy evolving an API will be. \n3.1.4\nMessage formats\nThe essence of IPC is the exchange of messages. Messages usually contain data, and so\nan important design decision is the format of that data. The choice of message format\ncan impact the efficiency of IPC, the usability of the API, and its evolvability. If you’re\nusing a messaging system or protocols such as HTTP, you get to pick your message for-\nmat. Some IPC mechanisms—such as gRPC, which you’ll learn about shortly—might\ndictate the message format. In either case, it’s essential to use a cross-language mes-\nsage format. Even if you’re writing your microservices in a single language today, it’s\nlikely that you’ll use other languages in the future. You shouldn’t, for example, use\nJava serialization.\n There are two main categories of message formats: text and binary. Let’s look at\neach one.\nTEXT-BASED MESSAGE FORMATS\nThe first category is text-based formats such as JSON and XML. An advantage of these\nformats is that not only are they human readable, they’re self describing. A JSON mes-\nsage is a collection of named properties. Similarly, an XML message is effectively a col-\nlection of named elements and values. This format enables a consumer of a message\nto pick out the values of interest and ignore the rest. Consequently, many changes to\nthe message schema can easily be backward-compatible.\n The structure of XML documents is specified by an XML schema (www.w3.org/\nXML/Schema). Over time, the developer community has come to realize that JSON also\nneeds a similar mechanism. One popular option is to use the JSON Schema standard\n(http://json-schema.org). A JSON schema defines the names and types of a message’s\nproperties and whether they’re optional or required. As well as being useful documenta-\ntion, a JSON schema can be used by an application to validate incoming messages.\n A downside of using a text-based messages format is that the messages tend to be\nverbose, especially XML. Every message has the overhead of containing the names of\n \n",
      "content_length": 2873,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "72\nCHAPTER 3\nInterprocess communication in a microservice architecture\nthe attributes in addition to their values. Another drawback is the overhead of parsing\ntext, especially when messages are large. Consequently, if efficiency and performance\nare important, you may want to consider using a binary format. \nBINARY MESSAGE FORMATS\nThere are several different binary formats to choose from. Popular formats include\nProtocol Buffers (https://developers.google.com/protocol-buffers/docs/overview)\nand Avro (https://avro.apache.org). Both formats provide a typed IDL for defining\nthe structure of your messages. A compiler then generates the code that serializes and\ndeserializes the messages. You’re forced to take an API-first approach to service\ndesign! Moreover, if you write your client in a statically typed language, the compiler\nchecks that it uses the API correctly.\n One difference between these two binary formats is that Protocol Buffers uses\ntagged fields, whereas an Avro consumer needs to know the schema in order to inter-\npret messages. As a result, handling API evolution is easier with Protocol Buffers\nthan with Avro. This blog post (http://martin.kleppmann.com/2012/12/05/schema-\nevolution-in-avro-protocol-buffers-thrift.html) is an excellent comparison of Thrift,\nProtocol Buffers, and Avro.\n Now that we’ve looked at message formats, let’s look at specific IPC mechanisms\nthat transport the messages, starting with the Remote procedure invocation (RPI)\npattern. \n3.2\nCommunicating using the synchronous Remote \nprocedure invocation pattern\nWhen using a remote procedure invocation-based IPC mechanism, a client sends a\nrequest to a service, and the service processes the request and sends back a response.\nSome clients may block waiting for a response, and others might have a reactive, non-\nblocking architecture. But unlike when using messaging, the client assumes that the\nresponse will arrive in a timely fashion.\n Figure 3.1 shows how RPI works. The business logic in the client invokes a proxy\ninterface , implemented by an RPI proxy adapter class. The RPI proxy makes a request to\nthe service. The request is handled by an RPI server adapter class, which invokes the\nservice’s business logic via an interface. It then sends back a reply to the RPI proxy,\nwhich returns the result to the client’s business logic.\nThe proxy interface usually encapsulates the underlying communication protocol.\nThere are numerous protocols to choose from. In this section, I describe REST and\nPattern: Remote procedure invocation\nA client invokes a service using a synchronous, remote procedure invocation-based\nprotocol, such as REST (http://microservices.io/patterns/communication-style/\nmessaging.html).\n \n",
      "content_length": 2718,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "73\nCommunicating using the synchronous Remote procedure invocation pattern\ngRPC. I cover how to improve the availability of your services by properly handling\npartial failure and explain why a microservices-based application that uses RPI must\nuse a service discovery mechanism.\n Let’s first take a look at REST.\n3.2.1\nUsing REST\nToday, it’s fashionable to develop APIs in the RESTful style (https://en.wikipedia\n.org/wiki/Representational_state_transfer). REST is an IPC mechanism that (almost\nalways) uses HTTP. Roy Fielding, the creator of REST, defines REST as follows:\nREST provides a set of architectural constraints that, when applied as a whole, emphasizes\nscalability of component interactions, generality of interfaces, independent deployment of\ncomponents, and intermediary components to reduce interaction latency, enforce security,\nand encapsulate legacy systems.\nwww.ics.uci.edu/~fielding/pubs/dissertation/top.htm\nA key concept in REST is a resource, which typically represents a single business\nobject, such as a Customer or Product, or a collection of business objects. REST\nuses the HTTP verbs for manipulating resources, which are referenced using a\nURL. For example, a GET request returns the representation of a resource, which is\noften in the form of an XML document or JSON object, although other formats\nsuch as binary can be used. A POST request creates a new resource, and a PUT\nrequest updates a resource. The Order Service, for example, has a POST /orders\nendpoint for creating an Order and a GET /orders/{orderId} endpoint for retriev-\ning an Order.\nBusiness logic\ninvokes\nBusiness logic\nProxy interface\nService interface\nClient\nService\nRPI\nproxy\nRequest\nReply\nRPI\nserver\nFigure 3.1\nThe client’s business logic invokes an interface that is implemented by an RPI proxy \nadapter class. The RPI proxy class makes a request to the service. The RPI server adapter class \nhandles the request by invoking the service’s business logic.\n \n",
      "content_length": 1959,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "74\nCHAPTER 3\nInterprocess communication in a microservice architecture\n Many developers claim their HTTP-based APIs are RESTful. But as Roy Fielding\ndescribes in a blog post, not all of them actually are (http://roy.gbiv.com/untangled/\n2008/rest-apis-must-be-hypertext-driven). To understand why, let’s take a look at the\nREST maturity model.\nTHE REST MATURITY MODEL\nLeonard Richardson (no relation to your author) defines a very useful maturity model\nfor REST (http://martinfowler.com/articles/richardsonMaturityModel.html) that con-\nsists of the following levels:\nLevel 0—Clients of a level 0 service invoke the service by making HTTP POST\nrequests to its sole URL endpoint. Each request specifies the action to perform,\nthe target of the action (for example, the business object), and any parameters.\nLevel 1—A level 1 service supports the idea of resources. To perform an action\non a resource, a client makes a POST request that specifies the action to per-\nform and any parameters.\nLevel 2—A level 2 service uses HTTP verbs to perform actions: GET to retrieve,\nPOST to create, and PUT to update. The request query parameters and body, if\nany, specify the actions' parameters. This enables services to use web infrastruc-\nture such as caching for GET requests.\nLevel 3—The design of a level 3 service is based on the terribly named\nHATEOAS (Hypertext As The Engine Of Application State) principle. The\nbasic idea is that the representation of a resource returned by a GET request\ncontains links for performing actions on that resource. For example, a client\ncan cancel an order using a link in the representation returned by the GET\nrequest that retrieved the order. The benefits of HATEOAS include no longer\nhaving to hard-wire URLs into client code (www.infoq.com/news/2009/04/\nhateoas-restful-api-advantages).\nI encourage you to review the REST APIs at your organization to see which level they\ncorrespond to. \nSPECIFYING REST APIS\nAs mentioned earlier in section 3.1, you must define your APIs using an interface defi-\nnition language (IDL). Unlike older communication protocols like CORBA and\nSOAP, REST did not originally have an IDL. Fortunately, the developer community\nhas rediscovered the value of an IDL for RESTful APIs. The most popular REST IDL is\nthe Open API Specification (www.openapis.org), which evolved from the Swagger\nopen source project. The Swagger project is a set of tools for developing and docu-\nmenting REST APIs. It includes tools that generate client stubs and server skeletons\nfrom an interface definition. \nTHE CHALLENGE OF FETCHING MULTIPLE RESOURCES IN A SINGLE REQUEST\nREST resources are usually oriented around business objects, such as Consumer and\nOrder. Consequently, a common problem when designing a REST API is how to\n \n",
      "content_length": 2771,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "75\nCommunicating using the synchronous Remote procedure invocation pattern\nenable the client to retrieve multiple related objects in a single request. For example,\nimagine that a REST client wanted to retrieve an Order and the Order's Consumer. A\npure REST API would require the client to make at least two requests, one for the\nOrder and another for its Consumer. A more complex scenario would require even\nmore round-trips and suffer from excessive latency.\n One solution to this problem is for an API to allow the client to retrieve related\nresources when it gets a resource. For example, a client could retrieve an Order and its\nConsumer using GET /orders/order-id-1345?expand=consumer. The query parame-\nter specifies the related resources to return with the Order. This approach works well\nin many scenarios but it’s often insufficient for more complex scenarios. It’s also\npotentially time consuming to implement. This has led to the increasing popularity of\nalternative API technologies such as GraphQL (http://graphql.org) and Netflix Falcor\n(http://netflix.github.io/falcor/), which are designed to support efficient data fetching. \nTHE CHALLENGE OF MAPPING OPERATIONS TO HTTP VERBS\nAnother common REST API design problem is how to map the operations you want\nto perform on a business object to an HTTP verb. A REST API should use PUT for\nupdates, but there may be multiple ways to update an order, including cancelling it,\nrevising the order, and so on. Also, an update might not be idempotent, which is a\nrequirement for using PUT. One solution is to define a sub-resource for updating a\nparticular aspect of a resource. The Order Service, for example, has a POST /orders/\n{orderId}/cancel endpoint for cancelling orders, and a POST /orders/{orderId}/\nrevise endpoint for revising orders. Another solution is to specify a verb as a URL\nquery parameter. Sadly, neither solution is particularly RESTful.\n This problem with mapping operations to HTTP verbs has led to the growing pop-\nularity of alternatives to REST, such as gPRC, discussed shortly in section 3.2.2. But\nfirst let’s look at the benefits and drawbacks of REST. \nBENEFITS AND DRAWBACKS OF REST\nThere are numerous benefits to using REST:\nIt’s simple and familiar.\nYou can test an HTTP API from within a browser using, for example, the Post-\nman plugin, or from the command line using curl (assuming JSON or some\nother text format is used).\nIt directly supports request/response style communication.\nHTTP is, of course, firewall friendly.\nIt doesn’t require an intermediate broker, which simplifies the system’s archi-\ntecture.\nThere are some drawbacks to using REST:\nIt only supports the request/response style of communication.\nReduced availability. Because the client and service communicate directly with-\nout an intermediary to buffer messages, they must both be running for the\nduration of the exchange.\n \n",
      "content_length": 2892,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "76\nCHAPTER 3\nInterprocess communication in a microservice architecture\nClients must know the locations (URLs) of the service instances(s). As described\nin section 3.2.4, this is a nontrivial problem in a modern application. Clients must\nuse what is known as a service discovery mechanism to locate service instances.\nFetching multiple resources in a single request is challenging.\nIt’s sometimes difficult to map multiple update operations to HTTP verbs.\nDespite these drawbacks, REST seems to be the de facto standard for APIs, though\nthere are a couple of interesting alternatives. GraphQL, for example, implements\nflexible, efficient data fetching. Chapter 8 discusses GraphQL and covers the API\ngateway pattern.\n gRPC is another alternative to REST. Let’s take a look at how it works. \n3.2.2\nUsing gRPC\nAs mentioned in the preceding section, one challenge with using REST is that\nbecause HTTP only provides a limited number of verbs, it’s not always straightforward\nto design a REST API that supports multiple update operations. An IPC technology\nthat avoids this issue is gRPC (www.grpc.io), a framework for writing cross-language\nclients and servers (see https://en.wikipedia.org/wiki/Remote_procedure_call for\nmore). gRPC is a binary message-based protocol, and this means—as mentioned ear-\nlier in the discussion of binary message formats—you’re forced to take an API-first\napproach to service design. You define your gRPC APIs using a Protocol Buffers-based\nIDL, which is Google’s language-neutral mechanism for serializing structured data.\nYou use the Protocol Buffer compiler to generate client-side stubs and server-side skel-\netons. The compiler can generate code for a variety of languages, including Java, C#,\nNodeJS, and GoLang. Clients and servers exchange binary messages in the Protocol\nBuffers format using HTTP/2.\n A gRPC API consists of one or more services and request/response message defini-\ntions. A service definition is analogous to a Java interface and is a collection of strongly\ntyped methods. As well as supporting simple request/response RPC, gRPC support\nstreaming RPC. A server can reply with a stream of messages to the client. Alterna-\ntively, a client can send a stream of messages to the server.\n gRPC uses Protocol Buffers as the message format. Protocol Buffers is, as men-\ntioned earlier, an efficient, compact, binary format. It’s a tagged format. Each field of\na Protocol Buffers message is numbered and has a type code. A message recipient can\nextract the fields that it needs and skip over the fields that it doesn’t recognize. As a\nresult, gRPC enables APIs to evolve while remaining backward-compatible.\n Listing 3.1 shows an excerpt of the gRPC API for the Order Service. It defines sev-\neral methods, including createOrder(). This method takes a CreateOrderRequest as\na parameter and returns a CreateOrderReply.\nservice OrderService {\nrpc createOrder(CreateOrderRequest) returns (CreateOrderReply) {}\nListing 3.1\nAn excerpt of the gRPC API for the Order Service\n \n",
      "content_length": 3017,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "77\nCommunicating using the synchronous Remote procedure invocation pattern\nrpc cancelOrder(CancelOrderRequest) returns (CancelOrderReply) {}\nrpc reviseOrder(ReviseOrderRequest) returns (ReviseOrderReply) {}\n...\n}\nmessage CreateOrderRequest {\nint64 restaurantId = 1;\nint64 consumerId = 2;\nrepeated LineItem lineItems = 3;\n...\n}\nmessage LineItem {\nstring menuItemId = 1;\nint32 quantity = 2;\n}\nmessage CreateOrderReply {\nint64 orderId = 1;\n}\n...\nCreateOrderRequest and CreateOrderReply are typed messages. For example, Create-\nOrderRequest message has a restaurantId field of type int64. The field’s tag value is 1.\n gRPC has several benefits:\nIt’s straightforward to design an API that has a rich set of update operations.\nIt has an efficient, compact IPC mechanism, especially when exchanging large\nmessages.\nBidirectional streaming enables both RPI and messaging styles of communication.\nIt enables interoperability between clients and services written in a wide range\nof languages.\ngRPC also has several drawbacks:\nIt takes more work for JavaScript clients to consume gRPC-based API than\nREST/JSON-based APIs.\nOlder firewalls might not support HTTP/2.\ngRPC is a compelling alternative to REST, but like REST, it’s a synchronous communi-\ncation mechanism, so it also suffers from the problem of partial failure. Let’s take a\nlook at what that is and how to handle it. \n3.2.3\nHandling partial failure using the Circuit breaker pattern\nIn a distributed system, whenever a service makes a synchronous request to another\nservice, there is an ever-present risk of partial failure. Because the client and the ser-\nvice are separate processes, a service may not be able to respond in a timely way to a\nclient’s request. The service could be down because of a failure or for maintenance.\nOr the service might be overloaded and responding extremely slowly to requests.\n \n",
      "content_length": 1868,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "78\nCHAPTER 3\nInterprocess communication in a microservice architecture\nBecause the client is blocked waiting for a response, the danger is that the failure\ncould cascade to the client’s clients and so on and cause an outage.\nConsider, for example, the scenario shown in figure 3.2, where the Order Service is\nunresponsive. A mobile client makes a REST request to an API gateway, which, as dis-\ncussed in chapter 8, is the entry point into the application for API clients. The API\ngateway proxies the request to the unresponsive Order Service.\nA naive implementation of the OrderServiceProxy would block indefinitely, waiting\nfor a response. Not only would that result in a poor user experience, but in many\napplications it would consume a precious resource, such as a thread. Eventually the\nAPI gateway would run out of resources and become unable to handle requests. The\nentire API would be unavailable.\n It’s essential that you design your services to prevent partial failures from cascading\nthroughout the application. There are two parts to the solution:\nYou must use design RPI proxies, such as OrderServiceProxy, to handle unre-\nsponsive remote services.\nYou need to decide how to recover from a failed remote service.\nFirst we’ll look at how to write robust RPI proxies.\nPattern: Circuit breaker\nAn RPI proxy that immediately rejects invocations for a timeout period after the num-\nber of consecutive failures exceeds a specified threshold. See http://microservices\n.io/patterns/reliability/circuit-breaker.html.\nAPI\ngateway\nUnresponsive remote service\nMobile\napp\nOrder\nService\nOrder\nService\nproxy\nCreate\norder\nendpoint\nPOST/orders\nPOST/orders\nFigure 3.2\nAn API gateway must protect itself from unresponsive services, such as the Order \nService.\n \n",
      "content_length": 1757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "79\nCommunicating using the synchronous Remote procedure invocation pattern\nDEVELOPING ROBUST RPI PROXIES\nWhenever one service synchronously invokes another service, it should protect itself\nusing the approach described by Netflix (http://techblog.netflix.com/2012/02/fault-\ntolerance-in-high-volume.html). This approach consists of a combination of the fol-\nlowing mechanisms:\nNetwork timeouts—Never block indefinitely and always use timeouts when wait-\ning for a response. Using timeouts ensures that resources are never tied up\nindefinitely.\nLimiting the number of outstanding requests from a client to a service—Impose an upper\nbound on the number of outstanding requests that a client can make to a par-\nticular service. If the limit has been reached, it’s probably pointless to make\nadditional requests, and those attempts should fail immediately.\nCircuit breaker pattern—Track the number of successful and failed requests,\nand if the error rate exceeds some threshold, trip the circuit breaker so that\nfurther attempts fail immediately. A large number of requests failing suggests\nthat the service is unavailable and that sending more requests is pointless.\nAfter a timeout period, the client should try again, and, if successful, close the\ncircuit breaker.\nNetflix Hystrix (https://github.com/Netflix/Hystrix) is an open source library that\nimplements these and other patterns. If you’re using the JVM, you should definitely\nconsider using Hystrix when implementing RPI proxies. And if you’re running in a\nnon-JVM environment, you should use an equivalent library. For example, the Polly\nlibrary is popular in the .NET community (https://github.com/App-vNext/Polly). \nRECOVERING FROM AN UNAVAILABLE SERVICE\nUsing a library such as Hystrix is only part of the solution. You must also decide on a\ncase-by-case basis how your services should recover from an unresponsive remote ser-\nvice. One option is for a service to simply return an error to its client. For example,\nthis approach makes sense for the scenario shown in figure 3.2, where the request to\ncreate an Order fails. The only option is for the API gateway to return an error to the\nmobile client.\n In other scenarios, returning a fallback value, such as either a default value or a\ncached response, may make sense. For example, chapter 7 describes how the API gate-\nway could implement the findOrder() query operation by using the API composition\npattern. As figure 3.3 shows, its implementation of the GET /orders/{orderId} end-\npoint invokes several services, including the Order Service, Kitchen Service, and\nDelivery Service, and combines the results.\n It’s likely that each service’s data isn’t equally important to the client. The data\nfrom the Order Service is essential. If this service is unavailable, the API gateway\nshould return either a cached version of its data or an error. The data from the other\nservices is less critical. A client can, for example, display useful information to the user\neven if the delivery status was unavailable. If the Delivery Service is unavailable,\n \n",
      "content_length": 3063,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "80\nCHAPTER 3\nInterprocess communication in a microservice architecture\nthe API gateway should return either a cached version of its data or omit it from the\nresponse.\n It’s essential that you design your services to handle partial failure, but that’s not\nthe only problem you need to solve when using RPI. Another problem is that in order\nfor one service to invoke another service using RPI, it needs to know the network\nlocation of a service instance. On the surface this sounds simple, but in practice it’s\na challenging problem. You must use a service discovery mechanism. Let’s look at\nhow that works. \n3.2.4\nUsing service discovery\nSay you’re writing some code that invokes a service that has a REST API. In order to\nmake a request, your code needs to know the network location (IP address and port)\nof a service instance. In a traditional application running on physical hardware, the\nnetwork locations of service instances are usually static. For example, your code could\nread the network locations from a configuration file that’s occasionally updated. But\nin a modern, cloud-based microservices application, it’s usually not that simple. As is\nshown in figure 3.4, a modern application is much more dynamic.\n Service instances have dynamically assigned network locations. Moreover, the set of\nservice instances changes dynamically because of autoscaling, failures, and upgrades.\nConsequently, your client code must use a service discovery.\nAPI\ngateway\nHow to handle each\nunresponsive service?\nUnresponsive\nservice\nMobile\napp\nGet\norder\nendpoint\nGet/orders/xyz\nOrder\nService\nOrder\nService\nproxy\nGET/orders/xyz\nKitchen\nService\nKitchen\nService\nproxy\nGET/tickets?orderId=xyz\nDelivery\nService\nDelivery\nService\nproxy\nGET/deliveries?orderId-xyz\n...\nService\n...\nService\nproxy\nFigure 3.3\nThe API gateway implements the GET /orders/{orderId} endpoint using API \ncomposition. It calls several services, aggregates their responses, and sends a response to the \nmobile app. The code that implements the endpoint must have a strategy for handling the failure \nof each service that it calls.\n \n",
      "content_length": 2087,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "81\nCommunicating using the synchronous Remote procedure invocation pattern\nOVERVIEW OF SERVICE DISCOVERY\nAs you’ve just seen, you can’t statically configure a client with the IP addresses of the\nservices. Instead, an application must use a dynamic service discovery mechanism. Ser-\nvice discovery is conceptually quite simple: its key component is a service registry,\nwhich is a database of the network locations of an application’s service instances.\n The service discovery mechanism updates the service registry when service instances\nstart and stop. When a client invokes a service, the service discovery mechanism que-\nries the service registry to obtain a list of available service instances and routes the\nrequest to one of them.\n There are two main ways to implement service discovery:\nThe services and their clients interact directly with the service registry.\nThe deployment infrastructure handles service discovery. (I talk more about\nthat in chapter 12.)\nLet’s look at each option. \nAPPLYING THE APPLICATION-LEVEL SERVICE DISCOVERY PATTERNS\nOne way to implement service discovery is for the application’s services and their cli-\nents to interact with the service registry. Figure 3.5 shows how this works. A service\ninstance registers its network location with the service registry. A service client invokes\na service by first querying the service registry to obtain a list of service instances. It\nthen sends a request to one of those instances.\nService\ninstance 1\nOrder service\n10.232.23.1\n10.232.23.2\n10.232.23.3\nService\ninstance 2\nService\nclient\nService\ninstance 3\n?\nDynamically\nassigned IP\nDynamically created\nand destroyed\nFigure 3.4\nService instances have dynamically assigned IP addresses.\n \n",
      "content_length": 1713,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "82\nCHAPTER 3\nInterprocess communication in a microservice architecture\nThis approach to service discovery is a combination of two patterns. The first pat-\ntern is the Self registration pattern. A service instance invokes the service registry’s\nregistration API to register its network location. It may also supply a health check\nURL, described in more detail in chapter 11. The health check URL is an API end-\npoint that the service registry invokes periodically to verify that the service instance\nis healthy and available to handle requests. A service registry may require a service\ninstance to periodically invoke a “heartbeat” API in order to prevent its registration\nfrom expiring.\nThe second pattern is the Client-side discovery pattern. When a service client wants to\ninvoke a service, it queries the service registry to obtain a list of the service’s instances.\nTo improve performance, a client might cache the service instances. The service client\nPattern: Self registration\nA service instance registers itself with the service registry. See http://microser-\nvices.io/patterns/self-registration.html.\nService\ninstance 1\nOrder service\n10.232.23.1\nLoad balance request\n10.232.23.1\n10.232.23.2\n10.232.23.3\n10.232.23.2\nRegister(\"order-service\", \"10.232.23.1\")\nQuery(\"order-service\")\nQuery API\nRegistration API\n10.232.23.3\nService\ninstance 2\nService\ninstance 3\nService\ndiscovery library\nService\nclient\nService\norder-service\norder-service\norder-service\n...\nService registry\nIP address\n10.232.23.1\n10.232.23.2\n10.232.23.3\n...\nRPC/rest\nclient\nService\ndiscovery library\nService\ndiscovery library\nService\ndiscovery library\nClient-side discovery\nSelf registration pattern\nFigure 3.5\nThe service registry keeps track of the service instances. Clients query the service \nregistry to find network locations of available service instances.\n \n",
      "content_length": 1836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "83\nCommunicating using the synchronous Remote procedure invocation pattern\nthen uses a load-balancing algorithm, such as a round-robin or random, to select a ser-\nvice instance. It then makes a request to a select service instance.\nApplication-level service discovery has been popularized by Netflix and Pivotal. Netflix\ndeveloped and open sourced several components: Eureka, a highly available service\nregistry, the Eureka Java client, and Ribbon, a sophisticated HTTP client that supports\nthe Eureka client. Pivotal developed Spring Cloud, a Spring-based framework that\nmakes it remarkably easy to use the Netflix components. Spring Cloud-based services\nautomatically register with Eureka, and Spring Cloud-based clients automatically use\nEureka for service discovery.\n One benefit of application-level service discovery is that it handles the scenario\nwhen services are deployed on multiple deployment platforms. Imagine, for example,\nyou’ve deployed only some of services on Kubernetes, discussed in chapter 12, and the\nrest is running in a legacy environment. Application-level service discovery using\nEureka, for example, works across both environments, whereas Kubernetes-based ser-\nvice discovery only works within Kubernetes.\n One drawback of application-level service discovery is that you need a service dis-\ncovery library for every language—and possibly framework—that you use. Spring\nCloud only helps Spring developers. If you’re using some other Java framework or a\nnon-JVM language such as NodeJS or GoLang, you must find some other service dis-\ncovery framework. Another drawback of application-level service discovery is that\nyou’re responsible for setting up and managing the service registry, which is a distrac-\ntion. As a result, it’s usually better to use a service discovery mechanism that’s pro-\nvided by the deployment infrastructure. \nAPPLYING THE PLATFORM-PROVIDED SERVICE DISCOVERY PATTERNS\nLater in chapter 12 you’ll learn that many modern deployment platforms such as\nDocker and Kubernetes have a built-in service registry and service discovery mecha-\nnism. The deployment platform gives each service a DNS name, a virtual IP (VIP)\naddress, and a DNS name that resolves to the VIP address. A service client makes a\nrequest to the DNS name/VIP, and the deployment platform automatically routes the\nrequest to one of the available service instances. As a result, service registration, ser-\nvice discovery, and request routing are entirely handled by the deployment platform.\nFigure 3.6 shows how this works.\n The deployment platform includes a service registry that tracks the IP addresses of\nthe deployed services. In this example, a client accesses the Order Service using the\nPattern: Client-side discovery\nA service client retrieves the list of available service instances from the service reg-\nistry and load balances across them. See http://microservices.io/patterns/client-\nside-discovery.html.\n \n",
      "content_length": 2933,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "84\nCHAPTER 3\nInterprocess communication in a microservice architecture\nDNS name order-service, which resolves to the virtual IP address 10.1.3.4. The\ndeployment platform automatically load balances requests across the three instances\nof the Order Service.\n This approach is a combination of two patterns:\n3rd party registration pattern—Instead of a service registering itself with the ser-\nvice registry, a third party called the registrar, which is typically part of the\ndeployment platform, handles the registration.\nServer-side discovery pattern—Instead of a client querying the service registry, it\nmakes a request to a DNS name, which resolves to a request router that queries\nthe service registry and load balances requests.\nService\norder-service\norder-service\norder-service\n...\nService registry\nIP address\n10.232.23.1\n10.232.23.2\n10.232.23.3\n...\nService\nclient\nGET http://order-service/...\nDeployment platform\nRPC/rest\nclient\nService\ninstance 1\nOrder service\nObserves\n10.232.23.1\n10.232.24.99\nService\ninstance 2\nService\ninstance 3\nPlatform\nrouter\nQueries\nUpdates\n10.232.23.2\n10.232.23.3\nRegistrar\n3rd party registration\nServer-side discovery\nService DNS name\nresolves to service VIP\nService virtual IP address (VIP)\nFigure 3.6\nThe platform is responsible for service registration, discovery, and request routing. Service \ninstances are registered with the service registry by the registrar. Each service has a network location, \na DNS name/virtual IP address. A client makes a request to the service’s network location. The router \nqueries the service registry and load balances requests across the available service instances.\n \n",
      "content_length": 1639,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "85\nCommunicating using the Asynchronous messaging pattern\nThe key benefit of platform-provided service discovery is that all aspects of service dis-\ncovery are entirely handled by the deployment platform. Neither the services nor the\nclients contain any service discovery code. Consequently, the service discovery mecha-\nnism is readily available to all services and clients regardless of which language or\nframework they’re written in.\n One drawback of platform-provided service discovery is that it only supports the\ndiscovery of services that have been deployed using the platform. For example, as\nmentioned earlier when describing application-level discovery, Kubernetes-based dis-\ncovery only works for services running on Kubernetes. Despite this limitation, I rec-\nommend using platform-provided service discovery whenever possible.\n Now that we’ve looked at synchronous IPC using REST or gRPC, let’s take a look at\nthe alternative: asynchronous, message-based communication. \n3.3\nCommunicating using the Asynchronous messaging \npattern\nWhen using messaging, services communicate by asynchronously exchanging mes-\nsages. A messaging-based application typically uses a message broker, which acts as an\nintermediary between the services, although another option is to use a brokerless\narchitecture, where the services communicate directly with each other. A service client\nmakes a request to a service by sending it a message. If the service instance is expected\nto reply, it will do so by sending a separate message back to the client. Because the\ncommunication is asynchronous, the client doesn’t block waiting for a reply. Instead,\nthe client is written assuming that the reply won’t be received immediately.\nI start this section with an overview of messaging. I show how to describe a messaging\narchitecture independently of messaging technology. Next I compare and contrast\nPattern: 3rd party registration\nService instances are automatically registered with the service registry by a third party.\nSee http://microservices.io/patterns/3rd-party-registration.html.\nPattern: Server-side discovery\nA client makes a request to a router, which is responsible for service discovery. See\nhttp://microservices.io/patterns/server-side-discovery.html.\nPattern: Messaging\nA client invokes a service using asynchronous messaging. See http://microservices\n.io/patterns/communication-style/messaging.html.\n \n",
      "content_length": 2403,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "86\nCHAPTER 3\nInterprocess communication in a microservice architecture\nbrokerless and broker-based architectures and describe the criteria for selecting a\nmessage broker. I then discuss several important topics, including scaling consum-\ners while preserving message ordering, detecting and discarding duplicate messages,\nand sending and receiving messages as part of a database transaction. Let’s begin by\nlooking at how messaging works.\n3.3.1\nOverview of messaging\nA useful model of messaging is defined in the book Enterprise Integration Patterns\n(Addison-Wesley Professional, 2003) by Gregor Hohpe and Bobby Woolf. In this\nmodel, messages are exchanged over message channels. A sender (an application or\nservice) writes a message to a channel, and a receiver (an application or service) reads\nmessages from a channel. Let’s look at messages and then look at channels.\nABOUT MESSAGES\nA message consists of a header and a message body (www.enterpriseintegrationpatterns\n.com/Message.html). The header is a collection of name-value pairs, metadata that\ndescribes the data being sent. In addition to name-value pairs provided by the mes-\nsage’s sender, the message header contains name-value pairs, such as a unique message\nid generated by either the sender or the messaging infrastructure, and an optional\nreturn address, which specifies the message channel that a reply should be written to.\nThe message body is the data being sent, in either text or binary format.\n There are several different kinds of messages:\nDocument—A generic message that contains only data. The receiver decides how\nto interpret it. The reply to a command is an example of a document message.\nCommand—A message that’s the equivalent of an RPC request. It specifies the\noperation to invoke and its parameters.\nEvent—A message indicating that something notable has occurred in the sender.\nAn event is often a domain event, which represents a state change of a domain\nobject such as an Order, or a Customer.\nThe approach to the microservice architecture described in this book uses commands\nand events extensively.\n Let’s now look at channels, the mechanism by which services communicate. \nABOUT MESSAGE CHANNELS\nAs figure 3.7 shows, messages are exchanged over channels (www.enterpriseintegra-\ntionpatterns.com/MessageChannel.html). The business logic in the sender invokes a\nsending port interface, which encapsulates the underlying communication mechanism.\nThe sending port is implemented by a message sender adapter class, which sends a mes-\nsage to a receiver via a message channel. A message channel is an abstraction of the\nmessaging infrastructure. A message handler adapter class in the receiver is invoked to\nhandle the message. It invokes a receiving port interface implemented by the consumer’s\n \n",
      "content_length": 2786,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "87\nCommunicating using the Asynchronous messaging pattern\nbusiness logic. Any number of senders can send messages to a channel. Similarly, any\nnumber of receivers can receive messages from a channel.\n There are two kinds of channels: point-to-point (www.enterpriseintegrationpatterns\n.com/PointToPointChannel.html) and publish-subscribe (www.enterpriseintegration-\npatterns.com/PublishSubscribeChannel.html):\nA point-to-point channel delivers a message to exactly one of the consumers that\nis reading from the channel. Services use point-to-point channels for the one-\nto-one interaction styles described earlier. For example, a command message is\noften sent over a point-to-point channel.\nA publish-subscribe channel delivers each message to all of the attached consum-\ners. Services use publish-subscribe channels for the one-to-many interaction\nstyles described earlier. For example, an event message is usually sent over a\npublish-subscribe channel. \n3.3.2\nImplementing the interaction styles using messaging\nOne of the valuable features of messaging is that it’s flexible enough to support all the\ninteraction styles described in section 3.1.1. Some interaction styles are directly imple-\nmented by messaging. Others must be implemented on top of messaging.\n Let’s look at how to implement each interaction style, starting with request/response\nand asynchronous request/response.\nIMPLEMENTING REQUEST/RESPONSE AND ASYNCHRONOUS REQUEST/RESPONSE\nWhen a client and service interact using either request/response or asynchronous\nrequest/response, the client sends a request and the service sends back a reply. The\nBusiness\nlogic\ninvokes\ninvokes\nBusiness logic\nSending port\nReceiving port\nSender\nReceiver\nMessage\nsender\nMessage\nMessage\nchannel\nReceives\nSends\nHeader\nBody\nMessaging\ninfrastructure\nMessage\nhandler\nService\nFigure 3.7\nThe business logic in the sender invokes a sending port interface, which is implemented by a  message \nsender adapter. The message sender sends a message to a receiver via a message channel. The message channel \nis an abstraction of messaging infrastructure. A message handler adapter in the receiver is invoked to handle the \nmessage. It invokes the receiving port interface implemented by the receiver’s business logic.\n \n",
      "content_length": 2257,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "88\nCHAPTER 3\nInterprocess communication in a microservice architecture\ndifference between the two interaction styles is that with request/response the client\nexpects the service to respond immediately, whereas with asynchronous request/\nresponse there is no such expectation. Messaging is inherently asynchronous, so only\nprovides asynchronous request/response. But a client could block until a reply is\nreceived.\n The client and service implement the asynchronous request/response style inter-\naction by exchanging a pair of messages. As figure 3.8 shows, the client sends a com-\nmand message, which specifies the operation to perform, and parameters, to a point-\nto-point messaging channel owned by a service. The service processes the requests\nand sends a reply message, which contains the outcome, to a point-to-point channel\nowned by the client.\nThe client must tell the service where to send a reply message and must match reply mes-\nsages to requests. Fortunately, solving these two problems isn’t that difficult. The client\nsends a command message that has a reply channel header. The server writes the reply mes-\nsage, which contains a correlation id that has the same value as message identifier, to the reply\nchannel. The client uses the correlation id to match the reply message with the request.\n Because the client and service communicate using messaging, the interaction is\ninherently asynchronous. In theory, a messaging client could block until it receives a\nreply, but in practice the client will process replies asynchronously. What’s more,\nreplies are typically processed by any one of the client’s instances. \nRequest\nSends\nReads\nReads\nSends\nMessageId: msgId\nReturnAddress: ReplyChannel\nBody\nCorrelationId:msgId\nBody\nRequest channel\nReply channel\nReply\nSpeciﬁes\nClient\nService\nClient sends message containing\nmsgId and a reply channel.\nService sends reply to the speciﬁed reply\nchannel. The reply contains a correlationId,\nwhich is the request’s msgId.\nFigure 3.8\nImplementing asynchronous request/response by including a reply channel and message \nidentifier in the request message. The receiver processes the message and sends the reply to the \nspecified reply channel.\n \n",
      "content_length": 2195,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "89\nCommunicating using the Asynchronous messaging pattern\nIMPLEMENTING ONE-WAY NOTIFICATIONS\nImplementing one-way notifications is straightforward using asynchronous messaging.\nThe client sends a message, typically a command message, to a point-to-point channel\nowned by the service. The service subscribes to the channel and processes the mes-\nsage. It doesn’t send back a reply. \nIMPLEMENTING PUBLISH/SUBSCRIBE\nMessaging has built-in support for the publish/subscribe style of interaction. A client\npublishes a message to a publish-subscribe channel that is read by multiple consum-\ners. As described in chapters 4 and 5, services use publish/subscribe to publish\ndomain events, which represent changes to domain objects. The service that publishes\nthe domain events owns a publish-subscribe channel, whose name is derived from the\ndomain class. For example, the Order Service publishes Order events to an Order\nchannel, and the Delivery Service publishes Delivery events to a Delivery chan-\nnel. A service that’s interested in a particular domain object’s events only has to sub-\nscribe to the appropriate channel. \nIMPLEMENTING PUBLISH/ASYNC RESPONSES\nThe publish/async responses interaction style is a higher-level style of interaction that’s\nimplemented by combining elements of publish/subscribe and request/response. A cli-\nent publishes a message that specifies a reply channel header to a publish-subscribe\nchannel. A consumer writes a reply message containing a correlation id to the reply\nchannel. The client gathers the responses by using the correlation id to match the reply\nmessages with the request.\n Each service in your application that has an asynchronous API will use one or\nmore of these implementation techniques. A service that has an asynchronous API for\ninvoking operations will have a message channel for requests. Similarly, a service that\npublishes events will publish them to an event message channel.\n As described in section 3.1.2, it’s important to write an API specification for a ser-\nvice. Let’s look at how to do that for an asynchronous API. \n3.3.3\nCreating an API specification for a messaging-based service API\nThe specification for a service’s asynchronous API must, as figure 3.9 shows, specify\nthe names of the message channels, the message types that are exchanged over each\nchannel, and their formats. You must also describe the format of the messages using a\nstandard such as JSON, XML, or Protobuf. But unlike with REST and Open API, there\nisn’t a widely adopted standard for documenting the channels and the message types.\nInstead, you need to write an informal document.\n A service’s asynchronous API consists of operations, invoked by clients, and events,\npublished by the services. They’re documented in different ways. Let’s take a look at\neach one, starting with operations.\n \n",
      "content_length": 2830,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "90\nCHAPTER 3\nInterprocess communication in a microservice architecture\nDOCUMENTING ASYNCHRONOUS OPERATIONS\nA service’s operations can be invoked using one of two different interaction styles:\nRequest/async response-style API—This consists of the service’s command message\nchannel, the types and formats of the command message types that the service\naccepts, and the types and formats of the reply messages sent by the service.\nOne-way notification-style API—This consists of the service’s command message\nchannel and the types and format of the command message types that the ser-\nvice accepts.\nA service may use the same request channel for both asynchronous request/response\nand one-way notification. \nDOCUMENTING PUBLISHED EVENTS\nA service can also publish events using a publish/subscribe interaction style. The spec-\nification of this style of API consists of the event channel and the types and formats of\nthe event messages that are published by the service to the channel.\n The messages and channels model of messaging is a great abstraction and a good\nway to design a service’s asynchronous API. But in order to implement a service you\nneed to choose a messaging technology and determine how to implement your design\nusing its capabilities. Let’s take a look at what’s involved. \n3.3.4\nUsing a message broker\nA messaging-based application typically uses a message broker, an infrastructure service\nthrough which the service communicates. But a broker-based architecture isn’t the\nonly messaging architecture. You can also use a brokerless-based messaging architec-\nture, in which the services communicate with one another directly. The two approaches,\nshown in figure 3.10, have different trade-offs, but usually a broker-based architecture\nis a better approach.\nService\nCommand\nquery\nAPI\nService API\nReplies\nR\nR\nEvents\nR\nEvent\npublisher\n«Command channel»\n«Event channel»\n«Reply channel»\nCommands\nC\nC\nC\nFigure 3.9\nA service’s asynchronous API consists of message channels and command, reply, and \nevent message types.\n \n",
      "content_length": 2031,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "91\nCommunicating using the Asynchronous messaging pattern\nThis book focuses on broker-based architecture, but it’s worthwhile to take a quick look\nat the brokerless architecture, because there may be scenarios where you find it useful.\nBROKERLESS MESSAGING\nIn a brokerless architecture, services can exchange messages directly. ZeroMQ (http://\nzeromq.org) is a popular brokerless messaging technology. It’s both a specification\nand a set of libraries for different languages. It supports a variety of transports, includ-\ning TCP, UNIX-style domain sockets, and multicast.\n The brokerless architecture has some benefits:\nAllows lighter network traffic and better latency, because messages go directly\nfrom the sender to the receiver, instead of having to go from the sender to the\nmessage broker and from there to the receiver\nEliminates the possibility of the message broker being a performance bottle-\nneck or a single point of failure\nFeatures less operational complexity, because there is no message broker to set\nup and maintain\nAs appealing as these benefits may seem, brokerless messaging has significant drawbacks:\nServices need to know about each other’s locations and must therefore use one\nof the discovery mechanisms describer earlier in section 3.2.4.\nIt offers reduced availability, because both the sender and receiver of a message\nmust be available while the message is being exchanged.\nImplementing mechanisms, such as guaranteed delivery, is more challenging.\nService\nService\nService\nService\nService\nService\nMessage broker\nVs.\nBrokerless architecture\nBroker-based architecture\nFigure 3.10\nThe services in brokerless architecture communicate directly, whereas the services \nin a broker-based architecture communicate via a message broker.\n \n",
      "content_length": 1763,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "92\nCHAPTER 3\nInterprocess communication in a microservice architecture\nIn fact, some of these drawbacks, such as reduced availability and the need for service\ndiscovery, are the same as when using synchronous, response/response.\n Because of these limitations, most enterprise applications use a message broker-\nbased architecture. Let’s look at how that works. \nOVERVIEW OF BROKER-BASED MESSAGING\nA message broker is an intermediary through which all messages flow. A sender writes\nthe message to the message broker, and the message broker delivers it to the receiver.\nAn important benefit of using a message broker is that the sender doesn’t need to\nknow the network location of the consumer. Another benefit is that a message broker\nbuffers messages until the consumer is able to process them.\n There are many message brokers to chose from. Examples of popular open source\nmessage brokers include the following:\nActiveMQ (http://activemq.apache.org)\nRabbitMQ (https://www.rabbitmq.com)\nApache Kafka (http://kafka.apache.org)\nThere are also cloud-based messaging services, such as AWS Kinesis (https://aws.amazon\n.com/kinesis/) and AWS SQS (https://aws.amazon.com/sqs/).\n When selecting a message broker, you have various factors to consider, including\nthe following:\nSupported programming languages—You probably should pick one that supports a\nvariety of programming languages.\nSupported messaging standards—Does the message broker support any standards,\nsuch as AMQP and STOMP, or is it proprietary?\nMessaging ordering—Does the message broker preserve ordering of messages?\nDelivery guarantees—What kind of delivery guarantees does the broker make?\nPersistence—Are messages persisted to disk and able to survive broker crashes?\nDurability—If a consumer reconnects to the message broker, will it receive the\nmessages that were sent while it was disconnected?\nScalability—How scalable is the message broker?\nLatency—What is the end-to-end latency?\nCompeting consumers—Does the message broker support competing consumers?\nEach broker makes different trade-offs. For example, a very low-latency broker might\nnot preserve ordering, make no guarantees to deliver messages, and only store mes-\nsages in memory. A messaging broker that guarantees delivery and reliably stores\nmessages on disk will probably have higher latency. Which kind of message broker is\nthe best fit depends on your application’s requirements. It’s even possible that differ-\nent parts of your application will have different messaging requirements.\n It’s likely, though, that messaging ordering and scalability are essential. Let’s now\nlook at how to implement message channels using a message broker. \n \n",
      "content_length": 2686,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "93\nCommunicating using the Asynchronous messaging pattern\nIMPLEMENTING MESSAGE CHANNELS USING A MESSAGE BROKER\nEach message broker implements the message channel concept in a different way. As\ntable 3.2 shows, JMS message brokers such as ActiveMQ have queues and topics.\nAMQP-based message brokers such as RabbitMQ have exchanges and queues. Apache\nKafka has topics, AWS Kinesis has streams, and AWS SQS has queues. What’s more,\nsome message brokers offer more flexible messaging than the message and channels\nabstraction described in this chapter.\nAlmost all the message brokers described here support both point-to-point and publish-\nsubscribe channels. The one exception is AWS SQS, which only supports point-to-point\nchannels.\n Now let’s look at the benefits and drawbacks of broker-based messaging. \nBENEFITS AND DRAWBACKS OF BROKER-BASED MESSAGING\nThere are many advantages to using broker-based messaging:\nLoose coupling—A client makes a request by simply sending a message to the\nappropriate channel. The client is completely unaware of the service instances.\nIt doesn’t need to use a discovery mechanism to determine the location of a ser-\nvice instance.\nMessage buffering—The message broker buffers messages until they can be pro-\ncessed. With a synchronous request/response protocol such as HTTP, both the\nclient and service must be available for the duration of the exchange. With mes-\nsaging, though, messages will queue up until they can be processed by the con-\nsumer. This means, for example, that an online store can accept orders from\ncustomers even when the order-fulfillment system is slow or unavailable. The\nmessages will simply queue up until they can be processed.\nFlexible communication—Messaging supports all the interaction styles described\nearlier.\nExplicit interprocess communication—RPC-based mechanism attempts to make invok-\ning a remote service look the same as calling a local service. But due to the laws\nof physics and the possibility of partial failure, they’re in fact quite different.\nTable 3.2\nEach message broker implements the message channel concept in a different way.\nMessage broker\nPoint-to-point channel\nPublish-subscribe channel\nJMS\nQueue\nTopic\nApache Kafka\nTopic\nTopic\nAMQP-based brokers, such as \nRabbitMQ\nExchange + Queue\nFanout exchange and a queue per \nconsumer\nAWS Kinesis\nStream\nStream\nAWS SQS\nQueue\n—\n \n",
      "content_length": 2363,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "94\nCHAPTER 3\nInterprocess communication in a microservice architecture\nMessaging makes these differences very explicit, so developers aren’t lulled into\na false sense of security.\nThere are some downsides to using messaging:\nPotential performance bottleneck—There is a risk that the message broker could be\na performance bottleneck. Fortunately, many modern message brokers are\ndesigned to be highly scalable.\nPotential single point of failure—It’s essential that the message broker is highly\navailable—otherwise, system reliability will be impacted. Fortunately, most mod-\nern brokers have been designed to be highly available.\nAdditional operational complexity—The messaging system is yet another system\ncomponent that must be installed, configured, and operated.\nLet’s look at some design issues you might face. \n3.3.5\nCompeting receivers and message ordering\nOne challenge is how to scale out message receivers while preserving message order-\ning. It’s a common requirement to have multiple instances of a service in order to pro-\ncess messages concurrently. Moreover, even a single service instance will probably use\nthreads to concurrently process multiple messages. Using multiple threads and service\ninstances to concurrently process messages increases the throughput of the applica-\ntion. But the challenge with processing messages concurrently is ensuring that each\nmessage is processed once and in order.\n For example, imagine that there are three instances of a service reading from the\nsame point-to-point channel and that a sender publishes Order Created, Order Updated,\nand Order Cancelled event messages sequentially. A simplistic messaging implementa-\ntion could concurrently deliver each message to a different receiver. Because of delays\ndue to network issues or garbage collections, messages might be processed out of order,\nwhich would result in strange behavior. In theory, a service instance might process the\nOrder Cancelled message before another service processes the Order Created message!\n A common solution, used by modern message brokers like Apache Kafka and AWS\nKinesis, is to use sharded (partitioned) channels. Figure 3.11 shows how this works.\nThere are three parts to the solution:\n1\nA sharded channel consists of two or more shards, each of which behaves like\na channel.\n2\nThe sender specifies a shard key in the message’s header, which is typically an\narbitrary string or sequence of bytes. The message broker uses a shard key to\nassign the message to a particular shard/partition. It might, for example, select\nthe shard by computing the hash of the shard key modulo the number of shards.\n3\nThe messaging broker groups together multiple instances of a receiver and\ntreats them as the same logical receiver. Apache Kafka, for example, uses the\nterm consumer group. The message broker assigns each shard to a single receiver.\nIt reassigns shards when receivers start up and shut down.\n \n",
      "content_length": 2927,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "95\nCommunicating using the Asynchronous messaging pattern\nIn this example, each Order event message has the orderId as its shard key. Each event\nfor a particular order is published to the same shard, which is read by a single consumer\ninstance. As a result, these messages are guaranteed to be processed in order. \n3.3.6\nHandling duplicate messages\nAnother challenge you must tackle when using messaging is dealing with duplicate\nmessages. A message broker should ideally deliver each message only once, but guar-\nanteeing exactly-once messaging is usually too costly. Instead, most message brokers\npromise to deliver a message at least once.\n When the system is working normally, a message broker that guarantees at-least-\nonce delivery will deliver each message only once. But a failure of a client, network, or\nmessage broker can result in a message being delivered multiple times. Say a client\ncrashes after processing a message and updating its database—but before acknowledg-\ning the message. The message broker will deliver the unacknowledged message again,\neither to that client when it restarts or to another replica of the client.\n Ideally, you should use a message broker that preserves ordering when redeliver-\ning messages. Imagine that the client processes an Order Created event followed by\nan Order Cancelled event for the same Order, and that somehow the Order Created\nevent wasn’t acknowledged. The message broker should redeliver both the Order Cre-\nated and Order Cancelled events. If it only redelivers the Order Created, the client\nmay undo the cancelling of the Order.\n There are a couple of different ways to handle duplicate messages:\nWrite idempotent message handlers.\nTrack messages and discard duplicates.\nLet’s look at each option.\nRoutes based on a\nhash of the shard-key\nReceiver A\ninstance 1\nReceiver A\ninstance 2\nReceiver\nShard\nassignment\nReceiver\n...\nRouter\nShard 0\nChannel\nLogical receiver A\nShard 1\nShard ...\nCreate order\nrequest\nShard-key:orderId\nSender\nFigure 3.11\nScaling consumers while preserving message ordering by using a sharded (partitioned) message \nchannel. The sender includes the shard key in the message. The message broker writes the message to a shard \ndetermined by the shard key. The message broker assigns each partition to an instance of the replicated receiver.\n \n",
      "content_length": 2323,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "96\nCHAPTER 3\nInterprocess communication in a microservice architecture\nWRITING IDEMPOTENT MESSAGE HANDLERS\nIf the application logic that processes messages is idempotent, then duplicate mes-\nsages are harmless. Application logic is idempotent if calling it multiple times with the\nsame input values has no additional effect. For instance, cancelling an already-cancelled\norder is an idempotent operation. So is creating an order with a client-supplied ID.\nAn idempotent message handler can be safely executed multiple times, provided that\nthe message broker preserves ordering when redelivering messages.\n Unfortunately, application logic is often not idempotent. Or you may be using a\nmessage broker that doesn’t preserve ordering when redelivering messages. Duplicate\nor out-of-order messages can cause bugs. In this situation, you must write message\nhandlers that track messages and discard duplicate messages. \nTRACKING MESSAGES AND DISCARDING DUPLICATES\nConsider, for example, a message handler that authorizes a consumer credit card. It\nmust authorize the card exactly once for each order. This example of application logic\nhas a different effect each time it’s invoked. If duplicate messages caused the message\nhandler to execute this logic multiple times, the application would behave incorrectly.\nThe message handler that executes this kind of application logic must become idem-\npotent by detecting and discarding duplicate messages.\n A simple solution is for a message consumer to track the messages that it has pro-\ncessed using the message id and discard any duplicates. It could, for example, store\nthe message id of each message that it consumed in a database table. Figure 3.12\nshows how to do this using a dedicated table.\nWhen a consumer handles a message, it records the message id in the database table as\npart of the transaction that creates and updates business entities. In this example, the\nconsumer inserts a row containing the message id into a PROCESSED_MESSAGES table. If a\nmessage is a duplicate, the INSERT will fail and the consumer can discard the message.\nMSG_ID\nPROCESSED_MESSAGE table\nINSERT\nINSERT will fail for\nduplicate messages.\nUPDATE\n...\n...\nApplication table\nxyz\nTransaction\nMessage\nid: xyz\nConsumer\nFigure 3.12\nA consumer detects and discards duplicate messages by recording the IDs of \nprocessed messages in a database table. If a message has been processed before, the INSERT \ninto the PROCESSED_MESSAGES table will fail.\n \n",
      "content_length": 2469,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "97\nCommunicating using the Asynchronous messaging pattern\n Another option is for a  message handler to record message ids in an application\ntable instead of a dedicated table. This approach is particularly useful when using a\nNoSQL database that has a limited transaction model, so it doesn’t support updat-\ning two tables as part of a database transaction. Chapter 7 shows an example of this\napproach. \n3.3.7\nTransactional messaging\nA service often needs to publish messages as part of a transaction that updates the\ndatabase. For instance, throughout this book you see examples of services that publish\ndomain events whenever they create or update business entities. Both the database\nupdate and the sending of the message must happen within a transaction. Otherwise,\na service might update the database and then crash, for example, before sending the\nmessage. If the service doesn’t perform these two operations atomically, a failure\ncould leave the system in an inconsistent state.\n The traditional solution is to use a distributed transaction that spans the database\nand the message broker. But as you’ll learn in chapter 4, distributed transactions\naren’t a good choice for modern applications. Moreover, many modern brokers such\nas Apache Kafka don’t support distributed transactions.\n As a result, an application must use a different mechanism to reliably publish mes-\nsages. Let’s look at how that works.\nUSING A DATABASE TABLE AS A MESSAGE QUEUE\nLet’s imagine that your application is using a relational database. A straightforward\nway to reliably publish messages is to apply the Transactional outbox pattern. This\npattern uses a database table as a temporary message queue. As figure 3.13 shows, a\nservice that sends messages has an OUTBOX database table. As part of the database\nOrder\nService\nRead\nOUTBOX\ntable\nPublish\n...\n...\nORDER table\nINSERT,\nUPDATE,DELETE\nINSERT\nDatabase\nMessage\nrelay\nTransaction\nOUTBOX table\nMessage\nbroker\nFigure 3.13\nA service reliably publishes a message by inserting it into an OUTBOX table as part of the transaction \nthat updates the database. The Message Relay reads the OUTBOX table and publishes the messages to a \nmessage broker.\n \n",
      "content_length": 2179,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "98\nCHAPTER 3\nInterprocess communication in a microservice architecture\ntransaction that creates, updates, and deletes business objects, the service sends mes-\nsages by inserting them into the OUTBOX table. Atomicity is guaranteed because this is a\nlocal ACID transaction.\n The OUTBOX table acts a temporary message queue. The MessageRelay is a compo-\nnent that reads the OUTBOX table and publishes the messages to a message broker.\nYou can use a similar approach with some NoSQL databases. Each business entity\nstored as a record in the database has an attribute that is a list of messages that need\nto be published. When a service updates an entity in the database, it appends a mes-\nsage to that list. This is atomic because it’s done with a single database operation. The\nchallenge, though, is efficiently finding those business entities that have events and\npublishing them.\n There are a couple of different ways to move messages from the database to the\nmessage broker. We’ll look at each one. \nPUBLISHING EVENTS BY USING THE POLLING PUBLISHER PATTERN\nIf the application uses a relational database, a very simple way to publish the messages\ninserted into the OUTBOX table is for the MessageRelay to poll the table for unpub-\nlished messages. It periodically queries the table:\nSELECT * FROM OUTBOX ORDERED BY ... ASC\nNext, the MessageRelay publishes those messages to the message broker, sending one\nto its destination message channel. Finally, it deletes those messages from the OUTBOX\ntable:\nBEGIN\nDELETE FROM OUTBOX WHERE ID in (....)\nCOMMIT\nPolling the database is a simple approach that works reasonably well at low scale. The\ndownside is that frequently polling the database can be expensive. Also, whether you\ncan use this approach with a NoSQL database depends on its querying capabilities.\nThat’s because rather than querying an OUTBOX table, the application must query the\nPattern: Transactional outbox\nPublish an event or message as part of a database transaction by saving it in an OUT-\nBOX in the database. See http://microservices.io/patterns/data/transactional-out-\nbox.html.\nPattern: Polling publisher\nPublish messages by polling the outbox in the database. See http://microser-\nvices.io/patterns/data/polling-publisher.html.\n \n",
      "content_length": 2249,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "99\nCommunicating using the Asynchronous messaging pattern\nbusiness entities, and that may or may not be possible to do efficiently. Because of\nthese drawbacks and limitations, it’s often better—and in some cases, necessary—to\nuse the more sophisticated and performant approach of tailing the database transac-\ntion log. \nPUBLISHING EVENTS BY APPLYING THE TRANSACTION LOG TAILING PATTERN\nA sophisticated solution is for MessageRelay to tail the database transaction log (also\ncalled the commit log). Every committed update made by an application is repre-\nsented as an entry in the database’s transaction log. A transaction log miner can read\nthe transaction log and publish each change as a message to the message broker. Fig-\nure 3.14 shows how this approach works.\nThe Transaction Log Miner reads the transaction log entries. It converts each relevant\nlog entry corresponding to an inserted message into a message and publishes that mes-\nsage to the message broker. This approach can be used to publish messages written to\nan OUTBOX table in an RDBMS or messages appended to records in a NoSQL database.\nPattern: Transaction log tailing\nPublish changes made to the database by tailing the transaction log. See http://micro-\nservices.io/patterns/data/transaction-log-tailing.html.\nDatabase\nOUTBOX table\nTransaction log\nTransaction log\nminer\nINSERT INTO OUTBOX ...\nMessage\nbroker\nChanges\nPublish\nOrder\nService\nCommitted inserts into\nthe OUTBOX table are\nrecorded in the database’s\ntransaction log.\nReads the transaction log\nFigure 3.14\nA service publishes messages inserted into the OUTBOX table by mining \nthe database’s transaction log.\n \n",
      "content_length": 1641,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "100\nCHAPTER 3\nInterprocess communication in a microservice architecture\nThere are a few examples of this approach in use:\nDebezium (http://debezium.io)—An open source project that publishes data-\nbase changes to the Apache Kafka message broker.\nLinkedIn Databus (https://github.com/linkedin/databus)—An open source proj-\nect that mines the Oracle transaction log and publishes the changes as events.\nLinkedIn uses Databus to synchronize various derived data stores with the sys-\ntem of record.\nDynamoDB streams (http://docs.aws.amazon.com/amazondynamodb/latest/\ndeveloperguide/Streams.html)—DynamoDB streams contain the time-ordered\nsequence of changes (creates, updates, and deletes) made to the items in a\nDynamoDB table in the last 24 hours. An application can read those changes\nfrom the stream and, for example, publish them as events.\nEventuate Tram (https://github.com/eventuate-tram/eventuate-tram-core)—Your\nauthor’s very own open source transaction messaging library that uses MySQL\nbinlog protocol, Postgres WAL, or polling to read changes made to an OUTBOX\ntable and publish them to Apache Kafka.\nAlthough this approach is obscure, it works remarkably well. The challenge is that\nimplementing it requires some development effort. You could, for example, write low-\nlevel code that calls database-specific APIs. Alternatively, you could use an open source\nframework such as Debezium that publishes changes made by an application to MySQL,\nPostgres, or MongoDB to Apache Kafka. The drawback of using Debezium is that its\nfocus is capturing changes at the database level and that APIs for sending and receiving\nmessages are outside of its scope. That’s why I created the Eventuate Tram framework,\nwhich provides the messaging APIs as well as transaction tailing and polling. \n3.3.8\nLibraries and frameworks for messaging\nA service needs to use a library to send and receive messages. One approach is to use\nthe message broker’s client library, although there are several problems with using\nsuch a library directly:\nThe client library couples business logic that publishes messages to the message\nbroker APIs.\nA message broker’s client library is typically low level and requires many lines of\ncode to send or receive a message. As a developer, you don’t want to repeatedly\nwrite boilerplate code. Also, as the author of this book I don’t want the example\ncode cluttered with low-level boilerplate.\nThe client library usually provides only the basic mechanism to send and\nreceive messages and doesn’t support the higher-level interaction styles.\nA better approach is to use a higher-level library or framework that hides the low-level\ndetails and directly supports the higher-level interaction styles. For simplicity, the\nexamples in this book use my Eventuate Tram framework. It has a simple, easy-to-\nunderstand API that hides the complexity of using the message broker. Besides an API\n \n",
      "content_length": 2906,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "101\nCommunicating using the Asynchronous messaging pattern\nfor sending and receiving messages, Eventuate Tram also supports higher-level inter-\naction styles such as asynchronous request/response and domain event publishing.\nEventuate Tram also implements two important mechanisms:\nTransactional messaging—It publishes messages as part of a database transaction.\nDuplicate message detection—The Eventuate Tram message consumer detects and\ndiscards duplicate messages, which is essential for ensuring that a consumer\nprocesses messages exactly once, as discussed in section 3.3.6.\nLet’s take a look at the Eventuate Tram APIs.\nBASIC MESSAGING\nThe basic messaging API consists of two Java interfaces: MessageProducer and Message-\nConsumer. A producer service uses the MessageProducer interface to publish messages\nto message channels. Here’s an example of using this interface:\nMessageProducer messageProducer = ...;\nString channel = ...;\nString payload = ...;\nmessageProducer.send(destination, MessageBuilder.withPayload(payload).build())\nA consumer service uses the MessageConsumer interface to subscribe to messages:\nMessageConsumer messageConsumer;\nmessageConsumer.subscribe(subscriberId, Collections.singleton(destination), \nmessage -> { ... })\nMessageProducer and MessageConsumer are the foundation of the higher-level APIs\nfor asynchronous request/response and domain event publishing.\n Let’s talk about how to publish and subscribe to events. \nWhat!? Why the Eventuate frameworks?\nThe code samples in this book use the open source Eventuate frameworks I’ve devel-\noped for transactional messaging, event sourcing, and sagas. I chose to use my\nframeworks because, unlike with, say, dependency injection and the Spring frame-\nwork, there are no widely adopted frameworks for many of the features the microser-\nvice architecture requires. Without the Eventuate Tram framework, many examples\nwould have to use the low-level messaging APIs directly, making them much more\ncomplicated and obscuring important concepts. Or they would use a framework that\nisn’t widely adopted, which would also provoke criticism.\nInstead, the examples use the Eventuate Tram frameworks, which have a simple,\neasy-to-understand API that hides the implementation details. You can use these\nframeworks in your applications. Alternatively, you can study the Eventuate Tram\nframeworks and reimplement the concepts yourself.\n \n",
      "content_length": 2405,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "102\nCHAPTER 3\nInterprocess communication in a microservice architecture\nDOMAIN EVENT PUBLISHING\nEventuate Tram has APIs for publishing and consuming domain events. Chapter 5\nexplains that domain events are events that are emitted by an aggregate (business\nobject) when it’s created, updated, or deleted. A service publishes a domain event\nusing the DomainEventPublisher interface. Here is an example:\nDomainEventPublisher domainEventPublisher;\nString accountId = ...;\nDomainEvent domainEvent = new AccountDebited(...);\ndomainEventPublisher.publish(\"Account\", accountId, Collections.singletonList(\ndomainEvent));\nA service consumes domain events using the DomainEventDispatcher. An example\nfollows:\nDomainEventHandlers domainEventHandlers = DomainEventHandlersBuilder\n.forAggregateType(\"Order\")\n.onEvent(AccountDebited.class, domainEvent -> { ... })\n.build();\nnew DomainEventDispatcher(\"eventDispatcherId\",\ndomainEventHandlers,\nmessageConsumer);\nEvents aren’t the only high-level messaging pattern supported by Eventuate Tram. It\nalso supports command/reply-based messaging. \nCOMMAND/REPLY-BASED MESSAGING\nA client can send a command message to a service using the CommandProducer inter-\nface. For example\nCommandProducer commandProducer = ...;\nMap<String, String> extraMessageHeaders = Collections.emptyMap();\nString commandId = commandProducer.send(\"CustomerCommandChannel\",\nnew DoSomethingCommand(),\n\"ReplyToChannel\",\nextraMessageHeaders);\nA service consumes command messages using the CommandDispatcher class. Command-\nDispatcher uses the MessageConsumer interface to subscribe to specified events. It dis-\npatches each command message to the appropriate handler method. Here’s an example:\nCommandHandlers commandHandlers =CommandHandlersBuilder\n.fromChannel(commandChannel)\n.onMessage(DoSomethingCommand.class, (command) -\n> { ... ; return withSuccess(); })\n.build();\n \n",
      "content_length": 1874,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "103\nUsing asynchronous messaging to improve availability\nCommandDispatcher dispatcher = new CommandDispatcher(\"subscribeId\", \ncommandHandlers, messageConsumer, messageProducer);\nThroughout this book, you’ll see code examples that use these APIs for sending and\nreceiving messages.\n As you’ve seen, the Eventuate Tram framework implements transactional messag-\ning for Java applications. It provides a low-level API for sending and receiving messages\ntransactionally. It also provides the higher-level APIs for publishing and consuming\ndomain events and for sending and processing commands.\n Let’s now look at a service design approach that uses asynchronous messaging to\nimprove availability. \n3.4\nUsing asynchronous messaging to improve availability\nAs you’ve seen, a variety of IPC mechanisms have different trade-offs. One particular\ntrade-off is how your choice of IPC mechanism impacts availability. In this section,\nyou’ll learn that synchronous communication with other services as part of request\nhandling reduces application availability. As a result, you should design your services\nto use asynchronous messaging whenever possible.\n Let’s first look at the problem with synchronous communication and how it\nimpacts availability.\n3.4.1\nSynchronous communication reduces availability\nREST is an extremely popular IPC mechanism. You may be tempted to use it for inter-\nservice communication. The problem with REST, though, is that it’s a synchronous\nprotocol: an HTTP client must wait for the service to send a response. Whenever\nservices communicate using a synchronous protocol, the availability of the applica-\ntion is reduced.\n To see why, consider the scenario shown in figure 3.15. The Order Service has a\nREST API for creating an Order. It invokes the Consumer Service and the Restaurant\nService to validate the Order. Both of those services also have REST APIs.\nClient\nOrder\nService\nConsumer\nService\nRestaurant\nService\nPOST/orders\nGET/consumers/id\nGET/restaurant/id\nFigure 3.15\nThe Order Service invokes other services using REST. It’s straightforward, but it \nrequires all the services to be simultaneously available, which reduces the availability of the API.\n \n",
      "content_length": 2179,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "104\nCHAPTER 3\nInterprocess communication in a microservice architecture\nThe sequence of steps for creating an order is as follows:\n1\nClient makes an HTTP POST /orders request to the Order Service.\n2\nOrder Service retrieves consumer information by making an HTTP GET\n/consumers/id request to the Consumer Service.\n3\nOrder Service retrieves restaurant information by making an HTTP GET\n/restaurant/id request to the Restaurant Service.\n4\nOrder Taking validates the request using the consumer and restaurant infor-\nmation.\n5\nOrder Taking creates an Order.\n6\nOrder Taking sends an HTTP response to the client.\nBecause these services use HTTP, they must all be simultaneously available in order\nfor the FTGO application to process the CreateOrder request. The FTGO application\ncouldn’t create orders if any one of these three services is down. Mathematically\nspeaking, the availability of a system operation is the product of the availability of the\nservices that are invoked by that operation. If the Order Service and the two services\nthat it invokes are 99.5% available, the overall availability is 99.5%3 = 98.5%, which is\nsignificantly less. Each additional service that participates in handling a request fur-\nther reduces availability.\n This problem isn’t specific to REST-based communication. Availability is reduced\nwhenever a service can only respond to its client after receiving a response from\nanother service. This problem exists even if services communicate using request/\nresponse style interaction over asynchronous messaging. For example, the availability\nof the Order Service would be reduced if it sent a message to the Consumer Service\nvia a message broker and then waited for a response.\n If you want to maximize availability, you must minimize the amount of synchro-\nnous communication. Let’s look at how to do that. \n3.4.2\nEliminating synchronous interaction\nThere are a few different ways to reduce the amount of synchronous communication\nwith other services while handling synchronous requests. One solution is to avoid the\nproblem entirely by defining services that only have asynchronous APIs. That’s not\nalways possible, though. For example, public APIs are commonly RESTful. Services\nare therefore sometimes required to have synchronous APIs.\n Fortunately, there are ways to handle synchronous requests without making syn-\nchronous requests. Let’s talk about the options.\nUSE ASYNCHRONOUS INTERACTION STYLES\nIdeally, all interactions should be done using the asynchronous interaction styles\ndescribed earlier in this chapter. For example, say a client of the FTGO application\nused an asynchronous request/asynchronous response style of interaction to create\norders. A client creates an order by sending a request message to the Order Service.\n \n",
      "content_length": 2770,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "105\nUsing asynchronous messaging to improve availability\nThis service then asynchronously exchanges messages with other services and eventu-\nally sends a reply message to the client. Figure 3.16 shows the design.\nThe client and the services communicate asynchronously by sending messages via\nmessaging channels. No participant in this interaction is ever blocked waiting for a\nresponse.\n Such an architecture would be extremely resilient, because the message broker\nbuffers messages until they can be consumed. The problem, however, is that services\noften have an external API that uses a synchronous protocol such as REST, so it must\nrespond to requests immediately.\n If a service has a synchronous API, one way to improve availability is to replicate\ndata. Let’s see how that works. \nREPLICATE DATA\nOne way to minimize synchronous requests during request processing is to replicate\ndata. A service maintains a replica of the data that it needs when processing requests.\nIt keeps the replica up-to-date by subscribing to events published by the services that\nown the data. For example, Order Service could maintain a replica of data owned by\nConsumer Service and Restaurant Service. This would enable Order Service to\nhandle a request to create an order without having to interact with those services.\nFigure 3.17 shows the design.\n Consumer Service and Restaurant Service publish events whenever their data\nchanges. Order Service subscribes to those events and updates its replica.\n In some situations, replicating data is a useful approach. For example, chapter 5\ndescribes how Order Service replicates data from Restaurant Service so that it can\nvalidate and price menu items. One drawback of replication is that it can sometimes\nrequire the replication of large amounts of data, which is inefficient. For example, it\nmay not be practical for Order Service to maintain a replica of the data owned by\nConsumer Service, due to the large number of consumers. Another drawback of\nClient\nConsumer\nService\nRestaurant\nService\nOrder request\nchannel\nConsumer request\nchannel\nOrder Service\nreply channel\nRestaurant request\nchannel\nCreate order\nrequest\nCreate order\nresponse\nOrder\nService\nClient reply\nchannel\nFigure 3.16\nThe FTGO application has higher availability if its services communicate using asynchronous \nmessaging instead of synchronous calls.\n \n",
      "content_length": 2350,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "106\nCHAPTER 3\nInterprocess communication in a microservice architecture\nreplication is that it doesn’t solve the problem of how a service updates data owned by\nother services.\n One way to solve that problem is for a service to delay interacting with other ser-\nvices until after it responds to its client. We’ll next look at how that works.\nFINISH PROCESSING AFTER RETURNING A RESPONSE\nAnother way to eliminate synchronous communication during request processing is\nfor a service to handle a request as follows:\n1\nValidate the request using only the data available locally.\n2\nUpdate its database, including inserting messages into the OUTBOX table.\n3\nReturn a response to its client.\nWhile handling a request, the service doesn’t synchronously interact with any other\nservices. Instead, it asynchronously sends messages to other services. This approach\nensures that the services are loosely coupled. As you’ll learn in the next chapter, this is\noften implemented using a saga.\n For example, if Order Service uses this approach, it creates an order in a PENDING\nstate and then validates the order asynchronously by exchanging messages with other\nservices. Figure 3.18 shows what happens when the createOrder() operation is\ninvoked. The sequence of events is as follows:\n1\nOrder Service creates an Order in a PENDING state.\n2\nOrder Service returns a response to its client containing the order ID.\n3\nOrder Service sends a ValidateConsumerInfo message to Consumer Service.\nServices publish events\nwhen their data changes.\nReplicated data enables Order Service to\nhandle the createOrder() request without\nsynchronously invoking services.\nRestaurant\nService\nConsumer event\nchannel\nRestaurant event\nchannel\nOrder\nService\nConsumer\nService\nConsumer Service database\n«table»\nCONSUMERS\ncreateOrder()\nRestaurant Service database\n«table»\nRESTAURANTS\nOrder Service database\n«table»\nORDERS\n«table»\nCONSUMERS\n«table»\nRESTAURANTS\nFigure 3.17\nOrder Service is self-contained because it has replicas of the consumer and restaurant data.\n \n",
      "content_length": 2021,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "107\nUsing asynchronous messaging to improve availability\n4\nOrder Service sends a ValidateOrderDetails message to Restaurant Service.\n5\nConsumer Service receives a ValidateConsumerInfo message, verifies the con-\nsumer can place an order, and sends a ConsumerValidated message to Order\nService.\n6\nRestaurant Service receives a ValidateOrderDetails message, verifies the\nmenu item are valid and that the restaurant can deliver to the order’s delivery\naddress, and sends an OrderDetailsValidated message to Order Service.\n7\nOrder Service receives ConsumerValidated and OrderDetailsValidated and\nchanges the state of the order to VALIDATED.\n8\n…\nOrder Service can receive the ConsumerValidated and OrderDetailsValidated mes-\nsages in either order. It keeps track of which message it receives first by changing the\nstate of the order. If it receives the ConsumerValidated first, it changes the state of the\norder to CONSUMER_VALIDATED, whereas if it receives the OrderDetailsValidated mes-\nsage first, it changes its state to ORDER_DETAILS_VALIDATED. Order Service changes\nthe state of the Order to VALIDATED when it receives the other message.\nSynchronous\nKey\nAsynchronous\nOrder Service\nClient\nConsumer Service\nRestaurant Service\n...\ncreateOrder\nAsynchronous\nSynchronous\ncreate order\nupdate order\nupdate order\ncreateOrder\nValidateConsumerInfo\nValidateOrderDetails\nConsumerValidated\nOrderDetailsValidated\n...\nFigure 3.18\nOrder Service creates an order without invoking any other service. It then asynchronously \nvalidates the newly created Order by exchanging messages with other services, including Consumer Service \nand Restaurant Service.\n \n",
      "content_length": 1637,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "108\nCHAPTER 3\nInterprocess communication in a microservice architecture\n After the Order has been validated, Order Service completes the rest of the order-\ncreation process, discussed in the next chapter. What’s nice about this approach is\nthat even if Consumer Service is down, for example, Order Service still creates orders\nand responds to its clients. Eventually, Consumer Service will come back up and pro-\ncess any queued messages, and orders will be validated.\n A drawback of a service responding before fully processing a request is that it\nmakes the client more complex. For example, Order Service makes minimal guaran-\ntees about the state of a newly created order when it returns a response. It creates the\norder and returns immediately before validating the order and authorizing the con-\nsumer’s credit card. Consequently, in order for the client to know whether the order\nwas successfully created, either it must periodically poll or Order Service must send it\na notification message. As complex as it sounds, in many situations this is the pre-\nferred approach—especially because it also addresses the distributed transaction man-\nagement issues I discuss in the next chapter. In chapters 4 and 5, for example, I\ndescribe how Order Service uses this approach. \nSummary\nThe microservice architecture is a distributed architecture, so interprocess\ncommunication plays a key role.\nIt’s essential to carefully manage the evolution of a service’s API. Backward-\ncompatible changes are the easiest to make because they don’t impact clients. If\nyou make a breaking change to a service’s API, it will typically need to support\nboth the old and new versions until its clients have been upgraded.\nThere are numerous IPC technologies, each with different trade-offs. One key\ndesign decision is to choose either a synchronous remote procedure invocation\npattern or the asynchronous Messaging pattern. Synchronous remote proce-\ndure invocation-based protocols, such as REST, are the easiest to use. But ser-\nvices should ideally communicate using asynchronous messaging in order to\nincrease availability.\nIn order to prevent failures from cascading through a system, a service client\nthat uses a synchronous protocol must be designed to handle partial failures,\nwhich are when the invoked service is either down or exhibiting high latency. In\nparticular, it must use timeouts when making requests, limit the number of out-\nstanding requests, and use the Circuit breaker pattern to avoid making calls to a\nfailing service.\nAn architecture that uses synchronous protocols must include a service discov-\nery mechanism in order for clients to determine the network location of a ser-\nvice instance. The simplest approach is to use the service discovery mechanism\nimplemented by the deployment platform: the Server-side discovery and 3rd\nparty registration patterns. But an alternative approach is to implement service\ndiscovery at the application level: the Client-side discovery and Self registration\n \n",
      "content_length": 3007,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "109\nSummary\npatterns. It’s more work, but it does handle the scenario where services are run-\nning on multiple deployment platforms.\nA good way to design a messaging-based architecture is to use the messages and\nchannels model, which abstracts the details of the underlying messaging system.\nYou can then map that design to a specific messaging infrastructure, which is\ntypically message broker–based.\nOne key challenge when using messaging is atomically updating the database\nand publishing a message. A good solution is to use the Transactional outbox\npattern and first write the message to the database as part of the database trans-\naction. A separate process then retrieves the message from the database using\neither the Polling publisher pattern or the Transaction log tailing pattern and\npublishes it to the message broker. \n \n",
      "content_length": 836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "110\nManaging transactions\nwith sagas\nWhen Mary started investigating the microservice architecture, one of her biggest\nconcerns was how to implement transactions that span multiple services. Transac-\ntions are an essential ingredient of every enterprise application. Without transac-\ntions it would be impossible to maintain data consistency.\n ACID (Atomicity, Consistency, Isolation, Durability) transactions greatly simplify\nthe job of the developer by providing the illusion that each transaction has exclu-\nsive access to the data. In a microservice architecture, transactions that are within a\nsingle service can still use ACID transactions. The challenge, however, lies in imple-\nmenting transactions for operations that update data owned by multiple services.\nThis chapter covers\nWhy distributed transactions aren’t a good fit for \nmodern applications\nUsing the Saga pattern to maintain data \nconsistency in a microservice architecture\nCoordinating sagas using choreography and \norchestration\nUsing countermeasures to deal with the lack of \nisolation\n \n",
      "content_length": 1064,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "111\nTransaction management in a microservice architecture\nFor example, as described in chapter 2, the createOrder() operation spans numer-\nous services, including Order Service, Kitchen Service, and Accounting Service.\nOperations such as these need a transaction management mechanism that works\nacross services.\n Mary discovered that, as mentioned in chapter 2, the traditional approach to dis-\ntributed transaction management isn’t a good choice for modern applications.\nInstead of an ACID transactions, an operation that spans services must use what’s\nknown as a saga, a message-driven sequence of local transactions, to maintain data\nconsistency. One challenge with sagas is that they are ACD (Atomicity, Consistency,\nDurability). They lack the isolation feature of traditional ACID transactions. As a\nresult, an application must use what are known as countermeasures, design techniques\nthat prevent or reduce the impact of concurrency anomalies caused by the lack of\nisolation.\n In many ways, the biggest obstacle that Mary and the FTGO developers will face\nwhen adopting microservices is moving from a single database with ACID transactions\nto a multi-database architecture with ACD sagas. They’re used to the simplicity of the\nACID transaction model. But in reality, even monolithic applications such as the FTGO\napplication typically don’t use textbook ACID transactions. For example, many appli-\ncations use a lower transaction isolation level in order to improve performance. Also,\nmany important business processes, such as transferring money between accounts at\ndifferent banks, are eventually consistent. Not even Starbucks uses two-phase commit\n(www.enterpriseintegrationpatterns.com/ramblings/18_starbucks.html).\n I begin this chapter by looking at the challenges of transaction management in the\nmicroservice architecture and explain why the traditional approach to distributed\ntransaction management isn’t an option. Next I explain how to maintain data consis-\ntency using sagas. After that I look at the two different ways of coordinating sagas:\nchoreography, where participants exchange events without a centralized point of con-\ntrol, and orchestration, where a centralized controller tells the saga participants what\noperation to perform. I discuss how to use countermeasures to prevent or reduce the\nimpact of concurrency anomalies caused by the lack of isolation between sagas. Finally, I\ndescribe the implementation of an example saga.\n Let’s start by taking a look at the challenge of managing transactions in a micro-\nservice architecture.\n4.1\nTransaction management in a microservice \narchitecture\nAlmost every request handled by an enterprise application is executed within a data-\nbase transaction. Enterprise application developers use frameworks and libraries that\nsimplify transaction management. Some frameworks and libraries provide a program-\nmatic API for explicitly beginning, committing, and rolling back transactions. Other\nframeworks, such as the Spring framework, provide a declarative mechanism. Spring\nprovides an @Transactional annotation that arranges for method invocations to be\n \n",
      "content_length": 3127,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "112\nCHAPTER 4\nManaging transactions with sagas\nautomatically executed within a transaction. As a result, it’s straightforward to write\ntransactional business logic.\n Or, to be more precise, transaction management is straightforward in a monolithic\napplication that accesses a single database. Transaction management is more chal-\nlenging in a complex monolithic application that uses multiple databases and mes-\nsage brokers. And in a microservice architecture, transactions span multiple services,\neach of which has its own database. In this situation, the application must use a more\nelaborate mechanism to manage transactions. As you’ll learn, the traditional approach\nof using distributed transactions isn’t a viable option for modern applications. Instead, a\nmicroservices-based application must use sagas.\n Before I explain sagas, let’s first look at why transaction management is challeng-\ning in a microservice architecture.\n4.1.1\nThe need for distributed transactions in a microservice \narchitecture\nImagine that you’re the FTGO developer responsible for implementing the create-\nOrder() system operation. As described in chapter 2, this operation must verify that\nthe consumer can place an order, verify the order details, authorize the consumer’s\ncredit card, and create an Order in the database. It’s relatively straightforward to\nimplement this operation in the monolithic FTGO application. All the data required\nto validate the order is readily accessible. What’s more, you can use an ACID transac-\ntion to ensure data consistency. You might use Spring’s @Transactional annotation\non the createOrder() service method.\n In contrast, implementing the same operation in a microservice architecture is\nmuch more complicated. As figure 4.1 shows, the needed data is scattered around\nmultiple services. The createOrder() operation accesses data in numerous services.\nIt reads data from Consumer Service and updates data in Order Service, Kitchen\nService, and Accounting Service.\n Because each service has its own database, you need to use a mechanism to main-\ntain data consistency across those databases. \n4.1.2\nThe trouble with distributed transactions\nThe traditional approach to maintaining data consistency across multiple services,\ndatabases, or message brokers is to use distributed transactions. The de facto standard\nfor distributed transaction management is the X/Open Distributed Transaction Pro-\ncessing (DTP) Model (X/Open XA—see https://en.wikipedia.org/wiki/X/Open_XA).\nXA uses two-phase commit (2PC) to ensure that all participants in a transaction either\ncommit or rollback. An XA-compliant technology stack consists of XA-compliant data-\nbases and message brokers, database drivers, and messaging APIs, and an interprocess\ncommunication mechanism that propagates the XA global transaction ID. Most SQL\ndatabases are XA compliant, as are some message brokers. Java EE applications can,\nfor example, use JTA to perform distributed transactions.\n \n",
      "content_length": 2971,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "113\nTransaction management in a microservice architecture\nAs simple as this sounds, there are a variety of problems with distributed transac-\ntions. One problem is that many modern technologies, including NoSQL databases\nsuch as MongoDB and Cassandra, don’t support them. Also, distributed transactions\naren’t supported by modern message brokers such as RabbitMQ and Apache Kafka.\nAs a result, if you insist on using distributed transactions, you can’t use many mod-\nern technologies.\n Another problem with distributed transactions is that they are a form of synchro-\nnous IPC, which reduces availability. In order for a distributed transaction to commit,\nall the participating services must be available. As described in chapter 3, the availabil-\nity is the product of the availability of all of the participants in the transaction. If a dis-\ntributed transaction involves two services that are 99.5% available, then the overall\navailability is 99%, which is significantly less. Each additional service involved in a dis-\ntributed transaction further reduces availability. There is even Eric Brewer’s CAP theo-\nrem, which states that a system can only have two of the following three properties:\nAccount\nTicket\nConsumer\nData consistency required\nWrites\nWrites\ncreateOrder()\nReads\nAccounting Service\nKitchen Service\nOrder\nOrder Service\nConsumer Service\nThe createOrder() operation reads from\nConsumer Service and updates data\nin Order Service, Kitchen Service,\nand Accounting Service.\nOrder\ncontroller\nFigure 4.1\nThe createOrder() operation updates data in several services. It must use a \nmechanism to maintain data consistency across those services.\n \n",
      "content_length": 1654,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "114\nCHAPTER 4\nManaging transactions with sagas\nconsistency, availability, and partition tolerance (https://en.wikipedia.org/wiki/CAP\n_theorem). Today, architects prefer to have a system that’s available rather than one\nthat’s consistent.\n On the surface, distributed transactions are appealing. From a developer’s per-\nspective, they have the same programming model as local transactions. But because of\nthe problems mentioned so far, distributed transactions aren’t a viable technology for\nmodern applications. Chapter 3 described how to send messages as part of a database\ntransaction without using distributed transactions. To solve the more complex prob-\nlem of maintaining data consistency in a microservice architecture, an application\nmust use a different mechanism that builds on the concept of loosely coupled, asyn-\nchronous services. This is where sagas come in. \n4.1.3\nUsing the Saga pattern to maintain data consistency\nSagas are mechanisms to maintain data consistency in a microservice architecture\nwithout having to use distributed transactions. You define a saga for each system com-\nmand that needs to update data in multiple services. A saga is a sequence of local\ntransactions. Each local transaction updates data within a single service using the\nfamiliar ACID transaction frameworks and libraries mentioned earlier.\nThe system operation initiates the first step of the saga. The completion of a local\ntransaction triggers the execution of the next local transaction. Later, in section 4.2,\nyou’ll see how coordination of the steps is implemented using asynchronous messag-\ning. An important benefit of asynchronous messaging is that it ensures that all the\nsteps of a saga are executed, even if one or more of the saga’s participants is temporar-\nily unavailable.\n Sagas differ from ACID transactions in a couple of important ways. As I describe in\ndetail in section 4.3, they lack the isolation property of ACID transactions. Also, because\neach local transaction commits its changes, a saga must be rolled back using compensat-\ning transactions. I talk more about compensating transactions later in this section. Let’s\ntake a look at an example saga.\nAN EXAMPLE SAGA: THE CREATE ORDER SAGA\nThe example saga used throughout this chapter is the Create Order Saga, which is\nshown in figure 4.2. The Order Service implements the createOrder() operation\nusing this saga. The saga’s first local transaction is initiated by the external request to\ncreate an order. The other five local transactions are each triggered by completion of\nthe previous one.\nPattern: Saga\nMaintain data consistency across services using a sequence of local transactions\nthat are coordinated using asynchronous messaging. See http://microservices.io/\npatterns/data/saga.html.\n \n",
      "content_length": 2771,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "115\nTransaction management in a microservice architecture\nThis saga consists of the following local transactions:\n1\nOrder Service—Create an Order in an APPROVAL_PENDING state.\n2\nConsumer Service—Verify that the consumer can place an order.\n3\nKitchen Service—Validate order details and create a Ticket in the CREATE\n_PENDING.\n4\nAccounting Service—Authorize consumer’s credit card.\n5\nKitchen Service—Change the state of the Ticket to AWAITING_ACCEPTANCE.\n6\nOrder Service—Change the state of the Order to APPROVED.\nLater, in section 4.2, I describe how the services that participate in a saga communi-\ncate using asynchronous messaging. A service publishes a message when a local trans-\naction completes. This message then triggers the next step in the saga. Not only does\nusing messaging ensure the saga participants are loosely coupled, it also guarantees\nthat a saga completes. That’s because if the recipient of a message is temporarily\nunavailable, the message broker buffers the message until it can be delivered.\n On the surface, sagas seem straightforward, but there are a few challenges to using\nthem. One challenge is the lack of isolation between sagas. Section 4.3 describes how\nto handle this problem. Another challenge is rolling back changes when an error\noccurs. Let’s take a look at how to do that. \nSAGAS USE COMPENSATING TRANSACTIONS TO ROLL BACK CHANGES\nA great feature of traditional ACID transactions is that the business logic can easily\nroll back a transaction if it detects the violation of a business rule. It executes a ROLL-\nBACK statement, and the database undoes all the changes made so far. Unfortunately,\nsagas can’t be automatically rolled back, because each step commits its changes to the\nlocal database. This means, for example, that if the authorization of the credit card\nfails in the fourth step of the Create Order Saga, the FTGO application must explicitly\nOrder Service\nSaga\nCreate order\nTxn:1\nApprove order\nTxn:6\nConsumer Service\nCreate order\nTxn:1\nVerify consumer\nTxn:2\nKitchen Service\nCreate ticket\nTxn:3\nApprove ticket\nTxn:5\nAccounting Service\nAuthorize card\nTxn:4\nFigure 4.2\nCreating an Order using a saga. The createOrder() operation is implemented by a \nsaga that consists of local transactions in several services.\n \n",
      "content_length": 2264,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "116\nCHAPTER 4\nManaging transactions with sagas\nundo the changes made by the first three steps. You must write what are known as com-\npensating transactions.\n Suppose that the (n + 1)th transaction of a saga fails. The effects of the previous n\ntransactions must be undone. Conceptually, each of those steps, Ti, has a correspond-\ning compensating transaction, Ci, which undoes the effects of the Ti. To undo the\neffects of those first n steps, the saga must execute each Ci in reverse order. The\nsequence of steps is T1 … Tn, Cn … C1, as shown in figure 4.3. In this example, Tn+1\nfails, which requires steps T1 … Tn to be undone.\nThe saga executes the compensation transactions in reverse order of the forward\ntransactions: Cn … C1. The mechanics of sequencing the Cis aren’t any different than\nsequencing the Tis. The completion of Ci must trigger the execution of Ci-1.\n Consider, for example, the Create Order Saga. This saga can fail for a variety of\nreasons:\nThe consumer information is invalid or the consumer isn’t allowed to create\norders.\nThe restaurant information is invalid or the restaurant is unable to accept orders.\nThe authorization of the consumer’s credit card fails.\nIf a local transaction fails, the saga’s coordination mechanism must execute compen-\nsating transactions that reject the Order and possibly the Ticket. Table 4.1 shows the\ncompensating transactions for each step of the Create Order Saga. It’s important to\nnote that not all steps need compensating transactions. Read-only steps, such as verify-\nConsumerDetails(), don’t need compensating transactions. Nor do steps such as\nauthorizeCreditCard() that are followed by steps that always succeed.\n Section 4.3 discusses how the first three steps of the Create Order Saga are termed\ncompensatable transactions because they’re followed by steps that can fail, how the\nfourth step is termed the saga’s pivot transaction because it’s followed by steps that\nSaga\nT1\n...\nTn\nTn+1\nFAILS\nCn\n...\nC\nThe changes made by T1...Tn\nhave been committed.\nThe compensating transactions undo\nthe changes made by T1...Tn.\n1\nFigure 4.3\nWhen a step of a saga fails because of a business rule violation, the saga must explicitly \nundo the updates made by previous steps by executing compensating transactions.\n \n",
      "content_length": 2275,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "117\nCoordinating sagas\nnever fail, and how the last two steps are termed retriable transactions because they\nalways succeed.\n To see how compensating transactions are used, imagine a scenario where the\nauthorization of the consumer’s credit card fails. In this scenario, the saga executes\nthe following local transactions:\n1\nOrder Service—Create an Order in an APPROVAL_PENDING state.\n2\nConsumer Service—Verify that the consumer can place an order.\n3\nKitchen Service—Validate order details and create a Ticket in the CREATE\n_PENDING state.\n4\nAccounting Service—Authorize consumer’s credit card, which fails.\n5\nKitchen Service—Change the state of the Ticket to CREATE_REJECTED.\n6\nOrder Service—Change the state of the Order to REJECTED.\nThe fifth and sixth steps are compensating transactions that undo the updates made\nby Kitchen Service and Order Service, respectively. A saga’s coordination logic is\nresponsible for sequencing the execution of forward and compensating transactions.\nLet’s look at how that works. \n4.2\nCoordinating sagas\nA saga’s implementation consists of logic that coordinates the steps of the saga.\nWhen a saga is initiated by system command, the coordination logic must select and\ntell the first saga participant to execute a local transaction. Once that transaction\ncompletes, the saga’s sequencing coordination selects and invokes the next saga\nparticipant. This process continues until the saga has executed all the steps. If any\nlocal transaction fails, the saga must execute the compensating transactions in\nreverse order. There are a couple of different ways to structure a saga’s coordina-\ntion logic:\nChoreography—Distribute the decision making and sequencing among the saga\nparticipants. They primarily communicate by exchanging events.\nTable 4.1\nThe compensating transactions for the Create Order Saga\nStep\nService\nTransaction\nCompensating transaction\n1\nOrder Service\ncreateOrder()\nrejectOrder()\n2\nConsumer Service\nverifyConsumerDetails()\n—\n3\nKitchen Service\ncreateTicket()\nrejectTicket()\n4\nAccounting Service\nauthorizeCreditCard()\n—\n5\nKitchen Service\napproveTicket()\n—\n6\nOrder Service\napproveOrder()\n—\n \n",
      "content_length": 2139,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "118\nCHAPTER 4\nManaging transactions with sagas\nOrchestration—Centralize a saga’s coordination logic in a saga orchestrator class.\nA saga orchestrator sends command messages to saga participants telling them\nwhich operations to perform.\nLet’s look at each option, starting with choreography.\n4.2.1\nChoreography-based sagas\nOne way you can implement a saga is by using choreography. When using choreogra-\nphy, there’s no central coordinator telling the saga participants what to do. Instead,\nthe saga participants subscribe to each other’s events and respond accordingly. To\nshow how choreography-based sagas work, I’ll first describe an example. After that, I’ll\ndiscuss a couple of design issues that you must address. Then I’ll discuss the benefits\nand drawbacks of using choreography.\nIMPLEMENTING THE CREATE ORDER SAGA USING CHOREOGRAPHY\nFigure 4.4 shows the design of the choreography-based version of the Create Order\nSaga. The participants communicate by exchanging events. Each participant, starting\nwith the Order Service, updates its database and publishes an event that triggers the\nnext participant.\nAccounting Service\n4. createPendingAuthorization()\n6. authorizeCard()\nKitchen Service\n3. createTicket()\n6. approveTicket()\nOrder\nService\n1. createOrder()\n7. approveOrder()\nConsumer Service\n2. verifyConsumerDetails()\nOrder events\nMessage broker\nConsumer veriﬁed\nPublish\nKey\nSubscribe\nConsumer events\nTicket events\nCredit card events\n2\nOrder created\n1\nTicket created\n3\nCredit card authorized\n5\n6\n4\n5a\n7\n5b\nFigure 4.4\nImplementing the Create Order Saga using choreography. The saga participants communicate by \nexchanging events.\n \n",
      "content_length": 1641,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "119\nCoordinating sagas\nThe happy path through this saga is as follows:\n1\nOrder Service creates an Order in the APPROVAL_PENDING state and publishes\nan OrderCreated event.\n2\nConsumer Service consumes the OrderCreated event, verifies that the con-\nsumer can place the order, and publishes a ConsumerVerified event.\n3\nKitchen Service consumes the OrderCreated event, validates the Order, cre-\nates a Ticket in a CREATE_PENDING state, and publishes the TicketCreated event.\n4\nAccounting Service consumes the OrderCreated event and creates a Credit-\nCardAuthorization in a PENDING state.\n5\nAccounting Service consumes the TicketCreated and ConsumerVerified\nevents, charges the consumer’s credit card, and publishes the CreditCard-\nAuthorized event.\n6\nKitchen Service consumes the CreditCardAuthorized event and changes the\nstate of the Ticket to AWAITING_ACCEPTANCE.\n7\nOrder Service receives the CreditCardAuthorized events, changes the state of\nthe Order to APPROVED, and publishes an OrderApproved event.\nThe Create Order Saga must also handle the scenario where a saga participant rejects\nthe Order and publishes some kind of failure event. For example, the authorization of\nthe consumer’s credit card might fail. The saga must execute the compensating trans-\nactions to undo what’s already been done. Figure 4.5 shows the flow of events when\nthe AccountingService can’t authorize the consumer’s credit card.\n The sequence of events is as follows:\n1\nOrder Service creates an Order in the APPROVAL_PENDING state and publishes\nan OrderCreated event.\n2\nConsumer Service consumes the OrderCreated event, verifies that the con-\nsumer can place the order, and publishes a ConsumerVerified event.\n3\nKitchen Service consumes the OrderCreated event, validates the Order, creates\na Ticket in a CREATE_PENDING state, and publishes the TicketCreated event.\n4\nAccounting Service consumes the OrderCreated event and creates a Credit-\nCardAuthorization in a PENDING state.\n5\nAccounting Service consumes the TicketCreated and ConsumerVerified\nevents, charges the consumer’s credit card, and publishes a Credit Card\nAuthorization Failed event.\n6\nKitchen Service consumes the Credit Card Authorization Failed event and\nchanges the state of the Ticket to REJECTED.\n7\nOrder Service consumes the Credit Card Authorization Failed event and\nchanges the state of the Order to REJECTED.\nAs you can see, the participants of choreography-based sagas interact using publish/\nsubscribe. Let’s take a closer look at some issues you’ll need to consider when imple-\nmenting publish/subscribe-based communication for your sagas. \n \n",
      "content_length": 2597,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "120\nCHAPTER 4\nManaging transactions with sagas\nRELIABLE EVENT-BASED COMMUNICATION\nThere are a couple of interservice communication-related issues that you must con-\nsider when implementing choreography-based sagas. The first issue is ensuring that a\nsaga participant updates its database and publishes an event as part of a database\ntransaction. Each step of a choreography-based saga updates the database and pub-\nlishes an event. For example, in the Create Order Saga, Kitchen Service receives a\nConsumer Verified event, creates a Ticket, and publishes a Ticket Created event.\nIt’s essential that the database update and the publishing of the event happen atomi-\ncally. Consequently, to communicate reliably, the saga participants must use transac-\ntional messaging, described in chapter 3.\n The second issue you need to consider is ensuring that a saga participant must\nbe able to map each event that it receives to its own data. For example, when Order\nService receives a Credit Card Authorized event, it must be able to look up the\ncorresponding Order. The solution is for a saga participant to publish events con-\ntaining a correlation id, which is data that enables other participants to perform the\nmapping.\nAccounting Service\n4. createPendingAuthorization()\n6. authorizeCard()\nKitchen Service\n3. createTicket()\n6. rejectTicket()\nOrder\nService\n1. createOrder()\n7. rejectOrder()\nConsumer Service\n2. verifyConsumerDetails()\nOrder events\nMessage broker\nConsumer veriﬁed\nPublish\nKey\nSubscribe\nConsumer events\nTicket events\nCredit card events\n2\nOrder created\n1\nTicket created\n3\nCredit card authorization failed\n5\n6\n4\n5a\n7\n5b\nFigure 4.5\nThe sequence of events in the Create Order Saga when the authorization of the consumer’s credit \ncard fails. Accounting Service publishes the Credit Card Authorization Failed event, which causes \nKitchen Service to reject the Ticket, and Order Service to reject the Order.\n \n",
      "content_length": 1914,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "121\nCoordinating sagas\n For example, the participants of the Create Order Saga can use the orderId as a\ncorrelation ID that’s passed from one participant to the next. Accounting Service pub-\nlishes a Credit Card Authorized event containing the orderId from the Ticket-\nCreated event. When Order Service receives a Credit Card Authorized event, it uses\nthe orderId to retrieve the corresponding Order. Similarly, Kitchen Service uses the\norderId from that event to retrieve the corresponding Ticket. \nBENEFITS AND DRAWBACKS OF CHOREOGRAPHY-BASED SAGAS\nChoreography-based sagas have several benefits:\nSimplicity—Services publish events when they create, update, or delete business\nobjects.\nLoose coupling —The participants subscribe to events and don’t have direct knowl-\nedge of each other.\nAnd there are some drawbacks:\nMore difficult to understand—Unlike with orchestration, there isn’t a single place\nin the code that defines the saga. Instead, choreography distributes the imple-\nmentation of the saga among the services. Consequently, it’s sometimes difficult\nfor a developer to understand how a given saga works.\nCyclic dependencies between the services—The saga participants subscribe to each\nother’s events, which often creates cyclic dependencies. For example, if you\ncarefully examine figure 4.4, you’ll see that there are cyclic dependencies, such\nas Order Service  Accounting Service  Order Service. Although this isn’t\nnecessarily a problem, cyclic dependencies are considered a design smell.\nRisk of tight coupling—Each saga participant needs to subscribe to all events that\naffect them. For example, Accounting Service must subscribe to all events that\ncause the consumer’s credit card to be charged or refunded. As a result, there’s\na risk that it would need to be updated in lockstep with the order lifecycle\nimplemented by Order Service.\nChoreography can work well for simple sagas, but because of these drawbacks it’s\noften better for more complex sagas to use orchestration. Let’s look at how orches-\ntration works. \n4.2.2\nOrchestration-based sagas\nOrchestration is another way to implement sagas. When using orchestration, you\ndefine an orchestrator class whose sole responsibility is to tell the saga participants\nwhat to do. The saga orchestrator communicates with the participants using command/\nasync reply-style interaction. To execute a saga step, it sends a command message to a\nparticipant telling it what operation to perform. After the saga participant has per-\nformed the operation, it sends a reply message to the orchestrator. The orchestrator\nthen processes the message and determines which saga step to perform next.\n \n",
      "content_length": 2661,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "122\nCHAPTER 4\nManaging transactions with sagas\n To show how orchestration-based sagas work, I’ll first describe an example. Then\nI’ll describe how to model orchestration-based sagas as state machines. I’ll discuss how\nto make use of transactional messaging to ensure reliable communication between\nthe saga orchestrator and the saga participants. I’ll then describe the benefits and\ndrawbacks of using orchestration-based sagas.\nIMPLEMENTING THE CREATE ORDER SAGA USING ORCHESTRATION\nFigure 4.6 shows the design of the orchestration-based version of the Create Order\nSaga. The saga is orchestrated by the CreateOrderSaga class, which invokes the saga\nparticipants using asynchronous request/response. This class keeps track of the pro-\ncess and sends command messages to saga participants, such as Kitchen Service and\nConsumer Service. The CreateOrderSaga class reads reply messages from its reply\nchannel and then determines the next step, if any, in the saga.\nAccounting\nService\nKitchen\nService\nConsumer\nService\nConsumer Service\nrequest channel\nConsumer veriﬁed\n2\n4\n6\nVerify consumer\n1\nApprove\nrestaurant\norder\n7\nApprove\norder\n8\nCreate\nticket\n3\nAuthorize\ncard\n5\nCard\nauthorized\nMessage broker\nOrder Service\nCommand message\nKey\nReply message\nCreate order\nsaga reply channel\nKitchen Service\nrequest channel\nAccounting Service\nrequest channel\nOrder Service\nrequest channel\nCreate\norder saga\norchestrator\nTicket created\nFigure 4.6\nImplementing the Create Order Saga using orchestration. Order Service \nimplements a saga orchestrator, which invokes the saga participants using asynchronous request/\nresponse.\n \n",
      "content_length": 1608,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "123\nCoordinating sagas\nOrder Service first creates an Order and a Create Order Saga orchestrator. After that,\nthe flow for the happy path is as follows:\n1\nThe saga orchestrator sends a Verify Consumer command to Consumer Service.\n2\nConsumer Service replies with a Consumer Verified message.\n3\nThe saga orchestrator sends a Create Ticket command to Kitchen Service.\n4\nKitchen Service replies with a Ticket Created message.\n5\nThe saga orchestrator sends an Authorize Card message to Accounting Service.\n6\nAccounting Service replies with a Card Authorized message.\n7\nThe saga orchestrator sends an Approve Ticket command to Kitchen Service.\n8\nThe saga orchestrator sends an Approve Order command to Order Service.\nNote that in final step, the saga orchestrator sends a command message to Order\nService, even though it’s a component of Order Service. In principle, the Create\nOrder Saga could approve the Order by updating it directly. But in order to be consis-\ntent, the saga treats Order Service as just another participant.\n Diagrams such as figure 4.6 each depict one scenario for a saga, but a saga is likely\nto have numerous scenarios. For example, the Create Order Saga has four scenarios.\nIn addition to the happy path, the saga can fail due to a failure in either Consumer\nService, Kitchen Service, or Accounting Service. It’s useful, therefore, to model a\nsaga as a state machine, because it describes all possible scenarios. \nMODELING SAGA ORCHESTRATORS AS STATE MACHINES\nA good way to model a saga orchestrator is as a state machine. A state machine con-\nsists of a set of states and a set of transitions between states that are triggered by\nevents. Each transition can have an action, which for a saga is the invocation of a\nsaga participant. The transitions between states are triggered by the completion of a\nlocal transaction performed by a saga participant. The current state and the specific\noutcome of the local transaction determine the state transition and what action, if\nany, to perform. There are also effective testing strategies for state machines. As a\nresult, using a state machine model makes designing, implementing, and testing\nsagas easier.\n Figure 4.7 shows the state machine model for the Create Order Saga. This state\nmachine consists of numerous states, including the following:\n\nVerifying Consumer—The initial state. When in this state, the saga is waiting\nfor the Consumer Service to verify that the consumer can place the order.\n\nCreating Ticket—The saga is waiting for a reply to the Create Ticket command.\n\nAuthorizing Card—Waiting for Accounting Service to authorize the con-\nsumer’s credit card.\n\nOrder Approved—A final state indicating that the saga completed successfully.\n\nOrder Rejected—A final state indicating that the Order was rejected by one of\nthe participants.\n \n",
      "content_length": 2819,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "124\nCHAPTER 4\nManaging transactions with sagas\nThe state machine also defines numerous state transitions. For example, the state\nmachine transitions from the Creating Ticket state to either the Authorizing Card\nor the Rejected Order state. It transitions to the Authorizing Card state when it\nreceives a successful reply to the Create Ticket command. Alternatively, if Kitchen\nService couldn’t create the Ticket, the state machine transitions to the Rejected\nOrder state.\n The state machine’s initial action is to send the VerifyConsumer command to\nConsumer Service. The response from Consumer Service triggers the next state tran-\nsition. If the consumer was successfully verified, the saga creates the Ticket and tran-\nsitions to the Creating Ticket state. But if the consumer verification failed, the saga\nrejects the Order and transitions to the Rejecting Order state. The state machine\nundergoes numerous other state transitions, driven by the responses from saga partici-\npants, until it reaches a final state of either Order Approved or Order Rejected. \nVeriﬁng\nconsumer\nRejecting\norder\nCreating\nticket\nAuthorizing\ncard\nRejecting\nticket\nApproving\nticket\nApproving\norder\nOrder\napproved\nOrder\nrejected\n/Send VerifyConsumer\nConsumerVeriﬁcationFailed/\nsend RejectOrder\nTicket creation failed/\nsend RejectOrder\nConsumerVeriﬁed/\nsend CreateRestaurantOrder\nTicket created/\nsend AuthorizeCard\nCard authorized/\nsend ApproveTicket\nTicket approved/\nsend ApproveOrder\nOrder approved\nCard authorization failed/\nsend RejectTicket\nFigure 4.7\nThe state machine model for the Create Order Saga\n \n",
      "content_length": 1586,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "125\nCoordinating sagas\nSAGA ORCHESTRATION AND TRANSACTIONAL MESSAGING\nEach step of an orchestration-based saga consists of a service updating a database\nand publishing a message. For example, Order Service persists an Order and a\nCreate Order Saga orchestrator and sends a message to the first saga participant. A\nsaga participant, such as Kitchen Service, handles a command message by updat-\ning its database and sending a reply message. Order Service processes the partici-\npant’s reply message by updating the state of the saga orchestrator and sending a\ncommand message to the next saga participant. As described in chapter 3, a service\nmust use transactional messaging in order to atomically update the database and\npublish messages. Later on in section 4.4, I’ll describe the implementation of the\nCreate Order Saga orchestrator in more detail, including how it uses transaction\nmessaging.\n Let’s take a look at the benefits and drawbacks of using saga orchestration. \nBENEFITS AND DRAWBACKS OF ORCHESTRATION-BASED SAGAS\nOrchestration-based sagas have several benefits:\nSimpler dependencies—One benefit of orchestration is that it doesn’t introduce\ncyclic dependencies. The saga orchestrator invokes the saga participants, but\nthe participants don’t invoke the orchestrator. As a result, the orchestrator\ndepends on the participants but not vice versa, and so there are no cyclic\ndependencies.\nLess coupling—Each service implements an API that is invoked by the orches-\ntrator, so it does not need to know about the events published by the saga\nparticipants.\nImproves separation of concerns and simplifies the business logic—The saga coordina-\ntion logic is localized in the saga orchestrator. The domain objects are simpler\nand have no knowledge of the sagas that they participate in. For example, when\nusing orchestration, the Order class has no knowledge of any of the sagas, so it\nhas a simpler state machine model. During the execution of the Create Order\nSaga, it transitions directly from the APPROVAL_PENDING state to the APPROVED\nstate. The Order class doesn’t have any intermediate states corresponding to the\nsteps of the saga. As a result, the business is much simpler.\nOrchestration also has a drawback: the risk of centralizing too much business logic in\nthe orchestrator. This results in a design where the smart orchestrator tells the dumb\nservices what operations to do. Fortunately, you can avoid this problem by designing\norchestrators that are solely responsible for sequencing and don’t contain any other\nbusiness logic.\n I recommend using orchestration for all but the simplest sagas. Implementing the\ncoordination logic for your sagas is just one of the design problems you need to solve.\nAnother, which is perhaps the biggest challenge that you’ll face when using sagas, is\nhandling the lack of isolation. Let’s take a look at that problem and how to solve it. \n \n",
      "content_length": 2897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "126\nCHAPTER 4\nManaging transactions with sagas\n4.3\nHandling the lack of isolation\nThe I in ACID stands for isolation. The isolation property of ACID transactions ensures\nthat the outcome of executing multiple transactions concurrently is the same as if they\nwere executed in some serial order. The database provides the illusion that each ACID\ntransaction has exclusive access to the data. Isolation makes it a lot easier to write busi-\nness logic that executes concurrently.\n The challenge with using sagas is that they lack the isolation property of ACID\ntransactions. That’s because the updates made by each of a saga’s local transactions\nare immediately visible to other sagas once that transaction commits. This behavior\ncan cause two problems. First, other sagas can change the data accessed by the saga\nwhile it’s executing. And other sagas can read its data before the saga has completed\nits updates, and consequently can be exposed to inconsistent data. You can, in fact,\nconsider a saga to be ACD:\nAtomicity—The saga implementation ensures that all transactions are executed\nor all changes are undone.\nConsistency—Referential integrity within a service is handled by local databases.\nReferential integrity across services is handled by the services.\nDurability—Handled by local databases.\nThis lack of isolation potentially causes what the database literature calls anomalies. An\nanomaly is when a transaction reads or writes data in a way that it wouldn’t if transac-\ntions were executed one at time. When an anomaly occurs, the outcome of executing\nsagas concurrently is different than if they were executed serially.\n On the surface, the lack of isolation sounds unworkable. But in practice, it’s com-\nmon for developers to accept reduced isolation in return for higher performance. An\nRDBMS lets you specify the isolation level for each transaction (https://dev.mysql\n.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html). The default iso-\nlation level is usually an isolation level that’s weaker than full isolation, also known as\nserializable transactions. Real-world database transactions are often different from\ntextbook definitions of ACID transactions.\n The next section discusses a set of saga design strategies that deal with the lack of\nisolation. These strategies are known as countermeasures. Some countermeasures imple-\nment isolation at the application level. Other countermeasures reduce the business\nrisk of the lack of isolation. By using countermeasures, you can write saga-based busi-\nness logic that works correctly.\n I’ll begin the section by describing the anomalies that are caused by the lack of iso-\nlation. After that, I’ll talk about countermeasures that either eliminate those anoma-\nlies or reduce their business risk.\n \n \n \n",
      "content_length": 2782,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "127\nHandling the lack of isolation\n4.3.1\nOverview of anomalies\nThe lack of isolation can cause the following three anomalies:\nLost updates—One saga overwrites without reading changes made by another saga.\nDirty reads—A transaction or a saga reads the updates made by a saga that has\nnot yet completed those updates.\nFuzzy/nonrepeatable reads—Two different steps of a saga read the same data and\nget different results because another saga has made updates.\nAll three anomalies can occur, but the first two are the most common and the most\nchallenging. Let’s take a look at those two types of anomaly, starting with lost updates.\nLOST UPDATES\nA lost update anomaly occurs when one saga overwrites an update made by another\nsaga. Consider, for example, the following scenario:\n1\nThe first step of the Create Order Saga creates an Order.\n2\nWhile that saga is executing, the Cancel Order Saga cancels the Order.\n3\nThe final step of the Create Order Saga approves the Order.\nIn this scenario, the Create Order Saga ignores the update made by the Cancel Order\nSaga and overwrites it. As a result, the FTGO application will ship an order that the\ncustomer had cancelled. Later in this section, I’ll show how to prevent lost updates. \nDIRTY READS\nA dirty read occurs when one saga reads data that’s in the middle of being updated by\nanother saga. Consider, for example, a version of the FTGO application store where\nconsumers have a credit limit. In this application, a saga that cancels an order consists\nof the following transactions:\n\nConsumer Service—Increase the available credit.\n\nOrder Service—Change the state of the Order to cancelled.\n\nDelivery Service—Cancel the delivery.\nLet’s imagine a scenario that interleaves the execution of the Cancel Order and Create\nOrder Sagas, and the Cancel Order Saga is rolled back because it’s too late to cancel\nthe delivery. It’s possible that the sequence of transactions that invoke the Consumer\nService is as follows:\n1\nCancel Order Saga—Increase the available credit.\n2\nCreate Order Saga—Reduce the available credit.\n3\nCancel Order Saga—A compensating transaction that reduces the available credit.\nIn this scenario, the Create Order Saga does a dirty read of the available credit that\nenables the consumer to place an order that exceeds their credit limit. It’s likely that\nthis is an unacceptable risk to the business.\n Let’s look at how to prevent this and other kinds of anomalies from impacting an\napplication. \n \n",
      "content_length": 2466,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "128\nCHAPTER 4\nManaging transactions with sagas\n4.3.2\nCountermeasures for handling the lack of isolation\nThe saga transaction model is ACD, and its lack of isolation can result in anomalies\nthat cause applications to misbehave. It’s the responsibility of the developer to write\nsagas in a way that either prevents the anomalies or minimizes their impact on the\nbusiness. This may sound like a daunting task, but you’ve already seen an example of a\nstrategy that prevents anomalies. An Order’s use of *_PENDING states, such as APPROVAL\n_PENDING, is an example of one such strategy. Sagas that update Orders, such as the\nCreate Order Saga, begin by setting the state of an Order to *_PENDING. The *_PENDING\nstate tells other transactions that the Order is being updated by a saga and to act\naccordingly.\n An Order’s use of *_PENDING states is an example of what the 1998 paper “Seman-\ntic ACID properties in multidatabases using remote procedure calls and update prop-\nagations” by Lars Frank and Torben U. Zahle calls a semantic lock countermeasure\n(https://dl.acm.org/citation.cfm?id=284472.284478). The paper describes how to deal\nwith the lack of transaction isolation in multi-database architectures that don’t use dis-\ntributed transactions. Many of its ideas are useful when designing sagas. It describes a\nset of countermeasures for handling anomalies caused by lack of isolation that either\nprevent one or more anomalies or minimize their impact on the business. The counter-\nmeasures described by this paper are as follows:\nSemantic lock—An application-level lock.\nCommutative updates—Design update operations to be executable in any order.\nPessimistic view—Reorder the steps of a saga to minimize business risk.\nReread value—Prevent dirty writes by rereading data to verify that it’s unchanged\nbefore overwriting it.\nVersion file—Record the updates to a record so that they can be reordered.\nBy value—Use each request’s business risk to dynamically select the concur-\nrency mechanism.\nLater in this section, I describe each of these countermeasures, but first I want to\nintroduce some terminology for describing the structure of a saga that’s useful when\ndiscussing countermeasures.\nTHE STRUCTURE OF A SAGA\nThe countermeasures paper mentioned in the last section defines a useful model for\nthe structure of a saga. In this model, shown in figure 4.8, a saga consists of three types\nof transactions:\nCompensatable transactions—Transactions that can potentially be rolled back using\na compensating transaction.\nPivot transaction—The go/no-go point in a saga. If the pivot transaction com-\nmits, the saga will run until completion. A pivot transaction can be a transaction\nthat’s neither compensatable nor retriable. Alternatively, it can be the last com-\npensatable transaction or the first retriable transaction.\n \n",
      "content_length": 2830,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "129\nHandling the lack of isolation\nRetriable transactions—Transactions that follow the pivot transaction and are guar-\nanteed to succeed.\nIn the Create Order Saga, the createOrder(), verifyConsumerDetails(), and create-\nTicket() steps are compensatable transactions. The createOrder() and create-\nTicket() transactions have compensating transactions that undo their updates. The\nverifyConsumerDetails() transaction is read-only, so doesn’t need a compensating\ntransaction. The authorizeCreditCard() transaction is this saga’s pivot transaction. If\nthe consumer’s credit card can be authorized, this saga is guaranteed to complete. The\napproveTicket() and approveOrder() steps are retriable transactions that follow the\npivot transaction.\n The distinction between compensatable transactions and retriable transactions is\nespecially important. As you’ll see, each type of transaction plays a different role in the\ncountermeasures. Chapter 13 states that when migrating to microservices, the mono-\nlith must sometimes participate in sagas and that it’s significantly simpler if the mono-\nlith only ever needs to execute retriable transactions.\n Let’s now look at each countermeasure, starting with the semantic lock counter-\nmeasure.\nCOUNTERMEASURE: SEMANTIC LOCK\nWhen using the semantic lock countermeasure, a saga’s compensatable transaction\nsets a flag in any record that it creates or updates. The flag indicates that the record\nStep\n1\n2\n3\n4\n5\n6\nService\nOrder Service\nConsumer Service\nKitchen Service\nAccounting Service\nRestaurant Order Service\nOrder Service\nTransaction\ncreateOrder()\nverifyConsumerDetails()\ncreateTicket()\nauthorizeCreditCard()\napproveRestaurantOrder()\napproveOrder()\nCompensation Transaction\nrejectOrder()\n-\nrejectTicket()\n-\n-\n-\nCompensatable transactions:\nMust support roll back\nPivot transactions:\nThe saga’s go/no-go transaction.\nIf it succeeds, then the saga runs\nto completion.\nRetriable transactions:\nGuaranteed to complete\nFigure 4.8\nA saga consists of three different types of transactions: compensatable transactions, \nwhich can be rolled back, so have a compensating transaction, a pivot transaction, which is the \nsaga’s go/no-go point, and retriable transactions, which are transactions that don’t need to be \nrolled back and are guaranteed to complete.\n \n",
      "content_length": 2289,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "130\nCHAPTER 4\nManaging transactions with sagas\nisn’t committed and could potentially change. The flag can either be a lock that prevents\nother transactions from accessing the record or a warning that indicates that other\ntransactions should treat that record with suspicion. It’s cleared by either a retriable\ntransaction—saga is completing successfully—or by a compensating transaction: the\nsaga is rolling back.\n The Order.state field is a great example of a semantic lock. The *_PENDING states,\nsuch as APPROVAL_PENDING and REVISION_PENDING, implement a semantic lock. They\ntell other sagas that access an Order that a saga is in the process of updating the Order.\nFor instance, the first step of the Create Order Saga, which is a compensatable trans-\naction, creates an Order in an APPROVAL_PENDING state. The final step of the Create\nOrder Saga, which is a retriable transaction, changes the field to APPROVED. A compen-\nsating transaction changes the field to REJECTED.\n Managing the lock is only half the problem. You also need to decide on a case-by-\ncase basis how a saga should deal with a record that has been locked. Consider, for\nexample, the cancelOrder() system command. A client might invoke this operation\nto cancel an Order that’s in the APPROVAL_PENDING state.\n There are a few different ways to handle this scenario. One option is for the cancel-\nOrder() system command to fail and tell the client to try again later. The main benefit\nof this approach is that it’s simple to implement. The drawback, however, is that it\nmakes the client more complex because it has to implement retry logic.\n Another option is for cancelOrder() to block until the lock is released. A benefit\nof using semantic locks is that they essentially recreate the isolation provided by ACID\ntransactions. Sagas that update the same record are serialized, which significantly\nreduces the programming effort. Another benefit is that they remove the burden of\nretries from the client. The drawback is that the application must manage locks. It\nmust also implement a deadlock detection algorithm that performs a rollback of a\nsaga to break a deadlock and re-execute it. \nCOUNTERMEASURE: COMMUTATIVE UPDATES\nOne straightforward countermeasure is to design the update operations to be com-\nmutative. Operations are commutative if they can be executed in any order. An\nAccount’s debit() and credit() operations are commutative (if you ignore overdraft\nchecks). This countermeasure is useful because it eliminates lost updates.\n Consider, for example, a scenario where a saga needs to be rolled back after a com-\npensatable transaction has debited (or credited) an account. The compensating trans-\naction can simply credit (or debit) the account to undo the update. There’s no\npossibility of overwriting updates made by other sagas. \nCOUNTERMEASURE: PESSIMISTIC VIEW\nAnother way to deal with the lack of isolation is the pessimistic view countermeasure. It\nreorders the steps of a saga to minimize business risk due to a dirty read. Consider, for\nexample, the scenario earlier used to describe the dirty read anomaly. In that scenario,\nthe Create Order Saga performed a dirty read of the available credit and created an\n \n",
      "content_length": 3207,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "131\nHandling the lack of isolation\norder that exceeded the consumer credit limit. To reduce the risk of that happening,\nthis countermeasure would reorder the Cancel Order Saga:\n1\nOrder Service—Change the state of the Order to cancelled.\n2\nDelivery Service—Cancel the delivery.\n3\nCustomer Service—Increase the available credit.\nIn this reordered version of the saga, the available credit is increased in a retriable\ntransaction, which eliminates the possibility of a dirty read. \nCOUNTERMEASURE: REREAD VALUE\nThe reread value countermeasure prevents lost updates. A saga that uses this counter-\nmeasure rereads a record before updating it, verifies that it’s unchanged, and then\nupdates the record. If the record has changed, the saga aborts and possibly restarts. This\ncountermeasure is a form of the Optimistic Offline Lock pattern (https://martinfowler\n.com/eaaCatalog/optimisticOfflineLock.html).\n The Create Order Saga could use this countermeasure to handle the scenario\nwhere the Order is cancelled while it’s in the process of being approved. The transac-\ntion that approves the Order verifies that the Order is unchanged since it was created\nearlier in the saga. If it’s unchanged, the transaction approves the Order. But if the\nOrder has been cancelled, the transaction aborts the saga, which causes its compensat-\ning transactions to be executed. \nCOUNTERMEASURE: VERSION FILE\nThe version file countermeasure is so named because it records the operations that are\nperformed on a record so that it can reorder them. It’s a way to turn noncommutative\noperations into commutative operations. To see how this countermeasure works, con-\nsider a scenario where the Create Order Saga executes concurrently with a Cancel\nOrder Saga. Unless the sagas use the semantic lock countermeasure, it’s possible that\nthe Cancel Order Saga cancels the authorization of the consumer’s credit card before\nthe Create Order Saga authorizes the card.\n One way for the Accounting Service to handle these out-of-order requests is for it\nto record the operations as they arrive and then execute them in the correct order. In\nthis scenario, it would first record the Cancel Authorization request. Then, when the\nAccounting Service receives the subsequent Authorize Card request, it would notice\nthat it had already received the Cancel Authorization request and skip authorizing\nthe credit card. \nCOUNTERMEASURE: BY VALUE\nThe final countermeasure is the by value countermeasure. It’s a strategy for selecting\nconcurrency mechanisms based on business risk. An application that uses this\ncountermeasure uses the properties of each request to decide between using sagas\nand distributed transactions. It executes low-risk requests using sagas, perhaps apply-\ning the countermeasures described in the preceding section. But it executes high-risk\nrequests involving, for example, large amounts of money, using distributed transactions.\n \n",
      "content_length": 2913,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "132\nCHAPTER 4\nManaging transactions with sagas\nThis strategy enables an application to dynamically make trade-offs about business\nrisk, availability, and scalability.\n It’s likely that you’ll need to use one or more of these countermeasures when\nimplementing sagas in your application. Let’s look at the detailed design and imple-\nmentation of the Create Order Saga, which uses the semantic lock countermeasure. \n4.4\nThe design of the Order Service and \nthe Create Order Saga\nNow that we’ve looked at various saga design and implementation issues, let’s see an\nexample. Figure 4.9 shows the design of Order Service. The service’s business logic\nconsists of traditional business logic classes, such as Order Service and the Order\nDeﬁnes the Restaurant Order\nService’s messaging API\nSends commands to\nsaga participants\nOrchestrator for the\nCreate Order Saga\nProcesses replies from\nsaga participants\nHandles commands sent by the the\nCreate Order Saga to the Order Service\nOrderServiceRequests\nAccountingServiceRequests\nConsumerServiceRequests\nKitchenServiceRequests\ncreateOrder()\ncancelOrder()\n...\nOrder Service\ncontroller\nOrder\ncommand\nhandlers\nCommand\nmessage\npublisher\nCreateOrderSagaReplies\nReply\nconsumer\nOrderService\nCreateOrder\nSaga\nOrderService\nProxy\nKitchenService\nProxy\nOrder\ncreateOrder()\ncancelOrder()\napproveOrder()\nrejectOrder()\n...\nOrderRepository\nsave()\nﬁndById()\n...\nFigure 4.9\nThe design of the Order Service and its sagas\n \n",
      "content_length": 1440,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "133\nThe design of the Order Service and the Create Order Saga\nentity. There are also saga orchestrator classes, including the CreateOrderSaga class,\nwhich orchestrates Create Order Saga. Also, because Order Service participates in its\nown sagas, it has an OrderCommandHandlers adapter class that handles command mes-\nsages by invoking OrderService.\n Some parts of Order Service should look familiar. As in a traditional application,\nthe core of the business logic is implemented by the OrderService, Order, and Order-\nRepository classes. In this chapter, I’ll briefly describe these classes. I describe them\nin more detail in chapter 5.\n What’s less familiar about Order Service are the saga-related classes. This service is\nboth a saga orchestrator and a saga participant. Order Service has several saga orches-\ntrators, such as CreateOrderSaga. The saga orchestrators send command messages to a\nsaga participant using a saga participant proxy class, such as KitchenServiceProxy and\nOrderServiceProxy. A saga participant proxy defines a saga participant’s messaging\nAPI. Order Service also has an OrderCommandHandlers class, which handles the com-\nmand messages sent by sagas to Order Service.\n Let’s look in more detail at the design, starting with the OrderService class.\n4.4.1\nThe OrderService class\nThe OrderService class is a domain service called by the service’s API layer. It’s\nresponsible for creating and managing orders. Figure 4.10 shows OrderService and\nsome of its collaborators. OrderService creates and updates Orders, invokes the\nOrderRepository to persist Orders, and creates sagas, such as the CreateOrderSaga,\nusing the SagaManager. The SagaManager class is one of the classes provided by the\nEventuate Tram Saga framework, which is a framework for writing saga orchestrators\nand participants, and is discussed a little later in this section.\ncreateOrder()\n...\nOrderService\nsave()\nﬁndOne()\n...\nOrderRepository\ncreate(..)\nSagaManager\nOrder\nCreateOrder\nSaga\nFigure 4.10\nOrderService creates and updates Orders, invokes the \nOrderRepository to persist Orders, and creates sagas, including the \nCreateOrderSaga.\n \n",
      "content_length": 2131,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "134\nCHAPTER 4\nManaging transactions with sagas\nI’ll discuss this class in more detail in chapter 5. For now, let’s focus on the create-\nOrder() method. The following listing shows OrderService’s createOrder() method.\nThis method first creates an Order and then creates an CreateOrderSaga to validate\nthe order.\n@Transactional\npublic class OrderService {\n@Autowired\nprivate SagaManager<CreateOrderSagaState> createOrderSagaManager;\n@Autowired\nprivate OrderRepository orderRepository;\n@Autowired\nprivate DomainEventPublisher eventPublisher;\npublic Order createOrder(OrderDetails orderDetails) {\n...\nResultWithEvents<Order> orderAndEvents = Order.createOrder(...);  \nOrder order = orderAndEvents.result;\norderRepository.save(order);\neventPublisher.publish(Order.class,\nLong.toString(order.getId()),\norderAndEvents.events);\nCreateOrderSagaState data =\nnew CreateOrderSagaState(order.getId(), orderDetails);\n  \ncreateOrderSagaManager.create(data, Order.class, order.getId());\nreturn order;\n}\n...\n}\nThe createOrder() method creates an Order by calling the factory method Order\n.createOrder(). It then persists the Order using the OrderRepository, which is a JPA-\nbased repository. It creates the CreateOrderSaga by calling SagaManager.create(),\npassing a CreateOrderSagaState containing the ID of the newly saved Order and the\nOrderDetails. The SagaManager instantiates the saga orchestrator, which causes it to\nsend a command message to the first saga participant, and persists the saga orchestra-\ntor in the database.\n Let’s look at the CreateOrderSaga and its associated classes. \nListing 4.1\nThe OrderService class and its createOrder() method\nEnsure that service \nmethods are transactional.\nCreate the\nOrder.\nPersist the Order \nin the database.\nPublish domain \nevents.\nCreate a\nCreateOrderSaga.\n \n",
      "content_length": 1796,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "135\nThe design of the Order Service and the Create Order Saga\n4.4.2\nThe implementation of the Create Order Saga\nFigure 4.11 shows the classes that implement the Create Order Saga. The responsibil-\nities of each class are as follows:\n\nCreateOrderSaga—A singleton class that defines the saga’s state machine. It\ninvokes the CreateOrderSagaState to create command messages and sends\nthem to participants using message channels specified by the saga participant\nproxy classes, such as KitchenServiceProxy.\nEventuate tram sagas\ncreate()\n...\nSagaManager\n«interface»\nSimpleSaga\nSagaDeﬁnition\nCommandEndpoint\nSagaDeﬁnition\ngetSagaDeﬁnition()\n«table»\nSAGA_INSTANCE\nEventuate tram\nUses\nImplements\nCreates\nManages\nInvokes\nOrder database\nCreateOrderSaga\nreplies\nOrderService\nrequests\nStores the state\nof saga instances\nThe state of a saga\nDescribes a\nmessage channel\nDescribes the\nsteps of a saga\nAbstract superclass\nfor saga\norchestrators\norderId\norderDetails\n...\nCreateOrder\nSagaState\nCreateOrder\nSaga\nKitchen\nServiceProxy\nthis.sagaDeﬁnition=\nstep()\n.withCompensation(...)\n.step()\n.invokeParticipant(...)\n.step()\n.invokeParticipant(...)\n.onReply(...)\n.withCompensation(...)\nOrderService\nProxy\nOrderService\nThe SagaManager handles persisting a saga,\nsending the command messages that it\ngenerates, subscribing to reply messages,\nand invoking the saga to handle replies.\nFigure 4.11\nThe OrderService's sagas, such as Create Order Saga, are implemented using \nthe Eventuate Tram Saga framework.\n \n",
      "content_length": 1485,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "136\nCHAPTER 4\nManaging transactions with sagas\n\nCreateOrderSagaState—A saga’s persistent state, which creates command\nmessages.\nSaga participant proxy classes, such as KitchenServiceProxy—Each proxy class\ndefines a saga participant’s messaging API, which consists of the command\nchannel, the command message types, and the reply types.\nThese classes are written using the Eventuate Tram Saga framework.\n The Eventuate Tram Saga framework provides a domain-specific language (DSL) for\ndefining a saga’s state machine. It executes the saga’s state machine and exchanges mes-\nsages with saga participants using the Eventuate Tram framework. The framework also\npersists the saga’s state in the database.\n Let’s take a closer look at the implementation of Create Order Saga, starting with\nthe CreateOrderSaga class.\nTHE CREATEORDERSAGA ORCHESTRATOR\nThe CreateOrderSaga class implements the state machine shown earlier in figure 4.7.\nThis class implements SimpleSaga, a base interface for sagas. The heart of the Create-\nOrderSaga class is the saga definition shown in the following listing. It uses the DSL\nprovided by the Eventuate Tram Saga framework to define the steps of the Create\nOrder Saga.\npublic class CreateOrderSaga implements SimpleSaga<CreateOrderSagaState> {\nprivate SagaDefinition<CreateOrderSagaState> sagaDefinition;\npublic CreateOrderSaga(OrderServiceProxy orderService,\nConsumerServiceProxy consumerService,\nKitchenServiceProxy kitchenService,\nAccountingServiceProxy accountingService) {\nthis.sagaDefinition =\nstep()\n.withCompensation(orderService.reject,\nCreateOrderSagaState::makeRejectOrderCommand)\n.step()\n.invokeParticipant(consumerService.validateOrder,\nCreateOrderSagaState::makeValidateOrderByConsumerCommand)\n.step()\n.invokeParticipant(kitchenService.create,\nCreateOrderSagaState::makeCreateTicketCommand)\n.onReply(CreateTicketReply.class,\nCreateOrderSagaState::handleCreateTicketReply)\n.withCompensation(kitchenService.cancel,\nCreateOrderSagaState::makeCancelCreateTicketCommand)\n.step()\n.invokeParticipant(accountingService.authorize,\nCreateOrderSagaState::makeAuthorizeCommand)\n.step()\n.invokeParticipant(kitchenService.confirmCreate,\nCreateOrderSagaState::makeConfirmCreateTicketCommand)\nListing 4.2\nThe definition of the CreateOrderSaga\n \n",
      "content_length": 2270,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "137\nThe design of the Order Service and the Create Order Saga\n.step()\n.invokeParticipant(orderService.approve,\nCreateOrderSagaState::makeApproveOrderCommand)\n.build();\n}\n@Override\npublic SagaDefinition<CreateOrderSagaState> getSagaDefinition() {\nreturn sagaDefinition;\n}\nThe CreateOrderSaga’s constructor creates the saga definition and stores it in the\nsagaDefinition field. The getSagaDefinition() method returns the saga definition.\n To see how CreateOrderSaga works, let’s look at the definition of the third step of\nthe saga, shown in the following listing. This step of the saga invokes the Kitchen Ser-\nvice to create a Ticket. Its compensating transaction cancels that Ticket. The\nstep(), invokeParticipant(), onReply(), and withCompensation() methods are\npart of the DSL provided by Eventuate Tram Saga.\npublic class CreateOrderSaga ...\npublic CreateOrderSaga(..., KitchenServiceProxy kitchenService,\n...) {\n...\n.step()\n.invokeParticipant(kitchenService.create,\nCreateOrderSagaState::makeCreateTicketCommand)\n.onReply(CreateTicketReply.class,\nCreateOrderSagaState::handleCreateTicketReply)\n.withCompensation(kitchenService.cancel,\n  \nCreateOrderSagaState::makeCancelCreateTicketCommand)\n...\n;\nThe call to invokeParticipant() defines the forward transaction. It creates the Create-\nTicket command message by calling CreateOrderSagaState.makeCreateTicket-\nCommand() and sends it to the channel specified by kitchenService.create. The call\nto onReply() specifies that CreateOrderSagaState.handleCreateTicketReply()\nshould be called when a successful reply is received from Kitchen Service. This\nmethod stores the returned ticketId in the CreateOrderSagaState. The call to\nwithCompensation() defines the compensating transaction. It creates a RejectTicket-\nCommand command message by calling CreateOrderSagaState.makeCancelCreate-\nTicket() and sends it to the channel specified by kitchenService.create.\n The other steps of the saga are defined in a similar fashion. The CreateOrder-\nSagaState creates each message, which is sent by the saga to the messaging endpoint\nListing 4.3\nThe definition of the third step of the saga\nDefine the forward \ntransaction.\nCall handleCreateTicketReply() when\na successful reply is received.\nDefine the compensating\ntransaction.\n \n",
      "content_length": 2270,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "138\nCHAPTER 4\nManaging transactions with sagas\ndefined by a KitchenServiceProxy. Let’s take a look at each of those classes, starting\nwith CreateOrderSagaState. \nTHE CREATEORDERSAGASTATE CLASS\nThe CreateOrderSagaState class, shown in the following listing, represents the state\nof a saga instance. An instance of this class is created by OrderService and is persisted\nin the database by the Eventuate Tram Saga framework. Its primary responsibility is to\ncreate the messages that are sent to saga participants.\npublic class CreateOrderSagaState {\nprivate Long orderId;\nprivate OrderDetails orderDetails;\nprivate long ticketId;\npublic Long getOrderId() {\nreturn orderId;\n}\nprivate CreateOrderSagaState() {\n}\npublic CreateOrderSagaState(Long orderId, OrderDetails orderDetails) {  \nthis.orderId = orderId;\nthis.orderDetails = orderDetails;\n}\nCreateTicket makeCreateTicketCommand() {\nreturn new CreateTicket(getOrderDetails().getRestaurantId(),\ngetOrderId(), makeTicketDetails(getOrderDetails()));\n}\nvoid handleCreateTicketReply(CreateTicketReply reply) {   \nlogger.debug(\"getTicketId {}\", reply.getTicketId());\nsetTicketId(reply.getTicketId());\n}\nCancelCreateTicket makeCancelCreateTicketCommand() {\n  \nreturn new CancelCreateTicket(getOrderId());\n}\n...\nThe CreateOrderSaga invokes the CreateOrderSagaState to create the command\nmessages. It sends those command messages to the endpoints defined by the Saga-\nParticipantProxy classes. Let’s take a look at one of those classes: KitchenService-\nProxy. \nListing 4.4\nCreateOrderSagaState stores the state of a saga instance\nInvoked by the\nOrderService to\ninstantiate a\nCreateOrderSagaState\nCreates a CreateTicket \ncommand message\nSaves the ID \nof the newly \ncreated Ticket\nCreates \nCancelCreateTicket \ncommand message\n \n",
      "content_length": 1765,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "139\nThe design of the Order Service and the Create Order Saga\nTHE KITCHENSERVICEPROXY CLASS\nThe KitchenServiceProxy class, shown in listing 4.5, defines the command message\nendpoints for Kitchen Service. There are three endpoints:\n\ncreate—Creates a Ticket\n\nconfirmCreate—Confirms the creation\n\ncancel—Cancels a Ticket\nEach CommandEndpoint specifies the command type, the command message’s destina-\ntion channel, and the expected reply types.\npublic class KitchenServiceProxy {\npublic final CommandEndpoint<CreateTicket> create =\nCommandEndpointBuilder\n.forCommand(CreateTicket.class)\n.withChannel(\nKitchenServiceChannels.kitchenServiceChannel)\n.withReply(CreateTicketReply.class)\n.build();\npublic final CommandEndpoint<ConfirmCreateTicket> confirmCreate =\nCommandEndpointBuilder\n.forCommand(ConfirmCreateTicket.class)\n.withChannel(\nKitchenServiceChannels.kitchenServiceChannel)\n.withReply(Success.class)\n.build();\npublic final CommandEndpoint<CancelCreateTicket> cancel =\nCommandEndpointBuilder\n.forCommand(CancelCreateTicket.class)\n.withChannel(\nKitchenServiceChannels.kitchenServiceChannel)\n.withReply(Success.class)\n.build();\n}\nProxy classes, such as KitchenServiceProxy, aren’t strictly necessary. A saga could sim-\nply send command messages directly to participants. But proxy classes have two import-\nant benefits. First, a proxy class defines static typed endpoints, which reduces the chance\nof a saga sending the wrong message to a service. Second, a proxy class is a well-defined\nAPI for invoking a service that makes the code easier to understand and test. For exam-\nple, chapter 10 describes how to write tests for KitchenServiceProxy that verify that\nOrder Service correctly invokes Kitchen Service. Without KitchenServiceProxy, it\nwould be impossible to write such a narrowly scoped test. \nListing 4.5\nKitchenServiceProxy defines the command message endpoints for \n Kitchen Service\n \n",
      "content_length": 1900,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "140\nCHAPTER 4\nManaging transactions with sagas\nTHE EVENTUATE TRAM SAGA FRAMEWORK\nThe Eventuate Tram Saga, shown in figure 4.12, is a framework for writing both saga\norchestrators and saga participants. It uses transactional messaging capabilities of Even-\ntuate Tram, discussed in chapter 3.\nThe saga orchestration package is the most complex part of the framework. It pro-\nvides SimpleSaga, a base interface for sagas, and a SagaManager class, which creates\nand manages saga instances. The SagaManager handles persisting a saga, sending the\ncommand messages that it generates, subscribing to reply messages, and invoking\nthe saga to handle replies. Figure 4.13 shows the sequence of events when OrderService\ncreates a saga. The sequence of events is as follows:\n1\nOrderService creates the CreateOrderSagaState.\n2\nIt creates an instance of a saga by invoking the SagaManager.\n3\nThe SagaManager executes the first step of the saga definition.\n4\nThe CreateOrderSagaState is invoked to generate a command message.\nParticipant\nOrchestration\ncreate(sagaState)\n...\nSagaManager\nSimpleSaga\nSagaDeﬁnition\nCommandEndpoint\nSagaCommand\nDispatcher\nSagaCommand\nHandlersBuilder\nSagaDeﬁnition\ngetSagaDeﬁnition()\n«table»\nSAGA_INSTANCE\nEventuate tram\nEventuate tram saga framework\nUses\nSends\nand receives\nOrder database\nChannels\nThe SagaManager handles persisting a\nsaga, sending the command messages\nthat it generates, subscribing to reply\nmessages, and invoking the saga to\nhandle replies.\nAbstract superclass\nfor saga orchestrators\nDescribes a\nmessage channel\nRoutes command\nmessages to\nmessage handlers\nDescribes the\nsteps of a saga\nStores the state of\nsaga instances\nFigure 4.12\nEventuate Tram Saga is a framework for writing both saga orchestrators and saga \nparticipants.\n \n",
      "content_length": 1763,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "141\nThe design of the Order Service and the Create Order Saga\n5\nThe SagaManager sends the command message to the saga participant (the\nConsumer Service).\n6\nThe SagaManager saves the saga instance in the database.\nFigure 4.14 shows the sequence of events when SagaManager receives a reply from\nConsumer Service.\nThe sequence of events is as follows:\n1\nEventuate Tram invokes SagaManager with the reply from Consumer Service.\n2\nSagaManager retrieves the saga instance from the database.\n3\nSagaManager executes the next step of the saga definition.\nOrderService\nCreateOrderSagaState\nSagaManager\nCreateOrderSaga\nSagaDeﬁnition\nEventuateTram\nDatabase\nnew()\ncreate(sagaState)\ngetSagaDeﬁnition()\nexecuteFirstStep(sagaState)\nmakeValidateOrderByConsumerCommand()\nsendMessage(command)\nsaveSagaInstance(sagaState)\nFigure 4.13\nThe sequence of events when OrderService creates an instance of Create Order Saga\nCreateOrderSagaState\nSagaManager\nCreateOrderSaga\nSagaDeﬁnition\nEventuateTram\nDatabase\nhandleMessage()\nloadSagaInstance()\ngetSagaDeﬁnition()\nexecuteFirstStep(sagaState)\nmakeValidateOrderByConsumerCommand()\nsendMessage\n(command)\nsaveSagaInstance\n(sageState)\nFigure 4.14\nThe sequence of events when the SagaManager receives a reply message from a saga participant\n \n",
      "content_length": 1259,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "142\nCHAPTER 4\nManaging transactions with sagas\n4\nCreateOrderSagaState is invoked to generate a command message.\n5\nSagaManager sends the command message to the specified saga participant\n(Kitchen Service).\n6\nSagaManager saves the update saga instance in the database.\nIf a saga participant fails, SagaManager executes the compensating transactions in\nreverse order.\n The other part of the Eventuate Tram Saga framework is the saga participant\npackage. It provides the SagaCommandHandlersBuilder and SagaCommandDispatcher\nclasses for writing saga participants. These classes route command messages to han-\ndler methods, which invoke the saga participants’ business logic and generate reply\nmessages. Let’s take a look at how these classes are used by Order Service. \n4.4.3\nThe OrderCommandHandlers class\nOrder Service participates in its own sagas. For example, CreateOrderSaga invokes\nOrder Service to either approve or reject an Order. The OrderCommandHandlers class,\nshown in figure 4.15, defines the handler methods for the command messages sent by\nthese sagas.\n Each handler method invokes OrderService to update an Order and makes a\nreply message. The SagaCommandDispatcher class routes the command messages to\nthe appropriate handler method and sends the reply.\napproveOrder()\nrejectOrder()\n...\nOrderCommandHandlers\nEventuate\nTram Sagas\napproveOrder()\nrejectOrder()\n...\nOrderService\nInvokes\nInvokes\nUses\nReads\nSends\nSagaCommand\nDispatcher\nEventuate tram\nOrderService\nrequests\nCreateOrderSaga\nreplies\nRoutes command messages to\nhandlers and sends replies\nFigure 4.15\nOrderCommandHandlers implements command handlers for the commands that are \nsent by the various Order Service sagas.\n \n",
      "content_length": 1690,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "143\nThe design of the Order Service and the Create Order Saga\nThe following listing shows the OrderCommandHandlers class. Its commandHandlers()\nmethod maps command message types to handler methods. Each handler method\ntakes a command message as a parameter, invokes OrderService, and returns a reply\nmessage.\npublic class OrderCommandHandlers {\n@Autowired\nprivate OrderService orderService;\npublic CommandHandlers commandHandlers() {\n  \nreturn SagaCommandHandlersBuilder\n.fromChannel(\"orderService\")\n.onMessage(ApproveOrderCommand.class, this::approveOrder)\n.onMessage(RejectOrderCommand.class, this::rejectOrder)\n...\n.build();\n}\npublic Message approveOrder(CommandMessage<ApproveOrderCommand> cm) {\nlong orderId = cm.getCommand().getOrderId();\norderService.approveOrder(orderId);\nreturn withSuccess();\n}\npublic Message rejectOrder(CommandMessage<RejectOrderCommand> cm) {\nlong orderId = cm.getCommand().getOrderId();\norderService.rejectOrder(orderId);\nreturn withSuccess();\n}\nThe approveOrder() and rejectOrder() methods update the specified Order by\ninvoking OrderService. The other services that participate in sagas have similar com-\nmand handler classes that update their domain objects. \n4.4.4\nThe OrderServiceConfiguration class\nThe Order Service uses the Spring framework. The following listing is an excerpt of\nthe OrderServiceConfiguration class, which is an @Configuration class that instanti-\nates and wires together the Spring @Beans.\n@Configuration\npublic class OrderServiceConfiguration {\n@Bean\npublic OrderService orderService(RestaurantRepository restaurantRepository,\nListing 4.6\nThe command handlers for Order Service\nListing 4.7\nThe OrderServiceConfiguration is a Spring @Configuration\n class that defines the Spring @Beans for the Order Service.\nRoute each command \nmessage to the appropriate \nhandler method.\nChange the state \nof the Order to \nauthorized.\nReturn a generic \nsuccess message.\nChange the state of \nthe Order to rejected.\n \n",
      "content_length": 1959,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "144\nCHAPTER 4\nManaging transactions with sagas\n...\nSagaManager<CreateOrderSagaState>\ncreateOrderSagaManager,\n...) {\nreturn new OrderService(restaurantRepository,\n...\ncreateOrderSagaManager\n...);\n}\n@Bean\npublic SagaManager<CreateOrderSagaState> createOrderSagaManager(CreateOrderS\naga saga) {\nreturn new SagaManagerImpl<>(saga);\n}\n@Bean\npublic CreateOrderSaga createOrderSaga(OrderServiceProxy orderService,\nConsumerServiceProxy consumerService,\n...) {\nreturn new CreateOrderSaga(orderService, consumerService, ...);\n}\n@Bean\npublic OrderCommandHandlers orderCommandHandlers() {\nreturn new OrderCommandHandlers();\n}\n@Bean\npublic SagaCommandDispatcher\norderCommandHandlersDispatcher(OrderCommandHan\ndlers orderCommandHandlers) {\nreturn new SagaCommandDispatcher(\"orderService\", orderCommandHandlers.comma\nndHandlers());\n}\n@Bean\npublic KitchenServiceProxy kitchenServiceProxy() {\nreturn new KitchenServiceProxy();\n}\n@Bean\npublic OrderServiceProxy orderServiceProxy() {\nreturn new OrderServiceProxy();\n}\n...\n}\nThis class defines several Spring @Beans including orderService, createOrder-\nSagaManager, createOrderSaga, orderCommandHandlers, and orderCommandHandlers-\nDispatcher. It also defines Spring @Beans for the various proxy classes, including\nkitchenServiceProxy and orderServiceProxy.\n \n",
      "content_length": 1289,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "145\nSummary\n CreateOrderSaga is only one of Order Service’s many sagas. Many of its other sys-\ntem operations also use sagas. For example, the cancelOrder() operation uses a Can-\ncel Order Saga, and the reviseOrder() operation uses a Revise Order Saga. As a\nresult, even though many services have an external API that uses a synchronous proto-\ncol, such as REST or gRPC, a large amount of interservice communication will use\nasynchronous messaging.\n As you can see, transaction management and some aspects of business logic design\nare quite different in a microservice architecture. Fortunately, saga orchestrators are\nusually quite simple state machines, and you can use a saga framework to simplify your\ncode. Nevertheless, transaction management is certainly more complicated than in a\nmonolithic architecture. But that’s usually a small price to pay for the tremendous\nbenefits of microservices. \nSummary\nSome system operations need to update data scattered across multiple services.\nTraditional, XA/2PC-based distributed transactions aren’t a good fit for mod-\nern applications. A better approach is to use the Saga pattern. A saga is sequence\nof local transactions that are coordinated using messaging. Each local transac-\ntion updates data in a single service. Because each local transaction commits its\nchanges, if a saga must roll back due to the violation of a business rule, it must\nexecute compensating transactions to explicitly undo changes.\nYou can use either choreography or orchestration to coordinate the steps of a\nsaga. In a choreography-based saga, a local transaction publishes events that trig-\nger other participants to execute local transactions. In an orchestration-based\nsaga, a centralized saga orchestrator sends command messages to participants\ntelling them to execute local transactions. You can simplify development and test-\ning by modeling saga orchestrators as state machines. Simple sagas can use chore-\nography, but orchestration is usually a better approach for complex sagas.\nDesigning saga-based business logic can be challenging because, unlike ACID\ntransactions, sagas aren’t isolated from one another. You must often use counter-\nmeasures, which are design strategies that prevent concurrency anomalies\ncaused by the ACD transaction model. An application may even need to use\nlocking in order to simplify the business logic, even though that risks deadlocks. \n \n",
      "content_length": 2408,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "146\nDesigning\nbusiness logic in\na microservice architecture\nThe heart of an enterprise application is the business logic, which implements the\nbusiness rules. Developing complex business logic is always challenging. The FTGO\napplication’s business logic implements some quite complex business logic, espe-\ncially for order management and delivery management. Mary had encouraged her\nteam to apply object-oriented design principles, because in her experience this was\nthe best way to implement complex business logic. Some of the business logic used\nthe procedural Transcription script pattern. But the majority of the FTGO applica-\ntion’s business logic is implemented in an object-oriented domain model that’s\nmapped to the database using JPA.\n Developing complex business logic is even more challenging in a microservice\narchitecture where the business logic is spread over multiple services. You need to\nThis chapter covers\nApplying the business logic organization patterns: \nTransaction script pattern and Domain model \npattern\nDesigning business logic with the Domain-driven \ndesign (DDD) aggregate pattern\nApplying the Domain event pattern in a \nmicroservice architecture\n \n",
      "content_length": 1183,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "147\nBusiness logic organization patterns\naddress two key challenges. First, a typical domain model is a tangled web of intercon-\nnected classes. Although this isn’t a problem in a monolithic application, in a micro-\nservice architecture, where classes are scattered around different services, you need to\neliminate object references that would otherwise span service boundaries. The second\nchallenge is designing business logic that works within the transaction management\nconstraints of a microservice architecture. Your business logic can use ACID transac-\ntions within services, but as described in chapter 4, it must use the Saga pattern to\nmaintain data consistency across services.\n Fortunately, we can address these issues by using the Aggregate pattern from\nDDD. The Aggregate pattern structures a service’s business logic as a collection of\naggregates. An aggregate is a cluster of objects that can be treated as a unit. There are\ntwo reasons why aggregates are useful when developing business logic in a micro-\nservice architecture:\nAggregates avoid any possibility of object references spanning service boundar-\nies, because an inter-aggregate reference is a primary key value rather than an\nobject reference.\nBecause a transaction can only create or update a single aggregate, aggregates\nfit the constraints of the microservices transaction model.\nAs a result, an ACID transaction is guaranteed to be within a single service.\n I begin this chapter by describing the different ways of organizing business logic:\nthe Transcription script pattern and the Domain model pattern. Next I introduce the\nconcept of a DDD aggregate and explain why it’s a good building block for a service’s\nbusiness logic. After that, I describe the Domain event pattern events and explain why\nit’s useful for a service to publish events. I end this chapter with a couple of examples\nof business logic from Kitchen Service and Order Service.\n Let’s now look at business logic organization patterns.\n5.1\nBusiness logic organization patterns\nFigure 5.1 shows the architecture of a typical service. As described in chapter 2, the\nbusiness logic is the core of a hexagonal architecture. Surrounding the business logic\nare the inbound and outbound adapters. An inbound adapter handles requests from cli-\nents and invokes the business logic. An outbound adapter, which is invoked by the busi-\nness logic, invokes other services and applications.\n This service consists of the business logic and the following adapters:\n\nREST API adapter—An inbound adapter that implements a REST API which\ninvokes the business logic\n\nOrderCommandHandlers—An inbound adapter that consumes command mes-\nsages from a message channel and invokes the business logic\n\nDatabase Adapter—An outbound adapter that’s invoked by the business logic\nto access the database\n\nDomain Event Publishing Adapter—An outbound adapter that publishes events\nto a message broker\n \n",
      "content_length": 2925,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "148\nCHAPTER 5\nDesigning business logic in a microservice architecture\nThe business logic is typically the most complex part of the service. When develop-\ning business logic, you should consciously organize your business logic in the way\nthat’s most appropriate for your application. After all, I’m sure you’ve experienced\nthe frustration of having to maintain someone else’s badly structured code. Most\nenterprise applications are written in an object-oriented language such as Java, so\nthey consist of classes and methods. But using an object-oriented language doesn’t\nguarantee that the business logic has an object-oriented design. The key decision you\nmust make when developing business logic is whether to use an object-oriented\napproach or a procedural approach. There are two main patterns for organizing\nOutbound adapters\nInbound adapters\nOrder\nService requests\nPOST/orders\nGET/order/Id\nREST API\nOrder database\nOrder Service\nbusiness logic\nOrder\ncommand\nhandlers\nOrder events\nDomain event\npublisher adapter\nDatabase\nadapter\nFigure 5.1\nThe Order Service has a hexagonal architecture. It consists of the business logic \nand one or more adapters that interface with external applications and other services.\n \n",
      "content_length": 1215,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "149\nBusiness logic organization patterns\nbusiness logic: the procedural Transaction script pattern, and the object-oriented\nDomain model pattern.\n5.1.1\nDesigning business logic using the Transaction script pattern\nAlthough I’m a strong advocate of the object-oriented approach, there are some situa-\ntions where it is overkill, such as when you are developing simple business logic. In such\na situation, a better approach is to write procedural code and use what the book Patterns\nof Enterprise Application Architecture by Martin Fowler (Addison-Wesley Professional, 2002)\ncalls the Transaction script pattern. Rather than doing any object-oriented design, you\nwrite a method called a transaction script to handle each request from the presentation\ntier. As figure 5.2 shows, an important characteristic of this approach is that the classes\nthat implement behavior are separate from those that store state.\nWhen using the Transaction script pattern, the scripts are usually located in service\nclasses, which in this example is the OrderService class. A service class has one\nmethod for each request/system operation. The method implements the business\nlogic for that request. It accesses the database using data access objects (DAOs), such\nas the OrderDao. The data objects, which in this example is the Order class, are pure\ndata with little or no behavior.\nThis style of design is highly procedural and relies on few of the capabilities of object-\noriented programming (OOP) languages. This what you would create if you were writ-\ning the application in C or another non-OOP language. Nevertheless, you shouldn’t be\nPattern: Transaction script\nOrganize the business logic as a collection of procedural transaction scripts, one for\neach type of request.\ncreateOrder()\nreviseOrder()\ncancelOrder()\n...\nOrderService\nClasses with\nbehavior\nClasses\nwith state\nsave(Order)\nﬁndOrderById()\n...\nOrderDao\norderId\norderLineItems\n...\nOrder\nFigure 5.2\nOrganizing business logic \nas transaction scripts. In a typical \ntransaction script–based design, one \nset of classes implements behavior \nand another set stores state. The \ntransaction scripts are organized into \nclasses that typically have no state. \nThe scripts use data classes, which \ntypically have no behavior.\n \n",
      "content_length": 2259,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "150\nCHAPTER 5\nDesigning business logic in a microservice architecture\nashamed to use a procedural design when it’s appropriate. This approach works well\nfor simple business logic. The drawback is that this tends not to be a good way to\nimplement complex business logic. \n5.1.2\nDesigning business logic using the Domain model pattern\nThe simplicity of the procedural approach can be quite seductive. You can write code with-\nout having to carefully consider how to organize the classes. The problem is that if your\nbusiness logic becomes complex, you can end up with code that’s a nightmare to main-\ntain. In fact, in the same way that a monolithic application has a habit of continually grow-\ning, transaction scripts have the same problem. Consequently, unless you’re writing an\nextremely simple application, you should resist the temptation to write procedural code\nand instead apply the Domain model pattern and develop an object-oriented design.\nIn an object-oriented design, the business logic consists of an object model, a network\nof relatively small classes. These classes typically correspond directly to concepts from\nthe problem domain. In such a design some classes have only either state or behavior,\nbut many contain both, which is the hallmark of a well-designed class. Figure 5.3\nshows an example of the Domain model pattern.\nPattern: Domain model\nOrganize the business logic as an object model consisting of classes that have state\nand behavior.\ncreateOrder()\nreviseOrder()\ncancelOrder()\n...\nOrderService\ndeliveryTime\ndeliveryAddress\nDeliveryInformation\nﬁndOrderById()\n...\nOrderRepository\nSome classes have only state.\nMany classes have\nstate and behavior.\nUses\nSome classes have only behavior.\n«private»\norderId\norderLineItems\n...\nrevise()\ncancel()\n«static»\ncreate()\nOrder\nFigure 5.3\nOrganizing business logic as a domain model. The majority of \nthe business logic consists of classes that have state and behavior.\n \n",
      "content_length": 1935,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "151\nBusiness logic organization patterns\nAs with the Transaction script pattern, an OrderService class has a method for each\nrequest/system operation. But when using the Domain model pattern, the service\nmethods are usually simple. That’s because a service method almost always delegates\nto persistent domain objects, which contain the bulk of the business logic. A service\nmethod might, for example, load a domain object from the database and invoke one\nof its methods. In this example, the Order class has both state and behavior. Moreover,\nits state is private and can only be accessed indirectly via its methods.\n Using an object-oriented design has a number of benefits. First, the design is\neasy to understand and maintain. Instead of consisting of one big class that does\neverything, it consists of a number of small classes that each have a small number of\nresponsibilities. In addition, classes such as Account, BankingTransaction, and\nOverdraftPolicy closely mirror the real world, which makes their role in the design\neasier to understand. Second, our object-oriented design is easier to test: each class\ncan and should be tested independently. Finally, an object-oriented design is easier to\nextend because it can use well-known design patterns, such as the Strategy pattern and\nthe Template method pattern, that define ways of extending a component without\nmodifying the code.\n The Domain model pattern works well, but there are a number of problems with\nthis approach, especially in a microservice architecture. To address those problems,\nyou need to use a refinement of OOD known as DDD. \n5.1.3\nAbout Domain-driven design\nDDD, which is described in the book Domain-Driven Design by Eric Evans (Addison-\nWesley Professional, 2003), is a refinement of OOD and is an approach for developing\ncomplex business logic. I introduced DDD in chapter 2 when discussing the useful-\nness of DDD subdomains when decomposing an application into services. When using\nDDD, each service has its own domain model, which avoids the problems of a single,\napplication-wide domain model. Subdomains and the associated concept of Bounded\nContext are two of the strategic DDD patterns.\n DDD also has some tactical patterns that are building blocks for domain models.\nEach pattern is a role that a class plays in a domain model and defines the characteris-\ntics of the class. The building blocks that have been widely adopted by developers\ninclude the following:\nEntity—An object that has a persistent identity. Two entities whose attributes\nhave the same values are still different objects. In a Java EE application, classes\nthat are persisted using JPA @Entity are usually DDD entities.\nValue object—An object that is a collection of values. Two value objects whose\nattributes have the same values can be used interchangeably. An example of a\nvalue object is a Money class, which consists of a currency and an amount.\nFactory—An object or method that implements object creation logic that’s too\ncomplex to be done directly by a constructor. It can also hide the concrete\n \n",
      "content_length": 3067,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "152\nCHAPTER 5\nDesigning business logic in a microservice architecture\nclasses that are instantiated. A factory might be implemented as a static method\nof a class.\nRepository—An object that provides access to persistent entities and encapsu-\nlates the mechanism for accessing the database.\nService—An object that implements business logic that doesn’t belong in an\nentity or a value object.\nThese building blocks are used by many developers. Some are supported by frame-\nworks such as JPA and the Spring framework. There is one more building block that\nhas been generally ignored (myself included!) except by DDD purists: aggregates. As\nit turns out, aggregates are an extremely useful concept when developing microser-\nvices. Let’s first look at some subtle problems with classic OOD that are solved by\nusing aggregates. \n5.2\nDesigning a domain model using the \nDDD aggregate pattern\nIn traditional object-oriented design, a domain model is a collection of classes and\nrelationships between classes. The classes are usually organized into packages. For\nexample, figure 5.4 shows part of a domain model for the FTGO application. It’s a typ-\nical domain model consisting of a web of interconnected classes.\nThis example has several classes corresponding to business objects: Consumer, Order,\nRestaurant, and Courier. But interestingly, the explicit boundaries of each business\nobject are missing from this kind of traditional domain model. It doesn’t specify, for\nConsumer\nOrder\nstate\n...\ncreditcardId\n...\ndeliveryTime\nquantity\nname\nprice\nstreet1\nstreet2\ncity\nstate\nzip\nname\n...\navailable\n...\nlat\nlon\nRestaurant\nCourier\nLocation\nPaymentInfo\nDeliveryInfo\nOrderLineItem\nMenuItem\nAddress\nPlaced by\nFor\nAssigned to\nPaid using\nPays using\nFigure 5.4\nA traditional domain model is a web of interconnected classes. It doesn’t explicitly specify the \nboundaries of business objects, such as Consumer and Order.\n \n",
      "content_length": 1904,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "153\nDesigning a domain model using the DDD aggregate pattern\nexample, which classes are part of the Order business object. This lack of boundaries\ncan sometimes cause problems, especially in microservice architecture.\n I begin this section with an example problem caused by the lack of explicit bound-\naries. Next I describe the concept of an aggregate and how it has explicit boundaries.\nAfter that, I describe the rules that aggregates must obey and how they make aggre-\ngates a good fit for the microservice architecture. I then describe how to carefully\nchoose the boundaries of your aggregates and why it matters. Finally, I discuss how to\ndesign business logic using aggregates. Let’s first take a look at the problems caused\nby fuzzy boundaries.\n5.2.1\nThe problem with fuzzy boundaries\nImagine, for example, that you want to perform an operation, such as a load or delete,\non an Order business object. What exactly does that mean? What is the scope an oper-\nation? You would certainly load or delete the Order object. But in reality there’s more\nto an Order than simply the Order object. There are also the order line items, the pay-\nment information, and so on. Figure 5.4 leaves the boundaries of a domain object to\nthe developer’s intuition.\n Besides a conceptual fuzziness, the lack of explicit boundaries causes problems\nwhen updating a business object. A typical business object has invariants, business\nrules that must be enforced at all times. An Order has a minimum order amount, for\nexample. The FTGO application must ensure that any attempt to update an order\ndoesn’t violate an invariant such as the minimum order amount. The challenge is that\nin order to enforce invariants, you must design your business logic carefully.\n For example, let’s look at how to ensure the order minimum is met when multiple\nconsumers work together to create an order. Two consumers—Sam and Mary—are\nworking together on an order and simultaneously decide that the order exceeds their\nbudget. Sam reduces the quantity of samosas, and Mary reduces the quantity of naan\nbread. From the application’s perspective, both consumers retrieve the order and its\nline items from the database. Both consumers then update a line item to reduce the\ncost of the order. From each consumer’s perspective the order minimum is preserved.\nHere’s the sequence of database transactions.\nConsumer - Mary\nConsumer - Sam\nBEGIN TXN\nSELECT ORDER_TOTAL FROM ORDER\nWHERE ORDER ID = X\nSELECT * FROM ORDER_LINE_ITEM\nWHERE ORDER_ID = X\n...\nEND TXN\nBEGIN TXN\nSELECT ORDER_TOTAL FROM ORDER\nWHERE ORDER ID = X\nSELECT * FROM ORDER_LINE_ITEM\nWHERE ORDER_ID = X\n...\nEND TXN\nVerify minimum is met\n \n",
      "content_length": 2658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "154\nCHAPTER 5\nDesigning business logic in a microservice architecture\nEach consumer changes a line item using a sequence of two transactions. The first\ntransaction loads the order and its line items. The UI verifies that the order minimum\nis satisfied before executing the second transaction. The second transaction updates\nthe line item quantity using an optimistic offline locking check that verifies that the\norder line is unchanged since it was loaded by the first transaction.\n In this scenario, Sam reduces the order total by $X and Mary reduces it by $Y. As a\nresult, the Order is no longer valid, even though the application verified that the order\nstill satisfied the order minimum after each consumer’s update. As you can see, directly\nupdating part of a business object can result in the violation of the business rules. DDD\naggregates are intended to solve this problem. \n5.2.2\nAggregates have explicit boundaries\nAn aggregate is a cluster of domain objects within a boundary that can be treated as a\nunit. It consists of a root entity and possibly one or more other entities and value\nobjects. Many business objects are modeled as aggregates. For example, in chapter 2\nwe created a rough domain model by analyzing the nouns used in the requirements\nand by domain experts. Many of these nouns, such as Order, Consumer, and Restau-\nrant, are aggregates.\nFigure 5.5 shows the Order aggregate and its boundary. An Order aggregate consists of\nan Order entity, one or more OrderLineItem value objects, and other value objects\nsuch as a delivery Address and PaymentInformation.\nBEGIN TXN\nUPDATE ORDER_LINE_ITEM\nSET VERSION=..., QUANTITY=...\nWHERE VERSION = <loaded version>\nAND ID = ...\nEND TXN\nVerify minimum is met\nBEGIN TXN\nUPDATE ORDER_LINE_ITEM\nSET VERSION=..., QUANTITY=...\nWHERE VERSION = <loaded version>\nAND ID = ...\nEND TXN\nPattern: Aggregate\nOrganize a domain model as a collection of aggregates, each of which is a graph of\nobjects that can be treated as a unit.\n \n",
      "content_length": 1983,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "155\nDesigning a domain model using the DDD aggregate pattern\nAggregates decompose a domain model into chunks, which are individually easier to\nunderstand. They also clarify the scope of operations such as load, update, and delete.\nThese operations act on the entire aggregate rather than on parts of it. An aggregate\nis often loaded in its entirety from the database, thereby avoiding any complications of\nlazy loading. Deleting an aggregate removes all of its objects from a database.\nAGGREGATES ARE CONSISTENCY BOUNDARIES\nUpdating an entire aggregate rather than its parts solves the consistency issues, such\nas the example described earlier. Update operations are invoked on the aggregate\nroot, which enforces invariants. Also, concurrency is handled by locking the aggregate\nroot using, for example, a version number or a database-level lock. For example,\ninstead of updating line items’ quantities directly, a client must invoke a method on\nthe root of the Order aggregate, which enforces invariants such as the minimum order\namount. Note, though, that this approach doesn’t require the entire aggregate to be\nupdated in the database. An application might, for example, only update the rows cor-\nresponding to the Order object and the updated OrderLineItem. \nIDENTIFYING AGGREGATES IS KEY\nIn DDD, a key part of designing a domain model is identifying aggregates, their\nboundaries, and their roots. The details of the aggregates’ internal structure is sec-\nondary. The benefit of aggregates, however, goes far beyond modularizing a domain\nmodel. That’s because aggregates must obey certain rules. \n5.2.3\nAggregate rules\nDDD requires aggregates to obey a set of rules. These rules ensure that an aggregate is\na self-contained unit that can enforce its invariants. Let’s look at each of the rules.\n«\n»\naggregate root\nOrder\nquantity\n«\n»\naggregate root\nConsumer\nOrder aggregate\nConsumer aggregate\n...\n«\n»\naggregate root\nRestaurant\n«\n»\nvalue object\nOrderLineItem\nRestaurant aggregate\n«\n»\nvalue object\nDeliveryInfo\n«\n»\nvalue object\nPaymentInfo\n«\n»\nvalue object\nDeliveryInfo\n«\n»\nvalue object\nPaymentInfo\nFigure 5.5\nStructuring a domain model as a set of aggregates makes the boundaries explicit.\n \n",
      "content_length": 2195,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "156\nCHAPTER 5\nDesigning business logic in a microservice architecture\nRULE #1: REFERENCE ONLY THE AGGREGATE ROOT\nThe previous example illustrated the perils of updating OrderLineItems directly. The\ngoal of the first aggregate rule is to eliminate this problem. It requires that the root\nentity be the only part of an aggregate that can be referenced by classes outside of the\naggregate. A client can only update an aggregate by invoking a method on the aggre-\ngate root.\n A service, for example, uses a repository to load an aggregate from the database\nand obtain a reference to the aggregate root. It updates an aggregate by invoking a\nmethod on the aggregate root. This rule ensures that the aggregate can enforce its\ninvariant.\nRULE #2: INTER-AGGREGATE REFERENCES MUST USE PRIMARY KEYS\nAnother rule is that aggregates reference each other by identity (for example, primary\nkey) instead of object references. For example, as figure 5.6 shows, an Order refer-\nences its Consumer using a consumerId rather than a reference to the Consumer object.\nSimilarly, an Order references a Restaurant using a restaurantId.\nThis approach is quite different from traditional object modeling, which considers\nforeign keys in the domain model to be a design smell. It has a number of benefits.\nThe use of identity rather than object references means that the aggregates are loosely\ncoupled. It ensures that the aggregate boundaries between aggregates are well\ndefined and avoids accidentally updating a different aggregate. Also, if an aggregate is\npart of another service, there isn’t a problem of object references that span services.\n This approach also simplifies persistence since the aggregate is the unit of storage.\nIt makes it easier to store aggregates in a NoSQL database such as MongoDB. It also\nconsumerId\nrestaurantId\n...\n«\n»\naggregate root\nOrder\nquantity\nOrderLineItem\nDeliveryInfo\n...\n«\n»\naggregate root\nConsumer\nOrder aggregate\nConsumer aggregate\n...\n«\n»\naggregate root\nRestaurant\nRestaurant aggregate\nPaymentInfo\nDeliveryInfo\nPaymentInfo\nFigure 5.6\nReferences between aggregates are by primary key rather than by object reference. The \nOrder aggregate has the IDs of the Consumer and Restaurant aggregates. Within an aggregate, \nobjects have references to one another.\n \n",
      "content_length": 2275,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "157\nDesigning a domain model using the DDD aggregate pattern\neliminates the need for transparent lazy loading and its associated problems. Scaling\nthe database by sharding aggregates is relatively straightforward.\nRULE #3: ONE TRANSACTION CREATES OR UPDATES ONE AGGREGATE\nAnother rule that aggregates must obey is that a transaction can only create or update\na single aggregate. When I first read about it many years ago, this rule made no sense! At\nthe time, I was developing traditional monolithic applications that used an RDBMS, so\ntransactions could update multiple aggregates. Today, this constraint is perfect for the\nmicroservice architecture. It ensures that a transaction is contained within a service.\nThis constraint also matches the limited transaction model of most NoSQL databases.\n This rule makes it more complicated to implement operations that need to create\nor update multiple aggregates. But this is exactly the problem that sagas (described in\nchapter 4) are designed to solve. Each step of the saga creates or updates exactly one\naggregate. Figure 5.7 shows how this works.\nIn this example, the saga consists of three transactions. The first transaction updates\naggregate X in service A. The other two transactions are both in service B. One transac-\ntion updates aggregate X, and the other updates aggregate Y.\n An alternative approach to maintaining consistency across multiple aggregates\nwithin a single service is to cheat and update multiple aggregates within a transaction.\nFor example, service B could update aggregates Y and Z in a single transaction. This is\nonly possible when using a database, such as an RDBMS, that supports a rich transac-\ntion model. If you’re using a NoSQL database that only has simple transactions,\nthere’s no other option except to use sagas.\n Or is there? It turns out that aggregate boundaries are not set in stone. When\ndeveloping a domain model, you get to choose where the boundaries lie. But like a\n20th century colonial power drawing national boundaries, you need to be careful. \nService A\nSaga\nLocal transaction 1\nCreate/update\nAggregate X\nService B\nLocal transaction 2\nAggregate Y\nLocal transaction 3\nAggregate Z\nCreate/update\nCreate/update\nFigure 5.7\nA transaction can only create or update a single aggregate, so an application uses a saga \nto update multiple aggregates. Each step of the saga creates or updates one aggregate.\n \n",
      "content_length": 2399,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "158\nCHAPTER 5\nDesigning business logic in a microservice architecture\n5.2.4\nAggregate granularity\nWhen developing a domain model, a key decision you must make is how large to\nmake each aggregate. On one hand, aggregates should ideally be small. Because\nupdates to each aggregate are serialized, more fine-grained aggregates will increase\nthe number of simultaneous requests that the application can handle, improving scal-\nability. It will also improve the user experience because it reduces the chance of two\nusers attempting conflicting updates of the same aggregate. On the other hand, because\nan aggregate is the scope of transaction, you may need to define a larger aggregate in\norder to make a particular update atomic.\n For example, earlier I mentioned how in the FTGO application’s domain model\nOrder and Consumer are separate aggregates. An alternative design is to make Order\npart of the Consumer aggregate. Figure 5.8 shows this alternative design.\nA benefit of this larger Consumer aggregate is that the application can atomically\nupdate a Consumer and one or more of its Orders. A drawback of this approach is that\nit reduces scalability. Transactions that update different orders for the same customer\nwould be serialized. Similarly, two users would conflict if they attempted to edit differ-\nent orders for the same customer.\n Another drawback of this approach in a microservice architecture is that it is an\nobstacle to decomposition. The business logic for Orders and Consumers, for exam-\nple, must be collocated in the same service, which makes the service larger. Because of\nthese issues, making aggregates as fine-grained as possible is best. \nrestaurantId\n...\nOrder\nquantity\nOrderLineItem\nDeliveryInfo\n...\n<<aggregate root>>\nConsumer\nConsumer aggregate\n...\n<<aggregate root>>\nRestaurant\nRestaurant aggregate\nPaymentInfo\nDeliveryInfo\nPaymentInfo\nFigure 5.8\nAn alternative design defines a Customer aggregate that contains the Customer and \nOrder classes. This design enables an application to atomically update a Consumer and one or more \nof its Orders.\n \n",
      "content_length": 2076,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "159\nDesigning a domain model using the DDD aggregate pattern\n5.2.5\nDesigning business logic with aggregates\nIn a typical (micro)service, the bulk of the business logic consists of aggregates. The\nrest of the business logic resides in the domain services and the sagas. The sagas orches-\ntrate sequences of local transactions in order to enforce data consistency. The services\nare the entry points into the business logic and are invoked by inbound adapters. A\nservice uses a repository to retrieve aggregates from the database or save aggregates to\nthe database. Each repository is implemented by an outbound adapter that accesses\nthe database. Figure 5.9 shows the aggregate-based design of the business logic for the\nOrder Service.\nThe business logic consists of the Order aggregate, the OrderService service class, the\nOrderRepository, and one or more sagas. The OrderService invokes the Order-\nRepository to save and load Orders. For simple requests that are local to the service,\nREST API\nDomain\nevent\npublisher\n«service»\nOrderService\n«saga»\nCreateOrder\nSaga\n«saga»\nReviseOrder\nSaga\ncreateOrder()\nreviseOrder()\ncancelOrder()\n«value object»\nOrderLineItem\nquantity\nmenuItem\nname\nOrder\ncommand\nhandlers\nDatabase\nadapter\n«aggregate»\nOrder\nid\n...\n«repository»\nOrderRepository\nvoidSave(Order)\nOrer ﬁndOne(id)\n...\nFigure 5.9\nAn aggregate-based design for the Order Service business logic\n \n",
      "content_length": 1388,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "160\nCHAPTER 5\nDesigning business logic in a microservice architecture\nthe service updates an Order aggregate. If an update request spans multiple services,\nthe OrderService will also create a saga, as described in chapter 4.\n We’ll take a look at the code—but first, let’s examine a concept that’s closely\nrelated to aggregates: domain events. \n5.3\nPublishing domain events\nMerriam-Webster (https://www.merriam-webster.com/dictionary/event) lists several\ndefinitions of the word event, including these:\n1\nSomething that happens\n2\nA noteworthy happening\n3\nA social occasion or activity\n4\nAn adverse or damaging medical occurrence, a heart attack or other cardiac event\nIn the context of DDD, a domain event is something that has happened to an aggre-\ngate. It’s represented by a class in the domain model. An event usually represents a\nstate change. Consider, for example, an Order aggregate in the FTGO application. Its\nstate-changing events include Order Created, Order Cancelled, Order Shipped, and\nso forth. An Order aggregate might, if there are interested consumers, publish one of\nthe events each time it undergoes a state transition.\n5.3.1\nWhy publish change events?\nDomain events are useful because other parties—users, other applications, or other\ncomponents within the same application—are often interested in knowing about an\naggregate’s state changes. Here are some example scenarios:\nMaintaining data consistency across services using choreography-based sagas,\ndescribed in chapter 4.\nNotifying a service that maintains a replica that the source data has changed.\nThis approach is known as Command Query Responsibility Segregation (CQRS),\nand it’s described in chapter 7.\nNotifying a different application via a registered webhook or via a message bro-\nker in order to trigger the next step in a business process.\nNotifying a different component of the same application in order, for example,\nto send a WebSocket message to a user’s browser or update a text database such\nas ElasticSearch.\nSending notifications—text messages or emails—to users informing them that\ntheir order has shipped, their Rx prescription is ready for pick up, or their\nflight is delayed.\nPattern: Domain event\nAn aggregate publishes a domain event when it’s created or undergoes some other\nsignificant change.\n \n",
      "content_length": 2304,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "161\nPublishing domain events\nMonitoring domain events to verify that the application is behaving correctly.\nAnalyzing events to model user behavior.\nThe trigger for the notification in all these scenarios is the state change of an aggre-\ngate in an application’s database. \n5.3.2\nWhat is a domain event?\nA domain event is a class with a name formed using a past-participle verb. It has proper-\nties that meaningfully convey the event. Each property is either a primitive value or a\nvalue object. For example, an OrderCreated event class has an orderId property.\n A domain event typically also has metadata, such as the event ID, and a timestamp.\nIt might also have the identity of the user who made the change, because that’s useful\nfor auditing. The metadata can be part of the event object, perhaps defined in a\nsuperclass. Alternatively, the event metadata can be in an envelope object that wraps\nthe event object. The ID of the aggregate that emitted the event might also be part of\nthe envelope rather than an explicit event property.\n The OrderCreated event is an example of a domain event. It doesn’t have any\nfields, because the Order’s ID is part of the event envelope. The following listing\nshows the OrderCreated event class and the DomainEventEnvelope class.\ninterface DomainEvent {}\ninterface OrderDomainEvent extends DomainEvent {}\nclass OrderCreated implements OrderDomainEvent {}\nclass DomainEventEnvelope<T extends DomainEvent> {\nprivate String aggregateType;      \nprivate Object aggregateId;\nprivate T event;\n...\n}\nThe DomainEvent interface is a marker interface that identifies a class as a domain\nevent. OrderDomainEvent is a marker interface for events, such as OrderCreated, which\nare published by the Order aggregate. The DomainEventEnvelope is a class that con-\ntains event metadata and the event object. It’s a generic class that’s parameterized by\nthe domain event type. \n5.3.3\nEvent enrichment\nLet’s imagine, for example, that you’re writing an event consumer that processes Order\nevents. The OrderCreated event class shown previously captures the essence of what has\nhappened. But your event consumer may need the order details when processing an\nListing 5.1\nThe OrderCreated event and the DomainEventEnvelope class\nThe event’s \nmetadata\n \n",
      "content_length": 2271,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "162\nCHAPTER 5\nDesigning business logic in a microservice architecture\nOrderCreated event. One option is for it to retrieve that information from the Order-\nService. The drawback of an event consumer querying the service for the aggregate is\nthat it incurs the overhead of a service request.\n An alternative approach known as event enrichment is for events to contain informa-\ntion that consumers need. It simplifies event consumers because they no longer need\nto request that data from the service that published the event. In the OrderCreated\nevent, the Order aggregate can enrich the event by including the order details. The\nfollowing listing shows the enriched event.\nclass OrderCreated implements OrderEvent {\nprivate List<OrderLineItem> lineItems;\nprivate DeliveryInformation deliveryInformation;       \nprivate PaymentInformation paymentInformation;\nprivate long restaurantId;\nprivate String restaurantName;\n...\n}\nBecause this version of the OrderCreated event contains the order details, an event\nconsumer, such as the Order History Service (discussed in chapter 7) no longer\nneeds to fetch that data when processing an OrderCreated event.\n Although event enrichment simplifies consumers, the drawback is that it risks mak-\ning the event classes less stable. An event class potentially needs to change whenever\nthe requirements of its consumers change. This can reduce maintainability because\nthis kind of change can impact multiple parts of the application. Satisfying every con-\nsumer can also be a futile effort. Fortunately, in many situations it’s fairly obvious\nwhich properties to include in an event.\n Now that we’ve covered the basics of domain events, let’s look at how to discover\nthem. \n5.3.4\nIdentifying domain events\nThere are a few different strategies for identifying domain events. Often the require-\nments will describe scenarios where notifications are required. The requirements\nmight include language such as “When X happens do Y.” For example, one require-\nment in the FTGO application is “When an Order is placed send the consumer an\nemail.” A requirement for a notification suggests the existence of a domain event.\n Another approach, which is increasing in popularity, is to use event storming. Event\nstorming is an event-centric workshop format for understanding a complex domain. It\ninvolves gathering domain experts in a room, lots of sticky notes, and a very large sur-\nface—a whiteboard or paper roll—to stick the notes on. The result of event storming\nis an event-centric domain model consisting of aggregates and events.\n \nListing 5.2\nThe enriched OrderCreated event \nData that its \nconsumers \ntypically need\n \n",
      "content_length": 2651,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "163\nPublishing domain events\n Event storming consist of three main steps:\n1\nBrainstorm events—Ask the domain experts to brainstorm the domain events.\nDomain events are represented by orange sticky notes that are laid out in a\nrough timeline on the modeling surface.\n2\nIdentify event triggers—Ask the domain experts to identify the trigger of each\nevent, which is one of the following:\n– User actions, represented as a command using a blue sticky note\n– External system, represented by a purple sticky note\n– Another domain event\n– Passing of time\n3\nIdentify aggregates—Ask the domain experts to identify the aggregate that con-\nsumes each command and emits the corresponding event. Aggregates are rep-\nresented by yellow sticky notes.\nFigure 5.10 shows the result of an event-storming workshop. In just a couple of hours,\nthe participants identified numerous domain events, commands, and aggregates. It\nwas a good first step in the process of creating a domain model.\nEvent storming is a useful technique for quickly creating a domain model.\n Now that we’ve covered the basics of domain events, let’s look at the mechanics of\ngenerating and publishing them. \nEvent\nCommand\nAggregate\nPolicy\nFigure 5.10\nThe result of an event-storming workshop that lasted a couple of hours. The sticky notes \nare events, which are laid out along a timeline; commands, which represent user actions; and \naggregates, which emit events in response to a command.\n \n",
      "content_length": 1444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "164\nCHAPTER 5\nDesigning business logic in a microservice architecture\n5.3.5\nGenerating and publishing domain events\nCommunicating using domain events is a form of asynchronous messaging, discussed\nin chapter 3. But before the business logic can publish them to a message broker, it\nmust first create them. Let’s look at how to do that.\nGENERATING DOMAIN EVENTS\nConceptually, domain events are published by aggregates. An aggregate knows when\nits state changes and hence what event to publish. An aggregate could invoke a mes-\nsaging API directly. The drawback of this approach is that because aggregates can’t\nuse dependency injection, the messaging API would need to be passed around as a\nmethod argument. That would intertwine infrastructure concerns and business logic,\nwhich is extremely undesirable.\n A better approach is to split responsibility between the aggregate and the service\n(or equivalent class) that invokes it. Services can use dependency injection to obtain a\nreference to the messaging API, easily publishing events. The aggregate generates the\nevents whenever its state changes and returns them to the service. There are a couple\nof different ways an aggregate can return events back to the service. One option is for\nthe return value of an aggregate method to include a list of events. For example, the\nfollowing listing shows how a Ticket aggregate’s accept() method can return a Ticket-\nAcceptedEvent to its caller.\npublic class Ticket {\npublic List<DomainEvent> accept(ZonedDateTime readyBy) {\n...\nthis.acceptTime = ZonedDateTime.now();\n  \nthis.readyBy = readyBy;\nreturn singletonList(new TicketAcceptedEvent(readyBy));      \n}\n}\nThe service invokes the aggregate root’s method, and then publishes the events. For\nexample, the following listing shows how KitchenService invokes Ticket.accept() and\npublishes the events.\npublic class KitchenService {\n@Autowired\nprivate TicketRepository ticketRepository;\n@Autowired\nprivate DomainEventPublisher domainEventPublisher;\nListing 5.3\nThe Ticket aggregate’s accept() method\nListing 5.4\nKitchenService calls Ticket.accept() \nUpdates \nthe Ticket\nReturns \nan event\n \n",
      "content_length": 2131,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "165\nPublishing domain events\npublic void accept(long ticketId, ZonedDateTime readyBy) {\nTicket ticket =\nticketRepository.findById(ticketId)\n.orElseThrow(() ->\nnew TicketNotFoundException(ticketId));\nList<DomainEvent> events = ticket.accept(readyBy);\ndomainEventPublisher.publish(Ticket.class, orderId, events);\n}\nThe accept() method first invokes the TicketRepository to load the Ticket from the\ndatabase. It then updates the Ticket by calling accept(). KitchenService then pub-\nlishes events returned by Ticket by calling DomainEventPublisher.publish(),\ndescribed shortly.\n This approach is quite simple. Methods that would otherwise have a void return\ntype now return List<Event>. The only potential drawback is that the return type of\nnon-void methods is now more complex. They must return an object containing the\noriginal return value and List<Event>. You’ll see an example of such a method soon.\n Another option is for the aggregate root to accumulate events in a field. The ser-\nvice then retrieves the events and publishes them. For example, the following listing\nshows a variant of the Ticket class that works this way.\npublic class Ticket extends AbstractAggregateRoot {\npublic void accept(ZonedDateTime readyBy) {\n...\nthis.acceptTime = ZonedDateTime.now();\nthis.readyBy = readyBy;\nregisterDomainEvent(new TicketAcceptedEvent(readyBy));\n}\n}\nTicket extends AbstractAggregateRoot, which defines a registerDomainEvent()\nmethod that records the event. A service would call AbstractAggregateRoot.get-\nDomainEvents() to retrieve those events.\n My preference is for the first option: the method returning events to the service.\nBut accumulating events in the aggregate root is also a viable option. In fact, the\nSpring Data Ingalls release train (https://spring.io/blog/2017/01/30/what-s-new-in-\nspring-data-release-ingalls) implements a mechanism that automatically publishes\nevents to the Spring ApplicationContext. The main drawback is that to reduce code\nduplication, aggregate roots should extend a superclass such as AbstractAggregate-\nRoot, which might conflict with a requirement to extend some other superclass. Another\nissue is that although it’s easy for the aggregate root’s methods to call register-\nDomainEvent(), methods in other classes in the aggregate would find it challenging.\nThey would mostly likely need to somehow pass the events to the aggregate root. \nListing 5.5\nThe Ticket extends a superclass, which records domain events\nPublishes \ndomain \nevents\n \n",
      "content_length": 2482,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "166\nCHAPTER 5\nDesigning business logic in a microservice architecture\nHOW TO RELIABLY PUBLISH DOMAIN EVENTS?\nChapter 3 talks about how to reliably send messages as part of a local database transac-\ntion. Domain events are no different. A service must use transactional messaging to\npublish events to ensure that they’re published as part of the transaction that updates\nthe aggregate in the database. The Eventuate Tram framework, described in chapter 3,\nimplements such a mechanism. It insert events into an OUTBOX table as part of the\nACID transaction that updates the database. After the transaction commits, the events\nthat were inserted into the OUTBOX table are then published to the message broker.\n The Tram framework provides a DomainEventPublisher interface, shown in the\nfollowing listing. It defines several overloaded publish() methods that take the aggre-\ngate type and ID as parameters, along with a list of domain events.\npublic interface DomainEventPublisher {\nvoid publish(String aggregateType, Object aggregateId, \nList<DomainEvent> domainEvents);\nIt uses the Eventuate Tram framework’s MessageProducer interface to publish those\nevents transactionally.\n A service could call the DomainEventPublisher publisher directly. But one draw-\nback of doing so is that it doesn’t ensure that a service only publishes valid events.\nKitchenService, for example, should only publish events that implement Ticket-\nDomainEvent, which is the marker interface for the Ticket aggregate’s events. A better\noption is for services to implement a subclass of AbstractAggregateDomainEvent-\nPublisher, which is shown in listing 5.7. AbstractAggregateDomainEventPublisher\nis an abstract class that provides a type-safe interface for publishing domain events.\nIt’s a generic class that has two type parameters, A, the aggregate type, and E, the\nmarker interface type for the domain events. A service publishes events by calling\nthe publish() method, which has two parameters: an aggregate of type A and a list of\nevents of type E.\npublic abstract class AbstractAggregateDomainEventPublisher<A, E extends Doma\ninEvent> {\nprivate Function<A, Object> idSupplier;\nprivate DomainEventPublisher eventPublisher;\nprivate Class<A> aggregateType;\nprotected AbstractAggregateDomainEventPublisher(\nDomainEventPublisher eventPublisher,\nClass<A> aggregateType,\nFunction<A, Object> idSupplier) {\nthis.eventPublisher = eventPublisher;\nthis.aggregateType = aggregateType;\nListing 5.6\nThe Eventuate Tram framework’s DomainEventPublisher interface\nListing 5.7\nThe abstract superclass of type-safe domain event publishers\n \n",
      "content_length": 2598,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "167\nPublishing domain events\nthis.idSupplier = idSupplier;\n}\npublic void publish(A aggregate, List<E> events) {\neventPublisher.publish(aggregateType, idSupplier.apply(aggregate), \n(List<DomainEvent>) events);\n}\n}\nThe publish() method retrieves the aggregate’s ID and invokes DomainEventPublisher\n.publish(). The following listing shows the TicketDomainEventPublisher, which\npublishes domain events for the Ticket aggregate.\npublic class TicketDomainEventPublisher extends \nAbstractAggregateDomainEventPublisher<Ticket, TicketDomainEvent> {\npublic TicketDomainEventPublisher(DomainEventPublisher eventPublisher) {\nsuper(eventPublisher, Ticket.class, Ticket::getId);\n}\n}\nThis class only publishes events that are a subclass of TicketDomainEvent.\n Now that we’ve looked at how to publish domain events, let’s see how to con-\nsume them. \n5.3.6\nConsuming domain events\nDomain events are ultimately published as messages to a message broker, such as\nApache Kafka. A consumer could use the broker’s client API directly. But it’s more\nconvenient to use a higher-level API such as the Eventuate Tram framework’s Domain-\nEventDispatcher, described in chapter 3. A DomainEventDispatcher dispatches\ndomain events to the appropriate handle method. Listing 5.9 shows an example event\nhandler class. KitchenServiceEventConsumer subscribes to events published by\nRestaurant Service whenever a restaurant’s menu is updated. It’s responsible for\nkeeping Kitchen Service’s replica of the data up-to-date.\npublic class KitchenServiceEventConsumer {\n@Autowired\nprivate RestaurantService restaurantService;\npublic DomainEventHandlers domainEventHandlers() {       \nreturn DomainEventHandlersBuilder\n.forAggregateType(\"net.chrisrichardson.ftgo.restaurantservice.Restaurant\")\n.onEvent(RestaurantMenuRevised.class, this::reviseMenu)\nListing 5.8\nA type-safe interface for publishing Ticket aggregates' domain events\nListing 5.9\nDispatching events to event handler methods\nMaps events to \nevent handlers\n \n",
      "content_length": 1979,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "168\nCHAPTER 5\nDesigning business logic in a microservice architecture\n.build();\n}\npublic void reviseMenu(DomainEventEnvelope<RestaurantMenuRevised> de) {  \nlong id = Long.parseLong(de.getAggregateId());\nRestaurantMenu revisedMenu = de.getEvent().getRevisedMenu();\nrestaurantService.reviseMenu(id, revisedMenu);\n}\n}\nThe reviseMenu() method handles RestaurantMenuRevised events. It calls restaurant-\nService.reviseMenu(), which updates the restaurant’s menu. That method returns a\nlist of domain events, which are published by the event handler.\n Now that we’ve looked at aggregates and domain events, it’s time to consider some\nexample business logic that’s implemented using aggregates. \n5.4\nKitchen Service business logic\nThe first example is Kitchen Service, which enables a restaurant to manage their\norders. The two main aggregates in this service are the Restaurant and Ticket aggre-\ngates. The Restaurant aggregate knows the restaurant’s menu and opening hours\nand can validate orders. A Ticket represents an order that a restaurant must prepare\nfor pickup by a courier. Figure 5.11 shows these aggregates and other key parts of the\nservice’s business logic, as well as the service’s adapters.\n In addition to the aggregates, the other main parts of Kitchen Service’s business\nlogic are KitchenService, TicketRepository, and RestaurantRepository. Kitchen-\nService is the business logic’s entry. It defines methods for creating and updating\nthe Restaurant and Ticket aggregates. TicketRepository and RestaurantRepository\ndefine methods for persisting Tickets and Restaurants respectively.\n The Kitchen Service service has three inbound adapters:\n\nREST API—The REST API invoked by the user interface used by workers at the\nrestaurant. It invokes KitchenService to create and update Tickets.\n\nKitchenServiceCommandHandler—The asynchronous request/response-based\nAPI that’s invoked by sagas. It invokes KitchenService to create and update\nTickets.\n\nKitchenServiceEventConsumer—Subscribes to events published by Restaurant\nService. It invokes KitchenService to create and update Restaurants.\nThe service also has two outbound adapters:\n\nDB adapter—Implements the TicketRepository and the RestaurantRepository\ninterfaces and accesses the database.\n\nDomainEventPublishingAdapter—Implements the DomainEventPublisher inter-\nface and publishes Ticket domain events.\nAn event handler for the\nRestaurantMenuRevised\nevent\n \n",
      "content_length": 2422,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "169\nKitchen Service business logic\nLet’s take a closer look at the design of KitchenService, starting with the Ticket\naggregate.\n5.4.1\nThe Ticket aggregate\nTicket is one of the aggregates of Kitchen Service. As described in chapter 2, when\ntalking about the concept of a Bounded Context, this aggregate represents the restau-\nrant kitchen’s view of an order. It doesn’t contain information about the consumer,\nsuch as their identity, the delivery information, or payment details. It’s focused on\nenabling a restaurant’s kitchen to prepare the Order for pickup. Moreover, Kitchen-\nService doesn’t generate a unique ID for this aggregate. Instead, it uses the ID sup-\nplied by OrderService.\n Let’s first look at the structure of this class and then we’ll examine its methods.\nKitchen Service\ncommand channel\nRestaurant Events\nchannel\nTicket events\nchannel\nKitchen Service\ndatabase\nCreate ticket\nConﬁrm create ticket\nRestaurant created\nRestaurant menu revised\naccept\nreject\npreparing\nreadyForPickup\npickedUp\nREST API\nRestaurant\nKitchen\nService\nDomain event\npublisher\n«aggregate»\nTicket\n«aggregate»\nrestaurant\n«repository»\nTicket\nRepository\n«repository»\nRestaurant\nRepository\nKitchenService\nCommandHandler\nKitchenService\nEventConsumer\nDatabase\nadapter\nDomain event\npublishing adapter\nFigure 5.11\nThe design of Kitchen Service\n \n",
      "content_length": 1324,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "170\nCHAPTER 5\nDesigning business logic in a microservice architecture\nSTRUCTURE OF THE TICKET CLASS\nThe following listing shows an excerpt of the code for this class. The Ticket class is\nsimilar to a traditional domain class. The main difference is that references to other\naggregates are by primary key.\n@Entity(table=\"tickets\")\npublic class Ticket {\n@Id\nprivate Long id;\nprivate TicketState state;\nprivate Long restaurantId;\n@ElementCollection\n@CollectionTable(name=\"ticket_line_items\")\nprivate List<TicketLineItem> lineItems;\nprivate ZonedDateTime readyBy;\nprivate ZonedDateTime acceptTime;\nprivate ZonedDateTime preparingTime;\nprivate ZonedDateTime pickedUpTime;\nprivate ZonedDateTime readyForPickupTime;\n...\nThis class is persisted with JPA and is mapped to the TICKETS table. The restaurantId\nfield is a Long rather than an object reference to a Restaurant. The readyBy field\nstores the estimate of when the order will be ready for pickup. The Ticket class has\nseveral fields that track the history of the order, including acceptTime, preparing-\nTime, and pickupTime. Let’s look at this class’s methods. \nBEHAVIOR OF THE TICKET AGGREGATE\nThe Ticket aggregate defines several methods. As you saw earlier, it has a static create()\nmethod, which is a factory method that creates a Ticket. There are also some meth-\nods that are invoked when the restaurant updates the state of the order:\n\naccept()—The restaurant has accepted the order.\n\npreparing()—The restaurant has started preparing the order, which means the\norder can no longer be changed or cancelled.\n\nreadyForPickup()—The order can now be picked up.\nThe following listing shows some of its methods.\n \n \n \n \nListing 5.10\nPart of the Ticket class, which is a JPA entity\n \n",
      "content_length": 1735,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "171\nKitchen Service business logic\npublic class Ticket {\npublic static ResultWithAggregateEvents<Ticket, TicketDomainEvent> \ncreate(Long id, TicketDetails details) {\nreturn new ResultWithAggregateEvents<>(new Ticket(id, details), new \nTicketCreatedEvent(id, details));\n}\npublic List<TicketPreparationStartedEvent> preparing() {\nswitch (state) {\ncase ACCEPTED:\nthis.state = TicketState.PREPARING;\nthis.preparingTime = ZonedDateTime.now();\nreturn singletonList(new TicketPreparationStartedEvent());\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\npublic List<TicketDomainEvent> cancel() {\nswitch (state) {\ncase CREATED:\ncase ACCEPTED:\nthis.state = TicketState.CANCELLED;\nreturn singletonList(new TicketCancelled());\ncase READY_FOR_PICKUP:\nthrow new TicketCannotBeCancelledException();\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\nThe create() method creates a Ticket. The preparing() method is called when the\nrestaurant starts preparing the order. It changes the state of the order to PREPARING,\nrecords the time, and publishes an event. The cancel() method is called when a user\nattempts to cancel an order. If the cancellation is allowed, this method changes the\nstate of the order and returns an event. Otherwise, it throws an exception. These\nmethods are invoked in response to REST API requests as well as events and com-\nmand messages. Let’s look at the classes that invoke the aggregate’s method. \nTHE KITCHENSERVICE DOMAIN SERVICE\nKitchenService is invoked by the service’s inbound adapters. It defines various meth-\nods for changing the state of an order, including accept(), reject(), preparing(), and\nothers. Each method loads the specifies aggregate, calls the corresponding method on\nthe aggregate root, and publishes any domain events. The following listing shows its\naccept() method.\nListing 5.11\nSome of the Ticket's methods\n \n",
      "content_length": 1883,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "172\nCHAPTER 5\nDesigning business logic in a microservice architecture\npublic class KitchenService {\n@Autowired\nprivate TicketRepository ticketRepository;\n@Autowired\nprivate TicketDomainEventPublisher domainEventPublisher;\npublic void accept(long ticketId, ZonedDateTime readyBy) {\nTicket ticket =\nticketRepository.findById(ticketId)\n.orElseThrow(() ->\nnew TicketNotFoundException(ticketId));\nList<TicketDomainEvent> events = ticket.accept(readyBy);\ndomainEventPublisher.publish(ticket, events);          \n}\n}\nThe accept() method is invoked when the restaurant accepts a new order. It has two\nparameters:\n\norderId—ID of the order to accept\n\nreadyBy—Estimated time when the order will be ready for pickup\nThis method retrieves the Ticket aggregate and calls its accept() method. It pub-\nlishes any generated events.\n Now let’s look at the class that handles asynchronous commands. \nTHE KITCHENSERVICECOMMANDHANDLER CLASS\nThe KitchenServiceCommandHandler class is an adapter that’s responsible for handling\ncommand messages sent by the various sagas implemented by Order Service. This class\ndefines a handler method for each command, which invokes KitchenService to create\nor update a Ticket. The following listing shows an excerpt of this class.\npublic class KitchenServiceCommandHandler {\n@Autowired\nprivate KitchenService kitchenService;\n public CommandHandlers commandHandlers() {\n  \nreturn CommandHandlersBuilder\n.fromChannel(\"orderService\")\n.onMessage(CreateTicket.class, this::createTicket)\n.onMessage(ConfirmCreateTicket.class,\nthis::confirmCreateTicket)\nListing 5.12\nThe service’s accept() method updates Ticket\nListing 5.13\nHandling command messages sent by sagas\nPublish \ndomain \nevents\nMaps  command messages \nto message handlers\n \n",
      "content_length": 1743,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "173\nOrder Service business logic\n.onMessage(CancelCreateTicket.class,\nthis::cancelCreateTicket)\n.build();\n}\nprivate Message createTicket(CommandMessage<CreateTicket>\ncm) {\nCreateTicket command = cm.getCommand();\nlong restaurantId = command.getRestaurantId();\nLong ticketId = command.getOrderId();\nTicketDetails ticketDetails =\ncommand.getTicketDetails();\ntry {\nTicket ticket =\n   \nkitchenService.createTicket(restaurantId,\nticketId, ticketDetails);\nCreateTicketReply reply =\nnew CreateTicketReply(ticket.getId());\nreturn withSuccess(reply);\n  \n} catch (RestaurantDetailsVerificationException e) {\nreturn withFailure();\n}\n}\nprivate Message confirmCreateTicket\n(CommandMessage<ConfirmCreateTicket> cm) {\n  \nLong ticketId = cm.getCommand().getTicketId();\nkitchenService.confirmCreateTicket(ticketId);\nreturn withSuccess();\n}\n...\nAll the command handler methods invoke KitchenService and reply with either a\nsuccess or a failure reply.\n Now that you’ve seen the business logic for a relatively simple service, we’ll look at\na more complex example: Order Service. \n5.5\nOrder Service business logic\nAs mentioned in earlier chapters, Order Service provides an API for creating, updat-\ning, and canceling orders. This API is primarily invoked by the consumer. Figure 5.12\nshows the high-level design of the service. The Order aggregate is the central aggre-\ngate of Order Service. But there’s also a Restaurant aggregate, which is a partial\nreplica of data owned by Restaurant Service. It enables Order Service to validate\nand price an Order’s line items.\n In addition to the Order and Restaurant aggregates, the business logic consists of\nOrderService, OrderRepository, RestaurantRepository, and various sagas such as\nthe CreateOrderSaga described in chapter 4. OrderService is the primary entry\npoint into the business logic and defines methods for creating and updated Orders\nInvokes KitchenService \nto create the Ticket\nSends back a \nsuccessful reply\nSends back a \nfailure reply\nConfirms \nthe order\n \n",
      "content_length": 1997,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "174\nCHAPTER 5\nDesigning business logic in a microservice architecture\nand Restaurants. OrderRepository defines methods for persisting Orders, and\nRestaurantRepository has methods for persisting Restaurants. Order Service has\nseveral inbound adapters:\n\nREST API—The REST API invoked by the user interface used by consumers. It\ninvokes OrderService to create and update Orders.\nRestaurant Events\nchannel\nOrder Service\ncommand channel\nConsumer Service\ncommand channel\nKitchen Service\ncommand channel\nAccounting Service\ncommand channel\nCreate order saga\nreply channel\nCancel order saga\nreply channel\nRevise order saga\nreply channel\nTicket events\nchannel\ncreateOrder()\ncancelOrder()\nreviseOrder()\nREST API\nConsumer\nOrderService\nDomain event\npublisher\nCommand\nproducer\n«aggregate»\nRestaurant\n«aggregate»\nOrder\n«repository»\nOrderRepository\n«repository»\nRestaurant\nRepository\nOrderEvent\nconsumer\nOrder\ncommand\nhandlers\nSagaReply\nmessage\nadapter\nDatabase\nadapter\nOutbound\ncommand\nmessage\nadapter\n«saga»\n*OrderSaga\nOrder Service\ndatabase\nDomain event\npublishing\nadapter\nFigure 5.12\nThe design of the Order Service. It has a REST API for managing orders. It exchanges messages \nand events with other services via several message channels.\n \n",
      "content_length": 1231,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "175\nOrder Service business logic\n\nOrderEventConsumer—Subscribes to events published by Restaurant Service. It\ninvokes OrderService to create and update its replica of Restaurants.\n\nOrderCommandHandlers—The asynchronous request/response-based API that’s\ninvoked by sagas. It invokes OrderService to update Orders.\n\nSagaReplyAdapter—Subscribes to the saga reply channels and invokes the sagas.\nThe service also has some outbound adapters:\n\nDB adapter—Implements the OrderRepository interface and accesses the Order\nService database\n\nDomainEventPublishingAdapter—Implements the DomainEventPublisher inter-\nface and publishes Order domain events\n\nOutboundCommandMessageAdapter—Implements the CommandPublisher inter-\nface and sends command messages to saga participants\nLet’s first take a closer look at the Order aggregate and then examine OrderService.\n5.5.1\nThe Order Aggregate\nThe Order aggregate represents an order placed by a consumer. We’ll first look at the\nstructure of the Order aggregate and then check out its methods.\nTHE STRUCTURE OF THE ORDER AGGREGATE\nFigure 5.13 shows the structure of the Order aggregate. The Order class is the root of\nthe Order aggregate. The Order aggregate also consists of value objects such as Order-\nLineItem, DeliveryInfo, and PaymentInfo.\n«value object»\nAddress\nstreet1\nstreet2\ncity\nstate\nzip\n«aggregate»\nOrder\nstate\nconsumerId\nrestaurantId\n...\nPrice\nOrder minimum\n«value object»\nOrderLineItem\nquantity\nmenuItem\nname\n«value object»\nDeliveryInfo\ndeliveryTime\n«value object»\nMoney\namount\n«value object»\nPaymentInfo\npaymentMethodId\nFigure 5.13\nThe design of the Order aggregate, which consists of the Order aggregate root \nand various value objects.\n \n",
      "content_length": 1695,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "176\nCHAPTER 5\nDesigning business logic in a microservice architecture\nThe Order class has a collection of OrderLineItems. Because the Order’s Consumer\nand Restaurant are other aggregates, it references them by primary key value. The\nOrder class has a DeliveryInfo class, which stores the delivery address and the\ndesired delivery time, and a PaymentInfo, which stores the payment info. The follow-\ning listing shows the code.\n@Entity\n@Table(name=\"orders\")\n@Access(AccessType.FIELD)\npublic class Order {\n@Id\n@GeneratedValue\nprivate Long id;\n@Version\nprivate Long version;\nprivate OrderState state;\nprivate Long consumerId;\nprivate Long restaurantId;\n@Embedded\nprivate OrderLineItems orderLineItems;\n@Embedded\nprivate DeliveryInformation deliveryInformation;\n@Embedded\nprivate PaymentInformation paymentInformation;\n@Embedded\nprivate Money orderMinimum;\nThis class is persisted with JPA and is mapped to the ORDERS table. The id field is the\nprimary key. The version field is used for optimistic locking. The state of an Order is\nrepresented by the OrderState enumeration. The DeliveryInformation and Payment-\nInformation fields are mapped using the @Embedded annotation and are stored as col-\numns of the ORDERS table. The orderLineItems field is an embedded object that\ncontains the order line items. The Order aggregate consists of more than just fields. It\nalso implements business logic, which can be described by a state machine. Let’s take\na look at the state machine. \nTHE ORDER AGGREGATE STATE MACHINE\nIn order to create or update an order, Order Service must collaborate with other ser-\nvices using sagas. Either OrderService or the first step of the saga invokes an Order\nmethod that verifies that the operation can be performed and changes the state of the\nOrder to a pending state. A pending state, as explained in chapter 4, is an example of\nListing 5.14\nThe Order class and its fields\n \n",
      "content_length": 1900,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "177\nOrder Service business logic\na semantic lock countermeasure, which helps ensure that sagas are isolated from one\nanother. Eventually, once the saga has invoked the participating services, it then\nupdates the Order to reflect the outcome. For example, as described in chapter 4, the\nCreate Order Saga has multiple participant services, including Consumer Service,\nAccounting Service, and Kitchen Service. OrderService first creates an Order in an\nAPPROVAL_PENDING state, and then later changes its state to either APPROVED or\nREJECTED. The behavior of an Order can be modeled as the state machine shown in\nfigure 5.14.\nSimilarly, other Order Service operations such as revise() and cancel() first change\nthe Order to a pending state and use a saga to verify that the operation can be per-\nformed. Then, once the saga has verified that the operation can be performed, it\nchanges the Order transitions to some other state that reflects the successful outcome\nof the operation. If the verification of the operation fails, the Order reverts to the pre-\nvious state. For example, the cancel() operation first transitions the Order to the\nCANCEL_PENDING state. If the order can be cancelled, the Cancel Order Saga changes\nthe state of the Order to the CANCELLED state. Otherwise, if a cancel() operation is\nrejected because, for example, it’s too late to cancel the order, then the Order transi-\ntions back to the APPROVED state.\n Let’s now look at the how the Order aggregate implements this state machine. \nTHE ORDER AGGREGATE’S METHODS\nThe Order class has several groups of methods, each of which corresponds to a saga.\nIn each group, one method is invoked at the start of the saga, and the other methods\nare invoked at the end. I’ll first discuss the business logic that creates an Order. After\nthat we’ll look at how an Order is updated. The following listing shows the Order’s\nmethods that are invoked during the process of creating an Order.\nAPPROVAL_PENDING\nCANCEL_PENDING\ncancelRejected\ncancelConﬁrmed\nrevise\nrejected\nauthorized\ncancel\nreviseConﬁrmed\nreviseRejected\nREVISION_PENDING\nAPPROVED\nCANCELLED\nREJECTED\n...\nInitial state\nFigure 5.14\nPart of the state machine model of the Order aggregate\n \n",
      "content_length": 2204,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "178\nCHAPTER 5\nDesigning business logic in a microservice architecture\npublic class Order { ...\npublic static ResultWithDomainEvents<Order, OrderDomainEvent>\n createOrder(long consumerId, Restaurant restaurant,\nList<OrderLineItem> orderLineItems) {\nOrder order = new Order(consumerId, restaurant.getId(), orderLineItems);\nList<OrderDomainEvent> events = singletonList(new OrderCreatedEvent(\nnew OrderDetails(consumerId, restaurant.getId(), orderLineItems,\norder.getOrderTotal()),\nrestaurant.getName()));\nreturn new ResultWithDomainEvents<>(order, events);\n}\npublic Order(OrderDetails orderDetails) {\nthis.orderLineItems = new OrderLineItems(orderDetails.getLineItems());\nthis.orderMinimum = orderDetails.getOrderMinimum();\nthis.state = APPROVAL_PENDING;\n}\n...\npublic List<DomainEvent> noteApproved() {\nswitch (state) {\ncase APPROVAL_PENDING:\nthis.state = APPROVED;\nreturn singletonList(new OrderAuthorized());\n...\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\npublic List<DomainEvent> noteRejected() {\nswitch (state) {\ncase APPROVAL_PENDING:\nthis.state = REJECTED;\nreturn singletonList(new OrderRejected());\n...\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\nThe createOrder() method is a static factory method that creates an Order and pub-\nlishes an OrderCreatedEvent. The OrderCreatedEvent is enriched with the details of\nthe Order, including the line items, the total amount, the restaurant ID, and the\nrestaurant name. Chapter 7 discusses how Order History Service uses Order events,\nincluding OrderCreatedEvent, to maintain an easily queried replica of Orders.\nListing 5.15\nThe methods that are invoked during order creation\n \n",
      "content_length": 1673,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "179\nOrder Service business logic\n The initial state of the Order is APPROVAL_PENDING. When the CreateOrderSaga\ncompletes, it will invoke either noteApproved() or noteRejected(). The note-\nApproved() method is invoked when the consumer’s credit card has been successfully\nauthorized. The noteRejected() method is called when one of the services rejects\nthe order or authorization fails. As you can see, the state of the Order aggregate\ndetermines the behavior of most of its methods. Like the Ticket aggregate, it also\nemits events.\n In addition to createOrder(), the Order class defines several update methods. For\nexample, the Revise Order Saga revises an order by first invoking the revise() method\nand then, once it’s verified that the revision can be made, it invokes the confirm-\nRevised() method. The following listing shows these methods.\nclass Order ...\npublic List<OrderDomainEvent> revise(OrderRevision orderRevision) {\nswitch (state) {\ncase APPROVED:\nLineItemQuantityChange change =\norderLineItems.lineItemQuantityChange(orderRevision);\nif (change.newOrderTotal.isGreaterThanOrEqual(orderMinimum)) {\nthrow new OrderMinimumNotMetException();\n}\nthis.state = REVISION_PENDING;\nreturn singletonList(new OrderRevisionProposed(orderRevision,\nchange.currentOrderTotal, change.newOrderTotal));\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\npublic List<OrderDomainEvent> confirmRevision(OrderRevision orderRevision) {\nswitch (state) {\ncase REVISION_PENDING:\nLineItemQuantityChange licd =\norderLineItems.lineItemQuantityChange(orderRevision);\norderRevision\n.getDeliveryInformation()\n.ifPresent(newDi -> this.deliveryInformation = newDi);\nif (!orderRevision.getRevisedLineItemQuantities().isEmpty()) {\norderLineItems.updateLineItems(orderRevision);\n}\nthis.state = APPROVED;\nreturn singletonList(new OrderRevised(orderRevision,\nlicd.currentOrderTotal, licd.newOrderTotal));\nListing 5.16\nThe Order method for revising an Order\n \n",
      "content_length": 1948,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "180\nCHAPTER 5\nDesigning business logic in a microservice architecture\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\n}\nThe revise() method is called to initiate the revision of an order. Among other\nthings, it verifies that the revised order won’t violate the order minimum and changes\nthe state of the order to REVISION_PENDING. Once Revise Order Saga has successfully\nupdated Kitchen Service and Accounting Service, it then calls confirmRevision()\nto complete the revision.\n These methods are invoked by OrderService. Let’s take a look at that class. \n5.5.2\nThe OrderService class\nThe OrderService class defines methods for creating and updating Orders. It’s the\nmain entry point into the business logic and is invoked by various inbound adapters,\nsuch as the REST API. Most of its methods create a saga to orchestrate the creation and\nupdating of Order aggregates. As a result, this service is more complicated than the\nKitchenService class discussed earlier. The following listing shows an excerpt of this\nclass. OrderService is injected with various dependencies, including OrderRepository,\nOrderDomainEventPublisher, and several saga managers. It defines several methods,\nincluding createOrder() and reviseOrder().\n@Transactional\npublic class OrderService {\n@Autowired\nprivate OrderRepository orderRepository;\n@Autowired\nprivate SagaManager<CreateOrderSagaState, CreateOrderSagaState>\ncreateOrderSagaManager;\n@Autowired\nprivate SagaManager<ReviseOrderSagaState, ReviseOrderSagaData>\nreviseOrderSagaManagement;\n@Autowired\nprivate OrderDomainEventPublisher orderAggregateEventPublisher;\npublic Order createOrder(OrderDetails orderDetails) {\nRestaurant restaurant = restaurantRepository.findById(restaurantId)\n.orElseThrow(() -\n> new RestaurantNotFoundException(restaurantId));\nListing 5.17\nThe OrderService class has methods for creating and managing orders\n \n",
      "content_length": 1884,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "181\nOrder Service business logic\nList<OrderLineItem> orderLineItems =\n  \nmakeOrderLineItems(lineItems, restaurant);\nResultWithDomainEvents<Order, OrderDomainEvent> orderAndEvents =\nOrder.createOrder(consumerId, restaurant, orderLineItems);\nOrder order = orderAndEvents.result;\norderRepository.save(order);\norderAggregateEventPublisher.publish(order, orderAndEvents.events);  \nOrderDetails orderDetails =\nnew OrderDetails(consumerId, restaurantId, orderLineItems,\norder.getOrderTotal());\nCreateOrderSagaState data = new CreateOrderSagaState(order.getId(),\norderDetails);\ncreateOrderSagaManager.create(data, Order.class, order.getId());  \nreturn order;\n}\npublic Order reviseOrder(Long orderId, Long expectedVersion,\nOrderRevision orderRevision)\n{\npublic Order reviseOrder(long orderId, OrderRevision orderRevision) {\nOrder order = orderRepository.findById(orderId)\n  \n.orElseThrow(() -> new OrderNotFoundException(orderId));\nReviseOrderSagaData sagaData =\nnew ReviseOrderSagaData(order.getConsumerId(), orderId,\nnull, orderRevision);\nreviseOrderSagaManager.create(sagaData);   \nreturn order;\n}\n}\nThe createOrder() method first creates and persists an Order aggregate. It then pub-\nlishes the domain events emitted by the aggregate. Finally, it creates a CreateOrder-\nSaga. The reviseOrder() retrieves the Order and then creates a ReviseOrderSaga.\n In many ways, the business logic for a microservices-based application is not that\ndifferent from that of a monolithic application. It’s comprised of classes such as ser-\nvices, JPA-backed entities, and repositories. There are some differences, though. A\ndomain model is organized as a set of DDD aggregates that impose various design\nconstraints. Unlike in a traditional object model, references between classes in differ-\nent aggregates are in terms of primary key value rather than object references. Also, a\ntransaction can only create or update a single aggregate. It’s also useful for aggregates\nto publish domain events when their state changes.\n Another major difference is that services often use sagas to maintain data consis-\ntency across multiple services. For example, Kitchen Service merely participates in\nsagas, it doesn’t initiate them. In contrast, Order Service relies heavily on sagas when\nCreates the Order \naggregate\nPersists the Order \nin the database\nPublishes\ndomain\nevents\nCreates the Create\nOrder Saga\nRetrieves\nthe Order\nCreates the \nRevise Order \nSaga\n \n",
      "content_length": 2429,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "182\nCHAPTER 5\nDesigning business logic in a microservice architecture\ncreating and updating orders. That’s because Orders must be transactionally consis-\ntent with data owned by other services. As a result, most OrderService methods create\na saga rather than update an Order directly.\n This chapter has covered how to implement business logic using a traditional\napproach to persistence. That has involved integrating messaging and event publish-\ning with database transaction management. The event publishing code is intertwined\nwith the business logic. The next chapter looks at event sourcing, an event-centric\napproach to writing business logic where event generation is integral to the business\nlogic rather than being bolted on. \nSummary\nThe procedural Transaction script pattern is often a good way to implement\nsimple business logic. But when implementing complex business logic you should\nconsider using the object-oriented Domain model pattern.\nA good way to organize a service’s business logic is as a collection of DDD aggre-\ngates. DDD aggregates are useful because they modularize the domain model,\neliminate the possibility of object reference between services, and ensure that\neach ACID transaction is within a service.\nAn aggregate should publish domain events when it’s created or updated.\nDomain events have a wide variety of uses. Chapter 4 discusses how they can\nimplement choreography-based sagas. And, in chapter 7, I talk about how to\nuse domain events to update replicated data. Domain event subscribers can also\nnotify users and other applications, and publish WebSocket messages to a user’s\nbrowser. \n \n",
      "content_length": 1633,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "183\nDeveloping business\nlogic with event sourcing\nMary liked the idea, described in chapter 5, of structuring business logic as a collec-\ntion of DDD aggregates that publish domain events. She could imagine the use of\nthose events being extremely useful in a microservice architecture. Mary planned\nto use events to implement choreography-based sagas, which maintain data consis-\ntency across services and are described in chapter 4. She also expected to use CQRS\nviews, replicas that support efficient querying that are described in chapter 7.\n She was, however, worried that the event publishing logic might be error prone.\nOn one hand, the event publishing logic is reasonably straightforward. Each of an\naggregate’s methods that initializes or changes the state of the aggregate returns a\nlist of events. The domain service then publishes those events. But on the other\nThis chapter covers\nUsing the Event sourcing pattern to develop \nbusiness logic\nImplementing an event store\nIntegrating sagas and event sourcing-based \nbusiness logic\nImplementing saga orchestrators using event \nsourcing\n \n",
      "content_length": 1101,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "184\nCHAPTER 6\nDeveloping business logic with event sourcing\nhand, the event publishing logic is bolted on to the business logic. The business logic\ncontinues to work even when the developer forgets to publish an event. Mary was con-\ncerned that this way of publishing events might be a source of bugs.\n Many years ago, Mary had learned about event sourcing, an event-centric way of writ-\ning business logic and persisting domain objects. At the time she was intrigued by its\nnumerous benefits, including how it preserves the complete history of the changes to\nan aggregate, but it remained a curiosity. Given the importance of domain events\nin microservice architecture, she now wonders whether it would be worthwhile to\nexplore using event sourcing in the FTGO application. After all, event sourcing elimi-\nnates a source of programming errors by guaranteeing that an event will be published\nwhenever an aggregate is created or updated.\n I begin this chapter by describing how event sourcing works and how you can use it\nto write business logic. I describe how event sourcing persists each aggregate as a\nsequence of events in what is known as an event store. I discuss the benefits and draw-\nbacks of event sourcing and cover how to implement an event store. I describe a sim-\nple framework for writing event sourcing-based business logic. After that, I discuss\nhow event sourcing is a good foundation for implementing sagas. Let’s start by look-\ning at how to develop business logic with event sourcing.\n6.1\nDeveloping business logic using event sourcing\nEvent sourcing is a different way of structuring the business logic and persisting aggre-\ngates. It persists an aggregate as a sequence of events. Each event represents a state\nchange of the aggregate. An application recreates the current state of an aggregate by\nreplaying the events.\nEvent sourcing has several important benefits. For example, it preserves the history of\naggregates, which is valuable for auditing and regulatory purposes. And it reliably\npublishes domain events, which is particularly useful in a microservice architecture.\nEvent sourcing also has drawbacks. It involves a learning curve, because it’s a different\nway to write your business logic. Also, querying the event store is often difficult, which\nrequires you to use the CQRS pattern, described in chapter 7.\n I begin this section by describing the limitations of traditional persistence. I then\ndescribe event sourcing in detail and talk about how it overcomes those limitations.\nAfter that, I show how to implement the Order aggregate using event sourcing. Finally,\nI describe the benefits and drawbacks of event sourcing.\n Let’s first look at the limitations of the traditional approach to persistence.\nPattern: Event sourcing\nPersist an aggregate as a sequence of domain events that represent state changes.\nSee http://microservices.io/patterns/data/event-sourcing.html.\n \n",
      "content_length": 2913,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "185\nDeveloping business logic using event sourcing\n6.1.1\nThe trouble with traditional persistence\nThe traditional approach to persistence maps classes to database tables, fields of those\nclasses to table columns, and instances of those classes to rows in those tables. For\nexample, figure 6.1 shows how the Order aggregate, described in chapter 5, is mapped\nto the ORDER table. Its OrderLineItems are mapped to the ORDER_LINE_ITEM table.\nThe application persists an order instance as rows in the ORDER and ORDER_LINE_ITEM\ntables. It might do that using an ORM framework such as JPA or a lower-level frame-\nwork such as MyBATIS.\n This approach clearly works well because most enterprise applications store data\nthis way. But it has several drawbacks and limitations:\nObject-Relational impedance mismatch.\nLack of aggregate history.\nImplementing audit logging is tedious and error prone.\nEvent publishing is bolted on to the business logic.\nLet’s look at each of these problems, starting with the Object-Relational impedance\nmismatch problem.\nOBJECT-RELATIONAL IMPEDANCE MISMATCH\nOne age-old problem is the so-called Object-Relational impedance mismatch problem.\nThere’s a fundamental conceptual mismatch between the tabular relational schema\nand the graph structure of a rich domain model with its complex relationships.\nSome aspects of this problem are reflected in polarized debates over the suitability of\nObject/Relational mapping (ORM) frameworks. For example, Ted Neward has said\nthat “Object-Relational mapping is the Vietnam of Computer Science” (http://blogs\n.tedneward.com/post/the-vietnam-of-computer-science/). To be fair, I’ve used\n«class»\nOrder\nID\n1234\nCUSTOMER_ID\ncustomer-abc\nORDER_TOTAL\n1234.56\n...\n...\n«class»\nOrderLineItem\nID\n567\nORDER_ID\n1234\nORDER table\nORDER_LINE_ITEM table\nQUANTITY\n2\n...\n...\nFigure 6.1\nThe traditional approach to persistence maps classes to tables and objects to rows in \nthose tables.\n \n",
      "content_length": 1933,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "186\nCHAPTER 6\nDeveloping business logic with event sourcing\nHibernate successfully to develop applications where the database schema has been\nderived from the object model. But the problems are deeper than the limitations of\nany particular ORM framework. \nLACK OF AGGREGATE HISTORY\nAnother limitation of traditional persistence is that it only stores the current state of\nan aggregate. Once an aggregate has been updated, its previous state is lost. If an\napplication must preserve the history of an aggregate, perhaps for regulatory pur-\nposes, then developers must implement this mechanism themselves. It is time con-\nsuming to implement an aggregate history mechanism and involves duplicating code\nthat must be synchronized with the business logic. \nIMPLEMENTING AUDIT LOGGING IS TEDIOUS AND ERROR PRONE\nAnother issue is audit logging. Many applications must maintain an audit log that\ntracks which users have changed an aggregate. Some applications require auditing for\nsecurity or regulatory purposes. In other applications, the history of user actions is an\nimportant feature. For example, issue trackers and task-management applications\nsuch as Asana and JIRA display the history of changes to tasks and issues. The chal-\nlenge of implementing auditing is that besides being a time-consuming chore, the\nauditing logging code and the business logic can diverge, resulting in bugs. \nEVENT PUBLISHING IS BOLTED ON TO THE BUSINESS LOGIC\nAnother limitation of traditional persistence is that it usually doesn’t support publishing\ndomain events. Domain events, discussed in chapter 5, are events that are published by\nan aggregate when its state changes. They’re a useful mechanism for synchronizing data\nand sending notifications in microservice architecture. Some ORM frameworks, such\nas Hibernate, can invoke application-provided callbacks when data objects change.\nBut there’s no support for automatically publishing messages as part of the transac-\ntion that updates the data. Consequently, as with history and auditing, developers\nmust bolt on event-generation logic, which risks not being synchronized with the busi-\nness logic. Fortunately, there’s a solution to these issues: event sourcing. \n6.1.2\nOverview of event sourcing\nEvent sourcing is an event-centric technique for implementing business logic and per-\nsisting aggregates. An aggregate is stored in the database as a series of events. Each\nevent represents a state change of the aggregate. An aggregate’s business logic is struc-\ntured around the requirement to produce and consume these events. Let’s see how\nthat works.\nEVENT SOURCING PERSISTS AGGREGATES USING EVENTS\nEarlier, in section 6.1.1, I discussed how traditional persistence maps aggregates to\ntables, their fields to columns, and their instances to rows. Event sourcing is a very\ndifferent approach to persisting aggregates that builds on the concept of domain\nevents. It persists each aggregate as a sequence of events in the database, known as\nan event store.\n \n",
      "content_length": 2996,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "187\nDeveloping business logic using event sourcing\n Consider, for example, the Order aggregate. As figure 6.2 shows, rather than store\neach Order as a row in an ORDER table, event sourcing persists each Order aggregate as\none or more rows in an EVENTS table. Each row is a domain event, such as Order\nCreated, Order Approved, Order Shipped, and so on.\nWhen an application creates or updates an aggregate, it inserts the events emitted by\nthe aggregate into the EVENTS table. An application loads an aggregate from the event\nstore by retrieving its events and replaying them. Specifically, loading an aggregate\nconsists of the following three steps:\n1\nLoad the events for the aggregate.\n2\nCreate an aggregate instance by using its default constructor.\n3\nIterate through the events, calling apply().\nFor example, the Eventuate Client framework, covered later in section 6.2.2, uses code\nsimilar to the following to reconstruct an aggregate:\nClass aggregateClass = ...;\nAggregate aggregate = aggregateClass.newInstance();\nfor (Event event : events) {\naggregate = aggregate.applyEvent(event);\n}\n// use aggregate...\nIt creates an instance of the class and iterates through the events, calling the aggre-\ngate’s applyEvent() method. If you’re familiar with functional programming, you\nmay recognize this as a fold or reduce operation.\nevent_id\n102\n103\n104\n105\n...\nEVENTS table\nevent_type\nOrder\nCreated\nOrder\nApproved\nOrder\nShipped\nOrder\nDelivered\n...\nentity_type\nOrder\nOrder\nOrder\nOrder\n...\nentity_id\n101\n101\n101\n101\n...\nevent_data\n{...}\n{...}\n{...}\n{...}\n...\nUnique event ID\nThe type of the event\nIdentiﬁes the aggregate\nThe serialized event,\nsuch as JSON\nFigure 6.2\nEvent sourcing persists each aggregate as a sequence of events. A RDBMS-based \napplication can, for example, store the events in an EVENTS table.\n \n",
      "content_length": 1810,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "188\nCHAPTER 6\nDeveloping business logic with event sourcing\n It may be strange and unfamiliar to reconstruct the in-memory state of an aggre-\ngate by loading the events and replaying events. But in some ways, it’s not all that dif-\nferent from how an ORM framework such as JPA or Hibernate loads an entity. An\nORM framework loads an object by executing one or more SELECT statements to\nretrieve the current persisted state, instantiating objects using their default construc-\ntors. It uses reflection to initialize those objects. What’s different about event sourcing\nis that the reconstruction of the in-memory state is accomplished using events.\n Let’s now look at the requirements event sourcing places on domain events. \nEVENTS REPRESENT STATE CHANGES\nChapter 5 defines domain events as a mechanism for notifying subscribers of changes\nto aggregates. Events can either contain minimal data, such as just the aggregate ID,\nor can be enriched to contain data that’s useful to a typical consumer. For example,\nthe Order Service can publish an OrderCreated event when an order is created. An\nOrderCreated event may only contain the orderId. Alternatively, the event could con-\ntain the complete order so consumers of that event don’t have to fetch the data from\nthe Order Service. Whether events are published and what those events contain are\ndriven by the needs of the consumers. With event sourcing, though, it’s primarily the\naggregate that determines the events and their structure.\n Events aren’t optional when using event sourcing. Every state change of an aggre-\ngate, including its creation, is represented by a domain event. Whenever the aggregate’s\nstate changes, it must emit an event. For example, an Order aggregate must emit an\nOrderCreated event when it’s created, and an Order* event whenever it is updated.\nThis is a much more stringent requirement than before, when an aggregate only emit-\nted events that were of interest to consumers.\n What’s more, an event must contain the data that the aggregate needs to perform\nthe state transition. The state of an aggregate consists of the values of the fields of the\nobjects that comprise the aggregate. A state change might be as simple as changing\nthe value of the field of an object, such as Order.state. Alternatively, a state change\ncan involve adding or removing objects, such as revising an Order’s line items.\n Suppose, as figure 6.3 shows, that the current state of the aggregate is S and the\nnew state is S'. An event E that represents the state change must contain the data such\nthat when an Order is in state S, calling order.apply(E) will update the Order to state\nS'. In the next section you’ll see that apply() is a method that performs the state\nchange represented by an event.\n Some events, such as the Order Shipped event, contain little or no data and just\nrepresent the state transition. The apply() method handles an Order Shipped event\nby changing the Order’s status field to SHIPPED. Other events, however, contain a lot\nof data. An OrderCreated event, for example, must contain all the data needed by the\napply() method to initialize an Order, including its line items, payment information,\ndelivery information, and so on. Because events are used to persist an aggregate, you\nno longer have the option of using a minimal OrderCreated event that contains the\norderId. \n \n",
      "content_length": 3357,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "189\nDeveloping business logic using event sourcing\nAGGREGATE METHODS ARE ALL ABOUT EVENTS\nThe business logic handles a request to update an aggregate by calling a command\nmethod on the aggregate root. In a traditional application, a command method typi-\ncally validates its arguments and then updates one or more of the aggregate’s fields.\nCommand methods in an event sourcing-based application work because they must\ngenerate events. As figure 6.4 shows, the outcome of invoking an aggregate’s com-\nmand method is a sequence of events that represent the state changes that must be\nmade. These events are persisted in the database and applied to the aggregate to\nupdate its state.\nThe requirement to generate events and apply them requires a restructuring—albeit\nmechanical—of the business logic. Event sourcing refactors a command method into\ntwo or more methods. The first method takes a command object parameter, which\nrepresents the request, and determines what state changes need to be performed. It\nvalidates its arguments, and without changing the state of the aggregate, returns a list\nof events representing the state changes. This method typically throws an exception if\nthe command cannot be performed.\nObjects and ﬁeld values\nUpdated objects\nand ﬁeld values\n«aggregate»\nOrder\nS\nEvent\napply()\n«aggregate»\nOrder\nS’\nFigure 6.3\nApplying event E \nwhen the Order is in state S \nmust change the Order state to \nS'. The event must contain the \ndata necessary to perform the \nstate change.\n«aggregate»\nOrder\nS\nEvent\napply()\nProcess(command)\n«aggregate»\nOrder\nS’\n«aggregate»\nOrder\nS\nEvent\nFigure 6.4\nProcessing a command \ngenerates events without changing \nthe state of the aggregate. An \naggregate is updated by applying \nan event.\n \n",
      "content_length": 1737,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "190\nCHAPTER 6\nDeveloping business logic with event sourcing\n The other methods each take a particular event type as a parameter and update\nthe aggregate. There’s one of these methods for each event. It’s important to note\nthat these methods can’t fail, because an event represents a state change that has hap-\npened. Each method updates the aggregate based on the event.\n The Eventuate Client framework, an event-sourcing framework described in more\ndetail in section 6.2.2, names these methods process() and apply(). A process()\nmethod takes a command object, which contains the arguments of the update\nrequest, as a parameter and returns a list of events. An apply() method takes an event\nas a parameter and returns void. An aggregate will define multiple overloaded ver-\nsions of these methods: one process() method for each command class and one\napply() method for each event type emitted by the aggregate. Figure 6.5 shows an\nexample.\nReturns events without updating the Order\nApplies events to update the Order\npublic class Order {\npublic List<Event> process(ReviseOrder command) {\nOrderRevision orderRevision = command.getOrderRevision();\nswitch (state) {\ncase AUTHORIZED:\nLineItemQuantityChange change =\norderLineItems.lineItemQuantityChange(orderRevision);\nif (change.newOrderTotal.isGreaterThanOrEqual(orderMinimum)) {\nthrow new OrderMinimumNotMetException();\n}\nreturn singletonList(\nnew OrderRevisionProposed(\norderRevision, change.currentOrderTotal,\nchange.newOrderTotal));\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\npublic class Order {\npublic void apply(OrderRevisionProposed event) {\nthis.state = REVISION_PENDING;\n}\npublic class Order {\npublic List<DomainEvent> revise(OrderRevision orderRevision) {\nswitch (state) {\ncase AUTHORIZED:\nLineItemQuantityChange change =\norderLineItems.lineItemQuantityChange(orderRevision);\nif (change.newOrderTotal.isGreaterThanOrEqual(orderMinimum)) {\nthrow new OrderMinimumNotMetException();\n}\nthis.state = REVISION_PENDING;\nreturn …;\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\nFigure 6.5\nEvent sourcing splits a method that updates an aggregate into a process() method, which takes \na command and returns events, and one or more apply() methods, which take an event and update the \naggregate.\n \n",
      "content_length": 2292,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "191\nDeveloping business logic using event sourcing\nIn this example, the reviseOrder() method is replaced by a process() method and\nan apply() method. The process() method takes a ReviseOrder command as a\nparameter. This command class is defined by applying Introduce Parameter Object refactor-\ning (https://refactoring.com/catalog/introduceParameterObject.html) to the revise-\nOrder() method. The process() method either returns an OrderRevisionProposed\nevent, or throws an exception if it’s too late to revise the Order or if the proposed revi-\nsion doesn’t meet the order minimum. The apply() method for the OrderRevision-\nProposed event changes the state of the Order to REVISION_PENDING.\n An aggregate is created using the following steps:\n1\nInstantiate aggregate root using its default constructor.\n2\nInvoke process() to generate the new events.\n3\nUpdate the aggregate by iterating through the new events, calling its apply().\n4\nSave the new events in the event store.\nAn aggregate is updated using the following steps:\n1\nLoad aggregate’s events from the event store.\n2\nInstantiate the aggregate root using its default constructor.\n3\nIterate through the loaded events, calling apply() on the aggregate root.\n4\nInvoke its process() method to generate new events.\n5\nUpdate the aggregate by iterating through the new events, calling apply().\n6\nSave the new events in the event store.\nTo see this in action, let’s now look at the event sourcing version of the Order aggregate. \nEVENT SOURCING-BASED ORDER AGGREGATE\nListing 6.1 shows the Order aggregate’s fields and the methods responsible for creat-\ning it. The event sourcing version of the Order aggregate has some similarities to the\nJPA-based version shown in chapter 5. Its fields are almost identical, and it emits simi-\nlar events. What’s different is that its business logic is implemented in terms of pro-\ncessing commands that emit events and applying those events, which updates its state.\nEach method that creates or updates the JPA-based aggregate, such as createOrder()\nand reviseOrder(), is replaced in the event sourcing version by process() and\napply() methods.\npublic class Order {\nprivate OrderState state;\nprivate Long consumerId;\nprivate Long restaurantId;\nprivate OrderLineItems orderLineItems;\nprivate DeliveryInformation deliveryInformation;\nprivate PaymentInformation paymentInformation;\nprivate Money orderMinimum;\nListing 6.1\nThe Order aggregate’s fields and its methods that initialize an instance\n \n",
      "content_length": 2480,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "192\nCHAPTER 6\nDeveloping business logic with event sourcing\npublic Order() {\n}\npublic List<Event> process(CreateOrderCommand command) {   \n... validate command ...\nreturn events(new OrderCreatedEvent(command.getOrderDetails()));\n}\npublic void apply(OrderCreatedEvent event) {\n  \nOrderDetails orderDetails = event.getOrderDetails();\nthis.orderLineItems = new OrderLineItems(orderDetails.getLineItems());\nthis.orderMinimum = orderDetails.getOrderMinimum();\nthis.state = APPROVAL_PENDING;\n}\nThis class’s fields are similar to those of the JPA-based Order. The only difference is\nthat the aggregate’s id isn’t stored in the aggregate. The Order’s methods are quite\ndifferent. The createOrder() factory method has been replaced by process() and\napply() methods. The process() method takes a CreateOrder command and emits\nan OrderCreated event. The apply() method takes the OrderCreated and initializes\nthe fields of the Order.\n We’ll now look at the slightly more complex business logic for revising an order.\nPreviously this business logic consisted of three methods: reviseOrder(), confirm-\nRevision(), and rejectRevision(). The event sourcing version replaces these three\nmethods with three process() methods and some apply() methods. The following list-\ning shows the event sourcing version of reviseOrder() and confirmRevision().\npublic class Order {\npublic List<Event> process(ReviseOrder command) {\n  \nOrderRevision orderRevision = command.getOrderRevision();\nswitch (state) {\ncase APPROVED:\nLineItemQuantityChange change =\norderLineItems.lineItemQuantityChange(orderRevision);\nif (change.newOrderTotal.isGreaterThanOrEqual(orderMinimum)) {\nthrow new OrderMinimumNotMetException();\n}\nreturn singletonList(new OrderRevisionProposed(orderRevision,\nchange.currentOrderTotal, change.newOrderTotal));\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\npublic void apply(OrderRevisionProposed event) {\n  \nthis.state = REVISION_PENDING;\n}\nListing 6.2\nThe process() and apply() methods that revise an Order aggregate\nValidates the command and\nreturns an OrderCreatedEvent\nApply the OrderCreatedEvent by\ninitializing the fields of the Order.\nVerify that the Order \ncan be revised and \nthat the revised \norder meets the \norder minimum.\nChange the state of the Order \nto REVISION_PENDING.\n \n",
      "content_length": 2298,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "193\nDeveloping business logic using event sourcing\npublic List<Event> process(ConfirmReviseOrder command) {\n  \nOrderRevision orderRevision = command.getOrderRevision();\nswitch (state) {\ncase REVISION_PENDING:\nLineItemQuantityChange licd =\norderLineItems.lineItemQuantityChange(orderRevision);\nreturn singletonList(new OrderRevised(orderRevision,\nlicd.currentOrderTotal, licd.newOrderTotal));\ndefault:\nthrow new UnsupportedStateTransitionException(state);\n}\n}\npublic void apply(OrderRevised event) {\nOrderRevision orderRevision = event.getOrderRevision();\nif (!orderRevision.getRevisedLineItemQuantities().isEmpty()) {\norderLineItems.updateLineItems(orderRevision);\n}\nthis.state = APPROVED;\n}\nAs you can see, each method has been replaced by a process() method and one or\nmore apply() methods. The reviseOrder() method has been replaced by process\n(ReviseOrder) and apply(OrderRevisionProposed). Similarly, confirmRevision()\nhas been replaced by process(ConfirmReviseOrder) and apply(OrderRevised). \n6.1.3\nHandling concurrent updates using optimistic locking\nIt’s not uncommon for two or more requests to simultaneously update the same\naggregate. An application that uses traditional persistence often uses optimistic lock-\ning to prevent one transaction from overwriting another’s changes. Optimistic locking\ntypically uses a version column to detect whether an aggregate has changed since it\nwas read. The application maps the aggregate root to a table that has a VERSION col-\numn, which is incremented whenever the aggregate is updated. The application\nupdates the aggregate using an UPDATE statement like this:\nUPDATE AGGREGATE_ROOT_TABLE\nSET VERSION = VERSION + 1 ...\nWHERE VERSION = <original version>\nThis UPDATE statement will only succeed if the version is unchanged from when the\napplication read the aggregate. If two transactions read the same aggregate, the first\none that updates the aggregate will succeed. The second one will fail because the ver-\nsion number has changed, so it won’t accidentally overwrite the first transaction’s\nchanges.\n An event store can also use optimistic locking to handle concurrent updates. Each\naggregate instance has a version that’s read along with the events. When the applica-\ntion inserts events, the event store verifies that the version is unchanged. A simple\nVerify that the \nrevision can be \nconfirmed and \nreturn an Order-\nRevised event.\nRevise the \nOrder.\n \n",
      "content_length": 2413,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "194\nCHAPTER 6\nDeveloping business logic with event sourcing\napproach is to use the number of events as the version number. Alternatively, as you’ll\nsee below in section 6.2, an event store could maintain an explicit version number. \n6.1.4\nEvent sourcing and publishing events\nStrictly speaking, event sourcing persists aggregates as events and reconstructs the cur-\nrent state of an aggregate from those events. You can also use event sourcing as a reli-\nable event publishing mechanism. Saving an event in the event store is an inherently\natomic operation. We need to implement a mechanism to deliver all persisted events\nto interested consumers.\n Chapter 3 describes a couple of different mechanisms—polling and transaction log\ntailing—for publishing messages that are inserted into the database as part of a transac-\ntion. An event sourcing-based application can publish events using one of these mecha-\nnisms. The main difference is that it permanently stores events in an EVENTS table rather\nthan temporarily saving events in an OUTBOX table and then deleting them. Let’s take a\nlook at each approach, starting with polling.\nUSING POLLING TO PUBLISH EVENTS\nIf events are stored in the EVENTS table shown in figure 6.6, an event publisher can\npoll the table for new events by executing a SELECT statement and publish the events\nto a message broker. The challenge is determining which events are new. For exam-\nple, imagine that eventIds are monotonically increasing. The superficially appealing\napproach is for the event publisher to record the last eventId that it has processed. It\nwould then retrieve new events using a query like this: SELECT * FROM EVENTS where\nevent_id > ? ORDER BY event_id ASC.\n The problem with this approach is that transactions can commit in an order that’s\ndifferent from the order in which they generate events. As a result, the event pub-\nlisher can accidentally skip over an event. Figure 6.6 shows such as a scenario.\nTransaction A\nTransaction B\nCOMMIT\nBEGIN\nBEGIN\nCOMMIT\nINSERT event with\nEVENT_ID = 1020\nSELECT * FROM EVENTS\nWHERE EVENT_ID > ....\nSELECT * FROM EVENTS\nWHERE EVENT_ID > 1020...\nINSERT event with\nEVENT_ID = 1010\nRetrieves event 1020\nCommits last\nSkips event 1010 because\n1010 <= event 1020\nFigure 6.6\nA scenario where an event is skipped because its transaction A commits after \ntransaction B. Polling sees eventId=1020 and then later skips eventId=1010.\n \n",
      "content_length": 2411,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "195\nDeveloping business logic using event sourcing\nIn this scenario, Transaction A inserts an event with an EVENT_ID of 1010. Next, trans-\naction B inserts an event with an EVENT_ID of 1020 and then commits. If the event\npublisher were now to query the EVENTS table, it would find event 1020. Later on, after\ntransaction A committed and event 1010 became visible, the event publisher would\nignore it.\n One solution to this problem is to add an extra column to the EVENTS table that\ntracks whether an event has been published. The event publisher would then use the\nfollowing process:\n1\nFind unpublished events by executing this SELECT statement: SELECT * FROM\nEVENTS where PUBLISHED = 0 ORDER BY event_id ASC.\n2\nPublish events to the message broker.\n3\nMark the events as having been published: UPDATE EVENTS SET PUBLISHED = 1\nWHERE EVENT_ID in.\nThis approach prevents the event publisher from skipping events. \nUSING TRANSACTION LOG TAILING TO RELIABLY PUBLISH EVENTS\nMore sophisticated event stores use transaction log tailing, which, as chapter 3 describes,\nguarantees that events will be published and is also more performant and scalable.\nFor example, Eventuate Local, an open source event store, uses this approach. It reads\nevents inserted into an EVENTS table from the database transaction log and pub-\nlishes them to the message broker. Section 6.2 discusses how Eventuate Local works\nin more detail. \n6.1.5\nUsing snapshots to improve performance\nAn Order aggregate has relatively few state transitions, so it only has a small number of\nevents. It’s efficient to query the event store for those events and reconstruct an Order\naggregate. Long-lived aggregates, though, can have a large number of events. For\nexample, an Account aggregate potentially has a large number of events. Over time, it\nwould become increasingly inefficient to load and fold those events.\n A common solution is to periodically persist a snapshot of the aggregate’s state.\nFigure 6.7 shows an example of using a snapshot. The application restores the state of\nThe application only needs\nto retrieve the snapshot and\nevents that occur after it.\nEvent 1\nEvent 2\nEvent ...\nEvent N\nEvent\n+1\nN\nSnapshot\nversion N\nEvent\n+2\nN\nFigure 6.7\nUsing a snapshot improves performance by eliminating the need to load all \nevents. An application only needs to load the snapshot and the events that occur after it.\n \n",
      "content_length": 2379,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "196\nCHAPTER 6\nDeveloping business logic with event sourcing\nan aggregate by loading the most recent snapshot and only those events that have\noccurred since the snapshot was created.\n In this example, the snapshot version is N. The application only needs to load the\nsnapshot and the two events that follow it in order to restore the state of the aggre-\ngate. The previous N events are not loaded from the event store.\n When restoring the state of an aggregate from a snapshot, an application first creates\nan aggregate instance from the snapshot and then iterates through the events, applying\nthem. For example, the Eventuate Client framework, described in section 6.2.2, uses\ncode similar to the following to reconstruct an aggregate:\nClass aggregateClass = ...;\nSnapshot snapshot = ...;\nAggregate aggregate = recreateFromSnapshot(aggregateClass, snapshot);\nfor (Event event : events) {\naggregate = aggregate.applyEvent(event);\n}\n// use aggregate...\nWhen using snapshots, the aggregate instance is recreated from the snapshot instead\nof being created using its default constructor. If an aggregate has a simple, easily seri-\nalizable structure, the snapshot can be, for example, its JSON serialization. More com-\nplex aggregates can be snapshotted using the Memento pattern (https://en.wikipedia\n.org/wiki/Memento_pattern).\n The Customer aggregate in the online store example has a very simple structure:\nthe customer’s information, their credit limit, and their credit reservations. A snap-\nshot of a Customer is the JSON serialization of its state. Figure 6.8 shows how to recre-\nate a Customer from a snapshot corresponding to the state of a Customer as of event\n#103. The Customer Service needs to load the snapshot and the events that have\noccurred after event #103.\nThe Customer Service recreates the Customer by deserializing the snapshot’s JSON\nand then loading and applying events #104 through #106. \nevent_id\n...\n103\n104\n105\n106\nEVENTS\nevent_type\n...\n...\nCredit\nReserved\nAddress\nChanged\nCredit\nReserved\nentity_type\n...\nCustomer\nCustomer\nCustomer\nCustomer\nentity_id\n...\n101\n101\n101\n101\nevent_data\n...\n{...}\n{...}\n{...}\n{...}\nevent_id\n...\n103\n...\n...\nSNAPSHOTS\nentity_type\n...\nCustomer\n...\n...\nsnapshot_data\n...\n{name: “...” , ...}\n...\n...\nevent_id\n...\n101\n...\n...\nFigure 6.8\nThe Customer Service recreates the Customer by deserializing the snapshot’s JSON and then \nloading and applying events #104 through #106.\n \n",
      "content_length": 2425,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "197\nDeveloping business logic using event sourcing\n6.1.6\nIdempotent message processing\nServices often consume messages from other applications or other services. A service\nmight, for example, consume domain events published by aggregates or command\nmessages sent by a saga orchestrator. As described in chapter 3, an important issue\nwhen developing a message consumer is ensuring that it’s idempotent, because a mes-\nsage broker might deliver the same message multiple times.\n A message consumer is idempotent if it can safely be invoked with the same mes-\nsage multiple times. The Eventuate Tram framework, for example, implements idem-\npotent message handling by detecting and discarding duplicate messages. It records\nthe ids of processed messages in a PROCESSED_MESSAGES table as part of the local\nACID transaction used by the business logic to create or update aggregates. If the ID\nof a message is in the PROCESSED_MESSAGES table, it’s a duplicate and can be dis-\ncarded. Event sourcing-based business logic must implement an equivalent mecha-\nnism. How this is done depends on whether the event store uses an RDBMS or a\nNoSQL database.\nIDEMPOTENT MESSAGE PROCESSING WITH AN RDBMS-BASED EVENT STORE\nIf an application uses an RDBMS-based event store, it can use an identical approach to\ndetect and discard duplicates messages. It inserts the message ID into the PROCESSED\n_MESSAGES table as part of the transaction that inserts events into the EVENTS table. \nIDEMPOTENT MESSAGE PROCESSING WHEN USING A NOSQL-BASED EVENT STORE\nA NoSQL-based event store, which has a limited transaction model, must use a different\nmechanism to implement idempotent message handling. A message consumer must\nsomehow atomically persist events and record the message ID. Fortunately, there’s a\nsimple solution. A message consumer stores the message’s ID in the events that are\ngenerated while processing it. It detects duplicates by verifying that none of an aggre-\ngate’s events contains the message ID.\n One challenge with using this approach is that processing a message might not\ngenerate any events. The lack of events means there’s no record of a message having\nbeen processed. A subsequent redelivery and reprocessing of the same message might\nresult in incorrect behavior. For example, consider the following scenario:\n1\nMessage A is processed but doesn’t update an aggregate.\n2\nMessage B is processed, and the message consumer updates the aggregate.\n3\nMessage A is redelivered, and because there’s no record of it having been pro-\ncessed, the message consumer updates the aggregate.\n4\nMessage B is processed again….\nIn this scenario, the redelivery of events results in a different and possibly erroneous\noutcome.\n One way to avoid this problem is to always publish an event. If an aggregate doesn’t\nemit an event, an application saves a pseudo event solely to record the message ID.\nEvent consumers must ignore these pseudo events. \n \n",
      "content_length": 2929,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "198\nCHAPTER 6\nDeveloping business logic with event sourcing\n6.1.7\nEvolving domain events\nEvent sourcing, at least conceptually, stores events forever—which is a double-edged\nsword. On one hand, it provides the application with an audit log of changes that’s\nguaranteed to be accurate. It also enables an application to reconstruct the historical\nstate of an aggregate. On the other hand, it creates a challenge, because the structure\nof events often changes over time.\n An application must potentially deal with multiple versions of events. For example,\na service that loads an Order aggregate could potentially need to fold multiple ver-\nsions of events. Similarly, an event subscriber might potentially see multiple versions.\n Let’s first look at the different ways that events can change, and then I’ll describe a\ncommonly used approach for handling changes.\nEVENT SCHEMA EVOLUTION\nConceptually, an event sourcing application has a schema that’s organized into\nthree levels:\nConsists of one or more aggregates\nDefines the events that each aggregate emits\nDefines the structure of the events\nTable 6.1 shows the different types of changes that can occur at each level.\nThese changes occur naturally as a service’s domain model evolves over time—for\nexample, when a service’s requirements change or as its developers gain deeper insight\ninto a domain and improve the domain model. At the schema level, developers add,\nremove, and rename aggregate classes. At the aggregate level, the types of events\nTable 6.1\nThe different ways that an application’s events can evolve\nLevel\nChange\nBackward compatible\nSchema\nDefine a new aggregate type\nYes\nRemove aggregate\nRemove an existing aggregate\nNo\nRename aggregate\nChange the name of an aggregate type\nNo\nAggregate\nAdd a new event type\nYes\nRemove event\nRemove an event type\nNo\nRename event\nChange the name of an event type\nNo\nEvent\nAdd a new field\nYes\nDelete field\nDelete a field\nNo\nRename field\nRename a field\nNo\nChange type of field\nChange the type of a field\nNo\n \n",
      "content_length": 2013,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "199\nDeveloping business logic using event sourcing\nemitted by a particular aggregate can change. Developers can change the structure of\nan event type by adding, removing, and changing the name or type of a field.\n Fortunately, many of these types of changes are backward-compatible changes. For\nexample, adding a field to an event is unlikely to impact consumers. A consumer\nignores unknown fields. Other changes, though, aren’t backward compatible. For\nexample, changing the name of an event or the name of a field requires consumers of\nthat event type to be changed. \nMANAGING SCHEMA CHANGES THROUGH UPCASTING\nIn the SQL database world, changes to a database schema are commonly handled\nusing schema migrations. Each schema change is represented by a migration, a SQL\nscript that changes the schema and migrates the data to a new schema. The schema\nmigrations are stored in a version control system and applied to a database using a\ntool such as Flyway.\n An event sourcing application can use a similar approach to handle non-backward-\ncompatible changes. But instead of migrating events to the new schema version in\nsitu, event sourcing frameworks transform events when they’re loaded from the event\nstore. A component commonly called an upcaster updates individual events from an\nold version to a newer version. As a result, the application code only ever deals with\nthe current event schema.\n Now that we’ve looked at how event sourcing works, let’s consider its benefits and\ndrawbacks. \n6.1.8\nBenefits of event sourcing\nEvent sourcing has both benefits and drawbacks. The benefits include the following:\nReliably publishes domain events\nPreserves the history of aggregates\nMostly avoids the O/R impedance mismatch problem\nProvides developers with a time machine\nLet’s examine each benefit in more detail.\nRELIABLY PUBLISHES DOMAIN EVENTS\nA major benefit of event sourcing is that it reliably publishes events whenever the state\nof an aggregate changes. That’s a good foundation for an event-driven microservice\narchitecture. Also, because each event can store the identity of the user who made the\nchange, event sourcing provides an audit log that’s guaranteed to be accurate. The\nstream of events can be used for a variety of other purposes, including notifying users,\napplication integration, analytics, and monitoring. \nPRESERVES THE HISTORY OF AGGREGATES\nAnother benefit of event sourcing is that it stores the entire history of each aggregate.\nYou can easily implement temporal queries that retrieve the past state of an aggregate.\nTo determine the state of an aggregate at a given point in time, you fold the events\n \n",
      "content_length": 2634,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "200\nCHAPTER 6\nDeveloping business logic with event sourcing\nthat occurred up until that point. It’s straightforward, for example, to calculate the\navailable credit of a customer at some point in the past. \nMOSTLY AVOIDS THE O/R IMPEDANCE MISMATCH PROBLEM\nEvent sourcing persists events rather than aggregating them. Events typically have a\nsimple, easily serializable structure. As mentioned earlier, a service can snapshot a\ncomplex aggregate by serializing a memento of its state, which adds a level of indirec-\ntion between an aggregate and its serialized representation. \nPROVIDES DEVELOPERS WITH A TIME MACHINE\nEvent sourcing stores a history of everything that’s happened in the lifetime of an\napplication. Imagine that the FTGO developers need to implement a new require-\nment to customers who added an item to their shopping cart and then removed it. A\ntraditional application wouldn’t preserve this information, so could only market to\ncustomers who add and remove items after the feature is implemented. In contrast, an\nevent sourcing-based application can immediately market to customers who have done\nthis in the past. It’s as if event sourcing provides developers with a time machine for\ntraveling to the past and implementing unanticipated requirements. \n6.1.9\nDrawbacks of event sourcing\nEvent sourcing isn’t a silver bullet. It has the following drawbacks:\nIt has a different programming model that has a learning curve.\nIt has the complexity of a messaging-based application.\nEvolving events can be tricky.\nDeleting data is tricky.\nQuerying the event store is challenging.\nLet’s look at each drawback.\nDIFFERENT PROGRAMMING MODEL THAT HAS A LEARNING CURVE\nIt’s a different and unfamiliar programming model, and that means a learning curve.\nIn order for an existing application to use event sourcing, you must rewrite its busi-\nness logic. Fortunately, that’s a fairly mechanical transformation that you can do when\nyou migrate your application to microservices. \nCOMPLEXITY OF A MESSAGING-BASED APPLICATION\nAnother drawback of event sourcing is that message brokers usually guarantee at-least-\nonce delivery. Event handlers that aren’t idempotent must detect and discard dupli-\ncate events. The event sourcing framework can help by assigning each event a mono-\ntonically increasing ID. An event handler can then detect duplicate events by tracking\nthe highest-seen event ID. This even happens automatically when event handlers\nupdate aggregates. \n \n",
      "content_length": 2471,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "201\nDeveloping business logic using event sourcing\nEVOLVING EVENTS CAN BE TRICKY\nWith event sourcing, the schema of events (and snapshots!) will evolve over time.\nBecause events are stored forever, aggregates potentially need to fold events corre-\nsponding to multiple schema versions. There’s a real risk that aggregates may become\nbloated with code to deal with all the different versions. As mentioned in section 6.1.7,\na good solution to this problem is to upgrade events to the latest version when they’re\nloaded from the event store. This approach separates the code that upgrades events\nfrom the aggregate, which simplifies the aggregates because they only need to apply\nthe latest version of the events. \nDELETING DATA IS TRICKY\nBecause one of the goals of event sourcing is to preserve the history of aggregates, it\nintentionally stores data forever. The traditional way to delete data when using event\nsourcing is to do a soft delete. An application deletes an aggregate by setting a\ndeleted flag. The aggregate will typically emit a Deleted event, which notifies any\ninterested consumers. Any code that accesses that aggregate can check the flag and\nact accordingly.\n Using a soft delete works well for many kinds of data. One challenge, however, is\ncomplying with the General Data Protection Regulation (GDPR), a European data\nprotection and privacy regulation that grants individuals the right to erasure (https://\ngdpr-info.eu/art-17-gdpr/). An application must have the ability to forget a user’s per-\nsonal information, such as their email address. The issue with an event sourcing-based\napplication is that the email address might either be stored in an AccountCreated\nevent or used as the primary key of an aggregate. The application somehow must for-\nget about the user without deleting the events.\n Encryption is one mechanism you can use to solve this problem. Each user has an\nencryption key, which is stored in a separate database table. The application uses that\nencryption key to encrypt any events containing the user’s personal information\nbefore storing them in an event store. When a user requests to be erased, the applica-\ntion deletes the encryption key record from the database table. The user’s personal\ninformation is effectively deleted, because the events can no longer be decrypted.\n Encrypting events solves most problems with erasing a user’s personal information.\nBut if some aspect of a user’s personal information, such as email address, is used as\nan aggregate ID, throwing away the encryption key may not be sufficient. For exam-\nple, section 6.2 describes an event store that has an entities table whose primary key\nis the aggregate ID. One solution to this problem is to use the technique of pseud-\nonymization, replacing the email address with a UUID token and using that as the\naggregate ID. The application stores the association between the UUID token and the\nemail address in a database table. When a user requests to be erased, the application\ndeletes the row for their email address from that table. This prevents the application\nfrom mapping the UUID back to the email address. \n \n",
      "content_length": 3136,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "202\nCHAPTER 6\nDeveloping business logic with event sourcing\nQUERYING THE EVENT STORE IS CHALLENGING\nImagine you need to find customers who have exhausted their credit limit. Because\nthere isn’t a column containing the credit, you can’t write SELECT * FROM CUSTOMER\nWHERE CREDIT_LIMIT = 0. Instead, you must use a more complex and potentially ineffi-\ncient query that has a nested SELECT to compute the credit limit by folding events that\nset the initial credit and adjusting it. To make matters worse, a NoSQL-based event\nstore will typically only support primary key-based lookup. Consequently, you must\nimplement queries using the CQRS approach described in chapter 7. \n6.2\nImplementing an event store\nAn application that uses event sourcing stores its events in an event store. An event store\nis a hybrid of a database and a message broker. It behaves as a database because it has\nan API for inserting and retrieving an aggregate’s events by primary key. And it\nbehaves as a message broker because it has an API for subscribing to events.\n There are a few different ways to implement an event store. One option is to imple-\nment your own event store and event sourcing framework. You can, for example, per-\nsist events in an RDBMS. A simple, albeit low-performance, way to publish events is for\nsubscribers to poll the EVENTS table for events. But, as noted in section 6.1.4, one chal-\nlenge is ensuring that a subscriber processes all events in order.\n Another option is to use a special-purpose event store, which typically provides a\nrich set of features and better performance and scalability. There are several of these\nto chose from:\nEvent Store—A .NET-based open source event store developed by Greg Young,\nan event sourcing pioneer (https://eventstore.org).\nLagom—A microservices framework developed by Lightbend, the company for-\nmerly known as Typesafe (www.lightbend.com/lagom-framework).\nAxon—An open source Java framework for developing event-driven applications\nthat use event sourcing and CQRS (www.axonframework.org).\nEventuate—Developed by my startup, Eventuate (http://eventuate.io). There are\ntwo versions of Eventuate: Eventuate SaaS, a cloud service, and Eventuate Local,\nan Apache Kafka/RDBMS-based open source project.\nAlthough these frameworks differ in the details, the core concepts remain the same.\nBecause Eventuate is the framework I’m most familiar with, that’s the one I cover\nhere. It has a straightforward, easy-to-understand architecture that illustrates event\nsourcing concepts. You can use it in your applications, reimplement the concepts\nyourself, or apply what you learn here to build applications with one of the other\nevent sourcing frameworks.\n I begin the following sections by describing how the Eventuate Local event store\nworks. Then I describe the Eventuate Client framework for Java, an easy-to-use frame-\nwork for writing event sourcing-based business logic that uses the Eventuate Local\nevent store.\n \n",
      "content_length": 2959,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "203\nImplementing an event store\n6.2.1\nHow the Eventuate Local event store works\nEventuate Local is an open source event store. Figure 6.9 shows the architecture.\nEvents are stored in a database, such as MySQL. Applications insert and retrieve\naggregate events by primary key. Applications consume events from a message broker,\nsuch as Apache Kafka. A transaction log tailing mechanism propagates events from\nthe database to the message broker.\nLet’s look at the different Eventuate Local components, starting with the database\nschema.\nTHE SCHEMA OF EVENTUATE LOCAL’S EVENT DATABASE\nThe event database consists of three tables:\n\nevents—Stores the events\n\nentities—One row per entity\n\nsnapshots—Stores snapshots\nThe central table is the events table. The structure of this table is very similar to the\ntable shown in figure 6.2. Here’s its definition:\nevent_id\n102\n103\n...\nEVENTS\nEvent database\nEvent broker\nOrder topic\nEvent relay\nEvent relay\nApplication\nCustomer topic\nevent_type\nOrder\nCreated\nOrder\nApproved\n...\nentity_type\nOrder\nOrder\n...\nentity_id\n101\n101\n...\nevent_data\n{...}\n{...}\n...\nENTITIES\nentity_type\n...\nentity_version\n...\nentity_id\n...\n...\n...\nSNAPSHOTS\nentity_type\n...\nentity_version\n...\nentity_id\n...\n...\n...\nStores the events\nPublishes events stored\nin the database to\nthe message broker\nFigure 6.9\nThe architecture of Eventuate Local. It consists of an event database (such as MySQL) \nthat stores the events, an event broker (like Apache Kafka) that delivers events to subscribers, and an \nevent relay that publishes events stored in the event database to the event broker.\n \n",
      "content_length": 1595,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "204\nCHAPTER 6\nDeveloping business logic with event sourcing\ncreate table events (\nevent_id varchar(1000) PRIMARY KEY,\nevent_type varchar(1000),\nevent_data varchar(1000) NOT NULL,\nentity_type VARCHAR(1000) NOT NULL,\nentity_id VARCHAR(1000) NOT NULL,\ntriggering_event VARCHAR(1000)\n);\nThe triggering_event column is used to detect duplicate events/messages. It stores\nthe ID of the message/event whose processing generated this event.\n The entities table stores the current version of each entity. It’s used to imple-\nment optimistic locking. Here’s the definition of this table:\ncreate table entities (\nentity_type VARCHAR(1000),\nentity_id VARCHAR(1000),\nentity_version VARCHAR(1000) NOT NULL,\nPRIMARY KEY(entity_type, entity_id)\n);\nWhen an entity is created, a row is inserted into this table. Each time an entity is\nupdated, the entity_version column is updated.\n The snapshots table stores the snapshots of each entity. Here’s the definition of\nthis table:\ncreate table snapshots (\nentity_type VARCHAR(1000),\nentity_id VARCHAR(1000),\nentity_version VARCHAR(1000),\nsnapshot_type VARCHAR(1000) NOT NULL,\nsnapshot_json VARCHAR(1000) NOT NULL,\ntriggering_events VARCHAR(1000),\nPRIMARY KEY(entity_type, entity_id, entity_version)\n)\nThe entity_type and entity_id columns specify the snapshot’s entity. The snapshot\n_json column is the serialized representation of the snapshot, and the snapshot_type\nis its type. The entity_version specifies the version of the entity that this is a snap-\nshot of.\n The three operations supported by this schema are find(), create(), and\nupdate(). The find() operation queries the snapshots table to retrieve the latest\nsnapshot, if any. If a snapshot exists, the find() operation queries the events table to\nfind all events whose event_id is greater than the snapshot’s entity_version. Other-\nwise, find() retrieves all events for the specified entity. The find() operation also\nqueries the entity table to retrieve the entity’s current version.\n The create() operation inserts a row into the entity table and inserts the events\ninto the events table. The update() operation inserts events into the events table. It\n \n",
      "content_length": 2148,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "205\nImplementing an event store\nalso performs an optimistic locking check by updating the entity version in the\nentities table using this UPDATE statement:\nUPDATE entities SET entity_version = ?\nWHERE entity_type = ? and entity_id = ? and entity_version = ?\nThis statement verifies that the version is unchanged since it was retrieved by the find()\noperation. It also updates the entity_version to the new version. The update() opera-\ntion performs these updates within a transaction in order to ensure atomicity.\n Now that we’ve looked at how Eventuate Local stores an aggregate’s events and snap-\nshots, let’s see how a client subscribes to events using Eventuate Local’s event broker. \nCONSUMING EVENTS BY SUBSCRIBING TO EVENTUATE LOCAL’S EVENT BROKER\nServices consume events by subscribing to the event broker, which is implemented\nusing Apache Kafka. The event broker has a topic for each aggregate type. As described\nin chapter 3, a topic is a partitioned message channel. This enables consumers to scale\nhorizontally while preserving message ordering. The aggregate ID is used as the parti-\ntion key, which preserves the ordering of events published by a given aggregate. To\nconsume an aggregate’s events, a service subscribes to the aggregate’s topic.\n Let’s now look at the event relay—the glue between the event database and the\nevent broker. \nTHE EVENTUATE LOCAL EVENT RELAY PROPAGATES EVENTS FROM THE DATABASE TO \nTHE MESSAGE BROKER\nThe event relay propagates events inserted into the event database to the event bro-\nker. It uses transaction log tailing whenever possible and polling for other databases.\nFor example, the MySQL version of the event relay uses the MySQL master/slave rep-\nlication protocol. The event relay connects to the MySQL server as if it were a slave\nand reads the MySQL binlog, a record of updates made to the database. Inserts into\nthe EVENTS table, which correspond to events, are published to the appropriate\nApache Kafka topic. The event relay ignores any other kinds of changes.\n The event relay is deployed as a standalone process. In order to restart correctly,\nit periodically saves the current position in the binlog—filename and offset—in a\nspecial Apache Kafka topic. On startup, it first retrieves the last recorded position\nfrom the topic. The event relay then starts reading the MySQL binlog from that\nposition.\n The event database, message broker, and event relay comprise the event store.\nLet’s now look at the framework a Java application uses to access the event store. \n6.2.2\nThe Eventuate client framework for Java\nThe Eventuate client framework enables developers to write event sourcing-based\napplications that use the Eventuate Local event store. The framework, shown in fig-\nure 6.10, provides the foundation for developing event sourcing-based aggregates, ser-\nvices, and event handlers.\n \n",
      "content_length": 2852,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "206\nCHAPTER 6\nDeveloping business logic with event sourcing\nThe framework provides base classes for aggregates, commands, and events. There’s\nalso an AggregateRepository class that provides CRUD functionality. And the frame-\nwork has an API for subscribing to events.\n Let’s briefly look at each of the types shown in figure 6.10.\nDEFINING AGGREGATES WITH THE REFLECTIVEMUTABLECOMMANDPROCESSINGAGGREGATE CLASS\nReflectiveMutableCommandProcessingAggregate is the base class for aggregates. It’s\na generic class that has two type parameters: the first is the concrete aggregate class,\nand the second is the superclass of the aggregate’s command classes. As its rather\nlong name suggests, it uses reflection to dispatch command and events to the appro-\npriate method. Commands are dispatched to a process() method, and events to an\napply() method.\n The Order class you saw earlier extends ReflectiveMutableCommandProcessing-\nAggregate. The following listing shows the Order class.\npublic class Order extends ReflectiveMutableCommandProcessingAggregate<Order,\nOrderCommand> {\npublic List<Event> process(CreateOrderCommand command) { ... }\npublic void apply(OrderCreatedEvent event) { ... }\nListing 6.3\nThe Eventuate version of the Order class\nOrderService\nEventHandlers\ncreditReserved()\n«interface»\nOrderEvent\n«interface»\nOrderCommand\n«event»\nOrderCreated\n«command»\nCreateOrder\nOrder\nprocess()\napply()\nOrder\nService\ncreateOrder()\n«annotation»\nEvent\nSubscriber\n«interface»\nEvent\n«interface»\nCommand\n«abstract»\nReﬂectiveMutableCommand\nProcessingAggregate\nAggregate\nRepository\nEventuate client framework\nOrder Service\nsave()\nﬁnd()\nupdate()\nAbstract classes and interfaces that\napplication classes extend or implement\nFigure 6.10\nThe main classes and interfaces provided by the Eventuate client framework for Java\n \n",
      "content_length": 1807,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "207\nImplementing an event store\n...\n}\nThe two type parameters passed to ReflectiveMutableCommandProcessingAggregate\nare Order and OrderCommand, which is the base interface for Order’s commands. \nDEFINING AGGREGATE COMMANDS\nAn aggregate’s command classes must extend an aggregate-specific base interface,\nwhich itself must extend the Command interface. For example, the Order aggregate’s\ncommands extend OrderCommand:\npublic interface OrderCommand extends Command {\n}\npublic class CreateOrderCommand implements OrderCommand { ... }\nThe OrderCommand interface extends Command, and the CreateOrderCommand com-\nmand class extends OrderCommand. \nDEFINING DOMAIN EVENTS\nAn aggregate’s event classes must extend the Event interface, which is a marker inter-\nface with no methods. It’s also useful to define a common base interface, which\nextends Event for all of an aggregate’s event classes. For example, here’s the defini-\ntion of the OrderCreated event:\ninterface OrderEvent extends Event {\n}\npublic class OrderCreated extends OrderEvent { ... }\nThe OrderCreated event class extends OrderEvent, which is the base interface for the\nOrder aggregate’s event classes. The OrderEvent interface extends Event. \nCREATING, FINDING, AND UPDATING AGGREGATES WITH THE AGGREGATEREPOSITORY CLASS\nThe framework provides several ways to create, find, and update aggregates. The sim-\nplest approach, which I describe here, is to use an AggregateRepository. Aggregate-\nRepository is a generic class that’s parameterized by the aggregate class and the\naggregate’s base command class. It provides three overloaded methods:\n\nsave()—Creates an aggregate\n\nfind()—Finds an aggregate\n\nupdate()—Updates an aggregate\nThe save () and update() methods are particularly convenient because they encapsu-\nlate the boilerplate code required for creating and updating aggregates. For instance,\nsave() takes a command object as a parameter and performs the following steps:\n1\nInstantiates the aggregate using its default constructor\n2\nInvokes process() to process the command\n \n",
      "content_length": 2043,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "208\nCHAPTER 6\nDeveloping business logic with event sourcing\n3\nApplies the generated events by calling apply()\n4\nSaves the generated events in the event store\nThe update() method is similar. It has two parameters, an aggregate ID and a com-\nmand, and performs the following steps:\n1\nRetrieves the aggregate from the event store\n2\nInvokes process() to process the command\n3\nApplies the generated events by calling apply()\n4\nSaves the generated events in the event store\nThe AggregateRepository class is primarily used by services, which create and update\naggregates in response to external requests. For example, the following listing shows\nhow OrderService uses an AggregateRepository to create an Order.\npublic class OrderService {\nprivate AggregateRepository<Order, OrderCommand> orderRepository;\npublic OrderService(AggregateRepository<Order, OrderCommand> orderRepository)\n{\nthis.orderRepository = orderRepository;\n}\npublic EntityWithIdAndVersion<Order> createOrder(OrderDetails orderDetails) {\nreturn orderRepository.save(new CreateOrder(orderDetails));\n}\n}\nOrderService is injected with an AggregateRepository for Orders. Its create()\nmethod invokes AggregateRepository.save() with a CreateOrder command. \nSUBSCRIBING TO DOMAIN EVENTS\nThe Eventuate Client framework also provides an API for writing event handlers. List-\ning 6.5 shows an event handler for CreditReserved events. The @EventSubscriber\nannotation specifies the ID of the durable subscription. Events that are published when\nthe subscriber isn’t running will be delivered when it starts up. The @EventHandler-\nMethod annotation identifies the creditReserved() method as an event handler.\n@EventSubscriber(id=\"orderServiceEventHandlers\")\npublic class OrderServiceEventHandlers {\n@EventHandlerMethod\npublic void creditReserved(EventHandlerContext<CreditReserved> ctx) {\nCreditReserved event = ctx.getEvent();\n...\n}\nListing 6.4\nOrderService uses an AggregateRepository\nListing 6.5\nAn event handler for OrderCreatedEvent\n \n",
      "content_length": 1987,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "209\nUsing sagas and event sourcing together\nAn event handler has a parameter of type EventHandlerContext, which contains the\nevent and its metadata.\n Now that we’ve looked at how to write event sourcing-based business logic using\nthe Eventuate client framework, let’s look at how to use event sourcing-based business\nlogic with sagas. \n6.3\nUsing sagas and event sourcing together\nImagine you’ve implemented one or more services using event sourcing. You’ve prob-\nably written services similar to the one shown in listing 6.4. But if you’ve read chapter 4,\nyou know that services often need to initiate and participate in sagas, sequences of\nlocal transactions used to maintain data consistency across services. For example,\nOrder Service uses a saga to validate an Order. Kitchen Service, Consumer Service,\nand Accounting Service participate in that saga. Consequently, you must integrate\nsagas and event sourcing-based business logic.\n Event sourcing makes it easy to use choreography-based sagas. The participants\nexchange the domain events emitted by their aggregates. Each participant’s aggre-\ngates handle events by processing commands and emitting new events. You need to\nwrite the aggregates and the event handler classes, which update the aggregates.\n But integrating event sourcing-based business logic with orchestration-based sagas\ncan be more challenging. That’s because the event store’s concept of a transaction\nmight be quite limited. When using some event stores, an application can only create\nor update a single aggregate and publish the resulting event(s). But each step of a\nsaga consists of several actions that must be performed atomically:\nSaga creation—A service that initiates a saga must atomically create or update an\naggregate and create the saga orchestrator. For example, Order Service’s\ncreateOrder() method must create an Order aggregate and a CreateOrderSaga.\nSaga orchestration—A saga orchestrator must atomically consume replies, update\nits state, and send command messages.\nSaga participants—Saga participants, such as Kitchen Service and Order Service,\nmust atomically consume messages, detect and discard duplicates, create or\nupdate aggregates, and send reply messages.\nBecause of this mismatch between these requirements and the transactional capabili-\nties of an event store, integrating orchestration-based sagas and event sourcing poten-\ntially creates some interesting challenges.\n A key factor in determining the ease of integrating event sourcing and orchestration-\nbased sagas is whether the event store uses an RDBMS or a NoSQL database. The\nEventuate Tram saga framework described in chapter 4 and the underlying Tram mes-\nsaging framework described in chapter 3 rely on flexible ACID transactions provided\nby the RDBMS. The saga orchestrator and the saga participants use ACID transactions\nto atomically update their databases and exchange messages. If the application uses\nan RDBMS-based event store, such as Eventuate Local, then it can cheat and invoke the\n \n",
      "content_length": 3015,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "210\nCHAPTER 6\nDeveloping business logic with event sourcing\nEventuate Tram saga framework and update the event store within an ACID transac-\ntion. But if the event store uses a NoSQL database, which can’t participate in the same\ntransaction as the Eventuate Tram saga framework, it will have to take a different\napproach.\n Let’s take a closer look at some of the different scenarios and issues you’ll need to\naddress:\nImplementing choreography-based sagas\nCreating an orchestration-based saga\nImplementing an event sourcing-based saga participant\nImplementing saga orchestrators using event sourcing\nWe’ll begin by looking at how to implement choreography-based sagas using event\nsourcing.\n6.3.1\nImplementing choreography-based sagas using event sourcing\nThe event-driven nature of event sourcing makes it quite straightforward to imple-\nment choreography-based sagas. When an aggregate is updated, it emits an event.\nAn event handler for a different aggregate can consume that event and update its\naggregate. The event sourcing framework automatically makes each event handler\nidempotent.\n For example, chapter 4 discusses how to implement Create Order Saga using cho-\nreography. ConsumerService, KitchenService, and AccountingService subscribe to\nthe OrderService’s events and vice versa. Each service has an event handler similar\nto the one shown in listing 6.5. The event handler updates the corresponding aggre-\ngate, which emits another event.\n Event sourcing and choreography-based sagas work very well together. Event sourc-\ning provides the mechanisms that sagas need, including messaging-based IPC, mes-\nsage de-duplication, and atomic updating of state and message sending. Despite its\nsimplicity, choreography-based sagas have several drawbacks. I talk about some draw-\nbacks in chapter 4, but there’s a drawback that’s specific to event sourcing.\n The problem with using events for saga choreography is that events now have a\ndual purpose. Event sourcing uses events to represent state changes, but using events\nfor saga choreography requires an aggregate to emit an event even if there is no state\nchange. For example, if updating an aggregate would violate a business rule, then the\naggregate must emit an event to report the error. An even worse problem is when a\nsaga participant can’t create an aggregate. There’s no aggregate that can emit an\nerror event.\n Because of these kinds of issues, it’s best to implement more complex sagas using\norchestration. The following sections explain how to integrate orchestration-based\nsagas and event sourcing. As you’ll see, it involves solving some interesting problems.\n Let’s first look at how a service method such as OrderService.createOrder() cre-\nates a saga orchestrator. \n \n",
      "content_length": 2744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "211\nUsing sagas and event sourcing together\n6.3.2\nCreating an orchestration-based saga\nSaga orchestrators are created by some service methods. Other service methods, such\nas OrderService.createOrder(), do two things: create or update an aggregate and\ncreate a saga orchestrator. The service must perform both actions in a way that guar-\nantees that if it does the first action, then the second action will be done eventually.\nHow the service ensures that both of these actions are performed depends on the\nkind of event store it uses.\nCREATING A SAGA ORCHESTRATOR WHEN USING AN RDBMS-BASED EVENT STORE\nIf a service uses an RDBMS-based event store, it can update the event store and create\na saga orchestrator within the same ACID transaction. For example, imagine that the\nOrderService uses Eventuate Local and the Eventuate Tram saga framework. Its\ncreateOrder() method would look like this:\nclass OrderService\n@Autowired\nprivate SagaManager<CreateOrderSagaState> createOrderSagaManager;\n@Transactional\n      \npublic EntityWithIdAndVersion<Order> createOrder(OrderDetails orderDetails) {\nEntityWithIdAndVersion<Order> order =\norderRepository.save(new CreateOrder(orderDetails));  \nCreateOrderSagaState data =\nnew CreateOrderSagaState(order.getId(), orderDetails);\n  \ncreateOrderSagaManager.create(data, Order.class, order.getId());\nreturn order;\n}\n...\nIt’s a combination of the OrderService in listing 6.4 and the OrderService described\nin chapter 4. Because Eventuate Local uses an RDBMS, it can participate in the same\nACID transaction as the Eventuate Tram saga framework. But if a service uses a\nNoSQL-based event store, creating a saga orchestrator isn’t as straightforward. \nCREATING A SAGA ORCHESTRATOR WHEN USING A NOSQL-BASED EVENT STORE\nA service that uses a NoSQL-based event store will most likely be unable to atomically\nupdate the event store and create a saga orchestrator. The saga orchestration frame-\nwork might use an entirely different database. Even if it uses the same NoSQL data-\nbase, the application won’t be able to create or update two different objects atomically\nbecause of the NoSQL database’s limited transaction model. Instead, a service must\nhave an event handler that creates the saga orchestrator in response to a domain\nevent emitted by the aggregate.\n For example, figure 6.11 shows how Order Service creates a CreateOrderSaga\nusing an event handler for the OrderCreated event. Order Service first creates an\nEnsure the createOrder() executes\nwithin a database transaction.\nCreate the Order \naggregate.\nCreate the \nCreateOrderSaga.\n \n",
      "content_length": 2572,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "212\nCHAPTER 6\nDeveloping business logic with event sourcing\nOrder aggregate and persists it in the event store. The event store publishes the Order-\nCreated event, which is consumed by the event handler. The event handler invokes\nthe Eventuate Tram saga framework to create a CreateOrderSaga.\nOne issue to keep in mind when writing an event handler that creates a saga orches-\ntrator is that it must handle duplicate events. At-least-once message delivery means\nthat the event handler that creates the saga might be invoked multiple times. It’s\nimportant to ensure that only one saga instance is created.\n A straightforward approach is to derive the ID of the saga from a unique attribute\nof the event. There are a couple of different options. One is to use the ID of the aggre-\ngate that emits the event as the ID of the saga. This works well for sagas that are cre-\nated in response to aggregate creation events.\n Another option is to use the event ID as the saga ID. Because event IDs are unique,\nthis will guarantee that the saga ID is unique. If an event is a duplicate, the event han-\ndler’s attempt to create the saga will fail because the ID already exists. This option is\nuseful when multiple instances of the same saga can exist for a given aggregate\ninstance.\n A service that uses an RDBMS-based event store can also use the same event-driven\napproach to create sagas. A benefit of this approach is that it promotes loose coupling\nbecause services such as OrderService no longer explicitly instantiate sagas.\n Now that we’ve looked at how to reliably create a saga orchestrator, let’s see how\nevent sourcing-based services can participate in orchestration-based sagas. \nCreate a CreateOrderSaga\nin response to an\nOrderCreated event.\nPersist an\nOrderCreated\nevent.\nOrderCreated\nOrderCreated\nOrder\nOrderCreated\nEventHandler\nCreateOrderSaga\nEvent store\nPersisted as\nOrder Service\nFigure 6.11\nUsing an event handler to reliably create a saga after a service creates an event \nsourcing-based aggregate\n \n",
      "content_length": 2010,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "213\nUsing sagas and event sourcing together\n6.3.3\nImplementing an event sourcing-based saga participant\nImagine that you used event sourcing to implement a service that needs to participate\nin an orchestration-based saga. Not surprisingly, if your service uses an RDBMS-based\nevent store such as Eventuate Local, you can easily ensure that it atomically processes\nsaga command messages and sends replies. It can update the event store as part of the\nACID transaction initiated by the Eventuate Tram framework. But you must use an\nentirely different approach if your service uses an event store that can’t participate in\nthe same transaction as the Eventuate Tram framework.\n You must address a couple of different issues:\nIdempotent command message handling\nAtomically sending a reply message\nLet’s first look at how to implement idempotent command message handlers.\nIDEMPOTENT COMMAND MESSAGE HANDLING\nThe first problem to solve is how an event sourcing-based saga participant can detect\nand discard duplicate messages in order to implement idempotent command message\nhandling. Fortunately, this is an easy problem to address using the idempotent mes-\nsage handling mechanism described earlier. A saga participant records the message\nID in the events that are generated when processing the message. Before updating an\naggregate, the saga participant verifies that it hasn’t processed the message before by\nlooking for the message ID in the events. \nATOMICALLY SENDING REPLY MESSAGES\nThe second problem to solve is how an event sourcing-based saga participant can\natomically send replies. In principle, a saga orchestrator could subscribe to the events\nemitted by an aggregate, but there are two problems with this approach. The first is\nthat a saga command might not actually change the state of an aggregate. In this sce-\nnario, the aggregate won’t emit an event, so no reply will be sent to the saga orchestra-\ntor. The second problem is that this approach requires the saga orchestrator to treat\nsaga participants that use event sourcing differently from those that don’t. That’s\nbecause in order to receive domain events, the saga orchestrator must subscribe to the\naggregate’s event channel in addition to its own reply channel.\n A better approach is for the saga participant to continue to send a reply message to\nthe saga orchestrator’s reply channel. But rather than send the reply message directly,\na saga participant uses a two-step process:\n1\nWhen a saga command handler creates or updates an aggregate, it arranges for\na SagaReplyRequested pseudo event to be saved in the event store along with\nthe real events emitted by the aggregate.\n2\nAn event handler for the SagaReplyRequested pseudo event uses the data con-\ntained in the event to construct the reply message, which it then writes to the\nsaga orchestrator’s reply channel.\nLet’s look at an example to see how this works.\n \n",
      "content_length": 2895,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 244,
      "content": "214\nCHAPTER 6\nDeveloping business logic with event sourcing\nEXAMPLE EVENT SOURCING-BASED SAGA PARTICIPANT\nThis example looks at Accounting Service, one of the participants of Create Order\nSaga. Figure 6.12 shows how Accounting Service handles the Authorize Command\nsent by the saga. Accounting Service is implemented using the Eventuate Saga frame-\nwork. The Eventuate Saga framework is an open source framework for writing sagas\nthat use event sourcing. It’s built on the Eventuate Client framework.\nThis figure shows how Create Order Saga and AccountingService interact. The\nsequence of events is as follows:\nAccountCreated\n....\nAccountAuthorized\nAccountAuthorized\nSagaReplyRequested\nEvent store\nEvent dispatcher\nEventuate API\nAccounting Service\nSagaReplyRequested\nOrder Service\nAggregate\nrepository\nSagaReply\nrequested\nEventHandler\nEventuate saga framework\nSaga command\ndispatcher\nAuthorize\ncommand\nAuthorize\nreply\nAccount\ncommand channel\nCreate order saga\nreply channel\nCreate\norder\nsaga\nAccount\nauthorize()\nAuthorize account\ncommand handler\nAuthorize\nthe account.\nSend command to\naccounting service.\nHandle SagaReply\nrequested event\nand send reply.\nEmit\nSagaReply\nrequested\nevent.\nFigure 6.12\nHow the event sourcing-based Accounting Service participates in Create \nOrder Saga\n \n",
      "content_length": 1283,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "215\nUsing sagas and event sourcing together\n1\nCreate Order Saga sends an AuthorizeAccount command to Accounting-\nService via a messaging channel. The Eventuate Saga framework’s SagaCommand-\nDispatcher invokes AccountingServiceCommandHandler to handle the command\nmessage.\n2\nAccountingServiceCommandHandler sends the command to the specified\nAccount aggregate.\n3\nThe aggregate emits two events, AccountAuthorized and SagaReplyRequested-\nEvent.\n4\nSagaReplyRequestedEventHandler handles SagaReplyRequestedEvent by send-\ning a reply message to CreateOrderSaga.\nThe AccountingServiceCommandHandler shown in the following listing handles the\nAuthorizeAccount command message by calling AggregateRepository.update() to\nupdate the Account aggregate.\npublic class AccountingServiceCommandHandler {\n@Autowired\nprivate AggregateRepository<Account, AccountCommand> accountRepository;\npublic void authorize(CommandMessage<AuthorizeCommand> cm) {\nAuthorizeCommand command = cm.getCommand();\naccountRepository.update(command.getOrderId(),\ncommand,\nreplyingTo(cm)\n.catching(AccountDisabledException.class,\n() -> withFailure(new AccountDisabledReply()))\n.build());\n}\n...\nThe authorize() method invokes an AggregateRepository to update the Account\naggregate. The third argument to update(), which is the UpdateOptions, is computed\nby this expression:\nreplyingTo(cm)\n.catching(AccountDisabledException.class,\n() -> withFailure(new AccountDisabledReply()))\n.build()\nThese UpdateOptions configure the update() method to do the following:\n1\nUse the message id as an idempotency key to ensure that the message is pro-\ncessed exactly once. As mentioned earlier, the Eventuate framework stores the\nidempotency key in all generated events, enabling it to detect and ignore dupli-\ncate attempts to update an aggregate.\nListing 6.6\nHandles command messages sent by sagas\n \n",
      "content_length": 1845,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "216\nCHAPTER 6\nDeveloping business logic with event sourcing\n2\nAdd a SagaReplyRequestedEvent pseudo event to the list of events saved in the\nevent store. When SagaReplyRequestedEventHandler receives the SagaReply-\nRequestedEvent pseudo event, it sends a reply to the CreateOrderSaga’s reply\nchannel.\n3\nSend an AccountDisabledReply instead of the default error reply when the\naggregate throws an AccountDisabledException.\nNow that we’ve looked at how to implement saga participants using event sourcing,\nlet’s find out how to implement saga orchestrators. \n6.3.4\nImplementing saga orchestrators using event sourcing\nSo far in this section, I’ve described how event sourcing-based services can initiate\nand participate in sagas. You can also use event sourcing to implement saga orches-\ntrators. This will enable you to develop applications that are entirely based on an\nevent store.\n There are three key design problems you must solve when implementing a saga\norchestrator:\n1\nHow can you persist a saga orchestrator?\n2\nHow can you atomically change the state of the orchestrator and send com-\nmand messages?\n3\nHow can you ensure that a saga orchestrator processes reply messages exactly\nonce?\nChapter 4 discusses how to implement an RDBMS-based saga orchestrator. Let’s look\nat how to solve these problems when using event sourcing.\nPERSISTING A SAGA ORCHESTRATOR USING EVENT SOURCING\nA saga orchestrator has a very simple lifecycle. First, it’s created. Then it’s updated in\nresponse to replies from saga participants. We can, therefore, persist a saga using the\nfollowing events:\n\nSagaOrchestratorCreated—The saga orchestrator has been created.\n\nSagaOrchestratorUpdated—The saga orchestrator has been updated.\nA saga orchestrator emits a SagaOrchestratorCreated event when it’s created and a\nSagaOrchestratorUpdated event when it has been updated. These events contain the\ndata necessary to re-create the state of the saga orchestrator. For example, the events\nfor CreateOrderSaga, described in chapter 4, would contain a serialized (for example,\nJSON) CreateOrderSagaState. \nSENDING COMMAND MESSAGES RELIABLY\nAnother key design issue is how to atomically update the state of the saga and send a\ncommand. As described in chapter 4, the Eventuate Tram-based saga implementa-\ntion does this by updating the orchestrator and inserting the command message\ninto a message table as part of the same transaction. An application that uses an\n \n",
      "content_length": 2438,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 247,
      "content": "217\nUsing sagas and event sourcing together\nRDBMS-based event store, such as Eventuate Local, can use the same approach. An\napplication that uses a NoSQL-based event store, such as Eventuate SaaS, can use an\nanalogous approach, despite having a very limited transaction model.\n The trick is to persist a SagaCommandEvent, which represents a command to send.\nAn event handler then subscribes to SagaCommandEvents and sends each command\nmessage to the appropriate channel. Figure 6.13 shows how this works.\nThe saga orchestrator uses a two-step process to send commands:\n1\nA saga orchestrator emits a SagaCommandEvent for each command that it wants\nto send. SagaCommandEvent contains all the data needed to send the command,\nsuch as the destination channel and the command object. These events are per-\nsisted in the event store.\n2\nAn event handler processes these SagaCommandEvents and sends command\nmessages to the destination message channel.\nThis two-step approach guarantees that the command will be sent at least once.\n Because the event store provides at-least-once delivery, an event handler might be\ninvoked multiple times with the same event. That will cause the event handler for\nSagaCommandEvents to send duplicate command messages. Fortunately, though, a\nsaga participant can easily detect and discard duplicate commands using the following\n2. Handle SagaCommandEvent\nby sending a command.\n1. Emit a SagaCommandEvent\nfor each command to send.\nSagaCommandEvent\nSagaCreatedEvent\nSagaCommandEvent\nSagaUpdatedEvent\nSagaCommandEvent\n«saga»\nCreateOrderSaga\nSagaCommand\nEventHandler\nEvent store\nPersisted as\nService\nService Command\nChannel\nSends\ncommand\nMessage broker\nFigure 6.13\nHow an event sourcing-based saga orchestrator sends commands to saga participants\n \n",
      "content_length": 1769,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "218\nCHAPTER 6\nDeveloping business logic with event sourcing\nmechanism. The ID of SagaCommandEvent, which is guaranteed to be unique, is used\nas the ID of the command message. As a result, the duplicate messages will have the\nsame ID. A saga participant that receives a duplicate command message will discard it\nusing the mechanism described earlier. \nPROCESSING REPLIES EXACTLY ONCE\nA saga orchestrator also needs to detect and discard duplicate reply messages, which it\ncan do using the mechanism described earlier. The orchestrator stores the reply mes-\nsage’s ID in the events that it emits when processing the reply. It can then easily deter-\nmine whether a message is a duplicate.\n As you can see, event sourcing is a good foundation for implementing sagas. This\nis in addition to the other benefits of event sourcing, including the inherently reli-\nable generation of events whenever data changes, reliable audit logging, and the\nability to do temporal queries. Event sourcing isn’t a silver bullet, though. It involves\na significant learning curve. Evolving the event schema isn’t always straightforward.\nBut despite these drawbacks, event sourcing has a major role to play in a micro-\nservice architecture. In the next chapter, we’ll switch gears and look at how to tackle\na different distributed data management challenge in a microservice architecture:\nqueries. I’ll describe how to implement queries that retrieve data scattered across\nmultiple services. \nSummary\nEvent sourcing persists an aggregate as a sequence of events. Each event rep-\nresents either the creation of the aggregate or a state change. An application\nrecreates the state of an aggregate by replaying events. Event sourcing preserves\nthe history of a domain object, provides an accurate audit log, and reliably pub-\nlishes domain events.\nSnapshots improve performance by reducing the number of events that must\nbe replayed.\nEvents are stored in an event store, a hybrid of a database and a message broker.\nWhen a service saves an event in an event store, it delivers the event to subscribers.\nEventuate Local is an open source event store based on MySQL and Apache\nKafka. Developers use the Eventuate client framework to write aggregates and\nevent handlers.\nOne challenge with using event sourcing is handling the evolution of events. An\napplication potentially must handle multiple event versions when replaying\nevents. A good solution is to use upcasting, which upgrades events to the latest\nversion when they’re loaded from the event store.\nDeleting data in an event sourcing application is tricky. An application must use\ntechniques such as encryption and pseudonymization in order to comply with\nregulations like the European Union’s GDPR that requires an application to\nerase an individual’s data.\n \n",
      "content_length": 2792,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "219\nSummary\nEvent sourcing is a simple way to implement choreography-based sagas. Ser-\nvices have event handlers that listen to the events published by event sourcing-\nbased aggregates.\nEvent sourcing is a good way to implement saga orchestrators. As a result, you\ncan write applications that exclusively use an event store. \n \n",
      "content_length": 330,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "220\nImplementing queries in a\nmicroservice architecture\nMary and her team were just starting to get comfortable with the idea of using sagas\nto maintain data consistency. Then they discovered that transaction management\nwasn’t the only distributed data-related challenge they had to worry about when\nmigrating the FTGO application to microservices. They also had to figure out how\nto implement queries.\n In order to support the UI, the FTGO application implements a variety of\nquery operations. Implementing these queries in the existing monolithic applica-\ntion is relatively straightforward, because it has a single database. For the most\npart, all the FTGO developers needed to do was write SQL SELECT statements\nand define the necessary indexes. As Mary discovered, writing queries in a micro-\nservice architecture is challenging. Queries often need to retrieve data that’s scattered\nThis chapter covers\nThe challenges of querying data in a microservice \narchitecture\nWhen and how to implement queries using the \nAPI composition pattern\nWhen and how to implement queries using the \nCommand query responsibility segregation \n(CQRS) pattern\n \n",
      "content_length": 1148,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "221\nQuerying using the API composition pattern\namong the databases owned by multiple services. You can’t, however, use a traditional\ndistributed query mechanism, because even if it were technically possible, it violates\nencapsulation.\n Consider, for example, the query operations for the FTGO application described\nin chapter 2. Some queries retrieve data that’s owned by just one service. The find-\nConsumerProfile() query, for example, returns data from Consumer Service. But\nother FTGO query operations, such as findOrder() and findOrderHistory(), return\ndata owned by multiple services. Implementing these query operations is not as\nstraightforward.\n There are two different patterns for implementing query operations in a microser-\nvice architecture:\nThe API composition pattern—This is the simplest approach and should be used\nwhenever possible. It works by making clients of the services that own the data\nresponsible for invoking the services and combining the results.\nThe Command query responsibility segregation (CQRS) pattern—This is more power-\nful than the API composition pattern, but it’s also more complex. It maintains\none or more view databases whose sole purpose is to support queries.\nAfter discussing these two patterns, I will talk about how to design CQRS views, fol-\nlowed by the implementation of an example view. Let’s start by taking a look at the\nAPI composition pattern.\n7.1\nQuerying using the API composition pattern\nThe FTGO application implements numerous query operations. Some queries, as\nmentioned earlier, retrieve data from a single service. Implementing these queries is\nusually straightforward—although later in this chapter, when I cover the CQRS pat-\ntern, you’ll see examples of single service queries that are challenging to implement.\n There are also queries that retrieve data from multiple services. In this section, I\ndescribe the findOrder() query operation, which is an example of a query that\nretrieves data from multiple services. I explain the challenges that often crop up when\nimplementing this type of query in a microservice architecture. I then describe the\nAPI composition pattern and show how you can use it to implement queries such as\nfindOrder().\n7.1.1\nThe findOrder() query operation\nThe findOrder() operation retrieves an order by its primary key. It takes an orderId\nas a parameter and returns an OrderDetails object, which contains information\nabout the order. As shown in figure 7.1, this operation is called by a frontend module,\nsuch as a mobile device or a web application, that implements the Order Status view.\n The information displayed by the Order Status view includes basic information\nabout the order, including its status, payment status, status of the order from the\n \n",
      "content_length": 2751,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "222\nCHAPTER 7\nImplementing queries in a microservice architecture\nrestaurant’s perspective, and delivery status, including its location and estimated deliv-\nery time if in transit.\n Because its data resides in a single database, the monolithic FTGO application can\neasily retrieve the order details by executing a single SELECT statement that joins the\nvarious tables. In contrast, in the microservices-based version of the FTGO applica-\ntion, the data is scattered around the following services:\n\nOrder Service—Basic order information, including the details and status\n\nKitchen Service—Status of the order from the restaurant’s perspective and the\nestimated time it will be ready for pickup\n\nDelivery Service—The order’s delivery status, estimated delivery information,\nand its current location\n\nAccounting Service—The order’s payment status\nAny client that needs the order details must ask all of these services. \n7.1.2\nOverview of the API composition pattern\nOne way to implement query operations, such as findOrder(), that retrieve data owned\nby multiple services is to use the API composition pattern. This pattern implements a\nOrder\nOrder Service\nid:3492-2323\nrestaurant:Ajanta\nTicket\nKitchen Service\nFTGO application\nOrderDetails ﬁndOrder(orderId)\nFTGO frontend\nOrder status view\nOrder id:\nRestaurant:\nStatus:\nETA:\nPayment:\n3492-2323\nAjanta\nEn route\n6:25 pm\nPaid\nid:3492-2323\nstatus:PREPARED\nDelivery\nDelivery Service\nid:45-4545\norderId:3492-2323\nstatus:ENROUTE\neta:6:25 pm\nBill\nAccounting Service\nid:343-45611\norderId:3492-2323\nstatus:PAID\nOrder status\nData from multiple services\nMobile device or web application\nFigure 7.1\nThe findOrder() operation is invoked by a FTGO frontend module and returns the \ndetails of an Order.\n \n",
      "content_length": 1740,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "223\nQuerying using the API composition pattern\nquery operation by invoking the services that own the data and combining the results.\nFigure 7.2 shows the structure of this pattern. It has two types of participants:\nAn API composer—This implements the query operation by querying the pro-\nvider services.\nA provider service—This is a service that owns some of the data that the query\nreturns.\nFigure 7.2 shows three provider services. The API composer implements the query by\nretrieving data from the provider services and combining the results. An API com-\nposer might be a client, such as a web application, that needs the data to render a web\npage. Alternatively, it might be a service, such as an API gateway and its Backends for\nfrontends variant described in chapter 8, which exposes the query operation as an API\nendpoint.\nWhether you can use this pattern to implement a particular query operation depends\non several factors, including how the data is partitioned, the capabilities of the APIs\nexposed by the services that own the data, and the capabilities of the databases used\nby the services. For instance, even if the Provider services have APIs for retrieving the\nPattern: API composition\nImplement a query that retrieves data from several services by querying each service\nvia its API and combining the results. See http://microservices.io/patterns/data/api-\ncomposition.html.\nquery()\nAPI composer\nProvider Service A\nDatabase A\nqueryA()\nProvider Service B\nDatabase B\nqueryB()\nProvider Service C\nDatabase C\nqueryC()\nImplements the query operation\nby invoking the providers and\ncombining the results.\nServices that own data\nFigure 7.2\nThe API composition pattern consists of an API composer and two or more provider \nservices. The API composer implements a query by querying the providers and combining the results.\n \n",
      "content_length": 1831,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "224\nCHAPTER 7\nImplementing queries in a microservice architecture\nrequired data, the aggregator might need to perform an inefficient, in-memory join\nof large datasets. Later on, you’ll see examples of query operations that can’t be\nimplemented using this pattern. Fortunately, though, there are many scenarios where\nthis pattern is applicable. To see it in action, we’ll look at an example. \n7.1.3\nImplementing the findOrder() query operation using the API \ncomposition pattern\nThe findOrder() query operation corresponds to a simple primary key-based equi-\njoin query. It’s reasonable to expect that each of the Provider services has an API end-\npoint for retrieving the required data by orderId. Consequently, the findOrder()\nquery operation is an excellent candidate to be implemented by the API composition\npattern. The API composer invokes the four services and combines the results together.\nFigure 7.3 shows the design of the Find Order Composer.\nIn this example, the API composer is a service that exposes the query as a REST endpoint.\nThe Provider services also implement REST APIs. But the concept is the same if the ser-\nvices used some other interprocess communication protocol, such as gRPC, instead of\nHTTP. The Find Order Composer implements a REST endpoint GET /order/{orderId}.\nIt invokes the four services and joins the responses using the orderId. Each Provider ser-\nvice implements a REST endpoint that returns a response corresponding to a single\naggregate. The OrderService retrieves its version of an Order by primary key and the\nother services use the orderId as a foreign key to retrieve their aggregates.\n As you can see, the API composition pattern is quite simple. Let’s look at a couple\nof design issues you must address when applying this pattern. \nFind Order\nComposer\nOrder Service\n«aggregate»\nOrder\nGET/orders/\n{orderId}\nGET/charges?\norderId=\n{orderId}\nGET/tickets?\norderId=\n{orderId}\nGET/deliveries?\norderId=\n{orderId}\nKitchen Service\n«aggregate»\nRestaurantOrder\nDelivery Service\n«aggregate»\nDelivery\nAccounting Service\n«aggregate»\nCharge\nGET/order/{orderId}\nFigure 7.3\nImplementing findOrder() using the API composition pattern\n \n",
      "content_length": 2164,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "225\nQuerying using the API composition pattern\n7.1.4\nAPI composition design issues\nWhen using this pattern, you have to address a couple of design issues:\nDeciding which component in your architecture is the query operation’s API\ncomposer\nHow to write efficient aggregation logic\nLet’s look at each issue.\nWHO PLAYS THE ROLE OF THE API COMPOSER?\nOne decision that you must make is who plays the role of the query operation’s API\ncomposer. You have three options. The first option, shown in figure 7.4, is for a client of\nthe services to be the API composer.\nA frontend client such as a web application, that implements the Order Status view\nand is running on the same LAN, could efficiently retrieve the order details using this\npattern. But as you’ll learn in chapter 8, this option is probably not practical for cli-\nents that are outside of the firewall and access services via a slower network.\n The second option, shown in figure 7.5, is for an API gateway, which implements the\napplication’s external API, to play the role of an API composer for a query operation.\n This option makes sense if the query operation is part of the application’s external\nAPI. Instead of routing a request to another service, the API gateway implements the\nAPI composition logic. This approach enables a client, such as a mobile device, that’s\nrunning outside of the firewall to efficiently retrieve data from numerous services with\na single API call. I discuss the API gateway in chapter 8.\n The third option, shown in figure 7.6, is to implement an API composer as a stand-\nalone service.\nClient, such as web application\nOrder\nService\nDelivery\nService\nKitchen\nService\nAccounting\nService\nAPI composer\nFigure 7.4\nImplementing API \ncomposition in a client. The \nclient queries the provider \nservices to retrieve the data.\n \n",
      "content_length": 1810,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "226\nCHAPTER 7\nImplementing queries in a microservice architecture\nAPI gateway\nExternal client, such as\nmobile application\nOrder\nService\nDelivery\nService\nKitchen\nService\nAccounting\nService\nﬁndOrder()\nAPI composer\nFigure 7.5\nImplementing \nAPI composition in the API \ngateway. The API queries the \nprovider services to retrieve \nthe data, combines the \nresults, and returns a \nresponse to the client.\nOrder\nService\nDelivery\nService\nKitchen\nService\nAccounting\nService\nFind Order Service\nClients\nﬁndOrder()\nAPI composer\nFigure 7.6\nImplement a query \noperation used by multiple \nclients and services as a \nstandalone service.\n \n",
      "content_length": 622,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "227\nQuerying using the API composition pattern\nYou should use this option for a query operation that’s used internally by multiple ser-\nvices. This operation can also be used for externally accessible query operations whose\naggregation logic is too complex to be part of an API gateway. \nAPI COMPOSERS SHOULD USE A REACTIVE PROGRAMMING MODEL\nWhen developing a distributed system, minimizing latency is an ever-present concern.\nWhenever possible, an API composer should call provider services in parallel in order to\nminimize the response time for a query operation. The Find Order Aggregator\nshould, for example, invoke the four services concurrently because there are no\ndependencies between the calls. Sometimes, though, an API composer needs the result\nof one Provider service in order to invoke another service. In this case, it will need to\ninvoke some—but hopefully not all—of the provider services sequentially.\n The logic to efficiently execute a mixture of sequential and parallel service invo-\ncations can be complex. In order for an API composer to be maintainable as well as\nperformant and scalable, it should use a reactive design based on Java Completable-\nFuture’s, RxJava observables, or some other equivalent abstraction. I discuss this topic\nfurther in chapter 8 when I cover the API gateway pattern. \n7.1.5\nThe benefits and drawbacks of the API composition pattern\nThis pattern is a simple and intuitive way to implement query operations in a micro-\nservice architecture. But it has some drawbacks:\nIncreased overhead\nRisk of reduced availability\nLack of transactional data consistency\nLet’s take a look at them.\nINCREASED OVERHEAD\nOne drawback of this pattern is the overhead of invoking multiple services and query-\ning multiple databases. In a monolithic application, a client can retrieve data with a\nsingle request, which will often execute a single database query. In comparison, using\nthe API composition pattern involves multiple requests and database queries. As a\nresult, more computing and network resources are required, increasing the cost of\nrunning the application. \nRISK OF REDUCED AVAILABILITY\nAnother drawback of this pattern is reduced availability. As described in chapter 3, the\navailability of an operation declines with the number of services that are involved.\nBecause the implementation of a query operation involves at least three services—the\nAPI composer and at least two provider services—its availability will be significantly less\nthan that of a single service. For example, if the availability of an individual service is\n99.5%, then the availability of the findOrder() endpoint, which invokes four provider\nservices, is 99.5%(4+1) = 97.5%!\n There are couple of strategies you can use to improve availability. The first strat-\negy is for the API composer to return previously cached data when a Provider service is\n \n",
      "content_length": 2871,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "228\nCHAPTER 7\nImplementing queries in a microservice architecture\nunavailable. An API composer sometimes caches the data returned by a Provider service in\norder to improve performance. It can also use this cache to improve availability. If a\nprovider is unavailable, the API composer can return data from the cache, though it\nmay be potentially stale.\n Another strategy for improving availability is for the API composer to return incom-\nplete data. For example, imagine that Kitchen Service is temporarily unavailable.\nThe API Composer for the findOrder() query operation could omit that service’s data\nfrom the response, because the UI can still display useful information. You’ll see more\ndetails on API design, caching, and reliability in chapter 8. \nLACK OF TRANSACTIONAL DATA CONSISTENCY\nAnother drawback of the API composition pattern is the lack of data consistency. A\nmonolithic application typically executes a query operation using a single database\ntransaction. ACID transactions—subject to the fine print about isolation levels—ensure\nthat an application has a consistent view of the data, even if it executes multiple data-\nbase queries. In contrast, the API composition pattern executes multiple database que-\nries against multiple databases. There’s a risk, therefore, that a query operation will\nreturn inconsistent data.\n For example, an Order retrieved from Order Service might be in the CANCELLED\nstate, whereas the corresponding Ticket retrieved from Kitchen Service might not\nyet have been cancelled. The API composer must resolve this discrepancy, which increases\nthe code complexity. To make matters worse, an API composer might not always be able\nto detect inconsistent data, and will return it to the client.\n Despite these drawbacks, the API composition pattern is extremely useful. You can\nuse it to implement many query operations. But there are some query operations that\ncan’t be efficiently implemented using this pattern. A query operation might, for\nexample, require the API composer to perform an in-memory join of large datasets.\n It’s usually better to implement these types of query operations using the CQRS\npattern. Let’s take a look at how this pattern works. \n7.2\nUsing the CQRS pattern\nMany enterprise applications use an RDBMS as the transactional system of record and\na text search database, such as Elasticsearch or Solr, for text search queries. Some\napplications keep the databases synchronized by writing to both simultaneously. Oth-\ners periodically copy data from the RDBMS to the text search engine. Applications\nwith this architecture leverage the strengths of multiple databases: the transactional\nproperties of the RDBMS and the querying capabilities of the text database.\nPattern: Command query responsibility segregation\nImplement a query that needs data from several services by using events to maintain\na read-only view that replicates data from the services. See http://microservices\n.io/patterns/data/cqrs.html.\n \n",
      "content_length": 2975,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "229\nUsing the CQRS pattern\nCQRS is a generalization of this kind of architecture. It maintains one or more view\ndatabases—not just text search databases—that implement one or more of the appli-\ncation’s queries. To understand why this is useful, we’ll look at some queries that can’t\nbe efficiently implemented using the API composition pattern. I’ll explain how CQRS\nworks and then talk about the benefits and drawbacks of CQRS. Let’s take a look at\nwhen you need to use CQRS.\n7.2.1\nMotivations for using CQRS\nThe API composition pattern is a good way to implement many queries that must\nretrieve data from multiple services. Unfortunately, it’s only a partial solution to the\nproblem of querying in a microservice architecture. That’s because there are multiple\nservice queries the API composition pattern can’t implement efficiently.\n What’s more, there are also single service queries that are challenging to imple-\nment. Perhaps the service’s database doesn’t efficiently support the query. Alterna-\ntively, it sometimes makes sense for a service to implement a query that retrieves data\nowned by a different service. Let’s take a look at these problems, starting with a multi-\nservice query that can’t be efficiently implemented using API composition.\nIMPLEMENTING THE FINDORDERHISTORY() QUERY OPERATION\nThe findOrderHistory() operation retrieves a consumer’s order history. It has sev-\neral parameters:\n\nconsumerId—Identifies the consumer\n\npagination—Page of results to return\n\nfilter—Filter criteria, including the max age of the orders to return, an\noptional order status, and optional keywords that match the restaurant name and\nmenu items\nThis query operation returns an OrderHistory object that contains a summary of the\nmatching orders sorted by increasing age. It’s called by the module that implements\nthe Order History view. This view displays a summary of each order, which includes\nthe order number, order status, order total, and estimated delivery time.\n On the surface, this operation is similar to the findOrder() query operation. The\nonly difference is that it returns multiple orders instead of just one. It may appear that\nthe API composer only has to execute the same query against each Provider service and\ncombine the results. Unfortunately, it’s not that simple.\n That’s because not all services store the attributes that are used for filtering or\nsorting. For example, one of the findOrderHistory() operation’s filter criteria is a\nkeyword that matches against a menu item. Only two of the services, Order Service\nand Kitchen Service, store an Order’s menu items. Neither Delivery Service nor\nAccounting Service stores the menu items, so can’t filter their data using this key-\nword. Similarly, neither Kitchen Service nor Delivery Service can sort by the\norderCreationDate attribute.\n \n",
      "content_length": 2820,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "230\nCHAPTER 7\nImplementing queries in a microservice architecture\n There are two ways an API composer could solve this problem. One solution is for the\nAPI composer to do an in-memory join, as shown in figure 7.7. It retrieves all orders for\nthe consumer from Delivery Service and Accounting Service and performs a join\nwith the orders retrieved from Order Service and Kitchen Service.\nThe drawback of this approach is that it potentially requires the API composer to retrieve\nand join large datasets, which is inefficient.\n The other solution is for the API composer to retrieve matching orders from Order\nService and Kitchen Service and then request orders from the other services by ID.\nBut this is only practical if those services have a bulk fetch API. Requesting orders\nindividually will likely be inefficient because of excessive network traffic.\n Queries such as findOrderHistory() require the API composer to duplicate the\nfunctionality of an RDBMS’s query execution engine. On one hand, this potentially\nmoves work from the less scalable database to the more scalable application. On the\nother hand, it’s less efficient. Also, developers should be writing business functionality,\nnot a query execution engine.\n Next I show you how to apply the CQRS pattern and use a separate datastore,\nwhich is designed to efficiently implement the findOrderHistory() query operation.\nFigure 7.7\nAPI composition can’t efficiently retrieve a consumer’s orders, because some providers, \nsuch as Delivery Service, don’t store the attributes used for filtering.\nFind orders\ncomposer\nOrder Service\n«aggregate»\nOrder\nGET/orders?\nconsumerId=\n&keyword=\nGET/charges?\nconsumerId=\nGET/tickets?\nconsumerId=\n&keyword=\nGET/deliveries?\nconsumerId=\nKitchen Service\n«aggregate»\nRestaurantOrder\nDelivery Service\n«aggregate»\nDelivery\nAccounting Service\n«aggregate»\nCharge\nGET/order?consumerId=&keyword=\nThese services don’t store the data needed for a keyword\nsearch, so will return all of a consumer’s orders.\n \n",
      "content_length": 1989,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "231\nUsing the CQRS pattern\nBut first, let’s look at an example of a query operation that’s challenging to imple-\nment, despite being local to a single service. \nA CHALLENGING SINGLE SERVICE QUERY: FINDAVAILABLERESTAURANTS()\nAs you’ve just seen, implementing queries that retrieve data from multiple services\ncan be challenging. But even queries that are local to a single service can be difficult\nto implement. There are a couple of reasons why this might be the case. One is\nbecause, as discussed shortly, sometimes it’s not appropriate for the service that owns\nthe data to implement the query. The other reason is that sometimes a service’s data-\nbase (or data model) doesn’t efficiently support the query.\n Consider, for example, the findAvailableRestaurants() query operation. This\nquery finds the restaurants that are available to deliver to a given address at a given\ntime. The heart of this query is a geospatial (location-based) search for restaurants\nthat are within a certain distance of the delivery address. It’s a critical part of the order\nprocess and is invoked by the UI module that displays the available restaurants.\n The key challenge when implementing this query operation is performing an effi-\ncient geospatial query. How you implement the findAvailableRestaurants() query\ndepends on the capabilities of the database that stores the restaurants. For example,\nit’s straightforward to implement the findAvailableRestaurants() query using\neither MongoDB or the Postgres and MySQL geospatial extensions. These databases\nsupport geospatial datatypes, indexes, and queries. When using one of these databases,\nRestaurant Service persists a Restaurant as a database record that has a location\nattribute. It finds the available restaurants using a geospatial query that’s optimized by\na geospatial index on the location attribute.\n If the FTGO application stores restaurants in some other kind of database, imple-\nmenting the findAvailableRestaurant() query is more challenging. It must main-\ntain a replica of the restaurant data in a form that’s designed to support the geospatial\nquery. The application could, for example, use the Geospatial Indexing Library for\nDynamoDB (https://github.com/awslabs/dynamodb-geo) that uses a table as a geo-\nspatial index. Alternatively, the application could store a replica of the restaurant data\nin an entirely different type of database, a situation very similar to using a text search\ndatabase for text queries.\n The challenge with using replicas is keeping them up-to-date whenever the origi-\nnal data changes. As you’ll learn below, CQRS solves the problem of synchronizing\nreplicas. \nTHE NEED TO SEPARATE CONCERNS\nAnother reason why single service queries are challenging to implement is that some-\ntimes the service that owns the data shouldn’t be the one that implements the query.\nThe findAvailableRestaurants() query operation retrieves data that is owned by\nRestaurant Service. This service enables restaurant owners to manage their restau-\nrant’s profile and menu items. It stores various attributes of a restaurant, including its\nname, address, cuisines, menu, and opening hours. Given that this service owns the\n \n",
      "content_length": 3181,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "232\nCHAPTER 7\nImplementing queries in a microservice architecture\ndata, it makes sense, at least on the surface, for it to implement this query operation.\nBut data ownership isn’t the only factor to consider.\n You must also take into account the need to separate concerns and avoid overload-\ning services with too many responsibilities. For example, the primary responsibility\nof the team that develops Restaurant Service is enabling restaurant managers to\nmaintain their restaurants. That’s quite different from implementing a high-\nvolume, critical query. What’s more, if they were responsible for the findAvailable-\nRestaurants() query operation, the team would constantly live in fear of deploying a\nchange that prevented consumers from placing orders.\n It makes sense for Restaurant Service to merely provide the restaurant data to\nanother service that implements the findAvailableRestaurants() query operation\nand is most likely owned by the Order Service team. As with the findOrderHistory()\nquery operation, and when needing to maintain geospatial index, there’s a require-\nment to maintain an eventually consistent replica of some data in order to implement\na query. Let’s look at how to accomplish that using CQRS. \n7.2.2\nOverview of CQRS\nThe examples described in section 7.2.1 highlighted three problems that are commonly\nencountered when implementing queries in a microservice architecture:\nUsing the API composition pattern to retrieve data scattered across multiple\nservices results in expensive, inefficient in-memory joins.\nThe service that owns the data stores the data in a form or in a database that\ndoesn’t efficiently support the required query.\nThe need to separate concerns means that the service that owns the data isn’t\nthe service that should implement the query operation.\nThe solution to all three of these problems is to use the CQRS pattern.\nCQRS SEPARATES COMMANDS FROM QUERIES\nCommand Query Responsibility Segregation, as the name suggests, is all about segrega-\ntion, or the separation of concerns. As figure 7.8 shows, it splits a persistent data model\nand the modules that use it into two parts: the command side and the query side. The\ncommand side modules and data model implement create, update, and delete opera-\ntions (abbreviated CUD—for example, HTTP POSTs, PUTs, and DELETEs). The\nquery-side modules and data model implement queries (such as HTTP GETs). The\nquery side keeps its data model synchronized with the command-side data model by\nsubscribing to the events published by the command side.\n Both the non-CQRS and CQRS versions of the service have an API consisting of\nvarious CRUD operations. In a non-CQRS-based service, those operations are typically\nimplemented by a domain model that’s mapped to a database. For performance, a few\nqueries might bypass the domain model and access the database directly. A single per-\nsistent data model supports both commands and queries.\n \n",
      "content_length": 2931,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "233\nUsing the CQRS pattern\nIn a CQRS-based service, the command-side domain model handles CRUD operations\nand is mapped to its own database. It may also handle simple queries, such as non-\njoin, primary key-based queries. The command side publishes domain events when-\never its data changes. These events might be published using a framework such as\nEventuate Tram or using event sourcing.\n A separate query model handles the nontrivial queries. It’s much simpler than the\ncommand side because it’s not responsible for implementing the business rules. The\nquery side uses whatever kind of database makes sense for the queries that it must sup-\nport. The query side has event handlers that subscribe to domain events and update\nthe database or databases. There may even be multiple query models, one for each\ntype of query. \nCQRS AND QUERY-ONLY SERVICES\nNot only can CQRS be applied within a service, but you can also use this pattern to\ndefine query services. A query service has an API consisting of only query opera-\ntions—no command operations. It implements the query operations by querying a\ndatabase that it keeps up-to-date by subscribing to events published by one or more\nother services. A query-side service is a good way to implement a view that’s built by\nService\nCRUD\nCRUD operations\nR\nDomain model\nAggregate\nQuery\nbypass\nAggregate\nDatabase\nOne database for creates, updates, and deletes. A\nseparate database for queries. It is kept up-to-date\nby using events that are published whenever the\ncommand-side database changes.\nSingle database for all CRUD\nService\nCUD\nCRUD operations\nR\nCommand/domain model\nEvents\nCQRS\nNon-CQRS\nAggregate\nEvent\nhandler\nAggregate\nCommand-side\ndatabase\nQuery database\nQuery model\nFigure 7.8\nOn the left is the non-CQRS version of the service, and on the right is the CQRS version. \nCQRS restructures a service into command-side and query-side modules, which have separate \ndatabases.\n \n",
      "content_length": 1926,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": "234\nCHAPTER 7\nImplementing queries in a microservice architecture\nsubscribing to events published by multiple services. This kind of view doesn’t belong\nto any particular service, so it makes sense to implement it as a standalone service. A\ngood example of such a service is Order History Service, which is a query service\nthat implements the findOrderHistory() query operation. As figure 7.9 shows, this\nservice subscribes to events published by several services, including Order Service,\nDelivery Service, and so on.\nOrder History Service has event handlers that subscribe to events published by sev-\neral services and update the Order History View Database. I describe the implemen-\ntation of this service in more detail in section 7.4.\n A query service is also a good way to implement a view that replicates data owned\nby a single service yet because of the need to separate concerns isn’t part of that service.\nFor example, the FTGO developers can define an Available Restaurants Service,\nwhich implements the findAvailableRestaurants() query operation described ear-\nlier. It subscribes to events published by Restaurant Service and updates a database\ndesigned for efficient geospatial queries.\n In many ways, CQRS is an event-based generalization of the popular approach of\nusing RDBMS as the system of record and a text search engine, such as Elasticsearch,\nto handle text queries. What’s different is that CQRS uses a broader range of database\nOrder Service\nKitchen Service\nOrder History\nService\nﬁndOrderHistory()\nﬁndOrder()\nDelivery Service\nAccounting Service\nOrder history\nview database\nEvent\nhandlers\nOrder\nevents\nTicket\nevents\nDelivery\nevents\nAccounting\nevents\nFigure 7.9\nThe design of Order History Service, which is a query-side service. It \nimplements the findOrderHistory() query operation by querying a database, which \nit maintains by subscribing to events published by multiple other services.\n \n",
      "content_length": 1916,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "235\nUsing the CQRS pattern\ntypes—not just a text search engine. Also, CQRS query-side views are updated in near\nreal time by subscribing to events.\n Let’s now look at the benefits and drawbacks of CQRS. \n7.2.3\nThe benefits of CQRS\nCQRS has both benefits and drawbacks. The benefits are as follows:\nEnables the efficient implementation of queries in a microservice architecture\nEnables the efficient implementation of diverse queries\nMakes querying possible in an event sourcing-based application\nImproves separation of concerns\nENABLES THE EFFICIENT IMPLEMENTATION OF QUERIES IN A MICROSERVICE ARCHITECTURE\nOne benefit of the CQRS pattern is that it efficiently implements queries that retrieve\ndata owned by multiple services. As described earlier, using the API composition pat-\ntern to implement queries sometimes results in expensive, inefficient in-memory joins\nof large datasets. For those queries, it’s more efficient to use an easily queried CQRS\nview that pre-joins the data from two or more services.\nENABLES THE EFFICIENT IMPLEMENTATION OF DIVERSE QUERIES\nAnother benefit of CQRS is that it enables an application or service to efficiently\nimplement a diverse set of queries. Attempting to support all queries using a single\npersistent data model is often challenging and in some cases impossible. Some\nNoSQL databases have very limited querying capabilities. Even when a database has\nextensions to support a particular kind of query, using a specialized database is often\nmore efficient. The CQRS pattern avoids the limitations of a single datastore by defin-\ning one or more views, each of which efficiently implements specific queries. \nENABLES QUERYING IN AN EVENT SOURCING-BASED APPLICATION\nCQRS also overcomes a major limitation of event sourcing. An event store only sup-\nports primary key-based queries. The CQRS pattern addresses this limitation by defin-\ning one or more views of the aggregates, which are kept up-to-date, by subscribing to\nthe streams of events that are published by the event sourcing-based aggregates. As a\nresult, an event sourcing-based application invariably uses CQRS. \nIMPROVES SEPARATION OF CONCERNS\nAnother benefit of CQRS is that it separates concerns. A domain model and its corre-\nsponding persistent data model don’t handle both commands and queries. The CQRS\npattern defines separate code modules and database schemas for the command and\nquery sides of a service. By separating concerns, the command side and query side are\nlikely to be simpler and easier to maintain.\n Moreover, CQRS enables the service that implements a query to be different than\nthe service that owns the data. For example, earlier I described how even though\nRestaurant Service owns the data that’s queried by the findAvailableRestaurants\nquery operation, it makes sense for another service to implement such a critical,\n \n",
      "content_length": 2852,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "236\nCHAPTER 7\nImplementing queries in a microservice architecture\nhigh-volume query. A CQRS query service maintains a view by subscribing to the events\npublished by the service or services that own the data. \n7.2.4\nThe drawbacks of CQRS\nEven though CQRS has several benefits, it also has significant drawbacks:\nMore complex architecture\nDealing with the replication lag\nLet’s look at these drawbacks, starting with the increased complexity.\nMORE COMPLEX ARCHITECTURE\nOne drawback of CQRS is that it adds complexity. Developers must write the query-\nside services that update and query the views. There is also the extra operational com-\nplexity of managing and operating the extra datastores. What’s more, an application\nmight use different types of databases, which adds further complexity for both devel-\nopers and operations. \nDEALING WITH THE REPLICATION LAG\nAnother drawback of CQRS is dealing with the “lag” between the command-side and\nthe query-side views. As you might expect, there’s delay between when the command\nside publishes an event and when that event is processed by the query side and the\nview updated. A client application that updates an aggregate and then immediately\nqueries a view may see the previous version of the aggregate. It must often be written\nin a way that avoids exposing these potential inconsistencies to the user.\n One solution is for the command-side and query-side APIs to supply the client with\nversion information that enables it to tell that the query side is out-of-date. A client\ncan poll the query-side view until it’s up-to-date. Shortly I’ll discuss how the service\nAPIs can enable a client to do this.\n A UI application such as a native mobile application or single page JavaScript\napplication can handle replication lag by updating its local model once the command\nis successful without issuing a query. It can, for example, update its model using data\nreturned by the command. Hopefully, when a user action triggers a query, the view\nwill be up-to-date. One drawback of this approach is that the UI code may need to\nduplicate server-side code in order to update its model.\n As you can see, CQRS has both benefits and drawbacks. As mentioned earlier, you\nshould use the API composition whenever possible and use CQRS only when you must.\n Now that you’ve seen the benefits and drawbacks of CQRS, let’s now look at how to\ndesign CQRS views. \n7.3\nDesigning CQRS views\nA CQRS view module has an API consisting of one more query operations. It imple-\nments these query operations by querying a database that it maintains by subscribing\nto events published by one or more services. As figure 7.10 shows, a view module con-\nsists of a view database and three submodules.\n \n",
      "content_length": 2718,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "237\nDesigning CQRS views\nThe data access module implements the database access logic. The event handlers\nand query API modules use the data access module to update and query the database.\nThe event handlers module subscribes to events and updates the database. The query\nAPI module implements the query API.\n You must make some important design decisions when developing a view module:\nYou must choose a database and design the schema.\nWhen designing the data access module, you must address various issues, includ-\ning ensuring that updates are idempotent and handling concurrent updates.\nWhen implementing a new view in an existing application or changing the\nschema of an existing application, you must implement a mechanism to effi-\nciently build or rebuild the view.\nYou must decide how to enable a client of the view to cope with the replication\nlag, described earlier.\nLet’s look at each of these issues.\n7.3.1\nChoosing a view datastore\nA key design decision is the choice of database and the design of the schema. The pri-\nmary purpose of the database and the data model is to efficiently implement the view\nmodule’s query operations. It’s the characteristics of those queries that are the pri-\nmary consideration when selecting a database. But the database must also efficiently\nimplement the update operations performed by the event handlers.\nSQL VS. NOSQL DATABASES\nNot that long ago, there was one type of database to rule them all: the SQL-based\nRDBMS. As the Web grew in popularity, though, various companies discovered that\nan RDBMS couldn’t satisfy their web scale requirements. That led to the creation of\nCQRS view module\nEvent\nhandlers\nquery()\nupdate()\nQuery API\nData access\nView database\nEvents\nﬁnd...()\n...\nImplements data\naccess logic\nFigure 7.10\nThe design of a CQRS \nview module. Event handlers update \nthe view database, which is queried \nby the Query API module.\n \n",
      "content_length": 1895,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "238\nCHAPTER 7\nImplementing queries in a microservice architecture\nthe so-called NoSQL databases. A NoSQL database typically has a limited form of trans-\nactions and less general querying capabilities. For certain use cases, these databases\nhave certain advantages over SQL databases, including a more flexible data model\nand better performance and scalability.\n A NoSQL database is often a good choice for a CQRS view, which can leverage its\nstrengths and ignore its weaknesses. A CQRS view benefits from the richer data model,\nand performance of a NoSQL database. It’s unaffected by the limitations of a NoSQL\ndatabase, because it only uses simple transactions and executes a fixed set of queries.\n Having said that, sometimes it makes sense to implement a CQRS view using a SQL\ndatabase. A modern RDBMS running on modern hardware has excellent perfor-\nmance. Developers, database administrators, and IT operations are, in general, much\nmore familiar with SQL databases than they are with NoSQL databases. As mentioned\nearlier, SQL databases often have extensions for non-relational features, such as geo-\nspatial datatypes and queries. Also, a CQRS view might need to use a SQL database in\norder to support a reporting engine.\n As you can see in table 7.1, there are lots of different options to choose from. And\nto make the choice even more complicated, the differences between the different\ntypes of database are starting to blur. For example, MySQL, which is an RDBMS, has\nexcellent support for JSON, which is one of the strengths of MongoDB, a JSON-style\ndocument-oriented database.\nNow that I’ve discussed the different kinds of databases you can use to implement a\nCQRS view, let’s look at the problem of how to efficiently update a view. \nSUPPORTING UPDATE OPERATIONS\nBesides efficiently implementing queries, the view data model must also efficiently\nimplement the update operations executed by the event handlers. Usually, an event\nTable 7.1\nQuery-side view stores\nIf you need\nUse\nExample\nPK-based lookup of JSON \nobjects\nA document store such as MongoDB \nor DynamoDB, or a key value store \nsuch as Redis\nImplement order history by main-\ntaining a MongoDB document \ncontaining the per-customer.\nQuery-based lookup of JSON \nobjects\nA document store such as MongoDB \nor DynamoDB\nImplement customer view using \nMongoDB or DynamoDB.\nText queries\nA text search engine such as Elastic-\nsearch\nImplement text search for orders \nby maintaining a per-order Elas-\nticsearch document.\nGraph queries\nA graph database such as Neo4j\nImplement fraud detection by \nmaintaining a graph of custom-\ners, orders, and other data.\nTraditional SQL reporting/BI\nAn RDBMS\nStandard business reports and \nanalytics.\n \n",
      "content_length": 2702,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "239\nDesigning CQRS views\nhandler will update or delete a record in the view database using its primary key. For\nexample, soon I’ll describe the design of a CQRS view for the findOrderHistory()\nquery. It stores each Order as a database record using the orderId as the primary key.\nWhen this view receives an event from Order Service, it can straightforwardly update\nthe corresponding record.\n Sometimes, though, it will need to update or delete a record using the equiva-\nlent of a foreign key. Consider, for instance, the event handlers for Delivery*\nevents. If there is a one-to-one correspondence between a Delivery and an Order,\nthen Delivery.id might be the same as Order.id. If it is, then Delivery* event han-\ndlers can easily update the order’s database record.\n But suppose a Delivery has its own primary key or there is a one-to-many relation-\nship between an Order and a Delivery. Some Delivery* events, such as the Delivery-\nCreated event, will contain the orderId. But other events, such as a DeliveryPickedUp\nevent, might not. In this scenario, an event handler for DeliveryPickedUp will need\nto update the order’s record using the deliveryId as the equivalent of a foreign key.\n Some types of database efficiently support foreign-key-based update operations.\nFor example, if you’re using an RDBMS or MongoDB, you create an index on the nec-\nessary columns. However, non-primary key-based updates are not straightforward\nwhen using other NOSQL databases. The application will need to maintain some kind\nof database-specific mapping from a foreign key to a primary key in order to deter-\nmine which record to update. For example, an application that uses DynamoDB,\nwhich only supports primary key-based updates and deletes, must first query a Dyna-\nmoDB secondary index (discussed shortly) to determine the primary keys of the items\nto update or delete. \n7.3.2\nData access module design\nThe event handlers and the query API module don’t access the datastore directly.\nInstead they use the data access module, which consists of a data access object (DAO)\nand its helper classes. The DAO has several responsibilities. It implements the update\noperations invoked by the event handlers and the query operations invoked by the\nquery module. The DAO maps between the data types used by the higher-level code\nand the database API. It also must handle concurrent updates and ensure that updates\nare idempotent.\n Let’s look at these issues, starting with how to handle concurrent updates.\nHANDLING CONCURRENCY\nSometimes a DAO must handle the possibility of multiple concurrent updates to the\nsame database record. If a view subscribes to events published by a single aggregate\ntype, there won’t be any concurrency issues. That’s because events published by a par-\nticular aggregate instance are processed sequentially. As a result, a record correspond-\ning to an aggregate instance won’t be updated concurrently. But if a view subscribes to\nevents published by multiple aggregate types, then it’s possible that multiple events\nhandlers update the same record simultaneously.\n \n",
      "content_length": 3080,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "240\nCHAPTER 7\nImplementing queries in a microservice architecture\n For example, an event handler for an Order* event might be invoked at the same\ntime as an event handler for a Delivery* event for the same order. Both event han-\ndlers then simultaneously invoke the DAO to update the database record for that\nOrder. A DAO must be written in a way that ensures that this situation is handled cor-\nrectly. It must not allow one update to overwrite another. If a DAO implements updates\nby reading a record and then writing the updated record, it must use either pessimistic\nor optimistic locking. In the next section you’ll see an example of a DAO that handles\nconcurrent updates by updating database records without reading them first.\nIDEMPOTENT EVENT HANDLERS\nAs mentioned in chapter 3, an event handler may be invoked with the same event\nmore than once. This is generally not a problem if a query-side event handler is idem-\npotent. An event handler is idempotent if handling duplicate events results in the cor-\nrect outcome. In the worst case, the view datastore will temporarily be out-of-date. For\nexample, an event handler that maintains the Order History view might be invoked\nwith the (admittedly improbable) sequence of events shown in figure 7.11: Delivery-\nPickedUp, DeliveryDelivered, DeliveryPickedUp, and DeliveryDelivered. After\ndelivering the DeliveryPickedUp and DeliveryDelivered events the first time, the\nmessage broker, perhaps because of a network error, starts delivering the events from\nan earlier point in time, and so redelivers DeliveryPickedUp and DeliveryDelivered.\nAfter the event handler processes the second DeliveryPickedUp event, the Order\nHistory view temporarily contains the out-of-date state of the Order until the Delivery-\nDelivered is processed. If this behavior is undesirable, then the event handler should\ndetect and discard duplicate events, like a non-idempotent event handler.\n An event handler isn’t idempotent if duplicate events result in an incorrect out-\ncome. For example, an event handler that increments the balance of a bank account\nisn’t idempotent. A non-idempotent event handler must, as explained in chapter 3,\ndetect and discard duplicate events by recording the IDs of events that it has pro-\ncessed in the view datastore.\nDelivery picked up\nOrder History View\nOrderId: 123\nState: PICKED_UP\nTemporarily out of date\nDelivery delivered\nOrderId: 123\nState: DELIVERED\nDelivery picked up\nOrderId: 123\nState: PICKED_UP\nDelivery delivered\nOrderId: 123\nState: DELIVERED\nTime\nFigure 7.11\nThe DeliveryPickedUp and DeliveryDelivered events are delivered \ntwice, which causes the order state in view to be temporarily out-of-date.\n \n",
      "content_length": 2683,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "241\nDesigning CQRS views\n In order to be reliable, the event handler must record the event ID and update the\ndatastore atomically. How to do this depends on the type of database. If the view data-\nbase store is a SQL database, the event handler could insert processed events into a\nPROCESSED_EVENTS table as part of the transaction that updates the view. But if the\nview datastore is a NoSQL database that has a limited transaction model, the event\nhandler must save the event in the datastore “record” (for example, a MongoDB doc-\nument or DynamoDB table item) that it updates.\n It’s important to note that the event handler doesn’t need to record the ID of\nevery event. If, as is the case with Eventuate, events have a monotonically increasing\nID, then each record only needs to store the max(eventId) that’s received from a\ngiven aggregate instance. Furthermore, if the record corresponds to a single aggre-\ngate instance, then the event handler only needs to record max(eventId). Only\nrecords that represent joins of events from multiple aggregates must contain a map\nfrom [aggregate type, aggregate id] to max(eventId).\n For example, you’ll soon see that the DynamoDB implementation of the Order\nHistory view contains items that have attributes for tracking events that look like this:\n{...\n\"Order3949384394-039434903\" : \"0000015e0c6fc18f-0242ac1100e50002\",\n\"Delivery3949384394-039434903\" : \"0000015e0c6fc264-0242ac1100e50002\",\n}\nThis view is a join of events published by various services. The name of each of these\nevent-tracking attributes is «aggregateType»«aggregateId», and the value is the\neventId. Later on, I describe how this works in more detail. \nENABLING A CLIENT APPLICATION TO USE AN EVENTUALLY CONSISTENT VIEW\nAs I said earlier, one issue with using CQRS is that a client that updates the command\nside and then immediately executes a query might not see its own update. The view is\neventually consistent because of the unavoidable latency of the messaging infrastructure.\n The command and query module APIs can enable the client to detect an inconsis-\ntency using the following approach. A command-side operation returns a token con-\ntaining the ID of the published event to the client. The client then passes the token to\na query operation, which returns an error if the view hasn’t been updated by that\nevent. A view module can implement this mechanism using the duplicate event-\ndetection mechanism. \n7.3.3\nAdding and updating CQRS views\nCQRS views will be added and updated throughout the lifetime of an application.\nSometimes you need to add a new view to support a new query. At other times you\nmight need to re-create a view because the schema has changed or you need to fix a\nbug in code that updates the view.\n Adding and updating views is conceptually quite simple. To create a new view, you\ndevelop the query-side module, set up the datastore, and deploy the service. The query\n \n",
      "content_length": 2912,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "242\nCHAPTER 7\nImplementing queries in a microservice architecture\nside module’s event handlers process all the events, and eventually the view will be\nup-to-date. Similarly, updating an existing view is also conceptually simple: you change\nthe event handlers and rebuild the view from scratch. The problem, however, is that\nthis approach is unlikely to work in practice. Let’s look at the issues.\nBUILD CQRS VIEWS USING ARCHIVED EVENTS\nOne problem is that message brokers can’t store messages indefinitely. Traditional\nmessage brokers such as RabbitMQ delete a message once it’s been processed by a\nconsumer. Even more modern brokers such as Apache Kafka, that retain messages for\na configurable retention period, aren’t intended to store events indefinitely. As a\nresult, a view can’t be built by only reading all the needed events from the message\nbroker. Instead, an application must also read older events that have been archived in,\nfor example, AWS S3. You can do this by using a scalable big data technology such as\nApache Spark. \nBUILD CQRS VIEWS INCREMENTALLY\nAnother problem with view creation is that the time and resources required to process\nall events keep growing over time. Eventually, view creation will become too slow and\nexpensive. The solution is to use a two-step incremental algorithm. The first step peri-\nodically computes a snapshot of each aggregate instance based on its previous snap-\nshot and events that have occurred since that snapshot was created. The second step\ncreates a view using the snapshots and any subsequent events. \n7.4\nImplementing a CQRS view with AWS DynamoDB\nNow that we’ve looked at the various design issues you must address when using\nCQRS, let’s consider an example. This section describes how to implement a CQRS\nview for the findOrderHistory() operation using DynamoDB. AWS DynamoDB is\na scalable, NoSQL database that’s available as a service on the Amazon cloud. The\nDynamoDB data model consists of tables that contain items that, like JSON objects,\nare collections of hierarchical name-value pairs. AWS DynamoDB is a fully man-\naged database, and you can scale the throughput capacity of a table up and down\ndynamically.\n The CQRS view for the findOrderHistory() consumes events from multiple ser-\nvices, so it’s implemented as a standalone Order View Service. The service has an API\nthat implements two operations: findOrderHistory() and findOrder(). Even though\nfindOrder() can be implemented using API composition, this view provides this oper-\nation for free. Figure 7.12 shows the design of the service. Order History Service is\nstructured as a set of modules, each of which implements a particular responsibility\nin order to simplify development and testing. The responsibility of each module is\nas follows:\n\nOrderHistoryEventHandlers—Subscribes to events published by the various\nservices and invokes the OrderHistoryDAO\n\nOrderHistoryQuery APImodule—Implements the REST endpoints described earlier\n \n",
      "content_length": 2966,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "243\nImplementing a CQRS view with AWS DynamoDB\n\nOrderHistoryDataAccess—Contains the OrderHistoryDAO, which defines the\nmethods that update and query the ftgo-order-history DynamoDB table and\nits helper classes\n\nftgo-order-history DynamoDB table—The table that stores the orders\nLet’s look at the design of the event handlers, the DAO, and the DynamoDB table in\nmore detail.\n7.4.1\nThe OrderHistoryEventHandlers module\nThis module consists of the event handlers that consume events and update the\nDynamoDB table. As the following listing shows, the event handlers are simple meth-\nods. Each method is a one-liner that invokes an OrderHistoryDao method with argu-\nments that are derived from the event.\n \n \n \n \n \nOrder History Service\nOrderHistory\nEvent\nHandlers\nQuery\nUpdate\nOrderHistory\nQuery\nOrderHistoryDataAccess\n<DynamoDB table>\nftgo-order-history\nOrder\ndelivery\n...\nevents\nﬁndOrderHistory()\nﬁndOrder\nOrderHistoryDAO\nFigure 7.12\nThe design of OrderHistoryService. OrderHistory-\nEventHandlers updates the database in response to events. The \nOrderHistoryQuery module implements the query operations by query-\ning the database. These two modules use the OrderHistory-\nDataAccess module to access the database.\n \n",
      "content_length": 1215,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "244\nCHAPTER 7\nImplementing queries in a microservice architecture\npublic class OrderHistoryEventHandlers {\nprivate OrderHistoryDao orderHistoryDao;\npublic OrderHistoryEventHandlers(OrderHistoryDao orderHistoryDao) {\nthis.orderHistoryDao = orderHistoryDao;\n}\npublic void handleOrderCreated(DomainEventEnvelope<OrderCreated> dee) {\norderHistoryDao.addOrder(makeOrder(dee.getAggregateId(), dee.getEvent()),\nmakeSourceEvent(dee));\n}\nprivate Order makeOrder(String orderId, OrderCreatedEvent event) {\n...\n}\npublic void handleDeliveryPickedUp(DomainEventEnvelope<DeliveryPickedUp>\ndee) {\norderHistoryDao.notePickedUp(dee.getEvent().getOrderId(),\nmakeSourceEvent(dee));\n}\n...\nEach event handler has a single parameter of type DomainEventEnvelope, which\ncontains the event and some metadata describing the event. For example, the\nhandleOrderCreated() method is invoked to handle an OrderCreated event. It calls\norderHistoryDao.addOrder() to create an Order in the database. Similarly, the\nhandleDeliveryPickedUp() method is invoked to handle a DeliveryPickedUp event.\nIt calls orderHistoryDao.notePickedUp() to update the status of the Order in the\ndatabase.\n Both methods call the helper method makeSourceEvent(), which constructs a\nSourceEvent containing the type and ID of the aggregate that emitted the event and\nthe event ID. In the next section you’ll see that OrderHistoryDao uses SourceEvent to\nensure that update operations are idempotent.\n Let’s now look at the design of the DynamoDB table and after that examine\nOrderHistoryDao. \n7.4.2\nData modeling and query design with DynamoDB\nLike many NoSQL databases, DynamoDB has data access operations that are much\nless powerful than those that are provided by an RDBMS. Consequently, you must\ncarefully design how the data is stored. In particular, the queries often dictate the\ndesign of the schema. We need to address several design issues:\nDesigning the ftgo-order-history table\nDefining an index for the findOrderHistory query\nListing 7.1\nEvent handlers that call the OrderHistoryDao\n \n",
      "content_length": 2040,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "245\nImplementing a CQRS view with AWS DynamoDB\nImplementing the findOrderHistory query\nPaginating the query results\nUpdating orders\nDetecting duplicate events\nWe’ll look at each one in turn.\nDESIGNING THE FTGO-ORDER-HISTORY TABLE\nThe DynamoDB storage model consists of tables, which contain items, and indexes,\nwhich provide alternative ways to access a table’s items (discussed shortly). An item is a\ncollection of named attributes. An attribute value is either a scalar value such as a string,\na multivalued collection of strings, or a collection of named attributes. Although an item\nis the equivalent to a row in an RDBMS, it’s a lot more flexible and can store an entire\naggregate.\n This flexibility enables the OrderHistoryDataAccess module to store each Order\nas a single item in a DynamoDB table called ftgo-order-history. Each field of the\nOrder class is mapped to an item attribute, as shown in figure 7.13. Simple fields such\nas orderCreationTime and status are mapped to single-value item attributes. The\nlineItems field is mapped to an attribute that is a list of maps, one map per time line.\nIt can be considered to be a JSON array of objects.\nAn important part of the definition of a table is its primary key. A DynamoDB applica-\ntion inserts, updates, and retrieves a table’s items by primary key. It would seem to\nmake sense for the primary key to be orderId. This enables Order History Service\nto insert, update, and retrieve an order by orderId. But before finalizing this decision,\nlet’s first explore how a table’s primary key impacts the kinds of data access operations\nit supports. \nDEFINING AN INDEX FOR THE FINDORDERHISTORY QUERY\nThis table definition supports primary key-based reads and writes of Orders. But it\ndoesn’t support a query such as findOrderHistory() that returns multiple matching\norders sorted by increasing age. That’s because, as you will see later in this section,\nthis query uses the DynamoDB query() operation, which requires a table to have a\norderId\n...\n...\nPrimary key\nftgo-order-history table\nconsumerId\nxyz-abc\n...\norderCreationTime\n22939283232\n...\nstatus\nCREATED\n...\nlineItems\n[{...}.\n{...},\n....]\n....\n...\n...\n...\nFigure 7.13\nPreliminary structure of the DynamoDB OrderHistory table\n \n",
      "content_length": 2242,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "246\nCHAPTER 7\nImplementing queries in a microservice architecture\ncomposite primary key consisting of two scalar attributes. The first attribute is a parti-\ntion key. The partition key is so called because DynamoDB’s Z-axis scaling (described in\nchapter 1) uses it to select an item’s storage partition. The second attribute is the sort\nkey. A query() operation returns those items that have the specified partition key,\nhave a sort key in the specified range, and match the optional filter expression. It\nreturns items in the order specified by the sort key.\n The findOrderHistory() query operation returns a consumer’s orders sorted by\nincreasing age. It therefore requires a primary key that has the consumerId as the par-\ntition key and the orderCreationDate as the sort key. But it doesn’t make sense for\n(consumerId, orderCreationDate) to be the primary key of the ftgo-order-history\ntable, because it’s not unique.\n The solution is for findOrderHistory() to query what DynamoDB calls a secondary\nindex on the ftgo-order-history table. This index has (consumerId, orderCreation-\nDate) as its non-unique key. Like an RDBMS index, a DynamoDB index is automati-\ncally updated whenever its table is updated. But unlike a typical RDBMS index, a\nDynamoDB index can have non-key attributes. Non-key attributes improve performance\nbecause they’re returned by the query, so the application doesn’t have to fetch them\nfrom the table. Also, as you’ll soon see, they can be used for filtering. Figure 7.14\nshows the structure of the table and this index.\n The index is part of the definition of the ftgo-order-history table and is called\nftgo-order-history-by-consumer-id-and-creation-time. The index’s attributes\norderId\ncde-fgh\n...\nPrimary key\nftgo-order-history table\nconsumerId\nxyz-abc\n...\norderCreationTime\n22939283232\n...\nstatus\nCREATED\n...\nlineItems\n[{...}.\n{...},\n....]\n....\n...\n...\n...\nPrimary key\nftgo-order-history-by-consumer-id-and-creation-time\nglobal secondary index\nconsumerId\nxyz-abc\n...\norderCreationTime\n22939283232\n...\norderId\ncde-fgh\n...\n...\n...\n...\nstatus\nCREATED\n...\nFigure 7.14\nThe design of the OrderHistory table and index\n \n",
      "content_length": 2145,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "247\nImplementing a CQRS view with AWS DynamoDB\ninclude the primary key attributes, consumerId and orderCreationTime, and non-key\nattributes, including orderId and status.\n The ftgo-order-history-by-consumer-id-and-creation-time index enables\nthe OrderHistoryDaoDynamoDb to efficiently retrieve a consumer’s orders sorted by\nincreasing age.\n Let’s now look at how to retrieve only those orders that match the filter criteria. \nIMPLEMENTING THE FINDORDERHISTORY QUERY\nThe findOrderHistory() query operation has a filter parameter that specifies the\nsearch criteria. One filter criterion is the maximum age of the orders to return. This is\neasy to implement because the DynamoDB Query operation’s key condition expression\nsupports a range restriction on the sort key. The other filter criteria correspond to\nnon-key attributes and can be implemented using a filter expression , which is a Boolean\nexpression. A DynamoDB Query operation returns only those items that satisfy the filter\nexpression. For example, to find Orders that are CANCELLED, the OrderHistoryDao-\nDynamoDb uses a query expression orderStatus = :orderStatus, where :orderStatus\nis a placeholder parameter.\n The keyword filter criteria is more challenging to implement. It selects orders\nwhose restaurant name or menu items match one of the specified keywords. The\nOrderHistoryDaoDynamoDb enables the keyword search by tokenizing the restaurant\nname and menu items and storing the set of keywords in a set-valued attribute called\nkeywords. It finds the orders that match the keywords by using a filter expression\nthat uses the contains() function, for example contains(keywords, :keyword1)\nOR contains(keywords, :keyword2), where :keyword1 and :keyword2 are placehold-\ners for the specified keywords. \nPAGINATING THE QUERY RESULTS\nSome consumers will have a large number of orders. It makes sense, therefore, for the\nfindOrderHistory() query operation to use pagination. The DynamoDB Query oper-\nation has an operation pageSize parameter, which specifies the maximum number of\nitems to return. If there are more items, the result of the query has a non-null Last-\nEvaluatedKey attribute. A DAO can retrieve the next page of items by invoking the\nquery with the exclusiveStartKey parameter set to LastEvaluatedKey.\n As you can see, DynamoDB doesn’t support position-based pagination. Conse-\nquently, Order History Service returns an opaque pagination token to its client. The\nclient uses this pagination token to request the next page of results.\n Now that I’ve described how to query DynamoDB for orders, let’s look at how to\ninsert and update them. \nUPDATING ORDERS\nDynamoDB supports two operations for adding and updating items: PutItem() and\nUpdateItem(). The PutItem() operation creates or replaces an entire item by its\nprimary key. In theory, OrderHistoryDaoDynamoDb could use this operation to insert\n \n",
      "content_length": 2874,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "248\nCHAPTER 7\nImplementing queries in a microservice architecture\nand update orders. One challenge, however, with using PutItem() is ensuring that\nsimultaneous updates to the same item are handled correctly.\n Consider, for example, the scenario where two event handlers simultaneously\nattempt to update the same item. Each event handler calls OrderHistoryDaoDynamoDb\nto load the item from DynamoDB, change it in memory, and update it in DynamoDB\nusing PutItem(). One event handler could potentially overwrite the change made by\nthe other event handler. OrderHistoryDaoDynamoDb can prevent lost updates by using\nDynamoDB’s optimistic locking mechanism. But an even simpler and more efficient\napproach is to use the UpdateItem() operation.\n The UpdateItem() operation updates individual attributes of the item, creating\nthe item if necessary. Since different event handlers update different attributes of the\nOrder item, using UpdateItem makes sense. This operation is also more efficient\nbecause there’s no need to first retrieve the order from the table.\n One challenge with updating the database in response to events is, as mentioned\nearlier, detecting and discarding duplicate events. Let’s look at how to do that when\nusing DynamoDB. \nDETECTING DUPLICATE EVENTS\nAll of Order History Service’s event handlers are idempotent. Each one sets one\nor more attributes of the Order item. Order History Service could, therefore, sim-\nply ignore the issue of duplicate events. The downside of ignoring the issue, though,\nis that Order item will sometimes be temporarily out-of-date. That’s because an\nevent handler that receives a duplicate event will set an Order item’s attributes to\nprevious values. The Order item won’t have the correct values until later events are\nredelivered.\n As described earlier, one way to prevent data from becoming out-of-date is to\ndetect and discard duplicate events. OrderHistoryDaoDynamoDb can detect duplicate\nevents by recording in each item the events that have caused it to be updated. It can\nthen use the UpdateItem() operation’s conditional update mechanism to only update\nan item if an event isn’t a duplicate.\n A conditional update is only performed if a condition expression is true. A condition\nexpression tests whether an attribute exists or has a particular value. The Order-\nHistoryDaoDynamoDb DAO can track events received from each aggregate instance\nusing an attribute called «aggregateType»«aggregateId» whose value is the highest\nreceived event ID. An event is a duplicate if the attribute exists and its value is less\nthan or equal to the event ID. The OrderHistoryDaoDynamoDb DAO uses this condi-\ntion expression:\nattribute_not_exists(«aggregateType»«aggregateId») \nOR «aggregateType»«aggregateId» < :eventId\nThe condition expression only allows the update if the attribute doesn’t exist or the\neventId is greater than the last processed event ID.\n \n",
      "content_length": 2898,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "249\nImplementing a CQRS view with AWS DynamoDB\n For example, suppose an event handler receives a DeliveryPickup event whose ID\nis 123323-343434 from a Delivery aggregate whose ID is 3949384394-039434903.\nThe name of the tracking attribute is Delivery3949384394-039434903. The event\nhandler should consider the event to be a duplicate if the value of this attribute is\ngreater than or equal to 123323-343434. The query() operation invoked by the event\nhandler updates the Order item using this condition expression:\nattribute_not_exists(Delivery3949384394-039434903) \nOR Delivery3949384394-039434903 < :eventId\nNow that I’ve described the DynamoDB data model and query design, let’s take a look\nat OrderHistoryDaoDynamoDb, which defines the methods that update and query the\nftgo-order-history table. \n7.4.3\nThe OrderHistoryDaoDynamoDb class\nThe OrderHistoryDaoDynamoDb class implements methods that read and write items\nin the ftgo-order-history table. Its update methods are invoked by OrderHistory-\nEventHandlers, and its query methods are invoked by OrderHistoryQuery API. Let’s\ntake a look at some example methods, starting with the addOrder() method.\nTHE ADDORDER() METHOD\nThe addOrder() method, which is shown in listing 7.2, adds an order to the ftgo-\norder-history table. It has two parameters: order and sourceEvent. The order\nparameter is the Order to add, which is obtained from the OrderCreated event. The\nsourceEvent parameter contains the eventId and the type and ID of the aggregate\nthat emitted the event. It’s used to implement the conditional update.\npublic class OrderHistoryDaoDynamoDb ...\n@Override\npublic boolean addOrder(Order order, Optional<SourceEvent> eventSource) {\nUpdateItemSpec spec = new UpdateItemSpec()\n.withPrimaryKey(\"orderId\", order.getOrderId())\n  \n.withUpdateExpression(\"SET orderStatus = :orderStatus, \" +  \n\"creationDate = :cd, consumerId = :consumerId, lineItems =\" +\n\" :lineItems, keywords = :keywords, restaurantName = \" +\n\":restaurantName\")\n.withValueMap(new Maps()\n                \n.add(\":orderStatus\", order.getStatus().toString())\n.add(\":cd\", order.getCreationDate().getMillis())\n.add(\":consumerId\", order.getConsumerId())\n.add(\":lineItems\", mapLineItems(order.getLineItems()))\n.add(\":keywords\", mapKeywords(order))\n.add(\":restaurantName\", order.getRestaurantName())\n.map())\n.withReturnValues(ReturnValue.NONE);\nreturn idempotentUpdate(spec, eventSource);\n}\nListing 7.2\nThe addOrder() method adds or updates an Order\nThe primary key of the\nOrder item to update\nThe update\nexpression that\nupdates the\nattributes\nThe values of the\nplaceholders in\nthe update\nexpression\n \n",
      "content_length": 2617,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": "250\nCHAPTER 7\nImplementing queries in a microservice architecture\nThe addOrder() method creates an UpdateSpec, which is part of the AWS SDK and\ndescribes the update operation. After creating the UpdateSpec, it calls idempotent-\nUpdate(), a helper method that performs the update after adding a condition expres-\nsion that guards against duplicate updates. \nTHE NOTEPICKEDUP() METHOD\nThe notePickedUp() method, shown in listing 7.3, is called by the event handler for\nthe DeliveryPickedUp event. It changes the deliveryStatus of the Order item to\nPICKED_UP.\npublic class OrderHistoryDaoDynamoDb ...\n@Override\npublic void notePickedUp(String orderId, Optional<SourceEvent> eventSource) {\nUpdateItemSpec spec = new UpdateItemSpec()\n.withPrimaryKey(\"orderId\", orderId)\n.withUpdateExpression(\"SET #deliveryStatus = :deliveryStatus\")\n.withNameMap(Collections.singletonMap(\"#deliveryStatus\",\nDELIVERY_STATUS_FIELD))\n.withValueMap(Collections.singletonMap(\":deliveryStatus\",\nDeliveryStatus.PICKED_UP.toString()))\n.withReturnValues(ReturnValue.NONE);\nidempotentUpdate(spec, eventSource);\n}\nThis method is similar to addOrder(). It creates an UpdateItemSpec and invokes\nidempotentUpdate(). Let’s look at the idempotentUpdate() method. \nTHE IDEMPOTENTUPDATE() METHOD\nThe following listing shows the idempotentUpdate() method, which updates the item\nafter possibly adding a condition expression to the UpdateItemSpec that guards against\nduplicate updates.\npublic class OrderHistoryDaoDynamoDb ...\nprivate boolean idempotentUpdate(UpdateItemSpec spec, Optional<SourceEvent>\neventSource) {\ntry {\ntable.updateItem(eventSource.map(es -> es.addDuplicateDetection(spec))\n.orElse(spec));\nreturn true;\n} catch (ConditionalCheckFailedException e) {\n// Do nothing\nreturn false;\n}\n}\nListing 7.3\nThe notePickedUp() method changes the order status to PICKED_UP\nListing 7.4\nThe idempotentUpdate() method ignores duplicate events\n \n",
      "content_length": 1905,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "251\nImplementing a CQRS view with AWS DynamoDB\nIf the sourceEvent is supplied, idempotentUpdate() invokes SourceEvent.add-\nDuplicateDetection() to add to UpdateItemSpec the condition expression that was\ndescribed earlier. The idempotentUpdate() method catches and ignores the\nConditionalCheckFailedException, which is thrown by updateItem() if the event\nwas a duplicate.\n Now that we’ve seen the code that updates the table, let’s look at the query method. \nTHE FINDORDERHISTORY() METHOD\nThe findOrderHistory() method, shown in listing 7.5, retrieves the consumer’s orders by\nquerying the ftgo-order-history table using the ftgo-order-history-by-consumer-\nid-and-creation-time secondary index. It has two parameters: consumerId specifies\nthe consumer, and filter specifies the search criteria. This method creates Query-\nSpec—which, like UpdateSpec, is part of the AWS SDK—from its parameters, queries\nthe index, and transforms the returned items into an OrderHistory object.\npublic class OrderHistoryDaoDynamoDb ...\n@Override\npublic OrderHistory findOrderHistory(String consumerId, OrderHistoryFilter\nfilter) {\nQuerySpec spec = new QuerySpec()\n.withScanIndexForward(false)\n   \n.withHashKey(\"consumerId\", consumerId)\n.withRangeKeyCondition(new RangeKeyCondition(\"creationDate\")  \n.gt(filter.getSince().getMillis()));\nfilter.getStartKeyToken().ifPresent(token ->\nspec.withExclusiveStartKey(toStartingPrimaryKey(token)));\nMap<String, Object> valuesMap = new HashMap<>();\nString filterExpression = Expressions.and(\n      \nkeywordFilterExpression(valuesMap, filter.getKeywords()),\nstatusFilterExpression(valuesMap, filter.getStatus()));\nif (!valuesMap.isEmpty())\nspec.withValueMap(valuesMap);\nif (StringUtils.isNotBlank(filterExpression)) {\nspec.withFilterExpression(filterExpression);\n}\nfilter.getPageSize().ifPresent(spec::withMaxResultSize);  \nItemCollection<QueryOutcome> result = index.query(spec);\nreturn new OrderHistory(\nStreamSupport.stream(result.spliterator(), false)\nListing 7.5\nThe findOrderHistory() method retrieves a consumer’s matching orders\nSpecifies that query must \nreturn the orders in order \nof increasing age\nThe maximum\nage of the\norders to\nreturn\nConstruct a filter expression\nand placeholder value map\nfrom the OrderHistoryFilter.\nLimit the number \nof results if the \ncaller has specified \na page size.\n \n",
      "content_length": 2328,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "252\nCHAPTER 7\nImplementing queries in a microservice architecture\n.map(this::toOrder)\n      \n.collect(toList()),\nOptional.ofNullable(result\n.getLastLowLevelResult()\n.getQueryResult().getLastEvaluatedKey())\n.map(this::toStartKeyToken));\n}\nAfter building a QuerySpec, this method then executes a query and builds an Order-\nHistory, which contains the list of Orders, from the returned items.\n The findOrderHistory() method implements pagination by serializing the value\nreturned by getLastEvaluatedKey() into a JSON token. If a client specifies a start\ntoken in OrderHistoryFilter, then findOrderHistory() serializes it and invokes\nwithExclusiveStartKey() to set the start key.\n As you can see, you must address numerous issues when implementing a CQRS\nview, including picking a database, designing the data model that efficiently imple-\nments updates and queries, handling concurrent updates, and dealing with duplicate\nevents. The only complex part of the code is the DAO, because it must properly han-\ndle concurrency and ensure that updates are idempotent. \nSummary\nImplementing queries that retrieve data from multiple services is challenging\nbecause each service’s data is private.\nThere are two ways to implement these kinds of query: the API composition\npattern and the Command query responsibility segregation (CQRS) pattern.\nThe API composition pattern, which gathers data from multiple services, is the\nsimplest way to implement queries and should be used whenever possible.\nA limitation of the API composition pattern is that some complex queries require\ninefficient in-memory joins of large datasets.\nThe CQRS pattern, which implements queries using view databases, is more\npowerful but more complex to implement.\nA CQRS view module must handle concurrent updates as well as detect and dis-\ncard duplicate events.\nCQRS improves separation of concerns by enabling a service to implement a\nquery that returns data owned by a different service.\nClients must handle the eventual consistency of CQRS views. \nCreate an Order from \nan item returned by \nthe query.\n \n",
      "content_length": 2078,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "253\nExternal API patterns\nThe FTGO application, like many other applications, has a REST API. Its clients\ninclude the FTGO mobile applications, JavaScript running in the browser, and\napplications developed by partners. In such a monolithic architecture, the API\nthat’s exposed to clients is the monolith’s API. But when once the FTGO team\nstarts deploying microservices, there’s no longer one API, because each service has\nits own API. Mary and her team must decide what kind of API the FTGO applica-\ntion should now expose to its clients. For example, should clients be aware of the\nexistence of services and make requests to them directly?\nThis chapter covers\nThe challenge of designing APIs that support a \ndiverse set of clients\nApplying API gateway and Backends for frontends \npatterns\nDesigning and implementing an API gateway\nUsing reactive programming to simplify API \ncomposition\nImplementing an API gateway using GraphQL\n \n",
      "content_length": 938,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "254\nCHAPTER 8\nExternal API patterns\n The task of designing an application’s external API is made even more challenging\nby the diversity of its clients. Different clients typically require different data. A desktop\nbrowser-based UI usually displays far more information than a mobile application. Also,\ndifferent clients access the services over different kinds of networks. The clients within\nthe firewall use a high-performance LAN, and the clients outside of the firewall use the\ninternet or mobile network, which will have lower performance. Consequently, as you’ll\nlearn, it often doesn’t make sense to have a single, one-size-fits-all API.\n This chapter begins by describing various external API design issues. I then\ndescribe the external API patterns. I cover the API gateway pattern and then the Back-\nends for frontends pattern. After that, I discuss how to design and implement an API\ngateway. I review the various options that are available, which include off-the-shelf API\ngateway products and frameworks for developing your own. I describe the design and\nimplementation of an API gateway that’s built using the Spring Cloud Gateway frame-\nwork. I also describe how to build an API gateway using GraphQL, a framework that\nprovides graph-based query language.\n8.1\nExternal API design issues\nIn order to explore the various API-related issues, let’s consider the FTGO application.\nAs figure 8.1 shows, this application’s services are consumed by a variety of clients. Four\nkinds of clients consume the services’ APIs:\nWeb applications, such as Consumer web application, which implements the\nbrowser-based UI for consumers, Restaurant web application, which imple-\nments the browser-based UI for restaurants, and Admin web application, which\nimplements the internal administrator UI\nJavaScript applications running in the browser\nMobile applications, one for consumers and the other for couriers\nApplications written by third-party developers\nThe web applications run inside the firewall, so they access the services over a high-\nbandwidth, low-latency LAN. The other clients run outside the firewall, so they access\nthe services over the lower-bandwidth, higher-latency internet or mobile network.\n One approach to API design is for clients to invoke the services directly. On the\nsurface, this sounds quite straightforward—after all, that’s how clients invoke the API\nof a monolithic application. But this approach is rarely used in a microservice archi-\ntecture because of the following drawbacks:\nThe fine-grained service APIs require clients to make multiple requests to\nretrieve the data they need, which is inefficient and can result in a poor user\nexperience.\nThe lack of encapsulation caused by clients knowing about each service and its\nAPI makes it difficult to change the architecture and the APIs.\nServices might use IPC mechanisms that aren’t convenient or practical for cli-\nents to use, especially those clients outside the firewall.\n \n",
      "content_length": 2968,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "255\nExternal API design issues\nTo learn more about these drawbacks, let’s take a look at how the FTGO mobile appli-\ncation for consumers retrieves data from the services.\n8.1.1\nAPI design issues for the FTGO mobile client\nConsumers use the FTGO mobile client to place and manage their orders. Imagine\nyou’re developing the mobile client’s View Order view, which displays an order. As\ndescribed in chapter 7, the information displayed by this view includes basic order\ninformation, including its status, payment status, status of the order from the restau-\nrant’s perspective, and delivery status, including its location and estimated delivery\ntime if in transit.\n The monolithic version of the FTGO application has an API endpoint that returns\nthe order details. The mobile client retrieves the information it needs by making a sin-\ngle request. In contrast, in the microservices version of the FTGO application, the\norder details are, as described previously, scattered across several services, including\nthe following:\nLower-performance\ninternet\nHigher-performance\nLAN\nBackend services\nOrder Service\nFirewall\nAPI\nrequests\nAPI\nrequests\nAPI\nrequests\nWeb page\nrequests\nWeb\napplication\nConsumer\nService\nDelivery\nService\nKitchen\nService\nBrowser\niPhone/\nAndroid\napplication\n3rd-party\napplication\nHTML\nJavaScript\napplication\nFigure 8.1\nThe FTGO application’s services and their clients. There are several \ndifferent types of clients. Some are inside the firewall, and others are outside. \nThose outside the firewall access the services over the lower-performance \ninternet/mobile network. Those clients inside the firewall use a higher-\nperformance LAN.\n \n",
      "content_length": 1651,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "256\nCHAPTER 8\nExternal API patterns\n\nOrder Service—Basic order information, including the details and status\n\nKitchen Service—The status of the order from the restaurant’s perspective\nand the estimated time it will be ready for pickup\n\nDelivery Service—The order’s delivery status, its estimated delivery time, and\nits current location\n\nAccounting Service—The order’s payment status\nIf the mobile client invokes the services directly, then it must, as figure 8.2 shows, make\nmultiple calls to retrieve this data.\nFTGO backend services\nOrder Service\nFirewall\nMonolithic FTGO\napplication\nFirewall\nInternet\nInternet\ngetOrder()\ngetDelivery()\ngetOrderDetails()\ngetBill()\ngetTicket()\nDelivery\nService\nAccounting\nService\nKitchen\nService\niPhone/\nAndroid\nconsumer\napplication\niPhone/\nAndroid\nconsumer\napplication\nOne API required\nMany API calls required\nFigure 8.2\nA client can retrieve the order details from the monolithic FTGO application with a \nsingle request. But the client must make multiple requests to retrieve the same information in a \nmicroservice architecture.\n \n",
      "content_length": 1072,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "257\nExternal API design issues\nIn this design, the mobile application is playing the role of API composer. It invokes\nmultiple services and combines the results. Although this approach seems reasonable,\nit has several serious problems.\nPOOR USER EXPERIENCE DUE TO THE CLIENT MAKING MULTIPLE REQUESTS\nThe first problem is that the mobile application must sometimes make multiple\nrequests to retrieve the data it wants to display to the user. The chatty interaction\nbetween the application and the services can make the application seem unrespon-\nsive, especially when it uses the internet or a mobile network. The internet has much\nlower bandwidth and higher latency than a LAN, and mobile networks are even worse.\nThe latency of a mobile network (and internet) is typically 100x greater than a LAN.\n The higher latency might not be a problem when retrieving the order details,\nbecause the mobile application minimizes the delay by executing the requests concur-\nrently. The overall response time is no greater than that of a single request. But in\nother scenarios, a client may need to execute requests sequentially, which will result in\na poor user experience.\n What’s more, poor user experience due to network latency is not the only issue\nwith a chatty API. It requires the mobile developer to write potentially complex API\ncomposition code. This work is a distraction from their primary task of creating a\ngreat user experience. Also, because each network request consumes power, a chatty\nAPI drains the mobile device’s battery faster.\nLACK OF ENCAPSULATION REQUIRES FRONTEND DEVELOPERS TO CHANGE THEIR CODE IN LOCKSTEP \nWITH THE BACKEND\nAnother drawback of a mobile application directly accessing the services is the lack of\nencapsulation. As an application evolves, the developers of a service sometimes\nchange an API in a way that breaks existing clients. They might even change how the\nsystem is decomposed into services. Developers may add new services and split or\nmerge existing services. But if knowledge about the services is baked into a mobile\napplication, it can be difficult to change the services’ APIs.\n Unlike when updating a server-side application, it takes hours or perhaps even\ndays to roll out a new version of a mobile application. Apple or Google must approve\nthe upgrade and make it available for download. Users might not download the\nupgrade immediately—if ever. And you may not want to force reluctant users to\nupgrade. The strategy of exposing service APIs to mobile creates a significant obstacle\nto evolving those APIs.\nSERVICES MIGHT USE CLIENT-UNFRIENDLY IPC MECHANISMS\nAnother challenge with a mobile application directly calling services is that some ser-\nvices could use protocols that aren’t easily consumed by a client. Client applications\nthat run outside the firewall typically use protocols such as HTTP and WebSockets.\nBut as described in chapter 3, service developers have many protocols to choose\nfrom—not just HTTP. Some of an application’s services might use gRPC, whereas\nothers could use the AMQP messaging protocol. These kinds of protocols work well\n \n",
      "content_length": 3105,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": "258\nCHAPTER 8\nExternal API patterns\ninternally, but might not be easily consumed by a mobile client. Some aren’t even fire-\nwall friendly. \n8.1.2\nAPI design issues for other kinds of clients\nI picked the mobile client because it’s a great way to demonstrate the drawbacks of cli-\nents accessing services directly. But the problems created by exposing services to cli-\nents aren’t specific to just mobile clients. Other kinds of clients, especially those\noutside the firewall, also encounter these problems. As described earlier, the FTGO\napplication’s services are consumed by web applications, browser-based JavaScript\napplications, and third-party applications. Let’s take a look at the API design issues\nwith these clients.\nAPI DESIGN ISSUES FOR WEB APPLICATIONS\nTraditional server-side web applications, which handle HTTP requests from browsers\nand return HTML pages, run within the firewall and access the services over a LAN.\nNetwork bandwidth and latency aren’t obstacles to implementing API composition in\na web application. Also, web applications can use non-web-friendly protocols to access\nthe services. The teams that develop web applications are part of the same organiza-\ntion and often work in close collaboration with the teams writing the backend ser-\nvices, so a web application can easily be updated whenever the backend services are\nchanged. Consequently, it’s feasible for a web application to access the backend ser-\nvices directly. \nAPI DESIGN ISSUES FOR BROWSER-BASED JAVASCRIPT APPLICATIONS\nModern browser applications use some amount of JavaScript. Even if the HTML is pri-\nmarily generated by a server-side web application, it’s common for JavaScript running\nin the browser to invoke services. For example, all of the FTGO application web appli-\ncations—Consumer, Restaurant, and Admin—contain JavaScript that invokes the back-\nend services. The Consumer web application, for instance, dynamically refreshes the\nOrder Details page using JavaScript that invokes the service APIs.\n On one hand, browser-based JavaScript applications are easy to update when ser-\nvice APIs change. On the other hand, JavaScript applications that access the services\nover the internet have the same problems with network latency as mobile applications.\nTo make matters worse, browser-based UIs, especially those for the desktop, are usu-\nally more sophisticated and need to compose more services than mobile applications.\nIt’s likely that the Consumer and Restaurant applications, which access services over\nthe internet, won’t be able to compose service APIs efficiently. \nDESIGNING APIS FOR THIRD-PARTY APPLICATIONS\nFTGO, like many other organizations, exposes an API to third-party developers. The\ndevelopers can use the FTGO API to write applications that place and manage\norders. These third-party applications access the APIs over the internet, so API com-\nposition is likely to be inefficient. But the inefficiency of API composition is a rela-\ntively minor problem compared to the much larger challenge of designing an API\n \n",
      "content_length": 3039,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 289,
      "content": "259\nThe API gateway pattern\nthat’s used by third-party applications. That’s because third-party developers need\nan API that’s stable.\n Very few organizations can force third-party developers to upgrade to a new API.\nOrganizations that have an unstable API risk losing developers to a competitor.\nConsequently, you must carefully manage the evolution of an API that’s used by third-\nparty developers. You typically have to maintain older versions for a long time—pos-\nsibly forever.\n This requirement is a huge burden for an organization. It’s impractical to make\nthe developers of the backend services responsible for maintaining long-term back-\nward compatibility. Rather than expose services directly to third-party developers,\norganizations should have a separate public API that’s developed by a separate team.\nAs you’ll learn later, the public API is implemented by an architectural component\nknown as an API gateway. Let’s look at how an API gateway works. \n8.2\nThe API gateway pattern\nAs you’ve just seen, there are numerous drawbacks with services accessing services\ndirectly. It’s often not practical for a client to perform API composition over the inter-\nnet. The lack of encapsulation makes it difficult for developers to change service\ndecomposition and APIs. Services sometimes use communication protocols that\naren’t suitable outside the firewall. Consequently, a much better approach is to use an\nAPI gateway.\nAn API gateway is a service that’s the entry point into the application from the outside\nworld. It’s responsible for request routing, API composition, and other functions,\nsuch as authentication. This section covers the API gateway pattern. I discuss its bene-\nfits and drawbacks and describe various design issues you must address when develop-\ning an API gateway.\n8.2.1\nOverview of the API gateway pattern\nSection 8.1.1 described the drawbacks of clients, such as the FTGO mobile applica-\ntion, making multiple requests in order to display information to the user. A much\nbetter approach is for a client to make a single request to an API gateway, a service\nthat serves as the single entry point for API requests into an application from outside\nthe firewall. It’s similar to the Facade pattern from object-oriented design. Like a facade,\nan API gateway encapsulates the application’s internal architecture and provides an API\nto its clients. It may also have other responsibilities, such as authentication, monitoring,\nPattern: API gateway\nImplement a service that’s the entry point into the microservices-based application\nfrom external API clients. See http://microservices.io/patterns/apigateway.html.\n \n",
      "content_length": 2636,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": "260\nCHAPTER 8\nExternal API patterns\nand rate limiting. Figure 8.3 shows the relationship between the clients, the API gate-\nway, and the services.\nThe API gateway is responsible for request routing, API composition, and protocol\ntranslation. All API requests from external clients first go to the API gateway, which\nroutes some requests to the appropriate service. The API gateway handles other\nrequests using the API composition pattern and by invoking multiple services and\naggregating the results. It may also translate between client-friendly protocols such as\nHTTP and WebSockets and client-unfriendly protocols used by the services.\nREQUEST ROUTING\nOne of the key functions of an API gateway is request routing. An API gateway imple-\nments some API operations by routing requests to the corresponding service. When it\nreceives a request, the API gateway consults a routing map that specifies which service\nto route the request to. A routing map might, for example, map an HTTP method\nand path to the HTTP URL of a service. This function is identical to the reverse proxy-\ning features provided by web servers such as NGINX. \nLower-performance\ninternet\nHigher-performance\nLAN\nBackend services\nOrder Service\nFirewall\nAPI\nrequests\nAPI\nrequests\nAPI\nrequests\nWeb page\nrequests\nWeb\napplication\nConsumer\nService\nDelivery\nService\nBrowser\niPhone/\nAndroid\napplication\n3rd-party\napplication\nHTML\nJavaScript\napplication\nAPI\ngateway\nFigure 8.3\nThe API gateway is the single entry point into the application for API calls from outside \nthe firewall.\n \n",
      "content_length": 1544,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "261\nThe API gateway pattern\nAPI COMPOSITION\nAn API gateway typically does more than simply reverse proxying. It might also imple-\nment some API operations using API composition. The FTGO API gateway, for exam-\nple, implements the Get Order Details API operation using API composition. As\nfigure 8.4 shows, the mobile application makes one request to the API gateway, which\nfetches the order details from multiple services.\n The FTGO API gateway provides a coarse-grained API that enables mobile clients\nto retrieve the data they need with a single request. For example, the mobile client\nmakes a single getOrderDetails() request to the API gateway. \nFTGO backend services\nOrder Service\nFirewall\nInternet\ngetOrder()\nLAN\ngetDelivery()\ngetOrderDetails()\ngetBill()\ngetTicket()\nDelivery\nService\nAccounting\nService\nKitchen\nService\niPhone/\nAndroid\nconsumer\napplication\nAPI\ngateway\nFTGO backend services\nOrder Service\nFirewall\nInternet\ngetOrder()\ngetDelivery()\ngetBill()\ngetTicket()\nDelivery\nService\nAccounting\nService\nKitchen\nService\niPhone/\nAndroid\nconsumer\napplication\nMany API calls required\nOne API call required\nLower-performance\nnetwork\nHigher-performance\nnetwork\nFigure 8.4\nAn API gateway often does API composition, which enables a client such as a mobile \ndevice to efficiently retrieve data using a single API request.\n \n",
      "content_length": 1324,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": "262\nCHAPTER 8\nExternal API patterns\nPROTOCOL TRANSLATION\nAn API gateway might also perform protocol translation. It might provide a RESTful\nAPI to external clients, even though the application services use a mixture of protocols\ninternally, including REST and gRPC. When needed, the implementation of some\nAPI operations translates between the RESTful external API and the internal gRPC-\nbased APIs. \nTHE API GATEWAY PROVIDES EACH CLIENT WITH CLIENT-SPECIFIC API\nAn API gateway could provide a single one-size-fits-all (OSFA) API. The problem with\na single API is that different clients often have different requirements. For instance, a\nthird-party application might require the Get Order Details API operation to return\nthe complete Order details, whereas a mobile client only needs a subset of the data.\nOne way to solve this problem is to give clients the option of specifying in a request\nwhich fields and related objects the server should return. This approach is adequate\nfor a public API that must serve a broad range of third-party applications, but it often\ndoesn’t give clients the control they need.\n A better approach is for the API gateway to provide each client with its own API.\nFor example, the FTGO API gateway can provide the FTGO mobile client with an API\nthat’s specifically designed to meet its requirements. It may even have different APIs\nfor the Android and iPhone mobile applications. The API gateway will also implement\na public API for third-party developers to use. Later on, I’ll describe the Backends for\nfrontends pattern that takes this concept of an API-per-client even further by defining\na separate API gateway for each client. \nIMPLEMENTING EDGE FUNCTIONS\nAlthough an API gateway’s primary responsibilities are API routing and composition,\nit may also implement what are known as edge functions. An edge function is, as the\nname suggests, a request-processing function implemented at the edge of an applica-\ntion. Examples of edge functions that an application might implement include the\nfollowing:\nAuthentication—Verifying the identity of the client making the request.\nAuthorization—Verifying that the client is authorized to perform that particular\noperation.\nRate limiting —Limiting how many requests per second from either a specific cli-\nent and/or from all clients.\nCaching—Cache responses to reduce the number of requests made to the services.\nMetrics collection—Collect metrics on API usage for billing analytics purposes.\nRequest logging—Log requests.\nThere are three different places in your application where you could implement these\nedge functions. First, you can implement them in the backend services. This might\nmake sense for some functions, such as caching, metrics collection, and possibly autho-\nrization. But it’s generally more secure if the application authenticates requests on the\nedge before they reach the services.\n \n",
      "content_length": 2890,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "263\nThe API gateway pattern\n The second option is to implement these edge functions in an edge service that’s\nupstream from the API gateway. The edge service is the first point of contact for an\nexternal client. It authenticates the request and performs other edge processing\nbefore passing it to the API gateway.\n An important benefit of using a dedicated edge service is that it separates con-\ncerns. The API gateway focuses on API routing and composition. Another benefit is\nthat it centralizes responsibility for critical edge functions such as authentication.\nThat’s particularly valuable when an application has multiple API gateways that are\npossibly written using a variety of languages and frameworks. I’ll talk more about that\nlater. The drawback of this approach is that it increases network latency because of the\nextra hop. It also adds to the complexity of the application.\n As a result, it’s often convenient to use the third option and implement these edge\nfunctions, especially authorization, in the API gateway itself. There’s one less network\nhop, which improves latency. There are also fewer moving parts, which reduces com-\nplexity. Chapter 11 describes how the API gateway and the services collaborate to\nimplement security. \nAPI GATEWAY ARCHITECTURE\nAn API gateway has a layered, modular architecture. Its architecture, shown in figure 8.5,\nconsists of two layers: the API layer and a common layer. The API layer consists of\none or more independent API modules. Each API module implements an API for a\nAPI gateway\nMobile client\nMobile API\nAPI layer\nBrowser JavaScript\napplication\nBrowser API\nCommon layer\n3rd-party application\nPublic API\nFigure 8.5\nAn API gateway has a layered modular architecture. The API for each client is \nimplemented by a separate module. The common layer implements functionality common to all \nAPIs, such as authentication.\n \n",
      "content_length": 1874,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": "264\nCHAPTER 8\nExternal API patterns\nparticular client. The common layer implements shared functionality, including edge\nfunctions such as authentication.\n In this example, the API gateway has three API modules:\nMobile API—Implements the API for the FTGO mobile client\nBrowser API—Implements the API for the JavaScript application running in the\nbrowser\nPublic API—Implements the API for third-party developers\nAn API module implements each API operation in one of two ways. Some API opera-\ntions map directly to single service API operation. An API module implements these\noperations by routing requests to the corresponding service API operation. It might\nroute requests using a generic routing module that reads a configuration file describ-\ning the routing rules.\n An API module implements other, more complex API operations using API com-\nposition. The implementation of this API operation consists of custom code. Each API\noperation implementation handles requests by invoking multiple services and com-\nbining the results. \nAPI GATEWAY OWNERSHIP MODEL\nAn important question that you must answer is who is responsible for the develop-\nment of the API gateway and its operation? There are a few different options. One is\nfor a separate team to be responsible for the API gateway. The drawback to that is that\nit’s similar to SOA, where an Enterprise Service Bus (ESB) team was responsible for all\nESB development. If a developer working on the mobile application needs access to a\nparticular service, they must submit a request to the API gateway team and wait for\nthem to expose the API. This kind of centralized bottleneck in the organization is very\nmuch counter to the philosophy of the microservice architecture, which promotes\nloosely coupled autonomous teams.\n A better approach, which has been promoted by Netflix, is for the client teams—\nthe mobile, web, and public API teams—to own the API module that exposes their\nAPI. An API gateway team is responsible for developing the Common module and for\nthe operational aspects of the gateway. This ownership model, shown in figure 8.6,\ngives the teams control over their APIs.\n When a team needs to change their API, they check in the changes to the source\nrepository for the API gateway. To work well, the API gateway’s deployment pipeline\nmust be fully automated. Otherwise, the client teams will often be blocked waiting for\nthe API gateway team to deploy the new version. \nUSING THE BACKENDS FOR FRONTENDS PATTERN\nOne concern with an API gateway is that responsibility for it is blurred. Multiple teams\ncontribute to the same code base. An API gateway team is responsible for its opera-\ntion. Though not as bad as a SOA ESB, this blurring of responsibilities is counter to\nthe microservice architecture philosophy of “if you build it, you own it.”\n \n",
      "content_length": 2816,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 295,
      "content": "265\nThe API gateway pattern\nThe solution is to have an API gateway for each client, the so-called Backends for front-\nends (BFF) pattern, which was pioneered by Phil Calçado (http://philcalcado.com/)\nand his colleagues at SoundCloud. As figure 8.7 shows, each API module becomes its\nown standalone API gateway that’s developed and operated by a single client team.\nPattern: Backends for frontends\nImplement a separate API gateway for each type of client. See http://microservices\n.io/patterns/apigateway.html.\nAPI gateway\nMobile client\nMobile API\nAPI layer\nBrowser JavaScript\napplication\nBrowser API\nCommon layer\n3rd-party application\nPublic API\nMobile client team\nAPI gateway team\nBrowser client team\nOwns\nOwns\nOwns\nOwns\nPublic API team\nFigure 8.6\nA client team owns their API module. As they change the client, they can change the API \nmodule and not ask the API gateway team to make the changes.\n \n",
      "content_length": 901,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "266\nCHAPTER 8\nExternal API patterns\nThe public API team owns and operates their API gateway, the mobile team owns and\noperates theirs, and so on. In theory, different API gateways could be developed using\ndifferent technology stacks. But that risks duplicating code for common functionality,\nsuch as the code that implements edge functions. Ideally, all API gateways use the\nsame technology stack. The common functionality is a shared library implemented by\nthe API gateway team.\n Besides clearly defining responsibilities, the BFF pattern has other benefits. The\nAPI modules are isolated from one another, which improves reliability. One misbehav-\ning API can’t easily impact other APIs. It also improves observability, because different\nAPI modules are different processes. Another benefit of the BFF pattern is that each\nAPI is independently scalable. The BFF pattern also reduces startup time because\neach API gateway is a smaller, simpler application. \n \n \nMobile API\ngateway\nMobile client\nMobile API\nAPI layer\nCommon layer\nBrowser API\ngateway\nBrowser API\nAPI layer\nCommon layer\nPublic API\ngateway\nPublic API\nAPI layer\nCommon layer\nMobile client team\nBrowser client team\nOwns\nOwns\nOwns\nPublic API team\nBrowser JavaScript\napplication\n3rd-party application\nFigure 8.7\nThe Backends for frontends pattern defines a separate API gateway for each client. Each \nclient team owns their API gateway. An API gateway team owns the common layer.\n \n",
      "content_length": 1441,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "267\nThe API gateway pattern\n8.2.2\nBenefits and drawbacks of an API gateway\nAs you might expect, the API gateway pattern has both benefits and drawbacks.\nBENEFITS OF AN API GATEWAY\nA major benefit of using an API gateway is that it encapsulates internal structure of the\napplication. Rather than having to invoke specific services, clients talk to the gateway.\nThe API gateway provides each client with a client-specific API, which reduces the\nnumber of round-trips between the client and application. It also simplifies the client\ncode. \nDRAWBACKS OF AN API GATEWAY\nThe API gateway pattern also has some drawbacks. It is yet another highly available\ncomponent that must be developed, deployed, and managed. There’s also a risk that\nthe API gateway becomes a development bottleneck. Developers must update the API\ngateway in order to expose their services’s API. It’s important that the process for\nupdating the API gateway be as lightweight as possible. Otherwise, developers will be\nforced to wait in line in order to update the gateway. Despite these drawbacks, though,\nfor most real-world applications, it makes sense to use an API gateway. If necessary,\nyou can use the Backends for frontends pattern to enable the teams to develop and\ndeploy their APIs independently. \n8.2.3\nNetflix as an example of an API gateway\nA great example of an API gateway is the Netflix API. The Netflix streaming service is\navailable on hundreds of different kinds of devices including televisions, Blu-ray\nplayers, smartphones, and many more gadgets. Initially, Netflix attempted to have a\none-size-fits-all style API for its streaming service (www.programmableweb.com/news/\nwhy-rest-keeps-me-night/2012/05/15). But the company soon discovered that didn’t\nwork well because of the diverse range of devices and their different needs. Today,\nNetflix uses an API gateway that implements a separate API for each device. The client\ndevice team develops and owns the API implementation.\n In the first version of the API gateway, each client team implemented their API\nusing Groovy scripts that perform routing and API composition. Each script invoked\none or more service APIs using Java client libraries provided by the service teams. On\none hand, this works well, and client developers have written thousands of scripts. The\nNetflix API gateway handles billions of requests per day, and on average each API call\nfans out to six or seven backend services. On the other hand, Netflix has found this\nmonolithic architecture to be somewhat cumbersome.\n As a result, Netflix is now moving to an API gateway architecture similar to the\nBackends for frontends pattern. In this new architecture, client teams write API mod-\nules using NodeJS. Each API module runs its own Docker container, but the scripts\ndon’t invoke the services directly. Rather, they invoke a second “API gateway,” which\nexposes the service APIs using Netflix Falcor. Netflix Falcor is an API technology that\ndoes declarative, dynamic API composition and enables a client to invoke multiple\n \n",
      "content_length": 3035,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "268\nCHAPTER 8\nExternal API patterns\nservices using a single request. This new architecture has a number of benefits. The\nAPI modules are isolated from one another, which improves reliability and observabil-\nity, and the client API module is independently scalable. \n8.2.4\nAPI gateway design issues\nNow that we’ve looked at the API gateway pattern and its benefits and drawbacks, let’s\nexamine various API gateway design issues. There are several issues to consider when\ndesigning an API gateway:\nPerformance and scalability\nWriting maintainable code by using reactive programming abstractions\nHandling partial failure\nBeing a good citizen in the application’s architecture\nWe’ll look at each one.\nPERFORMANCE AND SCALABILITY\nAn API gateway is the application’s front door. All external requests must first pass\nthrough the gateway. Although most companies don’t operate at the scale of Netflix,\nwhich handles billions of requests per day, the performance and scalability of the API\ngateway is usually very important. A key design decision that affects performance and\nscalability is whether the API gateway should use synchronous or asynchronous I/O.\n In the synchronous I/O model , each network connection is handled by a dedicated\nthread. This is a simple programming model and works reasonably well. For example,\nit’s the basis of the widely used Java EE servlet framework, although this framework\nprovides the option of completing a request asynchronously. One limitation of syn-\nchronous I/O, however, is that operating system threads are heavyweight, so there is a\nlimit on the number of threads, and hence concurrent connections, that an API gate-\nway can have.\n The other approach is to use the asynchronous (nonblocking) I/O model . In this\nmodel, a single event loop thread dispatches I/O requests to event handlers. You have\na variety of asynchronous I/O technologies to choose from. On the JVM you can use\none of the NIO-based frameworks such as Netty, Vertx, Spring Reactor, or JBoss\nUndertow. One popular non-JVM option is NodeJS, a platform built on Chrome’s\nJavaScript engine.\n Nonblocking I/O is much more scalable because it doesn’t have the overhead of\nusing multiple threads. The drawback, though, is that the asynchronous, callback-\nbased programming model is much more complex. The code is more difficult to\nwrite, understand, and debug. Event handlers must return quickly to avoid blocking\nthe event loop thread.\n Also, whether using nonblocking I/O has a meaningful overall benefit depends on\nthe characteristics of the API gateway’s request-processing logic. Netflix had mixed results\nwhen it rewrote Zuul, its edge server, to use NIO (see https://medium.com/netflix-\ntechblog/zuul-2-the-netflix-journey-to-asynchronous-non-blocking-systems-45947377fb5c).\n \n",
      "content_length": 2787,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "269\nThe API gateway pattern\nOn one hand, as you would expect, using NIO reduced the cost of each network con-\nnection, due to the fact that there’s no longer a dedicated thread for each one. Also, a\nZuul cluster that ran I/O-intensive logic—such as request routing—had a 25% increase\nin throughput and a 25% reduction in CPU utilization. On the other hand, a Zuul clus-\nter that ran CPU-intensive logic—such as decryption and compression—showed no\nimprovement. \nUSE REACTIVE PROGRAMMING ABSTRACTIONS\nAs mentioned earlier, API composition consists of invoking multiple backend services.\nSome backend service requests depend entirely on the client request’s parameters.\nOthers might depend on the results of other service requests. One approach is for an\nAPI endpoint handler method to call the services in the order determined by the depen-\ndencies. For example, the following listing shows the handler for the findOrder()\nrequest that’s written this way. It calls each of the four services, one after the other.\n@RestController\npublic class OrderDetailsController {\n@RequestMapping(\"/order/{orderId}\")\npublic OrderDetails getOrderDetails(@PathVariable String orderId) {\nOrderInfo orderInfo = orderService.findOrderById(orderId);\nTicketInfo ticketInfo = kitchenService\n.findTicketByOrderId(orderId);\nDeliveryInfo deliveryInfo = deliveryService\n.findDeliveryByOrderId(orderId);\nBillInfo billInfo = accountingService\n.findBillByOrderId(orderId);\nOrderDetails orderDetails =\nOrderDetails.makeOrderDetails(orderInfo, ticketInfo,\ndeliveryInfo, billInfo);\nreturn orderDetails;\n}\n...\nThe drawback of calling the services sequentially is that the response time is the sum\nof the service response times. In order to minimize response time, the composition\nlogic should, whenever possible, invoke services concurrently. In this example, there\nare no dependencies between the service calls. All services should be invoked concur-\nrently, which significantly reduces response time. The challenge is to write concurrent\ncode that’s maintainable.\n This is because the traditional way to write scalable, concurrent code is to use\ncallbacks. Asynchronous, event-driven I/O is inherently callback-based. Even a Servlet\nListing 8.1\nFetching the order details by calling the backend services sequentially\n \n",
      "content_length": 2287,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "270\nCHAPTER 8\nExternal API patterns\nAPI-based API composer that invokes services concurrently typically uses callbacks. It\ncould execute requests concurrently by calling ExecutorService.submitCallable().\nThe problem there is that this method returns a Future, which has a blocking API. A\nmore scalable approach is for an API composer to call ExecutorService.submit\n(Runnable) and for each Runnable to invoke a callback with the outcome of the\nrequest. The callback accumulates results, and once all of them have been received it\nsends back the response to the client.\n Writing API composition code using the traditional asynchronous callback approach\nquickly leads you to callback hell. The code will be tangled, difficult to understand,\nand error prone, especially when composition requires a mixture of parallel and\nsequential requests. A much better approach is to write API composition code in a\ndeclarative style using a reactive approach. Examples of reactive abstractions for the\nJVM include the following:\nJava 8 CompletableFutures\nProject Reactor Monos\nRxJava (Reactive Extensions for Java) Observables, created by Netflix specifi-\ncally to solve this problem in its API gateway\nScala Futures\nA NodeJS-based API gateway would use JavaScript promises or RxJS, which is reactive\nextensions for JavaScript. Using one of these reactive abstractions will enable you to\nwrite concurrent code that’s simple and easy to understand. Later in this chapter, I\nshow an example of this style of coding using Project Reactor Monos and version 5 of\nthe Spring Framework. \nHANDLING PARTIAL FAILURES\nAs well as being scalable, an API gateway must also be reliable. One way to achieve reli-\nability is to run multiple instances of the gateway behind a load balancer. If one\ninstance fails, the load balancer will route requests to the other instances.\n Another way to ensure that an API gateway is reliable is to properly handle failed\nrequests and requests that have unacceptably high latency. When an API gateway\ninvokes a service, there’s always a chance that the service is slow or unavailable. An API\ngateway may wait a very long time, perhaps indefinitely, for a response, which con-\nsumes resources and prevents it from sending a response to its client. An outstanding\nrequest to a failed service might even consume a limited, precious resource such as a\nthread and ultimately result in the API gateway being unable to handle any other\nrequests. The solution, as described in chapter 3, is for an API gateway to use the Cir-\ncuit breaker pattern when invoking services. \nBEING A GOOD CITIZEN IN THE ARCHITECTURE\nIn chapter 3 I described patterns for service discovery, and in chapter 11, I cover\npatterns for observability. The service discovery patterns enable a service client,\nsuch as an API gateway, to determine the network location of a service instance so\nthat it can invoke it. The observability patterns enable developers to monitor the\n \n",
      "content_length": 2950,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "271\nImplementing an API gateway\nbehavior of an application and troubleshoot problems. An API gateway, like other ser-\nvices in the architecture, must implement the patterns that have been selected for the\narchitecture. \n8.3\nImplementing an API gateway\nLet’s now look at how to implement an API gateway. As mentioned earlier, the respon-\nsibilities of an API gateway are as follows:\nRequest routing—Routes requests to services using criteria such as HTTP request\nmethod and path. The API gateway must route using the HTTP request method\nwhen the application has one or more CQRS query services. As discussed in\nchapter 7, in such an architecture commands and queries are handled by sepa-\nrate services.\nAPI composition—Implements a GET REST endpoint using the API composition\npattern, described in chapter 7. The request handler combines the results of\ninvoking multiple services.\nEdge functions—Most notable among these is authentication.\nProtocol translation—Translates between client-friendly protocols and the client-\nunfriendly protocols used by services.\nBeing a good citizen in the application’s architecture.\nThere are a couple of different ways to implement an API gateway:\nUsing an off-the-shelf API gateway product/service—This option requires little or no\ndevelopment but is the least flexible. For example, an off-the-shelf API gateway\ntypically does not support API composition\nDeveloping your own API gateway using either an API gateway framework or a web frame-\nwork as the starting point—This is the most flexible approach, though it requires\nsome development effort.\nLet’s look at these options, starting with using an off-the-shelf API gateway product or\nservice.\n8.3.1\nUsing an off-the-shelf API gateway product/service\nSeveral off-the-self services and products implement API gateway features. Let’s first\nlook at a couple of services that are provided by AWS. After that, I’ll discuss some\nproducts that you can download, configure, and run yourself.\nAWS API GATEWAY\nThe AWS API gateway, one of the many services provided by Amazon Web Services, is\na service for deploying and managing APIs. An AWS API gateway API is a set of REST\nresources, each of which supports one or more HTTP methods. You configure the API\ngateway to route each (Method, Resource) to a backend service. A backend service is\neither an AWS Lambda Function, described later in chapter 12, an application-\ndefined HTTP service, or an AWS service. If necessary, you can configure the API\n \n",
      "content_length": 2487,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "272\nCHAPTER 8\nExternal API patterns\ngateway to transform request and response using a template-based mechanism. The\nAWS API gateway can also authenticate requests.\n The AWS API gateway fulfills some of the requirements for an API gateway that I\nlisted earlier. The API gateway is provided by AWS, so you’re not responsible for instal-\nlation and operations. You configure the API gateway, and AWS handles everything\nelse, including scaling.\n Unfortunately, the AWS API gateway has several drawbacks and limitations that\ncause it to not fulfill other requirements. It doesn’t support API composition, so you’d\nneed to implement API composition in the backend services. The AWS API gateway\nonly supports HTTP(S) with a heavy emphasis on JSON. It only supports the Server-\nside discovery pattern, described in chapter 3. An application will typically use an AWS\nElastic Load Balancer to load balance requests across a set of EC2 instances or ECS\ncontainers. Despite these limitations, unless you need API composition, the AWS API\ngateway is a good implementation of the API gateway pattern. \nAWS APPLICATION LOAD BALANCER\nAnother AWS service that provides API gateway-like functionality is the AWS Applica-\ntion Load Balancer, which is a load balancer for HTTP, HTTPS, WebSocket, and\nHTTP/2 (https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/).\nWhen configuring an Application Load Balancer, you define routing rules that route\nrequests to backend services, which must be running on AWS EC2 instances.\n Like the AWS API gateway, the AWS Application Load Balancer meets some of the\nrequirements for an API gateway. It implements basic routing functionality. It’s hosted,\nso you’re not responsible for installation or operations. Unfortunately, it’s quite lim-\nited. It doesn’t implement HTTP method-based routing. Nor does it implement API\ncomposition or authentication. As a result, the AWS Application Load Balancer\ndoesn’t meet the requirements for an API gateway. \nUSING AN API GATEWAY PRODUCT\nAnother option is to use an API gateway product such as Kong or Traefik . These are\nopen source packages that you install and operate yourself. Kong is based on the\nNGINX HTTP server, and Traefik is written in GoLang. Both products let you config-\nure flexible routing rules that use the HTTP method, headers, and path to select the\nbackend service. Kong lets you configure plugins that implement edge functions such\nas authentication. Traefik can even integrate with some service registries, described in\nchapter 3.\n Although these products implement edge functions and powerful routing capabil-\nities, they have some drawbacks. You must install, configure, and operate them your-\nself. They don’t support API composition. And if you want the API gateway to perform\nAPI composition, you must develop your own API gateway. \n \n",
      "content_length": 2837,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "273\nImplementing an API gateway\n8.3.2\nDeveloping your own API gateway\nDeveloping an API gateway isn’t particularly difficult. It’s basically a web application\nthat proxies requests to other services. You can build one using your favorite web\nframework. There are, however, two key design problems that you’ll need to solve:\nImplementing a mechanism for defining routing rules in order to minimize the\ncomplex coding\nCorrectly implementing the HTTP proxying behavior, including how HTTP\nheaders are handled\nConsequently, a better starting point for developing an API gateway is to use a frame-\nwork designed for that purpose. Its built-in functionality significantly reduces the\namount of code you need to write.\n We’ll take a look at Netflix Zuul, an open source project by Netflix, and then con-\nsider the Spring Cloud Gateway, an open source project from Pivotal.\nUSING NETFLIX ZUUL\nNetflix developed the Zuul framework to implement edge functions such as routing,\nrate limiting, and authentication (https://github.com/Netflix/zuul). The Zuul frame-\nwork uses the concept of filters, reusable request interceptors that are similar to servlet\nfilters or NodeJS Express middleware. Zuul handles an HTTP request by assembling a\nchain of applicable filters that then transform the request, invoke backend services,\nand transform the response before it’s sent back to the client. Although you can use\nZuul directly, using Spring Cloud Zuul, an open source project from Pivotal, is far eas-\nier. Spring Cloud Zuul builds on Zuul and through convention-over-configuration\nmakes developing a Zuul-based server remarkably easy.\n Zuul handles the routing and edge functionality. You can extend Zuul by defining\nSpring MVC controllers that implement API composition. But a major limitation of\nZuul is that it can only implement path-based routing. For example, it’s incapable of\nrouting GET /orders to one service and POST /orders to a different service. Conse-\nquently, Zuul doesn’t support the query architecture described in chapter 7. \nABOUT SPRING CLOUD GATEWAY\nNone of the options I’ve described so far meet all the requirements. In fact, I had\ngiven up in my search for an API gateway framework and had started developing an\nAPI gateway based on Spring MVC. But then I discovered the Spring Cloud Gate-\nway project (https://cloud.spring.io/spring-cloud-gateway/). It’s an API gateway\nframework built on top of several frameworks, including Spring Framework 5,\nSpring Boot 2, and Spring Webflux, which is a reactive web framework that's part of\nSpring Framework 5 and built on Project Reactor. Project Reactor is an NIO-based\nreactive framework for the JVM that provides the Mono abstraction used a little\nlater in this chapter.\n \n \n \n",
      "content_length": 2734,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "274\nCHAPTER 8\nExternal API patterns\n Spring Cloud Gateway provides a simple yet comprehensive way to do the following:\nRoute requests to backend services.\nImplement request handlers that perform API composition.\nHandle edge functions such as authentication.\nFigure 8.8 shows the key parts of an API gateway built using this framework.\nThe API gateway consists of the following packages:\n\nApiGatewayMain package—Defines the Main program for the API gateway.\nOne or more API packages—An API package implements a set of API endpoints.\nFor example, the Orders package implements the Order-related API endpoints.\nProxy package—Consists of proxy classes that are used by the API packages to\ninvoke the services.\n«@SpringBootApplication»\nApiGatewayApplication\n«@Bean»\norderProxyRouting\n«@Bean»\norderHandlerRouting\nGET/orders/{orderId}\n=>\nOrderHandlers::getOrderDetails\norders*\n=>\nhttp://orderservice\nmono<ServerResponse>\ngetOrderDetails(ServerRequest){\n...\n}\nmono<OrderInfo>\nﬁndOrderById()(orderId){\n...WebClient\n.get()\n.url(\"http://order-service/...\"}\n}\nstatic void main(String[]args){\n...\n}\nRemote proxies«package»\nOrders«API package»\n«proxy»\n....\n«proxy»\nDeliveryService\nﬁndDeliveryByOrder()\n«proxy»\nOrderService\nﬁndOrderById()\nOrder handlers\nSpring Cloud Gateway\nSpring 5\ngetOrderDetails()\nSpring webFlux\nProject reactor\n«Spring Conﬁguration»OrderConﬁguration\nFigure 8.8\nThe architecture of an API gateway built using Spring Cloud Gateway\n \n",
      "content_length": 1444,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "275\nImplementing an API gateway\nThe OrderConfiguration class defines the Spring beans responsible for routing\nOrder-related requests. A routing rule can match against some combination of the\nHTTP method, the headers, and the path. The orderProxyRoutes @Bean defines rules\nthat map API operations to backend service URLs. For example, it routes paths begin-\nning with /orders to the Order Service.\n The orderHandlers @Bean defines rules that override those defined by order-\nProxyRoutes. These rules map API operations to handler methods, which are the\nSpring WebFlux equivalent of Spring MVC controller methods. For example, order-\nHandlers maps the operation GET /orders/{orderId} to the OrderHandlers::get-\nOrderDetails() method.\n The OrderHandlers class implements various request handler methods, such as\nOrderHandlers::getOrderDetails(). This method uses API composition to fetch the\norder details (described earlier). The handle methods invoke backend services using\nremote proxy classes, such as OrderService. This class defines methods for invoking\nthe OrderService.\n Let’s take a look at the code, starting with the OrderConfiguration class. \nTHE ORDERCONFIGURATION CLASS\nThe OrderConfiguration class, shown in listing 8.2, is a Spring @Configuration class.\nIt defines the Spring @Beans that implement the /orders endpoints. The order-\nProxyRouting and orderHandlerRouting @Beans use the Spring WebFlux routing\nDSL to define the request routing. The orderHandlers @Bean implements the request\nhandlers that perform API composition.\n@Configuration\n@EnableConfigurationProperties(OrderDestinations.class)\npublic class OrderConfiguration {\n@Bean\npublic RouteLocator orderProxyRouting(OrderDestinations orderDestinations) {\nreturn Routes.locator()\n.route(\"orders\")\n.uri(orderDestinations.orderServiceUrl)\n.predicate(path(\"/orders\").or(path(\"/orders/*\")))\n  \n.and()\n...\n.build();\n}\n@Bean\npublic RouterFunction<ServerResponse>\norderHandlerRouting(OrderHandlers orderHandlers) {\nreturn RouterFunctions.route(GET(\"/orders/{orderId}\"),\n  \norderHandlers::getOrderDetails);\n}\nListing 8.2\nThe Spring @Beans that implement the /orders endpoints\nBy default, route all requests whose\npath begins with /orders to the URL\norderDestinations.orderServiceUrl.\nRoute a GET \n/orders/{orderId} \nto orderHandlers::\ngetOrderDetails.\n \n",
      "content_length": 2319,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 306,
      "content": "276\nCHAPTER 8\nExternal API patterns\n@Bean\npublic OrderHandlers orderHandlers(OrderService orderService,\nKitchenService kitchenService,\nDeliveryService deliveryService,\nAccountingService accountingService) {\nreturn new OrderHandlers(orderService, kitchenService,\n      \ndeliveryService, accountingService);\n}\n}\nOrderDestinations, shown in the following listing, is a Spring @Configuration-\nProperties class that enables the externalized configuration of backend service URLs.\n@ConfigurationProperties(prefix = \"order.destinations\")\npublic class OrderDestinations {\n@NotNull\npublic String orderServiceUrl;\npublic String getOrderServiceUrl() {\nreturn orderServiceUrl;\n}\npublic void setOrderServiceUrl(String orderServiceUrl) {\nthis.orderServiceUrl = orderServiceUrl;\n}\n...\n}\nYou can, for example, specify the URL of the Order Service either as the order\n.destinations.orderServiceUrl property in a properties file or as an operating sys-\ntem environment variable, ORDER_DESTINATIONS_ORDER_SERVICE_URL. \nTHE ORDERHANDLERS CLASS\nThe OrderHandlers class, shown in the following listing, defines the request handler\nmethods that implement custom behavior, including API composition. The getOrder-\nDetails() method, for example, performs API composition to retrieve information\nabout an order. This class is injected with several proxy classes that make requests to\nbackend services.\npublic class OrderHandlers {\nprivate OrderService orderService;\nprivate KitchenService kitchenService;\nprivate DeliveryService deliveryService;\nprivate AccountingService accountingService;\nListing 8.3\nThe externalized configuration of backend service URLs\nListing 8.4\nThe OrderHandlers class implements custom request-handling logic.\nThe @Bean, which implements the\ncustom request-handling logic\n \n",
      "content_length": 1774,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 307,
      "content": "277\nImplementing an API gateway\npublic OrderHandlers(OrderService orderService,\nKitchenService kitchenService,\nDeliveryService deliveryService,\nAccountingService accountingService) {\nthis.orderService = orderService;\nthis.kitchenService = kitchenService;\nthis.deliveryService = deliveryService;\nthis.accountingService = accountingService;\n}\npublic Mono<ServerResponse> getOrderDetails(ServerRequest serverRequest) {\nString orderId = serverRequest.pathVariable(\"orderId\");\nMono<OrderInfo> orderInfo = orderService.findOrderById(orderId);\nMono<Optional<TicketInfo>> ticketInfo =\nkitchenService\n.findTicketByOrderId(orderId)\n.map(Optional::of)\n      \n.onErrorReturn(Optional.empty());\n  \nMono<Optional<DeliveryInfo>> deliveryInfo =\ndeliveryService\n.findDeliveryByOrderId(orderId)\n.map(Optional::of)\n.onErrorReturn(Optional.empty());\nMono<Optional<BillInfo>> billInfo = accountingService\n.findBillByOrderId(orderId)\n.map(Optional::of)\n.onErrorReturn(Optional.empty());\nMono<Tuple4<OrderInfo, Optional<TicketInfo>,\n  \nOptional<DeliveryInfo>, Optional<BillInfo>>> combined =\nMono.when(orderInfo, ticketInfo, deliveryInfo, billInfo);\nMono<OrderDetails> orderDetails =\n               \ncombined.map(OrderDetails::makeOrderDetails);\nreturn orderDetails.flatMap(person -> ServerResponse.ok()   \n.contentType(MediaType.APPLICATION_JSON)\n.body(fromObject(person)));\n}\n}\nThe getOrderDetails() method implements API composition to fetch the order\ndetails. It’s written in a scalable, reactive style using the Mono abstraction , which is pro-\nvided by Project Reactor. A Mono, which is a richer kind of Java 8 CompletableFuture,\ncontains the outcome of an asynchronous operation that’s either a value or an\nexception. It has a rich API for transforming and combining the values returned by\nasynchronous operations. You can use Monos to write concurrent code in a style that’s\nTransform a TicketInfo into \nan Optional<TicketInfo>.\nIf the service invocation failed, \nreturn Optional.empty().\nCombine the four \nvalues into a single \nvalue, a Tuple4.\nTransform the Tuple4 \ninto an OrderDetails.\nTransform the\nOrderDetails into\na ServerResponse.\n \n",
      "content_length": 2127,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 308,
      "content": "278\nCHAPTER 8\nExternal API patterns\nsimple and easy to understand. In this example, the getOrderDetails() method\ninvokes the four services in parallel and combines the results to create an Order-\nDetails object.\n The getOrderDetails() method takes a ServerRequest, which is the Spring Web-\nFlux representation of an HTTP request, as a parameter and does the following:\n1\nIt extracts the orderId from the path.\n2\nIt invokes the four services asynchronously via their proxies, which return Monos.\nIn order to improve availability, getOrderDetails() treats the results of all ser-\nvices except the OrderService as optional. If a Mono returned by an optional\nservice contains an exception, the call to onErrorReturn() transforms it into a\nMono containing an empty Optional.\n3\nIt combines the results asynchronously using Mono.when(), which returns a\nMono<Tuple4> containing the four values.\n4\nIt transforms the Mono<Tuple4> into a Mono<OrderDetails> by calling Order-\nDetails::makeOrderDetails.\n5\nIt transforms the OrderDetails into a ServerResponse, which is the Spring\nWebFlux representation of the JSON/HTTP response.\nAs you can see, because getOrderDetails() uses Monos, it concurrently invokes the\nservices and combines the results without using messy, difficult-to-read callbacks. Let’s\ntake a look at one of the service proxies that return the results of a service API call\nwrapped in a Mono. \nTHE ORDERSERVICE CLASS\nThe OrderService class, shown in the following listing, is a remote proxy for the Order\nService. It invokes the Order Service using a WebClient, which is the Spring Web-\nFlux reactive HTTP client.\n@Service\npublic class OrderService {\nprivate OrderDestinations orderDestinations;\nprivate WebClient client;\npublic OrderService(OrderDestinations orderDestinations, WebClient client)\n{\nthis.orderDestinations = orderDestinations;\nthis.client = client;\n}\npublic Mono<OrderInfo> findOrderById(String orderId) {\nMono<ClientResponse> response = client\n.get()\nListing 8.5\nOrderService class—a remote proxy for Order Service\n \n",
      "content_length": 2037,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 309,
      "content": "279\nImplementing an API gateway\n.uri(orderDestinations.orderServiceUrl + \"/orders/{orderId}\",\norderId)\n.exchange();\n    \nreturn response.flatMap(resp -> resp.bodyToMono(OrderInfo.class));  \n}\n}\nThe findOrder() method retrieves the OrderInfo for an order. It uses the WebClient\nto make the HTTP request to the Order Service and deserializes the JSON response\nto an OrderInfo. WebClient has a reactive API, and the response is wrapped in a Mono.\nThe findOrder() method uses flatMap() to transform the Mono<ClientResponse>\ninto a Mono<OrderInfo>. As the name suggests, the bodyToMono() method returns the\nresponse body as a Mono. \nTHE APIGATEWAYAPPLICATION CLASS\nThe ApiGatewayApplication class, shown in the following listing, implements the API\ngateway’s main() method. It’s a standard Spring Boot main class.\n@SpringBootConfiguration\n@EnableAutoConfiguration\n@EnableGateway\n@Import(OrdersConfiguration.class)\npublic class ApiGatewayApplication {\npublic static void main(String[] args) {\nSpringApplication.run(ApiGatewayApplication.class, args);\n}\n}\nThe @EnableGateway annotation imports the Spring configuration for the Spring\nCloud Gateway framework.\n Spring Cloud Gateway is an excellent framework for implementing an API gateway.\nIt enables you to configure basic proxying using a simple, concise routing rules DSL.\nIt’s also straightforward to route requests to handler methods that perform API com-\nposition and protocol translation. Spring Cloud Gateway is built using the scalable,\nreactive Spring Framework 5 and Project Reactor frameworks. But there’s another\nappealing option for developing your own API gateway: GraphQL, a framework that\nprovides graph-based query language. Let’s look at how that works. \n8.3.3\nImplementing an API gateway using GraphQL\nImagine that you’re responsible for implementing the FTGO’s API Gateway’s GET\n/orders/{orderId} endpoint, which returns the order details. On the surface, imple-\nmenting this endpoint might appear to be simple. But as described in section 8.1, this\nendpoint retrieves data from multiple services. Consequently, you need to use the\nListing 8.6\nThe main() method for the API gateway\nInvoke the\nservice.\nConvert the response\nbody to an OrderInfo.\n \n",
      "content_length": 2211,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 310,
      "content": "280\nCHAPTER 8\nExternal API patterns\nAPI composition pattern and write code that invokes the services and combines\nthe results.\n Another challenge, mentioned earlier, is that different clients need slightly differ-\nent data. For example, unlike the mobile application, the desktop SPA application dis-\nplays your rating for the order. One way to tailor the data returned by the endpoint, as\ndescribed in chapter 3, is to give the client the ability to specify the data they need. An\nendpoint can, for example, support query parameters such as the expand parameter,\nwhich specifies the related resources to return, and the field parameter, which speci-\nfies the fields of each resource to return. The other option is to define multiple ver-\nsions of this endpoint as part of applying the Backends for frontends pattern. This is a\nlot of work for just one of the many API endpoints that the FTGO’s API Gateway\nneeds to implement.\n Implementing an API gateway with a REST API that supports a diverse set of cli-\nents well is time consuming. Consequently, you may want to consider using a graph-\nbased API framework, such as GraphQL, that’s designed to support efficient data\nfetching. The key idea with graph-based API frameworks is that, as figure 8.9 shows,\nthe server’s API consists of a graph-based schema. The graph-based schema defines a\nset of nodes (types), which have properties (fields) and relationships with other nodes.\nThe client retrieves data by executing a query that specifies the required data in terms\nof the graph’s nodes and their properties and relationships. As a result, a client can\nretrieve the data it needs in a single round-trip to the API gateway.\nGraph-based API technology has a couple of important benefits. It gives clients con-\ntrol over what data is returned. Consequently, developing a single API that’s flexible\nConsumer\nConsumer\nRestaurant\nDelivery\nConsumer Service\nAPI gateway\nGraph-based API framework\nGraph schema\nOrder\nOrder Service\nRestaurant\nRestaurant Service\nDelivery\nDelivery Service\nOrder\nSchema\n=>\nService\nmapping\nClient\nQuery\nQuery\nQuery\nQuery\nQuery\nFigure 8.9\nThe API gateway’s API consists of a graph-based schema that’s mapped to the services. A client \nissues a query that retrieves multiple graph nodes. The graph-based API framework executes the query by \nretrieving data from one or more services.\n \n",
      "content_length": 2355,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 311,
      "content": "281\nImplementing an API gateway\nenough to support diverse clients becomes feasible. Another benefit is that even though\nthe API is much more flexible, this approach significantly reduces the development\neffort. That’s because you write the server-side code using a query execution frame-\nwork that’s designed to support API composition and projections. It’s as if, rather than\nforce clients to retrieve data via stored procedures that you need to write and main-\ntain, you let them execute queries against the underlying database.\nThis section talks about how to develop an API gateway using Apollo GraphQL. I’m\nonly going to cover a few of the key features of GraphQL and Apollo GraphQL. For\nmore information, you should consult the GraphQL and Apollo GraphQL docu-\nmentation.\n The GraphQL-based API gateway, shown in figure 8.10, is written in JavaScript\nusing the NodeJS Express web framework and the Apollo GraphQL server. The key\nparts of the design are as follows:\nGraphQL schema—The GraphQL schema defines the server-side data model and\nthe queries it supports.\nResolver functions—The resolve functions map elements of the schema to the\nvarious backend services.\nProxy classes—The proxy classes invoke the FTGO application’s services.\nThere’s also a small amount of glue code that integrates the GraphQL server with the\nExpress web framework. Let’s look at each part, starting with the GraphQL schema.\nSchema-driven API technologies\nThe two most popular graph-based API technologies are GraphQL (http://graphql.org)\nand Netflix Falcor (http://netflix.github.io/falcor/). Netflix Falcor models server-side\ndata as a virtual JSON object graph. The Falcor client retrieves data from a Falcor\nserver by executing a query that retrieves properties of that JSON object. The client\ncan also update properties. In the Falcor server, the properties of the object graph\nare mapped to backend data sources, such as services with REST APIs. The server\nhandles a request to set or get properties by invoking one or more backend data\nsources.\nGraphQL, developed by Facebook and released in 2015, is another popular graph-\nbased API technology. It models the server-side data as a graph of objects that have\nfields and references to other objects. The object graph is mapped to backend data\nsources. GraphQL clients can execute queries that retrieve data and mutations that\ncreate and update data. Unlike Netflix Falcor, which is an implementation, GraphQL\nis a standard, with clients and servers available for a variety of languages, including\nNodeJS, Java, and Scala.\nApollo GraphQL is a popular JavaScript/NodeJS implementation (www.apollographql\n.com). It’s a platform that includes a GraphQL server and client. Apollo GraphQL\nimplements some powerful extensions to the GraphQL specification, such as sub-\nscriptions that push changed data to the client.\n \n",
      "content_length": 2856,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 312,
      "content": "282\nCHAPTER 8\nExternal API patterns\nDEFINING A GRAPHQL SCHEMA\nA GraphQL API is centered around a schema, which consists of a collection of types\nthat define the structure of the server-side data model and the operations, such as\nqueries, that a client can perform. GraphQL has several different kinds of types. The\nexample code in this section uses just two kinds of types: object types, which are the\ntype Query{\norders(consumerId:Int!): [Order]\norder(orderId : int!): Order\nconsumer(consumerId : int!): Consumer\n}\ntype Order {\norderId: ID,\nconsumerId: Int,\nconsumer: Consumer\nrestaurant: Restaurant\ndeliveryInfo : DeliveryInfo\n...\nconst resolvers = {\nQuery:{\norders: resolveOrders,\norder: resolveOrder,\n...\n},\nOrder:{\nconsumer: resolveOrderConsumer,\nrestaurant: resolveOrderRestaurant,\ndeliveryInfo: resolveOrderDeliveryInfo\n},\n...\nfunction resolveOrder(_. {orderId}, context){\nreturn context.orderServiceProxy.ﬁndOrder(orderI\nd);\n}\nfunction resolveOrderDeliveryInfo({orderId}, args,\ncontext) {\nreturn context.deliveryServiceProxy.ﬁndDeliveryF\norOrder(orderId);\n}\nApollo graphQL engine\nConsumerServiceProxy\nOrderServiceProxy\nRestaurantServiceProxy\nDeliveryServiceProxy\nConsumer Service\ninvokes\ninvokes\ninvokes\ninvokes\nOrder Service\nRestaurant Service\nDelivery Service\nExpress web framework\nApollo\ngraphQL\nclient\nFTGO API gateway\nhttp://.../graphql?query={orders(consumerId:1){orde\nrId,restaurant{id}}}\nFigure 8.10\nThe design of the GraphQL-based FTGO API Gateway\n \n",
      "content_length": 1467,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 313,
      "content": "283\nImplementing an API gateway\nprimary way of defining the data model, and enums, which are similar to Java enums.\nAn object type has a name and a collection of typed, named fields. A field can be a sca-\nlar type, such as a number, string, or enum; a list of scalar types; a reference to another\nobject type; or a collection of references to another object type. Despite resembling a\nfield of a traditional object-oriented class, a GraphQL field is conceptually a function\nthat returns a value. It can have arguments, which enable a GraphQL client to tailor\nthe data the function returns.\n GraphQL also uses fields to define the queries supported by the schema. You\ndefine the schema’s queries by declaring an object type, which by convention is called\nQuery. Each field of the Query object is a named query, which has an optional set of\nparameters, and a return type. I found this way of defining queries a little confusing\nwhen I first encountered it, but it helps to keep in mind that a GraphQL field is a\nfunction. It will become even clearer when we look at how fields are connected to the\nbackend data sources.\n The following listing shows part of the schema for the GraphQL-based FTGO API\ngateway. It defines several object types. Most of the object types correspond to the\nFTGO application’s Consumer, Order, and Restaurant entities. It also has a Query object\ntype that defines the schema’s queries.\ntype Query {\n   \norders(consumerId : Int!): [Order]\norder(orderId : Int!): Order\nconsumer(consumerId : Int!): Consumer\n}\ntype Consumer {\nid: ID\n  \nfirstName: String\nlastName: String\norders: [Order]\n     \n}\ntype Order {\norderId: ID,\nconsumerId : Int,\nconsumer: Consumer\nrestaurant: Restaurant\ndeliveryInfo : DeliveryInfo\n...\n}\ntype Restaurant {\nid: ID\nname: String\n...\n}\nListing 8.7\nThe GraphQL schema for the FTGO API gateway\nDefines the queries \nthat a client can \nexecute\nThe unique ID \nfor a Consumer\nA consumer has \na list of orders.\n \n",
      "content_length": 1950,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 314,
      "content": "284\nCHAPTER 8\nExternal API patterns\ntype DeliveryInfo {\nstatus : DeliveryStatus\nestimatedDeliveryTime : Int\nassignedCourier :String\n}\nenum DeliveryStatus {\nPREPARING\nREADY_FOR_PICKUP\nPICKED_UP\nDELIVERED\n}\nDespite having a different syntax, the Consumer, Order, Restaurant, and Delivery-\nInfo object types are structurally similar to the corresponding Java classes. One differ-\nence is the ID type, which represents a unique identifier.\n This schema defines three queries:\n\norders()—Returns the Orders for the specified Consumer\n\norder()—Returns the specified Order\n\nconsumer()—Returns the specified Consumer\nThese queries may seem not different from the equivalent REST endpoints, but\nGraphQL gives the client tremendous control over the data that’s returned. To under-\nstand why, let’s look at how a client executes GraphQL queries. \nEXECUTING GRAPHQL QUERIES\nThe principal benefit of using GraphQL is that its query language gives the client\nincredible control over the returned data. A client executes a query by making a\nrequest containing a query document to the server. In the simple case, a query docu-\nment specifies the name of the query, the argument values, and the fields of the result\nobject to return. Here’s a simple query that retrieves firstName and lastName of the\nconsumer with a particular ID:\nquery {\nconsumer(consumerId:1)\n  \n{\n  \nfirstName\nlastName\n}\n}\nThis query returns those fields of the specified Consumer.\n Here’s a more elaborate query that returns a consumer, their orders, and the ID\nand name of each order’s restaurant:\nquery {\nconsumer(consumerId:1)\n{\nid\nfirstName\nlastName\nSpecifies the query called consumer, \nwhich fetches a consumer\nThe fields of the \nConsumer to return\n \n",
      "content_length": 1714,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 315,
      "content": "285\nImplementing an API gateway\norders {\norderId\nrestaurant {\nid\nname\n}\ndeliveryInfo {\nestimatedDeliveryTime\nname\n}\n}\n}\n}\nThis query tells the server to return more than just the fields of the Consumer. It\nretrieves the consumer’s Orders and each Order’s restaurant. As you can see, a\nGraphQL client can specify exactly the data to return, including the fields of transi-\ntively related objects.\n The query language is more flexible than it might first appear. That’s because a\nquery is a field of the Query object, and a query document specifies which of those fields\nthe server should return. These simple examples retrieve a single field, but a query doc-\nument can execute multiple queries by specifying multiple fields. For each field, the\nquery document supplies the field’s arguments and specifies what fields of the result\nobject it’s interested in. Here’s a query that retrieves two different consumers:\nquery {\nc1: consumer (consumerId:1)\n{ id, firstName, lastName}\nc2: consumer (consumerId:2)\n{ id, firstName, lastName}\n}\nIn this query document, c1 and c2 are what GraphQL calls aliases. They’re used to dis-\ntinguish between the two Consumers in the result, which would otherwise both be\ncalled consumer. This example retrieves two objects of the same type, but a client\ncould retrieve several objects of different types.\n A GraphQL schema defines the shape of the data and the supported queries. To\nbe useful, it has to be connected to the source of the data. Let’s look at how to do that. \nCONNECTING THE SCHEMA TO THE DATA\nWhen the GraphQL server executes a query, it must retrieve the requested data from\none or more data stores. In the case of the FTGO application, the GraphQL server\nmust invoke the APIs of the services that own the data. You associate a GraphQL\nschema with the data sources by attaching resolver functions to the fields of the object\ntypes defined by the schema. The GraphQL server implements the API composition\npattern by invoking resolver functions to retrieve the data, first for the top-level query,\nand then recursively for the fields of the result object or objects.\n The details of how resolver functions are associated with the schema depend on\nwhich GraphQL server you are using. Listing 8.8 shows how to define the resolvers\n \n",
      "content_length": 2275,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 316,
      "content": "286\nCHAPTER 8\nExternal API patterns\nwhen using the Apollo GraphQL server. You create a doubly nested JavaScript object.\nEach top-level property corresponds to an object type, such as Query and Order. Each\nsecond-level property, such as Order.consumer, defines a field’s resolver function.\nconst resolvers = {\nQuery: {\norders: resolveOrders,\nconsumer: resolveConsumer,\norder: resolveOrder\n},\nOrder: {\nconsumer: resolveOrderConsumer,\n  \nrestaurant: resolveOrderRestaurant,\ndeliveryInfo: resolveOrderDeliveryInfo\n...\n};\nA resolver function has three parameters:\nObject—For a top-level query field, such as resolveOrders, object is a root\nobject that’s usually ignored by the resolver function. Otherwise, object is the\nvalue returned by the resolver for the parent object. For example, the resolver\nfunction for the Order.consumer field is passed the value returned by the Order’s\nresolver function.\nQuery arguments—These are supplied by the query document.\nContext—Global state of the query execution that’s accessible by all resolvers. It’s\nused, for example, to pass user information and dependencies to the resolvers.\nA resolver function might invoke a single service or it might implement the API com-\nposition pattern and retrieve data from multiple services. An Apollo GraphQL server\nresolver function returns a Promise, which is JavaScript’s version of Java’s Completable-\nFuture. The promise contains the object (or a list of objects) that the resolver func-\ntion retrieved from the data store. GraphQL engine includes the return value in the\nresult object.\n Let’s look at a couple of examples. Here’s the resolveOrders() function, which is\nthe resolver for the orders query:\nfunction resolveOrders(_, { consumerId }, context) {\nreturn context.orderServiceProxy.findOrders(consumerId);\n}\nThis function obtains the OrderServiceProxy from the context and invokes it to\nfetch a consumer’s orders. It ignores its first parameter. It passes the consumerId argu-\nment, provided by the query document, to OrderServiceProxy.findOrders(). The\nfindOrders() method retrieves the consumer’s orders from OrderHistoryService.\nListing 8.8\nAttaching the resolver functions to fields of the GraphQL schema\nThe resolver for \nthe orders query\nThe resolver for \nthe consumer field \nof an Order\n \n",
      "content_length": 2285,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 317,
      "content": "287\nImplementing an API gateway\n Here’s the resolveOrderRestaurant() function, which is the resolver for the\nOrder.restaurant field that retrieves an order’s restaurant:\nfunction resolveOrderRestaurant({restaurantId}, args, context) {\nreturn context.restaurantServiceProxy.findRestaurant(restaurantId);\n}\nIts first parameter is Order. It invokes RestaurantServiceProxy.findRestaurant()\nwith the Order’s restaurantId, which was provided by resolveOrders().\n GraphQL uses a recursive algorithm to execute the resolver functions. First, it exe-\ncutes the resolver function for the top-level query specified by the Query document.\nNext, for each object returned by the query, it iterates through the fields specified in\nthe Query document. If a field has a resolver, it invokes the resolver with the object\nand the arguments from the Query document. It then recurses on the object or\nobjects returned by that resolver.\n Figure 8.11 shows how this algorithm executes the query that retrieves a consumer’s\norders and each order’s delivery information and restaurant. First, the GraphQL engine\ninvokes resolveConsumer(), which retrieves Consumer. Next, it invokes resolve-\nConsumerOrders(), which is the resolver for the Consumer.orders field that returns\nthe consumer’s orders. The GraphQL engine then iterates through Orders, invok-\ning the resolvers for the Order.restaurant and Order.deliveryInfo fields.\nThe result of executing the resolvers is a Consumer object populated with data retrieved\nfrom multiple services.\n Let’s now look at how to optimize the executing of resolvers by using batching and\ncaching. \nResolver functions\nSchema\nQuery document\ntype Query{\nconsumer(consumerId:int!): Consumer\n}\ntype Order {\n...\nrestaurant: Restaurant\ndeliveryInfo : DeliveryInfo\n...\nquery{\nconsumer(consumerId:1){\nid\nﬁrstName\nlastName\norders{\norderId\nrestaurant{\nid\nname\n}\ndeliveryInfo{\nestimatedDeliveryTime\nname\n}\n}\n}\n}\nconsumer = resolveConsumer(..., 1)\norders = resolveConsumerOrders(consumer)\nresolveOrderRestaurant(order, ...)\nresolveOrderDeliveryInfo(order)\nQuery arguments passed to resolver\nFigure 8.11\nGraphQL executes a query by recursively invoking the resolver functions for the fields specified in \nthe Query document. First, it executes the resolver for the query, and then it recursively invokes the resolvers for \nthe fields in the result object hierarchy.\n \n",
      "content_length": 2365,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 318,
      "content": "288\nCHAPTER 8\nExternal API patterns\nOPTIMIZING LOADING USING BATCHING AND CACHING\nGraphQL can potentially execute a large number of resolvers when executing a query.\nBecause the GraphQL server executes each resolver independently, there’s a risk of\npoor performance due to excessive round-trips to the services. Consider, for example,\na query that retrieves a consumer, their orders, and the orders’ restaurants. If there\nare N orders, then a simplistic implementation would make one call to Consumer\nService, one call to Order History Service, and then N calls to Restaurant Service.\nEven though the GraphQL engine will typically make the calls to Restaurant Service\nin parallel, there’s a risk of poor performance. Fortunately, you can use a few tech-\nniques to improve performance.\n One important optimization is to use a combination of server-side batching and\ncaching. Batching turns N calls to a service, such as Restaurant Service, into a sin-\ngle call that retrieves a batch of N objects. Caching reuses the result of a previous\nfetch of the same object to avoid making an unnecessary duplicate call. The combi-\nnation of batching and caching significantly reduces the number of round-trips to\nbackend services.\n A NodeJS-based GraphQL server can use the DataLoader module to implement\nbatching and caching (https://github.com/facebook/dataloader). It coalesces loads\nthat occur within a single execution of the event loop and calls a batch loading func-\ntion that you provide. It also caches calls to eliminate duplicate loads. The following list-\ning shows how RestaurantServiceProxy can use DataLoader. The findRestaurant()\nmethod loads a Restaurant via DataLoader.\nconst DataLoader = require('dataloader');\nclass RestaurantServiceProxy {\nconstructor() {\nthis.dataLoader =\n      \nnew DataLoader(restaurantIds =>\nthis.batchFindRestaurants(restaurantIds));\n}\nfindRestaurant(restaurantId) {\n         \nreturn this.dataLoader.load(restaurantId);\n}\nbatchFindRestaurants(restaurantIds) {\n     \n...\n}\n}\nRestaurantServiceProxy and, hence, DataLoader are created for each request, so\nthere’s no possibility of DataLoader mixing together different users’ data.\n Let’s now look at how to integrate the GraphQL engine with a web framework so\nthat it can be invoked by clients. \nListing 8.9\nUsing a DataLoader to optimize calls to Restaurant Service\nCreate a DataLoader, which uses \nbatchFindRestaurants() as the \nbatch loading functions.\nLoad the specified Restaurant \nvia the DataLoader.\nLoad a batch of \nRestaurants.\n \n",
      "content_length": 2519,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 319,
      "content": "289\nImplementing an API gateway\nINTEGRATING THE APOLLO GRAPHQL SERVER WITH EXPRESS\nThe Apollo GraphQL server executes GraphQL queries. In order for clients to invoke\nit, you need to integrate it with a web framework. Apollo GraphQL server supports\nseveral web frameworks, including Express, a popular NodeJS web framework.\n Listing 8.10 shows how to use the Apollo GraphQL server in an Express applica-\ntion. The key function is graphqlExpress, which is provided by the apollo-server-\nexpress module. It builds an Express request handler that executes GraphQL queries\nagainst a schema. This example configures Express to route requests to the GET\n/graphql and POST /graphql endpoints of this GraphQL request handler. It also creates\na GraphQL context containing the proxies, which makes them available to the resolvers.\nconst {graphqlExpress} = require(\"apollo-server-express\");\nconst typeDefs = gql`\n   \ntype Query {\norders: resolveOrders,\n...\n}\ntype Consumer {\n...\nconst resolvers = {     \nQuery: {\n...\n}\n}\nconst schema = makeExecutableSchema({ typeDefs, resolvers });    \nconst app = express();\nfunction makeContextWithDependencies(req) {      \nconst orderServiceProxy = new OrderServiceProxy();\nconst consumerServiceProxy = new ConsumerServiceProxy();\nconst restaurantServiceProxy = new RestaurantServiceProxy();\n...\nreturn {orderServiceProxy, consumerServiceProxy,\nrestaurantServiceProxy, ...};\n}\nfunction makeGraphQLHandler() {\n   \nreturn graphqlExpress(req => {\nreturn {schema: schema, context: makeContextWithDependencies(req)}\n});\n}\napp.post('/graphql', bodyParser.json(), makeGraphQLHandler());\n  \napp.get('/graphql', makeGraphQLHandler());\napp.listen(PORT);\nListing 8.10\nIntegrating the GraphQL server with the Express web framework\nDefine the GraphQL \nschema.\nDefine the \nresolvers.\nCombine the \nschema with the \nresolvers to create \nan executable \nschema.\nInject repositories into \nthe context so they’re \navailable to resolvers.\nMake an express request handler \nthat executes GraphQL queries \nagainst the executable schema.\nRoute POST /graphql and GET\n/graphql endpoints to the\nGraphQL server.\n \n",
      "content_length": 2110,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 320,
      "content": "290\nCHAPTER 8\nExternal API patterns\nThis example doesn’t handle concerns such as security, but those would be straight-\nforward to implement. The API gateway could, for example, authenticate users using\nPassport, a NodeJS security framework described in chapter 11. The makeContext-\nWithDependencies() function would pass the user information to each repository’s\nconstructor so that they can propagate the user information to the services.\n Let’s now look at how a client can invoke this server to execute GraphQL queries. \nWRITING A GRAPHQL CLIENT\nThere are a couple of different ways a client application can invoke the GraphQL\nserver. Because the GraphQL server has an HTTP-based API, a client application\ncould use an HTTP library to make requests, such as GET http://localhost:3000/\ngraphql?query={orders(consumerId:1){orderId,restaurant{id}}}'. It’s easier,\nthough, to use a GraphQL client library, which takes care of properly formatting\nrequests and typically provides features such as client-side caching.\n The following listing shows the FtgoGraphQLClient class, which is a simple\nGraphQL-based client for the FTGO application. Its constructor instantiates Apollo-\nClient, which is provided by the Apollo GraphQL client library. The FtgoGraphQL-\nClient class defines a findConsumer() method that uses the client to retrieve the\nname of a consumer.\nclass FtgoGraphQLClient {\nconstructor(...) {\nthis.client = new ApolloClient({ ... });\n}\nfindConsumer(consumerId) {\nreturn this.client.query({\nvariables: { cid: consumerId},\n  \nquery: gql`\nquery foo($cid : Int!) {\n  \nconsumer(consumerId: $cid)\n{\n  \nid\nfirstName\nlastName\n}\n} `,\n})\n}\n}\nThe FtgoGraphQLClient class can define a variety of query methods, such as find-\nConsumer(). Each one executes a query that retrieves exactly the data needed by the\nclient.\nListing 8.11\nUsing the Apollo GraphQL client to execute queries\nSupply the value \nof the $cid.\nDefine $cid as a \nvariable of type Int.\nSet the value of \nquery parameter \nconsumerid to $cid.\n \n",
      "content_length": 2008,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 321,
      "content": "291\nSummary\n This section has barely scratched the surface of GraphQL’s capabilities. I hope I’ve\ndemonstrated that GraphQL is a very appealing alternative to a more traditional,\nREST-based API gateway. It lets you implement an API that’s flexible enough to sup-\nport a diverse set of clients. Consequently, you should consider using GraphQL to\nimplement your API gateway. \nSummary\nYour application’s external clients usually access the application’s services via an\nAPI gateway. An API gateway provides each client with a custom API. It’s respon-\nsible for request routing, API composition, protocol translation, and implemen-\ntation of edge functions such as authentication.\nYour application can have a single API gateway or it can use the Backends for\nfrontends pattern, which defines an API gateway for each type of client. The\nmain advantage of the Backends for frontends pattern is that it gives the client\nteams greater autonomy, because they develop, deploy, and operate their own\nAPI gateway.\nThere are numerous technologies you can use to implement an API gateway,\nincluding off-the-shelf API gateway products. Alternatively, you can develop\nyour own API gateway using a framework.\nSpring Cloud Gateway is a good, easy-to-use framework for developing an API\ngateway. It routes requests using any request attribute, including the method\nand the path. Spring Cloud Gateway can route a request either directly to a\nbackend service or to a custom handler method. It’s built using the scalable,\nreactive Spring Framework 5 and Project Reactor frameworks. You can write\nyour custom request handlers in a reactive style using, for example, Project\nReactor’s Mono abstraction.\nGraphQL, a framework that provides graph-based query language, is another\nexcellent foundation for developing an API Gateway. You write a graph-oriented\nschema to describe the server-side data model and its supported queries. You\nthen map that schema to your services by writing resolvers, which retrieve data.\nGraphQL-based clients execute queries against the schema that specify exactly\nthe data that the server should return. As a result, a GraphQL-based API gate-\nway can support diverse clients. \n \n",
      "content_length": 2188,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 322,
      "content": "292\nTesting microservices:\nPart 1\nFTGO, like many organizations, had adopted a traditional approach to testing. Test-\ning is primarily an activity that happens after development. The FTGO developers\nthrow their code over a wall to the QA team, who verify that the software works as\nexpected. What’s more, most of their testing is done manually. Sadly, this approach\nto testing is broken—for two reasons:\nManual testing is extremely inefficient—You should never ask a human to do\nwhat a machine can do better. Compared to machines, humans are slow and\ncan’t work 24/7. You won’t be able to deliver software rapidly and safely if\nyou rely on manual testing. It’s essential that you write automated tests.\nTesting is done far too late in the delivery process—There certainly is a role for tests\nthat critique an application after it’s been written, but experience has shown\nthat those tests are insufficient. A much better approach is for developers to\nThis chapter covers\nEffective testing strategies for microservices\nUsing mocks and stubs to test a software \nelement in isolation\nUsing the test pyramid to determine where to \nfocus testing efforts\nUnit testing the classes inside a service\n \n",
      "content_length": 1198,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 323,
      "content": "293\nwrite automated tests as part of development. It improves their productivity\nbecause, for example, they’ll have tests that provide immediate feedback while\nediting code.\nIn this regard, FTGO is a fairly typical organization. The Sauce Labs Testing Trends in\n2018 report paints a fairly gloomy picture of the state of test automation (https://\nsaucelabs.com/resources/white-papers/testing-trends-for-2018). It describes how only\n26% of organizations are mostly automated, and a minuscule 3% are fully automated!\n The reliance on manual testing isn’t because of a lack of tooling and frameworks.\nFor example, JUnit, a popular Java testing framework, was first released in 1998. The\nreason for the lack of automated tests is mostly cultural: “Testing is QA’s job,” “It’s not\nthe best use of a developers’s time,” and so on. It also doesn’t help that developing a\nfast-running, yet effective, maintainable test suite is challenging. And, a typical large,\nmonolithic application is extremely difficult to test.\n One key motivation for using the microservice architecture is, as described in\nchapter 2, improving testability. Yet at the same time, the complexity of the microser-\nvice architecture demands that you write automated tests. Furthermore, some aspects\nof testing microservices are challenging. That’s because we need to verify that services\ncan interact correctly while minimizing the number of slow, complex, and unreliable\nend-to-end-tests that launch many services.\n This chapter is the first of two chapters on testing. It’s an introduction to testing.\nChapter 10 covers more advanced testing concepts. The two chapters are long, but\ntogether they cover testing ideas and techniques that are essential to modern software\ndevelopment in general, and to the microservice architecture in particular.\n I begin this chapter by describing effective testing strategies for a microservices-\nbased application. These strategies enable you to be confident that your software\nworks, while minimizing test complexity and execution time. After that, I describe\nhow to write one particular kind of test for your services: unit tests. Chapter 10 covers\nthe other kinds of tests: integration, component, and end-to-end.\n Let’s start by taking a look at testing strategies for microservices.\nWhy an introduction to testing?\nYou may be wondering why this chapter includes an introduction to basic testing con-\ncepts. If you’re already familiar with concepts such as the test pyramid and the different\ntypes of tests, feel free to speed-read this chapter and move onto the next one, which\nfocuses on microservices-specific testing topics. But based on my experiences consult-\ning for and training clients all over the world, a fundamental weakness of many software\ndevelopment organizations is the lack of automated testing. That’s because if you want\nto deliver software quickly and reliably, it’s absolutely essential to do automated testing.\nIt’s the only way to have a short lead time, which is the time it takes to get committed\ncode into production. Perhaps even more importantly, automated testing is essential\nbecause it forces you to develop a testable application. It’s typically very difficult to\nintroduce automating testing into an already large, complex application. In other words,\nthe fast track to monolithic hell is to not write automated tests.\n \n",
      "content_length": 3361,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 324,
      "content": "294\nCHAPTER 9\nTesting microservices: Part 1\n9.1\nTesting strategies for microservice architectures\nLet’s say you’ve made a change to FTGO application’s Order Service. Naturally, the\nnext step is for you to run your code and verify that the change works correctly. One\noption is to test the change manually. First, you run Order Service and all its depen-\ndencies, which include infrastructure services such as a database and other applica-\ntion services. Then you “test” the service by either invoking its API or using the FTGO\napplication’s UI. The downside of this approach is that it’s a slow, manual way to test\nyour code.\n A much better option is to have automated tests that you can run during develop-\nment. Your development workflow should be: edit code, run tests (ideally with a single\nkeystroke), repeat. The fast-running tests quickly tell you whether your changes work\nwithin a few seconds. But how do you write fast-running tests? And are they sufficient\nor do you need more comprehensive tests? These are the kind of questions I answer in\nthis and other sections in this chapter.\n I start this section with an overview of important automated testing concepts. We’ll\nlook at the purpose of testing and the structure of a typical test. I cover the different\ntypes of tests that you’ll need to write. I also describe the test pyramid, which provides\nvaluable guidance about where you should focus your testing efforts. After covering\ntesting concepts, I discuss strategies for testing microservices. We’ll look at the distinct\nchallenges of testing applications that have a microservice architecture. I describe\ntechniques you can use to write simpler and faster, yet still-effective, tests for your\nmicroservices.\n Let’s take a look at testing concepts.\n9.1.1\nOverview of testing\nIn this chapter, my focus is on automated testing, and I use the term test as shorthand\nfor automated test. Wikipedia defines a test case, or test, as follows:\nA test case is a set of test inputs, execution conditions, and expected results developed for\na particular objective, such as to exercise a particular program path or to verify compliance\nwith a specific requirement.\nhttps://en.wikipedia.org/wiki/Test_case\nIn other words, the purpose of a test is, as figure 9.1 shows, to verify the behavior of\nthe System Under Test (SUT). In this definition, system is a fancy term that means the\nsoftware element being tested. It might be something as small as a class, as large as the\nentire application, or something in between, such as a cluster of classes or an individ-\nual service. A collection of related tests form a test suite.\n Let’s first look at the concept of an automated test. Then I discuss the different\nkinds of tests that you’ll need to write. After that, I discuss the test pyramid, which\ndescribes the relative proportions of the different types of tests that you should write.\n \n",
      "content_length": 2890,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 325,
      "content": "295\nTesting strategies for microservice architectures\nWRITING AUTOMATED TESTS\nAutomated tests are usually written using a testing framework. JUnit, for example, is a\npopular Java testing framework. Figure 9.2 shows the structure of an automated test.\nEach test is implemented by a test method, which belongs to a test class.\nAn automated test typically consists of four phases (http://xunitpatterns.com/\nFour%20Phase%20Test.html):\n1\nSetup—Initialize the test fixture, which consists of the SUT and its dependen-\ncies, to the desired initial state. For example, create the class under test and ini-\ntialize it to the state required for it to exhibit the desired behavior.\n2\nExercise—Invoke the SUT—for example, invoke a method on the class under test.\n3\nVerify—Make assertions about the invocation’s outcome and the state of the\nSUT. For example, verify the method’s return value and the new state of the class\nunder test.\nSystem Under\nTest (SUT)\nTest\nTest suite\nVeriﬁes behavior of\nFigure 9.1\nThe goal of a test is to \nverify the behavior of the system \nunder test. An SUT might be as \nsmall as a class or as large as an \nentire application.\nTest class\nSetup\nExecute\nVerify\nTeardown\nTest method\nTest method\nTest runner\nSUT\nFixture\nExecutes\nTest method\nFigure 9.2\nEach automated test is implemented by a test method, which belongs to a test class. A \ntest consists of four phases: setup, which initializes the test fixture, which is everything required to \nrun the test; execute, which invokes the SUT; verify, which verifies the outcome of the test; and \nteardown, which cleans up the test fixture.\n \n",
      "content_length": 1601,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 326,
      "content": "296\nCHAPTER 9\nTesting microservices: Part 1\n4\nTeardown—Clean up the test fixture, if necessary. Many tests omit this phase,\nbut some types of database test will, for example, roll back a transaction initi-\nated by the setup phase.\nIn order to reduce code duplication and simplify tests, a test class might have setup\nmethods that are run before a test method, and teardown methods that are run after-\nwards. A test suite is a set of test classes. The tests are executed by a test runner. \nTESTING USING MOCKS AND STUBS\nAn SUT often has dependencies. The trouble with dependencies is that they can com-\nplicate and slow down tests. For example, the OrderController class invokes Order-\nService, which ultimately depends on numerous other application services and\ninfrastructure services. It wouldn’t be practical to test the OrderController class by\nrunning a large portion of the system. We need a way to test an SUT in isolation.\n The solution, as figure 9.3 shows, is to replace the SUT’s dependencies with test\ndoubles. A test double is an object that simulates the behavior of the dependency.\nThere are two types of test doubles: stubs and mocks. The terms stubs  and mocks  are\noften used interchangeably, although they have slightly different behavior. A stub is a\ntest double that returns values to the SUT. A mock is a test double that a test uses to ver-\nify that the SUT correctly invokes a dependency. Also, a mock is often a stub.\n Later on in this chapter, you’ll see examples of test doubles in action. For example,\nsection 9.2.5 shows how to test the OrderController class in isolation by using a test\ndouble for the OrderService class. In that example, the OrderService test double is\nimplemented using Mockito, a popular mock object framework for Java. Chapter 10\nshows how to test Order Service using test doubles for the other services that it invokes.\nThose test doubles respond to command messages sent by Order Service.\n Let’s now look at the different types of tests. \nSlow, complex\ntest\nTests\nReplaced with\nTests\nSystem Under\nTest (SUT)\nDependency\nFaster, simpler\ntest\nSystem Under\nTest (SUT)\nTest double\nFigure 9.3\nReplacing a dependency with a test double enables the SUT to \nbe tested in isolation. The test is simpler and faster.\n \n",
      "content_length": 2260,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 327,
      "content": "297\nTesting strategies for microservice architectures\nTHE DIFFERENT TYPES OF TESTS\nThere are many different types of tests. Some tests, such as performance tests and\nusability tests, verify that the application satisfies its quality of service requirements. In\nthis chapter, I focus on automated tests that verify the functional aspects of the appli-\ncation or service. I describe how to write four different types of tests:\nUnit tests—Test a small part of a service, such as a class.\nIntegration tests—Verify that a service can interact with infrastructure services\nsuch as databases and other application services.\nComponent tests—Acceptance tests for an individual service.\nEnd-to-end tests—Acceptance tests for the entire application.\nThey differ primarily in scope. At one end of the spectrum are unit tests, which verify\nbehavior of the smallest meaningful program element. For an object-oriented lan-\nguage such as Java, that’s a class. At the other end of the spectrum are end-to-end\ntests, which verify the behavior of an entire application. In the middle are component\ntests, which test individual services. Integration tests, as you’ll see in the next chapter,\nhave a relatively small scope, but they’re more complex than pure unit tests. Scope is\nonly one way of characterizing tests. Another way is to use the test quadrant.\nUSING THE TEST QUADRANT TO CATEGORIZE TESTS\nA good way to categorize tests is Brian Marick’s test quadrant (www.exampler.com/old-\nblog/2003/08/21/#agile-testing-project-1). The test quadrant, shown in figure 9.4,\ncategorizes tests along two dimensions:\nWhether the test is business facing or technology facing—A business-facing test is\ndescribed using the terminology of a domain expert, whereas a technology-facing\ntest is described using the terminology of developers and the implementation.\nWhether the goal of the test is to support programming or critique the application—Devel-\nopers use tests that support programming as part of their daily work. Tests that\ncritique the application aim to identify areas that need improvement.\nCompile-time unit tests\nTesting is an integral part of development. The modern development workflow is to\nedit code, then run tests. Moreover, if you’re a Test-Driven Development (TDD) prac-\ntitioner, you develop a new feature or fix a bug by first writing a failing test and then\nwriting the code to make it pass. Even if you’re not a TDD adherent, an excellent way\nto fix a bug is to write a test that reproduces the bug and then write the code that\nfixes it.\nThe tests that you run as part of this workflow are known as compile-time tests. In a\nmodern IDE, such as IntelliJ IDEA or Eclipse, you typically don’t compile your code as\na separate step. Rather, you use a single keystroke to compile the code and run the\ntests. In order to stay in the flow, these tests need to execute quickly—ideally, no\nmore than a few seconds. \n \n",
      "content_length": 2911,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 328,
      "content": "298\nCHAPTER 9\nTesting microservices: Part 1\nThe test quadrant defines four different categories of tests:\nQ1—Support programming/technology facing: unit and integration tests\nQ2—Support programming/business facing: component and end-to-end test\nQ3—Critique application/business facing: usability and exploratory testing\nQ4—Critique application/technology facing: nonfunctional acceptance tests such\nas performance tests\nThe test quadrant isn’t the only way of organizing tests. There’s also the test pyramid,\nwhich provides guidance on how many tests of each type to write. \nUSING THE TEST PYRAMID AS A GUIDE TO FOCUSING YOUR TESTING EFFORTS\nWe must write different kinds of tests in order to be confident that our application\nworks. The challenge, though, is that the execution time and complexity of a test\nincrease with its scope. Also, the larger the scope of a test and the more moving parts\nit has, the less reliable it becomes. Unreliable tests are almost as bad as no tests,\nbecause if you can’t trust a test, you’re likely to ignore failures.\n On one end of the spectrum are unit tests for individual classes. They’re fast to\nexecute, easy to write, and reliable. At the other end of the spectrum are end-to-end\ntests for the entire application. These tend to be slow, difficult to write, and often\nunreliable because of their complexity. Because we don’t have unlimited budget for\ndevelopment and testing, we want to focus on writing tests that have small scope with-\nout compromising the effectiveness of the test suite.\n The test pyramid, shown in figure 9.5, is a good guide (https://martinfowler.com/\nbliki/TestPyramid.html). At the base of the pyramid are the fast, simple, and reliable\nunit tests. At the top of the pyramid are the slow, complex, and brittle end-to-end tests.\nLike the USDA food pyramid, although more useful and less controversial (https://en\n.wikipedia.org/wiki/History_of_USDA_nutrition_guides), the test pyramid describes\nthe relative proportions of each type of test.\n The key idea of the test pyramid is that as we move up the pyramid we should write\nfewer and fewer tests. We should write lots of unit tests and very few end-to-end tests.\nQ2 AUTOMATED\nQ3 MANUAL\nBusiness facing\nTechnology facing\nSupport programming\nCritique project\nQ1 AUTOMATED\nQ4 MANUAL/\nAUTOMATED\nFunctional/\nacceptance tests\nExploratory\ntesting, usability\ntesting\nUnit,\nintegration,\ncomponent\nNon-functional\nacceptance tests:\nperformance\nand more\nFigure 9.4\nThe test quadrant categorizes tests along \ntwo dimensions. The first dimension is whether a test \nis business facing or technology facing. The second is \nwhether the purpose of the test is to support \nprogramming or critique the application.\n \n",
      "content_length": 2717,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 329,
      "content": "299\nTesting strategies for microservice architectures\nAs you’ll see in this chapter, I describe a strategy that emphasizes testing the pieces of\na service. It even minimizes the number of component tests, which test an entire service.\n It’s clear how to test individual microservices such as Consumer Service, which\ndon’t depend on any other services. But what about services such as Order Service,\nthat do depend on numerous other services? And how can we be confident that the\napplication as a whole works? This is the key challenge of testing applications that\nhave a microservice architecture. The complexity of testing has moved from the\nindividual services to the interactions between them. Let’s look at how to tackle this\nproblem. \n9.1.2\nThe challenge of testing microservices\nInterprocess communication plays a much more important role in a microservices-\nbased application than in a monolithic application. A monolithic application might\ncommunicate with a few external clients and services. For example, the monolithic\nversion of the FTGO application uses a few third-party web services, such as Stripe\nfor payments, Twilio for messaging, and Amazon SES for email, which have stable\nAPIs. Any interaction between the modules of the application is through program-\nming language-based APIs. Interprocess communication is very much on the edge\nof the application.\n In contrast, interprocess communication is central to microservice architecture. A\nmicroservices-based application is a distributed system. Teams are constantly develop-\ning their services and evolving their APIs. It’s essential that developers of a service\nwrite tests that verify that their service interacts with its dependencies and clients.\n As described in chapter 3, services communicate with each other using a variety\nof interaction styles and IPC mechanisms. Some services use request/response-style\ninteraction that’s implemented using a synchronous protocol, such as REST or gRPC.\nEnd-to-end\nSlow, brittle, costly\nFast, reliable, cheap\nComponent\nIntegration\nUnit\nAcceptance tests for\nan application\nAcceptance tests\nfor a service\nVerify that a service\ncommunicates with\nits dependencies\nTest the business logic\nFigure 9.5\nThe test pyramid describes the relative proportions of each type of test that \nyou need to write. As you move up the pyramid, you should write fewer and fewer tests.\n \n",
      "content_length": 2376,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 330,
      "content": "300\nCHAPTER 9\nTesting microservices: Part 1\nOther services interact through request/asynchronous reply or publish/subscribe\nusing asynchronous messaging. For instance, figure 9.6 shows how some of the ser-\nvices in the FTGO application communicate. Each arrow points from a consumer ser-\nvice to a producer service.\nThe arrow points in the direction of the dependency, from the consumer of the API\nto the provider of the API. The assumptions that a consumer makes about an API\ndepend on the nature of the interaction:\nREST client  service—The API gateway routes requests to services and imple-\nments API composition.\nDomain event consumer  publisher—Order History Service consumes events pub-\nlished by Order Service.\nCommand message requestor  replier—Order Service sends command messages\nto various services and consumes the replies.\nREST client\nREST service\nKey\nSubscriber\nDomain event\npublisher\n(Command message)\nrequestor\nReplier\nE\nE\nE\nE\nE\nC\nC\nC\nC\nRestaurant\nService\nConsumer\nService\nOrder History\nService\nDelivery\nService\nAPI\ngateway\nInvokes services\nusing HTTP\nSubscribes to\norder* events\nOrder Service saga\nsends commands\nto various services.\nAccounting\nService\nOrder\nService\nKitchen\nService\nFigure 9.6\nSome of the interservice communication in the FTGO application. Each arrow points \nfrom a consumer service to a producer service.\n \n",
      "content_length": 1349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 331,
      "content": "301\nTesting strategies for microservice architectures\nEach interaction between a pair of services represents an agreement or contract\nbetween the two services. Order History Service and Order Service must, for exam-\nple, agree on the event message structure and the channel that they’re published to.\nSimilarly, the API gateway and the services must agree on the REST API endpoints.\nAnd Order Service and each service that it invokes using asynchronous request/\nresponse must agree on the command channel and the format of the command and\nreply messages.\n As a developer of a service, you need to be confident that the services you consume\nhave stable APIs. Similarly, you don’t want to unintentionally make breaking changes\nto your service’s API. For example, if you’re working on Order Service, you want to be\nsure that the developers of your service’s dependencies, such as Consumer Service and\nKitchen Service, don’t change their APIs in ways that are incompatible with your ser-\nvice. Similarly, you must ensure that you don’t change the Order Services’s API in a\nway that breaks the API Gateway or Order History Service.\n One way to verify that two services can interact is to run both services, invoke an API\nthat triggers the communication, and verify that it has the expected outcome. This will\ncertainly catch integration problems, but it’s basically an end-to-end. The test likely\nwould need to run numerous other transitive dependencies of those services. A test\nmight also need to invoke complex, high-level functionality such as business logic, even\nif its goal is to test relatively low-level IPC. It’s best to avoid writing end-to-end tests like\nthese. Somehow, we need to write faster, simpler, and more reliable tests that ideally test\nservices in isolation. The solution is to use what’s known as consumer-driven contract testing.\nCONSUMER-DRIVEN CONTRACT TESTING\nImagine that you’re a member of the team developing API Gateway, described in chap-\nter 8. The API Gateway’s OrderServiceProxy invokes various REST endpoints, includ-\ning the GET /orders/{orderId} endpoint. It’s essential that we write tests that verify that\nAPI Gateway and Order Service agree on an API. In the terminology of consumer con-\ntract testing, the two services participate in a consumer-provider relationship. API Gateway is\na consumer, and Order Service is a provider. A consumer contract test is an integration\ntest for a provider, such as Order Service, that verifies that its API matches the expecta-\ntions of a consumer, such as API Gateway.\n A consumer contract test focuses on verifying that the “shape” of a provider’s API\nmeets the consumer’s expectations. For a REST endpoint, a contract test verifies that\nthe provider implements an endpoint that\nHas the expected HTTP method and path\nAccepts the expected headers, if any\nAccepts a request body, if any\nReturns a response with the expected status code, headers, and body\nIt’s important to remember that contract tests don’t thoroughly test the provider’s\nbusiness logic. That’s the job of unit tests. Later on, you’ll see that consumer contract\ntests for a REST API are in fact mock controller tests.\n \n",
      "content_length": 3164,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 332,
      "content": "302\nCHAPTER 9\nTesting microservices: Part 1\n The team that develops the consumer writes a contract test suite and adds it (for\nexample, via a pull request) to the provider’s test suite. The developers of other ser-\nvices that invoke Order Service also contribute a test suite, as shown in figure 9.7.\nEach test suite will test those aspects of Order Service’s API that are relevant to each\nconsumer. The test suite for Order History Service, for example, verifies that Order\nService publishes the expected events.\nThese test suites are executed by the deployment pipeline for Order Service. If a con-\nsumer contract test fails, that failure tells the producer team that they’ve made a break-\ning change to the API. They must either fix the API or talk to the consumer team.\nConsumer-driven contract tests typically use testing by example. The interaction\nbetween a consumer and provider is defined by a set of examples, known as contracts.\nEach contract consists of example messages that are exchanged during one interaction.\nPattern: Consumer-driven contract test\nVerify that a service meets the expectations of its clients See http://microser-\nvices.io/patterns/testing/service-integration-contract-test.html.\nAPI gateway team\nWrites\nOrder Service deployment pipeline\nOrder\nService\nAPI gateway -\nOrder Service\ncontract test\nsuite\nOrder History Service team\nWrites\nTests\nTests\nTests\nOrder History\nService - Order\nService contract\ntest suite\n... Service team\nWrites\n... Service -\nOrder Service\ncontract test\nsuite\nFigure 9.7\nEach team that develops a service that consumes Order Service’s API contributes \na contract test suite. The test suite verifies that the API matches the consumer’s expectations. \nThis test suite, along with those contributed by other teams, is run by Order Service’s \ndeployment pipeline.\n \n",
      "content_length": 1816,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 333,
      "content": "303\nTesting strategies for microservice architectures\nFor instance, a contract for a REST API consists of an example HTTP request and\nresponse. On the surface, it may seem better to define the interaction using schemas\nwritten using, for example, OpenAPI or JSON schema. But it turns out schemas aren’t\nthat useful when writing tests. A test can validate the response using the schema but it\nstill needs to invoke the provider with an example request.\n What’s more, consumer tests also need example responses. That’s because even\nthough the focus of consumer-driven contract testing is to test a provider, contracts\nare also used to verify that the consumer conforms to the contract. For instance, a\nconsumer-side contract test for a REST client uses the contract to configure an HTTP\nstub service that verifies that the HTTP request matches the contract’s request and\nsends back the contract’s HTTP response. Testing both sides of interaction ensures\nthat the consumer and provider agree on the API. Later on we’ll look at examples of\nhow to write this kind of testing, but first let’s see how to write consumer contract tests\nusing Spring Cloud Contract.\nTESTING SERVICES USING SPRING CLOUD CONTRACT\nTwo popular contract testing frameworks are Spring Cloud Contract (https://cloud\n.spring.io/spring-cloud-contract/), which is a consumer contract testing framework\nfor Spring applications, and the Pact family of frameworks (https://github.com/pact-\nfoundation), which support a variety of languages. The FTGO application is a Spring\nframework-based application, so in this chapter I’m going to describe how to use\nSpring Cloud Contract. It provides a Groovy domain-specific language (DSL) for writ-\ning contracts. Each contract is a concrete example of an interaction between a con-\nsumer and a provider, such as an HTTP request and response. Spring Cloud Contract\ncode generates contract tests for the provider. It also configures mocks, such as a\nmock HTTP server, for consumer integration tests.\n Say, for example, you’re working on API Gateway and want to write a consumer\ncontract test for Order Service. Figure 9.8 shows the process, which requires you to col-\nlaborate with Order Service teams. You write contracts that define how API Gateway\ninteracts with Order Service. The Order Service team uses these contracts to test Order\nService, and you use them to test API Gateway. The sequence of steps is as follows:\n1\nYou write one or more contracts, such as the one shown in listing 9.1. Each con-\ntract consists of an HTTP request that API Gateway might send to Order Service\nand an expected HTTP response. You give the contracts, perhaps via a Git pull\nrequest, to the Order Service team.\n2\nThe Order Service team tests Order Service using consumer contract tests,\nwhich Spring Cloud Contract code generates from contracts.\nPattern: Consumer-side contract test\nVerify that the client of a service can communicate with the service. See https://\nmicroservices.io/patterns/testing/consumer-side-contract-test.html. \n \n",
      "content_length": 3026,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 334,
      "content": "304\nCHAPTER 9\nTesting microservices: Part 1\n3\nThe Order Service team publishes the contracts that tested Order Service to a\nMaven repository.\n4\nYou use the published contracts to write tests for API Gateway.\nBecause you test API Gateway using the published contracts, you can be confident that\nit works with the deployed Order Service.\n The contracts are the key part of this testing strategy. The following listing shows an\nexample Spring Cloud Contract. It consists of an HTTP request and an HTTP response.\norg.springframework.cloud.contract.spec.Contract.make {\nrequest {\n                   \nmethod 'GET'\nurl '/orders/1223232'\n}\nresponse {\n       \nstatus 200\nheaders {\nheader('Content-Type': 'application/json;charset=UTF-8')\n}\nbody(\"{ ... }\")\n}\n}\nListing 9.1\nA contract that describes how API Gateway invokes Order Service\nWrites\nCode generated\nfrom\nContract.make {\nrequest {..}\nresponse {...}\n}\n}\nOrder Service\nconsumer\ncontract tests\nAPI gateway\nAPI gateway team\nReads\nTests\nDevelops\nTests\nDevelops\nPublishes\nPublished\ncontract\nMaven repository\nOrder\nService\nOrder Service team\nAPI gateway\nintegration test\nFigure 9.8\nThe API Gateway team writes the contracts. The Order Service team \nuses those contracts to test Order Service and publishes them to a repository. The \nAPI Gateway team uses the published contracts to test API Gateway.\nThe HTTP request’s \nmethod and path\nThe HTTP response’s status \ncode, headers, and body\n \n",
      "content_length": 1432,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 335,
      "content": "305\nTesting strategies for microservice architectures\nThe request element is an HTTP request for the REST endpoint GET /orders/\n{orderId}. The response element is an HTTP response that describes an Order\nexpected by API Gateway. The Groovy contracts are part of the provider’s code base.\nEach consumer team writes contracts that describe how their service interacts with\nthe provider and gives them, perhaps via a Git pull request, to the provider team.\nThe provider team is responsible for packaging the contracts as a JAR and publish-\ning them to a Maven repository. The consumer-side tests download the JAR from the\nrepository.\n Each contract’s request and response play dual roles of test data and the specifi-\ncation of expected behavior. In a consumer-side test, the contract is used to config-\nure a stub, which is similar to a Mockito mock object and simulates the behavior of\nOrder Service. It enables API Gateway to be tested without running Order Service.\nIn the provider-side test, the generated test class invokes the provider with the con-\ntract’s request and verifies that it returns a response that matches the contract’s\nresponse. The next chapter discusses the details of how to use Spring Cloud Con-\ntract, but now we’re going to look at how to use consumer contract testing for mes-\nsaging APIs. \nCONSUMER CONTRACT TESTS FOR MESSAGING APIS\nA REST client isn’t the only kind of consumer that has expectations of a provider’s API.\nServices that subscribe to domain events and use asynchronous request/response-based\ncommunication are also consumers. They consume some other service’s messaging\nAPI, and make assumptions about the nature of that API. We must also write con-\nsumer contract tests for these services.\n Spring Cloud Contract also provides support for testing messaging-based interac-\ntions. The structure of a contract and how it’s used by the tests depend on the type of\ninteraction. A contract for domain event publishing consists of an example domain\nevent. A provider test causes the provider to emit an event and verifies that it matches\nthe contract’s event. A consumer test verifies that the consumer can handle that event.\nIn the next chapter, I describe an example test.\n A contract for an asynchronous request/response interaction is similar to an\nHTTP contract. It consists of a request message and a response message. A provider\ntest invokes the API with the contract’s request message and verifies that the response\nmatches the contract’s response. A consumer test uses the contract to configure a stub\nsubscriber, which listens for the contract’s request message and replies with the speci-\nfied response. The next chapter discusses an example test. But first we’ll take a look at\nthe deployment pipeline, which runs these and other tests. \n9.1.3\nThe deployment pipeline\nEvery service has a deployment pipeline. Jez Humble’s book, Continuous Delivery\n(Addison-Wesley, 2010) describes a deployment pipeline as the automated process of get-\nting code from the developer’s desktop into production. As figure 9.9 shows, it consists\n \n",
      "content_length": 3075,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 336,
      "content": "306\nCHAPTER 9\nTesting microservices: Part 1\nof a series of stages that execute test suites, followed by a stage that releases or deploys\nthe service. Ideally, it’s fully automated, but it might contain manual steps. A deploy-\nment pipeline is often implemented using a Continuous Integration (CI) server, such\nas Jenkins.\nAs code flows through the pipeline, the test suites subject it to increasingly more thor-\nough testing in environments that are more production like. At the same time, the\nexecution time of each test suite typically grows. The idea is to provide feedback about\ntest failures as rapidly as possible.\n The example deployment pipeline shown in figure 9.9 consists of the following\nstages:\nPre-commit tests stage—Runs the unit tests. This is executed by the developer\nbefore committing their changes.\nCommit tests stage—Compiles the service, runs the unit tests, and performs static\ncode analysis.\nIntegration tests stage—Runs the integration tests.\nComponent tests stage—Runs the component tests for the service.\nDeploy stage—Deploys the service into production.\nThe CI server runs the commit stage when a developer commits a change. It executes\nextremely quickly, so it provides rapid feedback about the commit. The later stages\ntake longer to run, providing less immediate feedback. If all the tests pass, the final\nstage is when this pipeline deploys it into production.\n In this example, the deployment pipeline is fully automated all the way from com-\nmit to deployment. There are, however, situations that require manual steps. For\nexample, you might need a manual testing stage, such as a staging environment. In\nsuch a scenario, the code progresses to the next stage when a tester clicks a button to\nindicate that it was successful. Alternatively, a deployment pipeline for an on-premise\nPre-commit\ntests\nSlow feedback\nFast feedback\nProduction\nready\nNot production\nready\nCommit\ntests\nstage\nDeployment pipeline\nIntegration\ntests\nstage\nComponent\ntests\nstage\nProduction\nenvironment\nDeploy\nstage\nFigure 9.9\nAn example deployment pipeline for Order Service. It consists of a series of stages. \nThe pre-commit tests are run by the developer prior to committing their code. The remaining stages \nare executed by an automated tool, such as the Jenkins CI server.\n \n",
      "content_length": 2289,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 337,
      "content": "307\nWriting unit tests for a service\nproduct would release the new version of the service. Later on, the released services\nwould be packaged into a product release and shipped to customers.\n Now that we’ve looked at the organization of the deployment pipeline and when it\nexecutes the different types of tests, let’s head to the bottom of the test pyramid and\nlook at how to write unit tests for a service. \n9.2\nWriting unit tests for a service\nImagine that you want to write a test that verifies that the FTGO application’s Order\nService correctly calculates the subtotal of an Order. You could write tests that run\nOrder Service, invoke its REST API to create an Order, and check that the HTTP\nresponse contains the expected values. The drawback of this approach is that not only\nis the test complex, it’s also slow. If these tests were the compile-time tests for the\nOrder class, you’d waste a lot of time waiting for it to finish. A much more productive\napproach is to write unit tests for the Order class.\n As figure 9.10 shows, unit tests are the lowest level of the test pyramid. They’re\ntechnology-facing tests that support development. A unit test verifies that a unit, which\nis a very small part of a service, works correctly. A unit is typically a class, so the goal of\nunit testing is to verify that it behaves as expected.\nEnd-to-end\nComponent\nIntegration\nUnit\nStub/mock\ndependency 1\nStub/mock\ndependency 2\nStub/mock\ndependency\n...\nDependency 1\nDependency 2\nDependency\n...\nSolitary\nunit test\nTests\nClass\nSocial\nunit test\nTests\nClass\nFigure 9.10\nUnit tests are the base of the pyramid. They’re fast running, easy to write, and reliable. \nA solitary unit test tests a class in isolation, using mocks or stubs for its dependencies. A sociable \nunit test tests a class and its dependencies.\n \n",
      "content_length": 1802,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 338,
      "content": "308\nCHAPTER 9\nTesting microservices: Part 1\nThere are two types of unit tests (https://martinfowler.com/bliki/UnitTest.html):\nSolitary unit test—Tests a class in isolation using mock objects for the class’s\ndependencies\nSociable unit test—Tests a class and its dependencies\nThe responsibilities of the class and its role in the architecture determine which type\nof test to use. Figure 9.11 shows the hexagonal architecture of a typical service and the\ntype of unit test that you’ll typically use for each kind of class. Controller and service\nclasses are often tested using solitary unit tests. Domain objects, such as entities and\nvalue objects, are typically tested using sociable unit tests.\n«Message Channel»\n«Message Channel»\nPOST/something\nGET/something/id\nDomain logic\nService\nEntity\nSolitary\nunit test\nSociable\nunit test\nValue\nobject\nSaga\nInbound\nmessage\nadapter\nOutbound\nmessage\nadapter\nDatabase\nadapter\nDatabase\nRepository\nController\nSolitary\nunit test\nFigure 9.11\nThe responsibilities of a class determine whether to use a solitary or sociable unit test.\n \n",
      "content_length": 1070,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 339,
      "content": "309\nWriting unit tests for a service\nThe typical testing strategy for each class is as follows:\nEntities, such as Order, which as described in chapter 5 are objects with per-\nsistent identity, are tested using sociable unit tests.\nValue objects, such as Money, which as described in chapter 5 are objects that are\ncollections of values, are tested using sociable unit tests.\nSagas, such as CreateOrderSaga, which as described in chapter 4 maintain data\nconsistency across services, are tested using sociable unit tests.\nDomain services, such as OrderService, which as described in chapter 5 are\nclasses that implement business logic that doesn’t belong in entities or value\nobjects, are tested using solitary unit tests.\nControllers, such as OrderController, which handle HTTP requests, are tested\nusing solitary unit tests.\nInbound and outbound messaging gateways are tested using solitary unit tests.\nLet’s begin by looking at how to test entities.\n9.2.1\nDeveloping unit tests for entities\nThe following listing shows an excerpt of OrderTest class, which implements the unit\ntests for the Order entity. The class has an @Before setUp() method that creates an Order\nbefore running each test. Its @Test methods might further initialize Order, invoke one of\nits methods, and then make assertions about the return value and the state of Order.\npublic class OrderTest {\nprivate ResultWithEvents<Order> createResult;\nprivate Order order;\n@Before\npublic void setUp() throws Exception {\ncreateResult = Order.createOrder(CONSUMER_ID, AJANTA_ID, CHICKEN_VINDALOO\n_LINE_ITEMS);\norder = createResult.result;\n}\n@Test\npublic void shouldCalculateTotal() {\nassertEquals(CHICKEN_VINDALOO_PRICE.multiply(CHICKEN_VINDALOO_QUANTITY),\norder.getOrderTotal());\n}\n...\n}\nThe @Test shouldCalculateTotal() method verifies that Order.getOrderTotal()\nreturns the expected value. Unit tests thoroughly test the business logic. They are\nListing 9.2\nA simple, fast-running unit test for the Order entity\n \n",
      "content_length": 1982,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 340,
      "content": "310\nCHAPTER 9\nTesting microservices: Part 1\nsociable unit tests for the Order class and its dependencies. You can use them as\ncompile-time tests because they execute extremely quickly. The Order class relies on\nthe Money value object, so it’s important to test that class as well. Let’s see how to do that. \n9.2.2\nWriting unit tests for value objects\nValue objects are immutable, so they tend to be easy to test. You don’t have to worry\nabout side effects. A test for a value object typically creates a value object in a particu-\nlar state, invokes one of its methods, and makes assertions about the return value. List-\ning 9.3 shows the tests for the Money value object, which is a simple class that\nrepresents a money value. These tests verify the behavior of the Money class’s methods,\nincluding add(), which adds two Money objects, and multiply(), which multiplies a\nMoney object by an integer. They are solitary tests because the Money class doesn’t\ndepend on any other application classes.\npublic class MoneyTest {\nprivate final int M1_AMOUNT = 10;\nprivate final int M2_AMOUNT = 15;\nprivate Money m1 = new Money(M1_AMOUNT);\nprivate Money m2 = new Money(M2_AMOUNT);\n@Test\npublic void shouldAdd() {\n         \nassertEquals(new Money(M1_AMOUNT + M2_AMOUNT), m1.add(m2));\n}\n@Test\npublic void shouldMultiply() {     \nint multiplier = 12;\nassertEquals(new Money(M2_AMOUNT * multiplier), m2.multiply(multiplier));\n}\n...\n}\nEntities and value objects are the building blocks of a service’s business logic. But\nsome business logic also resides in the service’s sagas and services. Let’s look at how to\ntest those. \n9.2.3\nDeveloping unit tests for sagas\nA saga, such as the CreateOrderSaga class, implements important business logic, so\nneeds to be tested. It’s a persistent object that sends command messages to saga partic-\nipants and processes their replies. As described in chapter 4, CreateOrderSaga\nexchanges command/reply messages with several services, such as Consumer Service\nand Kitchen Service. A test for this class creates a saga and verifies that it sends the\nListing 9.3\nA simple, fast-running test for the Money value object\nVerify that two \nMoney objects can \nbe added together.\nVerify that a Money \nobject can be multiplied \nby an integer.\n \n",
      "content_length": 2255,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 341,
      "content": "311\nWriting unit tests for a service\nexpected sequence of messages to the saga participants. One test you need to write is\nfor the happy path. You must also write tests for the various scenarios where the saga\nrolls back because a saga participant sent back a failure message.\n One approach would be to write tests that use a real database and message broker\nalong with stubs to simulate the various saga participants. For example, a stub for\nConsumer Service would subscribe to the consumerService command channel and\nsend back the desired reply message. But tests written using this approach would be\nquite slow. A much more effective approach is to write tests that mock those classes\nthat interact with the database and message broker. That way, we can focus on testing\nthe saga’s core responsibility.\n Listing 9.4 shows a test for CreateOrderSaga. It’s a sociable unit test that tests the\nsaga class and its dependencies. It’s written using the Eventuate Tram Saga testing\nframework (https://github.com/eventuate-tram/eventuate-tram-sagas). This frame-\nwork provides an easy-to-use DSL that abstracts away the details of interacting with\nsagas. With this DSL, you can create a saga and verify that it sends the correct com-\nmand messages. Under the covers, the Saga testing framework configures the Saga\nframework with mocks for the database and messaging infrastructure.\npublic class CreateOrderSagaTest {\n@Test\npublic void shouldCreateOrder() {\ngiven()\n.saga(new CreateOrderSaga(kitchenServiceProxy),     \nnew CreateOrderSagaState(ORDER_ID,\nCHICKEN_VINDALOO_ORDER_DETAILS)).\nexpect().\n              \ncommand(new ValidateOrderByConsumer(CONSUMER_ID, ORDER_ID,\nCHICKEN_VINDALOO_ORDER_TOTAL)).\nto(ConsumerServiceChannels.consumerServiceChannel).\nandGiven().\nsuccessReply().\n               \nexpect().\ncommand(new CreateTicket(AJANTA_ID, ORDER_ID, null)).   \nto(KitchenServiceChannels.kitchenServiceChannel);\n}\n@Test\npublic void shouldRejectOrderDueToConsumerVerificationFailed() {\ngiven()\n.saga(new CreateOrderSaga(kitchenServiceProxy),\nnew CreateOrderSagaState(ORDER_ID,\nCHICKEN_VINDALOO_ORDER_DETAILS)).\nexpect().\ncommand(new ValidateOrderByConsumer(CONSUMER_ID, ORDER_ID,\nCHICKEN_VINDALOO_ORDER_TOTAL)).\nto(ConsumerServiceChannels.consumerServiceChannel).\nandGiven().\nListing 9.4\nA simple, fast-running unit test for CreateOrderSaga\nCreate\nthe saga.\nVerify that it sends \na ValidateOrderBy-\nConsumer message \nto Consumer Service.\nSend a Success reply \nto that message.\nVerify that it sends \na CreateTicket \nmessage to \nKitchen Service.\n \n",
      "content_length": 2544,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 342,
      "content": "312\nCHAPTER 9\nTesting microservices: Part 1\nfailureReply().\n            \nexpect().\ncommand(new RejectOrderCommand(ORDER_ID)).\nto(OrderServiceChannels.orderServiceChannel);\n      \n}\n}\nThe @Test shouldCreateOrder() method tests the happy path. The @Test should-\nRejectOrderDueToConsumerVerificationFailed() method tests the scenario where\nConsumer Service rejects the order. It verifies that CreateOrderSaga sends a Reject-\nOrderCommand to compensate for the consumer being rejected. The CreateOrder-\nSagaTest class has methods that test other failure scenarios.\n Let’s now look at how to test domain services. \n9.2.4\nWriting unit tests for domain services\nThe majority of a service’s business logic is implemented by the entities, value objects,\nand sagas. Domain service classes, such as the OrderService class, implement the\nremainder. This class is a typical domain service class. Its methods invoke entities and\nrepositories and publish domain events. An effective way to test this kind of class is to\nuse a mostly solitary unit test, which mocks dependencies such as repositories and\nmessaging classes.\n Listing 9.5 shows the OrderServiceTest class, which tests OrderService. It defines\nsolitary unit tests, which use Mockito mocks for the service’s dependencies. Each test\nimplements the test phases as follows:\n1\nSetup—Configures the mock objects for the service’s dependencies\n2\nExecute—Invokes a service method\n3\nVerify—Verifies that the value returned by the service method is correct and that\nthe dependencies have been invoked correctly\npublic class OrderServiceTest {\nprivate OrderService orderService;\nprivate OrderRepository orderRepository;\nprivate DomainEventPublisher eventPublisher;\nprivate RestaurantRepository restaurantRepository;\nprivate SagaManager<CreateOrderSagaState> createOrderSagaManager;\nprivate SagaManager<CancelOrderSagaData> cancelOrderSagaManager;\nprivate SagaManager<ReviseOrderSagaData> reviseOrderSagaManager;\n@Before\npublic void setup() {\norderRepository = mock(OrderRepository.class);         \neventPublisher = mock(DomainEventPublisher.class);\nrestaurantRepository = mock(RestaurantRepository.class);\nListing 9.5\nA simple, fast-running unit test for the OrderService class\nSend a failure \nreply indicating \nthat Consumer \nService rejected \nOrder.\nVerify that the saga sends\na RejectOrderCommand\nmessage to Order Service.\nCreate Mockito \nmocks for \nOrderService’s \ndependencies.\n \n",
      "content_length": 2421,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 343,
      "content": "313\nWriting unit tests for a service\ncreateOrderSagaManager = mock(SagaManager.class);\ncancelOrderSagaManager = mock(SagaManager.class);\nreviseOrderSagaManager = mock(SagaManager.class);\norderService = new OrderService(orderRepository, eventPublisher,  \nrestaurantRepository, createOrderSagaManager,\ncancelOrderSagaManager, reviseOrderSagaManager);\n}\n@Test\npublic void shouldCreateOrder() {\nwhen(restaurantRepository\n       \n.findById(AJANTA_ID)).thenReturn(Optional.of(AJANTA_RESTAURANT_);\nwhen(orderRepository.save(any(Order.class))).then(invocation -> {  \nOrder order = (Order) invocation.getArguments()[0];\norder.setId(ORDER_ID);\nreturn order;\n});\nOrder order = orderService.createOrder(CONSUMER_ID,\n   \nAJANTA_ID, CHICKEN_VINDALOO_MENU_ITEMS_AND_QUANTITIES);\nverify(orderRepository).save(same(order));        \nverify(eventPublisher).publish(Order.class, ORDER_ID,     \nsingletonList(\nnew OrderCreatedEvent(CHICKEN_VINDALOO_ORDER_DETAILS)));\nverify(createOrderSagaManager)\n                  \n.create(new CreateOrderSagaState(ORDER_ID,\nCHICKEN_VINDALOO_ORDER_DETAILS),\nOrder.class, ORDER_ID);\n}\n}\nThe setUp() method creates an OrderService injected with mock dependencies.\nThe @Test shouldCreateOrder() method verifies that OrderService.createOrder()\ninvokes OrderRepository to save the newly created Order, publishes an OrderCreated-\nEvent, and creates a CreateOrderSaga.\n Now that we’ve seen how to unit test the domain logic classes, let’s look at how to\nunit test the adapters that interact with external systems. \n9.2.5\nDeveloping unit tests for controllers\nServices, such as Order Service, typically have one or more controllers that handle\nHTTP requests from other services and the API gateway. A controller class consists of\na set of request handler methods. Each method implements a REST API endpoint. A\nmethod’s parameters represent values from the HTTP request, such as path variables.\nIt typically invokes a domain service or a repository and returns a response object.\nCreate an OrderService injected\nwith mock dependencies.\nConfigure RestaurantRepository.findById() \nto return the Ajanta restaurant.\nConfigure OrderRepository.save()\nto set Order’s ID.\n Invoke\nOrderService\n.create().\nVerify that \nOrderService saved \nthe newly created \nOrder in the database.\nVerify that\nOrderService\npublished\nan Order-\nCreatedEvent.\nVerify that Order-\nService created a \nCreateOrderSaga.\n \n",
      "content_length": 2392,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 344,
      "content": "314\nCHAPTER 9\nTesting microservices: Part 1\nOrderController, for instance, invokes OrderService and OrderRepository. An\neffective testing strategy for controllers is solitary unit tests that mock the services\nand repositories.\n You could write a test class similar to the OrderServiceTest class to instantiate a\ncontroller class and invoke its methods. But this approach doesn’t test some import-\nant functionality, such as request routing. It’s much more effective to use a mock MVC\ntesting framework, such as Spring Mock Mvc, which is part of the Spring Framework,\nor Rest Assured Mock MVC, which builds on Spring Mock Mvc. Tests written using\none of these frameworks make what appear to be HTTP requests and make assertions\nabout HTTP responses. These frameworks enable you to test HTTP request routing\nand conversion of Java objects to and from JSON without having to make real network\ncalls. Under the covers, Spring Mock Mvc instantiates just enough of the Spring MVC\nclasses to make this possible.\nListing 9.6 shows the OrderControllerTest class, which tests Order Service’s Order-\nController. It defines solitary unit tests that use mocks for OrderController’s depen-\ndencies. It’s written using Rest Assured Mock MVC , which provides a simple DSL that\nabstracts away the details of interacting with controllers. Rest Assured makes it easy to\nsend a mock HTTP request to a controller and verify the response. OrderController-\nTest creates a controller that’s injected with Mockito mocks for OrderService and\nOrderRepository. Each test configures the mocks, makes an HTTP request, verifies that\nthe response is correct, and possibly verifies that the controller invoked the mocks.\npublic class OrderControllerTest {\nprivate OrderService orderService;\nprivate OrderRepository orderRepository;\n@Before\npublic void setUp() throws Exception {\norderService = mock(OrderService.class);\n      \norderRepository = mock(OrderRepository.class);\nAre these really unit tests?\nBecause these tests use the Spring Framework, you might argue that they’re not unit\ntests. They’re certainly more heavyweight than the unit tests I’ve described so far.\nThe Spring Mock Mvc documentation refers to these as out-of-servlet-container inte-\ngration tests (https://docs.spring.io/spring/docs/current/spring-framework-reference/\ntesting.html#spring-mvc-test-vs-end-to-end-integration-tests). Yet Rest Assured Mock\nMVC describes these tests as unit tests (https://github.com/rest-assured/rest-\nassured/wiki/Usage#spring-mock-mvc-module). Regardless of the debate over termi-\nnology, these are important tests to write.\nListing 9.6\nA simple, fast-running unit test for the OrderController class\nCreate mocks for \nOrderController’s \ndependencies.\n \n",
      "content_length": 2726,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 345,
      "content": "315\nWriting unit tests for a service\norderController = new OrderController(orderService, orderRepository);\n}\n@Test\npublic void shouldFindOrder() {\nwhen(orderRepository.findById(1L))\n.thenReturn(Optional.of(CHICKEN_VINDALOO_ORDER_);    \ngiven().\nstandaloneSetup(configureControllers(\n      \nnew OrderController(orderService, orderRepository))).\nwhen().\nget(\"/orders/1\").\n    \nthen().\nstatusCode(200).\n   \nbody(\"orderId\",\n            \nequalTo(new Long(OrderDetailsMother.ORDER_ID).intValue())).\nbody(\"state\",\nequalTo(OrderDetailsMother.CHICKEN_VINDALOO_ORDER_STATE.name())).\nbody(\"orderTotal\",\nequalTo(CHICKEN_VINDALOO_ORDER_TOTAL.asString()))\n;\n}\n@Test\npublic void shouldFindNotOrder() { ... }\nprivate StandaloneMockMvcBuilder controllers(Object... controllers) { ... }\n}\nThe shouldFindOrder() test method first configures the OrderRepository mock to\nreturn an Order. It then makes an HTTP request to retrieve the order. Finally, it\nchecks that the request was successful and that the response body contains the\nexpected data.\n Controllers aren’t the only adapters that handle requests from external systems.\nThere are also event/message handlers, so let’s talk about how to unit test those. \n9.2.6\nWriting unit tests for event and message handlers\nServices often process messages sent by external systems. Order Service, for example,\nhas OrderEventConsumer, which is a message adapter that handles domain events pub-\nlished by other services. Like controllers, message adapters tend to be simple classes\nthat invoke domain services. Each of a message adapter’s methods typically invokes a\nservice method with data from the message or event.\n We can unit test message adapters using an approach similar to the one we used\nfor unit testing controllers. Each test instances the message adapter, sends a message\nto a channel, and verifies that the service mock was invoked correctly. Behind the\nConfigure the mock \nOrderRepository to \nreturn an Order.\nConfigure \nOrderController.\nMake an\nHTTP\nrequest.\nVerify the response \nstatus code.\nVerify\nelements\nof the JSON\nresponse\nbody.\n \n",
      "content_length": 2077,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 346,
      "content": "316\nCHAPTER 9\nTesting microservices: Part 1\nscenes, though, the messaging infrastructure is stubbed, so no message broker is\ninvolved. Let’s look at how to test the OrderEventConsumer class.\n Listing 9.7 shows part of the OrderEventConsumerTest class, which tests Order-\nEventConsumer. It verifies that OrderEventConsumer routes each event to the appro-\npriate handler method and correctly invokes OrderService. The test uses the\nEventuate Tram Mock Messaging framework, which provides an easy-to-use DSL for\nwriting mock messaging tests that uses the same given-when-then format as Rest\nAssured. Each test instantiates OrderEventConsumer injected with a mock Order-\nService, publishes a domain event, and verifies that OrderEventConsumer correctly\ninvokes the service mock.\npublic class OrderEventConsumerTest {\nprivate OrderService orderService;\nprivate OrderEventConsumer orderEventConsumer;\n@Before\npublic void setUp() throws Exception {\norderService = mock(OrderService.class);\norderEventConsumer = new OrderEventConsumer(orderService);\n  \n}\n@Test\npublic void shouldCreateMenu() {\ngiven().\neventHandlers(orderEventConsumer.domainEventHandlers()).  \nwhen().\naggregate(\"net.chrisrichardson.ftgo.restaurantservice.domain.Restaurant\",\nAJANTA_ID).\npublishes(new RestaurantCreated(AJANTA_RESTAURANT_NAME,   \nRestaurantMother.AJANTA_RESTAURANT_MENU))\nthen().\nverify(() -> {\n    \nverify(orderService)\n.createMenu(AJANTA_ID,\nnew RestaurantMenu(RestaurantMother.AJANTA_RESTAURANT_MENU_ITEMS));\n})\n;\n}\n}\nThe setUp() method creates an OrderEventConsumer injected with a mock Order-\nService. The shouldCreateMenu() method publishes a RestaurantCreated event\nand verifies that OrderEventConsumer invoked OrderService.createMenu(). The\nOrderEventConsumerTest class and the other unit test classes execute extremely quickly.\nThe unit tests run in just a few seconds.\nListing 9.7\nA fast-running unit test for the OrderEventConsumer class\nInstantiate\nOrderEventConsumer with\nmocked dependencies.\nConfigure\nOrderEventConsumer\ndomain handlers.\nPublish a\nRestaurant-\nCreated\nevent.\nVerify that OrderEventConsumer \ninvoked OrderService.createMenu().\n \n",
      "content_length": 2135,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 347,
      "content": "317\nSummary\n But the unit tests don’t verify that a service, such as Order Service, properly inter-\nacts with other services. For example, the unit tests don’t verify that an Order can be\npersisted in MySQL. Nor do they verify that CreateOrderSaga sends command mes-\nsages in the right format to the right message channel. And they don’t verify that the\nRestaurantCreated event processed by OrderEventConsumer has the same structure\nas the event published by Restaurant Service. In order to verify that a service prop-\nerly interacts with other services, we must write integration tests. We also need to write\ncomponent tests that test an entire service in isolation. The next chapter discusses\nhow to conduct those types of tests, as well as end-to-end tests. \nSummary\nAutomated testing is the key foundation of rapid, safe delivery of software.\nWhat’s more, because of its inherent complexity, to fully benefit from the\nmicroservice architecture you must automate your tests.\nThe purpose of a test is to verify the behavior of the system under test (SUT). In\nthis definition, system is a fancy term that means the software element being\ntested. It might be something as small as a class, as large as the entire applica-\ntion, or something in between, such as a cluster of classes or an individual ser-\nvice. A collection of related tests form a test suite.\nA good way to simplify and speed up a test is to use test doubles. A test double is\nan object that simulates the behavior of a SUT’s dependency. There are two\ntypes of test doubles: stubs and mocks. A stub is a test double that returns values\nto the SUT. A mock is a test double that a test uses to verify that the SUT cor-\nrectly invokes a dependency.\nUse the test pyramid to determine where to focus your testing efforts for your\nservices. The majority of your tests should be fast, reliable, and easy-to-write unit\ntests. You must minimize the number of end-to-end tests, because they’re slow,\nbrittle, and time consuming to write.\n \n",
      "content_length": 2000,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 348,
      "content": "318\nTesting microservices:\nPart 2\nThis chapter builds on the previous chapter, which introduced testing concepts,\nincluding the test pyramid. The test pyramid describes the relative proportions of\nthe different types of tests that you should write. The previous chapter described\nhow to write unit tests, which are at the base of the testing pyramid. In this chapter,\nwe continue our ascent of the testing pyramid.\n This chapter begins with how to write integration tests, which are the level\nabove unit tests in the testing pyramid. Integration tests verify that a service can prop-\nerly interact with infrastructure services, such as databases, and other application\nservices. Next, I cover component tests, which are acceptance tests for services. A com-\nponent test tests a service in isolation by using stubs for its dependencies. After\nthat, I describe how to write end-to-end tests, which test a group of services or the\nThis chapter covers\nTechniques for testing services in isolation\nUsing consumer-driven contract testing to write \ntests that quickly yet reliably verify interservice \ncommunication\nWhen and how to do end-to-end testing of \napplications\n \n",
      "content_length": 1169,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 349,
      "content": "319\nWriting integration tests\nentire application. End-to-end tests are at the top of the test pyramid and should,\ntherefore, be used sparingly.\n Let’s start by taking a look at how to write integration tests.\n10.1\nWriting integration tests\nServices typically interact with other services. For example, Order Service, as fig-\nure 10.1 shows, interacts with several services. Its REST API is consumed by API Gateway,\nand its domain events are consumed by services, including Order History Service.\nOrder Service uses several other services. It persists Orders in MySQL. It also sends\ncommands to and consumes replies from several other services, such as Kitchen\nService.\nIn order to be confident that a service such as Order Service works as expected, we\nmust write tests that verify that the service can properly interact with infrastructure\nservices and other application services. One approach is to launch all the services and\ntest them through their APIs. This, however, is what’s known as end-to-end testing,\nwhich is slow, brittle, and costly. As explained in section 10.3, there’s a role for end-to-end\nOrder History\nService\nAPI\ngateway\nOrder\nhistory\nevent\nhandlers\nKitchen\nService\nOrder\nService\nKitchen\nService\ncommand\nhandler\nClass under test\nLegend\nTest\nTest\nTest\nTest\nDatabase\nOrder\naggregate\nevent\npublisher\nEvent\nchannel\nCommand\nchannel\nReply\nchannel\nProvider\nProvider\nConsumer\nConsumer\nProvider\nProvider\nOrder\ncontroller\nOrder\nRepository\nKitchen\nService\nproxy\nOrder\nService\nproxy\nFigure 10.1\nIntegration tests must verify that a service can communicate with its clients and \ndependencies. But rather than testing whole services, the strategy is to test the individual adapter \nclasses that implement the communication.\n \n",
      "content_length": 1734,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 350,
      "content": "320\nCHAPTER 10\nTesting microservices: Part 2\ntesting sometimes, but it’s at the top of the test pyramid, so you want to minimize the\nnumber of end-to-end tests.\n A much more effective strategy is to write what are known as integration tests. As fig-\nure 10.2 shows, integration tests are the layer above unit tests in the testing pyramid.\nThey verify that a service can properly interact with infrastructure services and other ser-\nvices. But unlike end-to-end tests, they don’t launch services. Instead, we use a couple of\nstrategies that significantly simplify the tests without impacting their effectiveness.\nThe first strategy is to test each of the service’s adapters, along with, perhaps, the\nadapter’s supporting classes. For example, in section 10.1.1 you’ll see a JPA per-\nsistence test that verifies that Orders are persisted correctly. Rather than test persistence\nthrough Order Service’s API, it directly tests the OrderRepository class. Similarly, in\nsection 10.1.3 you’ll see a test that verifies that Order Service publishes correctly\nstructured domain events by testing the OrderDomainEventPublisher class. The bene-\nfit of testing only a small number of classes rather than the entire service is that the\ntests are significantly simpler and faster.\n The second strategy for simplifying integration tests that verify interactions\nbetween application services is to use contracts, discussed in chapter 9. A contract is a\nconcrete example of an interaction between a pair of services. As table 10.1 shows, the\nstructure of a contract depends on the type of interaction between the services.\nTable 10.1\nThe structure of a contract depends on the type of interaction between the services.\nInteraction style\nConsumer\nProvider\nContract\nREST-based, \nrequest/response\nAPI Gateway\nOrder Service\nHTTP request and \nresponse\nPublish/subscribe\nOrder History Service\nOrder Service\nDomain event\nAsynchronous \nrequest/response\nOrder Service\nKitchen Service\nCommand message \nand reply message\nEnd-to-end\nComponent\nIntegration\nUnit\nFigure 10.2\nIntegration tests are the layer \nabove unit tests. They verify that a service \ncan communicate with its dependencies, \nwhich includes infrastructure services, such \nas the database, and application services.\n \n",
      "content_length": 2252,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 351,
      "content": "321\nWriting integration tests\nA contract consists of either one message, in the case of publish/subscribe style inter-\nactions, or two messages, in the case of request/response and asynchronous request/\nresponse style interactions.\n The contracts are used to test both the consumer and the provider, which ensures\nthat they agree on the API. They’re used in slightly different ways depending on\nwhether you’re testing the consumer or the provider:\nConsumer-side tests—These are tests for the consumer’s adapter. They use the\ncontracts to configure stubs that simulate the provider, enabling you to write\nintegration tests for a consumer that don’t require a running provider.\nProvider-side tests—These are tests for the provider’s adapter. They use the con-\ntracts to test the adapters using mocks for the adapters’s dependencies.\nLater in this section, I describe examples of these types of tests—but first let’s look at\nhow to write persistence tests.\n10.1.1 Persistence integration tests\nServices typically store data in a database. For instance, Order Service persists aggre-\ngates, such as Order, in MySQL using JPA. Similarly, Order History Service maintains\na CQRS view in AWS DynamoDB. The unit tests we wrote earlier only test in-memory\nobjects. In order to be confident that a service works correctly, we must write per-\nsistence integration tests, which verify that a service’s database access logic works as\nexpected. In the case of Order Service, this means testing the JPA repositories, such\nas OrderRepository.\n Each phase of a persistence integration test behaves as follows:\nSetup—Set up the database by creating the database schema and initializing it to\na known state. It might also begin a database transaction.\nExecute—Perform a database operation.\nVerify—Make assertions about the state of the database and objects retrieved\nfrom the database.\nTeardown—An optional phase that might undo the changes made to the database\nby, for example, rolling back the transaction that was started by the setup phase.\nListing 10.1 shows a persistent integration test for the Order aggregate and Order-\nRepository. Apart from relying on JPA to create the database schema, the persistence\nintegration tests don’t make any assumption about the state of the database. Conse-\nquently, tests don’t need to roll back the changes they make to the database, which\navoids problems with the ORM caching data changes in memory.\n@RunWith(SpringRunner.class)\n@SpringBootTest(classes = OrderJpaTestConfiguration.class)\npublic class OrderJpaTest {\nListing 10.1\nAn integration test that verifies that an Order can be persisted\n \n",
      "content_length": 2625,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 352,
      "content": "322\nCHAPTER 10\nTesting microservices: Part 2\n@Autowired\nprivate OrderRepository orderRepository;\n@Autowired\nprivate TransactionTemplate transactionTemplate;\n@Test\npublic void shouldSaveAndLoadOrder() {\nLong orderId = transactionTemplate.execute((ts) -> {\nOrder order =\nnew Order(CONSUMER_ID, AJANTA_ID, CHICKEN_VINDALOO_LINE_ITEMS);\norderRepository.save(order);\nreturn order.getId();\n});\ntransactionTemplate.execute((ts) -> {\nOrder order = orderRepository.findById(orderId).get();\nassertEquals(OrderState.APPROVAL_PENDING, order.getState());\nassertEquals(AJANTA_ID, order.getRestaurantId());\nassertEquals(CONSUMER_ID, order.getConsumerId().longValue());\nassertEquals(CHICKEN_VINDALOO_LINE_ITEMS, order.getLineItems());\nreturn null;\n});\n}\n}\nThe shouldSaveAndLoadOrder() test method executes two transactions. The first\nsaves a newly created Order in the database. The second transaction loads the Order\nand verifies that its fields are properly initialized.\n One problem you need to solve is how to provision the database that’s used in per-\nsistence integration tests. An effective solution to run an instance of the database during\ntesting is to use Docker. Section 10.2 describes how to use the Docker Compose Gradle\nplugin to automatically run services during component testing. You can use a similar\napproach to run MySQL, for example, during persistence integration testing.\n The database is only one of the external services a service interacts with. Let’s now\nlook at how to write integration tests for interservice communication between applica-\ntion services, starting with REST. \n10.1.2 Integration testing REST-based request/response style \ninteractions\nREST is a widely used interservice communication mechanism. The REST client and\nREST service must agree on the REST API, which includes the REST endpoints and\nthe structure of the request and response bodies. The client must send an HTTP\nrequest to the correct endpoint, and the service must send back the response that the\nclient expects.\n \n",
      "content_length": 2007,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 353,
      "content": "323\nWriting integration tests\n For example, chapter 8 describes how the FTGO application’s API Gateway makes\nREST API calls to numerous services, including ConsumerService, Order Service,\nand Delivery Service. The OrderService’s GET /orders/{orderId} endpoint is one\nof the endpoints invoked by the API Gateway. In order to be confident that API Gateway\nand Order Service can communicate without using an end-to-end test, we need to\nwrite integration tests.\n As stated in the preceding chapter, a good integration testing strategy is to use\nconsumer-driven contract tests. The interaction between API Gateway and GET\n/orders/{orderId} can be described using a set of HTTP-based contracts. Each con-\ntract consists of an HTTP request and an HTTP reply. The contracts are used to test\nAPI Gateway and Order Service.\n Figure 10.3 shows how to use Spring Cloud Contract to test REST-based interac-\ntions. The consumer-side API Gateway integration tests use the contracts to configure\nan HTTP stub server that simulates the behavior of Order Service. A contract’s\nrequest specifies an HTTP request from the API gateway, and the contract’s response\nspecifies the response that the stub sends back to the API gateway. Spring Cloud Con-\ntract uses the contracts to code-generate the provider-side Order Service integration\ntests, which test the controllers using Spring Mock MVC or Rest Assured Mock MVC.\nThe contract’s request specifies the HTTP request to make to the controller, and the\ncontract’s response specifies the controller’s expected response.\n The consumer-side OrderServiceProxyTest invokes OrderServiceProxy, which\nhas been configured to make HTTP requests to WireMock. WireMock is a tool for effi-\nciently mocking HTTP servers—in this test it simulates Order Service. Spring Cloud\nWiremock-\nbased HTTP\nstub server\nOrder\ncontroller\nProvider-side integration\ntest for Order Service\nConsumer-side integration\ntest forAPI gateway\nSpring Cloud\nContract\nTests\nTests\nUses\nConﬁgures\nConﬁgures\nGenerates\nReads\nHTTP\nOrderService\nProxyTest\nclass HttpTest\nextends BaseHttp {\n}\nabstract class BaseHttp {\n@Before\npublic void setup() {\nRestAssuredMockMvc\n.standaloneSetup(...);\n}\n}\nContract.make {\nrequest {..}\nresponse {...}\n}\n}\nOrderService\nProxy\nFigure 10.3\nThe contracts are used to verify that the adapter classes on both sides of the \nREST-based communication between API Gateway and Order Service conform to the contract. \nThe consumer-side tests verify that OrderServiceProxy invokes Order Service correctly. The \nprovider-side tests verify that OrderController implements the REST API endpoints correctly.\n \n",
      "content_length": 2610,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 354,
      "content": "324\nCHAPTER 10\nTesting microservices: Part 2\nContract manages WireMock and configures it to respond to the HTTP requests\ndefined by the contracts.\n On the provider side, Spring Cloud Contract generates a test class called HttpTest,\nwhich uses Rest Assured Mock MVC to test Order Service’s controllers. Test classes\nsuch as HttpTest must extend a handwritten base class. In this example, the base class\nBaseHttp instantiates OrderController injected with mock dependencies and calls\nRestAssuredMockMvc.standaloneSetup() to configure Spring MVC.\n Let’s take a closer look at how this works, starting with an example contract.\nAN EXAMPLE CONTRACT FOR A REST API\nA REST contract, such as the one shown in listing 10.2, specifies an HTTP request,\nwhich is sent by the REST client, and the HTTP response, which the client expects to\nget back from the REST server. A contract’s request specifies the HTTP method, the\npath, and optional headers. A contract’s response specifies the HTTP status code,\noptional headers, and, when appropriate, the expected body.\norg.springframework.cloud.contract.spec.Contract.make {\nrequest {\nmethod 'GET'\nurl '/orders/1223232'\n}\nresponse {\nstatus 200\nheaders {\nheader('Content-Type': 'application/json;charset=UTF-8')\n}\nbody('''{\"orderId\" : \"1223232\", \"state\" : \"APPROVAL_PENDING\"}''')\n}\n}\nThis particular contract describes a successful attempt by API Gateway to retrieve an\nOrder from Order Service. Let’s now look at how to use this contract to write integra-\ntion tests, starting with the tests for Order Service. \nCONSUMER-DRIVEN CONTRACT INTEGRATION TESTS FOR ORDER SERVICE\nThe consumer-driven contract integration tests for Order Service verify that its API\nmeets its clients’ expectations. Listing 10.3 shows HttpBase, which is the base class\nfor the test class code-generated by Spring Cloud Contract. It’s responsible for the\nsetup phase of the test. It creates the controllers injected with mock dependencies\nand configures those mocks to return values that cause the controller to generate the\nexpected response.\npublic abstract class HttpBase {\nprivate StandaloneMockMvcBuilder controllers(Object... controllers) {\n...\nListing 10.2\nA contract that describes an HTTP-based request/response style interaction\nListing 10.3\nThe abstract base class for the tests code-generated by Spring Cloud Contract\n \n",
      "content_length": 2339,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 355,
      "content": "325\nWriting integration tests\nreturn MockMvcBuilders.standaloneSetup(controllers)\n.setMessageConverters(...);\n}\n@Before\npublic void setup() {\nOrderService orderService = mock(OrderService.class);\n  \nOrderRepository orderRepository = mock(OrderRepository.class);\nOrderController orderController =\nnew OrderController(orderService, orderRepository);\nwhen(orderRepository.findById(1223232L))\n            \n.thenReturn(Optional.of(OrderDetailsMother.CHICKEN_VINDALOO_ORDER));\n...\nRestAssuredMockMvc.standaloneSetup(controllers(orderController));  \n}\n}\nThe argument 1223232L that’s passed to the mock OrderRepository’s findById()\nmethod matches the orderId specified in the contract shown in listing 10.3. This test\nverifies that Order Service has a GET /orders/{orderId} endpoint that matches its\nclient’s expectations.\n Let’s take a look at the corresponding client test. \nCONSUMER-SIDE INTEGRATION TEST FOR API GATEWAY’S ORDERSERVICEPROXY\nAPI Gateway’s OrderServiceProxy invokes the GET /orders/{orderId} endpoint. List-\ning 10.4 shows the OrderServiceProxyIntegrationTest test class, which verifies that\nit conforms to the contracts. This class is annotated with @AutoConfigureStubRunner,\nprovided by Spring Cloud Contract. It tells Spring Cloud Contract to run the Wire-\nMock server on a random port and configure it using the specified contracts. Order-\nServiceProxyIntegrationTest configures OrderServiceProxy to make requests to\nthe WireMock port.\n@RunWith(SpringRunner.class)\n@SpringBootTest(classes=TestConfiguration.class,\nwebEnvironment= SpringBootTest.WebEnvironment.NONE)\n@AutoConfigureStubRunner(ids =\n        \n{\"net.chrisrichardson.ftgo.contracts:ftgo-order-service-contracts\"},\nworkOffline = false)\n@DirtiesContext\npublic class OrderServiceProxyIntegrationTest {\n@Value(\"${stubrunner.runningstubs.ftgo-order-service-contracts.port}\")  \nListing 10.4\nA consumer-side integration test for API Gateway's \nOrderServiceProxy\nCreate\nOrderRepository\ninjected with mocks.\nConfigure OrderResponse to return an Order when findById()\nis invoked with the orderId specified in the contract.\nConfigure Spring MVC with\nOrderController.\nTell Spring Cloud Contract\nto configure WireMock with\nOrder Service’s contracts.\nObtain the randomly assigned port \nthat WireMock is running on.\n \n",
      "content_length": 2278,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 356,
      "content": "326\nCHAPTER 10\nTesting microservices: Part 2\nprivate int port;\nprivate OrderDestinations orderDestinations;\nprivate OrderServiceProxy orderService;\n@Before\npublic void setUp() throws Exception {\norderDestinations = new OrderDestinations();\nString orderServiceUrl = \"http://localhost:\" + port;\norderDestinations.setOrderServiceUrl(orderServiceUrl);\norderService = new OrderServiceProxy(orderDestinations,\n  \nWebClient.create());\n}\n@Test\npublic void shouldVerifyExistingCustomer() {\nOrderInfo result = orderService.findOrderById(\"1223232\").block();\nassertEquals(\"1223232\", result.getOrderId());\nassertEquals(\"APPROVAL_PENDING\", result.getState());\n}\n@Test(expected = OrderNotFoundException.class)\npublic void shouldFailToFindMissingOrder() {\norderService.findOrderById(\"555\").block();\n}\n}\nEach test method invokes OrderServiceProxy and verifies that either it returns the\ncorrect values or throws the expected exception. The shouldVerifyExisting-\nCustomer() test method verifies that findOrderById() returns values equal to those\nspecified in the contract’s response. The shouldFailToFindMissingOrder() attempts\nto retrieve a nonexistent Order and verifies that OrderServiceProxy throws an Order-\nNotFoundException. Testing both the REST client and the REST service using the\nsame contracts ensures that they agree on the API.\n Let’s now look at how to do the same kind of testing for services that interact using\nmessaging. \n10.1.3 Integration testing publish/subscribe-style interactions\nServices often publish domain events that are consumed by one or more other ser-\nvices. Integration testing must verify that the publisher and its consumers agree on the\nmessage channel and the structure of the domain events. Order Service, for example,\npublishes Order* events whenever it creates or updates an Order aggregate. Order\nHistory Service is one of the consumers of those events. We must, therefore, write\ntests that verify that these services can interact.\n Figure 10.4 shows the approach to integration testing publish/subscribe interac-\ntions. Its quite similar to the approach used for testing REST interactions. As before,\nthe interactions are defined by a set of contracts. What’s different is that each contract\nspecifies a domain event.\nCreate an OrderServiceProxy\nconfigured to make requests\nto WireMock.\n \n",
      "content_length": 2316,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 357,
      "content": "327\nWriting integration tests\nEach consumer-side test publishes the event specified by the contract and verifies that\nOrderHistoryEventHandlers invokes its mocked dependencies correctly.\n On the provider side, Spring Cloud Contract code-generates test classes that\nextend MessagingBase, which is a hand-written abstract superclass. Each test method\ninvokes a hook method defined by MessagingBase, which is expected to trigger the\npublication of an event by the service. In this example, each hook method invokes\nOrderDomainEventPublisher, which is responsible for publishing Order aggregate\nevents. The test method then verifies that OrderDomainEventPublisher published\nthe expected event. Let’s look at the details of how these tests work, starting with the\ncontract.\nTHE CONTRACT FOR PUBLISHING AN ORDERCREATED EVENT\nListing 10.5 shows the contract for an OrderCreated event. It specifies the event’s\nchannel, along with the expected body and message headers.\nProvider-side integration\ntest for Order Service\nConsumer-side\nintegration test for\nOrder History Service\nSpring cloud\ncontract\nTests\nTests\nReads from\nPublishes to\nConﬁgures\nCode\ngenerates\nPublishes to\nReads from\nInvokes\nUses\ncontract.make{\nlabel 'orderCreatedEvent'\ninput{\ntriggeredBy('orderCreated()')\n}\noutputMessage{...}\n}\nclass MessageTest extends MessagingBase{\n@Test\npublic void validate_orderCreatedEvent(){\norderCreated();\n...\n}\n}\nclass MessagingBase{\nvoid orderCreated(){\n}\nOrderHistory\nEventHandlers\nTest\nOrderHistory\nEventHandlers\nChannel\nOrder domain\nEventPublisher\nChannel\nMessaging stub\nClass under test\nClass under test\nTriggers\n'orderCreatedEvent'\nInvokes trigger function\nthat veriﬁes that the output\nmessage is published to the\nexpected channel\nFigure 10.4\nThe contracts are used to test both sides of the publish/subscribe interaction. The provider-side \ntests verify that OrderDomainEventPublisher publishes events that confirm to the contract. The \nconsumer-side tests verify that OrderHistoryEventHandlers consume the example events from \nthe contract.\n \n",
      "content_length": 2040,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 358,
      "content": "328\nCHAPTER 10\nTesting microservices: Part 2\npackage contracts;\norg.springframework.cloud.contract.spec.Contract.make {\nlabel 'orderCreatedEvent'\n  \ninput {\ntriggeredBy('orderCreated()')       \n}\noutputMessage {\nsentTo('net.chrisrichardson.ftgo.orderservice.domain.Order')\nbody('''{\"orderDetails\":{\"lineItems\":[{\"quantity\":5,\"menuItemId\":\"1\",\n                 \"name\":\"Chicken Vindaloo\",\"price\":\"12.34\",\"total\":\"61.70\"}],\n                 \"orderTotal\":\"61.70\",\"restaurantId\":1, \n        \"consumerId\":1511300065921},\"orderState\":\"APPROVAL_PENDING\"}''')\n        headers {\n            header('event-aggregate-type', \n                        'net.chrisrichardson.ftgo.orderservice.domain.Order')\n            header('event-aggregate-id', '1')\n        }\n}\n}\nThe contract also has two other important elements:\n\nlabel—is used by a consumer test to trigger publication of the event by Spring\nContact\n\ntriggeredBy—the name of the superclass method invoked by the generated\ntest method to trigger the publishing of the event\nLet’s look at how the contract is used, starting with the provider-side test for Order-\nService.\nCONSUMER-DRIVEN CONTRACT TESTS FOR ORDER SERVICE\nThe provider-side test for Order Service is another consumer-driven contract inte-\ngration test. It verifies that OrderDomainEventPublisher, which is responsible for\npublishing Order aggregate domain events, publishes events that match its clients’\nexpectations. Listing 10.6 shows MessagingBase, which is the base class for the test\nclasses code-generated by Spring Cloud Contract. It’s responsible for configuring the\nOrderDomainEventPublisher class to use in-memory messaging stubs. It also defines\nthe methods, such as orderCreated(), which are invoked by the generated tests to\ntrigger the publishing of the event.\n@RunWith(SpringRunner.class)\n@SpringBootTest(classes = MessagingBase.TestConfiguration.class,\nwebEnvironment = SpringBootTest.WebEnvironment.NONE)\n@AutoConfigureMessageVerifier\npublic abstract class MessagingBase {\nListing 10.5\nA contract for a publish/subscribe interaction style\nListing 10.6\nThe abstract base class for the Spring Cloud Contract provider-side tests\nUsed by the \nconsumer test to \ntrigger the event \nto be published\nInvoked by the code-\ngenerated provider test\nAn Order-\nCreated\ndomain\nevent\n \n",
      "content_length": 2294,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 359,
      "content": "329\nWriting integration tests\n@Configuration\n@EnableAutoConfiguration\n@Import({EventuateContractVerifierConfiguration.class,\nTramEventsPublisherConfiguration.class,\nTramInMemoryConfiguration.class})\npublic static class TestConfiguration {\n@Bean\npublic OrderDomainEventPublisher\nOrderDomainEventPublisher(DomainEventPublisher eventPublisher) {\nreturn new OrderDomainEventPublisher(eventPublisher);\n}\n}\n@Autowired\nprivate OrderDomainEventPublisher OrderDomainEventPublisher;\nprotected void orderCreated() {\n    \nOrderDomainEventPublisher.publish(CHICKEN_VINDALOO_ORDER,\nsingletonList(new OrderCreatedEvent(CHICKEN_VINDALOO_ORDER_DETAILS)\n));\n}\n}\nThis test class configures OrderDomainEventPublisher with in-memory messaging\nstubs. orderCreated() is invoked by the test method generated from the contract\nshown earlier in listing 10.5. It invokes OrderDomainEventPublisher to publish an\nOrderCreated event. The test method attempts to receive this event and then verifies\nthat it matches the event specified in the contract. Let’s now look at the correspond-\ning consumer-side tests. \nCONSUMER-SIDE CONTRACT TEST FOR THE ORDER HISTORY SERVICE\nOrder History Service consumes events published by Order Service. As I described\nin chapter 7, the adapter class that handles these events is the OrderHistoryEvent-\nHandlers class. Its event handlers invoke OrderHistoryDao to update the CQRS view.\nListing 10.7 shows the consumer-side integration test. It creates an OrderHistoryEvent-\nHandlers injected with a mock OrderHistoryDao. Each test method first invokes Spring\nCloud to publish the event defined in the contract and then verifies that OrderHistory-\nEventHandlers invokes OrderHistoryDao correctly.\n@RunWith(SpringRunner.class)\n@SpringBootTest(classes= OrderHistoryEventHandlersTest.TestConfiguration.class,\nwebEnvironment= SpringBootTest.WebEnvironment.NONE)\n@AutoConfigureStubRunner(ids =\n{\"net.chrisrichardson.ftgo.contracts:ftgo-order-service-contracts\"},\nworkOffline = false)\nListing 10.7\nThe consumer-side integration test for the OrderHistoryEventHandlers \nclass\norderCreated() is invoked by a\ncode-generated test subclass\nto publish the event.\n \n",
      "content_length": 2153,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 360,
      "content": "330\nCHAPTER 10\nTesting microservices: Part 2\n@DirtiesContext\npublic class OrderHistoryEventHandlersTest {\n@Configuration\n@EnableAutoConfiguration\n@Import({OrderHistoryServiceMessagingConfiguration.class,\nTramCommandProducerConfiguration.class,\nTramInMemoryConfiguration.class,\nEventuateContractVerifierConfiguration.class})\npublic static class TestConfiguration {\n@Bean\npublic OrderHistoryDao orderHistoryDao() {\nreturn mock(OrderHistoryDao.class);\n  \n}\n}\n@Test\npublic void shouldHandleOrderCreatedEvent() throws ... {\nstubFinder.trigger(\"orderCreatedEvent\");\n         \neventually(() -> {\n      \nverify(orderHistoryDao).addOrder(any(Order.class), any(Optional.class));\n});\n}\nThe shouldHandleOrderCreatedEvent() test method tells Spring Cloud Contract to\npublish the OrderCreated event. It then verifies that OrderHistoryEventHandlers\ninvoked orderHistoryDao.addOrder(). Testing both the domain event’s publisher and\nconsumer using the same contracts ensures that they agree on the API. Let’s now look at\nhow to do integration test services that interact using asynchronous request/response. \n10.1.4 Integration contract tests for asynchronous request/response \ninteractions\nPublish/subscribe isn’t the only kind of messaging-based interaction style. Services\nalso interact using asynchronous request/response. For example, in chapter 4 we saw\nthat Order Service implements sagas that send command messages to various ser-\nvices, such as Kitchen Service, and processes the reply messages.\n The two parties in an asynchronous request/response interaction are the requestor,\nwhich is the service that sends the command, and the replier, which is the service that\nprocesses the command and sends back a reply. They must agree on the name of com-\nmand message channel and the structure of the command and reply messages. Let’s\nlook at how to write integration tests for asynchronous request/response interactions.\n Figure 10.5 shows how to test the interaction between Order Service and Kitchen\nService. The approach to integration testing asynchronous request/response interac-\ntions is quite similar to the approach used for testing REST interactions. The interac-\ntions between the services are defined by a set of contracts. What’s different is that a\ncontract specifies an input message and an output message instead of an HTTP request\nand reply.\nCreate a mock OrderHistoryDao \nto inject into OrderHistory-\nEventHandlers.\nTrigger the \norderCreatedEvent \nstub, which emits an \nOrderCreated event.\nVerify that OrderHistoryEventHandlers\ninvoked orderHistoryDao.addOrder().\n \n",
      "content_length": 2572,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 361,
      "content": "331\nWriting integration tests\nThe consumer-side test verifies that the command message proxy class sends correctly\nstructured command messages and correctly processes reply messages. In this exam-\nple, KitchenServiceProxyTest tests KitchenServiceProxy. It uses Spring Cloud Con-\ntract to configure messaging stubs that verify that the command message matches a\ncontract’s input message and replies with the corresponding output message.\n The provider-side tests are code-generated by Spring Cloud Contract. Each test\nmethod corresponds to a contract. It sends the contract’s input message as a com-\nmand message and verifies that the reply message matches the contract’s output mes-\nsage. Let’s look at the details, starting with the contract.\nEXAMPLE ASYNCHRONOUS REQUEST/RESPONSE CONTRACT\nListing 10.8 shows the contract for one interaction. It consists of an input message and\nan output message. Both messages specify a message channel, message body, and mes-\nsage headers. The naming convention is from the provider’s perspective. The input\nmessage’s messageFrom element specifies the channel that the message is read from.\nProvider-side\nintegration test for\nKitchen Service\nConsumer-side\nintegration test for\nKitchen Service\nSpring cloud\ncontract\nReads\nTests\nSends to\nSends to\nReceives from\nConﬁgures\nCode\ngenerates\nReceives\nfrom\nExtends\nConﬁgures\nInvokes\nReads\ncommand\nReads reply\nSends\nreply\ncommand\nSends\ncommand\nMessage\nContract.make {\ninputMessage{...}\nOutputMessage{...}\n}\nabstract class BaseMessaging{\nvoid setUp(){...}\nclass MessageTest extends BaseMessaging{\n}\nKitchenService\nProxy\nIntegrationTest\nKitchenService\nProxy\nKitchenService\nCommandHandler\nReply\nchannel\nCommand\nchannel\nCommand\nchannel\nReply\nchannel\n«mock»\nKitchenService\nMessaging stub\nSends input message and\nveriﬁes that reply matches\ncontract’s output message\nFigure 10.5\nThe contracts are used to test the adapter classes that implement each side of the asynchronous \nrequest/response interaction. The provider-side tests verify that KitchenServiceCommandHandler handles \ncommands and sends back replies. The consumer-side tests verify KitchenServiceProxy sends commands \nthat conform to the contract, and that it handles the example replies from the contract.\n \n",
      "content_length": 2241,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 362,
      "content": "332\nCHAPTER 10\nTesting microservices: Part 2\nSimilarly, the output message’s sentTo element specifies the channel that the reply\nshould be sent to.\npackage contracts;\norg.springframework.cloud.contract.spec.Contract.make {\nlabel 'createTicket'\ninput {\n    \nmessageFrom('kitchenService')\nmessageBody('''{\"orderId\":1,\"restaurantId\":1,\"ticketDetails\":{...}}''')\nmessageHeaders {\nheader('command_type','net.chrisrichardson...CreateTicket')\nheader('command_saga_type','net.chrisrichardson...CreateOrderSaga')\nheader('command_saga_id',$(consumer(regex('[0-9a-f]{16}-[0-9a-f]\n{16}'))))\nheader('command_reply_to','net.chrisrichardson...CreateOrderSaga-Reply')\n}\n}\noutputMessage {\n     \nsentTo('net.chrisrichardson...CreateOrderSaga-reply')\nbody([\nticketId: 1\n])\nheaders {\nheader('reply_type', 'net.chrisrichardson...CreateTicketReply')\nheader('reply_outcome-type', 'SUCCESS')\n}\n}\n}\nIn this example contract, the input message is a CreateTicket command that’s sent to\nthe kitchenService channel. The output message is a successful reply that’s sent to the\nCreateOrderSaga’s reply channel. Let’s look at how to use this contract in tests, start-\ning with the consumer-side tests for Order Service. \nCONSUMER-SIDE CONTRACT INTEGRATION TEST FOR AN ASYNCHRONOUS REQUEST/RESPONSE \nINTERACTION\nThe strategy for writing a consumer-side integration test for an asynchronous request/\nresponse interaction is similar to testing a REST client. The test invokes the service’s\nmessaging proxy and verifies two aspects of its behavior. First, it verifies that the mes-\nsaging proxy sends a command message that conforms to the contract. Second, it ver-\nifies that the proxy properly handles the reply message.\n Listing 10.9 shows the consumer-side integration test for KitchenServiceProxy,\nwhich is the messaging proxy used by Order Service to invoke Kitchen Service. Each\ntest sends a command message using KitchenServiceProxy and verifies that it returns\nthe expected result. It uses Spring Cloud Contract to configure messaging stubs for\nListing 10.8\nContract describing how Order Service asynchronously invokes \nKitchen Service\nThe command message \nsent by Order Service \nto the kitchenService \nchannel\nThe reply message sent \nby Kitchen Service\n \n",
      "content_length": 2229,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 363,
      "content": "333\nWriting integration tests\nKitchen Service that find the contract whose input message matches the command\nmessage and sends its output message as the reply. The tests use in-memory messaging\nfor simplicity and speed.\n@RunWith(SpringRunner.class)\n@SpringBootTest(classes= \nKitchenServiceProxyIntegrationTest.TestConfiguration.class,\nwebEnvironment= SpringBootTest.WebEnvironment.NONE)\n@AutoConfigureStubRunner(ids =\n      \n{\"net.chrisrichardson.ftgo.contracts:ftgo-kitchen-service-contracts\"},\nworkOffline = false)\n@DirtiesContext\npublic class KitchenServiceProxyIntegrationTest {\n@Configuration\n@EnableAutoConfiguration\n@Import({TramCommandProducerConfiguration.class,\nTramInMemoryConfiguration.class,\nEventuateContractVerifierConfiguration.class})\npublic static class TestConfiguration { ... }\n@Autowired\nprivate SagaMessagingTestHelper sagaMessagingTestHelper;\n@Autowired\nprivate\nKitchenServiceProxy kitchenServiceProxy;\n@Test\npublic void shouldSuccessfullyCreateTicket() {\nCreateTicket command = new CreateTicket(AJANTA_ID,\nOrderDetailsMother.ORDER_ID,\nnew TicketDetails(Collections.singletonList(\nnew TicketLineItem(CHICKEN_VINDALOO_MENU_ITEM_ID,\nCHICKEN_VINDALOO,\nCHICKEN_VINDALOO_QUANTITY))));\nString sagaType = CreateOrderSaga.class.getName();\nCreateTicketReply reply =\nsagaMessagingTestHelper\n      \n.sendAndReceiveCommand(kitchenServiceProxy.create,\ncommand,\nCreateTicketReply.class, sagaType);\nassertEquals(new CreateTicketReply(OrderDetailsMother.ORDER_ID), reply);  \n}\n}\nListing 10.9\nThe consumer-side contract integration test for Order Service\nConfigure the stub\nKitchen Service to\nrespond to messages.\nSend the \ncommand and \nwait for a reply.\nVerify the\nreply.\n \n",
      "content_length": 1681,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 364,
      "content": "334\nCHAPTER 10\nTesting microservices: Part 2\nThe shouldSuccessfullyCreateTicket() test method sends a CreateTicket com-\nmand message and verifies that the reply contains the expected data. It uses Saga-\nMessagingTestHelper, which is a test helper class that synchronously sends and receives\nmessages.\n Let’s now look at how to write provider-side integration tests.\nWRITING PROVIDER-SIDE, CONSUMER-DRIVEN CONTRACT TESTS FOR ASYNCHRONOUS \nREQUEST/RESPONSE INTERACTIONS\nA provider-side integration test must verify that the provider handles a command mes-\nsage by sending the correct reply. Spring Cloud Contract generates test classes that\nhave a test method for each contract. Each test method sends the contract’s input\nmessage and verifies that the reply matches the contract’s output message.\n The provider-side integration tests for Kitchen Service test KitchenService-\nCommandHandler. The KitchenServiceCommandHandler class handles a message by\ninvoking KitchenService. The following listing shows the AbstractKitchenService-\nConsumerContractTest class, which is the base class for the Spring Cloud Contract-\ngenerated tests. It creates a KitchenServiceCommandHandler injected with a mock\nKitchenService.\n@RunWith(SpringRunner.class)\n@SpringBootTest(classes = \nAbstractKitchenServiceConsumerContractTest.TestConfiguration.class,\nwebEnvironment = SpringBootTest.WebEnvironment.NONE)\n@AutoConfigureMessageVerifier\npublic abstract class AbstractKitchenServiceConsumerContractTest {\n@Configuration\n@Import(RestaurantMessageHandlersConfiguration.class)\npublic static class TestConfiguration {\n...\n@Bean\npublic KitchenService kitchenService() {\n   \nreturn mock(KitchenService.class);\n}\n}\n@Autowired\nprivate KitchenService kitchenService;\n@Before\npublic void setup() {\nreset(kitchenService);\nwhen(kitchenService\n.createTicket(eq(1L), eq(1L),\n    \nany(TicketDetails.class)))\n.thenReturn(new Ticket(1L, 1L,\nListing 10.10\nSuperclass of provider-side, consumer-driven contract tests for Kitchen\nService\nOverrides the definition \nof the kitchenService \n@Bean with a mock\nConfigures the mock to \nreturn the values that match \na contract’s output message\n \n",
      "content_length": 2148,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 365,
      "content": "335\nDeveloping component tests\nnew TicketDetails(Collections.emptyList())));\n}\n}\nKitchenServiceCommandHandler invokes KitchenService with arguments that are\nderived from a contract’s input message and creates a reply message that’s derived\nfrom the return value. The test class’s setup() method configures the mock Kitchen-\nService to return the values that match the contract’s output message\n Integration tests and unit tests verify the behavior of individual parts of a service.\nThe integration tests verify that services can communicate with their clients and\ndependencies. The unit tests verify that a service’s logic is correct. Neither type of test\nruns the entire service. In order to verify that a service as a whole works, we’ll move up\nthe pyramid and look at how to write component tests. \n10.2\nDeveloping component tests\nSo far, we’ve looked at how to test individual classes and clusters of classes. But imag-\nine that we now want to verify that Order Service works as expected. In other words,\nwe want to write the service’s acceptance tests, which treat it as a black box and verify\nits behavior through its API. One approach is to write what are essentially end-to-end\ntests and deploy Order Service and all of its transitive dependencies. As you should\nknow by now, that’s a slow, brittle, and expensive way to test a service.\nA much better way to write acceptance tests for a service is to use component testing.\nAs figure 10.6 shows, component tests are sandwiched between integration tests and\nend-to-end tests. Component testing verifies the behavior of a service in isolation. It\nreplaces a service’s dependencies with stubs that simulate their behavior. It might even\nuse in-memory versions of infrastructure services such as databases. As a result, com-\nponent tests are much easier to write and faster to run.\n I begin by briefly describing how to use a testing DSL called Gherkin to write\nacceptance tests for services, such as Order Service. After that I discuss various com-\nponent testing design issues. I then show how to write acceptance tests for Order\nService.\n Let’s look at writing acceptance tests using Gherkin.\n \n \nPattern: Service component test\nTest a service in isolation. See http://microservices.io/patterns/testing/service-\ncomponent-test.html.\n \n",
      "content_length": 2292,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 366,
      "content": "336\nCHAPTER 10\nTesting microservices: Part 2\n10.2.1 Defining acceptance tests\nAcceptance tests are business-facing tests for a software component. They describe the\ndesired externally visible behavior from the perspective of the component’s clients\nrather than in terms of the internal implementation. These tests are derived from user\nstories or use cases. For example, one of the key stories for Order Service is the Place\nOrder story:\nAs a consumer of the Order Service\nI should be able to place an order\nWe can expand this story into scenarios such as the following:\nGiven a valid consumer\nGiven using a valid credit card\nGiven the restaurant is accepting orders\nWhen I place an order for Chicken Vindaloo at Ajanta\nThen the order should be APPROVED\nAnd an OrderAuthorized event should be published\nThis scenario describes the desired behavior of Order Service in terms of its API.\n Each scenario defines an acceptance test. The givens correspond to the test’s setup\nphase, the when maps to the execute phase, and the then and the and to the verifica-\ntion phase. Later, you see a test for this scenario that does the following:\n1\nCreates an Order by invoking the POST /orders endpoint\n2\nVerifies the state of the Order by invoking the GET /orders/{orderId} endpoint\n3\nVerifies that the Order Service published an OrderAuthorized event by sub-\nscribing to the appropriate message channel\nWe could translate each scenario into Java code. An easier option, though, is to write\nthe acceptance tests using a DSL such as Gherkin. \nEnd-to-end\nComponent\nIntegration\nUnit\nComponent\ntest\nTests\nService\nStub\ndependency 1\nStub\ndependency 2\nStub\ndependency\n...\nFigure 10.6\nA component test tests a service in isolation. It typically uses stubs for the service’s \ndependencies.\n \n",
      "content_length": 1771,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 367,
      "content": "337\nDeveloping component tests\n10.2.2 Writing acceptance tests using Gherkin\nWriting acceptance tests in Java is challenging. There’s a risk that the scenarios and\nthe Java tests diverge. There’s also a disconnect between the high-level scenarios and\nthe Java tests, which consist of low-level implementation details. Also, there’s a risk\nthat a scenario lacks precision or is ambiguous and can’t be translated into Java code.\nA much better approach is to eliminate the manual translation step and write execut-\nable scenarios.\n Gherkin is a DSL for writing executable specifications. When using Gherkin, you\ndefine your acceptance tests using English-like scenarios, such as the one shown ear-\nlier. You then execute the specifications using Cucumber, a test automation frame-\nwork for Gherkin. Gherkin and Cucumber eliminate the need to manually translate\nscenarios into runnable code.\n The Gherkin specification for a service such as Order Service consists of a set of fea-\ntures. Each feature is described by a set of scenarios such as the one you saw earlier. A sce-\nnario has the given-when-then structure. The givens are the preconditions, the when is\nthe action or event that occurs, and the then/and are the expected outcome.\n For example, the desired behavior of Order Service is defined by several features,\nincluding Place Order, Cancel Order, and Revise Order. Listing 10.11 is an excerpt of\nthe Place Order feature. This feature consists of several elements:\nName—For this feature, the name is Place Order.\nSpecification brief—This describes why the feature exists. For this feature, the\nspecification brief is the user story.\nScenarios—Order authorized and Order rejected due to expired credit card.\nFeature: Place Order\nAs a consumer of the Order Service\nI should be able to place an order\nScenario: Order authorized\nGiven a valid consumer\nGiven using a valid credit card\nGiven the restaurant is accepting orders\nWhen I place an order for Chicken Vindaloo at Ajanta\nThen the order should be APPROVED\nAnd an OrderAuthorized event should be published\nScenario: Order rejected due to expired credit card\nGiven a valid consumer\nGiven using an expired credit card\nGiven the restaurant is accepting orders\nWhen I place an order for Chicken Vindaloo at Ajanta\nThen the order should be REJECTED\nAnd an OrderRejected event should be published\n...\nListing 10.11\nThe Gherkin definition of the Place Order feature and some of its scenarios\n \n",
      "content_length": 2449,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 368,
      "content": "338\nCHAPTER 10\nTesting microservices: Part 2\nIn both scenarios, a consumer attempts to place an order. In the first scenario, they\nsucceed. In the second scenario, the order is rejected because the consumer’s credit\ncard has expired. For more information on Gherkin, see the book Writing Great Specifi-\ncations: Using Specification by Example and Gherkin by Kamil Nicieja (Manning, 2017).\nEXECUTING GHERKIN SPECIFICATIONS USING CUCUMBER\nCucumber is an automated testing framework that executes tests written in Gherkin.\nIt’s available in a variety of languages, including Java. When using Cucumber for Java,\nyou write a step definition class, such as the one shown in listing 10.12. A step definition\nclass consists of methods that define the meaning of each given-then-when step. Each\nstep definition method is annotated with either @Given, @When, @Then, or @And. Each\nof these annotations has a value element that’s a regular expression, which Cucum-\nber matches against the steps.\npublic class StepDefinitions ...\n{\n...\n@Given(\"A valid consumer\")\npublic void useConsumer() { ... }\n@Given(\"using a(.?) (.*) credit card\")\npublic void useCreditCard(String ignore, String creditCard) { ... }\n@When(\"I place an order for Chicken Vindaloo at Ajanta\")\npublic void placeOrder() { ... }\n@Then(\"the order should be (.*)\")\npublic void theOrderShouldBe(String desiredOrderState) { ... }\n@And(\"an (.*) event should be published\")\npublic void verifyEventPublished(String expectedEventClass)\n{ ... }\n}\nEach type of method is part of a particular phase of the test:\n\n@Given—The setup phase\n\n@When—The execute phase\n\n@Then and @And—The verification phase\nLater in section 10.2.4, when I describe this class in more detail, you’ll see that many\nof these methods make REST calls to Order Service. For example, the placeOrder()\nmethod creates Order by invoking the POST /orders REST endpoint. The the-\nOrderShouldBe() method verifies the status of the order by invoking GET /orders/\n{orderId}.\n But before getting into the details of how to write step classes, let’s explore some\ndesign issues with component tests. \nListing 10.12\nThe Java step definitions class makes the Gherkin scenarios executable.\n \n",
      "content_length": 2191,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 369,
      "content": "339\nDeveloping component tests\n10.2.3 Designing component tests\nImagine you’re implementing the component tests for Order Service. Section 10.2.2\nshows how to specify the desired behavior using Gherkin and execute it using Cucum-\nber. But before a component test can execute the Gherkin scenarios, it must first run\nOrder Service and set up the service’s dependencies. You need to test Order Service\nin isolation, so the component test must configure stubs for several services, including\nKitchen Service. It also needs to set up a database and the messaging infrastructure.\nThere are a few different options that trade off realism with speed and simplicity.\nIN-PROCESS COMPONENT TESTS\nOne option is to write in-process component tests. An in-process component test runs the\nservice with in-memory stubs and mocks for its dependencies. For example, you can\nwrite a component test for a Spring Boot-based service using the Spring Boot testing\nframework. A test class, which is annotated with @SpringBootTest, runs the service in\nthe same JVM as the test. It uses dependency injection to configure the service to use\nmocks and stubs. For instance, a test for Order Service would configure it to use an\nin-memory JDBC database, such as H2, HSQLDB, or Derby, and in-memory stubs for\nEventuate Tram. In-process tests are simpler to write and faster, but have the downside\nof not testing the deployable service. \nOUT-OF-PROCESS COMPONENT TESTING\nA more realistic approach is to package the service in a production-ready format and\nrun it as a separate process. For example, chapter 12 explains that it’s increasingly\ncommon to package services as Docker container images. An out-of-process component\ntest uses real infrastructure services, such as databases and message brokers, but uses\nstubs for any dependencies that are application services. For example, an out-of-process\ncomponent test for FTGO Order Service would use MySQL and Apache Kafka, and\nstubs for services including Consumer Service and Accounting Service. Because Order\nService interacts with those services using messaging, these stubs would consume\nmessages from Apache Kafka and send back reply messages.\n A key benefit of out-of-process component testing is that it improves test coverage,\nbecause what’s being tested is much closer to what’s being deployed. The drawback is\nthat this type of test is more complex to write, slower to execute, and potentially more\nbrittle than an in-process component test. You also have to figure out how to stub the\napplication services. Let’s look at how to do that.\nHOW TO STUB SERVICES IN OUT-OF-PROCESS COMPONENT TESTS\nThe service under test often invokes dependencies using interaction styles that involve\nsending back a response. Order Service, for example, uses asynchronous request/\nresponse and sends command messages to various services. API Gateway uses HTTP,\nwhich is a request/response interaction style. An out-of-process test must configure\nstubs for these kinds of dependencies, which handle requests and send back replies.\n One option is to use Spring Cloud Contract, which we looked at earlier in sec-\ntion 10.1 when discussing integration tests. We could write contracts that configure\n \n",
      "content_length": 3207,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 370,
      "content": "340\nCHAPTER 10\nTesting microservices: Part 2\nstubs for component tests. One thing to consider, though, is that it’s likely that these\ncontracts, unlike those used for integration, would only be used by the component tests.\n Another drawback of using Spring Cloud Contract for component testing is that\nbecause its focus is consumer contract testing, it takes a somewhat heavyweight\napproach. The JAR files containing the contracts must be deployed in a Maven\nrepository rather than merely being on the classpath. Handling interactions involving\ndynamically generated values is also challenging. Consequently, a simpler option is to\nconfigure stubs from within the test itself.\n A test can, for example, configure an HTTP stub using the WireMock stubbing\nDSL. Similarly, a test for a service that uses Eventuate Tram messaging can configure\nmessaging stubs. Later in this section I show an easy-to-use Java library that does this.\n Now that we’ve looked at how to design component tests, let’s consider how to\nwrite component tests for the FTGO Order Service. \n10.2.4 Writing component tests for the FTGO Order Service\nAs you saw earlier in this section, there are a few different ways to implement compo-\nnent tests. This section describes the component tests for Order Service that use the\nout-of-process strategy to test the service running as a Docker container. You’ll see\nhow the tests use a Gradle plugin to start and stop the Docker container. I discuss how\nto use Cucumber to execute the Gherkin-based scenarios that define the desired\nbehavior for Order Service.\n Figure 10.7 shows the design of the component tests for Order Service. Order-\nServiceComponentTest is the test class that runs Cucumber:\n@RunWith(Cucumber.class)\n@CucumberOptions(features = \"src/component-test/resources/features\")\npublic class OrderServiceComponentTest {\n}\nIt has an @CucumberOptions annotation that specifies where to find the Gherkin\nfeature files. It’s also annotated with @RunWith(Cucumber.class), which tells JUNIT\nto use the Cucumber test runner. But unlike a typical JUNIT-based test class, it\ndoesn’t have any test methods. Instead, it defines the tests by reading the Gherkin\nfeatures and uses the OrderServiceComponentTestStepDefinitions class to make\nthem executable.\n Using Cucumber with the Spring Boot testing framework requires a slightly unusual\nstructure. Despite not being a test class, OrderServiceComponentTestStepDefinitions\nis still annotated with @ContextConfiguration, which is part of the Spring Testing\nframework. It creates Spring ApplicationContext, which defines the various Spring\ncomponents, including messaging stubs. Let’s look at the details of the step definitions.\n \n \n \n",
      "content_length": 2697,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 371,
      "content": "341\nDeveloping component tests\nTHE ORDERSERVICECOMPONENTTESTSTEPDEFINITIONS CLASS\nThe OrderServiceComponentTestStepDefinitions class is the heart of the tests. This\nclass defines the meaning of each step in Order Service’s component tests. The fol-\nlowing listing shows the usingCreditCard() method, which defines the meaning of\nthe Given using … credit card step.\n@ContextConfiguration(classes = \nOrderServiceComponentTestStepDefinitions.TestConfiguration.class)\npublic class OrderServiceComponentTestStepDefinitions {\n...\n@Autowired\nprotected SagaParticipantStubManager sagaParticipantStubManager;\n@Given(\"using a(.?) (.*) credit card\")\npublic void useCreditCard(String ignore, String creditCard) {\nif (creditCard.equals(\"valid\"))\n  sagaParticipantStubManager\n   \n.forChannel(\"accountingService\")\n.when(AuthorizeCommand.class).replyWithSuccess();\nelse if (creditCard.equals(\"invalid\"))\nsagaParticipantStubManager\n    \nListing 10.13\nThe @GivenuseCreditCard() method defines the meaning of the \nGiven using … credit card step.\nAs a consumer of the Order Service\nI should be able to create an order\nScenario: Order authorized\nGiven a valid consumer\nGiven using a valid credit card\ndockerCompose {\n...\n}\nftgo-order-service:\nbuild: .\nports:\n- \"8082:8080\"\nOrderService\nComponent\nTest\nOrder Service\ndocker\ncontainer\nsrc/component-test/resources/\ncreateorder.feature\nDocker-compose.yml\nbuild.gradle\nWritten using the\nCucumber testing framework\nKafka\nMySQL\nInvokes\nREST API\nRuns\nRuns\nRuns\nReads command\nand sends\nreplies\nReads events\nReads\nUses\nUses\nOrderService\nComponent\nStep\nDeﬁnitions\nFigure 10.7\nThe component tests for Order Service use the Cucumber testing framework to \nexecute tests scenarios written using Gherkin acceptance testing DSL. The tests use Docker to run \nOrder Service along with its infrastructure services, such as Apache Kafka and MySQL.\nSend a \nsuccess reply.\nSend a failure \nreply.\n \n",
      "content_length": 1904,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 372,
      "content": "342\nCHAPTER 10\nTesting microservices: Part 2\n.forChannel(\"accountingService\")\n.when(AuthorizeCommand.class).replyWithFailure();\nelse\nfail(\"Don't know what to do with this credit card\");\n}\nThis method uses the SagaParticipantStubManager class, a test helper class that con-\nfigures stubs for saga participants. The useCreditCard() method uses it to configure\nthe Accounting Service stub to reply with either a success or a failure message,\ndepending on the specified credit card.\n The following listing shows the placeOrder() method, which defines the When I\nplace an order for Chicken Vindaloo at Ajanta step. It invokes the Order Service\nREST API to create Order and saves the response for validation in a later step.\n@ContextConfiguration(classes = \nOrderServiceComponentTestStepDefinitions.TestConfiguration.class)\npublic class OrderServiceComponentTestStepDefinitions {\nprivate int port = 8082;\nprivate String host = System.getenv(\"DOCKER_HOST_IP\");\nprotected String baseUrl(String path) {\nreturn String.format(\"http://%s:%s%s\", host, port, path);\n}\nprivate Response response;\n@When(\"I place an order for Chicken Vindaloo at Ajanta\")\npublic void placeOrder() {\n    response = given().                                               \n            body(new CreateOrderRequest(consumerId,\n                    RestaurantMother.AJANTA_ID, Collections.singletonList(\n                        new CreateOrderRequest.LineItem(\n                           RestaurantMother.CHICKEN_VINDALOO_MENU_ITEM_ID,\n                          OrderDetailsMother.CHICKEN_VINDALOO_QUANTITY)))).\n            contentType(\"application/json\").\n            when().\n            post(baseUrl(\"/orders\"));\n}\nThe baseUrl() help method returns the URL of the order service.\n Listing 10.15 shows the theOrderShouldBe() method, which defines the meaning\nof the Then the order should be … step. It verifies that Order was successfully created\nand that it’s in the expected state.\n \n \nListing 10.14\nThe placeOrder() method defines the When I place an order for \nChicken Vindaloo at Ajanta step.\nInvokes the Order \nService REST API \nto create Order\n \n",
      "content_length": 2112,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 373,
      "content": "343\nDeveloping component tests\n@ContextConfiguration(classes = \nOrderServiceComponentTestStepDefinitions.TestConfiguration.class)\npublic class OrderServiceComponentTestStepDefinitions {\n@Then(\"the order should be (.*)\")\npublic void theOrderShouldBe(String desiredOrderState) {\nInteger orderId =\n  \nthis.response. then(). statusCode(200).\nextract(). path(\"orderId\");\nassertNotNull(orderId);\neventually(() -> {\nString state = given().\nwhen().\nget(baseUrl(\"/orders/\" + orderId)).\nthen().\nstatusCode(200)\n.extract().\npath(\"state\");\nassertEquals(desiredOrderState, state);\n  \n});\n}\n]\nThe assertion of the expected state is wrapped in a call to eventually(), which\nrepeatedly executes the assertion.\n The following listing shows the verifyEventPublished() method, which defines\nthe And an … event should be published step. It verifies that the expected domain\nevent was published.\n@ContextConfiguration(classes = \nOrderServiceComponentTestStepDefinitions.TestConfiguration.class)\npublic class OrderServiceComponentTestStepDefinitions {\n@Autowired\nprotected MessageTracker messageTracker;\n@And(\"an (.*) event should be published\")\npublic void verifyEventPublished(String expectedEventClass) throws ClassNot\nFoundException {\nmessageTracker.assertDomainEventPublished(\"net.chrisrichardson.ftgo.order\nservice.domain.Order\",\nListing 10.15\nThe @ThentheOrderShouldBe() method verifies HTTP request was \nsuccessful.\nListing 10.16\nThe Cucumber step definitions class for the Order Service component \ntests\nVerify that Order \nwas created \nsuccessfully.\nVerify the \nstate of \nOrder.\n \n",
      "content_length": 1568,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 374,
      "content": "344\nCHAPTER 10\nTesting microservices: Part 2\n(Class<DomainEvent>)Class.forName(\"net.chrisrichardson.ftgo.order\nservice.domain.\" + expectedEventClass));\n}\n....\n}\nThe verifyEventPublished() method uses the MessageTracker class, a test helper\nclass that records the events that have been published during the test. This class\nand SagaParticipantStubManager are instantiated by the TestConfiguration\n@Configuration class.\n Now that we’ve looked at the step definitions, let’s look at how to run the compo-\nnent tests. \nRUNNING THE COMPONENT TESTS\nBecause these tests are relatively slow, we don’t want to run them as part of ./gradlew\ntest. Instead, we’ll put the test code in a separate src/component-test/java directory\nand run them using ./gradlew componentTest. Take a look at the ftgo-order-service/\nbuild.gradle file to see the Gradle configuration.\n The tests use Docker to run Order Service and its dependencies. As described in\nchapter 12, a Docker container is a lightweight operating system virtualization\nmechanism that lets you deploy a service instance in an isolated sandbox. Docker\nCompose is an extremely useful tool with which you can define a set of containers\nand start and stop them as a unit. The FTGO application has a docker-compose file\nin the root directory that defines containers for all the services, and the infrastruc-\nture service.\n We can use the Gradle Docker Compose plugin to run the containers before exe-\ncuting the tests and stop the containers once the tests complete:\napply plugin: 'docker-compose'\ndockerCompose.isRequiredBy(componentTest)\ncomponentTest.dependsOn(assemble)\ndockerCompose {\nstartedServices = [ 'ftgo-order-service']\n}\nThe preceding snippet of Gradle configuration does two things. First, it configures\nthe Gradle Docker Compose plugin to run before the component tests and start\nOrder Service along with the infrastructure services that it’s configured to depend\non. Second, it configures componentTest to depend on assemble so that the JAR file\nrequired by the Docker image is built first. With that in place, we can run these com-\nponent tests with the following commands:\n./gradlew\n:ftgo-order-service:componentTest\n \n",
      "content_length": 2175,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 375,
      "content": "345\nWriting end-to-end tests\nThose commands, which take a couple of minutes, perform the following actions:\n1\nBuild Order Service.\n2\nRun the service and its infrastructure services.\n3\nRun the tests.\n4\nStop the running services.\nNow that we’ve looked at how to test a service in isolation, we’ll see how to test the\nentire application. \n10.3\nWriting end-to-end tests\nComponent testing tests each service separately. End-to-end testing, though, tests the\nentire application. As figure 10.8 shows, end-to-end testing is the top of the test pyra-\nmid. That’s because these kinds of tests are—say it with me now—slow, brittle, and\ntime consuming to develop.\nEnd-to-end tests have a large number of moving parts. You must deploy multiple ser-\nvices and their supporting infrastructure services. As a result, end-to-end tests are slow.\nAlso, if your test needs to deploy a large number of services, there’s a good chance\none of them will fail to deploy, making the tests unreliable. Consequently, you should\nminimize the number of end-to-end tests.\n10.3.1 Designing end-to-end tests\nAs I’ve explained, it’s best to write as few of these as possible. A good strategy is to\nwrite user journey tests. A user journey test corresponds to a user’s journey through the\nsystem. For example, rather than test create order, revise order, and cancel order sep-\narately, you can write a single test that does all three. This approach significantly\nreduces the number of tests you must write and shortens the test execution time. \nEnd-to-end\nComponent\nIntegration\nUnit\nEnd-to-end\ntest\nTests\nService\nService 1\nService 2\nService ...\nFigure 10.8\nEnd-to-end tests are at the top of the test pyramid. They are slow, brittle, and time \nconsuming to develop. You should minimize the number of end-to-end tests.\n \n",
      "content_length": 1786,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 376,
      "content": "346\nCHAPTER 10\nTesting microservices: Part 2\n10.3.2 Writing end-to-end tests\nEnd-to-end tests are, like the acceptance tests covered in section 10.2, business-facing\ntests. It makes sense to write them in a high-level DSL that’s understood by the busi-\nness people. You can, for example, write the end-to-end tests using Gherkin and exe-\ncute them using Cucumber. The following listing shows an example of such a test. It’s\nsimilar to the acceptance tests we looked at earlier. The main difference is that rather\nthan a single Then, this test has multiple actions.\nFeature: Place Revise and Cancel\nAs a consumer of the Order Service\nI should be able to place, revise, and cancel an order\nScenario: Order created, revised, and cancelled\nGiven a valid consumer\nGiven using a valid credit card\nGiven the restaurant is accepting orders\nWhen I place an order for Chicken Vindaloo at Ajanta\n  \nThen the order should be APPROVED\nThen the order total should be 16.33\nAnd when I revise the order by adding 2 vegetable samosas   \nThen the order total should be 20.97\nAnd when I cancel the order\nThen the order should be CANCELLED\n      \nThis scenario places an order, revises it, and then cancels it. Let’s look at how to run it. \n10.3.3 Running end-to-end tests\nEnd-to-end tests must run the entire application, including any required infrastruc-\nture services. As you saw in earlier in section 10.2, the Gradle Docker Compose plugin\nprovides a convenient way to do this. Instead of running a single application service,\nthough, the Docker Compose file runs all the application’s services.\n Now that we’ve looked at different aspects of designing and writing end-to-end\ntests, let’s see an example end-to-end test.\n The ftgo-end-to-end-test module implements the end-to-end tests for the FTGO\napplication. The implementation of the end-to-end test is quite similar to the imple-\nmentation of the component tests discussed earlier in section 10.2. These tests are\nwritten using Gherkin and executed using Cucumber. The Gradle Docker Compose\nplugin runs the containers before the tests run. It takes around four to five minutes to\nstart the containers and run the tests.\n That may not seem like a long time, but this is a relatively simple application with\njust a handful of containers and tests. Imagine if there were hundreds of containers\nand many more tests. The tests could take quite a long time. Consequently, it’s best to\nfocus on writing tests that are lower down the pyramid. \nListing 10.17\nA Gherkin-based specification of a user journey\nCreate \nOrder.\nRevise \nOrder.\nCancel \nOrder.\n \n",
      "content_length": 2585,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 377,
      "content": "347\nSummary\nSummary\nUse contracts, which are example messages, to drive the testing of interactions\nbetween services. Rather than write slow-running tests that run both services\nand their transitive dependencies, write tests that verify that the adapters of\nboth services conform to the contracts.\nWrite component tests to verify the behavior of a service via its API. You should\nsimplify and speed up component tests by testing a service in isolation, using\nstubs for its dependencies.\nWrite user journey tests to minimize the number of end-to-end tests, which are\nslow, brittle, and time consuming. A user journey test simulates a user’s journey\nthrough the application and verifies high-level behavior of a relatively large\nslice of the application’s functionality. Because there are few tests, the amount\nof per-test overhead, such as test setup, is minimized, which speeds up the tests. \n \n",
      "content_length": 898,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 378,
      "content": "348\nDeveloping\nproduction-ready services\nMary and her team felt that they had mastered service decomposition, interservice\ncommunication, transaction management, querying and business logic design, and\ntesting. They were confident that they could develop services that met their func-\ntional requirements. But in order for a service to be ready to be deployed into\nproduction, they needed to ensure that it would also satisfy three critically import-\nant quality attributes: security, configurability, and observability.\nThis chapter covers:\nDeveloping secure services\nApplying the Externalized configuration pattern\nApplying the observability patterns:\n– Health check API\n– Log aggregation\n– Distributed tracing\n– Exception tracking\n– Application metrics\n– Audit logging\nSimplifying the development of services by \napplying the Microservice chassis pattern\n \n",
      "content_length": 864,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 379,
      "content": "349\nDeveloping secure services\n The first quality attribute is application security. It’s essential to develop secure appli-\ncations, unless you want your company to be in the headlines for a data breach. Fortu-\nnately, most aspects of security in a microservice architecture are not any different\nthan in a monolithic application. The FTGO team knew that much of what they had\nlearned over the years developing the monolith also applied to microservices. But the\nmicroservice architecture forces you to implement some aspects of application-level\nsecurity differently. For example, you need to implement a mechanism to pass the\nidentity of the user from one service to another.\n The second quality attribute you must address is service configurability. A service typ-\nically uses one or more external services, such as message brokers and databases. The\nnetwork location and credentials of each external service often depend on the envi-\nronment that the service is running in. You can’t hard-wire the configuration proper-\nties into the service. Instead, you must use an externalized configuration mechanism\nthat provides a service with configuration properties at runtime.\n The third quality attribute is observability. The FTGO team had implemented\nmonitoring and logging for the existing application. But a microservice architecture\nis a distributed system, and that presents some additional challenges. Every request\nis handled by the API gateway and at least one service. Imagine, for example, that\nyou’re trying to determine which of six services is causing a latency issue. Or imag-\nine trying to understand how a request is handled when the log entries are scattered\nacross five different services. In order to make it easier to understand the behavior\nof your application and troubleshoot problems, you must implement several observ-\nability patterns.\n I begin this chapter by describing how to implement security in a microservice\narchitecture. Next, I discuss how to design services that are configurable. I cover a\ncouple of different service configuration mechanisms. After that I talk about how to\nmake your services easier to understand and troubleshoot by using the observability\npatterns. I end the chapter by showing how to simplify the implementation of these\nand other concerns by developing your services on top of a microservice chassis\nframework.\n Let’s first look at security.\n11.1\nDeveloping secure services\nCybersecurity has become a critical issue for every organization. Almost every day\nthere are headlines about how hackers have stolen a company’s data. In order to\ndevelop secure software and stay out of the headlines, an organization needs to\ntackle a diverse range of security issues, including physical security of the hardware,\nencryption of data in transit and at rest, authentication and authorization, and pol-\nicies for patching software vulnerabilities. Most of these issues are the same regard-\nless of whether you’re using a monolithic or microservice architecture. This section\nfocuses on how the microservice architecture impacts security at the application\nlevel.\n \n",
      "content_length": 3114,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 380,
      "content": "350\nCHAPTER 11\nDeveloping production-ready services\n An application developer is primarily responsible for implementing four different\naspects of security:\nAuthentication—Verifying the identity of the application or human (a.k.a. the\nprincipal) that’s attempting to access the application. For example, an applica-\ntion typically verifies a principal’s credentials, such as a user ID and password or\nan application’s API key and secret.\nAuthorization—Verifying that the principal is allowed to perform the requested\noperation on the specified data. Applications often use a combination of role-\nbased security and access control lists (ACLs). Role-based security assigns each\nuser one or more roles that grant them permission to invoke particular opera-\ntions. ACLs grant users or roles permission to perform an operation on a partic-\nular business object, or aggregate.\nAuditing—Tracking the operations that a principal performs in order to detect\nsecurity issues, help customer support, and enforce compliance.\nSecure interprocess communication—Ideally, all communication in and out of ser-\nvices should be over Transport Layer Security (TLS). Interservice communica-\ntion may even need to use authentication.\nI describe auditing in detail in section 11.3 and touch on securing interservice com-\nmunication when discussing service meshes in section 11.4.1. This section focuses on\nimplementing authentication and authorization.\n I begin by first describing how security is implemented in the FTGO monolith\napplication. I then describe the challenges with implementing security in a microser-\nvice architecture and how techniques that work well in a monolithic architecture can’t\nbe used in a microservice architecture. After that I cover how to implement security in\na microservice architecture.\n Let’s start by reviewing how the monolithic FTGO application handles security.\n11.1.1 Overview of security in a traditional monolithic application\nThe FTGO application has several kinds of human users, including consumers, cou-\nriers, and restaurant staff. They access the application using browser-based web\napplications and mobile applications. All FTGO users must log in to access the appli-\ncation. Figure 11.1 shows how the clients of the monolithic FTGO application authen-\nticate and make requests.\n When a user logs in with their user ID and password, the client makes a POST\nrequest containing the user’s credentials to the FTGO application. The FTGO appli-\ncation verifies the credentials and returns a session token to the client. The client\nincludes the session token in each subsequent request to the FTGO application.\n Figure 11.2 shows a high-level view of how the FTGO application implements secu-\nrity. The FTGO application is written in Java and uses the Spring Security framework,\nbut I’ll describe the design using generic terms that are applicable to other frame-\nworks, such as Passport for NodeJS.\n \n",
      "content_length": 2926,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 381,
      "content": "351\nDeveloping secure services\nOne key part of the security architecture is the session, which stores the principal’s ID\nand roles. The FTGO application is a traditional Java EE application, so the session is\nan HttpSession in-memory session. A session is identified by a session token, which the\nclient includes in each request. It’s usually an opaque token such as a cryptographi-\ncally strong random number. The FTGO application’s session token is an HTTP\ncookie called JSESSIONID.\n The other key part of the security implementation is the security context, which\nstores information about the user making the current request. The Spring Security\nUsing a security framework\nImplementing authentication and authorization correctly is challenging. It’s best to\nuse a proven security framework. Which framework to use depends on your applica-\ntion’s technology stack. Some popular frameworks include the following:\nSpring Security (https://projects.spring.io/spring-security/)—A popular frame-\nwork for Java applications. It’s a sophisticated framework that handles authen-\ntication and authorization.\nApache Shiro (https://shiro.apache.org)—Another Java framework.\nPassport (http://www.passportjs.org)—A popular security framework for NodeJS\napplications that’s focused on authentication.\nLog in to obtain session\ntoken, which is a cookie.\nInclude session token cookie,\nwhich identiﬁes the user, in\nsubsequent requests.\nConsumer\nrestaurant\ncourier\nBrowser\nor mobile\napplication\nPOST /login\nid=...\npassword=...\nHTTP/1.1 200 OK\nSet-cookie: JSESSIONID=...\n...\nGET /orders/order-xyz\nCookie: JSESSIONID=...\nFTGO\napplication\nFigure 11.1\nA client of the FTGO application first logs in to obtain a session token, which is often a \ncookie. The client includes the session token in each subsequent request it makes to the application.\n \n",
      "content_length": 1830,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 382,
      "content": "352\nCHAPTER 11\nDeveloping production-ready services\nframework uses the standard Java EE approach of storing the security context in a\nstatic, thread-local variable, which is readily accessible to any code that’s invoked to han-\ndle the request. A request handler can call SecurityContextHolder.getContext()\n.getAuthentication() to obtain information about the current user, such as their\nidentity and roles. In contrast, the Passport framework stores the security context as\nthe user attribute of the request.\nThe sequence of events shown in Figure 11.2 is as follows:\n1\nThe client makes a login request to the FTGO application.\n2\nThe login request is handled by LoginHandler, which verifies the credentials, cre-\nates the session, and stores information about the principal in the session.\n3\nLogin Handler returns a session token to the client.\n4\nThe client includes the session token in requests that invoke operations.\n5\nThese requests are first processed by SessionBasedSecurityInterceptor. The\ninterceptor authenticates each request by verifying the session token and estab-\nlishes a security context. The security context describes the principal and its roles.\nUser\ndatabase\nLog in with user ID\nand password.\nInitializes\nProvides session cookie\nEstablishes\nReads\nReturn session cookie.\nJane\nLogin-based\nclient\nSessionBased\nSecurity\nInterceptor\nOrderDetails\nRequestHandler\nUserId: jane\nrules: [CONSUMER]\n...\nUserId: jane\nrules: [CONSUMER]\n...\nLogin\nhandler\nPOST /login\nuserId-Jane&password=..\nHTTP/1.1 200 OK\nSet-cookie: JSESSIONID=...\n...\nGET /orders/order-xyz\nCookie: JSESSIONID=...\nFTGO application\nRetrieves user information\nfrom database\nReads\nEstablishes\nSecurity context\nSession\nFigure 11.2\nWhen a client of the FTGO application makes a login request, Login Handler authenticates the \nuser, initializes the session user information, and returns a session token cookie, which securely identifies the \nsession. Next, when the client makes a request containing the session token, SessionBasedSecurity-\nInterceptor retrieves the user information from the specified session and establishes the security context. \nRequest handlers, such as OrderDetailsRequestHandler, retrieve the user information from the security \ncontext.\n \n",
      "content_length": 2234,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 383,
      "content": "353\nDeveloping secure services\n6\nA request handler uses the security context to determine whether to allow a\nuser to perform the requested operation and obtain their identity.\nThe FTGO application uses role-based authorization. It defines several roles corre-\nsponding to the different kinds of users, including CONSUMER, RESTAURANT, COURIER,\nand ADMIN. It uses Spring Security’s declarative security mechanism to restrict access to\nURLs and service methods to specific roles. Roles are also interwoven into the busi-\nness logic. For example, a consumer can only access their orders, whereas an adminis-\ntrator can access all orders.\n The security design used by the monolithic FTGO application is only one possible\nway to implement security. For example, one drawback of using an in-memory session\nis that it requires all requests for a particular session to be routed to the same applica-\ntion instance. This requirement complicates load balancing and operations. You must,\nfor example, implement a session draining mechanism that waits for all sessions to\nexpire before shutting down an application instance. An alternative approach, which\navoids these problems, is to store the session in a database.\n You can sometimes eliminate the server-side session entirely. For example, many\napplications have API clients that provide their credentials, such as an API key and\nsecret, in every request. As a result, there’s no need to maintain a server-side session.\nAlternatively, the application can store session state in the session token. Later in\nthis section, I describe one way to use a session token to store the session state. But\nlet’s begin by looking at the challenges of implementing security in a microservice\narchitecture. \n11.1.2 Implementing security in a microservice architecture\nA microservice architecture is a distributed architecture. Each external request is han-\ndled by the API gateway and at least one service. Consider, for example, the get-\nOrderDetails() query, discussed in chapter 8. The API gateway handles this query by\ninvoking several services, including Order Service, Kitchen Service, and Accounting\nService. Each service must implement some aspects of security. For instance, Order\nService must only allow a consumer to see their orders, which requires a combina-\ntion of authentication and authorization. In order to implement security in a micros-\nervice architecture we need to determine who is responsible for authenticating the\nuser and who is responsible for authorization.\n One challenge with implementing security in a microservices application is that we\ncan’t just copy the design from a monolithic application. That’s because two aspects of\nthe monolithic application’s security architecture are nonstarters for a microservice\narchitecture:\nIn-memory security context—Using an in-memory security context, such as a thread-\nlocal, to pass around user identity. Services can’t share memory, so they can’t\nuse an in-memory security context, such as a thread-local, to pass around the\n \n",
      "content_length": 3029,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 384,
      "content": "354\nCHAPTER 11\nDeveloping production-ready services\nuser identity. In a microservice architecture, we need a different mechanism for\npassing user identity from one service to another.\nCentralized session —Because an in-memory security context doesn’t make sense,\nneither does an in-memory session. In theory, multiple services could access a\ndatabase-based session, except that it would violate the principle of loose cou-\npling. We need a different session mechanism in a microservice architecture.\nLet’s begin our exploration of security in a microservice architecture by looking at\nhow to handle authentication.\nHANDLING AUTHENTICATION IN THE API GATEWAY\nThere are a couple of different ways to handle authentication. One option is for the\nindividual services to authenticate the user. The problem with this approach is that it\npermits unauthenticated requests to enter the internal network. It relies on every\ndevelopment team correctly implementing security in all of their services. As a result,\nthere’s a significant risk of an application containing security vulnerabilities.\n Another problem with implementing authentication in the services is that differ-\nent clients authenticate in different ways. Pure API clients supply credentials with\neach request using, for example, basic authentication. Other clients might first log in\nand then supply a session token with each request. We want to avoid requiring services\nto handle a diverse set of authentication mechanisms.\n A better approach is for the API gateway to authenticate a request before forward-\ning it to the services. Centralizing API authentication in the API gateway has the\nadvantage that there’s only one place to get right. As a result, there’s a much smaller\nchance of a security vulnerability. Another benefit is that only the API gateway has to\ndeal with the various different authentication mechanisms. It hides this complexity\nfrom the services.\n Figure 11.3 shows how this approach works. Clients authenticate with the API gate-\nway. API clients include credentials in each request. Login-based clients POST the\nuser’s credentials to the API gateway’s authentication and receive a session token.\nOnce the API gateway has authenticated a request, it invokes one or more services.\nA service invoked by the API gateway needs to know the principal making the request.\nIt must also verify that the request has been authenticated. The solution is for the API\ngateway to include a token in each service request. The service uses the token to vali-\ndate the request and obtain information about the principal. The API gateway might\nalso give the same token to session-oriented clients to use as the session token.\nPattern: Access token\nThe API gateway passes a token containing information about the user, such as their\nidentity and their roles, to the services that it invokes. See http://microservices.io/\npatterns/security/access-token.html.\n \n",
      "content_length": 2921,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 385,
      "content": "355\nDeveloping secure services\nThe sequence of events for API clients is as follows:\n1\nA client makes a request containing credentials.\n2\nThe API gateway authenticates the credentials, creates a security token, and\npasses that to the service or services.\nThe sequence of events for login-based clients is as follows:\n1\nA client makes a login request containing credentials.\n2\nThe API gateway returns a security token.\n3\nThe client includes the security token in requests that invoke operations.\n4\nThe API gateway validates the security token and forwards it to the service or\nservices.\nA little later in this chapter, I describe how to implement tokens, but let’s first look at\nthe other main aspect of security: authorization. \nOrder\nService\nAPI clients supply credentials\nin the Authorization header.\nPass token to services so\nthat they can identify and\nauthorize the user.\nInclude the security token\nin each request.\nLogin clients ﬁrst obtain\na security token.\nAuthentication\nInterceptor\nAPI gateway\nLogin-based\nclient\nGET /orders/1\nAuthorization: ...CREDENTIALS...\n...\nGET /orders/1\n...SECURITY_TOKEN...\nHTTP/1.1 200 OK\n...SECURITY_TOKEN...\nGET /orders/1\n...SECURITY_TOKEN...\nPOST /login\nid=...\npassword=...\nAPI client\nFigure 11.3\nThe API gateway authenticates requests from clients and includes a security token in the requests \nit makes to services. The services use the token to obtain information about the principal. The API gateway can \nalso use the security token as a session token.\n \n",
      "content_length": 1497,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 386,
      "content": "356\nCHAPTER 11\nDeveloping production-ready services\nHANDLING AUTHORIZATION\nAuthenticating a client’s credentials is important but insufficient. An application\nmust also implement an authorization mechanism that verifies that the client is\nallowed to perform the requested operation. For example, in the FTGO application\nthe getOrderDetails() query can only be invoked by the consumer who placed the\nOrder (an example of instance-based security) and a customer service agent who is\nhelping the consumer.\n One place to implement authorization is the API gateway. It can, for example,\nrestrict access to GET /orders/{orderId} to only users who are consumers and cus-\ntomer service agents. If a user isn’t allowed to access a particular path, the API gateway\ncan reject the request before forwarding it on to the service. As with authentication,\ncentralizing authorization within the API gateway reduces the risk of security vulnera-\nbilities. You can implement authorization in the API gateway using a security frame-\nwork, such as Spring Security.\n One drawback of implementing authorization in the API gateway is that it risks\ncoupling the API gateway to the services, requiring them to be updated in lockstep.\nWhat’s more, the API gateway can typically only implement role-based access to URL\npaths. It’s generally not practical for the API gateway to implement ACLs that control\naccess to individual domain objects, because that requires detailed knowledge of a ser-\nvice’s domain logic.\n The other place to implement authorization is in the services. A service can imple-\nment role-based authorization for URLs and for service methods. It can also implement\nACLs to manage access to aggregates. Order Service can, for example, implement the\nrole-based and ACL-based authorization mechanism for controlling access to orders.\nOther services in the FTGO application implement similar authorization logic. \nUSING JWTS TO PASS USER IDENTITY AND ROLES\nWhen implementing security in a microservice architecture, you need to decide which\ntype of token an API gateway should use to pass user information to the services.\nThere are two types of tokens to choose from. One option is to use opaque tokens,\nwhich are typically UUIDs. The downside of opaque tokens is that they reduce perfor-\nmance and availability and increase latency. That’s because the recipient of such a\ntoken must make a synchronous RPC call to a security service to validate the token\nand retrieve the user information.\n An alternative approach, which eliminates the call to the security service, is to use a\ntransparent token containing information about the user. One such popular standard\nfor transparent tokens is the JSON Web Token (JWT). JWT is standard way to securely\nrepresent claims, such as user identity and roles, between two parties. A JWT has a pay-\nload, which is a JSON object that contains information about the user, such as their\nidentity and roles, and other metadata, such as an expiration date. It’s signed with a\nsecret that’s only known to the creator of the JWT, such as the API gateway and the\nrecipient of the JWT, such as a service. The secret ensures that a malicious third party\ncan’t forge or tamper with a JWT.\n \n",
      "content_length": 3209,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 387,
      "content": "357\nDeveloping secure services\n One issue with JWT is that because a token is self-contained, it’s irrevocable. By\ndesign, a service will perform the request operation after verifying the JWT’s signature\nand expiration date. As a result, there’s no practical way to revoke an individual JWT\nthat has fallen into the hands of a malicious third party. The solution is to issue JWTs\nwith short expiration times, because that limits what a malicious party could do. One\ndrawback of short-lived JWTs, though, is that the application must somehow continually\nreissue JWTs to keep the session active. Fortunately, this is one of the many protocols\nthat are solved by a security standard calling OAuth 2.0. Let’s look at how that works. \nUSING OAUTH 2.0 IN A MICROSERVICE ARCHITECTURE\nLet’s say you want to implement a User Service for the FTGO application that man-\nages a user database containing user information, such as credentials and roles. The\nAPI gateway calls the User Service to authenticate a client request and obtain a JWT.\nYou could design a User Service API and implement it using your favorite web frame-\nwork. But that’s generic functionality that isn’t specific to the FTGO application—\ndeveloping such a service wouldn’t be an efficient use of development resources.\n Fortunately, you don’t need to develop this kind of security infrastructure. You can\nuse an off-the-shelf service or framework that implements a standard called OAuth 2.0.\nOAuth 2.0 is an authorization protocol that was originally designed to enable a user of\na public cloud service, such as GitHub or Google, to grant a third-party application\naccess to its information without revealing its password. For example, OAuth 2.0 is the\nmechanism that enables you to securely grant a third party cloud-based Continuous\nIntegration (CI) service access to your GitHub repository.\n Although the original focus of OAuth 2.0 was authorizing access to public cloud\nservices, you can also use it for authentication and authorization in your application.\nLet’s take a quick look at how a microservice architecture might use OAuth 2.0.\nThe key concepts in OAuth 2.0 are the following:\n\nAuthorization Server—Provides an API for authenticating users and obtain-\ning an access token and a refresh token. Spring OAuth is a great example of a\nframework for building an OAuth 2.0 authorization server.\n\nAccess Token—A token that grants access to a Resource Server. The format of\nthe access token is implementation dependent. But some implementations,\nsuch as Spring OAuth, use JWTs.\nAbout OAuth 2.0\nOAuth 2.0 is a complex topic. In this chapter, I can only provide a brief overview and\ndescribe how it can be used in a microservice architecture. For more information\non OAuth 2.0, check out the online book OAuth 2.0 Servers by Aaron Parecki\n(www.oauth.com). Chapter 7 of Spring Microservices in Action (Manning, 2017) also\ncovers this topic (https://livebook.manning.com/#!/book/spring-microservices-in-\naction/chapter-7/).\n \n",
      "content_length": 2990,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 388,
      "content": "358\nCHAPTER 11\nDeveloping production-ready services\n\nRefresh Token—A long-lived yet revocable token that a Client uses to obtain a\nnew AccessToken.\n\nResource Server—A service that uses an access token to authorize access. In a\nmicroservice architecture, the services are resource servers.\n\nClient—A client that wants to access a Resource Server. In a microservice\narchitecture, API Gateway is the OAuth 2.0 client.\nLater in this section, I describe how to support login-based clients. But first, let’s talk\nabout how to authenticate API clients.\n Figure 11.4 shows how the API gateway authenticates a request from an API client.\nThe API gateway authenticate the API client by making a request to the OAuth 2.0\nauthorization server, which returns an access token. The API gateway then makes one\nor more requests containing the access token to the services.\n The sequence of events shown in figure 11.4 is as follows:\n1\nThe client makes a request, supplying its credentials using basic authentication.\n2\nThe API gateway makes an OAuth 2.0 Password Grant request (www.oauth.com/\noauth2-servers/access-tokens/password-grant/) to the OAuth 2.0 authentication\nserver.\nOrder\nService\nUser\ndatabase\nContains the user\nID and their roles\nPassword grant request\nAPI gateway\nSpring OAuth2\nAuthentication\nServer\nGET /orders/1\nAuthorization: Basic...\n....\nPOST/oauth/token\nuserid=...&password=...\nGET /orders/1\nAuthorization: Bearer AccessToken\nHTTP/1.1 200 OK\n...\n{\n\"access_token\": \"AccessToken\"\n...\n}\nAPI client\nFigure 11.4\nAn API gateway authenticates an API client by making a Password Grant request to the OAuth 2.0 \nauthentication server. The server returns an access token, which the API gateway passes to the services. A service \nverifies the token’s signature and extracts information about the user, including their identity and roles.\n \n",
      "content_length": 1836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 389,
      "content": "359\nDeveloping secure services\n3\nThe authentication server validates the API client’s credentials and returns an\naccess token and a refresh token.\n4\nThe API gateway includes the access token in the requests it makes to the ser-\nvices. A service validates the access token and uses it to authorize the request.\nAn OAuth 2.0-based API gateway can authenticate session-oriented clients by using an\nOAuth 2.0 access token as a session token. What’s more, when the access token\nexpires, it can obtain a new access token using the refresh token. Figure 11.5 shows\nhow an API gateway can use OAuth 2.0 to handle session-oriented clients. An API cli-\nent initiates a session by POSTing its credentials to the API gateway’s /login end-\npoint. The API gateway returns an access token and a refresh token to the client. The\nAPI client then supplies both tokens when it makes requests to the API gateway.\nThe sequence of events is as follows:\n1\nThe login-based client POSTs its credentials to the API gateway.\n2\nThe API gateway’s Login Handler makes an OAuth 2.0 Password Grant request\n(www.oauth.com/oauth2-servers/access-tokens/password-grant/) to the OAuth\n2.0 authentication server.\nOrder\nService\nUser\ndatabase\nPassword grant request\nAPI gateway\nSpring OAuth2\nAuthentication\nServer\nPOST/login\nuserId=...&password=...\nGET/orders/1\nCookie: access_token=...;refresh_token...\nHTTP/1.1 200 OK\nSet-Cookie: access_token=...\nSet-Cookie:refresh_token=...\nPOST/oauth/token\nuserid=...&password=...\nGET /orders/1\nAuthorization: Bearer AccessToken\nHTTP/1.1 200 OK\n...\n{\n\"access_token\": \"AccessToken\"\n...\n}\nLogin-based\nclient\nLogin\nhandler\nSession\nauthentication\ninterceptor\nFigure 11.5\nA client logs in by POSTing its credentials to the API gateway. The API gateway authenticates the \ncredentials using the OAuth 2.0 authentication server and returns the access token and refresh token as cookies. \nA client includes these tokens in the requests it makes to the API gateway.\n \n",
      "content_length": 1956,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 390,
      "content": "360\nCHAPTER 11\nDeveloping production-ready services\n3\nThe authentication server validates the client’s credentials and returns an access\ntoken and a refresh token.\n4\nThe API gateway returns the access and refresh tokens to the client—as cookies,\nfor example.\n5\nThe client includes the access and refresh tokens in requests it makes to the API\ngateway.\n6\nThe API gateway’s Session Authentication Interceptor validates the access\ntoken and includes it in requests it makes to the services.\nIf the access token has expired or is about to expire, the API gateway obtains a new\naccess token by making an OAuth 2.0 Refresh Grant request (www.oauth.com/\noauth2-servers/access-tokens/refreshing-access-tokens/), which contains the refresh\ntoken, to the authorization server. If the refresh token hasn’t expired or been revoked,\nthe authorization server returns a new access token. API Gateway passes the new\naccess token to the services and returns it to the client.\n An important benefit of using OAuth 2.0 is that it’s a proven security standard.\nUsing an off-the-shelf OAuth 2.0 Authentication Server means you don’t have to\nwaste time reinventing the wheel or risk developing an insecure design. But OAuth\n2.0 isn’t the only way to implement security in a microservice architecture. Regardless\nof which approach you use, the three key ideas are as follows:\nThe API gateway is responsible for authenticating clients.\nThe API gateway and the services use a transparent token, such as a JWT, to pass\naround information about the principal.\nA service uses the token to obtain the principal’s identity and roles.\nNow that we’ve looked at how to make services secure, let’s see how to make them\nconfigurable. \n11.2\nDesigning configurable services\nImagine that you’re responsible for Order History Service. As figure 11.6 shows, the\nservice consumes events from Apache Kafka and reads and writes AWS DynamoDB\ntable items. In order for this service to run, it needs various configuration properties,\nincluding the network location of Apache Kafka and the credentials and network loca-\ntion for AWS DynamoDB.\n The values of these configuration properties depend on which environment the\nservice is running in. For example, the developer and production environments will\nuse different Apache Kafka brokers and different AWS credentials. It doesn’t make\nsense to hard-wire a particular environment’s configuration property values into the\ndeployable service because that would require it to be rebuilt for each environment.\nInstead, a service should be built once by the deployment pipeline and deployed into\nmultiple environments.\n Nor does it make sense to hard-wire different sets of configuration properties into\nthe source code and use, for example, the Spring Framework’s profile mechanism to\n \n",
      "content_length": 2789,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 391,
      "content": "361\nDesigning configurable services\nselect the appropriate set at runtime. That’s because doing so would introduce a secu-\nrity vulnerability and limit where it can be deployed. Additionally, sensitive data such\nas credentials should be stored securely using a secrets storage mechanism, such as\nHashicorp Vault (www.vaultproject.io) or AWS Parameter Store (https://docs.aws\n.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html).\nInstead, you should supply the appropriate configuration properties to the service at\nruntime by using the Externalized configuration pattern.\nAn externalized configuration mechanism provides the configuration property values\nto a service instance at runtime. There are two main approaches:\nPush model—The deployment infrastructure passes the configuration properties\nto the service instance using, for example, operating system environment vari-\nables or a configuration file.\nPull model—The service instance reads its configuration properties from a con-\nfiguration server.\nWe’ll look at each approach, starting with the push model.\nPattern: Externalized configuration\nSupply configuration property values, such as database credentials and network\nlocation, to a service at runtime. See http://microservices.io/patterns/externalized-\nconfiguration.html.\nOrder\nHistory\nService\nEnvironment-speciﬁc conﬁguration\nEnvironment-speciﬁc conﬁguration\nApache\nKafka\nconsumer\nApache Kafka\nbootstrap.servers=kafka1:9092\n..\naws.access.key.id=...\naws.secret.access.key=...\naws.region=...\n«Order event channel»\nDynamoDB\nadapter\nAWS DynamoDB\n«Delivery event channel»\nFigure 11.6\nOrder History Service uses Apache Kafka and AWS DynamoDB. It needs to be \nconfigured with each service’s network location, credentials, and so on.\n \n",
      "content_length": 1769,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 392,
      "content": "362\nCHAPTER 11\nDeveloping production-ready services\n11.2.1 Using push-based externalized configuration\nThe push model relies on the collaboration of the deployment environment and the\nservice. The deployment environment supplies the configuration properties when it\ncreates a service instance. It might, as figure 11.7 shows, pass the configuration prop-\nerties as environment variables. Alternatively, the deployment environment may sup-\nply the configuration properties using a configuration file. The service instance then\nreads the configuration properties when it starts up.\nThe deployment environment and the service must agree on how the configuration\nproperties are supplied. The precise mechanism depends on the specific deployment\nenvironment. For example, chapter 12 describes how you can specify the environment\nvariables of a Docker container.\n Let’s imagine that you’ve decided to supply externalized configuration property\nvalues using environment variables. Your application could call System.getenv() to\nobtain their values. But if you’re a Java developer, it’s likely that you’re using a frame-\nwork that provides a more convenient mechanism. The FTGO services are built using\nSpring Boot, which has an extremely flexible externalized configuration mechanism\nthat retrieves configuration properties from a variety of sources with well-defined pre-\ncedence rules (https://docs.spring.io/spring-boot/docs/current/reference/html/boot-\nfeatures-external-config.html). Let’s look at how it works.\n Spring Boot reads properties from a variety of sources. I find the following sources\nuseful in a microservice architecture:\n \nOrder\nHistory Service\ninstance\nProcess\nEnvironment variables\nDeployment\ninfrastructure\nConﬁgures\nCreates\nReads\nBOOTSTRAP_SERVERS=kafka1:9092\nAWS_ACCESS_KEY_ID=\nAWS_SECRET_ACCESS_KEY=...\nAWS_REGION=...\n....\nFigure 11.7\nWhen the deployment infrastructure creates an instance of Order History \nService, it sets the environment variables containing the externalized configuration. Order \nHistory Service reads those environment variables.\n \n",
      "content_length": 2074,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 393,
      "content": "363\nDesigning configurable services\n1\nCommand-line arguments\n2\nSPRING_APPLICATION_JSON, an operating system environment variable or JVM\nsystem property that contains JSON\n3\nJVM System properties\n4\nOperating system environment variables\n5\nA configuration file in the current directory\nA particular property value from a source earlier in this list overrides the same prop-\nerty from a source later in this list. For example, operating system environment vari-\nables override properties read from a configuration file.\n Spring Boot makes these properties available to the Spring Framework’s\nApplicationContext. A service can, for example, obtain the value of a property using\nthe @Value annotation:\npublic class OrderHistoryDynamoDBConfiguration {\n@Value(\"${aws.region}\")\nprivate String awsRegion;\nThe Spring Framework initializes the awsRegion field to the value of the aws.region\nproperty. This property is read from one of the sources listed earlier, such as a config-\nuration file or from the AWS_REGION environment variable.\n The push model is an effective and widely used mechanism for configuring a ser-\nvice. One limitation, however, is that reconfiguring a running service might be chal-\nlenging, if not impossible. The deployment infrastructure might not allow you to\nchange the externalized configuration of a running service without restarting it. You\ncan’t, for example, change the environment variables of a running process. Another\nlimitation is that there’s a risk of the configuration property values being scattered\nthroughout the definition of numerous services. As a result, you may want to consider\nusing a pull-based model. Let’s look at how it works. \n11.2.2 Using pull-based externalized configuration\nIn the pull model, a service instance reads its configuration properties from a configura-\ntion server. Figure 11.8 shows how it works. On startup, a service instance queries the\nconfiguration service for its configuration. The configuration properties for accessing\nthe configuration server, such as its network location, are provided to the service\ninstance via a push-based configuration mechanism, such as environment variables.\n There are a variety of ways to implement a configuration server, including the\nfollowing:\nVersion control system such as Git\nSQL and NoSQL databases\nSpecialized configuration servers, such as Spring Cloud Config Server, Hashicorp\nVault, which is a store for sensitive data such as credentials, and AWS Parameter\nStore\n \n",
      "content_length": 2481,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 394,
      "content": "364\nCHAPTER 11\nDeveloping production-ready services\nThe Spring Cloud Config project is a good example of a configuration server-based\nframework. It consists of a server and a client. The server supports a variety of backends\nfor storing configuration properties, including version control systems, databases, and\nHashicorp Vault. The client retrieves configuration properties from the server and\ninjects them into the Spring ApplicationContext.\n Using a configuration server has several benefits:\nCentralized configuration—All the configuration properties are stored in one\nplace, which makes them easier to manage. What’s more, in order to eliminate\nduplicate configuration properties, some implementations let you define global\ndefaults, which can be overridden on a per-service basis.\nTransparent decryption of sensitive data—Encrypting sensitive data such as database\ncredentials is a security best practice. One challenge of using encryption, though,\nis that usually the service instance needs to decrypt them, which means it needs\nthe encryption keys. Some configuration server implementations automatically\ndecrypt properties before returning them to the service.\nDynamic reconfiguration—A service could potentially detect updated property val-\nues by, for example, polling, and reconfigure itself.\nThe primary drawback of using a configuration server is that unless it’s provided by\nthe infrastructure, it’s yet another piece of infrastructure that needs to be set up and\nmaintained. Fortunately, there are various open source frameworks, such as Spring\nCloud Config, which make it easier to run a configuration server.\n Now that we’ve looked at how to design configurable services, let’s talk about how\nto design observable services. \n11.3\nDesigning observable services\nLet’s say you’ve deployed the FTGO application into production. You probably want\nto know what the application is doing: requests per second, resource utilization, and\nOrder\nHistory Service\ninstance\nProcess\nConﬁgures\nCreates\nCONFIG_SERVER_URL=...\ngetConﬁguration(“orderHistoryService”)\nBOOTSTRAP_SERVERS=kafka1:9092\nAWS_ACCESS_KEY_ID=\nAWS_SECRET_ACCESS_KEY=...\nAWS_REGION=...\n....\nEnvironment variables\nDeployment\ninfrastructure\nConﬁguration\nserver\nFigure 11.8\nOn startup, a service instance retrieves its configuration properties from a configuration server. The \ndeployment infrastructure provides the configuration properties for accessing the configuration server.\n \n",
      "content_length": 2453,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 395,
      "content": "365\nDesigning observable services\nso on. You also need to be alerted if there’s a problem, such as a failed service instance\nor a disk filling up—ideally before it impacts a user. And, if there’s a problem, you\nneed to be able to troubleshoot and identify the root cause.\n Many aspects of managing an application in production are outside the scope of\nthe developer, such as monitoring hardware availability and utilization. These are\nclearly the responsibility of operations. But there are several patterns that you, as a ser-\nvice developer, must implement to make your service easier to manage and trouble-\nshoot. These patterns, shown in figure 11.9, expose a service instance’s behavior and\nhealth. They enable a monitoring system to track and visualize the state of a service\nand generate alerts when there’s a problem. These patterns also make troubleshoot-\ning problems easier.\nYou can use the following patterns to design observable services:\nHealth check API—Expose an endpoint that returns the health of the service.\nLog aggregation—Log service activity and write logs into a centralized logging\nserver, which provides searching and alerting.\nPattern\nparticipant\nKey\nOperations\nresponsibility\nDistributed\ntracing\nserver\nException\nTracking\nService\nLogging\nServer\nLogging\naggregation\npipeline\nLog ﬁle\nMetrics\nService\nDeveloper\nresponsibility\nPattern\nObservable\nService\nDistributed\ntracing\nexporter\nException\nreporter\nMetrics\nexporter\nHealth\ncheck\nAPI\nHealth check\ninvoker, such as\nmonitoring service\nInvokes\nAudit\ndatabase\nadapter\nAuditing\ndatabase\nLogging\nadapter\nDistributed\ntracing pattern\nApplication\nmetrics pattern\nAudit\nlogging pattern\nHealth check\nAPI pattern\nException\ntracking pattern\nLog aggregation\npattern\nFigure 11.9\nThe observability patterns enable developers and operations to understand the behavior of an \napplication and troubleshoot problems. Developers are responsible for ensuring that their services are observable. \nOperations are responsible for the infrastructure that collects the information exposed by the services.\n \n",
      "content_length": 2059,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 396,
      "content": "366\nCHAPTER 11\nDeveloping production-ready services\nDistributed tracing—Assign each external request a unique ID and trace requests\nas they flow between services.\nException tracking—Report exceptions to an exception tracking service, which\nde-duplicates exceptions, alerts developers, and tracks the resolution of each\nexception.\nApplication metrics—Services maintain metrics, such as counters and gauges, and\nexpose them to a metrics server.\nAudit logging—Log user actions.\nA distinctive feature of most of these patterns is that each pattern has a developer\ncomponent and an operations component. Consider, for example, the Health check\nAPI pattern. The developer is responsible for ensuring that their service implements a\nhealth check endpoint. Operations is responsible for the monitoring system that peri-\nodically invokes the health check API. Similarly, for the Log aggregation pattern, a\ndeveloper is responsible for ensuring that their services log useful information,\nwhereas operations is responsible for log aggregation.\n Let’s take a look at each of these patterns, starting with the Health check API pattern.\n11.3.1 Using the Health check API pattern\nSometimes a service may be running but unable to handle requests. For instance, a\nnewly started service instance may not be ready to accept requests. The FTGO Con-\nsumer Service, for example, takes around 10 seconds to initialize the messaging and\ndatabase adapters. It would be pointless for the deployment infrastructure to route\nHTTP requests to a service instance until it’s ready to process them.\n Also, a service instance can fail without terminating. For example, a bug might\ncause an instance of Consumer Service to run out of database connections and\nbe unable to access the database. The deployment infrastructure shouldn’t route\nrequests to a service instance that has failed yet is still running. And, if the service\ninstance does not recover, the deployment infrastructure must terminate it and create\na new instance.\nA service instance needs to be able to tell the deployment infrastructure whether or\nnot it’s able to handle requests. A good solution is for a service to implement a health\ncheck endpoint, which is shown in figure 11.10. The Spring Boot Actuator Java library,\nfor example, implements a GET /actuator/health endpoint, which returns 200 if and\nonly if the service is healthy, and 503 otherwise. Similarly, the HealthChecks .NET\nPattern: Health check API\nA service exposes a health check API endpoint, such as GET /health, which returns\nthe health of the service. See http://microservices.io/patterns/observability/health-\ncheck-api.html.\n \n",
      "content_length": 2640,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 397,
      "content": "367\nDesigning observable services\nlibrary implements a GET /hc endpoint (https://docs.microsoft.com/en-us/dotnet/\nstandard/microservices-architecture/implement-resilient-applications/monitor-app-\nhealth). The deployment infrastructure periodically invokes this endpoint to determine\nthe health of the service instance and takes the appropriate action if it’s unhealthy.\nA Health Check Request Handler typically tests the service instance’s connections to\nexternal services. It might, for example, execute a test query against a database. If all\nthe tests succeed, Health Check Request Handler returns a healthy response, such as\nan HTTP 200 status code. If any of them fails, it returns an unhealthy response, such\nas an HTTP 500 status code.\n Health Check Request Handler might simply return an empty HTTP response with\nthe appropriate status code. Or it might return a detailed description of the health of\neach of the adapters. The detailed information is useful for troubleshooting. But\nbecause it may contain sensitive information, some frameworks, such as Spring Boot\nActuator, let you configure the level of detail in the health endpoint response.\n There are two issues you need to consider when using health checks. The first is\nthe implementation of the endpoint, which must report back on the health of the ser-\nvice instance. The second issue is how to configure the deployment infrastructure to\ninvoke the health check endpoint. Let’s first look at how to implement the endpoint.\nIMPLEMENTING THE HEALTH CHECK ENDPOINT\nThe code that implements the health check endpoint must somehow determine the\nhealth of the service instance. One simple approach is to verify that the service\ninstance can access its external infrastructure services. How to do this depends on the\nService\nChecks\nChecks\nHealth check\ninvoker\nInvokes\nHealth check\nendpoint\nHealth check\nrequest\nhandler\nMessaging\nadapter\nMessage\nbroker\nDatabase\nadapter\nMySQL\nTests the service’s connections\nto infrastructure services\nFor example: monitoring\nsystem, Service registry, and others\nFigure 11.10\nA service implements a health check endpoint, which is periodically invoked by the \ndeployment infrastructure to determine the health of the service instance.\n \n",
      "content_length": 2231,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 398,
      "content": "368\nCHAPTER 11\nDeveloping production-ready services\ninfrastructure service. The health check code can, for example, verify that it’s con-\nnected to an RDBMS by obtaining a database connection and executing a test query.\nA more elaborate approach is to execute a synthetic transaction that simulates the\ninvocation of the service’s API by a client. This kind of health check is more thorough,\nbut it’s likely to be more time consuming to implement and take longer to execute.\n A great example of a health check library is Spring Boot Actuator. As mentioned\nearlier, it implements a /actuator/health endpoint. The code that implements this\nendpoint returns the result of executing a set of health checks. By using convention\nover configuration, Spring Boot Actuator implements a sensible set of health checks\nbased on the infrastructure services used by the service. If, for example, a service uses\na JDBC DataSource, Spring Boot Actuator configures a health check that executes a\ntest query. Similarly, if the service uses the RabbitMQ message broker, it automatically\nconfigures a health check that verifies that the RabbitMQ server is up.\n You can also customize this behavior by implementing additional health checks for\nyour service. You implement a custom health check by defining a class that imple-\nments the HealthIndicator interface. This interface defines a health() method,\nwhich is called by the implementation of the /actuator/health endpoint. It returns\nthe outcome of the health check. \nINVOKING THE HEALTH CHECK ENDPOINT\nA health check endpoint isn’t much use if nobody calls it. When you deploy your ser-\nvice, you must configure the deployment infrastructure to invoke the endpoint. How\nyou do that depends on the specific details of your deployment infrastructure. For\nexample, as described in chapter 3, you can configure some service registries, such as\nNetflix Eureka, to invoke the health check endpoint in order to determine whether\ntraffic should be routed to the service instance. Chapter 12 discusses how to configure\nDocker and Kubernetes to invoke a health check endpoint. \n11.3.2 Applying the Log aggregation pattern\nLogs are a valuable troubleshooting tool. If you want to know what’s wrong with your\napplication, a good place to start is the log files. But using logs in a microservice archi-\ntecture is challenging. For example, imagine you’re debugging a problem with the\ngetOrderDetails() query. As described in chapter 8, the FTGO application imple-\nments this query using API composition. As a result, the log entries you need are scat-\ntered across the log files of the API gateway and several services, including Order\nService and Kitchen Service.\nPattern: Log aggregation\nAggregate the logs of all services in a centralized database that supports searching\nand alerting. See http://microservices.io/patterns/observability/application-logging\n.html.\n \n",
      "content_length": 2890,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 399,
      "content": "369\nDesigning observable services\nThe solution is to use log aggregation. As figure 11.11 shows, the log aggregation pipe-\nline sends the logs of all of the service instances to a centralized logging server. Once\nthe logs are stored by the logging server, you can view, search, and analyze them. You\ncan also configure alerts that are triggered when certain messages appear in the logs.\nThe logging pipeline and server are usually the responsibility of operations. But ser-\nvice developers are responsible for writing services that generate useful logs. Let’s first\nlook at how a service generates a log.\nHOW A SERVICE GENERATES A LOG\nAs a service developer, there are a couple of issues you need to consider. First you\nneed to decide which logging library to use. The second issue is where to write the log\nentries. Let’s first look at the logging library.\n Most programming languages have one or more logging libraries that make it easy\nto generate correctly structured log entries. For example, three popular Java logging\nlibraries are Logback, log4j, and JUL (java.util.logging). There’s also SLF4J, which is a\nlogging facade API for the various logging frameworks. Similarly, Log4JS is a popular\nlogging framework for NodeJS. One reasonable way to use logging is to sprinkle calls\nto one of these logging libraries in your service’s code. But if you have strict logging\nrequirements that can’t be enforced by the logging library, you may need to define\nyour own logging API that wraps a logging library.\n You also need to decide where to log. Traditionally, you would configure the log-\nging framework to write to a log file in a well-known location in the filesystem. But\nwith the more modern deployment technologies, such as containers and serverless,\nService A\ninstance 1\nLogging\nlibrary\nService B\ninstance 1\nLogging\nlibrary\nService A\ninstance 2\nLogging\nlibrary\nLog\nView\nNotify\nLog\nLog\nLog\npipeline\nLogging\nserver\nUser\nFigure 11.11\nThe log aggregation infrastructure ships the logs of each service instance to a \ncentralized logging server. Users can view and search the logs. They can also set up alerts, which are \ntriggered when log entries match search criteria.\n \n",
      "content_length": 2177,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 400,
      "content": "370\nCHAPTER 11\nDeveloping production-ready services\ndescribed in chapter 12, this is often not the best approach. In some environments,\nsuch as AWS Lambda, there isn’t even a “permanent” filesystem to write the logs to!\nInstead, your service should log to stdout. The deployment infrastructure will then\ndecide what to do with the output of your service. \nTHE LOG AGGREGATION INFRASTRUCTURE\nThe logging infrastructure is responsible for aggregating the logs, storing them, and\nenabling the user to search them. One popular logging infrastructure is the ELK\nstack. ELK consists of three open source products:\nElasticsearch—A text search-oriented NoSQL database that’s used as the logging\nserver\nLogstash—A log pipeline that aggregates the service logs and writes them to\nElasticsearch\nKibana—A visualization tool for Elasticsearch\nOther open source log pipelines include Fluentd and Apache Flume. Examples of log-\nging servers include cloud services, such as AWS CloudWatch Logs, as well as numerous\ncommercial offerings. Log aggregation is a useful debugging tool in a microservice\narchitecture.\n Let’s now look at distributed tracing, which is another way of understanding the\nbehavior of a microservices-based application. \n11.3.3 Using the Distributed tracing pattern\nImagine you’re a FTGO developer who is investigating why the getOrderDetails()\nquery has slowed down. You’ve ruled out the problem being an external networking\nissue. The increased latency must be caused by either the API gateway or one of the\nservices it has invoked. One option is to look at each service’s average response time.\nThe trouble with this option is that it’s an average across requests rather than the tim-\ning breakdown for an individual request. Plus more complex scenarios might involve\nmany nested service invocations. You may not even be familiar with all services. As a\nresult, it can be challenging to troubleshoot and diagnose these kinds of performance\nproblems in a microservice architecture.\nA good way to get insight into what your application is doing is to use distributed trac-\ning. Distributed tracing is analogous to a performance profiler in a monolithic applica-\ntion. It records information (for example, start time and end time) about the tree of\nservice calls that are made when handling a request. You can then see how the services\nPattern: Distributed tracing\nAssign each external request a unique ID and record how it flows through the system\nfrom one service to the next in a centralized server that provides visualization and\nanalysis. See http://microservices.io/patterns/observability/distributed-tracing.html.\n \n",
      "content_length": 2631,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 401,
      "content": "371\nDesigning observable services\ninteract during the handling of external requests, including a breakdown of where\nthe time is spent.\n Figure 11.12 shows an example of how a distributed tracing server displays what\nhappens when the API gateway handles a request. It shows the inbound request to the\nAPI gateway and the request that the gateway makes to Order Service. For each\nrequest, the distributed tracing server shows the operation that’s performed and the\ntiming of the request.\nFigure 11.12 shows what in distributed tracing terminology is called a trace. A trace\nrepresents an external request and consists of one or more spans. A span represents\nan operation, and its key attributes are an operation name, start timestamp, and end\ntime. A span can have one or more child spans, which represent nested operations.\nFor example, a top-level span might represent the invocation of the API gateway, as\nis the case in figure 11.12. Its child spans represent the invocations of services by the\nAPI gateway.\n A valuable side effect of distributed tracing is that it assigns a unique ID to each\nexternal request. A service can include the request ID in its log entries. When com-\nbined with log aggregation, the request ID enables you to easily find all log entries\nfor a particular external request. For example, here’s an example log entry from\nOrder Service:\n2018-03-04 17:38:12.032 DEBUG [ftgo-order-\nservice,8d8fdc37be104cc6,8d8fdc37be104cc6,false]\n7 --- [nio-8080-exec-6] org.hibernate.SQL\n:\nselect order0_.id as id1_3_0_, order0_.consumer_id as consumer2_3_0_, order\n0_.city as city3_3_0_,\norder0_.delivery_state as delivery4_3_0_, order0_.street1 as street5_3_0_,\norder0_.street2 as street6_3_0_, order0_.zip as zip7_3_0_,\norder0_.delivery_time as delivery8_3_0_, order0_.a\nParent span\nChild span\nTrace\nFigure 11.12\nThe Zipkin server shows how the FTGO application handles a request that’s routed \nby the API gateway to Order Service. Each request is represented by a trace. A trace is a set of \nspans. Each span, which can contain child spans, is the invocation of a service. Depending on the \nlevel of detail collected, a span can also represent the invocation of an operation inside a service.\n \n",
      "content_length": 2208,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 402,
      "content": "372\nCHAPTER 11\nDeveloping production-ready services\nThe [ftgo-order-service,8d8fdc37be104cc6,8d8fdc37be104cc6,false] part of the\nlog entry (the SLF4J Mapped Diagnostic Context—see www.slf4j.org/manual.html)\ncontains information from the distributed tracing infrastructure. It consists of four\nvalues:\n\nftgo-order-service—The name of the application\n\n8d8fdc37be104cc6—The traceId\n\n8d8fdc37be104cc6—The spanId\n\nfalse—Indicates that this span wasn’t exported to the distributed tracing server\nIf you search the logs for 8d8fdc37be104cc6, you’ll find all log entries for that request.\n Figure 11.13 shows how distributed tracing works. There are two parts to distrib-\nuted tracing: an instrumentation library, which is used by each service, and a distributed\ntracing server. The instrumentation library manages the traces and spans. It also adds\nSpan ABC: API gateway\nTrace XYZ\nAPI\ngateway\nGET/orders/1 HTTP/1.1\n....\nGET/orders/1 HTTP/1.1\nX-B3-TraceId: XYZ\nX-B3-ParentSpanId: ABC\nService: API gateway\nTraceId: XYZ\nParentSpan: NONE\nSpan: ABC\nViews traces\nService: Order Service\nTraceId: XYZ\nParentSpan: ABC\nSpan: DEF\nSpan DEF: Order Service\nTransport\nDistributed tracing server\nOrder\nService\nInstrumentation\nlibrary\nInstrumentation\nlibrary\nUser\nTrace\ndatabase\nFigure 11.13\nEach service (including the API gateway) uses an instrumentation library. The \ninstrumentation library assigns an ID to each external request, propagates tracing state between \nservices, and reports spans to the distributed tracing server.\n \n",
      "content_length": 1514,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 403,
      "content": "373\nDesigning observable services\ntracing information, such as the current trace ID and the parent span ID, to outbound\nrequests. For example, one common standard for propagating trace information is\nthe B3 standard (https://github.com/openzipkin/b3-propagation), which uses head-\ners such as X-B3-TraceId and X-B3-ParentSpanId. The instrumentation library also\nreports traces to the distributed tracing server. The distributed tracing server stores\nthe traces and provides a UI for visualizing them.\n Let’s take a look at the instrumentation library and the distribution tracing server,\nbeginning with the library.\nUSING AN INSTRUMENTATION LIBRARY\nThe instrumentation library builds the tree of spans and sends them to the distributed\ntracing server. The service code could call the instrumentation library directly, but that\nwould intertwine the instrumentation logic with business and other logic. A cleaner\napproach is to use interceptors or aspect-oriented programming (AOP).\n A great example of an AOP-based framework is Spring Cloud Sleuth. It uses the\nSpring Framework’s AOP mechanism to automagically integrate distributed tracing\ninto the service. As a result, you have to add Spring Cloud Sleuth as a project depen-\ndency. Your service doesn’t need to call a distributed tracing API except in those cases\nthat aren’t handled by Spring Cloud Sleuth. \nABOUT THE DISTRIBUTED TRACING SERVER\nThe instrumentation library sends the spans to a distributed tracing server. The dis-\ntributed tracing server stitches the spans together to form complete traces and stores\nthem in a database. One popular distributed tracing server is Open Zipkin. Zipkin was\noriginally developed by Twitter. Services can deliver spans to Zipkin using either\nHTTP or a message broker. Zipkin stores the traces in a storage backend, which is\neither a SQL or NoSQL database. It has a UI that displays traces, as shown earlier in\nfigure 11.12. AWS X-ray is another example of a distributed tracing server. \n11.3.4 Applying the Application metrics pattern\nA key part of the production environment is monitoring and alerting. As figure 11.14\nshows, the monitoring system gathers metrics, which provide critical information\nabout the health of an application, from every part of the technology stack. Metrics\nrange from infrastructure-level metrics, such as CPU, memory, and disk utilization, to\napplication-level metrics, such as service request latency and number of requests exe-\ncuted. Order Service, for example, gathers metrics about the number of placed,\napproved, and rejected orders. The metrics are collected by a metrics service, which\nprovides visualization and alerting.\nPattern: Application metrics\nServices report metrics to a central server that provides aggregation, visualization,\nand alerting.\n \n",
      "content_length": 2790,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 404,
      "content": "374\nCHAPTER 11\nDeveloping production-ready services\nMetrics are sampled periodically. A metric sample has the following three properties:\nName—The name of the metric, such as jvm_memory_max_bytes or placed_orders\nValue—A numeric value\nTimestamp—The time of the sample\nIn addition, some monitoring systems support the concept of dimensions, which are\narbitrary name-value pairs. For example, jvm_memory_max_bytes is reported with dimen-\nsions such as area=\"heap\",id=\"PS Eden Space\" and area=\"heap\",id=\"PS Old Gen\".\nDimensions are often used to provide additional information, such as the machine\nname or service name, or a service instance identifier. A monitoring system typically\naggregates (sums or averages) metric samples along one or more dimensions.\n Many aspects of monitoring are the responsibility of operations. But a service\ndeveloper is responsible for two aspects of metrics. First, they must instrument their\nservice so that it collects metrics about its behavior. Second, they must expose those\nservice metrics, along with metrics from the JVM and the application framework, to\nthe metrics server.\n Let’s first look at how a service collects metrics.\nCOLLECTING SERVICE-LEVEL METRICS\nHow much work you need to do to collect metrics depends on the frameworks that\nyour application uses and the metrics you want to collect. A Spring Boot-based service\ncan, for example, gather (and expose) basic metrics, such as JVM metrics, by including\nView\nNotify\nMetrics\nService\nUser\nService instance\nDeployment infrastructure\nMetrics sample:\nname=cpu_percent\nvalue=68\ntimestamp=34938934893\ndimensions:\nmachine=node1\n...\nApplication framework\nLanguage runtime\nApplication code\nMetrics library\nVisualization\nMetrics\ningestion\nAlerts\nMetrics\ndatabase\nFigure 11.14\nMetrics at every level of the stack are collected and stored in a metrics service, which \nprovides visualization and alerting.\n \n",
      "content_length": 1895,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 405,
      "content": "375\nDesigning observable services\nthe Micrometer Metrics library as a dependency and using a few lines of configura-\ntion. Spring Boot’s autoconfiguration takes care of configuring the metrics library and\nexposing the metrics. A service only needs to use the Micrometer Metrics API directly\nif it gathers application-specific metrics.\n The following listing shows how OrderService can collect metrics about the number\nof orders placed, approved, and rejected. It uses MeterRegistry, which is the interface-\nprovided Micrometer Metrics, to gather custom metrics. Each method increments an\nappropriately named counter.\npublic class OrderService {\n@Autowired\nprivate MeterRegistry meterRegistry;\n    \npublic Order createOrder(...) {\n...\nmeterRegistry.counter(\"placed_orders\").increment();    \nreturn order;\n}\npublic void approveOrder(long orderId) {\n...\nmeterRegistry.counter(\"approved_orders\").increment();\n}\npublic void rejectOrder(long orderId) {\n...\nmeterRegistry.counter(\"rejected_orders\").increment();      \n}\nDELIVERING METRICS TO THE METRICS SERVICE\nA service delivers metrics to the Metrics Service in one of two ways: push or pull. With\nthe push model, a service instance sends the metrics to the Metrics Service by invoking\nan API. AWS Cloudwatch metrics, for example, implements the push model.\n With the pull model, the Metrics Service (or its agent running locally) invokes a\nservice API to retrieve the metrics from the service instance. Prometheus, a popular\nopen source monitoring and alerting system, uses the pull model.\n The FTGO application’s Order Service uses the micrometer-registry-prometheus\nlibrary to integrate with Prometheus. Because this library is on the classpath, Spring\nBoot exposes a GET /actuator/prometheus endpoint, which returns metrics in the\nformat that Prometheus expects. The custom metrics from OrderService are reported\nas follows:\n$ curl -v http://localhost:8080/actuator/prometheus | grep _orders\n# HELP placed_orders_total\n# TYPE placed_orders_total counter\nListing 11.1\nOrderService tracks the number of orders placed, approved, and \nrejected.\nThe Micrometer Metrics \nlibrary API for managing \napplication-specific meters\nIncrements the \nplacedOrders counter \nwhen an order has \nsuccessfully been \nplaced\nIncrements the \napprovedOrders \ncounter when an \norder has been \napproved\nIncrements the \nrejectedOrders \ncounter when an \norder has been \nrejected\n \n",
      "content_length": 2402,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 406,
      "content": "376\nCHAPTER 11\nDeveloping production-ready services\nplaced_orders_total{service=\"ftgo-order-service\",} 1.0\n# HELP approved_orders_total\n# TYPE approved_orders_total counter\napproved_orders_total{service=\"ftgo-order-service\",} 1.0\nThe placed_orders counter is, for example, reported as a metric of type counter.\n The Prometheus server periodically polls this endpoint to retrieve metrics. Once\nthe metrics are in Prometheus, you can view them using Grafana, a data visualization\ntool (https://grafana.com). You can also set up alerts for these metrics, such as when\nthe rate of change for placed_orders_total falls below some threshold.\n Application metrics provide valuable insights into your application’s behavior.\nAlerts triggered by metrics enable you to quickly respond to a production issue, per-\nhaps before it impacts users. Let’s now look at how to observe and respond to another\nsource of alerts: exceptions. \n11.3.5 Using the Exception tracking pattern\nA service should rarely log an exception, and when it does, it’s important that you\nidentify the root cause. The exception might be a symptom of a failure or a program-\nming bug. The traditional way to view exceptions is to look in the logs. You might even\nconfigure the logging server to alert you if an exception appears in the log file. There\nare, however, several problems with this approach:\nLog files are oriented around single-line log entries, whereas exceptions consist\nof multiple lines.\nThere’s no mechanism to track the resolution of exceptions that occur in log\nfiles. You would have to manually copy/paste the exception into an issue tracker.\nThere are likely to be duplicate exceptions, but there’s no automatic mecha-\nnism to treat them as one.\nA better approach is to use an exception tracking service. As figure 11.15 shows, you\nconfigure your service to report exceptions to an exception tracking service via, for\nexample, a REST API. The exception tracking service de-duplicates exceptions, gener-\nates alerts, and manages the resolution of exceptions.\n There are a couple of ways to integrate the exception tracking service into your\napplication. Your service could invoke the exception tracking service’s API directly. A\nbetter approach is to use a client library provided by the exception tracking service.\nFor example, HoneyBadger’s client library provides several easy-to-use integration\nmechanisms, including a Servlet Filter that catches and reports exceptions.\nPattern: Exception tracking\nServices report exceptions to a central service that de-duplicates exceptions, gener-\nates alerts, and manages the resolution of exceptions. See http://microservices.io/\npatterns/observability/audit-logging.html.\n \n",
      "content_length": 2699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 407,
      "content": "377\nDesigning observable services\nThe Exception tracking pattern is a useful way to quickly identify and respond to pro-\nduction issues.\n It’s also important to track user behavior. Let’s look at how to do that. \n11.3.6 Applying the Audit logging pattern\nThe purpose of audit logging is to record each user’s actions. An audit log is typically\nused to help customer support, ensure compliance, and detect suspicious behavior.\nEach audit log entry records the identity of the user, the action they performed, and\nthe business object(s). An application usually stores the audit log in a database table.\nException tracking services\nThere are several exception tracking services. Some, such as Honeybadger (www\n.honeybadger.io), are purely cloud-based. Others, such as Sentry.io (https://sentry.io/\nwelcome/), also have an open source version that you can deploy on your own infra-\nstructure. These services receive exceptions from your application and generate alerts.\nThey provide a console for viewing exceptions and managing their resolution. An excep-\ntion tracking service typically provides client libraries in a variety of languages.\nPattern: Audit logging\nRecord user actions in a database in order to help customer support, ensure com-\npliance, and detect suspicious behavior. See http://microservices.io/patterns/\nobservability/audit-logging.html.\nView & manage\nNotify\nUser\nPOST/exceptions\njava.lang.NullPointerException\nat net.chrisrichardson.ftgo...\nat net.chrisrichardson.ftgo...\nat net.chrisrichardson.ftgo...\nOrder Service\nException tracking\nclient library\nException database\nException tracking service\nReport exception\nFigure 11.15\nA service reports exceptions to an exception tracking service, which de-duplicates \nexceptions and alerts developers. It has a UI for viewing and managing exceptions.\n \n",
      "content_length": 1814,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 408,
      "content": "378\nCHAPTER 11\nDeveloping production-ready services\nThere are a few different ways to implement audit logging:\nAdd audit logging code to the business logic.\nUse aspect-oriented programming (AOP).\nUse event sourcing.\nLet’s look at each option.\nADD AUDIT LOGGING CODE TO THE BUSINESS LOGIC\nThe first and most straightforward option is to sprinkle audit logging code through-\nout your service’s business logic. Each service method, for example, can create an\naudit log entry and save it in the database. The drawback with this approach is that it\nintertwines auditing logging code and business logic, which reduces maintainability.\nThe other drawback is that it’s potentially error prone, because it relies on the devel-\noper writing audit logging code. \nUSE ASPECT-ORIENTED PROGRAMMING\nThe second option is to use AOP. You can use an AOP framework, such as Spring\nAOP, to define advice that automatically intercepts each service method call and per-\nsists an audit log entry. This is a much more reliable approach, because it automati-\ncally records every service method invocation. The main drawback of using AOP is\nthat the advice only has access to the method name and its arguments, so it might be\nchallenging to determine the business object being acted upon and generate a business-\noriented audit log entry. \nUSE EVENT SOURCING\nThe third and final option is to implement your business logic using event sourcing.\nAs mentioned in chapter 6, event sourcing automatically provides an audit log for cre-\nate and update operations. You need to record the identity of the user in each event.\nOne limitation with using event sourcing, though, is that it doesn’t record queries. If\nyour service must create log entries for queries, then you’ll have to use one of the\nother options as well. \n11.4\nDeveloping services using the Microservice chassis \npattern\nThis chapter has described numerous concerns that a service must implement, includ-\ning metrics, reporting exceptions to an exception tracker, logging and health checks,\nexternalized configuration, and security. Moreover, as described in chapter 3, a ser-\nvice may also need to handle service discovery and implement circuit breakers. That’s\nnot something you’d want to set up from scratch each time you implement a new ser-\nvice. If you did, it would potentially be days, if not weeks, before you wrote your first\nline of business logic.\n \n \n \n",
      "content_length": 2401,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 409,
      "content": "379\nDeveloping services using the Microservice chassis pattern\nA much faster way to develop services is to build your services upon a microservices\nchassis. As figure 11.16 shows, a microservice chassis is a framework or set of frameworks\nthat handle these concerns. When using a microservice chassis, you write little, if any,\ncode to handle these concerns.\nIn this section, I first describe the concept of a microservice chassis and suggest some\nexcellent microservice chassis frameworks. After that I introduce the concept of a ser-\nvice mesh, which at the time of writing is emerging as an intriguing alternative to\nusing frameworks and libraries.\n Let’s first look at the idea of a microservice chassis.\n11.4.1 Using a microservice chassis\nA microservices chassis is a framework or set of frameworks that handle numerous\nconcerns including the following:\nExternalized configuration\nHealth checks\nApplication metrics\nService discovery\nPattern: Microservice chassis\nBuild services on a framework or collection of frameworks that handle cross-cutting\nconcerns, such as exception tracking, logging, health checks, externalized configu-\nration, and distributed tracing. See http://microservices.io/patterns/microservice-\nchassis.html.\nService\nService code\nCircuit breaker\nMicroservice chassis\nService discovery\nDistributed tracing\nApplication metrics\nLogging\nHealth check\nExternalized conﬁg.\n...\nFigure 11.16\nA microservice chassis \nis a framework that handles numerous \nconcerns, such as exception tracking, \nlogging, health checks, externalized \nconfiguration, and distributed tracing.\n \n",
      "content_length": 1594,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 410,
      "content": "380\nCHAPTER 11\nDeveloping production-ready services\nCircuit breakers\nDistributed tracing\nIt significantly reduces the amount of code you need to write. You may not even need\nto write any code. Instead, you configure the microservice chassis to fit your require-\nments. A microservice chassis enables you to focus on developing your service’s busi-\nness logic.\n The FTGO application uses Spring Boot and Spring Cloud as the microservice\nchassis. Spring Boot provides functions such as externalized configuration. Spring\nCloud provides functions such as circuit breakers. It also implements client-side ser-\nvice discovery, although the FTGO application relies on the infrastructure for service\ndiscovery. Spring Boot and Spring Cloud aren’t the only microservice chassis frame-\nworks. If, for example, you’re writing services in GoLang, you could use either Go Kit\n(https://github.com/go-kit/kit) or Micro (https://github.com/micro/micro).\n One drawback of using a microservice chassis is that you need one for every lan-\nguage/platform combination that you use to develop services. Fortunately, it’s likely\nthat many of the functions implemented by a microservice chassis will instead be\nimplemented by the infrastructure. For example, as described in chapter 3, many\ndeployment environments handle service discovery. What’s more, many of the network-\nrelated functions of a microservice chassis will be handled by what’s known as a service\nmesh, an infrastructure layer running outside of the services. \n11.4.2 From microservice chassis to service mesh\nA microservice chassis is a good way to implement various cross-cutting concerns, such\nas circuit breakers. But one obstacle to using a microservice chassis is that you need\none for each programming language you use. For example, Spring Boot and Spring\nCloud are useful if you’re a Java/Spring developer, but they aren’t any help if you\nwant to write a NodeJS-based service.\nAn emerging alternative that avoids this problem is to implement some of this func-\ntionality outside of the service in what’s known as a service mesh. A service mesh is net-\nworking infrastructure that mediates the communication between a service and other\nservices and external applications. As figure 11.17 shows, all network traffic in and out\nof a service goes through the service mesh. It implements various concerns including\ncircuit breakers, distributed tracing, service discovery, load balancing, and rule-based\ntraffic routing. A service mesh can also secure interprocess communication by using\nPattern: Service mesh\nRoute all network traffic in and out of services through a networking layer that imple-\nments various concerns, including circuit breakers, distributed tracing, service dis-\ncovery, load balancing, and rule-based traffic routing. See http://microservices.io/\npatterns/deployment/service-mesh.html.\n \n",
      "content_length": 2859,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 411,
      "content": "381\nDeveloping services using the Microservice chassis pattern\nTLS-based IPC between services. As a result, you no longer need to implement these\nparticular concerns in the services.\n When using a service mesh, the microservice chassis is much simpler. It only needs\nto implement concerns that are tightly integrated with the application code, such as\nexternalized configuration and health checks. The microservice chassis must support\ndistributed tracing by propagating distributed tracing information, such as the B3\nstandard headers I discussed earlier in section 11.3.3.\nThe service mesh concept is an extremely promising idea. It frees the developer from\nhaving to deal with various cross-cutting concerns. Also, the ability of a service mesh to\nThe current state of service mesh implementations\nThere are various service mesh implementations, including the following:\nIstio (https://istio.io)\nLinkerd (https://linkerd.io)\nConduit (https://conduit.io)\nAs of the time of writing, Linkerd is the most mature, with Istio and Conduit still under\nactive development. For more information about this exciting new technology, take a\nlook at each product’s documentation.\nAPI\ngateway\nMicroservice\nchassis\nOrder\nService\nService\nmesh\nMicroservice\nchassis\nRestaurant\nService\nMicroservice\nchassis\nDeployment infrastructure\nCircuit breaker\nService discovery\nDistributed tracing\nSmart trafﬁc routing\nLoad balancing\nLogging\nMicroservice chassis\nFunctionality moved from\nmicroservice chassis to\nservice mesh\nFewer functions\nExternalized conﬁg.\nDistributed tracing\nApplication metrics\nHealth check\n...\nSecure communications\nFigure 11.17\nAll network traffic in and out of a service flows through the service mesh. The service \nmesh implements various functions including circuit breakers, distributed tracing, service discovery, \nand load balancing. Fewer functions are implemented by the microservice chassis. It also secures \ninterprocess communication by using TLS-based IPC between services.\n \n",
      "content_length": 1988,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 412,
      "content": "382\nCHAPTER 11\nDeveloping production-ready services\nroute traffic enables you to separate deployment from release. It gives you the ability\nto deploy a new version of a service into production but only release it to certain users,\nsuch as internal test users. Chapter 12 discusses this concept further when describing\nhow to deploy services using Kubernetes. \nSummary\nIt’s essential that a service implements its functional requirements, but it must\nalso be secure, configurable, and observable.\nMany aspects of security in a microservice architecture are no different than in\na monolithic architecture. But there are some aspects of application security\nthat are necessarily different, including how user identity is passed between the\nAPI gateway and the services and who is responsible for authentication and autho-\nrization. A commonly used approach is for the API gateway to authenticate clients.\nThe API gateway includes a transparent token, such as a JWT, in each request to a\nservice. The token contains the identity of the principal and their roles. The ser-\nvices use the information in the token to authorize access to resources. OAuth 2.0\nis a good foundation for security in a microservice architecture.\nA service typically uses one or more external services, such as message brokers\nand databases. The network location and credentials of each external service\noften depend on the environment that the service is running in. You must apply\nthe Externalized configuration pattern and implement a mechanism that pro-\nvides a service with configuration properties at runtime. One commonly used\napproach is for the deployment infrastructure to supply those properties via\noperating system environment variables or a properties file when it creates a\nservice instance. Another option is for a service instance to retrieve its configu-\nration from a configuration properties server.\nOperations and developers share responsibility for implementing the observ-\nability patterns. Operations is responsible for the observability infrastructure,\nsuch as servers that handle log aggregation, metrics, exception tracking, and\ndistributed tracing. Developers are responsible for ensuring that their services\nare observable. Services must have health check API endpoints, generate log\nentries, collect and expose metrics, report exceptions to an exception tracking\nservice, and implement distributed tracing.\nIn order to simplify and accelerate development, you should develop services\non top of a microservices chassis. A microservices chassis is framework or set of\nframeworks that handle various cross-cutting concerns, including those described\nin this chapter. Over time, though, it’s likely that many of the networking-\nrelated functions of a microservice chassis will migrate into a service mesh, a\nlayer of infrastructure software through which all of a service’s network traffic\nflows. \n \n",
      "content_length": 2901,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 413,
      "content": "383\nDeploying microservices\nMary and her team at FTGO are almost finished writing their first service. Although\nit’s not yet feature complete, it’s running on developer laptops and the Jenkins CI\nserver. But that’s not good enough. Software has no value to FTGO until it’s run-\nning in production and available to users. FTGO needs to deploy their service into\nproduction.\nThis chapter covers\nThe four key deployment patterns, how they work, \nand their benefits and drawbacks:\n– Language-specific packaging format\n– Deploying a service as a VM\n– Deploying a service as a container\n– Serverless deployment\nDeploying services with Kubernetes\nUsing a service mesh to separate deployment \nfrom release\nDeploying services with AWS Lambda\nPicking a deployment pattern\n \n",
      "content_length": 769,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 414,
      "content": "384\nCHAPTER 12\nDeploying microservices\n Deployment is a combination of two interrelated concepts: process and architecture.\nThe deployment process consists of the steps that must be performed by people—\ndevelopers and operations—in order to get software into production. The deploy-\nment architecture defines the structure of the environment in which that software\nruns. Both aspects of deployment have changed radically since I first started develop-\ning Enterprise Java applications in the late 1990s. The manual process of developers\nthrowing code over the wall to production has become highly automated. As figure 12.1\nshows, physical production environments have been replaced by increasingly light-\nweight and ephemeral computing infrastructure.\nBack in the 1990s, if you wanted to deploy an application into production, the first\nstep was to throw your application along with a set of operating instructions over the\nwall to operations. You might, for example, file a trouble ticket asking operations to\ndeploy the application. Whatever happened next was entirely the responsibility of\noperations, unless they encountered a problem they needed your help to fix. Typi-\ncally, operations bought and installed expensive and heavyweight application servers\nsuch as WebLogic or WebSphere. Then they would log in to the application server\nconsole and deploy your applications. They would lovingly care for those machines, as\nif they were pets, installing patches and updating the software.\n In the mid 2000s, the expensive application servers were replaced with open\nsource, lightweight web containers such as Apache Tomcat and Jetty. You could still\nrun multiple applications on each web container, but having one application per web\ncontainer became feasible. Also, virtual machines started to replace physical machines.\nPhysical\nmachine\nApplication\nPhysical\nmachine\nVirtual\nmachine\nApplication\nPhysical\nmachine\nVirtual\nmachine\nContainer\nruntime\nApplication\nPhysical\nmachine\n1990s\n2006\n2013\n2014\nAWS EC2\nreleased\nInitial Docker\nrelease\nAWS Lambda\nintroduced\nHidden\ninfrastructure\nServerless\nruntime\nApplication\nLightweight,\nephemeral,\nautomated\nHeavyweight,\npermanent,\nmanual\nTime\nFigure 12.1\nHeavyweight and long-lived physical machines have been abstracted away \nby increasingly lightweight and ephemeral technologies.\n \n",
      "content_length": 2326,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 415,
      "content": "385\nBut machines were still treated as beloved pets, and deployment was still a fundamen-\ntally manual process.\n Today, the deployment process is radically different. Instead of handing off code to\na separate production team, the adoption of DevOps means that the development\nteam is also responsible for deploying their application or services. In some organiza-\ntions, operations provides developers with a console for deploying their code. Or, bet-\nter yet, once the tests pass, the deployment pipeline automatically deploys the code\ninto production.\n The computing resources used in a production environment have also changed rad-\nically with physical machines being abstracted away. Virtual machines running on a\nhighly automated cloud, such as AWS, have replaced the long-lived, pet-like physical and\nvirtual machines. Today’s virtual machines are immutable. They’re treated as disposable\ncattle instead of pets and are discarded and recreated rather than being reconfigured.\nContainers, an even more lightweight abstraction layer of top of virtual machines, are an\nincreasingly popular way of deploying applications. You can also use an even more light-\nweight serverless deployment platform, such as AWS Lambda, for many use cases.\n It’s no coincidence that the evolution of deployment processes and architectures has\ncoincided with the growing adoption of the microservice architecture. An application\nmight have tens or hundreds of services written in a variety of languages and frame-\nworks. Because each service is a small application, that means you have tens or hundreds\nof applications in production. It’s no longer practical, for example, for system adminis-\ntrators to hand configure servers and services. If you want to deploy microservices at\nscale, you need a highly automated deployment process and infrastructure.\n Figure 12.2 shows a high-level view of a production environment. The production\nenvironment enables developers to configure and manage their services, the deployment\nService\nA\nConsumes\nservices\nService\nC\nService\nB\nService\nD\nUser\nObserve and\ntroubleshoot\nservices\nUpdate\nservices\nConﬁgure\nand manage\nservices\nDeveloper\nRouting\nDash-\nboards\nMonitoring\nService\nmanagement\ninterface\nRuntime\nService\nmanagement\nAlerting\nDeployment\npipeline\nFigure 12.2\nA simplified view of the production environment. It provides four main capabilities: \nservice management enables developers to deploy and manage their services, runtime management \nensures that the services are running, monitoring visualizes service behavior and generates alerts, \nand request routing routes requests from users to the services.\n \n",
      "content_length": 2632,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 416,
      "content": "386\nCHAPTER 12\nDeploying microservices\npipeline to deploy new versions of services, and users to access functionality imple-\nmented by those services.\n A production environment must implement four key capabilities:\nService management interface—Enables developers to create, update, and config-\nure services. Ideally, this interface is a REST API invoked by command-line and\nGUI deployment tools.\nRuntime service management—Attempts to ensure that the desired number of ser-\nvice instances is running at all times. If a service instance crashes or is somehow\nunable to handle requests, the production environment must restart it. If a\nmachine crashes, the production environment must restart those service instances\non a different machine.\nMonitoring—Provides developers with insight into what their services are doing,\nincluding log files and metrics. If there are problems, the production environ-\nment must alert the developers. Chapter 11 describes monitoring, also called\nobservability.\nRequest routing—Routes requests from users to the services.\nIn this chapter I discuss the four main deployment options:\nDeploying services as language-specific packages, such as Java JAR or WAR files.\nIt’s worthwhile exploring this option, because even though I recommend using\none of the other options, its drawbacks motivate the other options.\nDeploying services as virtual machines, which simplifies deployment by packag-\ning a service as a virtual machine image that encapsulate the service’s technol-\nogy stack.\nDeploying services as containers, which are more lightweight than virtual\nmachines. I show how to deploy the FTGO application’s Restaurant Service\nusing Kubernetes, a popular Docker orchestration framework.\nDeploying services using serverless deployment, which is even more modern than\ncontainers. We’ll look at how to deploy Restaurant Service using AWS Lambda,\na popular serverless platform.\nLet’s first look at how to deploy services as language-specific packages.\n12.1\nDeploying services using the Language-specific \npackaging format pattern\nLet’s imagine that you want to deploy the FTGO application’s Restaurant Service,\nwhich is a Spring Boot-based Java application. One way to deploy this service is by\nusing the Service as a language-specific package pattern. When using this pattern,\nwhat’s deployed in production and what’s managed by the service runtime is a service\nin its language-specific package. In the case of Restaurant Service, that’s either the\nexecutable JAR file or a WAR file. For other languages, such as NodeJS, a service is a\ndirectory of source code and modules. For some languages, such as GoLang, a service\nis an operating system-specific executable.\n \n",
      "content_length": 2699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 417,
      "content": "387\nDeploying services using the Language-specific packaging format pattern\nTo deploy Restaurant Service on a machine, you would first install the necessary\nruntime, which in this case is the JDK. If it’s a WAR file, you also need to install a\nweb container such as Apache Tomcat. Once you’ve configured the machine, you\ncopy the package to the machine and start the service. Each service instance runs as\na JVM process.\n Ideally, you’ve set up your deployment pipeline to automatically deploy the service\nto production, as shown in figure 12.3. The deployment pipeline builds an executable\nJAR file or WAR file. It then invokes the production environment’s service manage-\nment interface to deploy the new version.\nA service instance is typically a single process but sometimes may be a group of pro-\ncesses. A Java service instance, for example, is a process running the JVM. A NodeJS\nservice might spawn multiple worker processes in order to process requests concur-\nrently. Some languages support deploying multiple service instances within the same\nprocess.\n Sometimes you might deploy a single service instance on a machine, while retain-\ning the option to deploy multiple service instances on the same machine. For exam-\nple, as figure 12.4 shows, you could run multiple JVMs on a single machine. Each JVM\nruns a single service instance.\nPattern: Language-specific packaging format\nDeploy a language-specific package into production. See http://microservices.io/\npatterns/deployment/language-specific-packaging.html.\nJVM\nprocess\nJVM\nprocess\nJVM\nprocess\nService instance\nBuild time\nRuntime\nService runtime management\nMachine\nProduction\nJDK/JRE\nMachine\nJDK/JRE\nService\ncode\nExecutable\nJAR/WAR ﬁle\nDeployment\npipeline\nFigure 12.3\nThe deployment pipeline builds an executable JAR file and deploys it into production. \nIn production, each service instance is a JVM running on a machine that has the JDK or JRE installed.\n \n",
      "content_length": 1925,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 418,
      "content": "388\nCHAPTER 12\nDeploying microservices\nSome languages also let you run multiple services instances in a single process. For\nexample, as figure 12.5 shows, you can run multiple Java services on a single Apache\nTomcat.\nThis approach is commonly used when deploying applications on traditional expen-\nsive and heavyweight application servers, such as WebLogic and WebSphere. You can\nalso package services as OSGI bundles and run multiple service instances in each\nOSGI container.\n The Service as a language-specific package pattern has both benefits and draw-\nbacks. Let’s first look at the benefits.\n12.1.1 Benefits of the Service as a language-specific package pattern\nThe Service as a language-specific package pattern has a few benefits:\nFast deployment\nEfficient resource utilization, especially when running multiple instances on\nthe same machine or within the same process\nLet’s look at each one.\nJVM\nProcess\nPhysical or virtual machine\nTomcat\nService\ninstance A\nJVM\nProcess\nTomcat\nService\ninstance B\nJVM\nProcess\nTomcat\nService\ninstance ...\nFigure 12.4\nDeploying multiple service \ninstances on the same machine. They \nmight be instances of the same service \nor instances of different services. The \noverhead of the OS is shared among the \nservice instances. Each service instance \nis a separate process, so there’s some \nisolation between them.\nProcess\nPhysical or virtual machine\nService\ninstance A\nJVM\nTomcat\nService\ninstance B\nService\ninstance ...\nFigure 12.5\nDeploying multiple \nservices instances on the same web \ncontainer or application server. They \nmight be instances of the same service \nor instances of different services. The \noverhead of the OS and runtime is shared \namong all the service instances. But \nbecause the service instances are in the \nsame process, there’s no isolation \nbetween them.\n \n",
      "content_length": 1819,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 419,
      "content": "389\nDeploying services using the Language-specific packaging format pattern\nFAST DEPLOYMENT\nOne major benefit of this pattern is that deploying a service instance is relatively\nfast: you copy the service to a host and start it. If the service is written in Java, you\ncopy a JAR or WAR file. For other languages, such as NodeJS or Ruby, you copy the\nsource code. In either case, the number of bytes copied over the network is rela-\ntively small.\n Also, starting a service is rarely time consuming. If the service is its own process,\nyou start it. Otherwise, if the service is one of several instances running in the same\ncontainer process, you either dynamically deploy it into the container or restart the\ncontainer. Because of the lack of overhead, starting a service is usually fast. \nEFFICIENT RESOURCE UTILIZATION\nAnother major benefit of this pattern is that it uses resources relatively efficiently. Mul-\ntiple service instances share the machine and its operating system. It’s even more effi-\ncient if multiple service instances run within the same process. For example, multiple\nweb applications could share the same Apache Tomcat server and JVM. \n12.1.2 Drawbacks of the Service as a language-specific package pattern\nDespite its appeal, the Service as a language-specific package pattern has several signif-\nicant drawbacks:\nLack of encapsulation of the technology stack.\nNo ability to constrain the resources consumed by a service instance.\nLack of isolation when running multiple service instances on the same machine.\nAutomatically determining where to place service instances is challenging.\nLet’s look at each drawback.\nLACK OF ENCAPSULATION OF THE TECHNOLOGY STACK\nThe operation team must know the specific details of how to deploy each and every\nservice. Each service needs a particular version of the runtime. A Java web application,\nfor example, needs particular versions of Apache Tomcat and the JDK. Operations\nmust install the correct version of each required software package.\n To make matters worse, services can be written in a variety of languages and frame-\nworks. They might also be written in multiple versions of those languages and frame-\nworks. Consequently, the development team must share lots of details with operations.\nThis complexity increases the risk of errors during deployment. A machine might, for\nexample, have the wrong version of the language runtime. \nNO ABILITY TO CONSTRAIN THE RESOURCES CONSUMED BY A SERVICE INSTANCE\nAnother drawback is that you can’t constrain the resources consumed by a service\ninstance. A process can potentially consume all of a machine’s CPU or memory, starv-\ning other service instances and operating systems of resources. This might happen, for\nexample, because of a bug. \n \n",
      "content_length": 2755,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 420,
      "content": "390\nCHAPTER 12\nDeploying microservices\nLACK OF ISOLATION WHEN RUNNING MULTIPLE SERVICE INSTANCES ON THE SAME MACHINE\nThe problem is even worse when running multiple instances on the same machine.\nThe lack of isolation means that a misbehaving service instance can impact other ser-\nvice instances. As a result, the application risks being unreliable, especially when run-\nning multiple service instances on the same machine. \nAUTOMATICALLY DETERMINING WHERE TO PLACE SERVICE INSTANCES IS CHALLENGING\nAnother challenge with running multiple service instances on the same machine is\ndetermining the placement of service instances. Each machine has a fixed set of\nresources, CPU, memory, and so on, and each service instance needs some amount of\nresources. It’s important to assign service instances to machines in a way that uses the\nmachines efficiently without overloading them. As I explain shortly, VM-based clouds\nand container orchestration frameworks handle this automatically. When deploying\nservices natively, it’s likely that you’ll need to manually decide the placement.\n As you can see, despite its familiarity, the Service as a language-specific package\npattern has some significant drawbacks. You should rarely use this approach, except\nperhaps when efficiency outweighs all other concerns.\n Let’s now look at modern ways of deploying services that avoid these problems. \n12.2\nDeploying services using the Service as a virtual \nmachine pattern\nOnce again, imagine you want to deploy the FTGO Restaurant Service, except this\ntime it’s on AWS EC2. One option would be to create and configure an EC2 instance\nand copy onto it the executable or WAR file. Although you would get some benefit\nfrom using the cloud, this approach suffers from the drawbacks described in the pre-\nceding section. A better, more modern approach is to package the service as an Ama-\nzon Machine Image (AMI), as shown in figure 12.6. Each service instance is an EC2\ninstance created from that AMI. The EC2 instances would typically be managed by an\nAWS Auto Scaling group, which attempts to ensure that the desired number of\nhealthy instances is always running.\nThe virtual machine image is built by the service’s deployment pipeline. The deploy-\nment pipeline, as figure 12.6 shows, runs a VM image builder to create a VM image\nthat contains the service’s code and whatever software is required to run it. For\nexample, the VM builder for a FTGO service installs the JDK and the service’s exe-\ncutable JAR. The VM image builder configures the VM image machine to run the\napplication when the VM boots, using Linux’s init system, such as upstart.\nPattern: Deploy a service as a VM\nDeploy services packaged as VM images into production. Each service instance is a\nVM. See http://microservices.io/patterns/deployment/service-per-vm.html.\n \n",
      "content_length": 2821,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 421,
      "content": "391\nDeploying services using the Service as a virtual machine pattern\nThere are a variety of tools that your deployment pipeline can use to build VM\nimages. One early tool for creating EC2 AMIs is Aminator, created by Netflix, which\nused it to deploy its video-streaming service on AWS (https://github.com/Netflix/\naminator). A more modern VM image builder is Packer, which unlike Aminator sup-\nports a variety of virtualization technologies, including EC2, Digital Ocean, Virtual\nBox, and VMware (www.packer.io). To use Packer to create an AMI, you write a config-\nuration file that specifies the base image and a set of provisioners that install software\nand configure the AMI.\nAbout Elastic Beanstalk\nElastic Beanstalk, which is provided by AWS, is an easy way to deploy your services\nusing VMs. You upload your code, such as a WAR file, and Elastic Beanstalk deploys\nit as one or more load-balanced and managed EC2 instances. Elastic Beanstalk is\nperhaps not quite as fashionable as, say, Kubernetes, but it’s an easy way to deploy\na microservices-based application on EC2.\nInterestingly, Elastic Beanstalk combines elements of the three deployment patterns\ndescribed in this chapter. It supports several packaging formats for several lan-\nguages, including Java, Ruby, and .NET. It deploys the application as VMs, but rather\nthan building an AMI, it uses a base image that installs the application on startup.\nBuild time\nRuntime\nRequests\nDeployed as\nService\nEC2 instance\nAutoscaling group\nService\nEC2 instance\nService\nEC2 instance\nService\ncode\nDeployment pipeline\nCreates\nVM image\nbuilder\nElastic load\nbalancer\nAMI\n(VM\nimage)\nFigure 12.6\nThe deployment pipeline packages a service as a virtual machine image, such as an EC2 \nAMI, containing everything required to run the service, including the language runtime. At runtime, \neach service instance is a VM, such as an EC2 instance, instantiated from that image. An EC2 Elastic \nLoad Balancer routes requests to the instances.\n \n",
      "content_length": 1983,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 422,
      "content": "392\nCHAPTER 12\nDeploying microservices\nLet’s look at the benefits and drawbacks of using this approach.\n12.2.1 The benefits of deploying services as VMs\nThe Service as a virtual machine pattern has a number of benefits:\nThe VM image encapsulates the technology stack.\nIsolated service instances.\nUses mature cloud infrastructure.\nLet’s look at each one.\nTHE VM IMAGE ENCAPSULATES THE TECHNOLOGY STACK\nAn important benefit of this pattern is that the VM image contains the service and all\nof its dependencies. It eliminates the error-prone requirement to correctly install and\nset up the software that a service needs in order to run. Once a service has been pack-\naged as a virtual machine, it becomes a black box that encapsulates your service’s tech-\nnology stack. The VM image can be deployed anywhere without modification. The API\nfor deploying the service becomes the VM management API. Deployment becomes\nmuch simpler and more reliable. \nSERVICE INSTANCES ARE ISOLATED\nA major benefit of virtual machines is that each service instance runs in complete iso-\nlation. That, after all, is one of the main goals of virtual machine technology. Each vir-\ntual machine has a fixed amount of CPU and memory and can’t steal resources from\nother services. \nUSES MATURE CLOUD INFRASTRUCTURE\nAnother benefit of deploying your microservices as virtual machines is that you can\nleverage mature, highly automated cloud infrastructure. Public clouds such as AWS\nattempt to schedule VMs on physical machines in a way that avoids overloading the\nmachine. They also provide valuable features such as load balancing of traffic across\nVMs and autoscaling. \n12.2.2 The drawbacks of deploying services as VMs\nThe Service as a VM pattern also has some drawbacks:\nLess-efficient resource utilization\nRelatively slow deployments\nSystem administration overhead\nLet’s look at each drawback in turn.\n(continued)\nElastic Beanstalk can also deploy Docker containers. Each EC2 instance runs a col-\nlection of one or more containers. Unlike a Docker orchestration framework, covered\nlater in the chapter, the unit of scaling is the EC2 instance rather than a container.\n \n",
      "content_length": 2150,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 423,
      "content": "393\nDeploying services using the Service as a container pattern\nLESS-EFFICIENT RESOURCE UTILIZATION\nEach service instance has the overhead of an entire virtual machine, including its\noperating system. Moreover, a typical public IaaS virtual machine offers a limited set\nof VM sizes, so the VM will probably be underutilized. This is less likely to be a prob-\nlem for Java-based services because they’re relatively heavyweight. But this pattern\nmight be an inefficient way of deploying lightweight NodeJS and GoLang services. \nRELATIVELY SLOW DEPLOYMENTS\nBuilding a VM image typically takes some number of minutes because of the size of\nthe VM. There are lots of bits to be moved over the network. Also, instantiating a VM\nfrom a VM image is time consuming because of, once again, the amount of data that\nmust be moved over the network. The operating system running inside the VM also\ntakes some time to boot, though slow is a relative term. This process, which perhaps\ntakes minutes, is much faster than the traditional deployment process. But it’s much\nslower than the more lightweight deployment patterns you’ll read about soon. \nSYSTEM ADMINISTRATION OVERHEAD\nYou’re responsible for patching the operation system and runtime. System administra-\ntion may seem inevitable when deploying software, but later in section 12.5, I describe\nserverless deployment, which eliminates this kind of system administration.\n Let’s now look at an alternative way to deploy microservices that’s more light-\nweight, yet still has many of the benefits of virtual machines. \n12.3\nDeploying services using the Service as \na container pattern\nContainers are a more modern and lightweight deployment mechanism. They’re an\noperating-system-level virtualization mechanism. A container, as figure 12.7 shows,\nconsists of usually one but sometimes multiple processes running in a sandbox, which\nisolates it from other containers. A container running a Java service, for example,\nwould typically consist of the JVM process.\n From the perspective of a process running in a container, it’s as if it’s running on\nits own machine. It typically has its own IP address, which eliminates port conflicts. All\nJava processes can, for example, listen on port 8080. Each container also has its own\nroot filesystem. The container runtime uses operating system mechanisms to isolate\nthe containers from each other. The most popular example of a container runtime is\nDocker, although there are others, such as Solaris Zones.\nPattern: Deploy a service as a container\nDeploy services packaged as container images into production. Each service instance\nis a container. See http://microservices.io/patterns/deployment/service-per-container\n.html.\n \n",
      "content_length": 2706,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 424,
      "content": "394\nCHAPTER 12\nDeploying microservices\nWhen you create a container, you can specify its CPU, memory resources, and, depend-\ning on the container implementation, perhaps the I/O resources. The container run-\ntime enforces these limits and prevents a container from hogging the resources of its\nmachine. When using a Docker orchestration framework such as Kubernetes, it’s espe-\ncially important to specify a container’s resources. That’s because the orchestration\nframework uses a container’s requested resources to select the machine to run the\ncontainer and thereby ensure that machines aren’t overloaded.\n Figure 12.8 shows the process of deploying a service as a container. At build-time,\nthe deployment pipeline uses a container image-building tool, which reads the ser-\nvice’s code and a description of the image, to create the container image and stores it\nin a registry. At runtime, the container image is pulled from the registry and used to\ncreate containers.\n Let’s take a look at build-time and runtime steps in more detail.\n \n \nContainer\nMachine\nService\nprocess\nContainer\nContainer runtime, such as Docker\nService\nprocess\nContainer\nService\nprocess\nOperating System\nEach container is a sandbox\nthat isolates the processes.\nShared by all of the containers\nFigure 12.7\nA container consists of one or more processes \nrunning in an isolated sandbox. Multiple containers usually run \non a single machine. The containers share the operating system.\n \n",
      "content_length": 1456,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 425,
      "content": "395\nDeploying services using the Service as a container pattern\n12.3.1 Deploying services using Docker\nTo deploy a service as a container, you must package it as a container image. A container\nimage is a filesystem image consisting of the application and any software required to\nrun the service. It’s often a complete Linux root filesystem, although more lightweight\nimages are also used. For example, to deploy a Spring Boot-based service, you build a\ncontainer image containing the service’s executable JAR and the correct version of\nthe JDK. Similarly, to deploy a Java web application, you would build a container\nimage containing the WAR file, Apache Tomcat, and the JDK.\nBUILDING A DOCKER IMAGE\nThe first step in building an image is to create a Dockerfile. A Dockerfile describes how\nto build a Docker container image. It specifies the base container image, a series of\ninstructions for installing software and configuring the container, and the shell com-\nmand to run when the container is created. Listing 12.1 shows the Dockerfile used to\nbuild an image for Restaurant Service. It builds a container image containing the\nservice’s executable JAR file. It configures the container to run the java -jar com-\nmand on startup.\nBuild time\nRuntime\n$ docker build ...\nDeployed\nas\nDeployed\nas\nService\ninstance\nContainer\nVM\nVM\nContainer\nimage registry\nService\ninstance\nContainer\nService\ninstance\nContainer\nService\ncode\nContainer runtime\nContainer runtime\nDeployment pipeline\nCreates\nContainer\nbuilder tool\nDocker\nﬁle\nService\ncontainer\nimage\nFigure 12.8\nA service is packaged as a container image, which is stored in a registry. At runtime \nthe service consists of multiple containers instantiated from that image. Containers typically run on \nvirtual machines. A single VM will usually run multiple containers.\n \n",
      "content_length": 1815,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 426,
      "content": "396\nCHAPTER 12\nDeploying microservices\nFROM openjdk:8u171-jre-alpine\nRUN apk --no-cache add curl\n  \nCMD java ${JAVA_OPTS} -jar ftgo-restaurant-service.jar\n  \nHEALTHCHECK --start-period=30s --\ninterval=5s CMD curl http://localhost:8080/actuator/health || exit 1      \nCOPY build/libs/ftgo-restaurant-service.jar .\n  \nThe base image openjdk:8u171-jre-alpine is a minimal footprint Linux image con-\ntaining the JRE. The Dockerfile copies the service’s JAR into the image and config-\nures the image to execute the JAR on startup. It also configures Docker to periodically\ninvoke the health check endpoint, described in chapter 11. The HEALTHCHECK direc-\ntive says to invoke the health check endpoint API, described in chapter 11, every 5 sec-\nonds after an initial 30-second delay, which gives the service time to start.\n Once you’ve written the Dockerfile, you can then build the image. The following\nlisting shows the shell commands to build the image for Restaurant Service. The\nscript builds the service’s JAR file and executes the docker build command to create\nthe image.\ncd ftgo-restaurant-service\n../gradlew assemble\n  \ndocker build -t ftgo-restaurant-service .\n  \nThe docker build command has two arguments: the -t argument specifies the name\nof the image, and the . specifies what Docker calls the context. The context, which in\nthis example is the current directory, consists of Dockerfile and the files used to\nbuild the image. The docker build command uploads the context to the Docker dae-\nmon, which builds the image. \nPUSHING A DOCKER IMAGE TO A REGISTRY\nThe final step of the build process is to push the newly built Docker image to what is\nknown as a registry. A Docker registry is the equivalent of a Java Maven repository for\nJava libraries, or a NodeJS npm registry for NodeJS packages. Docker hub is an exam-\nple of a public Docker registry and is equivalent to Maven Central or NpmJS.org. But\nfor your applications you’ll probably want to use a private registry provided by ser-\nvices, such as Docker Cloud registry or AWS EC2 Container Registry.\n You must use two Docker commands to push an image to a registry. First, you use\nthe docker tag command to give the image a name that’s prefixed with the hostname\nListing 12.1\nThe Dockerfile used to build Restaurant Service\nListing 12.2\nThe shell commands used to build the container image for \nRestaurant Service\nThe base image\nInstall curl for \nuse by the \nhealth check.\nConfigure Docker \nto run java -jar .. \nwhen the container \nis started.\nConfigure Docker to\ninvoke the health\ncheck endpoint.\nCopies the JAR in Gradle’s build\ndirectory into the image\nChange to the \nservice’s directory.\nBuild the \nservice’s JAR.\nBuild the image.\n \n",
      "content_length": 2703,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 427,
      "content": "397\nDeploying services using the Service as a container pattern\nand optional port of the registry. The image name is also suffixed with the version,\nwhich will be important when you make a new release of the service. For example, if\nthe hostname of the registry is registry.acme.com, you would use this command to\ntag the image:\ndocker tag ftgo-restaurant-service registry.acme.com/ftgo-restaurant-\nservice:1.0.0.RELEASE\nNext you use the docker push command to upload that tagged image to the registry:\ndocker push registry.acme.com/ftgo-restaurant-service:1.0.0.RELEASE\nThis command often takes much less time than you might expect. That’s because a\nDocker image has what’s known as a layered file system, which enables Docker to only\ntransfer part of the image over the network. An image’s operating system, Java run-\ntime, and the application are in separate layers. Docker only needs to transfer those\nlayers that don’t exist in the destination. As a result, transferring an image over a net-\nwork is quite fast when Docker only has to move the application’s layers, which are a\nsmall fraction of the image.\n Now that we’ve pushed the image to a registry, let’s look at how to create a\ncontainer. \nRUNNING A DOCKER CONTAINER\nOnce you’ve packaged your service as a container image, you can then create one or\nmore containers. The container infrastructure will pull the image from the registry\nonto a production server. It will then create one or more containers from that image.\nEach container is an instance of your service.\n As you might expect, Docker provides a docker run command that creates and\nstarts a container. Listing 12.3 shows how to use this command to run Restaurant\nService. The docker run command has several arguments, including the container\nimage and a specification of environment variables to set in the runtime container.\nThese are used to pass an externalized configuration, such as the database’s network\nlocation and more.\ndocker run \\\n-d\n\\\n  \n--name ftgo-restaurant-service\n\\\n   \n-p 8082:8080\n\\\n   \n-e SPRING_DATASOURCE_URL=... -e SPRING_DATASOURCE_USERNAME=...\n\\  \n-e SPRING_DATASOURCE_PASSWORD=... \\\nregistry.acme.com/ftgo-restaurant-service:1.0.0.RELEASE\n  \nListing 12.3\nUsing docker run to run a containerized service\nRuns it as a \nbackground daemon\nThe name of \nthe container\nBinds port 8080 of the \ncontainer to port 8082 \nof the host machine\nEnvironment \nvariables\nImage to run\n \n",
      "content_length": 2418,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 428,
      "content": "398\nCHAPTER 12\nDeploying microservices\nThe docker run command pulls the image from the registry if necessary. It then cre-\nates and starts the container, which runs the java -jar command specified in the\nDockerfile.\n Using the docker run command may seem simple, but there are a couple of prob-\nlems. One is that docker run isn’t a reliable way to deploy a service, because it creates\na container running on a single machine. The Docker engine provides some basic\nmanagement features, such as automatically restarting containers if they crash or if\nthe machine is rebooted. But it doesn’t handle machine crashes.\n Another problem is that services typically don’t exist in isolation. They depend on\nother services, such as databases and message brokers. It would be nice to deploy or\nundeploy a service and its dependencies as a unit.\n A better approach that’s especially useful during development is to use Docker\nCompose. Docker Compose is a tool that lets you declaratively define a set of contain-\ners using a YAML file, and then start and stop those containers as a group. What’s\nmore, the YAML file is a convenient way to specify numerous externalized configura-\ntion properties. To learn more about Docker Compose, I recommend reading Docker\nin Action by Jeff Nickoloff (Manning, 2016) and looking at the docker-compose.yml\nfile in the example code.\n The problem with Docker Compose, though, is that it’s limited to a single machine.\nTo deploy services reliably, you must use a Docker orchestration framework, such as\nKubernetes, which turns a set of machines into a pool of resources. I describe how to\nuse Kubernetes later, in section 12.4. First, let’s review the benefits and drawbacks of\nusing containers. \n12.3.2 Benefits of deploying services as containers\nDeploying services as containers has several benefits. First, containers have many of\nthe benefits of virtual machines:\nEncapsulation of the technology stack in which the API for managing your ser-\nvices becomes the container API.\nService instances are isolated.\nService instances’s resources are constrained.\nBut unlike virtual machines, containers are a lightweight technology. Container\nimages are typically fast to build. For example, on my laptop it takes as little as five sec-\nonds to package a Spring Boot application as a container image. Moving a container\nimage over the network, such as to and from the container registry, is also relatively\nfast, primarily because only a subset of an image’s layers need to be transferred. Con-\ntainers also start very quickly, because there’s no lengthy OS boot process. When a\ncontainer starts, all that runs is the service. \n \n",
      "content_length": 2650,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 429,
      "content": "399\nDeploying the FTGO application with Kubernetes\n12.3.3 Drawbacks of deploying services as containers\nOne significant drawback of containers is that you’re responsible for the undifferenti-\nated heavy lifting of administering the container images. You must patch the operat-\ning system and runtime. Also, unless you’re using a hosted container solution such as\nGoogle Container Engine or AWS ECS, you must administer the container infrastruc-\nture and possibly the VM infrastructure it runs on. \n12.4\nDeploying the FTGO application with Kubernetes\nNow that we’ve looked at containers and their trade-offs, let’s look at how to deploy\nthe FTGO application’s Restaurant Service using Kubernetes. Docker Compose,\ndescribed in section 12.3.1, is great for development and testing. But to reliably run\ncontainerized services in production, you need to use a much more sophisticated con-\ntainer runtime, such as Kubernetes. Kubernetes is a Docker orchestration framework,\na layer of software on top of Docker that turns a set of machines into a single pool of\nresources for running services. It endeavors to keep the desired number of instances\nof each service running at all times, even when service instances or machines crash.\nThe agility of containers combined with the sophistication of Kubernetes is a compel-\nling way to deploy services.\n In this section, I first give an overview of Kubernetes, its functionality, and its archi-\ntecture. After that, I show how to deploy a service using Kubernetes. Kubernetes is a\ncomplex topic, and covering it exhaustively is beyond the scope of this book, so I only\nshow how to use Kubernetes from the perspective of a developer. For more informa-\ntion, I recommend Kubernetes in Action by Marko Luksa (Manning, 2018).\n12.4.1 Overview of Kubernetes\nKubernetes is a Docker orchestration framework. A Docker orchestration framework treats\na set of machines running Docker as a pool of resources. You tell the Docker orches-\ntration framework to run N instances of your service, and it handles the rest. Figure 12.9\nshows the architecture of a Docker orchestration framework.\n A Docker orchestration framework, such as Kubernetes , has three main functions:\nResource management—Treats a cluster of machines as a pool of CPU, memory,\nand storage volumes, turning the collection of machines into a single machine.\nScheduling—Selects the machine to run your container. By default, scheduling\nconsiders the resource requirements of the container and each node’s available\nresources. It might also implement affinity, which colocates containers on the\nsame node, and anti-affinity, which places containers on different nodes.\nService management—Implements the concept of named and versioned services\nthat map directly to services in the microservice architecture. The orchestration\nframework ensures that the desired number of healthy instances is running at\nall times. It load balances requests across them. The orchestration framework\nperforms rolling upgrades of services and lets you roll back to an old version.\n \n",
      "content_length": 3055,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 430,
      "content": "400\nCHAPTER 12\nDeploying microservices\nDocker orchestration frameworks are an increasingly popular way to deploy applica-\ntions. Docker Swarm is part of the Docker engine, so is easy to set up and use. Kuber-\nnetes is much more complex to set up and administer, but it’s much more sophisticated.\nAt the time of writing, Kubernetes has tremendous momentum, with a massive open\nsource community. Let’s take a closer look at how it works.\nKUBERNETES ARCHITECTURE\nKubernetes runs on a cluster of machines. Figure 12.10 shows the architecture of a\nKubernetes cluster. Each machine in a Kubernetes cluster is either a master or a node.\nA typical cluster has a small number of masters—perhaps just one—and many nodes.\nA master machine is responsible for managing the cluster. A node is a worker than runs\none or more pods. A pod is Kubernetes’s unit of deployment and consists of a set of\ncontainers.\n A master runs several components, including the following:\nAPI server—The REST API for deploying and managing services, used by the\nkubectl command-line interface, for example.\nEtcd—A key-value NoSQL database that stores the cluster data.\nSVC\nA\nSVC\nB\nSVC\nC\nContainer\nDocker orchestration framework\nContainer\nContainer\nDocker\nOperating\nsystem\nMachine\nDocker\nOperating\nsystem\nMachine\nDocker\nOperating\nsystem\nMachine\nService management\nScheduling\nResource management\nFigure 12.9\nA Docker orchestration \nframework turns a set of machines running \nDocker into a cluster of resources. It assigns \ncontainers to machines. The framework \nattempts to keep the desired number of \nhealthy containers running at all times.\n \n",
      "content_length": 1610,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 431,
      "content": "401\nDeploying the FTGO application with Kubernetes\nScheduler—Selects a node to run a pod.\nController manager—Runs the controllers, which ensure that the state of the clus-\nter matches the intended state. For example, one type of controller known as a\nreplication controller ensures that the desired number of instances of a service\nare running by starting and terminating instances.\nA node runs several components, including the following:\nKubelet—Creates and manages the pods running on the node\nKube-proxy—Manages networking, including load balancing across pods\nPods—The application services\nSVC\nPod\nKubernetes master\netcd\nKubelet\nKube-proxy\nKubernetes node\nSVC\nPod\nKubelet\nKube-proxy\nKubernetes node\nApplication\nrequests\nConﬁguration\ncommands\nDeveloper\nAplication\nuser\nDeployment\npipeline\nKubecti\nCLI\nAPI Server\nController\nmanagement\nScheduler\nFigure 12.10\nA Kubernetes cluster consists of a master, which manages the cluster, and nodes, \nwhich run the services. Developers and the deployment pipeline interact with Kubernetes through the \nAPI server, which along with other cluster-management software runs on the master. Application \ncontainers run on nodes. Each node runs a Kubelet, which manages the application container, and a \nkube-proxy, which routes application requests to the pods, either directly as a proxy or indirectly by \nconfiguring iptables routing rules built into the Linux kernel.\n \n",
      "content_length": 1414,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 432,
      "content": "402\nCHAPTER 12\nDeploying microservices\nLet’s now look at key Kubernetes concepts you’ll need to master to deploy services on\nKubernetes. \nKEY KUBERNETES CONCEPTS\nAs mentioned in the introduction to this section, Kubernetes is quite complex. But it’s\npossible to use Kubernetes productively once you master a few key concepts, called\nobjects. Kubernetes defines many types of objects. From a developer’s perspective, the\nmost important objects are the following:\nPod—A pod is the basic unit of deployment in Kubernetes. It consists of one or\nmore containers that share an IP address and storage volumes. The pod for a\nservice instance often consists of a single container, such as a container running\nthe JVM. But in some scenarios a pod contains one or more sidecar containers,\nwhich implement supporting functions. For example, an NGINX server could\nhave a sidecar that periodically does a git pull to download the latest version\nof the website. A pod is ephemeral, because either the pod’s containers or the\nnode it’s running on might crash.\nDeployment—A declarative specification of a pod. A deployment is a controller\nthat ensures that the desired number of instances of the pod (service instances)\nare running at all times. It supports versioning with rolling upgrades and roll-\nbacks. Later in section 12.4.2, you’ll see that each service in a microservice\narchitecture is a Kubernetes deployment.\nService—Provides clients of an application service with a static/stable network\nlocation. It’s a form of infrastructure-provided service discovery, described in\nchapter 3. A service has an IP address and a DNS name that resolves to that IP\naddress and load balances TCP and UDP traffic across one or more pods. The\nIP address and a DNS name are only accessible within the Kubernetes. Later, I\ndescribe how to configure services that are accessible from outside the cluster.\nConfigMap—A named collection of name-value pairs that defines the external-\nized configuration for one or more application services (see chapter 11 for an\noverview of externalized configuration). The definition of a pod’s container\ncan reference a ConfigMap to define the container’s environment variables. It\ncan also use a ConfigMap to create configuration files inside the container. You\ncan store sensitive information, such as passwords, in a form of ConfigMap\ncalled a Secret.\nNow that we’ve reviewed the key Kubernetes concepts, let’s see them in action by look-\ning at how to deploy an application service on Kubernetes. \n12.4.2 Deploying the Restaurant service on Kubernetes\nAs mentioned earlier, to deploy a service on Kubernetes, you need to define a deploy-\nment. The easiest way to create a Kubernetes object such as a deployment is by writing\na YAML file. Listing 12.4 is a YAML file defining a deployment for Restaurant Service.\nThis deployment specifies running two replicas of a pod. The pod has just one container.\n \n",
      "content_length": 2917,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 433,
      "content": "403\nDeploying the FTGO application with Kubernetes\nThe container definition specifies the Docker image running along with other attri-\nbutes, such as the values of environment variables. The container’s environment vari-\nables are the service’s externalized configuration. They are read by Spring Boot and\nmade available as properties in the application context.\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\nname: ftgo-restaurant-service\nspec:\nreplicas: 2\ntemplate:\nmetadata:\nlabels:\napp: ftgo-restaurant-service\n   \nspec:\n  \n   containers:\n- name: ftgo-restaurant-service\n  image: msapatterns/ftgo-restaurant-service:latest\n  imagePullPolicy: Always\n  ports:\n  - containerPort: 8080          \n    name: httpport\n  env:                              \n    - name: JAVA_OPTS\n      value: \"-Dsun.net.inetaddr.ttl=30\"\n    - name: SPRING_DATASOURCE_URL\n      value: jdbc:mysql://ftgo-mysql/eventuate\n    - name: SPRING_DATASOURCE_USERNAME\n      valueFrom:\n        secretKeyRef:\n          name: ftgo-db-secret\n          key: username\n    - name: SPRING_DATASOURCE_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: ftgo-db-secret            \n          key: password\n    - name: SPRING_DATASOURCE_DRIVER_CLASS_NAME\n      value: com.mysql.jdbc.Driver\n    - name: EVENTUATELOCAL_KAFKA_BOOTSTRAP_SERVERS\n      value: ftgo-kafka:9092\n    - name: EVENTUATELOCAL_ZOOKEEPER_CONNECTION_STRING\n      value: ftgo-zookeeper:2181\n  livenessProbe:\n    httpGet:\n      path: /actuator/health\n      port: 8080\n    initialDelaySeconds: 60\n    periodSeconds: 20\n  readinessProbe:\nListing 12.4\nKubernetes Deployment for ftgo-restaurant-service\nSpecifies that this is an \nobject of type Deployment\nThe name of the deployment\nNumber of pod replicas\nGives each pod a label \ncalled app whose value is \nftgo-restaurant-service\nThe specification of \nthe pod, which defines \njust one container\n The container’s port\nThe container’s environment \nvariables, which are read by \nSpring Boot\nSensitive values that \nare retrieved from the \nKubernetes Secret \ncalled ftgo-db-secret\nConfigure Kubernetes \nto invoke the health \ncheck endpoint.\n \n",
      "content_length": 2129,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 434,
      "content": "404\nCHAPTER 12\nDeploying microservices\n    httpGet:\n      path: /actuator/health\n      port: 8080\n    initialDelaySeconds: 60\n    periodSeconds: 20\nThis deployment definition configures Kubernetes to invoke Restaurant Service’s\nhealth check endpoint. As described in chapter 11, a health check endpoint enables\nKubernetes to determine the health of the service instance. Kubernetes implements\ntwo different checks. The first check is readinessProbe, which it uses to determine\nwhether it should route traffic to a service instance. In this example, Kubernetes\ninvokes the /actuator/health HTTP endpoint every 20 seconds after an initial 30-\nsecond delay, which gives it a chance to initialize. If some number (default is 1) of\nconsecutive readinessProbes succeeds, Kubernetes considers the service to be ready,\nwhereas if some number (default, 3) of consecutive readinessProbes fail, it’s consid-\nered not to be ready. Kubernetes will only route traffic to the service instance when\nthe readinessProbe indicates that it’s ready.\n The second health check is the livenessProbe. It’s configured the same way as the\nreadinessProbe. But rather than determine whether traffic should be routed to a ser-\nvice instance, the livenessProbe determines whether Kubernetes should terminate\nand restart the service instance. If some number (default, 3) of consecutive liveness-\nProbes fail in a row, Kubernetes will terminate and restart the service.\n Once you’ve written the YAML file, you can create or update the deployment by\nusing the kubectl apply command:\nkubectl apply -f ftgo-restaurant-service/src/deployment/kubernetes/ftgo-\nrestaurant-service.yml\nThis command makes a request to the Kubernetes API server that results in the cre-\nation of the deployment and the pods.\n To create this deployment, you must first create the Kubernetes Secret called\nftgo-db-secret. One quick and insecure way to do that is as follows:\nkubectl create secret generic ftgo-db-secret \\\n--from-literal=username=mysqluser --from-literal=password=mysqlpw\nThis command creates a secret containing the database user ID and password speci-\nfied on the command line. See the Kubernetes documentation (https://kubernetes\n.io/docs/concepts/configuration/secret/#creating-your-own-secrets) for more secure\nways to create secrets.\nCREATING A KUBERNETES SERVICE\nAt this point the pods are running, and the Kubernetes deployment will do its best to\nkeep them running. The problem is that the pods have dynamically assigned IP\naddresses and, as such, aren’t that useful to a client that wants to make an HTTP\nrequest. As described in chapter 3, the solution is to use a service discovery mechanism.\n \n",
      "content_length": 2662,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 435,
      "content": "405\nDeploying the FTGO application with Kubernetes\nOne approach is to use a client-side discovery mechanism and install a service registry,\nsuch as Netflix OSS Eureka. Fortunately, we can avoid doing that by using the service\ndiscovery mechanism built in to Kubernetes and define a Kubernetes service.\n A service is a Kubernetes object that provides the clients of one or more pods with a\nstable endpoint. It has an IP address and a DNS name that resolves that IP address.\nThe service load balances traffic to that IP address across the pods. Listing 12.5\nshows the Kubernetes service for Restaurant Service. This service routes traffic\nfrom http://ftgo-restaurant-service:8080 to the pods defined by the deploy-\nment shown in the listing.\napiVersion: v1\nkind: Service\nmetadata:\nname: ftgo-restaurant-service\n \nspec:\nports:\n- port: 8080\ntargetPort: 8080\nselector:\n app: ftgo-restaurant-service\n  \n---\nThe key part of the service definition is selector, which selects the target pods. It selects\nthose pods that have a label named app with the value ftgo-restaurant-service. If\nyou look closely, you’ll see that the container defined in listing 12.4 has such a label.\n Once you’ve written the YAML file, you can create the service using this command:\nkubectl apply -f ftgo-restaurant-service-service.yml\nNow that we’ve created the Kubernetes service, any clients of Restaurant Service\nthat are running inside the Kubernetes cluster can access its REST API via http://\nftgo-restaurant-service:8080. Later, I discuss how to upgrade running services,\nbut first let’s take a look at how to make the services accessible from outside the\nKubernetes cluster. \n12.4.3 Deploying the API gateway\nThe Kubernetes service for Restaurant Service, shown in listing 12.5, is only accessi-\nble from within the cluster. That’s not a problem for Restaurant Service, but what\nabout API Gateway? Its role is to route traffic from the outside world to the service. It\ntherefore needs to be accessible from outside the cluster. Fortunately, a Kubernetes\nservice supports this use case as well. The service we looked at earlier is a ClusterIP\nservice, which is the default, but there are, however, two other types of services: Node-\nPort and LoadBalancer.\nListing 12.5\nThe YAML definition of the Kubernetes service for \nftgo-restaurant-service\nThe name of the service, \nalso the DNS name\nThe exposed \nport\nThe container port \nto route traffic to\nSelects the containers \nto route traffic to\n \n",
      "content_length": 2467,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 436,
      "content": "406\nCHAPTER 12\nDeploying microservices\n A NodePort service is accessible via a cluster-wide port on all the nodes in the clus-\nter. Any traffic to that port on any cluster node is load balanced to the backend pods.\nYou must select an available port in the range of 30000–32767. For example, listing 12.6\nshows a service that routes traffic to port 30000 of Consumer Service.\napiVersion: v1\nkind: Service\nmetadata:\nname: ftgo-api-gateway\nspec:\ntype: NodePort\n  \nports:\n- nodePort: 30000\n  \nport: 80\ntargetPort: 8080\nselector:\napp: ftgo-api-gateway\n---\nAPI Gateway is within the cluster using the URL http://ftgo-api-gateway and out-\nside the URL http://<node-ip-address>:3000/, where node-ip-address is the IP\naddress of one of the nodes. After configuring a NodePort service you can, for exam-\nple, configure an AWS Elastic Load Balancer (ELB) to load balance requests from the\ninternet across the nodes. A key benefit of this approach is that the ELB is entirely\nunder your control. You have complete flexibility when configuring it.\n A NodePort type service isn’t the only option, though. You can also use a Load-\nBalancer service, which automatically configures a cloud-specific load balancer. The\nload balancer will be an ELB if Kubernetes is running on AWS. One benefit of this\ntype of service is that you no longer have to configure your own load balancer. The\ndrawback, however, is that although Kubernetes does give a few options for configur-\ning the ELB, such the SSL certificate, you have a lot less control over its configuration. \n12.4.4 Zero-downtime deployments\nImagine you’ve updated Restaurant Service and want to deploy those changes into\nproduction. Updating a running service is a simple three-step process when using\nKubernetes:\n1\nBuild a new container image and push it to the registry using the same process\ndescribed earlier. The only difference is that the image will be tagged with a dif-\nferent version tag—for example, ftgo-restaurant-service:1.1.0.RELEASE.\n2\nEdit the YAML file for the service’s deployment so that it references the new image.\n3\nUpdate the deployment using the kubectl apply -f command.\nKubernetes will then perform a rolling upgrade of the pods. It will incrementally cre-\nate pods running version 1.1.0.RELEASE and terminate the pods running version\nListing 12.6\nThe YAML definition of a NodePort service that routes traffic to port \n8082 of Consumer Service\nSpecifies a type \nof NodePort\nThe cluster-\nwide port\n \n",
      "content_length": 2462,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 437,
      "content": "407\nDeploying the FTGO application with Kubernetes\n1.0.0.RELEASE. What’s great about how Kubernetes does this is that it doesn’t ter-\nminate old pods until their replacements are ready to handle requests. It uses the\nreadinessProbe mechanism, a health check mechanism described earlier in this\nsection, to determine whether a pod is ready. As a result, there will always be pods\navailable to handle requests. Eventually, assuming the new pods start successfully, all\nthe deployment’s pods will be running the new version.\n But what if there’s a problem and the version 1.1.0.RELEASE pods don’t start?\nPerhaps there’s a bug, such as a misspelled container image name or a missing envi-\nronment variable for a new configuration property. If the pods fail to start, the deploy-\nment will become stuck. At that point, you have two options. One option is to fix the\nYAML file and rerun kubectl apply -f to update the deployment. The other option is\nto roll back the deployment.\n A deployment maintains the history of what are termed rollouts. Each time you\nupdate the deployment, it creates a new rollout. As a result, you can easily roll back a\ndeployment to a previous version by executing the following command:\nkubectl rollout undo deployment ftgo-restaurant-service\nKubernetes will then replace the pods running version 1.1.0.RELEASE with pods run-\nning the older version, 1.0.0.RELEASE.\n A Kubernetes deployment is a good way to deploy a service without downtime. But\nwhat if a bug only appears after the pod is ready and receiving production traffic? In\nthat situation, Kubernetes will continue to roll out new versions, so a growing number\nof users will be impacted. Though your monitoring system will hopefully detect the issue\nand quickly roll back the deployment, you won’t avoid impacting at least some users. To\naddress this issue and make rolling out a new version of a service more reliable, we need\nto separate deploying, which means getting the service running in production, from\nreleasing the service, which means making it available to handle production traffic.\nLet’s look at how to accomplish that using a service mesh. \n12.4.5 Using a service mesh to separate deployment from release\nThe traditional way to roll out a new version of a service is to first test it in a staging\nenvironment. Then, once it’s passed the test in staging, you deploy in production by\ndoing a rolling upgrade that replaces old instances of the service with new service\ninstances. On one hand, as you just saw, Kubernetes deployments make doing a roll-\ning upgrade very straightforward. On the other hand, this approach assumes that\nonce a service version has passed the tests in the staging environment, it will work in\nproduction. Sadly, this is not always the case.\n One reason is because staging is unlikely to be an exact clone, if for no other reason\nthan the production environment is likely to be much larger and handle much more\ntraffic. It’s also time consuming to keep the two environments synchronized. As a result\nof discrepancies, it’s likely that some bugs will only show up in production. And even it\nwere an exact clone, you can’t guarantee that testing will catch all bugs.\n \n",
      "content_length": 3187,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 438,
      "content": "408\nCHAPTER 12\nDeploying microservices\n A much more reliable way to roll out a new version is to separate deployment from\nrelease:\nDeployment—Running in the production environment\nReleasing a service—Making it available to end users\nYou then deploy a service into production using the following steps:\n1\nDeploy the new version into production without routing any end-user requests\nto it.\n2\nTest it in production.\n3\nRelease it to a small number of end users.\n4\nIncrementally release it to an increasingly larger number of users until it’s han-\ndling all the production traffic.\n5\nIf at any point there’s an issue, revert back to the old version—otherwise, once\nyou’re confident the new version is working correctly, delete the old version.\nIdeally, those steps will be performed by a fully automated deployment pipeline that\ncarefully monitors the newly deployed service for errors.\n Traditionally, separating deployments and releases in this way has been challeng-\ning because it requires a lot of work to implement it. But one of the benefits of using a\nservice mesh is that using this style of deployment is a lot easier. A service mesh is, as\ndescribed in chapter 11, networking infrastructure that mediates all communication\nbetween a service and other services and external applications. In addition to taking\non some of the responsibilities of the microservice chassis framework, a service mesh\nprovides rule-based load balancing and traffic routing that lets you safely run multiple\nversions of your services simultaneously. Later in this section, you’ll see that you can\nroute test users to one version of a service and end-users to a different version, for\nexample.\n As described in chapter 11, there are several service meshes to choose from. In this\nsection, I show you how to use Istio, a popular, open source service mesh originally\ndeveloped by Google, IBM, and Lyft. I begin by providing a brief overview of Istio and\na few of its many features. Next I describe how to deploy an application using Istio.\nAfter that, I show how to use its traffic-routing capabilities to deploy and release an\nupgrade to a service.\nOVERVIEW OF THE ISTIO SERVICE MESH\nThe Istio website describes Istio as an “An open platform to connect, manage, and\nsecure microservices” (https://istio.io). It’s a networking layer through which all of\nyour services’ network traffic flows. Istio has a rich set of features organized into four\nmain categories:\nTraffic management—Includes service discovery, load balancing, routing rules,\nand circuit breakers\nSecurity—Secures interservice communication using Transport Layer Security\n(TLS)\n \n",
      "content_length": 2627,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 439,
      "content": "409\nDeploying the FTGO application with Kubernetes\nTelemetry—Captures metrics about network traffic and implements distributed\ntracing\nPolicy enforcement—Enforces quotas and rate limits\nThis section focuses on Istio’s traffic-management capabilities.\n Figure 12.11 shows Istio’s architecture. It consists of a control plane and a data\nplane. The control plane implements management functions, including configuring\nthe data plane to route traffic. The data plane consists of Envoy proxies, one per ser-\nvice instance.\n The two main components of the control plane are the Pilot and the Mixer. The Pilot\nextracts information about deployed services from the underlying infrastructure. When\nrunning on Kubernetes, for example, the Pilot retrieves the services and healthy pods. It\nconfigures the Envoy proxies to route traffic according to the defined routing rules. The\nMixer collects telemetry from the Envoy proxies and enforces policies.\nAPI Gateway\ncontainer\nGET/consumers/1\nGET/consumers/1\nGET/consumers/1\nHost: ftgo-consumer-service\nGET/consumers/1\nHost: ftgo-consumer-service\nPod\nService registry\nConsumer\nService\ncontainer\nIstio Envoy\ncontainer\nLogging\nServer\nService\nPod\nMetrics\nServer\nIstio Envoy\ncontainer\nMixer\nPilot\nIstio control plane\nConﬁgures\nChecks\nTelemetry\nKubernetes\nPod\nKey\nConﬁguration\nRequests\nPolicy check\nTelemetry\nMonitoring infrastructure\nIstio data plane\nQueries for deployed services\nFigure 12.11\nIstio consists of a control plane, whose components include the Pilot and the Mixer, and a data \nplane, which consists of Envoy proxy servers. The Pilot extracts information about deployed services from the \nunderlying infrastructure and configures the data plane. The Mixer enforces policies such as quotas and gathers \ntelemetry, reporting it to the monitoring infrastructure servers. The Envoy proxy servers route traffic in and out of \nservices. There’s one Envoy proxy server per service instance.\n \n",
      "content_length": 1932,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 440,
      "content": "410\nCHAPTER 12\nDeploying microservices\nThe Istio Envoy proxy is a modified version of Envoy (www.envoyproxy.io). It’s a high-\nperformance proxy that supports a variety of protocols, including TCP, low-level pro-\ntocols such as HTTP and HTTPS, and higher-level protocols. It also understands\nMongoDB, Redis, and DynamoDB protocols. Envoy also supports robust interservice\ncommunication with features such as circuit breakers, rate limiting, and automatic\nretries. It can secure communication within the application by using TLS for inter-\nEnvoy communication.\n Istio uses Envoy as a sidecar, a process or container that runs alongside the service\ninstance and implements cross-cutting concerns. When running on Kubernetes, the\nEnvoy proxy is a container within the service’s pod. In other environments that don’t\nhave the pod concept, Envoy runs in the same container as the service. All traffic to\nand from a service flows through its Envoy proxy, which routes traffic according to the\nrouting rules given to it by the control plane. For example, direct Service  Service\ncommunication becomes Service  Source Envoy  Destination Envoy  Service.\nIstio is configured using Kubernetes-style YAML configuration files. It has a command-\nline tool called istioctl that’s similar to kubectl. You use istioctl for creating,\nupdating, and deleting rules and policies. When using Istio on Kubernetes, you can\nalso use kubectl.\n Let’s look at how to deploy a service with Istio. \nDEPLOYING A SERVICE WITH ISTIO\nDeploying a service on Istio is quite straightforward. You define a Kubernetes Service\nand a Deployment for each of your application’s services. Listing 12.7 shows the defini-\ntion of Service and Deployment for Consumer Service. Although it’s almost identical\nto the definitions I showed earlier, there are a few differences. That’s because Istio has\na few requirements for the Kubernetes services and pods:\nA Kubernetes service port must use the Istio naming convention of <proto-\ncol>[-<suffix>], where protocol is http, http2, grpc, mongo, or redis. If the\nport is unnamed, Istio will treat the port as a TCP port and won’t apply rule-\nbased routing.\nA pod should have an app label such as app: ftgo-consumer-service, which\nidentifies the service, in order to support Istio distributed tracing.\nIn order to run multiple versions of a service simultaneously, the name of a\nKubernetes deployment must include the version, such as ftgo-consumer-\nservice-v1, ftgo-consumer-service-v2, and so on. A deployment’s pods should\nhave a version label, such as version: v1, which specifies the version, so that\nIstio can route to a specific version.\nPattern: Sidecar\nImplement cross-cutting concerns in a sidecar process or container that runs alongside\nthe service instance. See http://microservices.io/patterns/deployment/sidecar.html.\n \n",
      "content_length": 2837,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 441,
      "content": "411\nDeploying the FTGO application with Kubernetes\napiVersion: v1\nkind: Service\nmetadata:\nname: ftgo-consumer-service\nspec:\nports:\n- name: http\n  \nport: 8080\ntargetPort: 8080\nselector:\napp: ftgo-consumer-service\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\nname: ftgo-consumer-service-v2\n  \nspec:\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\napp: ftgo-consumer-service\n  \nversion: v2\nspec:\ncontainers:\n- image: image: ftgo-consumer-service:v2\n  \n...\nBy now, you may be wondering how to run the Envoy proxy container in the service’s\npod. Fortunately, Istio makes that remarkably easy by automating modifying the pod\ndefinition to include the Envoy proxy. There are two ways to do that. The first is to use\nmanual sidecar injection  and run the istioctl kube-inject command:\nistioctl kube-inject -f ftgo-consumer-service/src/deployment/kubernetes/ftgo-\nconsumer-service.yml | kubectl apply -f -\nThis command reads a Kubernetes YAML file and outputs the modified configura-\ntion containing the Envoy proxy. The modified configuration is then piped into\nkubectl apply.\n The second way to add the Envoy sidecar to the pod is to use automatic sidecar injec-\ntion. When this feature is enabled, you deploy a service using kubectl apply. Kubernetes\nautomatically invokes Istio to modify the pod definition to include the Envoy proxy.\n If you describe your service’s pod, you’ll see that it consists of more than your ser-\nvice’s container:\n$ kubectl describe po ftgo-consumer-service-7db65b6f97-q9jpr\nName:\nftgo-consumer-service-7db65b6f97-q9jpr\nNamespace:\ndefault\n...\nListing 12.7\nDeploying Consumer Service with Istio\nNamed port\nVersioned \ndeployment\nRecommended \nlabels\nImage \nversion\n \n",
      "content_length": 1695,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 442,
      "content": "412\nCHAPTER 12\nDeploying microservices\nInit Containers:\nistio-init:\n \nImage:\ndocker.io/istio/proxy_init:0.8.0\n....\nContainers:\nftgo-consumer-service:\n \nImage:\nmsapatterns/ftgo-consumer-service:latest\n...\nistio-proxy:\nImage:\ndocker.io/istio/proxyv2:0.8.0\n  \n...\nNow that we’ve deployed the service, let’s look at how to define routing rules. \nCREATE ROUTING RULES TO ROUTE TO THE V1 VERSION\nLet’s imagine that you deployed the ftgo-consumer-service-v2 deployment. In the\nabsence of routing rules, Istio load balances requests across all versions of a service. It\nwould, therefore, load balance across versions 1 and 2 of ftgo-consumer-service,\nwhich defeats the purpose of using Istio. In order to safely roll out a new version, you\nmust define a routing rule that routes all traffic to the current v1 version.\nInitializes the pod\nThe service \ncontainer\nThe Envoy \ncontainer\nAPI gateway\npod\nVirtualService\nDestinationRule\nConsumer\nService\nv1 pod\nmetadata:\nlabels:\napp: ftgo-consumer-service\nversion: v1\nConsumer\nService\nv2 pod\nRoutes to the v\nsubset\n1\nRouting rule for the\nConsumer Service\nDeﬁnes subsets of\npods of a service\nNo trafﬁc routed to v2.\nDeﬁnes subsets\nv\nand v2\n1\nAll trafﬁc routed to v1\nmetadata:\nlabels:\napp: ftgo-consumer-service\nversion: v2\nkind: DestinationRule\nmetadata:\nname:ftgo-consumer-service\nspec:\nhost: ftgo-consumer-service\nsubsets:\n-name: v1\nlabels:\nversion: v1\n-name: v2\nlabels:\nversion: v2\nkind: VirtualService\nmetadata:\nname:ftgo-consumer-service\nspec:\nhosts:\n-ftgo-consumer-service\nhttp:\n-route:\n-destination:\nhost: ftgo-consumer-service\nsubset: v1\nweight: 100\nGET/consumers/1\nhost:ftgo-consumer-\nservice\nFigure 12.12\nThe routing rule for Consumer Service, which routes all traffic to the v1 pods. It consists of a \nVirtualService, which routes its traffic to the v1 subset, and a DestinationRule, which defines the v1 \nsubset as the pods labeled with version: v1. Once you’ve defined this rule, you can safely deploy a new version \nwithout routing any traffic to it initially.\n \n",
      "content_length": 2010,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 443,
      "content": "413\nDeploying the FTGO application with Kubernetes\nFigure 12.12 shows the routing rule for Consumer Service that routes all traffic to v1.\nIt consists of two Istio objects: a VirtualService and a DestinationRule.\n A VirtualService defines how to route requests for one or more hostnames. In this\nexample, VirtualService defines the routes for a single hostname: ftgo-consumer-\nservice. Here’s the definition of VirtualService for Consumer Service:\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: ftgo-consumer-service\nspec:\nhosts:\n- ftgo-consumer-service\n  \nhttp:\n- route:\n- destination:\nhost: ftgo-consumer-service\n  \nsubset: v1\nIt routes all requests for the v1 subset of the pods of Consumer Service. Later, I show\nmore complex examples that route based on HTTP requests and load balance across\nmultiple weighted destinations.\n In addition to VirtualService, you must also define a DestinationRule, which\ndefines one or more subsets of pods for a service. A subset of pods is typically a service\nversion. A DestinationRule can also define traffic policies, such as the load-balancing\nalgorithm. Here’s the DestinationRule for Consumer Service:\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\nname: ftgo-consumer-service\nspec:\nhost: ftgo-consumer-service\nsubsets:\n- name: v1\n  \nlabels:\nversion: v1\n  \n- name: v2\nlabels:\nversion: v2\nThis DestinationRule defines two subsets of pods: v1 and v2. The v1 subset selects\npods with the label version: v1. The v2 subset selects pods with the label version: v2.\n Once you’ve defined these rules, Istio will only route traffic pods labeled version:\nv1. It’s now safe to deploy v2. \n \n \nApplies to the \nConsumer Service\nRoutes to \nConsumer Service\nThe v1 subset\nThe name of \nthe subset\nThe pod selector \nfor the subset\n \n",
      "content_length": 1817,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 444,
      "content": "414\nCHAPTER 12\nDeploying microservices\nDEPLOYING VERSION 2 OF CONSUMER SERVICE\nHere’s an excerpt of the version 2 Deployment for Consumer Service:\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\nname: ftgo-consumer-service-v2\n  \nspec:\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\napp: ftgo-consumer-service\nversion: v2\n  \n...\nThis deployment is called ftgo-consumer-service-v2. It labels its pods with version:\nv2. After creating this deployment, both versions of the ftgo-consumer-service will be\nrunning. But because of the routing rules, Istio won’t route any traffic to v2. You’re\nnow ready to route some test traffic to v2. \nROUTING TEST TRAFFIC TO VERSION 2\nOnce you’ve deployed a new version of a service, the next step is to test it. Let’s sup-\npose that requests from test users have a testuser header . We can enhance the ftgo-\nconsumer-service VirtualService to route requests with this header to v2 instances\nby making the following change:\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: ftgo-consumer-service\nspec:\nhosts:\n- ftgo-consumer-service\nhttp:\n- match:\n- headers:\ntestuser:\nregex: \"^.+$\"\n  \nroute:\n- destination:\nhost: ftgo-consumer-service\nsubset: v2\n  \n- route:\n- destination:\nhost: ftgo-consumer-service\nsubset: v1\n  \nIn addition to the original default route, VirtualService has a routing rule that\nroutes requests with the testuser header to the v2 subset. After you’ve updated the\nrules, you can now test Consumer Service. Then, once you feel confident that the v2 is\nworking, you can route some production traffic to it. Let’s look at how to do that. \nVersion 2\nPod is labeled \nwith the version\nMatches a nonblank \ntestuser header\nRoutes test \nusers to v2\nRoutes everyone \nelse to v1\n \n",
      "content_length": 1750,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 445,
      "content": "415\nDeploying services using the Serverless deployment pattern\nROUTING PRODUCTION TRAFFIC TO VERSION 2\nAfter you’ve tested a newly deployed service, the next step is to start routing produc-\ntion traffic to it. A good strategy is to initially only route a small amount of traffic.\nHere, for example, is a rule that routes 95% of traffic to v1 and 5% to v2:\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\nname: ftgo-consumer-service\nspec:\nhosts:\n- ftgo-consumer-service\nhttp:\n- route:\n- destination:\nhost: ftgo-consumer-service\nsubset: v1\nweight: 95\n- destination:\nhost: ftgo-consumer-service\nsubset: v2\nweight: 5\nAs you gain confidence that the service can handle production traffic, you can incre-\nmentally increase the amount of traffic going to the version 2 pods until it reaches\n100%. At that point, Istio isn’t routing any traffic to the v1 pods. You could leave them\nrunning for a little while longer before deleting the version 1 Deployment.\n By letting you easily separate deployment from release, Istio makes rolling out a\nnew version of a service much more reliable. Yet I’ve barely scratched the surface of\nIstio’s capabilities. As of the time of writing, the current version of Istio is 0.8. I’m\nexcited to watch it and the other service meshes mature and become a standard part\nof a production environment. \n12.5\nDeploying services using the Serverless deployment \npattern\nThe Language-specific packaging (section 12.1), Service as a VM (section 12.2), and\nService as a container (section 12.3) patterns are all quite different, but they share\nsome common characteristics. The first is that with all three patterns you must prepro-\nvision some computing resources—either physical machines, virtual machines, or con-\ntainers. Some deployment platforms implement autoscaling, which dynamically adjusts\nthe number of VMs or containers based on the load. But you’ll always need to pay for\nsome VMs or containers, even if they’re idle.\n Another common characteristic is that you’re responsible for system administra-\ntion. If you’re running any kind of machine, you must patch the operating system. In\nthe case of physical machines, this also includes racking and stacking. You’re also\nresponsible for administering the language runtime. This is an example of what Ama-\nzon called “undifferentiated heavy lifting.” Since the early days of computing, system\n \n",
      "content_length": 2393,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 446,
      "content": "416\nCHAPTER 12\nDeploying microservices\nadministration has been one of those things you need to do. As it turns out, though,\nthere’s a solution: serverless.\n12.5.1 Overview of serverless deployment with AWS Lambda\nAt AWS Re:Invent 2014, Werner Vogels, the CTO of Amazon, introduced AWS\nLambda with the amazing phrase “magic happens at the intersection of functions,\nevents, and data.” As this phrase suggests, AWS Lambda was initially for deploying\nevent-driven services. It’s “magic” because, as you’ll see, AWS Lambda is an example of\nserverless deployment technology.\nAWS Lambda supports Java, NodeJS, C#, GoLang, and Python. A lambda function is a\nstateless service. It typically handles requests by invoking AWS services. For example, a\nlambda function that’s invoked when an image is uploaded to an S3 bucket could\ninsert an item into a DynamoDB IMAGES table and publish a message to Kinesis to\ntrigger image processing. A lambda function can also invoke third-party web services.\n To deploy a service, you package your application as a ZIP file or JAR file, upload it\nto AWS Lambda, and specify the name of the function to invoke to handle a request\n(also called an event). AWS Lambda automatically runs enough instances of your\nmicroservice to handle incoming requests. You’re billed for each request based on the\ntime taken and the memory consumed. Of course, the devil is in the details, and later\nyou’ll see that AWS Lambda has limitations. But the notion that neither you as a devel-\noper nor anyone in your organization need worry about any aspect of servers, virtual\nmachines, or containers is incredibly powerful.\nServerless deployment technologies\nThe main public clouds all provide a serverless deployment option, although AWS\nLambda is the most advanced. Google Cloud has Google Cloud functions, which as\nof the time writing is in beta (https://cloud.google.com/functions/). Microsoft Azure\nhas Azure functions (https://azure.microsoft.com/en-us/services/functions).\nThere are also open source serverless frameworks, such as Apache Openwhisk\n(https://openwhisk.apache.org) and Fission for Kubernetes (https://fission.io), that\nyou can run on your own infrastructure. But I’m not entirely convinced of their value.\nYou need to manage the infrastructure that runs the serverless framework—which\ndoesn’t exactly sound like serverless. Moreover, as you’ll see later in this section,\nserverless provides a constrained programming model in exchange for minimal sys-\ntem administration. If you need to manage infrastructure, then you have the con-\nstraints without the benefit.\nPattern: Serverless deployment\nDeploy services using a serverless deployment mechanism provided by a public cloud.\nSee http://microservices.io/patterns/deployment/serverless-deployment.html. \n \n",
      "content_length": 2782,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 447,
      "content": "417\nDeploying services using the Serverless deployment pattern\n12.5.2 Developing a lambda function\nUnlike when using the other three patterns, you must use a different programming\nmodel for your lambda functions. A lambda function’s code and the packaging\ndepend on the programming language. A Java lambda function is a class that imple-\nments the generic interface RequestHandler, which is defined by the AWS Lambda\nJava core library and shown in the following listing. This interface takes two type\nparameters: I, which is the input type, and O, which is the output type. The type of I\nand O depend on the specific kind of request that the lambda handles.\npublic interface RequestHandler<I, O> {\npublic O handleRequest(I input, Context context);\n}\nThe RequestHandler interface defines a single handleRequest() method. This method\nhas two parameters, an input object and a context, which provide access to the lambda\nexecution environment, such as the request ID. The handleRequest() method\nreturns an output object. For lambda functions that handle HTTP requests that are\nproxied by an AWS API Gateway, I and O are APIGatewayProxyRequestEvent and\nAPIGatewayProxyResponseEvent, respectively. As you’ll soon see, the handler func-\ntions are quite similar to old-style Java EE servlets.\n A Java lambda is packaged as either a ZIP file or a JAR file. A JAR file is an uber JAR\n(or fat JAR) created by, for example, the Maven Shade plugin. A ZIP file has the\nclasses in the root directory and JAR dependencies in the lib directory. Later, I show\nhow a Gradle project can create a ZIP file. But first, let’s look at the different ways of\ninvoking lambda function. \n12.5.3 Invoking lambda functions\nThere are four ways to invoke a lambda function:\nHTTP requests\nEvents generated by AWS services\nScheduled invocations\nDirectly using an API call\nLet’s look at each one.\nHANDLING HTTP REQUESTS\nOne way to invoke a lambda function is to configure an AWS API Gateway to route\nHTTP requests to your lambda. The API gateway exposes your lambda function as\nan HTTPS endpoint. It functions as an HTTP proxy, invokes the lambda function\nwith an HTTP request object, and expects the lambda function to return an HTTP\nresponse object. By using the API gateway with AWS Lambda you can, for example,\ndeploy RESTful services as lambda functions. \nListing 12.8\nA Java lambda function is a class that implements the RequestHandler \ninterface.\n \n",
      "content_length": 2427,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 448,
      "content": "418\nCHAPTER 12\nDeploying microservices\nHANDLING EVENTS GENERATED BY AWS SERVICES\nThe second way to invoke a lambda function is to configure your lambda function to\nhandle events generated by an AWS service. Examples of events that can trigger a\nlambda function include the following:\nAn object is created in an S3 bucket.\nAn item is created, updated, or deleted in a DynamoDB table.\nA message is available to read from a Kinesis stream.\nAn email is received via the Simple email service.\nBecause of this integration with other AWS services, AWS Lambda is useful for a wide\nrange of tasks. \nDEFINING SCHEDULED LAMBDA FUNCTIONS\nAnother way to invoke a lambda function is to use a Linux cron-like schedule. You can\nconfigure your lambda function to be invoked periodically—for example, every minute,\n3 hours, or 7 days. Alternatively, you can use a cron expression to specify when AWS\nshould invoke your lambda. cron expressions give you tremendous flexibility. For exam-\nple, you can configure a lambda to be invoked at 2:15 p.m. Monday through Friday. \nINVOKING A LAMBDA FUNCTION USING A WEB SERVICE REQUEST\nThe fourth way to invoke a lambda function is for your application to invoke it using a\nweb service request. The web service request specifies the name of the lambda function\nand the input event data. Your application can invoke a lambda function synchronously\nor asynchronously. If your application invokes the lambda function synchronously, the\nweb service’s HTTP response contains the response of the lambda function. Other-\nwise, if it invokes the lambda function asynchronously, the web service response indi-\ncates whether the execution of the lambda was successfully initiated. \n12.5.4 Benefits of using lambda functions\nDeploying services using lambda functions has several benefits:\nIntegrated with many AWS services—It’s remarkably straightforward to write lamb-\ndas that consume events generated by AWS services, such as DynamoDB and\nKinesis, and handle HTTP requests via the AWS API Gateway.\nEliminates many system administration tasks—You’re no longer responsible for low-\nlevel system administration. There are no operating systems or runtimes to\npatch. As a result, you can focus on developing your application.\nElasticity—AWS Lambda runs as many instances of your application as are needed\nto handle the load. You don’t have the challenge of predicting needed capacity or\nrun the risk of underprovisioning or overprovisioning VMs or containers.\nUsage-based pricing—Unlike a typical IaaS cloud, which charges by the minute or\nhour for a VM or container even when it’s idle, AWS Lambda only charges you\nfor the resources that are consumed while processing each request. \n \n",
      "content_length": 2702,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 449,
      "content": "419\nDeploying a RESTful service using AWS Lambda and AWS Gateway\n12.5.5 Drawbacks of using lambda functions\nAs you can see, AWS Lambda is an extremely convenient way to deploy services, but\nthere are some significant drawbacks and limitations:\nLong-tail latency—Because AWS Lambda dynamically runs your code, some\nrequests have high latency because of the time it takes for AWS to provision an\ninstance of your application and for the application to start. This is particularly\nchallenging when running Java-based services because they typically take at least\nseveral seconds to start. For instance, the example lambda function described in\nthe next section takes a while to start up. Consequently, AWS Lambda may not\nbe suited for latency-sensitive services.\nLimited event/request-based programming model—AWS Lambda isn’t intended to be\nused to deploy long-running services, such as a service that consumes messages\nfrom a third-party message broker.\nBecause of these drawbacks and limitations, AWS Lambda isn’t a good fit for all\nservices. But when choosing a deployment pattern, I recommend first evaluating\nwhether serverless deployment supports your service’s requirements before consid-\nering alternatives. \n12.6\nDeploying a RESTful service using AWS Lambda \nand AWS Gateway\nLet’s take a look at how to deploy Restaurant Service using AWS Lambda. It’s a ser-\nvice that has a REST API for creating and managing restaurants. It doesn’t have long-\nlived connections to Apache Kafka, for example, so it’s a good fit for AWS lambda. Fig-\nure 12.13 shows the deployment architecture for this service. The service consists of\nseveral lambda functions, one for each REST endpoint. An AWS API Gateway is\nresponsible for routing HTTP requests to the lambda functions.\n Each lambda function has a request handler class. The ftgo-create-restaurant\nlambda function invokes the CreateRestaurantRequestHandler class, and the ftgo-\nfind-restaurant lambda function invokes FindRestaurantRequestHandler. Because\nthese request handler classes implement closely related aspects of the same service,\nthey’re packaged together in the same ZIP file, restaurant-service-aws-lambda\n.zip. Let’s look at the design of the service, including those handler classes.\n12.6.1 The design of the AWS Lambda version of Restaurant Service\nThe architecture of the service, shown in figure 12.14, is quite similar to that of a tra-\nditional service. The main difference is that Spring MVC controllers have been\nreplaced by AWS Lambda request handler classes. The rest of the business logic is\nunchanged.\n The service consists of a presentation tier consisting of the request handlers, which\nare invoked by AWS Lambda to handle the HTTP requests, and a traditional business\n \n",
      "content_length": 2745,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 450,
      "content": "420\nCHAPTER 12\nDeploying microservices\nAPI gateway\nAWS Lambda\nfunctions\nftgo-create-restaurant\nftgo-ﬁnd-restaurant\nftgo-restaurant-service-aws-lambda.zip\nftgo-...\nPOST/restaurant\nGET/restaurant/\n{restaurantId}\n«class»\nCreate\nRestaurant\nRequest\nHandler\n«class»\nFindRestaurant\nRequest\nHandler\n«class»\n...\nRequest\nHandler\nFigure 12.13\nDeploying Restaurant Service as AWS Lambda functions. The \nAWS API Gateway routes HTTP requests to the AWS Lambda functions, which are \nimplemented by request handler classes defined by Restaurant Service.\nCreate\nRestaurant\nRequest\nHandler\nPresentation layer\nPOST/restaurant\nGET/restaurant/{restaurantId}\nBusiness and\ndata access layer\nFind\nRestaurant\nRequest\nHandler\nRestaurantService\nRestaurantRepository\nRestaurant\n...\nRequest\nHandler\nFigure 12.14\nThe design of the AWS Lambda-based Restaurant Service. The \npresentation layer consists of request handler classes, which implement the lambda \nfunctions. They invoke the business tier, which is written in a traditional style \nconsisting of a service class, an entity, and a repository.\n \n",
      "content_length": 1072,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 451,
      "content": "421\nDeploying a RESTful service using AWS Lambda and AWS Gateway\ntier. The business tier consists of RestaurantService, the Restaurant JPA entity, and\nRestaurantRepository, which encapsulates the database.\n Let’s take a look at the FindRestaurantRequestHandler class.\nTHE FINDRESTAURANTREQUESTHANDLER CLASS\nThe FindRestaurantRequestHandler class implements the GET /restaurant/\n{restaurantId} endpoint. This class along with the other request handler classes are\nthe leaves of the class hierarchy shown in figure 12.15. The root of the hierarchy is\nRequestHandler, which is part of the AWS SDK. Its abstract subclasses handle errors\nand inject dependencies.\nThe AbstractHttpHandler class is the abstract base class for HTTP request handlers.\nIt catches unhandled exceptions thrown during request handling and returns a 500 -\ninternal server error response. The AbstractAutowiringHttpRequestHandler class\nimplements dependency injection for request handlers. I’ll describe these abstract\nsuperclasses shortly, but first let’s look at the code for FindRestaurantRequestHandler.\n Listing 12.9 shows the code for the FindRestaurantRequestHandler class. The\nFindRestaurantRequestHandler class has a handleHttpRequest() method, which\ntakes an APIGatewayProxyRequestEvent representing an HTTP request as a parame-\nter. It invokes RestaurantService to find the restaurant and returns an APIGateway-\nProxyResponseEvent describing the HTTP response.\nRequest\nHandler\nAbstract\nHttpHandler\nAbstract\nAutowiring\nHttpRequest\nHandler\nCreate\nRestaurant\nRequest\nHandler\nFind\nRestaurant\nRequest\nHandler\n...\nRequest\nHandler\nFigure 12.15\nThe design of the request handler \nclasses. The abstract superclasses implement \ndependency injection and error handling.\n \n",
      "content_length": 1740,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 452,
      "content": "422\nCHAPTER 12\nDeploying microservices\npublic class FindRestaurantRequestHandler \nextends AbstractAutowiringHttpRequestHandler {\n@Autowired\nprivate RestaurantService restaurantService;\n@Override\nprotected Class<?> getApplicationContextClass() {\nreturn CreateRestaurantRequestHandler.class;\n  \n}\n@Override\nprotected APIGatewayProxyResponseEvent\nhandleHttpRequest(APIGatewayProxyRequestEvent request, Context context) {\nlong restaurantId;\ntry {\nrestaurantId = Long.parseLong(request.getPathParameters()\n          .get(\"restaurantId\"));\n} catch (NumberFormatException e) {\nreturn makeBadRequestResponse(context);\n  \n}\nOptional<Restaurant> possibleRestaurant = restaurantService.findById(restaur\nantId);\nreturn possibleRestaurant\n  \n.map(this::makeGetRestaurantResponse)\n.orElseGet(() -> makeRestaurantNotFoundResponse(context,\nrestaurantId));\n}\nprivate APIGatewayProxyResponseEvent makeBadRequestResponse(Context context) {\n...\n}\nprivate APIGatewayProxyResponseEvent\nmakeRestaurantNotFoundResponse(Context context, long restaurantId) { ... }\nprivate\nAPIGatewayProxyResponseEvent\nmakeGetRestaurantResponse(Restaurant restaurant) { ... }\n}\nAs you can see, it’s quite similar to a servlet, except that instead of a service()\nmethod, which takes an HttpServletRequest and returns HttpServletResponse, it\nhas a handleHttpRequest(), which takes an APIGatewayProxyRequestEvent and returns\nAPIGatewayProxyResponseEvent.\n Let’s now take a look at its superclass, which implements dependency injection. \nListing 12.9\nThe handler class for GET /restaurant/{restaurantId}\nThe Spring Java \nconfiguration class to use \nfor the application context\nReturns a 400 - bad request \nresponse if the restaurantId \nis missing or invalid\nReturns either the \nrestaurant or a 404 - \nnot found response\n \n",
      "content_length": 1775,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 453,
      "content": "423\nDeploying a RESTful service using AWS Lambda and AWS Gateway\nDEPENDENCY INJECTION USING THE ABSTRACTAUTOWIRINGHTTPREQUESTHANDLER CLASS\nAn AWS Lambda function is neither a web application nor an application with a\nmain() method. But it would be a shame to not be able to use the features of Spring\nBoot that we’ve been accustomed to. The AbstractAutowiringHttpRequestHandler\nclass, shown in the following listing, implements dependency injection for request han-\ndlers. It creates an ApplicationContext using SpringApplication.run() and autowires\ndependencies prior to handling the first request. Subclasses such as FindRestaurant-\nRequestHandler must implement the getApplicationContextClass() method.\npublic abstract class AbstractAutowiringHttpRequestHandler \nextends AbstractHttpHandler {\nprivate static ConfigurableApplicationContext ctx;\nprivate ReentrantReadWriteLock ctxLock = new ReentrantReadWriteLock();\nprivate boolean autowired = false;\nprotected synchronized ApplicationContext getAppCtx() {   \nctxLock.writeLock().lock();\ntry {\nif (ctx == null) {\nctx =\nSpringApplication.run(getApplicationContextClass());\n}\nreturn ctx;\n} finally {\nctxLock.writeLock().unlock();\n}\n}\n@Override\nprotected void\nbeforeHandling(APIGatewayProxyRequestEvent request, Context context) {\nsuper.beforeHandling(request, context);\nif (!autowired) {\ngetAppCtx().getAutowireCapableBeanFactory().autowireBean(this);  \nautowired = true;\n}\n}\nprotected abstract Class<?> getApplicationContextClass();\n  \n}\nThis class overrides the beforeHandling() method defined by AbstractHttpHandler.\nIts beforeHandling() method injects dependencies using autowiring before handling\nthe first request. \nTHE ABSTRACTHTTPHANDLER CLASS\nThe request handlers for Restaurant Service ultimately extend AbstractHttpHandler,\nshown in listing 12.11. This class implements RequestHandler<APIGatewayProxy-\nRequestEvent and APIGatewayProxyResponseEvent>. Its key responsibility is to catch\nexceptions thrown when handling a request and throw a 500 error code. \nListing 12.10\nAn abstract RequestHandler that implements dependency injection\nCreates the Spring \nBoot application \ncontext just once\nInjects dependencies into\nthe request handler using\nautowiring before handling\nthe first request\nReturns the @Configuration\nclass used to create\nApplicationContext\n \n",
      "content_length": 2317,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 454,
      "content": "424\nCHAPTER 12\nDeploying microservices\npublic abstract class AbstractHttpHandler implements\nRequestHandler<APIGatewayProxyRequestEvent, APIGatewayProxyResponseEvent> {\nprivate Logger log = LoggerFactory.getLogger(this.getClass());\n@Override\npublic APIGatewayProxyResponseEvent handleRequest(\nAPIGatewayProxyRequestEvent input, Context context) {\nlog.debug(\"Got request: {}\", input);\ntry {\nbeforeHandling(input, context);\nreturn handleHttpRequest(input, context);\n} catch (Exception e) {\nlog.error(\"Error handling request id: {}\", context.getAwsRequestId(), e);\nreturn buildErrorResponse(new AwsLambdaError(\n\"Internal Server Error\",\n\"500\",\ncontext.getAwsRequestId(),\n\"Error handling request: \" + context.getAwsRequestId() + \" \" \n+ input.toString()));\n}\n}\nprotected void beforeHandling(APIGatewayProxyRequestEvent request, \nContext context) {\n// do nothing\n}\nprotected abstract APIGatewayProxyResponseEvent handleHttpRequest(\nAPIGatewayProxyRequestEvent request, Context context);\n}\n12.6.2 Packaging the service as ZIP file\nBefore the service can be deployed, we must package it as a ZIP file. We can easily\nbuild the ZIP file using the following Gradle task:\ntask buildZip(type: Zip) {\nfrom compileJava\nfrom processResources\ninto('lib') {\nfrom configurations.runtime\n}\n}\nThis task builds a ZIP with the classes and resources at the top level and the JAR\ndependencies in the lib directory.\n Now that we’ve built the ZIP file, let’s look at how to deploy the lambda function. \nListing 12.11\nAn abstract RequestHandler that catches exceptions and returns \na 500 HTTP response\n \n",
      "content_length": 1574,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 455,
      "content": "425\nDeploying a RESTful service using AWS Lambda and AWS Gateway\n12.6.3 Deploying lambda functions using the Serverless framework\nUsing the tools provided by AWS to deploy lambda functions and configure the API\ngateway is quite tedious. Fortunately, the Serverless open source project makes using\nlambda functions a lot easier. When using Serverless, you write a simple server-\nless.yml file that defines your lambda functions and their RESTful endpoints.\nServerless then deploys the lambda functions and creates and configures an API gate-\nway that routes requests to them.\n The following listing is an excerpt of the serverless.yml that deploys Restaurant\nService as a lambda.\nservice: ftgo-application-lambda\nprovider:\nname: aws\n  \nruntime: java8\ntimeout: 35\nregion: ${env:AWS_REGION}\nstage: dev\nenvironment:\n  \nSPRING_DATASOURCE_DRIVER_CLASS_NAME: com.mysql.jdbc.Driver\nSPRING_DATASOURCE_URL: ...\nSPRING_DATASOURCE_USERNAME: ...\nSPRING_DATASOURCE_PASSWORD: ...\npackage:\n  \nartifact: ftgo-restaurant-service-aws-lambda/build/distributions/\nftgo-restaurant-service-aws-lambda.zip\nfunctions:\n  \ncreate-restaurant:\nhandler: net.chrisrichardson.ftgo.restaurantservice.lambda\n.CreateRestaurantRequestHandler\nevents:\n- http:\npath: restaurants\nmethod: post\nfind-restaurant:\nhandler: net.chrisrichardson.ftgo.restaurantservice.lambda\n.FindRestaurantRequestHandler\nevents:\n- http:\npath: restaurants/{restaurantId}\nmethod: get\nYou can then use the serverless deploy command, which reads the serverless.yml\nfile, deploys the lambda functions, and configures the AWS API Gateway. After a short\nListing 12.12\nThe serverless.yml deploys Restaurant Service.\nTells serverless to \ndeploy on AWS\nSupplies the service’s \nexternalized configuration \nvia environment variables\nThe ZIP file \ncontaining the \nlambda functions\nLambda function definitions \nconsisting of the handler \nfunction and HTTP endpoint\n \n",
      "content_length": 1891,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 456,
      "content": "426\nCHAPTER 12\nDeploying microservices\nwait, your service will be accessible via the API gateway’s endpoint URL. AWS Lambda\nwill provision as many instances of each Restaurant Service lambda function that are\nneeded to support the load. If you change the code, you can easily update the lambda\nby rebuilding the ZIP file and rerunning serverless deploy. No servers involved!\n The evolution of infrastructure is remarkable. Not that long ago, we manually\ndeployed applications on physical machines. Today, highly automated public clouds\nprovide a range of virtual deployment options. One option is to deploy services as vir-\ntual machines. Or better yet, we can package services as containers and deploy them\nusing sophisticated Docker orchestration frameworks such as Kubernetes. Sometimes\nwe even avoid thinking about infrastructure entirely and deploy services as light-\nweight, ephemeral lambda functions. \nSummary\nYou should choose the most lightweight deployment pattern that supports your\nservice’s requirements. Evaluate the options in the following order: serverless,\ncontainers, virtual machines, and language-specific packages.\nA serverless deployment isn’t a good fit for every service, because of long-tail\nlatencies and the requirement to use an event/request-based programming\nmodel. When it is a good fit, though, serverless deployment is an extremely\ncompelling option because it eliminates the need to administer operating sys-\ntems and runtimes and provides automated elastic provisioning and request-\nbased pricing.\nDocker containers, which are a lightweight, OS-level virtualization technol-\nogy, are more flexible than serverless deployment and have more predictable\nlatency. It’s best to use a Docker orchestration framework such as Kuberne-\ntes, which manages containers on a cluster of machines. The drawback of\nusing containers is that you must administer the operating systems and run-\ntimes and most likely the Docker orchestration framework and the VMs that\nit runs on.\nThe third deployment option is to deploy your service as a virtual machine. On\none hand, virtual machines are a heavyweight deployment option, so deploy-\nment is slower and it will most likely use more resources than the second\noption. On the other hand, modern clouds such as Amazon EC2 are highly\nautomated and provide a rich set of features. Consequently, it may sometimes\nbe easier to deploy a small, simple application using virtual machines than to\nset up a Docker orchestration framework.\nDeploying your services as language-specific packages is generally best avoided\nunless you only have a small number of services. For example, as described in\nchapter 13, when starting on your journey to microservices you’ll probably\ndeploy the services using the same mechanism you use for your monolithic\napplication, which is most likely this option. You should only consider setting\n \n",
      "content_length": 2887,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 457,
      "content": "427\nSummary\nup a sophisticated deployment infrastructure such as Kubernetes once you’ve\ndeveloped some services.\nOne of the many benefits of using a service mesh—a networking layer that\nmediates all network traffic in and out of services—is that it enables you to\ndeploy a service in production, test it, and only then route production traffic to\nit. Separating deployment from release improves the reliability of rolling out\nnew versions of services. \n \n",
      "content_length": 456,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 458,
      "content": "428\nRefactoring to\nmicroservices\nI hope that this book has given you a good understanding of the microservice\narchitecture, its benefits and drawbacks, and when to use it. There is, however, a\nfairly good chance you’re working on a large, complex monolithic application.\nYour daily experience of developing and deploying your application is slow and\npainful. Microservices, which appear like a good fit for your application, seem like\ndistant nirvana. Like Mary and the rest of the FTGO development team, you’re\nwondering how on earth you can adopt the microservice architecture?\n Fortunately, there are strategies you can use to escape from monolithic hell\nwithout having to rewrite your application from scratch. You incrementally convert\nThis chapter covers\nWhen to migrate a monolithic application to a \nmicroservice architecture\nWhy using an incremental approach is essential \nwhen refactoring a monolithic application to \nmicroservices\nImplementing new features as services\nExtracting services from the monolith\nIntegrating a service and the monolith\n \n",
      "content_length": 1064,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 459,
      "content": "429\nOverview of refactoring to microservices\nyour monolith into microservices by developing what’s known as a strangler applica-\ntion. The idea of a strangler application comes from strangler vines, which grow in\nrain forests by enveloping and sometimes killing trees. A strangler application is a new\napplication consisting of microservices that you develop by implementing new func-\ntionality as services and extracting services from the monolith. Over time, as the stran-\ngler application implements more and more functionality, it shrinks and ultimately\nkills the monolith. An important benefit of developing a strangler application is that,\nunlike a big bang rewrite, it delivers value to the business early and often.\n I begin this chapter by describing the motivations for refactoring a monolith to a\nmicroservice architecture. I then describe how to develop the strangler application\nby implementing new functionality as services and extracting services from the\nmonolith. Next, I cover various design topics, including how to integrate the mono-\nlith and services, how to maintain database consistency across the monolith and\nservices, and how to handle security. I end the chapter by describing a couple of\nexample services. One service is Delayed Order Service, which implements brand\nnew functionality. The other service is Delivery Service, which is extracted from\nthe monolith. Let’s start by taking a look at the concept of refactoring to a micro-\nservice architecture.\n13.1\nOverview of refactoring to microservices\nPut yourself in Mary’s shoes. You’re responsible for the FTGO application, a large and\nold monolithic application. The business is extremely frustrated with engineering’s\ninability to deliver features rapidly and reliably. FTGO appears to be suffering from a\nclassic case of monolithic hell. Microservices seem, at least on the surface, to be the\nanswer. Should you propose diverting development resources away from feature devel-\nopment to migrating to a microservice architecture?\n I start this section by discussing why you should consider refactoring to microser-\nvices. I also discuss why it’s important to be sure that your software development prob-\nlems are because you’re in monolithic hell rather than in, for example, a poor\nsoftware development process. I then describe strategies for incrementally refactoring\nyour monolith to a microservice architecture. Next, I discuss the importance of deliv-\nering improvements earlier and often in order to maintain the support of the busi-\nness. I then describe why you should avoid investing in a sophisticated deployment\ninfrastructure until you’ve developed a few services. Finally, I describe the various\nstrategies you can use to introduce services into your architecture, including imple-\nmenting new features as services and extracting services from the monolith.\n13.1.1 Why refactor a monolith?\nThe microservice architecture has, as described in chapter 1, numerous benefits. It\nhas much better maintainability, testability, and deployability, so it accelerates devel-\nopment. The microservice architecture is more scalable and improves fault isolation.\nIt’s also much easier to evolve your technology stack. But refactoring a monolith to\n \n",
      "content_length": 3233,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 460,
      "content": "430\nCHAPTER 13\nRefactoring to microservices\nmicroservices is a significant undertaking. It will divert resources away from new fea-\nture development. As a result, it’s likely that the business will only support the adop-\ntion of microservices if it solves a significant business problem.\n If you’re in monolithic hell, it’s likely that you already have at least one business\nproblem. Here are some examples of business problems caused by monolithic hell:\nSlow delivery—The application is difficult to understand, maintain, and test, so\ndeveloper productivity is low. As a result, the organization is unable to compete\neffectively and risks being overtaken by competitors.\nBuggy software releases—The lack of testability means that software releases are\noften buggy. This makes customers unhappy, which results in losing customers\nand reduced revenue.\nPoor scalability—Scaling a monolithic application is difficult because it combines\nmodules with very different resource requirements into one executable compo-\nnent. The lack of scalability means that it’s either impossible or prohibitively\nexpensive to scale the application beyond a certain point. As a result, the appli-\ncation can’t support the current or predicted needs of the business.\nIt’s important to be sure that these problems are there because you’ve outgrown your\narchitecture. A common reason for slow delivery and buggy releases is a poor software\ndevelopment process. For example, if you’re still relying on manual testing, then\nadopting automated testing alone can significantly increase development velocity.\nSimilarly, you can sometimes solve scalability problems without changing your archi-\ntecture. You should first try simpler solutions. If, and only if, you still have software\ndelivery problems should you then migrate to the microservice architecture. Let’s\nlook at how to do that. \n13.1.2 Strangling the monolith\nThe process of transforming a monolithic application into microservices is a form of\napplication modernization (https://en.wikipedia.org/wiki/Software_modernization).\nApplication modernization is the process of converting a legacy application to one having\na modern architecture and technology stack. Developers have been modernizing appli-\ncations for decades. As a result, there is wisdom accumulated through experience we\ncan use when refactoring an application into a microservice architecture. The most\nimportant lesson learned over the years is to not do a big bang rewrite.\n A big bang rewrite is when you develop a new application—in this case, a microservices-\nbased application—from scratch. Although starting from scratch and leaving the legacy\ncode base behind sounds appealing, it’s extremely risky and will likely end in failure.\nYou will spend months, possibly years, duplicating the existing functionality, and only\nthen can you implement the features that the business needs today! Also, you’ll\nneed to develop the legacy application anyway, which diverts effort away from the\nrewrite and means that you have a constantly moving target. What’s more, it’s possible\n \n",
      "content_length": 3078,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 461,
      "content": "431\nOverview of refactoring to microservices\nthat you’ll waste time reimplementing features that are no longer needed. As Martin\nFowler reportedly said, “the only thing a Big Bang rewrite guarantees is a Big Bang!”\n(www.randyshoup.com/evolutionary-architecture).\n Instead of doing a big bang rewrite, you should, as figure 13.1 shows, incrementally\nrefactor your monolithic application. You gradually build a new application, which is\ncalled a strangler application. It consists of microservices that runs in conjunction\nwith your monolithic application. Over time, the amount of functionality imple-\nmented by the monolithic application shrinks until either it disappears entirely or it\nbecomes just another microservice. This strategy is akin to servicing your car while\ndriving down the highway at 70 mph. It’s challenging, but is far less risky that attempt-\ning a big bang rewrite.\nMartin Fowler refers to this application modernization strategy as the Strangler appli-\ncation pattern (www.martinfowler.com/bliki/StranglerApplication.html). The name\ncomes from the strangler vine (or strangler fig—see https://en.wikipedia.org/wiki/\nStrangler_fig) that is found in rain forests. A strangler vine grows around a tree in\nThe monolith shrinks over time.\nThe strangler application\ngrows larger over time.\nMonolith\nMonolith\nService\nStrangler application\nMonolith\nService\nMonolith\nService\n...\n...\nMonolith\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nService\nTime\nFigure 13.1\nThe monolith is incrementally replaced by a strangler application comprised of services. \nEventually, the monolith is replaced entirely by the strangler application or becomes another \nmicroservice.\n \n",
      "content_length": 1775,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 462,
      "content": "432\nCHAPTER 13\nRefactoring to microservices\norder to reach the sunlight above the forest canopy. Often the tree dies, because\neither it’s killed by the vine or it dies of old age, leaving a tree-shaped vine.\nThe refactoring process typically takes months, or years. For example, according to\nSteve Yegge (https://plus.google.com/+RipRowan/posts/eVeouesvaVX) it took Ama-\nzon.com a couple of years to refactor its monolith. In the case of a very large system,\nyou may never complete the process. You could, for example, get to a point where you\nhave tasks that are more important than breaking up the monolith, such as imple-\nmenting revenue-generating features. If the monolith isn’t an obstacle to ongoing\ndevelopment, you may as well leave it alone. \nDEMONSTRATE VALUE EARLY AND OFTEN\nAn important benefit of incrementally refactoring to a microservice architecture is\nthat you get an immediate return on your investment. That’s very different than a big\nbang rewrite, which doesn’t deliver any benefit until it’s complete. When incremen-\ntally refactoring the monolith, you can develop each new service using a new technology\nstack and a modern, high-velocity, DevOps-style development and delivery process. As a\nresult, your team’s delivery velocity steadily increases over time.\n What’s more, you can migrate the high-value areas of your application to microser-\nvices first. For instance, imagine you’re working on the FTGO application. The business\nmight, for example, decide that the delivery scheduling algorithm is a key competitive\nadvantage. It’s likely that delivery management will be an area of constant, ongoing\ndevelopment. By extracting delivery management into a standalone service, the delivery\nmanagement team will be able to work independently of the rest of the FTGO develop-\ners and significantly increase their development velocity. They’ll be able to frequently\ndeploy new versions of the algorithm and evaluate their effectiveness.\n Another benefit of being able to deliver value earlier is that it helps maintain the\nbusiness’s support for the migration effort. Their ongoing support is essential, because\nthe refactoring effort will mean that less time is spent on developing features. Some\norganizations have difficulty eliminating technical debt because past attempts were\ntoo ambitious and didn’t provide much benefit. As a result, the business becomes\nreluctant to invest in further cleanup efforts. The incremental nature of refactoring to\nmicroservices means that the development team is able to demonstrate value early\nand often. \nMINIMIZE CHANGES TO THE MONOLITH\nA recurring theme in this chapter is that you should avoid making widespread\nchanges to the monolith when migrating to a microservice architecture. It’s inevitable\nPattern: Strangler application\nModernize an application by incrementally developing a new (strangler) application\naround the legacy application. See http://microservices.io/patterns/refactoring/\nstrangler-application.html.\n \n",
      "content_length": 2989,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 463,
      "content": "433\nStrategies for refactoring a monolith to microservices\nthat you’ll need to make some changes in order to support migration to services. Sec-\ntion 13.3.2 talks about how the monolith often needs to be modified so that it can par-\nticipate in sagas that maintain data consistency across the monolith and services. The\nproblem with making widespread changes to the monolith is that it’s time consuming,\ncostly, and risky. After all, that’s probably why you want to migrate to microservices in\nthe first place.\n Fortunately, there are strategies you can use for reducing the scope of the changes\nyou need to make. For example, in section 13.2.3, I describe the strategy of replicating\ndata from an extracted service back to the monolith’s database. And in section 13.3.2,\nI show how you can carefully sequence the extraction of services to reduce the impact\non the monolith. By applying these strategies, you can reduce the amount of work\nrequired to refactor the monolith. \nTECHNICAL DEPLOYMENT INFRASTRUCTURE: YOU DON’T NEED ALL OF IT YET\nThroughout this book I’ve discussed a lot of shiny new technology, including deploy-\nment platforms such as Kubernetes and AWS Lambda and service discovery mecha-\nnisms. You might be tempted to begin your migrating to microservices by selecting\ntechnologies and building out that infrastructure. You might even feel pressure from\nthe business people and from your friendly PaaS vendor to start spending money on\nthis kind of infrastructure.\n As tempting as it seems to build out this infrastructure up front, I recommend only\nmaking a minimal up-front investment in developing it. The only thing you can’t live\nwithout is a deployment pipeline that performs automating testing. For example, if\nyou only have a handful of services, you don’t need a sophisticated deployment and\nobservability infrastructure. Initially, you can even get away with just using a hard-\ncoded configuration file for service discovery. I suggest deferring any decisions about\ntechnical infrastructure that involve significant investment until you’ve gained real\nexperience with the microservice architecture. It’s only once you have a few services\nrunning that you’ll have the experience to pick technologies.\n Let’s now look at the strategies you can use for migrating to a microservice\narchitecture. \n13.2\nStrategies for refactoring a monolith to microservices\nThere are three main strategies for strangling the monolith and incrementally replac-\ning it with microservices:\n1\nImplement new features as services.\n2\nSeparate the presentation tier and backend.\n3\nBreak up the monolith by extracting functionality into services.\nThe first strategy stops the monolith from growing. It’s typically a quick way to\ndemonstrate the value of microservices, helping build support for the migration\neffort. The other two strategies break apart the monolith. When refactoring your\nmonolith, you might sometimes use the second strategy, but you’ll definitely use the\n \n",
      "content_length": 2974,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 464,
      "content": "434\nCHAPTER 13\nRefactoring to microservices\nthird strategy, because it’s how functionality is migrated from the monolith into the\nstrangler application.\n Let’s take a look at each of these strategies, starting with implementing new fea-\ntures as services.\n13.2.1 Implement new features as services\nThe Law of Holes states that “if you find yourself in a hole, stop digging” (https://\nen.m.wikipedia.org/wiki/Law_of_holes). This is great advice to follow when your mono-\nlithic application has become unmanageable. In other words, if you have a large, com-\nplex monolithic application, don’t implement new features by adding code to the\nmonolith. That will make your monolith even larger and more unmanageable. Instead,\nyou should implement new features as services.\n This is a great way to begin migrating your monolithic application to a microser-\nvice architecture. It reduces the growth rate of the monolith. It accelerates the devel-\nopment of the new features, because you’re doing development in a brand new code\nbase. It also quickly demonstrates the value of adopting the microservice architecture.\nINTEGRATING THE NEW SERVICE WITH THE MONOLITH\nFigure 13.2 shows the application’s architecture after implementing a new feature as a\nservice. Besides the new service and monolith, the architecture includes two other ele-\nments that integrate the service into the application:\nAPI gateway—Routes requests for new functionality to the new service and\nroutes legacy requests to the monolith.\nIntegration glue code—Integrates the service with the monolith. It enables the ser-\nvice to access data owned by the monolith and to invoke functionality imple-\nmented by the monolith.\nThe integration glue code isn’t a standalone component. Instead, it consists of adapt-\ners in the monolith and the service that use one or more interprocess communication\nmechanisms. For example, integration glue for Delayed Delivery Service, described\nin section 13.4.1, uses both REST and domain events. The service retrieves customer\ncontract information from the monolith by invoking a REST API. The monolith pub-\nlishes Order domain events so that Delayed Delivery Service can track the state of\nOrders and respond to orders that won’t be delivered on time. Section 13.3.1 describes\nthe integration glue code in more detail.\nWHEN TO IMPLEMENT A NEW FEATURE AS A SERVICE\nIdeally, you should implement every new feature in the strangler application rather\nthan in the monolith. You’ll implement a new feature as either a new service or as part\nof an existing service. This way you’ll avoid ever having to touch the monolith code\nbase. Unfortunately, though, not every new feature can be implemented as a service.\n That’s because the essence of a microservice architecture is a set of loosely coupled\nservices that are organized around business capabilities. A feature might, for instance,\nbe too small to be a meaningful service. You might, for example, just need to add a\n \n",
      "content_length": 2961,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 465,
      "content": "435\nStrategies for refactoring a monolith to microservices\nfew fields and methods to an existing class. Or the new feature might be too tightly\ncoupled to the code in the monolith. If you attempted to implement this kind of fea-\nture as a service you would typically find that performance would suffer because of\nexcessive interprocess communication. You might also have problems maintaining\ndata consistency. If a new feature can’t be implemented as a service, the solution is\noften to initially implement the new feature in the monolith. Later on, you can then\nextract that feature along with other related features into their own service.\n Implementing new features as services accelerates the development of those fea-\ntures. It’s a good way to quickly demonstrate the value of the microservice architec-\nture. It also reduces the monolith’s growth rate. But ultimately, you need to break\napart the monolith using the two other strategies. You need to migrate functionality to\nthe strangler application by extracting functionality from the monolith into services.\nYou might also be able to improve development velocity by splitting the monolith hor-\nizontally. Let’s look at how to do that. \nMonolith\nOutbound\nadapter\nAPI gateway\nOld features\nNew features\nIntegration\nglue\nInbound\nadapter\nInbound\nadapter\nDatabase\nadapter\nDatabase\nadapter\nInbound\nadapter\nEvent\nsubscriber\nadapter\nEvent\npublisher\nadapter\nService\ndatabase\nMonolith\ndatabase\n«aggregate»\nDelayedDelivery\nService\n«aggregate»\nOrder\n«aggregate»\nNotification\nService\nimplementing\nnew feature\nFigure 13.2\nA new feature is implemented as a service that’s part of the strangler application. The \nintegration glue integrates the service with the monolith and consists of adapters that implement \nsynchronous and asynchronous APIs. An API gateway routes requests that invoke new functionality \nto the service.\n \n",
      "content_length": 1870,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 466,
      "content": "436\nCHAPTER 13\nRefactoring to microservices\n13.2.2 Separate presentation tier from the backend\nOne strategy for shrinking a monolithic application is to split the presentation layer\nfrom the business logic and data access layers. A typical enterprise application consists\nof the following layers:\nPresentation logic—This consists of modules that handle HTTP requests and gener-\nate HTML pages that implement a web UI. In an application that has a sophisti-\ncated user interface, the presentation tier is often a substantial body of code.\nBusiness logic—This consists of modules that implement the business rules, which\ncan be complex in an enterprise application.\nData access logic—This consists of modules that access infrastructure services\nsuch as databases and message brokers.\nThere is usually a clean separation between the presentation logic and the business\nand data access logic. The business tier has a coarse-grained API consisting of one or\nmore facades that encapsulate the business logic. This API is a natural seam along\nwhich you can split the monolith into two smaller applications, as shown in figure 13.3.\nBusiness logic\nBusiness logic\nREST\nAPI\nREST\nclient\nWeb\napp\nBrowser\nBrowser\nHTML pages\nHTML pages\nMonolith containing\npresentation logic and\nbackend business logic\nSmaller, independently\ndeployable presentation\nlogic monolith\nMySQL\nDatabase\nadapter\nMySQL\nDatabase\nadapter\nWeb\napplication\nSplit\nSmaller, independently\ndeployable backend\nmonolith\nAn API that is callable\nby any future services\nFigure 13.3\nSplitting the frontend from the backend enables each to be deployed independently. It also exposes \nan API for services to invoke.\n \n",
      "content_length": 1664,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 467,
      "content": "437\nStrategies for refactoring a monolith to microservices\nOne application contains the presentation layer, and the other contains the business\nand data access logic. After the split, the presentation logic application makes remote\ncalls to the business logic application.\n Splitting the monolith in this way has two main benefits. It enables you to develop,\ndeploy, and scale the two applications independently of one another. In particular, it\nallows the presentation layer developers to rapidly iterate on the user interface and\neasily perform A/B testing, for example, without having to deploy the backend.\nAnother benefit of this approach is that it exposes a remote API that can be called by\nthe microservices you develop later.\n But this strategy is only a partial solution. It’s very likely that at least one or both of\nthe resulting applications will still be an unmanageable monolith. You need to use the\nthird strategy to replace the monolith with services. \n13.2.3 Extract business capabilities into services\nImplementing new features as services and splitting the frontend web application\nfrom the backend will only get you so far. You’ll still end up doing a lot of develop-\nment in the monolithic code base. If you want to significantly improve your applica-\ntion’s architecture and increase your development velocity, you need to break apart\nthe monolith by incrementally migrating business capabilities from the monolith to\nservices. For example, section 13.5 describes how to extract delivery management\nfrom the FTGO monolith into a new Delivery Service. When you use this strategy,\nover time the number of business capabilities implemented by the services grows, and\nthe monolith gradually shrinks.\n The functionality you want extract into a service is a vertical slice through the\nmonolith. The slice consists of the following:\nInbound adapters that implement API endpoints\nDomain logic\nOutbound adapters such as database access logic\nThe monolith’s database schema\nAs figure 13.4 shows, this code is extracted from the monolith and moved into a stand-\nalone service. An API gateway routes requests that invoke the extracted business capa-\nbility to the service and routes the other requests to the monolith. The monolith and\nthe service collaborate via the integration glue code. As described in section 13.3.1,\nthe integration glue consists of adapters in the service and monolith that use one or\nmore interprocess communication (IPC) mechanisms.\n Extracting services is challenging. You need to determine how to split the mono-\nlith’s domain model into two separate domain models, one of which becomes the ser-\nvice’s domain model. You need to break dependencies such as object references. You\nmight even need to split classes in order to move functionality into the service. You\nalso need to refactor the database.\n Extracting a service is often time consuming, especially because the monolith’s\ncode base is likely to be messy. Consequently, you need to carefully think about which\n \n",
      "content_length": 3014,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 468,
      "content": "438\nCHAPTER 13\nRefactoring to microservices\nservices to extract. It’s important to focus on refactoring those parts of the application\nthat provide a lot of value. Before extracting a service, ask yourself what the benefit is\nof doing that.\n For example, it’s worthwhile to extract a service that implements functionality\nthat’s critical to the business and constantly evolving. It’s not valuable to invest effort\nin extracting services when there’s not much benefit from doing so. Later in this sec-\ntion I describe some strategies for determining what to extract and when. But first,\nlet’s look in more detail at some of the challenges you’ll face when extracting a service\nand how to address them.\n You’ll encounter a couple of challenges when extracting a service:\nSplitting the domain model\nRefactoring the database\nLet’s look at each one, starting with splitting the domain model.\nOutbound\nadapter\nAPI gateway\nService containing\nextracted code\nIntegration\nglue\nInbound\nadapter\nInbound\nadapter\nDatabase\nadapter\nDatabase\nadapter\nInbound\nadapter\nOutbound\nadapter\nInbound\nadapter\nService\ndatabase\nMonolith\ndatabase\nInbound\nadapter\nDatabase\nadapter\nMonolith\ndatabase\n«\n»\nservice\nOrder Service\n«aggregate»\nCourier\n«aggregate»\nPlan\n«service»\nOrder Service\n«aggregate»\nCourier\n«aggregate»\nPlan\nCode to\nextract into\na service\nMonolith\n«\n»\nservice\nOrder Service\n«aggregate»\nOrder\n«\n»\naggregate\nOrder\nGlue code integrating\nservice with monolith\nFigure 13.4\nBreak apart the monolith by extracting services. You identify a slice of functionality, which consists \nof business logic and adapters, to extract into a service. You move that code into the service. The newly extracted \nservice and the monolith collaborate via the APIs provided by the integration glue.\n \n",
      "content_length": 1761,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 469,
      "content": "439\nStrategies for refactoring a monolith to microservices\nSPLITTING THE DOMAIN MODEL\nIn order to extract a service, you need to extract its domain model out of the monolith’s\ndomain model. You’ll need to perform major surgery to split the domain models. One\nchallenge you’ll encounter is eliminating object references that would otherwise span\nservice boundaries. It’s possible that classes that remain in the monolith will reference\nclasses that have been moved to the service or vice versa. For example, imagine that, as\nfigure 13.5 shows, you extract Order Service, and as a result its Order class references\nthe monolith’s Restaurant class. Because a service instance is typically a process, it\ndoesn’t make sense to have object references that cross service boundaries. Somehow\nyou need to eliminate these types of object reference.\nOne good way to solve this problem is to think in terms of DDD aggregates, described\nin chapter 5. Aggregates reference each other using primary keys rather than object ref-\nerences. You would, therefore, think of the Order and Restaurant classes as aggre-\ngates and, as figure 13.6 shows, replace the reference to Restaurant in the Order class\nwith a restaurantId field that stores the primary key value.\nFTGO monolith\nExtracted service\n«Entity»\nRestaurant\nObject reference that spans\nservice boundaries\n«Entity»\nOrder\nDelivery Service\nFTGO monolith\n?\n«Entity»\nRestaurant\n«Entity»\nOrder\nFigure 13.5\nThe Order domain class has a reference to a Restaurant class. If we extract \nOrder into a separate service, we need to do something about its reference to Restaurant, \nbecause object references between processes don’t make sense.\nReplace with primary key.\nDelivery Service\nFTGO monolith\n«Entity»\nRestaurant\n«Entity»\nOrder\nrestaurantId\nObject reference that spans\nservice boundaries\nDelivery Service\nFTGO monolith\n?\n«Entity»\nRestaurant\n«Entity»\nOrder\nFigure 13.6\nThe Order class’s reference to Restaurant is replaced with the Restaurant's \nprimary key in order to eliminate an object that would span process boundaries.\n \n",
      "content_length": 2060,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 470,
      "content": "440\nCHAPTER 13\nRefactoring to microservices\nOne issue with replacing object references with primary keys is that although this is a\nminor change to the class, it can potentially have a large impact on the clients of the\nclass, which expect an object reference. Later in this section, I describe how to reduce\nthe scope of the change by replicating data between the service and monolith. Delivery\nService, for example, could define a Restaurant class that’s a replica of the mono-\nlith’s Restaurant class.\n Extracting a service is often much more involved than moving entire classes into a\nservice. An even greater challenge with splitting a domain model is extracting func-\ntionality that’s embedded in a class that has other responsibilities. This problem often\noccurs in god classes, described in chapter 2, that have an excessive number of responsi-\nbilities. For example, the Order class is one of the god classes in the FTGO applica-\ntion. It implements multiple business capabilities, including order management,\ndelivery management, and so on. Later in section 13.5, I discuss how extracting the\ndelivery management into a service involves extracting a Delivery class from the\nOrder class. The Delivery entity implements the delivery management functionality\nthat was previously bundled with other functionality in the Order class. \nREFACTORING THE DATABASE\nSplitting a domain model involves more than just changing code. Many classes in a\ndomain model are persistent. Their fields are mapped to a database schema. Conse-\nquently, when you extract a service from the monolith, you’re also moving data. You\nneed to move tables from the monolith’s database to the service’s database.\n Also, when you split an entity you need to split the corresponding database table\nand move the new table to the service. For example, when extracting delivery manage-\nment into a service, you split the Order entity and extract a Delivery entity. At the\ndatabase level, you split the ORDERS table and define a new DELIVERY table. You then\nmove the DELIVERY table to the service.\n The book Refactoring Databases by Scott W. Ambler and Pramod J. Sadalage (Addison-\nWesley, 2011) describes a set of refactorings for a database schema. For example, it\ndescribes the Split Table refactoring, which splits a table into two or more tables.\nMany of the technique in that book are useful when extracting services from the\nmonolith. One such technique is the idea of replicating data in order to allow you to\nincrementally update clients of the database to use the new schema. We can adapt\nthat idea to reduce the scope of the changes you must make to the monolith when\nextracting a service.\nREPLICATE DATA TO AVOID WIDESPREAD CHANGES\nAs mentioned, extracting a service requires you to change to the monolith’s domain\nmodel. For example, you replace object references with primary keys and split classes.\nThese types of changes can ripple through the code base and require you to make\nwidespread changes to the monolith. For example, if you split the Order entity and\nextract a Delivery entity, you’ll have to change every place in the code that references\nthe fields that have been moved. Making these kinds of changes can be extremely\ntime consuming and can become a huge barrier to breaking up the monolith.\n \n",
      "content_length": 3292,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 471,
      "content": "441\nStrategies for refactoring a monolith to microservices\n A great way to delay and possibly avoid making these kinds of expensive changes is\nto use an approach that’s similar to the one described in Refactoring Databases. A major\nobstacle to refactoring a database is changing all the clients of that database to use the\nnew schema. The solution proposed in the book is to preserve the original schema for\na transition period and use triggers to synchronize the original and new schemas. You\nthen migrate clients from the old schema to the new schema over time.\n We can use a similar approach when extracting services from the monolith. For\nexample, when extracting the Delivery entity, we leave the Order entity mostly\nunchanged for a transition period. As figure 13.7 shows, we make the delivery-related\nfields read-only and keep them up-to-date by replicating data from Delivery Service\nback to the monolith. As a result, we only need to find the places in the monolith’s code\nthat update those fields and change them to invoke the new Delivery Service.\n Preserving the structure of the Order entity by replicating data from Delivery\nService significantly reduces the amount of work we need to do immediately. Over\ntime, we can migrate code that uses the delivery-related Order entity fields or ORDERS\ntable columns to Delivery Service. What’s more, it’s possible that we never need to\nRead-only\ndelivery-related\nﬁelds\nORDER_ID\n...\nORDER table\nRESTAURANT_ID\n...\nSCHEDULED_PICKUP_TIME\n...\nSCHEDULED_DELIVERY_TIME\n...\n...\n...\n«Entity»\nOrder\nFTGO monolith\n...\nconsumerId\nscheduledPickupTime\nscheduledDeliveryTime\n...\nORDER_ID\n...\nORDER table\nRESTAURANT_ID\n...\nSCHEDULED_PICKUP_TIME\n...\nSCHEDULED_DELIVERY_TIME\n...\n...\n...\n«Entity»\nOrder\nFTGO monolith\n...\nconsumerId\nscheduledPickupTime\nscheduledDeliveryTime\n...\nORDER_ID\n...\nDELIVERY table\nSCHEDULED_PICKUP_TIME\n...\nSCHEDULED_DELIVERY_TIME\n...\n...\n...\n«Entity»\nDelivery\nDelivery Service\nExtract Order Service and move columns from\nORDER\nDELIVERY\ntable to a new\ntable.\nReplicate data from Delivery Service to FTGO monolith.\n...\norderId\nscheduledPickupTime\nscheduledDeliveryTime\n...\nFigure 13.7\nMinimize the scope of the changes to the FTGO monolith by replicating delivery-related data from the \nnewly extracted Delivery Service back to the monolith’s database.\n \n",
      "content_length": 2315,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 472,
      "content": "442\nCHAPTER 13\nRefactoring to microservices\nmake that change in the monolith. If that code is subsequently extracted into a ser-\nvice, then the service can access Delivery Service.\nWHAT SERVICES TO EXTRACT AND WHEN\nAs I mentioned, breaking apart the monolith is time consuming. It diverts effort away\nfrom implementing features. As a result, you must carefully decide the sequence in\nwhich you extract services. You need to focus on extracting services that give the larg-\nest benefit. What’s more, you want to continually demonstrate to the business that\nthere’s value in migrating to a microservice architecture.\n On any journey, it’s essential to know where you’re going. A good way to start the\nmigration to microservices is with a time-boxed architecture definition effort. You\nshould spend a short amount of time, such as a couple of weeks, brainstorming your\nideal architecture and defining a set of services. This gives you a destination to aim\nfor. It’s important, though, to remember that this architecture isn’t set in stone. As\nyou break apart the monolith and gain experience, you should revise the architecture\nto take into account what you’ve learned.\n Once you’ve determined the approximate destination, the next step is to start\nbreaking apart the monolith. There are a couple of different strategies you can use to\ndetermine the sequence in which you extract services.\n One strategy is to effectively freeze development of the monolith and extract ser-\nvices on demand. Instead of implementing features or fixing bugs in the monolith,\nyou extract the necessary service or service(s) and change those. One benefit of this\napproach is that it forces you to break up the monolith. One drawback is that the\nextraction of services is driven by short-term requirements rather than long-term\nneeds. For instance, it requires you to extract services even if you’re making a small\nchange to a relatively stable part of the system. As a result, you risk doing a lot of work\nfor minimal benefit.\n An alternative strategy is a more planned approach, where you rank the modules\nof an application by the benefit you anticipate getting from extracting them. There\nare a few reasons why extracting a service is beneficial:\nAccelerates development—If your application’s roadmap suggests that a particular\npart of your application will undergo a lot of development over the next year,\nthen converting it to a service accelerates development.\nSolves a performance, scaling, or reliability problem—If a particular part of your appli-\ncation has a performance or scalability problem or is unreliable, then it’s valu-\nable to convert it to a service.\nEnables the extraction of some other services—Sometimes extracting one service sim-\nplifies the extraction of another service, due to dependencies between modules.\nYou can use these criteria to add refactoring tasks to your application’s backlog,\nranked by expected benefit. The benefit of this approach is that it’s more strategic\nand much more closely aligned with the needs of the business. During sprint plan-\nning, you decide whether it’s more valuable to implement features or extract services. \n \n",
      "content_length": 3154,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 473,
      "content": "443\nDesigning how the service and the monolith collaborate\n13.3\nDesigning how the service and the monolith \ncollaborate\nA service is rarely standalone. It usually needs to collaborate with the monolith. Some-\ntimes a service needs to access data owned by the monolith or invoke its operations.\nFor example, Delayed Delivery Service, described in detail in section 13.4.1, requires\naccess to the monolith’s orders and customer contact info. The monolith might also\nneed to access data owned by the service or invoke its operations. For example, later\nin section 13.5, when discussing how to extract delivery management into a service, I\ndescribe how the monolith needs to invoke Delivery Service.\n One important concern is maintaining data consistency between the service and\nmonolith. In particular, when you extract a service from the monolith, you invariably\nsplit what were originally ACID transactions. You must be careful to ensure that data\nconsistency is still maintained. As described later in this section, sometimes you use\nsagas to maintain data consistency.\n The interaction between a service and the monolith is, as described earlier, facili-\ntated by integration glue code. Figure 13.8 shows the structure of the integration glue.\nIt consists of adapters in the service and monolith that communicate using some kind\nof IPC mechanism. Depending on the requirements, the service and monolith might\ninteract over REST or they might use messaging. They might even communicate using\nmultiple IPC mechanisms.\nFor example, Delayed Delivery Service uses both REST and domain events. It\nretrieves customer contact info from the monolith using REST. It tracks the state of\nOrders by subscribing to domain events published by the monolith.\nMonolith\nService\nInbound\nadapter\nIntegration\nglue\nAPI\nadapter\nAPI\nadapter\nOutbound\nadapter\nOutbound\nadapter\nInbound\nadapter\nFigure 13.8\nWhen migrating a monolith to microservices, the services and monolith often need to access each \nother’s data. This interaction is facilitated by the integration glue, which consists of adapters that implement APIs. \nSome APIs are messaging based. Other APIs are RPI based.\n \n",
      "content_length": 2155,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 474,
      "content": "444\nCHAPTER 13\nRefactoring to microservices\n In this section, I first describe the design of the integration glue. I talk about the prob-\nlems it solves and the different implementation options. After that I describe transaction\nmanagement strategies, including the use of sagas. I discuss how sometimes the require-\nment to maintain data consistency changes the order in which you extract services.\n Let’s first look at the design of the integration glue.\n13.3.1 Designing the integration glue\nWhen implementing a feature as a service or extracting a service from the monolith,\nyou must develop the integration glue that enables a service to collaborate with the\nmonolith. It consists of code in both the service and monolith that uses some kind of\nIPC mechanism. The structure of the integration glue depends on the type of IPC\nmechanism that is used. If, for example, the service invokes the monolith using REST,\nthen the integration glue consists of a REST client in the service and web controllers\nin the monolith. Alternatively, if the monolith subscribes to domain events published\nby the service, then the integration glue consists of an event-publishing adapter in the\nservice and event handlers in the monolith.\nDESIGNING THE INTEGRATION GLUE API\nThe first step in designing the integration glue is to decide what APIs it provides to\nthe domain logic. There are a couple of different styles of interface to choose from,\ndepending on whether you’re querying data or updating data. Let’s say you’re work-\ning on Delayed Delivery Service, which needs to retrieve customer contact info\nfrom the monolith. The service’s business logic doesn’t need to know the IPC mecha-\nnism that the integration glue uses to retrieve the information. Therefore, that mecha-\nnism should be encapsulated by an interface. Because Delayed Delivery Service is\nquerying data, it makes sense to define a CustomerContactInfoRepository:\ninterface CustomerContactInfoRepository {\nCustomerContactInfo findCustomerContactInfo(long customerId)\n}\nThe service’s business logic can invoke this API without knowing how the integration\nglue retrieves the data.\n Let’s consider a different service. Imagine that you’re extracting delivery manage-\nment from the FTGO monolith. The monolith needs to invoke Delivery Service to\nschedule, reschedule, and cancel deliveries. Once again, the details of the underlying\nIPC mechanism aren’t important to the business logic and should be encapsulated by\nan interface. In this scenario, the monolith must invoke a service operation, so using a\nrepository doesn’t make sense. A better approach is to define a service interface, such\nas the following:\ninterface DeliveryService {\nvoid scheduleDelivery(...);\nvoid rescheduleDelivery(...);\nvoid cancelDelivery(...);\n}\n \n",
      "content_length": 2777,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 475,
      "content": "445\nDesigning how the service and the monolith collaborate\nThe monolith’s business logic invokes this API without knowing how it’s implemented\nby the integration glue.\n Now that we’ve seen interface design, let’s look at interaction styles and IPC\nmechanisms. \nPICKING AN INTERACTION STYLE AND IPC MECHANISM\nAn important design decision you must make when designing the integration glue is\nselecting the interaction styles and IPC mechanisms that enable the service and the\nmonolith to collaborate. As described in chapter 3, there are several interaction\nstyles and IPC mechanisms to choose from. Which one you should use depends on\nwhat one party—the service or monolith—needs in order to query or update the\nother party.\n If one party needs to query data owned by the other party, there are several\noptions. One option is, as figure 13.9 shows, for the adapter that implements the\nrepository interface to invoke an API of the data provider. This API will typically use a\nrequest/response interaction style, such as REST or gRPC. For example, Delayed\nDelivery Service might retrieve the customer contact info by invoking a REST API\nimplemented by the FTGO monolith.\nIn this example, the Delayed Delivery Service’s domain logic retrieves the customer\ncontact info by invoking the CustomerContactInfoRepository interface. The imple-\nmentation of this interface invokes the monolith’s REST API.\n An important benefit of querying data by invoking a query API is its simplicity. The\nmain drawback is that it’s potentially inefficient. A consumer might need to make a\nlarge number of requests. A provider might return a large amount of data. Another\ndrawback is that it reduces availability because it’s synchronous IPC. As a result, it\nmight not be practical to use a query API.\nDelayed\nDelivery Service\nCustomer\nContactInfo\nRepository\nGET/customers/{customerId}\nFTGO\nmonolith\nMonolith\ndatabase\nREST\nAPI\nREST\nclient\nFigure 13.9\nThe adapter that implements the CustomerContactInfoRepository interface invokes the \nmonolith’s REST API to retrieve the customer information.\n \n",
      "content_length": 2070,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 476,
      "content": "446\nCHAPTER 13\nRefactoring to microservices\n An alternative approach is for the data consumer to maintain a replica of the data,\nas shown in figure 13.10. The replica is essentially a CQRS view. The data consumer\nkeeps the replica up-to-date by subscribing to domain events published by the data\nprovider.\nUsing a replica has several benefits. It avoids the overhead of repeatedly querying the\ndata provider. Instead, as discussed when describing CQRS in chapter 7, you can\ndesign the replica to support efficient queries. One drawback of using a replica,\nthough, is the complexity of maintaining it. A potential challenge, as described later\nin this section, is the need to modify the monolith to publish domain events.\n Now that we’ve discussed how to do queries, let’s consider how to do updates. One\nchallenge with performing updates is the need to maintain data consistency across the\nservice and monolith. The party making the update request (the requestor) has\nupdated or needs to update its database. So it’s essential that both updates happen.\nThe solution is for the service and monolith to communicate using transactional mes-\nsaging implemented by a framework, such as Eventuate Tram. In simple scenarios, the\nrequestor can send a notification message or publish an event to trigger an update. In\nmore complex scenarios, the requestor must use a saga to maintain data consistency.\nSection 13.3.2 discusses the implications of using sagas. \nIMPLEMENTING AN ANTI-CORRUPTION LAYER\nImagine you’re implementing a new feature as a brand new service. You’re not con-\nstrained by the monolith’s code base, so you can use modern development techniques\nDelayed\nDelivery Service\nFTGO\nmonolith\nMonolith\ndatabase\nService\ndatabase\nEvent\npublisher\nCustomer event channel\nCustomer\ndomain\nevent\nEvent\nsubscriber\nDatabase\nadapter\nCustomer\nContactInfo\nRepository\nquery()\nupdate()\nFigure 13.10\nThe integration glue replicates data from the monolith to the service. The monolith publishes \ndomain events, and an event handler implemented by the service updates the service’s database.\n \n",
      "content_length": 2078,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 477,
      "content": "447\nDesigning how the service and the monolith collaborate\nsuch as DDD and develop a pristine new domain model. Also, because the FTGO\nmonolith’s domain is poorly defined and somewhat out-of-date, you’ll probably model\nconcepts differently. As a result, your service’s domain model will have different class\nnames, field names, and field values. For example, Delayed Delivery Service has a\nDelivery entity with narrowly focused responsibilities, whereas the FTGO monolith\nhas an Order entity with an excessive number of responsibilities. Because the two\ndomain models are different, you must implement what DDD calls an anti-corruption\nlayer (ACL) in order for the service to communicate with the monolith.\nThe goal of an ACL is to prevent a legacy monolith’s domain model from polluting a\nservice’s domain model. It’s a layer of code that translates between the different\ndomain models. For example, as figure 13.11 shows, Delayed Delivery Service has a\nCustomerContactInfoRepository interface, which defines a findCustomerContact-\nInfo() method that returns CustomerContactInfo. The class that implements the\nCustomerContactInfoRepository interface must translate between the ubiquitous\nlanguage of Delayed Delivery Service and that of the FTGO monolith.\nThe implementation of findCustomerContactInfo() invokes the FTGO monolith to\nretrieve the customer information and translates the response to CustomerContact-\nInfo. In this example, the translation is quite simple, but in other scenarios it could\nbe quite complex and involve, for example, mapping values such as status codes.\nPattern: Anti-corruption layer\nA software layer that translates between two different domain models in order to\nprevent concepts from one model polluting another. See https://microservices.io/\npatterns/refactoring/anti-corruption-layer.html.\nDelayed\nDelivery Service\nFTGO\nmonolith\nREST\nAPI\nAPI\nTranslation layer\nGET/user/{userId}\nMonolith layer\nREST client\nCustomer\nContactInfo\nRepository\nUbiquitous language of service\nUbiquitous language of monolith\nAnti-corruption layer\nFigure 13.11\nA service adapter that invokes the monolith must translate between the service’s domain model \nand the monolith’s domain model.\n \n",
      "content_length": 2201,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 478,
      "content": "448\nCHAPTER 13\nRefactoring to microservices\n An event subscriber, which consumes domain events, also has an ACL. Domain\nevents are part of the publisher’s domain model. An event handler must translate\ndomain events to the subscriber’s domain model. For example, as figure 13.12 shows,\nthe FTGO monolith publishes Order domain events. Delivery Service has an event\nhandler that subscribes to those events.\nThe event handler must translate domain events from the monolith’s domain lan-\nguage to that of Delivery Service. It might need to map class and attribute names\nand potentially attribute values.\n It’s not just services that use an anti-corruption layer. A monolith also uses an ACL\nwhen invoking the service and when subscribing to domain events published by a ser-\nvice. For example, the FTGO monolith schedules a delivery by sending a notification\nmessage to Delivery Service. It sends the notification by invoking a method on the\nDeliveryService interface. The implementation class translates its parameters into a\nmessage that Delivery Service understands. \nHOW THE MONOLITH PUBLISHES AND SUBSCRIBES TO DOMAIN EVENTS\nDomain events are an important collaboration mechanism. It’s straightforward for a\nnewly developed service to publish and consume events. It can use one of the mech-\nanisms described in chapter 3, such as the Eventuate Tram framework. A service\nmight even publish events using event sourcing, described in chapter 6. It’s poten-\ntially challenging, though, to change the monolith to publish and consume events.\nLet’s look at why.\n There are a couple of different ways that a monolith can publish domain events.\nOne approach is to use the same domain event publishing mechanism used by the\nDelayed\nDelivery\nService\nFTGO\nmonolith\nEvent handler\nTranslation layer\nMessaging client\nUbiquitous language of service\nUbiquitous language of monolith\nAnti-corruption layer\nEvent channel\nOrder\nevent\nEvent\npublisher\nFigure 13.12\nAn event handler must translate from the event publisher’s domain model to the subscriber’s domain \nmodel.\n \n",
      "content_length": 2052,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 479,
      "content": "449\nDesigning how the service and the monolith collaborate\nservices. You find all the places in the code that change a particular entity and insert a\ncall to an event publishing API. The problem with this approach is that changing a\nmonolith isn’t always easy. It might be time consuming and error prone to locate all\nthe places and insert calls to publish events. To make matters worse, some of the\nmonolith’s business logic might consist of stored procedures that can’t easily publish\ndomain events.\n Another approach is to publish domain events at the database level. You can, for\nexample, use either transaction logic tailing or polling, described in chapter 3. A key\nbenefit of using transaction tailing is that you don’t have to change the monolith. The\nmain drawback of publishing events at the database level is that it’s often difficult to\nidentify the reason for the update and publish the appropriate high-level business\nevent. As a result, the service will typically publish events representing changes to\ntables rather than business entities.\n Fortunately, it’s usually easier for the monolith to subscribe to domain events pub-\nlished as services. Quite often, you can write event handlers using a framework, such\nas Eventuate Tram. But sometimes it’s even challenging for the monolith to subscribe\nto events. For example, the monolith might be written in a language that doesn’t have\na message broker client. In that situation, you need to write a small “helper” applica-\ntion that subscribes to events and updates the monolith’s database directly.\n Now that we’ve looked at how to design the integration glue that enables a ser-\nvice and the monolith to collaborate, let’s look at another challenge you might face\nwhen migrating to microservices: maintaining data consistency across a service and\na monolith. \n13.3.2 Maintaining data consistency across a service and a monolith\nWhen you develop a service, you might find it challenging to maintain data consis-\ntency across the service and the monolith. A service operation might need to update\ndata in the monolith, or a monolith operation might need to update data in the ser-\nvice. For example, imagine you extracted Kitchen Service from the monolith. You\nwould need to change the monolith’s order-management operations, such as create-\nOrder() and cancelOrder(), to use sagas in order to keep the Ticket consistent with\nthe Order.\n The problem with using sagas, however, is that the monolith might not be a will-\ning participant. As described in chapter 4, sagas must use compensating transactions\nto undo changes. Create Order Saga, for example, includes a compensating transac-\ntion that marks an Order as rejected if it’s rejected by Kitchen Service. The prob-\nlem with compensating transactions in the monolith is that you might need to make\nnumerous and time-consuming changes to the monolith in order to support them.\nThe monolith might also need to implement countermeasures to handle the lack of\nisolation between sagas. The cost of these code changes can be a huge obstacle to\nextracting a service.\n \n",
      "content_length": 3080,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 480,
      "content": "450\nCHAPTER 13\nRefactoring to microservices\nFortunately, many sagas are straightforward to implement. As covered in chapter 4, if\nthe monolith’s transactions are either pivot transactions or retriable transactions, then\nimplementing sagas should be straightforward. You may even be able to simplify\nimplementation by carefully ordering the sequence of service extractions so that the\nmonolith’s transactions never need to be compensatable. Or it may be relatively diffi-\ncult to change the monolith to support compensating transactions. To understand\nwhy implementing compensating transactions in the monolith is sometimes challeng-\ning, let’s look at some examples, beginning with a particularly troublesome one.\nTHE CHALLENGE OF CHANGING THE MONOLITH TO SUPPORT COMPENSATABLE TRANSACTIONS\nLet’s dig into the problem of compensating transactions that you’ll need to solve when\nextracting Kitchen Service from the monolith. This refactoring involves splitting the\nOrder entity and creating a Ticket entity in Kitchen Service. It impacts numerous\ncommands implemented by the monolith, including createOrder().\n The monolith implements the createOrder() command as a single ACID transac-\ntion consisting of the following steps:\n1\nValidate order details.\n2\nVerify that the consumer can place an order.\n3\nAuthorize consumer’s credit card.\n4\nCreate an Order.\nYou need to replace this ACID transaction with a saga consisting of the following steps:\n1\nIn the monolith\n– Create an Order in an APPROVAL_PENDING state.\n– Verify that the consumer can place an order.\nKey saga terminology\nI cover sagas in chapter 4. Here are some key terms:\nSaga—A sequence of local transactions coordinated through asynchronous\nmessaging.\nCompensating transaction—A transaction that undoes the updates made by a\nlocal transaction.\nCountermeasure—A design technique used to handle the lack of isolation\nbetween sagas.\nSemantic lock—A countermeasure that sets a flag in a record that is being\nupdated by a saga.\nCompensatable transaction—A transaction that needs a compensating trans-\naction because one of the transactions that follows it in the saga can fail.\nPivot transaction—A transaction that is the saga’s go/no-go point. If it suc-\nceeds, then the saga will run to completion.\nRetriable transaction—A transaction that follows the pivot transaction and is\nguaranteed to succeed.\n \n",
      "content_length": 2366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 481,
      "content": "451\nDesigning how the service and the monolith collaborate\n2\nIn the Kitchen Service\n– Validate order details.\n– Create a Ticket in the CREATE_PENDING state.\n3\nIn the monolith\n– Authorize consumer’s credit card.\n– Change state of Order to APPROVED.\n4\nIn Kitchen Service\n– Change the state of the Ticket to AWAITING_ACCEPTANCE.\nThis saga is similar to CreateOrderSaga described in chapter 4. It consists of four local\ntransactions, two in the monolith and two in Kitchen Service. The first transaction\ncreates an Order in the APPROVAL_PENDING state. The second transaction creates a\nTicket in the CREATE_PENDING state. The third transaction authorizes the Consumer\ncredit card and changes the state of the order to APPROVED. The fourth and final trans-\naction changes the state of the Ticket to AWAITING_ACCEPTANCE.\n The challenge with implementing this saga is that the first step, which creates the\nOrder, must be compensatable. That’s because the second local transaction, which\noccurs in Kitchen Service, might fail and require the monolith to undo the updates\nperformed by the first local transaction. As a result, the Order entity needs to have an\nAPPROVAL_PENDING, a semantic lock countermeasure, described in chapter 4, that\nindicates an Order is in the process of being created.\n The problem with introducing a new Order entity state is that it potentially requires\nwidespread changes to the monolith. You might need to change every place in the\ncode that touches an Order entity. Making these kinds of widespread changes to the\nmonolith is time consuming and not the best investment of development resources.\nIt’s also potentially risky, because the monolith is often difficult to test. \nSAGAS DON’T ALWAYS REQUIRE THE MONOLITH TO SUPPORT COMPENSATABLE TRANSACTIONS\nSagas are highly domain-specific. Some, such as the one we just looked at, require the\nmonolith to support compensating transactions. But it’s quite possible that when you\nextract a service, you may be able to design sagas that don’t require the monolith to\nimplement compensating transactions. That’s because a monolith only needs to sup-\nport compensating transactions if the transactions that follow the monolith’s transac-\ntion can fail. If each of the monolith’s transactions is either a pivot transaction or a\nretriable transaction, then the monolith never needs to execute a compensating trans-\naction. As a result, you only need to make minimal changes to the monolith to sup-\nport sagas.\n For example, imagine that instead of extracting Kitchen Service, you extract Order\nService. This refactoring involves splitting the Order entity and creating a slimmed-\ndown Order entity in Order Service. It also impacts numerous commands, including\ncreateOrder(), which is moved from the monolith to Order Service. In order to\nextract Order Service, you need to change the createOrder() command to use a\nsaga, using the following steps:\n \n",
      "content_length": 2912,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 482,
      "content": "452\nCHAPTER 13\nRefactoring to microservices\n1\nOrder Service\n– Create an Order in an APPROVAL_PENDING state.\n2\nMonolith\n– Verify that the consumer can place an order.\n– Validate order details and create a Ticket.\n– Authorize consumer’s credit card.\n3\nOrder Service\n– Change state of Order to APPROVED.\nThis saga consists of three local transactions, one in the monolith and two in Order\nService. The first transaction, which is in Order Service, creates an Order in the\nAPPROVAL_PENDING state. The second transaction, which is in the monolith, verifies\nthat the consumer can place orders, authorizes their credit card, and creates a\nTicket. The third transaction, which is in Order Service, changes the state of the\nOrder to APPROVED.\n The monolith’s transaction is the saga’s pivot transaction—the point of no return\nfor the saga. If the monolith’s transaction completes, then the saga will run until com-\npletion. Only the first and second steps of this saga can fail. The third transaction\ncan’t fail, so the second transaction in the monolith never needs to be rolled back. As\na result, all the complexity of supporting compensatable transactions is in Order\nService, which is much more testable than the monolith.\n If all the sagas that you need to write when extracting a service have this struc-\nture, you’ll need to make far fewer changes to the monolith. What’s more, it’s possi-\nble to carefully sequence the extraction of services to ensure that the monolith’s\ntransactions are either pivot transactions or retriable transactions. Let’s look at how\nto do that. \nSEQUENCING THE EXTRACTION OF SERVICES TO AVOID IMPLEMENTING COMPENSATING TRANSACTIONS \nIN THE MONOLITH\nAs we just saw, extracting Kitchen Service requires the monolith to implement com-\npensating transactions, whereas extracting Order Service doesn’t. This suggests that\nthe order in which you extract services matters. By carefully ordering the extraction of\nservices, you can potentially avoid having to make widespread modifications to the\nmonolith to support compensatable transactions. We can ensure that the monolith’s\ntransactions are either pivot transactions or retriable transactions. For example, if we\nfirst extract Order Service from the FTGO monolith and then extract Consumer\nService, extracting Kitchen Service will be straightforward. Let’s take a closer look\nat how to do that.\n Once we have extracted Consumer Service, the createOrder() command uses the\nfollowing saga:\n1\nOrder Service: create an Order in an APPROVAL_PENDING state.\n2\nConsumer Service: verify that the consumer can place an order.\n \n",
      "content_length": 2591,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 483,
      "content": "453\nDesigning how the service and the monolith collaborate\n3\nMonolith\n– Validate order details and create a Ticket.\n– Authorize consumer’s credit card.\n4\nOrder Service: change state of Order to APPROVED.\nIn this saga, the monolith’s transaction is the pivot transaction. Order Service imple-\nments the compensatable transaction.\n Now that we’ve extracted Consumer Service, we can extract Kitchen Service. If we\nextract this service, the createOrder() command uses the following saga:\n1\nOrder Service: create an Order in an APPROVAL_PENDING state.\n2\nConsumer Service: verify that the consumer can place an order.\n3\nKitchen Service: validate order details and create a PENDING Ticket.\n4\nMonolith: authorize consumer’s credit card.\n5\nKitchen Service: change state of Ticket to APPROVED.\n6\nOrder Service: change state of Order to APPROVED.\nIn this saga, the monolith’s transaction is still the pivot transaction. Order Service and\nKitchen Service implement the compensatable transactions.\n We can even continue to refactor the monolith by extracting Accounting Service. If\nwe extract this service, the createOrder() command uses the following saga:\n1\nOrder Service: create an Order in an APPROVAL_PENDING state.\n2\nConsumer Service: verify that the consumer can place an order.\n3\nKitchen Service: validate order details and create a PENDING Ticket.\n4\nAccounting Service: authorize consumer’s credit card.\n5\nKitchen Service: change state of Ticket to APPROVED.\n6\nOrder Service: change state of Order to APPROVED.\nAs you can see, by carefully sequencing the extractions, you can avoid using sagas that\nrequire making complex changes to the monolith. Let’s now look at how to handle\nsecurity when migrating to a microservice architecture. \n13.3.3 Handling authentication and authorization\nAnother design issue you need to tackle when refactoring a monolithic application to\na microservice architecture is adapting the monolith’s security mechanism to support\nthe services. Chapter 11 describes how to handle security in a microservice architec-\nture. A microservices-based application uses tokens, such as JSON Web tokens (JWT),\nto pass around user identity. That’s quite different than a typical traditional, mono-\nlithic application that uses in-memory session state and passes around the user iden-\ntity using a thread local. The challenge when transforming a monolithic application\nto a microservice architecture is that you need to support both the monolithic and\nJWT-based security mechanisms simultaneously.\n Fortunately, there’s a straightforward way to solve this problem that only requires\nyou to make one small change to the monolith’s login request handler. Figure 13.13\n \n",
      "content_length": 2677,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 484,
      "content": "454\nCHAPTER 13\nRefactoring to microservices\nshows how this works. The login handler returns an additional cookie, which in this\nexample I call USERINFO, that contains user information, such as the user ID and roles.\nThe browser includes that cookie in every request. The API gateway extracts the infor-\nmation from the cookie and includes it in the HTTP requests that it makes to a ser-\nvice. As a result, each service has access to the needed user information.\nThe sequence of events is as follows:\n1\nThe client makes a login request containing the user’s credentials.\n2\nAPI Gateway routes the login request to the FTGO monolith.\n3\nThe monolith returns a response containing the JSESSIONID session cookie\nand the USERINFO cookie, which contains the user information, such as ID\nand roles.\n4\nThe client makes a request, which includes the USERINFO cookie, in order to\ninvoke an operation.\n5\nAPI Gateway validates the USERINFO cookie and includes it in the Authoriza-\ntion header of the request that it makes to the service. The service validates the\nUSERINFO token and extracts the user information.\nLet’s look at LoginHandler and API Gateway in more detail.\nTHE MONOLITH’S LOGINHANDLER SETS THE USERINFO COOKIE\nLoginHandler processes the POST of the user’s credentials. It authenticates the user\nand stores information about the user in the session. It’s often implemented by a\nFTGO Monolith\nOrder History Service\nPOST/login\nGET/orders\nAuthorization: TOKEN\n...\nHTTP/1.1 200 OK\nSet-cookie: JSESSIONID=...\nSet-cookie: USERINFO=TOKEN\n...\nGET/orders\nCookie: JSESSIONID=...\nCookie: USERINFO=TOKEN\n...\nBrowser-based\nSPA application\nLog in with user\nID and password.\nUser\ndatabase\nAPI\ngateway\nuserId: xxx\nroles:[a, b, c]\n...\nOrderHistory\nRequestHandler\nLogin\nhandler\nInitializes\nQuery\nPOST/login\nLog in with user\nID and password.\nReturn session cookie.\nProvide JWT.\nProvide session cookie.\nContains user information,\nsuch as ID and roles\nIn-memory\nsession\nFigure 13.13\nThe login handler is enhanced to set a USERINFO cookie, which is a JWT containing user \ninformation. API Gateway transfers the USERINFO cookie to an authorization header when it invokes a \nservice.\n \n",
      "content_length": 2163,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 485,
      "content": "455\nImplementing a new feature as a service: handling misdelivered orders\nsecurity framework, such as Spring Security or Passport for NodeJS. If the applica-\ntion is configured to use the default in-memory session, the HTTP response sets a ses-\nsion cookie, such as JSESSIONID. In order to support the migration to microservices,\nLoginHandler must also set the USERINFO cookie containing the JWT that describes\nthe user. \nTHE API GATEWAY MAPS THE USERINFO COOKIE TO THE AUTHORIZATION HEADER\nThe API gateway, as described in chapter 8, is responsible for request routing and API\ncomposition. It handles each request by making one or more requests to the monolith\nand the services. When the API gateway invokes a service, it validates the USERINFO\ncookie and passes it to the service in the HTTP request’s Authorization header. By\nmapping the cookie to the Authorization header, the API gateway ensures that it\npasses the user identity to the service in a standard way that’s independent of the type\nof client.\n Eventually, we’ll most likely extract login and user management into services. But\nas you can see, by only making one small change to the monolith’s login handler, it’s\nnow possible for services to access user information. This enables you focus on devel-\noping services that provide the greatest value to the business and delay extracting less\nvaluable services, such as user management.\n Now that we’ve looked at how to handle security when refactoring to microser-\nvices, let’s see an example of implementing a new feature as a service. \n13.4\nImplementing a new feature as a service: handling \nmisdelivered orders\nLet’s say you’ve been tasked with improving how FTGO handles misdelivered orders.\nA growing number of customers have been complaining about how customer ser-\nvice handles orders not being delivered. The majority of orders are delivered on\ntime, but from time to time orders are either delivered late or not at all. For exam-\nple, the courier gets delayed by unexpectedly bad traffic, so the order is picked up\nand delivered late. Or perhaps by the time the courier arrives at the restaurant, it’s\nclosed, and the delivery can’t be made. To make matters worse, the first time cus-\ntomer service hears about the misdelivery is when they receive an angry email from\nan unhappy customer.\nA true story: My missing ice cream\nOne Saturday night I was feeling lazy and placed an order using a well-known food\ndelivery app to have ice cream delivered from Smitten. It never showed up. The only\ncommunication from the company was an email the next morning saying my order had\nbeen canceled. I also got a voicemail from a very confused customer service agent\nwho clearly didn’t know what she was calling about. Perhaps the call was prompted\nby one of my tweets describing what happened. Clearly, the delivery company had not\nestablished any mechanisms for properly handling inevitable mistakes.\n \n",
      "content_length": 2913,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 486,
      "content": "456\nCHAPTER 13\nRefactoring to microservices\nThe root cause for many of these delivery problems is the primitive delivery schedul-\ning algorithm used by the FTGO application. A more sophisticated scheduler is under\ndevelopment but won’t be finished for a few months. The interim solution is for\nFTGO to proactively handle delayed or canceled orders by apologizing to the cus-\ntomer, and in some cases offering compensation before the customer complains.\n Your job is to implement a new feature that will do the following:\n1\nNotify the customer when their order won’t be delivered on time.\n2\nNotify the customer when their order can’t be delivered because it can’t be\npicked up before the restaurant closes.\n3\nNotify customer service when an order can’t be delivered on time so that they\ncan proactively rectify the situation by compensating the customer.\n4\nTrack delivery statistics.\nThis new feature is fairly simple. The new code must track the state of each Order, and\nif an Order can’t be delivered as promised, the code must notify the customer and cus-\ntomer support, by, for example, sending an email.\n But how—or perhaps more precisely, where—should you implement this new fea-\nture? One approach is to implement a new module in the monolith. The problem\nthere is that developing and testing this code will be difficult. What’s more, this\napproach increases the size of the monolith and thereby makes monolith hell even\nworse. Remember the Law of Holes from earlier: when you’re in a hole, it’s best to stop\ndigging. Rather than make the monolith larger, a much better approach is to imple-\nment these new features as a service.\n13.4.1 The design of Delayed Delivery Service\nWe’ll implement this feature as a service called Delayed Order Service. Figure 13.14\nshows the FTGO application’s architecture after implementing this service. The appli-\ncation consists of the FTGO monolith, the new Delayed Delivery Service, and an\nAPI Gateway. Delayed Delivery Service has an API that defines a single query opera-\ntion called getDelayedOrders(), which returns the currently delayed or undeliver-\nable orders. API Gateway routes the getDelayedOrders() request to the service and all\nother requests to the monolith. The integration glue provides Delayed Order Service\nwith access to the monolith’s data.\n The Delayed Order Service’s domain model consists of various entities, including\nDelayedOrderNotification, Order, and Restaurant. The core logic is implemented\nby the DelayedOrderService class. It’s periodically invoked by a timer to find orders\nthat won’t be delivered on time. It does that by querying Orders and Restaurants. If\nan Order can’t be delivered on time, DelayedOrderService notifies the consumer and\ncustomer service.\n Delayed Order Service doesn’t own the Order and Restaurant entities. Instead,\nthis data is replicated from the FTGO monolith. What’s more, the service doesn’t\nstore the customer contact information, but instead retrieves it from the monolith.\n \n",
      "content_length": 2983,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 487,
      "content": "457\nImplementing a new feature as a service: handling misdelivered orders\nLet’s look at the design of the integration glue that provides Delayed Order Service\naccess to the monolith’s data. \n13.4.2 Designing the integration glue for Delayed Delivery Service\nEven though a service that implements a new feature defines its own entity classes, it\nusually accesses data that’s owned by the monolith. Delayed Delivery Service is no\nexception. It has a DelayedOrderNotification entity, which represents a notification\nthat it has sent to the consumer. But as I just mentioned, its Order and Restaurant enti-\nties replicate data from the FTGO monolith. It also needs to query user contact infor-\nmation in order to notify the user. Consequently, we need to implement integration\nglue that enables Delivery Service to access the monolith’s data.\n Figure 13.15 shows the design of the integration glue. The FTGO monolith pub-\nlishes Order and Restaurant domain events. Delivery Service consumes these events\nand updates its replicas of those entities. The FTGO monolith implements a REST\nMonolith\n???\nAPI gateway\nREST\nAPI\nIntegration\nglue\nDelayed\nOrder\nService\nGetDelayedOrders()\nREST\nAPI\nNotiﬁcation\nService\nCRM system\nCreate case.\nSend apology\nnotiﬁcation.\nNeed to design.\n???\n«Service»\nDelayedDelivery\nService\n«stereotype»\nOrder\n«entity»\nNotiﬁcation\n«entity»\nRestaurant\n«repository»\nCustomer\nContactInfo\nRepository\n«entity»\nOpeningHours\nFigure 13.14\nThe design of Delayed Delivery Service. The integration glue provides Delayed Delivery \nService access to data owned by the monolith, such as the Order and Restaurant entities, and the customer \ncontact information.\n \n",
      "content_length": 1663,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 488,
      "content": "458\nCHAPTER 13\nRefactoring to microservices\nendpoint for querying the customer contact information. Delivery Service calls this\nendpoint when it needs to notify a user that their order cannot be delivered on time.\nLet’s look at the design of each part of the integration, starting with the REST API for\nretrieving customer contact information.\nQUERYING CUSTOMER CONTACT INFORMATION USING CUSTOMERCONTACTINFOREPOSITORY\nAs described in section 13.3.1, there are a couple of different ways that a service such\nas Delayed Delivery Service could read the monolith’s data. The simplest option is\nfor Delayed Order Service to retrieve data using the monolith’s query API. This\napproach makes sense when retrieving the User contact information. There aren’t\nany latency or performance, issues because Delayed Delivery Service rarely needs to\nretrieve a user’s contact information, and the amount of data is quite small.\n CustomerContactInfoRepository is an interface that enables Delayed Delivery\nService to retrieve a consumer’s contact info. It’s implemented by a Customer-\nContactInfoProxy, which retrieves the user information by invoking the monolith’s\ngetCustomerContactInfo() REST endpoint. \nPUBLISHING AND CONSUMING ORDER AND RESTAURANT DOMAIN EVENTS\nUnfortunately, it isn’t practical for Delayed Delivery Service to query the mono-\nlith for the state of all open Orders and Restaurant hours. That’s because it’s ineffi-\ncient to repeatedly transfer a large amount of data over the network. Consequently,\nDelayed Delivery Service must use the second, more complex option and main-\ntain a replica of Orders and Restaurants by subscribing to events published by the\nmonolith. It’s important to remember that the replica isn’t a complete copy of the\ndata from the monolith—it just stores a small subset of the attributes of Order and\nRestaurant entities.\nMonolith\nEvent\nsubscriber\nDelayed Order Service\nDomain\nevent\npublisher\nREST\nendpoint\nCustomer\nContactInfo\nProxy\n<Repository>\nCustomer\nContactInfo\nRepository\nRestaurant events\ngetCustomerContactInfo()\nOrder events\nRestaurant\nevents\nOrder\nevents\nFigure 13.15\nThe integration glue provides Delayed Delivery Service with access to the data owned by \nthe monolith.\n \n",
      "content_length": 2214,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 489,
      "content": "459\nBreaking apart the monolith: extracting delivery management\n As described earlier in section 13.3.1, there are a couple of different ways that we\ncan change the FTGO monolith so that it publishes Order and Restaurant domain\nevents. One option is to modify all the places in the monolith that update Orders and\nRestaurants to publish high-level domain events. The second option is to tail the\ntransaction log to replicate the changes as events. In this particular scenario, we need\nto synchronize the two databases. We don’t require the FTGO monolith to publish\nhigh-level domain events, so either approach is fine.\n Delayed Order Service implements event handlers that subscribe to events from\nthe monolith and update its Order and Restaurant entities. The details of the event\nhandlers depend on whether the monolith publishes specific high-level events or low-\nlevel change events. In either case, you can think of an event handler as translating an\nevent in the monolith’s bounded context to the update of an entity in the service’s\nbounded context.\n An important benefit of using a replica is that it enables Delayed Order Service\nto efficiently query the orders and the restaurant opening hours. One drawback,\nhowever, is that it’s more complex. Another drawback is that it requires the mono-\nlith to publish the necessary Order and Restaurant events. Fortunately, because\nDelayed Delivery Service only needs what’s essentially a subset of the columns of\nthe ORDERS and RESTAURANT tables, we shouldn’t encounter the problems described\nin section 13.3.1.\n Implementing a new feature such as delayed order management as a standalone\nservice accelerates its development, testing, and deployment. What’s more, it enables\nyou to implement the feature using a brand new technology stack instead of the\nmonolith’s older one. It also stops the monolith from growing. Delayed order man-\nagement is just one of many new features planned for the FTGO application. The\nFTGO team can implement many of these features as separate services.\n Unfortunately, you can’t implement all changes as new services. Quite often you\nmust make extensive changes to the monolith to implement new features or change\nexisting features. Any development involving the monolith will mostly likely be slow\nand painful. If you want to accelerate the delivery of these features, you must break up\nthe monolith by migrating functionality from the monolith into services. Let’s look at\nhow to do that. \n13.5\nBreaking apart the monolith: extracting delivery \nmanagement\nTo accelerate the delivery of features that are implemented by a monolith, you need\nto break up the monolith into services. For example, let’s imagine that you want to\nenhance FTGO delivery management by implementing a new routing algorithm. A\nmajor obstacle to developing delivery management is that it’s entangled with order\nmanagement and is part of the monolithic code base. Developing, testing, and deploy-\ning delivery management is likely to be slow. In order to accelerate its development,\nyou need to extract delivery management into a Delivery Service.\n \n",
      "content_length": 3105,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 490,
      "content": "460\nCHAPTER 13\nRefactoring to microservices\n I start this section by describing delivery management and how it’s currently\nembedded within the monolith. Next I discuss the design of the new, standalone\nDelivery Service and its API. I then describe how Delivery Service and the FTGO\nmonolith collaborate. Finally I talk about some of the changes we need to make to the\nmonolith to support Delivery Service.\n Let’s begin by reviewing the existing design.\n13.5.1 Overview of existing delivery management functionality\nDelivery management is responsible for scheduling the couriers that pick up orders at\nrestaurants and deliver them to consumers. Each courier has a plan that is a schedule\nof pickup and deliver actions. A pickup action tells the Courier to pick up an order\nfrom a restaurant at a particular time. A deliver action tells the Courier to deliver an\norder to a consumer. The plans are revised whenever orders are placed, canceled, or\nrevised, and as the location and availability of couriers changes.\n Delivery management is one of the oldest parts of the FTGO application. As fig-\nure 13.16 shows, it’s embedded within order management. Much of the code for man-\naging deliveries is in OrderService. What’s more, there’s no explicit representation of\na Delivery. It’s embedded within the Order entity, which has various delivery-related\nfields, such as scheduledPickupTime and scheduledDeliveryTime.\n Numerous commands implemented by the monolith invoke delivery manage-\nment, including the following:\n\nacceptOrder()—Invoked when a restaurant accepts an order and commits to\npreparing it by a certain time. This operation invokes delivery management to\nschedule a delivery.\n\ncancelOrder()—Invoked when a consumer cancels an order. If necessary, it\ncancels the delivery.\n\nnoteCourierLocationUpdated()—Invoked by the courier’s mobile application\nto update the courier’s location. It triggers the rescheduling of deliveries.\n\nnoteCourierAvailabilityChanged()—Invoked by the courier’s mobile applica-\ntion to update the courier’s availability. It triggers the rescheduling of deliveries.\nAlso, various queries retrieve data maintained by delivery management, including the\nfollowing:\n\ngetCourierPlan()—Invoked by the courier’s mobile application and returns\nthe courier’s plan\n\ngetOrderStatus()—Returns the order’s status, which includes delivery-related\ninformation such as the assigned courier and the ETA\n\ngetOrderHistory()—Returns similar information as getOrderStatus() except\nabout multiple orders\nQuite often what’s extracted into a service is, as mentioned in section 13.2.3, an entire\nvertical slice, with controllers at the top and database tables at the bottom. We could\n \n",
      "content_length": 2699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 491,
      "content": "461\nBreaking apart the monolith: extracting delivery management\nconsider the Courier-related commands and queries to be part of delivery manage-\nment. After all, delivery management creates the courier plans and is the primary con-\nsumer of the Courier location and availability information. But in order to minimize\nthe development effort, we’ll leave those operations in the monolith and just extract\nthe core of the algorithm. Consequently, the first iteration of Delivery Service won’t\nexpose a publicly accessible API. Instead, it will only be invoked by the monolith.\nNext, let’s explore the design of Delivery Service. \nAPI\nFTGO monolith\n«Service»\nOrderService\n«Service»\nCourierService\n...\n«delivery management»\nscheduleDelivery()\nrescheduleDelivery()\ncancelDelivery()\nreviseSchedule()\n...\nacceptOrder()\ncancelOrder()\ngetOrderStatus()\ngetOrderHistory()\nupdateCourierLocation()\nupdateCourierAvailability()\ngetCourierPlan()\nOrder operations:\nCourier operations:\n«entity»\nCourier\n«value object»\nPlan\n«entity»\nOrder\n«entity»\nRestaurant\n«value object»\nAction\n«value object»\nDropoff\n«value object»\nPickup\nFigure 13.16\nDelivery management is entangled with order management within the FTGO monolith.\n \n",
      "content_length": 1202,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 492,
      "content": "462\nCHAPTER 13\nRefactoring to microservices\n13.5.2 Overview of Delivery Service\nThe proposed new Delivery Service is responsible for scheduling, rescheduling, and\ncanceling deliveries. Figure 13.17 shows a high-level view of the architecture of the\nFTGO application after extracting Delivery Service. The architecture consists of\nthe FTGO monolith and Delivery Service. They collaborate using the integration\nglue, which consists of APIs in both the service and monolith. Delivery Service has\nits own domain model and database.\nIn order to flesh out this architecture and determine the service’s domain model, we\nneed to answer the following questions:\nWhich behavior and data are moved to Delivery Service?\nWhat API does Delivery Service expose to the monolith?\nWhat API does the monolith expose to Delivery Service?\nThese issues are interrelated because the distribution of responsibilities between the\nmonolith and the service affects the APIs. For instance, Delivery Service will need to\ninvoke an API provided by the monolith to access the data in the monolith’s data-\nbase and vice versa. Later, I’ll describe the design of the integration glue that enables\nMonolith\ndomain model\nIntegration glue\nWhat API does the Delivery Service\nexpose to the monolith?\nDelivery Service\ndomain model\nFTGO Monolith\nDelivery Service\nDelivery\nService\ndatabase\nMonolith\ndatabase\nAdapter\nAdapter\nWhat API does the monolith\nexpose to the Delivery Service?\nWhich behavior and\ndata is moved to the\nDelivery Service?\nFigure 13.17\nThe high-level view of the FTGO application after extracting Delivery Service. The FTGO \nmonolith and Delivery Service collaborate using the integration glue, which consists of APIs in each of them. \nThe two key decisions that need to be made are which functionality and data are moved to Delivery Service \nand how do the monolith and Delivery Service collaborate via APIs?\n \n",
      "content_length": 1892,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 493,
      "content": "463\nBreaking apart the monolith: extracting delivery management\nDelivery Service and the FTGO monolith to collaborate. But first, let’s look at the\ndesign of Delivery Service’s domain model. \n13.5.3 Designing the Delivery Service domain model\nTo be able to extract delivery management, we first need to identify the classes that\nimplement it. Once we’ve done that, we can decide which classes to move to Delivery\nService to form its domain logic. In some cases, we’ll need to split classes. We’ll\nalso need to decide which data to replicate between the service and the monolith.\n Let’s start by identifying the classes that implement delivery management.\nIDENTIFYING WHICH ENTITIES AND THEIR FIELDS ARE PART OF DELIVERY MANAGEMENT\nThe first step in the process of designing Delivery Service is to carefully review the\ndelivery management code and identify the participating entities and their fields. Fig-\nure 13.18 shows the entities and fields that are part of delivery management. Some\nfields are inputs to the delivery-scheduling algorithm, and others are the outputs. The\nfigure shows which of those fields are also used by other functionality implemented by\nthe monolith.\nThe delivery scheduling algorithm reads various attributes including the Order’s\nrestaurant, promisedDeliveryTime, and deliveryAddress, and the Courier’s location,\navailability, and current plans. It updates the Courier’s plans, the Order’s scheduled-\nPickupTime, and scheduledDeliveryTime. As you can see, the fields used by delivery\nmanagement are also used by the monolith. \nOrder\n«Monolith Read/Write»\n«Service Read»\nstate\ndeliveryAddress\npromisedDeliveryTime\npreparedByTime\n«Service Read/Write»\n«Monolith Read»\nscheduledPickupTime\nscheduledDeliveryTime\nRestaurant\n«Read»\naddress\nCourier\n«Monolith Read/Write»\n«Service Read»\nLocation\navailability\n«Service Read/Write»\n«Monolith Read»\nPlan\nTask\nFigure 13.18\nThe entities and fields that are accessed by delivery management \nand other functionality implemented by the monolith. A field can be read or written \nor both. It can be accessed by delivery management, the monolith, or both.\n \n",
      "content_length": 2117,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 494,
      "content": "464\nCHAPTER 13\nRefactoring to microservices\nDECIDING WHICH DATA TO MIGRATE TO DELIVERY SERVICE\nNow that we’ve identified which entities and fields participate in delivery manage-\nment, the next step is to decide which of them we should move to the service. In an\nideal scenario, the data accessed by the service is used exclusively by the service, so we\ncould simply move that data to the service and be done. Sadly, it’s rarely that simple,\nand this situation is no exception. All the entities and fields used by the delivery man-\nagement are also used by other functionality implemented by the monolith.\n As a result, when determining which data to move to the service, we need to keep\nin mind two issues. The first is: how does the service access the data that remains in\nthe monolith? The second is: how does the monolith access data that’s moved to the\nservice? Also, as described earlier in section 13.3, we need to carefully consider how to\nmaintain data consistency between the service and the monolith.\n The essential responsibility of Delivery Service is managing courier plans and\nupdating the Order’s scheduledPickupTime and scheduledDeliveryTime fields. It\nmakes sense, therefore, for it to own those fields. We could also move the Cou-\nrier.location and Courier.availability fields to Delivery Service. But because\nwe’re trying to make the smallest possible change, we’ll leave those fields in the mono-\nlith for now. \nTHE DESIGN OF THE DELIVERY SERVICE DOMAIN LOGIC\nFigure 13.19 shows the design of the Delivery Service’s domain model. The core of\nthe service consists of domain classes such as Delivery and Courier. The Delivery-\nServiceImpl class is the entry point into the delivery management business logic. It\nimplements the DeliveryService and CourierService interfaces, which are invoked\nby DeliveryServiceEventsHandler and DeliveryServiceNotificationsHandlers,\ndescribed later in this section.\n The delivery management business logic is mostly code copied from the monolith.\nFor example, we’ll copy the Order entity from the monolith to Delivery Service,\nrename it to Delivery, and delete all fields except those used by delivery manage-\nment. We’ll also copy the Courier entity and delete most of its fields. In order to\ndevelop the domain logic for Delivery Service, we will need to untangle the code\nfrom the monolith. We’ll need to break numerous dependencies, which is likely to be\ntime consuming. Once again, it’s a lot easier to refactor code when using a statically\ntyped language, because the compiler will be your friend.\n Delivery Service is not a standalone service. Let’s look at the design of the inte-\ngration glue that enables Delivery Service and the FTGO monolith to collaborate. \n \n \n \n \n \n",
      "content_length": 2733,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 495,
      "content": "465\nBreaking apart the monolith: extracting delivery management\n13.5.4 The design of the Delivery Service integration glue\nThe FTGO monolith needs to invoke Delivery Service to manage deliveries. The\nmonolith also needs to exchange data with Delivery Service. This collaboration is\nenabled by the integration glue. Figure 13.20 shows the design of the Delivery Ser-\nvice integration glue. Delivery Service has a delivery management API. It also pub-\nlishes Delivery and Courier domain events. The FTGO monolith publishes Courier\ndomain events.\n Let’s look at the design of each part of the integration glue, starting with Delivery\nService’s API for managing deliveries.\nTHE DESIGN OF THE DELIVERY SERVICE API\nDelivery Service must provide an API that enables the monolith to schedule, revise,\nand cancel deliveries. As you’ve seen throughout this book, the preferred approach is\nto use asynchronous messaging, because it promotes loose coupling and increases\navailability. One approach is for Delivery Service to subscribe to Order domain\nevents published by the monolith. Depending on the type of the event, it creates,\nDelivery Service\nDeliveryServiceImpl\n«interface»\nDeliveryService\nvoid schedule(...)\nvoid reschedule(...)\nvoid cancel(...)\n«interface»\nCourierService\nnoteCourierLocationUpdated(...)\nnoteCourierAvailabilityUpdated(...)\n«entity»\nCourier\n«entity»\nDelivery\n«value object»\nPlan\nDeliveryService\nEventsHandlers\nDeliveryService\nNotiﬁcationHandlers\nFigure 13.19\nThe design of the Delivery Service's domain model\n \n",
      "content_length": 1525,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 496,
      "content": "466\nCHAPTER 13\nRefactoring to microservices\nrevises, and cancels a Delivery. A benefit of this approach is that the monolith doesn’t\nneed to explicitly invoke Delivery Service. The drawback of relying on domain events\nis that it requires Delivery Service to know how each Order event impacts the corre-\nsponding Delivery.\n A better approach is for Delivery Service to implement a notification-based API\nthat enables the monolith to explicitly tell Delivery Service to create, revise, and\ncancel deliveries. Delivery Service’s API consists of a message notification channel\nand three message types: ScheduleDelivery, ReviseDelivery, or CancelDelivery. A\nnotification message contains Order information needed by Delivery Service. For\nexample, a ScheduleDelivery notification contains the pickup time and location and\nthe delivery time and location. An important benefit of this approach is that Delivery\nService doesn’t have detailed knowledge of the Order lifecycle. It’s entirely focused\non managing deliveries and has no knowledge of orders.\n This API isn’t the only way that Delivery Service and the FTGO monolith collab-\norate. They also need to exchange data. \nHOW THE DELIVERY SERVICE ACCESSES THE FTGO MONOLITH’S DATA\nDelivery Service needs to access the Courier location and availability data, which is\nowned by the monolith. Because that’s potentially a large amount of data, it’s not practi-\ncal for the service to repeatedly query the monolith. Instead, a better approach is for the\nmonolith to replicate the data to Delivery Service by publishing Courier domain\nevents, CourierLocationUpdated and CourierAvailabilityUpdated. Delivery Service\nhas a CourierEventSubscriber that subscribes to the domain events and updates its\nversion of the Courier. It might also trigger the rescheduling of deliveries. \nDelivery\nService\nFTGO\nmonolith\nCourier events\nCourier events\nDelivery events\nDelivery Service\nnotiﬁcations\nDelivery events\nCourier events\nCourier\nevent\nsubscriber\nDelivery\nevent\nsubscriber\nDelivery\nService\nproxy\nMessaging\nadapter\nMessaging\nadapter\nDelivery\nService\nnotiﬁcations\nhandlers\n«interface»\nDeliveryService\n«interface»\nDeliveryService\n«interface»\nCourierService\nFigure 13.20\nThe design of the Delivery Service integration glue. Delivery Service has a delivery \nmanagement API. The service and the FTGO monolith synchronize data by exchanging domain events.\n \n",
      "content_length": 2382,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 497,
      "content": "467\nBreaking apart the monolith: extracting delivery management\nHOW THE FTGO MONOLITH ACCESSES THE DELIVERY SERVICE DATA\nThe FTGO monolith needs to read the data that’s been moved to Delivery Service,\nsuch as the Courier plans. In theory, the monolith could query the service, but that\nrequires extensive changes to the monolith. For the time being, it’s easier to leave the\nmonolith’s domain model and database schema unchanged and replicate data from\nthe service back to the monolith.\n The easiest way to accomplish that is for Delivery Service to publish Courier and\nDelivery domain events. The service publishes a CourierPlanUpdated event when it\nupdates a Courier’s plan, and a DeliveryScheduleUpdate event when it updates a\nDelivery. The monolith consumes these domain events and updates its database.\n Now that we’ve looked at how the FTGO monolith and Delivery Service interact,\nlet’s see how to change the monolith. \n13.5.5 Changing the FTGO monolith to interact with Delivery Service\nIn many ways, implementing Delivery Service is the easier part of the extraction\nprocess. Modifying the FTGO monolith is much more difficult. Fortunately, replicat-\ning data from the service back to the monolith reduces the size of the change. But we\nstill need to change the monolith to manage deliveries by invoking Delivery Service.\nLet’s look at how to do that.\nDEFINING A DELIVERYSERVICE INTERFACE\nThe first step is to encapsulate the delivery management code with a Java interface\ncorresponding to the messaging-based API defined earlier. This interface, shown in\nfigure 13.21, defines methods for scheduling, rescheduling, and canceling deliveries.\n«interface»\nDeliveryService\nDeliveryServiceImpl\nvoid schedule(...)\nvoid reschedule(...)\nvoid cancel(...)\nDelivery\nmanagement\nDelivery\nmanagement\nclient\nFigure 13.21\nThe first step is to define DeliveryService, which \nis a coarse-grained, remotable API for invoking the delivery \nmanagement logic.\n \n",
      "content_length": 1949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 498,
      "content": "468\nCHAPTER 13\nRefactoring to microservices\nEventually, we’ll implement this interface with a proxy that sends messages to the\ndelivery service. But initially, we’ll implement this API with a class that calls the deliv-\nery management code.\n The DeliveryService interface is a coarse-grained interface that’s well suited to\nbeing implemented by an IPC mechanism. It defines schedule(), reschedule(), and\ncancel() methods, which correspond to the notification message types defined earlier. \nREFACTORING THE MONOLITH TO CALL THE DELIVERYSERVICE INTERFACE\nNext, as figure 13.22 shows, we need to identify all the places in the FTGO monolith\nthat invoke delivery management and change them to use the DeliveryService inter-\nface. This may take some time and is one of the most challenging aspects of extracting\na service from the monolith.\nIt certainly helps if the monolith is written in a statically typed language, such as Java,\nbecause the tools do a better job of identifying dependencies. If not, then hopefully\nyou have some automated tests with sufficient coverage of the parts of the code that\nneed to be changed. \nIMPLEMENTING THE DELIVERYSERVICE INTERFACE\nThe final step is to replace the DeliveryServiceImpl class with a proxy that sends\nnotification messages to the standalone Delivery Service. But rather than discard\nthe existing implementation right away, we’ll use a design, shown in figure 13.23, that\nenables the monolith to dynamically switch between the existing implementation and\nDelivery Service. We’ll implement the DeliveryService interface with a class that\nuses a dynamic feature toggle to determine whether to invoke the existing implemen-\ntation or Delivery Service.\n«interface»\nDeliveryService\nDeliveryServiceImpl\nvoid schedule(...)\nvoid reschedule(...)\nvoid cancel(...)\nDelivery\nmanagement\nDelivery\nmanagement\nclient\nFigure 13.22\nThe second step is to change the FTGO monolith to \ninvoke delivery management via the DeliveryService interface.\n \n",
      "content_length": 1974,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 499,
      "content": "469\nBreaking apart the monolith: extracting delivery management\nUsing a feature toggle significantly reduces the risk of rolling out Delivery Service. We\ncan deploy Delivery Service and test it. And then, once we’re sure it works, we can flip\nthe toggle to route traffic to it. If we then discover that Delivery Service isn’t working\nas expected, we can switch back to the old implementation.\nOnce we’re sure that Delivery Service is working as expected, we can then remove\nthe delivery management code from the monolith.\n Delivery Service and Delayed Order Service are examples of the services that\nthe FTGO team will develop during their journey to the microservice architecture.\nWhere they go next after implementing these services depends on the priorities of the\nbusiness. One possible path is to extract Order History Service, described in chap-\nter 7. Extracting this service partially eliminates the need for Delivery Service to\nreplicate data back to the monolith.\nAbout feature toggles\nFeature toggles, or feature flags, let you deploy code changes without necessarily\nreleasing them to users. They also enable you to dynamically change the behavior\nof the application by deploying new code. This article by Martin Fowler provides an\nexcellent overview of the topic: https://martinfowler.com/articles/feature-toggles\n.html.\n«interface»\nDeliveryService\nvoid schedule(...)\nvoid reschedule(...)\nvoid cancel(...)\nFeatureToggleBased\nDeliveryServiceImpl\nDeliveryServiceImpl\nDeliveryServiceProxy\nDelivery\nmanagement\nInvokes\nInvokes\nSends\nmessage\nDelivery\nmanagement\nclient\nDelivery notiﬁcations\nFigure 13.23\nThe final step is to implement DeliveryService with a proxy class that sends \nmessages Delivery Service. A feature toggle controls whether the FTGO monolith uses the old \nimplementation or the new Delivery Service.\n \n",
      "content_length": 1828,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 500,
      "content": "470\nCHAPTER 13\nRefactoring to microservices\n After implementing Order History Service, the FTGO team can then extract the\nservices in the order described in section 13.3.2: Order Service, Consumer Service,\nKitchen Service, and so on. As the FTGO team extracts each service, the maintain-\nability and testability of their application gradually improves, and their development\nvelocity increases. \nSummary\nBefore migrating to a microservice architecture, it’s important to be sure that\nyour software delivery problems are a result of having outgrown your mono-\nlithic architecture. You might be able to accelerate delivery by improving your\nsoftware development process.\nIt’s important to migrate to microservices by incrementally developing a stran-\ngler application. A strangler application is a new application consisting of\nmicroservices that you build around the existing monolithic application. You\nshould demonstrate value early and often in order to ensure that the business\nsupports the migration effort.\nA great way to introduce microservices into your architecture is to implement\nnew features as services. Doing so enables you to quickly and easily develop a\nfeature using a modern technology and development process. It’s a good way to\nquickly demonstrate the value of migrating to microservices.\nOne way to break up the monolith is to separate the presentation tier from the\nbackend, which results in two smaller monoliths. Although it’s not a huge\nimprovement, it does mean that you can deploy each monolith independently.\nThis allows, for example, the UI team to iterate more easily on the UI design\nwithout impacting the backend.\nThe main way to break up the monolith is by incrementally migrating function-\nality from the monolith into services. It’s important to focus on extracting the\nservices that provide the most benefit. For example, you’ll accelerate develop-\nment if you extract a service that implements functionality that’s being actively\ndeveloped.\nNewly developed services almost always have to interact with the monolith. A\nservice often needs to access a monolith’s data and invoke its functionality. The\nmonolith sometimes needs to access a service’s data and invoke its functionality.\nTo implement this collaboration, develop integration glue, which consists of\ninbound and outbound adapters in the monolith.\nTo prevent the monolith’s domain model from polluting the service’s domain\nmodel, the integration glue should use an anti-corruption layer, which is a layer\nof software that translates between domain models.\nOne way to minimize the impact on the monolith of extracting a service is to\nreplicate the data that was moved to the service back to the monolith’s data-\nbase. Because the monolith’s schema is left unchanged, this eliminates the\nneed to make potentially widespread changes to the monolith code base.\n \n",
      "content_length": 2861,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 501,
      "content": "471\nSummary\nDeveloping a service often requires you to implement sagas that involve the\nmonolith. But it can be challenging to implement a compensatable transaction\nthat requires making widespread changes to the monolith. Consequently, you\nsometimes need to carefully sequence the extraction of services to avoid imple-\nmenting compensatable transactions in the monolith.\nWhen refactoring to a microservice architecture, you need to simultaneously\nsupport the monolithic application’s existing security mechanism, which is often\nbased on an in-memory session, and the token-based security mechanism used\nby the services. Fortunately, a simple solution is to modify the monolith’s login\nhandler to generate a cookie containing a security token, which is then for-\nwarded to the services by the API gateway. \n \n",
      "content_length": 811,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 503,
      "content": "473\nindex\nNumerics\n2PC (two-phase commit) 112\n3rd party registration pattern 84–85, 108\n4+1 view model of software architecture 35–37\n500 status code, HTTP 367\nA\nAbstractAutowiringHttpRequestHandler class 423\nAbstractHttpHandler class 423\naccept() method 165, 172\nacceptance tests 335–338\ndefining 336\nexecuting specifications using Cucumber 338\nwriting using Gherkin 337–338\nacceptOrder() method 460\nAccess Token 28, 354, 357\nACD (Atomicity, Consistency, Durability) 111\nACID (Atomicity, Consistency, Isolation, Dur-\nability) transactions 98, 110\nACLs (access control lists) 350\nActiveMQ message broker 92\nadd() method 310\naddOrder() method 249–250\nAggregateRepository class 206–208\naggregates 147, 374, 439\nconsistency boundaries 155\ncreating, finding, and updating 207–208\ndefining aggregate commands 207\ndefining with ReflectiveMutableCommand-\nProcessingAggregate class 206–207\ndesigning business logic with 159–160\nevent sourcing\naggregate history 186, 199–200\naggregate methods and events 189–191\nevent sourcing-based Order aggregate\n191–193\npersisting aggregates using events 186–188\nevent sourcing and aggregate history 199–200\nexplicit boundaries 154–155\ngranularity 158\nidentifying 155\nOrder aggregate 175–180\nmethods 177–180\nstate machine 176–177\nstructure of 175–176\nrules for 155–157\nTicket aggregate 169–173\nbehavior of 170–171\nKitchenService domain service 171–172\nKitchenServiceCommandHandler class\n172–173\nstructure of Ticket class 170\ntraditional persistence and aggregate \nhistory 186\naliases 285\nAlternative pattern 22\nAMI (Amazon Machine Image) 390\nanomalies 126\nAnti-corruption layer pattern 447\nAOP (aspect-oriented programming) 373, 378\nApache Flume 370\nApache Kafka 92\nApache Openwhisk 416\nApache Shiro 351\nAPI composition pattern 221–228\nbenefits and drawbacks of 227–228\nincreased overhead 227\nlack of transactional data consistency\n228\nrisk of reduced availability 227–228\n \n",
      "content_length": 1903,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 504,
      "content": "INDEX\n474\nAPI composition pattern (continued)\ndesign issues 225–227\nreactive programming model 227\nrole of API composer 225–227\nfindOrder() query operation 221–222, 224\noverview of 222–224\nAPI gateway 259–291\nauthentication 354–355\nbenefits of 267\ndesign issues 268–271\nbeing good citizen in architecture 270–271\nhandling partial failures 270\nperformance and scalability 268–269\nreactive programming abstractions 269–270\ndrawbacks of 267\nimplementation using GraphQL 279–291\nconnecting schema to data 285–287\ndefining schema 282–284\nexecuting queries 284–285\nintegrating Apollo GraphQL server with \nExpress 289–290\noptimizing loading using batching and \ncaching 288\nwriting client 290–291\nimplementation using Netflix Zuul 273\nimplementation using off-the-shelf products/\nservices 271–272\nAPI gateway products 272\nAWS API gateway service 271–272\nAWS Application Load Balancer service 272\nimplementation using Spring Cloud \nGateway 273–275\nApiGatewayApplication class 279\nOrderConfiguration class 275–276\nOrderHandlers class 276–278\nOrderService class 278–279\nmapping USERINFO cookie to Authorization \nheader 455\nNetflix example 267–268\noverview of 259–266\nAPI composition 261\narchitecture 263–264\nBackends for frontends pattern 264–266\nclient-specific API 262\nedge functions 262–263\nownership model 264\nprotocol translation 262\nrequest routing 260\nApiGatewayApplication class 279\nApiGatewayMain package 274\nAPIGatewayProxyRequestEvent 417, 421–422\nAPIGatewayProxyResponseEvent 417, 422\nAPIs\ndefining in microservice architecture 68–69\ninterprocess communication 69–71\ncreating specification for messaging-based \nservice API 89–90\nmajor, breaking changes 70–71\nminor, backward-compatible changes 70\nsemantic versioning 70\nspecifying REST APIs 74\nrefactoring to microservices 444–445, 465–466\ntesting microservices\nconsumer contract tests for messaging \nAPIs 305\nconsumer-side integration test for API gate-\nway’s OrderServiceProxy 325–326\nexample contract for REST API 324\nSee also API gateways\nApplication architecture patterns\nMicroservice architecture 8–18, 40\nMonolithic architecture 2–7, 22–34, 40\napplication infrastructure 24\napplication metrics 28, 366, 373–376\ncollecting service-level metrics 374–375\ndelivering metrics to metrics service 375–376\napplication modernization 23–24, 430–432\napplication security 349\napply() method 188, 193\narchitectural styles 37–40\nhexagonal 38–40\nlayered 37–38\nmicroservice architecture 40–43\nloose coupling, defined 42–43\nrelative unimportance of size of service 43\nrole of shared libraries 43\nservices, defined 41–42\naspect-oriented programming (AOP) 373, 378\nasynchronous (nonblocking) I/O model 268\nasynchronous interactions 67\nAsynchronous messaging pattern 85–103\ncompeting receivers and message ordering\n94–95\ncreating API specification 89–90\ndocumenting asynchronous operations 90\ndocumenting published events 90\nduplicate messages 95–97\ntracking messages and discarding \nduplicates 96–97\nwriting idempotent message handlers 96\nimproving availability 103–108\neliminating synchronous interaction\n104–108\nsynchronous communication and \navailability 103–104\ninteraction styles 87–89\none-way notifications 89\npublish/subscribe 89\nrequest/response and asynchronous request/\nresponse 87–88\n \n",
      "content_length": 3236,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 505,
      "content": "INDEX\n475\nAsynchronous messaging pattern (continued)\nlibraries and frameworks for 100–103\nbasic messaging 101\ncommand/reply-based messaging 102–103\ndomain event publishing 102\nmessage brokers 90–94\nbenefits and drawbacks of 93–94\nbrokerless messaging 91–92\nimplementing message channels using 93\noverview of 92\noverview of 86–87\ntransactional messaging 97–100\npublishing events using Polling publisher \npattern 98–99\npublishing events using Transaction log tail-\ning pattern 99–100\nusing database table as message queue\n97–98\nasynchronous request/response interactions\nimplementing 87–88\nintegration tests for\nconsumer-side contract tests 332–335\ncontract tests 330–335\nexample contract 331–332\nAtomicity, Consistency, Durability (ACD) 111\nAtomicity, Consistency, Isolation, Durability \n(ACID) transactions 98, 110\nattribute value 245\naudit logging 28, 186, 366, 377–378\nadding code to business logic 378\naspect-oriented programming 378\nevent sourcing 378\nauditing 350\nauthentication and authorization\nrefactoring to microservices 453–455\nAPI gateway maps USERINFO cookie to \nAuthorization header 455\nLoginHandler sets USERINFO cookie\n454–455\nsecurity in microservice architecture\nhandling authentication 354–355\nhandling authorization 356\nAuthorization Server concept 357\nautomated testing 28, 293, 295–296\nautomatic sidecar injection 411\nAvro 72\nAWS API gateway service 271–272\nAWS Application Load Balancer service 272\nAWS DynamoDB 242–252\ndata modeling and query design 244–249\ndetecting duplicate events 248–249\nfindOrderHistory query 245–247\nFTGO-order-history table 245\npaginating query results 247\nupdating orders 247–248\nOrderHistoryDaoDynamoDb class 249–252\naddOrder() method 249–250\nfindOrderHistory() method 251–252\nidempotentUpdate() method 250–251\nnotePickedUp() method 250\nOrderHistoryEventHandlers module 243–244\nAWS Gateway, deploying RESTful services \nusing 419–426\ndeploying lambda functions using Serverless \nframework 425–426\ndesign of Restaurant Service 419–423\npackaging service as ZIP file 424\nAWS Lambda\nbenefits of lambda functions 418\ndeveloping lambda functions 417\ndrawbacks of lambda functions 419\ninvoking lambda functions 417–418\ndefining scheduled lambda functions 418\nhandling events 418\nhandling HTTP requests 417\ninvoking lambda functions using web service \nrequests 418\noverview of 416\nRESTful services 419–426\ndeploying lambda functions using Serverless \nframework 425–426\ndesign of Restaurant Service 419–423\npackaging service as ZIP file 424\naws.region property 363\nAxon 202\nAzure functions, Microsoft 416\nB\nBackends for frontends (BFF) pattern 264–266\nbatching 288\n@Before setUp() method 309\nbeforeHandling() method 423\nBig Ball of Mud pattern 2\nbig bang rewrite 430\nbinary message formats 72\nbounded context 55\nbroker-based messaging 90–94\nbenefits and drawbacks of 93–94\nimplementing message channels using 93\noverview of 92\nbrokerless messaging 91–92\nBrowser API module 264\nbusiness capability 40\nbusiness logic 146–219\nadding audit logging code to 378\ndomain events 160–168\nconsuming 167–168\ndefined 161\nevent enrichment 161–162\n \n",
      "content_length": 3076,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 506,
      "content": "INDEX\n476\nbusiness logic (continued)\ngenerating 164–165\nidentifying 162–163\npublishing 166–167\nreasons to publish 160–161\ndomain model design 152–160\naggregates 154–160\nproblem with fuzzy boundaries 153–154\nevent sourcing 184–202\nbenefits of 199–200\ndrawbacks of 200–202\nevent publishing 194–195\nevolving domain events 198–199\nhandling concurrent updates using optimis-\ntic locking 193–194\nidempotent message processing 197\noverview of 186–193\nsnapshots, improving performance with\n195–196\ntraditional persistence 185–186\nevent store implementation 202–209\nEventuate client framework for Java 205–209\nEventuate Local event store 203–205\nKitchen Service business logic 168–173\nOrder Service business logic 173–182\nOrder aggregate 175–180\nOrderService class 180–182\norganization patterns 147–152\nDomain model pattern 150–151\ndomain-driven design 151–152\nTransaction script pattern 149–150\nsagas and event sourcing together 209–218\ncreating orchestration-based saga 211–212\nimplementing choreography-based sagas \nusing event sourcing 210\nimplementing event sourcing-based saga \nparticipant 213–216\nimplementing saga orchestrators using event \nsourcing 216–218\nBusiness logic design patterns\nAggregate 147, 152–160\nDomain event 160\nDomain model 150–151\nEvent sourcing 184\nTransaction script 149–150\nbusiness logic layer 38, 436\nby value countermeasure 131–132\nC\ncaching 262, 288\ncancel() operation 177\ncancelOrder() method 460\nCAP theorem 113\nCCP (Common Closure Principle) 56\ncentralized sessions 354\nchange failure rate 31\nchoreography 111\nchoreography-based sagas 118–121\nbenefits and drawbacks of 121\nimplementing Create Order saga 118–119\nimplementing using event sourcing 210\nreliable event-based communication 120–121\nCI (Continuous Integration) 6, 306, 357\nCircuit breaker pattern 77–80\ndeveloping robust RPI proxies 79\nrecovering from unavailable services 79–80\nClient concept 358\nClient-side discovery pattern 82–83\ncommand message 86\nCommand query responsibility segregation. See \nCQRS pattern\ncommand/reply-based messaging 102–103\ncommands 41\ncommit tests stage 306\ncommitted records 130\nCommon Closure Principle (CCP) 56–57\ncommunication\nflexible 93\nsecure interprocess 350\ncommunication patterns 23–25\ncommutative update countermeasure 130\ncompensatable transactions 116, 128, 450\ncompensating transaction 450\ncompile-time tests 297\ncomponent tests 306, 339–340\nfor FTGO Order Service\nOrderServiceComponentTestStepDefinitions \nclass 341–344\nrunning 344–345\nwriting 340–345\nin-process component tests 339\nout-of-process component tests 339–340\ncondition expression 248\nConduit 381\nConfigMap 402\nconfigurable services 360–364\npull-based externalized configuration 363–364\npush-based externalized configuration 362–363\n@ConfigurationProperties class 276\nconsumer contract testing 301–303\nfor asynchronous request/response \ninteraction 332–335\nfor messaging APIs 305\nfor publish/subscribe-style interactions\n328–330\nfor REST-based request/response style \ninteractions 324–326\nconsumer group 94\nconsumer-driven contract test 28, 302\n \n",
      "content_length": 3040,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 507,
      "content": "INDEX\n477\nconsumerId parameter 229\nconsumer-provider relationship 301\nconsumer-side contract test 28, 302\ncontainers\ncontainer image 395\nDeploy a service as a container 22, 393\nDocker 395–398\ncontinuous deployment 5\ndeployment pipeline 305–307\nContinuous Integration (CI) 6, 306, 357\ncontrollers, unit tests for 313–315\nConway, Melvin 30\nConway’s law 30\ncorrelation ID 88–89, 120\ncountermeasures 111, 126, 450\nCQRS (Command query responsibility \nsegregation) 26, 63, 160, 228–236\nbenefits of 235–236\nefficient implementation 235\nimproved separation of concerns 235–236\nquerying in event sourcing-based \napplication 235\ndrawbacks of 236\nmore complex architecture 236\nreplication lag 236\nmotivations for using 229–232\nfindAvailableRestaurants() query \noperation 231\nfindOrderHistory() query operation 229–231\nneed to separate concerns 231–232\noverview of 232–235\nquery-only services 233–235\nseparating commands from queries 232–233\nviews\nadding and updating 241–242\ndesigning 236–242\nimplementing with AWS DynamoDB\n242–252\nCreate Order saga 114–115, 135–142\nCreateOrderSaga orchestrator 136–138\nCreateOrderSagaState class 138\nEventuate Tram Saga framework 140–142\nimplementing using choreography 118–119\nimplementing using orchestration 122–123\nKitchenServiceProxy class 139\ncreate, update, and delete (CRUD) \noperations 232\ncreate() method 171, 204\ncreateOrder() operation 114\nCreateOrderSaga orchestrator 136–138\nCreateOrderSagaState class 138\nCreateOrderSagaTest class 312\nCross-cutting concerns patterns\nExternalized configuration 28, 361\nMicroservice chassis 28, 378–382\nCRUD (create, update, and delete) \noperations 232\nCucumber framework 338\nCustomerContactInfoRepository interface 445, \n458\nD\nDAO (data access object) 39, 149, 239\ndata access logic layer 436\ndata consistency 449–453\nAPI composition pattern and 228\nmaintaining across services 58\nrefactoring to microservices\nsagas and compensatable transactions\n451–452\nsequencing extraction of services 452–453\nsupporting compensatable transactions\n450–451\nSaga pattern 25–26, 114–117\ndata consistency patterns 25\nSaga pattern 25–26, 114–117\nDataLoader module 288\nDDD (domain-driven design) 24, 34\nDDD aggregate pattern 152–160\nDebezium 100\nDecompose by business capability pattern 51–54\ndecomposition 52–54\nidentifying business capabilities 51–52\npurpose of business capabilities 51\ndecomposition 33–64\nDecompose by subdomain 54\ndefining application’s microservice \narchitecture 44–64\ndefining service APIs 61–64\nguidelines for decomposition 56–57\nidentifying system operations 45–50\nobstacles to decomposition 57–61\nservice definition with Decompose by business \ncapability pattern 51–54\nservice definition with Decompose by sub-\ndomain pattern 54–55\nguidelines for 56–57\nCommon Closure Principle 56–57\nSingle Responsibility Principle 56\nobstacles to 57–61\ngod classes 58–61\nmaintaining data consistency across \nservices 58\nnetwork latency 57\nobtaining consistent view of data 58\nsynchronous interprocess communication 57\npatterns\nDecompose by business capability 24, 51–54\nDecompose by subdomain 24, 54\n \n",
      "content_length": 3066,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 508,
      "content": "INDEX\n478\nDelayed Delivery Service\nchanging FTGO monolith to interact with\n467–470\ndefining interface 467–468\nimplementing interface 468–470\nrefactoring monolith to call interface\n468\ndesign for 456–457\ndomain model 463–464\ndeciding which data to migrate 464\ndesign of domain logic 464\nidentifying which entities and fields are \npart of delivery management 463\nexisting delivery functionality 460–461\nintegration glue for 457–459, 465–467\nCustomerContactInfoRepository \ninterface 458\ndesign of API 465–466\nhow Delivery Service accesses FTGO \ndata 466\nhow FTGO accesses data 467\npublishing and consuming Order and Restau-\nrant domain events 458–459\noverview of 462–463\ndeleted flag 201\ndeliver action 460\nDeliveryServiceImpl class 468\ndependencies 125\ndeploy stage 306\ndeployment 383–427\nLanguage-specific packaging format \npattern 386–390\nbenefits of 388–389\ndrawbacks of 389–390\nRESTful services using AWS Lambda and AWS \nGateway 419–426\ndeploying lambda functions using Serverless \nframework 425–426\ndesign of Restaurant Service 419–423\npackaging service as ZIP file 424\nServerless deployment pattern 415–419\nbenefits of lambda functions 418\ndeveloping lambda functions 417\ndrawbacks of lambda functions 419\ninvoking lambda functions 417–418\noverview of 416\nService as container pattern 393–399\nbenefits of 398\nDocker 395–398\ndrawbacks of 399\nService as virtual machine pattern 390–393\nbenefits of 392\ndrawbacks of 392–393\nService mesh pattern 380\nSidecar pattern 410\nwith Kubernetes 399–415\ndeploying API gateway 405–406\ndeploying Restaurant Service 402–405\noverview of 399–402\nservice meshes 407–415\nzero-downtime deployments 406–407\ndeployment frequency 31\nDeployment patterns\nDeploy a service as a container 22, 393\nDeploy a service as a VM 390, 392\nLanguage-specific packaging format 386, 390\nServerless deployment 415–419\nService mesh 380\nSidecar 410\ndeployment pipeline 305–307\nDeployment view 36\nDestinationRule 413\ndirty reads 127\nDistributed tracing pattern 28, 366, 370–373\ndistributed tracing server 373\ninstrumentation libraries 373\nDistributed Transaction Processing (DTP) 112\nDocker 395–398\nbuilding images 395–396\npushing images to registry 396–397\nrunning containers 397–398\ndocker build command 396\nDocker containers 267\ndocker push command 397\ndocker run command 397\ndocker tag command 396\ndocument message 86\ndomain event publishing 102\ndomain events 160–168, 198–199\nconsuming 167–168, 458–459\ndefined 161\ndefining 207\nevent enrichment 161–162\nevent schema evolution 198–199\ngenerating 164–165\nidentifying 162–163\nmanaging schema changes through \nupcasting 199\npublishing 102, 166–167, 448–449, 458–459\nreasons to publish 160–161\nsubscribing to 208–209, 448–449\ndomain model 54, 150–160\naggregates\nconsistency boundaries 155\ndesigning business logic with 159–160\nexplicit boundaries 154–155\ngranularity 158\nidentifying aggregates 155\nrules for 155–157\ncreating high-level domain model 46–48\n \n",
      "content_length": 2916,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 509,
      "content": "INDEX\n479\ndomain model (continued)\nDelivery Service 463–464\ndeciding which data to migrate 464\ndesign of domain logic 464\nidentifying which entities and fields are part \nof delivery management 463\nproblem with fuzzy boundaries 153–154\nsplitting 439–440\ndomain services\nKitchenService 171–172\nunit tests for 312–313\ndomain-driven design (DDD) 24, 34\nDSL (domain-specific language) 303\nDTP (Distributed Transaction Processing)\n112\ndumb pipes 14\nduplicate messages 95–97\ntracking messages and discarding \nduplicates 96–97\nwriting idempotent message handlers 96\nDynamoDB streams 100\nE\nedge functions 271\nElastic Beanstalk 391\nElasticsearch 370\n@EnableGateway annotation 279\nend-to-end tests 345–346\ndesigning 345\nrunning 346\nwriting 346\nEnterprise Service Bus (ESB) 264\nentities, unit tests for 309–310\nEntity object, DDD 151\nenums 283\nESB (Enterprise Service Bus) 264\nevent. See Domain events\nevent handlers\nevents generated by AWS services 418\nidempotent 240–241\nunit tests for 315–317\nevent message 86\nevent publishing 194–195\nAsynchronous messaging pattern 89–90, \n98–100, 102\ndomain events 160–168\nconsuming 167–168\ndefined 161\nevent enrichment 161–162\ngenerating and publishing 164–167\nidentifying 162–163\nreasons for 160–161\nevent sourcing 194–195, 199\ntraditional persistence and 186\nusing polling 194–195\nusing transaction log tailing 195\nevent sourcing 184–202\naudit logging 378\nbenefits of 199–200\navoids O/R impedance mismatch \nproblem 200\npreserves aggregate history 199–200\nreliable domain event publishing 199\ntime machine for developers 200\nconcurrent updates and optimistic locking\n193–194\ndrawbacks of 200–202\ncomplexity 200\ndeleting data 201\nevolving events 201\nlearning curve 200\nquerying event store 202\nevent publishing 194–195\nusing polling 194–195\nusing transaction log tailing 195\nevolving domain events 198–199\nevent schema evolution 198–199\nmanaging schema changes through \nupcasting 199\nidempotent message processing 197\nwith NoSQL-based event store 197\nwith RDBMS-based event store 197\noverview of 186–193\naggregate methods required to generate \nevents 189–191\nevent sourcing-based Order aggregate 191–193\nevents representing state changes 188\npersisting aggregates using events 186–188\nsagas and 209–218\ncreating orchestration-based saga 211–212\nimplementing choreography-based sagas \nusing event sourcing 210\nimplementing event sourcing-based saga \nparticipant 213–216\nimplementing saga orchestrators using event \nsourcing 216–218\nsnapshots and performance improvement\n195–196\ntrouble with traditional persistence 185–186\naudit logging 186\nevent publishing bolted to business logic 186\nlack of aggregate history 186\nObject-Relational impedance mismatch\n185–186\nEvent Store 202\nevent store implementation 202–209\nEventuate client framework for Java 205–209\nAggregateRepository class 207–208\ndefining aggregate commands 207\n \n",
      "content_length": 2851,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 510,
      "content": "INDEX\n480\nevent store implementation (continued)\ndefining aggregates with ReflectiveMutable-\nCommandProcessingAggregate \nclass 206–207\ndefining domain events 207\nsubscribing to domain events 208–209\nEventuate Local event store 203–205\nconsuming events by subscribing to event \nbroker 205\nevent relay propagates events from database \nto message broker 205\nschema 203–205\nevent storming 162\nevent-driven I/O 269\n@EventHandlerMethod annotation 208\nevents. See Domain events\n@EventSubscriber annotation 208\nEventuate framework 101, 202, 205–209\nand updating aggregates with the Aggregate-\nRepository class 207–208\ndefining aggregate commands 207\ndefining aggregates with ReflectiveMutable-\nCommandProcessingAggregate class\n206–207\ndefining domain events 207\nsubscribing to domain events 208–209\nEventuate Local event store 203–205\nconsuming events by subscribing to event \nbroker 205\nevent relay propagates events from database to \nmessage broker 205\nschema 203–205\nEventuate Tram 100, 166\nEventuate Tram Saga framework 140–142\nException tracking pattern 28, 366, 376–377\nExpress framework 289–290\nexternal API patterns 253–291\nAPI gateway 76, 227, 254, 259–272\nAPI gateway implementation 271–291\nusing GraphQL 279–291\nusing Netflix Zuul 273\nusing off-the-shelf products/services 271–272\nusing Spring Cloud Gateway 273–275\nAPI gateway pattern 76, 227, 254, 259–271\nbenefits of 267\ndesign issues 268–271\ndrawbacks of 267\nNetflix example 267–268\noverview of 259–266\nBackends for frontends 254, 262, 264–266\ndesign issues 254–259\nbrowser-based JavaScript applications 258\nFTGO mobile client 255–258\nthird-party applications 258–259\nweb applications 258\nexternalized configuration 361\npull-based 363–364\npush-based 262–263\nExternalized Configuration pattern 28, 361\nF\nFactory object, DDD 151\nfault isolation 6\nfeature flags 469\nfeature toggles 469\nfilter expression 247\nfilter parameter 229\nfind() operation 204\nfindAvailableRestaurants() query operation 231\nfindCustomerContactInfo() method 447\nfindOrder() operation 221–222, 224\nfindOrderHistory() query operation 229–231, \n251–252\ndefining index for 245–247\nimplementing 247\nFindRestaurantRequestHandler class 421–422\nFission framework 416\nFluentd 370\nFlume 370\nfold operation 187\nFTGO application\nAPI design issues for mobile client 255–258\nchanging monolith to interact with Delivery \nService 467–470\ncomponent tests for Order Service 340–345\ndeploying with Kubernetes 399–415\nAPI gateway 405–406\nRestaurant Service 402–405\nservice meshes 407–415\nzero-downtime deployments 406–407\nmicroservice architecture of 12–13\nmonolithic architecture of 3–4\nftgo-db-secret 404\nFtgoGraphQLClient class 290\nfunctional decomposition 10\nfuzzy boundaries 153–154\nG\nGDPR (General Data Protection Regulation) 201\ngeneralization pattern 22\nGET REST endpoint 271\ngetDelayedOrders() method 456\ngetOrderDetails() query 368\nGherkin\nexecuting specifications using Cucumber 338\nwriting acceptance tests 337–338\nGo Kit 380\ngod classes 58–61\n \n",
      "content_length": 2964,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 511,
      "content": "INDEX\n481\nGoLang (Go language) 4, 380\nGoogle Cloud functions 416\ngraph-based schema 280\nGraphQL 279, 281–291\nconnecting schema to data 285–287\ndefining schema 282–284\nexecuting queries 284–285\nintegrating Apollo GraphQL server with \nExpress 289–290\nload optimization using batching and caching 288\nwriting client 290–291\ngRPC 76–77\nH\nhandleHttpRequest() method 421\nhandleRequest() method 417\nhealth check 82, 365\nHealth check API pattern 27, 366–368\nimplementing endpoint 367–368\ninvoking endpoint 368\nhexagonal architecture 3, 38–40\nhigh-level design patterns 20\nHoneybadger 377\nHttpServletResponse 422\nHumble, Jez 30\nI\nidempotent message processing 96, 197\nCQRS views 240–241\nevent sourcing-based saga participant 213\nwith NoSQL-based event store 197\nwith RDBMS-based event store 197\nidempotentUpdate() method 250–251\nIDL (interface definition language) 69\n-ilities 8, 34, 37\nImplementation view 35\ninbound adapters 3, 38\ninfrastructure patterns 23–24\ninit system, Linux 390\nin-memory security context 353\ninstrumentation libraries 373\nintegration glue 444–449\ndesigning API for 444–445\nfor Delayed Delivery Service 457–459, 465–467\nCustomerContactInfoRepository \ninterface 458\ndesign of API 465–466\nhow Delivery Service accesses FTGO data 466\nhow FTGO accesses data 467\npublishing and consuming Order and \nRestaurant domain events 458–459\nhow monolith publishes and subscribes to \ndomain events 448–449\nimplementing anti-corruption layer 446–448\npicking interaction style and IPC \nmechanism 445–446\nintegration tests 319–335\nasynchronous request/response \ninteractions 330–335\nexample contract 331–332\ntests for asynchronous request/response \ninteraction 332–335\npersistence integration tests 321–322\npublish/subscribe-style interactions 326–330\ncontract for publishing OrderCreated \nevent 327–328\ntests for Order History Service 329–330\ntests for Order Service 328–329\nREST-based request/response style \ninteractions 322–326\nexample contract 324\ntests for API gateway OrderServiceProxy\n325–326\ntests for Order Service 324–325\ninteraction styles 67–68, 87–89\nasynchronous 104–105\none-way notifications 89\npublish/async responses 89\npublish/subscribe 89\nrequest/response and asynchronous request/\nresponse 87–88\nselecting 445–446\ninterface definition language (IDL) 69\ninvariants 153\nIPC (interprocess communication) 24, 65, \n93–109\noverview of 66–72\ndefining APIs 68–69\nevolving APIs 69–71\ninteraction styles 67–68\nmessage formats 71–72\nusing asynchronous Messaging pattern 85–103\ncompeting receivers and message \nordering 94–95\ncreating API specification 89–90\nduplicate messages 95–97\nimproving availability 103–108\ninteraction styles 87–89\nlibraries and frameworks for 100–103\nmessage brokers 90–94\noverview of 86–87\ntransactional messaging 97–100\nusing synchronous Remote procedure invoca-\ntion pattern 72–85\nCircuit breaker pattern 77–80\ngRPC 76–77\nREST 73–76\nservice discovery 80–85\n \n",
      "content_length": 2894,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 512,
      "content": "INDEX\n482\nIstio 381\ndeploying services 410–412\nEnvoy proxy 410\nservice meshes 408–410\nJ\njava -jar command 395\nJenkins 306\nJSESSIONID cookie 351\nJSON message 71\nJUL (java.util.logging) 369\nJWT (JSON Web Token) 28, 356–357\nK\nKafka 92\nkey condition expression 247\nKibana 370\nKitchen Service\nbusiness logic 168–173\nTicket aggregate 169–173\nKitchenServiceCommandHandler class 172–173\nKitchenServiceProxy class 139\nKong package 272\nkubectl apply command 404\nkubectl apply -f command 406\nKubernetes 399–415\ndeploying API gateway 405–406\ndeploying Restaurant Service 402–405\noverview of 399–402\narchitecture 400–402\nkey concepts 402\nservice meshes 407–415\ndeploying services 410–412\ndeploying v2 of Consumer Service 414\nIstio 408–412\nrouting production traffic to v2 415\nrouting rules to route to v1 version\n412–413\nrouting test traffic to v2 414\nzero-downtime deployments 406–407\nL\nLagom 202\nlambda functions 271, 416\nbenefits of 418\ndeploying using Serverless framework 425–426\ndeveloping 417\ndrawbacks of 419\ninvoking 417–418\ndefining scheduled lambda functions 418\nhandling events generated by AWS \nservices 418\nhandling HTTP requests 417\nusing web service request 418\nLanguage-specific packaging format pattern\n386–390\nbenefits of 388–389\nefficient resource utilization 389\nfast deployment 389\ndrawbacks of 389–390\nautomatically determining where to place ser-\nvice instances 390\nlack of encapsulation of technology stack 389\nlack of isolation 390\nno ability to constrain resources \nconsumed 389\nlatency 419\nlayered architectural style 37–38\nlayered file system 397\nlead time 31, 293\nlines of code (LOC) application 5\nLinkedIn Databus 100\nLinkerd 381\nlivenessProbe 404\nLoadBalancer service 405\nLOC (lines of code) application 5\nLog aggregation pattern 27, 365, 368–370\nlog aggregation infrastructure 370\nlog generation 369–370\nlog4j 369\nLogback 369\nLogical view 35\nLoginHandler 352, 454–455\nLogstash 370\nloose coupling 93, 121\nlost updates 127\nM\nMAJOR part, Semvers 70\nmakeContextWithDependencies() function 290\nmanual sidecar injection 411\nMartin, Robert C. 57\nmaster machine 400\nmean time to recover 31\nMemento pattern 196\nmessage brokers 85, 90–94\nbenefits and drawbacks of 93–94\nimplementing message channels using 93\noverview of 92\nmessage buffering 93\nmessage channels 86–87, 93\nmessage handler adapter class 86\nmessage handlers, unit tests for 315–317\nmessage identifier 88\nmessage ordering 94–95\nmessage sender adapter class 86\n \n",
      "content_length": 2435,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 513,
      "content": "INDEX\n483\nmessaging. See Asynchronous messaging pattern\nMessaging style patterns. See Asynchronous messag-\ning pattern\nmetrics collection 262\nMicro framework 380\nmicrometer-registry-prometheus library 375\nmicroservice architecture 8–14, 34, 43\nas form of modularity 11–12\nbenefits of 14–17\ncontinuous delivery and deployment of large, \ncomplex applications 15\nfault isolation improvement 16\nindependently scalable services 16\nnew technology experimentation and \nadoption 16–17\nsmall, easily maintained services 15\ndefining 44–64\ndecomposition guidelines 56–57\ndefining service APIs 61–64\nidentifying system operations 45–50\nobstacles to decomposing an application into \nservices 57–61\nservice definition with Decompose by business \ncapability pattern 51–54\nservice definition with Decompose by sub-\ndomain pattern 54–55\ndrawbacks of 17–19\nadoption timing 18–19\nchallenge of finding right services 17\ncomplex distributed systems 17–18\ndeployment coordination 18\neach service has own database 12\nFTGO application 12–13\nloose coupling, defined 42–43\nnot silver bullet 19–20\nrelationships between process, organization, \nand 29–32\nhuman side of adopting microservices\n31–32\nsoftware development and delivery \norganization 29–30\nsoftware development and delivery \nprocess 30–31\nrelative unimportance of size of service 43\nrole of shared libraries 43\nscale cube 8–11\nX-axis scaling 9\nY-axis scaling 10–11\nZ-axis scaling 9–10\nservice-oriented architecture versus 13–14\nservices, defined 41–42\nsoftware architecture 34–37\n4+1 view model of 35–37\ndefinition of 35\nrelevance of 37\ntransaction management 111–117\nmaintaining data consistency 114–117\nneed for distributed transactions 112\ntrouble with distributed transactions 112–114\nMicroservice chassis pattern 28, 378–382\nservice meshes 380–382\nusing 379–380\nMINOR part, Semvers 70\nMixer 409\nMobile API module 264\nMockito 305\nmocks 296\nmodularity, microservice architecture as form \nof 11–12\nMono abstraction 277\nmonolithic architecture 1–32, 40\nbenefits of 4\ncauses of monolithic hell 4–7\nintimidation due to complexity 4–5\nlong and arduous path from commit to \ndeployment 5–6\nreliability challenges 6\nscaling challenges 6\nslow development 5\ntechnology stack obsolescence 6–7\nFTGO monolithic architecture 3–4\nmultiply() method 310\nMyBATIS 185\nN\nNetflix Falcor 281\nNetflix Hystrix 79\nNetflix Zuul 273\nNetflix, as API gateway 267–268\nnetwork latency 57\nnetwork timeouts 79\nNodePort service 406\nnodes 280, 400\nnonblocking I/O 268\nnonfunctional requirements 8\nnon-key attributes 246\nNoSQL-based event store\ncreating saga orchestrator when using 211–212\nidempotent message processing when using 197\nSQL versus 237–238\nnotePickedUp() method 250\nO\nO/R (Object-Relational) impedance \nmismatch 185–186, 200\nOAuth 2.0 protocol 357–360\nobject-oriented design pattern 20\nobject-oriented programming (OOP) 149\n \n",
      "content_length": 2842,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 514,
      "content": "INDEX\n484\nObject-Relational (O/R) impedance \nmismatch 185–186, 200\nobservability 349\nobservability patterns 27–28\nApplication metrics 373–376\nAudit logging 377–378\nDistributed tracing 370–373\nException tracking 376–377\nHealth check API 366–368\nLog aggregation 366, 368–370\nobservable services 364–378\nApplication metrics pattern 373–376\ncollecting service-level metrics 374–375\ndelivering metrics to metrics service 375–376\nAudit logging pattern 377–378\nadding code to business logic 378\naspect-oriented programming 378\nevent sourcing 378\nDistributed tracing pattern 370–373\ndistributed tracing server 373\ninstrumentation libraries 373\nException tracking pattern 376–377\nHealth check API pattern 366–368\nimplementing endpoint 367–368\ninvoking endpoint 368\nLog aggregation pattern 368–370\nlog generation 369–370\nlogging aggregation infrastructure 370\nole-based authorization 353\none-size-fits-all (OSFA) 262\none-to-many interaction 67\none-to-one interaction 67\none-way notifications 68, 89\none-way notification-style API 90\nOOP (object-oriented programming) 149\nopaque tokens 356\nOpenwhisk 416\noptimistic locking 193–194\nOptimistic Offline Lock pattern 131\norchestration 111, 399\norchestration-based sagas 121–125\nbenefits and drawbacks of 125\ncreating 211–212\nimplementing Create Order saga 122–123\nimplementing using event sourcing 216–218\nmodeling saga orchestrators as state \nmachines 123–124\ntransactional messaging and 125\nOrder aggregate 175–180\nevent sourcing-based 191–193\nmethods 177–180\nstate machine 176–177\nstructure of 175–176\nOrder domain events, publishing and \nconsuming 458–459\nOrder History Service 329–330\nOrder Service\nbusiness logic 173–182\nOrder aggregate 175–180\nOrderService class 180–182\nconsumer-driven contract integration tests \nfor 324–325\nconsumer-driven contract tests for 328–329\nOrderCommandHandlers class 142–143\nOrderService class 133–134\nOrderServiceConfiguration class 143–145\nOrderCommandHandlers class 142–143\nOrderConfiguration class 275–276\nOrderCreated event 327–328\nOrderDetailsRequestHandler 352\nOrderHandlers class 276–278\nOrderHistoryDaoDynamoDb class 249–252\naddOrder() method 249–250\nfindOrderHistory() method 251–252\nidempotentUpdate() method 250–251\nnotePickedUp() method 250\nOrderHistoryEventHandlers module 243–244\nOrderService class 133–134, 180–182, 278–279\nOrderServiceComponentTestStepDefinitions \nclass 341–344\nOrderServiceConfiguration class 143–145\nOrderServiceProxy 325–326\nOSFA (one-size-fits-all) 262\noutbound adapters 3, 38, 147\noutstanding requests 79\nP\npagination parameter 229\npartition key 246\nPassport framework 351\nPATCH part, Semvers 70\npatterns and pattern language 20–23\nby name\n3rd party registration 85\nAccess token 354\nAggregate 150\nAnti-corruption layer 447\nAPI composition 223\nAPI gateway 259\nApplication metrics 373\nAudit logging 377\nBackends for frontends 265\nCircuit breaker 78\nClient-side discovery 83\nCommand query responsibility \nsegregation 228\nConsumer-driven contract test 302\nConsumer-side contract test 303\nDecompose by business capability 51\n \n",
      "content_length": 3033,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 515,
      "content": "INDEX\n485\npatterns and pattern language (continued)\nDecompose by subdomain 54\nDeploy a service as a container 393\nDeploy a service as a VM 390\nDistributed tracing 370\nDomain event 160\nDomain model 150\nEvent sourcing 184\nException tracking 376\nExternalized configuration 361\nHealth check API 366\nLanguage-specific packaging format 387\nLog aggregation 368\nMessaging 85\nMicroservice architecture 40\nMicroservice chassis 379\nMonolithic architecture 40\nPolling publisher 98\nRemote procedure invocation 72\nSaga 114\nSelf registration 82\nServerless deployment 416\nServer-side discovery 85\nService component test 335\nService mesh 380\nSidecar 410\nStrangler application 432\nTransaction log tailing 99\nTransaction script 149\nTransactional outbox 98\ngroups of patterns 23–29\ncommunication patterns 24–25\ndata consistency patterns 25\nfor automated testing of services 28\nfor decomposing applications into \nservices 24\nfor handling cross-cutting concerns 28\nfor querying data 25–26\nobservability patterns 27–28\nsecurity patterns 28–29\nservice deployment patterns 26\nsections of patterns\nforces 21\nrelated patterns 21–23\nresulting context 21\npending state 176\npersistence\npersisting aggregates using events 186–188\ntraditional approach to 185–186\naudit logging 186\nevent publishing bolted to business \nlogic 186\nlack of aggregate history 186\nobject-relational impedance mismatch\n185–186\npersistence integration tests 321–322\nPersistence layer 38\npessimistic view countermeasure 130–131\npickup action 460\nPilot 409\npivot transaction 128, 450\npods 402\npoint-to-point channel 87\npolicy enforcement 409\npolling 194–195\nPolling publisher pattern 98–99\nports 38\npre-commit tests stage 306\npredecessor pattern 21\nPresentation layer 38\npresentation logic 436\nprimary key-based queries 235\nProcess view 36\nprocess() method 190, 193\nproduction-ready service development\n348–382\nconfigurable services 360–364\npull-based externalized configuration\n363–364\npush-based externalized configuration\n362–363\nMicroservice chassis pattern 378–382\nservice meshes 380–382\nusing 379–380\nobservable services 364–378\nApplication metrics pattern 373–376\nAudit logging pattern 377–378\nDistributed tracing pattern 370–373\nException tracking pattern 376–377\nHealth check API pattern 366–368\nLog aggregation pattern 368–370\nsecure services 349–360\nhandling authentication in API gateway\n354–355\nhandling authorization 356\nin traditional monolithic application\n350–353\nusing JWTs to pass user identity and \nroles 356–357\nusing OAuth 2.0 357–360\nPrometheus 375\nproperties, graph-based schema 280\nProtocol Buffers 72\nprovider service 223\nproxy classes 274\nproxy interface 72\npseudonymization 201\nPublic API module 264\npublish() method 166\npublish/async responses 89\n \n",
      "content_length": 2719,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 516,
      "content": "INDEX\n486\npublish/subscribe-style interaction\nimplementing 89\nintegration tests for 326–330\ncontract for publishing OrderCreated \nevent 327–328\ntests for Order History Service 329–330\ntests for Order Service 328–329\npublish-subscribe channel 87\npull model of externalized configuration 361, 375\npush model of externalized configuration 361, \n375\nQ\nquality attributes 8, 34, 37\nquality of service 8, 37\nqueries 41\nquery arguments 286\nquery() operation 246, 249\nquerying patterns 220–252\nAPI composition pattern 26, 64, 79, 221–228\nbenefits and drawbacks of 227–228\ndesign issues 225–227\nfindOrder() query operation 221–222, 224\noverview of 222–224\nCQRS pattern 26, 63, 160, 184, 221, 228–236\nbenefits of 235–236\ndrawbacks of 236\nmotivations for using 229–232\noverview of 232–235\nR\nRabbitMQ 92\nrate limiting 262\nRDBMS-based event store\ncreating saga orchestrator when using 211\nidempotent message processing with 197\nreactive programming model 227\nreadinessProbe 404, 407\nreceiving port interface 86\nreduce operation 187\nrefactoring 428–471\napplication modernization 430–432\ndemonstrating value 432\ndesigning how service and monolith \ncollaborate 443–455\nauthentication and authorization 453–455\ndata consistency 449–453\nintegration glue 444–449\nextracting delivery management 459–470\nchanging FTGO monolith to interact with \nDelivery Service 467–470\ndesigning Delivery Service domain \nmodel 463–464\ndesigning Delivery Service integration \nglue 465–467\nexisting delivery functionality 460–461\noverview of Delivery Service 462–463\nimplementing new features as services 455–459\ndesign for Delayed Delivery Service 456–457\nintegration glue for Delayed Delivery \nService 457–459\nminimizing changes 432–433\noverview of 429–433\nreasons for 429–430\nstrategies for 433–442\nextracting business capabilities into \nservices 437–442\nimplementing new features as services\n434–435\nseparating presentation tier from \nbackend 436–437\ntechnical deployment infrastructure 433\nRefactoring to microservices patterns\nAnti-corruption layer 446–447\nStrangler application 431–432\nReflectiveMutableCommandProcessingAggregate \nclass 206–207\nRefresh Token concept 358\nReleasing services 408\nReliable communications pattern\nCircuit breaker 77–80, 108\nRemote procedure invocation (RPI) pattern\n72–85\nCircuit breaker pattern 77–80\ndeveloping robust RPI proxies 79\nrecovering from unavailable services 79–80\ngRPC 76–77\nREST 73–76\nbenefits and drawbacks of 75–76\nfetching multiple resources in single \nrequest 74–75\nmapping operations to HTTP verbs 75\nREST maturity model 74\nspecifying REST APIs 74\nservice discovery 80–85\noverview of 81\nusing application-level service discovery \npatterns 81–83\nusing platform-provided service discovery \npatterns 83–85\nreply channel header 88–89\nRepository object, DDD 152\nrequest attribute 10\nrequest logging 262\nrequest/async response-style API 90\nrequest/response interactions 87–89\nasynchronous 87–88\nintegration tests for REST-based 322–326\n \n",
      "content_length": 2949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 517,
      "content": "INDEX\n487\nRequestHandler interface 417\nreread value countermeasure 131\nResource Server concept 358\nREST 73–76\nbenefits and drawbacks of 75–76\nfetching multiple resources in single \nrequest 74–75\nmapping operations to HTTP verbs 75\nREST maturity model 74\nspecifying REST APIs 74\nRest Assured Mock MVC 314\nRestaurant domain events 458–459\nRestaurant Service\ncreating services 404–405\ndeploying 402–405\ndesign of 419–423\nAbstractAutowiringHttpRequestHandler \nclass 423\nAbstractHttpHandler class 423\nFindRestaurantRequestHandler class\n421–422\nREST-based request/response style interactions, \nintegration tests for 322–326\nexample contract 324\ntests for API gateway OrderServiceProxy\n325–326\ntests for Order Service 324–325\nRESTful services 419–426\ndeploying lambda functions using Serverless \nframework 425–426\ndesign of Restaurant Service 419–423\npackaging service as ZIP file 424\nretriable transactions 117, 129, 450\nrevise() method 179\nS\nSaas (Software-as-a-Service) 5\nsaga orchestration package 140\nSaga pattern 26\nSagaOrchestratorCreated event 216\nSagaOrchestratorUpdated event 216\nSagaReplyRequested pseudo event 213\nsagas 17, 58, 106, 110–145, 209–218, 450\ncoordinating 117–125\nchoreography-based sagas 118–121\norchestration-based sagas 121–125\nCreate Order saga 135–142\nCreateOrderSaga orchestrator 136–138\nCreateOrderSagaState class 138\nEventuate Tram Saga framework 140–142\nKitchenServiceProxy class 139\ncreating orchestration-based saga 211–212\nwith a NoSQL-based event store 211–212\nwith RDBMS-based event store 211\nimplementing choreography-based sagas using \nevent sourcing 210\nimplementing event sourcing-based saga \nparticipant 213–216\nimplementing saga orchestrators using event \nsourcing 216–218\npersisting using event sourcing 216\nprocessing replies exactly once 218\nsending command messages reliably\n216–218\nlack of isolation 126–132\nanomalies caused by 127\ncountermeasures for handling 128–132\nOrder Service\nOrderCommandHandlers class 142–143\nOrderService class 133–134\nOrderServiceConfiguration class 143–145\ntransaction management 111–117\nmaintaining data consistency 114–117\nneed for distributed transactions 112\ntrouble with distributed transactions 112–114\nunit tests for 310–312\nSATURN conference 34\nsave() method 207\nscalability 430\nscale cube 8–11\nX-axis scaling 9\nY-axis scaling 10–11\nZ-axis scaling 9–10\nsecure services 349–360\nauthentication in API gateway 354–355\nauthorization 356\nin traditional monolithic application 350–353\nusing JWTs to pass user identity and roles\n356–357\nusing OAuth 2.0 357–360\nsecurity patterns 28–29\nAccess token 28, 38, 354\nSELECT statements 188\nSelf registration pattern 82\nsemantic lock 450\nsemantic lock countermeasure 129–130\nsending port interface 86\nServerless deployment with lambda 415–419\nbenefits of lambda functions 418\ndeveloping lambda functions 417\ndrawbacks of lambda functions 419\ninvoking lambda functions 417–418\ndefining scheduled lambda functions\n418\nhandling events generated by AWS \nservices 418\nhandling HTTP requests 417\nusing web service request 418\noverview of 416\n \n",
      "content_length": 3050,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 518,
      "content": "INDEX\n488\nServerless framework 425–426\nserver-side discovery pattern 84–85\nservice API definition 61–64\nassigning system operations to services\n61–62\ndetermining APIs required to support \ncollaboration between services\n62–64\nService as a container pattern 393–399\nbenefits of 398\nDocker 395–398\nbuilding Docker images 395–396\npushing Docker images to registry\n396–397\nrunning Docker containers 397–398\ndrawbacks of 399\nService as a virtual machine pattern\n390–393\nbenefits of 392\nmature cloud infrastructure 392\nservice instances are isolated 392\nVM image encapsulates technology \nstack 392\ndrawbacks of 392–393\nless-efficient resource utilization 393\nrelatively slow deployments 393\nsystem administration overhead 393\nservice component test 28, 335\nservice configurability 349\nservice definition 76\nDecompose by business capability pattern\n51–54\ndecomposition 52–54\nidentifying business capabilities 51–52\npurpose of business capabilities 51\nDecompose by sub-domain pattern\n54–55\nservice deployment patterns 26\nservice discovery 80–85\n3rd party registration 84–85, 108\nClient-side discovery 82–83\noverview of 81\nSelf registration 82\nServer-side discovery 84–85\nservice meshes 380–382, 407–415\ndeploying v2 of Consumer Service 414\nIstio 408–412\nrouting production traffic to v2 415\nrouting rules to route to v1 version\n412–413\nrouting test traffic to v2 414\nService object, DDD 152\nservice() method 422\nservice-oriented architecture (SOA)\n13–14\nSES (Simple Email Service) 2\nSessionBasedSecurityInterceptor 352\nsessions 351\nsetUp() method 313\nsharded channel 94\nShiro 351\nSidecar pattern 410\nSimple Email Service (SES) 2\nSingle persistence layer 38\nSingle presentation layer 38\nSingle Responsibility Principle (SRP) 56\nsmart pipes 14\nsnapshots 195–196, 201\nSOA (service-oriented architecture) 13\nsociable unit test 308\nsoftware architecture 34–37\n4+1 view model of 35–37\ndefinition of 35\nrelevance of 37\nsoftware pattern 20\nSoftware-as-a-Service (SaaS) 5\nsolitary unit test 308\nSoundCloud 265\nspecialization pattern 22\nSpring Cloud Contract 303–305\nSpring Cloud Gateway 273–275\nApiGatewayApplication class 279\nOrderConfiguration class 275–276\nOrderHandlers class 276–278\nOrderService class 278–279\nSpring Mock MVC 314\nSpring Security 351\nSPRING_APPLICATION_JSON variable\n363\nSQL 237–238\nSRP (Single Responsibility Principle) 56\nstate machines\nmodeling saga orchestrators as\n123–124\nOrder aggregate 176–177\nStrangler Application pattern 431–432\nStrategy pattern 20\nstubs 296, 339–340\nsuccessor pattern 21\nSUT (system under test) 294\nsynchronous I/O model 268\nsynchronous interactions 67\nsystem operations 45\nassigning to services 61–62\ncreating high-level domain model 46–48\ndefining 48–50\nidentifying 45–50\nsystem under test (SUT) 294\nSystem.getenv() method 362\n \n",
      "content_length": 2763,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 519,
      "content": "INDEX\n489\nT\ntelemetry 409\ntest cases 294\ntest double 296\ntest pyramid 298–299\ntest quadrant 297–298\n@Test shouldCalculateTotal() method 309\n@Test shouldCreateOrder() method 312\ntest suites 294\ntesting 292–347\nacceptance tests 335–338\ndefining 336\nwriting using Gherkin 337–338\nchallenge of 299–305\nconsumer contract testing 301–303\nconsumer contract testing for messaging \nAPIs 305\nSpring Cloud Contract 303–305\ncomponent tests 339–340\nfor FTGO Order Service 340–345\nin-process component tests 339\nout-of-process component tests\n339–340\nConsumer-driven contract test 28, \n301–302\nConsumer-side contract test 28, 303\ndeployment pipeline 305–307\nend-to-end tests 345–346\ndesigning 345\nrunning 346\nwriting 346\nintegration tests 319–335\ncontract tests for asynchronous request/\nresponse interactions 330–335\npersistence integration tests 321–322\npublish/subscribe-style interactions\n326–330\nREST-based request/response style \ninteractions 322–326\noverview of 294–299\nautomated tests 295–296\ndifferent types of tests 297\nmocks and stubs 296\ntest pyramid 298–299\ntest quadrant 297–298\nService component test 28, 335\nunit tests 307–317\nfor controllers 313–315\nfor domain services 312–313\nfor entities 309–310\nfor event and message handlers 315–317\nfor sagas 310–312\nfor value objects 310\ntestuser header 414\ntext-based message formats 71–72\nTicket aggregate 169–173\nbehavior of 170–171\nKitchenService domain service 171–172\nKitchenServiceCommandHandler class\n172–173\nstructure of Ticket class 170\ntight coupling 121\ntimeouts 79\nTLS (Transport Layer Security) 350\ntokens 356\nTraefik 272\ntraffic management 408\ntransaction log tailing 99–100, 195\ntransaction management 111–117\nmaintaining data consistency 114–117\nneed for distributed transactions 112\ntrouble with distributed transactions\n112–114\nSee also sagas\nTransaction script pattern 149–150\n@Transactional annotation 111\ntransactional messaging 97–100\nPolling publisher pattern 98–99\nTransaction log tailing pattern 99–100\nTransactional outbox pattern 97–98, 109\nusing database table as message queue\n97–98\ntransparent tokens 356\nTransport Layer Security (TLS) 350\ntwo-phase commit (2PC) 112\nU\nUbiquitous Language 54\nunit tests 307–317\nfor controllers 313–315\nfor domain services 312–313\nfor entities 309–310\nfor event and message handlers 315–317\nfor sagas 310–312\nfor value objects 310\nupcasting 199\nUPDATE statement 193\nupdate() method 204, 207, 215\nUpdateItem() operation 248\nUSERINFO cookie\nLoginHandler and 454–455\nmapping to Authorization header 455\nV\nValue object, DDD 151\nvalue objects, unit tests for 310\nversion file countermeasure 131\n \n",
      "content_length": 2598,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 520,
      "content": "INDEX\n490\nVIP (virtual IP) address 83\nVirtualService 413\nVMs (virtual machines) 26\nW\nWAR (Web Application Archive) file 2\nWebSockets 257\nX\nXML message 71\nZ\nZeroMQ 91\nZipkin 373\n \n",
      "content_length": 179,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 521,
      "content": "Enables\nEnables\nArchitecture:\nMicroservice\narchitecture\nOrganization:\nSmall, autonomous,\ncross-functional teams\nProcess:\nDevOps/continuous delivery/deployment\nEnables\nRapid, frequent,\nand reliable delivery\nof software\nThe rapid, frequent, and reliable delivery of large, complex applications requires \na combination of DevOps, which includes continuous delivery/deployment, small, \nautonomous teams, and the microservice architecture.\nSmall, autonomous,\nloosely coupled teams\nEach service has\nits own source\ncode repository.\nEach service has\nits own automated\ndeployment pipeline.\nSmall, simple,\nreliable, easy to\nmaintain services\nOrder management team\nRestaurant management team\nDelivery management team\nFTGO development\nProduction\nJenkins Cl\nDeployment pipeline\nOrder Service\nsource code\nrepository\nOrder Service\nJenkins Cl\nDeployment pipeline\nRestaurant Service\nsource code\nrepository\nRestaurant Service\nJenkins Cl\nDeployment pipeline\nDelivery Service\nsource code\nrepository\nDelivery Service\nThe microservice architecture structures an application as a set of loosely coupled services that are \norganized around business capabilities. Each team develops, tests, and deploys their services \nindependently.\n \n",
      "content_length": 1211,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 522,
      "content": "Chris Richardson\nS\nuccessfully developing microservices-based applications \nrequires mastering a new set of architectural insights and \npractices. In this unique book, microservice architecture \npioneer and Java Champion Chris Richardson collects, cata-\nlogues, and explains 44 patterns that solve problems such as \nservice decomposition, transaction management, querying, \nand inter-service communication.\nMicroservices Patterns teaches you how to develop and deploy \nproduction-quality microservices-based applications. This \ninvaluable set of design patterns builds on decades of dis-\ntributed system experience, adding new patterns for writing \nservices and composing them into systems that scale and \nperform reliably under real-world conditions. More than just \na patterns catalog, this practical guide offers experience-driven \nadvice to help you design, implement, test, and deploy your \nmicroservices-based application. \nWhat’s Inside\n● How (and why!) to use the microservice architecture\n● Service decomposition strategies\n● Transaction management and querying patterns\n● Effective testing strategies\n● Deployment patterns including containers and serverless\nWritten for enterprise developers familiar with standard enter-\nprise application architecture. Examples are in Java.\nChris Richardson is a Java Champion, a JavaOne rock star, \nauthor of Manning’s POJOs in Action, and the creator of the \noriginal CloudFoundry.com.\nTo download their free eBook in PDF, ePub, and Kindle formats, owners \nof this book should visit manning.com/books/microservices-patterns\n$49.99 / Can $65.99  [INCLUDING eBOOK]\nMicroservices Patterns\nSOFTWARE DEVELOPMENT\nM A N N I N G\n“\nA comprehensive overview \nof the challenges teams face \nwhen moving to microservices, \nwith industry-tested solutions \nto these problems.”\n \n—Tim Moore, Lightbend\n“\nPragmatic treatment of \nan important new \n architectural landscape.”\n \n—Simeon Leyzerzon\nExcelsior Software\n“\nA solid compendium of \ninformation that will quicken \nyour migration to this modern \ncloud-based architecture.”\n—John Guthrie, Dell/EMC \n“\nHow to understand the \nmicroservices approach, and \nhow to use it in real life.”\n \n—Potito Coluccelli\nBizmatica Econocom\nSee first page\n",
      "content_length": 2221,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}