{
  "metadata": {
    "title": "Managing Risk and Information Security",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 208,
    "conversion_date": "2025-12-25T18:14:38.867309",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Managing Risk and Information Security.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 1-10)",
      "start_page": 1,
      "end_page": 10,
      "detection_method": "topic_boundary",
      "content": "Managing Risk \nand Information \nSecurity\nProtect to Enable\n—\nSecond Edition\n—\nMalcolm W Harkins \n\n\n Managing Risk and \nInformation Security \n Protect to Enable  \n Second Edition \n Malcolm W. Harkins\n \n\n\nManaging Risk and Information Security: Protect to Enable\nMalcolm W. Harkins \n \n \n \nFolsom, California, USA  \n \nISBN-13 (pbk): 978-1-4842-1456-5  \nISBN-13 (electronic): 978-1-4842-1455-8\nDOI 10.1007/978-1-4842-1455-8\nLibrary of Congress Control Number: 2016949414\nCopyright © 2016 by Malcolm W. Harkins\nApressOpen Rights: You have the right to copy, use and distribute this Work in its entirety, electronically without \nmodification, for non-commercial purposes only. However, you have the additional right to use or alter any source \ncode in this Work for any commercial or non-commercial purpose which must be accompanied by the licenses in \n(2) and (3) below to distribute the source code for instances of greater than 5 lines of code. Licenses (1), (2) and (3) \nbelow and the intervening text must be provided in any use of the text of the Work and fully describes the license \ngranted herein to the Work.\n(1) License for Distribution of the Work: This Work is copyrighted by Malcolm Harkins, all rights reserved. Use \nof this Work other than as provided for in this license is prohibited. By exercising any of the rights herein, you \nare accepting the terms of this license. You have the non-exclusive right to copy, use and distribute this English \nlanguage Work in its entirety, electronically without modification except for those modifications necessary for \nformatting on specific devices, for all non-commercial purposes, in all media and formats known now or hereafter. \nWhile the advice and information in this Work are believed to be true and accurate at the date of publication, \nneither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions \nthat may be made. The publisher makes no warranty, express or implied, with respect to the material contained \nherein.\nIf your distribution is solely Apress source code or uses Apress source code intact, the following licenses (2) and (3) \nmust accompany the source code. If your use is an adaptation of the source code provided by Apress in this Work, \nthen you must use only license (3).\n(2) License for Direct Reproduction of Apress Source Code: This source code, from Intel® Trusted Execution \nTechnology for Server Platforms, ISBN 978-1-4302-6148-3 is copyrighted by Apress Media, LLC, all rights reserved. \nAny direct reproduction of this Apress source code is permitted but must contain this license. The following license \nmust be provided for any use of the source code from this product of greater than 5 lines wherein the code is \nadapted or altered from its original Apress form. This Apress code is presented AS IS and Apress makes no claims \nto, representations or warrantees as to the function, usability, accuracy or usefulness of this code.\n(3) License for Distribution of Adaptation of Apress Source Code: Portions of the source code provided are used \nor adapted from Intel® Trusted Execution Technology for Server Platforms, ISBN 978-1-4302-6148-3 copyright \nApress Media LLC. Any use or reuse of this Apress source code must contain this License. This Apress code is made \navailable at Apress.com/9781484214565 as is and Apress makes no claims to, representations or warrantees as to \nthe function, usability, accuracy or usefulness of this code.\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with every \noccurrence of a trademarked name, logo, or image we use the names, logos, and images only in an editorial fashion \nand to the benefit of the trademark owner, with no intention of infringement of the trademark. The use in this \npublication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is \nnot to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.\nWhile the advice and information in this book are believed to be true and accurate at the date of publication, \nneither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions \nthat may be made. The publisher makes no warranty, express or implied, with respect to the material contained \nherein.\nCover image designed by Freepik.\nManaging Director: Welmoed Spahr\nLead Editor: Robert Hutchinson\nDevelopment Editor: James Markham\nEditorial Board: Steve Anglin, Pramila Balen, Aaron Black, Louise Corrigan, Jonathan Gennick, Robert \nHutchinson, Celestin Suresh John, Nikhil Karkal, James Markham, Susan McDermott, Matthew Moodie, \nNatalie Pao, Gwenan Spearing\nCoordinating Editor: Melissa Maldonado\nCopy Editor: Mary Behr\nCompositor: SPi Global\nIndexer: SPi Global\nArtist: SPi Global\nDistributed to the book trade worldwide by Springer Science+Business Media New York, \n233 Spring Street, 6th Floor, New York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505, e-mail  \n orders-ny@springer-sbm.com   , or visit  www.springer.com   . Apress Media, LLC is a California LLC \nand the sole member (owner) is Springer Science + Business Media Finance Inc (SSBM Finance Inc). \nSSBM Finance Inc is a Delaware corporation. \nFor information on translations, please e-mail  rights@apress.com   , or visit  www.apress.com   . \nApress and friends of ED books may be purchased in bulk for academic, corporate, or promotional use. eBook \nversions and licenses are also available for most titles. For more information, reference our Special Bulk Sales–eBook \nLicensing web page at  www.apress.com/bulk-sales   .\nAny source code or other supplementary materials referenced by the author in this text is available \nto readers at  www.apress.com   . For detailed information about how to locate your book’s source code, go to  www.\napress.com/source-code/   .\nPrinted on acid-free paper\n\n\niii\n About ApressOpen \n What Is ApressOpen? \n• \n ApressOpen is an open access book program that publishes \nhigh-quality technical and business information. \n• \n ApressOpen eBooks are available for global, free, \nnoncommercial use. \n• \n ApressOpen eBooks are available in PDF, ePub, and Mobi formats. \n• \n The user friendly ApressOpen free eBook license is presented on \nthe copyright page of this book. \n\n\n This book is dedicated to my family. \n\n\nvii\nContents at a Glance\nForeword ......................................................................................... xv\nPraise for the second edition of Managing Risk and \nInformation Security ...................................................................... xvii\nAbout the Author ............................................................................ xxi\nAcknowledgments ........................................................................ xxiii\nPreface ...........................................................................................xxv\n \n■Chapter 1: Introduction .................................................................. 1\n \n■Chapter 2: The Misperception of Risk .......................................... 17\n \n■ Chapter 3: Governance and Internal Partnerships: \nHow to Sense, Interpret, and Act on Risk ..................................... 31\n \n■ Chapter 4: External Partnerships: The Power of Sharing \nInformation ................................................................................... 49\n \n■Chapter 5: People Are the Perimeter ............................................ 65\n \n■ Chapter 6: Emerging Threats and Vulnerabilities: \nReality and Rhetoric ..................................................................... 81\n \n■ Chapter 7: A New Security Architecture to Improve \nBusiness Agility ............................................................................ 99\n \n■ Chapter 8: Looking to the Future: Emerging \nSecurity Capabilities .................................................................. 117\n\n\n■ CONTENTS AT A GLANCE\nviii\n \n■ Chapter 9: Corporate Social Responsibility: The Ethics of \nManaging Information Risk ........................................................ 129\n \n■Chapter 10: The 21st Century CISO ............................................ 139\n \n■Chapter 11: Performance Coaching ............................................ 155\n \n■Appendix A: References .............................................................. 171\nIndex .............................................................................................. 181\n\n\nix\nContents\nForeword ......................................................................................... xv\nPraise for the second edition of Managing Risk and \nInformation Security ...................................................................... xvii\nAbout the Author ............................................................................ xxi\nAcknowledgments ........................................................................ xxiii\nPreface ...........................................................................................xxv\n \n■Chapter 1: Introduction .................................................................. 1\nProtect to Enable® ................................................................................... 5\nBuilding Trust ............................................................................................................8\nKeeping the Company Legal: The Regulatory Flood .................................................8\nThe Rapid Proliferation of Information, Devices, and Things ..................................12\nThe Changing Threat Landscape ............................................................................13\nA New Approach to Managing Risk ........................................................................16\n \n■Chapter 2: The Misperception of Risk .......................................... 17\nThe Subjectivity of Risk Perception ....................................................... 18\nHow Employees Misperceive Risk ......................................................... 18\nThe Lure of the Shiny Bauble..................................................................................20\nHow Security Professionals Misperceive Risk ...................................... 20\nSecurity and Privacy ...............................................................................................22\nHow Decision Makers Misperceive Risk ............................................... 23\n\n\n■ CONTENTS\nx\nHow to Mitigate the Misperception of Risk ........................................... 24\nUncovering New Perspectives During Risk Assessments .......................................25\nCommunication Is Essential .................................................................. 26\nBuilding Credibility .................................................................................................28\n \n■ Chapter 3: Governance and Internal Partnerships: \nHow to Sense, Interpret, and Act on Risk ..................................... 31\nInformation Risk Governance ................................................................ 32\nFinding the Right Governance Structure ............................................... 34\nBuilding Internal Partnerships ............................................................... 37\nLegal .......................................................................................................................38\nHuman Resources ..................................................................................................42\nFinance ...................................................................................................................43\nCorporate Risk Management ..................................................................................44\nPrivacy ....................................................................................................................45\nCorporate Security ..................................................................................................45\nBusiness Group Managers ......................................................................................46\nConclusion ............................................................................................. 47\n \n■ Chapter 4: External Partnerships: The Power of Sharing \nInformation ................................................................................... 49\nThe Value of External Partnerships ....................................................... 51\nExternal Partnerships: Types and Tiers .................................................. 52\n1:1 Partnerships .....................................................................................................55\nCommunities ...........................................................................................................57\nCommunity Characteristics ....................................................................................57\nCommunity Goals ....................................................................................................59\nSharing Information about Threats and Vulnerabilities ...........................................59\nSharing Best Practices and Benchmarking ............................................................60\n\n\n■ CONTENTS\nxi\nInﬂ uencing Regulations and Standards ..................................................................62\nCorporate Citizenship .............................................................................................63\nConclusion ............................................................................................. 63\n \n■Chapter 5: People Are the Perimeter ............................................ 65\nThe Shifting Perimeter .......................................................................... 65\nCompliance or Commitment? ................................................................ 66\nExamining the Risks .............................................................................. 68\nAdjusting Behavior ................................................................................ 69\nA Model for Improving Security Awareness .......................................... 71\nBroadening the Awareness Model ......................................................... 74\nThe Security Beneﬁ ts of Personal Use .................................................. 74\nRoundabouts and Stop Signs ................................................................ 75\nThe Technology Professional ................................................................. 77\nInsider Threats....................................................................................... 78\nDeter .......................................................................................................................79\nDetect .....................................................................................................................79\nDiscipline ................................................................................................................80\nFinding the Balance ............................................................................... 80\n \n■ Chapter 6: Emerging Threats and Vulnerabilities: \nReality and Rhetoric ..................................................................... 81\nStructured Methods for Identifying Threat Trends ................................. 82\nThe Product Life Cycle Model .................................................................................83\nUnderstanding Threat Agents .................................................................................88\nPlaying War Games .................................................................................................90\nTrends That Span the Threat Landscape ............................................... 91\nTrust Is an Attack Surface .......................................................................................91\nBarriers to Entry Are Crumbling ..............................................................................92\n",
      "page_number": 1
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 11-21)",
      "start_page": 11,
      "end_page": 21,
      "detection_method": "topic_boundary",
      "content": "■ CONTENTS\nxii\nThe Rise of Edge Case Insecurity ...........................................................................92\nThe Enemy Knows the System ...............................................................................93\nKey Threat Activity Areas ....................................................................... 94\nThe Industry of Malware .........................................................................................94\nThe Web Expands to the Internet of Things ........................................... 94\nSmartphones ......................................................................................... 96\nWeb Applications ....................................................................................................97\nConclusion ............................................................................................. 97\n \n■ Chapter 7: A New Security Architecture to Improve \nBusiness Agility ............................................................................ 99\nThe 9 Box of Controls, Business Trends, and \nArchitecture Requirements ................................................................. 101\n9 Box of Controls ..................................................................................................101\nIT Consumerization ...............................................................................................102\nNew Business Needs ............................................................................................103\nCloud Computing ..................................................................................................104\nChanging Threat Landscape .................................................................................104\nPrivacy and Regulatory Requirements..................................................................105\nNew Architecture ................................................................................. 105\nTrust Calculation ...................................................................................................106\nSecurity Zones ......................................................................................................109\nBalanced Controls.................................................................................................113\nUsers, Data, and the Internet of Things: The New Perimeters ..............................115\nConclusion ........................................................................................... 116\n \n■Chapter 8: Looking to the Future: Emerging \nSecurity Capabilities ...................................................................... 117\nInternet of Things ................................................................................ 120\nConsistent User Experience Across Devices ....................................... 121\n\n\n■ CONTENTS\nxiii\nCloud Computing ................................................................................. 122\nBig Data Analytics ............................................................................... 122\nArtiﬁ cial Intelligence ........................................................................... 122\nBusiness Beneﬁ ts and Risks ............................................................... 123\nNew Security Capabilities .....................................................................................123\nBaseline Security ..................................................................................................124\nContext-Aware Security ........................................................................................126\nConclusion ........................................................................................... 127\n \n■ Chapter 9: Corporate Social Responsibility: The Ethics of \nManaging Information Risk ........................................................ 129\nThe Expanding Scope of Corporate Social Responsibility ................... 130\nThe Evolution of Technology and Its Impact ........................................ 132\nMaintaining Society’s Trust ................................................................. 134\nThe Ethics of Managing Information Risk ........................................... 135\nConclusion ........................................................................................... 137\n \n■Chapter 10: The 21st Century CISO ............................................ 139\nChief Trust Ofﬁ cer ................................................................................ 139\nThe Z-Shaped Individual...................................................................... 141\nFoundational Skills .............................................................................. 142\nBecoming a Storyteller ........................................................................ 143\nFear Is Junk Food ................................................................................ 144\nAccentuating the Positive .....................................................................................145\nDemonstrating the Reality of Risk ....................................................... 146\nThe CISO’s Sixth Sense ....................................................................... 147\nTaking Action at the Speed of Trust ......................................................................148\nThe CISO as a Leader .......................................................................... 148\nLearning from Other Business Leaders ................................................................149\n\n\n■ CONTENTS\nxiv\nVoicing Our Values ................................................................................................150\nDiscussing Information Risk at Board Level .........................................................151\nConclusion ........................................................................................... 153\n \n■Chapter 11: Performance Coaching ............................................ 155\nHow to Use the Tables ......................................................................... 156\nIndependence and Initiative .................................................................................157\nEfﬁ ciency and Effectiveness .................................................................................158\nCommitment .........................................................................................................160\nProfessionalism ....................................................................................................161\nDiscipline  .............................................................................................................161\nTeamwork .............................................................................................................162\nProblem-Solving ...................................................................................................163\nCommunication .....................................................................................................164\nGoal-Setting ..........................................................................................................168\nConclusion ........................................................................................... 169\n \n■Appendix A: References .............................................................. 171\nIndex .............................................................................................. 181\n\n\nxv\n Foreword \n Security and first-person shooter video games have one obvious thing in common: if \nyou’re not continuously moving, you’re dead. In this second edition of  Managing Risk \nand Information Security , Malcolm Harkins helps us move our thinking into areas of risk \nthat have become more prominent over the last several years. \n Because there is so much new content in this edition, I will focus on a topic that has \nrisen to greater prominence since the first edition: people are the perimeter. When we \nreflect on what has changed in recent years, with an eye to the vulnerabilities that result \nin real-world compromises, a pattern emerges: virtually all the major breaches that we \nhave seen involve manipulation of people. When nearly everyone has heard of phishing, \nwe have to ask ourselves: why is it still such an effective tool? \n The obvious theory is that we haven’t managed people risk as well as we should. \nPerhaps we have been standing still and need to learn how to dodge and experiment \nwith the way we drive better people-security outcomes. Unfortunately, the path is not \n100% clear. Unlike technology, the field of influencing human behavior in security is \nremarkably complicated and supported by limited research. \n Malcolm provides us with a great foundation and framework to build our \n“security engagement” functions. I like to use the word “engagement” because it \nspeaks to how the security organization relates to the workforce in a manner that isn’t \nsimply bounded by the more traditional term “training and awareness.” Engagement \nencompasses anything that shifts the desired behavior outcome in the direction we want \nit to go. I have seen remarkable shifts in measured behavior from the use of \nnon-traditional tools such as security gamification and simulation. \n The way Malcolm differentiates between “compliance” and “commitment” is key. \n Managing Risk and Information Security is an ever-evolving classic in the field of security \nmanagement.\n —Patrick Heim \n Head of Trust & Security, Dropbox \n\n\nxvii\n Praise for the second edition \nof Managing Risk and \nInformation Security \n We assign Malcolm’s book to our Carnegie Mellon CISO-Executive \nProgram students on their first day of class. It is relevant, pragmatic, and \nsolution oriented. Our adversaries are changing their practices and so \nmust we. Malcolm’s book is a terrific tool for the modern-day info sec \nleader who wants to shift from security as a restriction to security as a \nbusiness enabler. \n —Andy Wasser \n Associate Dean, CMU Heinz College \n Malcolm is a top-notch executive, security leader, and innovator, with \na keen ability to convey thought-provoking and valuable insights. His \nlatest effort demonstrates remarkable foresight into the skills necessary \nto excel as a security leader today and tomorrow. \n —Clayton J. Pummill \n Executive Director, Security Advisor Alliance \n I could go on and on about what I liked specifically—there was \nmuch, including the discussion about governance models and social \nresponsibility—but here is the net: this is the first time I’ve seen \nsomeone be able to speak to security specifics while also raising the \nconversation to a much higher level. It begins to take on an Alvin Toffler \nfeel from his astounding book, The Third Wave . Malcolm’s thoughts are \nphilosophically sweeping while at the same time imminently practical. \n —Todd Ruback, Esq., CIPP-US/E, CIPT \n Chief Privacy & Security Officer & V.P. Legal Affairs, Ghostery \n\n\n ■ PRAISE FOR THE SECOND EDITION OF MANAGING RISK AND INFORMATION SECURITY\nxviii\n Malcolm Harkins is a foremost expert at managing risk and information \nsecurity. In this latest book, he further expands his Protect to Enable \nphilosophy and does so in a way that offers practical and actionable \ninitiatives that any risk manager or CISO can implement to protect their \nenterprise while enabling business growth. A must-read for CISOs and \ntheir teams! \n —Tim Rahschulte, Ph.D. \n Chief Learning Officer & Content Officer, Evanta \n Malcolm Harkins is a visionary thought leader on cyber security and risk \nmanagement. Managing Risk and Information Security  is a must read. \nMalcolm helps readers immediately take the information and apply it to \ntheir own organizations. You will find that this book cuts through the fog \nand provides a clear picture of where and what to focus on to effectively \nmanage cyber business risk. \n —Phil Ferraro \n Global CISO and Cyber Security Consultant \n The CISO is more than just a technology expert; she must be savvy \nabout leadership, influence, and change across complex organizations; \nsomeone who sees her mission not to just drive implementation of a \nlarge system, but to foster sustainable culture change at every level. As \nan organizational psychologist, I recognize Harkins’ keen eye for group \ndynamics and leadership tactics that enable CISOs to enhance enterprise \nsecurity. He puts his finger on the habits, assumptions, and decision \nprocesses typical of many employees and teams, as they unknowingly \nincrease security risk, and for that alone this book is a gem. It should be \nrequired reading for aspiring CISOs and for anyone who has a role in the \nrecruitment and hiring of CISOs. \n —Marc Sokol, PhD \n Executive Editor, People + Strategy \n Malcolm Harkins’ take on information security and risk is a refreshing \nchange from the increasingly frequent alarm bells raised in the press \nwith regard to the “brave new world” where technology is presented as \nan ever-escalating conflict between our seemingly insatiable appetite for \nconnectivity, cool applications, and customized information, on the one \nhand, and a desire to control who has our information and how they may \nuse it, on the other. Harkins instead offers a cool, clear-eyed perspective \nwhere managing information and risk are placed in a wider context. His \nprescriptions and frameworks are recipes for well-managed organizations \nin the broadest sense. They allow us to embrace our new-found \n\n\n ■ PRAISE FOR THE SECOND EDITION OF MANAGING RISK AND INFORMATION SECURITY\nxix\ntechnological abilities without fear because we have defined their purpose \ncapaciously enough to be a positive good, to be of service to all a company’s \nstakeholders. That is, once we set a truly human course, technology serves \nrather than threatens us. Organization purpose, when defined in this way, \nis an expression of our values and is empowered by that fuel. Harkins’ book \nis a practical as well as purposeful guide to a values-driven implementation \nof information technology. \n —Mary C. Gentile, PhD \n Author of  Giving Voice To Values: How To Speak Your Mind \nWhen You Know What’s Right (Yale University Press) \n In today’s rapidly evolving security landscape, security professionals are \nnavigating a complex set of dynamics across the enterprise. In Managing \nRisk and Information Security , Malcolm Harkins draws on his rich \nsecurity experience to present a connected view of where companies \nshould be focused. He puts forth a valuable perspective, as organizations \naround the world look to create a necessary balance of protection and \ninnovation, which ultimately enables business success. \n —Bret Arsenault \n Corporate Vice President and CISO, Microsoft Corporation \n Malcolm generously shares through personal experiences and story \ntelling the formula for a successful 21st century CISO. It is one part \nmulti-disciplinary leader and one part trusted advisor to the business, \ncombined with behavioral models required for balanced risk decision \nmaking. A must-read for all new CISOs. Malcolm lives his beliefs. \n —Nasrin Rezai \n GE Corporate Security & Compliance Officer \n In the second edition of his book, Malcolm seamlessly articulates the \nfuture horizon of cyber security and the critical role that the CISO and \nsecurity professionals will need to fulfill in order to defend both the \ncompany and consumers they serve. The guidance he provides into the \nskills, leadership, and approach required for successfully navigating \nthe emerging challenges of securing a digital economy is invaluable. \nRegardless of your current role, this is a must-read for everyone who has \naccepted this great responsibility and privilege. \n —Steven Young \n CISO, Kellogg Company \n\n\n ■ PRAISE FOR THE SECOND EDITION OF MANAGING RISK AND INFORMATION SECURITY\nxx\n While other security officers are looking to the traditional or the latest \n“cool” product, Harkins goes against the tide and asks the questions that \nneed addressing. His forward-thinking mindset and Protect to Enable \napproach inspire others to innovate and go beyond the mainstream. \nIf you cannot bring Harkins to your company for mentoring, this book \nwill at least spark thought and will change how your engineers view \nsecurity within the business. \n —Charles Lebo \n Vice President and CISO, Kindred Healthcare \n Malcolm’s vast experience makes him one of the most credible security \nleaders on the international stage and serves as the perfect platform for \nthis book. Rational, compelling, and authoritative writing is far too rare \nin the world of risk and information security, but Malcolm completely \nnails it in Managing Risk and Information Security  with invaluable \nadvice and recommendations for anyone planning a future in the \nsecurity world. His extensive experience in business before becoming \na CISO is one of the missing ingredients in many security executives’ \nprofessional toolbox, which is which is why this is such an important \nbook. Make sure to keep a highlighter and notepad handy because there \nare a lot of nuggets in here you’ll want to remember on your journey to \nbecoming a better security professional. \n —Mark Weatherford \n Chief Cybersecurity Strategist at vArmour and \nformer Deputy Under Secretary for Cybersecurity \nat the US Department of Homeland Security \n I’ve had the privilege of working with many talented CISOs over the \nyears and Malcolm is one of the best. His logical, methodical approach \nto solving the most complex cybersecurity problems is reflected in his \nlucid style. An enlightened approach to understanding risk that unites \nall stakeholders and a systemic intelligence-based approach to security \ninfrastructure are the only ways to reduce the threat to manageable \nlevels. This is our best path forward if we are ever to realize the vast \npotential of the innovative digital world we are creating. In Managing \nRisk and Information Security , Malcolm shines a light on that path in a \ncomprehensive yet very readable way. \n —Art Coviello \n Former CEO and Executive Chairman, RSA \n \n\n\nxxi\n About the Author \n Malcolm  Harkins  is the Chief Security and Trust Officer \n(CSTO) at Cylance Inc. In this role, he reports to the CEO \nand is responsible for enabling business growth through \ntrusted infrastructure, systems, and business processes. \nHe has direct organizational responsibility for information \ntechnology, information risk, and security, as well as \nsecurity and privacy policy. Malcolm is also responsible \nfor peer outreach activities to drive improvement across \nthe world in the understanding of cyber risks and best \npractices to manage and mitigate those risks. \n Previously, Malcolm was Vice President and \nChief Security and Privacy Officer (CSPO) at Intel \nCorporation. In that role, Malcolm was responsible \nfor managing the risk, controls, privacy, security, and \nother related compliance activities for all of Intel’s \ninformation assets, products, and services. \n Before becoming Intel’s first CSPO, he was \nthe Chief Information Security Officer (CISO) \nreporting into the Chief Information Officer. Malcolm also held roles in finance, \nprocurement, and various business operations. He has managed IT benchmarking and \nSarbanes-Oxley–compliance initiatives. Harkins acted as the profit and loss manager for \nthe Flash Product Group at Intel; was the general manager of Enterprise Capabilities, \nresponsible for the delivery and support of Intel’s Finance and HR systems; and worked in \nan Intel business venture focusing on e-commerce hosting. \n Malcolm previously taught at the CIO Institute at the UCLA Anderson School of \nManagement and was an adjunct faculty member at Susquehanna University in 2009. In \n2010, he received the RSA Conference Excellence in the Field of Security Practices Award. \nHe was recognized by Computerworld as one of the Premier 100 Information Technology \nLeaders for 2012. (ISC)2 recognized Malcolm in 2012 with the Information Security \nLeadership Award. In September 2013, Malcolm was recognized as one of the Top 10 \nBreakaway Leaders at the Global CISO Executive Summit. In November 2015, he received \nthe Security Advisor Alliance Excellence in Innovation Award. He is a Fellow with the \nInstitute for Critical Infrastructure Technology, a non-partisan think-tank that provides \ncybersecurity briefings and expert testimony to the U.S. Congress and federal agencies. \nMalcolm is a sought-after speaker for industry events. He has authored many white \n\n\n ■ ABOUT THE AUTHOR\nxxii\npapers and in December 2012 published his first book,  Managing Risk and Information \nSecurity . He also was a contributing author to  Introduction to IT Privacy , published in \n2014 by the International Association of Privacy Professionals. \n Malcolm received his bachelor’s degree in economics from the University of California \nat Irvine and an MBA in finance and accounting from the University of California at Davis. \n  \n\n\nxxiii\n Acknowledgments \n I received valuable feedback from many readers of the first edition of this book. That \nfeedback helped me to expand the book with additional insights, clarifications, and \nupdated examples. It also encouraged me to add two more chapters to the second \nedition: one on corporate social responsibility, and the other on performance coaching. \n Special thanks to Mike Faden: without his help this book would not have happened. \n As I noted in the first edition, many people during my journey at Intel helped me \nlearn and grow. A number of them published material that is still referenced in this \nsecond edition. \n Other experts who have helped me come from a variety of different peer groups. \nThey include members of the Bay Area CSO Council, the Executive Security Action \nForum, the members and staff of CEB and its Information Risk Leadership Council, \nparticipants in the Evanta CISO Executive Summits and the CISO coalition, as well as the \nSecurity Advisor Alliance. \n Finally, I wish to thank Stuart McClure for giving me the opportunity to join Cylance. \n",
      "page_number": 11
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 22-30)",
      "start_page": 22,
      "end_page": 30,
      "detection_method": "topic_boundary",
      "content": "xxv\n Preface \n If you don’t believe in the messenger, you won’t believe the message. \n You can’t believe in the messenger if you don’t know what the messenger \nbelieves. \n You can’t be the messenger until you’re clear about what you believe. \n —James Kouzes and Barry Posner, \n in  The Leadership Challenge \n A great deal has transpired since the first edition of this book was published in January \n2013, both in the world of information risk and in my personal life and career. To briefly \ncover the latter, in January 2013, I was named Intel’s Chief Security and Privacy Officer. \nMy broad role was one of the first of its kind in corporate America: I was charged with \nmanaging and mitigating risk for Intel’s products and services worldwide, in addition to \nIntel’s internal IT environment. In June 2015, I left Intel to become CISO at Cylance Inc., \nand in May 2016, I was named Cylance’s Chief Security and Trust Officer. \n These career changes occurred during an extraordinary period of escalating \ninformation risk, as evidenced by an almost continuous stream of major hacks and \nbreaches, and a corresponding rise in society’s awareness of risk. Some key examples:\n• \n May 2013: Edward Snowden flies to Hong Kong after leaving \nhis job at an NSA facility in Hawaii. The following month, he \nreveals thousands of classified NSA documents. The disclosures, \nincluding previously unknown government surveillance \nprograms, continue to cause worldwide repercussions today. \n• \n December 2013: The blog Krebs On Security reports a massive \ndata breach at Target. The company confirms the breach the next \nday. Within months, Target’s CIO and CEO both resign amid the \nfallout. \n• \n May 2014: A U.S. grand jury indicts five Chinese military officers \non charges of hacking American companies and stealing trade \nsecrets. \n• \n November 2014: Employees at Sony Pictures arrive at work to \ndiscover their network has been hacked. Attackers steal and then \nerase data on thousands of systems, forcing studio employees to \nrevert to using fax machines and pen and paper. The attackers \nthen dump huge batches of confidential business and personal \ninformation online. \n\n\n ■ PREFACE\nxxvi\n• \n March 2015: Google’s Project Zero hacking team demonstrates \nthe ability to exploit a fundamental flaw in DDR3 SDRAM to \nperform privilege escalation attacks on systems containing the \nchips. Some mitigation approaches are available, other than \nreplacing the DDR3 memory in millions of systems worldwide. \n• \n June 2015: The US Office of Personnel Management announces \na data breach targeting the personal data of up to 4 million \npeople. The attack, which includes security clearance-related \ninformation, is one of the largest-ever breaches of government \ndata. By July, the estimated number of stolen records increases to \n21.5 million. \n• \n February 2016: The Hollywood Presbyterian Medical Center in \nLos Angeles says it has paid a bitcoin ransom to attackers who \nheld its systems hostage, encrypting data and blocking access by \nhospital staff. Some believe the healthcare industry is the next \nmajor target for cyber criminals. \n Given this escalating cycle of risk, and the potential catastrophic societal \nimplications of today’s attacks, we must all be ready to be held accountable. This may \nrequire a large mental shift for those used to simply assigning responsibility and blame \nfor a breach to the people who traditionally perform post-attack cleanup: corporate IT \ndepartments, internal information security teams, and investigations and computer \nforensics groups. Everyone, from corporate executives to security practitioners, shares \nresponsibility for security and privacy. We must all step back and contemplate our own \npersonal responsibilities, not only to the organizations we work for and the customers we \nserve, but also to society as a whole. \n The challenge we sometimes face is how to characterize that responsibility. Is our \nresponsibility to limit liability for our organizations? Or is it a duty of care to the people \nwhose information we store? What values are we using when we make decisions about \ncyber risk, and what bias do those values create in our decisions? Are we forward-\nlooking enough, or will the decisions we make to fix our problems today create other \nproblems in the future? As Benjamin Franklin once said, “All human situations have their \ninconveniences. We feel those of the present but neither see nor feel those of the future; \nand hence we often make troublesome changes without amendment, and frequently for \nthe worse.” \n As security and privacy professionals, a key part of our role is to ensure the right \ndialogue and debate occurs. We need to ask “high-contrast” questions that sharply \ndefine the implications of the choices our organizations make. We need to make sure \nthat the opportunities are as clearly defined as the obligations to mitigate risk, so that \nour organizations make the right decisions. And we need to take equal responsibility for \nthe outcomes of those choices, as opposed to abdicating that responsibility solely to the \nbusiness. Once the choice is made, we must transition out of the debate about what is \nright and focus on taking the right actions—on making tomorrow better than today. \n We can think of this as doing what’s right. We can think of it as protecting our \ncustomers and partners and keeping our markets healthy for everyone. No matter what \nmotivates us, thoughtfully building systems to support a culture of genuine responsibility \nfor privacy and security is not only good corporate responsibility; it is also good for \n\n\n ■ PREFACE\nxxvii\nbusiness. For computing to continue to improve the world we live in rather than endanger \nit, it needs to be trustworthy. And for that trust to be deliverable, we need to ensure the \ndata we enter into our computers is both secure and private. As an organization, we \ndemonstrate and build trust through our approach to solving these cyber-risk challenges. \n In the preface of the first edition, I said “ Managing Risk and Information Security is \na journey, but there is no finish line. Our approach to managing information risk must \ncontinue to evolve as rapidly as the pace of business and technology change. My hope is \nthat people will read this book and begin their own journey.” \n I still firmly believe what I said then. But I also believe that, as General George \nMarshall once said, “The only way human beings can win a war is to prevent it.” We \nare at war against adversaries who wish to harm the users of technology. But there is \nalso a battle among those responsible for protecting security and privacy. On one side \nare organizations that would like to continue on the current path because they profit \nfrom the insecurity of computing, or that approach the duty of care with a bias towards \nlimiting liability rather than protecting their customers. On the other side are those who \nbelieve that our role is to generate trust. We do that by protecting to enable people and \nbusinesses. It’s a hard road; I know, because I experience it every day. But we shouldn’t \nback away from something just because it is hard. We need to plant our feet and stand \nfirm. The only question is where we plant our feet. \n\n\n1\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_1\n CHAPTER 1 \n Introduction \n There are two primary choices in life: to accept conditions as they exist, \nor accept the responsibility for changing them . \n —Denis Waitley \n In January 2002, I was hired to run a new Intel internal program called Security and \nBusiness Continuity. The program had been created following the major security events \nof the previous year (9/11 and the Code Red/Nimda viruses) and it focused primarily \non the availability risks at that time. I had no background in technical security, but I \nhad been at Intel for nearly 10 years in a variety of business-related positions, mostly \nin finance. As I learned about information risk during the first few months, it became \napparent to me that the world was starting to change rapidly and that a “perfect storm” \nof risk was beginning to brew. In June 2002, I put together a diagram (Figure  1-1 ) to \nexplain the risks to my manager, Intel’s CIO, and anyone who would listen to me. \nThe diagram has been updated slightly since then to more explicitly highlight the \ngeo-political forces that are a key part of the threat, vulnerability, and regulatory \nrisk landscape.  \n\n\nCHAPTER 1 ■ INTRODUCTION\n2\n Today, it is clear that my view of the world was essentially accurate. Security breaches \nand intrusions are reported almost daily at organizations of all sizes, legal and regulatory \nissues related to technology use continue to grow, and geo-politics have surged to the \nforefront of some of these discussions in a post-Snowden era. Cyber attacks and data \nbreaches are now considered the biggest threats to business continuity, according to a \nrecent survey (Business Continuity Institute 2016). \n But the key question that I asked in the first edition of this book is still valid. Is \ninformation security really effective? Given the rapid evolution of new technologies and \nuses, does the information security group even need to exist? \n Obviously, this is a somewhat rhetorical question. I cannot imagine that any sizeable \norganization would operate well without an information security function. But the real \nissue is whether the information security group should continue to exist as it does today, \nwith its  traditional mission and vision . It is clear from the prevalence of breaches and \ncompromises that we have not kept up with the threats, and we appear to be slipping \nfarther behind as the world grows more volatile, uncertain, and ambiguous. It is no \nwonder that we have fallen behind: as the world of technology expands exponentially, \nso do the technology-related threats and vulnerabilities, yet our ability to manage \nthose security and privacy risks has progressed only at a linear rate. As a result, there \nis a widening gap between the risks and the controls. In fact, many organizations have \nessentially given up actively trying to prevent compromises and have defaulted to \nreliance on after-the-fact detection and response tools. \n Figure 1-1.  The  perfect storm of information risk \n \n\n\nCHAPTER 1 ■ INTRODUCTION\n3\n As information risk and security professionals, we should be asking ourselves \npointed questions if we wish to remain valuable and relevant to our organizations. Why \ndo we exist? What should our role be? How are new consumer and  Internet of Things \n(IoT) technologies shaping what we do, and can we shape the world of these new \ntechnologies and usage models? How is the evolving threat landscape shaping us, and \ncan we shape the threat landscape? Given the bewildering pace at which technology \nchanges and new threats appear, how do we focus and prioritize our workload? What \nskills do we need? \n Traditionally, information security groups in  businesses and other organizations \nhave taken a relatively narrow view of security risks, which resulted in a correspondingly \nnarrow charter. We focused on specific types of threats, such as malware. To combat \nthese threats, we applied technical security controls. In an attempt to protect against \nattacks and stop them reaching business applications and employees’ PCs, we fortified \nthe network perimeter using firewalls and intrusion detection software. To prevent \nunauthorized entry to data centers, we installed physical access control systems. Overall, \nour thinking revolved around how to lock down information assets to minimize security \nrisks, and how to reactively detect and respond to risks as they presented themselves. \n Today, however, I believe that this narrow scope not only fails to reflect the full \nrange of technology-related risk to the business; it is detrimental to the business overall. \nBecause this limited view misses many of the risks that affect the organization, it leaves \nareas of risk unmitigated and therefore leaves the organization vulnerable in those \nareas. It also makes us vulnerable to missing the interplay between risks and controls: by \nimplementing controls to mitigate one risk, we may actually create a different risk. And \nby focusing primarily on detection and response, we are not preventing harm; we are just \ntrying to limit the damage. \n As I’ll explain in this book, we need to shift our primary focus to adopt a broader \nview of risk that reflects the pervasiveness of technology today. Organizations still need \ntraditional security controls, but they are only part of the picture. \n There are several reasons for this. All stem from the reality that technology plays an \nessential role in most business activities and in people’s daily lives. \n Technology has become the  central nervous system of a business, supporting the flow \nof information that drives each business process from product development to sales. In \naddition, as I’ll discuss throughout this book, almost every company is becoming a supplier \nof technology in some form, as technology becomes a vital element of most products, \nservices, and infrastructure from cars and household appliances to the power grid. \n The role of  technology in peoples’ personal lives has expanded dramatically, too, and \nthe boundaries between business and personal use of technology are blurring. Marketers \nwant to use social media to reach more consumers. Employees want to use their personal \n smartphones to access corporate e-mail. \n Meanwhile, the  regulatory environment is expanding rapidly, affecting the way that \ninformation systems must manage personal, financial, and other information in order to \ncomply—and introducing a whole new area of IT-related business risks. \n Threats are also evolving quickly, as attackers develop more sophisticated \ntechniques, often targeted at individuals, which can penetrate or bypass controls \nsuch as network firewalls, traditional antivirus solutions, and outdated access control \nmechanisms such as passwords. \n\n\nCHAPTER 1 ■ INTRODUCTION\n4\n In combination, these factors create a set of  interdependent risks to a business’s \ninformation and technology, from its internal information systems to the products and \nservices provided to its customers, as shown in Figure  1-2 . \n Figure 1-2.  Managing the  interdependent set of technology-related risks \n Traditional security or other control type thinkers would respond to this situation \nby saying “no” to any technology that introduces new risks. Or perhaps they would \nallow a new technology but try to heavily restrict it to a narrow segment of the employee \npopulation. An example of this over the past few years was the view at some companies \nthat  marketers should not engage consumers with social media on the company’s web \nsite because this meant accumulating personal information that increased the risk of \nnoncompliance with privacy regulations. Another example was that some companies \ndidn’t allow employees to use personal devices because they were less secure than \nmanaged business PCs. \n The reality is that because IT is now integrated into everything that an organization \ndoes, security groups cannot simply focus on locking down information assets to \nminimize risk. Restricting the use of information can constrain or even disable \nthe organization, hindering its ability to act and slowing its response to changing \nmarket conditions. A narrow focus on minimizing risk therefore introduces a larger \ndanger: it can threaten a business’s ability to compete in an increasingly  fast-moving \nenvironment . \n \n\n\nCHAPTER 1 ■ INTRODUCTION\n5\n \nTHE CHALLENGES OF RISING SECURITY COSTS AND \nSKILLS SHORTAGES\n Growing recognition of the importance of  security and privacy , triggered largely by \nhighly publicized breaches, has led to sharply increasing security spending and \nan accompanying skills shortage. If the current trajectory continues, Gartner Inc. \npredicts that by 2017 the typical IT organization will spend up to 30 percent of its \nbudget on risk, security, and compliance, and will allocate 10 percent of its people \nto these security functions. That is triple the levels of 2011 (Gartner 2015b). At the \nsame time, skill shortages may worsen; more than a third of security managers \nsurveyed in 2015 reported significant obstacles in implementing security projects \ndue to inadequate staffing (Morgan 2015). One question is how much of the \nprojected cost increase is due to under-investment in the past, and how much is due \nto the fact that organizations have invested in technologies that do not adequately \nreduce risk. To break the cycle, as I’ll explain in Chapter  7 , we need a new security \nmodel and tools that create a demonstrable decrease in the risk curve, with a \ngreater focus on effective prevention and machine learning to reduce cost and \nmanual effort. \n Protect to Enable ® \n To understand how the role of information security needs to change, we need to \nre-examine our purpose. We need to  Start with Why , as author Simon Sinek argues \nconvincingly in his book of the same name (Portfolio, 2009). Why does the information \nsecurity group exist? \n As I considered this question back in 2010, and discussed it with other members \nof the risk and security team that I led at Intel, I realized that we needed to redefine our \nmission. Like the IT organization as a whole, we exist to enable the business, to help \ndeliver IT capabilities that provide competitive differentiation. Rather than focusing \nprimarily on locking down assets, the mission of the information risk and security group \nmust shift to enabling the business while applying a reasonable level of protection. To \nput it another way, we provide the protection that enables information to flow through \nthe organization, our partners, and our customers. We also provide the protection for the \ntechnology that our organizations create to provide new experiences and opportunities \nfor our customers. \n The  core competencies of information security groups—such as risk analysis, \nbusiness continuity, incident response, and security controls—remain equally relevant as \nthe scope of information-related risk expands to new areas, such as technology-enabled \nproducts and services, as well as privacy and financial regulations. But rather than saying \n“no” to new initiatives, we need to figure out how to say “yes” and think creatively about \nhow to manage the risk. \n\n\nCHAPTER 1 ■ INTRODUCTION\n6\n During my time at Intel, the security group’s mission evolved toward this goal as \nwe helped define solutions to a variety of technology challenges. For example, my team \nrecognized as early as 2002 that implementing wireless  networks within Intel’s offices \ncould help make the workforce more productive and increase their job satisfaction by \nletting them more easily connect using their laptops from meeting rooms, cafeterias, and \nother locations. At the time, many businesses avoided installing wireless networks within \ntheir facilities because of the risk of eavesdropping or because of the cost. We learned \npretty quickly that when we restricted wireless LAN deployments or charged departments \nadditional fees to connect, we actually generated more risks. This was because the \ndepartments would buy their own access points and operate them in an insecure \nfashion. We recognized that the benefits of installing wireless LANs across the company \noutweighed the risks, and we mitigated those risks using security controls such as device \nauthentication and transport encryption. By 2004, that approach had enabled ubiquitous \nwireless and mobile computing that propelled productivity and actually reduced risks. \n A more recent example that many organizations have experienced: for years, Intel \ndidn’t allow employees to use personal smartphones for business, due to concerns about \nprivacy and other risks such data theft. However, we experienced growing demand from \nemployees soon after the launch of the iPhone 3 in 2009.  We realized that letting them use \nthese consumer devices to access e-mail and other corporate systems would help boost \nemployee satisfaction and productivity. \n By working closely with  legal and human resources (HR) groups , we defined security \ncontrols and usage policies that enabled us to begin allowing access to corporate e-mail \nand calendars from employee-owned smartphones in early 2010. The initiative was highly \nsuccessful, with a massive uptake by employees, overwhelmingly positive feedback, and proven \nproductivity benefits (Evered and Rub 2010, Miller and Varga 2011). The success of the initiative \nled to its selection for an in-depth Ivey Business School case study (Compeau et al. 2013). \n The  transformation within the information security group was reflected in changes \nto our mission statement and top priorities over the years. In 2003, the internal mission \nstatement reflected the traditional focus and scope of information security organizations: \nthe overarching goal was to protect information assets and minimize business disruption. \n By 2010 it was clear to me that we needed to simplify our purpose and also broaden \nthe scope. So in 2011, I changed our mission to Protect to Enable to express the idea that \nour primary goal was to find ways to enable the business while providing the protection \nnecessary to reduce the risk to an acceptable level. \n For a few years after this, I thought of information risk and security as a  balancing \nact . I felt that we needed to try to find the right balance between providing open access to \ntechnology and information to enable the business and locking down assets. Providing \nopen access allows greater business agility. The business can move more quickly with \nfewer restrictions. Employees can work more freely, and the faster flow of information \nallows the company to grow and transform. \n But as my  responsibilities grew to encompass security and privacy not only for \ninternal systems but also for all aspects of products and services, I realized that a \nbalancing act was the wrong analogy. We should not start from a position of making \ntrade-offs between risks and enablement, or between security and privacy. So I began \nusing a different model that I now feel more accurately represents the challenges of \nmanaging information risk: we should take on the harder task of optimizing what is \nreally a multivariate equation of risk dynamics and business objectives in order to create \nsolutions that are “ tuned to target ,” as shown in Figure  1-3 . \n",
      "page_number": 22
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 31-38)",
      "start_page": 31,
      "end_page": 38,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 1 ■ INTRODUCTION\n7\n For each problem and solution, we try to optimize or “tune” five  primary variables :\n• \n Risk and Compliance : Meeting security, privacy and compliance \nrequirements, based on the organization’s risk tolerance and \nsecurity and privacy principles. \n• \n Cost and Maintenance: The total cost of controls, factoring in \ndeployment and maintenance costs. \n• \n Productivity and User Experience : The extent to which controls \nhinder business velocity by making it harder for users to do their \njobs. I call this  control friction . In addition, if we make it difficult \nor time-consuming for users to follow security policies or use \nsecurity tools, they’ll ignore them, thus creating more risks. (See \nthe discussion of the 9 Box of Controls in Chapter  7 ). \n• \n Market Objectives : The company’s goals, such as increased \nmarket share. \n• \n Customer Needs: Our customer’s privacy and security needs, as \nwell as their overall experience. \n Ultimately there may be cases where we cannot fully optimize each item and we \nneed to make trade-offs, but that doesn’t mean we shouldn’t try. \n Figure 1-3.  Tuned to target: optimizing the equation to meet business objectives and \ncustomer  needs \n \n\n\nCHAPTER 1 ■ INTRODUCTION\n8\n I hope that this model may help information security groups at other organizations \nthink about how these priorities relate to their own businesses. The optimization points \nfor each variable and objective will depend on factors such as the organization’s overall \nculture, technical acumen, and appetite for risk. \n Building  Trust \n I believe that if computing is to continue to improve the world we live in, rather than \nendanger it, it must be trustworthy. Unfortunately, as I describe in Chapter  9 , the privacy \nand security breaches that have hit the headlines in recent years have weakened the \npublic’s trust in technology, according to the Edelman Trust Barometer, a widely used \nindicator. The rapid implementation of new technologies emerged as a new factor in \ndepressing trust overall. “By a two-to-one margin, respondents in all nations feel the new \ndevelopments in business are going too fast and there is not adequate testing,” the study \nconcluded (Edelman 2015). \n To rebuild trust in technology, we need to ensure the data we enter into our systems \nis both secure and private. At Cylance, we strive to cultivate a work environment where \nsecurity, privacy, and trust are an integral part of the evolving culture of the company and \nfoundational to the design, development, and delivery of our products and services. \n To analyze the context that led to my approach to the risk and security mission, and \nhelped to shape top priorities, I’ll explore some of the key changes in the landscape: \nthe rapidly expanding regulatory environment, the emergence of new devices and \ntechnologies, and the changing threat landscape. \n Keeping the Company Legal: The Regulatory Flood \n Until the early 2000s, I didn’t see regulatory compliance as a top priority for information \nsecurity. That’s simply because there weren’t many regulations that impacted IT, at least \nin the United States. There were a few exceptions that affected a subset of companies, \nincluding Intel, such as controls on certain  high-tech exports . And in European \ncountries, there were already regulations that sought to protect personal information. \nBut in general, IT groups didn’t have to dedicate much of their time, or budget, to \nregulatory compliance. \n The change in the last decade has been extraordinary. We have seen a flood of new \nregulations implemented at local, national, and international levels. They affect the \n storage and protection of information across the entire business, from the use of personal \ninformation for HR and marketing purposes, to financial data, to the discovery of almost \nany type of document or electronic communication in response to lawsuits. And with \ngrowing concerns about cyberwarfare, cyberterrorism, and hacktivism, several countries \nare evaluating additional  cybersecurity legislation in an attempt to protect critical \ninfrastructure and make industries more accountable for strengthening security controls. \n In most cases, these regulations do not aim to specifically define  IT capabilities ; \nhowever, because information is stored electronically, there are huge implications \nfor IT. The controls defined in the regulations ultimately must be implemented in the \norganization’s systems. These systems include more than just technology: they consist of \n\n\nCHAPTER 1 ■ INTRODUCTION\n9\npeople, procedures, devices, and applications. The business risk includes a significant \nIT-related component, but we must take a holistic view of risk management. Noncompliance \ncan damage a company’s brand image, profitability, and stock price—not just through \nresulting legal problems, but through bad publicity. \n Let’s take a brief look at some of the key areas and regulations that are having the \nbiggest impact. \n Privacy:  Protecting Personal Information \n For many US companies, the wake-up call was the California data security breach \nnotification law (State Bill 1386), which became effective in 2003. A key aspect of this \nlaw requires companies that store personal information to notify the owner of the \ninformation in the event of a known or suspected security breach. Businesses could \nreduce their exposure, as well as the risk to individuals, by encrypting personal data. \n After this, other states quickly followed suit, implementing regulations that generally \nfollow the basic tenets of California’s original law: companies must promptly disclose a \ndata breach to customers, usually in writing. \n In addition, federal laws, such as the  Health Insurance Portability and Accountability \nAct (HIPAA) , have addressed specific categories of personal information. Further \nregulations have been added in other countries, too, such as the updated data-protection \nprivacy laws implemented in Europe (European Commission 2011, 2012). \n The implications of these local and national regulations extend beyond geographical \nboundaries. As companies do more business online, they’re increasingly likely to acquire \nand store information about customers from other countries, and find that they also \nneed to comply with regulations around the world. Those regulations may change, with \nimplications for businesses in multiple countries. In late 2015, for example, Europe’s \nhighest court struck down the so-called “safe harbor” agreement that had allowed \ncompanies to move information about consumers between the European Union and \nthe United States. The replacement EU-US Privacy Shield, agreed after three months of \nnegotiations, aimed to address European privacy concerns with written guarantees that \nUS intelligence agencies would not have indiscriminate access to Europeans’ personal \ndata stored in the US (Scott 2016). \n The issue can become even more complex when businesses outsource application \ndevelopment or HR functions to providers located in yet another country. Now, software \ndevelopers in India may be building and operating the systems that collect information \nabout Europeans for US companies, making it even more difficult for businesses to \nnavigate compliance with all relevant privacy regulations. \n Personalization vs. Privacy \n Privacy concerns are set to become even more important over time, as businesses \nincreasingly seek to create online experiences tailored to the needs of individual users. \nThe more a business knows about each individual, the more it can personalize services \nand offer targeted advertising based on income and preferences. \n\n\nCHAPTER 1 ■ INTRODUCTION\n10\n Many users also like personalized services. If a web site “remembers” them, they \ndon’t need to enter the same information each time they visit the site, and they’re more \nlikely to see content and offers relevant to their needs. In fact, companies may be at a \ndisadvantage if they don’t personalize services because users may prefer a web site from a \ncompetitor that offers a more streamlined experience. \n However, there’s an inevitable conflict between personalization and privacy. \nThe personalization trend is fueling the growth of an industry focused on collecting, \nanalyzing, and reselling information about individuals. This industry existed long before \nthe Web; personal information has been used in mass-mailing campaigns for decades. \nHowever, the Web is both increasing demand for this information while providing \nnew ways to collect it. Companies now have opportunities to collect information from \nmultiple online sources, correlate and analyze this information, and then sell it to others. \nAnd of course, consumers’ fears that information will be lost or misused have increased \naccordingly. \n For businesses, however, offering personalized services also can increase \ncompliance concerns. As companies store more personal information, they are \nresponsible for safeguarding that information and are liable for any loss or compromise. \nIn many parts of the world, companies are also required to explain why they are collecting \npersonal data, how they are protecting it, and how long they will keep it. \n We can expect continuing tension due to conflicting desires for personalization and \nprivacy—and more regulation as a result. Governments clearly believe that businesses \ncannot be relied upon to regulate themselves, so they will continue to add regulations \ndesigned to protect the privacy of individuals. Meanwhile, businesses will seek new ways \nto collect more information so that they can further personalize services. Developing \ncompliance strategies and guidelines becomes even more pressing. \n Financial Regulations \n Financial regulation surfaced as a top priority in the United States with the Sarbanes-\nOxley Act ( SOX ), which emerged from the public outrage over corporate and financial \naccounting scandals at companies such as Enron and WorldCom. These scandals cost \ninvestors billions of dollars and damaged public confidence. To help avoid similar \ncatastrophes in the future, SOX imposed financial tracking requirements designed to \nensure that a company’s financial reporting is accurate and that there hasn’t been fraud \nor manipulation. Once enacted, SOX required publicly held companies to meet specific \nfinancial reporting requirements by the end of 2004. \n Although the Sarbanes-Oxley Act doesn’t mandate specific technology controls, \nit has major implications for IT. Ensuring financial integrity requires controls to be \nimplemented within everyday financial processes. In practice, this means they must \nbe enforced within the IT applications and infrastructure that support those processes. \nPurchases above specific thresholds may require approval from the finance group; the \nunderlying applications have to support this workflow, and to be sure the applications \nfunction correctly, businesses need to establish the integrity of the underlying computer \ninfrastructure. Compliance with financial regulations therefore creates a series of IT \nrequirements, from making sure that applications provide the right functionality to \nimplementing access controls and updating software. \n\n\nCHAPTER 1 ■ INTRODUCTION\n11\n E-Discovery \n Regulations governing the discovery of information for litigation purposes officially \nextended their reach into the electronic realm in 2006. That’s when the US Supreme \nCourt’s amendments to the Federal Rules of Civil Procedure explicitly created the \nrequirement for e-discovery—the requirement to archive and retrieve electronic records \nsuch as e-mail and instant messages. \n This created an immediate need not just to archive information, but to automate its \nretrieval. This is because records must be produced in a timely way, and manual retrieval \nwould take too long and be prohibitively expensive. The business risks of noncompliance \nare considerable: unlike many countries, US practice allows for potentially massive \ninformation disclosure obligations in litigation. Companies that fail to meet e-discovery \nrequirements may experience repercussions that include legal sanctions. The \nimplications are correspondingly onerous. Lawsuits may draw on information that is \nseveral years old, so businesses must have the capability to quickly search and access \narchived information as well as current data. E-discovery is further complicated by the \ngrowth of cloud computing models such as software as a service (SaaS). As organizations \noutsource more business processes and data to cloud service suppliers, they need to \nensure that their suppliers comply with their e-discovery needs. \n Expanding Scope of  Regulation  \n The regulatory universe continues to expand, with the likelihood of more regulations \nthat explicitly address IT, as new technologies emerge and governments try to control its \nuse and inevitable misuse. In the US, lawmakers have proposed legislation to increase \nthe security and privacy of connected cars, following a widely publicized demonstration \nin which researchers hacked into a Jeep and took over its controls. The Food and Drug \nAdministration (FDA) has published cybersecurity guidelines describing requirements \nfor manufacturers of Internet-connected medical devices (FDA 2016). \n The attempts by various governments to gain access to technology for the purposes \nof combating terrorism have generated considerable impact and controversy. In China, \na new anti-terrorism law requires that technology companies hand over technical \ninformation and help with decryption when the police or state security agents demand \nit for investigating or preventing terrorist cases (Buckley 2015). In the US, even greater \ncontroversy was generated by the US Government’s attempts to force Apple Computer \nto create “back doors” that make it easier to access information on iPhones used by \nterrorists or criminals. In India, after terrorists used unsecured Wi-Fi access points \nto communicate information about their attacks, the government created a legal \nrequirement that any access point must be secured (Government of India Department of \nTelecommunications 2009). \n In other countries, businesses that operate unsecured Wi-Fi access points (a \ncommon way to provide Internet access for visitors) may find themselves facing other \nlegal problems. For example, unscrupulous individuals may tap into the network to \naccess web sites for purposes such as illegally downloading music or pornography. \nAccess appears to originate from the company hosting the access point, which may then \nfind itself on the receiving end of correspondence or raids from the music industry or \ngovernment agencies. \n\n\nCHAPTER 1 ■ INTRODUCTION\n12\n The Rapid Proliferation of Information, Devices, \nand  Things \n The computing environment is growing as rapidly as the regulatory environment. The \nsheer volume of information is exploding, and it is being stored across a rapidly growing \narray of devices. The Internet of Things will drive yet another exponential increase: \nGartner, Inc. estimates that during 2016, 5.5 million new “things” will be connected every \nday, and Cisco expects 50 billion connected devices by 2020. In the not too distant future, \nalmost any device with a power supply may have an IP address and will be capable of \ncommunicating—and being attacked—over the Internet. \n Recent headlines have highlighted the growing threat activity focused on IoT, as I’ll \ndiscuss further in Chapter  7 . Researchers hacked into a Jeep via its Internet-connected \nentertainment system and remotely controlled the vehicle’s functions (Greenberg 2015); \nother researchers showed that thousands of medical devices in hospitals are vulnerable \nto attack. \n At the same time, the boundaries between work and personal technology have in \nsome cases completely dissolved. Whether businesses officially allow it or not, employees \nare increasingly using their personal devices for work by sending e-mails from and storing \ninformation on their personal smartphones and computers. Furthermore, people may \nforward e-mail from business accounts to personal accounts created on external systems, \nwithout considering that when they signed up for the personal account, they agreed to a \nlicense that allows the external provider to scrutinize their e-mails. \n The use of personal technology such as smartphones can considerably enhance \nbusiness productivity because employees can now communicate from anywhere at \nany time. However, this also creates a more complex, fragmented environment with \nmore potential points of attack. Information is now exposed on millions of new devices \nand disparate external networks, many of which do not have the same type of security \ncontrols as corporate PCs, and all of which are outside corporate network firewalls. Not \nsurprisingly, mobile malware has become a major industry, and is still growing: one \nsurvey found more than 1,200 known families of Android malware in 2014, more than \ndouble the number found the previous year (Millman 2015). \n The boundaries between work and personal lives are dissolving in other ways, \ntoo. Employees store more information on the Internet—on business and consumer \nsocial media sites, for example—than ever before. These sites are powerful tools for \ncommunicating with audiences outside the corporate firewall. \n However, just as there’s an industry gathering and analyzing personal information \nfor marketing purposes, information on the Web can be used for competitive intelligence \nor for less legitimate purposes. Users store snippets of information in multiple places \non the Web. Although each of these snippets may not provide much information, when \npieced together they can provide new intelligence not just about the individual, but also \nabout the organizations to which the person belongs. Each item is like a single pixel in \na digital picture. Alone, it doesn’t convey much information; but step back, aggregating \ninformation from a wider range of sources, and those pixels combine to form a portrait. \nIn the same way, pieces of information strewn across a variety of unrelated web sites—the \nname of a department, workmates, pet names that might be used as passwords—can be \nlinked together to create a picture of an individual and used for malicious purposes. \n\n\nCHAPTER 1 ■ INTRODUCTION\n13\n The Changing Threat Landscape \n The  threat landscape is evolving rapidly, with an increase in highly organized and well-\nfunded groups capable of executing sustained attacks to achieve long-term goals, including \ncyberespionage, cyberterrorism, and cyberwarfare. These attackers, generally known as \n advanced persistent threats ( APTs ), were originally thought to focus mainly on governments \nbut more recently have also been shown to target private-sector organizations, with the \ngoal of stealing intellectual property or simply causing damage. APTs include nation-state \norganizations, “hacktivist” groups attempting to publicize or further their cause, and \norganized crime. Hacktivists who said they were targeting oppressive regimes claimed \nresponsibility for an attack that disabled more than 30,000 computers at the world’s biggest \noil producer, Saudi Aramco. The FBI blamed North Korea for a crippling attack on Sony \nPictures (Schmidt et al. 2015). In 2014, the US Justice Department indicted five Chinese \nmilitary hackers for stealing trade secrets and other information from US companies in the \nnuclear power, metals, and solar industries (Department of Justice 2014); in 2016, the US \ncharged seven hackers linked to the Iranian government with hacking US banks and dam \noperations (Nakashima and Zapotosky 2016). \n The steady rise of organized  cybercrime online  is entirely logical. As the exchange \nof money and information has moved online, organized crime has followed, focusing on \ntheft of valuable assets such as intellectual property. This has spawned a mature malware \nindustry that increasingly resembles the legitimate software industry, complete with a \nbroad set of services, guarantees, and price competition among suppliers.  Ransomware , \nwhich encrypts a victim’s data until a ransom is paid, is a recent trend. \n Stealthy Malware \n This evolving set of threat agents is using new, more sophisticated tools and methods \nto mount attacks. Once upon a time, attackers were amateurish and often driven by \npersonal motives such as the prestige of bringing down a big company’s network. \nAccordingly, the arrival of malware on a user’s machine was easy to detect: the malware \nannounced itself with icons or messages, and the system often became unusable. \n Now the trend is toward malware that is stealthy and uses sophisticated techniques \nto avoid detection. Attackers plant malware that lies undetected over a long period while \nit captures information. Another common technique is to quietly spread malware by \ninjecting malicious code into an unsuspecting company’s web site; users who visit the site \nthen unknowingly download the code onto their systems. \n Accompanying this is a shift from spam mass e-mails to carefully crafted \n spearphishing attacks aimed at individuals or specific  groups . These typically use social \nengineering techniques, such as providing enough contextual or personal information in \nan e-mail to tempt people to download malware or click on a link to an infected web site \ncreated specifically for that purpose. Though more expensive to mount, spearphishing \nattacks can be enormously profitable to cybercriminals; an analysis by a supplier of anti-\nphishing solutions found that they were the primary initial attack method used by APTs \nin 2015; 22% of attacks were motivated by financial fraud or other crimes (PhishLabs \n2016). We can expect these stealthy and targeted attacks to continue, with new methods \nemerging as necessary to circumvent defenses. \n\n\nCHAPTER 1 ■ INTRODUCTION\n14\n Nine Irrefutable Laws of Information Risk \n Over the years, I’ve identified a number of “laws” that encapsulate some of the lessons \nI’ve learned, and that seem to remain true despite the continually changing environment. \n I call these the Nine Irrefutable Laws of Information  Risk (with acknowledgements to \nCulp (2000), Venables (2008), Lindstrom (2008), and other sources):\n• \n Law #1:  Information wants to be free . People want to talk, post, \nand share information—and they increase risk by doing so. Some \nexamples: \n A senior executive at a major technology company updated \nhis profile on a business social networking site. In doing so, he \ninadvertently pre-announced a shift in his employer’s strategy—a \nmistake that was promptly and gleefully picked up by the press. \n An employee found a novel way to fix a piece of equipment \nmore quickly and, to help others across the company, decided to \nvideotape the procedure. Because video files are so large, it didn’t \nmake sense to e-mail the video, so the employee posted it online. \nUnfortunately, by doing so, he exposed confidential information. \n At one time or another, many people have experienced this \ndisconcerting event: when composing a message, the e-mail \nsoftware helpfully autofills the address field, but it selects the \nwrong name from the address book. You hit Send without \nrealizing the error, thus dispatching a company-confidential \nmessage to someone outside the organization. \n It’s worth noting that that this rule is not new. Information has \nalways wanted to be free: think of the World War II slogan “loose \nlips sink ships.” People communicate, and sometimes they share \nmore information than they should. It’s just the methods that \nhave changed, and the fact that, with the Internet, a carelessly \nmentioned detail is instantly available to anyone across the globe. \n• \n Law #2:  Code wants to be wrong . We will never have 100 percent \nerror-free software. In fact, the more widely used the software, \nthe more malicious individuals will hunt for vulnerabilities in the \ncode. They have found and exploited errors in the world’s most \nwidely used web sites, productivity applications, and enterprise \nbusiness software. \n• \n Law #3:  Services want to be on . On any computer, some \nbackground processes always need to be running, and these can \nbe exploited by attackers. These could even be security software \nprocesses used for everyday activities like keeping systems up-to-\ndate with software patches or monitoring for malware. \n",
      "page_number": 31
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 39-47)",
      "start_page": 39,
      "end_page": 47,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 1 ■ INTRODUCTION\n15\n• \n Law #4:  Users want to click . People naturally tend to click when \nthey see links, buttons, or prompts. Malware creators know this, \nand they take advantage of it. In fact, the entire phishing industry is \nbased on the assumption that users will click on enticing e-mails, \nweb sites, or pop-up ads, triggering the download of malicious \ncode to their systems. The evolution of highly targeted attacks such \nas spearphishing has taken this to a new level, as when e-mails \npurporting to be letters discussing legal action from a circuit court \nwere sent to senior executives at a number of companies. \n• \n Law #5:  Even a security feature can be used for harm. Security \ntools can be exploited by attackers, just like other software. This \nmeans that laws 2, 3, and 4 are true for security capabilities, too. \nNetworking equipment supplier Juniper Networks discovered \nthat its firewall software contained “unauthorized code” that \nsurreptitiously decrypted virtual private network traffic (Goodin \n2015). Security researchers have uncovered vulnerabilities that can \nbe exploited by attackers in products from well—known security \nsuppliers, including Kaspersky Labs and FireEye (Ashford 2015). \n• \n Law #6:  The efficacy of a control deteriorates with time . Once \nput in place, security controls tend to remain static, but the \nenvironment in which they operate is dynamic. Organizations \ntend to “set and forget”: to install security controls and then fail to \nupdate them with security patches or to properly maintain access \nlists. As attackers find new ways to circumvent or compromise the \ncontrols, their effectiveness progressively degrades. As Rob Joyce, \nwho heads the National Security Agency’s elite hacking unit, put \nit, an organization with static defenses will drift to the back of the \nherd, where it is easily picked off by a predator (see Chapter  6 ). \n• \n Law#7:   Code needs to execute. All software, good or bad, needs \nto execute in order to perform its intended function. Malware is \ncreated with malicious intent, but until it executes, it is dormant \nand can do no harm. Exploits can therefore be intercepted and \nstopped by security tools that inspect code before execution, \nidentify good from bad, and prevent bad code from executing. \n• \n Law #8:  Controls create friction. Security controls can slow users \nand business processes by impacting system performance \nor forcing them to use cumbersome processes. High-friction \ncontrols therefore impose a “drag coefficient” on business \nvelocity. Users react to a high degree of control friction by \ncircumventing the controls whenever possible; as a result, the \ncontrols can actually introduce new risks as business users go \naround IT to get their jobs done. Control friction is an important \nconsideration when designing security architectures (see the \ndiscussion on the 9 Box of Controls in Chapter  7 ) \n\n\nCHAPTER 1 ■ INTRODUCTION\n16\n• \n Law #9:  As our digital opportunities grow, so does our obligation \nto do the right thing. As technology becomes embedded into the \nfabric of our lives, exploits that take advantage of technology \nvulnerabilities may increasingly impact the well-being of almost \neveryone in society. So it is particularly important that we apply \nthe right ethical values to shape the way we design, develop, \nand implement these technologies. As I explain in Chapter  9 , \nsecurity and privacy should now be considered a corporate social \nresponsibility. \n A New Approach to Managing Risk \n Given the ever-broadening role of technology and the resulting information-related \nbusiness  risk , we need a new approach to information security built on the concept of \nprotecting to enable. This approach should\n• \n Incorporate privacy and regulatory compliance  by design, taking a \nholistic view of information risk . Also, because all companies are \nmoving toward using technology not only for internal operations \nbut also in products and services, the information security \norganization must work closely with other business groups to \nunderstand and manage risk. \n• \n Recognize that people and information, not the enterprise  network \nboundary,  are the security perimeter . Information is no longer \nrestricted to tightly managed systems within data centers; it now \nalso resides outside the firewall, on users’ personal devices, and \non the Internet. Managing risk therefore requires a range of new \ntools, including user awareness and effective security controls for \npersonal devices. \n• \n Be  dynamic and flexible  enough to quickly adapt to new \ntechnologies and threats. A static security model will inevitably \nbe overtaken by the dynamic nature of threats. We need security \narchitectures that can rapidly learn and adapt to new devices and \nevolving threats, with a high degree of automation. \n Above all, we need to accomplish a shift in thinking, adjusting our primary focus \nto enabling the business, and then thinking creatively about how we can do so while \nmanaging the risk. Our roles will only increase in importance as technology becomes \neven more prevalent. Our ability to protect information security and privacy will be \nessential to building the trust that enables our organizations to take advantage of new \ndigital opportunities. \n\n\n17\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_2\n CHAPTER 2 \n The Misperception of Risk \n The moment we want to believe something, we suddenly see all the \narguments for it, and become blind to the arguments against it. \n —George Bernard Shaw \n One hundred years ago, the “unsinkable”  Titanic foundered after striking an iceberg \noff the coast of Newfoundland. More than 1,500 people died in what became one of the \ndeadliest maritime accidents ever. Several factors contributed to this massive death toll, \nbut perhaps the most critical was that there simply weren’t enough lifeboats. The ship \ncarried 2,224 people, but fewer than half of them could squeeze into the boats. \n As we know, passengers who didn’t get a spot in one of those lifeboats quickly died \nin the freezing waters of the North Atlantic. What’s less well known is that the  Titanic ’s \nsupply of lifeboats was in full compliance with the British marine regulations in force at \ntime. The law required the ship to carry 16 lifeboats; the  Titanic actually had 20 lifeboats. \n The ship’s owners did a good job of providing enough boats to address the regulatory \nrisk of noncompliance. Unfortunately, meeting regulatory requirements did little to \nprevent the tragic loss of life. \n This is a case of  misperception  of risk . The owners focused on mitigating the \nregulatory risk, apparently blind to the much larger risk of disaster. They framed the \nlifeboat issue as a compliance item that needed to be addressed so that the ship could \nstart carrying passengers and generating revenue. One could argue that if they had \nstepped back and considered the potential consequence s for the customers rather than \nthe company’s short-term priorities, history might have unfolded differently. Reports \nsuggest that the  Titanic had enough capacity to easily add enough lifeboats for everyone \non board, had the owners chosen to do so. \n What does this example have to do with managing information risk? We encounter \nmisperceptions every day within the realm of enterprise risk and security. Every \norganization has a greater responsibility than simply complying with regulations. We have \nto think about whom is ultimately at risk: the company or the customer? Furthermore, \nas I’ll show in this chapter, everyone in the organization has their own priorities and \ntheir own subjective view of risk. Unless we mitigate these misperceptions, they can have \ndisastrous consequences. As a result, I believe that the misperception of risk is the most \nsignificant vulnerability facing enterprises today. \n\n\nCHAPTER 2 ■ THE MISPERCEPTION OF RISK\n18\n The Subjectivity of Risk  Perception \n As  security professionals , we tend to think about objective ways to estimate risk—to \nassess the likelihood and extent of harm that can occur due to specific threats and \nvulnerabilities. \n But in reality, the way people perceive risk has a strong subjective component. \n Economic and psychological factors greatly affect how each of us perceives the likelihood \nand potential impact of harm from specific actions or situations. Within an organization, \neach individual’s perception of risk varies depending on his or her job role, goals, \nbackground, and peer group. This means managers, security professionals, and end users \nall may have a different view of the risk associated with a specific technology or action. \n Misperceiving risk has serious consequences because our actions are shaped by \nour perception of risk. An employee may think that posting personal and work-related \ninformation on a social media  site is relatively harmless. However, hackers might use this \npublicly available information in phishing e-mails to gain access to enterprise systems via \nthe employee’s computer, ultimately resulting in detrimental security breaches. \n End users are not the only members of the organization who can misperceive risk. \nEveryone is capable of misperceiving risk, including risk and security professionals. As \nI’ll explain later in this chapter, misperceptions occur at the group level as well as the \nindividual level. Members of a group may share the same bias in their perception of risk \nand benefit. \n The decisions that result from these misperceptions can weaken the entire \norganization’s security  posture . If an organization underestimates a risk, it will \nunderspend on controls to mitigate that risk, increasing the likelihood and potential \nimpact of major problems such as data breaches. On the other hand, if the organization \noverestimates a risk, it will allocate a disproportionately large share of its security \nresources to the risk, leaving other parts of the risk landscape underprotected. \n In this chapter, I’ll discuss how and why different people within an organization \nmisperceive risk, whether they are acting as information technology users, security \nprofessionals, or managerial decision makers. To explore these misperceptions, I’ve drawn \non research across the broader field of risk psychology, notably  The Psychology of Risk , \na book by Professor Dame Glynis Breakwell, Vice Chancellor of the University of Bath \n(Cambridge University Press 2007). I’ll examine how these ideas about risk perception \napply to information risk and security. I’ll explain some of the consequences of those \nmisperceptions, and I’ll discuss some of the ways an organization can address them. \n How  Employees Misperceive Risk \n Research shows that if we like an activity, we tend to judge its benefits to be high and its \nrisk to be low (Slovic 2010). Conversely, if we dislike the activity, we judge it as low-benefit \nand high-risk. Because of this, the perception of risk by individuals and groups within an \norganization tends to be biased by their preferences, roles, and objectives. Everyone is \ntrying to achieve their individual or group goals within the organization, so they tend to \nsee activities and technologies that support those goals as beneficial, and therefore they \ntend to underestimate the risk. \n\n\nCHAPTER 2 ■ THE MISPERCEPTION OF RISK\n19\n So if employees like social media, their attraction to the technology skews their \nperception of benefit and risk. Because they judge the benefit to be high and the risk to be \nlow, they feel comfortable posting information such as their job title, location, and even \nthe projects they’re working on. They may even allow sites to capture their location, using \nthe global positioning system in their cell phone, and display the location in real time. \n Unfortunately, these employees may not think about how a malicious individual \ncould use the information. Today, as we’ve seen, an individual’s use of technology can \nharm not only the individual but the entire organization. Attackers exploit publicly \navailable personal information to craft spearphishing e-mails that are particularly \nconvincing because they appear to demonstrate a relationship with the recipient, making \nthe employee more likely to click on a link that downloads malware to the system. From \nthere, the attack spreads to the rest of the corporate network. In addition, information \nposted by individuals is now routinely aggregated, analyzed to identify patterns, and sold, \noften to a company’s competitors. \n The risk and security team may also misperceive the risk of social media, but in the \nopposite direction: they overestimate the risk and underestimate the benefits. They may \nnot like social media because it creates vulnerabilities, and their perception then drives \nthem to focus on minimizing the risk by trying to block the use of the technology. \n Other psychological factors also come into play in shaping end users’ risk \nperception. People in general tend to believe they are personally less likely than others \nto experience negative events and more likely to experience positive events, leading to \na sense of personal invulnerability (Breakwell 2007). In addition, users also are more \nlikely to behave in risky ways if their colleagues do so. “It’s conformity: being seen to be \ndoing what everybody else is doing,” Breakwell says (pers. comm.). Many social media \nsites encourage this conformist tendency; if all your friends are using a social media site, \nyou’re likely to join the site too because it enables you to see what they are doing and \nshare information with them more easily. \n The likelihood that individuals will behave in ways risky to the organization also \nincreases when their individual interests don’t align with the company’s. This divergence \nis most likely when employees are discontented, resentful, demoralized, or simply don’t \ntrust IT or the broader organization. \n In economic theory, the problem resulting from this lack of alignment is known as a \n moral hazard : a situation in which someone behaves differently from the way they would \nif they were fully exposed to the risk. A useful moral hazard analogy is renting a car with \nfull insurance coverage. People are likely to be less careful with the rental car than they \nwould be with their own car if they’re not responsible for the consequences. The attitude \nis “if it’s not mine, it doesn’t matter.” \n In the realm of enterprise IT, moral hazards may be a bigger concern than many \nappreciate. A Cisco survey (2011a) found that 61 percent of employees felt they were not \nresponsible for protecting information and devices, believing instead that their IT groups \nor IT service providers were accountable. Ominously, 70 percent of these surveyed \nemployees said they frequently ignored IT policies. \n One indicator of the extent of moral hazard within an organization may be how \nemployees treat company-provided laptops. Higher-than-average loss or damage rates \nmight suggest employees don’t care about the laptops and may be an indication they \ndon’t care about other corporate assets either. As I’ll discuss in Chapter  5 , I believe \nallowing reasonable personal use of laptops can help reduce the risk of moral hazard \nbecause it aligns personal interests with those of the organization. \n\n\nCHAPTER 2 ■ THE MISPERCEPTION OF RISK\n20\n More broadly, organizations can address the moral hazard issue by taking steps \nto align the goals and concerns of everyone involved: end users, information security \nprofessionals, and executives. This returns us to the theme of the book: as information \nsecurity professionals, our mission is to Protect to Enable. This mission aligns our security \ngoals with those of the business. It helps maintain the perception of shared values. \nResearch suggests that people with whom we share values are deemed more trustworthy \n(Breakwell 2007, 143). If employees trust us, they are more likely to believe our warnings \nand act on our recommendations. \n The Lure of the Shiny Bauble \n One further point to remember is that everyone in the organization, regardless of the job \nrole, is an end user. Therefore, we can all fall prey to the same tendencies. Our attraction \nto new consumer technologies can also cause us to ignore the risks. I call this magpie-\nlike attraction the  lure of the shiny bauble ; mesmerized by the appeal of gleaming new \ntechnologies, we downplay or even fail to notice the risks lurking in the shadows. \n How Security Professionals Misperceive  Risk \n While end users tend to underestimate the risks of a desirable activity or technology, \nsecurity professionals sometimes display the opposite tendency. We focus obsessively \non the information risk associated with a specific threat or vulnerability. In doing so, we \ncompletely miss bigger risks. \n This phenomenon is known as  target fixation , a  term originally coined to describe \na situation in which fighter-bomber pilots focus so intently on a target during a strafing \nor bombing run that they fail to notice the bigger risk to themselves and crash into the \ntarget as a result (Colgan 2010, 44). As information security professionals, we can develop \na similar fixation. We focus so intently on one risk that our awareness of larger hazards is \ndiminished. This target fixation can also occur in other groups with “control” functions \nwithin the organization, such as internal audit, legal compliance, and corporate risk \nmanagement. \n Here is an example from my own experience at Intel. Several years ago, we \ndiscovered that malware had been introduced onto our network from an employee’s \npersonal computer. We became so focused on this source of danger that we eliminated \nall personal devices from our network. We further fueled our target fixation by labelling \nthese devices “non-Intel managed systems ( NIMS ) ,” a term that reflected the frustration \nover our lack of control. I vowed we would never again allow network access from devices \nthat we didn’t fully control. \n However, by becoming fixated on a single threat, we may have created some larger \nrisks and additional costs. For example, we needed to issue contract employees with \ncorporate PCs, each of which allowed broader access to the Intel environment. If we \nhad instead focused on how we could provide limited access to the environment from \n“untrusted”  devices , we might have managed the risk with lower total cost and obtained \na head start in developing a key aspect of a more flexible security strategy, as I’ll describe \nin Chapter  7 . \n\n\nCHAPTER 2 ■ THE MISPERCEPTION OF RISK\n21\n It’s worth noting that security professionals can also suffer from a problem that’s \nalmost the opposite of target fixation:  alert fatigue . At many organizations, security \ngroups experience a constant deluge of thousands of alerts emanating from security \ntools across the enterprise. With so much noise, it’s easy to become overwhelmed and \nmiss important threats. \n As security professionals, we also may misperceive  risk due to the tendency to “set \nand forget” security controls. This common security loophole is described in the sixth \nIrrefutable Law of Information Security in Chapter  1 , which states that the efficacy of \na control deteriorates with time. Once in place, controls tend to remain static, but the \nthreats they are intended to mitigate continue to evolve and change, sometimes in \nvery dynamic ways. Controls that are initially very effective can become inadequate \nover time. Ultimately, an adverse event may occur and may even have disastrous \nconsequences. \n Think about the  history of major oil tanker spills. For years, regulations allowed \ntankers to be built with a single hull, instead of a double (inner and outer) hull to provide \nadditional protection in the event of a leak. Meanwhile, tankers grew steadily larger \nbecause bigger ships could transport oil more efficiently than smaller ones. It wasn’t \nuntil the  Exxon Valdez ran aground, puncturing its hull and creating a giant oil leak that \ncontaminated huge stretches of Alaska’s coast, that authorities were spurred to create new \nregulations requiring double hulls in oil tankers (EPA 2011). \n Within enterprise IT, a typical “set and forget”  error is the failure to keep controls \nup-to-date, particularly if the controls are designed to mitigate a relatively low risk. A \ncase in point:  distributed denial-of-service (DDoS)  threats were a big concern more than \na decade ago, due to widely publicized attacks by worms such as Code Red, Nimda, and \nSQL Slammer. These attacks disabled corporate web sites or flooded internal networks \nby overloading them with requests. To mitigate the availability risk, many organizations \ninvested in defenses against DDoS attacks. \n Over time, however, DDoS attacks became less frequent, and organizations were \nassailed by newer threats. With limited resources, information security groups focused \non mitigating these new threats rather than continuing to build defenses against DDoS \nattacks. At the same time, though, businesses were increasing their online presence. Web \nsites evolved from being used primarily for advertising and displaying static corporate \ninformation to managing business-critical data and applications. Some organizations \nbegan conducting all their business online. Even traditional brick-and-mortar \nbusinesses moved customer support, order management, and other critical business \nprocesses onto the Web. The larger online presence multiplied the potential impact of a \nsuccessful attack. As a result, when DDoS attacks from a variety of groups resurfaced in \nthe past few years, they created even greater disruption to business operations as well as \ndamage to corporate brands. \n Another example: over the past few years, many organizations have become much \nmore diligent about  scrubbing data from the hard drives of old computers before \ndisposing of them or reselling them. But they failed to follow similar precautions for other \nbusiness devices that have evolved to include hard drives. \n\n\nCHAPTER 2 ■ THE MISPERCEPTION OF RISK\n22\n Nearly every digital copier contains a drive storing an image of each document \ncopied, scanned, or e-mailed by the machine. When CBS News reporters visited a \ncompany that specialized in reselling used copiers, they found businesses and agencies \nhad discarded machines containing lists of wanted sex offenders, drug raid targets, pay \nstubs with Social Security numbers, and check images. One copier’s hard drive even \ncontained 300 pages of individual medical records, including a cancer diagnosis, which is \na potential breach of federal privacy law (Keteyian 2010). \n Security and Privacy \n As I explained earlier in the book, security professionals, and the broader security \nindustry, can sometimes be tone-deaf when it comes to privacy concerns. In their zeal to \ncollect data for security purposes, they may create risks that the data could be used in a \nway that may violate people’s privacy, or at least their expectations of privacy. \n The challenge of balancing privacy and security concerns in the enterprise bears \nmany similarities to the broader issue of balancing security and privacy in society, an \narea that has been extensively explored by privacy legal expert Daniel J. Solove. As he \nexplains in the book  Nothing to Hide: The False Tradeoff between Privacy and Security \n(Solove 2011) , the debate between security and privacy has often been incorrectly \nframed to imply that we must choose between one value and the other. “Security and \nprivacy often clash, but there need not be a zero-sum game,” he writes. “There is a way \nto reconcile privacy and security: by placing security programs under oversight, limiting \nfuture uses of personal data, and ensuring that programs are carried out in a balanced \nand controlled manner.” \n Solove’s conclusion is equally applicable to information security. Many in the \nsecurity profession think that security equals privacy. That is not the case. We need good \nsecurity to achieve privacy, but the two are not synonymous. Some security industry \nsolutions conduct broad-based bulk data collection, monitoring the activity of users \nand their machines, and siphoning the data to the cloud. That data is then used to \nbuild profiles and, combined with other information, to enable the solution to scan for \npotentially anomalous activity. Considered in isolation, some machine data has few, if \nany, privacy implications. However, the collection of thousands of pieces of information \nabout what the machine is doing, when and how, while someone is using it and even \nwhen not, creates a detailed digital profile of an individual and his or her behavior. That \nprofile is collected, stored in perpetuity and analyzed. As our lives become more digitized, \nthe richness of that profile will grow and evolve. We need to step back and ask ourselves \nwhether this is really necessary for our protection. \n As I’ve discussed elsewhere in this book, I believe that security and privacy programs \nshould be managed together as elements of an overall enterprise information risk \nmanagement strategy. Security and privacy are like the two halves of a zipper: when \nmeshed together, they create a strong bond, protecting the enterprise and the individual \nagainst risk. Managing them as isolated silos is more likely to result in dangerous \nmisperceptions of risk.  \n\n\nCHAPTER 2 ■ THE MISPERCEPTION OF RISK\n23\n \nMISMATCHING CONTROLS TO THREATS\n Businesses sometimes devote considerable time and resources to implement \nsecurity  controls that are completely irrelevant to the threats the companies are \ntrying to mitigate. These mismatches reveal a lack of understanding of the security \ntechnology and the threat. The controls may further add to the risk by providing \na false sense of security. In reality, deploying the wrong control is like carrying a \nlightning rod to protect oneself from getting wet in a storm. \n Typical mismatches include\n• \n Using firewalls to prevent data theft from applications that are \nallowed to operate through the  firewall \n• \n Using standard antivirus tools that are effective only against \npreviously identified threats, to protect against zero-day attacks \n• \n Using controls at the operating-system level to detect application-\nlayer attacks \n This mismatch does not mean that these controls are worthless. It simply means \nthat if our goal is to deal with a specific threat, we must understand both the attacks \nand the controls well enough to identify which controls are applicable, and where it \nis necessary to add other controls. For example, if a firewall cannot prevent attacks \nagainst an application, we might deploy an additional control behind the firewall. \n How  Decision Makers Misperceive Risk \n Managers make decisions based on information from technical specialists and other \nexperts. Therefore, the decisions that managers make are only as good as the information \nthey receive. Decision makers can misperceive risk when their decisions are based on \nbiased or incomplete information. \n Bias can influence these decisions every day. If people are trying to sell a particular \nproposal or point of view to their manager, what are they likely to do? They tend to select \ndata supporting their arguments and often ignore data contradicting those arguments. \n The danger of misperception is particularly acute when decision makers rely on \na narrow range of sources with similar viewpoints. Without obtaining a diversity of \nviewpoints, managers don’t get a full picture of the risk. Like-minded individuals tend \nto agree with each other, as you might expect. When a group is composed solely of \npeople with similar backgrounds and viewpoints, it may be particularly prone to  group \npolarization (Breakwell 2007, 99) and the group’s decision may be more extreme than the \nmean of their individual views. This problem may be especially acute when the people \ninvolved share the same mental model of the world, as is likely to be the case when the \ngroup consists only of specialists from the same organization. \n",
      "page_number": 39
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 48-55)",
      "start_page": 48,
      "end_page": 55,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 2 ■ THE MISPERCEPTION OF RISK\n24\n An even broader concern is how a focus on business goals can drive people to make \nunethical decisions. When these decisions are made by managers at the organizational \nlevel rather than at the individual level, the impact is compounded by the potential for \nwidespread disaster. \n After the  Challenger space shuttle exploded in 1986, extensive post-crash analysis \nrevealed the tragedy was caused because an O-ring on one of the shuttle’s booster rockets \nfailed to seal due to the low ambient temperature at launch time. \n However, it subsequently emerged that engineers had warned of the potential danger \nbefore the launch. Engineers from NASA contractor Morton Thiokol recommended the \nshuttle not be launched at low temperatures after analyzing data that indicated a link \nbetween low temperatures and O-ring problems. After NASA responded negatively to \nthe engineers’ recommendation, Morton Thiokol’s general manager reportedly decided \nto treat the question of whether to launch was a “management decision.” Against the \nobjections of their own engineers, Morton Thiokol’s managers then recommended NASA \ngo ahead and launch, and NASA quickly accepted this recommendation (Bazerman and \nTenbrunsel 2011, 13–16). \n For Morton Thiokol’s managers, the desire to meet the business goal of pleasing the \ncompany’s customer, NASA, apparently caused the ethical dimensions of the problem to \nfade from consideration—with terrible consequences. \n According to Tenbrunsel, this  ethical fading is not uncommon. The way a decision \nis framed can limit our perspective. If the decision is framed purely in terms of meeting \nbusiness goals, ethical considerations may fade from view. In fact, we may become blind \nto the fact that we are confronting an ethical problem at all (Joffe-Walt and Spiegel 2012). \n Another infamous ethical lapse involved the Ford Pinto, whose gas tank exploded \nin a number of rear-end collisions, resulting in fatalities. As Bazerman and Tenbrunsel \ndescribe (2011, 69–71), Ford discovered the dangers in preproduction testing. \nHowever, facing intense business competition, the company decided to go ahead \nwith manufacturing anyway. The decision was based on a cost-benefit analysis. Ford \napparently considered the choice as a business decision rather than an ethical decision \nand determined it would be cheaper to pay off lawsuits than make the repair. The impact \nof dehumanizing this risk decision was disastrous. \n In the past, many information technology risk decisions have often been considered \nonly in terms of their potential business impact. As information technology is integrated \ninto more and more products, decisions about information risk will increasingly affect \nthe lives of millions of people, making it essential to consider the ethical as well as the \nbusiness dimensions of information risks. It becomes even more important that we, \nas CISOs, keep ethical considerations to the forefront. What is the potential impact \nof a security breach when a car’s sensors and control systems can be accessed via the \nInternet? Or when medical life-support equipment can be remotely controlled using \nwireless links? \n How to Mitigate the Misperception of Risk \n It should be apparent by now that the tendency to misperceive risk is universal. We need to \nfind ways to help compensate for this misperception, given that it is our job to manage risk. \nAs security professionals and managers, how can we  mitigate the misperception of risk? \n\n\nCHAPTER 2 ■ THE MISPERCEPTION OF RISK\n25\n We can start by ensuring that we include a diversity of viewpoints when making risk \nmanagement decisions. Whenever possible, we should involve a broad cross-section of \nindividuals representing groups across the organization. This diversity helps compensate \nfor individual biases. \n However, assembling the right mix of people is only the first step in building a \nmore complete picture of risk. As information security and risk professionals, we need \nto ensure that the discussion brings up new perspectives and views. We must ask \npenetrating questions designed to bring alternative viewpoints to the surface. I think \nof these as  high-contrast questions because the process is analogous to adjusting the \ncontrast or colors of a photograph to highlight key elements of possible interest. This \nquestioning counteracts the  inevitable bias due to target fixation. We can also help \ncounter target fixation by simply recognizing it exists, and then consciously trying to see \nthe problem from someone else’s viewpoint. \n In addition, we need to continually seek out the minority report, the view that is \ncontrary to perceived wisdom. If the majority is telling us to turn right, are we missing \nsomething important that we’d find out by turning left? In a striking example, Israel’s \nDirectorate of Military Intelligence considered this viewpoint so important that it created \na devil’s advocate office as an institutional safeguard against group-think. The office’s job \nwas to criticize analysis coming from the Directorate’s other divisions and write papers \ncountering the analysis. In order to explore alternative assumptions and worst-case \nscenarios, it examined possible radical security developments scenarios, including those \nthat the defense establishment considered unlikely. Notably, the office was staffed by \nexperienced, highly regarded people known for their creative thinking, and its reports \nwent directly to all major decision-makers (Kuperwasser 2007). \n Uncovering New Perspectives During Risk  Assessments \n Risk  assessment models can be valuable tools for helping to evaluate risks and to \nprioritize security resources. But all models have limitations. If we base our decisions \nsolely on the results generated by a model, we may miss important risks. \n Many organizations use a risk assessment model based on a standard methodology. \nThe model scores each risk using the following formula: \n Impact of Asset Loss × Probability of Threat × Vulnerability Exposure = \nTotal Risk Points \n For each risk, we assign a rating to each of the three contributing factors in the \nformula. To illustrate, I’ll use a scale of 1 to 5. A high-value asset, such as a microprocessor \ndesign, might warrant a rating of 5. \n We then multiply the three ratings to obtain the total risk points. In this example, the \nmaximum possible risk score is therefore 53, or 125. \n A simple approach to risk management, using the output of the model, would be to \ndivide the security budget among the highest-scoring risks. \n\n\nCHAPTER 2 ■ THE MISPERCEPTION OF RISK\n26\n The model is valuable because it provides a consistent method for helping compare \nand prioritize a broad spectrum of risks. However, allocating resources based only on \nthe overall risk score can miss potentially disastrous “black swan” events that have very \nlow probability but extremely high impact (Taleb 2007). Because the formula simply \nmultiplies three ratings to obtain the overall score, black swans tend not to score as highly \nas lower-impact events with higher probability. \n To counteract this problem, we can examine the information in the model in more \ndetail, from different perspectives. We can create a list of the 20 most valuable assets and \nconsider whether they need additional controls. In the same way, we can examine the top \nthreats and vulnerability areas. \n The point is that any model used to calculate risk should be used as a framework to \ndrive a dialogue about all the variables and options, rather than as a tool that generates \nthe answers to our problems. By discussing the issues from a variety of perspectives, we \nmay identify important concerns we’d miss if we simply look at the overall risk scores. \n Before I moved into the information security field, I worked in finance. In our \nfinance group, we found the same principle held true when conducting ROI (return on \ninvestment) analysis. Our ROI model generated forecasts. However, it was by discussing \nthe model’s assumptions that we determined whether or not the model’s predicted \nfinancial returns were reasonable. \n Another method for prioritizing information systems risk management is to examine \nsystems from the perspective of critical business processes and to consider the impact of \na loss of confidentiality, integrity, or availability. \n An application that prints shipping labels may initially appear to be low priority \nbecause it is small, inexpensive, and doesn’t contain confidential data; it simply takes the \ninformation it needs from a customer information system on the network. However, if it’s \nunavailable because the network is experiencing problems, the impact is huge because \nthe company cannot ship products. \n The potential impact to a business process of losing confidentiality, integrity, or \navailability may also vary depending on the stage of the business cycle. Consider a payroll \nsystem. Information confidentiality and integrity are always important, but availability is \nexceptionally critical on payday. \n Communication Is Essential \n Communication is an essential part of any strategy to mitigate the misperception of risk. \nTo alter the way people behave, we need to change their perception of risk. To effect that \nchange, we must communicate with them. \n Changing  perceptions is difficult. We may need to address long-held preconceptions \nabout what is risky and what is not. Once people form an initial estimate of risk, they can \nbe remarkably resistant to adjusting their perception, even when given new information \n(Breakwell 2007, 59). \n\n\nCHAPTER 2 ■ THE MISPERCEPTION OF RISK\n27\n In addition, each person may have a different perception of risk. To communicate \neffectively, we may need to understand an individual’s viewpoint and then tailor our \ncommunication accordingly. Consider the example of taking laptops to countries with a \nhigh risk of information theft (see sidebar). People who are extremely concerned may need a \npatient, thorough explanation of the risks and benefits of taking their laptop versus leaving it \nin the office. A less fearful individual may just need a quick reassurance and a few basic facts. \n Although changing risk perceptions can be challenging, we don’t have any choice \nbut to try. Employees will use social media whether we like it or not. When they do, they \nmay not only put themselves at risk; they could be putting the company at risk too, if they \nare not careful. \n Communication can reduce the issue of misperception due to asymmetry of \n information . This asymmetry is created when security professionals know about risks but \ndon’t share the information with end users within their organization. When two parties \ndiffer in their knowledge of a threat or vulnerability, their perception of risk is likely to \ndiffer also. In other words, it is difficult for users to care about a hazard if they don’t even \nknow it exists. \n To succeed in changing users’ perceptions, we must communicate in ways that \nengage them, using language they understand rather than technical jargon. In my roles as \na security professional, I have always tried to employ entertaining, interactive video tools \nto help engage users and teach them how to spot dangers such as phishing web sites. As \nI’ll explain further in Chapter  5 , I have found these methods have been highly effective in \nchanging users’ awareness and perceptions, and ultimately in shaping their behavior. \n Patiently explaining to users the consequences of their actions can also help shape \ntheir perception of risk. In some countries,  pirating software is so commonplace that it \nis almost an accepted part of the culture. This poses a problem for many multinational \ncompanies. Employees in these countries may not even believe copying software \nis wrong, let alone view it as an illegal act. It can be useful to describe the potential \nconsequences of copyright infringement for the individual and for the organization. We \ncan explain to employees that a decision to pirate software can expose the company to \nsoftware license compliance risks. The consequences may be even more far-reaching if \nthe copied software is then incorporated into the company’s technology-based products \nor services. If a product is discovered to include stolen software, the company may be \nunable to ship it to customers, which means a significant loss of revenue. Of course, \nemployees may experience personal consequences too: if they copy software, they run a \nhigh risk of losing their jobs. \n Organizations as a whole may also be blind to risks, or may simply choose to ignore \nthem. One way to overcome this misperception is to patiently build up a list of examples \nshowing how other organizations ignored similar risks and experienced adverse \nconsequences as a result, according to Breakwell, the University of Bath psychologist \n(pers. comm. 2012). The more examples in the list, the harder they are to ignore. \n “Organizations stick their heads in the sand, ostrich-like,” she says. “But if you have \na database of examples illustrating where things have gone wrong elsewhere, it becomes \nharder and harder to find enough sand to stick your head in.” \n\n\nCHAPTER 2 ■ THE MISPERCEPTION OF RISK\n28\n CHALLENGING PRECONCEPTIONS: TAKING LAPTOPS TO \nHIGH-RISK COUNTRIES\n \n It may be necessary to challenge perceived wisdom in order to expose a clear \npicture of the real risks, and consequently make the right decision.  \n Some companies react to the higher rates of intellectual property theft in certain \ncountries by barring employees from taking their corporate laptops on business \ntrips to those countries. In some cases, the companies issue employees with a new \n“clean” system from which all corporate data has been purged. \n The goal is to prevent situations in which information theft might occur, such as \nwhen an employee leaves a laptop containing corporate data unattended in a hotel \nroom. A malicious individual could then get physical access to the system and copy \nthe data or implant software that will surreptitiously steal information over time. \n But does preventing employees from taking their familiar laptops really solve \nthe problem? Let’s suppose we issue employees with a new, data-free laptop. \nTo do their jobs, they’ll still need to use this system to log into their corporate \ne-mail and other applications—providing an opportunity for hackers to intercept \nthe network traffic. \n Furthermore, if attackers really want to target an individual, they have ways to do it \nwithout gaining physical access to the system. With a spearphishing attack, they can \ninduce the individual to click on a malicious link that remotely downloads malware. \n Preventing employees from taking their laptops and information also deprives the \norganization of the key business benefits of using a full-featured portable computing \ndevice; employees will likely be less productive as a result. So when assessing the \nrisks of traveling with mobile devices, an organization needs to think through the \ntradeoff between risk and benefit, including the cost of providing what they believe \nto be a “clean” system and the impact on the user. \n Building Credibility \n Ultimately, our ability to influence people’s risk perception depends on our credibility. \nWe need to build trusted relationships with executives and specialists across the \norganization to ensure our security concerns are seriously considered rather than seen as \nfear-mongering or target fixation. \n Trust is built in drips and lost in buckets; it is hard to create and easy to destroy. \nIf we create a security scare about a threat that turns out to be irrelevant or overblown, \nwe may be seen as just another source of misperception. If business groups think we are \nproviding unreliable and exaggerated information, will they trust us to provide \ntheir security? \n\n\nCHAPTER 2 ■ THE MISPERCEPTION OF RISK\n29\n We can establish credibility by demonstrating consistency, striving for objectivity, \nand showing that we can accurately predict the real security issues affecting the \norganization, and then communicate them in an effective and timely way. As I’ll describe \nin Chapter  10 , we need to communicate security issues more frequently at C-suite level; \nto do so, we need to be able to clearly explain security issues in terms of enterprise risk. \n Credibility is also built on the competence that comes from understanding the \nbusiness and technology as well as possessing core security skills. As the scope and \nimportance of information security continue to expand, creating this credibility provides \nan opportunity to step into a more valuable, high-profile role within the organization. \n\n\n31\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_3\n CHAPTER 3 \n Governance and Internal \nPartnerships: How to Sense, \nInterpret, and Act on Risk \n If we are together, nothing is impossible. If we are divided, all will fail. \n —Winston Churchill \n To reduce cost, our company’s human resources group wants to move all HR-related \nprocesses to a SaaS provider, a cloud-based business that’s less than five years old. \nAt first glance, this might seem a low-risk decision. There’s a clear business case, and \noutsourcing HR systems doesn’t seem to create risks to corporate information assets such \nas intellectual property. Most businesses regard HR systems as commodity applications, so \nthey might select the supplier who can deliver the required functionality at the lowest cost. \n But there’s more to consider. Employees’ personal information will be transferred \nto the outsourcer, potentially creating new privacy concerns. And imagine the impact \nif thousands of our employees don’t get paid because the supplier experiences system \nproblems on payday and lacks adequate disaster recovery  capabilities . \n Clearly, the HR group owns the HR business processes. However, outsourcing these \napplications and processes can introduce risks for the entire business. The systems \nthat support HR processes can create  information risks . Outsourcing also involves \nprocurement. The business needs a clear overview of all the factors, including the risks, \nin order to make the best decision. To provide this view, the HR, procurement, and \ninformation risk and security groups need to work together. \n A typical organization makes many decisions that require this kind of  internal \npartnership to manage the risk. A product group wants to outsource development work \nto bring a product to market more quickly. A marketing team wants to engage a developer \nfor a new social media initiative. \n Similar considerations also apply to internal  technology transitions such as OS \nand application upgrades. Each new technology introduces new capabilities and risks. \nSometimes, the technology also includes features or options designed to help reduce \nrisk. By carefully analyzing the risk and security implications, including privacy and \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n32\ne-discovery considerations, we can help manage the risk of the transition, and we can \noften capitalize on the new features to improve the risk picture overall. \n For example, when Intel IT was considering whether to migrate to Microsoft \nWindows 7, the information  security team partnered with other groups in a broad \nevaluation of the OS. We identified several features that could improve security compared \nwith previous versions of Microsoft Windows, and these security capabilities were an \nimportant factor in the decision to deploy Microsoft Windows 7 across Intel’s enterprise \nenvironment (Fong, Kohlenberg, and Philips 2010). \n The ability to make these decisions with an accurate view of risk depends on having \nthe right organizational structure in place. Because each organization is different, there’s \nno single, standard risk management structure that applies to all organizations. But at any \norganization, building an effective risk management structure involves considering two \n key areas , which I’ll discuss in this chapter:\n• \n Clearly defined information risk governance : Governance \ndefines who makes decisions, who can block them, and who is \nallowed to provide input. \n• \n Strong partnerships and multi-stakeholder collaboration : \nCollaboration between the information risk and security team and \nother internal groups is critical in forming an accurate view of risk \nand managing risk overall. Some partnerships are formally defined \nas part of the risk governance structure; others are informal \nrelationships. These formal and informal relationships are so \nimportant that I’ll dedicate a large part of this chapter to them.  \n Information Risk Governance \n Governance is about establishing a structure that enables the organization to effectively \nsense, interpret, and act on risk. Traditionally, information risk governance has been \nconsidered as a component of IT governance. The IT-centric view is encapsulated in a \ndefinition from the Massachusetts Institute of Technology Center for Information Systems \nResearch (MIT CISR): \n “ . . . A framework for decision rights and accountability to encourage \ndesirable behavior in the use of IT. Governance identifies who will make \nkey IT decisions and how will they be held accountable.” \n But as every company becomes to some extent a technology company, we need to \nbroaden this definition to include the information risk associated with technology-based \nproducts and services. Perhaps a better definition for this broader view is “Governance \nidentifies who will make key  information risk decisions and how will they be held \naccountable.” \n Information risk governance focuses on enabling the business while protecting the \nconfidentiality, integrity, and availability of information, whether it is corporate data or \npersonal information about employees or customers. It requires the involvement of the \nentire organization. To achieve effective information risk governance, the information \nrisk and security team must work closely with other groups. \n",
      "page_number": 48
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 56-66)",
      "start_page": 56,
      "end_page": 66,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n33\n A company’s primary areas of information risk are closely intertwined, underlining \nthe need for an effective governance structure that embraces all of these areas. For \nexample, a hacker might compromise the IT systems used by the company’s product \ndevelopers, and then use those systems as a way to introduce malware into the company’s \ntechnology-based products. \n Think about how easily security researchers were able to hack into Jeeps and other \nvehicles over the past couple of years, demonstrating their ability to remotely take control of the \ncar with potentially  life-threatening consequences . Clearly, security may not have adequately \nconsidered such a scenario when the car’s product groups designed those features. Yet any big \ncompany, including automakers, typically has large teams of people dedicated to managing \ninformation risk. It seems that in the case of the automakers, the companies perhaps lacked \nan effective structure for managing information risk wherever it occurs, whether that is in the \ncompany’s products and services or within back-office IT systems. \n To some people, the word governance may imply unnecessary bureaucracy, or \nperhaps even a  dictatorial approach.  MIT CISR notes that “good governance is enabling \nand reduces bureaucracy and dysfunctional politics by formalizing organizational \nlearning and thus avoiding the trap of making the same mistakes over and over again.” \n Research at  MIT CISR shows that the more businesses leverage the structure, tools, \nand techniques of governance, the greater the potential benefits. In fact, MIT CISR’s \nwork suggests that firms with effective IT governance enjoy profits that average at least 20 \npercent higher than their competitors (MIT CISR 2012). \n However, leveraging governance doesn’t imply slavishly following rules and \nprocedures. A few years ago, I encountered an IT professional who was regarded by some \npeople, including himself, as one of the best managers in IT. He rigorously based his \nproject decisions on the prescribed practices and procedures, and gathered the correct \nmetrics for reporting progress. Yet the projects he was responsible for generally turned \nout to be large, expensive failures. His obsession with correct procedures often impeded, \nrather than facilitated, the projects he was working on. \n To use an analogy, if you gave the same recipe to a top chef and an average cook, \nwould you expect them to produce exactly the same result? Probably not. Expert chefs \ndon’t simply follow the rules; they continually make adjustments using their senses and \nexperience to achieve the best results. The temperature of a cooking surface is not exactly \nuniform, so a chef may move the pots until they’re simmering just right. Fresh ingredients \nvary from day to day; the experienced chef is alert to the differences and tweaks the recipe \nand seasonings accordingly. \n Like  the procedure-obsessed IT project manager, we may scrupulously adhere to the \nrules but fail to achieve the desired outcome. \n This is one reason that partnerships with other groups are so critical. They provide \nchannels for dialogue, helping us sense changing business priorities so that we mitigate \nrisk based on those priorities rather than our preconceptions. \n Without a governance structure that facilitates this dialogue, organizations may take \ntoo rigid an approach when applying controls to manage and mitigate risks. For example, \nsome security groups try to ban the business use of social media due to the risks, but \nattempting to stop the use of external social media web sites is counterproductive and, \nin any case, impossible. At Intel, we found it was more effective to embrace social media \nand shape the way that employees use it, as I’ll describe in Chapter  5 . This approach, \ndeveloped in partnership with other internal groups, enabled the organization to enjoy \nthe benefits of social media while managing the risk. \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n34\n Finding the Right Governance  Structure \n No single governance structure will fit all companies (see Table  3-1 and the sidebar \n“IT Governance Archetypes”). Furthermore, organizations may shift between different \nrisk governance models over time. When most organizations’ information assets were \nprimarily managed in centralized IT systems, it was natural for information risk to be a \ncentralized function managed within the IT group. But now, information-related risks \nare much more distributed. To drive corporate revenue, many companies are developing \ntechnology-based products and services more or less independently from the central IT \norganization. At the same time, business groups are shifting to cloud-based applications \nthat store corporate and customer information at external cloud providers. \n Table 3-1.  IT Governance Archetypes. Source: Weill and Ross  2000 \n Style \n Who has decision or input rights \n Business Monarchy \n A group of business or individual executives (CxOs). \nIncludes committees of senior business executives \n(may include CIO). \n IT Monarchy \n IT executives. \n Feudal \n Business unit leaders, key process owners, or their \ndelegates. \n Federal \n C-level executives and business groups; may also \ninclude IT executives. Equivalent of central and \nstate governments working together. \n IT Duopoly \n IT executives and one other group (for example, \nCxO or business unit leaders). \n Anarchy \n Each individual user. \n \nIT GOVERNANCE ARCHETYPES\n When considering the right risk governance structure for your organization, it may \nbe entertaining to think about how your organization compares with the deliberately \nprovocative governance archetypes, ranging from a feudal structure to anarchy, \nidentified by MIT CISR in the influential book  IT Governance (Weill and Ross 2000, 59). \n In practice, organizations may shift between different risk governance models over \ntime—from an IT-centric monarchy during the mainframe era, toward a feudal \nmodel or business monarchy as distributed systems emerged, swinging back to a \nfederal model as they recognized there’s a role for centralized IT, then shifting again \ntowards a business monarchy with the focus on technology-based products and \ncloud computing. \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n35\n Today, many organizations may find that it makes sense to establish a  hybrid \ngovernance model that balances centralized and decentralized risk management \nfunctions. At the same time, the need for a single, broad view of all information-related \nrisks is driving organizations to create an executive role with overall responsibility for \ninformation risk. The executive often has the title of Chief Information Risk Officer \n(CIRO) or sometimes the  Chief Security and Privacy Officer (CSPO) . The executive’s \nbroad responsibilities encompass the roles of Chief Information Security Officer (CISO)/\nChief Security Officer (CSO) and Chief Privacy Officer (CPO). \n To consider how this model works, let’s first think about all the interrelated risks that \nan enterprise needs to manage. Figure  3-1 shows each primary area and the core elements \nthat are common to all of them. The CSPO’s role is to manage this “Rubik’s Cube of risk.” \n Now consider the governance model, the organization’s framework for managing \nthose risks, shown in Figure  3-2 . It consists of four main areas:\n• \n Oversight : This area focuses on making informed risk decisions \nand reviewing risks. It includes committees and review boards \nthat set strategic direction, and review key risk areas such as \nethics, compliance, and corporate investigations. \n• \n Monitoring :  Monitor (sense) risk through external and internal \nsources. External sources include industry research and analysis. \nInternal sources include internal partners who inform us of new \nbusiness risks or legal requirements. These internal sources also \ninclude our own security technology sensors. \n Figure 3-1.  Security and privacy : the primary areas of information risk, and the core \nelements of information risk management that apply to each area \n \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n36\n• \n Engagement : Participate in industry workgroups and in \npartnerships and dialogues with trusted peer organizations. \nThese external  engagements provide a valuable risk-sensing \nfunction and help influence key security initiatives. I’ll discuss \nexternal partnerships in more detail in Chapter  4 . \n• \n Operations : Day-to-day risk management activities and \nprocesses, including risk assessments, incident response, and \nexercises such as war games. \n Typically, the  corporate governance model should achieve a balance of \ncentralization and decentralization. At most large companies, risk is decentralized: at any \none time, our companies are planning or managing many technology-related initiatives \nand events across practically every part of the business. Therefore, we need decentralized \nrisk management processes; too much centralization can mean losing the ability to sense \nthreats and respond in an agile way. But at the same time, we need a broad centralized \nview of the dynamic risk landscape and the ability to set organization-wide policies in \nareas such as security, ethics, and privacy. So the model must allow a centralized view \nand ownership of key risk functions, along with the ability for decentralized execution. \n The CSPO and the information risk and security team are involved in all four \nquadrants of the model. The  CSPO tends to be more focused on oversight and \nengagement, while the team’s members naturally tend to be more involved with \nmonitoring and day-to-day operations.  \n Figure 3-2.  A  corporate information risk governance model \n \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n37\n For most functions, the CSPO and team work with other parts of the organization, \neither taking primary responsibility or operating in a participatory role. In the Oversight \nquadrant, for example, the CSPO may sit on the ethics committee and participate in \nbusiness unit risk management reviews. In monitoring, the CSPO’s team may have \nprimary responsibility for threat landscape reviews and threat indicators, but take more \nof a participatory role in internal audits and assessing business unit risks. In operations, \nthe team may own responsibility for the security development lifecycle and privacy \nby design, while participating in change control. It should be apparent that all of these \nfunctions require collaboration with other groups within the organization. \n Building Internal Partnerships \n By providing vehicles for dialogue and decision-making, internal partnerships and multi-\nstakeholder collaborations enable  information security teams to become more agile and \nresponsive to business needs. The number of potential partnerships has grown as the \nscope of information risk has broadened to include a range of privacy and regulatory \nconcerns as well as traditional security threats. \n In mature and proactive organizations, the information risk and security team \npartners with many internal groups for a variety of functions, including risk management \ndecisions, incident response, and monitoring. These groups include legal, finance, \nhuman resources, physical security, and business groups. \n Partnerships may include formal structures such as  standing committees as well  as \n a large number of informal and ad hoc relationships. These are created and maintained \nthrough everyday communication with people in other groups. We might initially contact \na business group to understand the potential impact of an emerging area of legislation. \nThe business group identifies risks and opportunities that we hadn’t even considered. Our \ninitial request thus sparks a dialogue about requirements and controls, and ultimately \nevolves into a partnership that helps us monitor risks and mitigate them. We also gain \nbusiness acumen, which helps us play a more valuable role within the organization.  \n In my roles running risk and security, partnerships and multi-stakeholder \ncollaboration have been critical to my success in understanding the broader risk picture, \nhelping the organization sense, interpret, and act on risk. Through these relationships, \nother groups can act as additional eyes and ears for the  information security group , such \nas security threats and compliance concerns. For example, the HR legal group might \nalert us to an employment-related regulation that creates new compliance concerns. \nInformation about risks flows in the other direction, too: we may alert our partner to new \nthreats that we’ve encountered. As we leverage other groups to look out for our interests, \nthey can also use us to look out for their interests. We also work with partners to interpret \nthis shared information through analysis and decide how to act in response. \n Internal partnerships may focus on just one of the areas shown in Figure  3-2 , \nor they may intersect multiple areas. For example, we partner with HR for incident \nresponse (operations) and to learn about new employment laws (monitoring). Multiple \npartnerships may also be required within each focus area: with the growing number of \nregulatory requirements, partnerships with internal groups such as HR, legal, corporate \nsecurity, and internal auditing become increasingly important and valuable in the area of \noperational investigations. \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n38\n Because no two organizations are identical, each organization may require a \ndifferent set of internal partnerships, depending on its structure and business needs. \nEvery partnership should be created with a clear purpose. The organization should \nalso clearly define who is involved and who makes the decisions. To determine the \npartnerships your information security group needs, as well as their structure and \npurpose, it may be useful to ask the following questions:\n• \n Who do we need to partner with and why? To put it another way, \nwho do I interact with every day, and why do I interact with them? \n• \n What benefits do I receive from that interaction, and what \nbenefits does my partner receive? \n In the remainder of this chapter, I’ll discuss some examples of important partnerships, \ndescribing how we can use them and the value they provide. I’ll start by examining \npartnerships with  fellow travelers who have complementary roles in managing business \nrisk and liability: legal, finance, human resources, corporate security, and corporate risk \nmanagement groups. Then, I’ll examine partnerships with business group managers. \n Legal \n Legal groups are among the information security group’s most important partners \nbecause of the many areas where their roles intersect with ours. They own the \nresponsibility for legal compliance and legal review. They interpret laws, analyzing \nthe implications and relaying the relevant information to the rest of the organization. \nKey partnership areas include privacy, litigation, intellectual property, contracts, and \ncompliance with financial regulations. \n As companies create more technology-based products and services, their initiatives \nare likely to come within the scope of a broader range of laws and regulations. Health-\nmonitoring products might fall within the purview of the Food and Drug Administration; \ncompanies thinking about using drones for photography need to think about Federal \nAviation Administration requirements. \n Privacy \n As privacy regulations continue to grow in complexity and reach, many  organizations \nneed to comply with multiple requirements at local, regional, and national levels. Legal \nspecialists across the organization can help us understand what’s required in each \ngeography, align policies and controls for protecting personal information, and decide \nhow to manage responses in the event of a breach. \n Even local regulations can have implications across the enterprise. For example, \ncitizens of European countries are subject to European and national privacy laws and \nregulations. The simple transfer of European employee personnel information to a US-\nbased server will trigger a need to comply with the EU data privacy laws regarding such \ntransfer of employee information. \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n39\n Litigation \n As one might expect, it’s essential to partner with legal specialists in situations where \nlitigation is possible or already in process. Examples are investigations of security \nbreaches, particularly when law enforcement is involved. Another area of partnership is \nin responding to subpoenas and litigation discovery orders; a legal group may need to \nwork with the information security team in order to collect the required information. To \nensure that data is available for discovery when needed, we may also need to collaborate \nwith the legal group to implement appropriate data retention policies. \n Intellectual Property \n Many organizations use a  data classification  structure to protect intellectual property, \nwith the most highly classified information receiving the greatest protection. We work \nwith legal groups to specify the classification structure and then implement controls \non management and distribution of such information to provide the appropriate level \nof protection. We also partner to respond to suspected or known IP thefts. Suppose \nan employee loses a laptop storing the designs of future products; a dialogue with IP \nattorneys is essential to understand the implications and decide how to respond. \n Contracts \n Almost every contract with a supplier or customer contains a confidentiality provision, \nwhich sets expectations about how each party will maintain the confidentiality of the \nbusiness transaction and any shared confidential information. We partner with the \nprocurement organization as well as the legal group to define and implement these \nrequirements into contracts. \n If our company decides to outsource a business application to an external supplier, \nwe’ll typically work with the procurement organization and legal team to define these \nconfidentiality and data security expectations, as well as the evidence we’ll need to \nvalidate that those controls are operating properly. For example, when hiring a company \nto manage health benefits, we set expectations about how they must protect our \nemployees’ personal health information. \n Our customers have expectations, too. Another company may need to share some \nIP with us to help us integrate our technology into their product. We need to understand \ntheir requirements and ensure that appropriate controls are implemented. \n A security technology supplier has to meet customer expectations that go beyond the \nproduct’s ability to provide protection. As I mentioned in Chapter  1 , one of the irrefutable \nlaws of security is that even a security feature can be used for harm. So suppliers must be \nable to discuss their security development lifecyle, privacy by design, and overall state of \ninternal controls, all of which could ultimately affect the efficacy of the product. \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n40\n Financial Compliance \n In the United States and other countries, public companies are legally required to \ndisclose “material events,” those likely to have significant financial impact that could \naffect investor decisions, including IT-related incidents. An important aspect of risk \ngovernance, therefore, is partnering with legal groups to understand the types of events \nand specific incidents that must be reported. \n Guidance from the US Securities and Exchange Commission specifically discusses \nthe obligation to disclose the impact of cyber attacks, including those that result in IP \nthefts. Companies are also required to disclose material increases in security spending in \nresponse to an attack, even if the attack didn’t result in a loss of IP (SEC 2011). \n The legal team cannot do this alone because it lacks the security context of the event: \nthe frequency of specific types of attack, the potential impact, and the cost of response. \nTherefore, the security team must be involved. \n In 2010, Google disclosed that it had been breached in the widely publicized \nOperation Aurora attack. At around the same time, Intel also experienced an incident \nof similar sophistication. This was before the SEC issued its guidance in 2011, but as I \npondered the potential ramifications of a cyber breach one sleepless night, I realized that \nI should call our SEC legal experts to discuss the incident. Subsequently, we disclosed the \nincident in our financial report for the first quarter of 2010 (Intel 2010). \n Legal Specialists Within  Business Groups \n At large companies, each business group may have embedded legal experts. We need \nto work with them for issues directly related to their group. In addition, because of their \nconnections within the group, these legal professionals can be extremely helpful in \ninfluencing the group’s controls and expectations. \n Marketing groups, for example, usually include individuals who want to explore new \nways to communicate with users via social media. This appetite for adventure is a good \nthing; it can benefit the business. But at the same time, we have to ensure that content is \nadequately protected and includes appropriate privacy protection and statements. If we \nbring up the issue directly with marketers, we may receive a lukewarm response, as they \ntend to view any controls as restrictions on their ability to move quickly. But the legal \nprofessionals within the marketing group understand the need for controls. So a good \nway to raise our concerns is to have a conversation with the business group’s attorney, \nwho can help persuade others in the group that controls are needed. \n While I was Chief Security and Privacy Officer at Intel, we implemented a program \nthat reviewed all new externally facing online projects and monitors for potential problems \n(see sidebar). The projects ranged from web sites to more sophisticated tools, such as an \napplication that users can download and use in conjunction with external social media sites. \n As part of the review, we asked the project group who their legal contacts were so \nthat we could verify that they’d received legal approval. We also asked whether trademark \nand branding teams had reviewed the initiative, which was essential in many cases—\nespecially if the project was planning to register a new web site. Sometimes the answer \nwas no, in which case we facilitated a dialogue with the trademarks and brands team. \nThis enabled the trademark and brand people to manage the risk and helped forge yet \nanother important relationship within the company. \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n41\n \nSECURING INTEL’S EXTERNAL ONLINE PRESENCE\n Intel’s business groups use hundreds of web sites and third-party solutions, \nincluding social media platforms, to communicate and conduct business with \ncustomers and business partners. Collectively, these externally facing Intel-branded \nsolutions were known as Intel’s  external presence . \n Until 2006, these web sites proliferated rapidly in response to business needs, \nwithout centralized oversight. Given this growth and following a number of security \nincidents and the identification of several significant risks, we established the  Intel \nSecure External Presence (ISEP) program to provide appropriate security for Intel’s \nexternal presence (Leon 2011). \n The goals of ISEP, which was a part of Intel’s information security group, were to \nprotect Intel’s information assets and customers against threats such as loss of \npersonal information and malware attacks, and to maintain compliance with laws, \nregulations, and standards. By achieving these goals, we also helped to protect \nIntel’s corporate image. \n We helped ensure this protection and compliance by reviewing all planned new \nexternal presence projects and by monitoring existing Intel-branded web sites. \nISEP review and approval was mandatory for new externally facing online projects. \nWe worked with Intel business groups to review planned projects before launch, \nwhether they were to be hosted within Intel or by a third party. \n Any  ISEP-like process for reviewing a company’s external presence should include \nseveral key aspects:\n• \n Ensure notification of new projects by working closely with business \ngroups and other stakeholders within the company. For example, the \ninformation risk and security team should be notified when business \ngroups request new Internet domain names or seek approval to land a \nnew application in the externally facing IT environment.  \n• \n Work with the business group on each project to review details \nof the planned approach to maintaining security and privacy \ncompliance. Verify that the project includes any required mitigating \ncontrols before giving approval. \n• \n Establish an overarching governance board , including senior \nmanagers from multiple stakeholder groups. This board should have \nenforcement powers including the ability to shut down web sites for \nnoncompliance. \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n42\n Human Resources \n The human resources group is the organization’s center of expertise on  employee \nprocedures,  include legal specialists who are the organization’s experts on employee-\nrelated laws. Because of its responsibilities, the HR group also tends to be heavily involved \nin insider risk considerations and applying action in any cases that are discovered. In some \norganizations, HR is also responsible for other functions, including internal and external \ncommunications. Because of this broad charter, the security team may form valuable \npartnerships with HR in several areas, including employee policies related to appropriate \nuse and protection of information assets, internal communications, and investigations. \n Setting Employee Expectations in  Security Policies \n Employees are part of the security perimeter, as I’ll discuss in Chapter  . Their behavior \ncan have as much impact on security as the technical controls we use—particularly since \na growing number of user interactions with the outside world take place on external web \nsites and networks, and on personal devices such as smartphones. \n It is therefore critical to create employee policies that set expectations for secure \nbehavior. If we can influence employees to behave in more secure ways, we can reduce \nrisk for the business overall. However, the security team cannot write these policies \nwithout partnering with HR, including HR legal specialists, to ensure that they comply \nwith employment laws and the organization’s existing rules. Then, if an employee \ndisregards the policies, we need to work with HR to take disciplinary action. \n Careless behavior can have highly damaging consequences. Imagine an IT employee \nwho decides to store some corporate data on a server at his home so that he can more \neasily work on projects when out of the office. But his home system is open to the \nInternet, and thus the data may be broadly exposed to anyone worldwide. \n The employee’s action has created a significant security risk. To explain the potential \nimpact to HR, it may help to use analogies. We could say it’s like an engineer taking \ncritical product designs home and showing them to her neighbors. Or a factory employee \ntaking dangerous chemicals home to experiment with them, and creating the danger of \nan explosion in his garage. If we have a good relationship with HR, we can have this kind \nof discussion and determine the appropriate consequences for the employee. \n Employee Communications \n The responsibilities of the employee communications group often include employee \ntraining, employee awareness, and internal distribution of other corporate information. \nThis group’s expertise can be very useful when we want to communicate security \nmessages to the workforce. The group already has established communication channels \nand knows how to align messages with corporate style guidelines. A good employee \ncommunications group also knows how to present information in ways that engage \nemployees rather than intimidate them. \n In my prior roles running security and privacy, I always worked extensively with \nthe employee communications group to create engaging security awareness messages, \nincluding interactive content that helps encourage secure practices when using social \nmedia and the Web. \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n43\n Investigations \n Partnership with HR is also essential in internal investigations, including investigations \ninto insider  threats responses. In other cases, we may already be pursuing an \ninvestigation and need help from HR legal specialists to access employee information. \n Finance \n The finance group typically takes the lead in managing enterprise-level risk and controls \nfor the organization overall. Therefore, we need to partner with the finance group to \nassess the business impact of damage to information assets—a loss of confidentiality, \nintegrity, or availability. This applies not only to internal systems that support business \noperations, but also to information technology-based products and services that generate \nrevenue. We also work together to determine the required controls. \n Sarbanes-Oxley  Compliance \n The corporate finance team usually has overall responsibility for Sarbanes-Oxley ( SOX) \n infrastructure. We also work with the finance group, as well as legal groups, to determine \nwhether we should categorize specific events as material and report them as required by \nSOX. This also includes product- or service-related vulnerabilities and controls that could \nhave a material effect on revenue or corporate liability. \n Working with  Business Groups \n Each sizeable business group is likely to have a group controller or other financial \nspecialist responsible for financial controls. These finance experts can become important \npartners for the security team. \n Because financial specialists focus on risk and controls, the culture among finance \nspecialists has some similarities with the culture of the information risk and security \nteams. This shared focus can make it easier for us to communicate our concerns, \nparticularly since the impact of information risk is often measured in financial terms. \nTherefore, the financial specialist can be a key contact point when we need to discuss \ninformation risk with business groups. \n Sometimes these risk conversations can evolve into productive multi-way \npartnerships. A recent example: an IT team presented plans for new systems to support \none of Intel’s new businesses. As we assessed the information risks, we noticed that the \nplan didn’t include fully redundant systems to ensure business continuity. When we \nasked why, it emerged that the business group hadn’t requested redundancy because it \nwould add cost. Revenue from this new business was initially expected to be modest, so \nthe group’s budget was limited. \n However, when we discussed the revenue projections with the finance specialists \nwho worked on the project, they expected the business to grow rapidly. This growth \nwould also increase the information-related risk because a system failure would have a \nmuch bigger impact on revenue. As we discussed the implications, it became clear that it \nwould make more sense to prepare for the anticipated growth by including redundancy \n",
      "page_number": 56
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 67-74)",
      "start_page": 67,
      "end_page": 74,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n44\nfrom the start. So we suggested that the business group negotiate a higher budget—and \nthat’s what happened through a partnership between the business group managers, the \ninformation security team, and IT finance and business system specialists. The business \ngroup allocated increased funding that allowed IT to implement a redundancy safety net \nthat would protect the growing business. \n Internal Audit \n Financial groups are often also responsible for an internal audit, which typically \nincludes an IT auditing function—a job with considerable potential for overlap with the \ninformation security group’s role. If the security team and internal auditors duplicate \neach other’s efforts, we’ll waste resources and annoy business groups. Imagine that we \ncontact a business manager to say that we need to conduct a risk evaluation of the group’s \nsystems. The next day, internal auditors contact the same group and say they’re planning \nto do an audit, which some business managers might perceive to be essentially the same \nas a risk evaluation. What kind of reception do you think the auditors would receive? \n We can minimize the overlap by partnering with internal auditors. This partnership \nbecomes a mechanism for effectively allocating risk management resources. If the \ninformation security team has already assessed a system, auditors may be able to increase the \nefficiency of an audit by leveraging the work that the security team has already performed. \n For effective partnership, our work must be thorough, transparent, and well \ndocumented so that auditors can see what we have done. We may also swap resources: \nsometimes security experts may act as guest auditors for specific projects because they \nhave skills that the financial group lacks. The partnership can also be used for valuable \ndialogue and mutual support. If we’re concerned about a system that internal auditors \nhave previously examined, we can ask for their opinion. We’ll sleep better knowing that \nanother group of objective, risk-focused specialists has analyzed the system. \n Corporate Risk Management \n Most large organizations employ people whose job includes purchasing insurance \nfor general business risks, including property and casualty insurance to protect the \norganization in the event of damage to a data center or another facility. When buying \ninsurance, the corporate risk management team may need information from us about \nthe organization’s IT business continuity and disaster recovery plans. Insurers ask for this \ninformation in order to set premiums. \n Today, the corporate risk management team usually focuses on physical risks. But \ntheir scope is rapidly expanding to include IT-related risks as well as risks associated with \nproducts and services. Privacy breaches or other compromises can have a major impact \non a company’s revenue, cost, and brand image. Because of this trend, insurance against \ncyber risks is a rapidly growing category, and we can expect a growing need to partner with \nthe corporate risk management team to ensure adequate coverage of information risks. \n Consider the case of Sony, which suffered a breach of its PlayStation Network—\nestimated by the company to cost at least USD 200 million (Perlroth 2011)—and then \nbecame embroiled in a legal dispute with its insurer, which claimed Sony’s insurance \npolicy did not cover cyber risk. The breach at Target, in which hackers stole the payment \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n45\ncard accounts and personal information of millions of customers, is estimated to have \ncost the company roughly $250 million. Reportedly, the insurance payout of $90 million \nleft the company $158 million in the hole, plus what it paid for cyberattack insurance. \n Privacy \n Privacy and security are closely linked. However, increasing security doesn’t always \nenhance privacy. In fact, it can have the opposite effect. Unfettered bulk collection and \nmonitoring of the information and activities of users and their machines may be capable \nof increasing security, but it may also intrude on personal privacy. This data store may \nalso be an attractive target for intruders. \n This creates inherent tension between security and privacy interests. This tension \nis apparent at a national level in the way that privacy advocates respond to the use of \nsurveillance and data mining. Government security organizations may feel that they \nprotect data extremely well, but privacy advocates still object to the fact that information \nis collected and the way it is used. \n Similar concerns apply at the enterprise level. We need to carefully manage the \nrelationship between security and privacy, ensuring that we apply the appropriate level of \ncontrols to protect information without infringing on personal privacy. \n The structure of this relationship varies between organizations. While at Intel, the \ninformation risk group that I managed for over a decade included the privacy team, \nwhich reported to me as the CISO. Then as we began to see growing confluence of the \nrisks shown in Figure  3-1 , I was promoted to a broader role as Chief Security and Privacy \nOfficer, to give us an integrated governance and accountability structure. At other \norganizations, privacy is the responsibility of a separate group headed by a Chief Privacy \nOfficer who is the CISO’s peer. This arrangement necessitates careful management of \nthe relationship between security and privacy teams to manage tension, align policies, \nand control breaches. In organizations with this structure, the security team sometimes \ncomplains that the privacy team is “getting in their way,” which usually means that the \nsecurity team wants to collect specific information and the privacy team objects. \n Regardless of the organizational structure, it is the security team that is logically \nresponsible for implementing IT controls. It is the product security team that is \nresponsible for security development lifecyle ( SDLC ) and  product security incident \nresponse processes (PSIRT) . Laws define privacy rights; the organization’s interpretation \nof those laws drives compliance requirements. It is the security team’s responsibility to \ndetermine how to implement controls to support those requirements. \n Corporate Security \n The corporate security team focuses on physical security concerns ranging from door \nlocks and guards to break-ins, fires, and natural disasters. By partnering with this team, \nwe can make sure we’re aligned on protection of key information assets. It wouldn’t make \nsense to implement sophisticated data-protection tools on the servers in the data center \nand then leave the data center doors unlocked. \n We also need to coordinate on other issues, including incidents that involve law \nenforcement. Not so long ago, assaults and harassment were almost always physical \nincidents handled by corporate security and the police. Today, there’s a much bigger \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n46\noverlap with information security. More crime is moving online, and we may encounter \nother problems, such as cyber bullying. Because of these trends, we may need to help \nassess the impact and drive the response. \n Business Group Managers \n Each business group has its own processes and applications, whether it’s a product-\nfocused unit responsible for generating revenue or an internal group managing finance \nor human resources. The information security team needs to partner with each group to \nimplement security controls that protect the group’s applications and information. \n As the business acumen of our information security team increases, we can better \nfulfill our Protect to Enable mission by focusing on controls that improve security without \nimpeding the business.  This applies not only to the systems that support business \noperations, but also to the technology-based products and services the business unit \ncreates. For example, we may discover product vulnerabilities through our security \ndevelopment lifecycle processes. We can partner with the business group to correct \nvulnerabilities before shipment, and we can work on training to prevent future mistakes \ndue to poor coding, design, or architecture. \n By working with business groups, we can also leverage their strengths. Business \ngroup managers can help drive decision-making and incident response. They can also \nhelp improve security by setting the “tone at the top,” publicly setting expectations for \ntheir employees’ security behavior. Suppose we notice that an increasing number of the \nemployees at a specific facility are experiencing laptop thefts. We discuss the trend with \nthe general manager and explain that we want to increase employees’ awareness with \nmessages about how to prevent theft. The business manager may offer to help by bringing \nup the topic at a site meeting or otherwise directly communicating with employees. This \nmanagement request may exert a more powerful influence on employee behavior than \nmessages sent by the security group. \n \nHOW TO RESPOND TO EMERGENCIES\n Defining a clear IT incident  response process is an essential aspect of IT governance. \nSimilarly, a clear PSIRT is an essential aspect of risk governance for technology-\nbased products and services. Over time, while I was at Intel, we developed a clearly \ndefined crisis management process for responding to emergencies and other \nsignificant incidents that affect IT infrastructure or services (Fleming and Tomizawa \n2012). The goal of the process was to prevent material impact to the organization and \nits employees. Similarly, the goal of a PSIRT process is to prevent material impact to \ncustomers or even to society in general, depending on the nature of the risks. \n Incidents that may trigger the process include cyber events and other information \nsecurity incidents; physical incidents such as fires, leaks, and major outages \nthat affect IT systems; and major disease outbreaks. A useful starting point for \ndeveloping the process is the incident management principles based on the US \nFederal Emergency Management Agency’s response to disasters. \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n47\n Once initiated, an IT emergency response process ( ITERP) • operates with a \ncommand-and-control structure, led by an incident commander who has overriding \nauthority to make decisions across IT for the duration of the emergency. The \nstructure consists of a virtual organization staffed on a volunteer basis by people \nfrom every discipline within IT. When an incident occurs, all team members perform \ntheir response roles instead of their normal duties until all issues are resolved. \n Following an incident, the team should quickly identify the state of critical business \nprocesses that must continue during the crisis. It determines the current status \nof the key steps in the product cycle: design, build, order, ship, pay, and close. It \nassesses the physical state of the infrastructure, and analyzes the legal and other \nimpacts if intellectual property or personal information is compromised. Decisions \nabout response and remediation are driven by the incident commander and \ndetermined by business priorities. \n PSIRT and privacy response processes should be structured along similar lines, \nfocused on their respective mission-critical priorities. \n While I was at Intel, the ITERP team, the PSIRT team, and the privacy incident \nresponse team proved to be essential components of the successful resolution of \nevery crisis management, coordination, control, and communication activity across \nthe company during my 13.5-year tenure.  \n Conclusion \n Information risk has become a major concern for the entire organization. Managing \ninformation risk therefore requires a clear governance structure that enables the \norganization to make the right security decisions quickly and effectively. \n Building the right governance structure can sometimes seem like a complex \nchallenge. I’ve found that a good way to simplify and focus the thought process is to \nconsider the following two cardinal rules. In my experience, these rules apply to all \norganizations, whether large or small, public, private, or non-profit.\n• \n Rule 1:  Structure drives behavior . Thinking about the behaviors \nthat you want to see in the areas of security and privacy will help \nlead you to a structure that encourages those behaviors. \n• \n Rule 2:  You get what you measure . Thinking about the desired \noutcomes will help you determine how you should measure your \norganization’s success in managing risk. \n Think about how your own organization manages information risk. Do you develop \nstrategies in close collaboration with business groups? Do you feel that you communicate \nwell enough with every group to understand their priorities and implement controls that \nreflect them? Have you clearly defined all of the processes required to respond to a major \nbreach or denial-of-service attack? If you answered “no” to any of these questions, you \nmay need to improve your information risk governance. \n\n\nCHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n48\n Effective governance relies on partnerships between the information security team \nand other internal groups across practically every part of organization. In this chapter, I’ve \ndescribed some of the most important partnerships and the value we can derive from them. \n To develop these partnerships, CSPOs as well as Chief Security Officers and Chief \nPrivacy Officers need more than just technical skills. We need to communicate in terms \nbusiness people understand and build relationships that enable us to influence people at \nall levels across the organization. As the scope of information security expands, we also \nneed extensive management and leadership skills, both to operate at an executive level \nand to coach and inspire our risk and security team. \n\n\n49\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_4\n CHAPTER 4 \n External Partnerships: \nThe Power of Sharing \nInformation \n Chance favors the connected mind. \n —Steven Johnson \n After spending a day at a conference, I was having dinner with a dozen or so peers when \na debate began about the dangers and benefits of sharing security information with other \ncompanies. One person turned to me and asked me whether, if I had information about a \nspecific new threat, I would share it with him. \n “You bet,” I said. \n “But what if I was your competitor? Would you still share?” he asked. \n “Our companies might compete for business,” I replied, “but in the security \narena, my real competitors are the malicious actors who want to harm my company’s \ninformation systems. Those are my competitors, and they’re your competitors, too.” \n As soon as I’d said this, several people at the table agreed. This agreement was \ngratifying—and not just because I felt that I had support for my views. The bigger implication \nwas that my peers saw the value of sharing information outside their companies. \n This hasn’t always been the case. Historically, many organizations frowned on the idea \nof sharing security information externally, and more than a few had policies forbidding it. \n However, attitudes are changing. Although there is still resistance at some companies, \nmany organizations now see the value of sharing information and have begun doing so. \nEvidence includes the growth of industry-specific information-sharing communities, \nsuch as the retail-industry group that formed after Target’s massive customer-information \nbreach in 2013. There are also innovative partnerships that have a regional rather than \nindustry-specific focus, such as the Arizona Cyber Threat Response Alliance. \n Supportive actions by the US Government have also helped encourage  information \nsharing . In 2014, the Federal Trade Commission and Department of Justice issued a \npolicy statement indicating that sharing threat information was unlikely to raise antitrust \nconcerns. This addressed a key reason that some big organizations had been reluctant to \n\n\nCHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n50\nshare information. “Cyber threats are increasing in number and sophistication, and sharing \ninformation about these threats, such as incident reports, indicators, and threat signatures, is \nsomething companies can do to protect their information systems,” said Bill Baer, an Assistant \nAttorney General in charge of the DoJ antitrust division (U.S. Department of Justice 2014). \n In 2015, the White House issued a statement encouraging information sharing as a \nway to help safeguard national and economic security, and directing the Department of \nHomeland Security to support the formation of information-sharing groups under the \numbrella term  Information Sharing and Analysis Organizations (ISAOs) . And in 2015, \nlegislation was proposed to promote sharing of threat information, although the effort \nstalled in Congress. \n Despite the overall shift in attitude, some organizations still have reservations \nabout sharing information. There are three major areas of concern. First, organizations \nworry about the legal and regulatory implications of  revealing  information about \nthreats. A second, related concern is the public relations  aspect . Both of these fears \nhave a valid basis. Information security has become an enterprise risk management \nissue of board-level interest because of the potential effects. Information leaks revealing \npotential intrusions and data breaches can have legal consequences: the organization \nmay be required to report the problems in order to comply with financial and privacy \nregulations, for example. If security issues become public, they may also damage the way \nthe organization is perceived by customers and by the business community, potentially \naffecting a company’s profitability and its stock price. The third major area of concern \nis privacy. This also has a valid basis. For example, sharing information that identifies \nthe victim of an attack, as some security specialists would like to do, clearly can expose \nmachine data that can potentially compromise the victim’s privacy. Some people also \nsee a risk, following the revelations of National Security Agency eavesdropping, that \nlegislation could be used to enable government surveillance. For these reasons, I believe \nthat any cybersecurity legislation must include appropriate privacy protection. \n What’s the payoff from sharing information? My personal experience is that I have \nobtained real value: information shared by others has helped me understand threats \nand take action. I have also seen that it’s possible to share useful information while \navoiding the issues mentioned above. Companies can share information about attacks \nwithout revealing personal information about the victim. They can share indicators of \ncompromise without revealing confidential information. They can alert other trusted \ncontacts during the early stages of investigating a threat, before it’s been determined \nwhether a compromise has occurred that requires regulatory disclosure. \n The growth of information-sharing groups shows that many other organizations now \nshare my belief in the value of sharing information  about  threats and best practices. As \nI’ll explain in this chapter, sharing security information can provide considerable benefits \nin managing the risk of moving into new business relationships and adopting new \ntechnologies. We just need to find ways to reduce the risk of sharing. The solution lies in \ncreating trusted information-sharing relationships with other organizations. The more we \ntrust the relationship, the more sensitive the information that can be shared. \n The need to  share security information is being driven by rapidly changing business, \ntechnology, and threat landscapes. Increasingly, companies are collaborating with a \nbroad variety of business partners. We share business information, and often we also \nuse the same technology, or we sell or share technology with each other. As we do so, we \nalso share risks. Understanding the risks faced by our partners, and the way they manage \nthose risks, can help us protect our own organizations. \n\n\nCHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n51\n Looking more broadly across the  technology landscape , all systems and devices are \nto some extent connected, whether they are owned by enterprises, individuals, or service \nproviders. Almost every aspect of society depends on a worldwide, rapidly evolving, \nhighly complex network of devices and services. This provides the central nervous system \nthat supports innovation, economic development, and social interaction worldwide. \nBut because we are all inherently interconnected, we share common risks. The threat \nlandscape is dynamic, global, and increasingly complex. Threats may originate in any \ncountry and then spread rapidly across national and enterprise boundaries, causing \nextensive damage to organizations and individuals worldwide. \n Because threats spread so quickly and the  threat landscape is so complex, it is hard \nfor any single organization to gain a clear view of all potential vulnerabilities, threats, \nand attacks. External partnerships can help. They provide additional intelligence that \nwe can use to improve our own security posture. By exchanging information with other \norganizations, we gain what I call  outsight , or a better understanding of what happens \noutside our own environment. We learn about new threats before they hit us directly. We \nsee how other organizations are managing those threats. We learn about best practices \nfor managing security operations. Using the information we gather from external \nrelationships, we can increase the organization’s ability to sense, interpret, and act on risk. \n The Value of External Partnerships \n Sharing security-related  information  can require initiative and courage. The idea of \nsharing information externally may run counter to the culture of the organization \noverall, including the culture within the security group. Organizations may view security \ninformation as proprietary and confidential, like intellectual property. Many still have \npolicies against sharing information. \n It’s true that much security information is sensitive, and sharing it can introduce \nrisks. Because of this, we need to be careful about what we share and with whom. \n But think about the broader context of how organizations are increasingly sharing \ninformation. Most organizations have already recognized that they need to share \nsensitive business information with partners in order to develop, manufacture, and \nmarket new products. Collaboration with other companies is becoming an integral part \nof many other business processes, too. As organizations share information, they benefit \nfrom their partners’ insights and expertise. As noted by Steven Johnson, author of  Where \nGood Ideas Come From: The Natural History of Innovation (Riverhead Books 2010), many \nof the best ideas have emerged not through the inspiration of a single mind, but through \nthe exchange of ideas. “You have half of an idea, somebody else has the other half, and \nif you’re in the right environment, they turn into something larger than the sum of their \nparts,” Johnson said in a speech at the 2010 TEDGlobal conference (Johnson 2010). “We \noften talk about the value of protecting intellectual property—building barricades, having \nsecretive R&D labs, patenting everything that we have, so that those ideas will remain \nvaluable … but I think there’s a case to be made that we should spend at least as much \ntime, if not more, valuing the premise of connecting ideas and not just protecting them.” \n I believe that there’s similar value in sharing security information. As we collaborate \nwith business partners, we need to understand the threats to their environment, and \nhow they manage risk, in order to determine what we need to do to protect our own \norganizations. Each partner in a value chain needs to protect information to a level \n",
      "page_number": 67
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 75-82)",
      "start_page": 75,
      "end_page": 82,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n52\nthat is adequate to protect the other partners; the weakest link in the chain can impact \neveryone. Note that throughout this chapter, I use the terms “partner” and “partnership” \nin the colloquial sense, not to imply any specific type of formal legal relationship. \n There are many other examples of how sharing information can benefit all \norganizations involved. If we are entering new markets through business partnerships, \nwe need to understand the nature of the threats in those markets from the companies \ncurrently operating there. The same logic applies to using new technologies. \nOrganizations are extending their environment to customers and becoming suppliers \nof mobile apps and web services in the process. As they do, they can learn from other \ncompanies’ experience how to manage the risks. Companies are increasingly sharing \ncloud capacity or other data-center infrastructure supplied by external providers, and can \nall benefit by sharing feedback with the provider about risks within the environment. \n Despite these trends, some organizations still have policies stipulating that employees \nshouldn’t share internal information about risks and threats with anyone outside the \ncompany. This is sometimes the case even when the same organization willingly shares \nother IT-related information such as helpdesk or e-mail management best practices. \n Without wishing to discount the real fears driving these policies, the value of sharing \ninformation often outweighs the risk of doing so. Let’s imagine that a CISO learns of a new \nthreat affecting companies in his industry sector. He shares information about the threat \nwith a peer at another company and, by doing so, gains insight that helps the organization \nmitigate an attack that has caused massive damage at other companies. By sharing \ninformation against company policy, the CISO took a personal risk. Yet by doing so, he \naverted the bigger risk of business disruption and damage to the organization’s reputation. \n Failure to share information with others introduces its own risks. If we don’t share \nwith peers, they won’t share with us, so we won’t benefit from their information and \ninsights. I’ve seen cases in which information security professionals wanted to participate \nin communities, but weren’t allowed by their companies to share any internal security-\nrelated information. So they attended meetings but couldn’t contribute. Ultimately, their \npeers wouldn’t tolerate a situation in which these people were receiving information but \ngiving nothing in return, and they were effectively voted off the island. \n External Partnerships: Types and Tiers \n Much of the publicity about information-sharing initiatives has focused on public-private \npartnerships related to critical infrastructure and national security. However, there are \nmany other types of formal and informal external information-sharing relationships, \nincluding 1:1 partnerships and groups comprised solely of private-sector organizations. \n External partnerships are most often used to share information about specific threats \nand best security practices. But some partnerships focus on other  types of information. \nFor example, security specialists within the high-tech sector share information in order to \ndevelop security standards, which are then implemented in various products. \n Much of this security information is sensitive. Because of this, we need to be able to \ntrust that the partners with whom we share information will treat it appropriately. The more \nsensitive the information, the greater the level of trust required. In general, the level of trust \ncan be higher in relationships with fewer people, allowing more-sensitive information to \nbe shared. As the number of people increases, there’s a greater chance that information will \nleak, so the level of trust tends to decrease and only less-sensitive information is shared. \n\n\nCHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n53\n Relationships therefore naturally tend to fall into a  tiered pyramid model , as shown \nin Figure  4-1 (Willis 2012). At the top of the pyramid are the most-trusted relationships \nwith the fewest partners; these are 1:1 partnerships between two individuals at different \norganizations, or between two security teams. \n Information-sharing relationships between more than two partners are often \nreferred to as communities. Because more people are involved, a legal or peer-enforced \nagreement is usually needed to define the level of trust and confidentiality expected \namong community members. \n The two middle tiers of the pyramid include groups with intermediate levels of trust, \nsharing information with varying levels of sensitivity. The  targeted tier typically consists \nof public-private partnerships aimed at protecting critical infrastructure. The  confidential \ntier includes many private-sector communities, including regional communities and \nthose focused on specific industry sectors. \n At the bottom of the pyramid is the  public tier , comprised of the largest communities \nwith the lowest level of trust. At this level, information is often public and may be broadcast \nvia the Internet. This tier might include groups that develop educational information about \nthreats for public distribution, or CISOs who share their insights via public webcasts. \n I should note that there is considerable overlap between these tiers. A group may have \ncharacteristics of both the targeted and confidential tiers, for example. Also, the number of \nmembers in groups within each tier (shown in Figure  4-1 ) is just a guideline: communities \nat all levels tend to grow over time as more organizations see the value and join. \n Figure 4-1.  Tiered pyramid model for trusted information-sharing partnerships and \ncommunities (adapted from Willis 2012). Source: Intel Corporation, 2012 \n \n\n\nCHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n54\n How can you get involved in information- sharing partnerships? One good method is \nto start by participating in communities in the public tier, where the information shared has \na relatively low level of sensitivity and therefore involves little risk. In these communities, \nyou’re likely to meet peers with whom you can begin to engage in 1:1 partnerships. As you \nbecome more knowledgeable about the communities that reflect your organization’s key \ninterests, you may then become involved in relationships in the middle tiers of the pyramid, \nwhere more confidential information is exchanged. I have always made sure that my teams \nand I actively participate in partnerships at all the tiers of the pyramid. \n 1:1  Partnerships Tier \n• \n Community structure : Direct communication between CISOs at \ntwo organizations or between their teams \n• \n Typical number of partners : 2 \n• \n Example partnership/community : Any two organizations who \nchoose to share information \n• \n Example goal : To mitigate shared threats by exchanging \ninformation with a business partner more quickly and in greater \ndetail than would be possible within a larger group \n• \n Trust framework : Personal trust and existing business \nrelationships \n Targeted Tier \n• \n Community structure : A relatively small number of critical \ninformation infrastructure owners and operators sharing \ninformation to protect the infrastructure. Also includes key \nsecurity ecosystem influencers, such as large security service \nproviders or vendors. \n• \n Typical number of partners : Up to about 50 \n• \n Example partnership/community : Information Sharing and \nAnalysis Centers (ISACs) \n• \n Example goal : To prevent advanced persistent threats (APTs) \nwithin the industrial base by sharing APT signature information \n• \n Trust framework : Strong information-sharing frameworks, such \nas national security clearances and nondisclosure agreements, \nare required. Trusted sharing mechanisms, such as encrypted \nweb portals with multifactor authentication, are also required. \n Confidential Tier \n• \n Community structure : Communities that represent industry \nsectors or other groupings, such as the banking sector and \nInternet service providers (ISPs), or regional forums \n• \n Typical number of partners : Up to about 100 \n\n\nCHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n55\n• \n Example partnership/community : BITS (financial services \nindustry), Bay Area CSO Council (regional), Regional CSO \nSummits \n• \n Example goal : To enable members to protect against common \nthreats and vulnerabilities affecting their industries. For example, \nISPs might share the command and control Internet addresses \nthat botnets use. \n• \n Trust framework : Communities typically use trust frameworks \nsuch as nondisclosure agreements or memoranda of \nunderstanding. \n Public Tier \n• \n Community structure : A broad range of communities that \nrepresent all user categories, including consumers, small- and \nmedium-sized businesses, and industry in general \n• \n Typical number of partners : 100s to 1,000s \n• \n Example partnership/community : Forum for Incident Response \nand Security Teams (FIRST), National Cyber Security Alliance \n• \n Example goal : To share best practices or informational bulletins \nabout widely known threats and vulnerabilities that affect a large \ncross-section of users. \n• \n Trust framework : Trust frameworks are not necessary; \ncommunities typically distribute information broadly through \nmechanisms such as e-mail distribution lists or public web sites. \n Let’s look at these tiers in more detail. \n 1:1  Partnerships \n In my experience, 1:1 partnerships are some of the most valuable security relationships. \nThey may be formal or informal, established at a corporate level or between individuals. \n As I explained, a key  advantage of a trusted 1:1 partnership is that we can more safely \nshare highly confidential information. We can often create a stronger bond with a single \nindividual than with a larger group. As a result, the shared information often has a depth \nand richness that’s lacking in information shared within larger communities. \n Another advantage is speed. Communication is often fastest in 1:1 partnerships, \npartly due to logistics. It’s much easier to set up a meeting between two people than it \nis to organize a meeting with a dozen people. To exchange information about the latest \ndevelopments, a CISO may be able to simply pick up the phone and have a conversation \nwith his or her peer. Quickly sharing information enables a faster response to threats—\nand in the security arena, timeliness is often critical.  \n\n\nCHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n56\n Here’s an example showing how 1:1 partnerships can develop and benefit both \npartners. Through my participation in a larger security community, I got to know \nthe CISO at a fast-growing e-commerce company whose customers were primarily \nconsumers. We both would contact each other periodically for advice and information \nas we puzzled over the latest security challenges. Over time, these conversations evolved \ninto open dialogues about best practices and benchmarking. \n The relationship eventually evolved to a point where we both realized we could learn \na great deal more by bringing our teams together in a face-to-face meeting. The resulting \nhalf-day meeting proved incredibly valuable to both teams. Our team was able to \nprovide insights and experiences about managing security in a large, complex enterprise \nenvironment. This was helpful to the security team at the fast-growing e-commerce \ncompany, which was in the process of building an enterprise environment to support \nits fast-growing business. In return, the team at the e-commerce company was able to \nshare the security challenges and experiences of operating a large consumer business \nwith millions of online customers. This was extremely valuable to us at Intel because we \nwere in the process of expanding our external online presence and were beginning to \nencounter some of the same challenges. \n The partnership thus expanded from ad hoc conversations to a productive relationship \nbetween teams sharing experiences and best practices at multiple levels. It’s hard to imagine \nthat this extensive information exchange could have occurred within a larger community. \n Another example: I met the CISO of a large manufacturing company at an industry \nevent, and we stayed in touch through occasional e-mails. Then, during a period of \nespecially large-scale industry attacks, our communications suddenly became much \nmore frequent and detailed. It was extremely valuable to be able to pick up the phone and \nsimply call a peer to share the latest knowledge about the attacks and responses. \n I have frequent 1:1 meetings with peers at other companies, sometimes as often as \nseveral times a week. These meetings can serve several purposes. A few years back, I met \nwith a team from a key supplier to discuss our strategy for securing employees’ personal \n(bring-your-own) devices. I shared our best practices with this team, and during the \nquestion-and-answer discussion, team members also provided information about how \nthey were addressing the same problem. The meeting served as a helpful benchmarking \nexercise for all of us. \n At the same time, the discussion clearly demonstrated each company’s commitment \nto protecting its partner’s business information. It showed the depth of each company’s \nstrategy for protecting information—revealing a commitment that extended far beyond \nthe desire to comply with contract confidentiality clauses. I felt more confident that if a \nsecurity issue ever arose, I could talk directly to my counterparts at the supplier company \nbecause their commitment to protecting information would enable a productive \napproach to resolving problems. \n Another recent discussion, this time with a potential customer, focused on the cloud. \nThe organization was concerned about our use of the cloud as part of our infrastructure, and \nalso as a part of the service connected to our product. Rather than respond to the lengthy \nsurvey they had put together, we met with them to discuss how Cylance uses the cloud and \nwhich data we store there. We discussed the risks that could exist in the cloud infrastructure, \nthe potential implications of those risks, and how we manage those risks. We also discussed \nother precautionary steps the customer could take to further mitigate the potential risks. This \ndiscussion helped develop a relationship that built the most customer trust. \n\n\nCHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n57\n Communities \n Participating in larger communities may not provide information that’s quite as rich and \ndeep as the information you’d obtain from a 1:1 partnership with a peer. But communities \nprovide value in other ways. \n Because they contain more people, communities provide breadth and diversity \nof perspective that help us make balanced risk decisions. With a larger number of \nparticipants, there’s a better chance that one of them will have developed a solution to a \nproblem, or can provide valuable new information about an industry attack. \n Some communities focus on sharing threat-related information; others on \nbenchmarking and best practices, influencing legislation, developing security standards, \nor public education. \n Communities can also present great networking opportunities. Through \nparticipation in communities, I’ve met several people with whom I’ve subsequently \ndeveloped closer 1:1 partnerships. \n Community Characteristics \n Like all groups, communities require a structure and a set of ground rules to be effective. \nSuccessful communities typically have the following characteristics:\n• \n Clear goals : The community shares clearly defined common \ngoals that benefit members, such as mitigating an industry-wide \nthreat. A community may have several goals. \n• \n A strong framework of trust, such as a legal or peer-enforced \nagreement, that addresses risks related to information sharing \namong community members : For example, the Industry \nConsortium for the Advancement of Security on the Internet \n(ICASI) has a strong multilateral nondisclosure agreement, while \nother communities, such as the Bay Area CSO Council, rely on a \npeer-enforced trust framework. \n• \n Trusted communications channels : Members can safely \ncontribute and access shared information using an effective \ntrusted communications channel or mechanism, such as a secure \nweb site. These channels are not always electronic; some regional \ngroups conduct face-to-face meetings to further reduce the risk of \ncompromise. \n An organization is most likely to benefit from joining communities if those \ncommunities align with the organization’s security goals. This means it’s important to \nfirst clearly define those organizational security goals. To do this, some organizations \nhave found it helpful to use a structured approach; they can more clearly categorize their \ngoals by mapping them to a standard risk management model, such as the “defense in \ndepth” model. Once an organization clearly understands its own security goals, it can \nidentify communities whose objectives align with these goals. \n\n\nCHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n58\n Because there is such a diverse range of organizations, security threats, and goals, it \nis unlikely that any single information-sharing community structure meets all the needs \nof a large organization. For example, a company might participate in one community for \nbenchmarking and another to tackle industry-specific threats. \n Information-sharing communities thrive only when the participating organizations \nfeel they’re receiving valuable information, creating incentives to continue to share \ninformation with others. \n What  constitutes valuable information ? A common definition is that information \nshould be timely, specific, relevant to participants’ concerns, and provides a suitable level \nof detail while protecting individual privacy (ENISA 2010). In practice, “valuable” usually \nmeans the information helps you achieve your security goals, whether those goals are \nlong-term and strategic, or short-term and operational. Information useful for strategic \ngoals might include an early warning that attackers are expected to target a specific \nindustry. This helps members of the community plan their defenses. Information useful \nfor operational goals typically includes more specific details, such as an attack signature. \nThis helps organizations more quickly identify an attack and respond when it occurs. \n As shown in Figure  4-1 (the targeted tier), some communities consist of government \nagencies working alongside an industry in what are usually known as  public-private \npartnerships (PPPs) . These PPPs can be particularly important for protecting critical \ninformation infrastructure. Internationally and within many nations, this infrastructure \nis largely owned and operated by the private sector, including carriers and network \nservice providers. Sharing information about  threats and attacks among public and \nprivate agencies therefore can help ensure security and resiliency of this infrastructure. \nBecause the shared information is highly sensitive, these PPPs usually have strong trust \nframeworks including national security clearances. \n An example of a much broader public-private community is InfraGard, a partnership \nbetween the FBI and private- and public-sector sector organizations that shares \ninformation and intelligence to prevent hostile acts against the U.S. \n Other communities are primarily comprised of  private-sector organizations . Some \nare industry-specific: members of an industry get together to  share threat information \nand best practices, helping to reduce risk for each company while enhancing the \nindustry’s reputation overall. Others involve sharing across industries, such as Evanta’s \nCISO Coalition, a cross-industry group of executives from large organizations. The \nCoalition is designed to facilitate secure, real-time interaction among members to vet \ncritical information security issues, and then share best practices for resolving them. As a \npart of my efforts to expand my external partnerships, I was fortunate enough to become \na founding member of this group’s advisory board. Another cross-industry group is the \nSecurity Advisor Alliance, a cybersecurity nonprofit dedicated to aligning CISOs to help \none another, supporting the information security community (including startups), and \ngiving back to schools and nonprofits. \n Some communities are regional, aimed at security professionals from private and \npublic-sector organizations located within a specific area. These  regional communities offer \nthe advantage of convenience. It takes less time, effort, and expense to attend a regional event, \nwhich makes participation more attractive. Examples of regional groups and forums include \nACTRA (see sidebar) and the San Francisco Bay Area CSO Council, described shortly. \n New communities arise frequently. A community may form in response to a specific \nthreat because companies are strongly motivated to share information about the threat in \norder to develop effective defenses. For example, the Conficker Work Group was formed \nspecifically to address the risk posed by the Conficker worm. \n\n\nCHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n59\n \nARIZONA CYBER THREAT RESPONSE ALLIANCE\n Innovative new models for information-sharing communities are springing up as the \nvalue of sharing security-related information becomes more widely recognized. An \nexample is the Arizona Cyber Threat Response  Alliance , Inc., a regional public-private \npartnership. This cross-sector group shares information about threats and other \nissues among partners from industry, academia, law enforcement, and intelligence. \n ACTRA grew out of relationships developed with FBI’s InfraGard, the public-sector \n Arizona Counter Terrorism Intelligence Center (ACTIC) , and the U.S. Department \nof Homeland Security. A key difference is that ACTRA is a nonprofit company with \na full-time president in addition to voluntary participants including a board and \ntechnical subject matter experts. The goal is to improve security for members with a \nflat, responsive organizational structure and without adding a burdensome layer of \nprocess. The group disseminates information ranging from alerts in near real time to \nwhite papers that provide insights and highlight best practices.  ACTRA has grown to \ninclude representatives from 14 critical infrastructure sectors. The group has found, \nbased on discussions with its members, that multi-sector sharing improves threat \nvisibility beyond the single-sector focus of industry-specific groups. \n Community Goals \n Communities may focus on narrowly defined goals, such as mitigating a specific threat, \nor they may have broader information-sharing goals, such as benchmarking security \ntechniques. A single community may pursue several goals. The most well-known types of \ngoals are sharing information about threats (to help member organizations mitigate those \nthreats) and sharing best practices (to improve efficiency). I’ll describe sharing goals next. \n Sharing Information about Threats and  Vulnerabilities \n Perhaps the best-known function of communities is to provide a trusted mechanism for \nsharing information about threats and vulnerabilities. Members of the community can \nuse this information to improve their tactical and strategic situational awareness. \n I’m often asked by peers how I measure the value of the information obtained \nfrom external partnerships. A key metric is whether the early threat information has \nhelped enable us to reduce risk. A single piece of information might make participation \nworthwhile if it helps us better mitigate risk and protect the company. \n Information from the community can also be useful for corroborating evidence \nthat we’ve already identified internally. If we observe a potential new threat within our \nenvironment, we may not feel that we have enough evidence to justify taking action. But \nwe can often discuss the issue within a community. If others are experiencing the same \nproblem, we can be more confident that it’s a real issue. This gives us enough reason to act. \n",
      "page_number": 75
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 83-90)",
      "start_page": 83,
      "end_page": 90,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n60\n Some examples of communities that share threat information include\n• \n Information Sharing and Analysis Centers ( ISACs ) : ISACs are \ntrusted industry-specific communities established by owners \nand operators of critical infrastructure resources. ISACs exist for \na number of industry sectors, including communications, retail, \nelectrical utility, health, and public transit. Services provided by \nISACs include risk mitigation, incident response, and alert and \ninformation sharing. \n• \n Bay Area CSO Council : This is a regional community that focuses \non improving the sharing of intelligence and best practices \namong CISOs in the San Francisco Bay Area. The Council \nserves as a vehicle for CISOs to safely and securely share their \nattack experiences. Members may share artifacts, such as attack \nsignatures, that they can then build into their organizations’ \ndetection and defense mechanisms (Jackson Higgins 2010). The \nforum uses a peer-enforced trust model rather than a formal legal \nframework. The group also creates subgroups to work on more \nhighly classified information. \n Sharing Best Practices and Benchmarking \n Many communities also serve as a forum for exchanging best practices and for \n benchmarking operations . By sharing security best practices, we may be able to increase \nthe efficiency and effectiveness of our own operations. \n Tapping into the expertise of others can help us avoid reinventing the wheel. A \ntypical example: A  CISO is trying to create a bring-your-own device policy for her own \norganization. So she sends a message to community members and receives detailed \nadvice from others who have already been through the process. This gives the CISO a \nhead start in creating a policy that meets her organization’s needs. \n Besides  enabling informal exchanges , communities may also operate formal \nbenchmarking exercises. Some of the best-known examples are the security-related \nprograms run by benchmarking firm CEB, Inc., which conducts studies and generates \nreports that compare companies in a variety of areas, from user security awareness to \ncontrols maturity (CEB 2015; also see the discussion of security awareness programs in \nChapter  5 ). Benchmarking information generated by communities can also be useful for \ndemonstrating the efficiency of security operations to other internal groups within your \norganization, such as an audit committee. \n Some benchmarking  information is sensitive and closely held because organizations \nfeel that it could reveal too much information about their security operations. Other \ninformation is more general and is sometimes publicly available, such as the webinars \nand presentations published online by Intel and others. Even this general benchmarking \ninformation may yield risk insights. Observing what other companies are focusing on, \nand how they are allocating resources, can help security professionals think about how \nthey need to manage risk within their own organizations. \n\n\nCHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n61\n One of the most established communities is the Forum for Incident Response and \nSecurity Teams ( FIRST ). This international group focuses on sharing best practices among \ncomputer security incident response teams. Trust relationships are peer-enforced. The group \npublishes a series of detailed best-practices guides and other documents for public use. Other \nactivities involve the exchange of information for cooperative incident management. \n Technology is helping to make information exchange more automated and therefore \neasier and faster, due in part to the adoption of standards for representing (STIX) and \ncommunicating (TAXII) information about threats. Platforms are emerging that use these \nstandards for rapid, secure information sharing. \n Many years ago, I was asked to manage Intel’s first major IT benchmarking activity. \nIt was a big task that entailed analyzing cost, quality, and other aspects of operations \nacross our entire IT environment. \n One of the first challenges was determining which organizations we should \nbenchmark ourselves against. At the time, the conventional wisdom at most \norganizations was that you should compare yourself with similar businesses. The \nlogic was that because these businesses were the most directly comparable, this \napproach would yield the most meaningful results. So the expectation was that I’d \nbenchmark our operations against a collection of other big high-tech companies. \n But I didn’t want to benchmark our operations against only high-tech companies. \nInstead, I wanted to benchmark against a broad base of companies in industries \nsuch as retail, banking, manufacturing, consumer goods, and utilities. \n The time came to present my selection of peer groups in a meeting with senior IT \nmanagement. By this time, I’d already started the benchmarking process, and as I \ndescribed the diversity of the companies included in the benchmark comparison, I \ncould sense the atmosphere becoming increasingly hostile. Practically everyone felt \nthat my approach was completely wrong. In fact, if there had been rotten tomatoes \nin the room, a few people would have been throwing them at me. \n So I asked for a moment of quiet so that I could explain. If we were an airline that \nwanted to benchmark operations, who would we compare ourselves with?” I asked. \nSeveral people said they’d benchmark against other airlines. \n “What do you think we would learn from that comparison?” I continued. “My guess \nis not much. We’d all have grown up in the same industry, and we’d probably have \nsimilar business processes. Many of our employees would have worked for the other \ncompanies and vice versa, so they’d probably implement similar practices. We might \nlearn about minor efficiency improvements, but I wouldn’t expect any breakthroughs.” \n BENCHMARKING: WHO SHOULD YOU COMPARE \nYOURSELF WITH?\n\n\nCHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n62\n “If I really wanted to dramatically improve the way I manage airline gate operations, \nI’d benchmark against a Formula 1 pit crew. Those crews can service a car and get \nit back on the road in 20 seconds or less. I’d think about what we could learn from \nstudying their processes, their technologies, and their ability to communicate and \norganize, and I’d try to figure out which aspects could cross over into airline data \noperations. If we want to make dramatic improvements, we need to look at people \nwho operate in an extreme operational environment—not at other airlines.” \n I’m happy to say that the managers in the room recognized that there might be \nvalue in the approach I was suggesting, even if many of them still disagreed with \nit. Ultimately, benchmarking against companies in a broad range of industries did \nhelp us achieve some dramatic improvements, and I received an internal award for \nthe initiative. The lesson is that sometimes we can learn more by looking outside a \nnarrowly defined, traditional peer group. People in the same industry may be facing \nthe same problems as we are and dealing with them the same way. For a fresh \nperspective, it can be worth looking farther afield. \n Influencing  Regulations and Standards \n All of us operate within an increasingly complex regulatory environment, and we’re all \naffected by evolving technology standards. \n It’s important to stay abreast of legislative developments. That can be a difficult and \ntime-consuming job for any single organization, and so it may be helpful to become \ninvolved in a community whose goals include tracking regulatory activity. \n In addition, communities can sometimes help influence public policy more \neffectively than a single organization can do alone. There’s strength in numbers, and \ncommunities often include some of the biggest companies in an industry. \n An example of a community that focuses on policy is BITS (  www.bits.org  ), the \ntechnology policy division of The Financial Services Roundtable, which represents 100 \nof the largest integrated providers of consumer financial services. Members of BITS \ncooperate on issues such as critical infrastructure protection, fraud prevention, and \nthe safety of financial services. The organization works to influence public policy by \ncommunicating with public agencies. It also publishes reports for use across the industry, \nincluding a financial services security assessment. Thus, communities that focus on \npolicy may help all participating companies and the reputation of the industry overall. \n Businesses who offer services in multiple countries have a particular interest in the \ninternational regulatory environment. These include multinationals, of course, which are \ndirectly affected by the complex web of regulations at international, national, and local levels. \n However, these regulations affect a surprisingly large number of other companies, \nincluding many that don’t have employees or facilities physically located in other \ncountries. Today, almost any business with a web-based service consumed in multiple \ncountries is effectively operating in a multinational environment. Regulations in those \ncountries have impacts that stretch beyond geographical boundaries. For example, \nregional and local regulations such as the California data breach bill (SB1386) and \nEuropean privacy guidelines require compliance by any company that stores information \nabout residents of those areas, no matter where the company is located. \n\n\nCHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n63\n Corporate Citizenship \n At many companies a large number of employees volunteer in ways that benefit their \nneighborhood or a wide variety of worthy causes. Businesses often provide support to \nhelp employees do this. There’s a growing trend to leverage the organization’s talent and \nexpertise in volunteer corporate citizenship initiatives that are more closely related to the \norganization’s goals and employees’ technical expertise. Examples might include offering \nexpert security advice to nonprofits or helping security initiatives in other countries. \n Security-related corporate citizenship initiatives include the National Cyber Security \nAlliance, whose mission is to educate and empower society to use the Internet safely and \nsecurely (see  staysafeonline.org ). The sponsors of the alliance include large high-\ntech companies such as Intel. Senior managers at those companies also are among the \ndirectors of the organization. \n Conclusion \n The knowledge we acquire via external partnerships can help us protect our own \norganizations. I’ve experienced this first hand; indicators of compromise shared by others \nhave helped me understand and respond to threats. The growth of information-sharing \ngroups shows that many organizations are coming to the same conclusion. As Ken \nAthanasiou, Global Information Security Director at American Eagle Outfitters, said in a \nstatement supporting the formation of the new retail ISAC: “Cyber-criminals work non-\nstop, and are becoming increasingly sophisticated in their methods of attack … by sharing \ninformation and leading practices and working together, the industry will be better \npositioned to combat these criminals” (Retail Cyber Intelligence Sharing Center 2015). \n Industry-specific groups such as the financial and industrial control ISACs have \nbeen widely acknowledged as helping companies quickly learn about threats and \nspecific measures for combating them. Other groups provide different kinds of valuable \ninformation. The Evanta CISO Coalition has published metrics that its members can use \nfor security benchmarking and dashboarding. Members of IASAP share information that \nhelps them improve their awareness programs. \n The  security landscape has become increasingly complex and dynamic, and \nit’s difficult to track and manage the risks without help from others. Sharing security \ninformation is also becoming more important as organizations increasingly collaborate \nwith business partners and adopt new technologies. Understanding the risks faced \nby our partners, and the way they manage those risks, can help us protect our own \norganizations. As businesses move into new markets and use technology in new ways, we \nneed to understand our biggest exposures and how to allocate resources most effectively \nto minimize business risk. Therefore, sharing information can help businesses remain \ncompetitive and successful. \n Organizations have often been reluctant to share security information, but if we \nwant help from other people, we have to be prepared to share information ourselves. By \ncarefully using trusted partnerships that align with our security goals, we can increase our \norganization’s ability to sense, interpret, and act on risk. \n\n\n65\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_5\n CHAPTER 5 \n People Are the Perimeter \n There’s a difference between interest and commitment. When you’re \ninterested in doing something, you do it only when circumstances permit. \nWhen you’re committed to something, you accept no excuses, only results. \n —Art Turock \n A few years ago, a senior manager began bringing his corporate laptop into the cafeteria \nat lunchtime. Typically, he’d find an empty table, set down the laptop, and then walk out \nof sight to get his lunch. As he perused the salads and main courses, made selections, and \npaid for his food, his laptop sat unattended in plain view of hundreds of people using the \nlarge cafeteria. \n My security team noticed the neglected laptop and pointed it out to me. I discussed \nthe issue with the manager a few times, but he continued leaving the laptop unattended. \nSo eventually, I began taking the laptop and leaving my business card in its place. \n Not surprisingly, the manager became somewhat annoyed. “Nobody’s going to steal \nthe laptop because there are all these people around,” he said. \n “Okay,” I responded. “I’ll never take your laptop or complain again on one condition. \nIf you really trust everybody here, you’ll take off your wedding ring and leave it on top of \nthe laptop. If you do that, you’ll never hear from me again.” \n He thought about this for a while. Then he said, “You made your point.” And he never \nagain left the laptop unattended. \n The  Shifting Perimeter \n This incident helped crystallize in my mind a new perspective about how we should \napproach information security. It demonstrated how each person’s daily decisions can \naffect the risk dynamics of the company overall. \n The traditional enterprise security paradigm, often expressed in castle-and-\ndrawbridge terms, described a wall of technology that isolated and completely protected \nthe workers behind it. To protect our people and information assets, we focused our \nefforts on fortifying the network perimeter and the physical perimeter of our buildings. \n\n\nCHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n66\n Today, however, a growing number of  user interactions with the outside world \nbypass the physical and network perimeters and the security controls these perimeters \noffer. They take place on external web sites and social networks, on laptops in coffee \nshops and homes, and on personal devices such as smartphones. As the Internet of \nThings unfolds, those interactions will also take place on many more “things,” such as \nwearables, cars, and even household appliances. \n This changing environment doesn’t mean the security perimeter has vanished. \nInstead, it has shifted to the user. The laptop left unattended in the cafeteria was clearly \ninside the physical perimeter, but the corporate information it contained was still \npotentially at risk due to the manager’s actions. People have become part of the perimeter. \nUsers’ decisions can have as much impact on security as the technical controls we use. \n Over the past few years, the idea of the people perimeter has won wider recognition \nand acceptance. Accordingly, organizations are placing more emphasis on employees’ \nsecurity awareness and behavior. \n One reason for this is the rash of high-profile insider exploits, such as the leaks by \nNational Security Agency contractor Edward Snowden. Another is that technical controls \nhave not kept pace with the attackers. Many exploits are reaching users because technical \ncontrols, particularly those on endpoint devices, have failed to prevent them. We are \ntherefore more reliant on the user’s ability to detect suspicious activity. We also have \nbeen forced to deploy more back-end detection and response tools and staff to handle \nthe flow of malware penetrating the corporate infrastructure. These ever-growing security \noperations teams, which become another layer of the people perimeter, typically are \nunable to keep up with the flood of malware and commit errors due to “alert fatigue.” \n There’s a continuing emphasis on  phishing attacks ; the  2015 Data Breach \nInvestigations Report found that the percentage of users deceived by phishing actually \nincreased from previous years, with 23% opening phishing messages and 11% clicking on \nattachments (Verizon 2015). \n Older  social-engineering techniques are also still effective, apparently. At hedge fund \nFortelus Capital Management in London, the chief financial officer received an alarming \nphone call one Friday afternoon. The caller said he was from the company’s bank, and \nwarned of possible fraudulent activity on the account. The CFO reluctantly agreed to \ngenerate codes enabling the caller to cancel 15 suspicious payments. When he logged \ninto the firm’s bank account the following Monday, $1.2 million was gone. The CFO lost \nhis job and was sued by his firm for failing to protect its assets (Chelel 2015). \n As almost every company becomes a technology developer as well as a technology \nconsumer, employee security awareness behavior will become an even bigger issue. \nSecurity lapses by the employees working on technology-based products can have far-\nreaching impacts, creating vulnerabilities in the digital services and physical products \ndelivered to millions of customers. \n Compliance or  Commitment ? \n Each day, employees make decisions that can affect the company’s information risk. Do \nI leave my computer unattended or not? Do I post this information on social media? Do I \ninstall this software on my device? Do I report this suspicious looking e-mail? When I’m \nin a coffee shop, do I connect to the corporate infrastructure via a secure virtual private \nnetwork, or do I engage directly over the Internet? \n\n\nCHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n67\n We could view each of these decisions purely in terms of the potential for increased \nrisk. However, there’s also a positive side. If users become more aware of security and make \nbetter decisions, they can strengthen the organization’s defenses by helping identify threats \nand prevent impact. Among CISOs surveyed recently by best-practices firm Corporate \nExecutive Board, 50% said that insecure behaviors cause more than half of all breaches; but \nthey also said employees are key to uncovering suspicious activity (CEB 2015). \n Therefore, as information security professionals, we are in the behavior modification \nbusiness. Our goals include creating a more security-conscious workforce so that users \nare more aware of threats and vulnerabilities, and make better security decisions. \nFurthermore, we need to influence employees’ behavior both within the workplace and \nwhen they are home or traveling. \n If the manager was comfortable leaving his laptop unattended in our cafeteria, \nwould he also leave it unattended at the local coffee shop? At the airport? Or somewhere \nelse where the risk of loss was even greater? My belief is he probably would. When trying \nto influence this person’s behavior, I wanted to achieve more than a level of compliance. I \nwanted to initiate a feeling of commitment. \n The term  compliant behavior implies making the minimum effort necessary to \nachieve good performance to a predefined standard. It’s like checking boxes on a list of \nsecurity compliance items. Ultimately, employees feel they are being compelled to follow \nsomeone else’s list of instructions. Because of this, compliance requires supervision and \npolicing, and employees may sometimes engage in lengthy recreational complaining. \nIf employees are simply following a checklist, what happens when they encounter a \nsituation that’s not on the list? They stop and await further instructions, or perhaps they \nare even unaware of the threat or ignore it. \n In contrast,  committed behavior is intrinsically motivated and self-directed. Being \ncommitted implies that people are emotionally impelled to invest in security; they \ntake responsibility and ownership. When people feel committed, they tend to deliver \nabove and beyond the bare minimum. Rather than simply following a predefined list of \ninstructions, they are empowered to make decisions and judgment calls in real time, with \na focus on how their actions affect others as well as themselves. \n If we can create this sense of commitment in our users, we can implement security \nnot as a wall but as a collective security force that permeates the entire organization. \nIndividually and as a group, every person in the corporation uses their skills in security to \nprotect the organization, handling known attacks today as well as quickly adapting to new \nthreats tomorrow. \n When I needed to influence the manager’s behavior, I looked for a way to establish \nthis level of commitment. I sought to change the way he felt about the laptop, and to do \nthis I tapped into his emotional connection to his wedding ring. \n Creating a culture of self-motivated commitment rather than compliance can make \na big difference, as shown in studies by management guru Dov Seidman. His group \nlooked at behavioral differences between businesses with a culture of self-governance, \nin which an organization’s purpose and values inform employee decision-making and \nbehavior, and those with a culture of blind obedience based on command-and-control \nand coercion. Organizations based on self-governance experienced three times more \nemployee loyalty and half as many incidents of misconduct, compared with organizations \nbased on blind obedience (Seidman 2011). \n\n\nCHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n68\n The implications for enterprise security are clear. As the boundaries between \npersonal and corporate computing dissolve, employees may be accessing information \nfrom any location, on any device. If users behave in an insecure way while they are in \nthe office, it’s likely they will also exhibit insecure behavior when they’re elsewhere. \nConversely, if we can create a feeling of commitment that causes them to own \nresponsibility for security, there’s a better chance they will behave more securely both \nwithin the workplace and when they are outside our physical perimeter. This change \nin behavior improves the security of the device they are using, the information they are \naccessing, their personal lives, and the enterprise.  \n Examining the  Risks \n Before discussing ways that we can modify user behavior, I’d like to briefly mention some \nexamples of what can happen if we don’t influence the ways that users think and act. \n As an experiment, the US Department of Homeland Security secretly dropped disks \nand thumb drives in the parking lots of government and private contractors’ buildings. \nTheir goal was to see whether people would pick them up and plug them into their \ncomputers. As reported by Bloomberg News (Edwards et al. 2011), up to 60 percent of the \npeople who picked up the items inserted them into their office computers. That number \nrose to 90 percent if the item included an official-looking logo. Clearly, the security \nbehavior of employees at these facilities left quite a bit to be desired. \n Insider threats unfortunately continue to make the news. A former JPMorgan Chase & \nCo. employee was arrested by the FBI on charges of stealing customer data and trying to sell it \nto an undercover informant. As noted by  CSO , similar incidents have occurred multiple times \nat the bank over the past few years, illustrating the company’s inability to account for insider \nthreats despite its substantial annual spending on security technology (Lambert 2015). \n Think about what can happen with newer, more sophisticated exploits. A \nsophisticated attack targeted government departments using fake voice-mails to \ndistract users while malware downloaded in the background. Using social engineering \nand targeted e-mails, the attackers tricked users into visiting web sites harboring self-\nextracting archives. The archives contained a recording media file purporting to be a \nvoice-mail from a female journalist seeking information for a news story, alongside other \nfiles that downloaded malicious content (CNET 2015). \n As in the example above, today’s threats may arrive in the form of carefully personalized \nspearphishing communications designed to win the trust of targeted users. These users then \nunwittingly provide access to the information the attackers want. In essence, trust—in this \ncase, the organization’s trust in the user—has become the attack surface. \n Let’s say a company is looking to hire a credit analyst with a very specific set of skills. \nAttackers notice this and apply online, using a résumé that lists the exact skills required for the \njob and contains the terms the company’s résumé-scanning software is likely to be looking \nfor. Suitably impressed, the company’s human-resources specialists forward the application \nto the company’s credit-department manager, who has access to all the systems storing \ncustomer financial data. The manager trusts this communication because it has been sent \nfrom another department within the same company. So she clicks on the link to the résumé. \nUnfortunately, that action triggers the execution of malicious code. The human-resources \nteam effectively acted as an infection agent, ensuring the attack reached its real target. \n",
      "page_number": 83
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 91-99)",
      "start_page": 91,
      "end_page": 99,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n69\n Social media accounts can become sources of risk even when they haven’t been \ncompromised. There have been several examples in which senior executives accidentally \nrevealed information that was confidential or problematic in other ways. In November \n2014, Twitter’s CFO accidentally publicly tweeted a plan to buy another company, \nincluding the fact that he wanted help to make the deal happen at a meeting the following \nmonth. The CFO was apparently trying to send the message privately (Frier 2014).  \n At Houston-based fashion retailer Francesca's Holdings, a former CFO frequently \nshared his thoughts via a personal blog, Facebook page, and Twitter feed (Silverman \n2012). Unfortunately, he also shared information that caused problems for his employer. \nThe company fired him because he \"improperly communicated company information \nthrough social media.” \n Users frequently post information on external social-media sites that attracts the \nattention of competitors or the media. To boost their job prospects, interns mention \nproduct features they helped develop during their summer job at a well-known company; \nsales representatives reveal the names of major clients; even senior executives have \nbeen known to unintentionally disclose key corporate strategies. In fact, services exist \nthat specialize in aggregating apparently minor snippets of information from social-\nmedia and other web sites to build an accurate view of a company’s size, geographical \ndistribution, and business strategy, including hiring patterns that indicate whether the \ncompany is expanding and which new areas it is moving into. \n Adjusting Behavior \n To counter these risks, we need to make employees aware and empowered so they act \nas an effective part of the security perimeter. Increasing recognition of this need has led \nto the development of a small ecosystem focused on increasing security awareness and \n changing behavior , ranging from companies offering best practices to groups focused on \ninternet safety for children. This includes companies that train users to avoid phishing \nexploits, using simulated phishing scenarios and other tools. Security awareness \nprofessionals have come together to share best practices (see sidebar). \n While I was Intel’s CISO and then Chief Security and Privacy Officer, we focused on \nbuilding security and privacy protection into the corporate culture, getting employees \nto own responsibility for protecting enterprise and personal information. Achieving \nthis required a lot of effort, and we realized that it took just as much work to maintain a \nculture of security and privacy as to build it. \n Training is a key part of security efforts at most companies, and Intel is no \nexception. We supplemented general training, which fulfills most legal requirements, \nwith specialized training for employees who have specific roles or access to sensitive \ninformation. Another effective technique was to embed security and privacy training into \nbusiness processes. When an employee requested access to an application that handles \nsensitive information, they were automatically prompted to take training that focused on \nthe related security and privacy concerns. We also used online training including video \nand other visually stimulating material as well as entertaining, interactive tools to help \nengage users (see Figure  5-1 ). \n\n\nCHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n70\n However, it is not enough to create good training. If nobody takes the training, the \neffort is wasted. We found that incentives such as public recognition helped ensure \nemployees underwent training and absorbed the lessons. Ultimately, if people continued \nto avoid security training, we escalated compliance efforts by directly contacting them \nand their managers. \n We also found we could help maintain and increase awareness by publishing security-\nrelated articles on Intel’s primary employee portal. Many of these articles included a \npersonal aspect, such as preventing identity theft, keeping children safe online, and home \nwireless security tips. The focus on personal concerns recognized that the way employees \nbehave outside the office is as important to enterprise security as their behavior in the office. \n How did we know our security efforts paid off? We accumulated a variety of \nevidence, including independent benchmark results from Corporate Executive Board \n(2011), which indicated that Intel employees consistently ranked in the top 10 percent \nof companies for secure behavior. We also observed that employees acted as part of the \nsecurity perimeter by alerting us to suspicious text messages or e-mails they’d received. \n Figure 5-1.  Intel’s internal “Find the Phish”  interactive training tool helps employees spot \nweb scams. Source: Intel Corporation, 2012 \n \n\n\nCHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n71\n A Model for Improving Security Awareness \n Some of the industry’s most valuable work to advance organizational understanding of \n security awareness comes from best-practices firm CEB. The company offers a security \nawareness service that includes surveying key security-related employee behaviors at \nmember organizations, and benchmarking the results against other organizations. The \nprogram attempts to understand the psychological reasons for insecure behavior; it then \nfocuses on those psychological drivers when suggesting tactics to change employees’ \nbehavior. To date, CEB has collected some 300,000 employee responses from over 400 \norganizations. \n The program has found that despite the importance of secure employee behavior, \nmost organizations deliver only moderate amounts of  training : just over an hour per year, \non average, and only three to four employee communications per month. Survey results \nsuggest that organizations can increase training time to as much as six hours a year before \nexperiencing diminishing returns. \n CEB emphasizes that organizations need to use an understanding of psychology to \ntailor their awareness efforts; awareness programs must target the specific root causes of \nemployees’ risky behavior in order to be effective. The company initially identified five \npsychological factors influencing security awareness and behavior: lack of knowledge of \npolicy, lack of self-interest in security, inadequate perception of the risk to the organization, \na low emotional commitment to security, and a perception that secure behavior imposes a \nhigh burden. It recently added a sixth factor: the ability to display good judgment. \n CEB’s findings suggest that the perceived burden of secure activities affects employee \nbehavior more than any other psychological driver. That’s the bad news. The good news \nis organizations can fix the perception of the burden both by reducing the burden itself \nand by addressing the other drivers. For example, employees’ emotional commitment \nto security increases if their managers engage with them directly to emphasize the risks. \nClear enforcement of policy compliance increases employees’ self-interest in secure \nbehavior and heightens their perception of risks. \n Based on survey data collected over the years, CEB has developed a model that \norganizations can use to help plan and assess their security awareness programs. \nThe model presents a four–stage progression toward higher security awareness and \ncommitment, from basic check-the-box compliance to active involvement in security. It \nrecognizes that in a complex threat environment, we can no longer rely only on policies \nthat prescribe specific employee behaviors: we also need to enable employees to actively \nsupport security activities including breach detection. \n\n\nCHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n72\n Employee awareness programs at Level 1 (Check the Box) simply respond to external \nregulatory requirements and don’t explicitly aim to change specific employee behaviors. At \nLevel 2, programs try to  encourage users to adopt specific, simple behaviors, such as avoiding \nsharing passwords and sending sensitive information to their own personal e-mail addresses. \n The third and fourth levels display greater levels of  judgment and commitment . \nAt Level 3, employees are able to make good judgment calls in the moment, especially \nin situations where the right answer is not immediately obvious. For example, they \nremember to pause before clicking to check whether an e-mail comes from a legitimate \nsource or contains a phishing link. At Level 4, employees become an extension of the \ninformation security organization; they not only avoid security risks, but also notify \ninformation security when they see something suspicious. A key behavior here is an \nincrease in reporting events such as spearphishing attempts. \n The  encouraging news is that that CEB surveys show a gradual improvement; the \npercentage of employees avoiding insecure behaviors such as password sharing has \nslowly increased over the past six years. Resistance to phishing has improved faster, \nthough from a lower base. “I think at progressive companies the aspiration is changing,” \nsays CEB practice leader Jeremy Bergsman. “Most companies have been moving \nfrom Level 1 to Level 2 over several years, and are starting to think about Level 3. But \nprogressive companies are moving beyond employee behavior as a risk to be reduced, \nand working on ways to make employees a control—an early warning system (Level 4).” \n For example, a large  telecommunications firm that participated in the CEB security \nawareness program wanted to empower employees to act as controls supporting \ninformation security. It provided each employee with a weekly report tracking his or her \nbehavior, including the documents they accessed, and the devices and external locations \nused to connect to corporate systems (Figure  5-3 ). Employees were responsible for \nreading the reports, thus sharing responsibility with information security for detecting \n Figure 5-2.  A  four-stage model for programs seeking to improve security awareness and \nbehavior. Source: CEB Inc., 2015 \n \n\n\nCHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n73\nbreaches. The firm found that employees detected suspicious activities faster than would \nhave otherwise been the case; users also proactively improved security by suggesting \nother activities that should be tracked and added to future reports. \n \n INTERNATIONAL ASSOCIATION OF SECURITY \nAWARENESS PROFESSIONALS\n Exchanging ideas with other security professionals can help improve security \nawareness programs. The International Association of Security Awareness \nProfessionals (  www.iasapgroup.org  ) is an independent association of corporate \nsecurity specialists who are seeking to do just that. IASAP is a non-profit, fee-based \nassociation dedicated solely to security awareness programs. Its goal is to serve \nas a trusted forum of security awareness professionals collaborating to improve \nemployee security behavior. “Clear guidance has not been as available for employee \nsecurity behaviors as it has been for technology solutions,” says IASAP board \nmember Michael Diamond. “Several awareness professionals noticed this gap, and \nthat ultimately led to the formation of IASAP.” \n Some members have built programs from scratch; others inherited established \nprograms in need of fresh ideas and new energy. Members meet in person two to \nthree times per year to learn about other members’ security programs and present \ntheir own. There’s also a members-only sharing platform supporting Q&A, feedback, \nbenchmarking surveys, member polls, guest speaker webinars, and teleconferences. \nSome members feel comfortable posting program resources that are available to \nother members for re-branding within their own programs. \n Figure 5-3.  Example of weekly  tracking report showing employees their activity. \nSource: CEB Inc., 2015 \n \n\n\nCHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n74\n Broadening the  Awareness Model \n I think that the CEB four-stage model shown in Figure  5-2 is a very useful tool. One \nlimitation, in my opinion, is that the model is based on the traditional organizational \nview of information security. I believe that we need to expand the model to capture a \nmore complete view of information risk. You might think of the following two additions as \nLevels 3a and 4a, respectively, of the model. \n• \n Privacy awareness, which has become a critical concern. Just \nconsider the number of breaches that have targeted personal \ninformation at retail, healthcare, and government organizations.  \n• \n A specific focus on engineers and other technology professionals, \nincluding those creating technology-based products and services. \nIf engineers don’t have a foundational understanding of privacy \nand security, they cannot design privacy and security into the \ntechnology they produce. As a result, a company’s products may \ncontain vulnerabilities that introduce significant risks for the \nbusiness and its customers. \n The  Security Benefits of Personal Use \n Employees use an ever-growing variety of  personal devices every day, both inside and \noutside the physical workplace. This trend started with smartphones and laptops; it \nalso includes wearable devices such as smartwatches and fitness monitors. Information \nsecurity specialists naturally tend to focus on the security risks of using these devices for \nbusiness purposes. As I discussed earlier in the book, I’ve found that the productivity \nbenefits of personal devices often outweigh the risks. But even the security implications \nare not as one-sided as they might seem at first glance. I believe that, in some respects, \nallowing personal use may actually encourage better security. \n In general, people are likely to take better care of their own possessions than \nsomeone else’s. They feel a stronger connection to their own car than to one provided \nby their employer. If people are using their own computing device, they may take better \nprecautions against theft or loss. And they may feel the same way if they are storing \npersonal information on a corporate device. At Intel, we allowed reasonable personal use \nof corporate laptops, and therefore many employees stored personal as well as corporate \ninformation on their laptops. Because of this, they had a personal stake in ensuring the \ndevices didn’t get lost or stolen. I believe this sense of ownership contributed to our \nlower-than-average laptop loss rates. \n Another company’s experience provided some empirical evidence supporting this \nidea. The company conducted a tablet pilot deployment in which, for the first time, it \nallowed personal use of corporate devices. The company found that breakage and loss \nrates were dramatically reduced compared to its past experience with mobile devices. \nThe CIO’s conclusion was that employees simply take better care of devices when they \nuse them for personal purposes. \n Perhaps we should be similarly open-minded when considering the security \nimplications of wearable devices. I met with managers at a large company who were \npondering the security implications of smartwatches and fitness monitors, which \n\n\nCHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n75\nemployees were already bringing into the workplace. Understandably, some people at the \ncompany wanted to make sure the devices could not interact with the corporate network. \nI observed that in the future, wearables could be harnessed to help identify users in ways \nthat are less cumbersome for users than traditional controls such as passwords. Fitness \ndevices, including some smartwatches, count the user’s steps and monitor heart rate, and \ncould therefore be used as biometric security devices in the future. As the devices evolve \nand accumulate more user data over time, they may become more adept at identifying \neach user’s physiological and behavioral “signature.” In addition, some smartphones \ninclude fingerprint recognition, which in itself can be a powerful authentication \nmechanism if the technology has been properly designed and implemented. \n As security professionals, shouldn’t we think about taking advantage of the benefits \nthese technologies offer? We should seek to integrate into security strategies the broader \nvariety of existing devices, which have useful features such as cameras and voice recognition \nand also contain data about our use patterns. Many of these devices already communicate \nwith each other; why not take the next step and use the technology to eliminate the pain of \nusing passwords? Why not find a way to reduce risk and cost, while providing a much better \nuser experience, by using these devices to authenticate us automatically? \n It may also be worthwhile to reexamine other assumptions about the security \nimplications of personal devices. Some companies have policies forbidding the use of \ncameras in their offices. However, a smartphone includes a camera that employees can \nuse to capture the off-the-cuff design sketches often scrawled on whiteboards during \nbrainstorming sessions. This intellectual property can then be stored and encrypted on a \nhard drive within the enterprise. Is it safer to allow employees to photograph the image, \nor to copy it onto a piece of paper, or to leave it on the whiteboard where anyone might \nsee it? Companies may come to different conclusions, depending on their culture and \nappetite for risk. But this is another illustration of the importance of considering all the \npossible business benefits as well as the risks when making technology decisions.  \n Roundabouts and Stop Signs \n To try to reduce driving accidents at a dangerous curve in Chicago, the city painted a \nseries of white lines across the road. As drivers approached the sharpest point of the \ncurve, the spacing between the lines progressively decreased, giving the drivers the \nillusion they were speeding up, and nudging them to tap their brakes. The result was a 36 \npercent drop in crashes, as described by Richard Thaler and Cass Sunstein in their book \n Nudge (Yale University Press, 2008). \n This  traffic-control method succeeded in making drivers more aware and improving \nsafety while keeping the traffic flowing with minimum disruption. I think this example \nprovides a useful metaphor for information security. Some security controls are like stop \nsigns or barriers: we simply block access to technology or data. But if we can shape the \nbehavior of employees rather than blocking them altogether, we’ll allow employees, and \ntherefore the company, to move faster. \n To use another traffic metaphor, a roundabout at an intersection typically results in \nmore efficient traffic flow than an intersection with stop signs, because drivers don’t have \nto come to a complete halt. The roundabout increases drivers’ awareness, but they can \nproceed without stopping if the way is clear. Statistics have shown roundabouts are often \nsafer than intersections. \n\n\nCHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n76\n Of course, we need to block access in some situations, such as with illegal web sites. \nBut there are cases where it’s more efficient and productive to make users aware of the \nrisks, yet leave them empowered to make the decisions themselves. \n Consider the case of a large multinational company whose business relied heavily \non its significant intellectual property. To protect that proprietary IP, the company \nimplemented data-loss protection software, including an application on employees’ \nlaptops. But instead of simply blocking transmission of information flagged as sensitive, \nthe company configured the software to warn employees whenever it detected potentially \ninsecure behavior. If an employee tried to transmit a confidential document, the software \ndisplayed a message that explained the potential risks and suggested ways to protect \nthe information, such as encryption. After all, users may have good reasons for sending \nconfidential documents, and preventing transmission could be detrimental rather than \nbeneficial to the business. The company found that this warning caused 70% of users \nto change their behavior, representing a major reduction in risk. Yet because of the way \nthe software was configured, users didn’t complain about the security burden. The \nroundabout approach reduced risk without interfering with users’ productivity. \n Here’s another  hypothetical example. It may make sense to warn users visiting \ncertain countries that they may be accessing material that is considered unacceptable. \nA US employee traveling on business might be working in a local office of a country with \nstrict religious guidelines. The employee has a daughter who’s in a beauty pageant, so it \nwould be natural to check the pageant web site from time to time. But the images could \nbe offensive in the country, so it makes sense to warn the employee to exercise caution. At \nIntel, we found that when we warn users in this way about potentially hazardous sites, the \nvast majority heed the warnings and don’t access the web sites.  \n In the case of information security, there’s an additional benefit of making controls \nas streamlined as possible. We all know if controls are too cumbersome or unreasonable, \nusers may simply find ways around them. We kept this concern in mind when developing \na social media strategy at Intel IT (Buczek and Harkins 2009). We were well aware of the \nrisks associated with social media, but attempting to stop the use of external social media \nweb sites would have been counterproductive and, in any case, impossible. We realized \nthat if we did not embrace social media and define ways to use it, we would lose the \nopportunity to shape employee behavior. \n As part of our initial investigation, we conducted a social media risk assessment. \nWe found social media does not create new risks, but can increase existing ones. For \nexample, there’s always been a risk that information can be sent to inappropriate people \noutside the organization. However, posting the same information on a blog or forum \nincreases the risk by immediately exposing the information to a much wider audience. \n So we developed a social media strategy that included several key elements. We \ndetermined that we could reduce risk by implementing social media tools within the \norganization, so we deployed internal capabilities such as wikis, forums, and blogs. \nInitially, employees used these tools mainly to connect socially rather than for core \nbusiness functions; we later integrated the tools into line-of-business applications to \nachieve project and business goals. We also worked with Intel’s human-resources groups \nto develop guidelines for employee participation in external social media sites, and \ndeveloped an instructional video that was posted on a public video-sharing site. The \nvideo candidly explained that Intel wanted to use social media to open communications \nchannels with customers, partners, and influencers, to encourage people to adopt the \ntechnology, and to close the feedback loop. The information also included guidance \n\n\nCHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n77\nabout how to create successful content and general usage guidelines. We also used \ntechnology to help ensure that employees followed the guidelines. We monitored the \nInternet for posts containing information that could expose us to risks, and we also \nmonitored internal social media sites to detect exposure of sensitive information and \nviolations of workplace ethics or privacy.  \n The Technology  Professional \n So far, I’ve focused mainly on the security roles of end users. But think about the \nbroadening roles that technology professionals play at many companies. Historically, \n technology professionals have performed back-office IT roles at most companies, such \nas managing infrastructure and internal applications. Many also work on web sites and \nonline services. We’re now moving into a future in which companies in all industries will \nbecome creators of technology embedded in physical as well as digital products, and \nthey’ll hire developers to create that technology. These technical professionals are also \npart of the people perimeter, and their actions can have major positive or negative effects. \n We’ve already seen several well-publicized problems caused by vulnerabilities in \nproducts. Fiat Chrysler recalled Jeeps in 2015 after researchers showed they could hack \ninto a 2014 model and hijack its steering, brakes, and transmission. The researchers used \nan unsecured communications port to execute the attack (Dark Reading 2015). Similar \nconcerns prompted the FDA to order organizations to stop using older drug infusion \npumps made by Hospira when it was found that an unauthorized user could hack into \nthem and change the dosage the pump delivers. \n In traditional IT roles, technical professionals manage almost every element of the \ntechnology spanning our networks, data centers, and users’ computing devices. They \ndevelop and install software. They configure, administer, and monitor systems. Their \nactions or inaction can make the difference between a system that is vulnerable and one \nthat is reasonably secure.  \n Those systems include servers, which are still the IT assets most commonly attacked \nand robbed of data. An attacker may initially gain access to your company by compromising a \nuser’s laptop, but the biggest prize—databases of corporate intellectual property and personal \ninformation—still reside on the enterprise servers. To steal that information, attackers now \ntypically often use a compromised end-user device to search the network for servers with \ninadequately configured access controls. Surveys show many attacks continue to exploit \nsecurity holes that organizations could easily have fixed. Among organizations surveyed for \nthe 2015  Data Breach Investigations Report , more than 30 of the exploited vulnerabilities had \nbeen identified as long ago as 1999, yet presumably not addressed at the victim organization. \nAs the report notes, “Apparently, hackers really do still party like it’s 1999.” \n Similar trends can be seen in the incidence of software errors. Many of the most \nserious, frequently exploited vulnerabilities in software are due to well-known errors that \nare “often easy to find, and easy to exploit,” as noted in the 2011 CWE/SANS Top 25 Most \nDangerous Software Errors (CWE/SANS 2011). Furthermore, the situation does not seem \nto be improving. As David Rice, author of  Geekonomics (Addison-Wesley Professional \n2007), puts it, most software is not sufficiently engineered to fulfill its designated role as \nthe foundation for our products, services, and infrastructure (Rice 2007). This is partly \ndue to the fact that incentives to improve quality are “missing, ineffectual, or even \ndistorted,” he concluded. To compete, suppliers focus on bringing products to market \n",
      "page_number": 91
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 100-107)",
      "start_page": 100,
      "end_page": 107,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n78\nfaster and adding new features, rather than on improving quality. Rice estimated, based \non government data, that “bad” error-ridden software cost the United States a staggering \nUSD 180 billion even back in 2007. \n Not surprisingly, the typical recommendations for improving IT security often \nsound remarkably familiar. That’s because they address problems already known to most \norganizations, but not fully addressed. As the Data Breach Investigations Report notes, \nthe question is not which vulnerabilities should be patched (all of them should): “The \nreal decision is whether a given vulnerability should be patched more quickly than your \nnormal cycle or if it can just be pushed with the rest.” Previous editions of the report \nhave recommended basic precautions such as ensuring passwords are unique; regularly \nreviewing user accounts to ensure they are valid and properly configured; securing \nremote access; increasing employee awareness using methods such as training; and \napplication testing and code review to prevent exploits such as SQL injection attacks and \ncross-site scripting, which take advantage of common software errors. \n The fact that these measures do not appear to be rigorously applied at many \norganizations takes us back to a key theme of this chapter: that the commitment \nof employees is as important as the policies and procedures you have in place. If \nadministrators and developers are committed rather than just following directives, if \nthey feel personally responsible for the security of the enterprise, and they will be more \nconscientious about ensuring the right technical controls are in place. \n Insider Threats \n High-profile national security breaches by insiders such as Edward Snowden and Chelsea \nManning have made insider threats a considerably more prominent issue during the \nthree years since the first edition of this book was published. \n Among the 557 organizations participating in the 2014 Cybersecurity Watch \nSurvey (CSO et al. 2014), 28 percent of cybercrime events were attributed to insiders. \nFurthermore, insiders accounted for the highest percentage of incidents in which \nsensitive or confidential information was stolen or unintentionally exposed. \n Insider attacks also cause additional harm that can be hard to quantify and recoup, \nsuch as damage to an  organization’s reputation . Insiders have a significant advantage \nbecause they can bypass physical and technical security measures such as firewalls and \nintrusion detection systems that were designed to prevent unauthorized access. The \norganization’s trust in the insider is used as the attack surface. In at least one case, the \ninsider was the person one might least suspect: the head of information security at the Iowa \nstate lottery, who hacked his employer's computer system, and rigged the lottery so he \ncould buy a winning ticket in a subsequent draw. By installing a rootkit on a lottery system, \nhe could secretly alter the lottery's random number generator, enabling him to calculate \nwinning numbers in advance and buy a winning ticket in advance (Thomson 2015). \n Unfortunately, even  security firms are not immune to compromise; well-known \ncybersecurity company FireEye hired an intern who was later discovered to be a \ntop Android malware developer. Unfortunately, his job at the security firm involved \nresearching and analyzing Android malware, which raises the concern that he could have \nused his inside knowledge to develop malware capable of evading technical controls \n(Fox-Brewster 2015). \n\n\nCHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n79\n Yet surveys have also suggested that many insider attacks are opportunistic, rather \nthan highly planned affairs. Many insiders take data after they’ve already accepted a job \noffer from a competitor or another company, and steal data to which they already have \nauthorized access. In some cases, misguided employees may simply feel they’re entitled \nto take information related to their job. \n Clearly, all organizations need to be aware of the insider threat. It may not be \npossible to thwart all insider exploits, but we can take actions to reduce their likelihood \nand impact. Perhaps the biggest step we can take is to instill a culture of commitment. \nUser behavior analytics technology can also help by detecting behaviors or access \nprivileges that are outside the norm; perhaps technology could have prevented the \ncase in which a former nursing assistant at an Orlando health network inappropriately \naccessed about 3,200 patient medical records, with no apparent motive. Besides \ndisclosing the breach, the health network had to notify the affected patients and offer \nsupport, fire the employee, reeducate the workforce, and increase its efforts to audit and \nmonitor access (Brinkmann 2015). \n To help manage insider threats, consider a  three-part approach : deter, detect, and \ndiscipline. Remember that successful implementation will require the involvement of the \nentire organization. \n Deter \n• \n Build security awareness and instill a culture of commitment, \nusing the techniques discussed in this chapter. \n• \n Make your company a great place to work. Employees are less \nlikely to get disgruntled, and therefore less likely to seek ways to \nharm the company. \n• \n Let people know you’re watching. Technology can help monitor \nusers’ activity. Showing users their activity reports can help \ninvolve them in protecting the business. It also lets potentially \nmalicious insiders know they’re being watched. \n Detect \n• \n See something, say something. A committed workforce will tell \nyou if they see something suspicious. \n• \n User behavior analytics tools are becoming more and more \neffective at finding anomalies in access permissions and user \nactivity, and identifying whether a user’s actions are far enough \noutside the norm that they merit investigation. \n• \n Form a team that focuses on insider threats and investigations. \nThis should operate as a cross-functional team with involvement \nfrom human resources, legal, physical security, and information \nsecurity groups. \n\n\nCHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n80\n Discipline \n When an insider incident occurs:\n• \n If it’s an honest mistake without a big impact, immediate remedial \ntraining may be the best remedy. \n• \n If the impact was low, and the incident seems more an error of \njudgment than a malicious act, a less heavy-handed approach \nmay be appropriate—perhaps a written warning or a comment in \nthe person’s performance review.  \n• \n If the intent is clearly malicious, or the impact is significant, \nconsider the options of termination and even engaging law \nenforcement. \n Finding the  Balance \n One reason that organizations are focusing more attention on security awareness is \nthat their technical controls have failed to prevent attacks from reaching employees \nand thus the core of the enterprise. Rapidly evolving new exploits, often involving social \nengineering as well as malware, have outstripped the capabilities of the security tools \ncompanies have relied on in the past. \n Now, innovative security technology is becoming available that uses machine \nlearning and artificial intelligence techniques to prevent malware much more effectively, \non every type of device. This is great news for all consumers of technology. The adoption \nof this technology should result in a substantial reduction in risk, due to a precipitous \ndrop in malware. The danger is that some will see this as an opportunity to dial back \ntheir security awareness efforts. I think this could be a mistake. We will always need to \nmaintain a level of diligence and discipline in security and privacy awareness. However, \nwe may be able to shift the emphasis of training toward prevention and future risks, and \nfocus on how we should design, develop, and deploy technology that better protects \nprivacy and resists attacks. \n No matter how good our technical controls are, we will still need people to act as part \nof the perimeter. We need to create a sense of personal commitment and security as well \nas privacy ownership among our employees. If we succeed in this goal, we will empower \nemployees to help protect the enterprise by making better security decisions both within \nand outside the workplace. \n\n\n81\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_6\n CHAPTER 6 \n Emerging Threats and \nVulnerabilities: Reality and \nRhetoric \n Curiosity is lying in wait for every secret. \n —Ralph Waldo Emerson \n These days it’s hard to read an online news source, pick up a newspaper, or watch TV \nwithout seeing reports of new threats: cybercrimes, data breaches, industrial espionage, \nand potential destruction of national infrastructure. These reports inevitably leave the \nimpression that we are drowning in an inexorable tide of new and terrifying threats. \n One has to question how much of this is  rhetoric , and how much is reality. There are \npolitical and profit-driven motives for making threats seem bigger and more imminent \nthan they really are. US government officials have warned that cyber attacks potentially \ncan be “devastating, approaching weapons of mass destruction in their effects” \n(Levin 2010). Such warnings have been used to justify requests for increased national \ncybersecurity funding, as well as proposed restrictions on private networks. It’s not \nsurprising, therefore, that some experts have expressed skepticism about the real extent \nof the threat. In fact, academics at the George Mason University Mercatus Center have \nwarned, “the United States may be witnessing a bout of threat inflation similar to that \nseen in the run-up to the Iraq War” (Brito and Watkins 2012). \n On the other hand, common sense tells us new cyber threats really are emerging \nand growing. More data is online and vulnerable to attack, and millions of new Internet-\nconnected devices are inevitably introducing new risks. Malware production has matured \ninto a sizable industry. Government agencies and businesses have suffered real attacks \nattributed to nation-state actors: in 2014, for example, the US Government charged five \nmembers of the Chinese military with stealing information from SolarWorld and other \ncompanies, during a trade dispute over solar-energy products. \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n82\n Given the flood of  often-conflicting information , how can we get an accurate picture \nof the threat landscape so that we can develop an appropriate security strategy? How do \nwe determine which threats directly affect our organizations, and distinguish them from \nthose that are irrelevant? How do we decide which threats require immediate defensive \nmeasures, as opposed to those that attract attention but don’t yet present significant risks? \n In this chapter, I’ll describe methods for identifying the real threat and vulnerability \ntrends among the rhetoric. I’ll also discuss some key areas of threat activity that have been \nanalyzed using these methods. My goal is to help information security groups stay ahead \nof the attackers and focus their limited resources on mitigating the most important threats. \n Structured Methods for Identifying Threat Trends \n To identify the real trends in emerging threats among the mass of news and speculation, \nwe need to carefully examine the available information using a structured, analytical \napproach. Unfortunately, many security groups absorb information about emerging \nthreats using methods that are unstructured and sometimes almost haphazard. \n A typical process looks something like this. The  security team  relies on external \nsources, such as news feeds and alerts, as well as informal anecdotes, to gather \ninformation about emerging threats. Based on this information, the team holds \nbrainstorming sessions to review the threat landscape. The output from these sessions is \na list of “top risks.” Security resources are then focused on mitigating the items on the list. \n There are several problems with this approach. Information comes from a narrow, \nlimited range of sources, resulting in a  blinkered security perspective that tends to stifle \ncreative thinking. Also, the information is usually fragmented, making it difficult for the \nteam to identify trends and gaps in the data. These deficiencies continue through security \nplanning and implementation. Because the team lacks a full view of the threat landscape, \nit’s hard to determine which threats require immediate attention and how much of the \nlimited security budget they deserve. As a result, risks are incorporated into plans on an \nad hoc basis, and not all risks are adequately mitigated. Finally, security teams often don’t \nhave a structured process for communicating threat information to other people within \ntheir organizations. Because of this, people outside the security group remain unaware of \nemerging risks and don’t know how to respond when they experience an attack. \n I realized the limitations of this approach several years ago, and began trying to inject \nmore rigor into the  risk-sensing strategy . Over time, those efforts progressively developed \ninto a more structured risk-sensing process that helps identify threats, prioritize them, \nplan responses, and deliver actionable information to those who may need it. Through \ncontinued use, risk sensing can become a systemic process within any organization. \n The process for  analyzing emerging threats includes several valuable techniques \nthat may be unfamiliar to some security groups. I have used a product life cycle analogy \nto track threats as they mature from theoretical risks into full-blown exploits. I have also \nused nontraditional analysis techniques, such as war games and threat agent profiles, \nto encourage creative thinking and identify threats that might otherwise be missed. I’ll \ndiscuss these methods in more detail later in this chapter. \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n83\n The process can be managed by a small core team, supplemented by a broad set \nof experts (including people outside the security group) across an organization. This \narrangement ensures continuity while enabling the team to mine a diverse variety of \nsources to get a more complete picture of immediate and future threats. \n Security team members should research a wide range of security topics in depth. \nThis diversity of perspective and discussion essentially creates a crowd-sourcing of \nintelligence and reduces the influence of any single person’s bias. Team members \nuse typical sources, such as external feeds and analysis; they also mine academic \nresearch and hacker discussion forums, and connect with security professionals at \nother organizations. Other team members may scan the regulatory horizon to identify \nupcoming laws and regulations with potential impact, or analyze internal investigations \nand other near-miss incident data. \n The team should hold regular meetings to analyze the  threat landscape . At these \nmeetings, each security domain expert explains his or her findings to other members \nof the security team. For each security topic, the discussion should include a review of \nrecent events and a look ahead to the future. This helps identify the key trends and the \nfactors driving those trends, provides context that can be used to analyze the current \nstate, and predicts the likely evolution of each threat. The structured evaluation uncovers \nemerging risks that the team might otherwise miss. It’s also useful to look back at previous \npredictions to see which ones were accurate, and to analyze the reasons why threats may \nnot have materialized in the way that was expected. \n It’s important to communicate the findings to stakeholders across your organization \nin regular reports and briefings, including a wide-ranging annual assessment of the threat \nlandscape. This communication provides further opportunities to get feedback from \nacross the organization and its business units, which can then be used to refine your  risk-\nsensing analysis . \n The  Product Life Cycle Model \n I have found that a product life cycle model is a useful way to track and prioritize \nemerging threats as they evolve and begin to present real risks to the enterprise. Almost \nall security groups have a limited budget, so they need to focus their resources on \neffectively mitigating the  highest-priority threats . \n This model, shown in Figure  6-1 , recognizes that many threats initially emerge as \ntheoretical risks, but are on a path to exploitation, and we need to evaluate and monitor them. \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n84\n Often, researchers or hackers first reveal a possible attack or vulnerability at a \nsecurity conference or publish information about it online. Next, attackers begin testing \nthe use of this technique and making their results publicly available. Once the method \nhas been proven, the threat enters the production phase as attackers start exploiting it in \nearnest. Ultimately, the threat becomes a mature  commodity—source code is often freely \navailable, many variants exist, and organizations treat the threat as part of the everyday \nlandscape and build defenses accordingly. \n This life cycle model enables security teams to systematically track the evolution of \nthreats. It helps us determine when to allocate resources to fighting each threat. As each \nthreat approaches maturity, we can examine how it is likely to affect our organizations \nand plan appropriate mitigation. \n In addition, this model provides a great way to communicate actionable information \nto business groups using terminology they already understand (the product life cycle). \nWhen we provide regular threat landscape assessments to stakeholders, each security \ntopic should include a description of the activity at each life cycle phase, thus providing \na context that helps the security team inform business groups about how they should act \non each of these emerging risks. \n L et’s examine some examples showing how this model can be used in real life. \nFigure  6-2 illustrates the evolution of threats targeting smartphones and other handheld \ndevices. Researchers and hackers began to take notice of handheld devices almost a \ndecade ago, demonstrating weaknesses and theoretical avenues of exploitation. Initially, \nthey focused on what were then known as personal digital assistants. As smartphones \n Figure 6-1.  The product life cycle model for tracking the  evolution of threats. \nSource : Intel Corporation, 2012 \n \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n85\ntook off, attackers shifted their attention to this bigger market, which rapidly became \na major area of threat activity. Monitoring trends at these earlier stages enables \norganizations to prepare. As threats mature and employees begin using smartphones \nmore widely at work, well-prepared organizations are in a better position to develop risk \nmitigation measures including technical controls and incident response plans. \n Figure 6-2.  How an organization could use the product life cycle model to track and \nrespond to  smartphone security threats  \n By visually comparing activity across multiple threat areas, as shown in Figure  6-3 , \nwe can quickly identify major areas of activity and see the likely timing and extent of their \nimpact. This chart also shows us areas in which there are numerous proof-of-concept \ntests and other activities that suggest major problems in the near future. And it indicates \nareas of focused research that may ripen into active exploitation over the long term. \n \n",
      "page_number": 100
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 108-115)",
      "start_page": 108,
      "end_page": 115,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n86\n Although the depth of detail in Figure  6-3 is valuable to the security team, I have \nfound a simpler, consolidated view such as the chart in Figure  6-4 can help communicate \nthe essential trends to a broader audience, supplementing  other  threat analysis materials . \nThese simpler charts are based on the activity identified using the product life cycle \nmodel, but add further trend analysis and group the activity areas into four main clusters, \ndepending on their level of activity and maturity potential and on their potential impact \nto the company. These clusters are \n• \n Sustained drivers : These are areas that already have a high \nimpact or otherwise cause considerable concern. Typically, \nthey are characterized by commoditized distribution and active \nexploitation by multiple threat agents. Today, examples include \nmalware and web attacks. \n• \n Critical trends : These areas have begun undergoing active \nexploitation, with growing adoption beginning to shift toward \ncommoditization. Current examples include social computing \nand smartphones. \n• \n Emerging trends : These areas have a low current level of \nexploitation, but considerable research and proof-of-concept \nactivity. Examples include embedded and cloud computing. \n Figure 6-3.  A visual comparison of  security-related activity across different technology \nareas. Data are for illustration purposes only. Source: Intel Corporation, 2012 \n \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n87\n• \n Disruptive trends : These are areas with little or no active \nexploitation, but significant research activity and the disruptive \npotential to cause a major security problem. Frequently, they are \ndiscussed as theoretical risks, and because of this, many people \nin the industry would be caught off guard by a significant event. \nExamples include virtualization, an area in which potential \nthreats and vulnerabilities have been exposed and a successful \nexploit could cause far-reaching damage. \n I have found that clustering threat analysis information in this way enhances \ncommunication with stakeholders. Representing the information in easy-to-understand \ncharts helps to convey the key trends and their potential impact to a broad cross-section \nof people, helping them quickly assess whether they need to make  adjustments  to \nsecurity strategy. \n \nASSESSING HOW TO RESPOND TO A NEW THREAT \nREPORT\n A continuous stream of new threat reports emerges from agencies, intelligence \nservices, and vendors. It can be hard to determine what to do with all the new \ninformation—especially since most security organizations have limited resources. Here \nare five questions you can ask yourself the next time you see a published  threat report .\n Figure 6-4.  Clustering areas of threat activity to highlight trends. \nSource: Intel Corporation, 2012 \n \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n88\n 1. \n Are we immediately affected? Are the indicators of compromise \nshared in the report found in our environment? If so, we have an \nincident that we must deal with. \n 2. \n If we’re not already affected, what is the likelihood that we \nwill be a future target? We’re more likely to be targeted if we \nwork in the same industry as a previous victim, or if we are \nconnected to them in another way (as a supplier, customer, or \npartner). If the attackers are hactivists or politically motivated \nthreat actors, we are more likely to be targeted if we align with \nthe victim’s philosophy. Note that we may be a target even if \nthere’s no obvious linkage to the victim. \n 3. \n How were the victims attacked? What compensating controls do \nI have in my security stack to mitigate the risks across the kill \nchain of a similar attack?  \n 4. \n Have we seen the same malware used, or families derived from \nit, against our assets? \n 5. \n Were any interesting tools, techniques, or procedures used that \nI should capture and share with my security team? This part of \nthe report can be used to educate responders, architects, and \nrisk managers so they can make better decisions. \n Based on a blog post by Steve Mancini, Director of Information Security at Cylance \n(Mancini 2016). \n Understanding Threat Agents \n Besides the product life cycle analogy, there are other techniques that can help us think \ncreatively about  threats and identify risks we might otherwise miss. \n Behind every threat is a human agent. To effectively plan defenses, it helps if we can \nunderstand why and how these agents operate: their motives, typical methods, and targets. \nHowever, I realized several years ago that we lacked agreed-upon definitions of threat \nagents, as well as a clear understanding of which agents actually pose the biggest risks to us. \n Some agents and their activities attract considerable publicity, resulting in the “TV news \neffect” in which the most-publicized agents appear to be the biggest threat, so they often \nreceive a disproportionately large percentage of limited mitigation resources. In reality, a \nwide spectrum of threat agents exists, some of which may be less well-known but pose bigger \nthreats. For example, hactivists often want to publicize their activities as much as possible \nto draw attention to their cause. This publicity makes them appear to be a bigger threat than \nother groups, such as organized crime syndicates, which try to conceal their exploits. \n In addition, terms often are used without clear agreement about what they mean. The \nphrase  advanced persistent threat has become a buzzword whose exact meaning depends \non who is using the term. It usually implies adaptive, long-term strategies employing a \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n89\nvariety of stealthy techniques and used by attackers with considerable resources. However, \nit’s important to remember that a variety of agents may be capable of generating this type \nof threat. One thing that all these threat agents have in common is the use of malicious \ncode to achieve their goals. But to understand and predict their likely motives and \nmethods, it is useful to clearly define the agents, whether they represent nations or other \npowerful groups, such as organized crime. To solve this problem, Tim Casey, a member \nof my security team at Intel at that time, developed a standard threat agent library that \nprovides a consistent, up-to-date reference describing the human agents that pose threats \nto our information assets (Casey 2007). The library helps risk management professionals \nquickly identify relevant threat agents and understand the importance of the threats.  \n The library acts as a collection point for information about each agent, making it \neasier to share information across your organization. It includes profiles of agents such \nas disgruntled employees, opportunistic employees, industrial spies, and politically \nmotivated attackers. The library also catalogs agents’ typical targets, objectives, skill \nlevels, current activity, and exploit outcomes. When used as part of regular threat \nassessments, this model can help determine which agents pose the biggest risks to your \norganization. The security team can then use the information about their typical methods \nand exploits to help plan its strategy. The library helps the team understand why specific \nevents and attack trends occur and what might happen next. \n \nNSA’S CHIEF HACKER EXPLAINS HOW TO DEFEND \nAGAINST THREATS\n It’s hard to imagine someone who is better placed to provide advice about defending \nagainst advanced adversaries than Rob Joyce, who heads the National Security \nAgency’s  Tailored Access Operations (TAO) elite hacking unit. So the audience \nlistened closely when he took the stage for an eye-opening talk at the 2016 Usenix \nEnigma conference. “My talk is to tell you, as a nation-state exploiter, what can you \ndo to defend yourself to make my life hard,” he said. \n Joyce said that six intrusion phases comprise what is typically referred to as the \n“kill chain:” reconnaissance; initial exploitation; establish persistence; install tools; \nmove laterally; and collect, exfiltrate, and exploit the data. Organizations can thwart \nattackers by disrupting the transition between any of these phases. For example, \nto help prevent reconnaissance turning into initial exploitation, you can reduce the \nattack surface by locking down or disabling devices that are unused or don’t need to \nbe open to access. “Don’t assume a crack is too small to be exploited,” he said. “We \nwill look for that esoteric edge case.” \n Contrary to popular belief, advanced adversaries don’t rely exclusively on zero-day \nexploits, Joyce added. Most intrusions occur via easier vectors: e-mail, web sites \n(using techniques such as waterholing—infecting web sites that are frequently \naccessed by users at the target organization), and removable media like USB drives. \nJoyce noted that you can’t rely on users not to click, even with the best security \npolicies and education (see my Irrefutable Laws of Information Security in Chapter \n 1 ), so you need technical controls that will prevent the execution of malicious code. \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n90\n Once advanced attackers have established a beachhead, they try to steal credentials \nthat enable them to maintain a presence, install tools, and move laterally to the \nprized assets they seek. Techniques such as segmenting the network, limiting \nadministrator privileges, and forcing two-factor authentication can help make this \nmore difficult. Joyce also said that he liked some of the new ideas emerging from \nthe industry such dynamic privileges, which is analogous to the granular trust model \ndescribed in Chapter  7 : the level of access provided depends on factors such as the \ndevice you’re using and your location. \n Finally, he stressed the need to continually evaluate and improve your defenses. An \norganization with static defenses will drift to the back of the herd, where it is easily \npicked off by a predator (see Irrefutable Law #6). “Don’t be that easy mark,” he said.  \n Playing War Games \n I like to conduct war games a few times a year. War games are intense role-playing exercises \nin which employees take on the role of attackers and attempt to compromise key assets using \nany feasible methods. I have found war games are particularly valuable for analyzing threats \nthat may have major consequences but whose vulnerabilities are not well understood. \n This technique provides the most comprehensive method of assessing threats to \nkey assets because the people playing the role of our adversaries are essentially allowed \nto use any method to achieve their goals. However, because of this, it is also resource-\nintensive and should be used selectively. \n Typical war games that I have overseen take one and a half days and may involve \neight to ten staff from a variety of roles, such as factory workers, business process \nleads, salespeople, and technical experts. Some war games can take much longer; in \n Wargaming for Leaders , written by wargaming experts at management-consulting firm \nBooz Allen Hamilton, (Herman, Frost, and Kurz 2009), the authors discuss games that \nmay last weeks and involve many more players across an organization. \n A typical game focuses on a specific target or scenario, such as disabling a key facility \nor stealing trade secrets. You can use war games to examine potentially catastrophic \nevents that have a low probability of occurrence, but a high probability of causing damage \nif they do occur. Team members are instructed about the threat agents involved and draw \non archetypes from a threat agent library or descriptions provided by the game architect. \nLed by a facilitator, the team takes on the attacker’s perspective and postulates ways to \nachieve the attack’s objectives. \n Because the team can propose any attack method, they often identify risks that might \nbe overlooked using conventional methods. As the authors of  Wargaming for Leaders put \nit, “We create the environment, the players engage, and what comes out of team play often \nsurprises and even stuns everyone involved.” For example, a malicious group might attempt \na devastating attack by purchasing a small but essential technology provider and inserting \nmalware into their products in order to infect their customers. After each game, security \nanalysts examine the results to determine how to address newly identified vulnerabilities. \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n91\n I also like to examine the cyber consequences of large physical events as part of \ndisaster recovery planning. These could include earthquakes and tsunamis that damage \ndata centers, or even solar flares that disrupt the communications that the business relies \non. Exercises can include drills that last a day or more. \n A large organization can justify the considerable effort involved in conducting these \nexercises because of the enormous potential benefit of mitigating the threats. In fact, \nsome organizations hire professionals to create and facilitate these games. Booz Allen \nHamilton, for example, has an extensive war gaming practice covering diverse subject \nareas including market dynamics, cybersecurity, geo-political events, and even real war \nscenarios. \n But smaller organizations can also benefit by considering extreme events and \nformulating response plans. If you prepare for the extreme, you’ll be more prepared \nto deal with everyday events. Planning doesn’t need to be as resource-intensive as a \nfull-blown war game. It can be as basic as bringing team members together to discuss \nlikely scenarios and responses in a shorter tabletop exercise lasting just a few hours. \nThis method enables members to get a feel for what it would be like to work together in \nthe event of a real disaster. Considering these extremes can also provide motivation for \nintroducing simple yet effective measures to reduce the risk that catastrophes will occur. \nYou might realize it is worth increasing investment in user education to reduce the risk \nof social engineering compromises, or becoming more diligent about analyzing logs and \nnetwork traffic to identify patterns that indicate suspicious activity. \n Trends That Span the Threat Landscape \n I’ve described some of the methods that can be used to analyze emerging threats. Now \nI’d like to turn to some key themes that have emerged from such threat analysis. These \nthemes paint a  broad-brush picture of threat and vulnerability trends spanning multiple \ntechnologies across the threat landscape. \n Trust Is an Attack Surface \n As the technology industry erects new technical defenses, attackers seek to bypass these \ncontrols by exploiting user trust, typically using social engineering techniques such as \n phishing . \n If an attacker can win a user’s trust with a sufficiently convincing e-mail or fake web \nsite, the user will make it easy for the attacker by clicking a link or downloading a file. \nThese actions usually undermine even the most rigorous system-level controls, initiating \na chain of compromises that ultimately can result in major damage. \n Whenever users place their trust in a new technology, attackers quickly follow. \nStudies have shown that users trust social media services more than other information \nsources. A user is more likely to click a link if it appears to have been sent by a social \nmedia “friend.” Exploiting this trend, attackers have spread malware via social computing \ncircles of trust such as friend networks. \n Attackers have also been quick to take advantage of the trust users place in their \n smartphones and in other appliances such as game consoles. The exploitation of trust \nalso extends to the relationships between systems. Once configured, communications \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n92\nbetween systems often operate autonomously, without manual oversight. Smartphones \nare set to automatically update applications from trusted app stores; other systems \nblindly trust firmware updates and dutifully install them. This automation provides \nconvenient opportunities to insert malicious code, abusing trust without the need to \ndirectly involve the user. \n In the near future, I anticipate trust will become a commodity that is bought and sold. \nThe digital reputation of systems and services will become critically important. In the past, \ntokens of trust, such as digital certificates and social computing credentials, were stolen \nfor immediate use. In the future, they will be stolen so they can be sold in underground \nmarkets. The value of these tokens depends upon the access they grant and the other circles \nof trust they can be used to penetrate. Already, attackers are using stolen digital certificates \nto sign their malware in an attempt to avoid detection by operating system defenses. \n I expect  social engineering attacks will continue to present significant risks because \nthey exploit human weaknesses and will adapt to take advantage of new technologies. \nSo we, as security professionals, need to focus on the role of users as part of the security \nperimeter, as I discussed in Chapter  5 . To reduce the risk to the enterprise, we need to \nmake users more security-aware and influence them to act in more secure ways. But it’s \nalso important to note that a successful phishing exploit is also ultimately a technology \nfailure that allowed malicious code to execute. \n Barriers to Entry Are Crumbling \n Our adversaries gravitate toward the path of least resistance. They tend to select targets \nthat are easy to access and analyze, and they typically use the most readily available and \ncheapest tools. \n They are much less likely to use methods with high barriers to entry such as the need \nfor specialized expertise, expensive hardware or software, or access to extensive compute \ncapacity. However, several of these barriers have begun to crumble as a result of trends \nsuch as cloud computing, lower-cost communications components, and commodity \nmalware toolsets. This trend ultimately is likely to result in new types of attack. \n A key factor is that security researchers are sharing not only their knowledge but also \nthe tools they design as part of their research. Recently publicized tools, such as rogue \nbase stations and Bluetooth sniffers, provide attackers with more accessible, low-cost \nways to intercept network traffic. Researchers have uncovered vulnerabilities in femtocell \ndevices (miniature, low-cost cell towers) that can be used to take control of the devices, \nlowering the barriers to attacks targeting cell phone data traffic. \n Ultimately, lower barriers to entry mean increased risk to enterprises. However, \nbecause several of these areas are still at the research stage, it will take time for them to \nmature into active exploitation. \n The Rise of  Edge Case Insecurity \n Each day, the environment becomes more complex with millions of new devices, each \nrunning its own operating system and collection of applications. This complexity generates \nnew edge cases—problems or situations that occur only in unexpected or extreme situations. \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n93\n Edge cases can include unlikely interactions between two familiar objects. A hacker \nteam recently demonstrated that, with a popular smartphone, a paperclip (used to pop \nout the phone’s SIM card at the critical moment), and a little patience, it’s possible to \ngain access to contact information, phone call logs and voice mail, e-mails, and other \ninformation stored on the phone. \n Overall, the growing number of third-party plug-ins and widgets introduce edge \ncases that are hard for developers to anticipate even if they use secure design techniques. \n Interoperability between programs has resulted in a new category of hybrid attacks \nwhere malicious objects are concealed in innocent-looking ones to thwart detection. \nOne proof of concept in 2011 demonstrated it was possible to conceal a fully functioning \nTrojan in an e-mail plug-in. \n Some of these hybrid attacks have shown they can circumvent new security features. \nAs web browsers and search engines try to protect users from malicious links, attackers \nare responding by hiding links in image search results, where they cannot be detected \nusing standard tools. Research into network intrusion methods has discovered over a \nhundred methods of evading detection by manipulating traffic to remain functional but \nundetectable by typical tools. \n There is no silver-bullet solution for eliminating edge-case insecurities. It’s unlikely that \neven the most rigorous testing could ever uncover them all. The best approach may be to \nexercise caution when adopting new technologies with the potential to generate edge cases. \n The Enemy Knows the System \n The technology industry has often relied on security through  obscurity:  the idea that if \nattackers can’t see the insecurities in code or other technology, they won’t exploit them. \n Over time, it has become clear that security through obscurity is poor security. To \nquote the maxim coined by Claude Shannon, one of the founders of modern computing, \n“The enemy knows the system.” \n It’s now relatively easy for attackers to get access to the same tools enterprises use, \nsuch as web hosting services and smartphone application development tools. Hackers \ncan now more easily engineer malware and attacks that take advantage of these elements. \nThe fact that static platform controls tend to become less effective over time (one of the \nIrrefutable Laws of Information Security noted in Chapter  1 ) is partly due to the ability of \nmalware authors to pretest their malicious code against technical controls. They can do \nthis by obtaining code from malware repositories that have already been tested against \nexisting controls, or by actually purchasing the technical controls. \n Even the success of social engineering demonstrates that the attackers’ knowledge \nof the target greatly increases the likelihood of successful deception. Today, competitors \nand other threat agents learn a great deal about a company and its employees by simply \nsearching information publicly available on web sites or social media accounts. \n Because we cannot assume insecure technology is safe just because it is hidden, we \nneed to design with security in mind. The ineffectiveness of security through obscurity is \nalso an argument in favor of standards and open-source solutions. This idea may initially \nseem counterintuitive, but the fact that open source is exposed to public scrutiny requires \nit to be secure. At a minimum, we should ensure devices are rigorously tested against \nindustry standards because the attackers will do so. \n",
      "page_number": 108
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 116-123)",
      "start_page": 116,
      "end_page": 123,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n94\n Key Threat Activity Areas \n Threats are evolving in many technology areas, from embedded systems to cloud \ncomputing. I’d like to discuss a few areas experiencing significant developments with \nimplications for enterprise IT. \n The Industry of Malware \n Malware has become a profitable industry that increasingly resembles the legitimate \nsoftware market, with market leaders, mergers, licensing agreements, real-time support, \nand open source. The organized business activity in this market reflects the extent to \nwhich well-crafted malware has become a viable career pursuit for members of the \ncriminal underground. \n Today, malware development and malware use may in some cases be distinct \nactivities carried out by different groups or individuals. Malware authors are producing \nstandardized toolkits, which have made life much easier for would-be attackers. These \nattackers can now simply buy or acquire a toolkit rather than expending the effort to \nidentify vulnerable web sites and develop their own exploits. \n The Zeus malware family provides a useful case study showing how complex this \nindustry has become and how hard it is to accurately track developments. Sold mainly \nin underground forums, Zeus has been used extensively for theft by creating botnet \nnodes. During 2011, a code merger was reported between Zeus and another popular \ncrimeware kit, complete with assurances of future support for the customers of both \nproducts. Around the same time, Zeus toolkit source code was made publicly available. \nSince then, multiple new variants have appeared and been used for a variety of attacks. At \none point, security researchers attempting to monitor Zeus exploits discovered a server \nthey believed was the hub of a Zeus botnet. However, the server was the equivalent of \nan espionage honey pot, allowing the botmasters to turn the tables by spying on the \nresearchers who were attempting to analyze the hub. \n Ransomware has also become a profitable activity for some organized crime \nelements. Ransomware was mostly at the validated proof of concept stage when I wrote the \nfirst edition of this book in 2012; it has since progressed to active exploitation with some \ncommoditization. Today’s ransomware exploits typically exploit system vulnerabilities \nusing Trojans and other methods, then lock or encrypt information so users cannot \naccess it and hold people and organizations hostage until they pay. In February 2016, a \nLos Angeles hospital paid a ransom in bitcoin after staff were locked out of the hospital’s \nown network for more than a week; during the same month, one ransomware variant was \nreported to be infecting more than 90,000 PCs per day (Fox-Brewster 2016). \n The Web Expands to the Internet of Things \n The Web continues to present a huge attack surface. And this attack surface is growing \nrapidly as it expands to include the Internet of  Things , encompassing  nontraditional \ndevices such as appliances and control systems, cars, wearable and medical devices, and \nthe “smart” grid. Each of these is a potential source of risks. \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n95\n Recent headlines have highlighted the growing threat activity focused on  IoT . \nResearchers hacked into a Jeep via its Internet-connected entertainment system and \nremotely controlled the vehicle’s functions, including turning off the transmission and brakes \nwhile someone was driving (Greenberg 2015). Other researchers showed that thousands of \ndevices in hospitals are vulnerable to attack, including x-ray machines, MRI scanners, and \ndrug infusion pumps, partly because medical equipment is increasingly connected to the \nInternet so that data can be fed into electronic patient records systems (Pauli 2015a). Yet \nanother researcher demonstrated the ability to hack into FitBit fitness trackers via Bluetooth \n(Pauli 2015b). Many IoT devices, including cars, wearables, and home appliances,  include \nwireless capabilities, so exploitation doesn’t require a physical network connection. \n Clearly, we should expect continued growth in IoT threat activity. However, should \nbe noted that the activity to date has generally has been at the research or early proof-of-\nconcept phase (see Figure  6-1 ). As the IoT expands and matures, we will see a progression \nto advanced active exploits over the next few years; given the rapid pace at which IoT \nis evolving, if companies don’t use good privacy and security design principles when \nbuilding their products, the time from research to active exploitation could be much \nshorter than has typically been the norm. \n Many  embedded devices that are already installed in businesses are similarly \nvulnerable. Companies have a history of deploying specialized devices without adequate \nsecurity controls, often because of the perception that specialized devices are “dumb” and do \nnot have a full set of capabilities. In reality, the opposite is often true: devices marketed for a \nspecific function are often capable of much more. Printers contain processors, use wireless \nconnections, and may be capable of acting as file servers, for example. As a result, embedded \ndevices can introduce as much risk, or more, to an organization as a traditional computing \ndevice since they lack security controls and administrators are generally unaware of the \ndanger. New devices may be vulnerable to new attack methods: recent research showed \nthat the sounds 3D-printer nozzles make as they cross the machine bed can be recorded by \nsmartphones, analysed, and then used to duplicate prototypes (Nelson 2016). \n The vulnerabilities in embedded  industrial control systems were exposed by \nthe widely publicized Stuxnet malware, which was used to sabotage the systems that \nsupported Iran’s uranium enrichment capabilities. The incorporation of computer-based \ncontrol and automation technology into the existing electrical power infrastructure—\nresulting in the “smart grid”—is another source of potential vulnerabilities. The US \ngovernment has warned of increasing threats to the grid, noting that many embedded \nsystems lack adequate security controls and are susceptible to known techniques such as \ncross-site scripting attacks (US GAO 2012). \n We might also see logical attacks as precursors to physical attacks. On a macro scale, \na nation state might attack another nation’s cyber infrastructure before staging a physical \nattack. This approach might also be applied at a more personal level. A burglar might \nremotely disable an Internet-connected alarm system before sneaking into a house, or \nperhaps even use the system’s video cameras to watch the owners and note when they \nleave the house unattended. \n Here are two more potential future  IoT scenarios in which innovative technology \ndesigned to do good could be exploited for harm, unless designed with strong security \nand privacy protection. Last year, doctors for the first time inserted an artificial “eye” that \nenabled a blind person to see. The device is a retinal implant that receives signals from \na video camera integrated into eyeglasses. Think ahead a few years, to a time when the \nimplants are more sophisticated and can see in much higher resolution, and also include \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n96\nsoftware to automatically interpret visual information, such as QR codes. Then imagine \nthat a malicious actor creates a QR code that triggers the vision system to download \nmalware. Like the PC malware that paralyzed Sony’s network in 2014, the malware then \ndemands a ransom to re-enable the person’s vision. Now consider the example of a \ncement company that’s embedding sensors in the concrete mix used to build a new road, \nthus enabling local authorities to monitor traffic patterns and adjust signals to optimize \nthe flow of vehicles. If the technology is not securely designed and implemented, all that a \nmalicious person needs is the ability to execute malicious code, in order to falsify the traffic \npattern in such a way that vehicles converge on the scene of a planned bomb attack. \n Smartphones \n Smartphones are attracting almost as much malicious interest as desktop and laptop \nplatforms. However, even though smartphone sales have outstripped PC sales, \nsmartphone malware isn’t yet as prevalent as PC malware and doesn’t cause the same \nkind of widespread damage. That’s partly because most valuable corporate and personal \ndata is still held on PCs and servers. Another factor is that smartphone vendors have \nsomewhat greater control over applications, since users generally access them via \nvendor-controlled app stores. \n Just as in legitimate software markets, malware authors are likely to maximize the \nvalue of their code by using tools that allow their software to run on multiple devices. \nThey are increasingly targeting applications, a trend also seen on other platforms. \nAttackers have purchased copies of applications, incorporated their malicious content \ninto the otherwise legitimate software, and then redistributed their code under a new \nname or as a “free” version of the original. On one smartphone platform, autodialing \nmalware was found in more than 20 applications. Variations of a Trojan were found in \ndozens of applications and are believed to have been downloaded by at least 30,000 users. \n A further development is the use of smartphones as bridges to traditional networks, \nresulting in the potential for enterprise network attacks that originate from within mobile \nnetworks. \n In the future, we could see greater exploitation of location-based services to deceive \nusers. Because smartphones contain location sensors such as  Global Positioning System \n(GPS) chips, knowledge of the phone’s location can be used to present targeted ads \nand useful information. For example, a user in a supermarket aisle might be presented \nwith online coupons for products on nearby shelves. But this information could also be \nexploited to present fake coupons that are all the more convincing because they suggest \nthat the sender knows the user’s preferences. \n Attackers could also exploit other smartphone capabilities to take advantage of the fact \nthat the devices are carried into confidential meetings and other highly sensitive situations. \nImagine being able to remotely control a device that has a microphone, a camera, or other \nrecording capabilities. Or think about a vulnerability in any of the popular web-conferencing \nservices that people use for confidential discussions and to exchange information. \n Current trends in the mobile platform space indicate that attackers are most \ninterested in stealing personal data. This trend is partly due to the increasing use of \nsmartphones for financial and banking transactions, which provides new opportunities \nfor identity thieves and other criminal groups. As a result, it is now important that \nsmartphone hardware and software developers focus on protecting personal data. \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n97\nSoftware developers should adopt the same discipline and commitment to following \nsecure design principles as traditional platform developers. Today, more and more \npeople are becoming app developers, creating software, and posting it online for others \nto use. One has to question how much security testing and validation has been applied \nto these applications. As users move more of their everyday activities onto smartphones \nand other small devices, the consequences of poor or insecure designs will have greater \nimpact on individuals and their employers. \n Web Applications \n Web applications , primarily comprising client browsers and server-based applications, \ncontinue to be heavily attacked. Threat analysis indicates that this area is experiencing \nfull exploitation activity and moving toward commoditization. There is also considerable \nresearch in this area, suggesting the number of attacks will continue to grow. \n Attackers have adopted new techniques to hide their intentions and deceive users \nlong enough to achieve their aims. As web browsers and search engines try to protect \nsystems from malicious links, attackers are instead obfuscating their links in image search \nresults, where they may not be detected. \n Techniques for hiding messages within images have been used within the security \nrealm since long before the invention of information technology. Now, this technique, \nknown as  steganography , is being used to hide malware and botnets on publicly used \nimage hosting sites. \n Search poisoning has also become a common method. Attackers using search \npoisoning tend to focus on events and topics of popular interest, optimizing their web \npages to achieve high search engine rankings. After a search query, the victim clicks a link \namong the search results. They are redirected multiple times and eventually land on a \npage that is used as a vector to deliver malware. \n Conclusion \n In this chapter, I’ve outlined some of the real threat trends and described methods \ninformation security groups can use to analyze the threat landscape as it continues to evolve. \n No doubt, new and more sophisticated types of exploitation will continue to emerge, \nand we need to stay aware of them. As Mustaque Ahamad, director of Georgia Tech \nInformation Security Center, noted in 2011, “We continue to witness cyber attacks of \nunprecedented sophistication and reach, demonstrating that malicious actors have the \nability to compromise and control millions of computers that belong to governments, \nprivate enterprises, and ordinary citizens.” \n Yet, as we try to make sense of the deluge of news about attacks and vulnerabilities, \nit’s essential to retain a sense of perspective. Most threats do not take place using exotic, \nobscure methods. Instead, they take the path of least resistance, exploiting well-known \nvulnerabilities. Therefore, business can mitigate many of these threats by implementing \nbasic, established security measures. To put it another way: when you hear hoof beats, \nthink horses—not zebras. \n\n\nCHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n98\n Social engineering will continue to be a key attack method because it takes \nadvantage of user trust and is hard to prevent using technical controls. Therefore, as I \ndiscussed in Chapter  5 , we need to continue to focus on educating users to become more \nsecurity-aware. By doing so, we can reduce the risk to the enterprise. \n Ultimately, while doing our best to prevent compromises and breaches, we must \nremember we cannot control the threat actors and their exploit attempts. Because \nall threat categories use malicious code in some way, advanced preventive tools that \neffectively stop the execution of malicious code can greatly reduce the potential of \ncompromise. But all organizations face the possibility of some level of compromise, \nmaking defense in depth as essential as ever. Losers ignore the trends. Winners survive by \nbeing able to predict, prevent, detect, and respond. \n\n\n99\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_7\n CHAPTER 7 \n A New Security Architecture \nto Improve Business Agility \n An organization’s ability to learn, and translate that learning into action \nrapidly, is the ultimate competitive advantage. \n —Jack Welch \n Some  Star Trek episodes feature suspense-filled battles in which adversaries use \nsophisticated phase-shifting weapons that can be rapidly adjusted until they find a way \nto penetrate static force-field defenses. For a beleaguered starship, the only effective \nresponse is to use similarly adaptable and fast-changing shields. \n As information security professionals, we also need extremely agile defenses that \nquickly adapt to meet new demands. Attackers are continually adapting, and defenders \nalso need to continually adapt. But rapidly evolving threats are only part of the challenge. \nWe also need to continually adapt our defenses to a rapidly changing technology \nlandscape. \n As information risk and security groups consider the future, it’s clear that we need \nto radically change our approach in order to face the challenges ahead and support the \nProtect to Enable mission. \n One problem in recent years has been that most of the protection offered by the \nindustry has not kept up with the attackers. Because these tools have failed to prevent \nharm, many companies have defaulted to a detect-and-respond approach. This means \nthey continue to expose themselves to high risks and higher long term costs since they are \nreactively responding to attacks that have already breached the organization’s defenses. \n We also need to consider whether our existing control architecture improves or \nimpedes business agility and velocity. It’s important to recognize that controls can \nplace a “drag coefficient” on the business. By hindering users, they can stifle business \nvelocity and innovation. Users react to this control friction by circumventing the controls \nwhenever possible; as a result, the controls can actually introduce new risks, as discussed \nin Chapter  2 . \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n100\n As we move forward, we will need an agile security architecture that quickly and \nautomatically learns and adapts to new challenges as they emerge. A learning system is \nharder to defeat because it can more quickly predict and thus prevent new attacks. The \npace of change is so rapid that we cannot predict all the challenges we will face, and \nmanual or semi-manual processes will not be enough to keep up. We will need solutions \nthat can learn to manage what we don’t know. \n The right  control architecture will enable flexibility that helps the business move \nmore quickly, allowing us to rapidly adopt new technologies and emerging usage models \nwhile continuing to provide security in the ever-evolving threat landscape. \n A few years ago, after intense brainstorming sessions, the information risk and \nsecurity team I led at Intel devised a new security architecture for the company. This \narchitecture represented our implementation of the Protect to Enable strategy, using the \ntechnologies that were current at that time. With the benefit of hindsight, I believe that we \ngot many things right—but there were also some omissions because we didn’t have a full \nunderstanding of the controls that would be needed. \n In this chapter, I’ll provide a high-level overview of a new security architecture and \ndescribe how it meets some key challenges. Some of this overview is based on the work at \nIntel a few years ago, but I have added a new perspective on controls that I have realized \nis lacking in the industry. An important aspect of this new perspective is the concept of \ncontrol friction. As I’ll explain later in the chapter, I’ve developed a simple framework \ncalled the 9 Box of Controls, which takes control friction into account when assessing the \nvalue of security controls. \n I believe that the architecture includes some  novel approaches that may be valuable \nto many organizations facing these universal challenges. My conversations with peers \nat other companies have validated this view. Many of them are considering similar \nstrategies and in some cases have begun implementing them. \n Any future security architecture must provide better prevention, and it must also be \nmore flexible, dynamic, and granular than traditional enterprise security models. This \nwill helps us all accommodate future evolving usage models. We can provide users with \ndifferent levels of access depending on factors such as the devices they are using and their \nlocation. To achieve this, the architecture dynamically adjusts a user’s access privileges \nas the level of risk changes. For example, an employee should have more limited access \nto our systems when using a less-secure device than when using a more hardened or \nperhaps fully managed enterprise-class system. \n The new  architecture greatly improves  threat management . As new attacks appear, \nwe need to be able to recognize good from bad in milliseconds, so that we can stop the \nbad and allow the good. For any attack that gets past the preventive controls, we need to \nlearn as much as we can without compromising the user’s computing performance or \nprivacy. This information enables us to investigate what occurred, so we can quickly take \naction to mitigate the risk and also learn how to prevent similar attacks in the future. A \ncontrol architecture should assume that attempts at compromise are inevitable, but we \nshould also understand that it’s possible to achieve real prevention for 99% or more of \nmalicious code. We can apply artificial intelligence and machine learning to analyze the \nfeatures of files, executables, and binaries to stop malicious code prior to execution. For \nthe remaining attacks, representing less than 1% of malware, we need to focus heavily on \nsurvivability. \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n101\n The 9 Box of Controls, Business Trends, and \nArchitecture Requirements \n Before diving into the specifics of the architecture, I’ll explain the 9 Box of Controls. Then \nI’ll recap some of the key business and technology trends, focusing on how they drive the \nneed for specific capabilities in security technology. \n 9 Box of Controls \n There are three primary types of security controls: prevention, detection, and response. \nPrevention occurs when an action or control prevents a risk before it affects users or \nthe environment. Detection is identifying the presence of something malicious that \nhas already entered the environment. Response is a reaction. From a risk perspective, \nprevention focuses on minimizing vulnerability and the potential for harm, while \ndetection and response focus on minimizing damage. \n There are also three primary levels of control automation: automated, semi-\nautomated, and manual. Automated control occurs entirely through machines. Semi-\nautomated involves some level of human intervention. Manual controls are managed \nentirely by hand. \n The combinations of these control types and automation levels comprise the cells of \nthe 9 Box, as shown in Figure  7-1 . Risk increases as we move from prevention to detection \nto response. Cost increases as we move from automated to semi-automated to manual \ncontrols. \n Figure 7-1.  The  9 Box of Controls \n \n",
      "page_number": 116
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 124-138)",
      "start_page": 124,
      "end_page": 138,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n102\n However, there is a third dimension to the 9 Box: control friction. As we know, \nfriction is the force that causes a moving object to slow down when it is in contact with \nanother object. Similarly, controls can impose a “drag coefficient” on business velocity—\nthey can slow the user or a business process. However, friction is not a fundamental, \nimmutable force like gravity or electromagnetism. Instead, we have the ability to \ndetermine how much control friction we apply. Apply too much control friction, and \nbusiness users will go around IT and its security controls. This adds cost: IT is no longer \nmanaging the technology; data and business silos are created, and the organization \nloses its volume purchasing power. It also adds risk: because the security team lacks \nvisibility into the technology, it cannot prevent compromises, detection is difficult, and \nin many cases response after the fact becomes the only option. If a business adheres to \nhigh-friction controls, the effect can be to generate systemic business risk. High-friction \ncontrols can hinder business velocity; the organization can lose time to market and the \nability to innovate, and over the long term it may even lose market leadership. \n IT Consumerization \n As I discussed in Chapter  5 , consumerization is a major IT theme with ever-broadening \nimpact. It includes several trends, including the adoption of new applications and \nsupport for consumer devices. \n Many highly mobile employees want to use their own consumer devices, such as \nsmartphones, wearables, and tablets, for work. This increases productivity by enabling \nemployees to collaborate and access information from anywhere, at any time. To support \nthis, organizations provide access to corporate e-mail and other applications from \nemployee-owned smartphones and tablets. \n Some people believe that in the future, all devices will be consumer-owned, and that \nenterprises will no longer purchase devices for their users. I believe this might be the case \nin some work environments, but I doubt that it will suit all organizations. For a company \nproviding call center services, with most employees working from home, it might make \nsense that employees exclusively use their own personal systems for work. But this \nstrategy could be more risky for a financial services company whose employees handle \nhighly sensitive information that is subject to extensive regulatory requirements. \n Nevertheless, the consumerization trend continues to grow at almost all \norganizations. Accordingly, we’ll need to provide employees with a level of access to \nresources from an expanding continuum of client devices, some of which may have much \nweaker security controls than today’s enterprise clients (see sidebar). \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n103\n \nCONSUMERIZING ENTERPRISE IT AND “ENTERPRISING” \nTHE CONSUMER\n Discussions of  IT consumerization tend to draw a clear line between business \ndevices that can be managed and trusted, and personal consumer devices that are \nessentially unmanaged and untrusted. \n However, not all consumer devices are created equal. From a security standpoint, \nit may be more valuable to think about a device’s capabilities than to categorize it \nbased solely on whether it’s marketed as an enterprise device or a personal device. \nThe security of a device depends on the inherent features of the hardware, operating \nsystem, and applications, and on whether it enables us to add further security and \nmanageability capabilities that mitigate the risks of enterprise use. \n As the variety of consumer devices, such as smartphones and wearables, continues \nto expand, users may choose from dozens of models with different levels of security \ncapabilities. Greater security and manageability means that IT can place greater trust in \nthe device and provide a correspondingly greater level of access to enterprise resources. \n Extending this idea further, the information security group could evaluate the \nsecurity of available consumer devices and provide guidance about the level of \nenterprise access that users will be allowed with each device. Users may prefer to \nbuy a more secure device because it will provide them more access. With greater \naccess, they can use the device for more of their daily work activities. This ability in \nturn enables them to be more productive. \n At the same time, employees increasingly expect to have available to them at work \nthe types of consumer services and cloud applications that they use in their personal \nlives. These include social computing applications such as blogs and wikis, video-sharing \nsites, and file-sharing services. \n We need a security architecture that enables us to more quickly support new devices \nand provide access to a greater range of applications and data, without increasing risk. We \nneed to be able to dynamically adjust the levels of access we provide and the monitoring \nwe perform, depending on the security controls of the client device. \n New  Business Needs \n Nearly all companies now rely on a growing network of business partners, and conduct \nmany of their interactions with those partners online. Many organizations are also \nexpanding into new markets through both organic growth and acquisitions. Because of \nthese business trends, most organizations will need to provide access to a broader range \nof users, many of whom are not employees. Many organizations also need to be able \nto smoothly integrate acquired companies and provide them with access to resources. \nIn general, we need to quickly provide new users access while minimizing risk and \nproviding selective, controlled access only to the resources they need. \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n104\n Cloud Computing \n Most organizations are already using cloud services in some form to achieve benefits \nsuch as greater agility and lower cost. Some are also implementing a private cloud \nbased on virtualized infrastructure while using external cloud services for noncritical \napplications. In the future, I expect greater use of hybrid clouds that use both internal and \nexternal resources, especially for organizations that are anchored to legacy environments. \nOrganizations able to let go of their legacy environments will predominantly use the \ncloud, with limited internal infrastructure. \n This trend means that IT services at many organizations will be provided by a \nmixture of traditional and cloud-based internal and external services. During a typical \nday, employees may access a variety of different services, some of which are internal and \nsome external. Ultimately, they should be able to easily move between these services \nwithout needing to log in multiple times or even know where the services are located. \n Securing access to cloud-based services presents challenges that aren’t easily \naddressed using conventional security controls. In cloud environments, systems and \ntheir data are virtualized and may migrate dynamically to different network locations. \nThis makes it difficult to effectively restrict access using traditional security controls such \nas firewalls, which rely on fixed locations of systems and a more static nature of the data. \nWe need much more granular and dynamic controls that are linked to the resources \nthemselves rather than just their network location. \n Changing  Threat Landscape \n The threat landscape is evolving rapidly. Increasingly, attackers have taken a stealthy \napproach, creating malware that quietly gains access and attempts to remain undetected \nin order to maintain access over time. This has been possible because the security \nsolutions deployed on endpoints in most organizations today do not adequately prevent \nmalicious code from executing. As the number of threats increases and new types of \nmalware emerge, we need to focus on the 9 Box of Controls and seek new prevention \nmethods reduce risk, reduce cost, and reduce control friction. \n Traditional enterprise security architectures have relied largely on protective \ncontrols such as firewalls located at the network perimeter and signature-based antivirus \nat the end points. At the same time, our focus has shifted to providing controlled access \nto a broader range of users and devices, rather than simply preventing access. Combine \nthis with a continually changing threat landscape, and we can assume that attempts to \ncompromise the environment are inevitable. Although existing perimeter controls such \nas firewalls will continue to have some value, we need tools that can dramatically increase \nthe ability to prevent attackers from gaining access to the environment, but in way that \ndoes not introduce a cost burden or a high degree of control friction. \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n105\n Privacy and Regulatory Requirements \n The growing emphasis on privacy requirements and the increasingly complex regulatory \nenvironment have many implications for the way we manage information. Some \nregulations create the need for more control over where information is stored and require \nspecific levels of protection and tracking. Our architecture must provide this assurance, \nallowing us to build a high-security environment and access controls appropriate for the \nprotection of highly regulated information. In addition, the security controls themselves \nmust not introduce privacy risks. \n New Architecture \n To meet these rapidly changing requirements, we need a highly flexible and dynamic \narchitecture. The architecture should enable us to more quickly adopt new devices, use \nmodels, and capabilities; provide security across an increasingly complex environment; \nand adapt to a changing threat landscape. \n Key goals include helping increase  employee productivity while supporting new \nbusiness requirements and technology trends, including IT consumerization, cloud \ncomputing, and access by a broader range of users. At the same time, the architecture \nshould be designed to prevent risk, reduce our attack surface, and improve survivability—\neven as the threat landscape grows in complexity and maliciousness. \n The architecture moves away from the traditional enterprise  trust model, which \nis binary and static. With this traditional model, a user is in general either granted or \ndenied access to resources; once granted, the level of access remains constant. The \nnew architecture replaces this with a dynamic, multi-tiered trust model that exercises \nmore fine-grained control over identity and access control, including access to specific \nresources. This means that for an individual user, the level of access provided may vary \ndynamically over time, depending on a variety of factors—such as whether the user is \naccessing the network from a highly secure managed device with advanced anti-malware \ncapabilities or an untrusted and perhaps unmanaged device. \n The architecture’s flexibility allows us to take advantage of trust based on real proof \nthat malware execution is being prevented. Increasingly, devices may include some level \nof  hardware-enforced security designed to ensure the integrity of the applications and \ndata on the device. The architecture takes this into account when determining whether to \nallow access to specific resources—a more-trusted platform can be allowed greater access \nthan a less-trusted one. The architecture is based on four cornerstones:\n• \n Trust calculation : This unique element of the architecture \nhandles user identity and access management, dynamically \ndetermining whether a user should be granted access to specific \nresources and, if so, what type of access should be granted. The \ncalculation is based on factors such as the user’s client device \nand location, the type of resources requested, and the security \ncontrols that are available.  \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n106\n• \n Security  zones : The infrastructure is divided into multiple \nsecurity zones that provide different levels of protection. These \nrange from trusted network zones containing critical data, with \ntightly controlled access, to untrusted zones containing less-\nvaluable data and allowing broader access. Communication \nbetween zones is controlled and monitored; this helps ensure \nusers can only access the resources for which they have been \nauthorized and prevents compromises from spreading across \nmultiple zones. \n• \n Balanced  controls : To increase flexibility and the ability to \nrecover from a successful attack, the model emphasizes the need \nfor preventative controls but also to balance them with detection \nand response. \n• \n User and data  perimeters : Recognizing that protecting the \nenterprise network boundary is no longer adequate, we need to \ntreat users and data as additional security perimeters and protect \nthem accordingly. This means an increased focus on the endpoint \ndevice and prevention of malicious code, in addition to increasing \nuser awareness and building data protection into the information \nassets. \n I’ll describe each of the four cornerstones in more detail. \n Trust Calculation \n The trust calculation plays an essential role in providing the flexibility required to support \na rapidly expanding number of  devices and usage models . The calculation enables us to \ndynamically adjust users’ levels of access, depending on factors such as the devices and \nnetworks they are currently using. \n It calculates trust in the interaction between the person or device requesting access \n(source) and the information requested (destination). The calculation consists of a source \nscore and a destination score, taking into account the controls available to mitigate risk. \nAs shown in Figure  7-2 , the result of this calculation determines whether the user is \nallowed access and the type of access provided.  \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n107\n Source Score \n Trust in the source, or requestor, is calculated based on the following factors:\n• \n Who : The identity of the user or service requesting access and \nour confidence level in the authentication mechanism used; how \nconfident are we that users are who they say they are? \n• \n What : The device type, its control capabilities, our ability to \nvalidate those controls, and the extent to which IT manages the \ndevice. \n• \n Where : The user’s or service’s location. For example, a user who is \ninside the enterprise network is more trusted than the same user \nconnecting through a public network. There may also be other \nconsiderations, such as the geographical region where the user is \nlocated. \n Figure 7-2.  Trust calculation. Source: Intel Corporation, 2012 \n \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n108\n Destination  Score \n This is calculated based on the same three factors, but these are considered from the \nperspective of the destination (the information the source is trying to access):\n• \n Who : The application that stores the requested data. Some \napplications can enforce greater controls, such as enterprise \nrights management (ERM), and therefore provide a higher level \nof trust. \n• \n What : The sensitivity of the information being requested \nand other considerations, such as our ability to recover it if \ncompromise occurs. \n• \n Where : The security zone in which the data resides. \n Available Controls \n The trust calculation also takes into account the security controls available for the zone. \nIf the only controls available are controls that simply block or allow access, we might \ndeny access due to lack of other options. However, if we have extensive preventative \ncontrols with highly granular levels of access, detailed logs, and highly tuned security \nmonitoring—as well as the ability to recover from or correct problems—then we can \nallow access without creating additional risk. \n Calculating Trust \n The  trust calculation adds the source score and the destination score to arrive at an initial \ntrust level. The available controls are then considered to make a final decision about \nwhether access is allowed and, if so, how. This calculation is performed by a logical entity \ncalled a  policy decision  point (PDP), which is part of the authentication infrastructure and \nmakes access control decisions based on a set of policies. \n Based on the results of this calculation, the PDP makes a decision, allocating a trust \nlevel that determines whether the user can access the requested resource and the type of \naccess that is allowed. Broadly, the decision will fall into one of the following categories:  \n• \n Allow access \n• \n Deny access \n• \n Allow access with limitations or mitigation \n This trust calculation therefore allows us to dynamically apply granular control \nover access to specific resources. For example, employees using IT-managed devices \nwith additional hardware features such as a  trusted platform module (TPM) ,  global \npositioning system (GPS) , and full disk encryption would be allowed access to more \nresources than when using devices that lack those features. \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n109\n Employees directly connected to the network would typically get greater access than \nwhen using a public network. If we are unable to verify the location of a high-security \ndevice such as a managed PC, we would allow less access. \n The trust calculation also can be used for more fine-grained distinctions between \ndifferent device models. For example, we could provide different levels of access based \non manageability, hardware-enabled authentication and encryption, and installed \napplications. \n We anticipate situations in which the trust level is not adequate to allow any access, \nbut there is still a business requirement to allow a connection or transaction to occur. In \nthese conditions, the result of the trust calculation could be a decision to allow access \nwith limitations or with compensating controls that mitigate the risk. For example, a \nuser might be allowed read-only access or might be permitted access only if additional \nmonitoring controls are in place. \n Today, the trust calculation makes decisions based on information gathered from \ncomponents at multiple levels of the infrastructure, such as network gateways, access \npoints, and user devices. Once the trust calculation mechanism is in place, we can extend \nit to include information from a broader range of sources. \n The trust calculation can be used to determine access to internal systems by  business \npartners as well as employees. Let’s say we’re collaborating with another company on \nthe design of a new product. An engineer at that company wants access to a specific \ndocument. We can add a variety of criteria to the trust calculation for deciding whether to \ngrant access. Did the engineer’s request originate within the business partner’s enterprise \nnetwork? Is it consistent with the type of request that we’d expect from an engineer? If so, \nwe have a higher level of trust in the requestor. \n If we cannot establish an adequate level of trust in the user’s device, but other factors \nprovide enough confidence to grant access, we might provide one-time access for a \nspecific job. We could do this by allowing a document to be downloaded, but only within \na container that ensures the document is completely removed from the user’s device once \nthe job is completed. \n Longer term, the trust calculation could become a mechanism that is used to \ndetermine access to both internal and external resources, including cloud-based \n applications . \n Security Zones \n The architecture divides the IT environment into multiple security zones. These range \nfrom untrusted zones that provide access to less valuable data and less important systems \nto trusted zones containing  critical data and resources . \n Because the higher-trust zones contain more valuable assets, they are protected with \na greater depth and range of controls, and we restrict access to fewer types of devices and \n applications , as shown in Figure  7-3 . However, devices allowed access to higher-trust \nzones also have more power; they may be able to perform actions that are not allowed \nwithin lower-trust zones, such as creating or modifying enterprise data. \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n110\n Aligning the infrastructure in this fashion provides an excellent way to right-size \nsecurity controls so that security resources are utilized effectively. It also helps improve \nthe user experience by enabling employees to choose from a wider range of devices, such \nas smartphones, for lower-risk activities. However, all devices should have, at a minimum, \nadvanced endpoint capabilities that prevent more than 99% of malware from executing. \n Access to zones is determined by the results of the trust calculation and is controlled \nby  policy enforcement points ( PEPs ).  PEPs may include a range of controls, including \nfirewalls, application proxies, intrusion detection and prevention systems, authentication \nsystems, and logging systems. \n Communication between zones is tightly restricted, monitored, and controlled. \nWe separate zones by locating them on different physical or virtual LANs; PEPs control \ncommunication between zones. This means that if one zone is compromised, we can \nprevent the problem from spreading to other zones or increase our chances of detection \nif it does spread. In addition, we can use PEP controls, such as application proxies, to \nprovide devices and applications in lower-trust zones with limited, controlled access to \nspecific resources in higher-trust zones when required. \n The architecture includes three primary categories of security zones: untrusted, \nselective, and trusted. Within the zones, there are multiple subzones. \n Untrusted Zones \n These zones host data and services (or the interfaces to them) that can be exposed to \nuntrusted entities. This allows us to provide widespread access to a limited set of resources \nfrom non-managed consumer devices, without increasing the risk to higher-value resources \nlocated in other zones. Untrusted zones might provide access to enterprise resources, such \nas corporate e-mail and calendars, or they might simply provide Internet access. \n These zones are regarded as “shark tanks,” with a high risk of attack and compromise. \nTherefore, detective and corrective controls are needed to mitigate this risk. These \ncontrols might include a high level of monitoring to detect suspect activity and correction \ncapabilities such as dynamic removal of user privilege. \n Figure 7-3.  As the value of an asset increases, the depth and span of controls increase, \nwhile the number of allowed devices, applications, and locations decrease. Source: Intel \nCorporation, 2012 \n \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n111\n We anticipate a need to provide controlled access from these zones to resources \nin higher-trust zones. For example, an employee using an untrusted device might be \nallowed limited, read-only access to customer data located in a trusted zone; or their \ndevice might need access to a directory server in a trusted zone to send e-mail. We expect \nto provide this controlled access using application proxies. These proxies act as secure \nintermediaries, evaluating the request from the device, gathering the information from \nthe resource in a trusted zone, and passing it to the device. \n Selective Zones \n Selective zones provide more protection than untrusted zones. Examples of services in \nthese zones include applications and data accessed by contractors, business partners, \nand employees, using client devices that are managed or otherwise provide a level of \ntrust. Selective zones do not contain critical data or high-value intellectual property. \nSeveral selective subzones provide access to different services or users. \n Trusted Zones \n Trusted zones host critical services, data, and infrastructure. They are highly secured \nand locked down. Examples of services within these zones are administrative access to \ndata center servers and network infrastructure, factory networks and devices, enterprise \nresource planning (ERP) applications, and design engineering systems containing \nintellectual property. Accordingly, we might only allow direct access to these resources \nfrom trusted systems located within the enterprise network, and all access would be \nmonitored closely to detect anomalous behavior. \n Many organizations have implemented secure high-trust zones as part of their \ntransition to an enterprise private cloud. Implementing these zones is a key step in \nallowing several types of applications to be moved onto virtualized cloud infrastructure, \nincluding applications requiring high security. The security features in these trusted \nzones include application hardening and increased monitoring. \n \nNEW SECURITY ARCHITECTURE IN ACTION: A DAY IN \nTHE LIFE OF AN EMPLOYEE\n This example (illustrated in Figure  7-4 ) describes how the new security architecture \ncould enable an organization’s sales force to access the information they need in the \ncourse of a day. At the same time, the architecture protects security by dynamically \nadjusting the level of access provided, based on the user’s device, its location, and its \ncapabilities for preventing malicious code, and by monitoring for anomalous behavior. \n The employee travels to a customer site. The employee is using a personal \nsmartphone with limited security features and so is allowed access only to services \nin untrusted zones. From here, the employee can view limited customer information, \nincluding recent orders, extracted from an  enterprise resource planning (ERP) \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n112\nsystem in a trusted zone—but only through an application proxy server, which \nprotects the trusted zone by acting as an intermediary, evaluating information \nrequests, accessing the ERP system, and relaying the information to the user. \n If a smartphone requests an abnormally large number of customer records—an \nindication that it may have been stolen—further access from the smartphone is \nblocked. To help understand the reason for the anomalous access, there is increased \nmonitoring of the employee’s attempts to access the system from any device. \n The employee reaches the customer site and logs into the enterprise network \nfrom a company-owned mobile business PC. Because this device is more trusted, \nthe employee now has access to additional capabilities available in selective \nzones, such as the ability to view pricing and create orders that are relayed by an \napplication proxy to the ERP system in a trusted zone. \n The employee returns to the company’s office and connects to the corporate \nnetwork. Now the employee is using a trusted device from a trusted location and \nhas direct access to the ERP system in a trusted zone. \n Figure 7-4.  The new security architecture dynamically adjusts the user’s access to \ninformation, based on factors such as the  user’s device and location . Source: Intel \nCorporation, 2012 \n \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n113\n Balanced Controls \n Over the past decade, enterprise security has focused heavily on controls such as \nfirewalls, signature based antivirus, and intrusion detection systems such as behavior-\nbased anomaly detection tools. As we have seen so often in the past few years, this \napproach is not working. At many companies, the default belief is now that prevention is \nnot possible and we can only correct problems after they have occurred. \n However, the new security model requires that we understand the implications of \nthe 9 Box. Preventative controls should not only stop malicious code from executing \nbut do so in a way that lowers our overall cost of controls and with low friction. More \neffective prevention will reduce the alert fatigue within due to the “whack-a–mole” effect \nassociated with over-reliance on detective (monitoring) and response controls. Detection \ncapabilities will also be more effective because effective prevention reduces the “noise” in \nthe environment. Over the long term, this approach will free up resources that can then \nbe applied to other corrective controls. \n By using the 9 Box to guide the control philosophy, and demanding solutions that \ncontinually shift down and to the left (reducing risk, cost, and control friction), we will be \nable to change the risk dynamics in the industry. \n \nUSING  SECURITY ANALYTICS TO DETECT SUSPICIOUS \nBEHAVIOR\n Almost all organizations have experienced security issues involving both \nexternal attackers and insiders, including attempts to steal intellectual property. \nInvestigations have identified markers and indicators that are frequently associated \nwith these events. If we can spot these indicators sooner, we can respond and \nmitigate the threats more quickly. \n Security analytics technology can be used to detect suspicious behavior as \nthe environment becomes more complex and attackers become more adept at \nconcealing compromises. The technology automates the process of analyzing large \nvolumes of data to detect and monitor anomalous activity, allowing companies to \ndetect problems that they might otherwise miss. These capabilities are similar to \nthose already implemented by financial institutions to prevent fraudulent credit-card \ntransactions, and by online consumer services to prevent theft of user data.  \n On a large scale, logging data generated by servers and sensors across the network \ncan be collected into a database for analysis. Security business intelligence can also \nbe applied at the level of individual users and devices, as long as we are careful to \nprotect users’ privacy. \n The balance between preventative, detective, and corrective controls will vary, \ndepending on the security zone. In high-trust zones, we implement extensive monitoring \nto detect possible attempts to steal data or compromise critical systems. Redundancy \nwithin each type of control can be used to provide additional protection. \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n114\n The following includes possible examples of using detective and  preventative \ncontrols:\n• \n An employee attempts to send a confidential document to an \nexternal e-mail address. Monitoring software detects the attempt, \nprevents the document from being sent outside the firewall, and \nasks the employee if he or she really intended to do this. If the \nemployee confirms that this was intended, the document may \nbe transmitted, or if the document is highly sensitive, a redacted \nversion may be sent. \n• \n Inappropriate use of a document protected with enterprise rights \nmanagement technology results in revocation of access to the \ndocument. \n• \n The system allows access to specific documents but tracks the \nactivity. A user can download a few documents without causing \nconcerns. However, if the user attempts to download hundreds \nof documents, the system slows down the speed of delivery (for \ninstance, only allowing ten to be checked out at a time) and alerts \nthe user’s manager. If the manager approves, the user is given \nfaster access.  \n• \n The detection of an infected system places the system on a \nremediation network, isolating the system and restricting access \nto enterprise information and applications. The system may \nretain some ability to access corporate assets, but all activity is \nclosely logged to enable incident response if necessary. \n• \n When a system is found to be compromised, we examine all its \nrecent activities and interactions with other systems. Additional \nmonitoring of those systems is automatically enabled. \n \nUSING MACHINE LEARNING TO IMPROVE  ANTI-MALWARE \nTECHNOLOGY \n Traditional antivirus software relies on recognizing characteristic signatures to \nidentify specific malware families. But today, adversaries have access to off-the-\nshelf malware toolkits that make it easy to create custom malware variants that \nsignature-based antivirus products cannot recognize. This custom malware sails \npast traditional antivirus products as if they didn’t exist. \n Machine learning technology provides a solution to the problem. Rather than relying \non humans to identify malware signatures, machine learning technology can \nautomatically analyze and classify hundreds of thousands of characteristics per file, \nbreaking them down to an atomic level to discern whether an object is “good” or \n“bad” in real time. \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n115\n The process works like this. A machine learning platform continuously collects \nvast amounts of data from many sources. It analyzes the data and extracts \nDNA-level features that the machine learning platform itself determines are \nunique characteristics of good and bad files. Most of these characteristics are \nso microscopic that human malware researchers and reverse engineers don’t \nunderstand their importance. The software constantly adjusts to the real-time \nthreatscape, thus learning to make higher-fidelity decisions. For each file, the \nplatform assigns a threat score that is used to automate policy-based protection \ndecisions—ignore, alert, block, or terminate file/process execution. A mathematical \nmodel encapsulating the platform’s intelligence is then periodically extracted and \nincorporated into an anti-malware solution that is installed on endpoints. Using this \nsolution, it’s possible to stop more than 99% of malware before execution.  \n Users, Data, and the Internet of Things: The New \nPerimeters \n The concept of balanced controls also extends to the protection of users and data. \n Traditional network security boundaries are dissolving with the proliferation of new \ndevices and users’ expectations that they should be able to access information from \nanywhere at any time. Users are under direct assault from a barrage of attacks designed to \ntrick them into taking actions that can compromise the information on their devices or on \nenterprise systems. These trends mean that we need to think more broadly about how we \nprotect information, as well as the users of this information. \n While we continue to implement enterprise network controls, such as perimeter \n defenses and the detective controls described earlier, we need to supplement these \ncontrols with a focus on the users and on the primary assets we are trying to protect such \nas intellectual property. The new architecture therefore expands our defenses to two \nadditional perimeters: the data itself and the users who have access to the data. \n Data Perimeter \n Important data should be protected at all times: when it is created, stored, and \ntransmitted. This becomes increasingly challenging as we move data to more and more \ndevices and let more people access it. How do we  protect information when it’s located \noutside the physical perimeter on a personal device? \n One approach is to use technologies that closely integrate protection with high-value \ndata so that the data remains protected as it moves to different devices and locations. \nTechnologies such as enterprise rights management and data leak prevention can be \nused to watermark and tag information so that we can track and manage its use. With \nenterprise rights management, the creator of a document can define exactly who has \naccess rights throughout the life of the document and can revoke access at any point. \nData loss prevention is used to tag documents, track their movements, and prevent \ntransfer outside the organization if necessary. \n\n\nCHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n116\n User Perimeter \n As I described in Chapter  5 , people are part of the security perimeter, and we need to treat \nthem as such. Users can become security risks for a variety of reasons. They are targeted \nmore frequently in social engineering attacks, and they are more vulnerable to these \nattacks because their personal information is often readily available on social networking \nsites. They may also click malicious links in e-mail, download malware, or store data \non portable devices that then are lost. A combination of training, incentives, and other \nactivities can help instill information  security and privacy protection into the corporate \nculture and successfully encourages employees to own responsibility for protecting \nenterprise and personal information. \n Internet of Things \n The  Internet of Things can be viewed as an extension of the user and data perimeters into \nnew connected devices and systems such as cars, wearables, and smart buildings. IoT \ndevices should be included in the security architecture; for example, the trust calculation \ncould be applied to access from IoT devices, so that the security of the device is a factor \nin determining the level of access provided. For machine-to-machine communications, \neach communicating machine can be considered conceptually as analogous to a user; \nthe security architecture focuses on preventing, detecting, and responding to behavior \nthat it identifies as anomalous. \n Conclusion \n This chapter describes a new control architecture designed to support the Protect to \nEnable mission. With this approach we can lower risks, lower costs, and lower control \nfriction. It will also allow for faster adoption of new services and capabilities because it \nhelps prevent risk and improve survivability. I believe that this architecture can be used \nto meet a broad range of evolving requirements, including new usage models and threats. \nThe architecture’s flexibility and granular trust model should also make it easier for the \nsecurity team to identify and contain anomalous activity that signals potential insider \nthreats. By publishing information about the architecture , I hope to encourage others \nto take advantage of these ideas. I also hope that making this information available will \nstimulate more discussion and ideas, and that others will build on these concepts to \ncreate further innovations that benefit all of us. \n",
      "page_number": 124
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 139-147)",
      "start_page": 139,
      "end_page": 147,
      "detection_method": "topic_boundary",
      "content": "117\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_8\n CHAPTER 8 \n Looking to the Future: \nEmerging Security \nCapabilities \n Learn from yesterday, live for today, hope for tomorrow. The important \nthing is not to stop questioning. \n —Albert Einstein \n The Web has existed for two decades, yet it’s only in the last few years that we’ve gained a \nclearer picture of what the Internet may become, and how the emerging capabilities may \nshape the future. \n As early as 1993, companies like AOL started offering access to online newsgroups, \nsoon followed by dial-up Internet access using early web browsers. As laptops became \nmore affordable, many people started accessing the Internet while on the move. The \nrise of smartphones introduced built-in sensors, such as cameras, global positioning \nsystem receivers, and touch-sensitive screens, into consumers’ everyday computing \nexperiences. Businesses began using the information gathered from users’ devices to offer \npersonalized experiences, ranging from location-based driving directions to selected \nadvertisements. The variety of Internet-connected devices rapidly expanded to include \ntablets, home DVRs, appliances, and cars. Devices also became smarter, with improved \nvoice and gesture recognition. \n We’re now entering a world in which these elements will be combined to create \nmuch richer context-aware experiences for users and new opportunities for businesses. \nOur devices will know us, and they will know other devices. In fact, devices may almost \nbecome part of us: many companies are already shipping wearable computers, including \nsmart athletic garments that work with smartphone apps to monitor your biometrics and \nsuggest ways to improve your performance. \n\n\nCHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n118\n Each day, billions of computing devices will perform functions on our behalf, often \ncommunicating among themselves to get the job done. Much more information will be \ncollected from sensors such as cameras, microphones, and GPS receivers embedded into \nthe user devices. This data will be combined with other information to create context-\naware experiences that are far more personalized and compelling. Already, cameras and \nimage recognition technology, combined with behind-the-scenes analytical software, \ncan be used to identify a user’s age bracket and gender, and tailor their experience \naccordingly. Early applications based on this technology are being piloted and in some \ncases deployed by large companies, including retailers (see sidebar). \n Estimates of the projected size of the context-aware computing market continue \nto grow. When the first edition of this book was printed, Gartner, Inc. (2011) expected \ncontext-aware technologies to create huge business opportunities affecting an estimated \n$96 billion in annual consumer spending worldwide by 2015. In 2013, forecasts suggested \nthe market would reach $120 billion by 2018 (MarketsandMarkets 2013). And a report in \nlate 2015 forecast the market will swell to $185 billion by 2020 (Global Industry Analysts \n2015). During this period, it’s expected that a significant percentage of all payment \ntransactions will be validated using contextual information. \n \nRICHER EXPERIENCES IN THE  RETAIL ENVIRONMENT \n As people buy more goods online, retailers are seeking to entice shoppers into brick-\nand-mortar stores by using technology to create richer, context-aware experiences. \n Macy’s and some other big-name stores are already using beacons, which detect \nthe smartphones of nearby shoppers and, if they have opted in, send them targeted \noffers or mobile games with gift-card prizes (Tode 2015). Brands including Kate \nSpade and Levi’s use smart display tables and shelves that sense when customers \npick up a product and engage them with relevant videos and product information. \nThe technology tracks every interaction, so stores can analyze shopper behavior and \nmeasure the impact on sales (Perch Interactive 2016). LEGO stores use augmented \nreality video screens to show kids what they can build with each LEGO box. The \nscreens recognize each box and display a 3D image of a toy that can be created \nfrom it, blended into a real-time video of the child in the store. Canadian sports \nretailer Sport Chek’s flagship stores integrate hundreds of screens in displays up \nto 16 feet tall, using gesture, touch, and RFID to sense customer input and display \ncustomized interactive content. \n As an Advertising Age column noted, technology may ultimately help transform the \nphysical store into a venue for interactive experiences that increase brand affinity—\nacting as an event space, gallery, help desk, or even a test kitchen. If that happens, \nonline sales may work in tandem with, rather than as a substitute for, a physical \nstore (Fulford 2015). \n\n\nCHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n119\n These new technologies also introduce new risks, as I described in the discussion \nof emerging threats and vulnerabilities in Chapter  6 . The sensors and other new \ncapabilities embedded into millions of intelligent new devices can be exploited for \ndangerous  purposes . Malicious individuals might be able to remotely access home \nsecurity surveillance systems to determine when you’re not at home. Researchers have \nalready demonstrated the ability to remotely control the brakes and other functions of \nan  Internet-enabled car . After remotely hijacking a Jeep Cherokee driven by a reporter, \nresearcher Charlie Miller commented, “Right now I could do that to every [Chrysler] \ncar in the United States on the Sprint network (Pagliery 2015).” The hack prompted Fiat \nChrysler to recall 1.4 million vehicles to fix the issue (Greenberg 2015b). \n As  security professionals , we may tend to focus obsessively on this darker side of \nthe picture. Looking for threats and vulnerabilities is part of our role. We’ve seen that \nattackers find ways to exploit new technologies almost as soon as they appear. Analysis \nof emerging threats by many firms indicates that this trend will continue. As attackers \nadapt, we must adapt, too. Our role will be more important than ever. As more aspects \nof people’s daily lives are based on technology, it will become increasingly important to \nsecure the technology. The Protect to Enable mission will expand accordingly; in fact, it is \nbecoming a corporate social responsibility, as I will explain further in Chapter  9 . \n The positive news is that new technologies can also be used to enhance security. \nAs information risk becomes an even more high-profile concern, suppliers are building \nmore security into their products and services. Devices will include a greater level of \nbaseline security hardening to reduce the likelihood of compromise and minimize the \nimpact. \n Context-aware computing also introduces new privacy concerns. By definition, \ncontext-awareness involves taking advantage of information about the user to create \npersonalized experiences. This makes it even more important to appropriately protect \nusers’ information and privacy. A clear organizational commitment to privacy will be \nimportant to ensure this protection. A growing number of other organizations have \nformally committed to complying with a single set of privacy principles worldwide—\nalthough this is becoming difficult due to the proliferation of localized privacy laws and \nthe elimination of the EU safe harbor agreement (see Chapter  1 ). \n An  organization’s privacy commitment must also extend to applications and \nsystems. Suppliers are becoming increasingly aware of this, and some are already taking \nadditional steps to ensure user data is collected anonymously. The new baseline security \ncapabilities built into products, such as  hardware-enforced protection and  accelerated \nencryption , may also help enhance privacy by protecting user data. In addition, the \ninformation provided by sensors can be used to create  context-aware security . Today, \nsome cars can automatically adjust seat, mirror, and pedal positions to suit different \ndrivers. They adjust these settings when they detect the presence of the driver’s personal \ncar key. In the future, as cars become more intelligent and include more sensors, they \nmight identify the driver using a camera and microphone. If they don’t recognize the \ndriver, they might disable the car and alert the owner via their built-in wireless Internet \nconnection. Cars might include a maintenance mode that lets mechanics drive it while \nwhen it’s being serviced, but only within a radius of a few miles. Similarly, as I’ll discuss \nlater in this chapter, the sensors in an enterprise-class device, such as a business laptop \nPC, could be used to prevent theft and help protect the information it contains. \n\n\nCHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n120\n From the perspective of the  enterprise information security team, these emerging \ncapabilities will allow increased trust in users and their devices. When we have a \nhigher level of trust, we can provide the user with greater access to sensitive enterprise \ninformation and other resources. \n I believe that this dynamic evaluation of trust is a key capability that new security \narchitectures should include, as I discussed in Chapter  7 . Employees may want to access \nour systems from a variety of devices and locations, including personal smartphones and \ntablets as well as business PCs. When a user requests access to  enterprise systems , the \narchitecture should dynamically calculate trust based on contextual information such as \nthe user’s identity, the security features of the device they’re using, their physical location, \nand the resources they’re trying to access. The architecture then will decide whether to \ngrant access and the level of access that should be allowed. As manufacturers increase the \nsecurity capabilities in their devices, the model will be able to take this into account. We’ll \nhave increased trust in a device, and we’ll be able to provide a correspondingly greater \nlevel of access. \n In this chapter, I’ll take a closer look at some of the emerging security capabilities \nthat we can expect in products and services. First, though, I’d like to set the stage by \nexamining some of the key underlying trends that make these security capabilities both \nnecessary and possible. \n Internet of  Things \n Many everyday objects are becoming more intelligent. They’re acquiring processors, \nsensors, software, and the ability to communicate. This trend is made possible by  Moore’s \nLaw : processors and other hardware components continually become faster and less \nexpensive, and, therefore, ubiquitous as a result. This accelerating trend is creating the \nInternet of Things, a massive expansion of the Internet as it swells to include billions of \ndevices and household objects. Intelligent devices in cars, home electronics, and other \n“things” will far outnumber those in more conventional computing platforms and even \nthose in mobile devices such as smartphones. Gartner, Inc. estimates that during 2016, \n5.5 million new “things” will be connected every day. Juniper Research expects 38.5 \nbillion connected devices by 2020 (Loechner 2015); Cisco expects an even higher number \nof 50 billion (Cisco Systems 2015b). \n Gartner, Inc. (2011b) identifies several key technologies and capabilities contributing \nto this trend, including sensors, image recognition, and wireless payments using  near \nfield communications ( NFC ) technology. Sensors that detect and communicate changes \nin their environment are being embedded not just in mobile devices, but in an increasing \nnumber of places and objects. Emerging applications will take advantage of this \ninformation. For example, camera-based image recognition technologies are expanding \nfrom mainly industrial applications to broad consumer and enterprise uses. These \nsystems gather information about users and then analyze this information to personalize \nthe user experience.  Wireless NFC , based on a communications standard analogous to \nthe Radio Frequency Identification ( RFID )  technology used for product-tracking, lets \nusers make payments by waving a mobile phone or smartwatch in front of a compatible \nreader. \n\n\nCHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n121\n With technologies such as NFC, the concept of the Internet may broaden to include \nan even wider variety of “dumb” objects, like drink cans or fertilizer bags (Gartner 2011b). \nThis trend will provide opportunities for innovations that were not previously possible. \nToday, items in stores may include 2D bar codes that can be read by smartphones. In \nthe future, store items may include NFC on the packaging or shelf label allowing them to \nwirelessly identify themselves to nearby devices, such as a  shopper’s smartphone . The \nshopper will then be able to learn not only about the product, but also alternatives, and \ncould even view cross-selling and up-selling suggestions. \n Devices such as the  Nest Learning Thermostat have provided a glimpse of the future. \nThis home heating controller is designed to be intuitive and simple to operate, replacing \ncomplex menus and instructions with a single big button and a dial. Users can remotely \nmonitor and set the temperature from their smartphones, so they know the house will be \nwarm by the time they get home. But perhaps the most interesting capability is that, as \nits name suggests, it can learn. The Nest monitors use of the heating system and attempts \nto learn the user’s preferences—when the heating is switched on and off, and the desired \ntemperature. After studying the use patterns for a while, the Nest begins to predict and \nautonomously set the temperature and timing itself. Since Nest launched many other \ncompanies have followed suit with similar devices not only for home heating but also for \nother sensors and alarms, including water sensors, motion sensors, and do-it-yourself \ninternet-based home security systems. \n I believe that devices like this are early examples of a much larger trend. As \nthe Internet of Things grows, more interactions will occur directly between devices, \nrather than between people and device. Devices and objects will interpret and act \non information provided by other objects. This will enable much more intuitive and \nstreamlined experiences in many different fields. Consider the following scenario, \ndescribed by Plantronics CTO Joe Burton (2012). A doctor visits a patient in a hospital \nroom. A smart device the doctor is wearing turns on the doctor’s workstation in the room, \nthen authenticates the doctor to the patient management system, detects which patient \nis near the doctor, and pulls up the patient’s record. When the doctor leaves the room, the \ninformation accumulated during the visit is saved and the workstation powers down. \n Consistent User Experience Across Devices \n Users now demand the same quality of experience in the workplace that they’ve become \naccustomed to in their personal lives. This includes the ability to access information \nacross a continuum of devices, including PCs, smartphones, and tablets. They expect \nto be able to move from one device to another. They also expect intuitive applications \non all of these devices, with the application’s features tailored to the device’s size and \ncapabilities. \n IT therefore needs to provide users with a  consistent experience across devices and \nthe ability to seamlessly transition between them. As enterprise information security \nprofessionals, we need to focus on the user experience and on enabling this broader \nrange of devices while managing the risks. \n\n\nCHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n122\n Cloud Computing \n The cloud is as much a new business model as it is a technology shift. The ability to \nobtain flexible IT services on demand lets businesses operate more dynamically—quickly \ntaking advantage of business opportunities and growing or shrinking infrastructure \ncapacity to meet demand. Cloud services can also potentially reduce cost. \n However, cloud computing can also add new security complexities and data-\nprotection concerns. Organizations may use multiple cloud providers, while also \noperating a private cloud for the most sensitive applications. Users need to be able to \neasily access services delivered from any of these multiple environments. From the \nenterprise perspective, we need to enable a seamless user experience while minimizing \nrisk. This implies a federated model in which the user needs to log in only once; the \nuser’s credentials can then be used to access multiple applications. However, this also \nmeans that an attacker may only need to gain access once in order to compromise several \nenvironments. \n Big  Data Analytics \n Businesses have quickly realized the value of analytical tools for real-time analysis of \nmassive amounts of unstructured data. In the future, these analytic capabilities will \nincreasingly be used to interpret data from sensors as well as from databases, social \nmedia, and other sources. The analysis of this information will then be used to create new \npersonalized experiences, like the retail examples discussed in the “Richer Experiences in \nthe Retail Environment” sidebar. \n This analysis can also  be  integrated with existing enterprise systems to create \nsophisticated customer-focused services. Here’s a scenario described by Accenture \n(2012): a rental car company automatically detects when an accident with one of its cars \nhas happened, initiates emergency services if needed, and issues a replacement rental \ncar to meet the renter at the scene, greatly improving the chances of creating a loyal \ncustomer for life. \n Artificial Intelligence \n Artificial intelligence is rapidly maturing, and it’s now clear that AI will help all of us in \na variety of ways, both in business and our personal lives. AI is already used to identify \nmeaningful patterns in data for many purposes, including information security, and \nto understand and translate speech. AI will certainly play a role in self-driving cars. \nOver time, AI will become capable of taking on broader and greater responsibilities. \nAs Alphabet Inc. executives Eric Schmidt and Jared Cohen put it: “Eventually it will \nbe possible to give a computer unstructured data—say, spreadsheets used to manage \nbusiness records—and receive quality advice on improving operations.” (Schmidt and \nCohen 2015) In our personal lives, perhaps we’ll have a helper like Jibo, a “social robot” \nthat recognizes your face, converses with you, helps manage your calendar and basic \ntasks, and learns your preferences so it can adapt and help you better. \n\n\nCHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n123\n Business Benefits and Risks \n By now, it should be apparent that the richer experiences enabled by these capabilities \nare as important to businesses as they are to users. New,  context-aware experiences may \nattract customers and create new revenue. Furthermore, focusing on the user experience \nmay be essential for business survival. If we don’t provide rich and appealing user \nexperiences, customers may gravitate toward competitors that do. \n Our challenge is to manage the risks associated with these new experiences. The \ngood news is that new security capabilities are emerging to help us do so. \n New Security Capabilities \n The IT ecosystem is increasingly focusing on  building security into hardware, software, \nand services. We’ll all be able to take advantage of this security to protect users and the \nenterprise. I think of these capabilities as the equivalent of termite-resistant building \nmaterials used in construction. They may not prevent termite attacks altogether, but \nthey can stop some of them and minimize the impact of others. For example, Dell is \nusing technology from Cylance to protect the BIOS firmware in its business PCs. The \ntechnology is designed to check if systems are secure when users boot them up; after the \nPC boots, the software checks a hash of the BIOS against a known good version stored in \na secure cloud. \n Suppliers will need to frequently enhance these defenses to ensure they remain \neffective. As I noted in Irrefutable Law #6 in Chapter  1 , security controls operate in a \ndynamic environment in which attackers are constantly learning and adapting their \napproach. Unless the defenses also adapt, they will lose their effectiveness over time. \n I expect the ecosystem will increasingly view these security features as a way \nto differentiate products to meet the needs of distinct categories of customers. As a \nparallel, think about how the auto and other consumer industries developed. Initially, \nmanufacturers focused on getting the public to buy cars en masse. Accordingly, the focus \nwas on mass-producing just a few models at the lowest cost. As Henry Ford famously \nsaid, \"Any customer can have a car painted any color that he wants so long as it is black\" \n(Ford and Crowther 1922). Ford’s  mass-production strategy was enormously successful \nin popularizing cars among the American public. By 1918, half of all cars in the United \nStates were Model Ts (The Henry Ford Museum 2003). But once consumers became \nmore familiar with cars, they started demanding models that met specific needs. As \nmanufacturers responded, the industry began to develop the huge variety of models that \nwe see today. \n In the same way, suppliers will offer a range of products or services with differing \nlevels of security, including higher-security versions for the most sensitive enterprise uses \nand less-secure versions for consumers. This trend has already been evident for some \ntime in products such as servers and PCs, and we’re beginning to see it in cloud services. \n In a closely connected trend, we’ll see increasing use of contextual information to \nimprove security. Some of this context will be provided by the sensors built into devices, \nsuch as cameras and GPS receivers. In addition, analytical and monitoring tools will be \nable to gather valuable  contextual information from the environment. For example, they \nmay examine databases containing information about users’ access history and other \nrelevant data. \n\n\nCHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n124\n Baseline Security \n A greater level of baseline,  hardware-enforced security features will be important in all \ncategories of devices, from smartphones to full-featured PCs. These capabilities will \nprotect the information on the device itself, and the information that is accessed from the \ndevice. They’ll enable greater trust in the device, and because of this trust we’ll be able \nto provide users of the device with access to more resources, as I described in Chapter  7 . \nThe potential business benefits include increased user satisfaction and productivity. \n I believe that these features will become particularly valuable as the Internet of \nThings takes shape. Many new, connected devices and objects won’t be powerful enough \nto run traditional software security controls. Do I expect the computers that control my \ncar or my home to run full intrusion prevention systems or traditional antivirus suites? \nNo, but it is possible to run lightweight AI-based agents that can determine good from \nbad in milliseconds. This capability has already been demonstrated: in the summer \nof 2015, Cylance showed its AI-based anti-malware agent running on a Raspberry Pi \nplatform, which is based on the ARM processors that are in many appliances and other \nIoT devices (Bradley 2015). I also believe that many of these new devices should include \nprotection that limits their functions to the desired purpose, reducing the risk that they \ncould be successfully attacked and manipulated via the Internet or a wireless network. \n For enterprise security, these baseline hardware security capabilities will provide \nhelp in key focus areas, including threat management, ID and access management, \ndata protection, and remote monitoring. Some expected baseline capabilities include \nprotected environments, encryption, hardware acceleration, enhanced recovery, and \nintegration with security software, as described next. \n Protected Environments \n Increasingly, hardware will provide protection for essential functions and data in the form \nof trusted layers and execution environments. I think of this approach as analogous, at the \nhardware level, to the way organizations are implementing network security zones within an \nenterprise environment (as described in Chapter  7 ). The most valuable and critical functions \nreceive the greatest protection, as well as increased monitoring and recovery capabilities. \n Attackers have become increasingly adept at compromises using tools, such as \nrootkits, that operate at or below the operating system level, making them harder to detect \nand prevent by most traditional security applications. Implementing protection at the \nhardware level can help prevent compromise of firmware, operating systems, hypervisors, \nand other fundamental system components. Hardware-level protection can also help \nalert security professionals to attempted attacks and aid in system recovery . However, \nhardware-level protection must be designed, developed, and implemented correctly or \nit could actually do more harm than good, because compromise at this level can give \nattackers wide-ranging access to the software and data on the system. Concerns have \nalready begun to surface and are growing. Researchers demonstrated the ability to hack \nthe microcontroller inside flash cards, enabling the execution of code that can be used to \nperform a man–in-the-middle attack (Paganini 2014). Networking equipment supplier \nJuniper Networks found that its firewall operating system contained “unauthorized \ncode” that surreptitiously decrypted virtual private network traffic (Goodin 2015). MIT \nresearchers suggested there are weaknesses in the implementation of key provisioning \nfor Intel Software Guard Extensions (SGX), a set of hardware instructions designed to \nimprove security by sealing software into hardware-protected enclaves (Chirgwin 2016). \n\n\nCHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n125\n Encryption \n Many organizations already use disk encryption to protect data against loss or theft. But \nin a world where devices are always on and always connected, traditional software-based \nhard disk encryption is not sufficient. New capabilities will make encryption an even more \npervasive technology used to protect information throughout its life, both when it is stored \nand when it is transmitted. Devices will include self-encrypting drives that maximize \nprotection while minimizing the performance impact; encrypted input-output will help \nprotect data during communications. Capabilities that currently exist in larger systems, \nsuch as total memory encryption, will become common in PCs and other end-user devices. \n Hardware Acceleration \n There’s often a trade-off between security and performance. Controls, such as software-\nbased encryption and malware scans, certainly help increase protection, but the \nperformance impact can also increase frustration for users, to such an extent that some \nmay avoid using the security features altogether (see the discussion of control friction \nand the 9 Box of Controls in Chapter  7 ). Accelerating functions in hardware can shift the \nbalance in favor of security by decreasing the impact, both on users and on enterprise \nsystems. For example, complex calculations required by standard encryption algorithms \ncan be accelerated using hardware instructions rather than executed entirely in software. \n Enhanced Recovery \n As I’ve discussed in previous chapters, we must assume that attempts to compromise \nare inevitable, despite our best efforts. As attacks become increasingly sophisticated, the \nability to recover from compromises will become even more important. Future capabilities \nwill help organizations recover from low-level attacks that target fundamental system \ncomponents such as firmware or the BIOS. The system will be able to detect changes in \nthese components, whether due to malicious attacks or accidental corruption. It will then \nbe able to take steps to restore the components to a known good state, alerting users and \nthe security team when necessary. Other anticipated recovery features include enhanced \ncapabilities to revoke cryptographic keys to reduce the spread and impact of compromise . \n AI-Based Security and Automation \n AI-based security applications will play valuable roles in preventing attacks. Today, for \nexample, Cylance uses AI-based agents to distinguish good from bad in milliseconds. \nThese applications will be able to provide an even greater level of protection when they are \nintegrated with hardware-based security, as exemplified by the Dell-Cylance BIOS protection \nagreement described earlier in this chapter. This kind of integration will enable software to \nmore closely monitor the underlying hardware and firmware for attacks that might otherwise \ngo undetected. For example, security software could use hardware features to detect \nsymptoms, such as memory state changes, caused by specific types of attack. Companies \nare also researching better ways to authenticate users by employing behavioral biometrics: \nidentifying users based on a combination of hard-to-duplicate characteristics such as they \nway they swipe characters on a smartphone or even how they walk when carrying the device. \n",
      "page_number": 139
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 148-155)",
      "start_page": 148,
      "end_page": 155,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n126\n AI will be used more broadly over time to enable a greater level of automation in \nthreat detection, prevention, and response. In the future, AI might be used to dynamically \nevaluate trust and the corresponding level of access that’s provided to a user (see the \ngranular trust model in Chapter  7 ). \n Context-Aware Security \n The theme of context awareness underlies many of the rich user experiences described \nin this chapter. Context awareness can also enhance security: the same  sensors and \nanalytical tools that help organizations create personalized experiences can also be used \nto mitigate risk. \n In the home, TVs might be able to recognize when a child is watching, and show \nonly appropriate channels. In supermarkets, cameras that are already used for physical \nsecurity could help increase the efficiency of automated checkout stations. As I described, \n image recognition technology can determine a shopper’s approximate age. By using this \ninformation, perhaps in conjunction with data from a scanned driver’s license, the system \ncould help avoid the need for cashiers to manually approve alcohol sales, leading to faster \ncheckouts for consumers and reduced costs for stores. \n The sensors in  portable devices , such as mobile PCs and smartphones, may also be \nused to help protect against theft and unauthorized use. A simple case might utilize the \ndevice’s camera, microphone, and GPS receiver to help authenticate you as the device’s \nowner. If the user looks and sounds like you, and the PC is at your house, we have more \nconfidence that the person using it is really the owner. \n Additional technologies in portable devices, such as NFC, will allow more \nsophisticated examples of context-aware security. Devices will know when they’re no \nlonger in proximity of their owner, and may enter a protected state to prevent data loss. If \nyour phone is near your laptop, we have greater confidence that you are the user trying to \naccess the information on the laptop. When your phone moves away, the laptop deduces \nthat you have moved away, too, and begins to armor itself by locking the screen. As you \nmove progressively farther away, the laptop first goes into standby to save power, and \nthen begins encrypting its contents for protection. \n The GPS receiver in a portable device can also be used to geofence the device and \nthe data it contains. If the receiver detects that a PC has moved outside a specific area, \nthe device could alert the owner and the enterprise support team. The same capabilities \ncould help protect data whose movement is restricted by specific geography-related \nrequirements such as export controls. The device could detect when it’s in a country \nsubject to these controls, and encrypt the data it contains to protect it. \n Cloud Security and Context Awareness \n Cloud service providers recognize that some organizations are still reluctant to move \ncritical data to external clouds due to security, regulatory, and privacy concerns. \nSuppliers have been working to add security capabilities designed to address these \nconcerns. As they do so, we can expect more cloud services that are differentiated based \non the level of trust they offer. \n\n\nCHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n127\n Suppliers might offer a “plain vanilla” cloud service for noncritical applications, \nalong with a more expensive high-trust cloud service. Besides offering additional \ntechnical controls, secure clouds might include guarantees that the supplier will meet \nspecific privacy and other data-protection regulatory requirements. This tiered strategy \nresembles the zoned approach to network security that organizations are implementing \nas part of their evolving security architecture. Zones that host critical applications are \nprotected by a variety of controls, ranging from network segmentation and hardened \nvirtualization host servers to additional monitoring. \n In the future, client-aware cloud services will be able to tailor the access they provide \nbased on the security capabilities of the client in order to mitigate risk. A fully managed \ndevice that includes hardware-based enterprise security features and a full software \nsecurity suite may get more access than an unsecured personal device. At the same \ntime, a cloud-aware client will be able to validate that the cloud service it is accessing is \ngenuine, and that it offers the required level of security. \n As businesses use a growing number of cloud services, security requirements \nbecome more complex. A single enterprise may use multiple external cloud services \nwhile also operating a private cloud and a traditional computing environment. It will be \nimportant to streamline access for users. We can expect more emphasis on technology \nthat eliminates the need for users to authenticate to each individual service. \n Security Analytics and Data  Protection \n Security context can be provided not only by sensors, but also by analyzing information \nabout the enterprise environment and the threat landscape. As attackers become \nstealthier, this analysis will become an increasingly important part of an organization’s \ndefenses. Within the industry, many are moving toward the use of security analytics \ntools to analyze patterns of network traffic and system use. I expect to see increasingly \nsophisticated external services that analyze a broad range of information in order to \nthwart attacks. \n As information is used on more devices outside the enterprise network perimeter, it \nwill also be increasingly important to focus on controls that are integrated with the data \nitself. Many organizations are already protecting information with technologies such as \nenterprise rights management. In the future, these capabilities are likely to become more \nsophisticated and automated, allowing businesses to define policies that automatically \nstore sensitive data in highly secured locations. \n Conclusion \n New technologies bring challenges, but they also bring opportunities for the CISO and for \nthe organization overall. \n The rich context-aware experiences that I’ve described in this chapter are entirely \ndependent on IT. To deliver these experiences, organizations will need to understand \nand manage the risks. As the experts in information risk, CISOs and other security \nprofessionals should have opportunities to become closely involved in the development \nand implementation of key business initiatives. This will result in a higher profile for the \ninformation risk and security team across the entire organization. \n\n\nCHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n128\n To fully take advantage of these opportunities, CISOs will need broad business and \npeople skills as well as a thorough knowledge of security controls. With the addition of \nthese skills, I believe the role will evolve into the chief security and trust officer (CSTO), \nwith broad responsibilities to enable the business through trusted infrastructure, \napplications, and business processes. As this transition occurs, the CSTO becomes the \nessential enterprise architect, with the IT organization becoming a peer or perhaps a \nsubordinate. I’ll discuss these skills further in the next chapter. \n\n\n129\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_9\n CHAPTER 9 \n Corporate Social \nResponsibility: The Ethics of \nManaging Information Risk \n Be the change you wish to see in the world. \n —Gandhi \n In the past year or so, we have passed a major inflection point; it has become clear that \nalmost every powered device will compute, communicate, and have an IP address. As \ntechnology becomes embedded into the fabric of our lives, exploits that take advantage \nof technology vulnerabilities may increasingly impact the well-being of almost everyone \nin society. This makes it particularly important that we apply the right ethical values to \nshape the way we design, develop, and implement these technologies.  \n The past few years have seen an escalating cycle of risk, with correspondingly \ngreater impacts for businesses and individuals. If that trajectory continues as technology \nbecomes more pervasive, the implications for society could be catastrophic. This \nmeans we should all, as security professionals, contemplate our ethical responsibilities \nnot only to the organizations we work for, the customers we serve, and the company’s \nshareholders, but also to society. To put it another way, I believe that information security \nand privacy are issues of corporate social responsibility. \n Yet even as it becomes even more important to consistently apply an ethical \napproach to managing information risk, business demands and other challenges can \nmake it increasingly difficult to do so. Companies’ continuous efforts to drive growth and \naccelerate time to market translate into demand for faster implementation of internal \nsystems and new technology-based products. At the same time, implementing effective \nsecurity and privacy is becoming more difficult due to a more complex threat landscape \nand the expanding, fragmented regulatory environment. \n These factors result in increasing pressure on technology and business professionals \nto take risky short cuts. In some cases, there may be clear conflicts between business \npriorities, such as the deadline for launching a new product, and “doing the right thing” \nin security and privacy terms. There are also many gray areas in which the right course \n\n\nCHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n130\nof action is not immediately clear; whether to expend resources on protection against a \nthreat that’s still on the distant horizon, for example. I’ll explore these ethical dilemmas, \nand offer suggestions about how to find solutions to them, later in this chapter. \n \nWHAT IS CORPORATE SOCIAL RESPONSIBILITY?\n Definitions of corporate social responsibility typically focus on the idea that \ncompanies look beyond their profits and legal obligations to their broader role \nin society. A common theme is that a company should take into account the \nsocial, ethical, and environmental effects of its activities on its employees and the \ncommunity around it. Here are three definitions that summarize some of the key \nconcepts: \n “The notion of companies looking beyond profits to their role in society is generally \ntermed  corporate social responsibility (CSR)… It refers to a company linking \nitself with ethical values, transparency, employee relations, compliance with legal \nrequirements, and overall respect for the communities in which they operate. It goes \nbeyond the occasional community service action, however, as CSR is a corporate \nphilosophy that drives strategic decision-making, partner selection, hiring practices, \nand, ultimately, brand development.” (McComb 2002) \n “CSR is about businesses and other organizations going beyond the legal obligations \nto manage the impact they have on the environment and society. In particular, this \ncould include how organizations interact with their employees, suppliers, customers, \nand the communities in which they operate, as well as the extent they attempt to \nprotect the environment.” (Lea 2002) \n “The continuing commitment by business to behave ethically and contribute to \neconomic development while improving the quality of life of the workforce and their \nfamilies as well as of the local community and society at large.” (World Business \nCouncil for Sustainable Development 2007) \n The Expanding  Scope of Corporate Social \nResponsibility \n Despite the obvious societal implications of security and privacy risks, most companies \ndon’t consider them to be CSR issues today. That may change over time, as public and \ncorporate awareness of the risks continues to expand. Already, some major technology \ncompanies include descriptions of how they manage security, privacy, and business \ncontinuity in their CSR reports (see sidebar). That trend may spread as companies in \nother industries add more technology-based products and services. \n\n\nCHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n131\n Consumer data protection is one area of information risk that is already widely \ntreated as a CSR issue; it is even included in the International Standards Organization \ncorporate social responsibility standard (ISO 26000). As Forrester Research analyst Heidi \nShey put it, “It’s time to start thinking of protecting customer data as a corporate social \nresponsibility, and not to check off boxes for compliance or a thing that must be done so \nyou can avoid some nasty breach costs.” (Shey 2014). \n In terms of the potential impact on society, security and privacy could be considered \na digital extension of consumer safety, which companies have viewed as a CSR issue for \nmany years. Furthermore, a quick review of the history of CSR shows that its scope has \ncontinually evolved and broadened to include new issues, typically as public awareness \nof those issues has increased. For example, it’s not so long ago that rivers and oceans were \nused not only as human sewage dumps but also as a convenient method for disposing of \nindustrial waste; as late as 1969, one large river in Ohio was so polluted that it regularly \ncaught fire. Yet today, discussions of environmental impacts are typical in CSR reports, \nand in the last few years have further evolved into a focus on climate change: in 2015, 82% \nof the world’s largest companies included data about carbon emissions in their reports \n(KPMG International 2015). \n While early social-responsibility efforts were often philanthropic in nature (such as \nthe funding for public libraries and education provided by Andrew Carnegie, founder of \nUS Steel), corporate social responsibility reporting is now a mainstream business practice \nworldwide, undertaken by more than 90% of the world’s largest companies. \n \nTECHNOLOGY COMPANIES THAT  TREAT INFORMATION \nRISK AS CSR\n Some large technology companies—including Cisco, Microsoft, and Intel—already \nposition information risk areas such as security, privacy, and business continuity \nas corporate social responsibility items, and discuss them in their CSR reports. \nWhile the reports devote space to the companies’ achievements, they also \ndescribe corporate positions and principles on key issues such as data protection \nand transparency. Cisco’s 2015 CSR report, for example, notes the company’s \ncommitment to produce a twice-yearly transparency report that includes data \nrequests or demands for customer data received from law enforcement and national \nsecurity agencies around the world (Cisco 2015). \n Apple CEO Tim Cook has also spoken out about his company’s commitment to privacy \nand security, particularly when protecting user data. In a letter published on the \ncompany’s web site, he said: “We don’t “monetize” the information you store on your \niPhone or in iCloud. And we don’t read your e-mail or your messages to get information \nto market to you.” Cook has argued vociferously that government should not have \n“back door” access to systems in order to thwart terrorism. “The reality is if you put a \nback door in, that back door's for everybody, for good guys and bad guys,” he said on \nCBS’ 60 Minutes (Rose 2015). \"I don't believe that the tradeoff here is privacy versus \nnational security. I think that's an overly simplistic view....we should have both.” \n\n\nCHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n132\n The Evolution of Technology and Its Impact \n To continue the exploration of why I believe security and privacy is a matter of corporate \nsocial responsibility, here’s another quick historical perspective, this time examining the \nemergence of information risk in the context of technology’s  evolution . \n The  march of technology can be viewed as a succession of major waves, each lasting \nroughly 100 years (Rifkin 2013). Each wave has brought transformative benefits to society, \nbut also significant challenges. The first wave, starting in the 1760s, included steam \npower, railways, and early factories as well as mass education and printing. The second \nwave, starting roughly in the 1860s and continuing well past the mid-1900s, included \nautomobiles, electricity, mass production, and had an even bigger effect on society. Many \nof today’s corporate social responsibility issues today are the negative impacts of those \nfirst two waves of technology: examples are environmental impacts due to industrial \nproduction, mining, and oil drilling; factory working conditions; and the safety of mass-\nproduced items. \n The third wave began in the 1960s, with early computers, but only really gained \nmomentum in the 1990s. It includes the Internet and smart “things,” molecular biology \nand genetic engineering, and renewable energy. Arguably, this technology wave may \nhave the broadest impact on society of any to date. Each previous wave lasted about 100 \nyears, so history suggests that we are far from reaching the crest. If this wave was a movie, \nwe’d still be watching the opening credits. \n If the opportunities presented by this third wave of technology are unparalleled, \nso are the risks to society. As I’ve argued in earlier chapters, as technology has spread \nexponentially, so have the threats and their impacts, while security controls have \nprogressed at a more linear, incremental rate. As a result, there’s a continually growing \ngap between the capabilities of the controls and the impact of exploits. If the impact of \nsecurity breaches seems big now, consider what the impact will be in 10, 20, or 50 years, \nwhen technology is even more pervasive throughout society.  \n Let’s consider some of the  potential impacts by reiterating two examples from \nChapter 6. Last year, doctors for the first time inserted an artificial “eye” that enabled \na blind person to see. The device is a retinal implant that receives signals from a video \ncamera integrated into eyeglasses. Think ahead a few years, to a time when the implants \nare more sophisticated and can see in much higher resolution, and also include software \nto automatically interpret visual information, such as QR codes. Then imagine that a \nmalicious actor creates a QR code that triggers the vision system to download malware. \n Table 9-1.  The March of Technology \n Version 1.0: 1760s \n Version 2.0: 1860s \n Version 3.0: 1990s \n Steam and coal \n Electric lights \n The Internet \n Railways \n Communications \n Molecular biology \n Factories \n Oil and gas \n Renewable energy \n Printing press \n Mass production \n “Smart” everything \n Mass education \n Automobiles \n\n\nCHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n133\nLike the PC malware that paralyzed Sony’s network in 2014, the malware then demands a \nransom to re-enable the person’s vision. Now consider the example of a cement company \nthat’s embedding sensors in the concrete mix used to build a new road, thus enabling \nlocal authorities to monitor traffic patterns and adjust signals to optimize the flow of \nvehicles. If the technology is not securely designed and implemented, all that a malicious \nperson needs is the ability to execute malicious code, in order to falsify the traffic pattern \nin such a way that vehicles converge on the scene of a planned bomb attack. \n Here’s example of a  real-life attack that unfortunately has already occurred. Over \na four-day period during November 2008, members of an Islamic militant organization \ncarried out a series of 12 coordinated shooting and bombing attacks across Mumbai. The \nattacks killed 164 people and wounded at least 308. Of the funding that enabled the attack, \n$2 million was raised by cyber crime (Goodman 2015). Think about how cyber crime \nworks. Typically, the cybercrime cycle starts with stealing someone’s identity by installing \nmalicious code on a device or by taking advantage of insecure behavior. So ask yourself: If I \ndon’t keep my systems up to date, if I don’t design and implement them well, and educate \nemployees to ensure they are security-aware, am I indirectly contributing to terrorism? \nThe answer is that you might be—although in most cases, you won’t even know it. \n As I discussed in Chapter 6, four  motivations account for the majority of serious \nexploits. Terrorism is one. The others are financial gain, warfare, and hacktivism. Each \nof these motivations can result in consequences with broad impacts across society: \neconomic damage, loss of services, damage to morale, degradation of government \nservices, and even human casualties. \n As all companies become technology companies, the technology they create and \ndeploy may be exposed to exploits with potential impact on society. The same applies, \nof course, to  public-sector organizations . Even though this idea is becoming more \nwidely accepted, I occasionally encounter people who don’t believe it applies to their \norganization. Recently, as I fielded questions after giving a talk, an audience member \ncommented that she was on the board of a local school and definitely didn’t see the \nschool as a technology organization. “Does your school have a web site that parents and \nkids can use to view and update information?” I asked. She said yes. Then I asked “Does \nyour school have an app that lets parents check whether their kids attend class?” No, \nshe said, but the school was considering it. “Let’s imagine you have a web site that’s not \nwell designed, and a malicious person decides to take advantage of that with a zero-day \nexploit,” I said. “He can compromise the site and the personal information of the parents \nand children that use it.” I added that if a school takes its technology to the next level by \nmaking an app available to parents or kids, it becomes even more clearly a technology \nsupplier—and its security concerns now include product vulnerabilities. By the time \nI’d finished explaining, the audience member asked me if I could come and explain the \nissues to her board, which of course I agreed to do. \n Here’s another school example, one that highlights the risks of failing to consider \nall the  ethical implications : A Pennsylvania school district issued laptops to some \n2,300 students, then remotely activated the laptops’ webcams—without informing the \nstudents—and used the webcams to secretly snap students at home, including in their \nbedrooms. Surveillance software on the laptops also tracked students’ chat logs and the \nweb sites they visited, and then transmitted the data to servers, where school authorities \nreviewed and shared the information and in at least one case used it to discipline a \nstudent. Ultimately, the school district was forced to settle a class-action lawsuit that \ncharged it had infringed on the students’ privacy rights (Bonus 2010). \n",
      "page_number": 148
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 156-163)",
      "start_page": 156,
      "end_page": 163,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n134\n Maintaining Society’s  Trust \n The third wave of technology offers opportunities for all organizations. But as the \nopportunities increase, so does the obligation to use technology responsibly. If we don’t \nimplement appropriate security and privacy protection, consumers won’t trust the \ntechnology. If they don’t trust the technology, they will be reluctant to use it. This could \npotentially affect any company that supplies technology, and impact the growth of the \ndigital economy overall. \n Unfortunately, the privacy and security breaches that have hit the headlines in \nrecent years have weakened that trust. As a result, consumers’ trust in technology sank \nlast year in 70 percent of countries surveyed worldwide, according to the Edelman Trust \nBarometer, a widely used indicator of trust in business and government. Worse, the rapid \nimplementation of new technologies that are changing everyday life, “from food to fuel \nto finance,” emerged as a new factor depressing trust overall. “By a two-to-one margin, \nrespondents in all nations feel the new developments in business are going too fast and \nthere is not adequate testing,” the study concluded (Edelman 2015). \n Top US regulators have urged companies to expand and clarify their privacy efforts. \nFederal Communications Commission chairman Tom Wheeler said Internet service \nproviders have a responsibility to make sure personal data is held securely and that \ncompanies are transparent about the data that’s being captured. “There's no question that \nwith connected devices, data is becoming today's currency, and we need to be aware of \nthe impact of that on consumers,” added Federal Trade Commission Chairwoman Edith \nRamirez, noting a recent Pew Research Center survey found that 47% of Americans lacked \nconfidence that they understand what companies will do with their personal information, \nand had mixed feelings about whether or not to share it (Hamblen 2016). The weakening \nof trust is a dangerous trend. Breaking someone’s trust is like crumpling up a perfect piece \nof paper: you can work to smooth it over, but it will never be the same again. \n All organizations inevitably experience security and privacy issues. The question \nis how we respond to them. We can manage them in way that focuses on limiting our \nliability, or we can focus on doing the right thing for those who may be impacted. I \nrecently participated in a peer group discussion that evolved into an intense debate on \nthis very issue. The discussion was prompted by the major breaches that occurred in \n2014 and 2015; as a group, we discussed how we might jointly develop the concept of a \n“minimum standard of care” for security and privacy. Some people wanted to focus on \nlimiting corporate liability for a breach. I believed that was the wrong goal, and argued \nthat the primary focus should be on protecting our customers. My reasoning was that if \nwe protected our customers, we would limit our liability as a natural consequence. But \nif we focused only on limiting liability, we would likely fail to take the necessary steps \nto protect our customers. Furthermore, I believed that the lens we chose to view the \nproblem with would bias strategy and outcomes over the long term. A liability-focused \nstandard would inevitably cause us to direct our efforts into seeking ways to limit our \nresponsibility for the technology we create and manage. But if the standard focused on \nprotecting the people who might be impacted, we would direct our efforts to thinking \nabout how best to prevent, detect, and respond to risks.  \n\n\nCHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n135\n The Ethics of  Managing Information Risk \n Some professions, such as certified public accountants and doctors, have ethical \nstandards that may require them in some cases to break ranks with their organizations, \nsuch as if they see signs of illegal activities or financial manipulation. We expect doctors \nto be personally accountable for decisions that affect the lives of their patients, rather \nthan simply deflecting responsibility for health decisions onto someone else within the \norganization. If CPAs or doctors fail to meet these professional and ethical  standards , they \nmay lose their ability to practice. \n Although there are many professional certifications for security and privacy \nprofessionals, there’s currently no equivalent to these medical or legal qualifications. \nSecurity and privacy managers are not automatically barred from practicing their trade if \nthey fail to meet professional standards. However, we should all assume a similar level of \npersonal accountability for our decisions—especially since our actions may have broader \nimplications for society. Regrettably, not all of us do. Some security and privacy managers \nsee their role as simply managing a risk register: they identify the risks, and perform the \nanalysis and associated cost estimates, but then they take the register to other executives \nwho then make the decisions. By doing so, they are abdicating responsibility and \ndeflecting accountability onto someone else. \n As the senior security and privacy professional within the organization, CSPOs \nshould share responsibility for information risk decisions equally with the other \ncorporate executives and the board. People are often told that they need to “think like \nan owner;” we need to act like an owner too. And ultimately, we need to think about our \nresponsibility to all the people we work for—including customers and anyone else in \nsociety impacted by our actions—as well as our responsibility to the executives we report \nto. If you don’t think your manager is right, think hard about the possible consequences \nof not speaking out and where your responsibility ultimately lies. \n The recent  events at automakers have shown all too clearly what can happen \nwhen corporate culture produces a system in which professionals are driven to behave \nunethically in order to meet business goals, or fail to take responsibility for their actions, \nwhile senior executives apparently remain ignorant. In the Volkswagen emissions-testing \nscandal, engineers included software specifically to deceive test equipment so that cars \ncould meet the emissions targets required for sale in the US. An investigation into General \nMotors ignition-switch problems that caused at least 13 deaths described the “GM \nSalute,” in which employees sat in meetings, with their arms folded and pointing outward \nat others, as if to say that the responsibility lay with those other people, not with the \nemployees (Maynard 2014). At both automakers, top executives said they were unaware \nof the actions of the lower-ranking employees who were directly involved in the issues. \n In our daily lives, we encounter many situations in which we need not only to decide \non the right course of action, but also to take responsibility for voicing our opinions so \nthat they are considered by the company as a whole. Suppose that a business manager \nis proposing an action that’s legal but conflicts with our security values and approach \nto protecting customers’ information. Or imagine that implementing the right level \nof protection risks the target dates for a critical product launch. Or that failing to tell \ncustomers or suppliers about a potential vulnerability becomes the equivalent of a lie. \n\n\nCHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n136\n In the book  Giving Voice to Values , author and educator Mary Gentile discusses \nthe ethical dilemmas that many people face in businesses today. Her assumption, as \nshe observes in the book, is that “in many if not most of the managerial and financial \nmisbehaviors we have seen in the past, there were enough people who recognized the \nlapses in ethics and judgment to have stopped them. The problem was that they did not \nbelieve it was possible to do so.” Gentile then focuses on providing techniques to help \npeople voice their concerns and take action at “those times and situations when we \nbelieve we know what is right and want to do it, but we experience external pressures—\nfrom our boss, our colleagues, our customers—to do otherwise. As a result, we are not \nsure how to raise our concerns.” \n \nDISCLOSING  SECURITY ISSUES : A TALE OF TWO \nCOMPANIES\n Questions about how to deal with the discovery and disclosure of security issues \nare likely to generate difficult ethical discussions for many companies. The following \nexamples show how two companies dealt with security issues in very different ways. \n In December 2015, networking vendor Juniper Networks disclosed that an internal \ncode review had discovered “unauthorized code” in its firewall operating system that \ncould allow hackers to gain administrative access and decrypt encrypted VPN traffic. \nThe company said it had not received any reports of exploits using the vulnerability; \nit said it had released patches to fix the problem and urged customers to update \ntheir systems (Worrall 2015). This is a case in which a company appears to have \nmanaged a difficult issue well, in my opinion. It highlights the tough questions and \ndiscussions that companies face when managing potential security issues. How \ndeeply do you test and review your code, knowing that the deeper you dig the more \nlikely you are to find vulnerabilities? If you do find a problem, how do you handle it? \nDo you disclose it, quietly fix it, or even ignore it? Does your company have the right \nvalue structure to ensure that decisions reflect its responsibilities to customers and \nto society? \n Now consider a contrasting example. In 2015, a vendor of dental practice-\nmanagement software agreed to pay $250,000 to settle US  Federal Trade \nCommission (FTC) charges that it falsely advertised the level of encryption it \nprovided to protect patient data (Federal Trade Commission 2016). According to the \nFTC, the company spent two years touting its “encryption capabilities” for protecting \npatient information and meeting “data protection regulations”—yet at the time, it \nwas well aware that its software didn’t provide the encryption required by HIPAA. It \nseems clear that a company that makes deceptive claims of this kind lacks a value \nstructure capable of ensuring ethical security and privacy decisions. \n\n\nCHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n137\n The challenges described in  Giving Voice to Values probably seem familiar to many \nof us who are responsible for managing information risk (see sidebar article). First, how \ndo we decide what is the ethical course of action? Then, how do we take action by voicing \nour opinions when it really matters? \n One starting point is to define the organization’s critical security and privacy \nprinciples, which then can serve to guide our decisions. These principles should be \nderived from the organization’s corporate values. For example, a company that prioritizes \ncustomer service should also be committed to protecting customer information, and \ntherefore its critical principles should include privacy by design. \n We then need to think about how to focus the company on those principles: how \nwe create the right language to express the principles to others, and how we enroll our \norganizations in principle-based decision making. We need to make security and privacy \nclearly visible in the decision-making process, not just within the information security \norganization but across the entire organization. That sends a message to everyone, \nincluding customers as well as people within the organization, that security and privacy \nare corporate priorities. By demonstrating our commitment to these principles, we can \ncreate trust in our organization and its technology. \n We can use our security and privacy principles as a compass to guide us through the \ndilemmas we encounter. We can approach these dilemmas using the same framework \nthat we apply to any source of information risk:  sense , interpret, and act (see Chapter 3).\n• \n Sense: Are changes on the way that that could conflict with our \nsecurity and privacy principles? What is the dilemma that we will \nface? \n• \n Interpret : Analyze the issue to determine the following: Can \nI make this decision? Which of our principles can guide my \ndecision? Who do I need to talk to? What actions can I take, and \nwhat are the direct and indirect consequences of each? \n• \n Act : Will my action align with the organization’s best interests? \nWhat about the interests of our customers, and of society in \ngeneral? Will my action or lack of action create embarrassment \nfor the company? Is my action practical? Who should I tell? \n Conclusion \n As we progress through the third wave of technology, and our reliance on technology \nexpands even further, so does the potential societal impact of security and privacy issues. \nOur professional and ethical responsibilities require that we hold ourselves accountable \nfor doing what we know is right. This is true today, and will be even more so in the future. \nThis means that we will have to take career risks to make sure that security and privacy \nare appropriately handled within the organization, including ensuring that issues are \ndiscussed at board level. I’ll discuss how to do this in more detail in the next chapter on \nthe 21st Century CISO. \n\n\n139\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_10\n CHAPTER 10 \n The 21st Century CISO \n Leadership is the art of mobilizing others to want to struggle for shared \naspirations. \n —Jim Kouzes and Barry Posner, \nThe Leadership Challenge \n The finance director sounded frustrated and exhausted. Our IT auditors had been \ntrying to tell her about an obscure yet important data backup problem that affected SOX \ncompliance. But her background was in accounting, not technology, and as the IT experts \npresented page after page of technical information elaborating the intricacies of backup \nprocesses, her eyes glazed over. The more they tried to explain by adding yet another \nlayer of detail, the more confused and frustrated she became. \n That’s when I thought of a solution. “Imagine,” I said, “we’ve got a passenger train \nrunning from station A to station B. That’s what our backups are like; they’re carrying data \nfrom our servers to tape.” \n “We know the train arrived at station B, so we know the backup occurred,” I said. \n“But we don’t know how many passengers got on at station A, and we don’t know how \nmany got off at station B. So we can’t definitively say we actually backed up all the \ninformation, and to comply with SOX, we need to be certain.” \n The finance director sat up. For the first time since the start of the presentation, she \nseemed alert and engaged. And from that point on, we made progress. She asked how we \nplanned to solve the problem, we briefly mentioned a couple of the possible solutions, \nand the meeting ended on an upbeat note. \n My storytelling, using an off-the-cuff metaphor, succeeded where the more traditional \napproach had failed. It communicated a technical security issue in terms that a senior \nbusinessperson could understand and remember. And it illustrates one of the key skills of \nthe 21st century CISO. We need to extend our reach outside the security organization to \ncommunicate with and influence people at all levels, from all backgrounds. \n Chief Trust Officer \n In this chapter, I’ll explain some of the skills and traits I believe CISOs need in order to \nfulfill their changing role. To set the stage, I’d like to step back for a moment and briefly \nrecap the changing focus of information security overall. \n\n\nCHAPTER 10 ■ THE 21ST CENTURY CISO\n140\n As I’ve discussed earlier in the book, every company is becoming a technology \ncompany. And as the potential impact of information risk expands, it is becoming \nessential to manage security and privacy as a corporate social responsibility. The CISO’s \nrole should therefore expand to span the full breadth of information-related risks, as \ndescribed in Chapter  1 . At many organizations, this is already happening. CISOs are \ntaking on responsibility for privacy, regulatory compliance, and product and service \nsecurity, in addition to more traditional IT security functions. \n This is a huge opportunity for CISOs to step into a more valuable, high-profile role \nwithin the organization. The core skills of information security professionals—evaluating \nand mitigating risk—are as essential for mitigating new risks associated with product \nsecurity, privacy, and regulatory compliance as they are for more traditional IT-related \nthreats. But perhaps this broader role requires a different title that more accurately \nreflects the convergence of risk responsibilities, such as Chief Trust Officer or Chief \nInformation Risk Officer. \n Taking on a larger role requires a broader view and a corresponding set of skills. We \nneed to communicate in terms that business people understand, and build relationships \nthat enable us to influence people at all levels across the organization. We also need \nextensive management and leadership skills, both to operate at an executive level and to \ninspire our expanded risk and security team.  \n The ability to manage the full range of information-related risks is a necessity, \nnot just for the CISO, but for the organization. If we do not step into a broader role, the \norganization must acquire these abilities elsewhere. Because of this, CISOs who do not \nadapt to this role run the risk of becoming irrelevant to the organization. Alternatively, \nthese risk areas will be managed in a stove-piped, fragmented way, in which case the \norganization may never discuss the aggregation of risks and the controls necessary to \nmanage them. If this occurs, organizations will certainly generate unmanaged risks to \nthemselves, their customers, and to society. \n Until recently, one of the CISO’s biggest challenges was obtaining funding for \nsecurity initiatives. Today, due to the prevalence of large breaches, it’s often easier to find \nfunding. But more funding doesn’t always lead to greater security or a better outcome for \nthe organization. Sometimes the fear of breaches drives organizations to invest heavily in \ncontrols that generate a high degree of control friction, restricting users’ ability to do their \njobs. For example, some organizations have installed controls that prevent users from \ndownloading apps or files, or even accessing some web sites. These controls threaten \nto stifle users’ ability to innovate and hinder overall business velocity. Furthermore, \ndetermined users will find ways around the controls, such as using less-secure personal \nsystems to access “forbidden” resources. \n CISOs need business acumen to understand the impact of security controls on \nothers in the organization. As I discussed earlier in the book, our approach to security \narchitecture should start with an understanding of the 9 Box of Controls, including the \nfriction that controls can generate. Business acumen is also necessary to communicate \ntechnical risks in language that nontechnical people in the business can grasp, and to \nunderstand that some risks are worth taking. Risk-taking is fundamental to business. \nWithout it, no business value would be created. \n\n\nCHAPTER 10 ■ THE 21ST CENTURY CISO\n141\n The  Z-Shaped Individual \n If we don’t already have the skills required of the 21st century CISO, we need to acquire \nthem. \n To some extent, this trend parallels what is happening in most technology-related \nprofessions: IT professionals need to acquire business acumen as well as depth of IT \nknowledge. The concept of “T-shaped”  individuals has been widely used to describe the \nidea that IT professionals need to be able to provide value horizontally, across business \ngroups in the organization, as well as vertically at all levels within IT. \n This concept is useful, but it doesn’t fully encompass the skills of the 21st century \nCISO. The unique role of CISOs and other security professionals might be better \nrepresented as a “Z-shaped” individual, as shown in Figure  10-1 . Adding the third \ndimension of core security skills, such as risk assessment and understanding of controls, \nallows us to deliver value across the business and all areas of IT. \n The 21st century CISO needs to understand business priorities and processes well \nenough to identify how security controls help or constrain the business. To gain this level \nof understanding, he or she has probably gained experience in areas that are central to \nthe company’s business, which, of course, vary depending on the company’s core focus. \nFor example, the CISO might previously have worked in manufacturing operations, \nservices, or mergers and acquisitions. \n The CISO needs technical knowledge too, although the depth of technical \nknowledge required remains a subject of intense debate among my peers. I’ve observed \nCISOs at smaller and less-complex organizations who feel they need deeper technical \nskills to do their jobs. This is not surprising. With much smaller security teams, CISOs at \nsmaller companies may need to be more involved in day-to-day technical details as well \nas managing people. At larger and more complex organizations, CISOs are less likely to \nspend time delving into technical detail. \n Figure 10-1.  The T-Shaped IT professional (left) and the Z-shaped CISO (right) \n \n\n\nCHAPTER 10 ■ THE 21ST CENTURY CISO\n142\n However, all CISOs need to be able to understand enough about the technology to \nabsorb the important issues and communicate these issues to other managers outside \nthe security group. This means that our technical knowledge must be broad, ranging from \ndevices to data centers. We need to know enough about devices, such as smartphones, \nPCs, tablets, and new evolving device types such as wearables, to understand the security \nimplications as well as the benefits. At the other end of the scale, we need to know enough \nabout data centers and physical access controls to understand and communicate the \nimportant security requirements and challenges. \n Our core risk management and security skills provide the link that completes the “Z” \nby connecting technology and business. We understand how to assess and manage risk \nby applying procedural, technical, and physical controls to meet the organization’s legal, \nprivacy, and security requirements. \n Foundational Skills \n Becoming a Z-shaped individual is the foundation for one of the 21st century CISO’s \nessential traits: establishing credibility across the organization. We must be credible \nin order to build trusted relationships with executives and specialists across the \norganization and to discuss the vast range of issues that affect the business. This \ncredibility is built on the competence that comes from understanding the business and \ntechnology as well as possessing core security skills. By becoming Z–shaped, we will also \nbe better positioned to influence risk management for the company’s product and service \nstrategy, as opposed to having those risks managed independently by another group. \n Our ability to influence the organization also springs from a clear mission. I use the \nterm  centered to describe this. We can effectively present our case because we have a \nstrong sense of purpose and a clear understanding of why the security group exists and \nwhat we are trying to achieve. \n This idea returns us to the theme of this book: Protect to Enable. In our global \neconomy, most companies operate in highly competitive markets. As the security \norganization, our mission is to enable the free flow of information and rapid \nimplementation of new capabilities to ensure success and long-term competitive \nsurvival. Other CISOs may work at more risk-averse organizations, and therefore some \naspects of their mission may differ. However, the mission always needs to be aligned with \nthe organization’s business priorities. It is essential that this mission becomes a part of \nwho we are and why we exist. It provides a sense of purpose that lends authenticity and \nconsistency to our actions and helps us build credibility across the organization. \n As we all know, security can be a particularly distracting profession, with a constant \nbarrage of day-to-day emergencies and diversions. So we need a clear mission in order \nto retain a strong sense of direction. Like expert sailors, we can progress toward our goal \namid the day-to-day distractions and diversions, making continual adjustments and \ncorrections to stay on course as the winds shift. \n We also need to retain a sense of curiosity. To engage with others, we need to be \ngenuinely interested in what they do. This curiosity enables us to continue to learn, \nbuilding on and broadening the competencies that then enhance our credibility. \n Another major reason we need to be learners is to stay ahead of the enemy. Threat \nagents are always learning because they must. As new threats emerge, we put in place \nnew controls. But once implemented, these controls tend to be static, while threat agents \n",
      "page_number": 156
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 164-171)",
      "start_page": 164,
      "end_page": 171,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n143\nare dynamic, coming up with new techniques to bypass the controls. Therefore, our \nthinking must also be dynamic, and we must continually learn in order to protect against \never-evolving threats. \n Becoming a  Storyteller \n We cannot influence people unless we communicate with them. And as the scope of \ninformation risk expands, we need to communicate with a wider range of people across \nthe organization. \n Communicating with people isn’t always easy, as most of us have discovered. If we \nstart relaying technology details to those who aren’t technologists, we won’t capture their \ninterest. In fact, we run the risk of doing the opposite, as I described in the example at the \nstart of this chapter. \n To communicate, CISOs must become chameleon-like, with the ability to blend \ninto a variety of environments. We need enough knowledge of each business domain to \nbe able to communicate with different groups using language they understand. And we \nneed to discuss these subjects at different levels. A CFO may only want to hear a high-\nlevel summary expressed in terms of financial impact and return, which is often not easy \nwhen discussing security investments targeting hard-to-quantify threats. Product group \nmanagers want to hear security issues expressed in terms that relate to sales, marketing, \nand operational efficiency. \n I’ve found storytelling to be a powerful tool for communicating with diverse people \nacross the organization. When I frame security issues as stories and images that people can \nunderstand, they relate better to the issues even if they lack a background in technology. \n I like to tell stories using metaphors and analogies. They are easily remembered, \nand they translate complex subjects into simple terms everyone can understand. In fact, \nthe metaphors I’ve used throughout this book, such as the perfect storm in Chapter  1 , \nthe train backup in this chapter, and the roundabouts and traffic lights in Chapter  5 , have \nhelped me communicate security issues to many people. To use yet another analogy, \norchestra conductor Benjamin Zander said, “The conductor of the orchestra doesn’t \nmake a sound. His power comes from awakening possibility in others.” (Zander and \nZander 2000). In the same way, I believe the power of the CISO comes from awakening \nthe awareness of risk among people across the organization. I use stories based on \nmetaphors to create that awareness. \n For example, employees often find it hard to understand the dangers of stealthy \nthreats. This is because the threats are unobtrusive, concealing themselves so they can steal \ninformation over the long term. Users are usually not even aware that a problem exists on \ntheir system. They still associate malware with obvious, annoying symptoms such as screen \nmessages and system crashes. So when we tell them we’ve detected dangerous software on \ntheir machine, they have a hard time believing that it matters. That is why we have to focus \non prevention using low-friction controls. If we do not achieve this as a profession, we will \nperpetuate the worsening cycle of risk we are experiencing today. \n To communicate the danger, and the need for effective preventative controls, I \nsometimes use the analogy of ants and termites. “Malware used to be like food-eating \nants in the kitchen,” I explain. “You’d know when you had an infestation because you’d \nsee ants crawling over the countertops and walls. Once you knew about them, you’d spray \nor set traps to eliminate them. \n\n\nCHAPTER 10 ■ THE 21ST CENTURY CISO\n144\n “But today, threats are more like the termites that can live in your walls. You can’t see \nthem, and you may not even know they are there. But they’re doing much more damage \nthan ants ever did. In fact, they may be destroying the structural integrity of your house.” \n I’ve found using analogies helps quickly drive home messages. People immediately \nunderstand that these invisible threats can undermine the structure of the computing \nenvironment, just as termites undermine houses. This makes them more likely to \naccept the next step, which is that we have to perform the digital equivalent of tenting \ntheir computer to eradicate the vermin, but without toxicity to users or the computing \nenvironment. \n \nTHE  NIST FRAMEWORK : A COMMON LANGUAGE \nFOR RISK MANAGEMENT\n To discuss information risk management across the organization, it’s helpful to use \na common language that everyone, including non-technical people, can understand. \nI’ve found the National Institute of Standards and Technology (NIST) Cybersecurity \nFramework to be a helpful tool for communicating the issues. Development of the \nframework was triggered by a 2013 presidential executive order on improving the \nsecurity of critical infrastructure. This led to a year-long private-sector–led effort to \ndevelop a voluntary how-to guide for organizations. Many companies contributed \ninput about standards, best practices, and guidelines to that effort. I was one of the \nfirst security leaders among the Fortune 500 companies to engage the framework. \n The framework creates a common taxonomy and terminology for managing risk, \nmaking it easier for security teams and others to communicate. It fosters collaboration. \nIn addition, each organization can measure its risk management maturity level \nagainst the framework. As the framework is used by more people, including business \nexecutives, it may help to increase the overall understanding of information risk and \nhow to manage it, which would be a good thing for all organizations. \n Fear Is Junk  Food \n Just as building trusted relationships is essential to influencing the organization, I also \nthink we need to transcend the doom-and-gloom that can pervade discussions of \nsecurity topics. \n The security industry has a tendency to use fear to sell products. Unfortunately, this \ntendency reflects the fact that many people in the security industry profit from insecurity: \ntheir revenue grows when more breaches and other incidents occur. Internally, as \nsecurity professionals, we sometimes share this tendency to use fear as a tool to obtain \nadditional budget or other resources. Of course, security really is about scary things: \nthreats, vulnerabilities, and risk. But focusing on fear as the primary motivator is like \nliving on a diet of junk food. It may provide immediate gratification, and it’s somewhat \naddictive, but ultimately it’s not healthy for either the CISO or the rest of the organization. \n\n\nCHAPTER 10 ■ THE 21ST CENTURY CISO\n145\n In the short term, fear can scare people into action and help drive funding for \nsecurity projects. However, relying on fear alone can only work for so long. Eventually, it \nhas the opposite effect. It causes the CISO to lose credibility. In fact, I think relying on fear \nmay even contribute to the high rate of job turnover among CISOs. Those who rely too \nmuch on selling fear are snacking on an unhealthy diet, and eventually the organization \nrealizes this and rejects them. \n Ultimately, fear doesn’t work for other reasons too. Most people don’t want to listen \nto a continuous stream of negativity. If we are always seen as the source of negativity, we \nwill lose our audience. If we are continually viewed as the group that says no, we will be \nignored. People will bypass security restrictions in order to meet their business needs. \n Even within the security organization, fear can become a gravitational force, a \nblack hole drawing ever-increasing attention to the negative side of security issues and \ndraining energy that should be directed to enabling the business. This is why we need \nto focus on solutions that deliver the three key benefits I discussed earlier in the book: a \ndemonstrable and sustainable bend in the curve of risk; the ability to lower the total cost \nof controls; and low control friction to improve business velocity and the user experience. \n Accentuating the Positive \n So how do we take a more  positive approach ? By focusing on our mission, which is to \nProtect to Enable. This mission shifts the emphasis from the negative to the positive: how \nwe can help the business achieve its goals by solving these information risk and security \nproblems. It puts hope and optimism before the challenge. \n This mission is aligned with the business. Rather than being antagonistic, it is based \non common values. It sets an optimistic tone, and, in the long term, optimism is a far \nbetter motivator than pessimism. Threats may be frightening, but our goal is to see past \nthe threats and identify the opportunities. To paraphrase the noted Stanford University \nbehavioral scientist Chip Heath, there’s no problem that cannot be solved without a new \nframework. Therefore, if we can’t see a solution, we have the wrong framework. Protect \nto Enable provides a new framework. So does the 9 Box of Controls, with its focus on \ncost efficiency and control friction as well as effectiveness. These tools help us focus on \nfinding solutions. \n Imagine you’re invited to attend a meeting to discuss whether the company should \nstart using a specific cloud-based business application from a new supplier. Clearly, this \nproduct introduces risks: it comes from an unfamiliar supplier, it’s accessed over the \nInternet, and it means sensitive data will be stored outside the enterprise. \n A narrow security view might focus solely on minimizing the risk. However, this \nnarrow view can lead to a Catch-22 situation, as discussed in Clayton Christensen’s \nbook  The Innovator’s Dilemma (Harvard Business School Press 1997). Typically, it goes \nsomething like this. To minimize the risk, the organization initially restricts the use of a \nnew technology. For example, the technology can only be used for low-risk data, or by a \nnarrow segment of employees. The problem with this approach is that it also reduces the \nbusiness benefit to the point that the benefit of the technology cannot justify the expense \nand effort of adopting it. So we reach an impasse. To make the technology a viable \nproposition, we need to be able to show a business benefit, but we can’t show a business \nbenefit because we won’t allow viable use of the technology. \n\n\nCHAPTER 10 ■ THE 21ST CENTURY CISO\n146\n Protect to Enable provides the new framework that frees us from the innovator’s \ndilemma. It allows us to focus on the opportunity and identify benefits that outweigh the \nrisks. For example, introducing a new supplier increases competition for our existing \nsuppliers, leading to future savings for our organization. This benefit aligns with the \nbusiness and is one that everyone in the organization understands. Perhaps less intuitive, \nbut equally important, the savings can be used to fund security controls to mitigate the \nrisk of using the technology more widely. Now our benefit/risk equation has a positive \nresult rather than a negative one. By enabling the technology to be used more widely, \nwe realize bigger business benefits that outweigh the additional cost of controls. This \nexample also underlines the need for CISOs to build business acumen that enables us to \nsee the opportunity and how it can be used to overcome the challenge of funding security \ninitiatives. \n Let’s look at another example, this time from my experience at Intel in the days \nbefore I had defined our Protect to Enable mission. Several years ago, a highly damaging \nworm was discovered in our environment, requiring a significant emergency response \nfrom our team. Upon investigating, we traced the origin of the worm to an employee’s \npersonal system. \n Our immediate response was that of a stereotypical security group. We shut down \nthis usage to eliminate the risk of future infections. We immediately tightened security \npolicy to ensure only corporate-owned PCs could access the network, and we ruthlessly \nwent through the environment and cut off access by any devices not managed by IT. \n Our response was successful in the sense that it reduced the risk of infection. But it \nled to other risks we hadn’t foreseen. Eliminating personally owned PCs from the network \nmeant we now needed to issue corporate PCs to contract employees. This meant that \nwe had to provide more people with devices that allowed full access to the corporate \nenvironment. It also, of course, increased capital costs. The broader impact was that \nit eliminated the potential business benefits of letting people use their own personal \ndevices for work.  \n Subsequently—driven largely by employee demand, as well as the massive \nproliferation of new consumer devices—we revisited this issue. This time, we examined \nit from the perspective of Protect to Enable. We looked at the business opportunities if \nwe allowed personally owned systems on the network, and then how we could mitigate \nthe risks. As I mentioned in Chapter  1 , we rapidly discovered that the business value \nis enormous. Helping employees communicate and collaborate at any time can drive \nsignificant productivity gains. It also helps make employees happy. They love using their \npersonal smartphones, PCs, and tablets, and they appreciate that we enable them to do so. \n These benefits easily outweigh the cost of the technology required to reduce the risk \nof allowing access by personal devices. True, some of this technology wasn’t available \nat the time we experienced the original security problem. But if we had focused on the \nopportunity first, perhaps we could have found ways to provide some level of access while \nmitigating the risk, and experienced at least some of the benefits we enjoy today.  \n Demonstrating the Reality of Risk \n Of course, the security organization’s role still centers on  managing risk , which includes \ndiscussing the negative consequences of people’s actions. If we frame this discussion \ncarefully, I believe we can inform without fearmongering. By describing possible \n\n\nCHAPTER 10 ■ THE 21ST CENTURY CISO\n147\noutcomes and solutions without using emotional language, in terms listeners can \nunderstand, we create a context in which the organization can make the decisions that \nare best for the business. \n Even when we have to highlight unpleasant outcomes, we’re not fearmongering if \nour information is based clearly on reality. Here’s another example from my experiences \nat Intel. As our customers’ use of the Internet expanded, Intel’s marketing groups \nnaturally wanted to expand their external online presence by creating new web sites. \nSo we, as Intel’s information security group, began assessing the risks and the security \ncontrols required. Some of our marketing teams didn’t find this an appealing prospect. \nThey needed to move quickly, with the freedom to communicate however they thought \nbest, and they viewed security procedures as bureaucracy that slowed them down and \nhindered their ability to communicate with customers and partners. \n What happened next was far more persuasive than any of our initial efforts to \nforestall potential problems. A few web sites were launched without rigorous quality \ncontrol. Hackers found the weaknesses in these sites, but they didn’t crash the sites or \nsteal information. Instead, they inserted links to porn sites. \n When this unfortunate fact was discovered, it provided the leverage we needed to \nimprove security procedures. I realized this was a case where a picture spoke a thousand \nwords. So, to illustrate the impact, I simply showed the links to people within the \ncompany. This wasn’t fearmongering. It was simply demonstrating the real consequences \nof their actions on the brand. Everyone could understand the implied question: Do \nwe want our brand to look like this? This ended, once and for all, any discussion about \nwhether we needed to apply rigorous quality control to external web sites. \n The CISO’s  Sixth Sense \n In the book  Blink: The Power of Thinking Without Thinking , author Malcolm Gladwell \n(Little, Brown & Co. 2005) describes an interesting experiment. Researchers asked \nsubjects to play a game in which they could maximize their winnings by turning over \ncards from either of two decks. What the subjects didn’t know was that the decks were \nsubtly stacked. They could win by selecting from one of the decks, but selecting from \nthe other deck would ultimately lead to disaster. After about 80 cards, the subjects could \nexplain the difference between the decks. But they had a hunch something was wrong \nmuch sooner, after only 50 cards. And they began showing signs of stress and changing \ntheir behavior even sooner, after only about 10 cards, long before they cognitively \nunderstood a difference existed. \n As CISOs, we develop a sixth sense about security issues. Often, my instincts suggest \na need to act or begin investigating a specific direction long before our group is able to \nfully understand or explain what is happening. This sixth sense is particularly relevant \nin the security realm, where our information is almost always imperfect or incomplete. \nWhen a threat strikes, we do not have time to conduct extensive research or wait for \nevidence to accumulate. Therefore, we need to act decisively based on imperfect \ninformation. \n I think we develop this sixth sense from the diverse experiences and skills we’ve \nacquired during our careers. We can also foster this sixth sense by being aware. Some \nsecurity professionals tend to be inwardly focused, looking only at the data and systems \nthey need to protect. As described in Chapter  4 , I have directed my teams to try to be \n\n\nCHAPTER 10 ■ THE 21ST CENTURY CISO\n148\nmore open and outward-looking, sharing information and seeking input from a variety \nof sources, including peers across our company and at other organizations. This can help \nCISOs spot early warning signals and correlate information to quickly identify threats. \nLike secret service agents scanning a crowd, our experience helps us spot anomalies, to \nsee the signals and ignore the noise. \n By identifying future risks early, we may be able to prevent them entirely, or at least \nminimize their impact. We may also reduce the overall effort needed to deal with the risk. \nEarly action may avoid the need for emergency response and a potentially major cleanup \neffort. \n Taking Action at the  Speed of Trust \n A sixth sense is only of value if the organization can act on it quickly. This requires two \nthings. First, we need the courage to take a leap of faith based on what we believe. This \ncourage is rooted in the attributes I discussed earlier in this chapter, such as being \ncentered and credible, with a clear sense of our mission. \n The second requirement is that the organization responds quickly when we inform \nthem about a security issue. This rapid response is only possible if we have established \ntrusted relationships with people across the organization. Because of these relationships, \nthe organization can act at the  Speed of Trust , as Stephen M. R. Covey describes it in \nthe book of the same name (Free Press 2008). Faster, frictionless decisions are possible \nbecause people know, from experience, that our information is reliable and that our focus \nis on enabling rather than spreading fear. \n The CISO as a  Leader \n Above all, 21st century CISOs must become effective leaders who can inspire their teams \nto enable and protect the organization \n Over the years, I’ve identified three essential themes I try to instill in my team and \nconstantly reinforce in our day-to-day interactions. Our security team members must \nbelieve in our mission; they must feel they belong within the security group and the \ncompany as a whole; and they must feel they matter. \n If I can make people feel that they believe, they belong, and they matter, they will \ntackle any challenge. As Kouzes and Posner put it in  The Leadership Challenge (Kouzes \nand Posner 2012), “leadership is the art of mobilizing others to want to struggle for \nshared aspirations.” If people understand the greater goal, it helps establish an emotional \nconnection that guides their everyday actions. This is a key reason that I have thought so \nmuch about defining the mission, and that I have spent so much time helping the teams I \nhave led to see how their jobs are connected to the business’s objectives and concerns. \n For example, a typical  operational goal might be to patch all systems within a week \nof a new software release. This goal is more meaningful if we establish the links to the \nbusiness using the  I believe, I belong, and I matter mantra: “I believe in the mission of \nProtect to Enable. If I'm not protecting to enable, the other employees at the organization \nI belong to cannot do their jobs effectively. The company doesn’t achieve its results, and \nthe company doesn’t execute its vision. Patching systems quickly matters because it helps \nour users do their jobs, which in turn helps the business achieve its goals.” \n\n\nCHAPTER 10 ■ THE 21ST CENTURY CISO\n149\n Learning from Other  Business Leaders \n As leaders, we can learn a lot from how other business leaders work. Today, managers \nare moving away from command-and-control to a more collaborative approach that \ntakes advantage of the diversity of employee ideas and strengths. I’m not talking about a \nconsensus process, which can lead to endless debate and indecision. Rather, a leader’s \ngoal is to ensure alignment to a common mission and accelerate decisions. Within this \nframework, differing viewpoints and debate spark creativity, generating new ideas and a \nproductive tension that can drive results. \n Because security can be frustrating, even daunting, it’s vital to find ways to help \nemployees stay motivated. It’s important to help employees feel they are making \nprogress, not just when they achieve major milestones, but in solving the smaller \nproblems they face every day. A key study found that even small wins boost motivation, \nproductivity, and creativity. In the  Harvard Business Review article describing the \nstudy, authors Teresa Amabile and Steven Kramer (2011) determined that the feeling \nof making progress is the most important contributor to an employee’s emotions, \nmotivations, and perceptions. \n Opportunities to lead occur continually, in every interaction with our teams, with \nother people in IT, and with business partners. The question we need to ask ourselves \nis whether we are seizing these opportunities to reinforce our mission and ultimately to \nhelp the organization achieve success.  \n In highly technical jobs and organizations, we have a tendency to focus on technical \nchallenges while overlooking the “people factor.” I think it’s important to remember the \nneed for personal connections, which foster the sense of belonging. When we know a \nlittle more about each other, we care more as a result. I think about this in my day-to-day \ninteractions. If a team member is making a presentation, are we paying attention and \nasking thought-provoking questions, or are we distracted? And if so, do we think they will \nfeel they belong? \n When we meet with a team member to discuss their struggles with a project, are we \nhelping them think through the issues and come up with solutions? Are we helping them \nbelieve they can overcome the challenges and that the results will matter to the company \nand to us? Or are we just taking them to task? Each interaction is an opportunity for \ncoaching and helping employees improve their performance. \n A final requirement of effective leadership is the ability to develop other leaders \nwithin the security group. Otherwise, the group’s strengths in managing risk for the \nbusiness will last only as long as the current CISO’s tenure. By building competence in \ndepth, the CISO can ensure that the organization delivers sustained performance over \ntime. We will discuss this in more depth in the next chapter. \n Table  10-1 shows research by executive-search firm Korn Ferry suggesting that \ncybersecurity leaders need a unique set of attributes, including the ability to think \noutside the box, dig deeply into issues, exercise judgment at board level, and be a credible \nbusiness partner (Alexander and Cummings 2016). \n\n\nCHAPTER 10 ■ THE 21ST CENTURY CISO\n150\n Table 10-1.  Attributes of Cybersecurity Leaders(Alexander and Cummings 2016) \n Key Attributes for Cybersecurity Executives \n Competence \n Experience \n Traits \n Drivers \n Strategic, global \nthinker (sees big \npicture) \n Depth of technical \nexperience \n Learning agile (can \nadapt to the new \nand different) \n Seeks high visibility \nand accountability \nroles \n Thinks outside the \nbox \n Understands the \nevolving legal \nand regulatory \nenvironment \n Flexible \n Strives to be agent \nof change (not agent \nof “no”) \n Analytical (digs \ndeeply into issues) \n Has successfully \nhandled security \nincidents in the past \n Tolerance for \nambiguity \n Must “thread the \nneedle to balance \ndriving change with \nmanaging enterprise \nrisk” \n Possesses business \nsavvy (understands \nhow information \nis used in daily \noperations) \n Intellectually \ncurious \n Pursues close \nengagement with \nbusiness leaders \n(works to add \nbusiness value) \n Balances competing \npriorities \n Bias for action \n Communicates and \ninfluences broadly \n(board, senior \nmanagement) \n Attracts, builds, and \nleverages talent \n \n Voicing Our  Values \n Obviously leadership means taking responsibility. Yet some CISOs seem to forget this, \nat least occasionally. A typical situation goes something like this. The CISO warned of \na security issue but couldn’t obtain the budget or resources to address it. So the CISO \nabdicated responsibility because someone else had made the decision not to fund \na solution. I take a different view. I believe even if we disagree with the decision, we \nshould do our best to voice our values. We need to articulate the potential impact to the \norganization, to our customers, and to society, as I discussed in Chapter  9 . \n As partners in the organization’s strategy, we should commit to the decision and \nshare full accountability and responsibility with our peers. Having said that, we also need \nto clearly express our personal values and stay true to our principles. Adhering to our \nvalues may mean taking career risks, as discussed in Chapter  9 . Therefore it is critical \nthat we take the time to reflect on what our principles and values really are. This personal \n",
      "page_number": 164
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 172-183)",
      "start_page": 172,
      "end_page": 183,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n151\njourney, which we all need to take, adds another dimension to the Z-shaped individual, \na dimension of values (Figure  10-2 ). As Mary Gentile, the author of  Giving Voice to Values \n(2010) , puts it, “We are more likely to voice our values if we have decided that the costs of \nnot doing so, and the benefits of trying, are important enough to us that we would pursue \nthem even though we cannot be certain of success in advance. In order to get to this place \nof clarity, we need to spend serious time thinking about our own identity, our personal \nand professional purpose, and our own definition of success and failure.” \n Figure 10-2.  Another dimension of the  Z-shaped individual : the personal values that guide \nour actions \n Discussing Information Risk at Board  Level \n Clearly, corporate discussions of any topics that have such far-reaching potential impact \non society should include participation by the executive board. Board awareness of \nsecurity has increased somewhat due to the spate of well-publicized breaches. Yet \nsurveys show that the majority of boards are still not aware of major security and privacy \nissues. A recent study found that only 32% of boards review security and privacy risks, and \nonly 45% have any involvement in security strategy (PWC 2015). \n In contrast, a significant number of security professionals believe that the CEO \nand executive boards are responsible to society for the sometimes disastrous impact of \nsecurity and privacy issues. In another recent survey, one sixth of security professionals \n \n\n\nCHAPTER 10 ■ THE 21ST CENTURY CISO\n152\nsaid they advocate arrest and a prison sentence for the CEO or board members after a \nbreach (Websense 2015). That seems to indicate that they feel their management is not \ntaking the problem seriously enough, or perhaps even chooses to look the other way, and \nthat they are concerned about the broader consequences to society. \n Given the broad and ever-growing importance of security and privacy, boards need \nbe much more involved in than they have been in the past. It is the CISO’s responsibility \nto bring important security and privacy issues to the board, and initiate a debate about \nthe potential impacts of those issues and the right response. Even with the current \nheightened awareness of security issues, it may not be easy to get the board’s attention, \nbecause board members have so many other business issues to worry about. It can help \nto hone in on the handful of risks with the largest potential financial impact or other \nmajor implications such as damage to the company’s brand. Key areas for boards to \nconsider include\n• \n Security and privacy strategy : Is it cohesive and complete? \n• \n The security and privacy leadership : Do they act with a level \nof independence? Do they take ownership of issues, or do they \nsimply manage a risk register? \n• \n Incident response planning and drills : Do they occur? Are they \nintegrated across the organization? \n• \n “Tone from the top :” Is the executive team engaged? Do their \nactions match their words? \n• \n Security and privacy governance : Does it have the appropriate \ndecision-making structure, including the right level of “tension” \nbetween different stakeholders? Is it set up to ask the “high \ncontrast” questions (as discussed in Chapter  2 )? \n The CISO must take responsibility for determining which issues merit the board’s \nattention. That determination will depend on the potential impact of an exploit \nconducted against the company’s internal systems or technology-based products and \nservices.  \n \nC-I-S-O ATTRIBUTES\n In this chapter, I have covered a range of abilities and characteristics that the 21st \ncentury CISO requires. Many of these probably sound familiar, but it’s all too easy \nto forget them amid the demands of hectic daily schedules. I’ve found a good way \nto remind myself of some of the key attributes is simply to look at my job title. The \nletters in CISO help me remember that we all need Character, Intuition, Skills, and \nObjectivity. So if you’re struggling to remember all the details in this chapter, just \nremember you’re a CISO. You need Character to ensure your actions demonstrate \nintegrity; Intuition to anticipate what’s needed and act accordingly, taking risks when \nnecessary; Skills that span business, technology, and a wide variety of risk areas; \nand Objectivity in order to avoid falling prey to fear-mongering. \n\n\nCHAPTER 10 ■ THE 21ST CENTURY CISO\n153\n Conclusion \n As the technology environment continues to evolve, many people believe we’re moving \ntoward a future in which  organizations outsource much of the delivery of IT services. If \nthis trend continues, what does it mean for the CISO? \n In this view of the future, the organization shifts away from IT implementation to \nprocurement and management of suppliers and services, while setting direction and \nestablishing an overall IT architecture. \n In addition to this, the organization will need to retain the core competency of the \nsecurity group: the management of information risk. Essentially, organizations cannot \noutsource risk. We can hire companies to deliver our business systems, but we’re still \nresponsible for compliance with regulations that affect our companies, such as SOX \nand HIPAA. And if a breach results in theft or leakage of personal information or critical \nintellectual property, we’re still responsible for reporting it. Furthermore, we still suffer \nthe damage to our brand, even if the breach was due a failure of the supplier’s systems. \nAs regulations proliferate and more and more personal information is stored in business \nsystems, the risks can only increase. \n Therefore the CISO’s abilities will remain essential, even if the job title changes. The \norganization must retain the management of information risk as a core competency. \nAs CISOs, we are poised to continue providing that core competency as long as we can \neffectively work within this new environment by developing the abilities I’ve described \nin this chapter and throughout this book. These abilities enable us to work with others to \nsupport the Protect to Enable mission. \n I’ll close this chapter with an excerpt from a speech by Teddy Roosevelt; the \nsentiments seem as relevant today as when he made the speech back in 1910. “It is not \nthe critic who counts; not the man who points out how the strong man stumbles, or \nwhere the doer of deeds could have done them better. The credit belongs to the man who \nis actually in the arena, whose face is marred by dust and sweat and blood; who strives \nvaliantly; who errs, who comes short again and again, because there is no effort without \nerror and shortcoming; but who does actually strive to do the deeds; who knows great \nenthusiasms, the great devotions; who spends himself in a worthy cause; who at the best \nknows in the end the triumph of high achievement, and who at the worst, if he fails, at \nleast fails while daring greatly, so that his place shall never be with those cold and timid \nsouls who neither know victory nor defeat.” (Roosevelt 1910) \n We need to be in the arena, and so do our teams. Our mission, as information \nsecurity and privacy professionals, is a worthy cause. With our efforts to prevent harm to \nour organizations, our customers, and to society, we can ensure that tomorrow is better \nthan today. \n\n\n155\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_11\n CHAPTER 11 \n Performance Coaching \n If your actions inspire others to dream more, learn more, do more, and \nbecome more, you are a leader. \n —John Quincy Adams \n Over the years I have attended and taught many management and leadership classes. \nI have also received and written countless performance reviews. I have overseen the \nratings and reviews for literally thousands of employees, starting when I ran a call center \nfor a large retailer back in the late 80s, before I attended graduate school. One thing that is \nclear to me, after so many years participating in these annual and semi-annual corporate \nrituals, is that there is the potential for considerable ambiguity, particularly when \nassessing soft skills, those that cannot be measured using hard metrics such as the ability \nto meet deadlines or deliver revenue commitments. \n This ambiguity makes it hard for employees to understand how to meet their \nmanager’s expectations. It makes it hard for them to understand the factors that may \nbe limiting their progress from a junior player in the organization to a more senior role. \nI believe this ambiguity can be clarified, although there will probably always be some \nqualitative differences in perspective between employee and manager, and even among \ndifferent managers. \n For these soft skills, I believe performance  coaching needs to be emphasized \nover performance  management . This is because at many organizations, performance \nmanagement focuses primarily on promoting the fittest and eliminating the weak. The \nprocess looks at who is getting the best ratings and who is getting the worst. Managers \nthen work to remove the lowest performers from the organization. This selection process \nis a natural cycle, and one that should continue to play a role. However, I believe that \ncoaching can yield better long-term results, both for individuals and for the organization. \nCoaching focuses on helping everyone in the organization, including ourselves, reach \ntheir full potential. The ultimate goal is to create a high-performance organization in \nwhich everyone performs to the utmost of his or her ability. \n To effectively coach people, we need to be able to  define the soft skills that are \nrequired at each level of the progression from entry-level employee to executive. Then \nwe can coach them about how to acquire these skills and move up the organization. The \ntables in this chapter are intended to provide those definitions, to provide some clarity \nin these areas of potential ambiguity. They are based on tables that I have used, adapted, \ntested, and refined over many years in a wide variety of roles. Although I created the \n\n\nCHAPTER 11 ■ PERFORMANCE COACHING\n156\ntables for my own employees, the skills listed in the tables are not specific to information \nrisk professionals; they may be equally applicable to employees in other disciplines. \n The  soft skills in the tables generally describe  how people work, which can be almost \nas important to the organization as  what they do. How people behave and communicate \naffects not only their own ability to achieve goals but also the performance of those \naround them. An individual contributor who interacts poorly with others may impair \nthe performance of his or her team, and cause interpersonal problems that the team’s \nmanager has to spend time fixing. A senior manager who lacks these soft skills can have \nan even broader impact, hindering the performance of the organization. \n I have published older versions of these tables to my employees, in the belief that \nfeedback should be multi-directional and that leaders as well as employees should be \nmeasured using the same publicly available criteria. I have also shared these tables with \nindustry peers. I am providing them in this book in the hope that they will be beneficial \nto others, and that they will generate comments and feedback that I can use to improve \nfuture iterations of this living document. \n How to Use the  Tables \n Each of the 11 tables in this chapter focuses on a specific area of soft skills, such as \ninitiative, commitment, professionalism, or communication. Each table follows the same \nformat, with five columns representing the skills required at progressively higher levels \nof the organization, from junior employees to emerging executives. The leftmost two \ncolumns represent individual contributors: entry-level employees and more seasoned \nintermediate professionals. The rightmost three columns represent increasingly senior \nmanagement positions: a line manager responsible for a team; a senior manager who \nmay be responsible for multiple teams, each headed by a line manager; and a leader who \nis responsible for an entire information risk organization and should be able to work \ndirectly with the company’s board and top executives. \n As one might expect when discussing soft skills, this is not an exact science. \nThe columns show a progression, but they do not represent a precise scale, and there is \noverlap in some areas. An implicit assumption throughout the tables is that someone \nin a more senior role has already acquired the skills needed in less-senior positions \n(i.e. in the columns farther to the left). The skills required at more junior levels tend to \nbe more narrowly defined and constrained; those required at more senior levels tend to \nbe broader in scope, with more far-reaching impact. For these reasons, the tables may \nbe easiest to absorb by reading down the columns (to see all the skills for each role) \nrather than across the rows. \n Over the years, I have used these tables in various ways. I have used them to help \nemployees understand where they need to enhance their skills and abilities if they want \nto move up to more senior positions. I’ve also used them to help employees self-assess. \nHere are some examples of ways to use the tables in everyday  work situations :\n• \n An employee believes he or she should be promoted to a more \nsenior position. You ask them to assess their own skills in each \narea. You also do your own assessment of their skills. Then the \ntwo of you discuss any differences between those assessments, \nand pinpoint areas that the employee should work on in order to \nacquire the skills needed for a higher-level position. \n\n\nCHAPTER 11 ■ PERFORMANCE COACHING\n157\n• \n You provide an entry-level employee, enthusiastic but fresh out \nof college, with a roadmap of the skills they’ll need to acquire if \nthey want to progress to VP level in the future. This gives them a \npractical tool that they can use to guide their personal and career \ndevelopment.  \n• \n You use the tables to identify your own Achilles’ heel, the weak \nspot that hinders your progression to an executive level. You \nnotice that even though your skills mostly match those in the \nEmerging Executive column, the skills in a few areas correspond \nto those that you’d expect in a more junior manager. Those are \nskills that you need to improve. \n• \n During a coaching session with an employee, you count roughly \nhow many of their skills are already at the next most senior level, \nthe next column in the table. If 80% of their skills match, they may \nbe ready to move up. If there’s only a 20% match, they need to \nwork on bringing the rest of their skills up to scratch. \n The tables cover the following areas: independence and initiative, efficiency and \neffectiveness, commitment, professionalism, discipline, teamwork, problem-solving, \ncommunication skills, and goal-setting. \n Independence and Initiative \n This category, as its name suggests, is all about someone’s ability to act independently and \ntake the initiative. As you’d expect, the expectations increase dramatically as one progresses \nup the organization. An entry-level employee may require very specific direction for each \nnew task. A more experienced employee (Intermediate) should be able to define action \nplans and complete small projects with minimal supervision. A line manager should take \nresponsibility for leading his or her team. An emerging executive can deal with tough issues \nat executive level, and take responsibility for risky independent decisions that he or she \nbelieves are in the best interest of the organization. See Table  11-1 .\n Table 11-1.  Independence and Initiative \n Entry-level \n Intermediate \n Line Manager \n Senior Manager  Leader/Emerging \nExecutive \n Takes direction \nand turns it \ninto results; \nassumes \nownership of \ndeliverables \n Acts \nindependently \nwith a specific \ncharter \n Embraces role \nas manager \nto lead his/\nher team; sets \ndirection in \nsupport of \nhigher level \ngoals \n Seeks, \nidentifies, \nand solves \nproblems \nwhile taking \nresponsibility \nfor the \noutcome \n Makes risky \nindependent \ndecisions \nand takes \nresponsibility \nfor the outcome \n(continued)\n\n\nCHAPTER 11 ■ PERFORMANCE COACHING\n158\n Entry-level \n Intermediate \n Line Manager \n Senior Manager  Leader/Emerging \nExecutive \n Handles \nmultiple \nsimultaneous \ntasks with \nsome \nsupervision \n Responds \ncreatively to \ncustomer needs \n Effectively \nsummarizes \nand reports \nteam’s activity \n Takes \nunpopular \npositions and \nmakes them \nhappen \n Deals with \ntough issues, \nwith no “air \ncover,” at an \nexecutive staff \nlevel \n Identifies \nroadblocks \nand resolves or \nescalates \n Shapes problem \nstatements and \ndefines action \nplan to complete \nassignments \n Holds self \naccountable for \nwork he or she \ndoesn’t directly \ncontrol \n Can foresee and \ntake action on \nproblems that \ndo not yet exist \n Works with \nmanager to \nestablish \nworkload \npriorities, \nclarify \nexpectations, \nand get \nfeedback \n Requires \nonly minimal \ndirection for \nsmall projects \n Assumes \nresponsibility \nfor work \nthat requires \nattention, even \nif it is outside \ndirect scope of \nhis/her role \n Identifies \nvalue-added \nactivities and \nsometimes \ninitiates \nactions \n Seeks buy-in \nfrom manager on \nworkload timing \nand prioritization \n Drives risk \nand security \ncharter among \nother managers \nacross the \norganization \n \n Efficiency and Effectiveness \n Efficiency and effectiveness are both important, related skills. An efficient employee \nworks quickly and uses fewer resources. An effective employee is highly productive. A \ncompany that combines effectiveness and efficiency achieves better results faster, using \nfewer resources. Table  11-2 shows the progression from an entry-level employee’s ability \nto follow efficient processes to a manager’s ability to manage the resources of a group or \nan entire organization.\nTable 11-1. (continued) \n\n\nCHAPTER 11 ■ PERFORMANCE COACHING\n159\n Table 11-2.  Efficiency and Effectiveness \n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Works at \nconsistent and \npredictable \npace \n Schedules \nwork and \ncommunicates \ntimelines for \noutput \n Delegates \nappropriately; \ngets results by \nworking through \nothers and with \nothers \n Manages \nstrategic \nplanning and \norganizational \nscheduling, and \nmakes good \ntradeoffs for the \norganization \n Recognizes \nthat what you \nsay “no” to is as \nvital for driving \norganizational \nefficiency as \nwhat you say \n“yes” to \n Demonstrates \neffective \nwork habits \nenabling \ntimely \ncompletion of \ntasks \n Demonstrates \nability to \nmanage to \nmultiple \nwork items \nwith inter-\ndependencies \n Plans, schedules, \nand balances \nresources \namong projects \nto avoid crises \nand minimize \nfire fighting \n Manages \nadministrative \nresources to \nincrease personal \nefficiency \n Learns from \nmistakes \nand applies \nlearnings to \nsubsequent \ntasks \n Remains \ncalm and in \ncontrol of work \ndemands while \nmaintaining \nwork/life \n balance \n Devotes time \nto improving \ngroup’s \nefficiency \n Dispositions \nitems and issues \nquickly \n Works with \nmanager to \nprioritize \nworkload \n Understands \npriorities, \nplans \naccordingly, \nand makes \nreal-time \nadjustments \n Uses project \nmanagement \ntools and \nstakeholder \ninput to \nmaximize output \nand leverage \nresources \n Communicates, \nand \ndemonstrates \nthrough his/\nher own actions, \nthat people are \nrewarded for \nresults, not hours \nworked \n Begins to \nquestion \ntime spent \non routine \ntasks with low \nadded value \n Networks \nwith others \nto identify \nshortcuts and \n efficiencies \n Actively \nprioritizes by \nweeding and \nfeeding the \nproject list \nto maximize \norganizational \neffectiveness \n\n\nCHAPTER 11 ■ PERFORMANCE COACHING\n160\n Commitment \n Commitment reflects someone’s loyalty to the organization and their willingness \nto devote time and energy to the cause. In an entry-level employee, commitment is \ndemonstrated by personal work ethic and willingness to take on more work. As people \nmove up the organization, they demonstrate commitment by taking ownership of bigger \nissues and focusing on driving the best outcome for the organization. See Table  11-3 .\n Table 11-3.  Commitment \n Entry-level \n Intermediate \n Line Manager \n Senior Manager  Leader/Emerging \nExecutive \n Demonstrates \nstrong personal \nwork ethic \n Aligns \nindividual \ngoals with \norganizational \ngoals \n Drives issues for \nthe benefit of \nmultiple groups \nacross the \norganization \n Holds self \naccountable \nfor company’s \nperformance \n Becomes a \nrole model, \ndemonstrating \nstrong sense of \n“company first” \nwith the right \ncorporate social \nresponsibility \n Readily takes on \nmore workload \nwithin job \nscope \n Takes \nownership of \nproblems \n Recognizes \nwhat is \nbest for the \norganization \nversus what \nmight be \nbest for the \ndepartment \n Demonstrates \na high level \nof dedication \nand personal \ncommitment to \nthe success of \nall employees \n Tolerates the \nindirect control \nand influence \nthat result \nfrom matrix \nmanagement \n Answers \nthe specific \nquestions asked \n(doesn’t drift) \n Provides \ncomplete \nanswers to \nquestions; \nanticipates \ndoubtful areas \nand works \nto eliminate \nconcerns \n Demonstrates \ncommitment \nto work/\nlife balance: \ncreating a good \nhome life as \nwell as a good \nwork life \n Demonstrates \nthat growth \nnever stops and \nthat we all need \nto continually \nlearn in order to \nimprove \n Makes specific \nrequests for \nnecessary \ninformation; \nasks only for \nwhat is needed \n Knows when to \nquit on a losing \ndecision but \nwilling to risk \nself to do the \nright  thing \n Subordinates \nego to the needs \nof others and of \nthe company \n\n\nCHAPTER 11 ■ PERFORMANCE COACHING\n161\n Professionalism \n Professionalism is the extent to which someone demonstrates the attitudes, skills, and \nmethods required to execute their professional role. For an entry-level employee, this \nincludes adhering to established company policies. For senior managers, it involves \ndemonstrating broader and deeper adoption of the company’s values and principles. See \nTable  11-4 .\n Table 11-4.  Professionalism \n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Demonstrates \npride in his/her \ncraft \n Sets high \npersonal \nstandards \n Modifies \nbehavior \nto embrace \ncorporate \nvalues \n Demonstrates \nunquestioned \nconfidentiality \nand adherence \nto the \norganization’s \ncode of conduct, \nvalues, and \nprinciples \n Becomes a \nrole model \nexemplifying \ncorporate \nvalues, growth, \nconsistency, \nintegrity, \ncomposure, \nrespect for \nothers, and \naccountability \n Has a courteous \nand businesslike \nmanner, \ndemonstrating \nunderstanding \nof basic values, \nrole, and \nappropriate \nbehavior \n Holds self \naccountable \nfor his or her \nactions \n Matches \nactions with \nwords \n Demonstrates \nstrong integrity \nand motivation \nwith the most \nhonorable \nintentions \n Respects \nconfidentiality \nof information, \nwith strict \nadherence to \nconfidentiality \npolicies \n Maintains \ncomposure \nand is not \ndefensive \n Aggressively \nseeks feedback \nand coaching to \ngrow into a role \nmodel \n Discipline  \n Discipline is the ability to remain focused and execute consistently despite the many \ndistractions of everyday working life. As employees rise to higher-level positions, the \ndistractions and demands increase, requiring greater focus and discipline. See Table  11-5 .\n\n\nCHAPTER 11 ■ PERFORMANCE COACHING\n162\n Table 11-5.  Discipline \n Entry-level \n Intermediate \n Line Manager \n Senior Manager  Leader/Emerging \nExecutive \n Approaches \nwork in an \norderly fashion \n Consistently \nmaintains high \nstandards of \naccuracy and \nthoroughness \n Stays on point, \neven with heavy \ndistraction \n Demonstrates \nthe stamina \nand fortitude \nto remain \nfocused and \nnot succumb \nto premature \nconclusions \n Prevents the \norganization \nfrom getting \ndistracted \n Consistently \nmeets routine \ndeadlines and \nexecutes well \n Consistently \ndocuments \nintentions and \nresults \n Can discern \nurgency from \nimportance, \nand prioritize \naccordingly \n Understands \nthe value of \n“silver bullets” \nand uses them \nwisely \n Overcomes \nbasic snags \nand remains \nfocused to \nstay on course \nand deliver \nexpected \noutputs \n Does not \ninitiate or \nperpetuate \nwasteful \ncommunication \n Doesn’t waste \nenergy on \nrhetoric or \nreactions that \nlead to no \nmeaningful \nconclusions \n Demonstrates \nprogression \nto greater \ndiscipline over \ntime \n \n Teamwork \n Individuals must be able to recognize the need to work with others as a team, share \nexpertise, and take on suitable team roles. Managers need to create, inspire, and lead \nteams, utilizing each member’s talents in the best way. See Table  11-6 .\n\n\nCHAPTER 11 ■ PERFORMANCE COACHING\n163\n Table 11-6.  Teamwork \n Entry-level \n Intermediate \n Line Manager \n Senior Manager  Leader/Emerging \nExecutive \n Comprehends \nthe importance \nof teamwork \n Willingly shares \nknowledge \nand leverages \nexpertise with \nothers in team \n Recognizes \nand assembles \nappropriate \nteam players; \nencourages \ndiversity and \nutilizes each \nmember’s \nunique talents \n Sponsors and \nleads teams \nacross broad \nentities \n Commissions \nteams to solve \nbroad, long-term \nproblems \n Requires some \ncoaching on \nappropriate \nlevel of team \ninvolvement \n Independently \ndetermines \nand executes \nappropriate \nteam role \nand level of \ninvolvement \n Provides \ntraining and \ncoaching to his/\nher team \n Nurtures \nmultiple teams \nwithin an \norganization \n Becomes a key \nplayer within the \nexecutive team \n Actively engages \nteam members \nand others to \ngenerate win-\nwin  solution \n Inspires teams \nto achieve an \nextraordinary \nlevel of \nperformance \n Recognizes \nwhen a team \nneeds course \ncorrection \n Willing to \nmake personal \nsacrifices for the \nsake of the team \n Problem-Solving \n Problem-solving is an important skill for any information risk management professional. \nIndividual contributors need to be able to analyze and solve problems. Managers need to \nhelp their teams solve problems and focus on broader issues including those that involve \nother organizations. See Table  11-7 .\n",
      "page_number": 172
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 184-192)",
      "start_page": 184,
      "end_page": 192,
      "detection_method": "topic_boundary",
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n164\n Table 11-7.  Problem-solving \n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Solves \nproblems with \ncoaching \n Takes \nownership \nof problem \nresolution \n Coaches \nteams to solve \nproblems \n Resolves \ncomplex \nproblems across \nthe organization \n Resolves strategic \nproblems, \nparticularly those \ninvolving external \nparties \n Understands \ncause and \neffect \n Drives analysis \nof cost, benefit, \nrisk, and \nprobability of \nsuccess \n Identifies \nand resolves \nproblems \nnot obvious \nto others, \nincluding \nthose beyond \nhis/her \nprevious \nexperience \n Champions \nenduring \nimprovements \nthrough \nstructured \napproaches such \nas task forces \n Identifies \nproactive and \npredictive \nprocesses to \nidentify the \nconsequences \nand solve the \nproblems of \nbroad business \ninitiatives \n Uses available \nresources \nand solid \nmethodology to \nsolve problems \nwithin  charter  \n Uses \nconsultative \nand consensus \nprocesses with \nease \n Acts as role \nmodel for \ncommitment \nto previously \nagreed process \nimprovements \ndesigned to \nsystematically \nsolve problems \n Communication \n Good  communication helps organizations thrive. It is essential in almost any role, from \nentry-level team members who must communicate with their colleagues and managers \nto executives who must communicate messages to the entire organization. Because \ncommunications skills are so important, I’ve divided them into three areas, each with its \nown table: listening, style, and clarity. \n Listening \n Communication starts with  listening.  For junior employees, the ability to listen helps \ncreate a clear understanding of what’s required. More senior employees actively solicit \nmultiple viewpoints, listen for the meaning behind the words, and intercept emotional \noutpourings that can overwhelm a situation. See Table  11-8 .\n\n\nCHAPTER 11 ■ PERFORMANCE COACHING\n165\n Table 11-8.  Communication Skills: Listening \n Entry-level \n Intermediate \n Line Manager \n Senior Manager  Leader/Emerging \nExecutive \n Confirms \nunderstanding \n Listens to \nthe broader \nmeaning of \nwhat is being \nsaid, and seeks \nopportunities to \nadd value \n Hears \nfrustrations and \nseeks advice \nabout how to \nrespond \n Can hear \nbeyond \nemotion and \nrespond with \nmeaningful \ncommitments \nand actions \n Finds the \npractical \nsolution amid \nthe noise from \nteam members \nand executives \n Makes listening \nan overt activity \n Listens \nto others’ \nideas, and \nincorporates \nthem into \nthe work; \ndemonstrates \nrespect for \nothers by \nensuring their \nentire message \nis heard \n Seeks others’ \nperspectives \nand listens to \nall viewpoints \nand ideas; \nencourages \nmutual \nunderstanding \n Steps back \nduring debates \nand identifies \nthe key issues \n Can listen to \nstrong-willed \nor irrational \nrequests \nand provide \nappropriate \n direction  \n Listens and \nresponds to \ncustomers and \nstakeholders \n Adds \ninformation \nor perceptions \nto expand the \nconcept or the \nopportunity \n Reinforces \nunderstanding \nthrough active \nlistening; builds \nconfidence \nin others that \ntheir message \nis being heard \n Intercepts \nescalating \nemotion before \nit overwhelms \na situation \ninvolving other \nemployees or \ncustomers \n Before ending \nconversation, \nsummarizes \nconversation \nand achieves \nclosure and \nagreement \n Comes to \nmeetings \nprepared to \nreview the \ndata and \ncommunicate \ninformation in a \nlogical fashion \n Smoothly cross-\nreferences prior \nconversations \nto ensure truth \nand consistency \n Knows when it \nis better to listen \nthan to talk \n \n\n\nCHAPTER 11 ■ PERFORMANCE COACHING\n166\n Style \n How you communicate can be as important as  what you communicate. Each person’s \ncommunication style should develop to match their role as they progress through the \norganization. See Table  11-9 .\n Table 11-9.  Communication Skills: Style \n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Communicates \nwell with \nothers without \ncreating \nconfusion or \nunnecessary \nconflict \n Delivery of \nanalysis is \ncomprehensive, \ninstructive, \nand easily \nunderstood \n Recognizes the \nrequirements \nof each \nsituation and \nadapts style \naccordingly \n Demonstrates \npatience, \npersistence, \nand polish in \n communications \n Develops own \nmotivating style \n Responds \nwillingly and \ncapably to \ndirect verbal \nor written \nquestions \n Detects when \nsomeone is \ntrying to direct \nthem in a \nconversation \nand can follow \nas opposed to \nveering off track \n Remains \ncomposed \nunder pointed \nfire \n Maintains a \nprofessional \ndemeanor under \npressure; can \ndeflect “fire” \n Has perfect \ntiming; times \ncommunication \nfor maximum \nimpact \n Interactions \nwith others \nare viewed \npositively; \nother people \ndo not avoid \nworking with \nthis person \n Recognizes and \nis not deterred \nby different \ncommunication \nstyles \n Seeks and \nresponds \neffectively \nto feedback \non own \nmanagement \nbehavior \n Credibly \nresponds to \nquestions \nwhen he or she \ndoesn’t know \nthe answer; can \nbluff but remain \ndirectionally \ncorrect or say “I \ndon’t know, but \nI’ll find out” \n Can make and \ncommunicate \ndecisions on \nthe fly with \nhigh precision \nand without \ndisrupting other \nactivities \n Uses post-\nmortems \neffectively \n Says the right \nthing at the right \ntime \n Creates and \ndelivers “state \nof the union \naddresses” \nand “one voice \nresponses” \nfor medium-\nsized and large \norganizations \n(continued)\n\n\nCHAPTER 11 ■ PERFORMANCE COACHING\n167\n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n \n Knows when \nto tell (give \ndirection) \nversus lead, \nand does both \nthings well \n Clarity \n Clear communication helps ensure that information and ideas are accurately shared \nthroughout the organization. Experienced staff should be able to summarize data and \ncreate clarity from a confusing mass of information. Senior managers create consistent \nand clear messages for diverse audiences. See Table  11-10 .\n Table 11-10.  Communication Skills: Clarity \n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Keeps \nmessages clear \nand concise \n Focuses on \nand highlights \nkey points \n Tells the story, \nnot the facts; \ndelivers the core \nmeaning and the \nanswer (what \nactions to take) \nwhen appropriate \n Takes multiple \nmessages from \nvarious sources \nand reconstitutes \nor links them \ninto a larger, \nmore meaningful \nmessage \n Sends clear \nand consistent \nmessages to a \nbroad audience, \nincluding \nexternal parties \n Presents facts \naccurately, \nusing relevant \ndata \n Remains clear \nabout the goal \nand does not \nmeander—\nstays on point \n Draws summary \nconclusions from \nlarge amounts of \ninformation \n Creates \nconsistent and \nclear messages \ndespite complex \nscope of  material \n Helps people \nfrom different \nbackgrounds \nquickly grasp \ncomplex \nsubjects at a \nhigh level \n Independently \ndetermines \nareas that need \nclarity, and \nseeks and adds \nappropriate \ndetails \n Demonstrates \nawareness \nof target \naudience, \nand tailors \nmessage \naccordingly \n Brings clarity \nto complex \nsituations; \nasks the right \nquestions to lead \nthe conversation \nto results, and \navoids stating \nopinion up front \n Lean \ncommunication: \nuses the \nminimum \nnumber of words \nto express a point \n Brings clarity \nto issues \nacross multiple \norganizations \nwho may have \nopposing \ninterests \nTable 11-9. (continued) \n(continued)\n\n\nCHAPTER 11 ■ PERFORMANCE COACHING\n168\n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Keeps work \nneat and well \norganized \n Is aware when \nhe or she has \nconfused senior \nmanagement \n Does not confuse \nexecutive \nmanagement \n Goal-Setting \n All experienced staff should be able  to  identify and set goals, from line managers setting \ngoals for their team to leaders defining the organization’s mission. See Table  11-11 .\n Table 11-11.  Goal-Setting \n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Drafts \nindividual \ngoals and \nreviews with \nmanager for \napproval \n Identifies \nand declares \nopportunities \n Set goals for team; \nensures goals \nare clear and \nstated in terms of \nmeasurable results \n Sets strategic as \nwell as tactical \ngoals \n Creates \nmissions \n Presents \ncompelling \ndata to support \nrecommended \ngoals \n Aligns goals and \nexpectations \nwith upper \nmanagement \n Demonstrates \nability to set goals \nwhen starting \nwith a blank sheet \n Challenges \nself, staff, and \npeers to take \non increasingly \nhigher leverage \nobjectives \n Anticipates \nneeds and \nrequirements \n Provides a degree \nof focus on \nstrategic issues; \ndemonstrates \nvision in areas of \nexpertise \n Can drive an \norganization \nto articulate \ncommitments, \nmaintain focus, \nadjust priorities, \nand raise the  bar \n Can drive \nconsensus on \nvision \n Fosters innovation \nand creative \nthinking; \nencourages \ndiscussion and \nfeedback in \nsetting goals \n Challenges \nexisting \nparadigms and \nexplores new \npossibilities \n Helps others \nmake the \nconnection \nbetween the \nvision and the \ndeliverables \nnecessary to \nachieve the \nhigher goals \nTable 11-10. (continued) \n\n\nCHAPTER 11 ■ PERFORMANCE COACHING\n169\n Conclusion \n I believe that performance coaching focused on soft skills can help everyone in \nthe organization achieve their full potential, and thus contribute to the creation of a \nhigh-performance organization. I’d like to conclude by examining what makes a manager \nan effective performance coach. A good performance  coach \n• \n Develops and mentors managers and other employees, managing \npeople to higher expectations and greater results. \n• \n Stretches others and themselves to achieve beyond the norm, and \nrejects mediocrity. \n• \n Creates more key players than he or she consumes, becoming a \nnet developer of people for the organization. \n• \n Holds people accountable for results and coaches them to \nachieve those results. \n• \n Distinguishes motion from progress, and separates the means \nfrom the end. \n• \n Responds positively to feedback about his or her own behavior as \na manager or individual. \n• \n Is sought out to provide performance coaching to senior players \nwho report to other managers. \n• \n Handles tough conversations with employees about their \nbehavior or performance crisply, without creating a litigation risk. \n• \n Saves senior players from self-destructing or falling short of their \npotential. \n• \n Demonstrates empathy and can save employees who are \nstruggling due to work-related or personal reasons and might \notherwise leave the organization. \n\n\n171\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8\n APPENDIX A \n References \n Accenture. 2012.  Accenture Technology Vision 2012 .  http://www.accenture.com/us-en/\ntechnology/technology-labs/Pages/insight-accenture-technology-vision-2012.aspx . \n Ahamad, Mustaque. 2011.  Georgia Tech Releases Cyber Threats Forecast for 2012 . \nComment in Georgia Tech press release.  http://www.scs.gatech.edu/content/\ngeorgia-tech-releases-cyber-threats-forecast-2012 . \n Alexander, Aileen and Jamey Cummings. 2016. “The Rise of the Chief Information \nSecurity Officer.”  People + Strategy 39:1. \n Alperovitch, Dmitri. 2012. Comment in  Georgia Tech Emerging Cyber Threats \nReport 2012.  http://www.iisp.gatech.edu/sites/default/files/documents/2016_\ngeorgiatech_cyberthreatsreport_onlinescroll.pdf . \n Amabile, Teresa M., and Steven J. Kramer. 2011. “The Power of Small Wins.”  Harvard \nBusiness Review 89:5. \n Ashford, Warwick. 2015. “Security researchers disclose flaws in Kaspersky \nand FireEye products.”  ComputerWeekly.com. September 7, 2015.  http://www.\ncomputerweekly.com/news/4500253029/Security-researchers-disclose-flaws-\nin-Kaspersky-and-FireEye-products . \n Bazerman , Max H. and  Ann E. Tenbrunsel . 2011.  Blind Spots: Why We Fail to Do \nWhat’s Right and What to Do about It. Princeton: Princeton University Press. \n Ben-Shalom, Omer, Manish Dave, Toby Kohlenberg, Dennis Morgan, Stacy Purcell, \nAlan Ross, Timothy Verrall, and Tarun Viswanathan. 2011. “Rethinking Information \nSecurity to Improve Business Agility.” Intel Corporation.  http://www.intel.com/Assets/\nPDF/whitepaper/Rethinking_Information_Security_Improve_Business_Agility.pdf . \n Bonus, Angelia. 2010. “Pennsylvania school district settles laptop privacy lawsuit.” \nCNN report, October 12, 2010.  http://www.cnn.com/2010/CRIME/10/12/pennsylvania.\nschool.webcams.settlement/ . \n Bradley, Tony. “Run Cylance Infinity OEM engine on Raspberry Pi.”  TechSpective \narticle posted online October 15, 2015.  https://techspective.net/2015/10/15/\nrun-cylance-infinity-oem-engine-on-raspberry-pi/ . \n Breakwell, Glynis. 2007.  The Psychology of Risk . Cambridge, UK: Cambridge \nUniversity Press. \n Brinkmann, Paul. 2015. “Orlando Health reports data breach for 3,200 patients.” \n Orlando Sentinel. Published online July 2, 2015.  http://www.orlandosentinel.\ncom/business/brinkmann-on-business/os-orlando-health-data-breach-20150702-\npost.html . \n\n\nAPPENDIX A ■ REFERENCES\n172\n Brito, Jerry and Tate Watkins. 2012. “Loving the Cyber Bomb? The Dangers of Threat \nInflation in Cybersecurity Policy.”  Harvard Law School National Security Journal, 3:39–83. \n Buckley, Chris. 2015. “China Passes Antiterrorism Law That Critics Fear May Overreach. \n New York Times. December 27, 2015.  http://www.nytimes.com/2015/12/28/world/asia/\nchina-passes-antiterrorism-law-that-critics-fear-may-overreach.html . \n Buczek, Laurie and Malcolm Harkins. 2009. “Developing an Enterprise Social \nComputing Strategy.” Intel Corporation.  http://www.intel.com/content/dam/doc/white-\npaper/intel-it-developing-enterprise-social-computing-strategy-paper.pdf . \n Business Continuity Institute. 2016. “Cyber attack top business threat for second \nyear running.” Press release published February 8, 2016. \n Carty, Matt, Vincent Pimont, and David W. Schmid. 2012. “Measuring the Value \nof Information Security Investments.” Intel Corporation.  http://www.intel.com/\ncontent/dam/www/public/us/en/documents/best-practices/information-security-\ninvestments-paper.pdf . \n Casey, Timothy. 2007. “Threat Agent Library Helps Identify Information Security \nRisks.” Intel Corporation.  http://www.intel.com/it/pdf/threat-agent-library.pdf . \n Casey, Tim and Brian Willis. 2008. “Wargames: Serious Play that Tests Enterprise \nSecurity Assumptions.” Intel Corporation.  http://www.intel.com/it/pdf/Wargames-\nSerious_Play_that_Tests_Enterprise_Security_Assumptions.pdf . \n Chelel, Kit. 2015. “A London Hedge Fund Lost $1.2 Million in a Friday Afternoon \nPhone Scam.”  Bloomberg Business. Published online July 7, 2015.  http://www.bloomberg.\ncom/news/articles/2015-07-07/friday-afternoon-scam-cost-hedge-fund-1-2-\nmillion-and-cfo-s-job . \n Chirgwin, Richard. “Intel’s SGX security extensions: Secure until you look at the \ndetail.”  The Register. February 1, 2016.  http://www.theregister.co.uk/2016/02/01/\nsgx_secure_until_you_look_at_the_detail/ . \n Christensen, Clayton M. 1997.  The Innovator’s Dilemma: When New Technologies \nCause Great Firms to Fail . Boston, Mass.: Harvard Business School Press. \n Cisco Systems, Inc. 2011a.  Cisco Connected World Technology Report 2011 .  http://\nwww.cisco.com/en/US/netsol/ns1120/index.html . \n Cisco Systems, Inc. 2011b.  Email Attacks: This Time It’s Personal.  http://www.\ncisco.com/en/US/prod/collateral/vpndevc/ps10128/ps10339/ps10354/targeted_\nattacks.pdf . \n Cisco Systems, Inc. 2015.  2015 Corporate Social Responsibility Report.  http://www.\ncisco.com/assets/csr/pdf/CSR_Report_2015.pdf . \n Cisco Systems 2015b.  Internet Of Things Will Deliver $1.9 Trillion Boost To Supply \nChain And Logistics Operations. Cisco Systems, Inc. press release. \n Clark , Sandy,  Stefan Frei ,  Matt Blaze ,  Jonathan Smith . 2010. “Familiarity \nBreeds Contempt: The Honeymoon Effect and the Role of Legacy Code in \nZero-Day Vulnerabilities.” In  Proceedings of the 26th Annual Computer Security \nApplications Conference. New York: Association for Computing Machinery. doi: \n10.1145/1920261.1920299. \n Colgan, William B. 2010 . Allied Strafing in World War II: A Cockpit View of Air to \nGround Battle. Jefferson, NC: McFarland. \n Compeau, Joseph, Nicole Haggerty, Ramasastry Chandrasekhar. 2013.  Intel \nCorp. - Bring Your Own Device. Ivey Business School case study. \n\n\nAPPENDIX A ■ REFERENCES\n173\n Corporate Executive Board Company, The (CEB). 2012. Information Risk Executive \nCouncil. Arlington, VA.  http://www.executiveboard.com/exbd/information-\ntechnology/it-risk/index.page . \n Corporate Executive Board Company, The (CEB). 2015. CEB March 2015 Information \nRisk Peer Perspective Survey. \n Covey, Stephen M. R. with Rebecca R. Merrill. 2008.  The Speed of Trust: The One \nThing That Changes Everything. New York: Free Press. \n CSO Magazine, US Secret Service, Software Engineering Institute CERT Program \nat Carnegie Mellon University, Deloitte. 2011.  2011 CyberSecurity Watch Survey: \nOrganizations Need More Skilled Cyber Professionals To Stay Secure. Press release. \n http://www.sei.cmu.edu/newsitems/cybersecurity_watch_survey_2011.cfm . \n Culp, Scott. 2010.  10 Immutable Laws of Security. Microsoft Corporation. \n http://technet.microsoft.com/library/cc722487.aspx . \n CWE/SANS. 2011.  CWE/SANS TOP 25 Most Dangerous Software Errors.  http://cwe.\nmitre.org/top25/ . \n Department of Justice. 2014. “U.S. Charges Five Chinese Military Hackers for \nCyber Espionage Against U.S. Corporations and a Labor Organization for Commercial \nAdvantage.” Press release. May 9, 2014. \n Department of Telecommunications, Government of India. 2009. Instructions to \nInternet service providers. Letter dated February 23, 2009, No. 820-1/2008-DS Pt. II. \n Edelman. 2015. 2015 Edelman Trust Barometer.  http://www.edelman.com/\ninsights/intellectual-property/2015-edelman-trust-barometer/ . \n Edwards, Cliff, Olga Kharif, and Michael Riley. 2011. “Human Errors Fuel Hacking \nas Test Shows Nothing Stops Idiocy.” Bloomberg News. Posted June 27, 2011.  http://\nwww.bloomberg.com/news/2011-06-27/human-errors-fuel-hacking-as-test-shows-\nnothing-prevents-idiocy.html . \n European Commission. 2011 . ePrivacy Directive: circumstances, procedures and \nformats for personal data breach notifications.  http://ec.europa.eu/information_\nsociety/policy/ecomm/doc/library/public_consult/data_breach/ePrivacy_\ndatabreach_consultation.pdf . \n European Commission. 2012.  Proposal for a regulation of the European Parliament \nand of the Council on the protection of individuals with regard to the processing of personal \ndata and on the free movement of such data (General Data Protection Regulation ). \n http://ec.europa.eu/justice/data-protection/document/review2012/com_\n2012_11_en.pdf . \n European Network and Information Security Agency (ENISA). 2010.  Incentives and \nChallenges for Information Sharing in the Context of Network and Information Security. \n http://www.enisa.europa.eu/activities/Resilience-and-CIIP/public-private-\npartnership/information-sharing-exchange/incentives-and-barriers-to-\ninformation-sharing . \n Evered, Rob and Jerzy Rub. 2010. “Maintaining Information Security while Allowing \nPersonal Hand-held Devices in the Enterprise.” Intel Corporation.  http://www.intel.com/\nAssets/PDF/whitepaper/Maintaining_Info_Security_Allowing_Personal_Hand_Held_\nDevices_Enterprise.pdf . \n Federal Trade Commisssion. 2016. “Dental Practice Software Provider Settles FTC \nCharges It Misled Customers About Encryption of Patient Data.” Press release issued \nJanuary 5, 2016.  https://www.ftc.gov/news-events/press-releases/2016/01/dental-\npractice-software-provider-settles-ftc-charges-it-misled . \n",
      "page_number": 184
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 193-202)",
      "start_page": 193,
      "end_page": 202,
      "detection_method": "topic_boundary",
      "content": "APPENDIX A ■ REFERENCES\n174\n Fleming, Virgil and Naoyuki Tomizawa. 2012. “Intel IT: Keeping the Business \nRunning in a Crisis.” Intel Corporation.  http://media12.connectedsocialmedia.com/\nintel/03/7906/Intel_IT_Keeping_Business_Running_in_Crisis.pdf . \n Fong, David, Toby Kohlenberg, and Justin Philips. 2010. “Enterprise Security Benefits \nof Microsoft Windows 7.” Intel Corporation.  http://www.intel.in/content/dam/www/\npublic/us/en/documents/case-studies/intel-it-windows-7-upgrade-security-\nbrief.pdf . \n Food and Drug Administration. 2016. “FDA outlines cybersecurity recommendations \nfor medical device manufacturers.” FDA press release.  http://www.fda.gov/NewsEvents/\nNewsroom/PressAnnouncements/ucm481968.htm . \n Fox-Brewster, Thomas. 2015. “Darkode Shutdown: FireEye Intern Accused Of \nCreating $65,000 Android Malware.”  Forbes . Published online July 15, 2015.  http://www.\nforbes.com/sites/thomasbrewster/2015/07/15/fireeye-intern-dendroid-charges/ . \n Fox-Brewster, Thomas. 2016. “As Ransomware Crisis Explodes, Hollywood Hospital \nCoughs Up $17,000 In Bitcoin.”  Forbes . Published online February 18, 2016. \n http://www.forbes.com/sites/thomasbrewster/2016/02/18/ransomware-hollywood-\npayment-locky-menace/ . \n Frier, Sarah. 2014. “Twitter CFO Noto Has an Oops Moment With Mistaken \nTweet.”  Bloomberg Business . November 24, 2014.  http://www.bloomberg.com/news/\narticles/2014-11-25/twitter-cfo-noto-has-an-oops-moment-with-mistaken-tweet . \n Fulford, Charles. 2015. “Retail Is About to Be Reinvented, Driven by Digital \nTechnologies.”  Advertising Age. August 28, 2015.  http://adage.com/article/\ndigitalnext/retail-reinvented/300129/ . \n Gartner, Inc. 2005. “Gartner Survey Shows Spending for Compliance and Corporate \nGovernance to Account for 10-15 Percent of an Enterprise’s 2006 IT Budget.” Gartner Inc. \nPress release.  http://www.gartner.com/press_releases/asset_141532_11.html . \n Gartner, Inc. 2011a. “Gartner Says Context-Aware Technologies Will Affect $96 Billion \nof Annual Consumer Spending Worldwide by 2015.” Gartner Inc. Press release.  http://\nwww.gartner.com/it/page.jsp?id=1827614 . \n Gartner, Inc. 2011b. “Gartner Identifies the Top 10 Strategic Technologies for 2012.” \nGartner Inc. Press release.  http://www.gartner.com/it/page.jsp?id=1826214 . \n Gartner, Inc. 2015. “Gartner Says 6.4 Billion Connected “Things” Will Be in Use in \n2016, Up 30 Percent From 2015.” Gartner Inc. Press release.  http://www.gartner.com/\nnewsroom/id/3165317 . \n Gartner, Inc. 2015b. “Gartner Says It’s Not Just About Big Data; It’s What You Do \nWith It: Welcome to the Algorithmic Economy.” Gartner Inc. Press release.  http://www.\ngartner.com/newsroom/id/3142917 . \n Gentile, Mary. 2010.  Giving Voice to Values. New Haven: Yale University Press \n Gladwell, Malcolm. 2005.  Blink: The Power of Thinking Without Thinking . New York: \nLittle, Brown & Co. \n Global Industry Analysts. 2015.  Context Aware Computing – A Global Strategic \nBusiness Report. Global Industry Analysts, Inc. research report.  http://www.strategyr.\ncom/MarketResearch/Context_Aware_Computing_CAC_Market_Trends.asp . \n Goodin, Dan. 2015. “‘Unauthorized code’ in Juniper firewalls decrypts \nencrypted VPN traffic.”  Ars Technica . December 17, 2015.  http://arstechnica.com/\nsecurity/2015/12/unauthorized-code-in-juniper-firewalls-decrypts-encrypted-\nvpn-traffic/ . \n\n\nAPPENDIX A ■ REFERENCES\n175\n Goodman, Mark.  Future Crimes: Everything Is Connected, Everyone Is Vulnerable and \nWhat We Can Do About It. New York: Doubleday. \n Greenberg, Andy. 2015. “Hackers Remotely Kill a Jeep on the Highway—With Me in \nIt.”  Wired . Posted July 21, 2015.  http://www.wired.com/2015/07/hackers-remotely-\nkill-jeep-highway/ . \n Greenberg, Andy. 2015b. “After Jeep Hack, Chrysler Recalls 1.4M Vehicles for Bug \nFix.”  Wired . Posted July 24, 2015.  http://www.wired.com/2015/07/jeep-hack-chrysler-\nrecalls-1-4m-vehicles-bug-fix/ . \n Gutierrez, Esteban, Toby Kohlenberg, Sridhar Mahankali, and Bill Sunderland. 2012. \n“Virtualizing High-security Servers in a Private Cloud.” Intel Corporation.  http://www.\nintel.de/content/dam/www/public/us/en/documents/best-practices/virtualizing-\nhigh-security-servers.pdf . \n Hamblen, Matt. 2016. “At CES, Feds prod companies to expand privacy efforts.” \nComputerworld January 6, 2016.  http://www.computerworld.com/article/3019832/\ndata-privacy/at-ces-feds-prod-companies-to-expand-privacy-efforts.html . \n Information Risk Executive Council. 2011.  Security Controls Maturity Benchmark \nSummary . Information published in  2011-2012 Intel IT Performance Report . Intel \nCorporation.  http://www.intel.com/content/dam/www/public/us/en/documents/\nbest-practices/intel-it-annual-performance-report-2011-12.pdf . \n Intel Corporation. 2010. Form 10-Q for the quarterly period ended March 27, 2010; \nFiled May 3, 2010.  http://www.intc.com/secfiling.cfm?filingID=950123-10-42822 . \n Intel Corporation. 2011.  Worldwide Device Estimates Year 2020—Intel One Smart \nNetwork Work. \n Intel Corporation. 2012a. “Thinking Differently About IT Value: 2011-2012 Intel \nIT Performance Report.”  http://www.intel.com/content/dam/www/public/us/en/\ndocuments/best-practices/intel-it-annual-performance-report-2011-12.pdf . \n Jackson Higgins, Kelly. 2010. “‘Operation Aurora’ Changing the Role of the CISO.” \n Dark Reading. March 16, 2010.  http://www.darkreading.com/attacks-breaches/\noperation-aurora-changing-the-role-of-the-ciso/d/d-id/1133225 . \n Joffe-Walt, Chana and Alix Spiegel. 2012. “Psychology Of Fraud: Why Good People \nDo Bad Things.” National Public Radio broadcast. Transcript accessed online May 28, \n2012.  http://www.npr.org/2012/05/01/151764534/psychology-of-fraud-why-good-\npeople-do-bad-things . \n Johnson, Steven. 2010.  Where Good Ideas Come From: The Natural History of \nInnovation . New York: Riverhead Books, a subsidiary of Penguin Books (USA). \n Johnson, Steven. 2010. Talk at TEDGlobal 2010.  http://www.ted.com/talks/\nsteven_johnson_where_good_ideas_come_from.html . \n Keteyian, Armen. 2010. “Digital Photocopiers Loaded With Secrets.” CBS News \narticle posted online April 20, 2010.  http://www.cbsnews.com/2100-18563_162-\n6412439.html . \n Kouzes, James and Barry Z. Posner. 2012.  The Leadership Challenge. San Francisco: \nJossey-Bass, an imprint of John Wiley & Sons. \n KPMG International. 2015.  The KPMG Survey of Corporate Responsibility Reporting. \nNovember 2015 \n Kupperwasser, Yosef. 2007.  Lessons from Israel’s Intelligence reforms. The Brookings \nInstitution.  http://www.brookings.edu/~/media/research/files/papers/2007/10/\nintelligence%20kuperwasser/10_intelligence_kuperwasser.pdf . \n\n\nAPPENDIX A ■ REFERENCES\n176\n Lambert, Leslie. 2015. “User behaviors can expose bad actors before it’s \ntoo late.” CSO article posted online July 27, 2015.  http://www.csoonline.com/\narticle/2951814/cyber-attacks-espionage/what-can-we-learn-from-jpmorgans-\ninsider-breaches.html . \n Lea, Ruth. “Corporate Social Responsibility: IoD Member Opinion Survey.” The \nInstitute of Directors, November 2002. \n Leon, Fred. 2011. “Securing Intel’s External Online Presence.” Intel Corporation. \n http://www.intel.com/content/dam/doc/white-paper/intel-it-securing-intels-\nexternal-online-presence-paper.pdf . \n Levin, Carl. 2010. Opening Statement of Senator Carl Levin, Senate Armed Services \nCommittee Hearing on Nominations of Vice Admiral James A. Winnefeld and Lieutenant \nGeneral Keith B. Alexander. \n Lindstrom, Pete. 2008. “Five Immutable Laws of Virtualization Security.” Burton \nGroup blog entry posted online January 08, 2008.  http://srmsblog.burtongroup.\ncom/2008/01/five-immutable.html . \n Loechner, Jack. 2015. “IoT Connected Devices Triples To 38 Billion By 2020.” Center \nfor Media Research brief. August 27, 2015.  http://www.mediapost.com/publications/\narticle/256678/iot-connected-devices-triples-to-38-billion-by-202.html . \n LosHuertos, Gary. 2010. “Herding Firesheep in New York City” Blog entry posted \nonline October 27, 2010.  http://money.cnn.com/2010/12/14/technology/firesheep_\nstarbucks/ . \n MarketsandMarkets. 2013.  Context Aware Computing Market - Global Advancements, \nEmerging Applications, Worldwide Forecasts and Analysis (2013 - 2018). MarketsandMarkets \nresearch report.  http://www.marketsandmarkets.com/PressReleases/context-aware-\ncomputing.asp . \n Massachusetts Institute of Technology Sloan School Center for Information Systems \nResearch. 2012. IT Governance.  http://cisr.mit.edu/research/research-overview/\nclassic-topics/it-governance/ . \n Maynard, Micheline. 2014. “‘The GM Nod’ And Other Cultural Flaws Exposed By \nThe Ignition Defect Report.”  Forbes, June 5, 2014.  http://www.forbes.com/sites/\nmichelinemaynard/2014/06/05/ignition-switch-report-spares-ceo-barra-but-\nexposes-gms-culture/#2715e4857a0b3ac65828408a . \n McAfee, Inc. 2011. Press release.  McAfee Q2 2011 Threats Report Shows Significant \nGrowth for Malware on Mobile Platforms.  http://www.mcafee.com/us/about/\nnews/2011/q3/20110823-01.aspx . \n McComb, Michael. 2002. “Profit to Be Found in Companies That Care,”  South China \nMorning Post . April 14, 2002. \n Miller, Ron and Joe Varga. 2011. “Benefits of Enabling Personal Handheld Devices \nin the Enterprise.”  http://www.intel.com/content/dam/doc/best-practices/inte-\nit-it-leadership-benefits-of-enabling-personal-handheld-devices-in-the-\nenterprise-practices.pdf . \n Millman, Rene. “Updated: 97% of malicious mobile malware targets Android.” \n SC Magazine UK. June 26, 2015.  http://www.scmagazineuk.com/updated-97-of-\nmalicious-mobile-malware-targets-android/article/422783/ . \n Morgan, Steve. 2015. “Cybersecurity job market to suffer severe workforce shortage.” \n CSO. July 28, 2015.  http://www.csoonline.com/article/2953258/it-careers/\ncybersecurity-job-market-figures-2015-to-2019-indicate-severe-workforce-\nshortage.html . \n\n\nAPPENDIX A ■ REFERENCES\n177\n Nakashima, Ellen and Matt Zapotosky. “U.S. charges Iran-linked hackers with \ntargeting banks, N.Y. dam.”  Washington Post. March 24, 2016. \n Nelson, Patrick. 2016. “3D printers wide-open to hacking.”  Network World . March 8, \n2016.  http://www.networkworld.com/article/3041436/security/3d-printers-wide-\nopen-to-hacking.html . \n Nest Labs. 2012. Nest Learning Thermostat web site.  http://www.nest.com/ . \n Paganini, Pierluigi. 2014. “Firmware vulnerability allows man-in-the-middle attack \nusing SD Memory cards.”  The Hacker News. January 01, 2014.  http://thehackernews.\ncom/2014/01/firmware-vulnerability-allows-man-in.html . \n Pagliery, Jose. “Chryslers can be hacked over the Internet.”  CNNMoney. July 22, 2015. \n http://money.cnn.com/2015/07/21/technology/chrysler-hack . \n Pauli, Darren. 2015a. “Thousands of ‘directly hackable’ hospital devices exposed \nonline.”  The Register . September 29 2015.  http://www.theregister.co.uk/2015/09/29/\nthousands_of_directly_hackable_hospital_devices_found_exposed/ . \n Pauli, Darren. 2015b. “‘10-second’ theoretical hack could jog Fitbits into \nmalware-spreading mode.”  The Register . October 21 2015.  http://www.theregister.\nco.uk/2015/10/21/fitbit_hack/ . \n Perch Interactive. 2015. Perch Interactive web site.   www.perchinteractive.com/  . \n Perlroth, Nicole. 2011. “Insurance Against Cyber Attacks Expected to Boom.”  New \nYork Times blog post December 29, 2011.  http://bits.blogs.nytimes.com/2011/12/23/\ninsurance-against-cyber-attacks-expected-to-boom/ . \n PhishLabs. 2016.  2016 Phishing Trends & Intelligence Report: Hacking the Human. \n https://pages.phishlabs.com/2016-Phishing-Trends-and-Intelligence-Report-\nHacking-the-Human_PTI.html . \n Rajab, Moheeb Abu, Lucas Ballard, Panayiotis Marvrommatis, Niels Provos, \nand Xin Zhao. 2010. “The Nocebo Effect on the Web: An Analysis of Fake Anti-Virus \nDistribution.” In  Large-Scale Exploits and Emergent Threats . Usenix.  http://static.\ngoogleusercontent.com/external_content/untrusted_dlcp/research.google.com/\nen/us/pubs/archive/36346.pdf . \n Retail Cyber Intelligence Sharing Center. 2015. “Retailers Launch Comprehensive \nCyber Intelligence Sharing Center.” Press release published May 14, 2015.  https://\nr-cisc.org/2015/05/14/retailers-launch-comprehensive-cyber-intelligence-\nsharing-center/ . \n Rice, David. 2007.  Geekonomics: The Real Cost of Insecure Software. Boston: Addison-\nWesley Professional. \n Rifkin, Jeremy.  The Third Industrial Revolution: How Lateral Power Is Transforming \nEnergy, the Economy, and the World. New York: St Martin’s Griffin. \n Roosevelt, Theodore. 1910. “The Man in the Arena.” Speech at the Sorbonne, Paris, \nFrance. April 23, 1910.  http://www.theodore-roosevelt.com/images/research/\nspeeches/maninthearena.pdf . \n Rose, Charlie. 2015. “Inside Apple, Part 2.”  60 Minutes interview, December 20, \n2015. CBS Interactive.  http://www.cbsnews.com/news/60-minutes-apple-tim-cook-\ncharlie-rose/ . \n Schmidt, Eric and Jared Cohen. 2015. “Inventive artificial intelligence will make all \nof us better.”  Time . December 21, 2015.  http://time.com/4154126/technology-essay-\neric-schmidt-jared-cohen/ . \n\n\nAPPENDIX A ■ REFERENCES\n178\n Schmidt, Michael, Nicole Perlroth and Matthew Goldstein. “F.B.I. Says Little \nDoubt North Korea Hit Sony.”  New York Times. January 7, 2015.  http://www.nytimes.\ncom/2015/01/08/business/chief-says-fbi-has-no-doubt-that-north-korea-\nattacked-sony.html . \n Scott, Mark. 2016. “U.S. and Europe in ‘Safe Harbor’ Data Deal, but Legal Fight \nMay Await.”  New York Times. February 2, 2016.  http://www.nytimes.com/2016/02/03/\ntechnology/us-europe-safe-harbor-data-deal.html?_r=1 . \n Seidman, Dov. 2011. “Measuring HOW We Do Business.”  Forbes article posted \nonline November 27, 2011.  http://www.forbes.com/sites/dovseidman/2011/11/27/\nmeasuring-how-we-do-business/ . \n Shey, Heidi. 2014. “Pet The Unicorns And Think Of Protecting Customer Data As A \nCorporate Social Responsibility.” Forrester Research blog post April 23, 2014.  http://\nblogs.forrester.com/heidi_shey/14-04-23-pet_the_unicorns_and_think_of_\nprotecting_customer_data_as_a_corporate_social_responsibility . \n Silverman, Rachel Emma. 2012. “Facebook and Twitter Postings Cost CFO His Job.” \n Wall Street Journal article posted online May 14, 2012.  http://www.wsj.com/articles/SB\n10001424052702303505504577404542168061590 . \n Sinek, Simon. 2009.  Start with Why: How Great Leaders Inspire Everyone to Take \nAction. New York: Portfolio. \n Slovic, Paul. 2010.  The Feeling of Risk: New Perspectives on Risk Perception. New York: \nRoutledge. \n Solove, Daniel J. 2011.  Nothing to Hide: The False Tradeoff between Privacy and \nSecurity. Yale University Press. \n Sunderland, Bill and Ajay Chandramouly. 2011. “Overcoming Security Challenges \nto Virtualize Internet-facing Applications.” Intel Corporation.  http://www.intel.com/\ncontent/dam/www/public/us/en/documents/white-papers/cloud-security-and-\nsecure-virtualization-paper.pdf . \n Taleb, Nassim Nicholas. 2007.  The Black Swan: The Impact of the Highly Improbable. \nNew York: Random House. \n Thaler, Richard H. Thaler and Cass R. Sunstein. 2008.  Nudge: Improving Decisions \nAbout Health, Wealth, and Happiness. New Haven, CT: Yale University Press. \n Thomson, Iain. 2015. “Lottery IT security boss guilty of hacking lotto computer to \nwin $14.3m.”  The Register. Published online July 22, 2015.  http://www.theregister.\nco.uk/2015/07/22/lotto_infosec_director_guilty/ . \n Tode, Chantal. “Macy’s peps up Black Friday shopping via beacon-triggered mobile \ngame.”  Mobile Commerce Daily. Published online October 30, 2015 \n Verizon. 2015.  2015 Data Breach Investigations Report .  http://www.\nverizonenterprise.com/DBIR/2015/ . \n US Department of Justice (DoJ). 2014. “Justice Department, Federal Trade Commission \nIssue Antitrust Policy Statement on Sharing Cybersecurity Information.” Press release \nissued April 10, 2014.  http://www.justice.gov/opa/pr/justice-department-federal-\ntrade-commission-issue-antitrust-policy-statement-sharing . \n US Environmental Protection Agency (EPA). 2011. “Oil Pollution Act Overview.” \n https://www.epa.gov/laws-regulations/summary-oil-pollution-act . \n US Government Accountability Office (GAO). 2012. “Challenges in Securing the \nModernized Electricity Grid.”  http://www.gao.gov/products/GAO-12-507T . \n\n\nAPPENDIX A ■ REFERENCES\n179\n US Securities and Exchange Commission. 2011. CF Disclosure Guidance: Topic \nNo. 2. Issued October 13, 2011.  http://www.sec.gov/divisions/corpfin/guidance/\ncfguidance-topic2.htm . \n Van Derbeken, Jaxon. “S.F. officials locked out of computer network.”  San Francisco \nChronicle . Published online Tuesday, July 15, 2008.  http://www.sfgate.com/bayarea/\narticle/S-F-officials-locked-out-of-computer-network-3205200.php . \n Venables, Philip. 2008. Speech at RSA Conference 2008. \n Websense. Inc. 2015. “Research: Penalties, Punishment & Prison for Serious Data \nBreaches say e-Crime Congress Respondents.” Press release, March 23 2015.  https://\ncommunity.websense.com/blogs/websense-news-releases/archive/2015/03/23/\nresearch-penalties-punishment-amp-prison-for-serious-data-breaches-say-e-\ncrime-congress-respondents.aspx . \n Weil, Peter and Jeanne W. Ross. 2004.  IT Governance: How Top Performers Manage IT \nDecision Rights for Superior Results . Boston, Mass.: Harvard Business School Press. \n Willis, Brian. 2012. “Sharing Cyber-Threat Information: An Outcomes-based \nApproach.” Intel Corporation.  https://www.sbs.ox.ac.uk/cybersecurity-capacity/\nsystem/files/Intel%20-%20Sharing%20Cyber-Threat%20Information.pdf . \n World Business Council for Sustainable Development. 2007.  Corporate Social \nResponsibility: Meeting changing expectations. \n Worral, Bob. 2015. “Important Announcement about ScreenOS.” Juniper Networks \nsecurity announcement. Posted online December 17, 2015.  http://forums.juniper.\nnet/t5/Security-Incident-Response/Important-Announcement-about-ScreenOS/ba-\np/285554 . \n Zander, Rosamund Stone and Benjamin Zander. 2000.  The Art of Possibility: \nTransforming Professional and Personal Life . Boston, Mass.: Harvard Business School \nPress. \n\n\n181\n© Malcolm W. Harkins 2016\nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8\n \n \n \n  A \n Advanced persistent threats (APTs) , 13 \n Architecture . See also  Security architecture \n balanced controls \n anti-malware technology , 114–115 \n defi nition , 106 \n detective and preventative \ncontrols , 114 \n intrusion prevention systems , 113 \n security analytics , 113 \n employee productivity , 105 \n hardware-enforced security , 105 \n security zones , 109 \n architecture , 111–112 \n critical data and resources , 109 \n defi nition , 106 \n devices and application types , 109 \n PEPs , 110 \n selective zones , 111 \n trusted zones , 111 \n untrusted zones , 110–111 \n user’s device and location , 112 \n traditional enterprise trust model , 105 \n trust calculation \n access type , 106 \n allow access , 108 \n available controls , 108 \n business partners , 109 \n defi nition , 105 \n destination score , 108 \n devices and usage models , 106 \n internal and external resources , 109 \n policy decision point (PDP) , 108 \n source score , 107 \n trust calculation , 108 \n user and data perimeters \n defenses and detective control , 115 \n Internet of Th ings (IoT) , 116 \n protect information , 115 \n security , 116 \n traditional network security , 115 \n user and data perimeters defi nition , 106 \n Arizona Counter Terrorism Intelligence \nCenter (ACTIC) , 59 \n Arizona Cyber Th reat Response Alliance \n(ACTRA) , 59 \n Assessment models , 25 \n \n \n \n  B \n Business benefi ts and risks \n baseline security \n AI-based security and \nautomation , 125–126 \n encryption , 125 \n enhanced recovery , 125 \n hardware acceleration , 125 \n hardware-enforced , 124 \n protected environments , 124 \n building security , 123 \n context-aware security \n business intelligence and data \nprotection , 127 \n cloud security and context \nawareness , 126 \n experiences , 123 \n image recognition technology , 126 \n portable devices , 126 \n sensors and analytical tools , 126 \n contextual information , 123 \n mass-production strategy , 123 \n Index \n\n\n■ INDEX\n182\n \n \n \n  C \n Chief information security offi  cer (CISO) \n chief trust offi  cer , 139–140 \n foundational skills , 142 \n junk food fear , 144 \n leader , 148 \n business leaders , 149–150 \n information risk-board \nlevel , 151–152 \n operational goal , 148 \n values , 150–151 \n Z-shaped individual , 151 \n managing risk , 146 \n organizations outsource , 153 \n positive approach , 145–146 \n sixth sense , 147 \n speed of trust , 148 \n storyteller , 143–144 \n T-shaped individuals , 141 \n Z-shaped individuals , 141 \n Chief Information Security \nOffi  cer (CISO) , 35 \n Chief Security and Privacy \nOffi  cer (CSPO) , 35–36 \n Coaching performances \n commitment , 160 \n communication , 164 \n clear communication , 167 \n goal-setting , 168 \n listening , 164–165 \n style , 166–167 \n defi nition , 155 \n discipline , 161–162 \n effi  ciency and eff ectiveness , 158–159 \n independence and \ninitiative , 157–158 \n management , 155 \n problem-solving , 163–164 \n professionalism , 161 \n soft skills , 156, 169 \n tables \n soft skills , 156 \n work situations , 156–157 \n teamwork , 162–163 \n Context-aware computing , 119 \n Corporate governance model , 36 \n Corporate social responsibility (CSR) , 130 \n defi nitions , 130 \n maintaining society trust , 134 \n managing information \n act , 137 \n events , 135 \n interpret , 137 \n risk professional and ethical \nstandards , 135 \n security issues , 136 \n sense , 137 \n managing information risk , 135 \n scope of , 130 \n technology \n control and impacts , 132 \n ethical implications , 133 \n evolution , 132 \n march of , 132 \n motivations , 133 \n potential impacts , 132 \n public-sector organizations , 133 \n real-life attack , 133 \n technology and business \nprofessionals , 129 \n treat information risk , 131 \n Cybersecurity legislation , 8 \n \n \n \n  D \n Distributed denial-of-service (DDoS) \nthreats , 21 \n \n \n \n  E \n Emerging security capabilities \n accelerated encryption , 119 \n artifi cial intelligence , 122 \n business benefi ts and risks (see \n Business benefi ts and risks ) \n cloud computing , 122 \n consistent experience across \ndevices , 121 \n context-aware computing and \nsecurity , 119 \n data analytics , 122 \n enterprise information security , 120 \n hardware-enforced protection , 119 \n information security , 120 \n Internet of Th ings (IoT) , 120 \n enabled car , 119 \n Moore’s law , 120 \n nest learning thermostat , 121 \n NFC , 120 \n RFID , 120 \n wireless NFC , 120 \n\n\n■ INDEX\n183\n malicious purposes , 119 \n organization’s privacy commitment , 119 \n security professionals , 119 \n shopper’s smartphone , 121 \n Enterprise resource planning (ERP) \nsystem , 111 \n External partnerships \n advantage of , 55 \n ACTRA , 59 \n attacks and threads , 58 \n benchmarks \n information , 61 \n operations , 60 \n CISO , 60 \n communities , 57 \n characteristics , 57 \n goals , 59 \n constitutes valuable information , 58 \n corporate citizenship , 63 \n enabling informal exchanges , 60 \n FIRST , 61 \n information sharing , 49 \n legal implications and revealing \nsecurity , 50 \n partnerships , 55 \n private-sector organizations , 58 \n public-relations aspect , 50 \n regional communities , 58 \n regulations and standards , 62 \n security landscape , 63 \n share security information , 50 \n share threat information , 58 \n technology landscape , 51 \n threat landscape , 51 \n tiered pyramid model \n confi dential tier , 53–55 \n information-sharing \nrelationships , 53–54 \n partnerships tier , 54 \n public tier , 53, 55 \n targeted tier , 53–54 \n types of , 52 \n value of , 51 \n vulnerabilities information and \nthreats , 59 \n \n \n \n  F \n Federal Trade Commission (FTC) , 136 \n Forum for Incident Response and Security \nTeams (FIRST) , 61 \n \n \n \n  G \n Global Positioning System (GPS) , 96, 108 \n Governance \n dictatorial approach , 33 \n information risk , 32 \n IT policies , 33 \n life-threatening consequences , 33 \n MIT CISR , 32–33 \n structure \n archetypes , 34, 90 \n corporate information , 36 \n CSPO , 36 \n engagements , 36 \n hybrid governance model , 35 \n monitor (sense) risk , 35 \n operations , 36 \n oversight , 35 \n security and privacy , 35 \n \n \n \n  H \n Health Insurance Portability and \nAccountability Act (HIPAA) , 9 \n Human resources (HR)-related processes \n capabilities , 31 \n information risks , 31 \n internal partnership , 31 \n key areas , 32 \n security team , 32 \n technology transitions , 31 \n \n \n \n  I, J, K, L \n Information security \n business enable ® , 5 \n balancing act , 6 \n core competencies , 5 \n legal and human resources (HR) \ngroups , 6 \n personal smartphones , 6 \n primary variables , 7 \n responsibilities grew , 6 \n transformation , 6 \n trust , 8 \n tuned to target , 6–7 \n wireless network implementation , 6 \n businesses and organization , 3 \n central nervous system , 3 \n fast-moving environment , 4 \n interdependent risks , 4 \n\n\n■ INDEX\n184\n management risk , 16 \n dynamic and fl exible , 16 \n incorporate privacy and \nregulatory compliance , 16 \n network boundary , 16 \n marketers , 4 \n perfect storm , 1–2 \n rapid proliferation (information and \ndevices) , 12 \n regulatory environment , 3 \n regulatory flood (see  Regulatory fl ood ) \n security and privacy , 5 \n smartphones , 3 \n technology , 3 \n threat landscape (see  Th reat \nlandscape ) \n traditional mission and vision , 2 \n Information Sharing and Analysis Centers \n(ISACs) , 60 \n Information Sharing and Analysis \nOrganizations (ISAOs) , 50 \n Insider threats , 78 \n detect , 79 \n deter , 79 \n discipline , 80 \n organization’s reputation , 78 \n security fi rms , 78 \n three-part approach , 79 \n Intel IT Emergency Response Process \n(ITERP) , 47 \n Intel Secure External Presence (ISEP) , 41 \n Internal partnerships \n business group managers , 46 \n corporate risk management , 44 \n corporate security , 45 \n far-reaching web , 37 \n fellow travelers , 38 \n fi nance group \n business groups , 43 \n internal audit , 44 \n SOX , 43 \n human resources \n communications , 42 \n procedures , 42 \n internal investigations , 43 \n security policies , 42 \n information security group \nand teams , 37 \n ITERP , 47 \n legal groups , 38 \n business groups , 40 \n contracts , 39 \n data classifi cation , 39 \n fi nancial compliance , 40 \n intellectual property , 39 \n ISEP-like process , 41 \n litigation , 39 \n privacy , 38 \n privacy , 45 \n response process , 46–47 \n risk review boards , 37 \n standing committees , 37 \n Internet of Th ings (IoT) , 94, 120 \n enabled car , 119 \n Moore’s law , 120 \n nest learning thermostat , 121 \n NFC , 120 \n RFID , 120 \n technologies , 3 \n wireless NFC , 120 \n Irrefutable Laws . See  Information Security, 14 \n \n \n \n  M \n Massachusetts Institute of Technology \nCenter for Information Systems \nResearch (MIT CISR) , 32 \n Misperception . See  Risk misperception \n Moore’s law , 120 \n \n \n \n  N \n National Institute of Standards and \nTechnology (NIST) , 144 \n Near fi eld communications (NFC) , 120 \n Non-Intel managed systems (NIMS) , 20 \n \n \n \n  O \n Organization’s privacy commitment , 119 \n \n \n \n  P, Q \n Perimeters \n balance fi nding , 80 \n changing behavior , 69 \n compliance/commitment , 66, 68 \n hypothetical web sites , 76 \n insider threats , 78 \nInformation security (cont.)\n",
      "page_number": 193
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 203-208)",
      "start_page": 203,
      "end_page": 208,
      "detection_method": "topic_boundary",
      "content": "■ INDEX\n185\n detect , 79 \n deter , 79 \n discipline , 80 \n organization’s reputation , 78 \n security fi rms , 78 \n three-part approach , 79 \n interactive training tool , 70 \n personal devices , 74 \n phishing attacks , 66 \n risk examination , 68–69 \n roundabouts and stop signs , 75–77 \n security awareness \n awareness model , 74 \n CEB , 71 \n employee awareness programs , 72 \n encourage users and news , 72 \n four-stage model , 72 \n international association , 73 \n judgment and commitment , 72 \n telecommunications fi rm , 72 \n tracking report , 73 \n training , 71 \n security benefi ts , 74–75 \n shifting perimeter , 65 \n social-engineering techniques , 66 \n technology professionals , 77 \n traffi  c-control method , 75 \n training , 69 \n user interactions , 66 \n Playing war games , 90 \n Policy decision point (PDP) , 108 \n Policy enforcement points (PEPs) , 110 \n Product life cycle model \n clustering areas , 87 \n commodity-source code , 84 \n critical trends , 86 \n disruptive trends , 87 \n emerging trends , 86 \n evolution of threats , 84 \n highest-priority threats , 83 \n security-related activity , 86 \n smartphone security threats , 85 \n sustained drivers , 86 \n threat analysis materials , 86 \n threat report , 87–88 \n Product security incident response \nprocesses (PSIRT) , 45 \n Public-private partnerships (PPPs) , 58 \n \n \n \n  R \n Radio Frequency Identifi cation (RFID) \ntechnology , 120 \n References , 171 \n Regulatory fl ood \n cyber-security legislation , 8 \n e-discovery , 11 \n fi nancial regulations , 10 \n high-tech exports , 8 \n IT capabilities , 8 \n personalization  vs. privacy , 9 \n protecting personal information , 9 \n scope , 11 \n storage and protection , 8 \n Retail environment , 118 \n Risk misperception , 17 \n assessment models , 25 \n communication \n asymmetry information , 27 \n building credibility , 28 \n laptops , 28 \n pirating software , 27 \n risk perceptions changing , 26 \n decision makers , 23 \n employees , 18 \n inevitable bias , 25 \n lure of the shiny bauble , 20 \n mitigate , 24 \n perception , 18 \n economic and psychological \nfactors , 18 \n organization security \nposture , 18 \n security professionals , 18 \n social-media site , 18 \n security professionals , 20 \n alert fatigue , 21 \n DDoS threats , 21 \n history of , 21 \n NIMS , 20 \n scrubbing data , 21 \n security and privacy , 22 \n set and forget error and security \ncontrols , 21 \n target fi xation , 20 \n threat controls , 23 \n untrusted devices , 20 \n\n\n■ INDEX\n186\n \n \n \n  S \n Sarbanes-Oxley Act (SOX) , 10, 43 \n Security architecture \n 9 box of controls , 101 \n business needs , 103 \n cloud computing , 104 \n control architecture , 100 \n IT consumerization , 102–103 \n novel approaches , 100 \n privacy and regulatory requirements , 105 \n threat landscape , 104 \n threat management , 100 \n Sharing security information . See  External \npartnerships \n Shifting perimeter , 65 \n Smartphones , 3, 96 \n security threats , 85 \n web applications , 97 \n Systems development life cycle (SDLC) , 45 \n \n \n \n  T, U, V, W, X, Y, Z \n Tailored Access Operations (TAO) , 89 \n Technology professionals , 77 \n Th reat landscape \n APTs , 13 \n cybercrime online , 13 \n irrefutable laws , 14 \n spearphishing attacks , 13 \n stealthy malware , 13 \n Th reats and vulnerabilities \n Malware industry , 94 \n often-confl icting information , 82 \n rhetoric , 81 \n smartphones , 96 \n structured methods \n analyzing emerging threats , 82 \n blinkered security \nperspective , 82 \n playing war games , 90 \n product life cycle (see  Product life \ncycle model ) \n risk-sensing analysis and \nstrategy , 82–83 \n security team , 82 \n threat landscape , 83 \n threat landscape \n barriers , 92 \n broad-brush picture , 91 \n edge case insecurity , 92 \n obscurity , 93 \n phishing , 91 \n smartphones , 91 \n social engineering attacks , 92 \n threats and identify risks , 88–89 \n web, attack surface \n embedded devices , 95 \n industrial control systems , 95 \n IoT , 95 \n nontraditional devices , 94 \n Trusted platform module (TPM) , 108 \n",
      "page_number": 203
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "Managing Risk \nand Information \nSecurity\nProtect to Enable\n—\nSecond Edition\n—\nMalcolm W Harkins \n",
      "content_length": 97,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 2,
      "content": " Managing Risk and \nInformation Security \n Protect to Enable  \n Second Edition \n Malcolm W. Harkins\n \n",
      "content_length": 102,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "Managing Risk and Information Security: Protect to Enable\nMalcolm W. Harkins \n \n \n \nFolsom, California, USA  \n \nISBN-13 (pbk): 978-1-4842-1456-5  \nISBN-13 (electronic): 978-1-4842-1455-8\nDOI 10.1007/978-1-4842-1455-8\nLibrary of Congress Control Number: 2016949414\nCopyright © 2016 by Malcolm W. Harkins\nApressOpen Rights: You have the right to copy, use and distribute this Work in its entirety, electronically without \nmodification, for non-commercial purposes only. However, you have the additional right to use or alter any source \ncode in this Work for any commercial or non-commercial purpose which must be accompanied by the licenses in \n(2) and (3) below to distribute the source code for instances of greater than 5 lines of code. Licenses (1), (2) and (3) \nbelow and the intervening text must be provided in any use of the text of the Work and fully describes the license \ngranted herein to the Work.\n(1) License for Distribution of the Work: This Work is copyrighted by Malcolm Harkins, all rights reserved. Use \nof this Work other than as provided for in this license is prohibited. By exercising any of the rights herein, you \nare accepting the terms of this license. You have the non-exclusive right to copy, use and distribute this English \nlanguage Work in its entirety, electronically without modification except for those modifications necessary for \nformatting on specific devices, for all non-commercial purposes, in all media and formats known now or hereafter. \nWhile the advice and information in this Work are believed to be true and accurate at the date of publication, \nneither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions \nthat may be made. The publisher makes no warranty, express or implied, with respect to the material contained \nherein.\nIf your distribution is solely Apress source code or uses Apress source code intact, the following licenses (2) and (3) \nmust accompany the source code. If your use is an adaptation of the source code provided by Apress in this Work, \nthen you must use only license (3).\n(2) License for Direct Reproduction of Apress Source Code: This source code, from Intel® Trusted Execution \nTechnology for Server Platforms, ISBN 978-1-4302-6148-3 is copyrighted by Apress Media, LLC, all rights reserved. \nAny direct reproduction of this Apress source code is permitted but must contain this license. The following license \nmust be provided for any use of the source code from this product of greater than 5 lines wherein the code is \nadapted or altered from its original Apress form. This Apress code is presented AS IS and Apress makes no claims \nto, representations or warrantees as to the function, usability, accuracy or usefulness of this code.\n(3) License for Distribution of Adaptation of Apress Source Code: Portions of the source code provided are used \nor adapted from Intel® Trusted Execution Technology for Server Platforms, ISBN 978-1-4302-6148-3 copyright \nApress Media LLC. Any use or reuse of this Apress source code must contain this License. This Apress code is made \navailable at Apress.com/9781484214565 as is and Apress makes no claims to, representations or warrantees as to \nthe function, usability, accuracy or usefulness of this code.\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark symbol with every \noccurrence of a trademarked name, logo, or image we use the names, logos, and images only in an editorial fashion \nand to the benefit of the trademark owner, with no intention of infringement of the trademark. The use in this \npublication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is \nnot to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.\nWhile the advice and information in this book are believed to be true and accurate at the date of publication, \nneither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions \nthat may be made. The publisher makes no warranty, express or implied, with respect to the material contained \nherein.\nCover image designed by Freepik.\nManaging Director: Welmoed Spahr\nLead Editor: Robert Hutchinson\nDevelopment Editor: James Markham\nEditorial Board: Steve Anglin, Pramila Balen, Aaron Black, Louise Corrigan, Jonathan Gennick, Robert \nHutchinson, Celestin Suresh John, Nikhil Karkal, James Markham, Susan McDermott, Matthew Moodie, \nNatalie Pao, Gwenan Spearing\nCoordinating Editor: Melissa Maldonado\nCopy Editor: Mary Behr\nCompositor: SPi Global\nIndexer: SPi Global\nArtist: SPi Global\nDistributed to the book trade worldwide by Springer Science+Business Media New York, \n233 Spring Street, 6th Floor, New York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505, e-mail  \n orders-ny@springer-sbm.com   , or visit  www.springer.com   . Apress Media, LLC is a California LLC \nand the sole member (owner) is Springer Science + Business Media Finance Inc (SSBM Finance Inc). \nSSBM Finance Inc is a Delaware corporation. \nFor information on translations, please e-mail  rights@apress.com   , or visit  www.apress.com   . \nApress and friends of ED books may be purchased in bulk for academic, corporate, or promotional use. eBook \nversions and licenses are also available for most titles. For more information, reference our Special Bulk Sales–eBook \nLicensing web page at  www.apress.com/bulk-sales   .\nAny source code or other supplementary materials referenced by the author in this text is available \nto readers at  www.apress.com   . For detailed information about how to locate your book’s source code, go to  www.\napress.com/source-code/   .\nPrinted on acid-free paper\n",
      "content_length": 5761,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "iii\n About ApressOpen \n What Is ApressOpen? \n• \n ApressOpen is an open access book program that publishes \nhigh-quality technical and business information. \n• \n ApressOpen eBooks are available for global, free, \nnoncommercial use. \n• \n ApressOpen eBooks are available in PDF, ePub, and Mobi formats. \n• \n The user friendly ApressOpen free eBook license is presented on \nthe copyright page of this book. \n",
      "content_length": 404,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": " This book is dedicated to my family. \n",
      "content_length": 39,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "vii\nContents at a Glance\nForeword ......................................................................................... xv\nPraise for the second edition of Managing Risk and \nInformation Security ...................................................................... xvii\nAbout the Author ............................................................................ xxi\nAcknowledgments ........................................................................ xxiii\nPreface ...........................................................................................xxv\n \n■Chapter 1: Introduction .................................................................. 1\n \n■Chapter 2: The Misperception of Risk .......................................... 17\n \n■ Chapter 3: Governance and Internal Partnerships: \nHow to Sense, Interpret, and Act on Risk ..................................... 31\n \n■ Chapter 4: External Partnerships: The Power of Sharing \nInformation ................................................................................... 49\n \n■Chapter 5: People Are the Perimeter ............................................ 65\n \n■ Chapter 6: Emerging Threats and Vulnerabilities: \nReality and Rhetoric ..................................................................... 81\n \n■ Chapter 7: A New Security Architecture to Improve \nBusiness Agility ............................................................................ 99\n \n■ Chapter 8: Looking to the Future: Emerging \nSecurity Capabilities .................................................................. 117\n",
      "content_length": 1577,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "■ CONTENTS AT A GLANCE\nviii\n \n■ Chapter 9: Corporate Social Responsibility: The Ethics of \nManaging Information Risk ........................................................ 129\n \n■Chapter 10: The 21st Century CISO ............................................ 139\n \n■Chapter 11: Performance Coaching ............................................ 155\n \n■Appendix A: References .............................................................. 171\nIndex .............................................................................................. 181\n",
      "content_length": 547,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "ix\nContents\nForeword ......................................................................................... xv\nPraise for the second edition of Managing Risk and \nInformation Security ...................................................................... xvii\nAbout the Author ............................................................................ xxi\nAcknowledgments ........................................................................ xxiii\nPreface ...........................................................................................xxv\n \n■Chapter 1: Introduction .................................................................. 1\nProtect to Enable® ................................................................................... 5\nBuilding Trust ............................................................................................................8\nKeeping the Company Legal: The Regulatory Flood .................................................8\nThe Rapid Proliferation of Information, Devices, and Things ..................................12\nThe Changing Threat Landscape ............................................................................13\nA New Approach to Managing Risk ........................................................................16\n \n■Chapter 2: The Misperception of Risk .......................................... 17\nThe Subjectivity of Risk Perception ....................................................... 18\nHow Employees Misperceive Risk ......................................................... 18\nThe Lure of the Shiny Bauble..................................................................................20\nHow Security Professionals Misperceive Risk ...................................... 20\nSecurity and Privacy ...............................................................................................22\nHow Decision Makers Misperceive Risk ............................................... 23\n",
      "content_length": 1976,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "■ CONTENTS\nx\nHow to Mitigate the Misperception of Risk ........................................... 24\nUncovering New Perspectives During Risk Assessments .......................................25\nCommunication Is Essential .................................................................. 26\nBuilding Credibility .................................................................................................28\n \n■ Chapter 3: Governance and Internal Partnerships: \nHow to Sense, Interpret, and Act on Risk ..................................... 31\nInformation Risk Governance ................................................................ 32\nFinding the Right Governance Structure ............................................... 34\nBuilding Internal Partnerships ............................................................... 37\nLegal .......................................................................................................................38\nHuman Resources ..................................................................................................42\nFinance ...................................................................................................................43\nCorporate Risk Management ..................................................................................44\nPrivacy ....................................................................................................................45\nCorporate Security ..................................................................................................45\nBusiness Group Managers ......................................................................................46\nConclusion ............................................................................................. 47\n \n■ Chapter 4: External Partnerships: The Power of Sharing \nInformation ................................................................................... 49\nThe Value of External Partnerships ....................................................... 51\nExternal Partnerships: Types and Tiers .................................................. 52\n1:1 Partnerships .....................................................................................................55\nCommunities ...........................................................................................................57\nCommunity Characteristics ....................................................................................57\nCommunity Goals ....................................................................................................59\nSharing Information about Threats and Vulnerabilities ...........................................59\nSharing Best Practices and Benchmarking ............................................................60\n",
      "content_length": 2808,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "■ CONTENTS\nxi\nInﬂ uencing Regulations and Standards ..................................................................62\nCorporate Citizenship .............................................................................................63\nConclusion ............................................................................................. 63\n \n■Chapter 5: People Are the Perimeter ............................................ 65\nThe Shifting Perimeter .......................................................................... 65\nCompliance or Commitment? ................................................................ 66\nExamining the Risks .............................................................................. 68\nAdjusting Behavior ................................................................................ 69\nA Model for Improving Security Awareness .......................................... 71\nBroadening the Awareness Model ......................................................... 74\nThe Security Beneﬁ ts of Personal Use .................................................. 74\nRoundabouts and Stop Signs ................................................................ 75\nThe Technology Professional ................................................................. 77\nInsider Threats....................................................................................... 78\nDeter .......................................................................................................................79\nDetect .....................................................................................................................79\nDiscipline ................................................................................................................80\nFinding the Balance ............................................................................... 80\n \n■ Chapter 6: Emerging Threats and Vulnerabilities: \nReality and Rhetoric ..................................................................... 81\nStructured Methods for Identifying Threat Trends ................................. 82\nThe Product Life Cycle Model .................................................................................83\nUnderstanding Threat Agents .................................................................................88\nPlaying War Games .................................................................................................90\nTrends That Span the Threat Landscape ............................................... 91\nTrust Is an Attack Surface .......................................................................................91\nBarriers to Entry Are Crumbling ..............................................................................92\n",
      "content_length": 2783,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "■ CONTENTS\nxii\nThe Rise of Edge Case Insecurity ...........................................................................92\nThe Enemy Knows the System ...............................................................................93\nKey Threat Activity Areas ....................................................................... 94\nThe Industry of Malware .........................................................................................94\nThe Web Expands to the Internet of Things ........................................... 94\nSmartphones ......................................................................................... 96\nWeb Applications ....................................................................................................97\nConclusion ............................................................................................. 97\n \n■ Chapter 7: A New Security Architecture to Improve \nBusiness Agility ............................................................................ 99\nThe 9 Box of Controls, Business Trends, and \nArchitecture Requirements ................................................................. 101\n9 Box of Controls ..................................................................................................101\nIT Consumerization ...............................................................................................102\nNew Business Needs ............................................................................................103\nCloud Computing ..................................................................................................104\nChanging Threat Landscape .................................................................................104\nPrivacy and Regulatory Requirements..................................................................105\nNew Architecture ................................................................................. 105\nTrust Calculation ...................................................................................................106\nSecurity Zones ......................................................................................................109\nBalanced Controls.................................................................................................113\nUsers, Data, and the Internet of Things: The New Perimeters ..............................115\nConclusion ........................................................................................... 116\n \n■Chapter 8: Looking to the Future: Emerging \nSecurity Capabilities ...................................................................... 117\nInternet of Things ................................................................................ 120\nConsistent User Experience Across Devices ....................................... 121\n",
      "content_length": 2852,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "■ CONTENTS\nxiii\nCloud Computing ................................................................................. 122\nBig Data Analytics ............................................................................... 122\nArtiﬁ cial Intelligence ........................................................................... 122\nBusiness Beneﬁ ts and Risks ............................................................... 123\nNew Security Capabilities .....................................................................................123\nBaseline Security ..................................................................................................124\nContext-Aware Security ........................................................................................126\nConclusion ........................................................................................... 127\n \n■ Chapter 9: Corporate Social Responsibility: The Ethics of \nManaging Information Risk ........................................................ 129\nThe Expanding Scope of Corporate Social Responsibility ................... 130\nThe Evolution of Technology and Its Impact ........................................ 132\nMaintaining Society’s Trust ................................................................. 134\nThe Ethics of Managing Information Risk ........................................... 135\nConclusion ........................................................................................... 137\n \n■Chapter 10: The 21st Century CISO ............................................ 139\nChief Trust Ofﬁ cer ................................................................................ 139\nThe Z-Shaped Individual...................................................................... 141\nFoundational Skills .............................................................................. 142\nBecoming a Storyteller ........................................................................ 143\nFear Is Junk Food ................................................................................ 144\nAccentuating the Positive .....................................................................................145\nDemonstrating the Reality of Risk ....................................................... 146\nThe CISO’s Sixth Sense ....................................................................... 147\nTaking Action at the Speed of Trust ......................................................................148\nThe CISO as a Leader .......................................................................... 148\nLearning from Other Business Leaders ................................................................149\n",
      "content_length": 2706,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "■ CONTENTS\nxiv\nVoicing Our Values ................................................................................................150\nDiscussing Information Risk at Board Level .........................................................151\nConclusion ........................................................................................... 153\n \n■Chapter 11: Performance Coaching ............................................ 155\nHow to Use the Tables ......................................................................... 156\nIndependence and Initiative .................................................................................157\nEfﬁ ciency and Effectiveness .................................................................................158\nCommitment .........................................................................................................160\nProfessionalism ....................................................................................................161\nDiscipline  .............................................................................................................161\nTeamwork .............................................................................................................162\nProblem-Solving ...................................................................................................163\nCommunication .....................................................................................................164\nGoal-Setting ..........................................................................................................168\nConclusion ........................................................................................... 169\n \n■Appendix A: References .............................................................. 171\nIndex .............................................................................................. 181\n",
      "content_length": 1910,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "xv\n Foreword \n Security and first-person shooter video games have one obvious thing in common: if \nyou’re not continuously moving, you’re dead. In this second edition of  Managing Risk \nand Information Security , Malcolm Harkins helps us move our thinking into areas of risk \nthat have become more prominent over the last several years. \n Because there is so much new content in this edition, I will focus on a topic that has \nrisen to greater prominence since the first edition: people are the perimeter. When we \nreflect on what has changed in recent years, with an eye to the vulnerabilities that result \nin real-world compromises, a pattern emerges: virtually all the major breaches that we \nhave seen involve manipulation of people. When nearly everyone has heard of phishing, \nwe have to ask ourselves: why is it still such an effective tool? \n The obvious theory is that we haven’t managed people risk as well as we should. \nPerhaps we have been standing still and need to learn how to dodge and experiment \nwith the way we drive better people-security outcomes. Unfortunately, the path is not \n100% clear. Unlike technology, the field of influencing human behavior in security is \nremarkably complicated and supported by limited research. \n Malcolm provides us with a great foundation and framework to build our \n“security engagement” functions. I like to use the word “engagement” because it \nspeaks to how the security organization relates to the workforce in a manner that isn’t \nsimply bounded by the more traditional term “training and awareness.” Engagement \nencompasses anything that shifts the desired behavior outcome in the direction we want \nit to go. I have seen remarkable shifts in measured behavior from the use of \nnon-traditional tools such as security gamification and simulation. \n The way Malcolm differentiates between “compliance” and “commitment” is key. \n Managing Risk and Information Security is an ever-evolving classic in the field of security \nmanagement.\n —Patrick Heim \n Head of Trust & Security, Dropbox \n",
      "content_length": 2045,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "xvii\n Praise for the second edition \nof Managing Risk and \nInformation Security \n We assign Malcolm’s book to our Carnegie Mellon CISO-Executive \nProgram students on their first day of class. It is relevant, pragmatic, and \nsolution oriented. Our adversaries are changing their practices and so \nmust we. Malcolm’s book is a terrific tool for the modern-day info sec \nleader who wants to shift from security as a restriction to security as a \nbusiness enabler. \n —Andy Wasser \n Associate Dean, CMU Heinz College \n Malcolm is a top-notch executive, security leader, and innovator, with \na keen ability to convey thought-provoking and valuable insights. His \nlatest effort demonstrates remarkable foresight into the skills necessary \nto excel as a security leader today and tomorrow. \n —Clayton J. Pummill \n Executive Director, Security Advisor Alliance \n I could go on and on about what I liked specifically—there was \nmuch, including the discussion about governance models and social \nresponsibility—but here is the net: this is the first time I’ve seen \nsomeone be able to speak to security specifics while also raising the \nconversation to a much higher level. It begins to take on an Alvin Toffler \nfeel from his astounding book, The Third Wave . Malcolm’s thoughts are \nphilosophically sweeping while at the same time imminently practical. \n —Todd Ruback, Esq., CIPP-US/E, CIPT \n Chief Privacy & Security Officer & V.P. Legal Affairs, Ghostery \n",
      "content_length": 1449,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": " ■ PRAISE FOR THE SECOND EDITION OF MANAGING RISK AND INFORMATION SECURITY\nxviii\n Malcolm Harkins is a foremost expert at managing risk and information \nsecurity. In this latest book, he further expands his Protect to Enable \nphilosophy and does so in a way that offers practical and actionable \ninitiatives that any risk manager or CISO can implement to protect their \nenterprise while enabling business growth. A must-read for CISOs and \ntheir teams! \n —Tim Rahschulte, Ph.D. \n Chief Learning Officer & Content Officer, Evanta \n Malcolm Harkins is a visionary thought leader on cyber security and risk \nmanagement. Managing Risk and Information Security  is a must read. \nMalcolm helps readers immediately take the information and apply it to \ntheir own organizations. You will find that this book cuts through the fog \nand provides a clear picture of where and what to focus on to effectively \nmanage cyber business risk. \n —Phil Ferraro \n Global CISO and Cyber Security Consultant \n The CISO is more than just a technology expert; she must be savvy \nabout leadership, influence, and change across complex organizations; \nsomeone who sees her mission not to just drive implementation of a \nlarge system, but to foster sustainable culture change at every level. As \nan organizational psychologist, I recognize Harkins’ keen eye for group \ndynamics and leadership tactics that enable CISOs to enhance enterprise \nsecurity. He puts his finger on the habits, assumptions, and decision \nprocesses typical of many employees and teams, as they unknowingly \nincrease security risk, and for that alone this book is a gem. It should be \nrequired reading for aspiring CISOs and for anyone who has a role in the \nrecruitment and hiring of CISOs. \n —Marc Sokol, PhD \n Executive Editor, People + Strategy \n Malcolm Harkins’ take on information security and risk is a refreshing \nchange from the increasingly frequent alarm bells raised in the press \nwith regard to the “brave new world” where technology is presented as \nan ever-escalating conflict between our seemingly insatiable appetite for \nconnectivity, cool applications, and customized information, on the one \nhand, and a desire to control who has our information and how they may \nuse it, on the other. Harkins instead offers a cool, clear-eyed perspective \nwhere managing information and risk are placed in a wider context. His \nprescriptions and frameworks are recipes for well-managed organizations \nin the broadest sense. They allow us to embrace our new-found \n",
      "content_length": 2515,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": " ■ PRAISE FOR THE SECOND EDITION OF MANAGING RISK AND INFORMATION SECURITY\nxix\ntechnological abilities without fear because we have defined their purpose \ncapaciously enough to be a positive good, to be of service to all a company’s \nstakeholders. That is, once we set a truly human course, technology serves \nrather than threatens us. Organization purpose, when defined in this way, \nis an expression of our values and is empowered by that fuel. Harkins’ book \nis a practical as well as purposeful guide to a values-driven implementation \nof information technology. \n —Mary C. Gentile, PhD \n Author of  Giving Voice To Values: How To Speak Your Mind \nWhen You Know What’s Right (Yale University Press) \n In today’s rapidly evolving security landscape, security professionals are \nnavigating a complex set of dynamics across the enterprise. In Managing \nRisk and Information Security , Malcolm Harkins draws on his rich \nsecurity experience to present a connected view of where companies \nshould be focused. He puts forth a valuable perspective, as organizations \naround the world look to create a necessary balance of protection and \ninnovation, which ultimately enables business success. \n —Bret Arsenault \n Corporate Vice President and CISO, Microsoft Corporation \n Malcolm generously shares through personal experiences and story \ntelling the formula for a successful 21st century CISO. It is one part \nmulti-disciplinary leader and one part trusted advisor to the business, \ncombined with behavioral models required for balanced risk decision \nmaking. A must-read for all new CISOs. Malcolm lives his beliefs. \n —Nasrin Rezai \n GE Corporate Security & Compliance Officer \n In the second edition of his book, Malcolm seamlessly articulates the \nfuture horizon of cyber security and the critical role that the CISO and \nsecurity professionals will need to fulfill in order to defend both the \ncompany and consumers they serve. The guidance he provides into the \nskills, leadership, and approach required for successfully navigating \nthe emerging challenges of securing a digital economy is invaluable. \nRegardless of your current role, this is a must-read for everyone who has \naccepted this great responsibility and privilege. \n —Steven Young \n CISO, Kellogg Company \n",
      "content_length": 2272,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": " ■ PRAISE FOR THE SECOND EDITION OF MANAGING RISK AND INFORMATION SECURITY\nxx\n While other security officers are looking to the traditional or the latest \n“cool” product, Harkins goes against the tide and asks the questions that \nneed addressing. His forward-thinking mindset and Protect to Enable \napproach inspire others to innovate and go beyond the mainstream. \nIf you cannot bring Harkins to your company for mentoring, this book \nwill at least spark thought and will change how your engineers view \nsecurity within the business. \n —Charles Lebo \n Vice President and CISO, Kindred Healthcare \n Malcolm’s vast experience makes him one of the most credible security \nleaders on the international stage and serves as the perfect platform for \nthis book. Rational, compelling, and authoritative writing is far too rare \nin the world of risk and information security, but Malcolm completely \nnails it in Managing Risk and Information Security  with invaluable \nadvice and recommendations for anyone planning a future in the \nsecurity world. His extensive experience in business before becoming \na CISO is one of the missing ingredients in many security executives’ \nprofessional toolbox, which is which is why this is such an important \nbook. Make sure to keep a highlighter and notepad handy because there \nare a lot of nuggets in here you’ll want to remember on your journey to \nbecoming a better security professional. \n —Mark Weatherford \n Chief Cybersecurity Strategist at vArmour and \nformer Deputy Under Secretary for Cybersecurity \nat the US Department of Homeland Security \n I’ve had the privilege of working with many talented CISOs over the \nyears and Malcolm is one of the best. His logical, methodical approach \nto solving the most complex cybersecurity problems is reflected in his \nlucid style. An enlightened approach to understanding risk that unites \nall stakeholders and a systemic intelligence-based approach to security \ninfrastructure are the only ways to reduce the threat to manageable \nlevels. This is our best path forward if we are ever to realize the vast \npotential of the innovative digital world we are creating. In Managing \nRisk and Information Security , Malcolm shines a light on that path in a \ncomprehensive yet very readable way. \n —Art Coviello \n Former CEO and Executive Chairman, RSA \n \n",
      "content_length": 2328,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "xxi\n About the Author \n Malcolm  Harkins  is the Chief Security and Trust Officer \n(CSTO) at Cylance Inc. In this role, he reports to the CEO \nand is responsible for enabling business growth through \ntrusted infrastructure, systems, and business processes. \nHe has direct organizational responsibility for information \ntechnology, information risk, and security, as well as \nsecurity and privacy policy. Malcolm is also responsible \nfor peer outreach activities to drive improvement across \nthe world in the understanding of cyber risks and best \npractices to manage and mitigate those risks. \n Previously, Malcolm was Vice President and \nChief Security and Privacy Officer (CSPO) at Intel \nCorporation. In that role, Malcolm was responsible \nfor managing the risk, controls, privacy, security, and \nother related compliance activities for all of Intel’s \ninformation assets, products, and services. \n Before becoming Intel’s first CSPO, he was \nthe Chief Information Security Officer (CISO) \nreporting into the Chief Information Officer. Malcolm also held roles in finance, \nprocurement, and various business operations. He has managed IT benchmarking and \nSarbanes-Oxley–compliance initiatives. Harkins acted as the profit and loss manager for \nthe Flash Product Group at Intel; was the general manager of Enterprise Capabilities, \nresponsible for the delivery and support of Intel’s Finance and HR systems; and worked in \nan Intel business venture focusing on e-commerce hosting. \n Malcolm previously taught at the CIO Institute at the UCLA Anderson School of \nManagement and was an adjunct faculty member at Susquehanna University in 2009. In \n2010, he received the RSA Conference Excellence in the Field of Security Practices Award. \nHe was recognized by Computerworld as one of the Premier 100 Information Technology \nLeaders for 2012. (ISC)2 recognized Malcolm in 2012 with the Information Security \nLeadership Award. In September 2013, Malcolm was recognized as one of the Top 10 \nBreakaway Leaders at the Global CISO Executive Summit. In November 2015, he received \nthe Security Advisor Alliance Excellence in Innovation Award. He is a Fellow with the \nInstitute for Critical Infrastructure Technology, a non-partisan think-tank that provides \ncybersecurity briefings and expert testimony to the U.S. Congress and federal agencies. \nMalcolm is a sought-after speaker for industry events. He has authored many white \n",
      "content_length": 2425,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": " ■ ABOUT THE AUTHOR\nxxii\npapers and in December 2012 published his first book,  Managing Risk and Information \nSecurity . He also was a contributing author to  Introduction to IT Privacy , published in \n2014 by the International Association of Privacy Professionals. \n Malcolm received his bachelor’s degree in economics from the University of California \nat Irvine and an MBA in finance and accounting from the University of California at Davis. \n  \n",
      "content_length": 451,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "xxiii\n Acknowledgments \n I received valuable feedback from many readers of the first edition of this book. That \nfeedback helped me to expand the book with additional insights, clarifications, and \nupdated examples. It also encouraged me to add two more chapters to the second \nedition: one on corporate social responsibility, and the other on performance coaching. \n Special thanks to Mike Faden: without his help this book would not have happened. \n As I noted in the first edition, many people during my journey at Intel helped me \nlearn and grow. A number of them published material that is still referenced in this \nsecond edition. \n Other experts who have helped me come from a variety of different peer groups. \nThey include members of the Bay Area CSO Council, the Executive Security Action \nForum, the members and staff of CEB and its Information Risk Leadership Council, \nparticipants in the Evanta CISO Executive Summits and the CISO coalition, as well as the \nSecurity Advisor Alliance. \n Finally, I wish to thank Stuart McClure for giving me the opportunity to join Cylance. \n",
      "content_length": 1089,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "xxv\n Preface \n If you don’t believe in the messenger, you won’t believe the message. \n You can’t believe in the messenger if you don’t know what the messenger \nbelieves. \n You can’t be the messenger until you’re clear about what you believe. \n —James Kouzes and Barry Posner, \n in  The Leadership Challenge \n A great deal has transpired since the first edition of this book was published in January \n2013, both in the world of information risk and in my personal life and career. To briefly \ncover the latter, in January 2013, I was named Intel’s Chief Security and Privacy Officer. \nMy broad role was one of the first of its kind in corporate America: I was charged with \nmanaging and mitigating risk for Intel’s products and services worldwide, in addition to \nIntel’s internal IT environment. In June 2015, I left Intel to become CISO at Cylance Inc., \nand in May 2016, I was named Cylance’s Chief Security and Trust Officer. \n These career changes occurred during an extraordinary period of escalating \ninformation risk, as evidenced by an almost continuous stream of major hacks and \nbreaches, and a corresponding rise in society’s awareness of risk. Some key examples:\n• \n May 2013: Edward Snowden flies to Hong Kong after leaving \nhis job at an NSA facility in Hawaii. The following month, he \nreveals thousands of classified NSA documents. The disclosures, \nincluding previously unknown government surveillance \nprograms, continue to cause worldwide repercussions today. \n• \n December 2013: The blog Krebs On Security reports a massive \ndata breach at Target. The company confirms the breach the next \nday. Within months, Target’s CIO and CEO both resign amid the \nfallout. \n• \n May 2014: A U.S. grand jury indicts five Chinese military officers \non charges of hacking American companies and stealing trade \nsecrets. \n• \n November 2014: Employees at Sony Pictures arrive at work to \ndiscover their network has been hacked. Attackers steal and then \nerase data on thousands of systems, forcing studio employees to \nrevert to using fax machines and pen and paper. The attackers \nthen dump huge batches of confidential business and personal \ninformation online. \n",
      "content_length": 2168,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": " ■ PREFACE\nxxvi\n• \n March 2015: Google’s Project Zero hacking team demonstrates \nthe ability to exploit a fundamental flaw in DDR3 SDRAM to \nperform privilege escalation attacks on systems containing the \nchips. Some mitigation approaches are available, other than \nreplacing the DDR3 memory in millions of systems worldwide. \n• \n June 2015: The US Office of Personnel Management announces \na data breach targeting the personal data of up to 4 million \npeople. The attack, which includes security clearance-related \ninformation, is one of the largest-ever breaches of government \ndata. By July, the estimated number of stolen records increases to \n21.5 million. \n• \n February 2016: The Hollywood Presbyterian Medical Center in \nLos Angeles says it has paid a bitcoin ransom to attackers who \nheld its systems hostage, encrypting data and blocking access by \nhospital staff. Some believe the healthcare industry is the next \nmajor target for cyber criminals. \n Given this escalating cycle of risk, and the potential catastrophic societal \nimplications of today’s attacks, we must all be ready to be held accountable. This may \nrequire a large mental shift for those used to simply assigning responsibility and blame \nfor a breach to the people who traditionally perform post-attack cleanup: corporate IT \ndepartments, internal information security teams, and investigations and computer \nforensics groups. Everyone, from corporate executives to security practitioners, shares \nresponsibility for security and privacy. We must all step back and contemplate our own \npersonal responsibilities, not only to the organizations we work for and the customers we \nserve, but also to society as a whole. \n The challenge we sometimes face is how to characterize that responsibility. Is our \nresponsibility to limit liability for our organizations? Or is it a duty of care to the people \nwhose information we store? What values are we using when we make decisions about \ncyber risk, and what bias do those values create in our decisions? Are we forward-\nlooking enough, or will the decisions we make to fix our problems today create other \nproblems in the future? As Benjamin Franklin once said, “All human situations have their \ninconveniences. We feel those of the present but neither see nor feel those of the future; \nand hence we often make troublesome changes without amendment, and frequently for \nthe worse.” \n As security and privacy professionals, a key part of our role is to ensure the right \ndialogue and debate occurs. We need to ask “high-contrast” questions that sharply \ndefine the implications of the choices our organizations make. We need to make sure \nthat the opportunities are as clearly defined as the obligations to mitigate risk, so that \nour organizations make the right decisions. And we need to take equal responsibility for \nthe outcomes of those choices, as opposed to abdicating that responsibility solely to the \nbusiness. Once the choice is made, we must transition out of the debate about what is \nright and focus on taking the right actions—on making tomorrow better than today. \n We can think of this as doing what’s right. We can think of it as protecting our \ncustomers and partners and keeping our markets healthy for everyone. No matter what \nmotivates us, thoughtfully building systems to support a culture of genuine responsibility \nfor privacy and security is not only good corporate responsibility; it is also good for \n",
      "content_length": 3453,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": " ■ PREFACE\nxxvii\nbusiness. For computing to continue to improve the world we live in rather than endanger \nit, it needs to be trustworthy. And for that trust to be deliverable, we need to ensure the \ndata we enter into our computers is both secure and private. As an organization, we \ndemonstrate and build trust through our approach to solving these cyber-risk challenges. \n In the preface of the first edition, I said “ Managing Risk and Information Security is \na journey, but there is no finish line. Our approach to managing information risk must \ncontinue to evolve as rapidly as the pace of business and technology change. My hope is \nthat people will read this book and begin their own journey.” \n I still firmly believe what I said then. But I also believe that, as General George \nMarshall once said, “The only way human beings can win a war is to prevent it.” We \nare at war against adversaries who wish to harm the users of technology. But there is \nalso a battle among those responsible for protecting security and privacy. On one side \nare organizations that would like to continue on the current path because they profit \nfrom the insecurity of computing, or that approach the duty of care with a bias towards \nlimiting liability rather than protecting their customers. On the other side are those who \nbelieve that our role is to generate trust. We do that by protecting to enable people and \nbusinesses. It’s a hard road; I know, because I experience it every day. But we shouldn’t \nback away from something just because it is hard. We need to plant our feet and stand \nfirm. The only question is where we plant our feet. \n",
      "content_length": 1640,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "1\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_1\n CHAPTER 1 \n Introduction \n There are two primary choices in life: to accept conditions as they exist, \nor accept the responsibility for changing them . \n —Denis Waitley \n In January 2002, I was hired to run a new Intel internal program called Security and \nBusiness Continuity. The program had been created following the major security events \nof the previous year (9/11 and the Code Red/Nimda viruses) and it focused primarily \non the availability risks at that time. I had no background in technical security, but I \nhad been at Intel for nearly 10 years in a variety of business-related positions, mostly \nin finance. As I learned about information risk during the first few months, it became \napparent to me that the world was starting to change rapidly and that a “perfect storm” \nof risk was beginning to brew. In June 2002, I put together a diagram (Figure  1-1 ) to \nexplain the risks to my manager, Intel’s CIO, and anyone who would listen to me. \nThe diagram has been updated slightly since then to more explicitly highlight the \ngeo-political forces that are a key part of the threat, vulnerability, and regulatory \nrisk landscape.  \n",
      "content_length": 1262,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "CHAPTER 1 ■ INTRODUCTION\n2\n Today, it is clear that my view of the world was essentially accurate. Security breaches \nand intrusions are reported almost daily at organizations of all sizes, legal and regulatory \nissues related to technology use continue to grow, and geo-politics have surged to the \nforefront of some of these discussions in a post-Snowden era. Cyber attacks and data \nbreaches are now considered the biggest threats to business continuity, according to a \nrecent survey (Business Continuity Institute 2016). \n But the key question that I asked in the first edition of this book is still valid. Is \ninformation security really effective? Given the rapid evolution of new technologies and \nuses, does the information security group even need to exist? \n Obviously, this is a somewhat rhetorical question. I cannot imagine that any sizeable \norganization would operate well without an information security function. But the real \nissue is whether the information security group should continue to exist as it does today, \nwith its  traditional mission and vision . It is clear from the prevalence of breaches and \ncompromises that we have not kept up with the threats, and we appear to be slipping \nfarther behind as the world grows more volatile, uncertain, and ambiguous. It is no \nwonder that we have fallen behind: as the world of technology expands exponentially, \nso do the technology-related threats and vulnerabilities, yet our ability to manage \nthose security and privacy risks has progressed only at a linear rate. As a result, there \nis a widening gap between the risks and the controls. In fact, many organizations have \nessentially given up actively trying to prevent compromises and have defaulted to \nreliance on after-the-fact detection and response tools. \n Figure 1-1.  The  perfect storm of information risk \n \n",
      "content_length": 1846,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "CHAPTER 1 ■ INTRODUCTION\n3\n As information risk and security professionals, we should be asking ourselves \npointed questions if we wish to remain valuable and relevant to our organizations. Why \ndo we exist? What should our role be? How are new consumer and  Internet of Things \n(IoT) technologies shaping what we do, and can we shape the world of these new \ntechnologies and usage models? How is the evolving threat landscape shaping us, and \ncan we shape the threat landscape? Given the bewildering pace at which technology \nchanges and new threats appear, how do we focus and prioritize our workload? What \nskills do we need? \n Traditionally, information security groups in  businesses and other organizations \nhave taken a relatively narrow view of security risks, which resulted in a correspondingly \nnarrow charter. We focused on specific types of threats, such as malware. To combat \nthese threats, we applied technical security controls. In an attempt to protect against \nattacks and stop them reaching business applications and employees’ PCs, we fortified \nthe network perimeter using firewalls and intrusion detection software. To prevent \nunauthorized entry to data centers, we installed physical access control systems. Overall, \nour thinking revolved around how to lock down information assets to minimize security \nrisks, and how to reactively detect and respond to risks as they presented themselves. \n Today, however, I believe that this narrow scope not only fails to reflect the full \nrange of technology-related risk to the business; it is detrimental to the business overall. \nBecause this limited view misses many of the risks that affect the organization, it leaves \nareas of risk unmitigated and therefore leaves the organization vulnerable in those \nareas. It also makes us vulnerable to missing the interplay between risks and controls: by \nimplementing controls to mitigate one risk, we may actually create a different risk. And \nby focusing primarily on detection and response, we are not preventing harm; we are just \ntrying to limit the damage. \n As I’ll explain in this book, we need to shift our primary focus to adopt a broader \nview of risk that reflects the pervasiveness of technology today. Organizations still need \ntraditional security controls, but they are only part of the picture. \n There are several reasons for this. All stem from the reality that technology plays an \nessential role in most business activities and in people’s daily lives. \n Technology has become the  central nervous system of a business, supporting the flow \nof information that drives each business process from product development to sales. In \naddition, as I’ll discuss throughout this book, almost every company is becoming a supplier \nof technology in some form, as technology becomes a vital element of most products, \nservices, and infrastructure from cars and household appliances to the power grid. \n The role of  technology in peoples’ personal lives has expanded dramatically, too, and \nthe boundaries between business and personal use of technology are blurring. Marketers \nwant to use social media to reach more consumers. Employees want to use their personal \n smartphones to access corporate e-mail. \n Meanwhile, the  regulatory environment is expanding rapidly, affecting the way that \ninformation systems must manage personal, financial, and other information in order to \ncomply—and introducing a whole new area of IT-related business risks. \n Threats are also evolving quickly, as attackers develop more sophisticated \ntechniques, often targeted at individuals, which can penetrate or bypass controls \nsuch as network firewalls, traditional antivirus solutions, and outdated access control \nmechanisms such as passwords. \n",
      "content_length": 3756,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "CHAPTER 1 ■ INTRODUCTION\n4\n In combination, these factors create a set of  interdependent risks to a business’s \ninformation and technology, from its internal information systems to the products and \nservices provided to its customers, as shown in Figure  1-2 . \n Figure 1-2.  Managing the  interdependent set of technology-related risks \n Traditional security or other control type thinkers would respond to this situation \nby saying “no” to any technology that introduces new risks. Or perhaps they would \nallow a new technology but try to heavily restrict it to a narrow segment of the employee \npopulation. An example of this over the past few years was the view at some companies \nthat  marketers should not engage consumers with social media on the company’s web \nsite because this meant accumulating personal information that increased the risk of \nnoncompliance with privacy regulations. Another example was that some companies \ndidn’t allow employees to use personal devices because they were less secure than \nmanaged business PCs. \n The reality is that because IT is now integrated into everything that an organization \ndoes, security groups cannot simply focus on locking down information assets to \nminimize risk. Restricting the use of information can constrain or even disable \nthe organization, hindering its ability to act and slowing its response to changing \nmarket conditions. A narrow focus on minimizing risk therefore introduces a larger \ndanger: it can threaten a business’s ability to compete in an increasingly  fast-moving \nenvironment . \n \n",
      "content_length": 1568,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "CHAPTER 1 ■ INTRODUCTION\n5\n \nTHE CHALLENGES OF RISING SECURITY COSTS AND \nSKILLS SHORTAGES\n Growing recognition of the importance of  security and privacy , triggered largely by \nhighly publicized breaches, has led to sharply increasing security spending and \nan accompanying skills shortage. If the current trajectory continues, Gartner Inc. \npredicts that by 2017 the typical IT organization will spend up to 30 percent of its \nbudget on risk, security, and compliance, and will allocate 10 percent of its people \nto these security functions. That is triple the levels of 2011 (Gartner 2015b). At the \nsame time, skill shortages may worsen; more than a third of security managers \nsurveyed in 2015 reported significant obstacles in implementing security projects \ndue to inadequate staffing (Morgan 2015). One question is how much of the \nprojected cost increase is due to under-investment in the past, and how much is due \nto the fact that organizations have invested in technologies that do not adequately \nreduce risk. To break the cycle, as I’ll explain in Chapter  7 , we need a new security \nmodel and tools that create a demonstrable decrease in the risk curve, with a \ngreater focus on effective prevention and machine learning to reduce cost and \nmanual effort. \n Protect to Enable ® \n To understand how the role of information security needs to change, we need to \nre-examine our purpose. We need to  Start with Why , as author Simon Sinek argues \nconvincingly in his book of the same name (Portfolio, 2009). Why does the information \nsecurity group exist? \n As I considered this question back in 2010, and discussed it with other members \nof the risk and security team that I led at Intel, I realized that we needed to redefine our \nmission. Like the IT organization as a whole, we exist to enable the business, to help \ndeliver IT capabilities that provide competitive differentiation. Rather than focusing \nprimarily on locking down assets, the mission of the information risk and security group \nmust shift to enabling the business while applying a reasonable level of protection. To \nput it another way, we provide the protection that enables information to flow through \nthe organization, our partners, and our customers. We also provide the protection for the \ntechnology that our organizations create to provide new experiences and opportunities \nfor our customers. \n The  core competencies of information security groups—such as risk analysis, \nbusiness continuity, incident response, and security controls—remain equally relevant as \nthe scope of information-related risk expands to new areas, such as technology-enabled \nproducts and services, as well as privacy and financial regulations. But rather than saying \n“no” to new initiatives, we need to figure out how to say “yes” and think creatively about \nhow to manage the risk. \n",
      "content_length": 2854,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "CHAPTER 1 ■ INTRODUCTION\n6\n During my time at Intel, the security group’s mission evolved toward this goal as \nwe helped define solutions to a variety of technology challenges. For example, my team \nrecognized as early as 2002 that implementing wireless  networks within Intel’s offices \ncould help make the workforce more productive and increase their job satisfaction by \nletting them more easily connect using their laptops from meeting rooms, cafeterias, and \nother locations. At the time, many businesses avoided installing wireless networks within \ntheir facilities because of the risk of eavesdropping or because of the cost. We learned \npretty quickly that when we restricted wireless LAN deployments or charged departments \nadditional fees to connect, we actually generated more risks. This was because the \ndepartments would buy their own access points and operate them in an insecure \nfashion. We recognized that the benefits of installing wireless LANs across the company \noutweighed the risks, and we mitigated those risks using security controls such as device \nauthentication and transport encryption. By 2004, that approach had enabled ubiquitous \nwireless and mobile computing that propelled productivity and actually reduced risks. \n A more recent example that many organizations have experienced: for years, Intel \ndidn’t allow employees to use personal smartphones for business, due to concerns about \nprivacy and other risks such data theft. However, we experienced growing demand from \nemployees soon after the launch of the iPhone 3 in 2009.  We realized that letting them use \nthese consumer devices to access e-mail and other corporate systems would help boost \nemployee satisfaction and productivity. \n By working closely with  legal and human resources (HR) groups , we defined security \ncontrols and usage policies that enabled us to begin allowing access to corporate e-mail \nand calendars from employee-owned smartphones in early 2010. The initiative was highly \nsuccessful, with a massive uptake by employees, overwhelmingly positive feedback, and proven \nproductivity benefits (Evered and Rub 2010, Miller and Varga 2011). The success of the initiative \nled to its selection for an in-depth Ivey Business School case study (Compeau et al. 2013). \n The  transformation within the information security group was reflected in changes \nto our mission statement and top priorities over the years. In 2003, the internal mission \nstatement reflected the traditional focus and scope of information security organizations: \nthe overarching goal was to protect information assets and minimize business disruption. \n By 2010 it was clear to me that we needed to simplify our purpose and also broaden \nthe scope. So in 2011, I changed our mission to Protect to Enable to express the idea that \nour primary goal was to find ways to enable the business while providing the protection \nnecessary to reduce the risk to an acceptable level. \n For a few years after this, I thought of information risk and security as a  balancing \nact . I felt that we needed to try to find the right balance between providing open access to \ntechnology and information to enable the business and locking down assets. Providing \nopen access allows greater business agility. The business can move more quickly with \nfewer restrictions. Employees can work more freely, and the faster flow of information \nallows the company to grow and transform. \n But as my  responsibilities grew to encompass security and privacy not only for \ninternal systems but also for all aspects of products and services, I realized that a \nbalancing act was the wrong analogy. We should not start from a position of making \ntrade-offs between risks and enablement, or between security and privacy. So I began \nusing a different model that I now feel more accurately represents the challenges of \nmanaging information risk: we should take on the harder task of optimizing what is \nreally a multivariate equation of risk dynamics and business objectives in order to create \nsolutions that are “ tuned to target ,” as shown in Figure  1-3 . \n",
      "content_length": 4112,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "CHAPTER 1 ■ INTRODUCTION\n7\n For each problem and solution, we try to optimize or “tune” five  primary variables :\n• \n Risk and Compliance : Meeting security, privacy and compliance \nrequirements, based on the organization’s risk tolerance and \nsecurity and privacy principles. \n• \n Cost and Maintenance: The total cost of controls, factoring in \ndeployment and maintenance costs. \n• \n Productivity and User Experience : The extent to which controls \nhinder business velocity by making it harder for users to do their \njobs. I call this  control friction . In addition, if we make it difficult \nor time-consuming for users to follow security policies or use \nsecurity tools, they’ll ignore them, thus creating more risks. (See \nthe discussion of the 9 Box of Controls in Chapter  7 ). \n• \n Market Objectives : The company’s goals, such as increased \nmarket share. \n• \n Customer Needs: Our customer’s privacy and security needs, as \nwell as their overall experience. \n Ultimately there may be cases where we cannot fully optimize each item and we \nneed to make trade-offs, but that doesn’t mean we shouldn’t try. \n Figure 1-3.  Tuned to target: optimizing the equation to meet business objectives and \ncustomer  needs \n \n",
      "content_length": 1219,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "CHAPTER 1 ■ INTRODUCTION\n8\n I hope that this model may help information security groups at other organizations \nthink about how these priorities relate to their own businesses. The optimization points \nfor each variable and objective will depend on factors such as the organization’s overall \nculture, technical acumen, and appetite for risk. \n Building  Trust \n I believe that if computing is to continue to improve the world we live in, rather than \nendanger it, it must be trustworthy. Unfortunately, as I describe in Chapter  9 , the privacy \nand security breaches that have hit the headlines in recent years have weakened the \npublic’s trust in technology, according to the Edelman Trust Barometer, a widely used \nindicator. The rapid implementation of new technologies emerged as a new factor in \ndepressing trust overall. “By a two-to-one margin, respondents in all nations feel the new \ndevelopments in business are going too fast and there is not adequate testing,” the study \nconcluded (Edelman 2015). \n To rebuild trust in technology, we need to ensure the data we enter into our systems \nis both secure and private. At Cylance, we strive to cultivate a work environment where \nsecurity, privacy, and trust are an integral part of the evolving culture of the company and \nfoundational to the design, development, and delivery of our products and services. \n To analyze the context that led to my approach to the risk and security mission, and \nhelped to shape top priorities, I’ll explore some of the key changes in the landscape: \nthe rapidly expanding regulatory environment, the emergence of new devices and \ntechnologies, and the changing threat landscape. \n Keeping the Company Legal: The Regulatory Flood \n Until the early 2000s, I didn’t see regulatory compliance as a top priority for information \nsecurity. That’s simply because there weren’t many regulations that impacted IT, at least \nin the United States. There were a few exceptions that affected a subset of companies, \nincluding Intel, such as controls on certain  high-tech exports . And in European \ncountries, there were already regulations that sought to protect personal information. \nBut in general, IT groups didn’t have to dedicate much of their time, or budget, to \nregulatory compliance. \n The change in the last decade has been extraordinary. We have seen a flood of new \nregulations implemented at local, national, and international levels. They affect the \n storage and protection of information across the entire business, from the use of personal \ninformation for HR and marketing purposes, to financial data, to the discovery of almost \nany type of document or electronic communication in response to lawsuits. And with \ngrowing concerns about cyberwarfare, cyberterrorism, and hacktivism, several countries \nare evaluating additional  cybersecurity legislation in an attempt to protect critical \ninfrastructure and make industries more accountable for strengthening security controls. \n In most cases, these regulations do not aim to specifically define  IT capabilities ; \nhowever, because information is stored electronically, there are huge implications \nfor IT. The controls defined in the regulations ultimately must be implemented in the \norganization’s systems. These systems include more than just technology: they consist of \n",
      "content_length": 3329,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "CHAPTER 1 ■ INTRODUCTION\n9\npeople, procedures, devices, and applications. The business risk includes a significant \nIT-related component, but we must take a holistic view of risk management. Noncompliance \ncan damage a company’s brand image, profitability, and stock price—not just through \nresulting legal problems, but through bad publicity. \n Let’s take a brief look at some of the key areas and regulations that are having the \nbiggest impact. \n Privacy:  Protecting Personal Information \n For many US companies, the wake-up call was the California data security breach \nnotification law (State Bill 1386), which became effective in 2003. A key aspect of this \nlaw requires companies that store personal information to notify the owner of the \ninformation in the event of a known or suspected security breach. Businesses could \nreduce their exposure, as well as the risk to individuals, by encrypting personal data. \n After this, other states quickly followed suit, implementing regulations that generally \nfollow the basic tenets of California’s original law: companies must promptly disclose a \ndata breach to customers, usually in writing. \n In addition, federal laws, such as the  Health Insurance Portability and Accountability \nAct (HIPAA) , have addressed specific categories of personal information. Further \nregulations have been added in other countries, too, such as the updated data-protection \nprivacy laws implemented in Europe (European Commission 2011, 2012). \n The implications of these local and national regulations extend beyond geographical \nboundaries. As companies do more business online, they’re increasingly likely to acquire \nand store information about customers from other countries, and find that they also \nneed to comply with regulations around the world. Those regulations may change, with \nimplications for businesses in multiple countries. In late 2015, for example, Europe’s \nhighest court struck down the so-called “safe harbor” agreement that had allowed \ncompanies to move information about consumers between the European Union and \nthe United States. The replacement EU-US Privacy Shield, agreed after three months of \nnegotiations, aimed to address European privacy concerns with written guarantees that \nUS intelligence agencies would not have indiscriminate access to Europeans’ personal \ndata stored in the US (Scott 2016). \n The issue can become even more complex when businesses outsource application \ndevelopment or HR functions to providers located in yet another country. Now, software \ndevelopers in India may be building and operating the systems that collect information \nabout Europeans for US companies, making it even more difficult for businesses to \nnavigate compliance with all relevant privacy regulations. \n Personalization vs. Privacy \n Privacy concerns are set to become even more important over time, as businesses \nincreasingly seek to create online experiences tailored to the needs of individual users. \nThe more a business knows about each individual, the more it can personalize services \nand offer targeted advertising based on income and preferences. \n",
      "content_length": 3126,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "CHAPTER 1 ■ INTRODUCTION\n10\n Many users also like personalized services. If a web site “remembers” them, they \ndon’t need to enter the same information each time they visit the site, and they’re more \nlikely to see content and offers relevant to their needs. In fact, companies may be at a \ndisadvantage if they don’t personalize services because users may prefer a web site from a \ncompetitor that offers a more streamlined experience. \n However, there’s an inevitable conflict between personalization and privacy. \nThe personalization trend is fueling the growth of an industry focused on collecting, \nanalyzing, and reselling information about individuals. This industry existed long before \nthe Web; personal information has been used in mass-mailing campaigns for decades. \nHowever, the Web is both increasing demand for this information while providing \nnew ways to collect it. Companies now have opportunities to collect information from \nmultiple online sources, correlate and analyze this information, and then sell it to others. \nAnd of course, consumers’ fears that information will be lost or misused have increased \naccordingly. \n For businesses, however, offering personalized services also can increase \ncompliance concerns. As companies store more personal information, they are \nresponsible for safeguarding that information and are liable for any loss or compromise. \nIn many parts of the world, companies are also required to explain why they are collecting \npersonal data, how they are protecting it, and how long they will keep it. \n We can expect continuing tension due to conflicting desires for personalization and \nprivacy—and more regulation as a result. Governments clearly believe that businesses \ncannot be relied upon to regulate themselves, so they will continue to add regulations \ndesigned to protect the privacy of individuals. Meanwhile, businesses will seek new ways \nto collect more information so that they can further personalize services. Developing \ncompliance strategies and guidelines becomes even more pressing. \n Financial Regulations \n Financial regulation surfaced as a top priority in the United States with the Sarbanes-\nOxley Act ( SOX ), which emerged from the public outrage over corporate and financial \naccounting scandals at companies such as Enron and WorldCom. These scandals cost \ninvestors billions of dollars and damaged public confidence. To help avoid similar \ncatastrophes in the future, SOX imposed financial tracking requirements designed to \nensure that a company’s financial reporting is accurate and that there hasn’t been fraud \nor manipulation. Once enacted, SOX required publicly held companies to meet specific \nfinancial reporting requirements by the end of 2004. \n Although the Sarbanes-Oxley Act doesn’t mandate specific technology controls, \nit has major implications for IT. Ensuring financial integrity requires controls to be \nimplemented within everyday financial processes. In practice, this means they must \nbe enforced within the IT applications and infrastructure that support those processes. \nPurchases above specific thresholds may require approval from the finance group; the \nunderlying applications have to support this workflow, and to be sure the applications \nfunction correctly, businesses need to establish the integrity of the underlying computer \ninfrastructure. Compliance with financial regulations therefore creates a series of IT \nrequirements, from making sure that applications provide the right functionality to \nimplementing access controls and updating software. \n",
      "content_length": 3570,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "CHAPTER 1 ■ INTRODUCTION\n11\n E-Discovery \n Regulations governing the discovery of information for litigation purposes officially \nextended their reach into the electronic realm in 2006. That’s when the US Supreme \nCourt’s amendments to the Federal Rules of Civil Procedure explicitly created the \nrequirement for e-discovery—the requirement to archive and retrieve electronic records \nsuch as e-mail and instant messages. \n This created an immediate need not just to archive information, but to automate its \nretrieval. This is because records must be produced in a timely way, and manual retrieval \nwould take too long and be prohibitively expensive. The business risks of noncompliance \nare considerable: unlike many countries, US practice allows for potentially massive \ninformation disclosure obligations in litigation. Companies that fail to meet e-discovery \nrequirements may experience repercussions that include legal sanctions. The \nimplications are correspondingly onerous. Lawsuits may draw on information that is \nseveral years old, so businesses must have the capability to quickly search and access \narchived information as well as current data. E-discovery is further complicated by the \ngrowth of cloud computing models such as software as a service (SaaS). As organizations \noutsource more business processes and data to cloud service suppliers, they need to \nensure that their suppliers comply with their e-discovery needs. \n Expanding Scope of  Regulation  \n The regulatory universe continues to expand, with the likelihood of more regulations \nthat explicitly address IT, as new technologies emerge and governments try to control its \nuse and inevitable misuse. In the US, lawmakers have proposed legislation to increase \nthe security and privacy of connected cars, following a widely publicized demonstration \nin which researchers hacked into a Jeep and took over its controls. The Food and Drug \nAdministration (FDA) has published cybersecurity guidelines describing requirements \nfor manufacturers of Internet-connected medical devices (FDA 2016). \n The attempts by various governments to gain access to technology for the purposes \nof combating terrorism have generated considerable impact and controversy. In China, \na new anti-terrorism law requires that technology companies hand over technical \ninformation and help with decryption when the police or state security agents demand \nit for investigating or preventing terrorist cases (Buckley 2015). In the US, even greater \ncontroversy was generated by the US Government’s attempts to force Apple Computer \nto create “back doors” that make it easier to access information on iPhones used by \nterrorists or criminals. In India, after terrorists used unsecured Wi-Fi access points \nto communicate information about their attacks, the government created a legal \nrequirement that any access point must be secured (Government of India Department of \nTelecommunications 2009). \n In other countries, businesses that operate unsecured Wi-Fi access points (a \ncommon way to provide Internet access for visitors) may find themselves facing other \nlegal problems. For example, unscrupulous individuals may tap into the network to \naccess web sites for purposes such as illegally downloading music or pornography. \nAccess appears to originate from the company hosting the access point, which may then \nfind itself on the receiving end of correspondence or raids from the music industry or \ngovernment agencies. \n",
      "content_length": 3478,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "CHAPTER 1 ■ INTRODUCTION\n12\n The Rapid Proliferation of Information, Devices, \nand  Things \n The computing environment is growing as rapidly as the regulatory environment. The \nsheer volume of information is exploding, and it is being stored across a rapidly growing \narray of devices. The Internet of Things will drive yet another exponential increase: \nGartner, Inc. estimates that during 2016, 5.5 million new “things” will be connected every \nday, and Cisco expects 50 billion connected devices by 2020. In the not too distant future, \nalmost any device with a power supply may have an IP address and will be capable of \ncommunicating—and being attacked—over the Internet. \n Recent headlines have highlighted the growing threat activity focused on IoT, as I’ll \ndiscuss further in Chapter  7 . Researchers hacked into a Jeep via its Internet-connected \nentertainment system and remotely controlled the vehicle’s functions (Greenberg 2015); \nother researchers showed that thousands of medical devices in hospitals are vulnerable \nto attack. \n At the same time, the boundaries between work and personal technology have in \nsome cases completely dissolved. Whether businesses officially allow it or not, employees \nare increasingly using their personal devices for work by sending e-mails from and storing \ninformation on their personal smartphones and computers. Furthermore, people may \nforward e-mail from business accounts to personal accounts created on external systems, \nwithout considering that when they signed up for the personal account, they agreed to a \nlicense that allows the external provider to scrutinize their e-mails. \n The use of personal technology such as smartphones can considerably enhance \nbusiness productivity because employees can now communicate from anywhere at \nany time. However, this also creates a more complex, fragmented environment with \nmore potential points of attack. Information is now exposed on millions of new devices \nand disparate external networks, many of which do not have the same type of security \ncontrols as corporate PCs, and all of which are outside corporate network firewalls. Not \nsurprisingly, mobile malware has become a major industry, and is still growing: one \nsurvey found more than 1,200 known families of Android malware in 2014, more than \ndouble the number found the previous year (Millman 2015). \n The boundaries between work and personal lives are dissolving in other ways, \ntoo. Employees store more information on the Internet—on business and consumer \nsocial media sites, for example—than ever before. These sites are powerful tools for \ncommunicating with audiences outside the corporate firewall. \n However, just as there’s an industry gathering and analyzing personal information \nfor marketing purposes, information on the Web can be used for competitive intelligence \nor for less legitimate purposes. Users store snippets of information in multiple places \non the Web. Although each of these snippets may not provide much information, when \npieced together they can provide new intelligence not just about the individual, but also \nabout the organizations to which the person belongs. Each item is like a single pixel in \na digital picture. Alone, it doesn’t convey much information; but step back, aggregating \ninformation from a wider range of sources, and those pixels combine to form a portrait. \nIn the same way, pieces of information strewn across a variety of unrelated web sites—the \nname of a department, workmates, pet names that might be used as passwords—can be \nlinked together to create a picture of an individual and used for malicious purposes. \n",
      "content_length": 3643,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "CHAPTER 1 ■ INTRODUCTION\n13\n The Changing Threat Landscape \n The  threat landscape is evolving rapidly, with an increase in highly organized and well-\nfunded groups capable of executing sustained attacks to achieve long-term goals, including \ncyberespionage, cyberterrorism, and cyberwarfare. These attackers, generally known as \n advanced persistent threats ( APTs ), were originally thought to focus mainly on governments \nbut more recently have also been shown to target private-sector organizations, with the \ngoal of stealing intellectual property or simply causing damage. APTs include nation-state \norganizations, “hacktivist” groups attempting to publicize or further their cause, and \norganized crime. Hacktivists who said they were targeting oppressive regimes claimed \nresponsibility for an attack that disabled more than 30,000 computers at the world’s biggest \noil producer, Saudi Aramco. The FBI blamed North Korea for a crippling attack on Sony \nPictures (Schmidt et al. 2015). In 2014, the US Justice Department indicted five Chinese \nmilitary hackers for stealing trade secrets and other information from US companies in the \nnuclear power, metals, and solar industries (Department of Justice 2014); in 2016, the US \ncharged seven hackers linked to the Iranian government with hacking US banks and dam \noperations (Nakashima and Zapotosky 2016). \n The steady rise of organized  cybercrime online  is entirely logical. As the exchange \nof money and information has moved online, organized crime has followed, focusing on \ntheft of valuable assets such as intellectual property. This has spawned a mature malware \nindustry that increasingly resembles the legitimate software industry, complete with a \nbroad set of services, guarantees, and price competition among suppliers.  Ransomware , \nwhich encrypts a victim’s data until a ransom is paid, is a recent trend. \n Stealthy Malware \n This evolving set of threat agents is using new, more sophisticated tools and methods \nto mount attacks. Once upon a time, attackers were amateurish and often driven by \npersonal motives such as the prestige of bringing down a big company’s network. \nAccordingly, the arrival of malware on a user’s machine was easy to detect: the malware \nannounced itself with icons or messages, and the system often became unusable. \n Now the trend is toward malware that is stealthy and uses sophisticated techniques \nto avoid detection. Attackers plant malware that lies undetected over a long period while \nit captures information. Another common technique is to quietly spread malware by \ninjecting malicious code into an unsuspecting company’s web site; users who visit the site \nthen unknowingly download the code onto their systems. \n Accompanying this is a shift from spam mass e-mails to carefully crafted \n spearphishing attacks aimed at individuals or specific  groups . These typically use social \nengineering techniques, such as providing enough contextual or personal information in \nan e-mail to tempt people to download malware or click on a link to an infected web site \ncreated specifically for that purpose. Though more expensive to mount, spearphishing \nattacks can be enormously profitable to cybercriminals; an analysis by a supplier of anti-\nphishing solutions found that they were the primary initial attack method used by APTs \nin 2015; 22% of attacks were motivated by financial fraud or other crimes (PhishLabs \n2016). We can expect these stealthy and targeted attacks to continue, with new methods \nemerging as necessary to circumvent defenses. \n",
      "content_length": 3561,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "CHAPTER 1 ■ INTRODUCTION\n14\n Nine Irrefutable Laws of Information Risk \n Over the years, I’ve identified a number of “laws” that encapsulate some of the lessons \nI’ve learned, and that seem to remain true despite the continually changing environment. \n I call these the Nine Irrefutable Laws of Information  Risk (with acknowledgements to \nCulp (2000), Venables (2008), Lindstrom (2008), and other sources):\n• \n Law #1:  Information wants to be free . People want to talk, post, \nand share information—and they increase risk by doing so. Some \nexamples: \n A senior executive at a major technology company updated \nhis profile on a business social networking site. In doing so, he \ninadvertently pre-announced a shift in his employer’s strategy—a \nmistake that was promptly and gleefully picked up by the press. \n An employee found a novel way to fix a piece of equipment \nmore quickly and, to help others across the company, decided to \nvideotape the procedure. Because video files are so large, it didn’t \nmake sense to e-mail the video, so the employee posted it online. \nUnfortunately, by doing so, he exposed confidential information. \n At one time or another, many people have experienced this \ndisconcerting event: when composing a message, the e-mail \nsoftware helpfully autofills the address field, but it selects the \nwrong name from the address book. You hit Send without \nrealizing the error, thus dispatching a company-confidential \nmessage to someone outside the organization. \n It’s worth noting that that this rule is not new. Information has \nalways wanted to be free: think of the World War II slogan “loose \nlips sink ships.” People communicate, and sometimes they share \nmore information than they should. It’s just the methods that \nhave changed, and the fact that, with the Internet, a carelessly \nmentioned detail is instantly available to anyone across the globe. \n• \n Law #2:  Code wants to be wrong . We will never have 100 percent \nerror-free software. In fact, the more widely used the software, \nthe more malicious individuals will hunt for vulnerabilities in the \ncode. They have found and exploited errors in the world’s most \nwidely used web sites, productivity applications, and enterprise \nbusiness software. \n• \n Law #3:  Services want to be on . On any computer, some \nbackground processes always need to be running, and these can \nbe exploited by attackers. These could even be security software \nprocesses used for everyday activities like keeping systems up-to-\ndate with software patches or monitoring for malware. \n",
      "content_length": 2555,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "CHAPTER 1 ■ INTRODUCTION\n15\n• \n Law #4:  Users want to click . People naturally tend to click when \nthey see links, buttons, or prompts. Malware creators know this, \nand they take advantage of it. In fact, the entire phishing industry is \nbased on the assumption that users will click on enticing e-mails, \nweb sites, or pop-up ads, triggering the download of malicious \ncode to their systems. The evolution of highly targeted attacks such \nas spearphishing has taken this to a new level, as when e-mails \npurporting to be letters discussing legal action from a circuit court \nwere sent to senior executives at a number of companies. \n• \n Law #5:  Even a security feature can be used for harm. Security \ntools can be exploited by attackers, just like other software. This \nmeans that laws 2, 3, and 4 are true for security capabilities, too. \nNetworking equipment supplier Juniper Networks discovered \nthat its firewall software contained “unauthorized code” that \nsurreptitiously decrypted virtual private network traffic (Goodin \n2015). Security researchers have uncovered vulnerabilities that can \nbe exploited by attackers in products from well—known security \nsuppliers, including Kaspersky Labs and FireEye (Ashford 2015). \n• \n Law #6:  The efficacy of a control deteriorates with time . Once \nput in place, security controls tend to remain static, but the \nenvironment in which they operate is dynamic. Organizations \ntend to “set and forget”: to install security controls and then fail to \nupdate them with security patches or to properly maintain access \nlists. As attackers find new ways to circumvent or compromise the \ncontrols, their effectiveness progressively degrades. As Rob Joyce, \nwho heads the National Security Agency’s elite hacking unit, put \nit, an organization with static defenses will drift to the back of the \nherd, where it is easily picked off by a predator (see Chapter  6 ). \n• \n Law#7:   Code needs to execute. All software, good or bad, needs \nto execute in order to perform its intended function. Malware is \ncreated with malicious intent, but until it executes, it is dormant \nand can do no harm. Exploits can therefore be intercepted and \nstopped by security tools that inspect code before execution, \nidentify good from bad, and prevent bad code from executing. \n• \n Law #8:  Controls create friction. Security controls can slow users \nand business processes by impacting system performance \nor forcing them to use cumbersome processes. High-friction \ncontrols therefore impose a “drag coefficient” on business \nvelocity. Users react to a high degree of control friction by \ncircumventing the controls whenever possible; as a result, the \ncontrols can actually introduce new risks as business users go \naround IT to get their jobs done. Control friction is an important \nconsideration when designing security architectures (see the \ndiscussion on the 9 Box of Controls in Chapter  7 ) \n",
      "content_length": 2924,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "CHAPTER 1 ■ INTRODUCTION\n16\n• \n Law #9:  As our digital opportunities grow, so does our obligation \nto do the right thing. As technology becomes embedded into the \nfabric of our lives, exploits that take advantage of technology \nvulnerabilities may increasingly impact the well-being of almost \neveryone in society. So it is particularly important that we apply \nthe right ethical values to shape the way we design, develop, \nand implement these technologies. As I explain in Chapter  9 , \nsecurity and privacy should now be considered a corporate social \nresponsibility. \n A New Approach to Managing Risk \n Given the ever-broadening role of technology and the resulting information-related \nbusiness  risk , we need a new approach to information security built on the concept of \nprotecting to enable. This approach should\n• \n Incorporate privacy and regulatory compliance  by design, taking a \nholistic view of information risk . Also, because all companies are \nmoving toward using technology not only for internal operations \nbut also in products and services, the information security \norganization must work closely with other business groups to \nunderstand and manage risk. \n• \n Recognize that people and information, not the enterprise  network \nboundary,  are the security perimeter . Information is no longer \nrestricted to tightly managed systems within data centers; it now \nalso resides outside the firewall, on users’ personal devices, and \non the Internet. Managing risk therefore requires a range of new \ntools, including user awareness and effective security controls for \npersonal devices. \n• \n Be  dynamic and flexible  enough to quickly adapt to new \ntechnologies and threats. A static security model will inevitably \nbe overtaken by the dynamic nature of threats. We need security \narchitectures that can rapidly learn and adapt to new devices and \nevolving threats, with a high degree of automation. \n Above all, we need to accomplish a shift in thinking, adjusting our primary focus \nto enabling the business, and then thinking creatively about how we can do so while \nmanaging the risk. Our roles will only increase in importance as technology becomes \neven more prevalent. Our ability to protect information security and privacy will be \nessential to building the trust that enables our organizations to take advantage of new \ndigital opportunities. \n",
      "content_length": 2376,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "17\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_2\n CHAPTER 2 \n The Misperception of Risk \n The moment we want to believe something, we suddenly see all the \narguments for it, and become blind to the arguments against it. \n —George Bernard Shaw \n One hundred years ago, the “unsinkable”  Titanic foundered after striking an iceberg \noff the coast of Newfoundland. More than 1,500 people died in what became one of the \ndeadliest maritime accidents ever. Several factors contributed to this massive death toll, \nbut perhaps the most critical was that there simply weren’t enough lifeboats. The ship \ncarried 2,224 people, but fewer than half of them could squeeze into the boats. \n As we know, passengers who didn’t get a spot in one of those lifeboats quickly died \nin the freezing waters of the North Atlantic. What’s less well known is that the  Titanic ’s \nsupply of lifeboats was in full compliance with the British marine regulations in force at \ntime. The law required the ship to carry 16 lifeboats; the  Titanic actually had 20 lifeboats. \n The ship’s owners did a good job of providing enough boats to address the regulatory \nrisk of noncompliance. Unfortunately, meeting regulatory requirements did little to \nprevent the tragic loss of life. \n This is a case of  misperception  of risk . The owners focused on mitigating the \nregulatory risk, apparently blind to the much larger risk of disaster. They framed the \nlifeboat issue as a compliance item that needed to be addressed so that the ship could \nstart carrying passengers and generating revenue. One could argue that if they had \nstepped back and considered the potential consequence s for the customers rather than \nthe company’s short-term priorities, history might have unfolded differently. Reports \nsuggest that the  Titanic had enough capacity to easily add enough lifeboats for everyone \non board, had the owners chosen to do so. \n What does this example have to do with managing information risk? We encounter \nmisperceptions every day within the realm of enterprise risk and security. Every \norganization has a greater responsibility than simply complying with regulations. We have \nto think about whom is ultimately at risk: the company or the customer? Furthermore, \nas I’ll show in this chapter, everyone in the organization has their own priorities and \ntheir own subjective view of risk. Unless we mitigate these misperceptions, they can have \ndisastrous consequences. As a result, I believe that the misperception of risk is the most \nsignificant vulnerability facing enterprises today. \n",
      "content_length": 2636,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "CHAPTER 2 ■ THE MISPERCEPTION OF RISK\n18\n The Subjectivity of Risk  Perception \n As  security professionals , we tend to think about objective ways to estimate risk—to \nassess the likelihood and extent of harm that can occur due to specific threats and \nvulnerabilities. \n But in reality, the way people perceive risk has a strong subjective component. \n Economic and psychological factors greatly affect how each of us perceives the likelihood \nand potential impact of harm from specific actions or situations. Within an organization, \neach individual’s perception of risk varies depending on his or her job role, goals, \nbackground, and peer group. This means managers, security professionals, and end users \nall may have a different view of the risk associated with a specific technology or action. \n Misperceiving risk has serious consequences because our actions are shaped by \nour perception of risk. An employee may think that posting personal and work-related \ninformation on a social media  site is relatively harmless. However, hackers might use this \npublicly available information in phishing e-mails to gain access to enterprise systems via \nthe employee’s computer, ultimately resulting in detrimental security breaches. \n End users are not the only members of the organization who can misperceive risk. \nEveryone is capable of misperceiving risk, including risk and security professionals. As \nI’ll explain later in this chapter, misperceptions occur at the group level as well as the \nindividual level. Members of a group may share the same bias in their perception of risk \nand benefit. \n The decisions that result from these misperceptions can weaken the entire \norganization’s security  posture . If an organization underestimates a risk, it will \nunderspend on controls to mitigate that risk, increasing the likelihood and potential \nimpact of major problems such as data breaches. On the other hand, if the organization \noverestimates a risk, it will allocate a disproportionately large share of its security \nresources to the risk, leaving other parts of the risk landscape underprotected. \n In this chapter, I’ll discuss how and why different people within an organization \nmisperceive risk, whether they are acting as information technology users, security \nprofessionals, or managerial decision makers. To explore these misperceptions, I’ve drawn \non research across the broader field of risk psychology, notably  The Psychology of Risk , \na book by Professor Dame Glynis Breakwell, Vice Chancellor of the University of Bath \n(Cambridge University Press 2007). I’ll examine how these ideas about risk perception \napply to information risk and security. I’ll explain some of the consequences of those \nmisperceptions, and I’ll discuss some of the ways an organization can address them. \n How  Employees Misperceive Risk \n Research shows that if we like an activity, we tend to judge its benefits to be high and its \nrisk to be low (Slovic 2010). Conversely, if we dislike the activity, we judge it as low-benefit \nand high-risk. Because of this, the perception of risk by individuals and groups within an \norganization tends to be biased by their preferences, roles, and objectives. Everyone is \ntrying to achieve their individual or group goals within the organization, so they tend to \nsee activities and technologies that support those goals as beneficial, and therefore they \ntend to underestimate the risk. \n",
      "content_length": 3437,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "CHAPTER 2 ■ THE MISPERCEPTION OF RISK\n19\n So if employees like social media, their attraction to the technology skews their \nperception of benefit and risk. Because they judge the benefit to be high and the risk to be \nlow, they feel comfortable posting information such as their job title, location, and even \nthe projects they’re working on. They may even allow sites to capture their location, using \nthe global positioning system in their cell phone, and display the location in real time. \n Unfortunately, these employees may not think about how a malicious individual \ncould use the information. Today, as we’ve seen, an individual’s use of technology can \nharm not only the individual but the entire organization. Attackers exploit publicly \navailable personal information to craft spearphishing e-mails that are particularly \nconvincing because they appear to demonstrate a relationship with the recipient, making \nthe employee more likely to click on a link that downloads malware to the system. From \nthere, the attack spreads to the rest of the corporate network. In addition, information \nposted by individuals is now routinely aggregated, analyzed to identify patterns, and sold, \noften to a company’s competitors. \n The risk and security team may also misperceive the risk of social media, but in the \nopposite direction: they overestimate the risk and underestimate the benefits. They may \nnot like social media because it creates vulnerabilities, and their perception then drives \nthem to focus on minimizing the risk by trying to block the use of the technology. \n Other psychological factors also come into play in shaping end users’ risk \nperception. People in general tend to believe they are personally less likely than others \nto experience negative events and more likely to experience positive events, leading to \na sense of personal invulnerability (Breakwell 2007). In addition, users also are more \nlikely to behave in risky ways if their colleagues do so. “It’s conformity: being seen to be \ndoing what everybody else is doing,” Breakwell says (pers. comm.). Many social media \nsites encourage this conformist tendency; if all your friends are using a social media site, \nyou’re likely to join the site too because it enables you to see what they are doing and \nshare information with them more easily. \n The likelihood that individuals will behave in ways risky to the organization also \nincreases when their individual interests don’t align with the company’s. This divergence \nis most likely when employees are discontented, resentful, demoralized, or simply don’t \ntrust IT or the broader organization. \n In economic theory, the problem resulting from this lack of alignment is known as a \n moral hazard : a situation in which someone behaves differently from the way they would \nif they were fully exposed to the risk. A useful moral hazard analogy is renting a car with \nfull insurance coverage. People are likely to be less careful with the rental car than they \nwould be with their own car if they’re not responsible for the consequences. The attitude \nis “if it’s not mine, it doesn’t matter.” \n In the realm of enterprise IT, moral hazards may be a bigger concern than many \nappreciate. A Cisco survey (2011a) found that 61 percent of employees felt they were not \nresponsible for protecting information and devices, believing instead that their IT groups \nor IT service providers were accountable. Ominously, 70 percent of these surveyed \nemployees said they frequently ignored IT policies. \n One indicator of the extent of moral hazard within an organization may be how \nemployees treat company-provided laptops. Higher-than-average loss or damage rates \nmight suggest employees don’t care about the laptops and may be an indication they \ndon’t care about other corporate assets either. As I’ll discuss in Chapter  5 , I believe \nallowing reasonable personal use of laptops can help reduce the risk of moral hazard \nbecause it aligns personal interests with those of the organization. \n",
      "content_length": 4026,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 48,
      "content": "CHAPTER 2 ■ THE MISPERCEPTION OF RISK\n20\n More broadly, organizations can address the moral hazard issue by taking steps \nto align the goals and concerns of everyone involved: end users, information security \nprofessionals, and executives. This returns us to the theme of the book: as information \nsecurity professionals, our mission is to Protect to Enable. This mission aligns our security \ngoals with those of the business. It helps maintain the perception of shared values. \nResearch suggests that people with whom we share values are deemed more trustworthy \n(Breakwell 2007, 143). If employees trust us, they are more likely to believe our warnings \nand act on our recommendations. \n The Lure of the Shiny Bauble \n One further point to remember is that everyone in the organization, regardless of the job \nrole, is an end user. Therefore, we can all fall prey to the same tendencies. Our attraction \nto new consumer technologies can also cause us to ignore the risks. I call this magpie-\nlike attraction the  lure of the shiny bauble ; mesmerized by the appeal of gleaming new \ntechnologies, we downplay or even fail to notice the risks lurking in the shadows. \n How Security Professionals Misperceive  Risk \n While end users tend to underestimate the risks of a desirable activity or technology, \nsecurity professionals sometimes display the opposite tendency. We focus obsessively \non the information risk associated with a specific threat or vulnerability. In doing so, we \ncompletely miss bigger risks. \n This phenomenon is known as  target fixation , a  term originally coined to describe \na situation in which fighter-bomber pilots focus so intently on a target during a strafing \nor bombing run that they fail to notice the bigger risk to themselves and crash into the \ntarget as a result (Colgan 2010, 44). As information security professionals, we can develop \na similar fixation. We focus so intently on one risk that our awareness of larger hazards is \ndiminished. This target fixation can also occur in other groups with “control” functions \nwithin the organization, such as internal audit, legal compliance, and corporate risk \nmanagement. \n Here is an example from my own experience at Intel. Several years ago, we \ndiscovered that malware had been introduced onto our network from an employee’s \npersonal computer. We became so focused on this source of danger that we eliminated \nall personal devices from our network. We further fueled our target fixation by labelling \nthese devices “non-Intel managed systems ( NIMS ) ,” a term that reflected the frustration \nover our lack of control. I vowed we would never again allow network access from devices \nthat we didn’t fully control. \n However, by becoming fixated on a single threat, we may have created some larger \nrisks and additional costs. For example, we needed to issue contract employees with \ncorporate PCs, each of which allowed broader access to the Intel environment. If we \nhad instead focused on how we could provide limited access to the environment from \n“untrusted”  devices , we might have managed the risk with lower total cost and obtained \na head start in developing a key aspect of a more flexible security strategy, as I’ll describe \nin Chapter  7 . \n",
      "content_length": 3246,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "CHAPTER 2 ■ THE MISPERCEPTION OF RISK\n21\n It’s worth noting that security professionals can also suffer from a problem that’s \nalmost the opposite of target fixation:  alert fatigue . At many organizations, security \ngroups experience a constant deluge of thousands of alerts emanating from security \ntools across the enterprise. With so much noise, it’s easy to become overwhelmed and \nmiss important threats. \n As security professionals, we also may misperceive  risk due to the tendency to “set \nand forget” security controls. This common security loophole is described in the sixth \nIrrefutable Law of Information Security in Chapter  1 , which states that the efficacy of \na control deteriorates with time. Once in place, controls tend to remain static, but the \nthreats they are intended to mitigate continue to evolve and change, sometimes in \nvery dynamic ways. Controls that are initially very effective can become inadequate \nover time. Ultimately, an adverse event may occur and may even have disastrous \nconsequences. \n Think about the  history of major oil tanker spills. For years, regulations allowed \ntankers to be built with a single hull, instead of a double (inner and outer) hull to provide \nadditional protection in the event of a leak. Meanwhile, tankers grew steadily larger \nbecause bigger ships could transport oil more efficiently than smaller ones. It wasn’t \nuntil the  Exxon Valdez ran aground, puncturing its hull and creating a giant oil leak that \ncontaminated huge stretches of Alaska’s coast, that authorities were spurred to create new \nregulations requiring double hulls in oil tankers (EPA 2011). \n Within enterprise IT, a typical “set and forget”  error is the failure to keep controls \nup-to-date, particularly if the controls are designed to mitigate a relatively low risk. A \ncase in point:  distributed denial-of-service (DDoS)  threats were a big concern more than \na decade ago, due to widely publicized attacks by worms such as Code Red, Nimda, and \nSQL Slammer. These attacks disabled corporate web sites or flooded internal networks \nby overloading them with requests. To mitigate the availability risk, many organizations \ninvested in defenses against DDoS attacks. \n Over time, however, DDoS attacks became less frequent, and organizations were \nassailed by newer threats. With limited resources, information security groups focused \non mitigating these new threats rather than continuing to build defenses against DDoS \nattacks. At the same time, though, businesses were increasing their online presence. Web \nsites evolved from being used primarily for advertising and displaying static corporate \ninformation to managing business-critical data and applications. Some organizations \nbegan conducting all their business online. Even traditional brick-and-mortar \nbusinesses moved customer support, order management, and other critical business \nprocesses onto the Web. The larger online presence multiplied the potential impact of a \nsuccessful attack. As a result, when DDoS attacks from a variety of groups resurfaced in \nthe past few years, they created even greater disruption to business operations as well as \ndamage to corporate brands. \n Another example: over the past few years, many organizations have become much \nmore diligent about  scrubbing data from the hard drives of old computers before \ndisposing of them or reselling them. But they failed to follow similar precautions for other \nbusiness devices that have evolved to include hard drives. \n",
      "content_length": 3511,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "CHAPTER 2 ■ THE MISPERCEPTION OF RISK\n22\n Nearly every digital copier contains a drive storing an image of each document \ncopied, scanned, or e-mailed by the machine. When CBS News reporters visited a \ncompany that specialized in reselling used copiers, they found businesses and agencies \nhad discarded machines containing lists of wanted sex offenders, drug raid targets, pay \nstubs with Social Security numbers, and check images. One copier’s hard drive even \ncontained 300 pages of individual medical records, including a cancer diagnosis, which is \na potential breach of federal privacy law (Keteyian 2010). \n Security and Privacy \n As I explained earlier in the book, security professionals, and the broader security \nindustry, can sometimes be tone-deaf when it comes to privacy concerns. In their zeal to \ncollect data for security purposes, they may create risks that the data could be used in a \nway that may violate people’s privacy, or at least their expectations of privacy. \n The challenge of balancing privacy and security concerns in the enterprise bears \nmany similarities to the broader issue of balancing security and privacy in society, an \narea that has been extensively explored by privacy legal expert Daniel J. Solove. As he \nexplains in the book  Nothing to Hide: The False Tradeoff between Privacy and Security \n(Solove 2011) , the debate between security and privacy has often been incorrectly \nframed to imply that we must choose between one value and the other. “Security and \nprivacy often clash, but there need not be a zero-sum game,” he writes. “There is a way \nto reconcile privacy and security: by placing security programs under oversight, limiting \nfuture uses of personal data, and ensuring that programs are carried out in a balanced \nand controlled manner.” \n Solove’s conclusion is equally applicable to information security. Many in the \nsecurity profession think that security equals privacy. That is not the case. We need good \nsecurity to achieve privacy, but the two are not synonymous. Some security industry \nsolutions conduct broad-based bulk data collection, monitoring the activity of users \nand their machines, and siphoning the data to the cloud. That data is then used to \nbuild profiles and, combined with other information, to enable the solution to scan for \npotentially anomalous activity. Considered in isolation, some machine data has few, if \nany, privacy implications. However, the collection of thousands of pieces of information \nabout what the machine is doing, when and how, while someone is using it and even \nwhen not, creates a detailed digital profile of an individual and his or her behavior. That \nprofile is collected, stored in perpetuity and analyzed. As our lives become more digitized, \nthe richness of that profile will grow and evolve. We need to step back and ask ourselves \nwhether this is really necessary for our protection. \n As I’ve discussed elsewhere in this book, I believe that security and privacy programs \nshould be managed together as elements of an overall enterprise information risk \nmanagement strategy. Security and privacy are like the two halves of a zipper: when \nmeshed together, they create a strong bond, protecting the enterprise and the individual \nagainst risk. Managing them as isolated silos is more likely to result in dangerous \nmisperceptions of risk.  \n",
      "content_length": 3366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "CHAPTER 2 ■ THE MISPERCEPTION OF RISK\n23\n \nMISMATCHING CONTROLS TO THREATS\n Businesses sometimes devote considerable time and resources to implement \nsecurity  controls that are completely irrelevant to the threats the companies are \ntrying to mitigate. These mismatches reveal a lack of understanding of the security \ntechnology and the threat. The controls may further add to the risk by providing \na false sense of security. In reality, deploying the wrong control is like carrying a \nlightning rod to protect oneself from getting wet in a storm. \n Typical mismatches include\n• \n Using firewalls to prevent data theft from applications that are \nallowed to operate through the  firewall \n• \n Using standard antivirus tools that are effective only against \npreviously identified threats, to protect against zero-day attacks \n• \n Using controls at the operating-system level to detect application-\nlayer attacks \n This mismatch does not mean that these controls are worthless. It simply means \nthat if our goal is to deal with a specific threat, we must understand both the attacks \nand the controls well enough to identify which controls are applicable, and where it \nis necessary to add other controls. For example, if a firewall cannot prevent attacks \nagainst an application, we might deploy an additional control behind the firewall. \n How  Decision Makers Misperceive Risk \n Managers make decisions based on information from technical specialists and other \nexperts. Therefore, the decisions that managers make are only as good as the information \nthey receive. Decision makers can misperceive risk when their decisions are based on \nbiased or incomplete information. \n Bias can influence these decisions every day. If people are trying to sell a particular \nproposal or point of view to their manager, what are they likely to do? They tend to select \ndata supporting their arguments and often ignore data contradicting those arguments. \n The danger of misperception is particularly acute when decision makers rely on \na narrow range of sources with similar viewpoints. Without obtaining a diversity of \nviewpoints, managers don’t get a full picture of the risk. Like-minded individuals tend \nto agree with each other, as you might expect. When a group is composed solely of \npeople with similar backgrounds and viewpoints, it may be particularly prone to  group \npolarization (Breakwell 2007, 99) and the group’s decision may be more extreme than the \nmean of their individual views. This problem may be especially acute when the people \ninvolved share the same mental model of the world, as is likely to be the case when the \ngroup consists only of specialists from the same organization. \n",
      "content_length": 2699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "CHAPTER 2 ■ THE MISPERCEPTION OF RISK\n24\n An even broader concern is how a focus on business goals can drive people to make \nunethical decisions. When these decisions are made by managers at the organizational \nlevel rather than at the individual level, the impact is compounded by the potential for \nwidespread disaster. \n After the  Challenger space shuttle exploded in 1986, extensive post-crash analysis \nrevealed the tragedy was caused because an O-ring on one of the shuttle’s booster rockets \nfailed to seal due to the low ambient temperature at launch time. \n However, it subsequently emerged that engineers had warned of the potential danger \nbefore the launch. Engineers from NASA contractor Morton Thiokol recommended the \nshuttle not be launched at low temperatures after analyzing data that indicated a link \nbetween low temperatures and O-ring problems. After NASA responded negatively to \nthe engineers’ recommendation, Morton Thiokol’s general manager reportedly decided \nto treat the question of whether to launch was a “management decision.” Against the \nobjections of their own engineers, Morton Thiokol’s managers then recommended NASA \ngo ahead and launch, and NASA quickly accepted this recommendation (Bazerman and \nTenbrunsel 2011, 13–16). \n For Morton Thiokol’s managers, the desire to meet the business goal of pleasing the \ncompany’s customer, NASA, apparently caused the ethical dimensions of the problem to \nfade from consideration—with terrible consequences. \n According to Tenbrunsel, this  ethical fading is not uncommon. The way a decision \nis framed can limit our perspective. If the decision is framed purely in terms of meeting \nbusiness goals, ethical considerations may fade from view. In fact, we may become blind \nto the fact that we are confronting an ethical problem at all (Joffe-Walt and Spiegel 2012). \n Another infamous ethical lapse involved the Ford Pinto, whose gas tank exploded \nin a number of rear-end collisions, resulting in fatalities. As Bazerman and Tenbrunsel \ndescribe (2011, 69–71), Ford discovered the dangers in preproduction testing. \nHowever, facing intense business competition, the company decided to go ahead \nwith manufacturing anyway. The decision was based on a cost-benefit analysis. Ford \napparently considered the choice as a business decision rather than an ethical decision \nand determined it would be cheaper to pay off lawsuits than make the repair. The impact \nof dehumanizing this risk decision was disastrous. \n In the past, many information technology risk decisions have often been considered \nonly in terms of their potential business impact. As information technology is integrated \ninto more and more products, decisions about information risk will increasingly affect \nthe lives of millions of people, making it essential to consider the ethical as well as the \nbusiness dimensions of information risks. It becomes even more important that we, \nas CISOs, keep ethical considerations to the forefront. What is the potential impact \nof a security breach when a car’s sensors and control systems can be accessed via the \nInternet? Or when medical life-support equipment can be remotely controlled using \nwireless links? \n How to Mitigate the Misperception of Risk \n It should be apparent by now that the tendency to misperceive risk is universal. We need to \nfind ways to help compensate for this misperception, given that it is our job to manage risk. \nAs security professionals and managers, how can we  mitigate the misperception of risk? \n",
      "content_length": 3526,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "CHAPTER 2 ■ THE MISPERCEPTION OF RISK\n25\n We can start by ensuring that we include a diversity of viewpoints when making risk \nmanagement decisions. Whenever possible, we should involve a broad cross-section of \nindividuals representing groups across the organization. This diversity helps compensate \nfor individual biases. \n However, assembling the right mix of people is only the first step in building a \nmore complete picture of risk. As information security and risk professionals, we need \nto ensure that the discussion brings up new perspectives and views. We must ask \npenetrating questions designed to bring alternative viewpoints to the surface. I think \nof these as  high-contrast questions because the process is analogous to adjusting the \ncontrast or colors of a photograph to highlight key elements of possible interest. This \nquestioning counteracts the  inevitable bias due to target fixation. We can also help \ncounter target fixation by simply recognizing it exists, and then consciously trying to see \nthe problem from someone else’s viewpoint. \n In addition, we need to continually seek out the minority report, the view that is \ncontrary to perceived wisdom. If the majority is telling us to turn right, are we missing \nsomething important that we’d find out by turning left? In a striking example, Israel’s \nDirectorate of Military Intelligence considered this viewpoint so important that it created \na devil’s advocate office as an institutional safeguard against group-think. The office’s job \nwas to criticize analysis coming from the Directorate’s other divisions and write papers \ncountering the analysis. In order to explore alternative assumptions and worst-case \nscenarios, it examined possible radical security developments scenarios, including those \nthat the defense establishment considered unlikely. Notably, the office was staffed by \nexperienced, highly regarded people known for their creative thinking, and its reports \nwent directly to all major decision-makers (Kuperwasser 2007). \n Uncovering New Perspectives During Risk  Assessments \n Risk  assessment models can be valuable tools for helping to evaluate risks and to \nprioritize security resources. But all models have limitations. If we base our decisions \nsolely on the results generated by a model, we may miss important risks. \n Many organizations use a risk assessment model based on a standard methodology. \nThe model scores each risk using the following formula: \n Impact of Asset Loss × Probability of Threat × Vulnerability Exposure = \nTotal Risk Points \n For each risk, we assign a rating to each of the three contributing factors in the \nformula. To illustrate, I’ll use a scale of 1 to 5. A high-value asset, such as a microprocessor \ndesign, might warrant a rating of 5. \n We then multiply the three ratings to obtain the total risk points. In this example, the \nmaximum possible risk score is therefore 53, or 125. \n A simple approach to risk management, using the output of the model, would be to \ndivide the security budget among the highest-scoring risks. \n",
      "content_length": 3071,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "CHAPTER 2 ■ THE MISPERCEPTION OF RISK\n26\n The model is valuable because it provides a consistent method for helping compare \nand prioritize a broad spectrum of risks. However, allocating resources based only on \nthe overall risk score can miss potentially disastrous “black swan” events that have very \nlow probability but extremely high impact (Taleb 2007). Because the formula simply \nmultiplies three ratings to obtain the overall score, black swans tend not to score as highly \nas lower-impact events with higher probability. \n To counteract this problem, we can examine the information in the model in more \ndetail, from different perspectives. We can create a list of the 20 most valuable assets and \nconsider whether they need additional controls. In the same way, we can examine the top \nthreats and vulnerability areas. \n The point is that any model used to calculate risk should be used as a framework to \ndrive a dialogue about all the variables and options, rather than as a tool that generates \nthe answers to our problems. By discussing the issues from a variety of perspectives, we \nmay identify important concerns we’d miss if we simply look at the overall risk scores. \n Before I moved into the information security field, I worked in finance. In our \nfinance group, we found the same principle held true when conducting ROI (return on \ninvestment) analysis. Our ROI model generated forecasts. However, it was by discussing \nthe model’s assumptions that we determined whether or not the model’s predicted \nfinancial returns were reasonable. \n Another method for prioritizing information systems risk management is to examine \nsystems from the perspective of critical business processes and to consider the impact of \na loss of confidentiality, integrity, or availability. \n An application that prints shipping labels may initially appear to be low priority \nbecause it is small, inexpensive, and doesn’t contain confidential data; it simply takes the \ninformation it needs from a customer information system on the network. However, if it’s \nunavailable because the network is experiencing problems, the impact is huge because \nthe company cannot ship products. \n The potential impact to a business process of losing confidentiality, integrity, or \navailability may also vary depending on the stage of the business cycle. Consider a payroll \nsystem. Information confidentiality and integrity are always important, but availability is \nexceptionally critical on payday. \n Communication Is Essential \n Communication is an essential part of any strategy to mitigate the misperception of risk. \nTo alter the way people behave, we need to change their perception of risk. To effect that \nchange, we must communicate with them. \n Changing  perceptions is difficult. We may need to address long-held preconceptions \nabout what is risky and what is not. Once people form an initial estimate of risk, they can \nbe remarkably resistant to adjusting their perception, even when given new information \n(Breakwell 2007, 59). \n",
      "content_length": 3030,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "CHAPTER 2 ■ THE MISPERCEPTION OF RISK\n27\n In addition, each person may have a different perception of risk. To communicate \neffectively, we may need to understand an individual’s viewpoint and then tailor our \ncommunication accordingly. Consider the example of taking laptops to countries with a \nhigh risk of information theft (see sidebar). People who are extremely concerned may need a \npatient, thorough explanation of the risks and benefits of taking their laptop versus leaving it \nin the office. A less fearful individual may just need a quick reassurance and a few basic facts. \n Although changing risk perceptions can be challenging, we don’t have any choice \nbut to try. Employees will use social media whether we like it or not. When they do, they \nmay not only put themselves at risk; they could be putting the company at risk too, if they \nare not careful. \n Communication can reduce the issue of misperception due to asymmetry of \n information . This asymmetry is created when security professionals know about risks but \ndon’t share the information with end users within their organization. When two parties \ndiffer in their knowledge of a threat or vulnerability, their perception of risk is likely to \ndiffer also. In other words, it is difficult for users to care about a hazard if they don’t even \nknow it exists. \n To succeed in changing users’ perceptions, we must communicate in ways that \nengage them, using language they understand rather than technical jargon. In my roles as \na security professional, I have always tried to employ entertaining, interactive video tools \nto help engage users and teach them how to spot dangers such as phishing web sites. As \nI’ll explain further in Chapter  5 , I have found these methods have been highly effective in \nchanging users’ awareness and perceptions, and ultimately in shaping their behavior. \n Patiently explaining to users the consequences of their actions can also help shape \ntheir perception of risk. In some countries,  pirating software is so commonplace that it \nis almost an accepted part of the culture. This poses a problem for many multinational \ncompanies. Employees in these countries may not even believe copying software \nis wrong, let alone view it as an illegal act. It can be useful to describe the potential \nconsequences of copyright infringement for the individual and for the organization. We \ncan explain to employees that a decision to pirate software can expose the company to \nsoftware license compliance risks. The consequences may be even more far-reaching if \nthe copied software is then incorporated into the company’s technology-based products \nor services. If a product is discovered to include stolen software, the company may be \nunable to ship it to customers, which means a significant loss of revenue. Of course, \nemployees may experience personal consequences too: if they copy software, they run a \nhigh risk of losing their jobs. \n Organizations as a whole may also be blind to risks, or may simply choose to ignore \nthem. One way to overcome this misperception is to patiently build up a list of examples \nshowing how other organizations ignored similar risks and experienced adverse \nconsequences as a result, according to Breakwell, the University of Bath psychologist \n(pers. comm. 2012). The more examples in the list, the harder they are to ignore. \n “Organizations stick their heads in the sand, ostrich-like,” she says. “But if you have \na database of examples illustrating where things have gone wrong elsewhere, it becomes \nharder and harder to find enough sand to stick your head in.” \n",
      "content_length": 3609,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "CHAPTER 2 ■ THE MISPERCEPTION OF RISK\n28\n CHALLENGING PRECONCEPTIONS: TAKING LAPTOPS TO \nHIGH-RISK COUNTRIES\n \n It may be necessary to challenge perceived wisdom in order to expose a clear \npicture of the real risks, and consequently make the right decision.  \n Some companies react to the higher rates of intellectual property theft in certain \ncountries by barring employees from taking their corporate laptops on business \ntrips to those countries. In some cases, the companies issue employees with a new \n“clean” system from which all corporate data has been purged. \n The goal is to prevent situations in which information theft might occur, such as \nwhen an employee leaves a laptop containing corporate data unattended in a hotel \nroom. A malicious individual could then get physical access to the system and copy \nthe data or implant software that will surreptitiously steal information over time. \n But does preventing employees from taking their familiar laptops really solve \nthe problem? Let’s suppose we issue employees with a new, data-free laptop. \nTo do their jobs, they’ll still need to use this system to log into their corporate \ne-mail and other applications—providing an opportunity for hackers to intercept \nthe network traffic. \n Furthermore, if attackers really want to target an individual, they have ways to do it \nwithout gaining physical access to the system. With a spearphishing attack, they can \ninduce the individual to click on a malicious link that remotely downloads malware. \n Preventing employees from taking their laptops and information also deprives the \norganization of the key business benefits of using a full-featured portable computing \ndevice; employees will likely be less productive as a result. So when assessing the \nrisks of traveling with mobile devices, an organization needs to think through the \ntradeoff between risk and benefit, including the cost of providing what they believe \nto be a “clean” system and the impact on the user. \n Building Credibility \n Ultimately, our ability to influence people’s risk perception depends on our credibility. \nWe need to build trusted relationships with executives and specialists across the \norganization to ensure our security concerns are seriously considered rather than seen as \nfear-mongering or target fixation. \n Trust is built in drips and lost in buckets; it is hard to create and easy to destroy. \nIf we create a security scare about a threat that turns out to be irrelevant or overblown, \nwe may be seen as just another source of misperception. If business groups think we are \nproviding unreliable and exaggerated information, will they trust us to provide \ntheir security? \n",
      "content_length": 2682,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "CHAPTER 2 ■ THE MISPERCEPTION OF RISK\n29\n We can establish credibility by demonstrating consistency, striving for objectivity, \nand showing that we can accurately predict the real security issues affecting the \norganization, and then communicate them in an effective and timely way. As I’ll describe \nin Chapter  10 , we need to communicate security issues more frequently at C-suite level; \nto do so, we need to be able to clearly explain security issues in terms of enterprise risk. \n Credibility is also built on the competence that comes from understanding the \nbusiness and technology as well as possessing core security skills. As the scope and \nimportance of information security continue to expand, creating this credibility provides \nan opportunity to step into a more valuable, high-profile role within the organization. \n",
      "content_length": 832,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "31\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_3\n CHAPTER 3 \n Governance and Internal \nPartnerships: How to Sense, \nInterpret, and Act on Risk \n If we are together, nothing is impossible. If we are divided, all will fail. \n —Winston Churchill \n To reduce cost, our company’s human resources group wants to move all HR-related \nprocesses to a SaaS provider, a cloud-based business that’s less than five years old. \nAt first glance, this might seem a low-risk decision. There’s a clear business case, and \noutsourcing HR systems doesn’t seem to create risks to corporate information assets such \nas intellectual property. Most businesses regard HR systems as commodity applications, so \nthey might select the supplier who can deliver the required functionality at the lowest cost. \n But there’s more to consider. Employees’ personal information will be transferred \nto the outsourcer, potentially creating new privacy concerns. And imagine the impact \nif thousands of our employees don’t get paid because the supplier experiences system \nproblems on payday and lacks adequate disaster recovery  capabilities . \n Clearly, the HR group owns the HR business processes. However, outsourcing these \napplications and processes can introduce risks for the entire business. The systems \nthat support HR processes can create  information risks . Outsourcing also involves \nprocurement. The business needs a clear overview of all the factors, including the risks, \nin order to make the best decision. To provide this view, the HR, procurement, and \ninformation risk and security groups need to work together. \n A typical organization makes many decisions that require this kind of  internal \npartnership to manage the risk. A product group wants to outsource development work \nto bring a product to market more quickly. A marketing team wants to engage a developer \nfor a new social media initiative. \n Similar considerations also apply to internal  technology transitions such as OS \nand application upgrades. Each new technology introduces new capabilities and risks. \nSometimes, the technology also includes features or options designed to help reduce \nrisk. By carefully analyzing the risk and security implications, including privacy and \n",
      "content_length": 2300,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n32\ne-discovery considerations, we can help manage the risk of the transition, and we can \noften capitalize on the new features to improve the risk picture overall. \n For example, when Intel IT was considering whether to migrate to Microsoft \nWindows 7, the information  security team partnered with other groups in a broad \nevaluation of the OS. We identified several features that could improve security compared \nwith previous versions of Microsoft Windows, and these security capabilities were an \nimportant factor in the decision to deploy Microsoft Windows 7 across Intel’s enterprise \nenvironment (Fong, Kohlenberg, and Philips 2010). \n The ability to make these decisions with an accurate view of risk depends on having \nthe right organizational structure in place. Because each organization is different, there’s \nno single, standard risk management structure that applies to all organizations. But at any \norganization, building an effective risk management structure involves considering two \n key areas , which I’ll discuss in this chapter:\n• \n Clearly defined information risk governance : Governance \ndefines who makes decisions, who can block them, and who is \nallowed to provide input. \n• \n Strong partnerships and multi-stakeholder collaboration : \nCollaboration between the information risk and security team and \nother internal groups is critical in forming an accurate view of risk \nand managing risk overall. Some partnerships are formally defined \nas part of the risk governance structure; others are informal \nrelationships. These formal and informal relationships are so \nimportant that I’ll dedicate a large part of this chapter to them.  \n Information Risk Governance \n Governance is about establishing a structure that enables the organization to effectively \nsense, interpret, and act on risk. Traditionally, information risk governance has been \nconsidered as a component of IT governance. The IT-centric view is encapsulated in a \ndefinition from the Massachusetts Institute of Technology Center for Information Systems \nResearch (MIT CISR): \n “ . . . A framework for decision rights and accountability to encourage \ndesirable behavior in the use of IT. Governance identifies who will make \nkey IT decisions and how will they be held accountable.” \n But as every company becomes to some extent a technology company, we need to \nbroaden this definition to include the information risk associated with technology-based \nproducts and services. Perhaps a better definition for this broader view is “Governance \nidentifies who will make key  information risk decisions and how will they be held \naccountable.” \n Information risk governance focuses on enabling the business while protecting the \nconfidentiality, integrity, and availability of information, whether it is corporate data or \npersonal information about employees or customers. It requires the involvement of the \nentire organization. To achieve effective information risk governance, the information \nrisk and security team must work closely with other groups. \n",
      "content_length": 3140,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n33\n A company’s primary areas of information risk are closely intertwined, underlining \nthe need for an effective governance structure that embraces all of these areas. For \nexample, a hacker might compromise the IT systems used by the company’s product \ndevelopers, and then use those systems as a way to introduce malware into the company’s \ntechnology-based products. \n Think about how easily security researchers were able to hack into Jeeps and other \nvehicles over the past couple of years, demonstrating their ability to remotely take control of the \ncar with potentially  life-threatening consequences . Clearly, security may not have adequately \nconsidered such a scenario when the car’s product groups designed those features. Yet any big \ncompany, including automakers, typically has large teams of people dedicated to managing \ninformation risk. It seems that in the case of the automakers, the companies perhaps lacked \nan effective structure for managing information risk wherever it occurs, whether that is in the \ncompany’s products and services or within back-office IT systems. \n To some people, the word governance may imply unnecessary bureaucracy, or \nperhaps even a  dictatorial approach.  MIT CISR notes that “good governance is enabling \nand reduces bureaucracy and dysfunctional politics by formalizing organizational \nlearning and thus avoiding the trap of making the same mistakes over and over again.” \n Research at  MIT CISR shows that the more businesses leverage the structure, tools, \nand techniques of governance, the greater the potential benefits. In fact, MIT CISR’s \nwork suggests that firms with effective IT governance enjoy profits that average at least 20 \npercent higher than their competitors (MIT CISR 2012). \n However, leveraging governance doesn’t imply slavishly following rules and \nprocedures. A few years ago, I encountered an IT professional who was regarded by some \npeople, including himself, as one of the best managers in IT. He rigorously based his \nproject decisions on the prescribed practices and procedures, and gathered the correct \nmetrics for reporting progress. Yet the projects he was responsible for generally turned \nout to be large, expensive failures. His obsession with correct procedures often impeded, \nrather than facilitated, the projects he was working on. \n To use an analogy, if you gave the same recipe to a top chef and an average cook, \nwould you expect them to produce exactly the same result? Probably not. Expert chefs \ndon’t simply follow the rules; they continually make adjustments using their senses and \nexperience to achieve the best results. The temperature of a cooking surface is not exactly \nuniform, so a chef may move the pots until they’re simmering just right. Fresh ingredients \nvary from day to day; the experienced chef is alert to the differences and tweaks the recipe \nand seasonings accordingly. \n Like  the procedure-obsessed IT project manager, we may scrupulously adhere to the \nrules but fail to achieve the desired outcome. \n This is one reason that partnerships with other groups are so critical. They provide \nchannels for dialogue, helping us sense changing business priorities so that we mitigate \nrisk based on those priorities rather than our preconceptions. \n Without a governance structure that facilitates this dialogue, organizations may take \ntoo rigid an approach when applying controls to manage and mitigate risks. For example, \nsome security groups try to ban the business use of social media due to the risks, but \nattempting to stop the use of external social media web sites is counterproductive and, \nin any case, impossible. At Intel, we found it was more effective to embrace social media \nand shape the way that employees use it, as I’ll describe in Chapter  5 . This approach, \ndeveloped in partnership with other internal groups, enabled the organization to enjoy \nthe benefits of social media while managing the risk. \n",
      "content_length": 4043,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n34\n Finding the Right Governance  Structure \n No single governance structure will fit all companies (see Table  3-1 and the sidebar \n“IT Governance Archetypes”). Furthermore, organizations may shift between different \nrisk governance models over time. When most organizations’ information assets were \nprimarily managed in centralized IT systems, it was natural for information risk to be a \ncentralized function managed within the IT group. But now, information-related risks \nare much more distributed. To drive corporate revenue, many companies are developing \ntechnology-based products and services more or less independently from the central IT \norganization. At the same time, business groups are shifting to cloud-based applications \nthat store corporate and customer information at external cloud providers. \n Table 3-1.  IT Governance Archetypes. Source: Weill and Ross  2000 \n Style \n Who has decision or input rights \n Business Monarchy \n A group of business or individual executives (CxOs). \nIncludes committees of senior business executives \n(may include CIO). \n IT Monarchy \n IT executives. \n Feudal \n Business unit leaders, key process owners, or their \ndelegates. \n Federal \n C-level executives and business groups; may also \ninclude IT executives. Equivalent of central and \nstate governments working together. \n IT Duopoly \n IT executives and one other group (for example, \nCxO or business unit leaders). \n Anarchy \n Each individual user. \n \nIT GOVERNANCE ARCHETYPES\n When considering the right risk governance structure for your organization, it may \nbe entertaining to think about how your organization compares with the deliberately \nprovocative governance archetypes, ranging from a feudal structure to anarchy, \nidentified by MIT CISR in the influential book  IT Governance (Weill and Ross 2000, 59). \n In practice, organizations may shift between different risk governance models over \ntime—from an IT-centric monarchy during the mainframe era, toward a feudal \nmodel or business monarchy as distributed systems emerged, swinging back to a \nfederal model as they recognized there’s a role for centralized IT, then shifting again \ntowards a business monarchy with the focus on technology-based products and \ncloud computing. \n",
      "content_length": 2340,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n35\n Today, many organizations may find that it makes sense to establish a  hybrid \ngovernance model that balances centralized and decentralized risk management \nfunctions. At the same time, the need for a single, broad view of all information-related \nrisks is driving organizations to create an executive role with overall responsibility for \ninformation risk. The executive often has the title of Chief Information Risk Officer \n(CIRO) or sometimes the  Chief Security and Privacy Officer (CSPO) . The executive’s \nbroad responsibilities encompass the roles of Chief Information Security Officer (CISO)/\nChief Security Officer (CSO) and Chief Privacy Officer (CPO). \n To consider how this model works, let’s first think about all the interrelated risks that \nan enterprise needs to manage. Figure  3-1 shows each primary area and the core elements \nthat are common to all of them. The CSPO’s role is to manage this “Rubik’s Cube of risk.” \n Now consider the governance model, the organization’s framework for managing \nthose risks, shown in Figure  3-2 . It consists of four main areas:\n• \n Oversight : This area focuses on making informed risk decisions \nand reviewing risks. It includes committees and review boards \nthat set strategic direction, and review key risk areas such as \nethics, compliance, and corporate investigations. \n• \n Monitoring :  Monitor (sense) risk through external and internal \nsources. External sources include industry research and analysis. \nInternal sources include internal partners who inform us of new \nbusiness risks or legal requirements. These internal sources also \ninclude our own security technology sensors. \n Figure 3-1.  Security and privacy : the primary areas of information risk, and the core \nelements of information risk management that apply to each area \n \n",
      "content_length": 1900,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n36\n• \n Engagement : Participate in industry workgroups and in \npartnerships and dialogues with trusted peer organizations. \nThese external  engagements provide a valuable risk-sensing \nfunction and help influence key security initiatives. I’ll discuss \nexternal partnerships in more detail in Chapter  4 . \n• \n Operations : Day-to-day risk management activities and \nprocesses, including risk assessments, incident response, and \nexercises such as war games. \n Typically, the  corporate governance model should achieve a balance of \ncentralization and decentralization. At most large companies, risk is decentralized: at any \none time, our companies are planning or managing many technology-related initiatives \nand events across practically every part of the business. Therefore, we need decentralized \nrisk management processes; too much centralization can mean losing the ability to sense \nthreats and respond in an agile way. But at the same time, we need a broad centralized \nview of the dynamic risk landscape and the ability to set organization-wide policies in \nareas such as security, ethics, and privacy. So the model must allow a centralized view \nand ownership of key risk functions, along with the ability for decentralized execution. \n The CSPO and the information risk and security team are involved in all four \nquadrants of the model. The  CSPO tends to be more focused on oversight and \nengagement, while the team’s members naturally tend to be more involved with \nmonitoring and day-to-day operations.  \n Figure 3-2.  A  corporate information risk governance model \n \n",
      "content_length": 1678,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n37\n For most functions, the CSPO and team work with other parts of the organization, \neither taking primary responsibility or operating in a participatory role. In the Oversight \nquadrant, for example, the CSPO may sit on the ethics committee and participate in \nbusiness unit risk management reviews. In monitoring, the CSPO’s team may have \nprimary responsibility for threat landscape reviews and threat indicators, but take more \nof a participatory role in internal audits and assessing business unit risks. In operations, \nthe team may own responsibility for the security development lifecycle and privacy \nby design, while participating in change control. It should be apparent that all of these \nfunctions require collaboration with other groups within the organization. \n Building Internal Partnerships \n By providing vehicles for dialogue and decision-making, internal partnerships and multi-\nstakeholder collaborations enable  information security teams to become more agile and \nresponsive to business needs. The number of potential partnerships has grown as the \nscope of information risk has broadened to include a range of privacy and regulatory \nconcerns as well as traditional security threats. \n In mature and proactive organizations, the information risk and security team \npartners with many internal groups for a variety of functions, including risk management \ndecisions, incident response, and monitoring. These groups include legal, finance, \nhuman resources, physical security, and business groups. \n Partnerships may include formal structures such as  standing committees as well  as \n a large number of informal and ad hoc relationships. These are created and maintained \nthrough everyday communication with people in other groups. We might initially contact \na business group to understand the potential impact of an emerging area of legislation. \nThe business group identifies risks and opportunities that we hadn’t even considered. Our \ninitial request thus sparks a dialogue about requirements and controls, and ultimately \nevolves into a partnership that helps us monitor risks and mitigate them. We also gain \nbusiness acumen, which helps us play a more valuable role within the organization.  \n In my roles running risk and security, partnerships and multi-stakeholder \ncollaboration have been critical to my success in understanding the broader risk picture, \nhelping the organization sense, interpret, and act on risk. Through these relationships, \nother groups can act as additional eyes and ears for the  information security group , such \nas security threats and compliance concerns. For example, the HR legal group might \nalert us to an employment-related regulation that creates new compliance concerns. \nInformation about risks flows in the other direction, too: we may alert our partner to new \nthreats that we’ve encountered. As we leverage other groups to look out for our interests, \nthey can also use us to look out for their interests. We also work with partners to interpret \nthis shared information through analysis and decide how to act in response. \n Internal partnerships may focus on just one of the areas shown in Figure  3-2 , \nor they may intersect multiple areas. For example, we partner with HR for incident \nresponse (operations) and to learn about new employment laws (monitoring). Multiple \npartnerships may also be required within each focus area: with the growing number of \nregulatory requirements, partnerships with internal groups such as HR, legal, corporate \nsecurity, and internal auditing become increasingly important and valuable in the area of \noperational investigations. \n",
      "content_length": 3737,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n38\n Because no two organizations are identical, each organization may require a \ndifferent set of internal partnerships, depending on its structure and business needs. \nEvery partnership should be created with a clear purpose. The organization should \nalso clearly define who is involved and who makes the decisions. To determine the \npartnerships your information security group needs, as well as their structure and \npurpose, it may be useful to ask the following questions:\n• \n Who do we need to partner with and why? To put it another way, \nwho do I interact with every day, and why do I interact with them? \n• \n What benefits do I receive from that interaction, and what \nbenefits does my partner receive? \n In the remainder of this chapter, I’ll discuss some examples of important partnerships, \ndescribing how we can use them and the value they provide. I’ll start by examining \npartnerships with  fellow travelers who have complementary roles in managing business \nrisk and liability: legal, finance, human resources, corporate security, and corporate risk \nmanagement groups. Then, I’ll examine partnerships with business group managers. \n Legal \n Legal groups are among the information security group’s most important partners \nbecause of the many areas where their roles intersect with ours. They own the \nresponsibility for legal compliance and legal review. They interpret laws, analyzing \nthe implications and relaying the relevant information to the rest of the organization. \nKey partnership areas include privacy, litigation, intellectual property, contracts, and \ncompliance with financial regulations. \n As companies create more technology-based products and services, their initiatives \nare likely to come within the scope of a broader range of laws and regulations. Health-\nmonitoring products might fall within the purview of the Food and Drug Administration; \ncompanies thinking about using drones for photography need to think about Federal \nAviation Administration requirements. \n Privacy \n As privacy regulations continue to grow in complexity and reach, many  organizations \nneed to comply with multiple requirements at local, regional, and national levels. Legal \nspecialists across the organization can help us understand what’s required in each \ngeography, align policies and controls for protecting personal information, and decide \nhow to manage responses in the event of a breach. \n Even local regulations can have implications across the enterprise. For example, \ncitizens of European countries are subject to European and national privacy laws and \nregulations. The simple transfer of European employee personnel information to a US-\nbased server will trigger a need to comply with the EU data privacy laws regarding such \ntransfer of employee information. \n",
      "content_length": 2884,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n39\n Litigation \n As one might expect, it’s essential to partner with legal specialists in situations where \nlitigation is possible or already in process. Examples are investigations of security \nbreaches, particularly when law enforcement is involved. Another area of partnership is \nin responding to subpoenas and litigation discovery orders; a legal group may need to \nwork with the information security team in order to collect the required information. To \nensure that data is available for discovery when needed, we may also need to collaborate \nwith the legal group to implement appropriate data retention policies. \n Intellectual Property \n Many organizations use a  data classification  structure to protect intellectual property, \nwith the most highly classified information receiving the greatest protection. We work \nwith legal groups to specify the classification structure and then implement controls \non management and distribution of such information to provide the appropriate level \nof protection. We also partner to respond to suspected or known IP thefts. Suppose \nan employee loses a laptop storing the designs of future products; a dialogue with IP \nattorneys is essential to understand the implications and decide how to respond. \n Contracts \n Almost every contract with a supplier or customer contains a confidentiality provision, \nwhich sets expectations about how each party will maintain the confidentiality of the \nbusiness transaction and any shared confidential information. We partner with the \nprocurement organization as well as the legal group to define and implement these \nrequirements into contracts. \n If our company decides to outsource a business application to an external supplier, \nwe’ll typically work with the procurement organization and legal team to define these \nconfidentiality and data security expectations, as well as the evidence we’ll need to \nvalidate that those controls are operating properly. For example, when hiring a company \nto manage health benefits, we set expectations about how they must protect our \nemployees’ personal health information. \n Our customers have expectations, too. Another company may need to share some \nIP with us to help us integrate our technology into their product. We need to understand \ntheir requirements and ensure that appropriate controls are implemented. \n A security technology supplier has to meet customer expectations that go beyond the \nproduct’s ability to provide protection. As I mentioned in Chapter  1 , one of the irrefutable \nlaws of security is that even a security feature can be used for harm. So suppliers must be \nable to discuss their security development lifecyle, privacy by design, and overall state of \ninternal controls, all of which could ultimately affect the efficacy of the product. \n",
      "content_length": 2897,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n40\n Financial Compliance \n In the United States and other countries, public companies are legally required to \ndisclose “material events,” those likely to have significant financial impact that could \naffect investor decisions, including IT-related incidents. An important aspect of risk \ngovernance, therefore, is partnering with legal groups to understand the types of events \nand specific incidents that must be reported. \n Guidance from the US Securities and Exchange Commission specifically discusses \nthe obligation to disclose the impact of cyber attacks, including those that result in IP \nthefts. Companies are also required to disclose material increases in security spending in \nresponse to an attack, even if the attack didn’t result in a loss of IP (SEC 2011). \n The legal team cannot do this alone because it lacks the security context of the event: \nthe frequency of specific types of attack, the potential impact, and the cost of response. \nTherefore, the security team must be involved. \n In 2010, Google disclosed that it had been breached in the widely publicized \nOperation Aurora attack. At around the same time, Intel also experienced an incident \nof similar sophistication. This was before the SEC issued its guidance in 2011, but as I \npondered the potential ramifications of a cyber breach one sleepless night, I realized that \nI should call our SEC legal experts to discuss the incident. Subsequently, we disclosed the \nincident in our financial report for the first quarter of 2010 (Intel 2010). \n Legal Specialists Within  Business Groups \n At large companies, each business group may have embedded legal experts. We need \nto work with them for issues directly related to their group. In addition, because of their \nconnections within the group, these legal professionals can be extremely helpful in \ninfluencing the group’s controls and expectations. \n Marketing groups, for example, usually include individuals who want to explore new \nways to communicate with users via social media. This appetite for adventure is a good \nthing; it can benefit the business. But at the same time, we have to ensure that content is \nadequately protected and includes appropriate privacy protection and statements. If we \nbring up the issue directly with marketers, we may receive a lukewarm response, as they \ntend to view any controls as restrictions on their ability to move quickly. But the legal \nprofessionals within the marketing group understand the need for controls. So a good \nway to raise our concerns is to have a conversation with the business group’s attorney, \nwho can help persuade others in the group that controls are needed. \n While I was Chief Security and Privacy Officer at Intel, we implemented a program \nthat reviewed all new externally facing online projects and monitors for potential problems \n(see sidebar). The projects ranged from web sites to more sophisticated tools, such as an \napplication that users can download and use in conjunction with external social media sites. \n As part of the review, we asked the project group who their legal contacts were so \nthat we could verify that they’d received legal approval. We also asked whether trademark \nand branding teams had reviewed the initiative, which was essential in many cases—\nespecially if the project was planning to register a new web site. Sometimes the answer \nwas no, in which case we facilitated a dialogue with the trademarks and brands team. \nThis enabled the trademark and brand people to manage the risk and helped forge yet \nanother important relationship within the company. \n",
      "content_length": 3683,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n41\n \nSECURING INTEL’S EXTERNAL ONLINE PRESENCE\n Intel’s business groups use hundreds of web sites and third-party solutions, \nincluding social media platforms, to communicate and conduct business with \ncustomers and business partners. Collectively, these externally facing Intel-branded \nsolutions were known as Intel’s  external presence . \n Until 2006, these web sites proliferated rapidly in response to business needs, \nwithout centralized oversight. Given this growth and following a number of security \nincidents and the identification of several significant risks, we established the  Intel \nSecure External Presence (ISEP) program to provide appropriate security for Intel’s \nexternal presence (Leon 2011). \n The goals of ISEP, which was a part of Intel’s information security group, were to \nprotect Intel’s information assets and customers against threats such as loss of \npersonal information and malware attacks, and to maintain compliance with laws, \nregulations, and standards. By achieving these goals, we also helped to protect \nIntel’s corporate image. \n We helped ensure this protection and compliance by reviewing all planned new \nexternal presence projects and by monitoring existing Intel-branded web sites. \nISEP review and approval was mandatory for new externally facing online projects. \nWe worked with Intel business groups to review planned projects before launch, \nwhether they were to be hosted within Intel or by a third party. \n Any  ISEP-like process for reviewing a company’s external presence should include \nseveral key aspects:\n• \n Ensure notification of new projects by working closely with business \ngroups and other stakeholders within the company. For example, the \ninformation risk and security team should be notified when business \ngroups request new Internet domain names or seek approval to land a \nnew application in the externally facing IT environment.  \n• \n Work with the business group on each project to review details \nof the planned approach to maintaining security and privacy \ncompliance. Verify that the project includes any required mitigating \ncontrols before giving approval. \n• \n Establish an overarching governance board , including senior \nmanagers from multiple stakeholder groups. This board should have \nenforcement powers including the ability to shut down web sites for \nnoncompliance. \n",
      "content_length": 2445,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n42\n Human Resources \n The human resources group is the organization’s center of expertise on  employee \nprocedures,  include legal specialists who are the organization’s experts on employee-\nrelated laws. Because of its responsibilities, the HR group also tends to be heavily involved \nin insider risk considerations and applying action in any cases that are discovered. In some \norganizations, HR is also responsible for other functions, including internal and external \ncommunications. Because of this broad charter, the security team may form valuable \npartnerships with HR in several areas, including employee policies related to appropriate \nuse and protection of information assets, internal communications, and investigations. \n Setting Employee Expectations in  Security Policies \n Employees are part of the security perimeter, as I’ll discuss in Chapter  . Their behavior \ncan have as much impact on security as the technical controls we use—particularly since \na growing number of user interactions with the outside world take place on external web \nsites and networks, and on personal devices such as smartphones. \n It is therefore critical to create employee policies that set expectations for secure \nbehavior. If we can influence employees to behave in more secure ways, we can reduce \nrisk for the business overall. However, the security team cannot write these policies \nwithout partnering with HR, including HR legal specialists, to ensure that they comply \nwith employment laws and the organization’s existing rules. Then, if an employee \ndisregards the policies, we need to work with HR to take disciplinary action. \n Careless behavior can have highly damaging consequences. Imagine an IT employee \nwho decides to store some corporate data on a server at his home so that he can more \neasily work on projects when out of the office. But his home system is open to the \nInternet, and thus the data may be broadly exposed to anyone worldwide. \n The employee’s action has created a significant security risk. To explain the potential \nimpact to HR, it may help to use analogies. We could say it’s like an engineer taking \ncritical product designs home and showing them to her neighbors. Or a factory employee \ntaking dangerous chemicals home to experiment with them, and creating the danger of \nan explosion in his garage. If we have a good relationship with HR, we can have this kind \nof discussion and determine the appropriate consequences for the employee. \n Employee Communications \n The responsibilities of the employee communications group often include employee \ntraining, employee awareness, and internal distribution of other corporate information. \nThis group’s expertise can be very useful when we want to communicate security \nmessages to the workforce. The group already has established communication channels \nand knows how to align messages with corporate style guidelines. A good employee \ncommunications group also knows how to present information in ways that engage \nemployees rather than intimidate them. \n In my prior roles running security and privacy, I always worked extensively with \nthe employee communications group to create engaging security awareness messages, \nincluding interactive content that helps encourage secure practices when using social \nmedia and the Web. \n",
      "content_length": 3407,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n43\n Investigations \n Partnership with HR is also essential in internal investigations, including investigations \ninto insider  threats responses. In other cases, we may already be pursuing an \ninvestigation and need help from HR legal specialists to access employee information. \n Finance \n The finance group typically takes the lead in managing enterprise-level risk and controls \nfor the organization overall. Therefore, we need to partner with the finance group to \nassess the business impact of damage to information assets—a loss of confidentiality, \nintegrity, or availability. This applies not only to internal systems that support business \noperations, but also to information technology-based products and services that generate \nrevenue. We also work together to determine the required controls. \n Sarbanes-Oxley  Compliance \n The corporate finance team usually has overall responsibility for Sarbanes-Oxley ( SOX) \n infrastructure. We also work with the finance group, as well as legal groups, to determine \nwhether we should categorize specific events as material and report them as required by \nSOX. This also includes product- or service-related vulnerabilities and controls that could \nhave a material effect on revenue or corporate liability. \n Working with  Business Groups \n Each sizeable business group is likely to have a group controller or other financial \nspecialist responsible for financial controls. These finance experts can become important \npartners for the security team. \n Because financial specialists focus on risk and controls, the culture among finance \nspecialists has some similarities with the culture of the information risk and security \nteams. This shared focus can make it easier for us to communicate our concerns, \nparticularly since the impact of information risk is often measured in financial terms. \nTherefore, the financial specialist can be a key contact point when we need to discuss \ninformation risk with business groups. \n Sometimes these risk conversations can evolve into productive multi-way \npartnerships. A recent example: an IT team presented plans for new systems to support \none of Intel’s new businesses. As we assessed the information risks, we noticed that the \nplan didn’t include fully redundant systems to ensure business continuity. When we \nasked why, it emerged that the business group hadn’t requested redundancy because it \nwould add cost. Revenue from this new business was initially expected to be modest, so \nthe group’s budget was limited. \n However, when we discussed the revenue projections with the finance specialists \nwho worked on the project, they expected the business to grow rapidly. This growth \nwould also increase the information-related risk because a system failure would have a \nmuch bigger impact on revenue. As we discussed the implications, it became clear that it \nwould make more sense to prepare for the anticipated growth by including redundancy \n",
      "content_length": 3038,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n44\nfrom the start. So we suggested that the business group negotiate a higher budget—and \nthat’s what happened through a partnership between the business group managers, the \ninformation security team, and IT finance and business system specialists. The business \ngroup allocated increased funding that allowed IT to implement a redundancy safety net \nthat would protect the growing business. \n Internal Audit \n Financial groups are often also responsible for an internal audit, which typically \nincludes an IT auditing function—a job with considerable potential for overlap with the \ninformation security group’s role. If the security team and internal auditors duplicate \neach other’s efforts, we’ll waste resources and annoy business groups. Imagine that we \ncontact a business manager to say that we need to conduct a risk evaluation of the group’s \nsystems. The next day, internal auditors contact the same group and say they’re planning \nto do an audit, which some business managers might perceive to be essentially the same \nas a risk evaluation. What kind of reception do you think the auditors would receive? \n We can minimize the overlap by partnering with internal auditors. This partnership \nbecomes a mechanism for effectively allocating risk management resources. If the \ninformation security team has already assessed a system, auditors may be able to increase the \nefficiency of an audit by leveraging the work that the security team has already performed. \n For effective partnership, our work must be thorough, transparent, and well \ndocumented so that auditors can see what we have done. We may also swap resources: \nsometimes security experts may act as guest auditors for specific projects because they \nhave skills that the financial group lacks. The partnership can also be used for valuable \ndialogue and mutual support. If we’re concerned about a system that internal auditors \nhave previously examined, we can ask for their opinion. We’ll sleep better knowing that \nanother group of objective, risk-focused specialists has analyzed the system. \n Corporate Risk Management \n Most large organizations employ people whose job includes purchasing insurance \nfor general business risks, including property and casualty insurance to protect the \norganization in the event of damage to a data center or another facility. When buying \ninsurance, the corporate risk management team may need information from us about \nthe organization’s IT business continuity and disaster recovery plans. Insurers ask for this \ninformation in order to set premiums. \n Today, the corporate risk management team usually focuses on physical risks. But \ntheir scope is rapidly expanding to include IT-related risks as well as risks associated with \nproducts and services. Privacy breaches or other compromises can have a major impact \non a company’s revenue, cost, and brand image. Because of this trend, insurance against \ncyber risks is a rapidly growing category, and we can expect a growing need to partner with \nthe corporate risk management team to ensure adequate coverage of information risks. \n Consider the case of Sony, which suffered a breach of its PlayStation Network—\nestimated by the company to cost at least USD 200 million (Perlroth 2011)—and then \nbecame embroiled in a legal dispute with its insurer, which claimed Sony’s insurance \npolicy did not cover cyber risk. The breach at Target, in which hackers stole the payment \n",
      "content_length": 3532,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n45\ncard accounts and personal information of millions of customers, is estimated to have \ncost the company roughly $250 million. Reportedly, the insurance payout of $90 million \nleft the company $158 million in the hole, plus what it paid for cyberattack insurance. \n Privacy \n Privacy and security are closely linked. However, increasing security doesn’t always \nenhance privacy. In fact, it can have the opposite effect. Unfettered bulk collection and \nmonitoring of the information and activities of users and their machines may be capable \nof increasing security, but it may also intrude on personal privacy. This data store may \nalso be an attractive target for intruders. \n This creates inherent tension between security and privacy interests. This tension \nis apparent at a national level in the way that privacy advocates respond to the use of \nsurveillance and data mining. Government security organizations may feel that they \nprotect data extremely well, but privacy advocates still object to the fact that information \nis collected and the way it is used. \n Similar concerns apply at the enterprise level. We need to carefully manage the \nrelationship between security and privacy, ensuring that we apply the appropriate level of \ncontrols to protect information without infringing on personal privacy. \n The structure of this relationship varies between organizations. While at Intel, the \ninformation risk group that I managed for over a decade included the privacy team, \nwhich reported to me as the CISO. Then as we began to see growing confluence of the \nrisks shown in Figure  3-1 , I was promoted to a broader role as Chief Security and Privacy \nOfficer, to give us an integrated governance and accountability structure. At other \norganizations, privacy is the responsibility of a separate group headed by a Chief Privacy \nOfficer who is the CISO’s peer. This arrangement necessitates careful management of \nthe relationship between security and privacy teams to manage tension, align policies, \nand control breaches. In organizations with this structure, the security team sometimes \ncomplains that the privacy team is “getting in their way,” which usually means that the \nsecurity team wants to collect specific information and the privacy team objects. \n Regardless of the organizational structure, it is the security team that is logically \nresponsible for implementing IT controls. It is the product security team that is \nresponsible for security development lifecyle ( SDLC ) and  product security incident \nresponse processes (PSIRT) . Laws define privacy rights; the organization’s interpretation \nof those laws drives compliance requirements. It is the security team’s responsibility to \ndetermine how to implement controls to support those requirements. \n Corporate Security \n The corporate security team focuses on physical security concerns ranging from door \nlocks and guards to break-ins, fires, and natural disasters. By partnering with this team, \nwe can make sure we’re aligned on protection of key information assets. It wouldn’t make \nsense to implement sophisticated data-protection tools on the servers in the data center \nand then leave the data center doors unlocked. \n We also need to coordinate on other issues, including incidents that involve law \nenforcement. Not so long ago, assaults and harassment were almost always physical \nincidents handled by corporate security and the police. Today, there’s a much bigger \n",
      "content_length": 3555,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n46\noverlap with information security. More crime is moving online, and we may encounter \nother problems, such as cyber bullying. Because of these trends, we may need to help \nassess the impact and drive the response. \n Business Group Managers \n Each business group has its own processes and applications, whether it’s a product-\nfocused unit responsible for generating revenue or an internal group managing finance \nor human resources. The information security team needs to partner with each group to \nimplement security controls that protect the group’s applications and information. \n As the business acumen of our information security team increases, we can better \nfulfill our Protect to Enable mission by focusing on controls that improve security without \nimpeding the business.  This applies not only to the systems that support business \noperations, but also to the technology-based products and services the business unit \ncreates. For example, we may discover product vulnerabilities through our security \ndevelopment lifecycle processes. We can partner with the business group to correct \nvulnerabilities before shipment, and we can work on training to prevent future mistakes \ndue to poor coding, design, or architecture. \n By working with business groups, we can also leverage their strengths. Business \ngroup managers can help drive decision-making and incident response. They can also \nhelp improve security by setting the “tone at the top,” publicly setting expectations for \ntheir employees’ security behavior. Suppose we notice that an increasing number of the \nemployees at a specific facility are experiencing laptop thefts. We discuss the trend with \nthe general manager and explain that we want to increase employees’ awareness with \nmessages about how to prevent theft. The business manager may offer to help by bringing \nup the topic at a site meeting or otherwise directly communicating with employees. This \nmanagement request may exert a more powerful influence on employee behavior than \nmessages sent by the security group. \n \nHOW TO RESPOND TO EMERGENCIES\n Defining a clear IT incident  response process is an essential aspect of IT governance. \nSimilarly, a clear PSIRT is an essential aspect of risk governance for technology-\nbased products and services. Over time, while I was at Intel, we developed a clearly \ndefined crisis management process for responding to emergencies and other \nsignificant incidents that affect IT infrastructure or services (Fleming and Tomizawa \n2012). The goal of the process was to prevent material impact to the organization and \nits employees. Similarly, the goal of a PSIRT process is to prevent material impact to \ncustomers or even to society in general, depending on the nature of the risks. \n Incidents that may trigger the process include cyber events and other information \nsecurity incidents; physical incidents such as fires, leaks, and major outages \nthat affect IT systems; and major disease outbreaks. A useful starting point for \ndeveloping the process is the incident management principles based on the US \nFederal Emergency Management Agency’s response to disasters. \n",
      "content_length": 3240,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n47\n Once initiated, an IT emergency response process ( ITERP) • operates with a \ncommand-and-control structure, led by an incident commander who has overriding \nauthority to make decisions across IT for the duration of the emergency. The \nstructure consists of a virtual organization staffed on a volunteer basis by people \nfrom every discipline within IT. When an incident occurs, all team members perform \ntheir response roles instead of their normal duties until all issues are resolved. \n Following an incident, the team should quickly identify the state of critical business \nprocesses that must continue during the crisis. It determines the current status \nof the key steps in the product cycle: design, build, order, ship, pay, and close. It \nassesses the physical state of the infrastructure, and analyzes the legal and other \nimpacts if intellectual property or personal information is compromised. Decisions \nabout response and remediation are driven by the incident commander and \ndetermined by business priorities. \n PSIRT and privacy response processes should be structured along similar lines, \nfocused on their respective mission-critical priorities. \n While I was at Intel, the ITERP team, the PSIRT team, and the privacy incident \nresponse team proved to be essential components of the successful resolution of \nevery crisis management, coordination, control, and communication activity across \nthe company during my 13.5-year tenure.  \n Conclusion \n Information risk has become a major concern for the entire organization. Managing \ninformation risk therefore requires a clear governance structure that enables the \norganization to make the right security decisions quickly and effectively. \n Building the right governance structure can sometimes seem like a complex \nchallenge. I’ve found that a good way to simplify and focus the thought process is to \nconsider the following two cardinal rules. In my experience, these rules apply to all \norganizations, whether large or small, public, private, or non-profit.\n• \n Rule 1:  Structure drives behavior . Thinking about the behaviors \nthat you want to see in the areas of security and privacy will help \nlead you to a structure that encourages those behaviors. \n• \n Rule 2:  You get what you measure . Thinking about the desired \noutcomes will help you determine how you should measure your \norganization’s success in managing risk. \n Think about how your own organization manages information risk. Do you develop \nstrategies in close collaboration with business groups? Do you feel that you communicate \nwell enough with every group to understand their priorities and implement controls that \nreflect them? Have you clearly defined all of the processes required to respond to a major \nbreach or denial-of-service attack? If you answered “no” to any of these questions, you \nmay need to improve your information risk governance. \n",
      "content_length": 2988,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "CHAPTER 3 ■ GOVERNANCE AND INTERNAL PARTNERSHIPS: HOW TO SENSE, INTERPRET, AND ACT ON RISK\n48\n Effective governance relies on partnerships between the information security team \nand other internal groups across practically every part of organization. In this chapter, I’ve \ndescribed some of the most important partnerships and the value we can derive from them. \n To develop these partnerships, CSPOs as well as Chief Security Officers and Chief \nPrivacy Officers need more than just technical skills. We need to communicate in terms \nbusiness people understand and build relationships that enable us to influence people at \nall levels across the organization. As the scope of information security expands, we also \nneed extensive management and leadership skills, both to operate at an executive level \nand to coach and inspire our risk and security team. \n",
      "content_length": 859,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "49\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_4\n CHAPTER 4 \n External Partnerships: \nThe Power of Sharing \nInformation \n Chance favors the connected mind. \n —Steven Johnson \n After spending a day at a conference, I was having dinner with a dozen or so peers when \na debate began about the dangers and benefits of sharing security information with other \ncompanies. One person turned to me and asked me whether, if I had information about a \nspecific new threat, I would share it with him. \n “You bet,” I said. \n “But what if I was your competitor? Would you still share?” he asked. \n “Our companies might compete for business,” I replied, “but in the security \narena, my real competitors are the malicious actors who want to harm my company’s \ninformation systems. Those are my competitors, and they’re your competitors, too.” \n As soon as I’d said this, several people at the table agreed. This agreement was \ngratifying—and not just because I felt that I had support for my views. The bigger implication \nwas that my peers saw the value of sharing information outside their companies. \n This hasn’t always been the case. Historically, many organizations frowned on the idea \nof sharing security information externally, and more than a few had policies forbidding it. \n However, attitudes are changing. Although there is still resistance at some companies, \nmany organizations now see the value of sharing information and have begun doing so. \nEvidence includes the growth of industry-specific information-sharing communities, \nsuch as the retail-industry group that formed after Target’s massive customer-information \nbreach in 2013. There are also innovative partnerships that have a regional rather than \nindustry-specific focus, such as the Arizona Cyber Threat Response Alliance. \n Supportive actions by the US Government have also helped encourage  information \nsharing . In 2014, the Federal Trade Commission and Department of Justice issued a \npolicy statement indicating that sharing threat information was unlikely to raise antitrust \nconcerns. This addressed a key reason that some big organizations had been reluctant to \n",
      "content_length": 2204,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n50\nshare information. “Cyber threats are increasing in number and sophistication, and sharing \ninformation about these threats, such as incident reports, indicators, and threat signatures, is \nsomething companies can do to protect their information systems,” said Bill Baer, an Assistant \nAttorney General in charge of the DoJ antitrust division (U.S. Department of Justice 2014). \n In 2015, the White House issued a statement encouraging information sharing as a \nway to help safeguard national and economic security, and directing the Department of \nHomeland Security to support the formation of information-sharing groups under the \numbrella term  Information Sharing and Analysis Organizations (ISAOs) . And in 2015, \nlegislation was proposed to promote sharing of threat information, although the effort \nstalled in Congress. \n Despite the overall shift in attitude, some organizations still have reservations \nabout sharing information. There are three major areas of concern. First, organizations \nworry about the legal and regulatory implications of  revealing  information about \nthreats. A second, related concern is the public relations  aspect . Both of these fears \nhave a valid basis. Information security has become an enterprise risk management \nissue of board-level interest because of the potential effects. Information leaks revealing \npotential intrusions and data breaches can have legal consequences: the organization \nmay be required to report the problems in order to comply with financial and privacy \nregulations, for example. If security issues become public, they may also damage the way \nthe organization is perceived by customers and by the business community, potentially \naffecting a company’s profitability and its stock price. The third major area of concern \nis privacy. This also has a valid basis. For example, sharing information that identifies \nthe victim of an attack, as some security specialists would like to do, clearly can expose \nmachine data that can potentially compromise the victim’s privacy. Some people also \nsee a risk, following the revelations of National Security Agency eavesdropping, that \nlegislation could be used to enable government surveillance. For these reasons, I believe \nthat any cybersecurity legislation must include appropriate privacy protection. \n What’s the payoff from sharing information? My personal experience is that I have \nobtained real value: information shared by others has helped me understand threats \nand take action. I have also seen that it’s possible to share useful information while \navoiding the issues mentioned above. Companies can share information about attacks \nwithout revealing personal information about the victim. They can share indicators of \ncompromise without revealing confidential information. They can alert other trusted \ncontacts during the early stages of investigating a threat, before it’s been determined \nwhether a compromise has occurred that requires regulatory disclosure. \n The growth of information-sharing groups shows that many other organizations now \nshare my belief in the value of sharing information  about  threats and best practices. As \nI’ll explain in this chapter, sharing security information can provide considerable benefits \nin managing the risk of moving into new business relationships and adopting new \ntechnologies. We just need to find ways to reduce the risk of sharing. The solution lies in \ncreating trusted information-sharing relationships with other organizations. The more we \ntrust the relationship, the more sensitive the information that can be shared. \n The need to  share security information is being driven by rapidly changing business, \ntechnology, and threat landscapes. Increasingly, companies are collaborating with a \nbroad variety of business partners. We share business information, and often we also \nuse the same technology, or we sell or share technology with each other. As we do so, we \nalso share risks. Understanding the risks faced by our partners, and the way they manage \nthose risks, can help us protect our own organizations. \n",
      "content_length": 4171,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n51\n Looking more broadly across the  technology landscape , all systems and devices are \nto some extent connected, whether they are owned by enterprises, individuals, or service \nproviders. Almost every aspect of society depends on a worldwide, rapidly evolving, \nhighly complex network of devices and services. This provides the central nervous system \nthat supports innovation, economic development, and social interaction worldwide. \nBut because we are all inherently interconnected, we share common risks. The threat \nlandscape is dynamic, global, and increasingly complex. Threats may originate in any \ncountry and then spread rapidly across national and enterprise boundaries, causing \nextensive damage to organizations and individuals worldwide. \n Because threats spread so quickly and the  threat landscape is so complex, it is hard \nfor any single organization to gain a clear view of all potential vulnerabilities, threats, \nand attacks. External partnerships can help. They provide additional intelligence that \nwe can use to improve our own security posture. By exchanging information with other \norganizations, we gain what I call  outsight , or a better understanding of what happens \noutside our own environment. We learn about new threats before they hit us directly. We \nsee how other organizations are managing those threats. We learn about best practices \nfor managing security operations. Using the information we gather from external \nrelationships, we can increase the organization’s ability to sense, interpret, and act on risk. \n The Value of External Partnerships \n Sharing security-related  information  can require initiative and courage. The idea of \nsharing information externally may run counter to the culture of the organization \noverall, including the culture within the security group. Organizations may view security \ninformation as proprietary and confidential, like intellectual property. Many still have \npolicies against sharing information. \n It’s true that much security information is sensitive, and sharing it can introduce \nrisks. Because of this, we need to be careful about what we share and with whom. \n But think about the broader context of how organizations are increasingly sharing \ninformation. Most organizations have already recognized that they need to share \nsensitive business information with partners in order to develop, manufacture, and \nmarket new products. Collaboration with other companies is becoming an integral part \nof many other business processes, too. As organizations share information, they benefit \nfrom their partners’ insights and expertise. As noted by Steven Johnson, author of  Where \nGood Ideas Come From: The Natural History of Innovation (Riverhead Books 2010), many \nof the best ideas have emerged not through the inspiration of a single mind, but through \nthe exchange of ideas. “You have half of an idea, somebody else has the other half, and \nif you’re in the right environment, they turn into something larger than the sum of their \nparts,” Johnson said in a speech at the 2010 TEDGlobal conference (Johnson 2010). “We \noften talk about the value of protecting intellectual property—building barricades, having \nsecretive R&D labs, patenting everything that we have, so that those ideas will remain \nvaluable … but I think there’s a case to be made that we should spend at least as much \ntime, if not more, valuing the premise of connecting ideas and not just protecting them.” \n I believe that there’s similar value in sharing security information. As we collaborate \nwith business partners, we need to understand the threats to their environment, and \nhow they manage risk, in order to determine what we need to do to protect our own \norganizations. Each partner in a value chain needs to protect information to a level \n",
      "content_length": 3879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n52\nthat is adequate to protect the other partners; the weakest link in the chain can impact \neveryone. Note that throughout this chapter, I use the terms “partner” and “partnership” \nin the colloquial sense, not to imply any specific type of formal legal relationship. \n There are many other examples of how sharing information can benefit all \norganizations involved. If we are entering new markets through business partnerships, \nwe need to understand the nature of the threats in those markets from the companies \ncurrently operating there. The same logic applies to using new technologies. \nOrganizations are extending their environment to customers and becoming suppliers \nof mobile apps and web services in the process. As they do, they can learn from other \ncompanies’ experience how to manage the risks. Companies are increasingly sharing \ncloud capacity or other data-center infrastructure supplied by external providers, and can \nall benefit by sharing feedback with the provider about risks within the environment. \n Despite these trends, some organizations still have policies stipulating that employees \nshouldn’t share internal information about risks and threats with anyone outside the \ncompany. This is sometimes the case even when the same organization willingly shares \nother IT-related information such as helpdesk or e-mail management best practices. \n Without wishing to discount the real fears driving these policies, the value of sharing \ninformation often outweighs the risk of doing so. Let’s imagine that a CISO learns of a new \nthreat affecting companies in his industry sector. He shares information about the threat \nwith a peer at another company and, by doing so, gains insight that helps the organization \nmitigate an attack that has caused massive damage at other companies. By sharing \ninformation against company policy, the CISO took a personal risk. Yet by doing so, he \naverted the bigger risk of business disruption and damage to the organization’s reputation. \n Failure to share information with others introduces its own risks. If we don’t share \nwith peers, they won’t share with us, so we won’t benefit from their information and \ninsights. I’ve seen cases in which information security professionals wanted to participate \nin communities, but weren’t allowed by their companies to share any internal security-\nrelated information. So they attended meetings but couldn’t contribute. Ultimately, their \npeers wouldn’t tolerate a situation in which these people were receiving information but \ngiving nothing in return, and they were effectively voted off the island. \n External Partnerships: Types and Tiers \n Much of the publicity about information-sharing initiatives has focused on public-private \npartnerships related to critical infrastructure and national security. However, there are \nmany other types of formal and informal external information-sharing relationships, \nincluding 1:1 partnerships and groups comprised solely of private-sector organizations. \n External partnerships are most often used to share information about specific threats \nand best security practices. But some partnerships focus on other  types of information. \nFor example, security specialists within the high-tech sector share information in order to \ndevelop security standards, which are then implemented in various products. \n Much of this security information is sensitive. Because of this, we need to be able to \ntrust that the partners with whom we share information will treat it appropriately. The more \nsensitive the information, the greater the level of trust required. In general, the level of trust \ncan be higher in relationships with fewer people, allowing more-sensitive information to \nbe shared. As the number of people increases, there’s a greater chance that information will \nleak, so the level of trust tends to decrease and only less-sensitive information is shared. \n",
      "content_length": 3986,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n53\n Relationships therefore naturally tend to fall into a  tiered pyramid model , as shown \nin Figure  4-1 (Willis 2012). At the top of the pyramid are the most-trusted relationships \nwith the fewest partners; these are 1:1 partnerships between two individuals at different \norganizations, or between two security teams. \n Information-sharing relationships between more than two partners are often \nreferred to as communities. Because more people are involved, a legal or peer-enforced \nagreement is usually needed to define the level of trust and confidentiality expected \namong community members. \n The two middle tiers of the pyramid include groups with intermediate levels of trust, \nsharing information with varying levels of sensitivity. The  targeted tier typically consists \nof public-private partnerships aimed at protecting critical infrastructure. The  confidential \ntier includes many private-sector communities, including regional communities and \nthose focused on specific industry sectors. \n At the bottom of the pyramid is the  public tier , comprised of the largest communities \nwith the lowest level of trust. At this level, information is often public and may be broadcast \nvia the Internet. This tier might include groups that develop educational information about \nthreats for public distribution, or CISOs who share their insights via public webcasts. \n I should note that there is considerable overlap between these tiers. A group may have \ncharacteristics of both the targeted and confidential tiers, for example. Also, the number of \nmembers in groups within each tier (shown in Figure  4-1 ) is just a guideline: communities \nat all levels tend to grow over time as more organizations see the value and join. \n Figure 4-1.  Tiered pyramid model for trusted information-sharing partnerships and \ncommunities (adapted from Willis 2012). Source: Intel Corporation, 2012 \n \n",
      "content_length": 1964,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n54\n How can you get involved in information- sharing partnerships? One good method is \nto start by participating in communities in the public tier, where the information shared has \na relatively low level of sensitivity and therefore involves little risk. In these communities, \nyou’re likely to meet peers with whom you can begin to engage in 1:1 partnerships. As you \nbecome more knowledgeable about the communities that reflect your organization’s key \ninterests, you may then become involved in relationships in the middle tiers of the pyramid, \nwhere more confidential information is exchanged. I have always made sure that my teams \nand I actively participate in partnerships at all the tiers of the pyramid. \n 1:1  Partnerships Tier \n• \n Community structure : Direct communication between CISOs at \ntwo organizations or between their teams \n• \n Typical number of partners : 2 \n• \n Example partnership/community : Any two organizations who \nchoose to share information \n• \n Example goal : To mitigate shared threats by exchanging \ninformation with a business partner more quickly and in greater \ndetail than would be possible within a larger group \n• \n Trust framework : Personal trust and existing business \nrelationships \n Targeted Tier \n• \n Community structure : A relatively small number of critical \ninformation infrastructure owners and operators sharing \ninformation to protect the infrastructure. Also includes key \nsecurity ecosystem influencers, such as large security service \nproviders or vendors. \n• \n Typical number of partners : Up to about 50 \n• \n Example partnership/community : Information Sharing and \nAnalysis Centers (ISACs) \n• \n Example goal : To prevent advanced persistent threats (APTs) \nwithin the industrial base by sharing APT signature information \n• \n Trust framework : Strong information-sharing frameworks, such \nas national security clearances and nondisclosure agreements, \nare required. Trusted sharing mechanisms, such as encrypted \nweb portals with multifactor authentication, are also required. \n Confidential Tier \n• \n Community structure : Communities that represent industry \nsectors or other groupings, such as the banking sector and \nInternet service providers (ISPs), or regional forums \n• \n Typical number of partners : Up to about 100 \n",
      "content_length": 2356,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n55\n• \n Example partnership/community : BITS (financial services \nindustry), Bay Area CSO Council (regional), Regional CSO \nSummits \n• \n Example goal : To enable members to protect against common \nthreats and vulnerabilities affecting their industries. For example, \nISPs might share the command and control Internet addresses \nthat botnets use. \n• \n Trust framework : Communities typically use trust frameworks \nsuch as nondisclosure agreements or memoranda of \nunderstanding. \n Public Tier \n• \n Community structure : A broad range of communities that \nrepresent all user categories, including consumers, small- and \nmedium-sized businesses, and industry in general \n• \n Typical number of partners : 100s to 1,000s \n• \n Example partnership/community : Forum for Incident Response \nand Security Teams (FIRST), National Cyber Security Alliance \n• \n Example goal : To share best practices or informational bulletins \nabout widely known threats and vulnerabilities that affect a large \ncross-section of users. \n• \n Trust framework : Trust frameworks are not necessary; \ncommunities typically distribute information broadly through \nmechanisms such as e-mail distribution lists or public web sites. \n Let’s look at these tiers in more detail. \n 1:1  Partnerships \n In my experience, 1:1 partnerships are some of the most valuable security relationships. \nThey may be formal or informal, established at a corporate level or between individuals. \n As I explained, a key  advantage of a trusted 1:1 partnership is that we can more safely \nshare highly confidential information. We can often create a stronger bond with a single \nindividual than with a larger group. As a result, the shared information often has a depth \nand richness that’s lacking in information shared within larger communities. \n Another advantage is speed. Communication is often fastest in 1:1 partnerships, \npartly due to logistics. It’s much easier to set up a meeting between two people than it \nis to organize a meeting with a dozen people. To exchange information about the latest \ndevelopments, a CISO may be able to simply pick up the phone and have a conversation \nwith his or her peer. Quickly sharing information enables a faster response to threats—\nand in the security arena, timeliness is often critical.  \n",
      "content_length": 2352,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n56\n Here’s an example showing how 1:1 partnerships can develop and benefit both \npartners. Through my participation in a larger security community, I got to know \nthe CISO at a fast-growing e-commerce company whose customers were primarily \nconsumers. We both would contact each other periodically for advice and information \nas we puzzled over the latest security challenges. Over time, these conversations evolved \ninto open dialogues about best practices and benchmarking. \n The relationship eventually evolved to a point where we both realized we could learn \na great deal more by bringing our teams together in a face-to-face meeting. The resulting \nhalf-day meeting proved incredibly valuable to both teams. Our team was able to \nprovide insights and experiences about managing security in a large, complex enterprise \nenvironment. This was helpful to the security team at the fast-growing e-commerce \ncompany, which was in the process of building an enterprise environment to support \nits fast-growing business. In return, the team at the e-commerce company was able to \nshare the security challenges and experiences of operating a large consumer business \nwith millions of online customers. This was extremely valuable to us at Intel because we \nwere in the process of expanding our external online presence and were beginning to \nencounter some of the same challenges. \n The partnership thus expanded from ad hoc conversations to a productive relationship \nbetween teams sharing experiences and best practices at multiple levels. It’s hard to imagine \nthat this extensive information exchange could have occurred within a larger community. \n Another example: I met the CISO of a large manufacturing company at an industry \nevent, and we stayed in touch through occasional e-mails. Then, during a period of \nespecially large-scale industry attacks, our communications suddenly became much \nmore frequent and detailed. It was extremely valuable to be able to pick up the phone and \nsimply call a peer to share the latest knowledge about the attacks and responses. \n I have frequent 1:1 meetings with peers at other companies, sometimes as often as \nseveral times a week. These meetings can serve several purposes. A few years back, I met \nwith a team from a key supplier to discuss our strategy for securing employees’ personal \n(bring-your-own) devices. I shared our best practices with this team, and during the \nquestion-and-answer discussion, team members also provided information about how \nthey were addressing the same problem. The meeting served as a helpful benchmarking \nexercise for all of us. \n At the same time, the discussion clearly demonstrated each company’s commitment \nto protecting its partner’s business information. It showed the depth of each company’s \nstrategy for protecting information—revealing a commitment that extended far beyond \nthe desire to comply with contract confidentiality clauses. I felt more confident that if a \nsecurity issue ever arose, I could talk directly to my counterparts at the supplier company \nbecause their commitment to protecting information would enable a productive \napproach to resolving problems. \n Another recent discussion, this time with a potential customer, focused on the cloud. \nThe organization was concerned about our use of the cloud as part of our infrastructure, and \nalso as a part of the service connected to our product. Rather than respond to the lengthy \nsurvey they had put together, we met with them to discuss how Cylance uses the cloud and \nwhich data we store there. We discussed the risks that could exist in the cloud infrastructure, \nthe potential implications of those risks, and how we manage those risks. We also discussed \nother precautionary steps the customer could take to further mitigate the potential risks. This \ndiscussion helped develop a relationship that built the most customer trust. \n",
      "content_length": 3964,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n57\n Communities \n Participating in larger communities may not provide information that’s quite as rich and \ndeep as the information you’d obtain from a 1:1 partnership with a peer. But communities \nprovide value in other ways. \n Because they contain more people, communities provide breadth and diversity \nof perspective that help us make balanced risk decisions. With a larger number of \nparticipants, there’s a better chance that one of them will have developed a solution to a \nproblem, or can provide valuable new information about an industry attack. \n Some communities focus on sharing threat-related information; others on \nbenchmarking and best practices, influencing legislation, developing security standards, \nor public education. \n Communities can also present great networking opportunities. Through \nparticipation in communities, I’ve met several people with whom I’ve subsequently \ndeveloped closer 1:1 partnerships. \n Community Characteristics \n Like all groups, communities require a structure and a set of ground rules to be effective. \nSuccessful communities typically have the following characteristics:\n• \n Clear goals : The community shares clearly defined common \ngoals that benefit members, such as mitigating an industry-wide \nthreat. A community may have several goals. \n• \n A strong framework of trust, such as a legal or peer-enforced \nagreement, that addresses risks related to information sharing \namong community members : For example, the Industry \nConsortium for the Advancement of Security on the Internet \n(ICASI) has a strong multilateral nondisclosure agreement, while \nother communities, such as the Bay Area CSO Council, rely on a \npeer-enforced trust framework. \n• \n Trusted communications channels : Members can safely \ncontribute and access shared information using an effective \ntrusted communications channel or mechanism, such as a secure \nweb site. These channels are not always electronic; some regional \ngroups conduct face-to-face meetings to further reduce the risk of \ncompromise. \n An organization is most likely to benefit from joining communities if those \ncommunities align with the organization’s security goals. This means it’s important to \nfirst clearly define those organizational security goals. To do this, some organizations \nhave found it helpful to use a structured approach; they can more clearly categorize their \ngoals by mapping them to a standard risk management model, such as the “defense in \ndepth” model. Once an organization clearly understands its own security goals, it can \nidentify communities whose objectives align with these goals. \n",
      "content_length": 2683,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n58\n Because there is such a diverse range of organizations, security threats, and goals, it \nis unlikely that any single information-sharing community structure meets all the needs \nof a large organization. For example, a company might participate in one community for \nbenchmarking and another to tackle industry-specific threats. \n Information-sharing communities thrive only when the participating organizations \nfeel they’re receiving valuable information, creating incentives to continue to share \ninformation with others. \n What  constitutes valuable information ? A common definition is that information \nshould be timely, specific, relevant to participants’ concerns, and provides a suitable level \nof detail while protecting individual privacy (ENISA 2010). In practice, “valuable” usually \nmeans the information helps you achieve your security goals, whether those goals are \nlong-term and strategic, or short-term and operational. Information useful for strategic \ngoals might include an early warning that attackers are expected to target a specific \nindustry. This helps members of the community plan their defenses. Information useful \nfor operational goals typically includes more specific details, such as an attack signature. \nThis helps organizations more quickly identify an attack and respond when it occurs. \n As shown in Figure  4-1 (the targeted tier), some communities consist of government \nagencies working alongside an industry in what are usually known as  public-private \npartnerships (PPPs) . These PPPs can be particularly important for protecting critical \ninformation infrastructure. Internationally and within many nations, this infrastructure \nis largely owned and operated by the private sector, including carriers and network \nservice providers. Sharing information about  threats and attacks among public and \nprivate agencies therefore can help ensure security and resiliency of this infrastructure. \nBecause the shared information is highly sensitive, these PPPs usually have strong trust \nframeworks including national security clearances. \n An example of a much broader public-private community is InfraGard, a partnership \nbetween the FBI and private- and public-sector sector organizations that shares \ninformation and intelligence to prevent hostile acts against the U.S. \n Other communities are primarily comprised of  private-sector organizations . Some \nare industry-specific: members of an industry get together to  share threat information \nand best practices, helping to reduce risk for each company while enhancing the \nindustry’s reputation overall. Others involve sharing across industries, such as Evanta’s \nCISO Coalition, a cross-industry group of executives from large organizations. The \nCoalition is designed to facilitate secure, real-time interaction among members to vet \ncritical information security issues, and then share best practices for resolving them. As a \npart of my efforts to expand my external partnerships, I was fortunate enough to become \na founding member of this group’s advisory board. Another cross-industry group is the \nSecurity Advisor Alliance, a cybersecurity nonprofit dedicated to aligning CISOs to help \none another, supporting the information security community (including startups), and \ngiving back to schools and nonprofits. \n Some communities are regional, aimed at security professionals from private and \npublic-sector organizations located within a specific area. These  regional communities offer \nthe advantage of convenience. It takes less time, effort, and expense to attend a regional event, \nwhich makes participation more attractive. Examples of regional groups and forums include \nACTRA (see sidebar) and the San Francisco Bay Area CSO Council, described shortly. \n New communities arise frequently. A community may form in response to a specific \nthreat because companies are strongly motivated to share information about the threat in \norder to develop effective defenses. For example, the Conficker Work Group was formed \nspecifically to address the risk posed by the Conficker worm. \n",
      "content_length": 4163,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n59\n \nARIZONA CYBER THREAT RESPONSE ALLIANCE\n Innovative new models for information-sharing communities are springing up as the \nvalue of sharing security-related information becomes more widely recognized. An \nexample is the Arizona Cyber Threat Response  Alliance , Inc., a regional public-private \npartnership. This cross-sector group shares information about threats and other \nissues among partners from industry, academia, law enforcement, and intelligence. \n ACTRA grew out of relationships developed with FBI’s InfraGard, the public-sector \n Arizona Counter Terrorism Intelligence Center (ACTIC) , and the U.S. Department \nof Homeland Security. A key difference is that ACTRA is a nonprofit company with \na full-time president in addition to voluntary participants including a board and \ntechnical subject matter experts. The goal is to improve security for members with a \nflat, responsive organizational structure and without adding a burdensome layer of \nprocess. The group disseminates information ranging from alerts in near real time to \nwhite papers that provide insights and highlight best practices.  ACTRA has grown to \ninclude representatives from 14 critical infrastructure sectors. The group has found, \nbased on discussions with its members, that multi-sector sharing improves threat \nvisibility beyond the single-sector focus of industry-specific groups. \n Community Goals \n Communities may focus on narrowly defined goals, such as mitigating a specific threat, \nor they may have broader information-sharing goals, such as benchmarking security \ntechniques. A single community may pursue several goals. The most well-known types of \ngoals are sharing information about threats (to help member organizations mitigate those \nthreats) and sharing best practices (to improve efficiency). I’ll describe sharing goals next. \n Sharing Information about Threats and  Vulnerabilities \n Perhaps the best-known function of communities is to provide a trusted mechanism for \nsharing information about threats and vulnerabilities. Members of the community can \nuse this information to improve their tactical and strategic situational awareness. \n I’m often asked by peers how I measure the value of the information obtained \nfrom external partnerships. A key metric is whether the early threat information has \nhelped enable us to reduce risk. A single piece of information might make participation \nworthwhile if it helps us better mitigate risk and protect the company. \n Information from the community can also be useful for corroborating evidence \nthat we’ve already identified internally. If we observe a potential new threat within our \nenvironment, we may not feel that we have enough evidence to justify taking action. But \nwe can often discuss the issue within a community. If others are experiencing the same \nproblem, we can be more confident that it’s a real issue. This gives us enough reason to act. \n",
      "content_length": 2992,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n60\n Some examples of communities that share threat information include\n• \n Information Sharing and Analysis Centers ( ISACs ) : ISACs are \ntrusted industry-specific communities established by owners \nand operators of critical infrastructure resources. ISACs exist for \na number of industry sectors, including communications, retail, \nelectrical utility, health, and public transit. Services provided by \nISACs include risk mitigation, incident response, and alert and \ninformation sharing. \n• \n Bay Area CSO Council : This is a regional community that focuses \non improving the sharing of intelligence and best practices \namong CISOs in the San Francisco Bay Area. The Council \nserves as a vehicle for CISOs to safely and securely share their \nattack experiences. Members may share artifacts, such as attack \nsignatures, that they can then build into their organizations’ \ndetection and defense mechanisms (Jackson Higgins 2010). The \nforum uses a peer-enforced trust model rather than a formal legal \nframework. The group also creates subgroups to work on more \nhighly classified information. \n Sharing Best Practices and Benchmarking \n Many communities also serve as a forum for exchanging best practices and for \n benchmarking operations . By sharing security best practices, we may be able to increase \nthe efficiency and effectiveness of our own operations. \n Tapping into the expertise of others can help us avoid reinventing the wheel. A \ntypical example: A  CISO is trying to create a bring-your-own device policy for her own \norganization. So she sends a message to community members and receives detailed \nadvice from others who have already been through the process. This gives the CISO a \nhead start in creating a policy that meets her organization’s needs. \n Besides  enabling informal exchanges , communities may also operate formal \nbenchmarking exercises. Some of the best-known examples are the security-related \nprograms run by benchmarking firm CEB, Inc., which conducts studies and generates \nreports that compare companies in a variety of areas, from user security awareness to \ncontrols maturity (CEB 2015; also see the discussion of security awareness programs in \nChapter  5 ). Benchmarking information generated by communities can also be useful for \ndemonstrating the efficiency of security operations to other internal groups within your \norganization, such as an audit committee. \n Some benchmarking  information is sensitive and closely held because organizations \nfeel that it could reveal too much information about their security operations. Other \ninformation is more general and is sometimes publicly available, such as the webinars \nand presentations published online by Intel and others. Even this general benchmarking \ninformation may yield risk insights. Observing what other companies are focusing on, \nand how they are allocating resources, can help security professionals think about how \nthey need to manage risk within their own organizations. \n",
      "content_length": 3056,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n61\n One of the most established communities is the Forum for Incident Response and \nSecurity Teams ( FIRST ). This international group focuses on sharing best practices among \ncomputer security incident response teams. Trust relationships are peer-enforced. The group \npublishes a series of detailed best-practices guides and other documents for public use. Other \nactivities involve the exchange of information for cooperative incident management. \n Technology is helping to make information exchange more automated and therefore \neasier and faster, due in part to the adoption of standards for representing (STIX) and \ncommunicating (TAXII) information about threats. Platforms are emerging that use these \nstandards for rapid, secure information sharing. \n Many years ago, I was asked to manage Intel’s first major IT benchmarking activity. \nIt was a big task that entailed analyzing cost, quality, and other aspects of operations \nacross our entire IT environment. \n One of the first challenges was determining which organizations we should \nbenchmark ourselves against. At the time, the conventional wisdom at most \norganizations was that you should compare yourself with similar businesses. The \nlogic was that because these businesses were the most directly comparable, this \napproach would yield the most meaningful results. So the expectation was that I’d \nbenchmark our operations against a collection of other big high-tech companies. \n But I didn’t want to benchmark our operations against only high-tech companies. \nInstead, I wanted to benchmark against a broad base of companies in industries \nsuch as retail, banking, manufacturing, consumer goods, and utilities. \n The time came to present my selection of peer groups in a meeting with senior IT \nmanagement. By this time, I’d already started the benchmarking process, and as I \ndescribed the diversity of the companies included in the benchmark comparison, I \ncould sense the atmosphere becoming increasingly hostile. Practically everyone felt \nthat my approach was completely wrong. In fact, if there had been rotten tomatoes \nin the room, a few people would have been throwing them at me. \n So I asked for a moment of quiet so that I could explain. If we were an airline that \nwanted to benchmark operations, who would we compare ourselves with?” I asked. \nSeveral people said they’d benchmark against other airlines. \n “What do you think we would learn from that comparison?” I continued. “My guess \nis not much. We’d all have grown up in the same industry, and we’d probably have \nsimilar business processes. Many of our employees would have worked for the other \ncompanies and vice versa, so they’d probably implement similar practices. We might \nlearn about minor efficiency improvements, but I wouldn’t expect any breakthroughs.” \n BENCHMARKING: WHO SHOULD YOU COMPARE \nYOURSELF WITH?\n",
      "content_length": 2928,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n62\n “If I really wanted to dramatically improve the way I manage airline gate operations, \nI’d benchmark against a Formula 1 pit crew. Those crews can service a car and get \nit back on the road in 20 seconds or less. I’d think about what we could learn from \nstudying their processes, their technologies, and their ability to communicate and \norganize, and I’d try to figure out which aspects could cross over into airline data \noperations. If we want to make dramatic improvements, we need to look at people \nwho operate in an extreme operational environment—not at other airlines.” \n I’m happy to say that the managers in the room recognized that there might be \nvalue in the approach I was suggesting, even if many of them still disagreed with \nit. Ultimately, benchmarking against companies in a broad range of industries did \nhelp us achieve some dramatic improvements, and I received an internal award for \nthe initiative. The lesson is that sometimes we can learn more by looking outside a \nnarrowly defined, traditional peer group. People in the same industry may be facing \nthe same problems as we are and dealing with them the same way. For a fresh \nperspective, it can be worth looking farther afield. \n Influencing  Regulations and Standards \n All of us operate within an increasingly complex regulatory environment, and we’re all \naffected by evolving technology standards. \n It’s important to stay abreast of legislative developments. That can be a difficult and \ntime-consuming job for any single organization, and so it may be helpful to become \ninvolved in a community whose goals include tracking regulatory activity. \n In addition, communities can sometimes help influence public policy more \neffectively than a single organization can do alone. There’s strength in numbers, and \ncommunities often include some of the biggest companies in an industry. \n An example of a community that focuses on policy is BITS (  www.bits.org  ), the \ntechnology policy division of The Financial Services Roundtable, which represents 100 \nof the largest integrated providers of consumer financial services. Members of BITS \ncooperate on issues such as critical infrastructure protection, fraud prevention, and \nthe safety of financial services. The organization works to influence public policy by \ncommunicating with public agencies. It also publishes reports for use across the industry, \nincluding a financial services security assessment. Thus, communities that focus on \npolicy may help all participating companies and the reputation of the industry overall. \n Businesses who offer services in multiple countries have a particular interest in the \ninternational regulatory environment. These include multinationals, of course, which are \ndirectly affected by the complex web of regulations at international, national, and local levels. \n However, these regulations affect a surprisingly large number of other companies, \nincluding many that don’t have employees or facilities physically located in other \ncountries. Today, almost any business with a web-based service consumed in multiple \ncountries is effectively operating in a multinational environment. Regulations in those \ncountries have impacts that stretch beyond geographical boundaries. For example, \nregional and local regulations such as the California data breach bill (SB1386) and \nEuropean privacy guidelines require compliance by any company that stores information \nabout residents of those areas, no matter where the company is located. \n",
      "content_length": 3582,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "CHAPTER 4 ■ EXTERNAL PARTNERSHIPS: THE POWER OF SHARING INFORMATION\n63\n Corporate Citizenship \n At many companies a large number of employees volunteer in ways that benefit their \nneighborhood or a wide variety of worthy causes. Businesses often provide support to \nhelp employees do this. There’s a growing trend to leverage the organization’s talent and \nexpertise in volunteer corporate citizenship initiatives that are more closely related to the \norganization’s goals and employees’ technical expertise. Examples might include offering \nexpert security advice to nonprofits or helping security initiatives in other countries. \n Security-related corporate citizenship initiatives include the National Cyber Security \nAlliance, whose mission is to educate and empower society to use the Internet safely and \nsecurely (see  staysafeonline.org ). The sponsors of the alliance include large high-\ntech companies such as Intel. Senior managers at those companies also are among the \ndirectors of the organization. \n Conclusion \n The knowledge we acquire via external partnerships can help us protect our own \norganizations. I’ve experienced this first hand; indicators of compromise shared by others \nhave helped me understand and respond to threats. The growth of information-sharing \ngroups shows that many organizations are coming to the same conclusion. As Ken \nAthanasiou, Global Information Security Director at American Eagle Outfitters, said in a \nstatement supporting the formation of the new retail ISAC: “Cyber-criminals work non-\nstop, and are becoming increasingly sophisticated in their methods of attack … by sharing \ninformation and leading practices and working together, the industry will be better \npositioned to combat these criminals” (Retail Cyber Intelligence Sharing Center 2015). \n Industry-specific groups such as the financial and industrial control ISACs have \nbeen widely acknowledged as helping companies quickly learn about threats and \nspecific measures for combating them. Other groups provide different kinds of valuable \ninformation. The Evanta CISO Coalition has published metrics that its members can use \nfor security benchmarking and dashboarding. Members of IASAP share information that \nhelps them improve their awareness programs. \n The  security landscape has become increasingly complex and dynamic, and \nit’s difficult to track and manage the risks without help from others. Sharing security \ninformation is also becoming more important as organizations increasingly collaborate \nwith business partners and adopt new technologies. Understanding the risks faced \nby our partners, and the way they manage those risks, can help us protect our own \norganizations. As businesses move into new markets and use technology in new ways, we \nneed to understand our biggest exposures and how to allocate resources most effectively \nto minimize business risk. Therefore, sharing information can help businesses remain \ncompetitive and successful. \n Organizations have often been reluctant to share security information, but if we \nwant help from other people, we have to be prepared to share information ourselves. By \ncarefully using trusted partnerships that align with our security goals, we can increase our \norganization’s ability to sense, interpret, and act on risk. \n",
      "content_length": 3307,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "65\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_5\n CHAPTER 5 \n People Are the Perimeter \n There’s a difference between interest and commitment. When you’re \ninterested in doing something, you do it only when circumstances permit. \nWhen you’re committed to something, you accept no excuses, only results. \n —Art Turock \n A few years ago, a senior manager began bringing his corporate laptop into the cafeteria \nat lunchtime. Typically, he’d find an empty table, set down the laptop, and then walk out \nof sight to get his lunch. As he perused the salads and main courses, made selections, and \npaid for his food, his laptop sat unattended in plain view of hundreds of people using the \nlarge cafeteria. \n My security team noticed the neglected laptop and pointed it out to me. I discussed \nthe issue with the manager a few times, but he continued leaving the laptop unattended. \nSo eventually, I began taking the laptop and leaving my business card in its place. \n Not surprisingly, the manager became somewhat annoyed. “Nobody’s going to steal \nthe laptop because there are all these people around,” he said. \n “Okay,” I responded. “I’ll never take your laptop or complain again on one condition. \nIf you really trust everybody here, you’ll take off your wedding ring and leave it on top of \nthe laptop. If you do that, you’ll never hear from me again.” \n He thought about this for a while. Then he said, “You made your point.” And he never \nagain left the laptop unattended. \n The  Shifting Perimeter \n This incident helped crystallize in my mind a new perspective about how we should \napproach information security. It demonstrated how each person’s daily decisions can \naffect the risk dynamics of the company overall. \n The traditional enterprise security paradigm, often expressed in castle-and-\ndrawbridge terms, described a wall of technology that isolated and completely protected \nthe workers behind it. To protect our people and information assets, we focused our \nefforts on fortifying the network perimeter and the physical perimeter of our buildings. \n",
      "content_length": 2132,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n66\n Today, however, a growing number of  user interactions with the outside world \nbypass the physical and network perimeters and the security controls these perimeters \noffer. They take place on external web sites and social networks, on laptops in coffee \nshops and homes, and on personal devices such as smartphones. As the Internet of \nThings unfolds, those interactions will also take place on many more “things,” such as \nwearables, cars, and even household appliances. \n This changing environment doesn’t mean the security perimeter has vanished. \nInstead, it has shifted to the user. The laptop left unattended in the cafeteria was clearly \ninside the physical perimeter, but the corporate information it contained was still \npotentially at risk due to the manager’s actions. People have become part of the perimeter. \nUsers’ decisions can have as much impact on security as the technical controls we use. \n Over the past few years, the idea of the people perimeter has won wider recognition \nand acceptance. Accordingly, organizations are placing more emphasis on employees’ \nsecurity awareness and behavior. \n One reason for this is the rash of high-profile insider exploits, such as the leaks by \nNational Security Agency contractor Edward Snowden. Another is that technical controls \nhave not kept pace with the attackers. Many exploits are reaching users because technical \ncontrols, particularly those on endpoint devices, have failed to prevent them. We are \ntherefore more reliant on the user’s ability to detect suspicious activity. We also have \nbeen forced to deploy more back-end detection and response tools and staff to handle \nthe flow of malware penetrating the corporate infrastructure. These ever-growing security \noperations teams, which become another layer of the people perimeter, typically are \nunable to keep up with the flood of malware and commit errors due to “alert fatigue.” \n There’s a continuing emphasis on  phishing attacks ; the  2015 Data Breach \nInvestigations Report found that the percentage of users deceived by phishing actually \nincreased from previous years, with 23% opening phishing messages and 11% clicking on \nattachments (Verizon 2015). \n Older  social-engineering techniques are also still effective, apparently. At hedge fund \nFortelus Capital Management in London, the chief financial officer received an alarming \nphone call one Friday afternoon. The caller said he was from the company’s bank, and \nwarned of possible fraudulent activity on the account. The CFO reluctantly agreed to \ngenerate codes enabling the caller to cancel 15 suspicious payments. When he logged \ninto the firm’s bank account the following Monday, $1.2 million was gone. The CFO lost \nhis job and was sued by his firm for failing to protect its assets (Chelel 2015). \n As almost every company becomes a technology developer as well as a technology \nconsumer, employee security awareness behavior will become an even bigger issue. \nSecurity lapses by the employees working on technology-based products can have far-\nreaching impacts, creating vulnerabilities in the digital services and physical products \ndelivered to millions of customers. \n Compliance or  Commitment ? \n Each day, employees make decisions that can affect the company’s information risk. Do \nI leave my computer unattended or not? Do I post this information on social media? Do I \ninstall this software on my device? Do I report this suspicious looking e-mail? When I’m \nin a coffee shop, do I connect to the corporate infrastructure via a secure virtual private \nnetwork, or do I engage directly over the Internet? \n",
      "content_length": 3656,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n67\n We could view each of these decisions purely in terms of the potential for increased \nrisk. However, there’s also a positive side. If users become more aware of security and make \nbetter decisions, they can strengthen the organization’s defenses by helping identify threats \nand prevent impact. Among CISOs surveyed recently by best-practices firm Corporate \nExecutive Board, 50% said that insecure behaviors cause more than half of all breaches; but \nthey also said employees are key to uncovering suspicious activity (CEB 2015). \n Therefore, as information security professionals, we are in the behavior modification \nbusiness. Our goals include creating a more security-conscious workforce so that users \nare more aware of threats and vulnerabilities, and make better security decisions. \nFurthermore, we need to influence employees’ behavior both within the workplace and \nwhen they are home or traveling. \n If the manager was comfortable leaving his laptop unattended in our cafeteria, \nwould he also leave it unattended at the local coffee shop? At the airport? Or somewhere \nelse where the risk of loss was even greater? My belief is he probably would. When trying \nto influence this person’s behavior, I wanted to achieve more than a level of compliance. I \nwanted to initiate a feeling of commitment. \n The term  compliant behavior implies making the minimum effort necessary to \nachieve good performance to a predefined standard. It’s like checking boxes on a list of \nsecurity compliance items. Ultimately, employees feel they are being compelled to follow \nsomeone else’s list of instructions. Because of this, compliance requires supervision and \npolicing, and employees may sometimes engage in lengthy recreational complaining. \nIf employees are simply following a checklist, what happens when they encounter a \nsituation that’s not on the list? They stop and await further instructions, or perhaps they \nare even unaware of the threat or ignore it. \n In contrast,  committed behavior is intrinsically motivated and self-directed. Being \ncommitted implies that people are emotionally impelled to invest in security; they \ntake responsibility and ownership. When people feel committed, they tend to deliver \nabove and beyond the bare minimum. Rather than simply following a predefined list of \ninstructions, they are empowered to make decisions and judgment calls in real time, with \na focus on how their actions affect others as well as themselves. \n If we can create this sense of commitment in our users, we can implement security \nnot as a wall but as a collective security force that permeates the entire organization. \nIndividually and as a group, every person in the corporation uses their skills in security to \nprotect the organization, handling known attacks today as well as quickly adapting to new \nthreats tomorrow. \n When I needed to influence the manager’s behavior, I looked for a way to establish \nthis level of commitment. I sought to change the way he felt about the laptop, and to do \nthis I tapped into his emotional connection to his wedding ring. \n Creating a culture of self-motivated commitment rather than compliance can make \na big difference, as shown in studies by management guru Dov Seidman. His group \nlooked at behavioral differences between businesses with a culture of self-governance, \nin which an organization’s purpose and values inform employee decision-making and \nbehavior, and those with a culture of blind obedience based on command-and-control \nand coercion. Organizations based on self-governance experienced three times more \nemployee loyalty and half as many incidents of misconduct, compared with organizations \nbased on blind obedience (Seidman 2011). \n",
      "content_length": 3756,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n68\n The implications for enterprise security are clear. As the boundaries between \npersonal and corporate computing dissolve, employees may be accessing information \nfrom any location, on any device. If users behave in an insecure way while they are in \nthe office, it’s likely they will also exhibit insecure behavior when they’re elsewhere. \nConversely, if we can create a feeling of commitment that causes them to own \nresponsibility for security, there’s a better chance they will behave more securely both \nwithin the workplace and when they are outside our physical perimeter. This change \nin behavior improves the security of the device they are using, the information they are \naccessing, their personal lives, and the enterprise.  \n Examining the  Risks \n Before discussing ways that we can modify user behavior, I’d like to briefly mention some \nexamples of what can happen if we don’t influence the ways that users think and act. \n As an experiment, the US Department of Homeland Security secretly dropped disks \nand thumb drives in the parking lots of government and private contractors’ buildings. \nTheir goal was to see whether people would pick them up and plug them into their \ncomputers. As reported by Bloomberg News (Edwards et al. 2011), up to 60 percent of the \npeople who picked up the items inserted them into their office computers. That number \nrose to 90 percent if the item included an official-looking logo. Clearly, the security \nbehavior of employees at these facilities left quite a bit to be desired. \n Insider threats unfortunately continue to make the news. A former JPMorgan Chase & \nCo. employee was arrested by the FBI on charges of stealing customer data and trying to sell it \nto an undercover informant. As noted by  CSO , similar incidents have occurred multiple times \nat the bank over the past few years, illustrating the company’s inability to account for insider \nthreats despite its substantial annual spending on security technology (Lambert 2015). \n Think about what can happen with newer, more sophisticated exploits. A \nsophisticated attack targeted government departments using fake voice-mails to \ndistract users while malware downloaded in the background. Using social engineering \nand targeted e-mails, the attackers tricked users into visiting web sites harboring self-\nextracting archives. The archives contained a recording media file purporting to be a \nvoice-mail from a female journalist seeking information for a news story, alongside other \nfiles that downloaded malicious content (CNET 2015). \n As in the example above, today’s threats may arrive in the form of carefully personalized \nspearphishing communications designed to win the trust of targeted users. These users then \nunwittingly provide access to the information the attackers want. In essence, trust—in this \ncase, the organization’s trust in the user—has become the attack surface. \n Let’s say a company is looking to hire a credit analyst with a very specific set of skills. \nAttackers notice this and apply online, using a résumé that lists the exact skills required for the \njob and contains the terms the company’s résumé-scanning software is likely to be looking \nfor. Suitably impressed, the company’s human-resources specialists forward the application \nto the company’s credit-department manager, who has access to all the systems storing \ncustomer financial data. The manager trusts this communication because it has been sent \nfrom another department within the same company. So she clicks on the link to the résumé. \nUnfortunately, that action triggers the execution of malicious code. The human-resources \nteam effectively acted as an infection agent, ensuring the attack reached its real target. \n",
      "content_length": 3773,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n69\n Social media accounts can become sources of risk even when they haven’t been \ncompromised. There have been several examples in which senior executives accidentally \nrevealed information that was confidential or problematic in other ways. In November \n2014, Twitter’s CFO accidentally publicly tweeted a plan to buy another company, \nincluding the fact that he wanted help to make the deal happen at a meeting the following \nmonth. The CFO was apparently trying to send the message privately (Frier 2014).  \n At Houston-based fashion retailer Francesca's Holdings, a former CFO frequently \nshared his thoughts via a personal blog, Facebook page, and Twitter feed (Silverman \n2012). Unfortunately, he also shared information that caused problems for his employer. \nThe company fired him because he \"improperly communicated company information \nthrough social media.” \n Users frequently post information on external social-media sites that attracts the \nattention of competitors or the media. To boost their job prospects, interns mention \nproduct features they helped develop during their summer job at a well-known company; \nsales representatives reveal the names of major clients; even senior executives have \nbeen known to unintentionally disclose key corporate strategies. In fact, services exist \nthat specialize in aggregating apparently minor snippets of information from social-\nmedia and other web sites to build an accurate view of a company’s size, geographical \ndistribution, and business strategy, including hiring patterns that indicate whether the \ncompany is expanding and which new areas it is moving into. \n Adjusting Behavior \n To counter these risks, we need to make employees aware and empowered so they act \nas an effective part of the security perimeter. Increasing recognition of this need has led \nto the development of a small ecosystem focused on increasing security awareness and \n changing behavior , ranging from companies offering best practices to groups focused on \ninternet safety for children. This includes companies that train users to avoid phishing \nexploits, using simulated phishing scenarios and other tools. Security awareness \nprofessionals have come together to share best practices (see sidebar). \n While I was Intel’s CISO and then Chief Security and Privacy Officer, we focused on \nbuilding security and privacy protection into the corporate culture, getting employees \nto own responsibility for protecting enterprise and personal information. Achieving \nthis required a lot of effort, and we realized that it took just as much work to maintain a \nculture of security and privacy as to build it. \n Training is a key part of security efforts at most companies, and Intel is no \nexception. We supplemented general training, which fulfills most legal requirements, \nwith specialized training for employees who have specific roles or access to sensitive \ninformation. Another effective technique was to embed security and privacy training into \nbusiness processes. When an employee requested access to an application that handles \nsensitive information, they were automatically prompted to take training that focused on \nthe related security and privacy concerns. We also used online training including video \nand other visually stimulating material as well as entertaining, interactive tools to help \nengage users (see Figure  5-1 ). \n",
      "content_length": 3419,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n70\n However, it is not enough to create good training. If nobody takes the training, the \neffort is wasted. We found that incentives such as public recognition helped ensure \nemployees underwent training and absorbed the lessons. Ultimately, if people continued \nto avoid security training, we escalated compliance efforts by directly contacting them \nand their managers. \n We also found we could help maintain and increase awareness by publishing security-\nrelated articles on Intel’s primary employee portal. Many of these articles included a \npersonal aspect, such as preventing identity theft, keeping children safe online, and home \nwireless security tips. The focus on personal concerns recognized that the way employees \nbehave outside the office is as important to enterprise security as their behavior in the office. \n How did we know our security efforts paid off? We accumulated a variety of \nevidence, including independent benchmark results from Corporate Executive Board \n(2011), which indicated that Intel employees consistently ranked in the top 10 percent \nof companies for secure behavior. We also observed that employees acted as part of the \nsecurity perimeter by alerting us to suspicious text messages or e-mails they’d received. \n Figure 5-1.  Intel’s internal “Find the Phish”  interactive training tool helps employees spot \nweb scams. Source: Intel Corporation, 2012 \n \n",
      "content_length": 1433,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n71\n A Model for Improving Security Awareness \n Some of the industry’s most valuable work to advance organizational understanding of \n security awareness comes from best-practices firm CEB. The company offers a security \nawareness service that includes surveying key security-related employee behaviors at \nmember organizations, and benchmarking the results against other organizations. The \nprogram attempts to understand the psychological reasons for insecure behavior; it then \nfocuses on those psychological drivers when suggesting tactics to change employees’ \nbehavior. To date, CEB has collected some 300,000 employee responses from over 400 \norganizations. \n The program has found that despite the importance of secure employee behavior, \nmost organizations deliver only moderate amounts of  training : just over an hour per year, \non average, and only three to four employee communications per month. Survey results \nsuggest that organizations can increase training time to as much as six hours a year before \nexperiencing diminishing returns. \n CEB emphasizes that organizations need to use an understanding of psychology to \ntailor their awareness efforts; awareness programs must target the specific root causes of \nemployees’ risky behavior in order to be effective. The company initially identified five \npsychological factors influencing security awareness and behavior: lack of knowledge of \npolicy, lack of self-interest in security, inadequate perception of the risk to the organization, \na low emotional commitment to security, and a perception that secure behavior imposes a \nhigh burden. It recently added a sixth factor: the ability to display good judgment. \n CEB’s findings suggest that the perceived burden of secure activities affects employee \nbehavior more than any other psychological driver. That’s the bad news. The good news \nis organizations can fix the perception of the burden both by reducing the burden itself \nand by addressing the other drivers. For example, employees’ emotional commitment \nto security increases if their managers engage with them directly to emphasize the risks. \nClear enforcement of policy compliance increases employees’ self-interest in secure \nbehavior and heightens their perception of risks. \n Based on survey data collected over the years, CEB has developed a model that \norganizations can use to help plan and assess their security awareness programs. \nThe model presents a four–stage progression toward higher security awareness and \ncommitment, from basic check-the-box compliance to active involvement in security. It \nrecognizes that in a complex threat environment, we can no longer rely only on policies \nthat prescribe specific employee behaviors: we also need to enable employees to actively \nsupport security activities including breach detection. \n",
      "content_length": 2861,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n72\n Employee awareness programs at Level 1 (Check the Box) simply respond to external \nregulatory requirements and don’t explicitly aim to change specific employee behaviors. At \nLevel 2, programs try to  encourage users to adopt specific, simple behaviors, such as avoiding \nsharing passwords and sending sensitive information to their own personal e-mail addresses. \n The third and fourth levels display greater levels of  judgment and commitment . \nAt Level 3, employees are able to make good judgment calls in the moment, especially \nin situations where the right answer is not immediately obvious. For example, they \nremember to pause before clicking to check whether an e-mail comes from a legitimate \nsource or contains a phishing link. At Level 4, employees become an extension of the \ninformation security organization; they not only avoid security risks, but also notify \ninformation security when they see something suspicious. A key behavior here is an \nincrease in reporting events such as spearphishing attempts. \n The  encouraging news is that that CEB surveys show a gradual improvement; the \npercentage of employees avoiding insecure behaviors such as password sharing has \nslowly increased over the past six years. Resistance to phishing has improved faster, \nthough from a lower base. “I think at progressive companies the aspiration is changing,” \nsays CEB practice leader Jeremy Bergsman. “Most companies have been moving \nfrom Level 1 to Level 2 over several years, and are starting to think about Level 3. But \nprogressive companies are moving beyond employee behavior as a risk to be reduced, \nand working on ways to make employees a control—an early warning system (Level 4).” \n For example, a large  telecommunications firm that participated in the CEB security \nawareness program wanted to empower employees to act as controls supporting \ninformation security. It provided each employee with a weekly report tracking his or her \nbehavior, including the documents they accessed, and the devices and external locations \nused to connect to corporate systems (Figure  5-3 ). Employees were responsible for \nreading the reports, thus sharing responsibility with information security for detecting \n Figure 5-2.  A  four-stage model for programs seeking to improve security awareness and \nbehavior. Source: CEB Inc., 2015 \n \n",
      "content_length": 2383,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n73\nbreaches. The firm found that employees detected suspicious activities faster than would \nhave otherwise been the case; users also proactively improved security by suggesting \nother activities that should be tracked and added to future reports. \n \n INTERNATIONAL ASSOCIATION OF SECURITY \nAWARENESS PROFESSIONALS\n Exchanging ideas with other security professionals can help improve security \nawareness programs. The International Association of Security Awareness \nProfessionals (  www.iasapgroup.org  ) is an independent association of corporate \nsecurity specialists who are seeking to do just that. IASAP is a non-profit, fee-based \nassociation dedicated solely to security awareness programs. Its goal is to serve \nas a trusted forum of security awareness professionals collaborating to improve \nemployee security behavior. “Clear guidance has not been as available for employee \nsecurity behaviors as it has been for technology solutions,” says IASAP board \nmember Michael Diamond. “Several awareness professionals noticed this gap, and \nthat ultimately led to the formation of IASAP.” \n Some members have built programs from scratch; others inherited established \nprograms in need of fresh ideas and new energy. Members meet in person two to \nthree times per year to learn about other members’ security programs and present \ntheir own. There’s also a members-only sharing platform supporting Q&A, feedback, \nbenchmarking surveys, member polls, guest speaker webinars, and teleconferences. \nSome members feel comfortable posting program resources that are available to \nother members for re-branding within their own programs. \n Figure 5-3.  Example of weekly  tracking report showing employees their activity. \nSource: CEB Inc., 2015 \n \n",
      "content_length": 1782,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n74\n Broadening the  Awareness Model \n I think that the CEB four-stage model shown in Figure  5-2 is a very useful tool. One \nlimitation, in my opinion, is that the model is based on the traditional organizational \nview of information security. I believe that we need to expand the model to capture a \nmore complete view of information risk. You might think of the following two additions as \nLevels 3a and 4a, respectively, of the model. \n• \n Privacy awareness, which has become a critical concern. Just \nconsider the number of breaches that have targeted personal \ninformation at retail, healthcare, and government organizations.  \n• \n A specific focus on engineers and other technology professionals, \nincluding those creating technology-based products and services. \nIf engineers don’t have a foundational understanding of privacy \nand security, they cannot design privacy and security into the \ntechnology they produce. As a result, a company’s products may \ncontain vulnerabilities that introduce significant risks for the \nbusiness and its customers. \n The  Security Benefits of Personal Use \n Employees use an ever-growing variety of  personal devices every day, both inside and \noutside the physical workplace. This trend started with smartphones and laptops; it \nalso includes wearable devices such as smartwatches and fitness monitors. Information \nsecurity specialists naturally tend to focus on the security risks of using these devices for \nbusiness purposes. As I discussed earlier in the book, I’ve found that the productivity \nbenefits of personal devices often outweigh the risks. But even the security implications \nare not as one-sided as they might seem at first glance. I believe that, in some respects, \nallowing personal use may actually encourage better security. \n In general, people are likely to take better care of their own possessions than \nsomeone else’s. They feel a stronger connection to their own car than to one provided \nby their employer. If people are using their own computing device, they may take better \nprecautions against theft or loss. And they may feel the same way if they are storing \npersonal information on a corporate device. At Intel, we allowed reasonable personal use \nof corporate laptops, and therefore many employees stored personal as well as corporate \ninformation on their laptops. Because of this, they had a personal stake in ensuring the \ndevices didn’t get lost or stolen. I believe this sense of ownership contributed to our \nlower-than-average laptop loss rates. \n Another company’s experience provided some empirical evidence supporting this \nidea. The company conducted a tablet pilot deployment in which, for the first time, it \nallowed personal use of corporate devices. The company found that breakage and loss \nrates were dramatically reduced compared to its past experience with mobile devices. \nThe CIO’s conclusion was that employees simply take better care of devices when they \nuse them for personal purposes. \n Perhaps we should be similarly open-minded when considering the security \nimplications of wearable devices. I met with managers at a large company who were \npondering the security implications of smartwatches and fitness monitors, which \n",
      "content_length": 3265,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n75\nemployees were already bringing into the workplace. Understandably, some people at the \ncompany wanted to make sure the devices could not interact with the corporate network. \nI observed that in the future, wearables could be harnessed to help identify users in ways \nthat are less cumbersome for users than traditional controls such as passwords. Fitness \ndevices, including some smartwatches, count the user’s steps and monitor heart rate, and \ncould therefore be used as biometric security devices in the future. As the devices evolve \nand accumulate more user data over time, they may become more adept at identifying \neach user’s physiological and behavioral “signature.” In addition, some smartphones \ninclude fingerprint recognition, which in itself can be a powerful authentication \nmechanism if the technology has been properly designed and implemented. \n As security professionals, shouldn’t we think about taking advantage of the benefits \nthese technologies offer? We should seek to integrate into security strategies the broader \nvariety of existing devices, which have useful features such as cameras and voice recognition \nand also contain data about our use patterns. Many of these devices already communicate \nwith each other; why not take the next step and use the technology to eliminate the pain of \nusing passwords? Why not find a way to reduce risk and cost, while providing a much better \nuser experience, by using these devices to authenticate us automatically? \n It may also be worthwhile to reexamine other assumptions about the security \nimplications of personal devices. Some companies have policies forbidding the use of \ncameras in their offices. However, a smartphone includes a camera that employees can \nuse to capture the off-the-cuff design sketches often scrawled on whiteboards during \nbrainstorming sessions. This intellectual property can then be stored and encrypted on a \nhard drive within the enterprise. Is it safer to allow employees to photograph the image, \nor to copy it onto a piece of paper, or to leave it on the whiteboard where anyone might \nsee it? Companies may come to different conclusions, depending on their culture and \nappetite for risk. But this is another illustration of the importance of considering all the \npossible business benefits as well as the risks when making technology decisions.  \n Roundabouts and Stop Signs \n To try to reduce driving accidents at a dangerous curve in Chicago, the city painted a \nseries of white lines across the road. As drivers approached the sharpest point of the \ncurve, the spacing between the lines progressively decreased, giving the drivers the \nillusion they were speeding up, and nudging them to tap their brakes. The result was a 36 \npercent drop in crashes, as described by Richard Thaler and Cass Sunstein in their book \n Nudge (Yale University Press, 2008). \n This  traffic-control method succeeded in making drivers more aware and improving \nsafety while keeping the traffic flowing with minimum disruption. I think this example \nprovides a useful metaphor for information security. Some security controls are like stop \nsigns or barriers: we simply block access to technology or data. But if we can shape the \nbehavior of employees rather than blocking them altogether, we’ll allow employees, and \ntherefore the company, to move faster. \n To use another traffic metaphor, a roundabout at an intersection typically results in \nmore efficient traffic flow than an intersection with stop signs, because drivers don’t have \nto come to a complete halt. The roundabout increases drivers’ awareness, but they can \nproceed without stopping if the way is clear. Statistics have shown roundabouts are often \nsafer than intersections. \n",
      "content_length": 3776,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n76\n Of course, we need to block access in some situations, such as with illegal web sites. \nBut there are cases where it’s more efficient and productive to make users aware of the \nrisks, yet leave them empowered to make the decisions themselves. \n Consider the case of a large multinational company whose business relied heavily \non its significant intellectual property. To protect that proprietary IP, the company \nimplemented data-loss protection software, including an application on employees’ \nlaptops. But instead of simply blocking transmission of information flagged as sensitive, \nthe company configured the software to warn employees whenever it detected potentially \ninsecure behavior. If an employee tried to transmit a confidential document, the software \ndisplayed a message that explained the potential risks and suggested ways to protect \nthe information, such as encryption. After all, users may have good reasons for sending \nconfidential documents, and preventing transmission could be detrimental rather than \nbeneficial to the business. The company found that this warning caused 70% of users \nto change their behavior, representing a major reduction in risk. Yet because of the way \nthe software was configured, users didn’t complain about the security burden. The \nroundabout approach reduced risk without interfering with users’ productivity. \n Here’s another  hypothetical example. It may make sense to warn users visiting \ncertain countries that they may be accessing material that is considered unacceptable. \nA US employee traveling on business might be working in a local office of a country with \nstrict religious guidelines. The employee has a daughter who’s in a beauty pageant, so it \nwould be natural to check the pageant web site from time to time. But the images could \nbe offensive in the country, so it makes sense to warn the employee to exercise caution. At \nIntel, we found that when we warn users in this way about potentially hazardous sites, the \nvast majority heed the warnings and don’t access the web sites.  \n In the case of information security, there’s an additional benefit of making controls \nas streamlined as possible. We all know if controls are too cumbersome or unreasonable, \nusers may simply find ways around them. We kept this concern in mind when developing \na social media strategy at Intel IT (Buczek and Harkins 2009). We were well aware of the \nrisks associated with social media, but attempting to stop the use of external social media \nweb sites would have been counterproductive and, in any case, impossible. We realized \nthat if we did not embrace social media and define ways to use it, we would lose the \nopportunity to shape employee behavior. \n As part of our initial investigation, we conducted a social media risk assessment. \nWe found social media does not create new risks, but can increase existing ones. For \nexample, there’s always been a risk that information can be sent to inappropriate people \noutside the organization. However, posting the same information on a blog or forum \nincreases the risk by immediately exposing the information to a much wider audience. \n So we developed a social media strategy that included several key elements. We \ndetermined that we could reduce risk by implementing social media tools within the \norganization, so we deployed internal capabilities such as wikis, forums, and blogs. \nInitially, employees used these tools mainly to connect socially rather than for core \nbusiness functions; we later integrated the tools into line-of-business applications to \nachieve project and business goals. We also worked with Intel’s human-resources groups \nto develop guidelines for employee participation in external social media sites, and \ndeveloped an instructional video that was posted on a public video-sharing site. The \nvideo candidly explained that Intel wanted to use social media to open communications \nchannels with customers, partners, and influencers, to encourage people to adopt the \ntechnology, and to close the feedback loop. The information also included guidance \n",
      "content_length": 4131,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n77\nabout how to create successful content and general usage guidelines. We also used \ntechnology to help ensure that employees followed the guidelines. We monitored the \nInternet for posts containing information that could expose us to risks, and we also \nmonitored internal social media sites to detect exposure of sensitive information and \nviolations of workplace ethics or privacy.  \n The Technology  Professional \n So far, I’ve focused mainly on the security roles of end users. But think about the \nbroadening roles that technology professionals play at many companies. Historically, \n technology professionals have performed back-office IT roles at most companies, such \nas managing infrastructure and internal applications. Many also work on web sites and \nonline services. We’re now moving into a future in which companies in all industries will \nbecome creators of technology embedded in physical as well as digital products, and \nthey’ll hire developers to create that technology. These technical professionals are also \npart of the people perimeter, and their actions can have major positive or negative effects. \n We’ve already seen several well-publicized problems caused by vulnerabilities in \nproducts. Fiat Chrysler recalled Jeeps in 2015 after researchers showed they could hack \ninto a 2014 model and hijack its steering, brakes, and transmission. The researchers used \nan unsecured communications port to execute the attack (Dark Reading 2015). Similar \nconcerns prompted the FDA to order organizations to stop using older drug infusion \npumps made by Hospira when it was found that an unauthorized user could hack into \nthem and change the dosage the pump delivers. \n In traditional IT roles, technical professionals manage almost every element of the \ntechnology spanning our networks, data centers, and users’ computing devices. They \ndevelop and install software. They configure, administer, and monitor systems. Their \nactions or inaction can make the difference between a system that is vulnerable and one \nthat is reasonably secure.  \n Those systems include servers, which are still the IT assets most commonly attacked \nand robbed of data. An attacker may initially gain access to your company by compromising a \nuser’s laptop, but the biggest prize—databases of corporate intellectual property and personal \ninformation—still reside on the enterprise servers. To steal that information, attackers now \ntypically often use a compromised end-user device to search the network for servers with \ninadequately configured access controls. Surveys show many attacks continue to exploit \nsecurity holes that organizations could easily have fixed. Among organizations surveyed for \nthe 2015  Data Breach Investigations Report , more than 30 of the exploited vulnerabilities had \nbeen identified as long ago as 1999, yet presumably not addressed at the victim organization. \nAs the report notes, “Apparently, hackers really do still party like it’s 1999.” \n Similar trends can be seen in the incidence of software errors. Many of the most \nserious, frequently exploited vulnerabilities in software are due to well-known errors that \nare “often easy to find, and easy to exploit,” as noted in the 2011 CWE/SANS Top 25 Most \nDangerous Software Errors (CWE/SANS 2011). Furthermore, the situation does not seem \nto be improving. As David Rice, author of  Geekonomics (Addison-Wesley Professional \n2007), puts it, most software is not sufficiently engineered to fulfill its designated role as \nthe foundation for our products, services, and infrastructure (Rice 2007). This is partly \ndue to the fact that incentives to improve quality are “missing, ineffectual, or even \ndistorted,” he concluded. To compete, suppliers focus on bringing products to market \n",
      "content_length": 3809,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n78\nfaster and adding new features, rather than on improving quality. Rice estimated, based \non government data, that “bad” error-ridden software cost the United States a staggering \nUSD 180 billion even back in 2007. \n Not surprisingly, the typical recommendations for improving IT security often \nsound remarkably familiar. That’s because they address problems already known to most \norganizations, but not fully addressed. As the Data Breach Investigations Report notes, \nthe question is not which vulnerabilities should be patched (all of them should): “The \nreal decision is whether a given vulnerability should be patched more quickly than your \nnormal cycle or if it can just be pushed with the rest.” Previous editions of the report \nhave recommended basic precautions such as ensuring passwords are unique; regularly \nreviewing user accounts to ensure they are valid and properly configured; securing \nremote access; increasing employee awareness using methods such as training; and \napplication testing and code review to prevent exploits such as SQL injection attacks and \ncross-site scripting, which take advantage of common software errors. \n The fact that these measures do not appear to be rigorously applied at many \norganizations takes us back to a key theme of this chapter: that the commitment \nof employees is as important as the policies and procedures you have in place. If \nadministrators and developers are committed rather than just following directives, if \nthey feel personally responsible for the security of the enterprise, and they will be more \nconscientious about ensuring the right technical controls are in place. \n Insider Threats \n High-profile national security breaches by insiders such as Edward Snowden and Chelsea \nManning have made insider threats a considerably more prominent issue during the \nthree years since the first edition of this book was published. \n Among the 557 organizations participating in the 2014 Cybersecurity Watch \nSurvey (CSO et al. 2014), 28 percent of cybercrime events were attributed to insiders. \nFurthermore, insiders accounted for the highest percentage of incidents in which \nsensitive or confidential information was stolen or unintentionally exposed. \n Insider attacks also cause additional harm that can be hard to quantify and recoup, \nsuch as damage to an  organization’s reputation . Insiders have a significant advantage \nbecause they can bypass physical and technical security measures such as firewalls and \nintrusion detection systems that were designed to prevent unauthorized access. The \norganization’s trust in the insider is used as the attack surface. In at least one case, the \ninsider was the person one might least suspect: the head of information security at the Iowa \nstate lottery, who hacked his employer's computer system, and rigged the lottery so he \ncould buy a winning ticket in a subsequent draw. By installing a rootkit on a lottery system, \nhe could secretly alter the lottery's random number generator, enabling him to calculate \nwinning numbers in advance and buy a winning ticket in advance (Thomson 2015). \n Unfortunately, even  security firms are not immune to compromise; well-known \ncybersecurity company FireEye hired an intern who was later discovered to be a \ntop Android malware developer. Unfortunately, his job at the security firm involved \nresearching and analyzing Android malware, which raises the concern that he could have \nused his inside knowledge to develop malware capable of evading technical controls \n(Fox-Brewster 2015). \n",
      "content_length": 3589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n79\n Yet surveys have also suggested that many insider attacks are opportunistic, rather \nthan highly planned affairs. Many insiders take data after they’ve already accepted a job \noffer from a competitor or another company, and steal data to which they already have \nauthorized access. In some cases, misguided employees may simply feel they’re entitled \nto take information related to their job. \n Clearly, all organizations need to be aware of the insider threat. It may not be \npossible to thwart all insider exploits, but we can take actions to reduce their likelihood \nand impact. Perhaps the biggest step we can take is to instill a culture of commitment. \nUser behavior analytics technology can also help by detecting behaviors or access \nprivileges that are outside the norm; perhaps technology could have prevented the \ncase in which a former nursing assistant at an Orlando health network inappropriately \naccessed about 3,200 patient medical records, with no apparent motive. Besides \ndisclosing the breach, the health network had to notify the affected patients and offer \nsupport, fire the employee, reeducate the workforce, and increase its efforts to audit and \nmonitor access (Brinkmann 2015). \n To help manage insider threats, consider a  three-part approach : deter, detect, and \ndiscipline. Remember that successful implementation will require the involvement of the \nentire organization. \n Deter \n• \n Build security awareness and instill a culture of commitment, \nusing the techniques discussed in this chapter. \n• \n Make your company a great place to work. Employees are less \nlikely to get disgruntled, and therefore less likely to seek ways to \nharm the company. \n• \n Let people know you’re watching. Technology can help monitor \nusers’ activity. Showing users their activity reports can help \ninvolve them in protecting the business. It also lets potentially \nmalicious insiders know they’re being watched. \n Detect \n• \n See something, say something. A committed workforce will tell \nyou if they see something suspicious. \n• \n User behavior analytics tools are becoming more and more \neffective at finding anomalies in access permissions and user \nactivity, and identifying whether a user’s actions are far enough \noutside the norm that they merit investigation. \n• \n Form a team that focuses on insider threats and investigations. \nThis should operate as a cross-functional team with involvement \nfrom human resources, legal, physical security, and information \nsecurity groups. \n",
      "content_length": 2542,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "CHAPTER 5 ■ PEOPLE ARE THE PERIMETER\n80\n Discipline \n When an insider incident occurs:\n• \n If it’s an honest mistake without a big impact, immediate remedial \ntraining may be the best remedy. \n• \n If the impact was low, and the incident seems more an error of \njudgment than a malicious act, a less heavy-handed approach \nmay be appropriate—perhaps a written warning or a comment in \nthe person’s performance review.  \n• \n If the intent is clearly malicious, or the impact is significant, \nconsider the options of termination and even engaging law \nenforcement. \n Finding the  Balance \n One reason that organizations are focusing more attention on security awareness is \nthat their technical controls have failed to prevent attacks from reaching employees \nand thus the core of the enterprise. Rapidly evolving new exploits, often involving social \nengineering as well as malware, have outstripped the capabilities of the security tools \ncompanies have relied on in the past. \n Now, innovative security technology is becoming available that uses machine \nlearning and artificial intelligence techniques to prevent malware much more effectively, \non every type of device. This is great news for all consumers of technology. The adoption \nof this technology should result in a substantial reduction in risk, due to a precipitous \ndrop in malware. The danger is that some will see this as an opportunity to dial back \ntheir security awareness efforts. I think this could be a mistake. We will always need to \nmaintain a level of diligence and discipline in security and privacy awareness. However, \nwe may be able to shift the emphasis of training toward prevention and future risks, and \nfocus on how we should design, develop, and deploy technology that better protects \nprivacy and resists attacks. \n No matter how good our technical controls are, we will still need people to act as part \nof the perimeter. We need to create a sense of personal commitment and security as well \nas privacy ownership among our employees. If we succeed in this goal, we will empower \nemployees to help protect the enterprise by making better security decisions both within \nand outside the workplace. \n",
      "content_length": 2184,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "81\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_6\n CHAPTER 6 \n Emerging Threats and \nVulnerabilities: Reality and \nRhetoric \n Curiosity is lying in wait for every secret. \n —Ralph Waldo Emerson \n These days it’s hard to read an online news source, pick up a newspaper, or watch TV \nwithout seeing reports of new threats: cybercrimes, data breaches, industrial espionage, \nand potential destruction of national infrastructure. These reports inevitably leave the \nimpression that we are drowning in an inexorable tide of new and terrifying threats. \n One has to question how much of this is  rhetoric , and how much is reality. There are \npolitical and profit-driven motives for making threats seem bigger and more imminent \nthan they really are. US government officials have warned that cyber attacks potentially \ncan be “devastating, approaching weapons of mass destruction in their effects” \n(Levin 2010). Such warnings have been used to justify requests for increased national \ncybersecurity funding, as well as proposed restrictions on private networks. It’s not \nsurprising, therefore, that some experts have expressed skepticism about the real extent \nof the threat. In fact, academics at the George Mason University Mercatus Center have \nwarned, “the United States may be witnessing a bout of threat inflation similar to that \nseen in the run-up to the Iraq War” (Brito and Watkins 2012). \n On the other hand, common sense tells us new cyber threats really are emerging \nand growing. More data is online and vulnerable to attack, and millions of new Internet-\nconnected devices are inevitably introducing new risks. Malware production has matured \ninto a sizable industry. Government agencies and businesses have suffered real attacks \nattributed to nation-state actors: in 2014, for example, the US Government charged five \nmembers of the Chinese military with stealing information from SolarWorld and other \ncompanies, during a trade dispute over solar-energy products. \n",
      "content_length": 2046,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n82\n Given the flood of  often-conflicting information , how can we get an accurate picture \nof the threat landscape so that we can develop an appropriate security strategy? How do \nwe determine which threats directly affect our organizations, and distinguish them from \nthose that are irrelevant? How do we decide which threats require immediate defensive \nmeasures, as opposed to those that attract attention but don’t yet present significant risks? \n In this chapter, I’ll describe methods for identifying the real threat and vulnerability \ntrends among the rhetoric. I’ll also discuss some key areas of threat activity that have been \nanalyzed using these methods. My goal is to help information security groups stay ahead \nof the attackers and focus their limited resources on mitigating the most important threats. \n Structured Methods for Identifying Threat Trends \n To identify the real trends in emerging threats among the mass of news and speculation, \nwe need to carefully examine the available information using a structured, analytical \napproach. Unfortunately, many security groups absorb information about emerging \nthreats using methods that are unstructured and sometimes almost haphazard. \n A typical process looks something like this. The  security team  relies on external \nsources, such as news feeds and alerts, as well as informal anecdotes, to gather \ninformation about emerging threats. Based on this information, the team holds \nbrainstorming sessions to review the threat landscape. The output from these sessions is \na list of “top risks.” Security resources are then focused on mitigating the items on the list. \n There are several problems with this approach. Information comes from a narrow, \nlimited range of sources, resulting in a  blinkered security perspective that tends to stifle \ncreative thinking. Also, the information is usually fragmented, making it difficult for the \nteam to identify trends and gaps in the data. These deficiencies continue through security \nplanning and implementation. Because the team lacks a full view of the threat landscape, \nit’s hard to determine which threats require immediate attention and how much of the \nlimited security budget they deserve. As a result, risks are incorporated into plans on an \nad hoc basis, and not all risks are adequately mitigated. Finally, security teams often don’t \nhave a structured process for communicating threat information to other people within \ntheir organizations. Because of this, people outside the security group remain unaware of \nemerging risks and don’t know how to respond when they experience an attack. \n I realized the limitations of this approach several years ago, and began trying to inject \nmore rigor into the  risk-sensing strategy . Over time, those efforts progressively developed \ninto a more structured risk-sensing process that helps identify threats, prioritize them, \nplan responses, and deliver actionable information to those who may need it. Through \ncontinued use, risk sensing can become a systemic process within any organization. \n The process for  analyzing emerging threats includes several valuable techniques \nthat may be unfamiliar to some security groups. I have used a product life cycle analogy \nto track threats as they mature from theoretical risks into full-blown exploits. I have also \nused nontraditional analysis techniques, such as war games and threat agent profiles, \nto encourage creative thinking and identify threats that might otherwise be missed. I’ll \ndiscuss these methods in more detail later in this chapter. \n",
      "content_length": 3646,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n83\n The process can be managed by a small core team, supplemented by a broad set \nof experts (including people outside the security group) across an organization. This \narrangement ensures continuity while enabling the team to mine a diverse variety of \nsources to get a more complete picture of immediate and future threats. \n Security team members should research a wide range of security topics in depth. \nThis diversity of perspective and discussion essentially creates a crowd-sourcing of \nintelligence and reduces the influence of any single person’s bias. Team members \nuse typical sources, such as external feeds and analysis; they also mine academic \nresearch and hacker discussion forums, and connect with security professionals at \nother organizations. Other team members may scan the regulatory horizon to identify \nupcoming laws and regulations with potential impact, or analyze internal investigations \nand other near-miss incident data. \n The team should hold regular meetings to analyze the  threat landscape . At these \nmeetings, each security domain expert explains his or her findings to other members \nof the security team. For each security topic, the discussion should include a review of \nrecent events and a look ahead to the future. This helps identify the key trends and the \nfactors driving those trends, provides context that can be used to analyze the current \nstate, and predicts the likely evolution of each threat. The structured evaluation uncovers \nemerging risks that the team might otherwise miss. It’s also useful to look back at previous \npredictions to see which ones were accurate, and to analyze the reasons why threats may \nnot have materialized in the way that was expected. \n It’s important to communicate the findings to stakeholders across your organization \nin regular reports and briefings, including a wide-ranging annual assessment of the threat \nlandscape. This communication provides further opportunities to get feedback from \nacross the organization and its business units, which can then be used to refine your  risk-\nsensing analysis . \n The  Product Life Cycle Model \n I have found that a product life cycle model is a useful way to track and prioritize \nemerging threats as they evolve and begin to present real risks to the enterprise. Almost \nall security groups have a limited budget, so they need to focus their resources on \neffectively mitigating the  highest-priority threats . \n This model, shown in Figure  6-1 , recognizes that many threats initially emerge as \ntheoretical risks, but are on a path to exploitation, and we need to evaluate and monitor them. \n",
      "content_length": 2698,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n84\n Often, researchers or hackers first reveal a possible attack or vulnerability at a \nsecurity conference or publish information about it online. Next, attackers begin testing \nthe use of this technique and making their results publicly available. Once the method \nhas been proven, the threat enters the production phase as attackers start exploiting it in \nearnest. Ultimately, the threat becomes a mature  commodity—source code is often freely \navailable, many variants exist, and organizations treat the threat as part of the everyday \nlandscape and build defenses accordingly. \n This life cycle model enables security teams to systematically track the evolution of \nthreats. It helps us determine when to allocate resources to fighting each threat. As each \nthreat approaches maturity, we can examine how it is likely to affect our organizations \nand plan appropriate mitigation. \n In addition, this model provides a great way to communicate actionable information \nto business groups using terminology they already understand (the product life cycle). \nWhen we provide regular threat landscape assessments to stakeholders, each security \ntopic should include a description of the activity at each life cycle phase, thus providing \na context that helps the security team inform business groups about how they should act \non each of these emerging risks. \n L et’s examine some examples showing how this model can be used in real life. \nFigure  6-2 illustrates the evolution of threats targeting smartphones and other handheld \ndevices. Researchers and hackers began to take notice of handheld devices almost a \ndecade ago, demonstrating weaknesses and theoretical avenues of exploitation. Initially, \nthey focused on what were then known as personal digital assistants. As smartphones \n Figure 6-1.  The product life cycle model for tracking the  evolution of threats. \nSource : Intel Corporation, 2012 \n \n",
      "content_length": 1982,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n85\ntook off, attackers shifted their attention to this bigger market, which rapidly became \na major area of threat activity. Monitoring trends at these earlier stages enables \norganizations to prepare. As threats mature and employees begin using smartphones \nmore widely at work, well-prepared organizations are in a better position to develop risk \nmitigation measures including technical controls and incident response plans. \n Figure 6-2.  How an organization could use the product life cycle model to track and \nrespond to  smartphone security threats  \n By visually comparing activity across multiple threat areas, as shown in Figure  6-3 , \nwe can quickly identify major areas of activity and see the likely timing and extent of their \nimpact. This chart also shows us areas in which there are numerous proof-of-concept \ntests and other activities that suggest major problems in the near future. And it indicates \nareas of focused research that may ripen into active exploitation over the long term. \n \n",
      "content_length": 1080,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n86\n Although the depth of detail in Figure  6-3 is valuable to the security team, I have \nfound a simpler, consolidated view such as the chart in Figure  6-4 can help communicate \nthe essential trends to a broader audience, supplementing  other  threat analysis materials . \nThese simpler charts are based on the activity identified using the product life cycle \nmodel, but add further trend analysis and group the activity areas into four main clusters, \ndepending on their level of activity and maturity potential and on their potential impact \nto the company. These clusters are \n• \n Sustained drivers : These are areas that already have a high \nimpact or otherwise cause considerable concern. Typically, \nthey are characterized by commoditized distribution and active \nexploitation by multiple threat agents. Today, examples include \nmalware and web attacks. \n• \n Critical trends : These areas have begun undergoing active \nexploitation, with growing adoption beginning to shift toward \ncommoditization. Current examples include social computing \nand smartphones. \n• \n Emerging trends : These areas have a low current level of \nexploitation, but considerable research and proof-of-concept \nactivity. Examples include embedded and cloud computing. \n Figure 6-3.  A visual comparison of  security-related activity across different technology \nareas. Data are for illustration purposes only. Source: Intel Corporation, 2012 \n \n",
      "content_length": 1499,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n87\n• \n Disruptive trends : These are areas with little or no active \nexploitation, but significant research activity and the disruptive \npotential to cause a major security problem. Frequently, they are \ndiscussed as theoretical risks, and because of this, many people \nin the industry would be caught off guard by a significant event. \nExamples include virtualization, an area in which potential \nthreats and vulnerabilities have been exposed and a successful \nexploit could cause far-reaching damage. \n I have found that clustering threat analysis information in this way enhances \ncommunication with stakeholders. Representing the information in easy-to-understand \ncharts helps to convey the key trends and their potential impact to a broad cross-section \nof people, helping them quickly assess whether they need to make  adjustments  to \nsecurity strategy. \n \nASSESSING HOW TO RESPOND TO A NEW THREAT \nREPORT\n A continuous stream of new threat reports emerges from agencies, intelligence \nservices, and vendors. It can be hard to determine what to do with all the new \ninformation—especially since most security organizations have limited resources. Here \nare five questions you can ask yourself the next time you see a published  threat report .\n Figure 6-4.  Clustering areas of threat activity to highlight trends. \nSource: Intel Corporation, 2012 \n \n",
      "content_length": 1430,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 114,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n88\n 1. \n Are we immediately affected? Are the indicators of compromise \nshared in the report found in our environment? If so, we have an \nincident that we must deal with. \n 2. \n If we’re not already affected, what is the likelihood that we \nwill be a future target? We’re more likely to be targeted if we \nwork in the same industry as a previous victim, or if we are \nconnected to them in another way (as a supplier, customer, or \npartner). If the attackers are hactivists or politically motivated \nthreat actors, we are more likely to be targeted if we align with \nthe victim’s philosophy. Note that we may be a target even if \nthere’s no obvious linkage to the victim. \n 3. \n How were the victims attacked? What compensating controls do \nI have in my security stack to mitigate the risks across the kill \nchain of a similar attack?  \n 4. \n Have we seen the same malware used, or families derived from \nit, against our assets? \n 5. \n Were any interesting tools, techniques, or procedures used that \nI should capture and share with my security team? This part of \nthe report can be used to educate responders, architects, and \nrisk managers so they can make better decisions. \n Based on a blog post by Steve Mancini, Director of Information Security at Cylance \n(Mancini 2016). \n Understanding Threat Agents \n Besides the product life cycle analogy, there are other techniques that can help us think \ncreatively about  threats and identify risks we might otherwise miss. \n Behind every threat is a human agent. To effectively plan defenses, it helps if we can \nunderstand why and how these agents operate: their motives, typical methods, and targets. \nHowever, I realized several years ago that we lacked agreed-upon definitions of threat \nagents, as well as a clear understanding of which agents actually pose the biggest risks to us. \n Some agents and their activities attract considerable publicity, resulting in the “TV news \neffect” in which the most-publicized agents appear to be the biggest threat, so they often \nreceive a disproportionately large percentage of limited mitigation resources. In reality, a \nwide spectrum of threat agents exists, some of which may be less well-known but pose bigger \nthreats. For example, hactivists often want to publicize their activities as much as possible \nto draw attention to their cause. This publicity makes them appear to be a bigger threat than \nother groups, such as organized crime syndicates, which try to conceal their exploits. \n In addition, terms often are used without clear agreement about what they mean. The \nphrase  advanced persistent threat has become a buzzword whose exact meaning depends \non who is using the term. It usually implies adaptive, long-term strategies employing a \n",
      "content_length": 2819,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n89\nvariety of stealthy techniques and used by attackers with considerable resources. However, \nit’s important to remember that a variety of agents may be capable of generating this type \nof threat. One thing that all these threat agents have in common is the use of malicious \ncode to achieve their goals. But to understand and predict their likely motives and \nmethods, it is useful to clearly define the agents, whether they represent nations or other \npowerful groups, such as organized crime. To solve this problem, Tim Casey, a member \nof my security team at Intel at that time, developed a standard threat agent library that \nprovides a consistent, up-to-date reference describing the human agents that pose threats \nto our information assets (Casey 2007). The library helps risk management professionals \nquickly identify relevant threat agents and understand the importance of the threats.  \n The library acts as a collection point for information about each agent, making it \neasier to share information across your organization. It includes profiles of agents such \nas disgruntled employees, opportunistic employees, industrial spies, and politically \nmotivated attackers. The library also catalogs agents’ typical targets, objectives, skill \nlevels, current activity, and exploit outcomes. When used as part of regular threat \nassessments, this model can help determine which agents pose the biggest risks to your \norganization. The security team can then use the information about their typical methods \nand exploits to help plan its strategy. The library helps the team understand why specific \nevents and attack trends occur and what might happen next. \n \nNSA’S CHIEF HACKER EXPLAINS HOW TO DEFEND \nAGAINST THREATS\n It’s hard to imagine someone who is better placed to provide advice about defending \nagainst advanced adversaries than Rob Joyce, who heads the National Security \nAgency’s  Tailored Access Operations (TAO) elite hacking unit. So the audience \nlistened closely when he took the stage for an eye-opening talk at the 2016 Usenix \nEnigma conference. “My talk is to tell you, as a nation-state exploiter, what can you \ndo to defend yourself to make my life hard,” he said. \n Joyce said that six intrusion phases comprise what is typically referred to as the \n“kill chain:” reconnaissance; initial exploitation; establish persistence; install tools; \nmove laterally; and collect, exfiltrate, and exploit the data. Organizations can thwart \nattackers by disrupting the transition between any of these phases. For example, \nto help prevent reconnaissance turning into initial exploitation, you can reduce the \nattack surface by locking down or disabling devices that are unused or don’t need to \nbe open to access. “Don’t assume a crack is too small to be exploited,” he said. “We \nwill look for that esoteric edge case.” \n Contrary to popular belief, advanced adversaries don’t rely exclusively on zero-day \nexploits, Joyce added. Most intrusions occur via easier vectors: e-mail, web sites \n(using techniques such as waterholing—infecting web sites that are frequently \naccessed by users at the target organization), and removable media like USB drives. \nJoyce noted that you can’t rely on users not to click, even with the best security \npolicies and education (see my Irrefutable Laws of Information Security in Chapter \n 1 ), so you need technical controls that will prevent the execution of malicious code. \n",
      "content_length": 3507,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n90\n Once advanced attackers have established a beachhead, they try to steal credentials \nthat enable them to maintain a presence, install tools, and move laterally to the \nprized assets they seek. Techniques such as segmenting the network, limiting \nadministrator privileges, and forcing two-factor authentication can help make this \nmore difficult. Joyce also said that he liked some of the new ideas emerging from \nthe industry such dynamic privileges, which is analogous to the granular trust model \ndescribed in Chapter  7 : the level of access provided depends on factors such as the \ndevice you’re using and your location. \n Finally, he stressed the need to continually evaluate and improve your defenses. An \norganization with static defenses will drift to the back of the herd, where it is easily \npicked off by a predator (see Irrefutable Law #6). “Don’t be that easy mark,” he said.  \n Playing War Games \n I like to conduct war games a few times a year. War games are intense role-playing exercises \nin which employees take on the role of attackers and attempt to compromise key assets using \nany feasible methods. I have found war games are particularly valuable for analyzing threats \nthat may have major consequences but whose vulnerabilities are not well understood. \n This technique provides the most comprehensive method of assessing threats to \nkey assets because the people playing the role of our adversaries are essentially allowed \nto use any method to achieve their goals. However, because of this, it is also resource-\nintensive and should be used selectively. \n Typical war games that I have overseen take one and a half days and may involve \neight to ten staff from a variety of roles, such as factory workers, business process \nleads, salespeople, and technical experts. Some war games can take much longer; in \n Wargaming for Leaders , written by wargaming experts at management-consulting firm \nBooz Allen Hamilton, (Herman, Frost, and Kurz 2009), the authors discuss games that \nmay last weeks and involve many more players across an organization. \n A typical game focuses on a specific target or scenario, such as disabling a key facility \nor stealing trade secrets. You can use war games to examine potentially catastrophic \nevents that have a low probability of occurrence, but a high probability of causing damage \nif they do occur. Team members are instructed about the threat agents involved and draw \non archetypes from a threat agent library or descriptions provided by the game architect. \nLed by a facilitator, the team takes on the attacker’s perspective and postulates ways to \nachieve the attack’s objectives. \n Because the team can propose any attack method, they often identify risks that might \nbe overlooked using conventional methods. As the authors of  Wargaming for Leaders put \nit, “We create the environment, the players engage, and what comes out of team play often \nsurprises and even stuns everyone involved.” For example, a malicious group might attempt \na devastating attack by purchasing a small but essential technology provider and inserting \nmalware into their products in order to infect their customers. After each game, security \nanalysts examine the results to determine how to address newly identified vulnerabilities. \n",
      "content_length": 3356,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n91\n I also like to examine the cyber consequences of large physical events as part of \ndisaster recovery planning. These could include earthquakes and tsunamis that damage \ndata centers, or even solar flares that disrupt the communications that the business relies \non. Exercises can include drills that last a day or more. \n A large organization can justify the considerable effort involved in conducting these \nexercises because of the enormous potential benefit of mitigating the threats. In fact, \nsome organizations hire professionals to create and facilitate these games. Booz Allen \nHamilton, for example, has an extensive war gaming practice covering diverse subject \nareas including market dynamics, cybersecurity, geo-political events, and even real war \nscenarios. \n But smaller organizations can also benefit by considering extreme events and \nformulating response plans. If you prepare for the extreme, you’ll be more prepared \nto deal with everyday events. Planning doesn’t need to be as resource-intensive as a \nfull-blown war game. It can be as basic as bringing team members together to discuss \nlikely scenarios and responses in a shorter tabletop exercise lasting just a few hours. \nThis method enables members to get a feel for what it would be like to work together in \nthe event of a real disaster. Considering these extremes can also provide motivation for \nintroducing simple yet effective measures to reduce the risk that catastrophes will occur. \nYou might realize it is worth increasing investment in user education to reduce the risk \nof social engineering compromises, or becoming more diligent about analyzing logs and \nnetwork traffic to identify patterns that indicate suspicious activity. \n Trends That Span the Threat Landscape \n I’ve described some of the methods that can be used to analyze emerging threats. Now \nI’d like to turn to some key themes that have emerged from such threat analysis. These \nthemes paint a  broad-brush picture of threat and vulnerability trends spanning multiple \ntechnologies across the threat landscape. \n Trust Is an Attack Surface \n As the technology industry erects new technical defenses, attackers seek to bypass these \ncontrols by exploiting user trust, typically using social engineering techniques such as \n phishing . \n If an attacker can win a user’s trust with a sufficiently convincing e-mail or fake web \nsite, the user will make it easy for the attacker by clicking a link or downloading a file. \nThese actions usually undermine even the most rigorous system-level controls, initiating \na chain of compromises that ultimately can result in major damage. \n Whenever users place their trust in a new technology, attackers quickly follow. \nStudies have shown that users trust social media services more than other information \nsources. A user is more likely to click a link if it appears to have been sent by a social \nmedia “friend.” Exploiting this trend, attackers have spread malware via social computing \ncircles of trust such as friend networks. \n Attackers have also been quick to take advantage of the trust users place in their \n smartphones and in other appliances such as game consoles. The exploitation of trust \nalso extends to the relationships between systems. Once configured, communications \n",
      "content_length": 3357,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n92\nbetween systems often operate autonomously, without manual oversight. Smartphones \nare set to automatically update applications from trusted app stores; other systems \nblindly trust firmware updates and dutifully install them. This automation provides \nconvenient opportunities to insert malicious code, abusing trust without the need to \ndirectly involve the user. \n In the near future, I anticipate trust will become a commodity that is bought and sold. \nThe digital reputation of systems and services will become critically important. In the past, \ntokens of trust, such as digital certificates and social computing credentials, were stolen \nfor immediate use. In the future, they will be stolen so they can be sold in underground \nmarkets. The value of these tokens depends upon the access they grant and the other circles \nof trust they can be used to penetrate. Already, attackers are using stolen digital certificates \nto sign their malware in an attempt to avoid detection by operating system defenses. \n I expect  social engineering attacks will continue to present significant risks because \nthey exploit human weaknesses and will adapt to take advantage of new technologies. \nSo we, as security professionals, need to focus on the role of users as part of the security \nperimeter, as I discussed in Chapter  5 . To reduce the risk to the enterprise, we need to \nmake users more security-aware and influence them to act in more secure ways. But it’s \nalso important to note that a successful phishing exploit is also ultimately a technology \nfailure that allowed malicious code to execute. \n Barriers to Entry Are Crumbling \n Our adversaries gravitate toward the path of least resistance. They tend to select targets \nthat are easy to access and analyze, and they typically use the most readily available and \ncheapest tools. \n They are much less likely to use methods with high barriers to entry such as the need \nfor specialized expertise, expensive hardware or software, or access to extensive compute \ncapacity. However, several of these barriers have begun to crumble as a result of trends \nsuch as cloud computing, lower-cost communications components, and commodity \nmalware toolsets. This trend ultimately is likely to result in new types of attack. \n A key factor is that security researchers are sharing not only their knowledge but also \nthe tools they design as part of their research. Recently publicized tools, such as rogue \nbase stations and Bluetooth sniffers, provide attackers with more accessible, low-cost \nways to intercept network traffic. Researchers have uncovered vulnerabilities in femtocell \ndevices (miniature, low-cost cell towers) that can be used to take control of the devices, \nlowering the barriers to attacks targeting cell phone data traffic. \n Ultimately, lower barriers to entry mean increased risk to enterprises. However, \nbecause several of these areas are still at the research stage, it will take time for them to \nmature into active exploitation. \n The Rise of  Edge Case Insecurity \n Each day, the environment becomes more complex with millions of new devices, each \nrunning its own operating system and collection of applications. This complexity generates \nnew edge cases—problems or situations that occur only in unexpected or extreme situations. \n",
      "content_length": 3381,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n93\n Edge cases can include unlikely interactions between two familiar objects. A hacker \nteam recently demonstrated that, with a popular smartphone, a paperclip (used to pop \nout the phone’s SIM card at the critical moment), and a little patience, it’s possible to \ngain access to contact information, phone call logs and voice mail, e-mails, and other \ninformation stored on the phone. \n Overall, the growing number of third-party plug-ins and widgets introduce edge \ncases that are hard for developers to anticipate even if they use secure design techniques. \n Interoperability between programs has resulted in a new category of hybrid attacks \nwhere malicious objects are concealed in innocent-looking ones to thwart detection. \nOne proof of concept in 2011 demonstrated it was possible to conceal a fully functioning \nTrojan in an e-mail plug-in. \n Some of these hybrid attacks have shown they can circumvent new security features. \nAs web browsers and search engines try to protect users from malicious links, attackers \nare responding by hiding links in image search results, where they cannot be detected \nusing standard tools. Research into network intrusion methods has discovered over a \nhundred methods of evading detection by manipulating traffic to remain functional but \nundetectable by typical tools. \n There is no silver-bullet solution for eliminating edge-case insecurities. It’s unlikely that \neven the most rigorous testing could ever uncover them all. The best approach may be to \nexercise caution when adopting new technologies with the potential to generate edge cases. \n The Enemy Knows the System \n The technology industry has often relied on security through  obscurity:  the idea that if \nattackers can’t see the insecurities in code or other technology, they won’t exploit them. \n Over time, it has become clear that security through obscurity is poor security. To \nquote the maxim coined by Claude Shannon, one of the founders of modern computing, \n“The enemy knows the system.” \n It’s now relatively easy for attackers to get access to the same tools enterprises use, \nsuch as web hosting services and smartphone application development tools. Hackers \ncan now more easily engineer malware and attacks that take advantage of these elements. \nThe fact that static platform controls tend to become less effective over time (one of the \nIrrefutable Laws of Information Security noted in Chapter  1 ) is partly due to the ability of \nmalware authors to pretest their malicious code against technical controls. They can do \nthis by obtaining code from malware repositories that have already been tested against \nexisting controls, or by actually purchasing the technical controls. \n Even the success of social engineering demonstrates that the attackers’ knowledge \nof the target greatly increases the likelihood of successful deception. Today, competitors \nand other threat agents learn a great deal about a company and its employees by simply \nsearching information publicly available on web sites or social media accounts. \n Because we cannot assume insecure technology is safe just because it is hidden, we \nneed to design with security in mind. The ineffectiveness of security through obscurity is \nalso an argument in favor of standards and open-source solutions. This idea may initially \nseem counterintuitive, but the fact that open source is exposed to public scrutiny requires \nit to be secure. At a minimum, we should ensure devices are rigorously tested against \nindustry standards because the attackers will do so. \n",
      "content_length": 3625,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n94\n Key Threat Activity Areas \n Threats are evolving in many technology areas, from embedded systems to cloud \ncomputing. I’d like to discuss a few areas experiencing significant developments with \nimplications for enterprise IT. \n The Industry of Malware \n Malware has become a profitable industry that increasingly resembles the legitimate \nsoftware market, with market leaders, mergers, licensing agreements, real-time support, \nand open source. The organized business activity in this market reflects the extent to \nwhich well-crafted malware has become a viable career pursuit for members of the \ncriminal underground. \n Today, malware development and malware use may in some cases be distinct \nactivities carried out by different groups or individuals. Malware authors are producing \nstandardized toolkits, which have made life much easier for would-be attackers. These \nattackers can now simply buy or acquire a toolkit rather than expending the effort to \nidentify vulnerable web sites and develop their own exploits. \n The Zeus malware family provides a useful case study showing how complex this \nindustry has become and how hard it is to accurately track developments. Sold mainly \nin underground forums, Zeus has been used extensively for theft by creating botnet \nnodes. During 2011, a code merger was reported between Zeus and another popular \ncrimeware kit, complete with assurances of future support for the customers of both \nproducts. Around the same time, Zeus toolkit source code was made publicly available. \nSince then, multiple new variants have appeared and been used for a variety of attacks. At \none point, security researchers attempting to monitor Zeus exploits discovered a server \nthey believed was the hub of a Zeus botnet. However, the server was the equivalent of \nan espionage honey pot, allowing the botmasters to turn the tables by spying on the \nresearchers who were attempting to analyze the hub. \n Ransomware has also become a profitable activity for some organized crime \nelements. Ransomware was mostly at the validated proof of concept stage when I wrote the \nfirst edition of this book in 2012; it has since progressed to active exploitation with some \ncommoditization. Today’s ransomware exploits typically exploit system vulnerabilities \nusing Trojans and other methods, then lock or encrypt information so users cannot \naccess it and hold people and organizations hostage until they pay. In February 2016, a \nLos Angeles hospital paid a ransom in bitcoin after staff were locked out of the hospital’s \nown network for more than a week; during the same month, one ransomware variant was \nreported to be infecting more than 90,000 PCs per day (Fox-Brewster 2016). \n The Web Expands to the Internet of Things \n The Web continues to present a huge attack surface. And this attack surface is growing \nrapidly as it expands to include the Internet of  Things , encompassing  nontraditional \ndevices such as appliances and control systems, cars, wearable and medical devices, and \nthe “smart” grid. Each of these is a potential source of risks. \n",
      "content_length": 3156,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n95\n Recent headlines have highlighted the growing threat activity focused on  IoT . \nResearchers hacked into a Jeep via its Internet-connected entertainment system and \nremotely controlled the vehicle’s functions, including turning off the transmission and brakes \nwhile someone was driving (Greenberg 2015). Other researchers showed that thousands of \ndevices in hospitals are vulnerable to attack, including x-ray machines, MRI scanners, and \ndrug infusion pumps, partly because medical equipment is increasingly connected to the \nInternet so that data can be fed into electronic patient records systems (Pauli 2015a). Yet \nanother researcher demonstrated the ability to hack into FitBit fitness trackers via Bluetooth \n(Pauli 2015b). Many IoT devices, including cars, wearables, and home appliances,  include \nwireless capabilities, so exploitation doesn’t require a physical network connection. \n Clearly, we should expect continued growth in IoT threat activity. However, should \nbe noted that the activity to date has generally has been at the research or early proof-of-\nconcept phase (see Figure  6-1 ). As the IoT expands and matures, we will see a progression \nto advanced active exploits over the next few years; given the rapid pace at which IoT \nis evolving, if companies don’t use good privacy and security design principles when \nbuilding their products, the time from research to active exploitation could be much \nshorter than has typically been the norm. \n Many  embedded devices that are already installed in businesses are similarly \nvulnerable. Companies have a history of deploying specialized devices without adequate \nsecurity controls, often because of the perception that specialized devices are “dumb” and do \nnot have a full set of capabilities. In reality, the opposite is often true: devices marketed for a \nspecific function are often capable of much more. Printers contain processors, use wireless \nconnections, and may be capable of acting as file servers, for example. As a result, embedded \ndevices can introduce as much risk, or more, to an organization as a traditional computing \ndevice since they lack security controls and administrators are generally unaware of the \ndanger. New devices may be vulnerable to new attack methods: recent research showed \nthat the sounds 3D-printer nozzles make as they cross the machine bed can be recorded by \nsmartphones, analysed, and then used to duplicate prototypes (Nelson 2016). \n The vulnerabilities in embedded  industrial control systems were exposed by \nthe widely publicized Stuxnet malware, which was used to sabotage the systems that \nsupported Iran’s uranium enrichment capabilities. The incorporation of computer-based \ncontrol and automation technology into the existing electrical power infrastructure—\nresulting in the “smart grid”—is another source of potential vulnerabilities. The US \ngovernment has warned of increasing threats to the grid, noting that many embedded \nsystems lack adequate security controls and are susceptible to known techniques such as \ncross-site scripting attacks (US GAO 2012). \n We might also see logical attacks as precursors to physical attacks. On a macro scale, \na nation state might attack another nation’s cyber infrastructure before staging a physical \nattack. This approach might also be applied at a more personal level. A burglar might \nremotely disable an Internet-connected alarm system before sneaking into a house, or \nperhaps even use the system’s video cameras to watch the owners and note when they \nleave the house unattended. \n Here are two more potential future  IoT scenarios in which innovative technology \ndesigned to do good could be exploited for harm, unless designed with strong security \nand privacy protection. Last year, doctors for the first time inserted an artificial “eye” that \nenabled a blind person to see. The device is a retinal implant that receives signals from \na video camera integrated into eyeglasses. Think ahead a few years, to a time when the \nimplants are more sophisticated and can see in much higher resolution, and also include \n",
      "content_length": 4172,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n96\nsoftware to automatically interpret visual information, such as QR codes. Then imagine \nthat a malicious actor creates a QR code that triggers the vision system to download \nmalware. Like the PC malware that paralyzed Sony’s network in 2014, the malware then \ndemands a ransom to re-enable the person’s vision. Now consider the example of a \ncement company that’s embedding sensors in the concrete mix used to build a new road, \nthus enabling local authorities to monitor traffic patterns and adjust signals to optimize \nthe flow of vehicles. If the technology is not securely designed and implemented, all that a \nmalicious person needs is the ability to execute malicious code, in order to falsify the traffic \npattern in such a way that vehicles converge on the scene of a planned bomb attack. \n Smartphones \n Smartphones are attracting almost as much malicious interest as desktop and laptop \nplatforms. However, even though smartphone sales have outstripped PC sales, \nsmartphone malware isn’t yet as prevalent as PC malware and doesn’t cause the same \nkind of widespread damage. That’s partly because most valuable corporate and personal \ndata is still held on PCs and servers. Another factor is that smartphone vendors have \nsomewhat greater control over applications, since users generally access them via \nvendor-controlled app stores. \n Just as in legitimate software markets, malware authors are likely to maximize the \nvalue of their code by using tools that allow their software to run on multiple devices. \nThey are increasingly targeting applications, a trend also seen on other platforms. \nAttackers have purchased copies of applications, incorporated their malicious content \ninto the otherwise legitimate software, and then redistributed their code under a new \nname or as a “free” version of the original. On one smartphone platform, autodialing \nmalware was found in more than 20 applications. Variations of a Trojan were found in \ndozens of applications and are believed to have been downloaded by at least 30,000 users. \n A further development is the use of smartphones as bridges to traditional networks, \nresulting in the potential for enterprise network attacks that originate from within mobile \nnetworks. \n In the future, we could see greater exploitation of location-based services to deceive \nusers. Because smartphones contain location sensors such as  Global Positioning System \n(GPS) chips, knowledge of the phone’s location can be used to present targeted ads \nand useful information. For example, a user in a supermarket aisle might be presented \nwith online coupons for products on nearby shelves. But this information could also be \nexploited to present fake coupons that are all the more convincing because they suggest \nthat the sender knows the user’s preferences. \n Attackers could also exploit other smartphone capabilities to take advantage of the fact \nthat the devices are carried into confidential meetings and other highly sensitive situations. \nImagine being able to remotely control a device that has a microphone, a camera, or other \nrecording capabilities. Or think about a vulnerability in any of the popular web-conferencing \nservices that people use for confidential discussions and to exchange information. \n Current trends in the mobile platform space indicate that attackers are most \ninterested in stealing personal data. This trend is partly due to the increasing use of \nsmartphones for financial and banking transactions, which provides new opportunities \nfor identity thieves and other criminal groups. As a result, it is now important that \nsmartphone hardware and software developers focus on protecting personal data. \n",
      "content_length": 3757,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n97\nSoftware developers should adopt the same discipline and commitment to following \nsecure design principles as traditional platform developers. Today, more and more \npeople are becoming app developers, creating software, and posting it online for others \nto use. One has to question how much security testing and validation has been applied \nto these applications. As users move more of their everyday activities onto smartphones \nand other small devices, the consequences of poor or insecure designs will have greater \nimpact on individuals and their employers. \n Web Applications \n Web applications , primarily comprising client browsers and server-based applications, \ncontinue to be heavily attacked. Threat analysis indicates that this area is experiencing \nfull exploitation activity and moving toward commoditization. There is also considerable \nresearch in this area, suggesting the number of attacks will continue to grow. \n Attackers have adopted new techniques to hide their intentions and deceive users \nlong enough to achieve their aims. As web browsers and search engines try to protect \nsystems from malicious links, attackers are instead obfuscating their links in image search \nresults, where they may not be detected. \n Techniques for hiding messages within images have been used within the security \nrealm since long before the invention of information technology. Now, this technique, \nknown as  steganography , is being used to hide malware and botnets on publicly used \nimage hosting sites. \n Search poisoning has also become a common method. Attackers using search \npoisoning tend to focus on events and topics of popular interest, optimizing their web \npages to achieve high search engine rankings. After a search query, the victim clicks a link \namong the search results. They are redirected multiple times and eventually land on a \npage that is used as a vector to deliver malware. \n Conclusion \n In this chapter, I’ve outlined some of the real threat trends and described methods \ninformation security groups can use to analyze the threat landscape as it continues to evolve. \n No doubt, new and more sophisticated types of exploitation will continue to emerge, \nand we need to stay aware of them. As Mustaque Ahamad, director of Georgia Tech \nInformation Security Center, noted in 2011, “We continue to witness cyber attacks of \nunprecedented sophistication and reach, demonstrating that malicious actors have the \nability to compromise and control millions of computers that belong to governments, \nprivate enterprises, and ordinary citizens.” \n Yet, as we try to make sense of the deluge of news about attacks and vulnerabilities, \nit’s essential to retain a sense of perspective. Most threats do not take place using exotic, \nobscure methods. Instead, they take the path of least resistance, exploiting well-known \nvulnerabilities. Therefore, business can mitigate many of these threats by implementing \nbasic, established security measures. To put it another way: when you hear hoof beats, \nthink horses—not zebras. \n",
      "content_length": 3122,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "CHAPTER 6 ■ EMERGING THREATS AND VULNERABILITIES: REALITY AND RHETORIC\n98\n Social engineering will continue to be a key attack method because it takes \nadvantage of user trust and is hard to prevent using technical controls. Therefore, as I \ndiscussed in Chapter  5 , we need to continue to focus on educating users to become more \nsecurity-aware. By doing so, we can reduce the risk to the enterprise. \n Ultimately, while doing our best to prevent compromises and breaches, we must \nremember we cannot control the threat actors and their exploit attempts. Because \nall threat categories use malicious code in some way, advanced preventive tools that \neffectively stop the execution of malicious code can greatly reduce the potential of \ncompromise. But all organizations face the possibility of some level of compromise, \nmaking defense in depth as essential as ever. Losers ignore the trends. Winners survive by \nbeing able to predict, prevent, detect, and respond. \n",
      "content_length": 969,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "99\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_7\n CHAPTER 7 \n A New Security Architecture \nto Improve Business Agility \n An organization’s ability to learn, and translate that learning into action \nrapidly, is the ultimate competitive advantage. \n —Jack Welch \n Some  Star Trek episodes feature suspense-filled battles in which adversaries use \nsophisticated phase-shifting weapons that can be rapidly adjusted until they find a way \nto penetrate static force-field defenses. For a beleaguered starship, the only effective \nresponse is to use similarly adaptable and fast-changing shields. \n As information security professionals, we also need extremely agile defenses that \nquickly adapt to meet new demands. Attackers are continually adapting, and defenders \nalso need to continually adapt. But rapidly evolving threats are only part of the challenge. \nWe also need to continually adapt our defenses to a rapidly changing technology \nlandscape. \n As information risk and security groups consider the future, it’s clear that we need \nto radically change our approach in order to face the challenges ahead and support the \nProtect to Enable mission. \n One problem in recent years has been that most of the protection offered by the \nindustry has not kept up with the attackers. Because these tools have failed to prevent \nharm, many companies have defaulted to a detect-and-respond approach. This means \nthey continue to expose themselves to high risks and higher long term costs since they are \nreactively responding to attacks that have already breached the organization’s defenses. \n We also need to consider whether our existing control architecture improves or \nimpedes business agility and velocity. It’s important to recognize that controls can \nplace a “drag coefficient” on the business. By hindering users, they can stifle business \nvelocity and innovation. Users react to this control friction by circumventing the controls \nwhenever possible; as a result, the controls can actually introduce new risks, as discussed \nin Chapter  2 . \n",
      "content_length": 2114,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n100\n As we move forward, we will need an agile security architecture that quickly and \nautomatically learns and adapts to new challenges as they emerge. A learning system is \nharder to defeat because it can more quickly predict and thus prevent new attacks. The \npace of change is so rapid that we cannot predict all the challenges we will face, and \nmanual or semi-manual processes will not be enough to keep up. We will need solutions \nthat can learn to manage what we don’t know. \n The right  control architecture will enable flexibility that helps the business move \nmore quickly, allowing us to rapidly adopt new technologies and emerging usage models \nwhile continuing to provide security in the ever-evolving threat landscape. \n A few years ago, after intense brainstorming sessions, the information risk and \nsecurity team I led at Intel devised a new security architecture for the company. This \narchitecture represented our implementation of the Protect to Enable strategy, using the \ntechnologies that were current at that time. With the benefit of hindsight, I believe that we \ngot many things right—but there were also some omissions because we didn’t have a full \nunderstanding of the controls that would be needed. \n In this chapter, I’ll provide a high-level overview of a new security architecture and \ndescribe how it meets some key challenges. Some of this overview is based on the work at \nIntel a few years ago, but I have added a new perspective on controls that I have realized \nis lacking in the industry. An important aspect of this new perspective is the concept of \ncontrol friction. As I’ll explain later in the chapter, I’ve developed a simple framework \ncalled the 9 Box of Controls, which takes control friction into account when assessing the \nvalue of security controls. \n I believe that the architecture includes some  novel approaches that may be valuable \nto many organizations facing these universal challenges. My conversations with peers \nat other companies have validated this view. Many of them are considering similar \nstrategies and in some cases have begun implementing them. \n Any future security architecture must provide better prevention, and it must also be \nmore flexible, dynamic, and granular than traditional enterprise security models. This \nwill helps us all accommodate future evolving usage models. We can provide users with \ndifferent levels of access depending on factors such as the devices they are using and their \nlocation. To achieve this, the architecture dynamically adjusts a user’s access privileges \nas the level of risk changes. For example, an employee should have more limited access \nto our systems when using a less-secure device than when using a more hardened or \nperhaps fully managed enterprise-class system. \n The new  architecture greatly improves  threat management . As new attacks appear, \nwe need to be able to recognize good from bad in milliseconds, so that we can stop the \nbad and allow the good. For any attack that gets past the preventive controls, we need to \nlearn as much as we can without compromising the user’s computing performance or \nprivacy. This information enables us to investigate what occurred, so we can quickly take \naction to mitigate the risk and also learn how to prevent similar attacks in the future. A \ncontrol architecture should assume that attempts at compromise are inevitable, but we \nshould also understand that it’s possible to achieve real prevention for 99% or more of \nmalicious code. We can apply artificial intelligence and machine learning to analyze the \nfeatures of files, executables, and binaries to stop malicious code prior to execution. For \nthe remaining attacks, representing less than 1% of malware, we need to focus heavily on \nsurvivability. \n",
      "content_length": 3850,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n101\n The 9 Box of Controls, Business Trends, and \nArchitecture Requirements \n Before diving into the specifics of the architecture, I’ll explain the 9 Box of Controls. Then \nI’ll recap some of the key business and technology trends, focusing on how they drive the \nneed for specific capabilities in security technology. \n 9 Box of Controls \n There are three primary types of security controls: prevention, detection, and response. \nPrevention occurs when an action or control prevents a risk before it affects users or \nthe environment. Detection is identifying the presence of something malicious that \nhas already entered the environment. Response is a reaction. From a risk perspective, \nprevention focuses on minimizing vulnerability and the potential for harm, while \ndetection and response focus on minimizing damage. \n There are also three primary levels of control automation: automated, semi-\nautomated, and manual. Automated control occurs entirely through machines. Semi-\nautomated involves some level of human intervention. Manual controls are managed \nentirely by hand. \n The combinations of these control types and automation levels comprise the cells of \nthe 9 Box, as shown in Figure  7-1 . Risk increases as we move from prevention to detection \nto response. Cost increases as we move from automated to semi-automated to manual \ncontrols. \n Figure 7-1.  The  9 Box of Controls \n \n",
      "content_length": 1465,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n102\n However, there is a third dimension to the 9 Box: control friction. As we know, \nfriction is the force that causes a moving object to slow down when it is in contact with \nanother object. Similarly, controls can impose a “drag coefficient” on business velocity—\nthey can slow the user or a business process. However, friction is not a fundamental, \nimmutable force like gravity or electromagnetism. Instead, we have the ability to \ndetermine how much control friction we apply. Apply too much control friction, and \nbusiness users will go around IT and its security controls. This adds cost: IT is no longer \nmanaging the technology; data and business silos are created, and the organization \nloses its volume purchasing power. It also adds risk: because the security team lacks \nvisibility into the technology, it cannot prevent compromises, detection is difficult, and \nin many cases response after the fact becomes the only option. If a business adheres to \nhigh-friction controls, the effect can be to generate systemic business risk. High-friction \ncontrols can hinder business velocity; the organization can lose time to market and the \nability to innovate, and over the long term it may even lose market leadership. \n IT Consumerization \n As I discussed in Chapter  5 , consumerization is a major IT theme with ever-broadening \nimpact. It includes several trends, including the adoption of new applications and \nsupport for consumer devices. \n Many highly mobile employees want to use their own consumer devices, such as \nsmartphones, wearables, and tablets, for work. This increases productivity by enabling \nemployees to collaborate and access information from anywhere, at any time. To support \nthis, organizations provide access to corporate e-mail and other applications from \nemployee-owned smartphones and tablets. \n Some people believe that in the future, all devices will be consumer-owned, and that \nenterprises will no longer purchase devices for their users. I believe this might be the case \nin some work environments, but I doubt that it will suit all organizations. For a company \nproviding call center services, with most employees working from home, it might make \nsense that employees exclusively use their own personal systems for work. But this \nstrategy could be more risky for a financial services company whose employees handle \nhighly sensitive information that is subject to extensive regulatory requirements. \n Nevertheless, the consumerization trend continues to grow at almost all \norganizations. Accordingly, we’ll need to provide employees with a level of access to \nresources from an expanding continuum of client devices, some of which may have much \nweaker security controls than today’s enterprise clients (see sidebar). \n",
      "content_length": 2836,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n103\n \nCONSUMERIZING ENTERPRISE IT AND “ENTERPRISING” \nTHE CONSUMER\n Discussions of  IT consumerization tend to draw a clear line between business \ndevices that can be managed and trusted, and personal consumer devices that are \nessentially unmanaged and untrusted. \n However, not all consumer devices are created equal. From a security standpoint, \nit may be more valuable to think about a device’s capabilities than to categorize it \nbased solely on whether it’s marketed as an enterprise device or a personal device. \nThe security of a device depends on the inherent features of the hardware, operating \nsystem, and applications, and on whether it enables us to add further security and \nmanageability capabilities that mitigate the risks of enterprise use. \n As the variety of consumer devices, such as smartphones and wearables, continues \nto expand, users may choose from dozens of models with different levels of security \ncapabilities. Greater security and manageability means that IT can place greater trust in \nthe device and provide a correspondingly greater level of access to enterprise resources. \n Extending this idea further, the information security group could evaluate the \nsecurity of available consumer devices and provide guidance about the level of \nenterprise access that users will be allowed with each device. Users may prefer to \nbuy a more secure device because it will provide them more access. With greater \naccess, they can use the device for more of their daily work activities. This ability in \nturn enables them to be more productive. \n At the same time, employees increasingly expect to have available to them at work \nthe types of consumer services and cloud applications that they use in their personal \nlives. These include social computing applications such as blogs and wikis, video-sharing \nsites, and file-sharing services. \n We need a security architecture that enables us to more quickly support new devices \nand provide access to a greater range of applications and data, without increasing risk. We \nneed to be able to dynamically adjust the levels of access we provide and the monitoring \nwe perform, depending on the security controls of the client device. \n New  Business Needs \n Nearly all companies now rely on a growing network of business partners, and conduct \nmany of their interactions with those partners online. Many organizations are also \nexpanding into new markets through both organic growth and acquisitions. Because of \nthese business trends, most organizations will need to provide access to a broader range \nof users, many of whom are not employees. Many organizations also need to be able \nto smoothly integrate acquired companies and provide them with access to resources. \nIn general, we need to quickly provide new users access while minimizing risk and \nproviding selective, controlled access only to the resources they need. \n",
      "content_length": 2965,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n104\n Cloud Computing \n Most organizations are already using cloud services in some form to achieve benefits \nsuch as greater agility and lower cost. Some are also implementing a private cloud \nbased on virtualized infrastructure while using external cloud services for noncritical \napplications. In the future, I expect greater use of hybrid clouds that use both internal and \nexternal resources, especially for organizations that are anchored to legacy environments. \nOrganizations able to let go of their legacy environments will predominantly use the \ncloud, with limited internal infrastructure. \n This trend means that IT services at many organizations will be provided by a \nmixture of traditional and cloud-based internal and external services. During a typical \nday, employees may access a variety of different services, some of which are internal and \nsome external. Ultimately, they should be able to easily move between these services \nwithout needing to log in multiple times or even know where the services are located. \n Securing access to cloud-based services presents challenges that aren’t easily \naddressed using conventional security controls. In cloud environments, systems and \ntheir data are virtualized and may migrate dynamically to different network locations. \nThis makes it difficult to effectively restrict access using traditional security controls such \nas firewalls, which rely on fixed locations of systems and a more static nature of the data. \nWe need much more granular and dynamic controls that are linked to the resources \nthemselves rather than just their network location. \n Changing  Threat Landscape \n The threat landscape is evolving rapidly. Increasingly, attackers have taken a stealthy \napproach, creating malware that quietly gains access and attempts to remain undetected \nin order to maintain access over time. This has been possible because the security \nsolutions deployed on endpoints in most organizations today do not adequately prevent \nmalicious code from executing. As the number of threats increases and new types of \nmalware emerge, we need to focus on the 9 Box of Controls and seek new prevention \nmethods reduce risk, reduce cost, and reduce control friction. \n Traditional enterprise security architectures have relied largely on protective \ncontrols such as firewalls located at the network perimeter and signature-based antivirus \nat the end points. At the same time, our focus has shifted to providing controlled access \nto a broader range of users and devices, rather than simply preventing access. Combine \nthis with a continually changing threat landscape, and we can assume that attempts to \ncompromise the environment are inevitable. Although existing perimeter controls such \nas firewalls will continue to have some value, we need tools that can dramatically increase \nthe ability to prevent attackers from gaining access to the environment, but in way that \ndoes not introduce a cost burden or a high degree of control friction. \n",
      "content_length": 3070,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n105\n Privacy and Regulatory Requirements \n The growing emphasis on privacy requirements and the increasingly complex regulatory \nenvironment have many implications for the way we manage information. Some \nregulations create the need for more control over where information is stored and require \nspecific levels of protection and tracking. Our architecture must provide this assurance, \nallowing us to build a high-security environment and access controls appropriate for the \nprotection of highly regulated information. In addition, the security controls themselves \nmust not introduce privacy risks. \n New Architecture \n To meet these rapidly changing requirements, we need a highly flexible and dynamic \narchitecture. The architecture should enable us to more quickly adopt new devices, use \nmodels, and capabilities; provide security across an increasingly complex environment; \nand adapt to a changing threat landscape. \n Key goals include helping increase  employee productivity while supporting new \nbusiness requirements and technology trends, including IT consumerization, cloud \ncomputing, and access by a broader range of users. At the same time, the architecture \nshould be designed to prevent risk, reduce our attack surface, and improve survivability—\neven as the threat landscape grows in complexity and maliciousness. \n The architecture moves away from the traditional enterprise  trust model, which \nis binary and static. With this traditional model, a user is in general either granted or \ndenied access to resources; once granted, the level of access remains constant. The \nnew architecture replaces this with a dynamic, multi-tiered trust model that exercises \nmore fine-grained control over identity and access control, including access to specific \nresources. This means that for an individual user, the level of access provided may vary \ndynamically over time, depending on a variety of factors—such as whether the user is \naccessing the network from a highly secure managed device with advanced anti-malware \ncapabilities or an untrusted and perhaps unmanaged device. \n The architecture’s flexibility allows us to take advantage of trust based on real proof \nthat malware execution is being prevented. Increasingly, devices may include some level \nof  hardware-enforced security designed to ensure the integrity of the applications and \ndata on the device. The architecture takes this into account when determining whether to \nallow access to specific resources—a more-trusted platform can be allowed greater access \nthan a less-trusted one. The architecture is based on four cornerstones:\n• \n Trust calculation : This unique element of the architecture \nhandles user identity and access management, dynamically \ndetermining whether a user should be granted access to specific \nresources and, if so, what type of access should be granted. The \ncalculation is based on factors such as the user’s client device \nand location, the type of resources requested, and the security \ncontrols that are available.  \n",
      "content_length": 3098,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n106\n• \n Security  zones : The infrastructure is divided into multiple \nsecurity zones that provide different levels of protection. These \nrange from trusted network zones containing critical data, with \ntightly controlled access, to untrusted zones containing less-\nvaluable data and allowing broader access. Communication \nbetween zones is controlled and monitored; this helps ensure \nusers can only access the resources for which they have been \nauthorized and prevents compromises from spreading across \nmultiple zones. \n• \n Balanced  controls : To increase flexibility and the ability to \nrecover from a successful attack, the model emphasizes the need \nfor preventative controls but also to balance them with detection \nand response. \n• \n User and data  perimeters : Recognizing that protecting the \nenterprise network boundary is no longer adequate, we need to \ntreat users and data as additional security perimeters and protect \nthem accordingly. This means an increased focus on the endpoint \ndevice and prevention of malicious code, in addition to increasing \nuser awareness and building data protection into the information \nassets. \n I’ll describe each of the four cornerstones in more detail. \n Trust Calculation \n The trust calculation plays an essential role in providing the flexibility required to support \na rapidly expanding number of  devices and usage models . The calculation enables us to \ndynamically adjust users’ levels of access, depending on factors such as the devices and \nnetworks they are currently using. \n It calculates trust in the interaction between the person or device requesting access \n(source) and the information requested (destination). The calculation consists of a source \nscore and a destination score, taking into account the controls available to mitigate risk. \nAs shown in Figure  7-2 , the result of this calculation determines whether the user is \nallowed access and the type of access provided.  \n",
      "content_length": 2018,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n107\n Source Score \n Trust in the source, or requestor, is calculated based on the following factors:\n• \n Who : The identity of the user or service requesting access and \nour confidence level in the authentication mechanism used; how \nconfident are we that users are who they say they are? \n• \n What : The device type, its control capabilities, our ability to \nvalidate those controls, and the extent to which IT manages the \ndevice. \n• \n Where : The user’s or service’s location. For example, a user who is \ninside the enterprise network is more trusted than the same user \nconnecting through a public network. There may also be other \nconsiderations, such as the geographical region where the user is \nlocated. \n Figure 7-2.  Trust calculation. Source: Intel Corporation, 2012 \n \n",
      "content_length": 849,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n108\n Destination  Score \n This is calculated based on the same three factors, but these are considered from the \nperspective of the destination (the information the source is trying to access):\n• \n Who : The application that stores the requested data. Some \napplications can enforce greater controls, such as enterprise \nrights management (ERM), and therefore provide a higher level \nof trust. \n• \n What : The sensitivity of the information being requested \nand other considerations, such as our ability to recover it if \ncompromise occurs. \n• \n Where : The security zone in which the data resides. \n Available Controls \n The trust calculation also takes into account the security controls available for the zone. \nIf the only controls available are controls that simply block or allow access, we might \ndeny access due to lack of other options. However, if we have extensive preventative \ncontrols with highly granular levels of access, detailed logs, and highly tuned security \nmonitoring—as well as the ability to recover from or correct problems—then we can \nallow access without creating additional risk. \n Calculating Trust \n The  trust calculation adds the source score and the destination score to arrive at an initial \ntrust level. The available controls are then considered to make a final decision about \nwhether access is allowed and, if so, how. This calculation is performed by a logical entity \ncalled a  policy decision  point (PDP), which is part of the authentication infrastructure and \nmakes access control decisions based on a set of policies. \n Based on the results of this calculation, the PDP makes a decision, allocating a trust \nlevel that determines whether the user can access the requested resource and the type of \naccess that is allowed. Broadly, the decision will fall into one of the following categories:  \n• \n Allow access \n• \n Deny access \n• \n Allow access with limitations or mitigation \n This trust calculation therefore allows us to dynamically apply granular control \nover access to specific resources. For example, employees using IT-managed devices \nwith additional hardware features such as a  trusted platform module (TPM) ,  global \npositioning system (GPS) , and full disk encryption would be allowed access to more \nresources than when using devices that lack those features. \n",
      "content_length": 2392,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n109\n Employees directly connected to the network would typically get greater access than \nwhen using a public network. If we are unable to verify the location of a high-security \ndevice such as a managed PC, we would allow less access. \n The trust calculation also can be used for more fine-grained distinctions between \ndifferent device models. For example, we could provide different levels of access based \non manageability, hardware-enabled authentication and encryption, and installed \napplications. \n We anticipate situations in which the trust level is not adequate to allow any access, \nbut there is still a business requirement to allow a connection or transaction to occur. In \nthese conditions, the result of the trust calculation could be a decision to allow access \nwith limitations or with compensating controls that mitigate the risk. For example, a \nuser might be allowed read-only access or might be permitted access only if additional \nmonitoring controls are in place. \n Today, the trust calculation makes decisions based on information gathered from \ncomponents at multiple levels of the infrastructure, such as network gateways, access \npoints, and user devices. Once the trust calculation mechanism is in place, we can extend \nit to include information from a broader range of sources. \n The trust calculation can be used to determine access to internal systems by  business \npartners as well as employees. Let’s say we’re collaborating with another company on \nthe design of a new product. An engineer at that company wants access to a specific \ndocument. We can add a variety of criteria to the trust calculation for deciding whether to \ngrant access. Did the engineer’s request originate within the business partner’s enterprise \nnetwork? Is it consistent with the type of request that we’d expect from an engineer? If so, \nwe have a higher level of trust in the requestor. \n If we cannot establish an adequate level of trust in the user’s device, but other factors \nprovide enough confidence to grant access, we might provide one-time access for a \nspecific job. We could do this by allowing a document to be downloaded, but only within \na container that ensures the document is completely removed from the user’s device once \nthe job is completed. \n Longer term, the trust calculation could become a mechanism that is used to \ndetermine access to both internal and external resources, including cloud-based \n applications . \n Security Zones \n The architecture divides the IT environment into multiple security zones. These range \nfrom untrusted zones that provide access to less valuable data and less important systems \nto trusted zones containing  critical data and resources . \n Because the higher-trust zones contain more valuable assets, they are protected with \na greater depth and range of controls, and we restrict access to fewer types of devices and \n applications , as shown in Figure  7-3 . However, devices allowed access to higher-trust \nzones also have more power; they may be able to perform actions that are not allowed \nwithin lower-trust zones, such as creating or modifying enterprise data. \n",
      "content_length": 3207,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n110\n Aligning the infrastructure in this fashion provides an excellent way to right-size \nsecurity controls so that security resources are utilized effectively. It also helps improve \nthe user experience by enabling employees to choose from a wider range of devices, such \nas smartphones, for lower-risk activities. However, all devices should have, at a minimum, \nadvanced endpoint capabilities that prevent more than 99% of malware from executing. \n Access to zones is determined by the results of the trust calculation and is controlled \nby  policy enforcement points ( PEPs ).  PEPs may include a range of controls, including \nfirewalls, application proxies, intrusion detection and prevention systems, authentication \nsystems, and logging systems. \n Communication between zones is tightly restricted, monitored, and controlled. \nWe separate zones by locating them on different physical or virtual LANs; PEPs control \ncommunication between zones. This means that if one zone is compromised, we can \nprevent the problem from spreading to other zones or increase our chances of detection \nif it does spread. In addition, we can use PEP controls, such as application proxies, to \nprovide devices and applications in lower-trust zones with limited, controlled access to \nspecific resources in higher-trust zones when required. \n The architecture includes three primary categories of security zones: untrusted, \nselective, and trusted. Within the zones, there are multiple subzones. \n Untrusted Zones \n These zones host data and services (or the interfaces to them) that can be exposed to \nuntrusted entities. This allows us to provide widespread access to a limited set of resources \nfrom non-managed consumer devices, without increasing the risk to higher-value resources \nlocated in other zones. Untrusted zones might provide access to enterprise resources, such \nas corporate e-mail and calendars, or they might simply provide Internet access. \n These zones are regarded as “shark tanks,” with a high risk of attack and compromise. \nTherefore, detective and corrective controls are needed to mitigate this risk. These \ncontrols might include a high level of monitoring to detect suspect activity and correction \ncapabilities such as dynamic removal of user privilege. \n Figure 7-3.  As the value of an asset increases, the depth and span of controls increase, \nwhile the number of allowed devices, applications, and locations decrease. Source: Intel \nCorporation, 2012 \n \n",
      "content_length": 2543,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n111\n We anticipate a need to provide controlled access from these zones to resources \nin higher-trust zones. For example, an employee using an untrusted device might be \nallowed limited, read-only access to customer data located in a trusted zone; or their \ndevice might need access to a directory server in a trusted zone to send e-mail. We expect \nto provide this controlled access using application proxies. These proxies act as secure \nintermediaries, evaluating the request from the device, gathering the information from \nthe resource in a trusted zone, and passing it to the device. \n Selective Zones \n Selective zones provide more protection than untrusted zones. Examples of services in \nthese zones include applications and data accessed by contractors, business partners, \nand employees, using client devices that are managed or otherwise provide a level of \ntrust. Selective zones do not contain critical data or high-value intellectual property. \nSeveral selective subzones provide access to different services or users. \n Trusted Zones \n Trusted zones host critical services, data, and infrastructure. They are highly secured \nand locked down. Examples of services within these zones are administrative access to \ndata center servers and network infrastructure, factory networks and devices, enterprise \nresource planning (ERP) applications, and design engineering systems containing \nintellectual property. Accordingly, we might only allow direct access to these resources \nfrom trusted systems located within the enterprise network, and all access would be \nmonitored closely to detect anomalous behavior. \n Many organizations have implemented secure high-trust zones as part of their \ntransition to an enterprise private cloud. Implementing these zones is a key step in \nallowing several types of applications to be moved onto virtualized cloud infrastructure, \nincluding applications requiring high security. The security features in these trusted \nzones include application hardening and increased monitoring. \n \nNEW SECURITY ARCHITECTURE IN ACTION: A DAY IN \nTHE LIFE OF AN EMPLOYEE\n This example (illustrated in Figure  7-4 ) describes how the new security architecture \ncould enable an organization’s sales force to access the information they need in the \ncourse of a day. At the same time, the architecture protects security by dynamically \nadjusting the level of access provided, based on the user’s device, its location, and its \ncapabilities for preventing malicious code, and by monitoring for anomalous behavior. \n The employee travels to a customer site. The employee is using a personal \nsmartphone with limited security features and so is allowed access only to services \nin untrusted zones. From here, the employee can view limited customer information, \nincluding recent orders, extracted from an  enterprise resource planning (ERP) \n",
      "content_length": 2936,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n112\nsystem in a trusted zone—but only through an application proxy server, which \nprotects the trusted zone by acting as an intermediary, evaluating information \nrequests, accessing the ERP system, and relaying the information to the user. \n If a smartphone requests an abnormally large number of customer records—an \nindication that it may have been stolen—further access from the smartphone is \nblocked. To help understand the reason for the anomalous access, there is increased \nmonitoring of the employee’s attempts to access the system from any device. \n The employee reaches the customer site and logs into the enterprise network \nfrom a company-owned mobile business PC. Because this device is more trusted, \nthe employee now has access to additional capabilities available in selective \nzones, such as the ability to view pricing and create orders that are relayed by an \napplication proxy to the ERP system in a trusted zone. \n The employee returns to the company’s office and connects to the corporate \nnetwork. Now the employee is using a trusted device from a trusted location and \nhas direct access to the ERP system in a trusted zone. \n Figure 7-4.  The new security architecture dynamically adjusts the user’s access to \ninformation, based on factors such as the  user’s device and location . Source: Intel \nCorporation, 2012 \n \n",
      "content_length": 1412,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n113\n Balanced Controls \n Over the past decade, enterprise security has focused heavily on controls such as \nfirewalls, signature based antivirus, and intrusion detection systems such as behavior-\nbased anomaly detection tools. As we have seen so often in the past few years, this \napproach is not working. At many companies, the default belief is now that prevention is \nnot possible and we can only correct problems after they have occurred. \n However, the new security model requires that we understand the implications of \nthe 9 Box. Preventative controls should not only stop malicious code from executing \nbut do so in a way that lowers our overall cost of controls and with low friction. More \neffective prevention will reduce the alert fatigue within due to the “whack-a–mole” effect \nassociated with over-reliance on detective (monitoring) and response controls. Detection \ncapabilities will also be more effective because effective prevention reduces the “noise” in \nthe environment. Over the long term, this approach will free up resources that can then \nbe applied to other corrective controls. \n By using the 9 Box to guide the control philosophy, and demanding solutions that \ncontinually shift down and to the left (reducing risk, cost, and control friction), we will be \nable to change the risk dynamics in the industry. \n \nUSING  SECURITY ANALYTICS TO DETECT SUSPICIOUS \nBEHAVIOR\n Almost all organizations have experienced security issues involving both \nexternal attackers and insiders, including attempts to steal intellectual property. \nInvestigations have identified markers and indicators that are frequently associated \nwith these events. If we can spot these indicators sooner, we can respond and \nmitigate the threats more quickly. \n Security analytics technology can be used to detect suspicious behavior as \nthe environment becomes more complex and attackers become more adept at \nconcealing compromises. The technology automates the process of analyzing large \nvolumes of data to detect and monitor anomalous activity, allowing companies to \ndetect problems that they might otherwise miss. These capabilities are similar to \nthose already implemented by financial institutions to prevent fraudulent credit-card \ntransactions, and by online consumer services to prevent theft of user data.  \n On a large scale, logging data generated by servers and sensors across the network \ncan be collected into a database for analysis. Security business intelligence can also \nbe applied at the level of individual users and devices, as long as we are careful to \nprotect users’ privacy. \n The balance between preventative, detective, and corrective controls will vary, \ndepending on the security zone. In high-trust zones, we implement extensive monitoring \nto detect possible attempts to steal data or compromise critical systems. Redundancy \nwithin each type of control can be used to provide additional protection. \n",
      "content_length": 3002,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n114\n The following includes possible examples of using detective and  preventative \ncontrols:\n• \n An employee attempts to send a confidential document to an \nexternal e-mail address. Monitoring software detects the attempt, \nprevents the document from being sent outside the firewall, and \nasks the employee if he or she really intended to do this. If the \nemployee confirms that this was intended, the document may \nbe transmitted, or if the document is highly sensitive, a redacted \nversion may be sent. \n• \n Inappropriate use of a document protected with enterprise rights \nmanagement technology results in revocation of access to the \ndocument. \n• \n The system allows access to specific documents but tracks the \nactivity. A user can download a few documents without causing \nconcerns. However, if the user attempts to download hundreds \nof documents, the system slows down the speed of delivery (for \ninstance, only allowing ten to be checked out at a time) and alerts \nthe user’s manager. If the manager approves, the user is given \nfaster access.  \n• \n The detection of an infected system places the system on a \nremediation network, isolating the system and restricting access \nto enterprise information and applications. The system may \nretain some ability to access corporate assets, but all activity is \nclosely logged to enable incident response if necessary. \n• \n When a system is found to be compromised, we examine all its \nrecent activities and interactions with other systems. Additional \nmonitoring of those systems is automatically enabled. \n \nUSING MACHINE LEARNING TO IMPROVE  ANTI-MALWARE \nTECHNOLOGY \n Traditional antivirus software relies on recognizing characteristic signatures to \nidentify specific malware families. But today, adversaries have access to off-the-\nshelf malware toolkits that make it easy to create custom malware variants that \nsignature-based antivirus products cannot recognize. This custom malware sails \npast traditional antivirus products as if they didn’t exist. \n Machine learning technology provides a solution to the problem. Rather than relying \non humans to identify malware signatures, machine learning technology can \nautomatically analyze and classify hundreds of thousands of characteristics per file, \nbreaking them down to an atomic level to discern whether an object is “good” or \n“bad” in real time. \n",
      "content_length": 2432,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n115\n The process works like this. A machine learning platform continuously collects \nvast amounts of data from many sources. It analyzes the data and extracts \nDNA-level features that the machine learning platform itself determines are \nunique characteristics of good and bad files. Most of these characteristics are \nso microscopic that human malware researchers and reverse engineers don’t \nunderstand their importance. The software constantly adjusts to the real-time \nthreatscape, thus learning to make higher-fidelity decisions. For each file, the \nplatform assigns a threat score that is used to automate policy-based protection \ndecisions—ignore, alert, block, or terminate file/process execution. A mathematical \nmodel encapsulating the platform’s intelligence is then periodically extracted and \nincorporated into an anti-malware solution that is installed on endpoints. Using this \nsolution, it’s possible to stop more than 99% of malware before execution.  \n Users, Data, and the Internet of Things: The New \nPerimeters \n The concept of balanced controls also extends to the protection of users and data. \n Traditional network security boundaries are dissolving with the proliferation of new \ndevices and users’ expectations that they should be able to access information from \nanywhere at any time. Users are under direct assault from a barrage of attacks designed to \ntrick them into taking actions that can compromise the information on their devices or on \nenterprise systems. These trends mean that we need to think more broadly about how we \nprotect information, as well as the users of this information. \n While we continue to implement enterprise network controls, such as perimeter \n defenses and the detective controls described earlier, we need to supplement these \ncontrols with a focus on the users and on the primary assets we are trying to protect such \nas intellectual property. The new architecture therefore expands our defenses to two \nadditional perimeters: the data itself and the users who have access to the data. \n Data Perimeter \n Important data should be protected at all times: when it is created, stored, and \ntransmitted. This becomes increasingly challenging as we move data to more and more \ndevices and let more people access it. How do we  protect information when it’s located \noutside the physical perimeter on a personal device? \n One approach is to use technologies that closely integrate protection with high-value \ndata so that the data remains protected as it moves to different devices and locations. \nTechnologies such as enterprise rights management and data leak prevention can be \nused to watermark and tag information so that we can track and manage its use. With \nenterprise rights management, the creator of a document can define exactly who has \naccess rights throughout the life of the document and can revoke access at any point. \nData loss prevention is used to tag documents, track their movements, and prevent \ntransfer outside the organization if necessary. \n",
      "content_length": 3093,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "CHAPTER 7 ■ A NEW SECURITY ARCHITECTURE TO IMPROVE BUSINESS AGILITY\n116\n User Perimeter \n As I described in Chapter  5 , people are part of the security perimeter, and we need to treat \nthem as such. Users can become security risks for a variety of reasons. They are targeted \nmore frequently in social engineering attacks, and they are more vulnerable to these \nattacks because their personal information is often readily available on social networking \nsites. They may also click malicious links in e-mail, download malware, or store data \non portable devices that then are lost. A combination of training, incentives, and other \nactivities can help instill information  security and privacy protection into the corporate \nculture and successfully encourages employees to own responsibility for protecting \nenterprise and personal information. \n Internet of Things \n The  Internet of Things can be viewed as an extension of the user and data perimeters into \nnew connected devices and systems such as cars, wearables, and smart buildings. IoT \ndevices should be included in the security architecture; for example, the trust calculation \ncould be applied to access from IoT devices, so that the security of the device is a factor \nin determining the level of access provided. For machine-to-machine communications, \neach communicating machine can be considered conceptually as analogous to a user; \nthe security architecture focuses on preventing, detecting, and responding to behavior \nthat it identifies as anomalous. \n Conclusion \n This chapter describes a new control architecture designed to support the Protect to \nEnable mission. With this approach we can lower risks, lower costs, and lower control \nfriction. It will also allow for faster adoption of new services and capabilities because it \nhelps prevent risk and improve survivability. I believe that this architecture can be used \nto meet a broad range of evolving requirements, including new usage models and threats. \nThe architecture’s flexibility and granular trust model should also make it easier for the \nsecurity team to identify and contain anomalous activity that signals potential insider \nthreats. By publishing information about the architecture , I hope to encourage others \nto take advantage of these ideas. I also hope that making this information available will \nstimulate more discussion and ideas, and that others will build on these concepts to \ncreate further innovations that benefit all of us. \n",
      "content_length": 2482,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "117\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_8\n CHAPTER 8 \n Looking to the Future: \nEmerging Security \nCapabilities \n Learn from yesterday, live for today, hope for tomorrow. The important \nthing is not to stop questioning. \n —Albert Einstein \n The Web has existed for two decades, yet it’s only in the last few years that we’ve gained a \nclearer picture of what the Internet may become, and how the emerging capabilities may \nshape the future. \n As early as 1993, companies like AOL started offering access to online newsgroups, \nsoon followed by dial-up Internet access using early web browsers. As laptops became \nmore affordable, many people started accessing the Internet while on the move. The \nrise of smartphones introduced built-in sensors, such as cameras, global positioning \nsystem receivers, and touch-sensitive screens, into consumers’ everyday computing \nexperiences. Businesses began using the information gathered from users’ devices to offer \npersonalized experiences, ranging from location-based driving directions to selected \nadvertisements. The variety of Internet-connected devices rapidly expanded to include \ntablets, home DVRs, appliances, and cars. Devices also became smarter, with improved \nvoice and gesture recognition. \n We’re now entering a world in which these elements will be combined to create \nmuch richer context-aware experiences for users and new opportunities for businesses. \nOur devices will know us, and they will know other devices. In fact, devices may almost \nbecome part of us: many companies are already shipping wearable computers, including \nsmart athletic garments that work with smartphone apps to monitor your biometrics and \nsuggest ways to improve your performance. \n",
      "content_length": 1795,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "CHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n118\n Each day, billions of computing devices will perform functions on our behalf, often \ncommunicating among themselves to get the job done. Much more information will be \ncollected from sensors such as cameras, microphones, and GPS receivers embedded into \nthe user devices. This data will be combined with other information to create context-\naware experiences that are far more personalized and compelling. Already, cameras and \nimage recognition technology, combined with behind-the-scenes analytical software, \ncan be used to identify a user’s age bracket and gender, and tailor their experience \naccordingly. Early applications based on this technology are being piloted and in some \ncases deployed by large companies, including retailers (see sidebar). \n Estimates of the projected size of the context-aware computing market continue \nto grow. When the first edition of this book was printed, Gartner, Inc. (2011) expected \ncontext-aware technologies to create huge business opportunities affecting an estimated \n$96 billion in annual consumer spending worldwide by 2015. In 2013, forecasts suggested \nthe market would reach $120 billion by 2018 (MarketsandMarkets 2013). And a report in \nlate 2015 forecast the market will swell to $185 billion by 2020 (Global Industry Analysts \n2015). During this period, it’s expected that a significant percentage of all payment \ntransactions will be validated using contextual information. \n \nRICHER EXPERIENCES IN THE  RETAIL ENVIRONMENT \n As people buy more goods online, retailers are seeking to entice shoppers into brick-\nand-mortar stores by using technology to create richer, context-aware experiences. \n Macy’s and some other big-name stores are already using beacons, which detect \nthe smartphones of nearby shoppers and, if they have opted in, send them targeted \noffers or mobile games with gift-card prizes (Tode 2015). Brands including Kate \nSpade and Levi’s use smart display tables and shelves that sense when customers \npick up a product and engage them with relevant videos and product information. \nThe technology tracks every interaction, so stores can analyze shopper behavior and \nmeasure the impact on sales (Perch Interactive 2016). LEGO stores use augmented \nreality video screens to show kids what they can build with each LEGO box. The \nscreens recognize each box and display a 3D image of a toy that can be created \nfrom it, blended into a real-time video of the child in the store. Canadian sports \nretailer Sport Chek’s flagship stores integrate hundreds of screens in displays up \nto 16 feet tall, using gesture, touch, and RFID to sense customer input and display \ncustomized interactive content. \n As an Advertising Age column noted, technology may ultimately help transform the \nphysical store into a venue for interactive experiences that increase brand affinity—\nacting as an event space, gallery, help desk, or even a test kitchen. If that happens, \nonline sales may work in tandem with, rather than as a substitute for, a physical \nstore (Fulford 2015). \n",
      "content_length": 3105,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "CHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n119\n These new technologies also introduce new risks, as I described in the discussion \nof emerging threats and vulnerabilities in Chapter  6 . The sensors and other new \ncapabilities embedded into millions of intelligent new devices can be exploited for \ndangerous  purposes . Malicious individuals might be able to remotely access home \nsecurity surveillance systems to determine when you’re not at home. Researchers have \nalready demonstrated the ability to remotely control the brakes and other functions of \nan  Internet-enabled car . After remotely hijacking a Jeep Cherokee driven by a reporter, \nresearcher Charlie Miller commented, “Right now I could do that to every [Chrysler] \ncar in the United States on the Sprint network (Pagliery 2015).” The hack prompted Fiat \nChrysler to recall 1.4 million vehicles to fix the issue (Greenberg 2015b). \n As  security professionals , we may tend to focus obsessively on this darker side of \nthe picture. Looking for threats and vulnerabilities is part of our role. We’ve seen that \nattackers find ways to exploit new technologies almost as soon as they appear. Analysis \nof emerging threats by many firms indicates that this trend will continue. As attackers \nadapt, we must adapt, too. Our role will be more important than ever. As more aspects \nof people’s daily lives are based on technology, it will become increasingly important to \nsecure the technology. The Protect to Enable mission will expand accordingly; in fact, it is \nbecoming a corporate social responsibility, as I will explain further in Chapter  9 . \n The positive news is that new technologies can also be used to enhance security. \nAs information risk becomes an even more high-profile concern, suppliers are building \nmore security into their products and services. Devices will include a greater level of \nbaseline security hardening to reduce the likelihood of compromise and minimize the \nimpact. \n Context-aware computing also introduces new privacy concerns. By definition, \ncontext-awareness involves taking advantage of information about the user to create \npersonalized experiences. This makes it even more important to appropriately protect \nusers’ information and privacy. A clear organizational commitment to privacy will be \nimportant to ensure this protection. A growing number of other organizations have \nformally committed to complying with a single set of privacy principles worldwide—\nalthough this is becoming difficult due to the proliferation of localized privacy laws and \nthe elimination of the EU safe harbor agreement (see Chapter  1 ). \n An  organization’s privacy commitment must also extend to applications and \nsystems. Suppliers are becoming increasingly aware of this, and some are already taking \nadditional steps to ensure user data is collected anonymously. The new baseline security \ncapabilities built into products, such as  hardware-enforced protection and  accelerated \nencryption , may also help enhance privacy by protecting user data. In addition, the \ninformation provided by sensors can be used to create  context-aware security . Today, \nsome cars can automatically adjust seat, mirror, and pedal positions to suit different \ndrivers. They adjust these settings when they detect the presence of the driver’s personal \ncar key. In the future, as cars become more intelligent and include more sensors, they \nmight identify the driver using a camera and microphone. If they don’t recognize the \ndriver, they might disable the car and alert the owner via their built-in wireless Internet \nconnection. Cars might include a maintenance mode that lets mechanics drive it while \nwhen it’s being serviced, but only within a radius of a few miles. Similarly, as I’ll discuss \nlater in this chapter, the sensors in an enterprise-class device, such as a business laptop \nPC, could be used to prevent theft and help protect the information it contains. \n",
      "content_length": 3974,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "CHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n120\n From the perspective of the  enterprise information security team, these emerging \ncapabilities will allow increased trust in users and their devices. When we have a \nhigher level of trust, we can provide the user with greater access to sensitive enterprise \ninformation and other resources. \n I believe that this dynamic evaluation of trust is a key capability that new security \narchitectures should include, as I discussed in Chapter  7 . Employees may want to access \nour systems from a variety of devices and locations, including personal smartphones and \ntablets as well as business PCs. When a user requests access to  enterprise systems , the \narchitecture should dynamically calculate trust based on contextual information such as \nthe user’s identity, the security features of the device they’re using, their physical location, \nand the resources they’re trying to access. The architecture then will decide whether to \ngrant access and the level of access that should be allowed. As manufacturers increase the \nsecurity capabilities in their devices, the model will be able to take this into account. We’ll \nhave increased trust in a device, and we’ll be able to provide a correspondingly greater \nlevel of access. \n In this chapter, I’ll take a closer look at some of the emerging security capabilities \nthat we can expect in products and services. First, though, I’d like to set the stage by \nexamining some of the key underlying trends that make these security capabilities both \nnecessary and possible. \n Internet of  Things \n Many everyday objects are becoming more intelligent. They’re acquiring processors, \nsensors, software, and the ability to communicate. This trend is made possible by  Moore’s \nLaw : processors and other hardware components continually become faster and less \nexpensive, and, therefore, ubiquitous as a result. This accelerating trend is creating the \nInternet of Things, a massive expansion of the Internet as it swells to include billions of \ndevices and household objects. Intelligent devices in cars, home electronics, and other \n“things” will far outnumber those in more conventional computing platforms and even \nthose in mobile devices such as smartphones. Gartner, Inc. estimates that during 2016, \n5.5 million new “things” will be connected every day. Juniper Research expects 38.5 \nbillion connected devices by 2020 (Loechner 2015); Cisco expects an even higher number \nof 50 billion (Cisco Systems 2015b). \n Gartner, Inc. (2011b) identifies several key technologies and capabilities contributing \nto this trend, including sensors, image recognition, and wireless payments using  near \nfield communications ( NFC ) technology. Sensors that detect and communicate changes \nin their environment are being embedded not just in mobile devices, but in an increasing \nnumber of places and objects. Emerging applications will take advantage of this \ninformation. For example, camera-based image recognition technologies are expanding \nfrom mainly industrial applications to broad consumer and enterprise uses. These \nsystems gather information about users and then analyze this information to personalize \nthe user experience.  Wireless NFC , based on a communications standard analogous to \nthe Radio Frequency Identification ( RFID )  technology used for product-tracking, lets \nusers make payments by waving a mobile phone or smartwatch in front of a compatible \nreader. \n",
      "content_length": 3489,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "CHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n121\n With technologies such as NFC, the concept of the Internet may broaden to include \nan even wider variety of “dumb” objects, like drink cans or fertilizer bags (Gartner 2011b). \nThis trend will provide opportunities for innovations that were not previously possible. \nToday, items in stores may include 2D bar codes that can be read by smartphones. In \nthe future, store items may include NFC on the packaging or shelf label allowing them to \nwirelessly identify themselves to nearby devices, such as a  shopper’s smartphone . The \nshopper will then be able to learn not only about the product, but also alternatives, and \ncould even view cross-selling and up-selling suggestions. \n Devices such as the  Nest Learning Thermostat have provided a glimpse of the future. \nThis home heating controller is designed to be intuitive and simple to operate, replacing \ncomplex menus and instructions with a single big button and a dial. Users can remotely \nmonitor and set the temperature from their smartphones, so they know the house will be \nwarm by the time they get home. But perhaps the most interesting capability is that, as \nits name suggests, it can learn. The Nest monitors use of the heating system and attempts \nto learn the user’s preferences—when the heating is switched on and off, and the desired \ntemperature. After studying the use patterns for a while, the Nest begins to predict and \nautonomously set the temperature and timing itself. Since Nest launched many other \ncompanies have followed suit with similar devices not only for home heating but also for \nother sensors and alarms, including water sensors, motion sensors, and do-it-yourself \ninternet-based home security systems. \n I believe that devices like this are early examples of a much larger trend. As \nthe Internet of Things grows, more interactions will occur directly between devices, \nrather than between people and device. Devices and objects will interpret and act \non information provided by other objects. This will enable much more intuitive and \nstreamlined experiences in many different fields. Consider the following scenario, \ndescribed by Plantronics CTO Joe Burton (2012). A doctor visits a patient in a hospital \nroom. A smart device the doctor is wearing turns on the doctor’s workstation in the room, \nthen authenticates the doctor to the patient management system, detects which patient \nis near the doctor, and pulls up the patient’s record. When the doctor leaves the room, the \ninformation accumulated during the visit is saved and the workstation powers down. \n Consistent User Experience Across Devices \n Users now demand the same quality of experience in the workplace that they’ve become \naccustomed to in their personal lives. This includes the ability to access information \nacross a continuum of devices, including PCs, smartphones, and tablets. They expect \nto be able to move from one device to another. They also expect intuitive applications \non all of these devices, with the application’s features tailored to the device’s size and \ncapabilities. \n IT therefore needs to provide users with a  consistent experience across devices and \nthe ability to seamlessly transition between them. As enterprise information security \nprofessionals, we need to focus on the user experience and on enabling this broader \nrange of devices while managing the risks. \n",
      "content_length": 3430,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "CHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n122\n Cloud Computing \n The cloud is as much a new business model as it is a technology shift. The ability to \nobtain flexible IT services on demand lets businesses operate more dynamically—quickly \ntaking advantage of business opportunities and growing or shrinking infrastructure \ncapacity to meet demand. Cloud services can also potentially reduce cost. \n However, cloud computing can also add new security complexities and data-\nprotection concerns. Organizations may use multiple cloud providers, while also \noperating a private cloud for the most sensitive applications. Users need to be able to \neasily access services delivered from any of these multiple environments. From the \nenterprise perspective, we need to enable a seamless user experience while minimizing \nrisk. This implies a federated model in which the user needs to log in only once; the \nuser’s credentials can then be used to access multiple applications. However, this also \nmeans that an attacker may only need to gain access once in order to compromise several \nenvironments. \n Big  Data Analytics \n Businesses have quickly realized the value of analytical tools for real-time analysis of \nmassive amounts of unstructured data. In the future, these analytic capabilities will \nincreasingly be used to interpret data from sensors as well as from databases, social \nmedia, and other sources. The analysis of this information will then be used to create new \npersonalized experiences, like the retail examples discussed in the “Richer Experiences in \nthe Retail Environment” sidebar. \n This analysis can also  be  integrated with existing enterprise systems to create \nsophisticated customer-focused services. Here’s a scenario described by Accenture \n(2012): a rental car company automatically detects when an accident with one of its cars \nhas happened, initiates emergency services if needed, and issues a replacement rental \ncar to meet the renter at the scene, greatly improving the chances of creating a loyal \ncustomer for life. \n Artificial Intelligence \n Artificial intelligence is rapidly maturing, and it’s now clear that AI will help all of us in \na variety of ways, both in business and our personal lives. AI is already used to identify \nmeaningful patterns in data for many purposes, including information security, and \nto understand and translate speech. AI will certainly play a role in self-driving cars. \nOver time, AI will become capable of taking on broader and greater responsibilities. \nAs Alphabet Inc. executives Eric Schmidt and Jared Cohen put it: “Eventually it will \nbe possible to give a computer unstructured data—say, spreadsheets used to manage \nbusiness records—and receive quality advice on improving operations.” (Schmidt and \nCohen 2015) In our personal lives, perhaps we’ll have a helper like Jibo, a “social robot” \nthat recognizes your face, converses with you, helps manage your calendar and basic \ntasks, and learns your preferences so it can adapt and help you better. \n",
      "content_length": 3053,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "CHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n123\n Business Benefits and Risks \n By now, it should be apparent that the richer experiences enabled by these capabilities \nare as important to businesses as they are to users. New,  context-aware experiences may \nattract customers and create new revenue. Furthermore, focusing on the user experience \nmay be essential for business survival. If we don’t provide rich and appealing user \nexperiences, customers may gravitate toward competitors that do. \n Our challenge is to manage the risks associated with these new experiences. The \ngood news is that new security capabilities are emerging to help us do so. \n New Security Capabilities \n The IT ecosystem is increasingly focusing on  building security into hardware, software, \nand services. We’ll all be able to take advantage of this security to protect users and the \nenterprise. I think of these capabilities as the equivalent of termite-resistant building \nmaterials used in construction. They may not prevent termite attacks altogether, but \nthey can stop some of them and minimize the impact of others. For example, Dell is \nusing technology from Cylance to protect the BIOS firmware in its business PCs. The \ntechnology is designed to check if systems are secure when users boot them up; after the \nPC boots, the software checks a hash of the BIOS against a known good version stored in \na secure cloud. \n Suppliers will need to frequently enhance these defenses to ensure they remain \neffective. As I noted in Irrefutable Law #6 in Chapter  1 , security controls operate in a \ndynamic environment in which attackers are constantly learning and adapting their \napproach. Unless the defenses also adapt, they will lose their effectiveness over time. \n I expect the ecosystem will increasingly view these security features as a way \nto differentiate products to meet the needs of distinct categories of customers. As a \nparallel, think about how the auto and other consumer industries developed. Initially, \nmanufacturers focused on getting the public to buy cars en masse. Accordingly, the focus \nwas on mass-producing just a few models at the lowest cost. As Henry Ford famously \nsaid, \"Any customer can have a car painted any color that he wants so long as it is black\" \n(Ford and Crowther 1922). Ford’s  mass-production strategy was enormously successful \nin popularizing cars among the American public. By 1918, half of all cars in the United \nStates were Model Ts (The Henry Ford Museum 2003). But once consumers became \nmore familiar with cars, they started demanding models that met specific needs. As \nmanufacturers responded, the industry began to develop the huge variety of models that \nwe see today. \n In the same way, suppliers will offer a range of products or services with differing \nlevels of security, including higher-security versions for the most sensitive enterprise uses \nand less-secure versions for consumers. This trend has already been evident for some \ntime in products such as servers and PCs, and we’re beginning to see it in cloud services. \n In a closely connected trend, we’ll see increasing use of contextual information to \nimprove security. Some of this context will be provided by the sensors built into devices, \nsuch as cameras and GPS receivers. In addition, analytical and monitoring tools will be \nable to gather valuable  contextual information from the environment. For example, they \nmay examine databases containing information about users’ access history and other \nrelevant data. \n",
      "content_length": 3554,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "CHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n124\n Baseline Security \n A greater level of baseline,  hardware-enforced security features will be important in all \ncategories of devices, from smartphones to full-featured PCs. These capabilities will \nprotect the information on the device itself, and the information that is accessed from the \ndevice. They’ll enable greater trust in the device, and because of this trust we’ll be able \nto provide users of the device with access to more resources, as I described in Chapter  7 . \nThe potential business benefits include increased user satisfaction and productivity. \n I believe that these features will become particularly valuable as the Internet of \nThings takes shape. Many new, connected devices and objects won’t be powerful enough \nto run traditional software security controls. Do I expect the computers that control my \ncar or my home to run full intrusion prevention systems or traditional antivirus suites? \nNo, but it is possible to run lightweight AI-based agents that can determine good from \nbad in milliseconds. This capability has already been demonstrated: in the summer \nof 2015, Cylance showed its AI-based anti-malware agent running on a Raspberry Pi \nplatform, which is based on the ARM processors that are in many appliances and other \nIoT devices (Bradley 2015). I also believe that many of these new devices should include \nprotection that limits their functions to the desired purpose, reducing the risk that they \ncould be successfully attacked and manipulated via the Internet or a wireless network. \n For enterprise security, these baseline hardware security capabilities will provide \nhelp in key focus areas, including threat management, ID and access management, \ndata protection, and remote monitoring. Some expected baseline capabilities include \nprotected environments, encryption, hardware acceleration, enhanced recovery, and \nintegration with security software, as described next. \n Protected Environments \n Increasingly, hardware will provide protection for essential functions and data in the form \nof trusted layers and execution environments. I think of this approach as analogous, at the \nhardware level, to the way organizations are implementing network security zones within an \nenterprise environment (as described in Chapter  7 ). The most valuable and critical functions \nreceive the greatest protection, as well as increased monitoring and recovery capabilities. \n Attackers have become increasingly adept at compromises using tools, such as \nrootkits, that operate at or below the operating system level, making them harder to detect \nand prevent by most traditional security applications. Implementing protection at the \nhardware level can help prevent compromise of firmware, operating systems, hypervisors, \nand other fundamental system components. Hardware-level protection can also help \nalert security professionals to attempted attacks and aid in system recovery . However, \nhardware-level protection must be designed, developed, and implemented correctly or \nit could actually do more harm than good, because compromise at this level can give \nattackers wide-ranging access to the software and data on the system. Concerns have \nalready begun to surface and are growing. Researchers demonstrated the ability to hack \nthe microcontroller inside flash cards, enabling the execution of code that can be used to \nperform a man–in-the-middle attack (Paganini 2014). Networking equipment supplier \nJuniper Networks found that its firewall operating system contained “unauthorized \ncode” that surreptitiously decrypted virtual private network traffic (Goodin 2015). MIT \nresearchers suggested there are weaknesses in the implementation of key provisioning \nfor Intel Software Guard Extensions (SGX), a set of hardware instructions designed to \nimprove security by sealing software into hardware-protected enclaves (Chirgwin 2016). \n",
      "content_length": 3952,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "CHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n125\n Encryption \n Many organizations already use disk encryption to protect data against loss or theft. But \nin a world where devices are always on and always connected, traditional software-based \nhard disk encryption is not sufficient. New capabilities will make encryption an even more \npervasive technology used to protect information throughout its life, both when it is stored \nand when it is transmitted. Devices will include self-encrypting drives that maximize \nprotection while minimizing the performance impact; encrypted input-output will help \nprotect data during communications. Capabilities that currently exist in larger systems, \nsuch as total memory encryption, will become common in PCs and other end-user devices. \n Hardware Acceleration \n There’s often a trade-off between security and performance. Controls, such as software-\nbased encryption and malware scans, certainly help increase protection, but the \nperformance impact can also increase frustration for users, to such an extent that some \nmay avoid using the security features altogether (see the discussion of control friction \nand the 9 Box of Controls in Chapter  7 ). Accelerating functions in hardware can shift the \nbalance in favor of security by decreasing the impact, both on users and on enterprise \nsystems. For example, complex calculations required by standard encryption algorithms \ncan be accelerated using hardware instructions rather than executed entirely in software. \n Enhanced Recovery \n As I’ve discussed in previous chapters, we must assume that attempts to compromise \nare inevitable, despite our best efforts. As attacks become increasingly sophisticated, the \nability to recover from compromises will become even more important. Future capabilities \nwill help organizations recover from low-level attacks that target fundamental system \ncomponents such as firmware or the BIOS. The system will be able to detect changes in \nthese components, whether due to malicious attacks or accidental corruption. It will then \nbe able to take steps to restore the components to a known good state, alerting users and \nthe security team when necessary. Other anticipated recovery features include enhanced \ncapabilities to revoke cryptographic keys to reduce the spread and impact of compromise . \n AI-Based Security and Automation \n AI-based security applications will play valuable roles in preventing attacks. Today, for \nexample, Cylance uses AI-based agents to distinguish good from bad in milliseconds. \nThese applications will be able to provide an even greater level of protection when they are \nintegrated with hardware-based security, as exemplified by the Dell-Cylance BIOS protection \nagreement described earlier in this chapter. This kind of integration will enable software to \nmore closely monitor the underlying hardware and firmware for attacks that might otherwise \ngo undetected. For example, security software could use hardware features to detect \nsymptoms, such as memory state changes, caused by specific types of attack. Companies \nare also researching better ways to authenticate users by employing behavioral biometrics: \nidentifying users based on a combination of hard-to-duplicate characteristics such as they \nway they swipe characters on a smartphone or even how they walk when carrying the device. \n",
      "content_length": 3389,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "CHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n126\n AI will be used more broadly over time to enable a greater level of automation in \nthreat detection, prevention, and response. In the future, AI might be used to dynamically \nevaluate trust and the corresponding level of access that’s provided to a user (see the \ngranular trust model in Chapter  7 ). \n Context-Aware Security \n The theme of context awareness underlies many of the rich user experiences described \nin this chapter. Context awareness can also enhance security: the same  sensors and \nanalytical tools that help organizations create personalized experiences can also be used \nto mitigate risk. \n In the home, TVs might be able to recognize when a child is watching, and show \nonly appropriate channels. In supermarkets, cameras that are already used for physical \nsecurity could help increase the efficiency of automated checkout stations. As I described, \n image recognition technology can determine a shopper’s approximate age. By using this \ninformation, perhaps in conjunction with data from a scanned driver’s license, the system \ncould help avoid the need for cashiers to manually approve alcohol sales, leading to faster \ncheckouts for consumers and reduced costs for stores. \n The sensors in  portable devices , such as mobile PCs and smartphones, may also be \nused to help protect against theft and unauthorized use. A simple case might utilize the \ndevice’s camera, microphone, and GPS receiver to help authenticate you as the device’s \nowner. If the user looks and sounds like you, and the PC is at your house, we have more \nconfidence that the person using it is really the owner. \n Additional technologies in portable devices, such as NFC, will allow more \nsophisticated examples of context-aware security. Devices will know when they’re no \nlonger in proximity of their owner, and may enter a protected state to prevent data loss. If \nyour phone is near your laptop, we have greater confidence that you are the user trying to \naccess the information on the laptop. When your phone moves away, the laptop deduces \nthat you have moved away, too, and begins to armor itself by locking the screen. As you \nmove progressively farther away, the laptop first goes into standby to save power, and \nthen begins encrypting its contents for protection. \n The GPS receiver in a portable device can also be used to geofence the device and \nthe data it contains. If the receiver detects that a PC has moved outside a specific area, \nthe device could alert the owner and the enterprise support team. The same capabilities \ncould help protect data whose movement is restricted by specific geography-related \nrequirements such as export controls. The device could detect when it’s in a country \nsubject to these controls, and encrypt the data it contains to protect it. \n Cloud Security and Context Awareness \n Cloud service providers recognize that some organizations are still reluctant to move \ncritical data to external clouds due to security, regulatory, and privacy concerns. \nSuppliers have been working to add security capabilities designed to address these \nconcerns. As they do so, we can expect more cloud services that are differentiated based \non the level of trust they offer. \n",
      "content_length": 3275,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "CHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n127\n Suppliers might offer a “plain vanilla” cloud service for noncritical applications, \nalong with a more expensive high-trust cloud service. Besides offering additional \ntechnical controls, secure clouds might include guarantees that the supplier will meet \nspecific privacy and other data-protection regulatory requirements. This tiered strategy \nresembles the zoned approach to network security that organizations are implementing \nas part of their evolving security architecture. Zones that host critical applications are \nprotected by a variety of controls, ranging from network segmentation and hardened \nvirtualization host servers to additional monitoring. \n In the future, client-aware cloud services will be able to tailor the access they provide \nbased on the security capabilities of the client in order to mitigate risk. A fully managed \ndevice that includes hardware-based enterprise security features and a full software \nsecurity suite may get more access than an unsecured personal device. At the same \ntime, a cloud-aware client will be able to validate that the cloud service it is accessing is \ngenuine, and that it offers the required level of security. \n As businesses use a growing number of cloud services, security requirements \nbecome more complex. A single enterprise may use multiple external cloud services \nwhile also operating a private cloud and a traditional computing environment. It will be \nimportant to streamline access for users. We can expect more emphasis on technology \nthat eliminates the need for users to authenticate to each individual service. \n Security Analytics and Data  Protection \n Security context can be provided not only by sensors, but also by analyzing information \nabout the enterprise environment and the threat landscape. As attackers become \nstealthier, this analysis will become an increasingly important part of an organization’s \ndefenses. Within the industry, many are moving toward the use of security analytics \ntools to analyze patterns of network traffic and system use. I expect to see increasingly \nsophisticated external services that analyze a broad range of information in order to \nthwart attacks. \n As information is used on more devices outside the enterprise network perimeter, it \nwill also be increasingly important to focus on controls that are integrated with the data \nitself. Many organizations are already protecting information with technologies such as \nenterprise rights management. In the future, these capabilities are likely to become more \nsophisticated and automated, allowing businesses to define policies that automatically \nstore sensitive data in highly secured locations. \n Conclusion \n New technologies bring challenges, but they also bring opportunities for the CISO and for \nthe organization overall. \n The rich context-aware experiences that I’ve described in this chapter are entirely \ndependent on IT. To deliver these experiences, organizations will need to understand \nand manage the risks. As the experts in information risk, CISOs and other security \nprofessionals should have opportunities to become closely involved in the development \nand implementation of key business initiatives. This will result in a higher profile for the \ninformation risk and security team across the entire organization. \n",
      "content_length": 3377,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "CHAPTER 8 ■ LOOKING TO THE FUTURE: EMERGING SECURITY CAPABILITIES\n128\n To fully take advantage of these opportunities, CISOs will need broad business and \npeople skills as well as a thorough knowledge of security controls. With the addition of \nthese skills, I believe the role will evolve into the chief security and trust officer (CSTO), \nwith broad responsibilities to enable the business through trusted infrastructure, \napplications, and business processes. As this transition occurs, the CSTO becomes the \nessential enterprise architect, with the IT organization becoming a peer or perhaps a \nsubordinate. I’ll discuss these skills further in the next chapter. \n",
      "content_length": 668,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "129\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_9\n CHAPTER 9 \n Corporate Social \nResponsibility: The Ethics of \nManaging Information Risk \n Be the change you wish to see in the world. \n —Gandhi \n In the past year or so, we have passed a major inflection point; it has become clear that \nalmost every powered device will compute, communicate, and have an IP address. As \ntechnology becomes embedded into the fabric of our lives, exploits that take advantage \nof technology vulnerabilities may increasingly impact the well-being of almost everyone \nin society. This makes it particularly important that we apply the right ethical values to \nshape the way we design, develop, and implement these technologies.  \n The past few years have seen an escalating cycle of risk, with correspondingly \ngreater impacts for businesses and individuals. If that trajectory continues as technology \nbecomes more pervasive, the implications for society could be catastrophic. This \nmeans we should all, as security professionals, contemplate our ethical responsibilities \nnot only to the organizations we work for, the customers we serve, and the company’s \nshareholders, but also to society. To put it another way, I believe that information security \nand privacy are issues of corporate social responsibility. \n Yet even as it becomes even more important to consistently apply an ethical \napproach to managing information risk, business demands and other challenges can \nmake it increasingly difficult to do so. Companies’ continuous efforts to drive growth and \naccelerate time to market translate into demand for faster implementation of internal \nsystems and new technology-based products. At the same time, implementing effective \nsecurity and privacy is becoming more difficult due to a more complex threat landscape \nand the expanding, fragmented regulatory environment. \n These factors result in increasing pressure on technology and business professionals \nto take risky short cuts. In some cases, there may be clear conflicts between business \npriorities, such as the deadline for launching a new product, and “doing the right thing” \nin security and privacy terms. There are also many gray areas in which the right course \n",
      "content_length": 2285,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "CHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n130\nof action is not immediately clear; whether to expend resources on protection against a \nthreat that’s still on the distant horizon, for example. I’ll explore these ethical dilemmas, \nand offer suggestions about how to find solutions to them, later in this chapter. \n \nWHAT IS CORPORATE SOCIAL RESPONSIBILITY?\n Definitions of corporate social responsibility typically focus on the idea that \ncompanies look beyond their profits and legal obligations to their broader role \nin society. A common theme is that a company should take into account the \nsocial, ethical, and environmental effects of its activities on its employees and the \ncommunity around it. Here are three definitions that summarize some of the key \nconcepts: \n “The notion of companies looking beyond profits to their role in society is generally \ntermed  corporate social responsibility (CSR)… It refers to a company linking \nitself with ethical values, transparency, employee relations, compliance with legal \nrequirements, and overall respect for the communities in which they operate. It goes \nbeyond the occasional community service action, however, as CSR is a corporate \nphilosophy that drives strategic decision-making, partner selection, hiring practices, \nand, ultimately, brand development.” (McComb 2002) \n “CSR is about businesses and other organizations going beyond the legal obligations \nto manage the impact they have on the environment and society. In particular, this \ncould include how organizations interact with their employees, suppliers, customers, \nand the communities in which they operate, as well as the extent they attempt to \nprotect the environment.” (Lea 2002) \n “The continuing commitment by business to behave ethically and contribute to \neconomic development while improving the quality of life of the workforce and their \nfamilies as well as of the local community and society at large.” (World Business \nCouncil for Sustainable Development 2007) \n The Expanding  Scope of Corporate Social \nResponsibility \n Despite the obvious societal implications of security and privacy risks, most companies \ndon’t consider them to be CSR issues today. That may change over time, as public and \ncorporate awareness of the risks continues to expand. Already, some major technology \ncompanies include descriptions of how they manage security, privacy, and business \ncontinuity in their CSR reports (see sidebar). That trend may spread as companies in \nother industries add more technology-based products and services. \n",
      "content_length": 2596,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "CHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n131\n Consumer data protection is one area of information risk that is already widely \ntreated as a CSR issue; it is even included in the International Standards Organization \ncorporate social responsibility standard (ISO 26000). As Forrester Research analyst Heidi \nShey put it, “It’s time to start thinking of protecting customer data as a corporate social \nresponsibility, and not to check off boxes for compliance or a thing that must be done so \nyou can avoid some nasty breach costs.” (Shey 2014). \n In terms of the potential impact on society, security and privacy could be considered \na digital extension of consumer safety, which companies have viewed as a CSR issue for \nmany years. Furthermore, a quick review of the history of CSR shows that its scope has \ncontinually evolved and broadened to include new issues, typically as public awareness \nof those issues has increased. For example, it’s not so long ago that rivers and oceans were \nused not only as human sewage dumps but also as a convenient method for disposing of \nindustrial waste; as late as 1969, one large river in Ohio was so polluted that it regularly \ncaught fire. Yet today, discussions of environmental impacts are typical in CSR reports, \nand in the last few years have further evolved into a focus on climate change: in 2015, 82% \nof the world’s largest companies included data about carbon emissions in their reports \n(KPMG International 2015). \n While early social-responsibility efforts were often philanthropic in nature (such as \nthe funding for public libraries and education provided by Andrew Carnegie, founder of \nUS Steel), corporate social responsibility reporting is now a mainstream business practice \nworldwide, undertaken by more than 90% of the world’s largest companies. \n \nTECHNOLOGY COMPANIES THAT  TREAT INFORMATION \nRISK AS CSR\n Some large technology companies—including Cisco, Microsoft, and Intel—already \nposition information risk areas such as security, privacy, and business continuity \nas corporate social responsibility items, and discuss them in their CSR reports. \nWhile the reports devote space to the companies’ achievements, they also \ndescribe corporate positions and principles on key issues such as data protection \nand transparency. Cisco’s 2015 CSR report, for example, notes the company’s \ncommitment to produce a twice-yearly transparency report that includes data \nrequests or demands for customer data received from law enforcement and national \nsecurity agencies around the world (Cisco 2015). \n Apple CEO Tim Cook has also spoken out about his company’s commitment to privacy \nand security, particularly when protecting user data. In a letter published on the \ncompany’s web site, he said: “We don’t “monetize” the information you store on your \niPhone or in iCloud. And we don’t read your e-mail or your messages to get information \nto market to you.” Cook has argued vociferously that government should not have \n“back door” access to systems in order to thwart terrorism. “The reality is if you put a \nback door in, that back door's for everybody, for good guys and bad guys,” he said on \nCBS’ 60 Minutes (Rose 2015). \"I don't believe that the tradeoff here is privacy versus \nnational security. I think that's an overly simplistic view....we should have both.” \n",
      "content_length": 3377,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 158,
      "content": "CHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n132\n The Evolution of Technology and Its Impact \n To continue the exploration of why I believe security and privacy is a matter of corporate \nsocial responsibility, here’s another quick historical perspective, this time examining the \nemergence of information risk in the context of technology’s  evolution . \n The  march of technology can be viewed as a succession of major waves, each lasting \nroughly 100 years (Rifkin 2013). Each wave has brought transformative benefits to society, \nbut also significant challenges. The first wave, starting in the 1760s, included steam \npower, railways, and early factories as well as mass education and printing. The second \nwave, starting roughly in the 1860s and continuing well past the mid-1900s, included \nautomobiles, electricity, mass production, and had an even bigger effect on society. Many \nof today’s corporate social responsibility issues today are the negative impacts of those \nfirst two waves of technology: examples are environmental impacts due to industrial \nproduction, mining, and oil drilling; factory working conditions; and the safety of mass-\nproduced items. \n The third wave began in the 1960s, with early computers, but only really gained \nmomentum in the 1990s. It includes the Internet and smart “things,” molecular biology \nand genetic engineering, and renewable energy. Arguably, this technology wave may \nhave the broadest impact on society of any to date. Each previous wave lasted about 100 \nyears, so history suggests that we are far from reaching the crest. If this wave was a movie, \nwe’d still be watching the opening credits. \n If the opportunities presented by this third wave of technology are unparalleled, \nso are the risks to society. As I’ve argued in earlier chapters, as technology has spread \nexponentially, so have the threats and their impacts, while security controls have \nprogressed at a more linear, incremental rate. As a result, there’s a continually growing \ngap between the capabilities of the controls and the impact of exploits. If the impact of \nsecurity breaches seems big now, consider what the impact will be in 10, 20, or 50 years, \nwhen technology is even more pervasive throughout society.  \n Let’s consider some of the  potential impacts by reiterating two examples from \nChapter 6. Last year, doctors for the first time inserted an artificial “eye” that enabled \na blind person to see. The device is a retinal implant that receives signals from a video \ncamera integrated into eyeglasses. Think ahead a few years, to a time when the implants \nare more sophisticated and can see in much higher resolution, and also include software \nto automatically interpret visual information, such as QR codes. Then imagine that a \nmalicious actor creates a QR code that triggers the vision system to download malware. \n Table 9-1.  The March of Technology \n Version 1.0: 1760s \n Version 2.0: 1860s \n Version 3.0: 1990s \n Steam and coal \n Electric lights \n The Internet \n Railways \n Communications \n Molecular biology \n Factories \n Oil and gas \n Renewable energy \n Printing press \n Mass production \n “Smart” everything \n Mass education \n Automobiles \n",
      "content_length": 3231,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "CHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n133\nLike the PC malware that paralyzed Sony’s network in 2014, the malware then demands a \nransom to re-enable the person’s vision. Now consider the example of a cement company \nthat’s embedding sensors in the concrete mix used to build a new road, thus enabling \nlocal authorities to monitor traffic patterns and adjust signals to optimize the flow of \nvehicles. If the technology is not securely designed and implemented, all that a malicious \nperson needs is the ability to execute malicious code, in order to falsify the traffic pattern \nin such a way that vehicles converge on the scene of a planned bomb attack. \n Here’s example of a  real-life attack that unfortunately has already occurred. Over \na four-day period during November 2008, members of an Islamic militant organization \ncarried out a series of 12 coordinated shooting and bombing attacks across Mumbai. The \nattacks killed 164 people and wounded at least 308. Of the funding that enabled the attack, \n$2 million was raised by cyber crime (Goodman 2015). Think about how cyber crime \nworks. Typically, the cybercrime cycle starts with stealing someone’s identity by installing \nmalicious code on a device or by taking advantage of insecure behavior. So ask yourself: If I \ndon’t keep my systems up to date, if I don’t design and implement them well, and educate \nemployees to ensure they are security-aware, am I indirectly contributing to terrorism? \nThe answer is that you might be—although in most cases, you won’t even know it. \n As I discussed in Chapter 6, four  motivations account for the majority of serious \nexploits. Terrorism is one. The others are financial gain, warfare, and hacktivism. Each \nof these motivations can result in consequences with broad impacts across society: \neconomic damage, loss of services, damage to morale, degradation of government \nservices, and even human casualties. \n As all companies become technology companies, the technology they create and \ndeploy may be exposed to exploits with potential impact on society. The same applies, \nof course, to  public-sector organizations . Even though this idea is becoming more \nwidely accepted, I occasionally encounter people who don’t believe it applies to their \norganization. Recently, as I fielded questions after giving a talk, an audience member \ncommented that she was on the board of a local school and definitely didn’t see the \nschool as a technology organization. “Does your school have a web site that parents and \nkids can use to view and update information?” I asked. She said yes. Then I asked “Does \nyour school have an app that lets parents check whether their kids attend class?” No, \nshe said, but the school was considering it. “Let’s imagine you have a web site that’s not \nwell designed, and a malicious person decides to take advantage of that with a zero-day \nexploit,” I said. “He can compromise the site and the personal information of the parents \nand children that use it.” I added that if a school takes its technology to the next level by \nmaking an app available to parents or kids, it becomes even more clearly a technology \nsupplier—and its security concerns now include product vulnerabilities. By the time \nI’d finished explaining, the audience member asked me if I could come and explain the \nissues to her board, which of course I agreed to do. \n Here’s another school example, one that highlights the risks of failing to consider \nall the  ethical implications : A Pennsylvania school district issued laptops to some \n2,300 students, then remotely activated the laptops’ webcams—without informing the \nstudents—and used the webcams to secretly snap students at home, including in their \nbedrooms. Surveillance software on the laptops also tracked students’ chat logs and the \nweb sites they visited, and then transmitted the data to servers, where school authorities \nreviewed and shared the information and in at least one case used it to discipline a \nstudent. Ultimately, the school district was forced to settle a class-action lawsuit that \ncharged it had infringed on the students’ privacy rights (Bonus 2010). \n",
      "content_length": 4193,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "CHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n134\n Maintaining Society’s  Trust \n The third wave of technology offers opportunities for all organizations. But as the \nopportunities increase, so does the obligation to use technology responsibly. If we don’t \nimplement appropriate security and privacy protection, consumers won’t trust the \ntechnology. If they don’t trust the technology, they will be reluctant to use it. This could \npotentially affect any company that supplies technology, and impact the growth of the \ndigital economy overall. \n Unfortunately, the privacy and security breaches that have hit the headlines in \nrecent years have weakened that trust. As a result, consumers’ trust in technology sank \nlast year in 70 percent of countries surveyed worldwide, according to the Edelman Trust \nBarometer, a widely used indicator of trust in business and government. Worse, the rapid \nimplementation of new technologies that are changing everyday life, “from food to fuel \nto finance,” emerged as a new factor depressing trust overall. “By a two-to-one margin, \nrespondents in all nations feel the new developments in business are going too fast and \nthere is not adequate testing,” the study concluded (Edelman 2015). \n Top US regulators have urged companies to expand and clarify their privacy efforts. \nFederal Communications Commission chairman Tom Wheeler said Internet service \nproviders have a responsibility to make sure personal data is held securely and that \ncompanies are transparent about the data that’s being captured. “There's no question that \nwith connected devices, data is becoming today's currency, and we need to be aware of \nthe impact of that on consumers,” added Federal Trade Commission Chairwoman Edith \nRamirez, noting a recent Pew Research Center survey found that 47% of Americans lacked \nconfidence that they understand what companies will do with their personal information, \nand had mixed feelings about whether or not to share it (Hamblen 2016). The weakening \nof trust is a dangerous trend. Breaking someone’s trust is like crumpling up a perfect piece \nof paper: you can work to smooth it over, but it will never be the same again. \n All organizations inevitably experience security and privacy issues. The question \nis how we respond to them. We can manage them in way that focuses on limiting our \nliability, or we can focus on doing the right thing for those who may be impacted. I \nrecently participated in a peer group discussion that evolved into an intense debate on \nthis very issue. The discussion was prompted by the major breaches that occurred in \n2014 and 2015; as a group, we discussed how we might jointly develop the concept of a \n“minimum standard of care” for security and privacy. Some people wanted to focus on \nlimiting corporate liability for a breach. I believed that was the wrong goal, and argued \nthat the primary focus should be on protecting our customers. My reasoning was that if \nwe protected our customers, we would limit our liability as a natural consequence. But \nif we focused only on limiting liability, we would likely fail to take the necessary steps \nto protect our customers. Furthermore, I believed that the lens we chose to view the \nproblem with would bias strategy and outcomes over the long term. A liability-focused \nstandard would inevitably cause us to direct our efforts into seeking ways to limit our \nresponsibility for the technology we create and manage. But if the standard focused on \nprotecting the people who might be impacted, we would direct our efforts to thinking \nabout how best to prevent, detect, and respond to risks.  \n",
      "content_length": 3673,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "CHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n135\n The Ethics of  Managing Information Risk \n Some professions, such as certified public accountants and doctors, have ethical \nstandards that may require them in some cases to break ranks with their organizations, \nsuch as if they see signs of illegal activities or financial manipulation. We expect doctors \nto be personally accountable for decisions that affect the lives of their patients, rather \nthan simply deflecting responsibility for health decisions onto someone else within the \norganization. If CPAs or doctors fail to meet these professional and ethical  standards , they \nmay lose their ability to practice. \n Although there are many professional certifications for security and privacy \nprofessionals, there’s currently no equivalent to these medical or legal qualifications. \nSecurity and privacy managers are not automatically barred from practicing their trade if \nthey fail to meet professional standards. However, we should all assume a similar level of \npersonal accountability for our decisions—especially since our actions may have broader \nimplications for society. Regrettably, not all of us do. Some security and privacy managers \nsee their role as simply managing a risk register: they identify the risks, and perform the \nanalysis and associated cost estimates, but then they take the register to other executives \nwho then make the decisions. By doing so, they are abdicating responsibility and \ndeflecting accountability onto someone else. \n As the senior security and privacy professional within the organization, CSPOs \nshould share responsibility for information risk decisions equally with the other \ncorporate executives and the board. People are often told that they need to “think like \nan owner;” we need to act like an owner too. And ultimately, we need to think about our \nresponsibility to all the people we work for—including customers and anyone else in \nsociety impacted by our actions—as well as our responsibility to the executives we report \nto. If you don’t think your manager is right, think hard about the possible consequences \nof not speaking out and where your responsibility ultimately lies. \n The recent  events at automakers have shown all too clearly what can happen \nwhen corporate culture produces a system in which professionals are driven to behave \nunethically in order to meet business goals, or fail to take responsibility for their actions, \nwhile senior executives apparently remain ignorant. In the Volkswagen emissions-testing \nscandal, engineers included software specifically to deceive test equipment so that cars \ncould meet the emissions targets required for sale in the US. An investigation into General \nMotors ignition-switch problems that caused at least 13 deaths described the “GM \nSalute,” in which employees sat in meetings, with their arms folded and pointing outward \nat others, as if to say that the responsibility lay with those other people, not with the \nemployees (Maynard 2014). At both automakers, top executives said they were unaware \nof the actions of the lower-ranking employees who were directly involved in the issues. \n In our daily lives, we encounter many situations in which we need not only to decide \non the right course of action, but also to take responsibility for voicing our opinions so \nthat they are considered by the company as a whole. Suppose that a business manager \nis proposing an action that’s legal but conflicts with our security values and approach \nto protecting customers’ information. Or imagine that implementing the right level \nof protection risks the target dates for a critical product launch. Or that failing to tell \ncustomers or suppliers about a potential vulnerability becomes the equivalent of a lie. \n",
      "content_length": 3823,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "CHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n136\n In the book  Giving Voice to Values , author and educator Mary Gentile discusses \nthe ethical dilemmas that many people face in businesses today. Her assumption, as \nshe observes in the book, is that “in many if not most of the managerial and financial \nmisbehaviors we have seen in the past, there were enough people who recognized the \nlapses in ethics and judgment to have stopped them. The problem was that they did not \nbelieve it was possible to do so.” Gentile then focuses on providing techniques to help \npeople voice their concerns and take action at “those times and situations when we \nbelieve we know what is right and want to do it, but we experience external pressures—\nfrom our boss, our colleagues, our customers—to do otherwise. As a result, we are not \nsure how to raise our concerns.” \n \nDISCLOSING  SECURITY ISSUES : A TALE OF TWO \nCOMPANIES\n Questions about how to deal with the discovery and disclosure of security issues \nare likely to generate difficult ethical discussions for many companies. The following \nexamples show how two companies dealt with security issues in very different ways. \n In December 2015, networking vendor Juniper Networks disclosed that an internal \ncode review had discovered “unauthorized code” in its firewall operating system that \ncould allow hackers to gain administrative access and decrypt encrypted VPN traffic. \nThe company said it had not received any reports of exploits using the vulnerability; \nit said it had released patches to fix the problem and urged customers to update \ntheir systems (Worrall 2015). This is a case in which a company appears to have \nmanaged a difficult issue well, in my opinion. It highlights the tough questions and \ndiscussions that companies face when managing potential security issues. How \ndeeply do you test and review your code, knowing that the deeper you dig the more \nlikely you are to find vulnerabilities? If you do find a problem, how do you handle it? \nDo you disclose it, quietly fix it, or even ignore it? Does your company have the right \nvalue structure to ensure that decisions reflect its responsibilities to customers and \nto society? \n Now consider a contrasting example. In 2015, a vendor of dental practice-\nmanagement software agreed to pay $250,000 to settle US  Federal Trade \nCommission (FTC) charges that it falsely advertised the level of encryption it \nprovided to protect patient data (Federal Trade Commission 2016). According to the \nFTC, the company spent two years touting its “encryption capabilities” for protecting \npatient information and meeting “data protection regulations”—yet at the time, it \nwas well aware that its software didn’t provide the encryption required by HIPAA. It \nseems clear that a company that makes deceptive claims of this kind lacks a value \nstructure capable of ensuring ethical security and privacy decisions. \n",
      "content_length": 2959,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "CHAPTER 9 ■ CORPORATE SOCIAL RESPONSIBILITY: THE ETHICS OF MANAGING INFORMATION RISK\n137\n The challenges described in  Giving Voice to Values probably seem familiar to many \nof us who are responsible for managing information risk (see sidebar article). First, how \ndo we decide what is the ethical course of action? Then, how do we take action by voicing \nour opinions when it really matters? \n One starting point is to define the organization’s critical security and privacy \nprinciples, which then can serve to guide our decisions. These principles should be \nderived from the organization’s corporate values. For example, a company that prioritizes \ncustomer service should also be committed to protecting customer information, and \ntherefore its critical principles should include privacy by design. \n We then need to think about how to focus the company on those principles: how \nwe create the right language to express the principles to others, and how we enroll our \norganizations in principle-based decision making. We need to make security and privacy \nclearly visible in the decision-making process, not just within the information security \norganization but across the entire organization. That sends a message to everyone, \nincluding customers as well as people within the organization, that security and privacy \nare corporate priorities. By demonstrating our commitment to these principles, we can \ncreate trust in our organization and its technology. \n We can use our security and privacy principles as a compass to guide us through the \ndilemmas we encounter. We can approach these dilemmas using the same framework \nthat we apply to any source of information risk:  sense , interpret, and act (see Chapter 3).\n• \n Sense: Are changes on the way that that could conflict with our \nsecurity and privacy principles? What is the dilemma that we will \nface? \n• \n Interpret : Analyze the issue to determine the following: Can \nI make this decision? Which of our principles can guide my \ndecision? Who do I need to talk to? What actions can I take, and \nwhat are the direct and indirect consequences of each? \n• \n Act : Will my action align with the organization’s best interests? \nWhat about the interests of our customers, and of society in \ngeneral? Will my action or lack of action create embarrassment \nfor the company? Is my action practical? Who should I tell? \n Conclusion \n As we progress through the third wave of technology, and our reliance on technology \nexpands even further, so does the potential societal impact of security and privacy issues. \nOur professional and ethical responsibilities require that we hold ourselves accountable \nfor doing what we know is right. This is true today, and will be even more so in the future. \nThis means that we will have to take career risks to make sure that security and privacy \nare appropriately handled within the organization, including ensuring that issues are \ndiscussed at board level. I’ll discuss how to do this in more detail in the next chapter on \nthe 21st Century CISO. \n",
      "content_length": 3048,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "139\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_10\n CHAPTER 10 \n The 21st Century CISO \n Leadership is the art of mobilizing others to want to struggle for shared \naspirations. \n —Jim Kouzes and Barry Posner, \nThe Leadership Challenge \n The finance director sounded frustrated and exhausted. Our IT auditors had been \ntrying to tell her about an obscure yet important data backup problem that affected SOX \ncompliance. But her background was in accounting, not technology, and as the IT experts \npresented page after page of technical information elaborating the intricacies of backup \nprocesses, her eyes glazed over. The more they tried to explain by adding yet another \nlayer of detail, the more confused and frustrated she became. \n That’s when I thought of a solution. “Imagine,” I said, “we’ve got a passenger train \nrunning from station A to station B. That’s what our backups are like; they’re carrying data \nfrom our servers to tape.” \n “We know the train arrived at station B, so we know the backup occurred,” I said. \n“But we don’t know how many passengers got on at station A, and we don’t know how \nmany got off at station B. So we can’t definitively say we actually backed up all the \ninformation, and to comply with SOX, we need to be certain.” \n The finance director sat up. For the first time since the start of the presentation, she \nseemed alert and engaged. And from that point on, we made progress. She asked how we \nplanned to solve the problem, we briefly mentioned a couple of the possible solutions, \nand the meeting ended on an upbeat note. \n My storytelling, using an off-the-cuff metaphor, succeeded where the more traditional \napproach had failed. It communicated a technical security issue in terms that a senior \nbusinessperson could understand and remember. And it illustrates one of the key skills of \nthe 21st century CISO. We need to extend our reach outside the security organization to \ncommunicate with and influence people at all levels, from all backgrounds. \n Chief Trust Officer \n In this chapter, I’ll explain some of the skills and traits I believe CISOs need in order to \nfulfill their changing role. To set the stage, I’d like to step back for a moment and briefly \nrecap the changing focus of information security overall. \n",
      "content_length": 2339,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n140\n As I’ve discussed earlier in the book, every company is becoming a technology \ncompany. And as the potential impact of information risk expands, it is becoming \nessential to manage security and privacy as a corporate social responsibility. The CISO’s \nrole should therefore expand to span the full breadth of information-related risks, as \ndescribed in Chapter  1 . At many organizations, this is already happening. CISOs are \ntaking on responsibility for privacy, regulatory compliance, and product and service \nsecurity, in addition to more traditional IT security functions. \n This is a huge opportunity for CISOs to step into a more valuable, high-profile role \nwithin the organization. The core skills of information security professionals—evaluating \nand mitigating risk—are as essential for mitigating new risks associated with product \nsecurity, privacy, and regulatory compliance as they are for more traditional IT-related \nthreats. But perhaps this broader role requires a different title that more accurately \nreflects the convergence of risk responsibilities, such as Chief Trust Officer or Chief \nInformation Risk Officer. \n Taking on a larger role requires a broader view and a corresponding set of skills. We \nneed to communicate in terms that business people understand, and build relationships \nthat enable us to influence people at all levels across the organization. We also need \nextensive management and leadership skills, both to operate at an executive level and to \ninspire our expanded risk and security team.  \n The ability to manage the full range of information-related risks is a necessity, \nnot just for the CISO, but for the organization. If we do not step into a broader role, the \norganization must acquire these abilities elsewhere. Because of this, CISOs who do not \nadapt to this role run the risk of becoming irrelevant to the organization. Alternatively, \nthese risk areas will be managed in a stove-piped, fragmented way, in which case the \norganization may never discuss the aggregation of risks and the controls necessary to \nmanage them. If this occurs, organizations will certainly generate unmanaged risks to \nthemselves, their customers, and to society. \n Until recently, one of the CISO’s biggest challenges was obtaining funding for \nsecurity initiatives. Today, due to the prevalence of large breaches, it’s often easier to find \nfunding. But more funding doesn’t always lead to greater security or a better outcome for \nthe organization. Sometimes the fear of breaches drives organizations to invest heavily in \ncontrols that generate a high degree of control friction, restricting users’ ability to do their \njobs. For example, some organizations have installed controls that prevent users from \ndownloading apps or files, or even accessing some web sites. These controls threaten \nto stifle users’ ability to innovate and hinder overall business velocity. Furthermore, \ndetermined users will find ways around the controls, such as using less-secure personal \nsystems to access “forbidden” resources. \n CISOs need business acumen to understand the impact of security controls on \nothers in the organization. As I discussed earlier in the book, our approach to security \narchitecture should start with an understanding of the 9 Box of Controls, including the \nfriction that controls can generate. Business acumen is also necessary to communicate \ntechnical risks in language that nontechnical people in the business can grasp, and to \nunderstand that some risks are worth taking. Risk-taking is fundamental to business. \nWithout it, no business value would be created. \n",
      "content_length": 3660,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n141\n The  Z-Shaped Individual \n If we don’t already have the skills required of the 21st century CISO, we need to acquire \nthem. \n To some extent, this trend parallels what is happening in most technology-related \nprofessions: IT professionals need to acquire business acumen as well as depth of IT \nknowledge. The concept of “T-shaped”  individuals has been widely used to describe the \nidea that IT professionals need to be able to provide value horizontally, across business \ngroups in the organization, as well as vertically at all levels within IT. \n This concept is useful, but it doesn’t fully encompass the skills of the 21st century \nCISO. The unique role of CISOs and other security professionals might be better \nrepresented as a “Z-shaped” individual, as shown in Figure  10-1 . Adding the third \ndimension of core security skills, such as risk assessment and understanding of controls, \nallows us to deliver value across the business and all areas of IT. \n The 21st century CISO needs to understand business priorities and processes well \nenough to identify how security controls help or constrain the business. To gain this level \nof understanding, he or she has probably gained experience in areas that are central to \nthe company’s business, which, of course, vary depending on the company’s core focus. \nFor example, the CISO might previously have worked in manufacturing operations, \nservices, or mergers and acquisitions. \n The CISO needs technical knowledge too, although the depth of technical \nknowledge required remains a subject of intense debate among my peers. I’ve observed \nCISOs at smaller and less-complex organizations who feel they need deeper technical \nskills to do their jobs. This is not surprising. With much smaller security teams, CISOs at \nsmaller companies may need to be more involved in day-to-day technical details as well \nas managing people. At larger and more complex organizations, CISOs are less likely to \nspend time delving into technical detail. \n Figure 10-1.  The T-Shaped IT professional (left) and the Z-shaped CISO (right) \n \n",
      "content_length": 2118,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n142\n However, all CISOs need to be able to understand enough about the technology to \nabsorb the important issues and communicate these issues to other managers outside \nthe security group. This means that our technical knowledge must be broad, ranging from \ndevices to data centers. We need to know enough about devices, such as smartphones, \nPCs, tablets, and new evolving device types such as wearables, to understand the security \nimplications as well as the benefits. At the other end of the scale, we need to know enough \nabout data centers and physical access controls to understand and communicate the \nimportant security requirements and challenges. \n Our core risk management and security skills provide the link that completes the “Z” \nby connecting technology and business. We understand how to assess and manage risk \nby applying procedural, technical, and physical controls to meet the organization’s legal, \nprivacy, and security requirements. \n Foundational Skills \n Becoming a Z-shaped individual is the foundation for one of the 21st century CISO’s \nessential traits: establishing credibility across the organization. We must be credible \nin order to build trusted relationships with executives and specialists across the \norganization and to discuss the vast range of issues that affect the business. This \ncredibility is built on the competence that comes from understanding the business and \ntechnology as well as possessing core security skills. By becoming Z–shaped, we will also \nbe better positioned to influence risk management for the company’s product and service \nstrategy, as opposed to having those risks managed independently by another group. \n Our ability to influence the organization also springs from a clear mission. I use the \nterm  centered to describe this. We can effectively present our case because we have a \nstrong sense of purpose and a clear understanding of why the security group exists and \nwhat we are trying to achieve. \n This idea returns us to the theme of this book: Protect to Enable. In our global \neconomy, most companies operate in highly competitive markets. As the security \norganization, our mission is to enable the free flow of information and rapid \nimplementation of new capabilities to ensure success and long-term competitive \nsurvival. Other CISOs may work at more risk-averse organizations, and therefore some \naspects of their mission may differ. However, the mission always needs to be aligned with \nthe organization’s business priorities. It is essential that this mission becomes a part of \nwho we are and why we exist. It provides a sense of purpose that lends authenticity and \nconsistency to our actions and helps us build credibility across the organization. \n As we all know, security can be a particularly distracting profession, with a constant \nbarrage of day-to-day emergencies and diversions. So we need a clear mission in order \nto retain a strong sense of direction. Like expert sailors, we can progress toward our goal \namid the day-to-day distractions and diversions, making continual adjustments and \ncorrections to stay on course as the winds shift. \n We also need to retain a sense of curiosity. To engage with others, we need to be \ngenuinely interested in what they do. This curiosity enables us to continue to learn, \nbuilding on and broadening the competencies that then enhance our credibility. \n Another major reason we need to be learners is to stay ahead of the enemy. Threat \nagents are always learning because they must. As new threats emerge, we put in place \nnew controls. But once implemented, these controls tend to be static, while threat agents \n",
      "content_length": 3689,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n143\nare dynamic, coming up with new techniques to bypass the controls. Therefore, our \nthinking must also be dynamic, and we must continually learn in order to protect against \never-evolving threats. \n Becoming a  Storyteller \n We cannot influence people unless we communicate with them. And as the scope of \ninformation risk expands, we need to communicate with a wider range of people across \nthe organization. \n Communicating with people isn’t always easy, as most of us have discovered. If we \nstart relaying technology details to those who aren’t technologists, we won’t capture their \ninterest. In fact, we run the risk of doing the opposite, as I described in the example at the \nstart of this chapter. \n To communicate, CISOs must become chameleon-like, with the ability to blend \ninto a variety of environments. We need enough knowledge of each business domain to \nbe able to communicate with different groups using language they understand. And we \nneed to discuss these subjects at different levels. A CFO may only want to hear a high-\nlevel summary expressed in terms of financial impact and return, which is often not easy \nwhen discussing security investments targeting hard-to-quantify threats. Product group \nmanagers want to hear security issues expressed in terms that relate to sales, marketing, \nand operational efficiency. \n I’ve found storytelling to be a powerful tool for communicating with diverse people \nacross the organization. When I frame security issues as stories and images that people can \nunderstand, they relate better to the issues even if they lack a background in technology. \n I like to tell stories using metaphors and analogies. They are easily remembered, \nand they translate complex subjects into simple terms everyone can understand. In fact, \nthe metaphors I’ve used throughout this book, such as the perfect storm in Chapter  1 , \nthe train backup in this chapter, and the roundabouts and traffic lights in Chapter  5 , have \nhelped me communicate security issues to many people. To use yet another analogy, \norchestra conductor Benjamin Zander said, “The conductor of the orchestra doesn’t \nmake a sound. His power comes from awakening possibility in others.” (Zander and \nZander 2000). In the same way, I believe the power of the CISO comes from awakening \nthe awareness of risk among people across the organization. I use stories based on \nmetaphors to create that awareness. \n For example, employees often find it hard to understand the dangers of stealthy \nthreats. This is because the threats are unobtrusive, concealing themselves so they can steal \ninformation over the long term. Users are usually not even aware that a problem exists on \ntheir system. They still associate malware with obvious, annoying symptoms such as screen \nmessages and system crashes. So when we tell them we’ve detected dangerous software on \ntheir machine, they have a hard time believing that it matters. That is why we have to focus \non prevention using low-friction controls. If we do not achieve this as a profession, we will \nperpetuate the worsening cycle of risk we are experiencing today. \n To communicate the danger, and the need for effective preventative controls, I \nsometimes use the analogy of ants and termites. “Malware used to be like food-eating \nants in the kitchen,” I explain. “You’d know when you had an infestation because you’d \nsee ants crawling over the countertops and walls. Once you knew about them, you’d spray \nor set traps to eliminate them. \n",
      "content_length": 3542,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n144\n “But today, threats are more like the termites that can live in your walls. You can’t see \nthem, and you may not even know they are there. But they’re doing much more damage \nthan ants ever did. In fact, they may be destroying the structural integrity of your house.” \n I’ve found using analogies helps quickly drive home messages. People immediately \nunderstand that these invisible threats can undermine the structure of the computing \nenvironment, just as termites undermine houses. This makes them more likely to \naccept the next step, which is that we have to perform the digital equivalent of tenting \ntheir computer to eradicate the vermin, but without toxicity to users or the computing \nenvironment. \n \nTHE  NIST FRAMEWORK : A COMMON LANGUAGE \nFOR RISK MANAGEMENT\n To discuss information risk management across the organization, it’s helpful to use \na common language that everyone, including non-technical people, can understand. \nI’ve found the National Institute of Standards and Technology (NIST) Cybersecurity \nFramework to be a helpful tool for communicating the issues. Development of the \nframework was triggered by a 2013 presidential executive order on improving the \nsecurity of critical infrastructure. This led to a year-long private-sector–led effort to \ndevelop a voluntary how-to guide for organizations. Many companies contributed \ninput about standards, best practices, and guidelines to that effort. I was one of the \nfirst security leaders among the Fortune 500 companies to engage the framework. \n The framework creates a common taxonomy and terminology for managing risk, \nmaking it easier for security teams and others to communicate. It fosters collaboration. \nIn addition, each organization can measure its risk management maturity level \nagainst the framework. As the framework is used by more people, including business \nexecutives, it may help to increase the overall understanding of information risk and \nhow to manage it, which would be a good thing for all organizations. \n Fear Is Junk  Food \n Just as building trusted relationships is essential to influencing the organization, I also \nthink we need to transcend the doom-and-gloom that can pervade discussions of \nsecurity topics. \n The security industry has a tendency to use fear to sell products. Unfortunately, this \ntendency reflects the fact that many people in the security industry profit from insecurity: \ntheir revenue grows when more breaches and other incidents occur. Internally, as \nsecurity professionals, we sometimes share this tendency to use fear as a tool to obtain \nadditional budget or other resources. Of course, security really is about scary things: \nthreats, vulnerabilities, and risk. But focusing on fear as the primary motivator is like \nliving on a diet of junk food. It may provide immediate gratification, and it’s somewhat \naddictive, but ultimately it’s not healthy for either the CISO or the rest of the organization. \n",
      "content_length": 2988,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n145\n In the short term, fear can scare people into action and help drive funding for \nsecurity projects. However, relying on fear alone can only work for so long. Eventually, it \nhas the opposite effect. It causes the CISO to lose credibility. In fact, I think relying on fear \nmay even contribute to the high rate of job turnover among CISOs. Those who rely too \nmuch on selling fear are snacking on an unhealthy diet, and eventually the organization \nrealizes this and rejects them. \n Ultimately, fear doesn’t work for other reasons too. Most people don’t want to listen \nto a continuous stream of negativity. If we are always seen as the source of negativity, we \nwill lose our audience. If we are continually viewed as the group that says no, we will be \nignored. People will bypass security restrictions in order to meet their business needs. \n Even within the security organization, fear can become a gravitational force, a \nblack hole drawing ever-increasing attention to the negative side of security issues and \ndraining energy that should be directed to enabling the business. This is why we need \nto focus on solutions that deliver the three key benefits I discussed earlier in the book: a \ndemonstrable and sustainable bend in the curve of risk; the ability to lower the total cost \nof controls; and low control friction to improve business velocity and the user experience. \n Accentuating the Positive \n So how do we take a more  positive approach ? By focusing on our mission, which is to \nProtect to Enable. This mission shifts the emphasis from the negative to the positive: how \nwe can help the business achieve its goals by solving these information risk and security \nproblems. It puts hope and optimism before the challenge. \n This mission is aligned with the business. Rather than being antagonistic, it is based \non common values. It sets an optimistic tone, and, in the long term, optimism is a far \nbetter motivator than pessimism. Threats may be frightening, but our goal is to see past \nthe threats and identify the opportunities. To paraphrase the noted Stanford University \nbehavioral scientist Chip Heath, there’s no problem that cannot be solved without a new \nframework. Therefore, if we can’t see a solution, we have the wrong framework. Protect \nto Enable provides a new framework. So does the 9 Box of Controls, with its focus on \ncost efficiency and control friction as well as effectiveness. These tools help us focus on \nfinding solutions. \n Imagine you’re invited to attend a meeting to discuss whether the company should \nstart using a specific cloud-based business application from a new supplier. Clearly, this \nproduct introduces risks: it comes from an unfamiliar supplier, it’s accessed over the \nInternet, and it means sensitive data will be stored outside the enterprise. \n A narrow security view might focus solely on minimizing the risk. However, this \nnarrow view can lead to a Catch-22 situation, as discussed in Clayton Christensen’s \nbook  The Innovator’s Dilemma (Harvard Business School Press 1997). Typically, it goes \nsomething like this. To minimize the risk, the organization initially restricts the use of a \nnew technology. For example, the technology can only be used for low-risk data, or by a \nnarrow segment of employees. The problem with this approach is that it also reduces the \nbusiness benefit to the point that the benefit of the technology cannot justify the expense \nand effort of adopting it. So we reach an impasse. To make the technology a viable \nproposition, we need to be able to show a business benefit, but we can’t show a business \nbenefit because we won’t allow viable use of the technology. \n",
      "content_length": 3710,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n146\n Protect to Enable provides the new framework that frees us from the innovator’s \ndilemma. It allows us to focus on the opportunity and identify benefits that outweigh the \nrisks. For example, introducing a new supplier increases competition for our existing \nsuppliers, leading to future savings for our organization. This benefit aligns with the \nbusiness and is one that everyone in the organization understands. Perhaps less intuitive, \nbut equally important, the savings can be used to fund security controls to mitigate the \nrisk of using the technology more widely. Now our benefit/risk equation has a positive \nresult rather than a negative one. By enabling the technology to be used more widely, \nwe realize bigger business benefits that outweigh the additional cost of controls. This \nexample also underlines the need for CISOs to build business acumen that enables us to \nsee the opportunity and how it can be used to overcome the challenge of funding security \ninitiatives. \n Let’s look at another example, this time from my experience at Intel in the days \nbefore I had defined our Protect to Enable mission. Several years ago, a highly damaging \nworm was discovered in our environment, requiring a significant emergency response \nfrom our team. Upon investigating, we traced the origin of the worm to an employee’s \npersonal system. \n Our immediate response was that of a stereotypical security group. We shut down \nthis usage to eliminate the risk of future infections. We immediately tightened security \npolicy to ensure only corporate-owned PCs could access the network, and we ruthlessly \nwent through the environment and cut off access by any devices not managed by IT. \n Our response was successful in the sense that it reduced the risk of infection. But it \nled to other risks we hadn’t foreseen. Eliminating personally owned PCs from the network \nmeant we now needed to issue corporate PCs to contract employees. This meant that \nwe had to provide more people with devices that allowed full access to the corporate \nenvironment. It also, of course, increased capital costs. The broader impact was that \nit eliminated the potential business benefits of letting people use their own personal \ndevices for work.  \n Subsequently—driven largely by employee demand, as well as the massive \nproliferation of new consumer devices—we revisited this issue. This time, we examined \nit from the perspective of Protect to Enable. We looked at the business opportunities if \nwe allowed personally owned systems on the network, and then how we could mitigate \nthe risks. As I mentioned in Chapter  1 , we rapidly discovered that the business value \nis enormous. Helping employees communicate and collaborate at any time can drive \nsignificant productivity gains. It also helps make employees happy. They love using their \npersonal smartphones, PCs, and tablets, and they appreciate that we enable them to do so. \n These benefits easily outweigh the cost of the technology required to reduce the risk \nof allowing access by personal devices. True, some of this technology wasn’t available \nat the time we experienced the original security problem. But if we had focused on the \nopportunity first, perhaps we could have found ways to provide some level of access while \nmitigating the risk, and experienced at least some of the benefits we enjoy today.  \n Demonstrating the Reality of Risk \n Of course, the security organization’s role still centers on  managing risk , which includes \ndiscussing the negative consequences of people’s actions. If we frame this discussion \ncarefully, I believe we can inform without fearmongering. By describing possible \n",
      "content_length": 3699,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n147\noutcomes and solutions without using emotional language, in terms listeners can \nunderstand, we create a context in which the organization can make the decisions that \nare best for the business. \n Even when we have to highlight unpleasant outcomes, we’re not fearmongering if \nour information is based clearly on reality. Here’s another example from my experiences \nat Intel. As our customers’ use of the Internet expanded, Intel’s marketing groups \nnaturally wanted to expand their external online presence by creating new web sites. \nSo we, as Intel’s information security group, began assessing the risks and the security \ncontrols required. Some of our marketing teams didn’t find this an appealing prospect. \nThey needed to move quickly, with the freedom to communicate however they thought \nbest, and they viewed security procedures as bureaucracy that slowed them down and \nhindered their ability to communicate with customers and partners. \n What happened next was far more persuasive than any of our initial efforts to \nforestall potential problems. A few web sites were launched without rigorous quality \ncontrol. Hackers found the weaknesses in these sites, but they didn’t crash the sites or \nsteal information. Instead, they inserted links to porn sites. \n When this unfortunate fact was discovered, it provided the leverage we needed to \nimprove security procedures. I realized this was a case where a picture spoke a thousand \nwords. So, to illustrate the impact, I simply showed the links to people within the \ncompany. This wasn’t fearmongering. It was simply demonstrating the real consequences \nof their actions on the brand. Everyone could understand the implied question: Do \nwe want our brand to look like this? This ended, once and for all, any discussion about \nwhether we needed to apply rigorous quality control to external web sites. \n The CISO’s  Sixth Sense \n In the book  Blink: The Power of Thinking Without Thinking , author Malcolm Gladwell \n(Little, Brown & Co. 2005) describes an interesting experiment. Researchers asked \nsubjects to play a game in which they could maximize their winnings by turning over \ncards from either of two decks. What the subjects didn’t know was that the decks were \nsubtly stacked. They could win by selecting from one of the decks, but selecting from \nthe other deck would ultimately lead to disaster. After about 80 cards, the subjects could \nexplain the difference between the decks. But they had a hunch something was wrong \nmuch sooner, after only 50 cards. And they began showing signs of stress and changing \ntheir behavior even sooner, after only about 10 cards, long before they cognitively \nunderstood a difference existed. \n As CISOs, we develop a sixth sense about security issues. Often, my instincts suggest \na need to act or begin investigating a specific direction long before our group is able to \nfully understand or explain what is happening. This sixth sense is particularly relevant \nin the security realm, where our information is almost always imperfect or incomplete. \nWhen a threat strikes, we do not have time to conduct extensive research or wait for \nevidence to accumulate. Therefore, we need to act decisively based on imperfect \ninformation. \n I think we develop this sixth sense from the diverse experiences and skills we’ve \nacquired during our careers. We can also foster this sixth sense by being aware. Some \nsecurity professionals tend to be inwardly focused, looking only at the data and systems \nthey need to protect. As described in Chapter  4 , I have directed my teams to try to be \n",
      "content_length": 3628,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n148\nmore open and outward-looking, sharing information and seeking input from a variety \nof sources, including peers across our company and at other organizations. This can help \nCISOs spot early warning signals and correlate information to quickly identify threats. \nLike secret service agents scanning a crowd, our experience helps us spot anomalies, to \nsee the signals and ignore the noise. \n By identifying future risks early, we may be able to prevent them entirely, or at least \nminimize their impact. We may also reduce the overall effort needed to deal with the risk. \nEarly action may avoid the need for emergency response and a potentially major cleanup \neffort. \n Taking Action at the  Speed of Trust \n A sixth sense is only of value if the organization can act on it quickly. This requires two \nthings. First, we need the courage to take a leap of faith based on what we believe. This \ncourage is rooted in the attributes I discussed earlier in this chapter, such as being \ncentered and credible, with a clear sense of our mission. \n The second requirement is that the organization responds quickly when we inform \nthem about a security issue. This rapid response is only possible if we have established \ntrusted relationships with people across the organization. Because of these relationships, \nthe organization can act at the  Speed of Trust , as Stephen M. R. Covey describes it in \nthe book of the same name (Free Press 2008). Faster, frictionless decisions are possible \nbecause people know, from experience, that our information is reliable and that our focus \nis on enabling rather than spreading fear. \n The CISO as a  Leader \n Above all, 21st century CISOs must become effective leaders who can inspire their teams \nto enable and protect the organization \n Over the years, I’ve identified three essential themes I try to instill in my team and \nconstantly reinforce in our day-to-day interactions. Our security team members must \nbelieve in our mission; they must feel they belong within the security group and the \ncompany as a whole; and they must feel they matter. \n If I can make people feel that they believe, they belong, and they matter, they will \ntackle any challenge. As Kouzes and Posner put it in  The Leadership Challenge (Kouzes \nand Posner 2012), “leadership is the art of mobilizing others to want to struggle for \nshared aspirations.” If people understand the greater goal, it helps establish an emotional \nconnection that guides their everyday actions. This is a key reason that I have thought so \nmuch about defining the mission, and that I have spent so much time helping the teams I \nhave led to see how their jobs are connected to the business’s objectives and concerns. \n For example, a typical  operational goal might be to patch all systems within a week \nof a new software release. This goal is more meaningful if we establish the links to the \nbusiness using the  I believe, I belong, and I matter mantra: “I believe in the mission of \nProtect to Enable. If I'm not protecting to enable, the other employees at the organization \nI belong to cannot do their jobs effectively. The company doesn’t achieve its results, and \nthe company doesn’t execute its vision. Patching systems quickly matters because it helps \nour users do their jobs, which in turn helps the business achieve its goals.” \n",
      "content_length": 3376,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n149\n Learning from Other  Business Leaders \n As leaders, we can learn a lot from how other business leaders work. Today, managers \nare moving away from command-and-control to a more collaborative approach that \ntakes advantage of the diversity of employee ideas and strengths. I’m not talking about a \nconsensus process, which can lead to endless debate and indecision. Rather, a leader’s \ngoal is to ensure alignment to a common mission and accelerate decisions. Within this \nframework, differing viewpoints and debate spark creativity, generating new ideas and a \nproductive tension that can drive results. \n Because security can be frustrating, even daunting, it’s vital to find ways to help \nemployees stay motivated. It’s important to help employees feel they are making \nprogress, not just when they achieve major milestones, but in solving the smaller \nproblems they face every day. A key study found that even small wins boost motivation, \nproductivity, and creativity. In the  Harvard Business Review article describing the \nstudy, authors Teresa Amabile and Steven Kramer (2011) determined that the feeling \nof making progress is the most important contributor to an employee’s emotions, \nmotivations, and perceptions. \n Opportunities to lead occur continually, in every interaction with our teams, with \nother people in IT, and with business partners. The question we need to ask ourselves \nis whether we are seizing these opportunities to reinforce our mission and ultimately to \nhelp the organization achieve success.  \n In highly technical jobs and organizations, we have a tendency to focus on technical \nchallenges while overlooking the “people factor.” I think it’s important to remember the \nneed for personal connections, which foster the sense of belonging. When we know a \nlittle more about each other, we care more as a result. I think about this in my day-to-day \ninteractions. If a team member is making a presentation, are we paying attention and \nasking thought-provoking questions, or are we distracted? And if so, do we think they will \nfeel they belong? \n When we meet with a team member to discuss their struggles with a project, are we \nhelping them think through the issues and come up with solutions? Are we helping them \nbelieve they can overcome the challenges and that the results will matter to the company \nand to us? Or are we just taking them to task? Each interaction is an opportunity for \ncoaching and helping employees improve their performance. \n A final requirement of effective leadership is the ability to develop other leaders \nwithin the security group. Otherwise, the group’s strengths in managing risk for the \nbusiness will last only as long as the current CISO’s tenure. By building competence in \ndepth, the CISO can ensure that the organization delivers sustained performance over \ntime. We will discuss this in more depth in the next chapter. \n Table  10-1 shows research by executive-search firm Korn Ferry suggesting that \ncybersecurity leaders need a unique set of attributes, including the ability to think \noutside the box, dig deeply into issues, exercise judgment at board level, and be a credible \nbusiness partner (Alexander and Cummings 2016). \n",
      "content_length": 3247,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n150\n Table 10-1.  Attributes of Cybersecurity Leaders(Alexander and Cummings 2016) \n Key Attributes for Cybersecurity Executives \n Competence \n Experience \n Traits \n Drivers \n Strategic, global \nthinker (sees big \npicture) \n Depth of technical \nexperience \n Learning agile (can \nadapt to the new \nand different) \n Seeks high visibility \nand accountability \nroles \n Thinks outside the \nbox \n Understands the \nevolving legal \nand regulatory \nenvironment \n Flexible \n Strives to be agent \nof change (not agent \nof “no”) \n Analytical (digs \ndeeply into issues) \n Has successfully \nhandled security \nincidents in the past \n Tolerance for \nambiguity \n Must “thread the \nneedle to balance \ndriving change with \nmanaging enterprise \nrisk” \n Possesses business \nsavvy (understands \nhow information \nis used in daily \noperations) \n Intellectually \ncurious \n Pursues close \nengagement with \nbusiness leaders \n(works to add \nbusiness value) \n Balances competing \npriorities \n Bias for action \n Communicates and \ninfluences broadly \n(board, senior \nmanagement) \n Attracts, builds, and \nleverages talent \n \n Voicing Our  Values \n Obviously leadership means taking responsibility. Yet some CISOs seem to forget this, \nat least occasionally. A typical situation goes something like this. The CISO warned of \na security issue but couldn’t obtain the budget or resources to address it. So the CISO \nabdicated responsibility because someone else had made the decision not to fund \na solution. I take a different view. I believe even if we disagree with the decision, we \nshould do our best to voice our values. We need to articulate the potential impact to the \norganization, to our customers, and to society, as I discussed in Chapter  9 . \n As partners in the organization’s strategy, we should commit to the decision and \nshare full accountability and responsibility with our peers. Having said that, we also need \nto clearly express our personal values and stay true to our principles. Adhering to our \nvalues may mean taking career risks, as discussed in Chapter  9 . Therefore it is critical \nthat we take the time to reflect on what our principles and values really are. This personal \n",
      "content_length": 2209,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n151\njourney, which we all need to take, adds another dimension to the Z-shaped individual, \na dimension of values (Figure  10-2 ). As Mary Gentile, the author of  Giving Voice to Values \n(2010) , puts it, “We are more likely to voice our values if we have decided that the costs of \nnot doing so, and the benefits of trying, are important enough to us that we would pursue \nthem even though we cannot be certain of success in advance. In order to get to this place \nof clarity, we need to spend serious time thinking about our own identity, our personal \nand professional purpose, and our own definition of success and failure.” \n Figure 10-2.  Another dimension of the  Z-shaped individual : the personal values that guide \nour actions \n Discussing Information Risk at Board  Level \n Clearly, corporate discussions of any topics that have such far-reaching potential impact \non society should include participation by the executive board. Board awareness of \nsecurity has increased somewhat due to the spate of well-publicized breaches. Yet \nsurveys show that the majority of boards are still not aware of major security and privacy \nissues. A recent study found that only 32% of boards review security and privacy risks, and \nonly 45% have any involvement in security strategy (PWC 2015). \n In contrast, a significant number of security professionals believe that the CEO \nand executive boards are responsible to society for the sometimes disastrous impact of \nsecurity and privacy issues. In another recent survey, one sixth of security professionals \n \n",
      "content_length": 1592,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n152\nsaid they advocate arrest and a prison sentence for the CEO or board members after a \nbreach (Websense 2015). That seems to indicate that they feel their management is not \ntaking the problem seriously enough, or perhaps even chooses to look the other way, and \nthat they are concerned about the broader consequences to society. \n Given the broad and ever-growing importance of security and privacy, boards need \nbe much more involved in than they have been in the past. It is the CISO’s responsibility \nto bring important security and privacy issues to the board, and initiate a debate about \nthe potential impacts of those issues and the right response. Even with the current \nheightened awareness of security issues, it may not be easy to get the board’s attention, \nbecause board members have so many other business issues to worry about. It can help \nto hone in on the handful of risks with the largest potential financial impact or other \nmajor implications such as damage to the company’s brand. Key areas for boards to \nconsider include\n• \n Security and privacy strategy : Is it cohesive and complete? \n• \n The security and privacy leadership : Do they act with a level \nof independence? Do they take ownership of issues, or do they \nsimply manage a risk register? \n• \n Incident response planning and drills : Do they occur? Are they \nintegrated across the organization? \n• \n “Tone from the top :” Is the executive team engaged? Do their \nactions match their words? \n• \n Security and privacy governance : Does it have the appropriate \ndecision-making structure, including the right level of “tension” \nbetween different stakeholders? Is it set up to ask the “high \ncontrast” questions (as discussed in Chapter  2 )? \n The CISO must take responsibility for determining which issues merit the board’s \nattention. That determination will depend on the potential impact of an exploit \nconducted against the company’s internal systems or technology-based products and \nservices.  \n \nC-I-S-O ATTRIBUTES\n In this chapter, I have covered a range of abilities and characteristics that the 21st \ncentury CISO requires. Many of these probably sound familiar, but it’s all too easy \nto forget them amid the demands of hectic daily schedules. I’ve found a good way \nto remind myself of some of the key attributes is simply to look at my job title. The \nletters in CISO help me remember that we all need Character, Intuition, Skills, and \nObjectivity. So if you’re struggling to remember all the details in this chapter, just \nremember you’re a CISO. You need Character to ensure your actions demonstrate \nintegrity; Intuition to anticipate what’s needed and act accordingly, taking risks when \nnecessary; Skills that span business, technology, and a wide variety of risk areas; \nand Objectivity in order to avoid falling prey to fear-mongering. \n",
      "content_length": 2880,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "CHAPTER 10 ■ THE 21ST CENTURY CISO\n153\n Conclusion \n As the technology environment continues to evolve, many people believe we’re moving \ntoward a future in which  organizations outsource much of the delivery of IT services. If \nthis trend continues, what does it mean for the CISO? \n In this view of the future, the organization shifts away from IT implementation to \nprocurement and management of suppliers and services, while setting direction and \nestablishing an overall IT architecture. \n In addition to this, the organization will need to retain the core competency of the \nsecurity group: the management of information risk. Essentially, organizations cannot \noutsource risk. We can hire companies to deliver our business systems, but we’re still \nresponsible for compliance with regulations that affect our companies, such as SOX \nand HIPAA. And if a breach results in theft or leakage of personal information or critical \nintellectual property, we’re still responsible for reporting it. Furthermore, we still suffer \nthe damage to our brand, even if the breach was due a failure of the supplier’s systems. \nAs regulations proliferate and more and more personal information is stored in business \nsystems, the risks can only increase. \n Therefore the CISO’s abilities will remain essential, even if the job title changes. The \norganization must retain the management of information risk as a core competency. \nAs CISOs, we are poised to continue providing that core competency as long as we can \neffectively work within this new environment by developing the abilities I’ve described \nin this chapter and throughout this book. These abilities enable us to work with others to \nsupport the Protect to Enable mission. \n I’ll close this chapter with an excerpt from a speech by Teddy Roosevelt; the \nsentiments seem as relevant today as when he made the speech back in 1910. “It is not \nthe critic who counts; not the man who points out how the strong man stumbles, or \nwhere the doer of deeds could have done them better. The credit belongs to the man who \nis actually in the arena, whose face is marred by dust and sweat and blood; who strives \nvaliantly; who errs, who comes short again and again, because there is no effort without \nerror and shortcoming; but who does actually strive to do the deeds; who knows great \nenthusiasms, the great devotions; who spends himself in a worthy cause; who at the best \nknows in the end the triumph of high achievement, and who at the worst, if he fails, at \nleast fails while daring greatly, so that his place shall never be with those cold and timid \nsouls who neither know victory nor defeat.” (Roosevelt 1910) \n We need to be in the arena, and so do our teams. Our mission, as information \nsecurity and privacy professionals, is a worthy cause. With our efforts to prevent harm to \nour organizations, our customers, and to society, we can ensure that tomorrow is better \nthan today. \n",
      "content_length": 2936,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "155\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8_11\n CHAPTER 11 \n Performance Coaching \n If your actions inspire others to dream more, learn more, do more, and \nbecome more, you are a leader. \n —John Quincy Adams \n Over the years I have attended and taught many management and leadership classes. \nI have also received and written countless performance reviews. I have overseen the \nratings and reviews for literally thousands of employees, starting when I ran a call center \nfor a large retailer back in the late 80s, before I attended graduate school. One thing that is \nclear to me, after so many years participating in these annual and semi-annual corporate \nrituals, is that there is the potential for considerable ambiguity, particularly when \nassessing soft skills, those that cannot be measured using hard metrics such as the ability \nto meet deadlines or deliver revenue commitments. \n This ambiguity makes it hard for employees to understand how to meet their \nmanager’s expectations. It makes it hard for them to understand the factors that may \nbe limiting their progress from a junior player in the organization to a more senior role. \nI believe this ambiguity can be clarified, although there will probably always be some \nqualitative differences in perspective between employee and manager, and even among \ndifferent managers. \n For these soft skills, I believe performance  coaching needs to be emphasized \nover performance  management . This is because at many organizations, performance \nmanagement focuses primarily on promoting the fittest and eliminating the weak. The \nprocess looks at who is getting the best ratings and who is getting the worst. Managers \nthen work to remove the lowest performers from the organization. This selection process \nis a natural cycle, and one that should continue to play a role. However, I believe that \ncoaching can yield better long-term results, both for individuals and for the organization. \nCoaching focuses on helping everyone in the organization, including ourselves, reach \ntheir full potential. The ultimate goal is to create a high-performance organization in \nwhich everyone performs to the utmost of his or her ability. \n To effectively coach people, we need to be able to  define the soft skills that are \nrequired at each level of the progression from entry-level employee to executive. Then \nwe can coach them about how to acquire these skills and move up the organization. The \ntables in this chapter are intended to provide those definitions, to provide some clarity \nin these areas of potential ambiguity. They are based on tables that I have used, adapted, \ntested, and refined over many years in a wide variety of roles. Although I created the \n",
      "content_length": 2788,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n156\ntables for my own employees, the skills listed in the tables are not specific to information \nrisk professionals; they may be equally applicable to employees in other disciplines. \n The  soft skills in the tables generally describe  how people work, which can be almost \nas important to the organization as  what they do. How people behave and communicate \naffects not only their own ability to achieve goals but also the performance of those \naround them. An individual contributor who interacts poorly with others may impair \nthe performance of his or her team, and cause interpersonal problems that the team’s \nmanager has to spend time fixing. A senior manager who lacks these soft skills can have \nan even broader impact, hindering the performance of the organization. \n I have published older versions of these tables to my employees, in the belief that \nfeedback should be multi-directional and that leaders as well as employees should be \nmeasured using the same publicly available criteria. I have also shared these tables with \nindustry peers. I am providing them in this book in the hope that they will be beneficial \nto others, and that they will generate comments and feedback that I can use to improve \nfuture iterations of this living document. \n How to Use the  Tables \n Each of the 11 tables in this chapter focuses on a specific area of soft skills, such as \ninitiative, commitment, professionalism, or communication. Each table follows the same \nformat, with five columns representing the skills required at progressively higher levels \nof the organization, from junior employees to emerging executives. The leftmost two \ncolumns represent individual contributors: entry-level employees and more seasoned \nintermediate professionals. The rightmost three columns represent increasingly senior \nmanagement positions: a line manager responsible for a team; a senior manager who \nmay be responsible for multiple teams, each headed by a line manager; and a leader who \nis responsible for an entire information risk organization and should be able to work \ndirectly with the company’s board and top executives. \n As one might expect when discussing soft skills, this is not an exact science. \nThe columns show a progression, but they do not represent a precise scale, and there is \noverlap in some areas. An implicit assumption throughout the tables is that someone \nin a more senior role has already acquired the skills needed in less-senior positions \n(i.e. in the columns farther to the left). The skills required at more junior levels tend to \nbe more narrowly defined and constrained; those required at more senior levels tend to \nbe broader in scope, with more far-reaching impact. For these reasons, the tables may \nbe easiest to absorb by reading down the columns (to see all the skills for each role) \nrather than across the rows. \n Over the years, I have used these tables in various ways. I have used them to help \nemployees understand where they need to enhance their skills and abilities if they want \nto move up to more senior positions. I’ve also used them to help employees self-assess. \nHere are some examples of ways to use the tables in everyday  work situations :\n• \n An employee believes he or she should be promoted to a more \nsenior position. You ask them to assess their own skills in each \narea. You also do your own assessment of their skills. Then the \ntwo of you discuss any differences between those assessments, \nand pinpoint areas that the employee should work on in order to \nacquire the skills needed for a higher-level position. \n",
      "content_length": 3615,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n157\n• \n You provide an entry-level employee, enthusiastic but fresh out \nof college, with a roadmap of the skills they’ll need to acquire if \nthey want to progress to VP level in the future. This gives them a \npractical tool that they can use to guide their personal and career \ndevelopment.  \n• \n You use the tables to identify your own Achilles’ heel, the weak \nspot that hinders your progression to an executive level. You \nnotice that even though your skills mostly match those in the \nEmerging Executive column, the skills in a few areas correspond \nto those that you’d expect in a more junior manager. Those are \nskills that you need to improve. \n• \n During a coaching session with an employee, you count roughly \nhow many of their skills are already at the next most senior level, \nthe next column in the table. If 80% of their skills match, they may \nbe ready to move up. If there’s only a 20% match, they need to \nwork on bringing the rest of their skills up to scratch. \n The tables cover the following areas: independence and initiative, efficiency and \neffectiveness, commitment, professionalism, discipline, teamwork, problem-solving, \ncommunication skills, and goal-setting. \n Independence and Initiative \n This category, as its name suggests, is all about someone’s ability to act independently and \ntake the initiative. As you’d expect, the expectations increase dramatically as one progresses \nup the organization. An entry-level employee may require very specific direction for each \nnew task. A more experienced employee (Intermediate) should be able to define action \nplans and complete small projects with minimal supervision. A line manager should take \nresponsibility for leading his or her team. An emerging executive can deal with tough issues \nat executive level, and take responsibility for risky independent decisions that he or she \nbelieves are in the best interest of the organization. See Table  11-1 .\n Table 11-1.  Independence and Initiative \n Entry-level \n Intermediate \n Line Manager \n Senior Manager  Leader/Emerging \nExecutive \n Takes direction \nand turns it \ninto results; \nassumes \nownership of \ndeliverables \n Acts \nindependently \nwith a specific \ncharter \n Embraces role \nas manager \nto lead his/\nher team; sets \ndirection in \nsupport of \nhigher level \ngoals \n Seeks, \nidentifies, \nand solves \nproblems \nwhile taking \nresponsibility \nfor the \noutcome \n Makes risky \nindependent \ndecisions \nand takes \nresponsibility \nfor the outcome \n(continued)\n",
      "content_length": 2523,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n158\n Entry-level \n Intermediate \n Line Manager \n Senior Manager  Leader/Emerging \nExecutive \n Handles \nmultiple \nsimultaneous \ntasks with \nsome \nsupervision \n Responds \ncreatively to \ncustomer needs \n Effectively \nsummarizes \nand reports \nteam’s activity \n Takes \nunpopular \npositions and \nmakes them \nhappen \n Deals with \ntough issues, \nwith no “air \ncover,” at an \nexecutive staff \nlevel \n Identifies \nroadblocks \nand resolves or \nescalates \n Shapes problem \nstatements and \ndefines action \nplan to complete \nassignments \n Holds self \naccountable for \nwork he or she \ndoesn’t directly \ncontrol \n Can foresee and \ntake action on \nproblems that \ndo not yet exist \n Works with \nmanager to \nestablish \nworkload \npriorities, \nclarify \nexpectations, \nand get \nfeedback \n Requires \nonly minimal \ndirection for \nsmall projects \n Assumes \nresponsibility \nfor work \nthat requires \nattention, even \nif it is outside \ndirect scope of \nhis/her role \n Identifies \nvalue-added \nactivities and \nsometimes \ninitiates \nactions \n Seeks buy-in \nfrom manager on \nworkload timing \nand prioritization \n Drives risk \nand security \ncharter among \nother managers \nacross the \norganization \n \n Efficiency and Effectiveness \n Efficiency and effectiveness are both important, related skills. An efficient employee \nworks quickly and uses fewer resources. An effective employee is highly productive. A \ncompany that combines effectiveness and efficiency achieves better results faster, using \nfewer resources. Table  11-2 shows the progression from an entry-level employee’s ability \nto follow efficient processes to a manager’s ability to manage the resources of a group or \nan entire organization.\nTable 11-1. (continued) \n",
      "content_length": 1731,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n159\n Table 11-2.  Efficiency and Effectiveness \n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Works at \nconsistent and \npredictable \npace \n Schedules \nwork and \ncommunicates \ntimelines for \noutput \n Delegates \nappropriately; \ngets results by \nworking through \nothers and with \nothers \n Manages \nstrategic \nplanning and \norganizational \nscheduling, and \nmakes good \ntradeoffs for the \norganization \n Recognizes \nthat what you \nsay “no” to is as \nvital for driving \norganizational \nefficiency as \nwhat you say \n“yes” to \n Demonstrates \neffective \nwork habits \nenabling \ntimely \ncompletion of \ntasks \n Demonstrates \nability to \nmanage to \nmultiple \nwork items \nwith inter-\ndependencies \n Plans, schedules, \nand balances \nresources \namong projects \nto avoid crises \nand minimize \nfire fighting \n Manages \nadministrative \nresources to \nincrease personal \nefficiency \n Learns from \nmistakes \nand applies \nlearnings to \nsubsequent \ntasks \n Remains \ncalm and in \ncontrol of work \ndemands while \nmaintaining \nwork/life \n balance \n Devotes time \nto improving \ngroup’s \nefficiency \n Dispositions \nitems and issues \nquickly \n Works with \nmanager to \nprioritize \nworkload \n Understands \npriorities, \nplans \naccordingly, \nand makes \nreal-time \nadjustments \n Uses project \nmanagement \ntools and \nstakeholder \ninput to \nmaximize output \nand leverage \nresources \n Communicates, \nand \ndemonstrates \nthrough his/\nher own actions, \nthat people are \nrewarded for \nresults, not hours \nworked \n Begins to \nquestion \ntime spent \non routine \ntasks with low \nadded value \n Networks \nwith others \nto identify \nshortcuts and \n efficiencies \n Actively \nprioritizes by \nweeding and \nfeeding the \nproject list \nto maximize \norganizational \neffectiveness \n",
      "content_length": 1803,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n160\n Commitment \n Commitment reflects someone’s loyalty to the organization and their willingness \nto devote time and energy to the cause. In an entry-level employee, commitment is \ndemonstrated by personal work ethic and willingness to take on more work. As people \nmove up the organization, they demonstrate commitment by taking ownership of bigger \nissues and focusing on driving the best outcome for the organization. See Table  11-3 .\n Table 11-3.  Commitment \n Entry-level \n Intermediate \n Line Manager \n Senior Manager  Leader/Emerging \nExecutive \n Demonstrates \nstrong personal \nwork ethic \n Aligns \nindividual \ngoals with \norganizational \ngoals \n Drives issues for \nthe benefit of \nmultiple groups \nacross the \norganization \n Holds self \naccountable \nfor company’s \nperformance \n Becomes a \nrole model, \ndemonstrating \nstrong sense of \n“company first” \nwith the right \ncorporate social \nresponsibility \n Readily takes on \nmore workload \nwithin job \nscope \n Takes \nownership of \nproblems \n Recognizes \nwhat is \nbest for the \norganization \nversus what \nmight be \nbest for the \ndepartment \n Demonstrates \na high level \nof dedication \nand personal \ncommitment to \nthe success of \nall employees \n Tolerates the \nindirect control \nand influence \nthat result \nfrom matrix \nmanagement \n Answers \nthe specific \nquestions asked \n(doesn’t drift) \n Provides \ncomplete \nanswers to \nquestions; \nanticipates \ndoubtful areas \nand works \nto eliminate \nconcerns \n Demonstrates \ncommitment \nto work/\nlife balance: \ncreating a good \nhome life as \nwell as a good \nwork life \n Demonstrates \nthat growth \nnever stops and \nthat we all need \nto continually \nlearn in order to \nimprove \n Makes specific \nrequests for \nnecessary \ninformation; \nasks only for \nwhat is needed \n Knows when to \nquit on a losing \ndecision but \nwilling to risk \nself to do the \nright  thing \n Subordinates \nego to the needs \nof others and of \nthe company \n",
      "content_length": 1950,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n161\n Professionalism \n Professionalism is the extent to which someone demonstrates the attitudes, skills, and \nmethods required to execute their professional role. For an entry-level employee, this \nincludes adhering to established company policies. For senior managers, it involves \ndemonstrating broader and deeper adoption of the company’s values and principles. See \nTable  11-4 .\n Table 11-4.  Professionalism \n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Demonstrates \npride in his/her \ncraft \n Sets high \npersonal \nstandards \n Modifies \nbehavior \nto embrace \ncorporate \nvalues \n Demonstrates \nunquestioned \nconfidentiality \nand adherence \nto the \norganization’s \ncode of conduct, \nvalues, and \nprinciples \n Becomes a \nrole model \nexemplifying \ncorporate \nvalues, growth, \nconsistency, \nintegrity, \ncomposure, \nrespect for \nothers, and \naccountability \n Has a courteous \nand businesslike \nmanner, \ndemonstrating \nunderstanding \nof basic values, \nrole, and \nappropriate \nbehavior \n Holds self \naccountable \nfor his or her \nactions \n Matches \nactions with \nwords \n Demonstrates \nstrong integrity \nand motivation \nwith the most \nhonorable \nintentions \n Respects \nconfidentiality \nof information, \nwith strict \nadherence to \nconfidentiality \npolicies \n Maintains \ncomposure \nand is not \ndefensive \n Aggressively \nseeks feedback \nand coaching to \ngrow into a role \nmodel \n Discipline  \n Discipline is the ability to remain focused and execute consistently despite the many \ndistractions of everyday working life. As employees rise to higher-level positions, the \ndistractions and demands increase, requiring greater focus and discipline. See Table  11-5 .\n",
      "content_length": 1737,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n162\n Table 11-5.  Discipline \n Entry-level \n Intermediate \n Line Manager \n Senior Manager  Leader/Emerging \nExecutive \n Approaches \nwork in an \norderly fashion \n Consistently \nmaintains high \nstandards of \naccuracy and \nthoroughness \n Stays on point, \neven with heavy \ndistraction \n Demonstrates \nthe stamina \nand fortitude \nto remain \nfocused and \nnot succumb \nto premature \nconclusions \n Prevents the \norganization \nfrom getting \ndistracted \n Consistently \nmeets routine \ndeadlines and \nexecutes well \n Consistently \ndocuments \nintentions and \nresults \n Can discern \nurgency from \nimportance, \nand prioritize \naccordingly \n Understands \nthe value of \n“silver bullets” \nand uses them \nwisely \n Overcomes \nbasic snags \nand remains \nfocused to \nstay on course \nand deliver \nexpected \noutputs \n Does not \ninitiate or \nperpetuate \nwasteful \ncommunication \n Doesn’t waste \nenergy on \nrhetoric or \nreactions that \nlead to no \nmeaningful \nconclusions \n Demonstrates \nprogression \nto greater \ndiscipline over \ntime \n \n Teamwork \n Individuals must be able to recognize the need to work with others as a team, share \nexpertise, and take on suitable team roles. Managers need to create, inspire, and lead \nteams, utilizing each member’s talents in the best way. See Table  11-6 .\n",
      "content_length": 1304,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n163\n Table 11-6.  Teamwork \n Entry-level \n Intermediate \n Line Manager \n Senior Manager  Leader/Emerging \nExecutive \n Comprehends \nthe importance \nof teamwork \n Willingly shares \nknowledge \nand leverages \nexpertise with \nothers in team \n Recognizes \nand assembles \nappropriate \nteam players; \nencourages \ndiversity and \nutilizes each \nmember’s \nunique talents \n Sponsors and \nleads teams \nacross broad \nentities \n Commissions \nteams to solve \nbroad, long-term \nproblems \n Requires some \ncoaching on \nappropriate \nlevel of team \ninvolvement \n Independently \ndetermines \nand executes \nappropriate \nteam role \nand level of \ninvolvement \n Provides \ntraining and \ncoaching to his/\nher team \n Nurtures \nmultiple teams \nwithin an \norganization \n Becomes a key \nplayer within the \nexecutive team \n Actively engages \nteam members \nand others to \ngenerate win-\nwin  solution \n Inspires teams \nto achieve an \nextraordinary \nlevel of \nperformance \n Recognizes \nwhen a team \nneeds course \ncorrection \n Willing to \nmake personal \nsacrifices for the \nsake of the team \n Problem-Solving \n Problem-solving is an important skill for any information risk management professional. \nIndividual contributors need to be able to analyze and solve problems. Managers need to \nhelp their teams solve problems and focus on broader issues including those that involve \nother organizations. See Table  11-7 .\n",
      "content_length": 1414,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n164\n Table 11-7.  Problem-solving \n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Solves \nproblems with \ncoaching \n Takes \nownership \nof problem \nresolution \n Coaches \nteams to solve \nproblems \n Resolves \ncomplex \nproblems across \nthe organization \n Resolves strategic \nproblems, \nparticularly those \ninvolving external \nparties \n Understands \ncause and \neffect \n Drives analysis \nof cost, benefit, \nrisk, and \nprobability of \nsuccess \n Identifies \nand resolves \nproblems \nnot obvious \nto others, \nincluding \nthose beyond \nhis/her \nprevious \nexperience \n Champions \nenduring \nimprovements \nthrough \nstructured \napproaches such \nas task forces \n Identifies \nproactive and \npredictive \nprocesses to \nidentify the \nconsequences \nand solve the \nproblems of \nbroad business \ninitiatives \n Uses available \nresources \nand solid \nmethodology to \nsolve problems \nwithin  charter  \n Uses \nconsultative \nand consensus \nprocesses with \nease \n Acts as role \nmodel for \ncommitment \nto previously \nagreed process \nimprovements \ndesigned to \nsystematically \nsolve problems \n Communication \n Good  communication helps organizations thrive. It is essential in almost any role, from \nentry-level team members who must communicate with their colleagues and managers \nto executives who must communicate messages to the entire organization. Because \ncommunications skills are so important, I’ve divided them into three areas, each with its \nown table: listening, style, and clarity. \n Listening \n Communication starts with  listening.  For junior employees, the ability to listen helps \ncreate a clear understanding of what’s required. More senior employees actively solicit \nmultiple viewpoints, listen for the meaning behind the words, and intercept emotional \noutpourings that can overwhelm a situation. See Table  11-8 .\n",
      "content_length": 1881,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n165\n Table 11-8.  Communication Skills: Listening \n Entry-level \n Intermediate \n Line Manager \n Senior Manager  Leader/Emerging \nExecutive \n Confirms \nunderstanding \n Listens to \nthe broader \nmeaning of \nwhat is being \nsaid, and seeks \nopportunities to \nadd value \n Hears \nfrustrations and \nseeks advice \nabout how to \nrespond \n Can hear \nbeyond \nemotion and \nrespond with \nmeaningful \ncommitments \nand actions \n Finds the \npractical \nsolution amid \nthe noise from \nteam members \nand executives \n Makes listening \nan overt activity \n Listens \nto others’ \nideas, and \nincorporates \nthem into \nthe work; \ndemonstrates \nrespect for \nothers by \nensuring their \nentire message \nis heard \n Seeks others’ \nperspectives \nand listens to \nall viewpoints \nand ideas; \nencourages \nmutual \nunderstanding \n Steps back \nduring debates \nand identifies \nthe key issues \n Can listen to \nstrong-willed \nor irrational \nrequests \nand provide \nappropriate \n direction  \n Listens and \nresponds to \ncustomers and \nstakeholders \n Adds \ninformation \nor perceptions \nto expand the \nconcept or the \nopportunity \n Reinforces \nunderstanding \nthrough active \nlistening; builds \nconfidence \nin others that \ntheir message \nis being heard \n Intercepts \nescalating \nemotion before \nit overwhelms \na situation \ninvolving other \nemployees or \ncustomers \n Before ending \nconversation, \nsummarizes \nconversation \nand achieves \nclosure and \nagreement \n Comes to \nmeetings \nprepared to \nreview the \ndata and \ncommunicate \ninformation in a \nlogical fashion \n Smoothly cross-\nreferences prior \nconversations \nto ensure truth \nand consistency \n Knows when it \nis better to listen \nthan to talk \n \n",
      "content_length": 1687,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n166\n Style \n How you communicate can be as important as  what you communicate. Each person’s \ncommunication style should develop to match their role as they progress through the \norganization. See Table  11-9 .\n Table 11-9.  Communication Skills: Style \n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Communicates \nwell with \nothers without \ncreating \nconfusion or \nunnecessary \nconflict \n Delivery of \nanalysis is \ncomprehensive, \ninstructive, \nand easily \nunderstood \n Recognizes the \nrequirements \nof each \nsituation and \nadapts style \naccordingly \n Demonstrates \npatience, \npersistence, \nand polish in \n communications \n Develops own \nmotivating style \n Responds \nwillingly and \ncapably to \ndirect verbal \nor written \nquestions \n Detects when \nsomeone is \ntrying to direct \nthem in a \nconversation \nand can follow \nas opposed to \nveering off track \n Remains \ncomposed \nunder pointed \nfire \n Maintains a \nprofessional \ndemeanor under \npressure; can \ndeflect “fire” \n Has perfect \ntiming; times \ncommunication \nfor maximum \nimpact \n Interactions \nwith others \nare viewed \npositively; \nother people \ndo not avoid \nworking with \nthis person \n Recognizes and \nis not deterred \nby different \ncommunication \nstyles \n Seeks and \nresponds \neffectively \nto feedback \non own \nmanagement \nbehavior \n Credibly \nresponds to \nquestions \nwhen he or she \ndoesn’t know \nthe answer; can \nbluff but remain \ndirectionally \ncorrect or say “I \ndon’t know, but \nI’ll find out” \n Can make and \ncommunicate \ndecisions on \nthe fly with \nhigh precision \nand without \ndisrupting other \nactivities \n Uses post-\nmortems \neffectively \n Says the right \nthing at the right \ntime \n Creates and \ndelivers “state \nof the union \naddresses” \nand “one voice \nresponses” \nfor medium-\nsized and large \norganizations \n(continued)\n",
      "content_length": 1869,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n167\n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n \n Knows when \nto tell (give \ndirection) \nversus lead, \nand does both \nthings well \n Clarity \n Clear communication helps ensure that information and ideas are accurately shared \nthroughout the organization. Experienced staff should be able to summarize data and \ncreate clarity from a confusing mass of information. Senior managers create consistent \nand clear messages for diverse audiences. See Table  11-10 .\n Table 11-10.  Communication Skills: Clarity \n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Keeps \nmessages clear \nand concise \n Focuses on \nand highlights \nkey points \n Tells the story, \nnot the facts; \ndelivers the core \nmeaning and the \nanswer (what \nactions to take) \nwhen appropriate \n Takes multiple \nmessages from \nvarious sources \nand reconstitutes \nor links them \ninto a larger, \nmore meaningful \nmessage \n Sends clear \nand consistent \nmessages to a \nbroad audience, \nincluding \nexternal parties \n Presents facts \naccurately, \nusing relevant \ndata \n Remains clear \nabout the goal \nand does not \nmeander—\nstays on point \n Draws summary \nconclusions from \nlarge amounts of \ninformation \n Creates \nconsistent and \nclear messages \ndespite complex \nscope of  material \n Helps people \nfrom different \nbackgrounds \nquickly grasp \ncomplex \nsubjects at a \nhigh level \n Independently \ndetermines \nareas that need \nclarity, and \nseeks and adds \nappropriate \ndetails \n Demonstrates \nawareness \nof target \naudience, \nand tailors \nmessage \naccordingly \n Brings clarity \nto complex \nsituations; \nasks the right \nquestions to lead \nthe conversation \nto results, and \navoids stating \nopinion up front \n Lean \ncommunication: \nuses the \nminimum \nnumber of words \nto express a point \n Brings clarity \nto issues \nacross multiple \norganizations \nwho may have \nopposing \ninterests \nTable 11-9. (continued) \n(continued)\n",
      "content_length": 1990,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n168\n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Keeps work \nneat and well \norganized \n Is aware when \nhe or she has \nconfused senior \nmanagement \n Does not confuse \nexecutive \nmanagement \n Goal-Setting \n All experienced staff should be able  to  identify and set goals, from line managers setting \ngoals for their team to leaders defining the organization’s mission. See Table  11-11 .\n Table 11-11.  Goal-Setting \n Entry-level \n Intermediate \n Line Manager \n Senior Manager \n Leader/Emerging \nExecutive \n Drafts \nindividual \ngoals and \nreviews with \nmanager for \napproval \n Identifies \nand declares \nopportunities \n Set goals for team; \nensures goals \nare clear and \nstated in terms of \nmeasurable results \n Sets strategic as \nwell as tactical \ngoals \n Creates \nmissions \n Presents \ncompelling \ndata to support \nrecommended \ngoals \n Aligns goals and \nexpectations \nwith upper \nmanagement \n Demonstrates \nability to set goals \nwhen starting \nwith a blank sheet \n Challenges \nself, staff, and \npeers to take \non increasingly \nhigher leverage \nobjectives \n Anticipates \nneeds and \nrequirements \n Provides a degree \nof focus on \nstrategic issues; \ndemonstrates \nvision in areas of \nexpertise \n Can drive an \norganization \nto articulate \ncommitments, \nmaintain focus, \nadjust priorities, \nand raise the  bar \n Can drive \nconsensus on \nvision \n Fosters innovation \nand creative \nthinking; \nencourages \ndiscussion and \nfeedback in \nsetting goals \n Challenges \nexisting \nparadigms and \nexplores new \npossibilities \n Helps others \nmake the \nconnection \nbetween the \nvision and the \ndeliverables \nnecessary to \nachieve the \nhigher goals \nTable 11-10. (continued) \n",
      "content_length": 1736,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "CHAPTER 11 ■ PERFORMANCE COACHING\n169\n Conclusion \n I believe that performance coaching focused on soft skills can help everyone in \nthe organization achieve their full potential, and thus contribute to the creation of a \nhigh-performance organization. I’d like to conclude by examining what makes a manager \nan effective performance coach. A good performance  coach \n• \n Develops and mentors managers and other employees, managing \npeople to higher expectations and greater results. \n• \n Stretches others and themselves to achieve beyond the norm, and \nrejects mediocrity. \n• \n Creates more key players than he or she consumes, becoming a \nnet developer of people for the organization. \n• \n Holds people accountable for results and coaches them to \nachieve those results. \n• \n Distinguishes motion from progress, and separates the means \nfrom the end. \n• \n Responds positively to feedback about his or her own behavior as \na manager or individual. \n• \n Is sought out to provide performance coaching to senior players \nwho report to other managers. \n• \n Handles tough conversations with employees about their \nbehavior or performance crisply, without creating a litigation risk. \n• \n Saves senior players from self-destructing or falling short of their \npotential. \n• \n Demonstrates empathy and can save employees who are \nstruggling due to work-related or personal reasons and might \notherwise leave the organization. \n",
      "content_length": 1420,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "171\n© Malcolm W. Harkins 2016 \nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8\n APPENDIX A \n References \n Accenture. 2012.  Accenture Technology Vision 2012 .  http://www.accenture.com/us-en/\ntechnology/technology-labs/Pages/insight-accenture-technology-vision-2012.aspx . \n Ahamad, Mustaque. 2011.  Georgia Tech Releases Cyber Threats Forecast for 2012 . \nComment in Georgia Tech press release.  http://www.scs.gatech.edu/content/\ngeorgia-tech-releases-cyber-threats-forecast-2012 . \n Alexander, Aileen and Jamey Cummings. 2016. “The Rise of the Chief Information \nSecurity Officer.”  People + Strategy 39:1. \n Alperovitch, Dmitri. 2012. Comment in  Georgia Tech Emerging Cyber Threats \nReport 2012.  http://www.iisp.gatech.edu/sites/default/files/documents/2016_\ngeorgiatech_cyberthreatsreport_onlinescroll.pdf . \n Amabile, Teresa M., and Steven J. Kramer. 2011. “The Power of Small Wins.”  Harvard \nBusiness Review 89:5. \n Ashford, Warwick. 2015. “Security researchers disclose flaws in Kaspersky \nand FireEye products.”  ComputerWeekly.com. September 7, 2015.  http://www.\ncomputerweekly.com/news/4500253029/Security-researchers-disclose-flaws-\nin-Kaspersky-and-FireEye-products . \n Bazerman , Max H. and  Ann E. Tenbrunsel . 2011.  Blind Spots: Why We Fail to Do \nWhat’s Right and What to Do about It. Princeton: Princeton University Press. \n Ben-Shalom, Omer, Manish Dave, Toby Kohlenberg, Dennis Morgan, Stacy Purcell, \nAlan Ross, Timothy Verrall, and Tarun Viswanathan. 2011. “Rethinking Information \nSecurity to Improve Business Agility.” Intel Corporation.  http://www.intel.com/Assets/\nPDF/whitepaper/Rethinking_Information_Security_Improve_Business_Agility.pdf . \n Bonus, Angelia. 2010. “Pennsylvania school district settles laptop privacy lawsuit.” \nCNN report, October 12, 2010.  http://www.cnn.com/2010/CRIME/10/12/pennsylvania.\nschool.webcams.settlement/ . \n Bradley, Tony. “Run Cylance Infinity OEM engine on Raspberry Pi.”  TechSpective \narticle posted online October 15, 2015.  https://techspective.net/2015/10/15/\nrun-cylance-infinity-oem-engine-on-raspberry-pi/ . \n Breakwell, Glynis. 2007.  The Psychology of Risk . Cambridge, UK: Cambridge \nUniversity Press. \n Brinkmann, Paul. 2015. “Orlando Health reports data breach for 3,200 patients.” \n Orlando Sentinel. Published online July 2, 2015.  http://www.orlandosentinel.\ncom/business/brinkmann-on-business/os-orlando-health-data-breach-20150702-\npost.html . \n",
      "content_length": 2469,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "APPENDIX A ■ REFERENCES\n172\n Brito, Jerry and Tate Watkins. 2012. “Loving the Cyber Bomb? The Dangers of Threat \nInflation in Cybersecurity Policy.”  Harvard Law School National Security Journal, 3:39–83. \n Buckley, Chris. 2015. “China Passes Antiterrorism Law That Critics Fear May Overreach. \n New York Times. December 27, 2015.  http://www.nytimes.com/2015/12/28/world/asia/\nchina-passes-antiterrorism-law-that-critics-fear-may-overreach.html . \n Buczek, Laurie and Malcolm Harkins. 2009. “Developing an Enterprise Social \nComputing Strategy.” Intel Corporation.  http://www.intel.com/content/dam/doc/white-\npaper/intel-it-developing-enterprise-social-computing-strategy-paper.pdf . \n Business Continuity Institute. 2016. “Cyber attack top business threat for second \nyear running.” Press release published February 8, 2016. \n Carty, Matt, Vincent Pimont, and David W. Schmid. 2012. “Measuring the Value \nof Information Security Investments.” Intel Corporation.  http://www.intel.com/\ncontent/dam/www/public/us/en/documents/best-practices/information-security-\ninvestments-paper.pdf . \n Casey, Timothy. 2007. “Threat Agent Library Helps Identify Information Security \nRisks.” Intel Corporation.  http://www.intel.com/it/pdf/threat-agent-library.pdf . \n Casey, Tim and Brian Willis. 2008. “Wargames: Serious Play that Tests Enterprise \nSecurity Assumptions.” Intel Corporation.  http://www.intel.com/it/pdf/Wargames-\nSerious_Play_that_Tests_Enterprise_Security_Assumptions.pdf . \n Chelel, Kit. 2015. “A London Hedge Fund Lost $1.2 Million in a Friday Afternoon \nPhone Scam.”  Bloomberg Business. Published online July 7, 2015.  http://www.bloomberg.\ncom/news/articles/2015-07-07/friday-afternoon-scam-cost-hedge-fund-1-2-\nmillion-and-cfo-s-job . \n Chirgwin, Richard. “Intel’s SGX security extensions: Secure until you look at the \ndetail.”  The Register. February 1, 2016.  http://www.theregister.co.uk/2016/02/01/\nsgx_secure_until_you_look_at_the_detail/ . \n Christensen, Clayton M. 1997.  The Innovator’s Dilemma: When New Technologies \nCause Great Firms to Fail . Boston, Mass.: Harvard Business School Press. \n Cisco Systems, Inc. 2011a.  Cisco Connected World Technology Report 2011 .  http://\nwww.cisco.com/en/US/netsol/ns1120/index.html . \n Cisco Systems, Inc. 2011b.  Email Attacks: This Time It’s Personal.  http://www.\ncisco.com/en/US/prod/collateral/vpndevc/ps10128/ps10339/ps10354/targeted_\nattacks.pdf . \n Cisco Systems, Inc. 2015.  2015 Corporate Social Responsibility Report.  http://www.\ncisco.com/assets/csr/pdf/CSR_Report_2015.pdf . \n Cisco Systems 2015b.  Internet Of Things Will Deliver $1.9 Trillion Boost To Supply \nChain And Logistics Operations. Cisco Systems, Inc. press release. \n Clark , Sandy,  Stefan Frei ,  Matt Blaze ,  Jonathan Smith . 2010. “Familiarity \nBreeds Contempt: The Honeymoon Effect and the Role of Legacy Code in \nZero-Day Vulnerabilities.” In  Proceedings of the 26th Annual Computer Security \nApplications Conference. New York: Association for Computing Machinery. doi: \n10.1145/1920261.1920299. \n Colgan, William B. 2010 . Allied Strafing in World War II: A Cockpit View of Air to \nGround Battle. Jefferson, NC: McFarland. \n Compeau, Joseph, Nicole Haggerty, Ramasastry Chandrasekhar. 2013.  Intel \nCorp. - Bring Your Own Device. Ivey Business School case study. \n",
      "content_length": 3314,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "APPENDIX A ■ REFERENCES\n173\n Corporate Executive Board Company, The (CEB). 2012. Information Risk Executive \nCouncil. Arlington, VA.  http://www.executiveboard.com/exbd/information-\ntechnology/it-risk/index.page . \n Corporate Executive Board Company, The (CEB). 2015. CEB March 2015 Information \nRisk Peer Perspective Survey. \n Covey, Stephen M. R. with Rebecca R. Merrill. 2008.  The Speed of Trust: The One \nThing That Changes Everything. New York: Free Press. \n CSO Magazine, US Secret Service, Software Engineering Institute CERT Program \nat Carnegie Mellon University, Deloitte. 2011.  2011 CyberSecurity Watch Survey: \nOrganizations Need More Skilled Cyber Professionals To Stay Secure. Press release. \n http://www.sei.cmu.edu/newsitems/cybersecurity_watch_survey_2011.cfm . \n Culp, Scott. 2010.  10 Immutable Laws of Security. Microsoft Corporation. \n http://technet.microsoft.com/library/cc722487.aspx . \n CWE/SANS. 2011.  CWE/SANS TOP 25 Most Dangerous Software Errors.  http://cwe.\nmitre.org/top25/ . \n Department of Justice. 2014. “U.S. Charges Five Chinese Military Hackers for \nCyber Espionage Against U.S. Corporations and a Labor Organization for Commercial \nAdvantage.” Press release. May 9, 2014. \n Department of Telecommunications, Government of India. 2009. Instructions to \nInternet service providers. Letter dated February 23, 2009, No. 820-1/2008-DS Pt. II. \n Edelman. 2015. 2015 Edelman Trust Barometer.  http://www.edelman.com/\ninsights/intellectual-property/2015-edelman-trust-barometer/ . \n Edwards, Cliff, Olga Kharif, and Michael Riley. 2011. “Human Errors Fuel Hacking \nas Test Shows Nothing Stops Idiocy.” Bloomberg News. Posted June 27, 2011.  http://\nwww.bloomberg.com/news/2011-06-27/human-errors-fuel-hacking-as-test-shows-\nnothing-prevents-idiocy.html . \n European Commission. 2011 . ePrivacy Directive: circumstances, procedures and \nformats for personal data breach notifications.  http://ec.europa.eu/information_\nsociety/policy/ecomm/doc/library/public_consult/data_breach/ePrivacy_\ndatabreach_consultation.pdf . \n European Commission. 2012.  Proposal for a regulation of the European Parliament \nand of the Council on the protection of individuals with regard to the processing of personal \ndata and on the free movement of such data (General Data Protection Regulation ). \n http://ec.europa.eu/justice/data-protection/document/review2012/com_\n2012_11_en.pdf . \n European Network and Information Security Agency (ENISA). 2010.  Incentives and \nChallenges for Information Sharing in the Context of Network and Information Security. \n http://www.enisa.europa.eu/activities/Resilience-and-CIIP/public-private-\npartnership/information-sharing-exchange/incentives-and-barriers-to-\ninformation-sharing . \n Evered, Rob and Jerzy Rub. 2010. “Maintaining Information Security while Allowing \nPersonal Hand-held Devices in the Enterprise.” Intel Corporation.  http://www.intel.com/\nAssets/PDF/whitepaper/Maintaining_Info_Security_Allowing_Personal_Hand_Held_\nDevices_Enterprise.pdf . \n Federal Trade Commisssion. 2016. “Dental Practice Software Provider Settles FTC \nCharges It Misled Customers About Encryption of Patient Data.” Press release issued \nJanuary 5, 2016.  https://www.ftc.gov/news-events/press-releases/2016/01/dental-\npractice-software-provider-settles-ftc-charges-it-misled . \n",
      "content_length": 3323,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "APPENDIX A ■ REFERENCES\n174\n Fleming, Virgil and Naoyuki Tomizawa. 2012. “Intel IT: Keeping the Business \nRunning in a Crisis.” Intel Corporation.  http://media12.connectedsocialmedia.com/\nintel/03/7906/Intel_IT_Keeping_Business_Running_in_Crisis.pdf . \n Fong, David, Toby Kohlenberg, and Justin Philips. 2010. “Enterprise Security Benefits \nof Microsoft Windows 7.” Intel Corporation.  http://www.intel.in/content/dam/www/\npublic/us/en/documents/case-studies/intel-it-windows-7-upgrade-security-\nbrief.pdf . \n Food and Drug Administration. 2016. “FDA outlines cybersecurity recommendations \nfor medical device manufacturers.” FDA press release.  http://www.fda.gov/NewsEvents/\nNewsroom/PressAnnouncements/ucm481968.htm . \n Fox-Brewster, Thomas. 2015. “Darkode Shutdown: FireEye Intern Accused Of \nCreating $65,000 Android Malware.”  Forbes . Published online July 15, 2015.  http://www.\nforbes.com/sites/thomasbrewster/2015/07/15/fireeye-intern-dendroid-charges/ . \n Fox-Brewster, Thomas. 2016. “As Ransomware Crisis Explodes, Hollywood Hospital \nCoughs Up $17,000 In Bitcoin.”  Forbes . Published online February 18, 2016. \n http://www.forbes.com/sites/thomasbrewster/2016/02/18/ransomware-hollywood-\npayment-locky-menace/ . \n Frier, Sarah. 2014. “Twitter CFO Noto Has an Oops Moment With Mistaken \nTweet.”  Bloomberg Business . November 24, 2014.  http://www.bloomberg.com/news/\narticles/2014-11-25/twitter-cfo-noto-has-an-oops-moment-with-mistaken-tweet . \n Fulford, Charles. 2015. “Retail Is About to Be Reinvented, Driven by Digital \nTechnologies.”  Advertising Age. August 28, 2015.  http://adage.com/article/\ndigitalnext/retail-reinvented/300129/ . \n Gartner, Inc. 2005. “Gartner Survey Shows Spending for Compliance and Corporate \nGovernance to Account for 10-15 Percent of an Enterprise’s 2006 IT Budget.” Gartner Inc. \nPress release.  http://www.gartner.com/press_releases/asset_141532_11.html . \n Gartner, Inc. 2011a. “Gartner Says Context-Aware Technologies Will Affect $96 Billion \nof Annual Consumer Spending Worldwide by 2015.” Gartner Inc. Press release.  http://\nwww.gartner.com/it/page.jsp?id=1827614 . \n Gartner, Inc. 2011b. “Gartner Identifies the Top 10 Strategic Technologies for 2012.” \nGartner Inc. Press release.  http://www.gartner.com/it/page.jsp?id=1826214 . \n Gartner, Inc. 2015. “Gartner Says 6.4 Billion Connected “Things” Will Be in Use in \n2016, Up 30 Percent From 2015.” Gartner Inc. Press release.  http://www.gartner.com/\nnewsroom/id/3165317 . \n Gartner, Inc. 2015b. “Gartner Says It’s Not Just About Big Data; It’s What You Do \nWith It: Welcome to the Algorithmic Economy.” Gartner Inc. Press release.  http://www.\ngartner.com/newsroom/id/3142917 . \n Gentile, Mary. 2010.  Giving Voice to Values. New Haven: Yale University Press \n Gladwell, Malcolm. 2005.  Blink: The Power of Thinking Without Thinking . New York: \nLittle, Brown & Co. \n Global Industry Analysts. 2015.  Context Aware Computing – A Global Strategic \nBusiness Report. Global Industry Analysts, Inc. research report.  http://www.strategyr.\ncom/MarketResearch/Context_Aware_Computing_CAC_Market_Trends.asp . \n Goodin, Dan. 2015. “‘Unauthorized code’ in Juniper firewalls decrypts \nencrypted VPN traffic.”  Ars Technica . December 17, 2015.  http://arstechnica.com/\nsecurity/2015/12/unauthorized-code-in-juniper-firewalls-decrypts-encrypted-\nvpn-traffic/ . \n",
      "content_length": 3360,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "APPENDIX A ■ REFERENCES\n175\n Goodman, Mark.  Future Crimes: Everything Is Connected, Everyone Is Vulnerable and \nWhat We Can Do About It. New York: Doubleday. \n Greenberg, Andy. 2015. “Hackers Remotely Kill a Jeep on the Highway—With Me in \nIt.”  Wired . Posted July 21, 2015.  http://www.wired.com/2015/07/hackers-remotely-\nkill-jeep-highway/ . \n Greenberg, Andy. 2015b. “After Jeep Hack, Chrysler Recalls 1.4M Vehicles for Bug \nFix.”  Wired . Posted July 24, 2015.  http://www.wired.com/2015/07/jeep-hack-chrysler-\nrecalls-1-4m-vehicles-bug-fix/ . \n Gutierrez, Esteban, Toby Kohlenberg, Sridhar Mahankali, and Bill Sunderland. 2012. \n“Virtualizing High-security Servers in a Private Cloud.” Intel Corporation.  http://www.\nintel.de/content/dam/www/public/us/en/documents/best-practices/virtualizing-\nhigh-security-servers.pdf . \n Hamblen, Matt. 2016. “At CES, Feds prod companies to expand privacy efforts.” \nComputerworld January 6, 2016.  http://www.computerworld.com/article/3019832/\ndata-privacy/at-ces-feds-prod-companies-to-expand-privacy-efforts.html . \n Information Risk Executive Council. 2011.  Security Controls Maturity Benchmark \nSummary . Information published in  2011-2012 Intel IT Performance Report . Intel \nCorporation.  http://www.intel.com/content/dam/www/public/us/en/documents/\nbest-practices/intel-it-annual-performance-report-2011-12.pdf . \n Intel Corporation. 2010. Form 10-Q for the quarterly period ended March 27, 2010; \nFiled May 3, 2010.  http://www.intc.com/secfiling.cfm?filingID=950123-10-42822 . \n Intel Corporation. 2011.  Worldwide Device Estimates Year 2020—Intel One Smart \nNetwork Work. \n Intel Corporation. 2012a. “Thinking Differently About IT Value: 2011-2012 Intel \nIT Performance Report.”  http://www.intel.com/content/dam/www/public/us/en/\ndocuments/best-practices/intel-it-annual-performance-report-2011-12.pdf . \n Jackson Higgins, Kelly. 2010. “‘Operation Aurora’ Changing the Role of the CISO.” \n Dark Reading. March 16, 2010.  http://www.darkreading.com/attacks-breaches/\noperation-aurora-changing-the-role-of-the-ciso/d/d-id/1133225 . \n Joffe-Walt, Chana and Alix Spiegel. 2012. “Psychology Of Fraud: Why Good People \nDo Bad Things.” National Public Radio broadcast. Transcript accessed online May 28, \n2012.  http://www.npr.org/2012/05/01/151764534/psychology-of-fraud-why-good-\npeople-do-bad-things . \n Johnson, Steven. 2010.  Where Good Ideas Come From: The Natural History of \nInnovation . New York: Riverhead Books, a subsidiary of Penguin Books (USA). \n Johnson, Steven. 2010. Talk at TEDGlobal 2010.  http://www.ted.com/talks/\nsteven_johnson_where_good_ideas_come_from.html . \n Keteyian, Armen. 2010. “Digital Photocopiers Loaded With Secrets.” CBS News \narticle posted online April 20, 2010.  http://www.cbsnews.com/2100-18563_162-\n6412439.html . \n Kouzes, James and Barry Z. Posner. 2012.  The Leadership Challenge. San Francisco: \nJossey-Bass, an imprint of John Wiley & Sons. \n KPMG International. 2015.  The KPMG Survey of Corporate Responsibility Reporting. \nNovember 2015 \n Kupperwasser, Yosef. 2007.  Lessons from Israel’s Intelligence reforms. The Brookings \nInstitution.  http://www.brookings.edu/~/media/research/files/papers/2007/10/\nintelligence%20kuperwasser/10_intelligence_kuperwasser.pdf . \n",
      "content_length": 3268,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "APPENDIX A ■ REFERENCES\n176\n Lambert, Leslie. 2015. “User behaviors can expose bad actors before it’s \ntoo late.” CSO article posted online July 27, 2015.  http://www.csoonline.com/\narticle/2951814/cyber-attacks-espionage/what-can-we-learn-from-jpmorgans-\ninsider-breaches.html . \n Lea, Ruth. “Corporate Social Responsibility: IoD Member Opinion Survey.” The \nInstitute of Directors, November 2002. \n Leon, Fred. 2011. “Securing Intel’s External Online Presence.” Intel Corporation. \n http://www.intel.com/content/dam/doc/white-paper/intel-it-securing-intels-\nexternal-online-presence-paper.pdf . \n Levin, Carl. 2010. Opening Statement of Senator Carl Levin, Senate Armed Services \nCommittee Hearing on Nominations of Vice Admiral James A. Winnefeld and Lieutenant \nGeneral Keith B. Alexander. \n Lindstrom, Pete. 2008. “Five Immutable Laws of Virtualization Security.” Burton \nGroup blog entry posted online January 08, 2008.  http://srmsblog.burtongroup.\ncom/2008/01/five-immutable.html . \n Loechner, Jack. 2015. “IoT Connected Devices Triples To 38 Billion By 2020.” Center \nfor Media Research brief. August 27, 2015.  http://www.mediapost.com/publications/\narticle/256678/iot-connected-devices-triples-to-38-billion-by-202.html . \n LosHuertos, Gary. 2010. “Herding Firesheep in New York City” Blog entry posted \nonline October 27, 2010.  http://money.cnn.com/2010/12/14/technology/firesheep_\nstarbucks/ . \n MarketsandMarkets. 2013.  Context Aware Computing Market - Global Advancements, \nEmerging Applications, Worldwide Forecasts and Analysis (2013 - 2018). MarketsandMarkets \nresearch report.  http://www.marketsandmarkets.com/PressReleases/context-aware-\ncomputing.asp . \n Massachusetts Institute of Technology Sloan School Center for Information Systems \nResearch. 2012. IT Governance.  http://cisr.mit.edu/research/research-overview/\nclassic-topics/it-governance/ . \n Maynard, Micheline. 2014. “‘The GM Nod’ And Other Cultural Flaws Exposed By \nThe Ignition Defect Report.”  Forbes, June 5, 2014.  http://www.forbes.com/sites/\nmichelinemaynard/2014/06/05/ignition-switch-report-spares-ceo-barra-but-\nexposes-gms-culture/#2715e4857a0b3ac65828408a . \n McAfee, Inc. 2011. Press release.  McAfee Q2 2011 Threats Report Shows Significant \nGrowth for Malware on Mobile Platforms.  http://www.mcafee.com/us/about/\nnews/2011/q3/20110823-01.aspx . \n McComb, Michael. 2002. “Profit to Be Found in Companies That Care,”  South China \nMorning Post . April 14, 2002. \n Miller, Ron and Joe Varga. 2011. “Benefits of Enabling Personal Handheld Devices \nin the Enterprise.”  http://www.intel.com/content/dam/doc/best-practices/inte-\nit-it-leadership-benefits-of-enabling-personal-handheld-devices-in-the-\nenterprise-practices.pdf . \n Millman, Rene. “Updated: 97% of malicious mobile malware targets Android.” \n SC Magazine UK. June 26, 2015.  http://www.scmagazineuk.com/updated-97-of-\nmalicious-mobile-malware-targets-android/article/422783/ . \n Morgan, Steve. 2015. “Cybersecurity job market to suffer severe workforce shortage.” \n CSO. July 28, 2015.  http://www.csoonline.com/article/2953258/it-careers/\ncybersecurity-job-market-figures-2015-to-2019-indicate-severe-workforce-\nshortage.html . \n",
      "content_length": 3190,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "APPENDIX A ■ REFERENCES\n177\n Nakashima, Ellen and Matt Zapotosky. “U.S. charges Iran-linked hackers with \ntargeting banks, N.Y. dam.”  Washington Post. March 24, 2016. \n Nelson, Patrick. 2016. “3D printers wide-open to hacking.”  Network World . March 8, \n2016.  http://www.networkworld.com/article/3041436/security/3d-printers-wide-\nopen-to-hacking.html . \n Nest Labs. 2012. Nest Learning Thermostat web site.  http://www.nest.com/ . \n Paganini, Pierluigi. 2014. “Firmware vulnerability allows man-in-the-middle attack \nusing SD Memory cards.”  The Hacker News. January 01, 2014.  http://thehackernews.\ncom/2014/01/firmware-vulnerability-allows-man-in.html . \n Pagliery, Jose. “Chryslers can be hacked over the Internet.”  CNNMoney. July 22, 2015. \n http://money.cnn.com/2015/07/21/technology/chrysler-hack . \n Pauli, Darren. 2015a. “Thousands of ‘directly hackable’ hospital devices exposed \nonline.”  The Register . September 29 2015.  http://www.theregister.co.uk/2015/09/29/\nthousands_of_directly_hackable_hospital_devices_found_exposed/ . \n Pauli, Darren. 2015b. “‘10-second’ theoretical hack could jog Fitbits into \nmalware-spreading mode.”  The Register . October 21 2015.  http://www.theregister.\nco.uk/2015/10/21/fitbit_hack/ . \n Perch Interactive. 2015. Perch Interactive web site.   www.perchinteractive.com/  . \n Perlroth, Nicole. 2011. “Insurance Against Cyber Attacks Expected to Boom.”  New \nYork Times blog post December 29, 2011.  http://bits.blogs.nytimes.com/2011/12/23/\ninsurance-against-cyber-attacks-expected-to-boom/ . \n PhishLabs. 2016.  2016 Phishing Trends & Intelligence Report: Hacking the Human. \n https://pages.phishlabs.com/2016-Phishing-Trends-and-Intelligence-Report-\nHacking-the-Human_PTI.html . \n Rajab, Moheeb Abu, Lucas Ballard, Panayiotis Marvrommatis, Niels Provos, \nand Xin Zhao. 2010. “The Nocebo Effect on the Web: An Analysis of Fake Anti-Virus \nDistribution.” In  Large-Scale Exploits and Emergent Threats . Usenix.  http://static.\ngoogleusercontent.com/external_content/untrusted_dlcp/research.google.com/\nen/us/pubs/archive/36346.pdf . \n Retail Cyber Intelligence Sharing Center. 2015. “Retailers Launch Comprehensive \nCyber Intelligence Sharing Center.” Press release published May 14, 2015.  https://\nr-cisc.org/2015/05/14/retailers-launch-comprehensive-cyber-intelligence-\nsharing-center/ . \n Rice, David. 2007.  Geekonomics: The Real Cost of Insecure Software. Boston: Addison-\nWesley Professional. \n Rifkin, Jeremy.  The Third Industrial Revolution: How Lateral Power Is Transforming \nEnergy, the Economy, and the World. New York: St Martin’s Griffin. \n Roosevelt, Theodore. 1910. “The Man in the Arena.” Speech at the Sorbonne, Paris, \nFrance. April 23, 1910.  http://www.theodore-roosevelt.com/images/research/\nspeeches/maninthearena.pdf . \n Rose, Charlie. 2015. “Inside Apple, Part 2.”  60 Minutes interview, December 20, \n2015. CBS Interactive.  http://www.cbsnews.com/news/60-minutes-apple-tim-cook-\ncharlie-rose/ . \n Schmidt, Eric and Jared Cohen. 2015. “Inventive artificial intelligence will make all \nof us better.”  Time . December 21, 2015.  http://time.com/4154126/technology-essay-\neric-schmidt-jared-cohen/ . \n",
      "content_length": 3176,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "APPENDIX A ■ REFERENCES\n178\n Schmidt, Michael, Nicole Perlroth and Matthew Goldstein. “F.B.I. Says Little \nDoubt North Korea Hit Sony.”  New York Times. January 7, 2015.  http://www.nytimes.\ncom/2015/01/08/business/chief-says-fbi-has-no-doubt-that-north-korea-\nattacked-sony.html . \n Scott, Mark. 2016. “U.S. and Europe in ‘Safe Harbor’ Data Deal, but Legal Fight \nMay Await.”  New York Times. February 2, 2016.  http://www.nytimes.com/2016/02/03/\ntechnology/us-europe-safe-harbor-data-deal.html?_r=1 . \n Seidman, Dov. 2011. “Measuring HOW We Do Business.”  Forbes article posted \nonline November 27, 2011.  http://www.forbes.com/sites/dovseidman/2011/11/27/\nmeasuring-how-we-do-business/ . \n Shey, Heidi. 2014. “Pet The Unicorns And Think Of Protecting Customer Data As A \nCorporate Social Responsibility.” Forrester Research blog post April 23, 2014.  http://\nblogs.forrester.com/heidi_shey/14-04-23-pet_the_unicorns_and_think_of_\nprotecting_customer_data_as_a_corporate_social_responsibility . \n Silverman, Rachel Emma. 2012. “Facebook and Twitter Postings Cost CFO His Job.” \n Wall Street Journal article posted online May 14, 2012.  http://www.wsj.com/articles/SB\n10001424052702303505504577404542168061590 . \n Sinek, Simon. 2009.  Start with Why: How Great Leaders Inspire Everyone to Take \nAction. New York: Portfolio. \n Slovic, Paul. 2010.  The Feeling of Risk: New Perspectives on Risk Perception. New York: \nRoutledge. \n Solove, Daniel J. 2011.  Nothing to Hide: The False Tradeoff between Privacy and \nSecurity. Yale University Press. \n Sunderland, Bill and Ajay Chandramouly. 2011. “Overcoming Security Challenges \nto Virtualize Internet-facing Applications.” Intel Corporation.  http://www.intel.com/\ncontent/dam/www/public/us/en/documents/white-papers/cloud-security-and-\nsecure-virtualization-paper.pdf . \n Taleb, Nassim Nicholas. 2007.  The Black Swan: The Impact of the Highly Improbable. \nNew York: Random House. \n Thaler, Richard H. Thaler and Cass R. Sunstein. 2008.  Nudge: Improving Decisions \nAbout Health, Wealth, and Happiness. New Haven, CT: Yale University Press. \n Thomson, Iain. 2015. “Lottery IT security boss guilty of hacking lotto computer to \nwin $14.3m.”  The Register. Published online July 22, 2015.  http://www.theregister.\nco.uk/2015/07/22/lotto_infosec_director_guilty/ . \n Tode, Chantal. “Macy’s peps up Black Friday shopping via beacon-triggered mobile \ngame.”  Mobile Commerce Daily. Published online October 30, 2015 \n Verizon. 2015.  2015 Data Breach Investigations Report .  http://www.\nverizonenterprise.com/DBIR/2015/ . \n US Department of Justice (DoJ). 2014. “Justice Department, Federal Trade Commission \nIssue Antitrust Policy Statement on Sharing Cybersecurity Information.” Press release \nissued April 10, 2014.  http://www.justice.gov/opa/pr/justice-department-federal-\ntrade-commission-issue-antitrust-policy-statement-sharing . \n US Environmental Protection Agency (EPA). 2011. “Oil Pollution Act Overview.” \n https://www.epa.gov/laws-regulations/summary-oil-pollution-act . \n US Government Accountability Office (GAO). 2012. “Challenges in Securing the \nModernized Electricity Grid.”  http://www.gao.gov/products/GAO-12-507T . \n",
      "content_length": 3184,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "APPENDIX A ■ REFERENCES\n179\n US Securities and Exchange Commission. 2011. CF Disclosure Guidance: Topic \nNo. 2. Issued October 13, 2011.  http://www.sec.gov/divisions/corpfin/guidance/\ncfguidance-topic2.htm . \n Van Derbeken, Jaxon. “S.F. officials locked out of computer network.”  San Francisco \nChronicle . Published online Tuesday, July 15, 2008.  http://www.sfgate.com/bayarea/\narticle/S-F-officials-locked-out-of-computer-network-3205200.php . \n Venables, Philip. 2008. Speech at RSA Conference 2008. \n Websense. Inc. 2015. “Research: Penalties, Punishment & Prison for Serious Data \nBreaches say e-Crime Congress Respondents.” Press release, March 23 2015.  https://\ncommunity.websense.com/blogs/websense-news-releases/archive/2015/03/23/\nresearch-penalties-punishment-amp-prison-for-serious-data-breaches-say-e-\ncrime-congress-respondents.aspx . \n Weil, Peter and Jeanne W. Ross. 2004.  IT Governance: How Top Performers Manage IT \nDecision Rights for Superior Results . Boston, Mass.: Harvard Business School Press. \n Willis, Brian. 2012. “Sharing Cyber-Threat Information: An Outcomes-based \nApproach.” Intel Corporation.  https://www.sbs.ox.ac.uk/cybersecurity-capacity/\nsystem/files/Intel%20-%20Sharing%20Cyber-Threat%20Information.pdf . \n World Business Council for Sustainable Development. 2007.  Corporate Social \nResponsibility: Meeting changing expectations. \n Worral, Bob. 2015. “Important Announcement about ScreenOS.” Juniper Networks \nsecurity announcement. Posted online December 17, 2015.  http://forums.juniper.\nnet/t5/Security-Incident-Response/Important-Announcement-about-ScreenOS/ba-\np/285554 . \n Zander, Rosamund Stone and Benjamin Zander. 2000.  The Art of Possibility: \nTransforming Professional and Personal Life . Boston, Mass.: Harvard Business School \nPress. \n",
      "content_length": 1794,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "181\n© Malcolm W. Harkins 2016\nM.W. Harkins, Managing Risk and Information Security, \nDOI 10.1007/978-1-4842-1455-8\n \n \n \n  A \n Advanced persistent threats (APTs) , 13 \n Architecture . See also  Security architecture \n balanced controls \n anti-malware technology , 114–115 \n defi nition , 106 \n detective and preventative \ncontrols , 114 \n intrusion prevention systems , 113 \n security analytics , 113 \n employee productivity , 105 \n hardware-enforced security , 105 \n security zones , 109 \n architecture , 111–112 \n critical data and resources , 109 \n defi nition , 106 \n devices and application types , 109 \n PEPs , 110 \n selective zones , 111 \n trusted zones , 111 \n untrusted zones , 110–111 \n user’s device and location , 112 \n traditional enterprise trust model , 105 \n trust calculation \n access type , 106 \n allow access , 108 \n available controls , 108 \n business partners , 109 \n defi nition , 105 \n destination score , 108 \n devices and usage models , 106 \n internal and external resources , 109 \n policy decision point (PDP) , 108 \n source score , 107 \n trust calculation , 108 \n user and data perimeters \n defenses and detective control , 115 \n Internet of Th ings (IoT) , 116 \n protect information , 115 \n security , 116 \n traditional network security , 115 \n user and data perimeters defi nition , 106 \n Arizona Counter Terrorism Intelligence \nCenter (ACTIC) , 59 \n Arizona Cyber Th reat Response Alliance \n(ACTRA) , 59 \n Assessment models , 25 \n \n \n \n  B \n Business benefi ts and risks \n baseline security \n AI-based security and \nautomation , 125–126 \n encryption , 125 \n enhanced recovery , 125 \n hardware acceleration , 125 \n hardware-enforced , 124 \n protected environments , 124 \n building security , 123 \n context-aware security \n business intelligence and data \nprotection , 127 \n cloud security and context \nawareness , 126 \n experiences , 123 \n image recognition technology , 126 \n portable devices , 126 \n sensors and analytical tools , 126 \n contextual information , 123 \n mass-production strategy , 123 \n Index \n",
      "content_length": 2041,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "■ INDEX\n182\n \n \n \n  C \n Chief information security offi  cer (CISO) \n chief trust offi  cer , 139–140 \n foundational skills , 142 \n junk food fear , 144 \n leader , 148 \n business leaders , 149–150 \n information risk-board \nlevel , 151–152 \n operational goal , 148 \n values , 150–151 \n Z-shaped individual , 151 \n managing risk , 146 \n organizations outsource , 153 \n positive approach , 145–146 \n sixth sense , 147 \n speed of trust , 148 \n storyteller , 143–144 \n T-shaped individuals , 141 \n Z-shaped individuals , 141 \n Chief Information Security \nOffi  cer (CISO) , 35 \n Chief Security and Privacy \nOffi  cer (CSPO) , 35–36 \n Coaching performances \n commitment , 160 \n communication , 164 \n clear communication , 167 \n goal-setting , 168 \n listening , 164–165 \n style , 166–167 \n defi nition , 155 \n discipline , 161–162 \n effi  ciency and eff ectiveness , 158–159 \n independence and \ninitiative , 157–158 \n management , 155 \n problem-solving , 163–164 \n professionalism , 161 \n soft skills , 156, 169 \n tables \n soft skills , 156 \n work situations , 156–157 \n teamwork , 162–163 \n Context-aware computing , 119 \n Corporate governance model , 36 \n Corporate social responsibility (CSR) , 130 \n defi nitions , 130 \n maintaining society trust , 134 \n managing information \n act , 137 \n events , 135 \n interpret , 137 \n risk professional and ethical \nstandards , 135 \n security issues , 136 \n sense , 137 \n managing information risk , 135 \n scope of , 130 \n technology \n control and impacts , 132 \n ethical implications , 133 \n evolution , 132 \n march of , 132 \n motivations , 133 \n potential impacts , 132 \n public-sector organizations , 133 \n real-life attack , 133 \n technology and business \nprofessionals , 129 \n treat information risk , 131 \n Cybersecurity legislation , 8 \n \n \n \n  D \n Distributed denial-of-service (DDoS) \nthreats , 21 \n \n \n \n  E \n Emerging security capabilities \n accelerated encryption , 119 \n artifi cial intelligence , 122 \n business benefi ts and risks (see \n Business benefi ts and risks ) \n cloud computing , 122 \n consistent experience across \ndevices , 121 \n context-aware computing and \nsecurity , 119 \n data analytics , 122 \n enterprise information security , 120 \n hardware-enforced protection , 119 \n information security , 120 \n Internet of Th ings (IoT) , 120 \n enabled car , 119 \n Moore’s law , 120 \n nest learning thermostat , 121 \n NFC , 120 \n RFID , 120 \n wireless NFC , 120 \n",
      "content_length": 2421,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "■ INDEX\n183\n malicious purposes , 119 \n organization’s privacy commitment , 119 \n security professionals , 119 \n shopper’s smartphone , 121 \n Enterprise resource planning (ERP) \nsystem , 111 \n External partnerships \n advantage of , 55 \n ACTRA , 59 \n attacks and threads , 58 \n benchmarks \n information , 61 \n operations , 60 \n CISO , 60 \n communities , 57 \n characteristics , 57 \n goals , 59 \n constitutes valuable information , 58 \n corporate citizenship , 63 \n enabling informal exchanges , 60 \n FIRST , 61 \n information sharing , 49 \n legal implications and revealing \nsecurity , 50 \n partnerships , 55 \n private-sector organizations , 58 \n public-relations aspect , 50 \n regional communities , 58 \n regulations and standards , 62 \n security landscape , 63 \n share security information , 50 \n share threat information , 58 \n technology landscape , 51 \n threat landscape , 51 \n tiered pyramid model \n confi dential tier , 53–55 \n information-sharing \nrelationships , 53–54 \n partnerships tier , 54 \n public tier , 53, 55 \n targeted tier , 53–54 \n types of , 52 \n value of , 51 \n vulnerabilities information and \nthreats , 59 \n \n \n \n  F \n Federal Trade Commission (FTC) , 136 \n Forum for Incident Response and Security \nTeams (FIRST) , 61 \n \n \n \n  G \n Global Positioning System (GPS) , 96, 108 \n Governance \n dictatorial approach , 33 \n information risk , 32 \n IT policies , 33 \n life-threatening consequences , 33 \n MIT CISR , 32–33 \n structure \n archetypes , 34, 90 \n corporate information , 36 \n CSPO , 36 \n engagements , 36 \n hybrid governance model , 35 \n monitor (sense) risk , 35 \n operations , 36 \n oversight , 35 \n security and privacy , 35 \n \n \n \n  H \n Health Insurance Portability and \nAccountability Act (HIPAA) , 9 \n Human resources (HR)-related processes \n capabilities , 31 \n information risks , 31 \n internal partnership , 31 \n key areas , 32 \n security team , 32 \n technology transitions , 31 \n \n \n \n  I, J, K, L \n Information security \n business enable ® , 5 \n balancing act , 6 \n core competencies , 5 \n legal and human resources (HR) \ngroups , 6 \n personal smartphones , 6 \n primary variables , 7 \n responsibilities grew , 6 \n transformation , 6 \n trust , 8 \n tuned to target , 6–7 \n wireless network implementation , 6 \n businesses and organization , 3 \n central nervous system , 3 \n fast-moving environment , 4 \n interdependent risks , 4 \n",
      "content_length": 2366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "■ INDEX\n184\n management risk , 16 \n dynamic and fl exible , 16 \n incorporate privacy and \nregulatory compliance , 16 \n network boundary , 16 \n marketers , 4 \n perfect storm , 1–2 \n rapid proliferation (information and \ndevices) , 12 \n regulatory environment , 3 \n regulatory flood (see  Regulatory fl ood ) \n security and privacy , 5 \n smartphones , 3 \n technology , 3 \n threat landscape (see  Th reat \nlandscape ) \n traditional mission and vision , 2 \n Information Sharing and Analysis Centers \n(ISACs) , 60 \n Information Sharing and Analysis \nOrganizations (ISAOs) , 50 \n Insider threats , 78 \n detect , 79 \n deter , 79 \n discipline , 80 \n organization’s reputation , 78 \n security fi rms , 78 \n three-part approach , 79 \n Intel IT Emergency Response Process \n(ITERP) , 47 \n Intel Secure External Presence (ISEP) , 41 \n Internal partnerships \n business group managers , 46 \n corporate risk management , 44 \n corporate security , 45 \n far-reaching web , 37 \n fellow travelers , 38 \n fi nance group \n business groups , 43 \n internal audit , 44 \n SOX , 43 \n human resources \n communications , 42 \n procedures , 42 \n internal investigations , 43 \n security policies , 42 \n information security group \nand teams , 37 \n ITERP , 47 \n legal groups , 38 \n business groups , 40 \n contracts , 39 \n data classifi cation , 39 \n fi nancial compliance , 40 \n intellectual property , 39 \n ISEP-like process , 41 \n litigation , 39 \n privacy , 38 \n privacy , 45 \n response process , 46–47 \n risk review boards , 37 \n standing committees , 37 \n Internet of Th ings (IoT) , 94, 120 \n enabled car , 119 \n Moore’s law , 120 \n nest learning thermostat , 121 \n NFC , 120 \n RFID , 120 \n technologies , 3 \n wireless NFC , 120 \n Irrefutable Laws . See  Information Security, 14 \n \n \n \n  M \n Massachusetts Institute of Technology \nCenter for Information Systems \nResearch (MIT CISR) , 32 \n Misperception . See  Risk misperception \n Moore’s law , 120 \n \n \n \n  N \n National Institute of Standards and \nTechnology (NIST) , 144 \n Near fi eld communications (NFC) , 120 \n Non-Intel managed systems (NIMS) , 20 \n \n \n \n  O \n Organization’s privacy commitment , 119 \n \n \n \n  P, Q \n Perimeters \n balance fi nding , 80 \n changing behavior , 69 \n compliance/commitment , 66, 68 \n hypothetical web sites , 76 \n insider threats , 78 \nInformation security (cont.)\n",
      "content_length": 2328,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "■ INDEX\n185\n detect , 79 \n deter , 79 \n discipline , 80 \n organization’s reputation , 78 \n security fi rms , 78 \n three-part approach , 79 \n interactive training tool , 70 \n personal devices , 74 \n phishing attacks , 66 \n risk examination , 68–69 \n roundabouts and stop signs , 75–77 \n security awareness \n awareness model , 74 \n CEB , 71 \n employee awareness programs , 72 \n encourage users and news , 72 \n four-stage model , 72 \n international association , 73 \n judgment and commitment , 72 \n telecommunications fi rm , 72 \n tracking report , 73 \n training , 71 \n security benefi ts , 74–75 \n shifting perimeter , 65 \n social-engineering techniques , 66 \n technology professionals , 77 \n traffi  c-control method , 75 \n training , 69 \n user interactions , 66 \n Playing war games , 90 \n Policy decision point (PDP) , 108 \n Policy enforcement points (PEPs) , 110 \n Product life cycle model \n clustering areas , 87 \n commodity-source code , 84 \n critical trends , 86 \n disruptive trends , 87 \n emerging trends , 86 \n evolution of threats , 84 \n highest-priority threats , 83 \n security-related activity , 86 \n smartphone security threats , 85 \n sustained drivers , 86 \n threat analysis materials , 86 \n threat report , 87–88 \n Product security incident response \nprocesses (PSIRT) , 45 \n Public-private partnerships (PPPs) , 58 \n \n \n \n  R \n Radio Frequency Identifi cation (RFID) \ntechnology , 120 \n References , 171 \n Regulatory fl ood \n cyber-security legislation , 8 \n e-discovery , 11 \n fi nancial regulations , 10 \n high-tech exports , 8 \n IT capabilities , 8 \n personalization  vs. privacy , 9 \n protecting personal information , 9 \n scope , 11 \n storage and protection , 8 \n Retail environment , 118 \n Risk misperception , 17 \n assessment models , 25 \n communication \n asymmetry information , 27 \n building credibility , 28 \n laptops , 28 \n pirating software , 27 \n risk perceptions changing , 26 \n decision makers , 23 \n employees , 18 \n inevitable bias , 25 \n lure of the shiny bauble , 20 \n mitigate , 24 \n perception , 18 \n economic and psychological \nfactors , 18 \n organization security \nposture , 18 \n security professionals , 18 \n social-media site , 18 \n security professionals , 20 \n alert fatigue , 21 \n DDoS threats , 21 \n history of , 21 \n NIMS , 20 \n scrubbing data , 21 \n security and privacy , 22 \n set and forget error and security \ncontrols , 21 \n target fi xation , 20 \n threat controls , 23 \n untrusted devices , 20 \n",
      "content_length": 2445,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "■ INDEX\n186\n \n \n \n  S \n Sarbanes-Oxley Act (SOX) , 10, 43 \n Security architecture \n 9 box of controls , 101 \n business needs , 103 \n cloud computing , 104 \n control architecture , 100 \n IT consumerization , 102–103 \n novel approaches , 100 \n privacy and regulatory requirements , 105 \n threat landscape , 104 \n threat management , 100 \n Sharing security information . See  External \npartnerships \n Shifting perimeter , 65 \n Smartphones , 3, 96 \n security threats , 85 \n web applications , 97 \n Systems development life cycle (SDLC) , 45 \n \n \n \n  T, U, V, W, X, Y, Z \n Tailored Access Operations (TAO) , 89 \n Technology professionals , 77 \n Th reat landscape \n APTs , 13 \n cybercrime online , 13 \n irrefutable laws , 14 \n spearphishing attacks , 13 \n stealthy malware , 13 \n Th reats and vulnerabilities \n Malware industry , 94 \n often-confl icting information , 82 \n rhetoric , 81 \n smartphones , 96 \n structured methods \n analyzing emerging threats , 82 \n blinkered security \nperspective , 82 \n playing war games , 90 \n product life cycle (see  Product life \ncycle model ) \n risk-sensing analysis and \nstrategy , 82–83 \n security team , 82 \n threat landscape , 83 \n threat landscape \n barriers , 92 \n broad-brush picture , 91 \n edge case insecurity , 92 \n obscurity , 93 \n phishing , 91 \n smartphones , 91 \n social engineering attacks , 92 \n threats and identify risks , 88–89 \n web, attack surface \n embedded devices , 95 \n industrial control systems , 95 \n IoT , 95 \n nontraditional devices , 94 \n Trusted platform module (TPM) , 108 \n",
      "content_length": 1540,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}