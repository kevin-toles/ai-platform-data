{
  "metadata": {
    "title": "Machine Learning in Microservices - Mohamed Abouahmed",
    "author": "Unknown Author",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 270,
    "conversion_date": "2025-12-19T17:34:22.377904",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Machine Learning in Microservices - Mohamed Abouahmed.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 2-10)",
      "start_page": 2,
      "end_page": 10,
      "detection_method": "topic_boundary",
      "content": "Machine Learning in Microservices\n\nCopyright © 2023 Packt Publishing\n\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.\n\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the authors, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to have been caused directly or indirectly by this book.\n\nPackt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.\n\nPublishing Product Manager: Dinesh Chaudhary Content Development Editor: Joseph Sunil Technical Editor: Rahul Limbachiya Copy Editor: Safis Editing Project Coordinator: Farheen Fathima Proofreader: Safis Editing Indexer: Subalakshmi Govindhan Production Designer: Prashant Ghare Marketing Coordinator: Shifa Ansari, Vinishka Kalra\n\nFirst published: February 2023\n\nProduction reference: 1170223\n\nPublished by Packt Publishing Ltd. Livery Place 35 Livery Street Birmingham B3 2PB, UK.\n\nISBN 978-1-80461-774-8\n\nwww.packtpub.com\n\nTo my mother, who endured a lot to shape me into who I am today. To my wife, whose unwavering support has been crucial in my journey. And to my late father, who I deeply miss and who continues to inspire me every step of the way. I love you all, thank you for everything you have done and continue to do for me. This book is a testament to your support and a small token of my appreciation to you all.\n\n– Mohamed Abouahmed\n\nTo my mother and father, who have been an inspiration, for pushing me to become the best version of myself. And to my brother, who has been like a great friend with his continual support and motivation. I wouldn’t be the person I am today without you all in my life. Thank you!\n\n– Omar Ahmed\n\nContributors\n\nAbout the authors\n\nMohamed Abouahmed is a principal architect and consultant providing a unique combination of technical and commercial expertise to resolve complex and business-critical issues through the design and delivery of innovative, technology-driven systems and solutions. He specializes in network automation solutions and smart system development and deployment.\n\nMohamed’s hands-on experience, underpinned by his strong project management and academic background, which includes a PMP certification, master’s of global management, master of business administration (international business), master of science in computer networking, and BSc in Electronics Engineering, has helped him develop and deliver robust solutions for multiple carriers, service providers, enterprises, and Fortune 200 clients.\n\nOmar Ahmed is a skilled computer engineer with experience at various start-ups and corporations working on a variety of projects, from building scalable enterprise systems to deploying powerful machine learning models.\n\nHe has a bachelor’s degree in computer engineering from the Georgia Institute of Technology and is currently finishing up his master’s in computer science with a specialization in machine learning at the Georgia Institute of Technology.\n\nAbout the reviewer\n\nSumedh Datar is a senior machine learning engineer with more than 6 years of work experience in the fields of deep learning, machine learning, and software engineering. He has a proven track record of single-handedly delivering end-to-end engineering solutions to real-world problems. He works at the intersection of engineering, research, and product and has developed deep learning-based products from scratch that have been used by a lot of end customers. Currently, Sumedh works in R&D, where he works on applied deep learning with less data, and has been granted several patents and applied for several more. Sumedh studied biomedical engineering focused on computer vision and then went on to pursue a master’s in computer science focused on AI.\n\nTable of Contents\n\nPreface\n\nPart 1: Overview of Microservices Design and Architecture\n\n1\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nWhy microservices? Pros and cons Advantages of microservices Disadvantages of microservices The benefits outweigh the detriments\n\nLoosely versus tightly coupled monolithic systems Service-driven, EDA, and MSA hybrid model architecture ACID transactions\n\n4 6 9 11\n\n12\n\n15 17\n\nSaga patterns Command Query Responsibility Segregation (CQRS)\n\nDevOps in MSA Why ML?\n\nSummary\n\n2\n\nRefactoring Your Monolith\n\nIdentifying the system’s microservices 27 29 The ABC monolith 30 The ABC-Monolith’s current functions 31 The ABC-Monolith’s database 32 The ABC workflow and current function calls\n\nData decomposition Request decomposition Summary\n\nFunction decomposition\n\n33\n\nxiii\n\n3\n\n17\n\n20\n\n22 25\n\n26\n\n27\n\n35 38 41\n\nviii\n\nTable of Contents\n\n3\n\nSolving Common MSA Enterprise System Challenges\n\nMSA isolation using an ACL Using an API gateway Service catalogs and orchestrators Microservices aggregators\n\n43 46 49 51\n\nGateways versus orchestrators versus aggregators Microservices circuit breaker ABC-MSA enhancements Summary\n\nPart 2: Overview of Machine Learning Algorithms and Applications\n\n4\n\nKey Machine Learning Algorithms and Concepts\n\nThe differences between artificial intelligence, machine learning, and deep learning Common deep learning and machine learning libraries used in Python Building regression models Building multiclass classification\n\n63\n\n67 74 80\n\nText sentiment analysis and topic modeling Pattern analysis and forecasting in machine learning Enhancing models using deep learning Summary\n\n5\n\nMachine Learning System Design\n\nMachine learning system components 97 100 Fit and transform interfaces 100 Transform 105 Fit\n\nTrain and serve interfaces Training Serving\n\nOrchestration Summary\n\n43\n\n54 56 58 60\n\n63\n\n82\n\n84\n\n87 95\n\n97\n\n106 106 108\n\n109 112\n\nTable of Contents\n\n6\n\nStabilizing the Machine Learning System\n\n113\n\nMachine learning parameterization and dataset shifts The causes of dataset shifts\n\n113 117\n\nIdentifying dataset shifts 117 Handling and stabilizing dataset shifts 121 124 Summary\n\n7\n\nHow Machine Learning and Deep Learning Help in MSA Enterprise Systems\n\n125\n\nMachine learning MSA enterprise system use cases Enhancing system supportability and time-to-resolution (TTR) with pattern analysis machine learning\n\n125\n\n129\n\nImplementing system self-healing with deep learning Summary\n\n130 132\n\nPart 3: Practical Guide to Deploying Machine Learning in MSA Systems\n\n8\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems 137\n\nDevOps and organizational structure alignment DevOps The DevOps team structure\n\nDevOps processes in enterprise MSA system operations The Agile methodology of development Automation\n\n138 138 140\n\n141 142 143\n\nApplying DevOps from the start to operations and maintenance Source code version control Configuration management and everything as a code CI/CD Code quality assurance Monitoring Disaster management\n\n145 145\n\n145 146 147 148 149\n\nSummary\n\n150\n\nix\n\nx\n\nTable of Contents\n\n9\n\nBuilding an MSA with Docker Containers\n\n151\n\nWhat are containers anyway, and why use them? Installing Docker Docker Engine installation Docker components\n\nCreating ABC-MSA containers ABC-MSA containers\n\n151 157 157 158\n\n162 163\n\nManaging your system’s containers\n\nABC-MSA microservice inter- communication The Docker network TCP/IP communication between containers/ microservices\n\nSummary\n\n170\n\n172 172\n\n173\n\n175\n\n10\n\nBuilding an Intelligent MSA Enterprise System\n\n177\n\nThe machine learning advantage Building your first AI microservice The anatomy of AI enhancements The self-healing process Building the necessary tools\n\nThe intelligent MSA system in action 187 188 Initializing the ABC-Intelligent-MSA system\n\n177 178 179 181 184\n\nBuilding and using the training data Simulating the ABC-Intelligent-MSA’s operation\n\nAnalyzing AI service operations The PBW in action The PAD in action\n\nSummary\n\n189\n\n191\n\n191 192 197\n\n201\n\n11\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\n203\n\nDeployment strategies Greenfield versus brownfield deployment Flexibility Scalability Technology stack Integration Cost\n\n204\n\n205 206 206 207 207 207\n\nTime-to-market Risks Staff onboarding User adoption\n\nOvercoming deployment challenges 210 211 Identify deployment risks 212 Prioritize risks\n\n208 208 208 209\n\nTable of Contents\n\nDeveloping and implementing a risk mitigation plan The rollback plan\n\n213 217\n\nTest, monitor, and adjust Post-deployment and pre-production review\n\n217 218\n\nSummary\n\n219\n\n12\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\n221\n\nOvercoming system dependencies Reusable ABC-Monolith components and dependencies Mitigating ABC-Intelligent-MSA deployment risks\n\nDeploying the MSA system The anti-corruption layer Integrating the MSA system’s services\n\n222\n\n222\n\n223\n\n225 225 227\n\nTesting and tuning the MSA system 231 233 The post-deployment review 233 Checking the new system’s performance 233 Identifying and fixing system defects 234 Compliance 234 System maintenance and updates 235 User satisfaction\n\nSummary\n\n236\n\nIndex\n\n237\n\nOther Books You May Enjoy\n\n246\n\nxi",
      "page_number": 2
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 11-19)",
      "start_page": 11,
      "end_page": 19,
      "detection_method": "topic_boundary",
      "content": "Preface\n\nMachine Learning (ML) has revolutionized the technology industry and our daily lives in ways that were previously thought impossible. By combining ML algorithms with Microservices Architecture (MSA), organizations are able to create intelligent, robust, flexible, and scalable enterprise systems that can adapt to changing business requirements and improve overall system performance.\n\nThis book is a comprehensive guide that covers different approaches to building intelligent MSA systems and solving common practical challenges faced in system design and operations.\n\nThe first part of the book provides a comprehensive introduction to MSA and its applications. You will learn about common enterprise system architectures, the concepts and value of MSA, and how it differs from traditional enterprise systems. In this part, you will gain an understanding of the design, deployment, and operation of MSA, including the basics of DevOps processes.\n\nThe second part of the book dives into ML and its applications in MSA systems. You will learn about the key ML algorithms and their applications in MSA, including regression models, multiclass classification, text analysis, and Deep Learning (DL). This part provides a comprehensive guide on how to develop the ML model, components, and sub-components, and how to apply them in an MSA system.\n\nThe final part of the book brings together everything covered in the previous parts. It provides a step- by-step guide to designing and developing an intelligent system, with hands-on examples and actual code that can be imported for real-life use cases. You will also learn about the application of DevOps in enterprise MSA systems, including organizational structure alignment, quality assurance testing, and change management.\n\nBy the end of this book, you will have a solid understanding of MSA and its benefits and will be equipped with the skills and knowledge necessary to build your own intelligent MSA system and take the first step toward achieving better business results, operational performance, and business continuity. Whether you are a beginner or an experienced developer, this book is the perfect guide to help you understand and apply MSA and ML in your enterprise systems.\n\nWho this book is for\n\nThis book is ideal for ML solution architects, system and ML developers, and system and solution integrators. These individuals will gain the most from this book as it covers the critical concepts and best practices in ML. The book is written to provide these professionals with the knowledge and skills necessary to implement intelligent MSA solutions.\n\nxiv\n\nPreface\n\nTo fully benefit from this book, you should have a basic understanding of system architecture and operations. Additionally, a working knowledge of the Python programming language is highly desired. This is because the examples and case studies included in the book are primarily implemented in Python. However, the concepts and best practices covered in this book can be applied to other programming languages and technologies as well. The book is designed to provide a solid foundation in ML, while also helping you to deepen your existing knowledge and skills.\n\nWhat this book covers\n\nChapter 1, Importance of MSA and Machine Learning in Enterprise Systems, provides an introduction to MSA and its role in delivering competitive and reliable enterprise systems. The chapter will compare MSA with traditional monolithic enterprise systems and discuss the benefits and challenges of deploying and operating MSA systems. It will also cover the key concepts of MSA, including service-driven and event-driven architecture and the importance of embracing DevOps in building MSA systems.\n\nChapter 2, Refactoring Your Monolith, focuses on the transition process from a monolithic architecture to an MSA. It emphasizes how to refactor the monolithic system to build a flexible and reliable MSA system. This chapter will explore the steps necessary to transition to MSA, including identifying microservices, breaking down business requirements, and decomposing functions and data. The chapter will provide insights into how to modernize an organization’s enterprise systems through MSA adoption.\n\nChapter 3, Solving Common MSA Enterprise System Challenges, discusses the methodologies of addressing the challenges of maintaining a reliable, durable, and smoothly operating MSA system. The chapter covers topics such as using an Anti-Corruption Layer (ACL) for MSA system isolation, API gateways, service catalogs and orchestrators, a microservices aggregator, and a microservices circuit breaker; the differences between gateways, orchestrators, and aggregators; and other MSA system enhancements.\n\nChapter 4, Key Machine Learning Algorithms and Concepts, provides a comprehensive understanding of the fundamental AI, ML, and DL concepts, to equip you with the necessary knowledge to build and deploy AI models in MSA systems. It covers the differences between these areas and provides an overview of common ML packages and libraries used in Python. The chapter then dives into various applications of ML, including building regression models, multiclass classification, text sentiment analysis and topic modeling, pattern analysis and forecasting, and building enhanced models using DL.\n\nChapter 5, Machine Learning System Design, provides a comprehensive understanding of the design considerations and components involved in building an ML pipeline and equips you with the knowledge necessary to build and deploy a robust and efficient ML system. The chapter covers the main concepts of fit and transform interfaces, train and serve interfaces, and orchestration.\n\nChapter 6, Stabilizing the Machine Learning System, arms you with a comprehensive understanding of the phenomenon of dataset shifts and how to address them in your ML systems to ensure stable and accurate results. The chapter discusses optimization methods that can be applied to address dataset shifts while maintaining their functional goals. The chapter covers details on the concepts of ML parameterization, the causes of dataset shifts, the methods for identifying dataset shifts, and the techniques for handling and stabilizing dataset shifts.\n\nChapter 7, How Machine Learning and Deep Learning Help in MSA Enterprise Systems, wraps up all the previous chapters by discussing the different use cases where you can apply ML and DL to your intelligent enterprise MSA system. You will learn some possible use cases, such as pattern analysis using a supervised linear regression model and self-healing using DL.\n\nChapter 8, The Role of DevOps in Building Intelligent MSA Systems, teaches you how to apply the concepts of DevOps in building and running an MSA system. The chapter covers the alignment of DevOps with the organizational structure, the DevOps process in enterprise MSA system operations, and the application of DevOps from the start to operations and maintenance.\n\nChapter 9, Building an MSA with Docker Containers, provides an introduction to containers and their use in building a simple project using Docker, a widely used platform in the field. The chapter covers an overview of containers and their purpose, the installation of Docker, the creation of our sample project’s containers, and inter-communication between microservices in the MSA project. The objective is to provide you with a comprehensive understanding of containers and how they can be utilized in the MSA.\n\nChapter 10, Building an Intelligent MSA System, combines the concepts of MSA and Artificial Intelligence (AI) to build a demo Intelligent-MSA system. The system will use various AI algorithms to enhance the performance and operations of the original MSA demo system created earlier in the book. The Intelligent-MSA will be able to detect potential problems in traffic patterns and self-rectify or self-adjust to prevent the problem from occurring. The chapter covers the advantages of using ML, building the first AI microservice, a demonstration of the Intelligent-MSA system in action, and the analysis of AI services’ operations. The goal is to provide you with a comprehensive understanding of how AI can be integrated into an MSA system to enhance its performance and operations.\n\nChapter 11, Managing the New System’s Deployment – Greenfield Versus Brownfield, introduces you to the deployment of Intelligent-MSA systems in greenfield and brownfield deployments. It provides ways to smoothly deploy the new system while maintaining overall system stability and business continuity. The chapter covers deployment strategies, the differences between greenfield and brownfield deployments, and ways to overcome deployment challenges, particularly in brownfield deployments where existing systems are already in production.\n\nPreface\n\nxv\n\nxvi\n\nPreface\n\nChapter 12, Deploying, Testing, and Operating an Intelligent MSA System, is the final chapter, and it integrates all the concepts covered in the book to provide hands-on and practical examples of deploying an intelligent MSA system. It teaches you how to apply the concepts learned throughout the book to your own deployment needs and criteria. The chapter assumes a brownfield environment with an existing monolithic architecture system and covers overcoming deployment dependencies, deploying the MSA system, testing and tuning the system, and conducting a post-deployment review.\n\nTo get the most out of this book\n\nTo maximize your learning experience, you should have a basic understanding of system architecture, software development concepts, DevOps, and database systems. Previous experience with MySQL and Python is not necessary, but it will help in understanding the concepts in the code examples more efficiently.\n\nSoftware/hardware covered in the book\n\nOperating system requirements\n\nDocker\n\nUbuntu Linux or macOS\n\nPython\n\nLinux, Windows, or macOS\n\nMySQL\n\nUbuntu Linux or macOS\n\nVirtualBox\n\nWindows or macOS\n\nWe recommend you install a Python IDE such as PyCharm to be able to follow the Python examples. PyCharm can be downloaded from https://www.jetbrains.com/lp/pycharm-anaconda/.\n\nVirtualBox is used to build the demo environment and create test virtual machines. VirtualBox can be downloaded from https://www.virtualbox.org/wiki/Downloads.\n\nUbuntu Linux is what we used in the book to install Docker and other utilities. To download the latest Ubuntu version, use the following link: https://ubuntu.com/desktop.\n\nIf you are using the digital version of this book, we advise you to type the code yourself or access the code from the book’s GitHub repository (a link is available in the next section). Doing so will help you avoid any potential errors related to the copying and pasting of code.\n\nDownload the example code files\n\nYou can download the example code files for this book from GitHub at https://github.com/ PacktPublishing/Machine-Learning-in-Microservices. If there’s an update to the code, it will be updated in the GitHub repository.\n\nWe also have other code bundles from our rich catalog of books and videos available at https:// github.com/PacktPublishing/. Check them out!\n\nConventions used\n\nThere are a number of text conventions used throughout this book.\n\nCode in text: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: “Once you are done composing your Dockerfile, you will then need to save it as Dockerfile to be able to use it to create the Docker image.”\n\nA block of code is set as follows:\n\nimport torch model = torch.nn.Sequential( # create a single layer Neural Network torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1) ) loss = torch.nn.MSELoss(reduction='sum')\n\nWhen we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:\n\nimport numpy as np from scipy import linalg a = np.array([[1,4,2], [3,9,7], [8,5,6]]) print(linalg.det(a)) # calculate the matrix determinate 57.0\n\nAny command-line input or output is written as follows:\n\n$ docker --version Docker version 20.10.18, build b40c2f6\n\nBold: Indicates a new term, an important word, or words that you see onscreen. For instance, words in menus or dialog boxes appear in bold. Here is an example: “Select System info from the Administration panel.”\n\nTips or important notes Appear like this.\n\nPreface\n\nxvii\n\nxviii\n\nPreface\n\nGet in touch\n\nFeedback from our readers is always welcome.\n\nGeneral feedback: If you have questions about any aspect of this book, email us at customercare@ packtpub.com and mention the book title in the subject of your message.\n\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit www.packtpub.com/support/errata and fill in the form.\n\nPiracy: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at copyright@packt.com with a link to the material.\n\nIf you are interested in becoming an author: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit authors.packtpub.com.\n\nShare Your Thoughts\n\nOnce you’ve read Machine Learning in Microservices, we’d love to hear your thoughts! Please click here to go straight to the Amazon review page for this book and share your feedback.\n\nYour review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.\n\nDownload a free PDF copy of this book\n\nThanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print books everywhere? Is your eBook purchase not compatible with the device of your choice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application.\n\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily\n\nFollow these simple steps to get the benefits:\n\n1. Scan the QR code or visit the link below\n\nhttps://packt.link/free-ebook/9781804617748\n\n2. Submit your proof of purchase\n\n3. That’s it! We’ll send your free PDF and other benefits to your email directly\n\nPreface\n\nxix\n\nPart 1: Overview of Microservices Design and Architecture\n\nWe will start Part 1 by providing a comprehensive introduction to Microservices Architecture (MSA) and its application in enterprise systems. We will learn about common enterprise system architectures, the concepts and value of MSA, and how it differs from traditional enterprise systems. Throughout this part, we will gain an understanding of the design, deployment, and operations of an MSA, including the basics of DevOps processes.\n\nWe will come to understand more details on the use cases in which each enterprise architecture model is best used. Part 1 also examines how to design a basic modular, flexible, scalable, and robust MSA, including how to translate business requirements into microservices. We will acquire in-depth information about the different methodologies used for transitioning into an MSA and the pros and cons of each approach.\n\nThis part discusses the challenges of designing a true MSA and provides the tools and techniques for tackling each of these challenges. We will learn about the enterprise system components used for optimizing system modularity, testability, deployability, and operations.\n\nPart 1 is designed to provide readers with a solid understanding of MSA, its benefits, and how to implement MSA in their own enterprise systems.\n\nThis part comprises the following chapters:\n\nChapter 1, Importance of MSA and Machine Learning in Enterprise Systems\n\nChapter 2, Refactoring Monolith\n\nChapter 3, Solving Common MSA Enterprise System Challenges\n\n1 Importance of MSA and Machine Learning in Enterprise Systems\n\nIn today’s market, the competition has never been fiercer, and user requirements for IT systems are constantly increasing. To be able to keep up with customer requirements and market demands, the need for a shorter time-to-market (TTM) for IT systems has never been more important, all of which has pushed for agile deployment and the need to streamline the development process and leverage as much code reuse as possible.\n\nMicroservices architecture (MSA) addresses these concerns and tries to deliver a more competitive, reliable, and rapid deployment and update delivery while maintaining an efficient, stable system operation.\n\nIn this chapter, we will learn more details about how microservices help build a modern, flexible, scalable, and resilient enterprise system. The chapter will go over key concepts in MSA and discuss the common enterprise system architectures, how each architecture is different from MSA, why they are different, and what you gain or lose when you adopt one or more architectures over the others.\n\nWe will cover the following areas as we go over the chapter:\n\nWhat MSA is and why\n\nMSA versus monolithic enterprise systems\n\nService-driven architecture, event-driven architecture (EDA), and how to incorporate that in MSA\n\nChallenges of deploying and operating MSA enterprise systems\n\nWhy it is important to embrace DevOps in building MSA",
      "page_number": 11
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 20-31)",
      "start_page": 20,
      "end_page": 31,
      "detection_method": "topic_boundary",
      "content": "4\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nWhy microservices? Pros and cons\n\nMicroservices is often likened to MSA. MSA refers to the way in which a complex system is built from a collection of smaller applications, where each application is designed for a specific limited-scope function. These small applications (or services, or microservices) are independently developed and can be independently deployed.\n\nEach microservice has an API interface for communicating with other microservices in the system. The way all these individual microservices are organized together forms the larger system function.\n\nIn order to understand the value of microservices and the challenges one faces in designing an MSA, it is imperative to understand how microservices communicate and interact with each other.\n\nMicroservices can communicate together in a linear or non-linear fashion. In a linear microservices pipeline, each microservice communicates with another microservice, processing data across the system in a sequential manner. The input is always passed to the first microservice, and the output is always generated by the last microservice in the system:\n\nFigure 1.1: Linear microservices pipeline\n\nPractically, however, most existing systems are formed using a non-linear microservices pipeline. In a non-linear microservices pipeline, data is distributed across different functions in the system. You can pass the input to any function in the system, and the output can be generated from any function in the system. You can therefore have multiple pipelines with multiple inputs, serving multiple functions and producing multiple outputs:\n\nFigure 1.2: Non-linear microservices pipeline\n\nWhy microservices? Pros and cons\n\nConsider the following diagram of a simplified order fulfillment process in a typical e-commerce system. Each function within the Placing an Order process represents a microservice. Once an order is placed by a customer, an API call is triggered to the Add/Update Customer Information microservice to save that customer’s information or update it if needed. This microservice sole responsibility is just that: manage customer information based on the data input it receives from the API caller.\n\nAnother API call is issued at the same time to the Verify Payment part of the process. The call will be directed to either the Process PayPal Payment or the Process Credit Card Payment microservice depending on the payment type of the API call. Notice here how the payment verification process is broken down into two different microservices—each is specifically designed and developed for a specific payment function. This enables the flexibility and portability of these microservices to other parts of the system or to another system if needed.\n\nAfter payment is processed, API calls are triggered simultaneously to other microservices in the system to fulfill the order:\n\nFigure 1.3: A non-linear microservices pipeline example – customer order\n\nThe order placement example shows how modular and flexible designing an MSA enterprise system can be. We will often use this example to show some of the advantages and challenges one may face when designing, deploying, and operating an MSA enterprise system.\n\nIt is essential that we go over some of the advantages and disadvantages of building enterprise systems using MSA to help decide whether MSA is a better option for your organization or not.\n\nNote that some of the advantages listed next could also be considered disadvantages in other situations (and vice versa).\n\n5\n\n6\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nAdvantages of microservices\n\nThere is some significant value to implementing MSA. The following are some of the advantages we see applicable to today’s market.\n\nAutonomy\n\nOne of the biggest advantages of microservices is their autonomy—it is the keystone for many of the other advantages of MSA. And because of their autonomy, microservices have their own technology stack, which means that each system service can be developed with completely different tools, libraries, frameworks, or programming languages than any other system service, yet they integrate with each other smoothly.\n\nMicroservices can be developed and tested independently of any other application within the system, which enables each microservice to have its own life cycle, including quality assurance (QA), change management, upgrades, updates, and so on, which in return greatly minimizes application dependencies.\n\nPortability\n\nMicroservices’ autonomy enables them to be portable across platforms, operating systems, and different systems, all independent of the coding language in which these services were written.\n\nReuse\n\nWhen reusing microservices, you don’t need to reinvent the wheel. Because of their autonomy, microservices can be reused without the need to add additional coding, changes, or testing. Each service can be reused as needed, which largely increases system flexibility and scalability, significantly reduces the development time, cost, and deployment time, and reduces the system’s TTM.\n\nLoosely coupled, highly modular, flexible, and scalable\n\nMicroservices form the main building blocks of an MSA enterprise system. Each block is loosely coupled with the other blocks in the system. Just like Lego blocks, the manner in which these blocks are organized together can form a complex enterprise MSA system building a specific business solution.\n\nThe following diagram shows an example of how we can build three different systems with multiple microservices.\n\nThe diagram shows nine services, and seven out of these services are organized in such a manner to reuse and build three different systems—system A, system B, and system C. This shows how loose coupling enables flexibility in MSA in such a way that you can reuse each service to build a different system function.\n\nYou can build a system with minimal development added to existing microservices either acquired by a third party or previously developed in house. This largely enables rapid system development, new feature releases, very short TTM, and reliable, flexible, and much more stable hot updates and\n\nWhy microservices? Pros and cons\n\nupgrades. All of this increases business continuity (BC) and makes the enterprise system much more scalable:\n\nFigure 1.4: Flexibility and modularity in microservices\n\nShorter release cycle and TTM\n\nBecause of the individual and independent services features we previously mentioned, the deployment of microservices becomes much easier and faster to perform. Automation can play a great role in reducing time-of-service testing and deployment, as we will discuss later in this chapter.\n\nFault tolerance and fault isolation\n\nEach microservice has its own separate fault domain. Failures in one microservice will be contained within that microservice, hence it is easier to troubleshoot and faster to fix and bring back the system to full operations.\n\nConsider the order fulfillment example we mentioned earlier; the system can still be functional if the Message/Email Customer microservice—for example—experiences any failures. And because of the nature of the failure and the small fault domain, it will be easy to pinpoint where that failure is and how to fix it. Mean Time to Resolution (MTTR) is therefore significantly reduced, and BC is greatly enhanced.\n\nArchitects are sometimes able to build the system with high embedded tolerance to prevent these failures to begin with or have other backup microservices on standby to take over once a failure is\n\n7\n\n8\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\ndetected in the primary microservice. One of the primary objectives of this book, as we will see later, is to be able to design a system with high enough intelligence to provide the desired high resilience.\n\nWhat software architects have to bear in mind, however, is that, with too many system components in the MSA, too many things can go wrong. Architects and developers, therefore, have to have solid fallback and error handling to manage the system’s resilience.\n\nThe communication between the different microservices, for example, can simply time out for whatever reason; it could be a network issue, a server issue, or too many API calls at the receiving microservices or at the event-handling mechanism developed in the system, overwhelming this system component and causing failures or delayed response.\n\nThere are many data flow streams and data processing points in the system that all need to be synchronized. A single failure, if not taken care of properly by the system, can create system-cascading failures, and accordingly could cause a failure to the entire system.\n\nHow fault tolerance is designed will be a big factor in how system performance and reliability are impacted.\n\nReliability and the Single Responsibility Principle (SRP)\n\nIf you come from the programming world, you are probably familiar with the SRP in object-oriented programming (OOP): A class should have one, and only one, reason to change. Every object, class, or function in the system should have a responsibility over only that functionality of the system, and hence that class, once developed, should only change for the reason it was originally created for. This principle is one of the main drivers of increased system reliability and BC in MSA.\n\nAt the initial phases of developing an MSA enterprise system, and during the phase of developing new microservices from scratch, the MSA enterprise system may not be fully tested or fully matured yet, and reliability may still be building up. When the system matures, changes to individual microservices are minimal—if any— and microservices’ code reliability is, therefore, higher, the operation is more stable, fault domains are contained, fault tolerance is high, and the system’s reliability thus becomes much higher than similar systems with a monolithic architecture. Reliability is highly contingent on how well the system is designed, developed, and deployed.\n\nReducing system development and operational cost\n\nReusing microservices largely reduces the development efforts and time needed to bring the system to life. The more microservices you can reuse, the lower the development time and cost will become.\n\nMicroservices do not have to be developed from scratch; you can purchase already developed microservices that you may need to plug into your MSA enterprise system, cutting the development time significantly.\n\nWhen these microservices are stable and mature, reliability is higher, MTTR is much shorter, and hence system faults are lower and BC is higher. All these factors can play a major role in reducing the development cost, operational cost, and total cost of ownership (TCO).\n\nWhy microservices? Pros and cons\n\nAutomation and operational orchestration are ideal for microservices; this enables agile development and can also decrease operational costs significantly.\n\nDisadvantages of microservices\n\nMicroservices come with a set of challenges that need to be taken into consideration before considering an MSA in your organization. The good news is that many of these challenges—if not all—can effectively be addressed to have in the end a robust MSA enterprise system.\n\nMentioned here are some of the challenges of microservices, and we will later in this chapter talk about some of the methodologies that help address these challenges.\n\nComplexity\n\nMSA systems contain many components that must work together and communicate together to form the overall solution. The system’s microservices in most cases are built with different frameworks, programming languages, and data structures.\n\nCommunication between microservices has to be in perfect synchronization for the system to properly function. Interface calls could at times overwhelm the microservice itself or the system as a whole, and therefore, system architects and developers have to continuously look for mechanisms to efficiently handle interface calls and try to eliminate dependencies as much as they can.\n\nDesigning the system to handle call loads, data flows, and data synchronization, along with the operational aspects of it, could be a very daunting process and creates layers of complexity that are hard to overlook.\n\nComplexity is one of the main trade-off factors in implementing and running an MSA enterprise system.\n\nInitial cost\n\nMSA systems usually require a large number of resources to be able to handle the individual processing needs of each microservice, the high level of communication between microservices, and the different development and staging environments for developing these microservices.\n\nIf these microservices are being developed from scratch, the initial cost of building an MSA system would therefore be too high. You have to account for the cost of the many individual development environments, the many microservices to develop and test, and the different teams to do all these tasks and integrate all these components. All this adds to the cost of the initial system development.\n\nTight API control\n\nEach microservice has its own API calls to be able to integrate with other microservices in the system. Any change in the API command reference set—such as updates in any API call arguments, deprecated APIs, or changes in the return values—may require a change in how other microservices handle the data flow from and to that updated microservice. This can pose a real challenge.\n\n9\n\n10\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nDevelopers have to either maintain backward compatibility (which can be a big constraint at times) or change the API calls’ code of every other component in the system that interacts with the updated microservice.\n\nSystem architects and developers have therefore to maintain very tight control over API changes in order to maintain system stability.\n\nData structure control and consistency\n\nThe drawback of having independent applications within the enterprise system is that each microservice will have to maintain its own data structure, which creates a challenge in maintaining data consistency across your system.\n\nIf we take the earlier example of customer order fulfillment, the Add/Update Customer Information microservice should have its own database totally independent from any other database in the system. Similarly, the Update Item Inventory microservice should be the microservice responsible for the item information database, the Update Orders Database microservice should have the orders database, and so on.\n\nThe challenge now is that the shipping database will need to be in sync with the customer information database, and the orders database will have to contain some of the customer information. Also, the Message/Email Customer microservice has to have a way to access customer information (or receive customer information through API calls), and so on. In a larger system, the process of keeping data consistent across the different microservices becomes problematic. The more microservices we have, the more complex the data synchronization becomes.\n\nOnce again, designing and developing a system with all that work in mind becomes another burden on the system architects and developers.\n\nPerformance\n\nAs we mentioned earlier, microservices have to communicate with each other to perform the entire system function. This communication, data flows, error handling, and fault-tolerance design—among many other factors—are susceptible to network latency, network congestions, network errors, application data processing time, database processing time, and data synchronization issues. All these factors greatly impact system performance.\n\nPerformance is another major trade-off factor in adopting and running an MSA enterprise system.\n\nSecurity\n\nBecause of microservices’ autonomy and their loose coupling, a high number of data exchanges between the different services is necessary for the MSA to function. This data flow, data storage within each microservice, data processing, the API call itself, and transaction logging all significantly increase the system attack surface and develop considerable security concerns.\n\nWhy microservices? Pros and cons\n\nOrganizational culture\n\nEach microservice in the MSA has its own development cycle and therefore has its silo of architects, developers, testers, and the entire development and release cycle teams, all to maintain the main objective of microservices: their autonomy.\n\nMSA enterprise systems are built from a large number of microservices and mechanisms to manage the interaction between the different system components. Developers have to therefore have system operational knowledge, and the operational teams need to have development knowledge.\n\nTesting such complex distributed environments that one will have in the MSA system becomes a very daunting process that needs a different set of expertise.\n\nThe traditional organizational structure of one big development team solely focused on development, one QA team only doing basic testing, and so on is no longer sufficient for the way MSA is structured and operated.\n\nAgile development and DevOps methodologies are very well suited for microservices development. You need agile processes to help maintain the fast development and release cycles MSA promises to deliver. You need DevOps teams who are very familiar with the end-to-end process of designing the application itself and how it fits in the big picture, testing the application, testing how it functions within the entire system, the release cycle, and how to monitor the application post release.\n\nAll this requires a cultural shift and significant organizational transformation that can enable DevOps and agile development.\n\nImportant note We rarely see a failure in MSA adoption because of technical limitations; rather, failure in adopting MSA is almost always due to a failure to shift the organization’s culture toward a true DevOps and agile culture.\n\nThe benefits outweigh the detriments\n\nThe main questions you need to answer now are: Is building an MSA worth it? Can we make it happen given the current organizational culture? How long will it take the organization to transform and be ready for MSA? Do we have the luxury of waiting? Can we do both the organizational transformation and the building of the MSA enterprise system at the same time? Do we have the resources and the caliber necessary for the new organizational structure? Is cost an issue, and do I have the budget to cover that?\n\nWell, first of all, if you are planning to build a large enterprise system, and you have the budget and necessary resources for starting this project, building the system as MSA is definitely worth it. All initial costs endured and time spent will eventually be offset by the long-term cost and time-saving benefits of having an MSA system.\n\n11\n\n12\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nNevertheless, you are still the one to best address all these previous questions. There are overwhelming and compelling advantages to adopting MSA, but as we have seen, this is not a simple undertaking; so, whether an organization is willing to walk that path or not is something it—and only it—can answer.\n\nNow we know what the advantages of deploying an MSA are, and the challenges that come with MSA adoption, we will now go over different enterprise architecture styles, what they are, and the differences between each other.\n\nLoosely versus tightly coupled monolithic systems\n\nTraditional applications back in the day were mostly built using a monolithic architecture, in which the entire application was one big code base. All system components and functions were tightly coupled together to deliver the business solution.\n\nAs shown in the following diagram, system functions are all part of the same code, tightly coupled with centralized governance. Each system function has to be developed within the same framework of the application.\n\nIn an MSA system, however, each function preserves its own anonymity—that is, loosely coupled with decentralized governance, giving each team the ability to work with its own preferred technology stack, with whichever tools, framework, and programming language it desires:\n\nFigure 1.5: Monolithic versus microservices systems\n\nLoosely versus tightly coupled monolithic systems\n\nAll functions in the monolithic architecture application are wrapped into the application itself. In the MSA, these functions are developed, packaged, and deployed separately. Therefore, we can run these services in multiple locations’ on-premises infrastructure, in the public cloud, or across both on-premises and the cloud in a hybrid-cloud fashion.\n\nIn monolithic systems, and because of the tight coupling, synchronizing the different system function changes is a development and operational nightmare. If one application (for whatever reason) becomes unstable, it could cause a failure to the entire system, and bringing the system back to a stable point becomes a real pain.\n\nIn the case of microservices, however, since each of these microservices is loosely coupled, changes and troubleshooting are limited to that particular microservice, as long as the microservice interface does not change.\n\nOne large piece of code, in the case of monolithic architecture, is very hard to manage and maintain. It is also hard to understand, especially in large organizations where multiple developers are working together.\n\nIn many cases such as employee turnover, for example, a developer may need to troubleshoot someone else’s code, and when the application is written in a single big piece of code, things tend to be complicated, hard to trace and understand, and hard to reverse engineer and fix. Code maintenance becomes a serious problem, while in the microservices case, this humongous line of code is broken into smaller chunks of code that are easier to read, understand, troubleshoot, and fix, totally independent of the other components of the system.\n\nWhen code changes are needed in monolithic architecture, a single change to part of the code may need changes to many other parts of the application, and accordingly, change updates will likely require a rewrite and a recompile of the entire application.\n\nWe can also reuse and package different applications together in a workflow to form a specific service, as shown previously in Figure 1.4.\n\nIt is just common sense to break down a complex application into multiple modules or microservices, each performing a specific function in the entire ecosystem for better scalability, higher portability, and more efficient development and operations.\n\nFor small, simple, and short-lived systems, monolithic applications may be a better fit for your organization, easier to design and deploy, cheaper to develop, and faster to release. As the business needs grow, MSA becomes a better long-term approach.\n\nSince monolithic systems are tightly coupled, there is no need for API communication between the different system functions; this significantly decreases the security surface of your system, lowering system security risks and increasing the system’s overall performance.\n\nThink of the deployment difference between both monolithic and MSA as the difference between an economy car and a Boeing 787. The car is a better, cheaper, and faster tool for traveling between two cities 50 miles apart, with no need for the security checks you experience in airports before boarding\n\n13\n\n14\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nyour flight. As the distance increases, however, driving the car becomes more hassle. At 5,000 miles, the Boeing 787 is likely to become a better, cheaper, and faster way to get to your destination, and you will likely be willing to put up with the hassle of security checks you have to undergo to be able to board your flight.\n\nThe following is a comparison summary between both monolithic and microservices applications:\n\nMonolithic\n\nMSA\n\nArchitecture\n\nHighly autonomous. System functions are split into independent loosely coupled chunks of smaller code.\n\nNo autonomy. System functions are all tightly coupled into one big piece of code.\n\nPortability\n\nHighly portable\n\nVery limited portability\n\nReuse\n\nHighly reusable\n\nVery limited ability to reuse code\n\nModularity and Scalability\n\nHighly modular and scalable\n\nLimited modularity and hard to scale\n\nInitial TTM\n\nHighly dependent on the readiness of individual system services. The more code reuses, the shorter the TTM is.\n\nLong TTM, especially in large systems. Shorter TTM in small and simple systems.\n\nIf the system microservices are being designed and developed from scratch, TTM is usually longer for monolithic architecture.\n\nRelease Cycle\n\nVery short release cycle, super- fast to deploy changes and patch updates\n\nLong and usually very time- consuming release cycles and patch updates\n\nInitial Cost\n\nUsually high. Depends on the system size.\n\nUsually low. The initial size becomes higher in large enterprise systems.\n\nThe initial cost is offset by operational cost savings.\n\nOperational Cost\n\nLow. Easier to maintain and operate.\n\nHigh. Hard to maintain and operate.\n\nComplexity\n\nHigh\n\nLow\n\nAPI Control\n\nHigh\n\nLow\n\nService-driven, EDA, and MSA hybrid model architecture\n\nMonolithic\n\nMSA\n\nData Structure Consistency\n\nDecentralized databases, hence data consistency is harder to maintain\n\nA centralized database, hence easier to maintain data consistency across the system\n\nPerformance\n\nUsually lower\n\nUsually higher\n\nSecurity\n\nMany security concerns\n\nLower security concerns\n\nOrganizational Adoption Hard to adopt depending on the organizational structure. Requires adoption of agile development and DevOps. Organizational transformation may be required and may take a long time to achieve.\n\nEasy to adopt. Minimal organizational transformation needed—if any.\n\nFault Tolerance\n\nUsually higher\n\nUsually lower\n\nTable 1.1: Summary of the differences between monolithic and MSA systems\n\nWe covered in this section the different aspects of a monolithic system; next, we go over service-driven architecture and EDA, and how to combine these architectural styles within MSA to address some of the MSA challenges discussed earlier.\n\nService-driven, EDA, and MSA hybrid model architecture\n\nPeople often get mixed up between MSA and service-driven architecture (aka service-oriented architecture or SOA). Both types of architecture try to break down the monolithic architecture system into smaller services. However, in MSA, the system services decomposition is extremely granular, breaking down the system into very fine specialized independent services. In the SOA, the system services decomposition is instead coarse-grained to the domain level.\n\nAll domains, as shown in the following diagram, share the same centralized database and may actually share other resources in between, creating some level of coupling and system dependencies that are non-existent in MSA. Data storage is a key difference between both architectural styles:\n\n15",
      "page_number": 20
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 32-40)",
      "start_page": 32,
      "end_page": 40,
      "detection_method": "topic_boundary",
      "content": "16\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nFigure 1.6: SOA architecture split into functional domains\n\nIn the case of the simplified MSA customer ordering example discussed earlier, there are eight different microservices. A similar implementation in SOA is likely to have all these microservices built together and tightly coupled in a single domain. Other domains within the system could be Cart Handling, Catalog Browsing and Suggestions, and so on.\n\nSOA has a holistic enterprise view, while in a microservice, development looks into the function itself in total isolation of the enterprise system in which the microservice is intended to be used.\n\nEDA is another architectural style that is largely adopted. While MSA’s main focus is on function and SOA emphasizes the domain, EDA instead focuses on system events.\n\nEDA is usually complemented by another main system architecture, such as SOA or MSA. In EDA, services are decoupled at a granularity level determined by its main architecture (MSA or SOA) and then communicate with each other through event-based transactions. In our order placement example, these events could be Order Created, Order Canceled, Order Shipped, and so on.\n\nIn order to maintain event synchronization and data consistency across the enterprise system, these events must be handled by a message broker. The message broker’s sole responsibility is to guarantee the delivery of these events to different services across the system. Therefore, it has to be highly available, highly responsive, fault-tolerant, and scalable and must be able to function under heavy load.\n\nWhen EDA is adopted within the MSA enterprise system, the message broker in that case will be handling events, API calls, and API calls’ responses.\n\nThe message broker has to be able to queue messages when a specific service is down or under heavy load and deliver that message whenever that service becomes available.\n\nService-driven, EDA, and MSA hybrid model architecture\n\nACID transactions\n\nAny system with some form of data storage always needs to ensure the integrity, reliability, and consistency of that data. In MSA, systems store and consume data across the workflow transactions, and for individual services to ensure integrity and reliability for the MSA system as a whole, data stored within the entire system have to comply with a certain set of principles called Atomicity, Consistency, Isolation, and Durability (ACID):\n\nAtomicity: All-or-nothing transactions. Either all transactions in the workflow are successfully executed and committed or they all fail and are canceled.\n\nConsistency: Any data change in one service has to maintain its integrity across the system or be canceled.\n\nIsolation: Each data transaction has its own sovereignty and should not impact or be impacted by other transactions in the system.\n\nDurability: Committed transactions are forever permanent, even in the case of a system failure.\n\nSaga patterns\n\nOne of the main challenges in MSA is distributed transactions, where data flow spans across multiple microservices in the system. This flow of data across the services creates a risk of violating the microservice autonomy. Data has to be managed within the microservice itself in total isolation from any other service in the system.\n\nIf you look at our order placement example again, you find that customer data (or part of it) spans across the different microservices in the example, which could create undesired dependencies in the MSA, and should be avoided at all costs.\n\nWhat if, for whatever reason, the Update Item Inventory service fails, or it just happens that the service reports back that the item is no longer available? The system in that case will need to roll back and update all individual services’ databases to ensure ACID transactions for the workflow.\n\nThe saga pattern manages the entire workflow of transactions. It sees all sets of transactions performed in a specific process as a workflow and ensures that all these transactions in that workflow are either successfully executed and committed or rolled back in case the workflow breaks for whatever reason, to maintain data consistency across the system.\n\nA saga participant service would have a local transaction part of that workflow. A local transaction is a transaction performed within the service itself and produces an event upon execution to trigger the next local transaction in the workflow. These transactions must comply with ACID principles. If one of these local transactions fails, the saga service initiates a set of compensating transactions to roll back any changes caused by the already executed local transactions in the workflow.\n\n17\n\n18\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nEach local transaction should have corresponding compensating transactions to be executed to roll back actions caused by the local transaction, as shown in the following diagram:\n\nFigure 1.7: Processing of local and compensating transactions\n\nThere are two ways to coordinate transactions’ workflow in a saga service: choreography and orchestration.\n\nIn choreography, saga participant services exchange events without the need for a centralized manager. As in EDA, a message broker is needed to handle event exchanges between services, as illustrated in the following diagram:\n\nService-driven, EDA, and MSA hybrid model architecture\n\nFigure 1.8: Choreography in a saga service\n\nIn orchestration, a saga pattern-centralized controller is introduced: an orchestrator. The workflow is configured in the orchestrator and the orchestrator sends requests to each saga participant service on which local transaction it needs to execute, receives events from saga participant services, checks the status of each request, and handles any local transaction failures by executing the necessary compensating transactions, as illustrated in the following diagram:\n\n19\n\n20\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nFigure 1.9: Orchestration in a saga service\n\nOrchestrators become the brain of the enterprise system and the single source for all steps that need to be taken to execute a specific system workflow. The orchestrator, therefore, must be implemented in a way to be highly resilient and highly available.\n\nCommand Query Responsibility Segregation (CQRS)\n\nIt is very common in traditional systems, and especially in monolithic applications, to have a common relational database deployed in the backend and accessed by a frontend application. That centralized database is accessed with Create-Read-Update-Delete (CRUD) operations.\n\nIn modern architecture, especially as the application scales, this traditional implementation poses a problem. With multiple CRUD requests being processed on the database, table joins are created with a high likelihood of database locking happening. Table locks introduce latency and resource competition, and greatly impact overall system performance.\n\nComplex queries have a large number of table joins and can lock the tables, preventing any write or update operations on them till the query is done and the database unlocks the tables. Database read operations are typically multiple times more than write operations, and in heavy transaction systems, the problem can multiply.\n\nService-driven, EDA, and MSA hybrid model architecture\n\nYou can see a comparison of CRUD and CQRS patterns here:\n\nFigure 1.10: CRUD versus CQRS patterns\n\n21\n\n22\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nWith CQRS, you simply separate one object into two objects. So, rather than doing both commands and queries on one object, we separate that object into two objects—one for the command, and one for the query. A command is an operation that changes the state of the object, while a query does not change the state of the system but instead returns a result.\n\nIn our case, the object here is the system database, and that database separation could be either physical or logical. Although it is a best practice to have two physical databases for CQRS, you can still use the same physical database for both commands and queries. You can, for example, split the database into two logical views—one for commands and one for queries.\n\nA replica is created from the master database when two physical databases are used in CQRS. The replica will, of course, need to be synchronized with the master for data consistency. The synchronization can be accomplished by implementing EDA where a message broker is handling all system events. The replica subscribes to the message broker, and whenever the master database publishes an event to the message broker, the replica database will synchronize that specific change.\n\nThere will be a delay between the exact time at which the master database was actually changed and when that change is reflected in the replica; the two databases are not 100% consistent during that period of time but will be eventually consistent. In CQRS, this synchronization is called eventual consistency synchronization.\n\nWhen applying CQRS design in MSA, database processing latency is greatly reduced, and hence communication between individual services’ performance is greatly enhanced, resulting in an overall system-enhanced performance.\n\nThe database used can be of any type, depending on the business case of that particular service in the MSA. It may very well be a relational database (RDB), document database, graph database, and so on. A NoSQL database could also be an excellent choice.\n\nWe discussed previously the MSA from a design and architecture perspective. Operating the MSA system is another aspect that the entire organization must consider for a successful business delivery process. In the next section, we discuss DevOps, how it fits into the MSA life cycle, and why it is important for a successful MSA adoption and operation.\n\nDevOps in MSA\n\nDevOps revolves around a set of operational guidelines in the software development and release cycles. The traditional development engineer is no longer living in their confined environment where all the focus is to convert functional specifications into code; rather, they should have an end-to-end awareness of the application.\n\nA DevOps engineer would oversee, understand, and be involved in the entire pipeline from the moment the entire application is planned out, converting business functions into code, building the application, testing it, releasing it, monitoring its operations, and coming back with the feedback necessary for enhancements and updates.\n\nDevOps in MSA\n\nThat does not necessarily mean that a DevOps engineer would be responsible for all development and operational task details. Individual responsibilities within the application team may vary in a way to guarantee a smooth continuous integration and continuous deployment (CI/CD) pipeline of the application:\n\nFigure 1.11: DevOps CI/CD pipeline\n\nOne of the main objectives of DevOps is to speed up the CI/CD pipeline; that’s why there is a lot of emphasis on automation in DevOps. Automation is essential to efficiently perform the pipeline.\n\nAutomation can help at every step of the way. In DevOps, many test cases that are part of your QA plan are automated, which significantly speeds up the QA process. The release management and monitoring of your application are also automated to provide high visibility, continuous learning, and quick fixes whenever needed. All of this will help organizations improve productivity, predictability, and scalability.\n\nDevOps is a holistic view of how the application is developed and managed. It is not a function for only the development team or operational team to adopt; rather, the entire organization should adopt it. It is therefore imperative for the organizational structure and the organization’s vision and goal to all align with the set of procedural and functional changes necessary to shift from the traditional way of developing software.\n\nJust to give you a gist of how traditional and DevOps models differ in terms of application development and release cycles, take a look at the following comparison table:\n\n23\n\n24\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nTraditional\n\nDevOps\n\nPlanning\n\nMonths\n\nDays to weeks\n\nLong time to plan due to the large application size and tight coupling between different application components\n\nVery short planning time since the application is broken down into small individual loosely coupled services\n\nDevelopment\n\nMonths\n\nDays to weeks, and even shorter in the case of patches and fixes\n\nTesting\n\nWeeks to months\n\nDays\n\nMostly manually intensive QA use case testing, which may sometimes jeopardize the reliability of the test’s outcome\n\nMostly automated QA use case execution that brings high reliability to the application\n\nRelease, Deploy\n\nDays\n\nHours\n\nUsually long manual work and more susceptible to human errors\n\nMostly automated\n\nOperate, Monitor Metrics reporting is mostly\n\nmanually pulled and analyzed\n\nMetrics are monitored and analyzed automatically and can even fix the problem in seconds. Moreover, machine learning (ML) tools can be used to enhance operations even further.\n\nTable 1.2: Traditional operational style versus DevOps\n\nIn traditional development environments, you have a big piece of code to write, maintain, and change when needed. Because of the code size, it is only normal to have a long release cycle, and it can only be feasible to deploy patches or new releases when only major changes or high-severity fixes are needed, as illustrated in the following diagram:",
      "page_number": 32
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 41-48)",
      "start_page": 41,
      "end_page": 48,
      "detection_method": "topic_boundary",
      "content": "DevOps in MSA\n\nFigure 1.12: Traditional development environment versus MSA DevOps\n\nIn MSA, teams are separated based on applications that do not function. That big chunk of code is split into a collection of much smaller code (microservices), and since teams are split to work independently for each team to focus on a specialized microservice, the development and release cycles are much shorter.\n\nSimilarly, in DevOps, the application is broken down into smaller pieces to enable the CI/CD pipeline, which makes DevOps the perfect model that fits MSA.\n\nWhy ML?\n\nUsing ML tools and algorithms in your MSA enterprise system can further enhance and accelerate your DevOps CI/CD pipeline. With ML, you can find patterns in your tests, monitor phases of your pipeline, automatically analyze where the faults may be, and suggest a resolution or automatically fix operational issues whenever possible.\n\nML can greatly shorten your MSA enterprise system’s TTM and make it more intelligent, self-healing, resilient, and supportable.\n\nWe will in this book discuss two aspects of ML: first, we’ll explain in detail how to add CI/CD pipeline intelligence to your MSA enterprise system, and second, we’ll look at how to build an ML enterprise system with MSA in mind:\n\n25\n\n26\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nFigure 1.13: Using ML in CI/CD pipeline\n\nSummary\n\nIn this chapter, we covered the concepts of MSA and how MSA is different from traditional monolithic architecture. By now, you should also have a clear understanding of the advantages of MSA and the challenges organizations may experience when adopting MSA.\n\nWe also covered the key concept of methodologies to consider when designing MSA, such as ACID, the saga pattern, and CQRS. All these concepts are essential to help overcome synchronization challenges and to maintain microservices anonymity.\n\nWe now understand the basics of DevOps and why it is important in MSA design, deployment, and operations, as well as how ML integration in MSA enterprise systems can help enhance system operations.\n\nIn the next chapter, we will go over common methodologies that organizations pursue to transition from running traditional monolithic systems to MSA systems. We will discuss how to break down the existing system into services that form the new MSA enterprise system.\n\n2 Refactoring Your Monolith\n\nNow we have decided that MSA is the right architectural style for our organization, what’s next?\n\nIn a recent report, 2022 APIs & Microservices Connectivity Report, published by Kong Inc., 75% of organizations have a lack of innovation and technology adoption.\n\nThe need for an IT system that quickly responds to customer and market needs has never been higher. Monolithic applications can no longer respond to high-paced market updates and needs. That’s one main reason for organizations to look to update their IT system, to stay in business.\n\nMSA is a primary enabler for a flexible and reliable enterprise system. Transitioning from a monolithic architecture into MSA is, therefore, becoming essential to modernizing an organization’s IT systems.\n\nWe will discuss, in this chapter, how to break up the business requirements of an existing running monolithic application in to microservices, and the steps necessary to transition toward MSA applications.\n\nWe will cover the following areas as we go over the chapter:\n\n\n\nIdentifying the system’s microservices\n\nThe ABC monolith\n\nFunction decomposition\n\nData decomposition\n\nRequest decomposition\n\nIdentifying the system’s microservices\n\nWhether it is a brownfield or greenfield enterprise system implementation, we still need to break up business requirements into basic functions as granularly as possible. This will later help us identify each microservice and successfully integrate it into our enterprise system.\n\nIn a brownfield system, business and system requirements have already been identified and implemented. They may, however, need to be revisited and updated according to new business criteria, changes, and requirements.\n\n28\n\nRefactoring Your Monolith\n\nThe objective of refactoring your application into simple services is to form highly granular functions that will eventually be built (or acquired) as microservices. You are very likely to add new functions to your new MSA in addition to some of the functions you will already extract from the monolithic system.\n\nWe, therefore, split the migration process into the following high-level steps:\n\n1. Define the to-be MSA system and the functions needed to build that MSA.\n\n2.\n\nIdentify what existing functions in the current monolithic system are to be reused in the new MSA and implemented as microservices.\n\n3.\n\nIdentify the delta between the existing functions to be reused and the functions needed to get to the to-be MSA system. These are the new functions to be implemented in the new MSA system.\n\n4. From the functions list identified in step 3, identify which functions will be developed as a microservice in-house, and the ones that can be acquired through third parties.\n\nDecomposing the monolith using a function-driven approach is a good starting point; nevertheless, using that approach alone is not enough. Since data stores are centralized in the monolith, data dependencies will still be a big concern in maintaining the microservices’ autonomy.\n\nThe interaction between the different functions in the monolith is another concern. We will need to look into how the function calls are being processed and handled, what data is being shared between these functions, and what data is being returned.\n\nExamining monolithic system functions, data, and function calls (requests) during the refactoring process is essential for maintaining the autonomy of microservices and achieving the desired level of granularity.\n\nImportant note Bear in mind that we must maintain the microservices autonomy principle during the entire monolith decomposition process. Too many microservices would cause a Nano-service anti-pattern effect, while too few would still leave your system with the same issues as a monolithic system.\n\nThe Nano-service anti-pattern creates too many expectations for most systems’ operations, which can in turn further complicate your MSA system and create a lack of stability, decreased reliability, and other system performance issues.\n\nImportant note As a general rule, apply the Common Closure Principle, where microservices that change for the same exact reason are better off packaged together in a single microservice.\n\nThe ABC monolith\n\nTo better explain the monolith transformation process to an MSA, in the following sections, we will design a simple hypothetical monolithic system, break up the system using the already mentioned three stages of system decomposition, build the different microservices, and then organize them together to build the MSA.\n\nThe ABC monolith\n\nABC is a simplified hypothetical product-ordering monolithic system built specifically to demonstrate the process and the steps needed in refactoring a monolithic application into an MSA. We will be using this ABC system throughout this book to demonstrate some examples of how to apply the concepts and methodologies.\n\nPlease note that we put the ABC-Monolith system together for demo purposes only and our aim here is not to discuss how the ABC-Monolith can be designed or structured better. We are more focused on the ABC-Monolith system refactoring process itself.\n\nIn the ABC-Monolith, the user can place an order from an existing product catalog and track the order’s shipping status. For simplicity, all sales are final, and products cannot be returned.\n\nThe system will be able to clear the order payment, assign a shipping courier to the order, and track all order and shipping updates.\n\nThe following diagram shows the high-level ABC-Monolith architecture. A user portal is used to add items to the cart, then send the order details to the ABC-Monolith. The ABC-Monolith has different tightly coupled functions with a centralized database, all to process the order from payment to delivery. The user is notified of all order and shipping updates throughout the order fulfillment process.\n\nFigure 2.1: The ABC-Monolith architecture\n\n29\n\n30\n\nRefactoring Your Monolith\n\nTo further understand the monolith, we will next go over the system As-Is state by discussing the existing monolith’s functions, the monolith database structure, and the workflow of the order placement process. We will close this section by comparing the As-Is to the To-Be state.\n\nThe ABC-Monolith’s current functions\n\nIt is imperative to start by understanding what current functions are implemented in the monolith and what their role is in the overall system. The following table lists the system functions we need to consider later in our system refactoring:\n\nFunction\n\nDescription\n\nplace_order()\n\nA function to create a record with all order information, and mark the order as “pending” awaiting the rest of the order placement process.\n\ncheck_inventory()\n\nTo check the availability of an item in the placed order.\n\nprocess_payment()\n\nVerify the payment of the total order amount. Will return an error code if the payment is not cleared.\n\nupdate_inventory()\n\nOnce an order is verified and the payment is successfully processed, the item inventory should be updated accordingly.\n\ncreate_order()\n\nThe order is now successfully processed; time to change the order status, and kick off the order preparation process (packing, etc.).\n\ncreate_shipping_request()\n\nStarts the order shipping request and notifies the courier with an available order for shipping.\n\norder_status_update()\n\nA function to update the order status with any changes such as preparing, shipping, exception, received, and so on.\n\nshipment_status_update()\n\nA function to update the shipping status with any changes such as, pending pickup, picked up, en route, exception, received, and so on.\n\nnotify_user()\n\nTo notify the user of any changes or updates to the placed order.\n\nregister_customer()\n\nA function that creates customer record information with a full name, address, phone, and other details.\n\nTable 2.1: The ABC-Monolith functions list\n\nIn the preceding table, we focused our description on the role of the function itself regardless of what the parameter passing is, or what the return values are.\n\nThe ABC monolith\n\nThe ABC-Monolith’s database\n\nAll the functions identified in the monolith share a centralized database. The following are the database tables being accessed by the functions:\n\nDatabase Table\n\nDescription\n\nCUSTOMER\n\nA table holding all customer information such as name, email, and phone.\n\nITEM\n\nThe product information is in the catalog. Product information includes product name, price, and stock quantity.\n\nORDER\n\nInformation on orders placed.\n\nORDER_ITEM\n\nORDER_STATUS\n\nSTATUS_CODE\n\nA many-to-many relationship normalization table between the ORDER and ITEM tables. The status of each placed order, with a reference to status_ code. Lookup table for order and shipment status codes.\n\nCOURIER\n\nShipping courier information, including courier name, contact, and so on.\n\nSHIPMENT_REQUEST SHIPMENT_REQUEST_STATUS The status of each shipment request, with a reference\n\nA list of all shipping requests for orders placed.\n\nto status_code. Table 2.2: The ABC-Monolith database tables list\n\nThe following is ABC’s Entity Relationship Diagram (ERD). Note that we needed to create the ORDER_ITEM normalization table to break up the many-to-many relationship between both the ORDER and ITEM tables:\n\nFigure 2.2: The ABC-Monolith ERD\n\n31\n\n32\n\nRefactoring Your Monolith\n\nKeep in mind that some of the monolith’s functions require full read/write access to specific tables with access to all fields in the table, while some other functions need only access to specific fields in the table. This information is important in system refactoring.\n\nIn the following section, we will go over the workflow to identify the ABC-Monolith As-Is state and determine how we can transition into the To-Be state. Along with the workflow information, the function database access requirements will help us refactor the monolith database into individual MSA databases for each microservice.\n\nThe ABC workflow and current function calls\n\nWe know so far what functions are used in the monolith and how the monolith’s database is structured. The next step is to examine the order placement workflow:\n\nFigure 2.3: The ABC-Monolith function requests/workflow\n\nAs shown in the preceding workflow diagram, the individual functions are all executed sequentially. Since it is all one tightly coupled system, there are no synchronization issues expected, and hence no orchestration is needed.\n\nAs we move toward the ABC-MSA, however, the decoupling of services creates the need to have a centralized point for managing the execution of these services in a specific sequence.\n\nShown in the following diagram are ABC’s As-Is and To-Be states. No centralized management in the As-Is state is needed; however, an orchestrator component is introduced in the To-Be state to manage the process flows between the services.",
      "page_number": 41
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 49-57)",
      "start_page": 49,
      "end_page": 57,
      "detection_method": "topic_boundary",
      "content": "Function decomposition\n\nEach of the individual services in the To-Be states has a dedicated database, as shown in the diagram. In the As-Is state, on the other hand, the database is centralized.\n\nFigure 2.4: The ABC As-Is and To-Be states\n\nNow that we know how our current ABC-Monolith is structured, and what both the as-is and to-be states are, it is time to start the ABC-Monolith refactoring process to transform into the ABC-MSA.\n\nWe will refactor the monolith in three stages. First, we will decompose the monolith functions and map these functions to microservices. Then, we will decompose the data to see how the individual databases will be designed. Finally, out of the monolith’s workflow, we will analyze the function requests, and build our MSA sagas from there.\n\nFunction decomposition\n\nThe first step in refactoring the ABC-monolith is to create the microservices based on the system functions we previously identified. This is a straightforward mapping between the existing functions and the microservices.\n\nThe key point here is that, by looking only at each function by itself without considering any function calls or data connections, you need to be as granular as possible in your function decomposition.\n\nAt first glance, the notify_user() function is doing too many things for a microservice, displaying a web user message status/update, notifying the user by email, and/or notifying the user by SMS. Each of these functions can have its own rules, design, issues, and concerns. Splitting the notify_user() function into three functions is a better approach from an MSA perspective to achieve the separation of concerns.\n\n33\n\n34\n\nRefactoring Your Monolith\n\nAccordingly, we split the notify_user() function into one function for handling web messages and notifications, one for handling email notifications, and one for SMS message notifications:\n\nweb_msg_notification()\n\nemail_notification()\n\nsms_notification()\n\nSimilarly, the process_payment() function can also be split into two different, more granular functions, one for handling direct credit card payments and one for handling PayPal payments:\n\nverify_cc_payment()\n\nverify_paypal_payment()\n\nThe following diagram shows how the ABC-Monolith is broken up so far. We haven’t yet looked into how the system’s functions are interacting with each other. The function interactions and the order fulfillment’s overall workflow will be handled at a later stage.\n\nFigure 2.5: The ABC-Monolith function decomposition\n\nAt this point, we are satisfied with the current level of granularity so far and we are ready to examine how the database tables are being accessed to see whether further decomposition is needed.\n\nData decomposition\n\nData decomposition\n\nDuring this stage, we need to look at how each function is accessing the database and what tables and even which parts of the database tables are accessed.\n\nThe following diagram shows what parts of the database the ABC-Monolith functions access. It is essential to know exactly which tables are accessed by which function and why. This will help us identify database dependencies, in order to later eliminate these dependencies and split the centralized ABC-Monolith database into separate data stores, each data store dedicated to each microservice.\n\nFigure 2.6: ABC-Monolith database access\n\n35\n\n36\n\nRefactoring Your Monolith\n\nWe are still bound by the microservice autonomy rule. The challenging part in the diagram and this refactoring phase is the shared tables. Sharing a table between two microservices creates coupling that would clearly violate the autonomy rule. On the other hand, creating multiple copies of the table across different microservices will create serious data consistency issues. So, how do we solve this conundrum?\n\nRemember the saga patterns that we previously discussed in Chapter 1? Saga patterns should be able to solve data consistency issues that arise from having a transaction that spans multiple services. In our example here, we can have duplicates of the ORDER table, for example, across the place_order(), create_order(), and process_payment() services of the ABC-MSA system. A similar approach is taken for check_inventory(), update_inventory(), and so on.\n\nSo, with saga patterns in mind, let’s reexamine the ABC-Monolith database access shown in the preceding diagram, to build a new database access diagram for services in the ABC-MSA system.\n\nThere are two ways to coordinate the data transactions, choreography and orchestration. In choreography, the ABC-MSA saga participant services will have to coordinate data transactions among themselves. In orchestration, a centralized orchestrator performs the coordination process and handles all workflow transactions.\n\nCertainly, we can choose either coordination methodology, but in our example, we would argue that orchestration creates a better decoupling model over choreography. For that reason, and to keep our example simple, we will be using orchestration for our ABC-MSA saga patterns.\n\nThe following diagram shows the ABC-MSA service database access. As you can see in the diagram, there are a few database tables that have been copied across the system. We will, in the next section, use saga patterns to maintain data consistency across the copied tables.\n\nData decomposition\n\nFigure 2.7: ABC-MSA database access\n\nWe notice in other services, such as web_msg_notification, email_notification, and sms_notification, that the database is identical for all three services. This is an indication that creating these three services off the original notify_user() function may not be a good idea anyway. You should only see small database access similarities between these different services, not a completely identical database. In a real scenario, we are better off combining these three services into only one service as it originally was.\n\nSimilarly, in a real-life scenario, the process_payment() function is likely to be mapped to a single service that includes clearing the payment overall, regardless of whether it is a credit card, PayPal, or any other form of payment. For demo purposes, we will split notify_user() and process_payment() into three and two different services respectively.\n\n37\n\n38\n\nRefactoring Your Monolith\n\nSo far, we have been able to build the ABC-MSA’s microservices from the ABC-Monolith functions, identify data access in the monolith, and decompose the monolith into separate microservices, each with its own database. In the next section, we will focus more on how to ensure isolation and separation of concerns for the microservices by looking into how the service requests are orchestrated in the new ABC-MSA system.\n\nRequest decomposition\n\nThe ABC-Monolith function request flow has already been identified and shown in Figure 1.3. We will now see how this flow is going to work in the ABC-MSA.\n\nIn the ABC-MSA, the sagas are programmed and configured in the centralized orchestrator. The orchestrator will initiate separate API calls to each service in the saga, in either a synchronous or asynchronous fashion, depending on the defined workflow, and wait for a response from each API call to determine what other API call(s) to initiate next and how.\n\nThe following diagram shows how the workflow would be in the ABC-MSA. Please note that all API calls in our scenario are being initiated from the orchestrator. As you can see from the sequence number, there are some API calls initiated in parallel, and in some other cases, the orchestrator decides the next course of action based on the response it receives from a previously executed service.\n\nFigure 2.8: The ABC-MSA workflow\n\nRequest decomposition\n\nThe user in the ABC-MSA workflow diagram initiates the order fulfillment process from a web interface, which will kick off the workflow from the orchestrator. Both the place_order and check_inventory services are launched at the same time by the orchestrator. place_order creates the order with all its information and marks its state as pending, waiting for the rest of the workflow to be processed.\n\nThe check_inventory service checks the inventory of items ordered and sends back a true or false response depending on whether the item is available or not. If any of the items ordered are not available, the web_msg_notification, email_notification, and sms_notification services are triggered.\n\nNow, here is the first challenge: all three notification services will require access to the CUSTOMER database in order to get the customer’s name, email address, phone number, and so on. But having one database for all three services creates undesired coupling that would violate the microservices autonomy principle. As we discussed earlier, we should instead create copies of that CUSTOMER database across all services to avoid service coupling. But how do we do that?\n\nFigure 2.9: Maintaining database consistency across MSA\n\n39\n\n40\n\nRefactoring Your Monolith\n\nThe CUSTOMER database is mainly managed by the register_customer service, which is triggered by the orchestrator through the user interface. To be able to maintain data consistency, and as shown in Figure 1.9, the orchestrator will need to simultaneously issue the same transaction on all copies of the CUSTOMER database whenever a record is edited, created, or deleted.\n\nThe orchestrator will need to wait for a success confirmation from all four services, register_customer, email_notification, sms_ notification, and web_ msg_notification, before the workflow is finalized. Now, what if, let’s say, updating the sms_ notification CUSTOMER database fails? You will end up with data inconsistency, which can be a serious issue later on.\n\nThat’s why all saga participants’ local transactions will need to have a set of compensating transactions to ensure a rollback in case of any failures in executing the transaction. In our example, the orchestrator will need to undo updates to the CUSTOMER database for all the other services.\n\nThe following diagram shows how a failure to update the CUSTOMER database should be rolled back using saga patterns.\n\nFigure 2.10: Compensating transactions for registering new customer information and placing an order\n\nSummary\n\nIn this chapter, we were able to go over the main steps of refactoring a monolith into an MSA, the steps necessary, the main things to consider, and the methodology of doing so. The simplified ABC-Monolith system was a good example; however, as systems get more complicated and the workflow gets more involved, data and process synchronization challenges start to arise.\n\nIn Chapter 1, we briefly discussed the challenges and the methodologies to be applied to overcome these challenges. In the next chapter, we will start applying the methodology to the ABC system we are trying to refactor.\n\nIn the next chapter, we will discuss how we can further maintain microservices’ autonomy and MSA stability and overcome some other operational challenges, and the role of API gateways, orchestrators, and microservice aggregators.\n\nSummary\n\n41",
      "page_number": 49
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 58-66)",
      "start_page": 58,
      "end_page": 66,
      "detection_method": "topic_boundary",
      "content": "3 Solving Common MSA Enterprise System Challenges\n\nIn the previous chapter, we learned how to decompose the monolith and refactor it into an MSA enterprise system. We built a simplified system as an example and then refactored the system to demonstrate this process. By doing so, we resolved some of the challenges of running a monolithic system. However, moving toward MSA introduces a completely different set of issues that need to be addressed.\n\nIn this chapter, we will discuss the main challenges introduced in MSA, how to address them, and what specific methodologies we need to apply to maintain the MSA system’s reliability, durability, and smooth operation.\n\nWe will cover the following topics in this chapter:\n\nMSA system isolation using an Anti-Corruption Layer (ACL)\n\nAPI gateways\n\nService catalogs and orchestrators\n\nMicroservices aggregators\n\nMicroservices circuit breaker\n\nGateways versus orchestrators versus aggregators\n\nABC-MSA enhancements\n\nMSA isolation using an ACL\n\nWhen adopting MSA in a brownfield, your migration from the monolithic system to MSA can either be done as a big bang migration or trickle migration.\n\n44\n\nSolving Common MSA Enterprise System Challenges\n\nIn big bang migration, you keep the old monolith system running as-is while building the entire MSA system. Once the MSA system has been completed, tested, and deployed, you can then completely switch to the new MSA system during your organization’s maintenance window, then decommission the old monolith. This type of migration, although usable for some scenarios, is usually not recommended in the case of large enterprise systems.\n\nSwitching users from the old to the new system should be done during the corporate’s off-peak hours or the corporate’s standard migration window. And the sudden switch of users can be a complex and cumbersome process due to the high potential for downtime, potential rollbacks, and risks of unexpected results when applying real traffic to the new system, all of which can impose large time constraints during the migration window.\n\nA common and safer migration approach in our case is trickle migration, where you perform a gradual shift from the old monolithic system to the new MSA system. A common way of doing that is by gradually extracting functions, services, and/or modules out of your monolith and moving them into standalone microservices as part of your new MSA. Gradually, we phase out the existing monolith’s functions and build an MSA system piece by piece.\n\nTo successfully perform a tickle migration, you need what’s called an Anti-Corruption Layer (ACL), which will act as an intermediate layer, a buffer, and a gateway between the old, messy monolith and your new clean MSA. The ACL layer will help temporarily integrate and glue the new extracted services back into the old system, to be able to communicate with old services, databases, and modules without fouling your new MSA system. You can see the ACL architecture in the following diagram:\n\nFigure 3.1: Anti-Corruption Layer (ACL)\n\nThe ACL lifespan is as long as the monolith system’s lifespan. Once the migration has been completed and the monolith has been decommissioned, the ACL will no longer be needed. Therefore, it is recommended that you have the ACL written either as a standalone service or as part of the monolith.\n\nMSA isolation using an ACL\n\nThe ACL has three main components:\n\nThe API component, which allows the ACL to communicate with the MSA system using the same language as the MSA system.\n\nThe ACL Facade, which is the interface that enables the ACL to talk to the monolith using the monolith’s language(s).\n\nThere are two options where the Facade can be placed; one is shown in Figure 3.1, where the Facade has been placed as part of the new standalone ACL microservice. The other option is placing the Facade as a component within the monolith, as shown in Figure 3.2:\n\nFigure 3.2: The Facade’s two implementation options\n\nThe choice would depend on the architects’ and developers’ preference regarding whether they would like to add more glue code inside the monolith itself, or completely isolate any development effort away from the monolith.\n\nThe ACL Adapter, which is a part of the ACL, works between the ACL’s northbound API and the Facade. The main function of the Adapter is to translate between the monolith and the MSA using the ACL Translator interface, as shown in Figure 3.1.\n\nACL is only needed when a trickle migration is adopted. There is no need for implementing an ACL in the big bang migration case. And since, between both migration styles, there are resources to be consumed, as well as advantages, risks, and tradeoffs, MSA project stakeholders will need to decide on which style is more suitable for the project and the organization.\n\nWhether an ACL is implemented or not, MSA systems would still need a component to act as an interface between the MSA system and external clients. Using an API gateway between the MSA and external API calls is considered a good MSA design practice. The next section discusses the roles of the API gateway in MSA systems, and the tradeoffs of having to adopt an API gateway in MSA design.\n\n45\n\n46\n\nSolving Common MSA Enterprise System Challenges\n\nUsing an API gateway\n\nAs we explained in Chapter 1, microservices can communicate directly with each other without the need for a centralized manager. As the MSA system becomes more mature, the number of microservices gradually increases, and direct communication between microservices can become a large overhead – especially with calls that need multiple round trips between the API consumer and the API provider.\n\nWith the microservices’ autonomy principle, each microservice can use its technology stack and may communicate with a different API contract than the other microservices in the same MSA system. One microservice, for example, may only understand a RESTful API with a JSON data structure, while others may only communicate with Thrift or Avro.\n\nMoreover, the location (IP and listening port) of the active instantiated microservices change dynamically within the MSA system. Therefore, the system will need to have a mechanism to identify the location at which the API consumer can point its calls towards.\n\nThere are also situations where you need to tie in your MSA system to legacy systems such as the mainframe, AS400, and more.\n\nAll of the previous situations require code to be embedded in each microservice in the MSA system. This code will help the microservices understand the legacy and non-REST communication patterns, discover the network location of other microservices in the system, and understand each microservice’s needs in general. Now, how independent and portable would such a set of microservices be?\n\nA better approach to addressing the preceding challenges is to use an API gateway where all system services talk to each other through that gateway. The API gateway receives API calls from the system’s API consumers, then maps the data received into a data structure and a protocol that API providers can understand and process:\n\nUsing an API gateway\n\nFigure 3.3: Moving from services direct communication to API gateway communication\n\n47\n\n48\n\nSolving Common MSA Enterprise System Challenges\n\nWith the API gateway, we significantly reduce direct 1-to-1 communication between services. Moreover, we offload the system’s microservices from having multiple translations, mapping code, and Authentication-Authorization-Accounting (AAA) tasks. Rather, we move the responsibility of discovering the location of microservices from the client to the API gateway, which, in turn, further reduces the code overhead and renders microservices as light and independent as possible.\n\nImportant note The MSA system’s availability is as good as the API gateway’s availability. Therefore, it is necessary for the API gateway to be developed, deployed, and managed as a high-performance and highly available mission-critical service.\n\nThe API gateway can be deployed as a standalone service that’s part of the MSA system. The main functions of the gateway are as follows:\n\nMinimize the API calls between microservices, which makes MSA inter-service communication much more efficient.\n\nMinimize API dependencies and breaking changes. In an MSA system with no API gateway, if, for whatever reason, one of the API providers changes its API, a breaking change will likely happen. This means we will need to create a change in every microservice communicating with the API provider. By using an API gateway, a change in the API provider will be limited to the API gateway only, to match the API contract between the provider and the consumers.\n\nTranslate and map between API contracts, which offloads the microservices from embedding translation code in their core function.\n\nRun a service discovery mechanism and offload clients from running that function.\n\nAct as the entry point to the MSA system’s external client calls.\n\nLoad-balance API calls across the different instances of high-availability microservices, and offload microservices in high-traffic situations.\n\nOffer better security by throttling sudden increases in API calls during Distributed Denial of Service (DDoS) and similar attacks.\n\nAuthenticate and authorize users to access different components in the MSA system.\n\nProvide comprehensive analytics to provide deep insights into system metrics and logs, which can help further enhance system design and performance.\n\nService catalogs and orchestrators\n\nDespite all these advantages and functions of the API gateway, there are still some drawbacks to having an API gateway in your MSA system:\n\nThe most obvious is complexity. The more protocols and API contract data structures we have in the system, the more complex the gateway becomes.\n\nThe MSA’s operation is highly dependent on the API gateway’s performance and availability, which may create an unwanted system performance bottleneck.\n\n\n\nIntroducing an additional intermediary component such as an API gateway in the path of intra-microservice communication increases service response time. And with chatty services, the increased response time can become considerable.\n\nEven with all the functions the API gateway provides, we still need a way to map each user request to specific tasks that the MSA system would need to run to fulfill that request. In the next section, we’ll discuss how the MSA system tasks are mapped to specific user services, and how these tasks are orchestrated in MSA.\n\nService catalogs and orchestrators\n\nOrchestration is one of the most commonly used communication patterns in MSA systems. We briefly discussed this concept in Chapter 1. In this section, we will dive into more details about orchestration.\n\nDetermining the most appropriate communication pattern between the different microservices depends on many factors. Among the factors that will help determine whether choreography or orchestration is the most suited communication pattern for the system, you must consider the number of microservices you have in the system, the level of interactions between the different microservices, the business logic itself, how dynamic business requirements change, and how dynamic system updates are.\n\nOrchestrators act as the central managers controlling all communication between the system’s microservices. They usually interact with the users through a dashboard interface that contains all service catalogs. The Service Catalog is a set of services the MSA system offers to users. Each service in the catalog is linked to a set of workflows. Workflows are the actions the orchestrator will trigger and coordinate between the system’s microservices to deliver the service the user selected from the catalog:\n\n49\n\n50\n\nSolving Common MSA Enterprise System Challenges\n\nFigure 3.4: Orchestrators in MSA\n\nThe orchestrator’s functions can be extended beyond managing workflows. Orchestrators can also manage the entire life cycle of microservices; this involves provisioning and deploying microservices, configuring the microservice, and performing upgrades, updates, monitoring, performance audits, and shutdowns when needed.\n\nImportant note The orchestrator is the main brain of the MSA system, and it is imperative to have the orchestrator deployed and managed as a high-performing and mission-critical component of the MSA system.\n\nSome of the benefits of running an orchestrator in MSA include the following:\n\nYou have a centralized management platform as a single source of truth for all of your workflows. Thus, you can build complex workflows in a complex MSA without having to worry about how many microservices you have and how they can scale.\n\nAs in the API gateway, you can tie in your legacy systems or part of your old monolith and completely isolate your microservices from having to couple with any other system component. This saves a lot of effort having to build code into the independent microservices and tremendously helps in scaling your MSA.\n\nMicroservices aggregators\n\nMicroservices are visible to the orchestrator and hence can be completely managed, audited, and monitored by the orchestrator. This can produce very helpful and insightful analytics that can further enhance the MSA system’s supportability and operations.\n\nThe orchestrator’s visibility can help in troubleshooting any operational issues and identifying problems quickly.\n\nOrchestrators can automatically detect and self-resolute some of the operational problems. Orchestrators can, for example, detect resource starvation and reroute requests to a backup microservice. Orchestrators can automatically vertically or horizontally scale a particular microservice when a problem is detected. Orchestrators can also try to automatically restart the service if the service is not responding.\n\nThe orchestrator solves many of the MSA operational problems, including some of the data synchronization challenges. When scaling the system, however, data synchronization and data consistency become a big challenge for the orchestrator to address by itself. Microservices aggregators help address data synchronization issues when the MSA system scales. In the next section, we will discuss what the aggregator pattern is, what it is used for, and how it works.\n\nMicroservices aggregators\n\nIn Chapter 2, we had to copy some schemas across multiple microservices and use saga patterns through the orchestrator to keep data consistent and preserve the microservice’s autonomy. This solution may be viable in a situation if you have a limited number of microservices within the MSA system. In a large number of microservices systems, copying schemas across different microservices to maintain the synchronization of the individual microservices database doesn’t scale well and can severely impact the system’s overall performance.\n\nConsider an MSA system with 100 microservices and copy schemas across about 20 of those microservices to maintain the microservice’s autonomy. Each time any part of any of the schema’s data is updated, the orchestrator will have to sync those 20 schemas.\n\nMoreover, even if we have all 100 microservices perfectly autonomous, what if one of the user’s operations needs to gather information from those 20 microservices? The orchestrator will have to issue at least 20 different API calls to 20 different microservices to get the information the user is looking for. Not to mention that some of these 20 microservices may need to exchange multiple API calls to send the result back to the user.\n\nTo put things into perspective, let’s revisit the ABC-MSA system we built in Chapter 2. We have the order, product, and inventory microservices. The product microservice is for managing product information, the inventory microservice is for managing the product inventory, and the order microservice is for placing and managing orders.\n\n51",
      "page_number": 58
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 67-76)",
      "start_page": 67,
      "end_page": 76,
      "detection_method": "topic_boundary",
      "content": "52\n\nSolving Common MSA Enterprise System Challenges\n\nLet’s assume we’re in a situation where a sales analyst is generating a report to check a product’s average quantity purchased per order and the product’s inventory level at the time at which the order was placed, as shown in the following diagram:\n\nFigure 3.5: A sample product order report\n\nThe orchestrator will have to send at least three API calls, one to each of the order, product, and inventory microservices, as shown in the following diagram:\n\nFigure 3.6: A user operation spanning multiple microservices\n\nMicroservices aggregators\n\nTo minimize dependencies and response time, a better approach for this particular situation is to use an aggregator. This aggregator will collect the different pieces of data from all order, product, and inventory microservices and update its database with the combined information.\n\nThe API gateway or consumer will only need to send one API call to the aggregator to get all the information it needs. The number of API calls is minimized and the overall response time is greatly reduced, especially in cases where the information required is distributed across a large number of microservices:\n\nFigure 3.7: An aggregator communication pattern\n\nThe aggregator communication pattern reduces the number of API calls users could trigger in various operational requests and further enhances the data synchronization’s design and performance, as well as the overall system performance, especially in high-latency networks.\n\nNow, we know the roles of the API gateway, the orchestrator, and the aggregator. In the next section, we will discuss how all these three components interact with each other in the MSA.\n\n53\n\n54\n\nSolving Common MSA Enterprise System Challenges\n\nGateways versus orchestrators versus aggregators\n\nFrom what we have described so far, there are some overlapping functions between the API gateway, the orchestrator, and the aggregator. In this section, we will answer some of the fundamental questions regarding how all three components interact in a single MSA system:\n\nHow do these three MSA components work together?\n\nCan an API gateway perform the aggregator and orchestrator functions?\n\nWhat are the best practices for deploying all these communication patterns in our MSA?\n\nFirst of all, theoretically speaking, you can have clients interact with the MSA microservices directly without an API gateway. However, this would not be a good practice. By having no gateway in your MSA system, you would need to have most of the gateway functions implemented within each microservice you have in the system.\n\nTo keep microservices as light and autonomous as possible, it is highly recommended to have an API gateway in your MSA system. The API gateway will handle all ingress and egress API traffic from the different types of clients. Clients can be a web dashboard, a mobile application, a tablet, a third-party integration system, and so on.\n\nWhether you add an aggregator or not will highly depend on your business logic and system design. You will only need aggregators if you have client use cases where requests need to span across multiple microservices at the backend.\n\nAggregators can be implemented as part of the gateway itself; however, the best practice is to add an aggregator only whenever it is needed and make it an independent standalone microservice.\n\nAn MSA system can have multiple aggregators, each with specific business logic, and fulfilling a specific collection of data from a different set of microservices.\n\nSimilarly, orchestration patterns can also be implemented in the API gateway; however, you need the API gateway to focus on doing the main functions it was created for and leave orchestration tasks to the orchestrator. The orchestrator is also best deployed as a standalone microservice:\n\nGateways versus orchestrators versus aggregators\n\nFigure 3.8: MSA high-level architecture\n\nThe preceding diagram shows the high-level architecture of having all these components working together within the MSA system. Clients always interact with the API gateway, and the API gateway will route the request to the appropriate service within the MSA system.\n\nClient API calls are routed to the appropriate microservice based on the API’s configuration. If the client request is something that is fulfilled by communicating with a single microservice, then the gateway will send that request directly to the microservice. API calls that span multiple microservices and are assigned to a specific aggregator in the system will be forwarded to that particular aggregator. Finally, for API requests that invoke specific workflows, the API gateway will forward those to the orchestrator.\n\nIn this section, we learned how the API gateway, the orchestrator, and the aggregator coexist in the same MSA system. In the next section, we will try to apply the concepts of all these three communication patterns to our previously developed ABC-MSA system.\n\n55\n\n56\n\nSolving Common MSA Enterprise System Challenges\n\nMicroservices circuit breaker\n\nAnother challenge in MSA systems is the stability and assurance of workflow execution. Saga patterns, which we discussed in Chapter 1, are used to ensure that all transactions within a specific workflow are either all successfully executed, or all fail. But is that enough to ensure the reliable execution of microservices?\n\nLet’s consider a scenario where the called microservice is too slow in responding to API calls. Requests get successfully executed, but the microservice’s response times out. The microservice consumer, in turn, may assume an execution failure, and accordingly repeat the operation, which can be very problematic.\n\nAs shown in the following diagram, when a response timeout takes place in the Payment microservice, the Payment microservice will process the payment, but the microservice consumer will assume that the payment has not been processed and may automatically (or upon user request) retry the process. This behavior will cause the payment to be processed multiple times, resulting in multiple charges for the same order, or for the order to be placed multiple times:\n\nFigure 3.9: Payment microservice with too slow of a response time\n\nIn MSA, when microservices get instantiated, they start with limited resources and threads to avoid one particular microservice from hogging all the system’s resources.\n\nWith system resources in mind, consider another scenario, as shown in Figure 3.10, where the Inventory microservice is part of a service workflow and the Payment microservice is neither processing nor responding to API calls for whatever reason. In this case, both the Order and Payment microservices will keep waiting for confirmation from the inventory before they start releasing their resources.\n\nMicroservices circuit breaker\n\nWith the Inventory microservice timing out requests, and under a system heavy load or high order volume, requests start to pile up for the Order and Payment microservices. Eventually, both the Order and Payment microservices start to run out of resources and become unable to respond to requests:\n\nFigure 3.10: Inventory microservice is down\n\nSimilar scenarios in MSA can result in a domino effect, causing a cascading failure to multiple microservices, which, in turn, causes an entire system failure.\n\nA microservice circuit breaker is used to prevent a system cascading failure from happening. A circuit breaker monitors microservice performance using real traffic metrics. It analyses parameters such as response time and successful response rate and then determines the health of the microservice in real time. Should the microservice become unhealthy, the circuit breaker immediately starts responding to the microservice consumers with an error.\n\nA circuit breaker does not prevent the microservice being monitored from failing; rather, it averts a cascading failure from taking place:\n\nFigure 3.11: The Inventory microservice with an inline circuit breaker\n\nWhen the circuit breaker assumes a microservice is unhealthy, it still needs to monitor and evaluate the microservice’s operational performance. The circuit breaker switches to a half-open state, where it only allows a small portion of requests to pass through to the microservice being monitored. Once the circuit breaker detects a healthy microservice response, the circuit breaker switches its state back to a closed state, where API traffic flows back normally to the microservice.\n\nCircuit breakers are not needed for every single microservice in the MSA system. We only need to deploy a circuit breaker on microservices that can cause a cascading failure. Architects will need to study and determine which microservices need circuit breaker protection.\n\n57\n\n58\n\nSolving Common MSA Enterprise System Challenges\n\nA circuit breaker can be deployed as a standalone microservice or be part of the API gateway. Whether circuit breakers are implemented as standalone microservices or part of the API gateway highly depends on the system’s business and operational requirements, as well as the patterns adopted in the architecture itself.\n\nThe circuit breaker pattern, as well as the orchestrator, aggregator, ACL, and API gateway, are all enhancements architects that can be applied to the MSA system for better reliability, resilience, and overall performance. In the next section, we will learn how to apply each of the patterns discussed here to our ABC-MSA system.\n\nABC-MSA enhancements\n\nIn Chapter 2, we refactored our ABC-Monolith into a simple ABC-MSA. The ABC-MSA we designed in Chapter 2 lacked many of the enhancements we are considering in this chapter. It is time to take what we have learned in this chapter and apply that to the ABC-MSA system to enhance its design and operations.\n\nFirst of all, in the ABC-MSA from Chapter 2, the orchestrator was doing both the API gateway functions and the orchestration function. So far, we have learned that combining both the gateway and orchestration functions in one service is not the best option. Therefore, we will add to our ABC-MSA system an API gateway dedicated to ingress and egress API calls, and other API gateway functions we discussed earlier in this chapter, such as authentication, authorization, audit, monitoring, and so on.\n\nThe API gateway will run as a separate standalone microservice serving direct client requests, including the system dashboard and user frontend. The orchestrator will also run as a standalone microservice serving the MSA’s workflows.\n\nThe aggregator(s) will depend on the use cases where multiple ABC-MSA microservices are used to fulfill the user requests.\n\nA simple use case for using an aggregator would be a user checking for an order’s shipping status. The status should include the order information, the products included in the order, and the shipping status of that order.\n\nTo show all this information to the user, we will need to pull information from three different microservices: Order Management, Product Management, and Shipping Management. We will deploy an aggregator as a standalone microservice to pull the data from all these microservices and make them available for user API consumption:\n\nABC-MSA enhancements\n\nFigure 3.12: The enhanced ABC-MSA architecture\n\nIn the preceding diagram, we added the Management and Orchestration layer as part of the ABC-MSA system. This layer will manage the orchestration workflows and the microservice’s life cycle, including, installation, configuration, instantiation, updates, upgrades, and shutdowns.\n\nWe will also need an ACL to be active during the transition from the ABC-Monolith to the ABC-MSA. The ACL will act as a buffer between both systems to maintain the neatness of the architecture and its operations. Once all the ABC-Monolith functions have been redeployed into the ABC-MSA, both the old ABC-Monolith and the ACL can be decommissioned.\n\n59\n\n60\n\nSolving Common MSA Enterprise System Challenges\n\nSummary\n\nIn this chapter, we discussed the different components of the MSA that can be introduced to maintain the system’s stability and enhance its performance.\n\nWe discussed how to use the ACL to protect our new MSA during its transition from the old monolithic system. Then, we covered the roles and functions of the API gateway, the aggregator, and the orchestrator. We also covered some of the drawbacks you may experience when adopting the various communication patterns in MSA.\n\nFinally, we redesigned our ABC-MSA to showcase how these different components can all function together in a typical MSA.\n\nChapter 1 to Chapter 3 covered the basics of the MSA. In the next chapter, we will start discussing, with hands-on examples, key machine learning and deep learning algorithms used in MSA enterprise systems, and go over some programming and tool examples of building machine learning and deep learning algorithms.\n\nPart 2: Overview of Machine Learning Algorithms and Applications\n\nIn this part, we will shift our focus to machine learning. We will learn about the different concepts of machine learning algorithms and how to design and build a machine learning system, maintain the model, and apply machine learning to an intelligent enterprise MSA.\n\nWe will first learn the fundamentals when it comes to identifying the difference between different machine learning models and their use cases. Once we’ve covered the basics, we will start to learn how to design a machine learning system pipeline. Once we have established a machine learning system pipeline, we will learn what data shifts are, how they can impact our system, and how we can identify and address them. Finally, having gone through all the basics, we will start to explore the different use cases for building our very own intelligent enterprise MSA.\n\nBy the end of Part 2, we will have a basic understanding of machine learning and the different algorithms, how to build and maintain a machine learning system, and, finally, the different use cases in which we can use machine learning for our intelligent enterprise MSA.\n\nThis part comprises the following chapters:\n\nChapter 4, Key Machine Learning Algorithms and Concepts\n\nChapter 5, Machine Learning System Design\n\nChapter 6, Stabilizing the Machine Learning System\n\nChapter 7, How Machine Learning and Deep Learning Help in MSA Enterprise Systems",
      "page_number": 67
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 77-88)",
      "start_page": 77,
      "end_page": 88,
      "detection_method": "topic_boundary",
      "content": "4 Key Machine Learning Algorithms and Concepts\n\nIn the previous chapters, we explored the different concepts of MSA and the role it plays when creating enterprise systems.\n\nIn the coming chapters, we will begin to shift our focus from learning about MSA concepts to learning about key machine learning concepts. We will also learn about the different libraries and packages being used in machine learning models using Python.\n\nWe will cover the following areas in this chapter:\n\nThe differences between artificial intelligence, machine learning, and deep learning\n\nCommon deep learning packages and libraries used in Python\n\nBuilding regression models\n\nBuilding multiclass classification\n\nText sentiment analysis and topic modeling\n\nPattern analysis and forecasting using machine learning\n\nBuilding enhanced models using deep learning\n\nThe differences between artificial intelligence, machine learning, and deep learning\n\nDespite the recent rise in popularity of artificial intelligence and machine learning, the field of artificial intelligence has been around since the 1960s. With different sub-fields emerging, it is important to be able to differentiate between them and understand them and what they entail.\n\n64\n\nKey Machine Learning Algorithms and Concepts\n\nTo start, artificial intelligence is the overarching field that encompasses all the sub-fields we see today, such as machine learning, deep learning, and more. Any system that perceives or receives information from its environment and carries out an action to maximize the reward or achieve its goal is considered to be an artificially intelligent machine.\n\nThis is commonly used today when it comes to robotics. Most of our machines are designed so that they can capture data using their sensors, such as cameras, sonars, or gyroscopes, and use the data captured to respond to a particular task most efficiently. This concept is very similar to how humans function. We use our senses to “capture” information from our environment and based on the information we receive, we carry out certain actions.\n\nArtificial intelligence is an expansive field, but it can be broken into different sub-fields, one we commonly know today as machine learning. What makes machine learning unique is that this field works on creating systems or machines that can continually learn and improve their model without explicitly being programmed.\n\nMachine learning does this by collecting data, also known as training data, and trying to find patterns in the data to make accurate predictions without being programmed to do so. There are many different methods used in machine learning to learn the data and the methods are tailored to the different problems we encounter.\n\nFigure 4.1: Different fields in artificial intelligence\n\nThe differences between artificial intelligence, machine learning, and deep learning\n\nMachine learning problems can be broken down into three different tasks: supervised learning, unsupervised learning, and reinforcement learning. For now, we will focus on supervised and unsupervised learning. This distinction is based on the training data that we have. Supervised learning is when we have the input data and the expected output for the particular set of data, which is also called the label. Unsupervised learning, on the other hand, only consists of the input without an expected output.\n\nSupervised learning works by understanding the relationship between the input and output data. One common example of supervised learning is predicting the price of a home in a certain city. We can collect data on existing homes by capturing their specifications and their current prices and then learn the pattern between the characteristics of these homes and their prices. We can then take a home, not in our training set, and test our model by inputting the features of the house into our program and have the model predict the price of the home.\n\nUnsupervised learning works by learning about the structure of the data either using grouping or clustering methods. This method is commonly used for marketing purposes. For example, a store wants to cluster its customers into different groups so that it can efficiently tailor its products to different demographics. It can capture the purchase history of its customers, use that data to learn about purchasing patterns, and suggest certain items or goods that would interest them, thus maximizing its revenue.\n\nBefore we can understand deep learning, which is a sub-field of machine learning, we must first understand what Artificial Neural Networks (ANNs) are. Taking inspiration from neurons in a brain, ANNs are models that comprise a network of fully connected nodes, also known as artificial neurons. They contain a set of inputs, hidden layers connecting the neurons, and also an output node. Each neuron has an input and output, which can be propagated throughout the network. In order to calculate the output of a neuron, we take the weighted sum of all the inputs, multiply it by the weight of the neuron, and then usually add a bias term.\n\nWe continue to perform these actions until we reach the last layer, which is the output neuron. We perform a nonlinear activation function, such as a sigmoid function, to give us the final prediction. We then take the predicted output value and input it in a cost function. This function tells us how well our network is learning. We take this value and backpropagate through our layers back to the first layer, adjusting the weights of the neurons depending on how our network is performing. With this, we can create strong models that can perform tasks such as handwriting recognition, game-playing AI, and much more.\n\nImportant Note A program is considered to be a machine learning model if it can take input data and learn the patterns to make predictions without being explicitly programmed to.\n\n65\n\n66\n\nKey Machine Learning Algorithms and Concepts\n\nFigure 4.2: An ANN\n\nWhile ANNs are capable of performing many tasks, there are significant downsides that limit their use in today’s market:\n\n\n\nIt can be difficult to understand how the model performs. As you add more hidden layers to the network, it becomes complicated to try and debug the network.\n\nTraining the model takes a long time, especially with copious amounts of training data, and can drain hardware resources, as it is difficult to perform all these mathematical operations on a CPU.\n\nThe biggest issue with ANNs is overfitting. As we add more hidden layers, there is a point at which the weights assigned to the neurons will be heavily tailored to our training data. This makes our network perform very poorly when we try to test it with data it has not seen before.\n\nThis is where deep learning comes into play. Deep learning can be categorized by these key features:\n\nThe hierarchical composition of layers: Rather than having only fully connected layers in a network, we can create and combine multiple different layers, consisting of non-linear and linear transformations. These different layers play a role in extracting key features in the data that would be otherwise difficult to find in an ANN.\n\nEnd-to-end learning: The network starts with a method called feature extraction. It looks at the data and finds a way to group redundant information and identify the important features of the data. The network then uses these features to train and predict or classify using fully connected layers.\n\nCommon deep learning and machine learning libraries used in Python\n\nA distributed representation of neurons: With feature extraction, the network can group neurons to encode a bigger feature of the data. Unlike in an ANN, no single neuron encodes everything. This allows the model to reduce the number of parameters it has to learn while still retaining the key elements in the data.\n\nDeep learning is prevalent in computer vision. Due to the advances in the technology of capturing photos and videos, it has become very difficult for ANNs to learn and perform well when it comes to image detection. For starters, when we use an image to train our model, we have to look at every pixel in an image as an input to the model. So, for an image of resolution 256x256, we would be looking at over 65,000 input parameters. Depending on the number of neurons in your fully connected layer, you could be looking at millions of parameters. With the sheer number of parameters, this will be bound to cause overfitting and could take days of training.\n\nWith deep learning, we can create a group of layers called Convolutional Neural Networks (CNNs). These layers are responsible for reducing the number of parameters that we have to learn in our model while still retaining the key features in our data. With these additions, we can learn how to extract certain features and use those to train our model to predict with efficiency and accuracy.\n\nFigure 4.3: A CNN\n\nIn the next section, we will be looking at the different Python libraries used for machine learning and deep learning and their different use cases.\n\nCommon deep learning and machine learning libraries used in Python\n\nNow that we have gone over the concepts of artificial intelligence and machine learning, we can start looking at the programming aspect of implementing these concepts. Many programming languages are used today when it comes to creating machine learning models. Commonly used are MATLAB, R, and Python. Among them, Python has grown to be the most popular programming language in machine learning due to its versatility as a programming language and the extensive number of libraries, which makes creating machine learning models easier. In this section, we will be going over the most commonly used libraries today.\n\n67\n\n68\n\nKey Machine Learning Algorithms and Concepts\n\nNumPy\n\nNumPy is an essential package when it comes to building machine learning models in Python. You will be mostly working with large, multi-dimensional matrices when building your models. Most of the effort is spent on transforming, splicing, and performing advanced mathematical operations on matrices, and NumPy provides the tools need to perform these actions while retaining speed and efficiency.\n\nFor more information on the different APIs that NumPy offers, you can visit the documentation on its website: https://numpy.org/doc/stable/reference/index.html.\n\nHere, we will look at the example code. This section shows us how we can initialize a NumPy array. In this example, we will create a 3x3 matrix with initialized values of 1 through 9:\n\nimport numpy as np # creates a 3x3 numpy array arr = np.array([[1,2,3],[4,5,6],[7, 8, 9]])\n\nHere, we will print out the results:\n\nprint(arr) [[1 2 3] [4 5 6] [7 8 9]]\n\nNow, we can show how we can splice and extract certain elements from our array.\n\nThis line of code allows us to pull all the values that are in the second column of our array. Keep in mind that in NumPy our arrays and lists are zero-indexed, meaning that the zero index refers to the first element in the array or list:\n\nprint(arr[:,1]) # print the second column of the array [2 5 8]\n\nIn this example, we extract all of the values in the row of 2 in our array:\n\nprint(arr[2,:]) # print the last row of the array [7 8 9]\n\nAnother useful aspect of NumPy arrays is that we can apply mathematical functions to our matrices without having to implement code to perform basic functions. Not only is this much easier but it also is much faster and more efficient.\n\nCommon deep learning and machine learning libraries used in Python\n\nIn this example, we simply perform a multiplication between our matrix and a scalar value of -1:\n\nprint(np.multiply(arr, -1)) # multiplies every element in the array by -1 [[-1 -2 -3] [-4 -5 -6] [-7 -8 -9]]\n\nMatplotlib\n\nIn order to see how your model is learning and performing, it is important to be able to visualize your results and your data. Matplotlib offers a simple way to graph your data, from something as simple as a line plot to more advanced plots, such as contour plots and 3D plots. What makes this library so popular is its seamlessness when working with NumPy.\n\nFor more information on their different functions, you can visit their website: https://matplotlib. org/stable/index.html.\n\nIn this example, we will create a simple line graph. We first initialize two arrays, x and y, and both arrays will contain values from 0 to 9. Then, using Matplotlib’s APIs, we can plot and show our simple graph:\n\nimport matplotlib.pyplot as plt import numpy as np x = np.arange(10) # creates an array from 0-9 y = np.arange(10) plt.plot(x,y) plt.show()\n\nFigure 4.4: A simple line graph using Matplotlib\n\n69\n\n70\n\nKey Machine Learning Algorithms and Concepts\n\nPandas\n\nWith the recent trend of storing data in CSV files, Pandas has become a staple in the Python community due to its ease and versatility. Pandas is commonly used for data analysis. It stores the data in a tabular format, and it provides users with simple functions to pre-process and manipulate the data to fit their needs. It has also become useful when dealing with time-series data, which is helpful when building forecasting models.\n\nFor more information on the different functions, you can view the documentation on its website: https://pandas.pydata.org/docs/.\n\nIn this example, first, we will simply initialize a DataFrame. This is the data structure used to store our data in a two-dimensional tabular format in Pandas. Usually, we store the data from the files we read from, but it is also possible to create a DataFrame with your own data:\n\nimport pandas as pd data = { \"Number of Bedrooms\": [5, 4, 2, 3], \"Year Build\": [2019, 2017, 2010, 2015], \"Size(Sq ft.)\": [14560, 12487, 9882, 10110], \"Has Garage\": [\"Yes\", \"Yes\", \"No\", \"Yes\"], \"Price\": [305000, 275600, 175000, 235000], } df = pd.DataFrame(data) print(df)\n\nFigure 4.5: Output of our DataFrame\n\nAs with NumPy, we can extract certain columns and rows of our DataFrame. In this code, we can view the first row of our DataFrame:\n\nprint(df.iloc[0]) # view the first entry in the table\n\nCommon deep learning and machine learning libraries used in Python\n\nFigure 4.6: Output of the first row of our DataFrame\n\nWith Pandas, we can also extract certain columns from our DataFrame by using the name of the column rather than the index:\n\nprint(df[\"Price\"]) # print all the values in the Prices column\n\nFigure 4.7: Output of all the values in the Price column\n\nTensorFlow and Keras\n\nTensorFlow and Keras are the foundation when it comes to building deep learning models. While both can be used individually, Keras is used as an interface for the TensorFlow framework, allowing users to easily create powerful deep learning models.\n\nTensorFlow, created by Google, functions as the backend when creating machine learning models. It works by creating static data flow graphs that specify how the data moves through the deep learning pipeline. The graph contains nodes and edges, where the nodes represent mathematical operations. It passes this data using multidimensional arrays known as Tensors.\n\nKeras, later to be integrated with TensorFlow, can be viewed as the frontend for designing deep learning models. It was implemented to be user-friendly by allowing users to focus on designing their neural network models without having to deal with a complicated backend. It is similar to object-oriented programming, as it replicates the style of creating objects. Users can freely add different types of layers, activation functions, and more. They can even use prebuilt neural networks for easy training and testing.\n\nIn the following example code, we can see how we can create a simple, hidden two-layer neural network. This block of code allows us to initialize a Sequential model, which consists of a simple stack of layers:\n\nimport tensorflow as tf from tensorflow import keras\n\n71\n\n72\n\nKey Machine Learning Algorithms and Concepts\n\nfrom tensorflow.keras import layers from keras.models import Sequential model = Sequential() model.add(Flatten(input_shape=[256,256]))\n\nDepending on our application, we can add multiple layers with different configurations, such as the number of nodes, the activation functions, and the kernel regularizer:\n\n#Adding First Hidden Layer model.add(tf.keras.layers.Dense(units=6,kernel_ regularizer='l2',activation=\"leaky_relu\")) #Adding Second Hidden Layer model.add(tf.keras.layers.Dense(units=1,kernel_ regularizer='l2',activation=\"leaky_relu\")) #Adding Output Layer model.add(tf.keras.layers.Dense(units=1,kernel_ regularizer='l2',activation=\"sigmoid\"))\n\nFinally, we can compile our model, which essentially gathers all the different layers and combines them into one simple neural network:\n\n#Compiling ANN model.compile(optimizer='sgd',loss=\"binary_ crossentropy\",metrics=['accuracy'])\n\nPyTorch\n\nPyTorch is another machine learning framework created by Meta, formally known as Facebook. Much like Keras/TensorFlow, it allows the users to create machine learning models. The framework is well suited to Natural Language Processing (NLP) and computer vision problems but can be tailored to most applications. What makes PyTorch unique is its dynamic computational graph. It has a module called Autograd, which allows you to perform automatic differentiation dynamically, compared to TensorFlow, in which it is static. Also, PyTorch is more in line with the Python language, which makes it easier to understand and takes advantage of useful features of Python such as parallel programming. For more information, visit the documentation on their website: https://pytorch.org/docs/ stable/index.html.\n\nIn this section of code, we can create a simple single-layer neural network. Similar to Keras, we can initialize a Sequential model and add layers depending on our needs:\n\nimport torch model = torch.nn.Sequential( # create a single layer Neural\n\nCommon deep learning and machine learning libraries used in Python\n\nNetwork torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1) ) loss = torch.nn.MSELoss(reduction='sum')\n\nSciPy\n\nThis library is designed for scientific computing. There are many built-in functions and methods used for linear algebra, optimization, and integration, which are commonly used in machine learning. This library is useful when trying to compute certain statistics and transformations as you build your machine learning model. For more information on the different functions it provides, view the documentation on its website: https://docs.scipy.org/doc/scipy/.\n\nIn this example code, we can create a 3x3 array using NumPy and then we can use SciPy to calculate the determinate:\n\nimport numpy as np from scipy import linalg a = np.array([[1,4,2], [3,9,7], [8,5,6]]) print(linalg.det(a)) # calculate the matrix determinate 57.0\n\nscikit-learn\n\nscikit-learn is a machine learning library that is an extension of SciPy and is built using NumPy and Matplotlib. It contains many prebuilt machine learning models, such as random forests, K-means, and support vector machines. For more information on the different APIs it provides, view the documentation by visiting its website: https://scikit-learn.org/stable/user_guide.html.\n\nIn the following example, we will use an example dataset provided by scikit-learn and build a simple logistic regression model. First, we import all the required libraries and then load the Iris dataset provided by scikit-learn. We can use a handy API from scikit-learn to split our data into training and test datasets:\n\nfrom sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import numpy as np # Load the iris dataset X, y = datasets.load_iris(return_X_y=True)\n\n73\n\n74\n\nKey Machine Learning Algorithms and Concepts\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_ size=0.20, random_state=1) Create linear regression object\n\nWe can then initialize our logistic regression model and simply run the fit function with our training data to train the model. Once we train our model, we can use it to make predictions and then measure its accuracy:\n\n# Create Logistic Regression model model = LogisticRegression() # Train the model using the training sets model.fit(X_train, y_train) # Make predictions using the testing set y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred))\n\nIn the next few sections, we will start looking at the different models we can build using these libraries. We will understand what makes these models unique, how they are structured, and for what purposes and applications they can best serve our needs.\n\nBuilding regression models\n\nFirst, we will look at regression models. Regression models or regression analysis are modeling techniques used to find the relationship between independent and dependent variables. The output of a regression model is typically a continuous value, also known as a quantitative variable. Some common examples are predicting the price of a home based on its features or predicting the sales of a certain product in a new store based on previous sales information.\n\nBefore building a regression model, we must first understand the data and how it is structured. The majority of regression models involve supervised learning. This consists of features and an output variable, known as a label. This will help the model by adjusting the weights to better fit the data we have observed so far. We usually denote our features as X and our labels as Y to help us understand the mathematical models used to solve regression models:",
      "page_number": 77
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 89-96)",
      "start_page": 89,
      "end_page": 96,
      "detection_method": "topic_boundary",
      "content": "Building regression models\n\nFigure 4.8: Example of a supervised learning data structure\n\nTypically, our data is split into two subsets, training and testing sets. The training dataset usually consists of between 70-80% of the original data and the testing dataset contains the rest. This is to allow the model to learn on the training dataset and validate its result on the testing dataset to show its performance. From the results, we can infer how our model is performing on the dataset.\n\nFor a linear regression model to perform effectively, our data must be structured linearly. The model uses this formula to train on and learn about the data:\n\n𝑦𝑦𝑖𝑖 = (cid:3)𝛽𝛽0 + 𝛽𝛽1𝑥𝑥1 + ⋯+ 𝛽𝛽𝑛𝑛𝑥𝑥𝑛𝑛\n\nrepresents the output of the model, or what we usually call the prediction. The In this equation, . The slope, which is also referred prediction is calculated by taking the intercept . When working with to as the weight, is applied to all the features in the data, which represents 0 the data, we usually represent it as a matrix, which makes it easy to understand and easy to work with when using Python:\n\nand the slope\n\n𝑦𝑦 = 𝑋𝑋𝑋𝑋\n\n𝑦𝑦 =\n\n𝑦𝑦1 𝑦𝑦2 . . . 𝑦𝑦𝑛𝑛] [\n\n𝑋𝑋 = [\n\n𝑥𝑥11 ⋯ 𝑥𝑥1𝑐𝑐 ⋮ ⋮ ⋱ 𝑥𝑥𝑟𝑟1 ⋯ 𝑥𝑥𝑟𝑟𝑐𝑐\n\n]\n\n75\n\n76\n\nKey Machine Learning Algorithms and Concepts\n\n𝛽𝛽0 𝛽𝛽1 . . . 𝛽𝛽𝑛𝑛] [ The number of features describes what type of problem you are solving. If your data only has one feature, it is considered a simple linear regression model. While it can solve straightforward problems, for more advanced data and problems, it can be difficult to map relationships. Therefore, you can ). This allows the model to be create a multiple linear regression model by adding more features ( more robust and find deeper relationships.\n\n𝛽𝛽 =\n\nFigure 4.9: A simple linear regression model\n\nOnce we train our model, we need to learn how to evaluate our model and understand how it performs against the test data. When it comes to linear regression, the two common metrics we use to assess our model are the Root Mean Square Error (RMSE) and the R2 metrics.\n\nThe RMSE is the standard deviation of the residual errors across the predictions. The residual is the measure of the distance from the actual data points to the regression line. The further the average distance of all the points is from the line, the higher the error is. This indicates a weak model, as it’s unable to find the correlation between the data points. This metric can be calculated by using this formula where\n\nis the actual value,\n\nis the predicted value, and\n\nis the number of data points:\n\n𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅 = √\n\n𝑁𝑁 𝑖𝑖=1\n\n∑ (𝑦𝑦𝑖𝑖 − 𝑦𝑦𝑖𝑖̂) 𝑁𝑁\n\n2\n\nBuilding regression models\n\nFigure 4.10: Calculating the residual of a linear regression model\n\nR2, also known as the coefficient of determination, measures the proportion of variance in the dependent variables (Y) that can be explained by the independent variables (X). It essentially tells us how well the data fits the model. Unlike the RMSE, which can be an arbitrary number, R2 is given as a percentage, which can be easier to understand. The higher the percentage, the better the correlation of data. Although useful, a higher percentage is not always indicative of a strong model. What determines a good R2 value depends on the application and how the user understands the data. R2 can be calculated by using this formula:\n\n𝑁𝑁 𝑖𝑖=1 𝑁𝑁 𝑖𝑖=1 Many more metrics can evaluate the effectiveness of your regression model, but these two are more than enough to get an understanding of how your model is performing. When building and evaluating your model, it is important to plot and visualize your data and model, as this can identify key points. The plots can help you determine whether your model is overfitting or underfitting.\n\n2\n\n∑ (𝑦𝑦𝑖𝑖 − 𝑦𝑦𝑖𝑖̂) ∑ (𝑦𝑦𝑖𝑖 − 𝑦𝑦𝑖𝑖̂)\n\n2\n\n𝑅𝑅\n\n= 1 −\n\n2\n\nOverfitting occurs when your model is too suited to your training data. Your RMSE will be really low, and you will have a training accuracy of almost 100%. While this seems tempting, it is an indication of a poor model. This can be caused by one of two things: not enough data or too many parameters. As a result, when you test your model on new data it has not seen before, it will perform very poorly due to it not being able to generalize the data.\n\n77\n\n78\n\nKey Machine Learning Algorithms and Concepts\n\nFigure 4.11: An overfitted linear regression model\n\nTo address overfitting, you can try to increase the amount of training data, or make the model less complex. It also helps to randomly shuffle your data before you split it into the training and testing set. Another important technique is called regularization. While there are many different regularization techniques (L1 or L2 regularization) depending on the model, they all work similarly in that they add bias or noise into the model to prevent overfitting. In the regression equation we previously saw, we can add another term,\n\n, to show that regularization is being applied to our model:\n\n𝑦𝑦𝑖𝑖 = (cid:3)𝛽𝛽0 + 𝛽𝛽1𝑥𝑥1 + ⋯+ 𝛽𝛽𝑛𝑛𝑥𝑥𝑛𝑛 + 𝜖𝜖 On the other end, underfitting occurs when your model is unable to find any meaningful correlation within the data. This is not as common as overfitting since it is easy to find patterns in most data. If this occurs, either your data has too much noise and is severely uncorrelated, your model is too simple and doesn’t have enough parameters, or the model is not effective for the application at hand. It is also useful to debug your code and make sure there are no bugs when it comes to preprocessing your data or setting up your model:\n\nBuilding regression models\n\nFigure 4.12: An under-fitted linear regression model\n\nTherefore, the goal is to find the best-fitting model, between an overfit and an underfit. It takes time and experimentation to find a model that works for your needs, but using the key indicators and metrics discussed here can help guide you in the right direction:\n\nFigure 4.13: A best-fitting linear regression model\n\n79\n\n80\n\nKey Machine Learning Algorithms and Concepts\n\nImportant Note Feature engineering is a critical part of building a comprehensive model. Understanding your data can help determine which features or parameters to include in your model so that you can capture the relationship between the independent and dependent variables without causing overfitting.\n\nThere are some key notes to keep in mind when collecting and working with the data for your model:\n\nNormalize your data: It is possible to have features with very high or low numbers, so to prevent them from overwhelming the model and creating biases, it is imperative to normalize all your data to make it uniform across the features.\n\nClean your data: In the real world, the data we collect isn’t always perfect and can contain missing or egregious data. It is important to deal with these issues because they can cause outliers and impact the model negatively.\n\nUnderstand the data: It is a common practice to perform statistical analysis, also known as Exploratory Data Analysis (EDA), on your data to get a better understanding of how the data can impact your model. This can include plotting graphs, running statical methods, and even using machine learning techniques to reduce the dimensionality of the data, which will be discussed later in the chapter.\n\nIn the next section, we will discuss classification models.\n\nBuilding multiclass classification\n\nUnlike regression models that produce a continuous output, models are considered classification models when they produce a finite output. Some examples include email spam detection, image classification, and speech recognition.\n\nClassification models are considered versatile since they can apply to both supervised and unsupervised learning while regression models are mostly used for supervised learning. There are some regression models (such as logistic regression and support vector machine) that are also considered classification models since they use a threshold to split the output of continuous values into different categories.\n\nUnsupervised learning is a common application used in today’s market. Although supervised learning usually performs better and provides meaningful results since we know the expected output, the majority of the data we collect is unlabeled. It costs companies time and money for human experts to sift through the data and label it. Unsupervised learning helps reduce the cost and time by getting the model to try and determine the labels for the data and extract meaningful information. They can even perform better than humans sometimes.\n\nBuilding multiclass classification\n\nThe number of categories in the output of a classification model determines what type of model it is. For models with only two outputs (i.e., spam and not spam), this is called a binary classifier, while models with more than two outputs are called multiclass classifiers:\n\nFigure 4.14: Binary and multiclass classifiers\n\nFrom those classifiers, there are two types of learners: lazy learners and eager learners.\n\nLazy learners essentially store the training data and wait until they receive new test data. Once they get the test data, the model classifies the new data based on the already existing data. These types of learners take less time when training since you can continuously add new data without having to retrain the entire model, but take longer when performing classification since they have to go through all the data points. One common type of lazy learner is the K-Nearest Neighbors (KNN) algorithm.\n\nOn the other hand, eager learners work in the opposite way. Whenever new data is added to the model, they have to retrain the model again. Although this takes more time compared to lazy learners, querying the model is much faster since they don’t have to go through all the data points. Some examples of eager learners are decision trees, naïve Bayes, and ANNs.\n\nImportant Note Supervised learning will generally perform better than unsupervised learning since we know what the expected output should be during training, but it is costly to have to collect and label the data, so unsupervised learning excels in this area of training on unlabeled data.\n\nIn the next few sections, we will be looking at a few niche models that can be used for unique problems that most basic classification or regression models can’t solve.\n\n81\n\n82\n\nKey Machine Learning Algorithms and Concepts\n\nText sentiment analysis and topic modeling\n\nA popular field in the machine learning field is topic modeling and text analysis. With a plethora of text on the internet, being able to understand that data and create complex models such as chatbots and translation services has become a hot topic. Interacting with human language using software is called NLP.\n\nDespite the amount of data we can use to train our models, it is a difficult task to create meaningful models. Language itself is complex and contains many grammar rules, especially when trying to translate between languages. Certain powerful techniques can help us when creating NLP models though.\n\nImportant Note Before implementing any NLP models, it is imperative to preprocess the data in some way. Documents and text tend to contain extraneous data, such as stopping words (the/a/and) or random characters, which can affect the model and produce flawed results.\n\nThe first idea we will discuss is topic modeling. This is the process of grouping text or words from documents into different topics or fields. This is useful when you have a document or text and want to classify and group it into a certain genre without having to go through the tedious process of reading documents one by one. There are many different models used for topic modeling:\n\nLatent Semantic Analysis (LSA)\n\nProbabilistic Latent Semantic Analysis (PLSA)\n\nLatent Dirichlet Allocation (LDA)\n\nWe will focus on LDA. LDA uses statistics to find patterns and repeated occurrences of words or phrases and groups them into their topics. It assumes that each document contains a mixture of topics and that each topic contains a mixture of words. LDA first starts the process of going through the documents and keeping a word matrix, where it contains the count of each word in each document:",
      "page_number": 89
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 97-104)",
      "start_page": 97,
      "end_page": 104,
      "detection_method": "topic_boundary",
      "content": "Text sentiment analysis and topic modeling\n\nFigure 4.15: A word matrix\n\n, that we want to split up the words After creating a word matrix, we determine the number of topics, into and use statistics to find the probability of the words belonging to a certain topic. Using Bayesian statistics, we can then calculate the probability and use that to cluster the words into different topics:\n\nFigure 4.16: An LDA model\n\nAnother rising application in NLP is sentiment analysis. This involves the process of taking words or text and understanding the user’s intent or emotion. This is common today when dealing with online reviews or social media posts. It determines whether a piece of text contains positive, neutral, or negative emotions.\n\nMany different methods and models can solve this problem. The simplest approach is through statistics by using Bayes’ theorem. This formula is used for predictive analysis, as it uses previous words in a text to update the model. The probability can be calculated using this formula:\n\n𝑃𝑃(𝐴𝐴|𝐵𝐵) =\n\n𝑃𝑃(𝐵𝐵|𝐴𝐴)𝑃𝑃(𝐴𝐴) 𝑃𝑃(𝐵𝐵)\n\n83\n\n84\n\nKey Machine Learning Algorithms and Concepts\n\nDeep learning has become a powerful tool for NLP and can be useful for sentiment analysis. CNNs and Recurrent Neural Networks (RNNs) are two types of deep learning models that can drastically improve models for NLP, especially for sentiment analysis. We will discuss these neural networks and how they perform more later in this chapter.\n\nPattern analysis and forecasting in machine learning\n\nWith the uncertainty of time, being able to predict certain trends and patterns has become a hot topic in today’s industry. Most regression models, while powerful, are not able to make confident time predictions. As a result, some researchers have devised models that take time into consideration when making certain predictions, such as gas prices, stock market, and sales forecasting. Before we go into the different models, we must first understand the different concepts in time-series analysis.\n\nThe first step when dealing with time-series problems is familiarizing yourself with the data. The data usually contains one of four data components:\n\nTrend – The data follows an increasing or decreasing continuous timeline and there are no periodic changes\n\nSeasonality – The data changes in a set periodic timeline\n\nCyclical – The data changes but there is no set periodic timeline\n\nIrregular – The data changes randomly with no pattern\n\nPattern analysis and forecasting in machine learning\n\nFigure 4.17: Different components of time-series data\n\nThese different trends can be split into two different data types:\n\nStationary – Certain attributes of the data, such as mean, variance, and covariance, do not change over time\n\nNon-stationary – Attributes of the data change over time\n\nOften, you will work with non-stationary data, and creating machine learning models using this type of data will generate unreliable results. To resolve this issue, we use certain techniques to change our data into stationary data. They include the following methods:\n\nDifferencing – A mathematical method used to normalize the mean and remove the variance. It can be calculated by using this formula:\n\n𝑦𝑦𝑡𝑡̂ = 𝑦𝑦𝑡𝑡 − 𝑦𝑦𝑡𝑡−1\n\n85\n\n86\n\nKey Machine Learning Algorithms and Concepts\n\nTransformation – Mathematical methods are used to remove the change in variance. Among the transformations, these are three commonly used:\n\n Log transform\n\n Square root\n\n Power transform\n\nFigure 4.18: Differencing non-stationary data\n\nImportant Note Time is already uncertain, and this makes it almost impossible to create a model that can confidently predict future trends. The more we can remove uncertainty in our data, the better our model can find the relationships in our data.\n\nOnce we can transform our data, we can start looking at models to help us with forecasting. Of the different models, the most popular model for time-series analysis is an Auto-Regressive Integrated Moving Average (ARIMA) model. This linear regression model consists of three subcomponents:\n\nAuto-Regression (AR) – A regression model that uses the dependencies of the current time and previous time to make predictions\n\nIntegrated (I) – The process of differencing in order to make the data stationary\n\nMoving Average (MA) – Models between the expected data and the residual error by calculating the MA of the lagged observed data\n\nEnhancing models using deep learning\n\nAlong with ARIMA, other machine learning models can be used for time-series problems. Another well-known model is the RNN model. This is a type of deep learning model used for data that has some sort of sequence. We will be going into more detail on how they work in the next section.\n\nEnhancing models using deep learning\n\nEarlier in the chapter, we briefly discussed deep learning and the advantages it brings when enhancing simple machine learning models. In this section, we will go into more information on the different deep learning models.\n\nBefore we can build our model, we will briefly go over the structure of the deep learning models. A simple ANN model usually contains about two to three fully connected layers and is usually strong enough to model most complex linear functions, but as we add more layers, the improvement to the model significantly diminishes, and it is unable to perform more complex applications due to overfitting.\n\nDeep learning allows us to add multiple hidden layers to our ANN while reducing our time to train the model and increasing the performance. We can do this by adding one or both types of hidden layers common in deep learning – a CNN or RNN.\n\nA CNN is mostly applied in the image detection and video recognition field due to how the neural network is structured. The CNN architecture comprises the following key features:\n\nConvolutional layers\n\nActivation layers\n\nPooling layers\n\nThe convolutional layer is the core element in the CNN model. Its primary task is to convolve or group sections of the data using a kernel and produces an output called a feature map. This map contains all the key features extracted from the data, which can then be used for training in the fully connected layer. Each element in the feature map indicates a receptive field, which is used to denote which part of the input is used to map to the output. As you add more convolutional layers, you can extract more features, and this allows your model to adapt to more complex models:\n\n87\n\n88\n\nKey Machine Learning Algorithms and Concepts\n\nFigure 4.19: Convolutional layer output\n\nIn Figure 4.19, we can see how the feature map is created in the convolutional layer. The layer slides the kernel across the input data, performs the dot operation, and produces an output matrix, which is the result of the convolution function. The size of the kernel and the step size can dictate the output size of the feature map. In this example, we use a kernel size of 2x2 and a stride or step size of 2, which gives us a feature map of size 2x2 based on our input size. The output of the feature map can then be used for more future convolutional layers depending on the requirements of the user.\n\nBefore we use our newly created feature map as an input for another convolutional or fully connected layer, we pass the feature map through an activation layer. It is important to pass your data through some type of nonlinear function during the training process, as this allows your model to map to more complex functions. Many different types of activation functions can be used throughout your model and have their benefits depending on the type of model you are planning to build. Among the many activation functions, these are the most commonly used:\n\nRectified Linear Activation (ReLU)\n\nLogistic (sigmoid)\n\nThe Hyperbolic Tangent (Tanh)\n\nThe ReLU function is the most popular activation function used today. It is very simple and helps the model learn and converge more quickly than most other activation functions. It is calculated using this function:\n\n𝑓𝑓(𝑥𝑥) = {\n\n0,(cid:3)(cid:3)𝑥𝑥 < 0 𝑥𝑥,(cid:3)(cid:3)𝑥𝑥 ≥ 0\n\nEnhancing models using deep learning\n\nFigure 4.20: ReLU function\n\nThe logistic function is another commonly used activation function. This is the same function used in logistical regression models. This function helps bound the output of the feature map between 0 and 1. While useful, this function is computationally heavy and may slow down the training process. It is calculated using this function:\n\n𝑓𝑓(𝑥𝑥) = (cid:3)\n\n1 1 + 𝑒𝑒\n\n−𝑥𝑥\n\nFigure 4.21: Sigmoid function\n\nThe Tanh function is similar to the sigmoid function in that it bounds the values from the feature map. Rather than bounding it from 0 to 1, it bounds the values from -1 to 1, and it usually performs better than a sigmoid function. The function is calculated using this formula:\n\n89\n\n90\n\nKey Machine Learning Algorithms and Concepts\n\n𝑥𝑥\n\n−𝑥𝑥\n\n𝑓𝑓(𝑥𝑥) = (cid:3)\n\n𝑒𝑒 𝑒𝑒\n\n𝑥𝑥\n\n− 𝑒𝑒 + 𝑒𝑒\n\n−𝑥𝑥\n\nFigure 4.22: Tanh function\n\nEach activation has its uses and benefits depending on the task or model at hand. The ReLU function is commonly used in CNN models while sigmoid and Tanh are mostly found in RNN models, but they can be used interchangeably and bring different results.\n\nAfter we run our feature map through an activation layer, we come to the final piece – the pooling layer. As mentioned before, a key element in deep learning is the reduction of parameters. This allows the model to train on fewer parameters while still retaining the important features extracted from our convolutional layers. The pooling layer is responsible for this step of downsizing our parameter size. There are many common pooling functions but the most commonly used is max pooling. This is similar to the convolutional layer, where we use a kernel or filter to slide through our input data and only take the maximum value from each window:",
      "page_number": 97
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 105-113)",
      "start_page": 105,
      "end_page": 113,
      "detection_method": "topic_boundary",
      "content": "Enhancing models using deep learning\n\nFigure 4.23: Max pooling layer output\n\nIn Figure 4.23, we are using a kernel size of 2x2 with a stride size of 2. Here, we can see our output where only the maximum value from each window is selected.\n\nOther layers and functions can be added to the model to help address certain issues and applications, such as a batch normalization layer, but with these three foundational layers, we can add multiple layers of different sizes and still build a powerful model.\n\nIn the final layer, we feed our output into the fully connected layer. By that time, we are able to extract the important features of the data and still learn more complex models with less time to train as compared to a simple ANN model.\n\nNext, we will go over the RNN architecture. Due to the nature of how the RNN model is structured, it is designed for tasks that need to take into consideration a set of sequence data, in which the data later in the sequence is dependent on earlier data. This model is commonly used for certain fields such as NLP and signal processing.\n\nThe basics of an RNN are built by having a hidden layer in which the output of the layer is fed back into the same hidden layer. This way, the model is able to learn based on previous data and adjust the weights accordingly.\n\n91\n\n92\n\nKey Machine Learning Algorithms and Concepts\n\nFigure 4.24: RNN architecture\n\nTo better understand how the model works, you can envision a single layer for each data point. Each . We then transfer the weights layer takes in the data point as an input between the layers and then take the total average of all the cost functions from all the layers in the model.\n\nand produces an output\n\nFigure 4.25: An unraveled RNN\n\nEnhancing models using deep learning\n\nThis model works for most simple problems. As the sequence increases, it encounters a few issues:\n\nVanishing gradient: This occurs during the training process when the gradient approaches zero. The weights aren’t updated properly as a result and the model performs poorly.\n\nLack of context: The model is unidirectional and cannot look further or previously into the data. Therefore, the model is only able to predict based on data around the current sequence point and is more likely to make a poor prediction based on incorrect context.\n\nThere are different variations of RNNs created to address some of the issues mentioned here. Among them, the most common one used today is the Long Short-Term Memory (LSTM) model. The LSTM model comprise three components:\n\nAn input gate\n\nAn output gate\n\nA forget gate\n\nFigure 4.26: An LSTM neural network\n\nThese gates work by regulating which data points are needed to contextualize the sequence. That way, the model can predict more accurately without being easily manipulated.\n\n93\n\n94\n\nKey Machine Learning Algorithms and Concepts\n\nThe forget gate is specifically responsible for removing previous data or context that is no longer needed. This gate uses the sigmoid function to determine whether it uses or “forgets” the data.\n\nThe input gate is used to determine whether the new data is relevant to the current sequence or not. This is so that only important data is being used to train the model and not redundant or irrelevant information.\n\nLastly, the output gate’s primary function is to filter the current state’s information and only send relevant information to the next state. As with the other gates, it uses the context from previous states to apply a filter, which helps the model properly contextualize the data.\n\nCNN and RNN models are mostly designed for supervised learning problems. When it comes to unsupervised learning, different models are needed to solve certain problems. Let’s discuss autoencoders.\n\nAutoencoders work by taking the input data, compressing it, and then reconstructing it by decompressing it. While straightforward, it can be used for some advanced applications, such as generating audio or images, or it can be used as an anomaly detector.\n\nThe autoencoder comprises two parts:\n\nAn encoder\n\nA decoder\n\nFigure 4.27: The components of an autoencoder\n\nThe encoder and decoder are usually built with a one-layer ANN. The encoder is responsible for taking the data and compressing or flattening the data. Then, the decoder works on taking the flattened data and trying to reconstruct the input data.\n\nThe hidden layer in the middle of the encode and decoder is usually referred to as the bottleneck. The number of nodes in the hidden layer must be less than those in the encoder and decoder. This forces the model to try and find the pattern or representation in the input data so that it can reconstruct the data with little information. Thus, the cost function is there to calculate and minimize the difference between the input and output data.\n\nOne aspect of autoencoders that is an integral part of deep learning is dimensionality reduction. This is the process of reducing the number of parameters or features used when training your model. As mentioned earlier in this chapter, to build a complex model that can build a deeper representation of the data, it is important to include more features. However, adding too many features can lead to overfitting, so how do we find the best number of features to use in our model?\n\nThere are many models and techniques, such as autoencoders, that can perform dimensionality reduction to help us find the best features to use in our model. Among the different techniques, Principle Component Analysis (PCA) is the most popular. This technique can take an N-dimensional dataset and reduce the number of dimensions in the data using linear algebra. It is a common practice to use a dimensionality reduction technique before using your data to train your model, as this can help to remove noise in the data and avoid overfitting.\n\nSummary\n\nIn this chapter, we discussed what is considered artificial intelligence and the different sub-fields that it contains.\n\nWe discussed the different characteristics of regression and classification models. From there, we also went over the structure of our data and how the model performs when training over our data. We then discussed the different ways of analyzing our model’s performance and how to address the different issues that we can come across when training our model.\n\nWe briefly viewed the different packages and libraries that are used today in machine learning models and their different use cases.\n\nWe also analyzed different topics such as topic modeling and time-series analysis and what they entail. With that, we were able to look at the different methods and techniques used to solve those types of problems.\n\nLastly, we went into deep learning and the different ways it improves on machine learning. We went over the two different types of neural networks – CNNs and RNNs – how they are structured, and their benefits and use cases.\n\nIn the next chapter, we will take what we have learned and start looking into how we can design and build an end-to-end machine learning system and the different components that it contains.\n\nSummary\n\n95\n\n5 Machine Learning System Design\n\nIn the last chapter, we delved into the different machine learning concepts and the packages and libraries used to create these models. Using that information, we will begin to discuss the design process when building a machine learning pipeline and the different components found in most machine learning pipelines.\n\nWe will cover the following areas in this chapter:\n\nMachine learning system components\n\nFit and transform interfaces\n\nTrain and serve interfaces\n\nOrchestration\n\nMachine learning system components\n\nThere are many moving parts required in order to build a robust machine learning system. Starting from gathering data to deploying your model to the user, each plays a vital role in keeping the system dynamic and scalable. Here, we will briefly discuss the different stages in the machine learning system life cycle and the role they play. These stages can be edited in order to suit the model or application at hand.\n\nThe majority of machine learning systems include the following stages, with some other stages depending on business needs:\n\nData collection\n\nDate preprocessing\n\nModel training\n\n98\n\nMachine Learning System Design\n\nModel testing\n\nModel serving\n\nRealistically, the majority of the time spent building machine learning systems is spent on the data. This is a key element in the process that can decide the effectiveness of your system since the model is dependent on the data it uses during training. Just like the human body, if you feed the model poor data or not enough data, it will output poor results.\n\nThe first part when it comes to data is the collection process. Understanding the application and the goal of the task can assist in the process of deciding how to collect data and what data to collect. We then determine the target value that we want to predict, such as the price of a home or the presence of a certain disease. These target values can be collected explicitly or implicitly. A target variable is explicit when we can directly determine the value of the variable we are trying to capture, while an implicit target value is found by using contextual data to determine the target value.\n\nDepending on the task, we usually store the data in a database (for either metadata or tabular data) such as MySQL or cloud storage (for images, video, or audio) such as Amazon S3:\n\nFigure 5.1: Data collection\n\nMachine learning system components\n\nOnce we set up continuous data collection, we must devise a procedure for cleaning and processing the data. Not everything we collect will be perfect. You will always find missing data and certain outliers, which can negatively impact our model. No matter how intuitive your model is, it will always perform poorly with garbage data.\n\nSome practices to deal with unclean data include removing outliers, normalizing certain features, or imputing missing data depending on the amount of data you have collected. Once the data has gone through the cleaning process, the next step is the feature selection/engineering process.\n\nUnderstanding the different features your data contains plays an important role when your model tries to find the relationship in its data. Exploratory Data Analysis (EDA) is the common process used when it comes to understanding the data you have collected and how the data is structured. This helps when it comes to determining which features to use in your model. As we previously mentioned in Chapter 4, when we include more features in our models, it allows them to map to more complex problems. However, adding too many features can lead to overfitting, so it is important to research the most important features for your model.\n\nWhile most machine learning models can find patterns and relationships in data, the best way of understanding the data you collect is via the experts in the field of the task you are trying to solve. Subject matter experts can provide the best insight into what features to focus on when creating your model. Some unsupervised machine learning models, such as PCA and t-SNE, can group and find features that can provide the most valuable information for your model.\n\nImportant note Having domain knowledge of the problem you are trying to solve is the most effective way of understanding your data and determining which features to use for training your machine learning model.\n\nOnce you have set up the processes to collect and clean the data, the next step is creating and training your model. Thanks to most machine learning libraries, you can import prebuilt models and even use weights from already trained models to use on your own model. Here, it is common practice to use different models and techniques to see which produces the best result, and from there, you can choose the best model and begin to fine-tune it by updating the hyperparameters. This process can take time depending on the amount of data you use.\n\nTesting your model is a critical element in your system’s pipeline. Depending on the application, a poor model can negatively impact your business and give your users a bad experience. To prevent that, you need to determine the different metrics and thresholds that need to be met for the model to be production-ready. If the model can’t meet these expectations, then you need to go back and understand the weaknesses of the model and address them before training again.\n\n99\n\n100\n\nMachine Learning System Design\n\nAfter performing tests and getting solid results from your model, you can now deploy your model to the user application. This varies from application to application. From then, the whole process can start from the beginning, where new data is inserted and follows the machine learning pipeline so it can dynamically grow based on user actions:\n\nFigure 5.2: The machine learning pipeline\n\nIn the following sections, we will look into the details of the different interfaces that constitute our machine learning pipeline.\n\nFit and transform interfaces\n\nNow that we have looked at the entire pipeline process, we will look in detail at the different interfaces that make up the machine learning system. The majority of the systems include the following interfaces:\n\nFit\n\nTransform\n\nTrain\n\nServe\n\nWhen it comes to the data and creating the model, we come across the fit and transform interfaces. We will start by looking at the transform interface.\n\nTransform\n\nThe transform interface is the process of taking in the collected data and preprocessing the data so that the model can train properly and extract meaningful information.",
      "page_number": 105
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 114-123)",
      "start_page": 114,
      "end_page": 123,
      "detection_method": "topic_boundary",
      "content": "Fit and transform interfaces\n\nIt is common for the data we collect to have missing values or outliers, which can cause bias in our model. To remove this bias, we can apply certain techniques that help remove the skew in the data and produce meaningful machine learning models. Some of the following techniques we will learn about fall into the following three types of transformations:\n\nScaling\n\nClipping\n\nLog\n\nLog transformation is the most common and simple transformation technique we can apply to our data. A lot of the time, our data is skewed in one direction, which can introduce bias. To help mitigate the skewed distribution, we can simply apply the log function to our data, and this shifts our data into more of a normal distribution, which allows the data to be more balanced.\n\nWe can perform this transformation by using the following code:\n\nimport numpy as np dataframe_log = np.log(dataframe[\"House Price\"])\n\nFigure 5.3: Performing log transformation on skewed data\n\n101\n\n102\n\nMachine Learning System Design\n\nOnce we apply the log transformation, we can start looking at the other transformations. The second transformation we can use is the clipping transformation. The more we make our data follow a normal distribution, the better, but we may encounter outliers that can skew our data. To help reduce the impact that outliers have on our data, we can apply a quantile function. The most common quantile range that people use is the 0.05 and 0.95 percentile. This means that any data below the 0.05 percentile will be rounded up to the lower bound while any data above the 0.95 percentile will be rounded down to the upper bound. This allows us to retain the majority of the data while reducing the impact that outliers have on the model. The upper and lower ranges can also be modified based on what makes sense for the distribution of the data.\n\nThis transformation can be performed using the following code:\n\nfrom sklearn.preprocessing import QuantileTransformer quantile = QuantileTransformer(output_distribution='normal', random_state=0) x_clipped = quantile.fit_transform(\"House Price\")\n\nFigure 5.4: Clipping transformation on data\n\nThe last major transformation technique is scaling transformations. A lot of the time, the data we collect have different types of metrics and values, which can skew our data and confuse our model. For example, one feature measures the revenue of companies in the millions while another feature measures the employee count in the thousands, and when using these features to train the model, the\n\nFit and transform interfaces\n\ndiscrepancy may put more emphasis on one feature over another. To prevent these kinds of problems, we can apply scaling transformations, which can be of the following types:\n\nMinMax\n\nStandard\n\nMax Abs\n\nRobust\n\nUnit Vector\n\nThe MinMax scaler is the simplest scaling transformation. It works best when the data is not distorted. This scales the data between 0 and 1. It can be calculated using this formula:\n\n𝑥𝑥𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 = (𝑥𝑥 − 𝑥𝑥𝑚𝑚𝑚𝑚𝑚𝑚)/ (𝑥𝑥𝑚𝑚𝑠𝑠𝑚𝑚 − 𝑥𝑥𝑚𝑚𝑚𝑚𝑚𝑚)\n\nWe can perform this scaling transformation using the following code:\n\nfrom sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() x_scaled = scaler.fit_transform(\"House Price\")\n\nThe MaxAbs scaler is similar to MinMax but rather than scaling the data between 0 to 1, it scales the data from -1 to 1. This can be calculated using the following formula:\n\nmax⁡(|𝑥𝑥|) 𝑥𝑥\n\n𝑥𝑥𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 =\n\nWe can perform this scaling transformation using the following code:\n\nfrom sklearn.preprocessing import MaxAbsScaler scaler = MaxAbsScaler() x_scaled = scaler.fit_transform(\"House Price\")\n\nThe Standard scaler is another popular scaling transformation. Rather than using the min and max like the MinMax scaler, this scales the data so that the mean is 0 and the standard deviation is 1. This scaler works on the assumption that the data is normally distributed. This can be calculated using the following formula:\n\n𝑥𝑥𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 = 𝑥𝑥 −\n\n𝜇𝜇 𝜎𝜎\n\n103\n\n104\n\nMachine Learning System Design\n\nWe can perform this scaling transformation using the following code:\n\nfrom sklearn.preprocessing import StandardScaler scaler = StandardScaler() x_scaled = scaler.fit_transform(\"House Price\")\n\nThe MinMax, MaxAbs, and Standard scalers, while powerful, can suffer from outliers and skewed distribution. To remedy this issue, we can use the Robust scaler. Rather than using the mean or max, this scaler works by removing the median from the data and then scaling the data using the interquartile range. This can be calculated using the following formula:\n\n𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝑡𝑡𝑡𝑡𝐼𝐼 𝑅𝑅𝐼𝐼𝐼𝐼𝑅𝑅𝐼𝐼 (𝐼𝐼𝐼𝐼𝑅𝑅) = 𝐼𝐼3 − 𝐼𝐼1\n\n𝑥𝑥𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 = (𝑥𝑥 − 𝑄𝑄1)/𝐼𝐼𝑄𝑄𝐼𝐼\n\nWe can perform this scaling transformation using the following code:\n\nfrom sklearn.preprocessing import RobustScalar scaler = RobustScalar() x_scaled = scaler.fit_transform(\"House Price\")\n\nFinally, we have the Unit Vector scaler, also known as a normalizer. While the other scaler functions work based on columns, this scaler normalizes based on rows. It uses the MinMax scaler formula and converts positive values between 0 and 1 and negative values between -1 and 1. There are two ways of performing this scaling:\n\nL1 norm – values in the column are converted so that the sum of their absolute value in the row equals 1\n\nL2 norm – values in the column are squared and added so that the sum of their absolute value in the row is equal to 1\n\nWe can perform this scaling transformation using the following code:\n\nfrom sklearn.preprocessing import Normalizer scaler = Normalizer() x_scaled = scaler.fit_transform(\"House Price\")\n\nThere are many more scaling and transforming techniques, but these are the most commonly used, as they provide stable and consistent results.\n\nFit and transform interfaces\n\nImportant note Much of the development process takes place in the transformation stage. Understanding how the data is structured and distributed helps dictate which transformation methods you will perform on your data. No matter how advanced your model is, poorly structured data will produce weak models.\n\nFit\n\nNow, we will look at the fit interface. This interface refers to the process of creating the machine learning model that will be used in training. With today’s technology, not much work or effort is needed to create the model used for training in the machine learning pipeline. There are already prebuilt models ready to be imported and used for any type of application.\n\nHere is a small example of creating a KNN classification model using the scikit-learn library.\n\nFirst, we import all the required libraries:\n\nfrom sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KneighborsClassifier from sklearn.datasets import load_iris\n\nWe then import the data, split the data into training and testing batches, and apply a standard scaler transformation:\n\niris = load_iris() X = iris.data y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_ size=0.30) scaler = StandardScaler() scaler.fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test)\n\nWe then initialize a KNN model with k = 3 and then perform training on the model:\n\nclassifier = KNeighborsClassifier(n_neighbors=3) classifier.fit(X_train, y_train)\n\n105\n\n106\n\nMachine Learning System Design\n\nThe main effort when using the fit interface is setting up the models that will be used for the training phase of the machine learning pipeline. Due to the simplicity of importing multiple prebuilt models, it is common practice to import multiple types of machine learning models and train all of them at once. This way, we are able to test different types of models and determine which one of them performs the best. Once we decide which model to use, we can then start to experiment with different hyperparameters to further fine-tune our model.\n\nTrain and serve interfaces\n\nThe transform and fit interfaces are responsible for preparing the data and setting up our machine learning models for our pipeline. Now that we have preprocessed the data, we need to begin looking at how we can begin the actual training process and take our trained models and deploy them for our clients to use.\n\nTraining\n\nNow that we have preprocessed the data and created our models, we can begin the training process. This stage can vary from time to time depending on the quality of data being trained on or the type of model being used during training.\n\nOnce we preprocess the data, we need to split the dataset into training and testing sets. This is done to prevent overfitting. We need the model to be able to generalize the data, and using all the data for training would defeat the purpose.\n\nA common practice is to split your data into 70% training and 30% testing. This way, the model has enough data to learn the relationships and uses the testing data to self-correct its training process.\n\nThere is a more robust approach to splitting the data, which is called K-Fold Cross-Validation. This process works best in cases where there may not be enough training data. To perform this, we split the data into k number of subsets and then we train on all subsets except for one. We then iterate through this process where a new subset is selected to be the test data. Finally, we measure the performance of the model by averaging the metrics for each iteration. This way, we can train and test using all the data without leaving any important features that may be useful when it comes to learning the data.\n\nTrain and serve interfaces\n\nFigure 5.5: K-Cross Validation\n\nOnce we have split the data, now comes the actual training part. This part is as simple as setting up the function used to train the model. This part depends on the type of library you use and the different APIs it offers.\n\nWe can create a simple example using the scikit-learn library:\n\nfrom sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn import metrics\n\ndiabetes = load_diabetes() features = diabetes.data target = diabetes.target\n\nx_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=1) linear_regression = LinearRegression() linear_regression.fit(features, target)\n\n107\n\n108\n\nMachine Learning System Design\n\ny_pred = linear_regression.predict(x_test) print(\"Linear Regression model MSE:\", metrics.mean_squared_ error(y_test, y_pred))\n\nAfter training your model, you must measure its performance. To prevent poor models from being deployed to users, it is a common practice to measure certain metrics and set certain thresholds that need to be met before a model is considered ready for production.\n\nDepending on the type of model you create, certain metrics need to be evaluated. For example, a regression model will typically look at the following metrics:\n\nMean Absolute Error (MAE)\n\nMean Squared Error (MSE)\n\nRoot Mean Squared Error (RMSE)\n\nR-Squared (R2)\n\nFor classification models, you will monitor the following metrics to determine the model’s strength:\n\nAccuracy\n\nPrecision and recall\n\nThe F1-score\n\nThe Area Under the Receiver Operating Characteristics Curve (AUROC)\n\nHaving domain knowledge helps immensely when determining what thresholds are applicable to the model you are training. In some cases, such as with cancer detection models, it is important to avoid false negatives, so it is important to set stricter thresholds for what models can be used confidently.\n\nImportant note Before serving your model, you need to make sure the model is viable for production. Setting up the metric thresholds that the model needs to pass is a fundamental way of validating your models before deploying them. If your model fails to pass these criteria, then there should be a process to redo the data transformation and model training phases until it can pass the thresholds.\n\nServing\n\nWhen it comes to serving our model, this is open and flexible depending on the user’s needs. In most cases, we are deploying our model into one of two types of systems:\n\nModel serving, where we deploy our model as an API\n\nModel embedding, where we deploy our model straight into an application or device\n\nOrchestration\n\nModel embedding is the simplest way of deploying your model. You create a binary file containing your model and you embed the file into your application code. This simplicity provides the best performance when making predictions, but this comes at a cost. Because you directly embed the file into your application, it is difficult to scale your model since you will have to recreate and reupload the file every time you make an update to your model. As such, this is not considered a recommended practice.\n\nModel serving is the most commonly used method on today’s market. This separation between the application and the model makes it easy for a developer to maintain and update the model without having to change the application itself. You simply create an API service that a user can access to make calls and predictions. Due to the separation, you can continuously update the model without having to redeploy the whole application.\n\nAn alternative to model embedding that includes model serving is creating a microservice that includes the binary file of the model, which could be accessed by other applications:\n\nFigure 5.6: Serving machine learning models\n\nOne of the more intuitive approaches is creating your own package or library that includes all the models that you have trained. That way, you can scale efficiently by allowing multiple applications to access the different models you have created.\n\nEverything we’ve seen so far is what it takes to build a simple machine learning pipeline. While this is doable for most applications, to be dynamic and robust, we need to look at orchestration and what it can offer us to support more advanced applications and problems.\n\nOrchestration\n\nNow that we understand the different interfaces and the roles they play in the machine learning pipeline, the next step is understanding how to wrap everything together into one seamless system. To understand the holistic system, we must first understand automation and orchestration.\n\n109\n\n110\n\nMachine Learning System Design\n\nAutomation refers to the process of automating small or simple tasks, such as uploading files to a server or deploying an application, without human intervention. Rather than having a person perform these repetitive tasks, we can program our system to handle these simple tasks, thus reducing wasted time and resources.\n\nThis is useful for most systems due to the linear nature of the pipeline. This highlights a common limitation of automation though – the lack of flexibility. Most systems today require a more dynamic process to be able to adapt to certain applications and processes, and automation alone isn’t enough:\n\nFigure 5.7: A linear system pipeline\n\nThis is where orchestration comes into action. Orchestration is the configuration and coordination of automated tasks to create a whole workflow. We can create a system to perform certain jobs or tasks based on a certain set of rules. It takes some planning and understanding to create a comprehensive orchestration workflow since the user determines what actions the system needs to take for certain cases.\n\nA simple example would be deploying an application to users. There can be many moving parts in the system, such as the following:\n\nConnecting to a server\n\nUploading certain files to certain servers\n\nHandling user requests\n\nStoring data or logs in a database\n\nLet’s say that after the recent changes have been deployed, the app has suffered critical errors, which may bring down the application. The system admin could set up rules for recovering and restoring the system, such as rolling back to a stable version. With the system able to self-recover, the developers can spend more time in development rather than dealing with overhead when it comes to recovery.\n\nDepending on certain outcomes, not all tasks may need to be performed. There may be backup actions that need to take place, or different paths that the system needs to go through to maintain a stable workflow. This way, the system can adapt to its environment and self-sustain without much human intervention:",
      "page_number": 114
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 124-131)",
      "start_page": 124,
      "end_page": 131,
      "detection_method": "topic_boundary",
      "content": "Orchestration\n\nFigure 5.8: A dynamic system pipeline (orchestration)\n\nThe different tasks in the machine learning system that can be automated are as follows:\n\nGathering and preprocessing the data\n\nTraining the machine learning model\n\nRunning tests and diagnostics on the trained model to evaluate its performance\n\nServing the machine learning model\n\nMonitoring the model in production\n\nWith these automated tasks, the system admin needs to orchestrate the stages of the pipeline to be dynamic and sustainable. The following components help create a robust system:\n\nScheduling: The system must be able to schedule and run different automated tasks in the pipeline individually while maintaining system dependencies.\n\nCI/CD Testing: After model training is complete, it is imperative to do automated testing on your model to measure its performance. If it fails to pass certain metrics, you must repeat the training process from the beginning to address the weaknesses of the model; otherwise, it cannot be deployed to production.\n\nDeployment: Depending on where you will deploy your model to production, setting up an automated process can help reduce the time spent on deployment and still maintain an updated version of the model.\n\n111\n\n112\n\nMachine Learning System Design\n\nMonitoring: After deploying your model, continuously monitoring the model’s performance in production is needed to maintain the model’s health without it decaying. This will give us an indication of when we need to update our pipeline or our model in order to stay efficient.\n\nImportant note Understanding what your business needs are and how your model functions gives you a good picture of how you want to orchestrate your machine learning pipeline. Setting up backup phases to address certain pitfalls in your system allows it to be more dynamic and adaptable to industry demands.\n\nSummary\n\nIn this chapter, we looked at the different key components that make up a machine learning pipeline.\n\nFrom there, we looked in detail at the interfaces that make up the components. We started with the transform interface, which is responsible for the data aspect of the pipeline. It takes the data and applies different types of data transformation that allow us to maintain clean and stable data, which we can later use in our machine learning model.\n\nAfter our transformation stage, we start creating our model in the fit interface. Here, we can use the prebuilt models that the libraries and packages offer to initialize our models. Due to the ease of creating models, it is a good practice to test different types of models to see which model performs the best based on our data.\n\nOnce we have created our model, we can begin the actual training of our model. We need to split our data into training and test sets to allow our model to understand the relationship in our data. From there, we can measure the different metrics in our model to validate the model’s performance.\n\nOnce we feel comfortable with our model’s performance, we can start to deploy our application to production. There are two major ways of deploying our model, whether it be embedded into our application or deployed as a service for our clients to use.\n\nFinally, wrapping everything together, we learned what orchestration consists of when it comes to machine learning. We learned what concepts need to be considered when orchestrating your machine learning pipeline and how to keep your system dynamic and robust to keep up with everyday demands.\n\nAs time passes and data changes, it is important that we adjust and maintain our models to handle certain situations that may arise in the real world. In the next chapter, we will look at how we can maintain our machine learning models when our data starts to shift and change.\n\n6 Stabilizing the Machine Learning System\n\nIn the last two chapters, we went over the different concepts in machine learning and how we can create a comprehensive machine learning system pipeline that can work and adapt to our needs.\n\nWhile our pipeline can address our expectations, it is important for us to be able to maintain our system in the face of external factors to which it may be hard for the system to self-adjust.\n\nIn this chapter, we will discuss the phenomenon of dataset shifts and how we can optimize our machine learning system to help address these issues while maintaining its functional goal without having to rebuild our system from scratch.\n\nWe will be going over the following concepts:\n\nMachine learning parameterization and dataset shifts\n\nThe causes of dataset shifts\n\n\n\nIdentifying dataset shifts\n\nHandling and stabilizing dataset shifts\n\nMachine learning parameterization and dataset shifts\n\nMaintaining our machine learning models is an integral part of creating a robust model. As time progresses, our data begins to morph and shift based on our environment, and while most models can detect and self-repair, sometimes, human intervention will be required to guide them back on track.\n\nIn this section, we will briefly go over two main concepts that will help us understand the impact on our model:\n\nParameterization\n\nDataset shifts\n\n114\n\nStabilizing the Machine Learning System\n\nOur machine learning model is represented by certain specifications that help define the learning process of our model. These include the following:\n\nParameters\n\nHyperparameters\n\nWe will first look at parameters. These specifications are internal within the model. During the training process, these parameters are updated and learned while the model is trying to learn the mapping between the input features and the target values.\n\nMost of the time, these parameters are set to an initial value of either zeros or random values. As the training process happens, the values are continuously updated by an optimization method, such as gradient descent. At the end of the training process, the final weights of the values are what constitute the model itself. These weights can even be used for other models, especially those with similar applications.\n\nSome examples of parameters include the following:\n\nNode weights and bias values for artificial neural networks\n\nCoefficients of linear and logistic regression models\n\nCluster centroids for clustering models\n\nWhile parameters play a core role in determining the performance of a model, they are mostly out of our control since the model itself is what updates the weights. This leads us to hyperparameters.\n\nHyperparameters are parameters that control the learning process of our machine learning model, which, in turn, affects the output weights that our model learns. These values are set from the beginning and stay fixed throughout the learning process.\n\nWe, as users, determine which values to set in the beginning for our model to use during the training process. As a result, it takes time and experience to figure out which values produce the best results. There is effort involved in testing and training multiple variations of hyperparameters to see which performs the best.\n\nThere are many hyperparameters and each model has its own unique set of hyperparameters that the user can modify. These hyperparameters can include the following:\n\nThe split ratio between the training and testing datasets\n\nThe learning rate used in optimization algorithms\n\nThe choice of optimization algorithm\n\nThe batch size\n\nThe number of epochs or iterations\n\nThe number of hidden layers\n\nMachine learning parameterization and dataset shifts\n\nThe number of nodes in each hidden layer\n\nThe choice of cost or loss function\n\nThe choice of activation function\n\nThe number of\n\nclusters\n\nSince there can be many hyperparameters to adjust and many different combinations to try, it can be very time-consuming to test these changes one by one. As discussed in the last chapter, it can be useful to have a section in our pipeline that automates this process by running multiple models with different combinations of hyperparameters to speed up the testing process and find the most optimal combination of hyperparameters.\n\nFigure 6.1: Hyperparameter and parameter tuning\n\nThere may be cases where adjusting our parameters and hyperparameters is not enough for us to prevent our model from degrading.\n\nFor example, let’s say we create a machine learning model with a model accuracy of 85%. This model continues to perform well for some time. We then begin to see our model accuracy deteriorate until it becomes unusable, as the model is unable to properly predict the new test data we collect.\n\n115\n\n116\n\nStabilizing the Machine Learning System\n\nAs we analyze our model, we can begin to see that our training data does not reflect the testing data we have recently collected. Here, we can see that there is a shift between the data distribution for our training and test datasets.\n\nBefore we work on resolving dataset shifts, we must first understand the background of dataset shifts, how they occur, and how we can adjust our machine learning system to help prevent dataset shifts from impacting our model.\n\nMachine learning systems are built under the assumption that the data distribution between the training and test sets is similar. Since the real world is ever-changing, new data distributions emerge and there may be a significant difference between the training and test sets.\n\nThe major difference in data distribution between the training and test sets is considered a dataset shift. This drastic difference will eventually degrade the model, as the model is biased to the training set and is unable to adapt to the test set:\n\nFigure 6.2: Outcome of a machine learning model due to a dataset shift\n\nSome examples of this occurring include a shift in consumer habits, a socioeconomic shift, or a global influence, such as a pandemic. These events can heavily impact the data we collect and observe, which, in turn, can sway our model’s performance.\n\nImportant Note First, try adjusting the hyperparameters of your machine learning model and see whether the newly learned parameters can improve your model significantly. If you still encounter major issues, it may be best to analyze the data and see whether a dataset shift has occurred.\n\nThe causes of dataset shifts\n\nThe causes of dataset shifts\n\nNow that we have learned what dataset shifts are, we can start to investigate the different causes of dataset shifts. While there are many different reasons dataset shifts can occur, we can split them into two categories:\n\nSample selection bias\n\nNon-stationary environments\n\nSample selection bias is self-explanatory in that there is a bias or issue when it comes to labeling or collecting the training data used for the model. Collecting biased data will result in a non-uniform sample selection for the training set. That bias, in essence, will fail to represent the actual sample distribution.\n\nNon-stationary environments are another cause for dataset shifts – we will go into further detail about the different types later in the chapter. Let’s assume that we have a model with a set of input features, , the . This dataset shift is caused , which reflect very much how the ( , )\n\n, a target or output variable\n\n. From there, we can also define the prior probability as\n\nconditional probability as by temporal or spatial changes, defined as real world operates.\n\n, and the joint distribution as\n\n( | )\n\n𝑃𝑃𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦,𝑥𝑥) ≠ 𝑃𝑃𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦,𝑥𝑥)\n\n( )\n\nThis causal effect can lead to different types of shifts:\n\nFor\n\n , giving us a covariate or concept shift\n\nproblems, non-stationary environments can make changes to either\n\n( )\n\nor\n\n\n\nFor\n\n( | )\n\nproblems, a change in\n\nor\n\ncan give us a prior probability or concept shift\n\n( )\n\n( | )\n\nIn the next section, we will look into the different types of shifts and how we can identify them.\n\nIdentifying dataset shifts\n\nAfter looking into the different causes of dataset shifts, we can begin to classify certain shifts into different groups that can help us easily identify the type of dataset shift we are dealing with.\n\nAmong the different dataset shifts we can encounter, we can classify data shifts into these categories:\n\nCovariate shifts\n\nPrior probability shifts\n\nConcept shifts\n\nWe will first look at covariate shifts. This is the most common dataset shift, as a covariate shift occurs when there is a change in the distribution of one or more of the input features of the training or test data. Despite the change, the target value remains the same.\n\n117\n\n118\n\nStabilizing the Machine Learning System\n\nIn mathematical terms, this dataset shift occurs only in X > Y problems. Whenever the input , but the distribution, , this conditional probability of the training and testing dataset stays the same, will cause a covariate shift.\n\n, changes between the training and testing datasets,\n\n𝑝𝑝(𝑥𝑥)\n\n𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥) ≠ 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥)\n\n𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦|𝑥𝑥) = 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦|𝑥𝑥)\n\nFor example, we can create a model that predicts the salary of the employees of a certain city. Let’s say that the majority of the employees in your training set consist of younger individuals. After time passes, the employees get older. If you were to try to predict the salary of the older employees, you would begin to see a significant error. This is due to the model being heavily biased toward the training set, which consisted of mostly younger employees and is unable to find the relationship among the older employees.\n\nFigure 6.3: Covariate dataset shifts\n\nNext, we will be looking into prior probability shifts, also known as label shifts. This is the opposite of a covariate shift, as this shift occurs when the output distribution changes for a given output but the input distribution remains the same.\n\nIn mathematical terms, this occurs only in Y -> X problems. When the prior probability changes,\n\n, but the conditional probability remains the same,\n\n𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦) ≠ 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦) , a prior probability shift occurs:\n\n𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥|𝑦𝑦) = 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥|𝑦𝑦)",
      "page_number": 124
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 132-140)",
      "start_page": 132,
      "end_page": 140,
      "detection_method": "topic_boundary",
      "content": "Identifying dataset shifts\n\nFigure 6.4: Prior probability shifts\n\nFinally, we will discuss concept shifts, also known as concept drifts. This shift occurs when the distribution of the training data remains the same but the conditional distribution for the output given the training data changes.\n\nIn mathematical terms, this can occur both in X -> Y or Y -> X problems:\n\nFor X -> Y problems, this occurs when the prior probability of the input variables remains , but the conditional the same in the training and testing datasets, ( probability changes,\n\n𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥) = 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥))\n\nFor Y -> X problems, this occurs when the prior probability of the target variables remains , but the conditional the same in the training and testing datasets, probability changes,\n\nFor Y -> X problems, this occurs when the prior probability of the target variables remains , but the conditional .\n\n(𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦) = 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦))\n\n(𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥|𝑦𝑦) ≠ 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥|𝑦𝑦))\n\nAs an example, a user’s purchasing behavior is affected due to the economy, but neither our training nor our test data contains any information regarding the economy’s performance. As a result, our model’s performance will degrade.\n\n119\n\n120\n\nStabilizing the Machine Learning System\n\nFigure 6.5: Concept shifts\n\nThis can be a tricky dataset shift since the distribution shift is not related to the data that we train on, but rather external information that our model may not have. Most of the time, these dataset shifts are cyclical and/or seasonal.\n\nImportant Note Visualizing your data and calculating the different probabilities with regard to your data is the best way to help determine and identify which dataset shift you are dealing with. From there, you can decide how you will address your dataset shift.\n\nWhen it comes to identifying most dataset shifts, there is a process that we can follow to help us. It includes the following steps:\n\nPreprocessing the data\n\nCreating random samples of your training and test sets on their own\n\nCombining the random samples into one dataset\n\nCreate a model using one feature at a time while using the origin as the output value\n\nPredicting on the test set and calculating the Area Under Curve – Receiver Operating Characteristics Curve (AUC-ROC)\n\n\n\nIf the AUC-ROC is greater than a certain threshold, for example, 80%, we can classify the data as having experienced a dataset shift\n\nHandling and stabilizing dataset shifts\n\nFigure 6.6: An example of an AUC-ROC graph (a value close to 1 indicates a strong model)\n\nHandling and stabilizing dataset shifts\n\nNow that we have established the methods for identifying the different types of dataset shifts, we can discuss the different ways of addressing these shifts and stabilizing our machine learning models.\n\nWhile there are many ways to address dataset shifts, we will be looking at the three main methods. They consist of the following:\n\nFeature dropping\n\nAdversarial search\n\nDensity ratio estimation\n\nWe will first look at feature dropping. This is the simplest form of adjusting dataset shifts. As we determine which features are classified as drifting, we can simply drop them from the machine learning model. We can also define a simple rule where any features with a drift value greater than a certain threshold, for example, 80%, can be dropped:\n\n121\n\n122\n\nStabilizing the Machine Learning System\n\nFigure 6.7: Feature Dropping Process\n\nWhile this is a simple change, this is something that needs to be considered carefully. If this feature is considered important when training your machine learning model, then it is worth reconsidering whether this feature needs to be dropped. Also, if the majority of your features pass the threshold for being dropped, you may want to revisit your data as a whole and consider a different approach when addressing your dataset shift.\n\nNext, we will look at adversarial search. This is a technique that requires training a binary classifier to predict whether the sample data is within the training or test datasets. We can then evaluate the performance of the classifier to determine whether there has been a dataset shift. If the performance of our classifier is close to that of a random guess (~50%), we can confidently determine that our training and test dataset distribution is consistent. On the other hand, if our classifier performs better than a random guess, then that will indicate an inconsistency between the distribution of the training and test datasets.\n\nThe adversarial search can be split into three parts:\n\n1. From the original dataset, we will remove the target value column and replace it with a new column that indicates the source of data (train = 0 and test = 1).\n\n2. We will create and train the new classifier with the new dataset. The output of the classifier is the probability that the sample data is part of the test dataset.\n\n3. Finally, we can observe the results and measure the performance of our classifier. If our classifier performance is close to 50%, then this indicates that the model is unable to differentiate whether the data is coming from the training or test set. This can tell us that the data distribution between the training and test datasets is consistent. On the flip side, if our performance is close to 100%, then the model is confident enough to find the difference between the training and test datasets, which then indicates a major difference between the distribution of the training and test datasets.\n\nHandling and stabilizing dataset shifts\n\nFigure 6.8: Adversarial search process\n\nUsing adversarial search, we can establish three methods to address the dataset shifts we encounter:\n\nUsing the results, we can use them as sample weights for the training process. The weights correspond to the nature of how the data is distributed. The data that is similar in the actual distribution will be assigned a larger weight while that with inconsistent distribution will be given a lower weight. This will help the model emphasize the data that actually represents the real distribution it is trying to learn.\n\nWe can use only the top-ranked adversarial validation results. Rather than mitigating the weights of inconsistent samples in the testing dataset, we can remove them altogether.\n\nAll data is used for training except for the top-ranked adversarial validation results. This method can address the issues that can arise from the second method by using all the data rather than dropping features. Rather than discarding unimportant data, we can incorporate some of the data in the training data for each fold when using K-fold cross-validation during training. This helps maintain consistency while using all the data.\n\nThe final method used to address dataset shifts is called the density ratio estimation method. This method is still under research and not a commonly used method to address dataset shifts.\n\n123\n\n124\n\nStabilizing the Machine Learning System\n\nWith this approach, we would first estimate the training and test dataset densities separately. Once we have done this, we will then estimate the importance of the dataset by taking the ratio of the estimated densities of the training and test datasets. Using this density ratio, we can use it as the weight for each data entry in our training dataset.\n\nThe reason this method is not preferred and is still under research is that it is computationally expensive, especially for higher dimensional datasets. Even then, the improvements it can bring to addressing dataset shifts are negligible and not worth the effort of pursuing this method.\n\nImportant Note Feature dropping is the easiest and simplest way to address dataset shifts. Consider using this approach before using the adversarial search approach, as that option, while effective, can be a little involved and may require more effort and resources to help mitigate the effect of dataset shifts.\n\nSummary\n\nIn this chapter, we went over the general concepts of dataset shifts and how they can negatively impact our machine learning model.\n\nFrom there, we delved in deeper into what causes these dataset shifts to occur and what different characteristics dataset shifts can exhibit. Using these characteristics, we can better identify the type of dataset shift – whether it was a covariate shift, prior probability shift, or concept shift.\n\nOnce we were able to analyze our data and identify the type of dataset shift, we looked at different methods to help us handle and stabilize these dataset shifts so that we could maintain our machine learning model. We went over some techniques, such as feature searching, adversarial search, and density ratio estimation, that can assist us when dealing with dataset shifts.\n\nUsing these processes and methods, we can prevent our model from suffering from common dataset shifts that occur in the real world and continuously maintain our machine learning model.\n\nNow that we have a firm understanding of machine learning and how to maintain a robust model, we can start looking into how we can incorporate our machine learning models into our Microservices Architecture (MSA).\n\n7 How Machine Learning and Deep Learning Help in MSA Enterprise Systems\n\nIn the previous chapters, we analyzed the different general concepts of artificial intelligence, machine learning, and deep learning, and how they can be used for certain applications and use cases. From there, we looked at how to create an end-to-end machine learning system pipeline and the advantages it brings when establishing a robust system. Finally, we examined the different ways our machine learning model can degrade over time through data shifts, and the different ways we can identify and address them.\n\nHaving a firm understanding of the basics of machine learning, we can now begin to explore the use cases of machine learning in our Microservice Service Architecture (MSA) enterprise. In this chapter, we will go over the different concepts we will be proposing when integrating machine learning in to an MSA enterprise system to establish an intelligent MSA.\n\nMachine learning MSA enterprise system use cases\n\nThe space for adding machine learning to MSA enterprise systems is broad and can be open for many use cases. We can use machine learning for different types of problems that we can encounter in MSA, such as the following:\n\nSystem Load Prediction: This will determine when a service is experiencing higher than usual loads and trigger measures to prevent the system from degrading due to excessive server loads.\n\nSystem Decay Prediction: Similar to system load prediction, this will monitor the microservices and try to predict and determine anomalies in the MSA enterprise, allowing users to act and prevent certain issues from arising and negatively impacting the performance.\n\n126\n\nHow Machine Learning and Deep Learning Help in MSA Enterprise Systems\n\nSystem Security: In the age of cybersecurity, it is important to be able to protect your MSA system from targeted attacks. By studying the behavior of your MSA system, the model can predict and detect attacks that could be impacting your system.\n\nSystem Resource Planning: As your system grows and evolves, being able to properly allocate resources and adapt to your system needs is a critical part when establishing your MSA enterprise system. With machine learning, we can learn which services require more resources and how much we need to scale in order to allocate the required resources efficiently and effectively.\n\nFigure 7.1: Use cases of machine learning in MSA\n\nWhile there are many more use cases of machine learning in MSA enterprise systems, most use cases fall under these four categories. Before getting into the implementation of the different models, we need to first get an overview of the different cases and how we need to solve these different problems.\n\nWe can start by looking at system load predictions. This is a common issue that we will encounter when it comes to dealing with services in general. MSA has an advantage compared to monolithic systems, where the resources are dedicated to each microservice, allowing easier maintenance and scalability. As discussed in previous chapters, though, there could be cases where, in MSA, a microservice experiences a high load and, as a result, causes a cascading effect where the failures expand to other microservices.\n\nWith an intelligent MSA, we can train a model using different features, such as the response time, to learn the patterns of the MSA system. Similar to a microservice circuit breaker, this model will be able to swiftly determine whether a microservice is experiencing a heavy load and address the issue before it becomes too late and starts negatively impacting the other microservices.\n\nMachine learning MSA enterprise system use cases\n\nFigure 7.2: System load prediction model\n\nJust like the system load prediction model, we can build a model to find anomalies within the MSA that could lead to decaying services. Rather than focusing only on the service load for a specific microservice, we can study the entire MSA and learn the different patterns of how it operates at a larger scale.\n\nCertain systems can experience different system loads and bugs over certain times and periods. For example, our service may encounter spikes in requests over certain periods such as holidays and seasonal events, where the user count may drastically increase. Allowing the model to learn and understand the MSA and how it operates over time can prepare the model to better detect anomalies and prevent false positives.\n\nAlso, rather than monitoring separate microservices, we can evaluate clusters of microservices and how they interact with the entire MSA. This way, we can identify certain bottlenecks and bugs that could arise in our MSA.\n\nFigure 7.3: System decay prediction model\n\n127",
      "page_number": 132
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 141-148)",
      "start_page": 141,
      "end_page": 148,
      "detection_method": "topic_boundary",
      "content": "128\n\nHow Machine Learning and Deep Learning Help in MSA Enterprise Systems\n\nMachine learning has been thriving in the security field. With more advanced attacks and methods, it has become imperative for users to protect their systems. Machine learning has made it easier for users to create robust models that can analyze and predict attacks before they can even impact their systems, and MSA is no different.\n\nDenial of Service (DoS) is a cyber-attack intended to prevent users from accessing certain services. These attacks are becoming more sophisticated with the advancements in technology. With machine learning, we can train our model to learn about our MSA and simulate DoS attacks such that it can be able to determine whether our MSA is under attack. With that, we can notify the security team or deploy countermeasures to fight back against certain attacks and maintain the integrity of our MSA.\n\nFigure 7.4: System security model\n\nA part of the self-healing process includes resource allocation for certain microservices when your MSA begins to grow and expand. After a time, you may experience a growth in users and as a result, your microservices will have increased request volume. A model may incorrectly identify a problem and offer solutions that wouldn’t address the core problem.\n\nThus, building an advanced model where it can track the gradual growth of the MSA and determine when certain services need more resources can be a critical part of the system’s self-healing process. A successful implementation of the model can greatly improve system reliability as it can properly and efficiently allocate resources more effectively.\n\nFigure 7.5: System resource planning model\n\nEnhancing system supportability and time-to-resolution (TTR) with pattern analysis machine learning\n\nImportant note The different types of models we can use in our MSA are not mutually exclusive. It is possible, and common, to combine the different use cases to build a more intelligent MSA. Understanding how your MSA operates and determining the different weaknesses it may have makes it easier for the user to determine which models to approach.\n\nWith certain use cases, some models can work better than others due to the nature of the problem. Now that we have looked at the different concepts where we can apply machine learning to our MSA, we can begin to dive deep into the different implementations and models we can use to build our machine learning models in the next few sections.\n\nEnhancing system supportability and time-to-resolution (TTR) with pattern analysis machine learning\n\nBefore we can begin to make our MSA intelligent, we must first understand how our system performs by leveraging machine learning models to learn the common trends and patterns for the performance of our services. From there, we can establish a baseline that can be used as a reference for other advanced models to use.\n\nAs discussed in Chapter 4, supervised learning can occur when we have a labeled test set. For our case, we can mostly use supervised learning because we can easily capture the response time of our services in the MSA and use that as our data label.\n\nFrom there, we have a wide variety of techniques that we can use to create our machine learning model. For simplicity, we can use a linear regression model to predict the expected response time for a particular microservice. Using this output, we can design a system where we can configure a set threshold where, if we detect that our MSA will reach a certain response time, we can notify the developers or initiate a program to resolve the issue before it occurs.\n\nIf we recall from Chapter 6, we discussed data shifts and how they can impact our model. It’s common for MSAs to grow and expand as time passes due to an increase in user counts or seasonal occasions. As a result, we may see a growth in response times and metrics for our MSA. This may falsely trigger an alert notifying us of abnormal response times when, in reality, it accurately depicts the normal behavior of the MSA.\n\n129\n\n130\n\nHow Machine Learning and Deep Learning Help in MSA Enterprise Systems\n\nTherefore, it is important to continuously collect data and train our model to adapt to expected changes such that it is able to learn how the system grows and to correctly identify changes that are not common to our MSA.\n\nFigure 7.6: Performance baseline system flow\n\nWhile this system is enough for simple problems, we can combine this model output with other advanced models to create a more end-to-end system, where we can understand the health of the MSA and make better decisions. In the next section, we will discuss how we can use deep learning to implement self-healing for our system.\n\nImportant note It’s important to start with a simple model, such as a linear regression model. Once the proof of concept works, you can improve your system by incorporating more advanced models and techniques.\n\nImplementing system self-healing with deep learning\n\nNow that we have determined the baseline for our system, we can use this to our advantage to create a more intelligent MSA, where we can detect anomalies and perform system self-healing. This way, we can be more proactive in resolving issues before they arise and save cost and time.\n\nAnomaly detection is an effective method for identifying any abnormal events or trends that may occur in a system or service. For example, we can use anomaly detection for determining credit card fraud. We can use the user’s purchasing trends and, based on that information, we can determine when the user has been a victim of credit card fraud.\n\nSimilar to credit card fraud detection, we can apply our anomaly detection to our MSA. Before we can go to the different models that we can use to achieve our anomaly detection, let us first understand the different types of anomalies:\n\nPoint Anomaly: This occurs when an individual point is far off from the rest of the data\n\nContextual Anomaly: Data is considered this way when it is not in line with the general data trend due to the context of the data\n\nCollective Anomaly: When a group of related data instances is anomalous with respect to the whole dataset\n\nImplementing system self-healing with deep learning\n\nFigure 7.7: Anomalous data\n\nAn anomaly detection model can be done in the following ways:\n\nSupervised Anomaly Detection\n\nUnsupervised Anomaly Detection\n\nA common model we can use for unsupervised learning is an autoencoder. As mentioned in Chapter 4, an autoencoder is a neural network composed of an encoder and a decoder. The general purpose of an autoencoder is to take the data and compress it to a lower dimension similar to PCA. That way, it is able to learn the correlations and patterns between the different data features. Once it learns the patterns, it can feed the compressed data forward to the decoder where it tries to “recreate” the original data with what it has learned in the encoder stage.\n\nWhile experts can study the data to determine what response times are considered an anomaly for a particular MSA, we can leverage machine learning to help us find patterns and relationships that may be hard to see even for an experienced developer.\n\nWith the learned parameters, we can then use this in our supervised regression models to achieve more accurate results when detecting anomalies and prevent false positives from occurring.\n\n131\n\n132\n\nHow Machine Learning and Deep Learning Help in MSA Enterprise Systems\n\nFigure 7.8: Self-healing using deep learning\n\nImportant note Labeling data to be used in a supervised machine learning problem can cost time and money. You can leverage unsupervised machine learning models to help you predict and label your unlabeled data. From there, you can feed your newly labeled data into your supervised machine learning problem, thus taking advantage of unsupervised learning. Keep the newly labeled data in mind and make sure it doesn’t negatively impact your supervised machine learning problem.\n\nThese are some of the ways in which we can take advantage of machine learning and deep learning to create an intelligent MSA, where it can detect anomalies in the system and react swiftly. These use cases can be adjusted and enhanced based on the user’s needs and the demands of their MSA by using different models and techniques.\n\nSummary\n\nThis chapter discussed how we can implement machine learning and deep learning in our MSA.\n\nWe first looked into the different use cases for how machine learning can be used to build an intelligent MSA. The uses cases can be grouped into four categories:\n\nSystem Load Prediction\n\nSystem Decay Prediction\n\nSystem Security\n\nSystem Resource Planning\n\nWe discussed each category and what role it plays when looking into creating an intelligent MSA.\n\nFrom there, we started looking into using supervised machine learning to create a pattern analysis model where it can learn our MSA and create a performance baseline model. Using this, we can determine whether our microservice performance is abnormal. We can then use this to either perform actions based on a threshold or use this baseline to build a more advanced model.\n\nAlong with our supervised machine learning model, we can use deep learning to create a more sophisticated model, such as autoencoders, to find anomalies in our MSA. Using the combination of these two models, we can create a set of rules to perform based on certain predictions, such as that our MSA can self-heal with minimal human intervention. This allows us to save time and money when repairing and debugging our MSA.\n\nIn the next chapters, we will be taking what we’ve learned so far and starting to build our own MSA with practical examples and integrating machine learning to create our very own simple intelligent MSA.\n\nSummary\n\n133\n\nPart 3: Practical Guide to Deploying Machine Learning in MSA Systems\n\nThe final part of this book will bring everything covered so far to life. It will walk you step by step through the design and development of an intelligent Microservices Architecture (MSA) system, with hands-on examples and actual code that can be imported for real-life use cases. The part will provide an in-depth understanding of how to apply the DevOps process to building and running an intelligent enterprise MSA system, from the very start to operations and maintenance.\n\nThe part starts with the basics of containers, Docker, and how to install and run Docker containers. We will also gain hands-on experience in handling data flows between containers to build a simple project. Additionally, the chapter will cover a practical guide on building specific-purpose AI and how to infuse AI services into an MSA system.\n\nThis part delves into the application of DevOps to enterprise MSA systems, with a focus on organizational structure alignment and how DevOps can impact the MSA and its operations. We will learn how to apply DevOps throughout the project life cycle, from start to operations and change management and maintenance.\n\nThe part also covers how to identify and minimize system dependencies, apply Quality Assurance (QA) testing strategies, build microservice and MSA test cases, and deploy system changes and hot updates. The section will also provide practical examples of how to overcome system dependencies and apply testing strategies effectively.\n\nIn conclusion, the final part of this book will provide you with a comprehensive guide on how to design, develop, and maintain an intelligent enterprise MSA system, with a focus on practical, hands-on experience and real-life use cases. By the end of this part, we will be equipped with the skills and knowledge necessary to build our own intelligent MSA system and take the first step toward achieving better business results, operational performance, and business continuity.\n\n136\n\nPart 3: Practical Guide to Deploying Machine Learning in MSA Systems\n\nThis part comprises the following chapters:\n\nChapter 8, The Role of DevOps in Building Intelligent MSA Systems\n\nChapter 9, Building an MSA with Docker Containers\n\nChapter 10, Building an Intelligent MSA System\n\nChapter 11, Managing the New System’s Deployment – Greenfield vs. Brownfield\n\nChapter 12, Deploying, Testing, and Operating an Intelligent MSA Systems",
      "page_number": 141
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 149-158)",
      "start_page": 149,
      "end_page": 158,
      "detection_method": "topic_boundary",
      "content": "8 The Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nIn previous chapters, we covered what MSA is and the advantages of MSA over monolithic architecture. Then, we discussed, with examples, how to refactor a monolithic application into an MSA, and then talked about different patterns and techniques to enhance the performance of an MSA system.\n\nWe also discussed the different ML and DL algorithms with hands-on examples, how they can be optimized, and how these ML and DL algorithms can help further enhance the stability, resilience, and supportability of an MSA system in order to build a “smart MSA” or “intelligent MSA” system.\n\nOver the next few chapters, we will further enhance our ABC-MSA system and try to apply what has been learned so far using some hands-on installations and code examples. However, before we do so, we need to discuss the different concepts of DevOps in this chapter, and how to apply the DevOps process to building and running an MSA system.\n\nIn Chapter 1, we briefly talked about DevOps in MSA. In this chapter, we will expand on the subject and dive into the details of the role of DevOps in building intelligent MSA.\n\nThe following topics are covered in this chapter:\n\nDevOps and organizational structure alignment\n\nDevOps processes in enterprise MSA system operations\n\nApplying DevOps from the beginning to operations and maintenance\n\n138\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nDevOps and organizational structure alignment\n\nIn a traditional software development organization, the software delivery process is matured and built according to how that traditional organization is structured. Typically, we have a business team that defines the core business specifications and requirements, followed by another team of architects that builds how the system is supposed to be structured. In the traditional software model, we also have design engineers who write the functional specs, a development team responsible for writing the code, a QA team to test the code quality, then a release team, an operations team for post-release operations, a support team, and so on.\n\nFigure 8.1: Traditional development structure\n\nWith all these teams involved in the pipeline in the traditional software release cycle, mostly sequential hand-offs between teams, silos, dependencies in between, cross-communication issues, and the possibility of finger-pointing during the process, the release cycle can take weeks or months to finish. For an MSA, this is not acceptable.\n\nImportant note The whole purpose of MSA is to simplify, speed up, and optimize software releases and updates. Applying the traditional methodology to MSA system development just doesn’t work and defeats the purpose of adopting an MSA to begin with.\n\nDevOps\n\nDevOps is one of the major processes adopted in modern software development organizations to help streamline the release process and optimize it so that an organization can make multiple seamless release updates every day with no service interruption whatsoever.\n\nDevOps and organizational structure alignment\n\nDevOps is a combination of processes that allow you to take an application from development to operation smoothly. Enterprises need dedicated and well-defined DevOps processes to manage their solution development, hosting, and operations.\n\nThe primary need of a DevOps team is to implement engineering techniques in managing the operations of applications. While this sounds simple to do, several mundane and random activities are carried out by the operations teams. Streamlining these tasks is the biggest challenge in adopting DevOps.\n\nFigure 8.2: Teams working together in a DevOps fashion\n\nThe primary responsibility of the development team is to build the application. However, they also need to take care of other aspects of the application, such as the application performance, usage analytics, code quality, activity logging, and solving code-level errors.\n\nOn the other hand, the operations team faces a completely different set of problems. Their concerns include managing the availability of the applications, ensuring performance through higher scalability, and improving the monitoring of the solution ecosystem, the allocation of resources, and the overall system analytics. DevOps processes handle all of these concerns for all parties involved in the process.\n\nFigure 8.3: DevOps life cycle\n\n139\n\n140\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nFigure 8.3 is similar to what we discussed in Figure 1.11. One new thing to add here is that the PLAN stage is where the software roadmap is defined and gets broken down into major requirements, called epics. These epics are broken down into a collection of short end user requirements, called user stories. More info on that will come in the next section.\n\nWell, OK then, if an organization is to adopt an MSA, they should embrace a DevOps culture as well. Simple, right? Not quite!\n\nAdopting a DevOps culture within a traditional organizational structure would have many misalignments that are guaranteed to hinder the DevOps cycle. The efficiency and speed of your release cycle will be as fast as the slowest process in your cycle. The software development organization itself has to shift its culture to align with DevOps, not the other way around. Many other methodologies and technologies will need to be adopted as part of the new shift to DevOps. The organizational structure itself may also need to be tweaked to align with the new DevOps methodologies.\n\nThe DevOps team structure\n\nSetting up a DevOps team is the first step toward organizational transformation. However, you cannot expect to have a fully-fledged DevOps team without considering the existing organizational structure and how the organization is aligned with the existing development cycle.\n\nIt is imperative to have an interim phase in which the development and operations teams can function reasonably within the existing traditional organization. Both traditional Dev and Ops teams then slowly morph themselves into a true DevOps structure as the organization modernizes its structure to fit into the new culture.\n\nOne of the recommended approaches in the organizational transformation scenario is to develop a small DevOps team to work as a link between the existing development team and the operations team. The DevOps team’s main objective in this particular case is to cross-function between both Dev and Ops teams to map deliverables in between, slowly familiarize both teams with the new methodology, and start applying basic DevOps methodologies within both teams so that they can be unified in the future.\n\nFigure 8.4: The DevOps team as a link between Dev and Ops during the organizational transition\n\nDevOps processes in enterprise MSA system operations\n\nTeam communication, collaboration, energy, trust, and a solid understanding of the entire development cycle are all paramount to the new DevOps team’s success. Therefore, you must identify the right skills and people who can push the activities of the DevOps team forward. These skills may include, but are not limited to, coding skills, mastering DevOps and Continuous Integration/Continuous Development (CI/CD) tools, and automation.\n\nAs the organizational structure and the teams mature and become more familiar with the new methodologies, merging the old Dev, old Ops, and the interim DevOps teams into a single new DevOps team becomes essential. Staying in the interim stage too long is likely to create even more disruptions than using the traditional development cycle for developing the MSA system.\n\nThe size of the DevOps team can be as small as 3 engineers, and as large as 12, depending on the organization’s size, existing structure, and the effort being put into the organizational transformation. Usually, a number between 3 and 12 is ideal. Having a larger team is likely to create more challenges than benefits and start negatively impacting the team’s overall performance.\n\nBegin the process of transformation in a step-by-step manner, starting with infrastructure codification, the automation of infrastructure provisioning, source code version control, infrastructure monitoring, code build automation, deployment automation, test orchestration, cloud service management, and so on.\n\nWe know now how the organizational structure is relevant and important when embracing DevOps. We still need to understand some other details on the processes that will complement DevOps in order to achieve our goal of developing an efficient, high-quality MSA system with a short time-to- market and seamless updates.\n\nIn the following section, we will examine some other considerations that need to be taken into account when developing an MSA system.\n\nDevOps processes in enterprise MSA system operations\n\nMicroservices development is a fast-paced process and requires all other development processes to run at the same pace. Right from the beginning of the development of the MSA system, source code management and configuration management are needed to provide the correct support to the DevOps team. This is followed by code scans and unit test orchestration in the development environment.\n\nHaving specific standard methodologies and best practices applied among the different team members is essential to manage the efficiency and fast pace of the development cycle. The following discusses what the Agile methodology of development is and how it helps in DevOps operations, and the importance of automation in DevOps.\n\n141\n\n142\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nThe Agile methodology of development\n\nDefining and accomplishing DevOps processes go hand in hand with adopting a development methodology that can fully support and leverage the power of DevOps. Although there are many ways to apply DevOps methodologies within your organization, the Agile methodology is the one best suited for DevOps.\n\nThe Agile development methodology breaks down the main requirements into small consumable changes – stories and epics. These small, consumable increments help the team achieve short wins throughout the journey of handling the project from start to end.\n\nAs shown in Figure 8.5, the Agile team members meet periodically, typically every week or two, to plan, define, and agree on the epics and stories. These requirements are then put into a backlog and, until the next Agile team meeting, the team members work to deliver the requirements from that backlog:\n\nFigure 8.5: Sprint cycle in Agile development\n\nIn Agile development, the weekly or biweekly recurring meetings are called Sprint Planning Meetings, and the time between these meetings when developers are working on the backlog is called a sprint.\n\nIn order for team members to check on the status of each defined epic and story, they usually meet daily to examine the sprint backlog and refine whatever needs to be refined to ensure timely delivery. This daily meeting is called a Daily Scrum.\n\nThe Agile team handles continuously evolving user stories and requirements within a sprint cycle.\n\nDevOps processes in enterprise MSA system operations\n\nIn an endeavor to deliver a high-quality product at a fast pace and low cost, Agile teams apply the following principles:\n\nNo blocking time for day-end activities, such as building and deploying the latest code\n\n\n\nImmediate feedback on the code quality and functional quality of the latest code\n\nStrong control, precision monitoring, and continuous improvement of the daily activities of the development team\n\nFaster decision-making for accepting new stories, releasing developed stories, and mitigating risks\n\nA reduced feedback loop with the testers, end users, and customers\n\nRegular review and introspection of the development and delivery processes\n\nA development team abiding by the Agile manifesto and following all the Agile principles should always look for ways to remove unwanted roadblocks from their process model.\n\nThe Agile methodology of development can be applied to develop and deliver all types of software projects; however, it is more suited to the development of microservices-based applications. It is important to view the scope and structure of microservices to align them with Agile and DevOps practices.\n\nOne of the most important pillars of the Agile and DevOps process is the use of on-demand, needs- based resources. This is usually catered to by the use of a cloud-based infrastructure. All the resources required by the Agile teams developing microservices need to be provisioned promptly and in the right quantity or with enough capacity. Cloud infrastructure is best suited to these requirements. Resources can be scaled up and down based on need and demand.\n\nOn-demand cloud workloads needed during the DevOps cycles are not necessarily deployed on the organization’s private infrastructure; they may very well be deployed using a public cloud provider, or they may be deployed in a hybrid cloud fashion.\n\nAutomation\n\nWith the increase in the complexity of the IT infrastructure and MSA adoption and the demand for an Agile development cycle and short time-to-market, the need to streamline the infrastructure management processes becomes the most pressing need for any organization. A big part of managing an MSA’s infrastructure, DevOps, CI/CD, and Agile development is automation.\n\nAutomation provides immense benefits to modern organizations. A few of these benefits include, but are not limited to, the following:\n\nBetter human resource utilization: With automation in place, staff can focus on other activities that may not be automatable, hence optimizing the use of the organization’s workforce, scaling better on other projects, and distributing responsibilities according to the available and required skill sets.\n\n143\n\n144\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nBetter time-to-market and better business agility: An automated process can certainly save a lot of time that would be otherwise consumed by manual repetitive work and potential dependencies. A job that may traditionally take days can be done in minutes when automation is in place.\n\nHigher reliability and greater business continuity: Complex and time-consuming tasks are simplified into simple keystrokes or mouse clicks. Accordingly, human error is significantly minimized, and operational reliability is largely increased.\n\nBetter compliance: Compliance can be built into automation tools, providing better policy enforcement with minimum effort. Compliance includes industry compliance, best practices, and organizational standards as well. Industry standards may include the General Data Protection Regulation (GDPR), Payment Card Industry Data Security Standard (PCI DSS), Health Insurance Portability and Accountability Act (HIPAA), and Safeguard Computer Security Evaluation Matrix (SCSEM).\n\nAutomation is often used for the fast-paced and high-quality delivery of applications. DevOps is the key process that helps automate various phases of development and delivery. In fact, DevOps is the culture that helps organizations avoid repeated, time-consuming manual steps and efforts. There are various tools, frameworks, and processes within the ambit of DevOps that are needed for successful automation.\n\nMost of the challenges within DevOps and MSA operations cannot be addressed manually – hence, the need for automation in DevOps and MSA is extremely high. Automation is needed in every area of delivery, from the time the microservice is developed to the time the microservice is deployed in the production environment.\n\nFigure 8.6: The four pillars of DevOps\n\nApplying DevOps from the start to operations and maintenance\n\nIn essence, modern enterprise system development needs DevOps to be able to respond to the dynamic and constantly growing needs of organizations, and DevOps depends heavily on four pillars: MSA, Agile Development, CI/CD, and Automation. These four pillars, as shown in the preceding diagram, play a significant part in DevOps success, and hence, in the success of modern enterprise system development.\n\nMoreover, as we will discuss later in this chapter, AI applications are very hard to test and manage manually, and automation plays a big part in managing the entire DevOps cycle of AI applications.\n\nApplying DevOps from the start to operations and maintenance\n\nEvery step of a microservices rollout requires a corresponding DevOps step. The confluence of the microservices development process with the DevOps process helps empower the Dev and Ops teams. The following is a detailed look at different facets of the DevOps process.\n\nSource code version control\n\nThe Agile teams working on microservices require specific version control to be in place. Three aspects of version control need to be carefully defined for each microservice:\n\nThe setup and management of version control tools, such as Git, SVN, CVS, and Mercurial.\n\nThe version format and nomenclature for the application, such as a format to indicate the application version, the major-change version, the minor-change version, and the build or patch number – for example, version 2.3.11.7.\n\nThe branching strategy for the source code. This is extremely important for microservices development with multiple teams working on separate microservices. Teams need to create separate repositories for each microservice and fork out different branches for each major or minor enhancement.\n\nConfiguration management and everything as a code\n\nConfiguration management is the practice of managing changes systematically across various environments so that the functional and technical performance of the system is at its best. This includes all the environments needed to develop, test, deploy, and run the MSA system components.\n\nWith so many moving parts in an MSA enterprise system, it is essential to identify which parts of the system need their configuration to be maintained and managed. Once these parts have been identified, their configuration will need to be controlled and regularly audited to maintain the overall health of the entire MSA system.\n\n145\n\n146\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nAs the DevOps process matures, and as the MSA system components mature, things become very complex to manage and configure manually, and automation becomes critical for smooth and successful configuration management.\n\nConfiguration management tools can automatically and seamlessly manage the different aspects of the system components. These tools make adjustments as needed during runtime and whenever else, and in accordance with the version of the application, the type of change, and the system load.\n\nOne of the objectives of DevOps is to codify all the aspects of development as well as deployment, including the infrastructure and the configuration. The entire environment can be built from the ground up and quickly provisioned using Infrastructure-as-a-Code (IaaC) and Configuration-as-a-Code (CaaC).\n\nIaaC and CaaC are essential components of configuration management. Both are descriptive files typically written in languages such as Ansible, Terraform, Puppet, Chef, or CloudFormation.\n\nWith IaaC and CaaC, DevOps teams can easily spin up new workloads for different purposes. Workloads can, for example, be configured for testing, specify the properties of each workload based on the test cases involved, and control deviations from the main workload parameters.\n\nCI/CD\n\nAs pointed out earlier in Chapter 1, CI/CD is an integral part of DevOps and plays the most important role in releasing MSA system updates. CI/CD ensures that the code is immediately and periodically built and pushed into the CI/CD pipeline for quick testing and feedback.\n\nAs shown in the following CI/CD pipeline diagram, developers focus primarily on working on the sprint backlog and push the code updates to the team repository, and it gets downloaded from there to the CI server.\n\nFigure 8.7: CI/CD pipeline and process flow",
      "page_number": 149
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 159-168)",
      "start_page": 159,
      "end_page": 168,
      "detection_method": "topic_boundary",
      "content": "Applying DevOps from the start to operations and maintenance\n\nThe CI server automatically runs preset test cases against the code and then pushes the code to the testers upon it passing all the test cases. Should any of the automated tests fail, the code doesn’t move further along the pipeline, and an error report of all the test failures is sent back to the developers.\n\nIn contrast to the traditional development cycle, in which developers may find out about their code test results days or weeks after their code has been submitted for testing, in CI/CD, developers will get a report of their code problems within minutes. This early visibility into code errors gives developers the chance to immediately work on fixing these errors while working on the original code. Hence, they can continuously enhance the code for release and deployment.\n\nUpon the code successfully passing all CI server tests, the code is tested further by the DevOps team testers. Testers then either push the code to release and deployment if no errors are found or return it for further fixes and enhancements.\n\nThis CI/CD pipeline enables developers to make frequent code merges; do unit testing, integration testing, code scans, and smoke testing; release; and deploy multiple times every single day – something that is not remotely possible using a traditional development cycle.\n\nThe DevOps team needs to identify a tool that can manage the entire CI/CD pipeline. DevOps helps add hooks and steps to include external executables and scripts for performing additional activities during the code build and deployment. Some of the most common and widely used CI/CD tools include Jenkins, Bamboo, and CircleCI.\n\nCode quality assurance\n\nEnsuring high-quality code, both in terms of coding standards and security vulnerabilities, is another important activity within DevOps. This is in addition to ensuring the accuracy of the application’s business logic itself.\n\nCode quality touches upon the concept of static and dynamic analysis of the code. Static analysis of the code is performed on the code itself before it gets executed. It is meant to uncover code smells, dirty code, vulnerable libraries, malicious openings in the code, and violations of code standards or best practices.\n\nDynamic code analysis is performed on the application during or after its execution. It is meant to uncover runtime errors due to the load, unexpected input, or unexpected runtime conditions in general.\n\nMany tools that help perform code scans as part of CI/CD are available. These include, but are not limited to, SonarQube, Fortify SCA, and Raxis.\n\nImportant note Testing AI applications is more challenging than testing regular applications. Certain aspects of AI applications do not exist in regular applications.\n\n147\n\n148\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nAI applications are non-deterministic – how they will behave in real situations is somewhat uncertain. Accordingly, expecting a specific outcome during AI application testing may not be viable. It may very well happen that the application being tested produces different outcomes with the same input or test criteria.\n\nMost AI applications are as good as their training data quality, which makes AI applications subject to training data bias or unconscious bias. Imagine, for example, you are writing an AI module to predict home prices in any part of the United States, but your training data is 90% from a specific region within a specific state. Your AI model will accordingly be biased toward the area from which 90% of the training data came, so testing the AI application may require running tests against the training data itself. This may sound easy in this home price prediction case, but how would you make sense of other pieces of training data in more complex situations?\n\nLet’s assume that we can accurately test AI/DL applications despite all the training data challenges and their non-deterministic behavior. AI/DL applications constantly learn, train, and change their behavior, so by the time the code is running in production, the application is already learning and changing its behavior. The tests that have been completed a day or a couple of days earlier may not be valid anymore.\n\nThere are, of course, ways to overcome all these challenges. First of all, you will need to curate and validate the training data. You may need to perform both automated and manual tasks to validate the training data, including checking for data biases, data skews, distribution levels, and so on.\n\nWe will also need to test the AI algorithm and how the regression model performs against different sets of test data. The variance and mean square error of the model will also need to be examined and analyzed.\n\nAI application testing tools are available on the market today and grow in number every day. The quality of these tools is constantly improving and can be a huge help to DevOps teams. AI testing tools are usually specialized based on the AI algorithms being used. Examples of different AI test tools include, but are not limited to, Applitools, Sauce Labs, and Testim.\n\nMonitoring\n\nWith the advent of DevOps, standard monitoring has upgraded to continuous monitoring and covers the entire development cycle, from planning to deployment and operations.\n\nMonitoring covers different aspects of the DevOps process and the components needed for the entire application to be developed, tested, deployed, and released, as well as for post-release operations to ensue. This includes infrastructure monitoring and the application itself.\n\nInfrastructure monitoring includes the on-premises infrastructure, virtual cloud environments, networks, communications, and security. Application monitoring, on the other hand, involves performance, scalability, availability, and reliability. Resource monitoring includes the management and distribution of resources across multiple pod replicas within and beyond the physical or virtual workloads.\n\nApplying DevOps from the start to operations and maintenance\n\nDevOps monitoring helps team members respond to any operational issues that arise during the DevOps pre-release or post-release cycles, hence enabling the DevOps team to be able to rectify, readjust, and make any necessary changes during the CI/CD pipeline.\n\nIdeally, monitoring alerts trigger automatic actions to try to respond and fix a problem that has been detected. However, knowing that’s not always possible, manual intervention is usually needed. Monitoring helps the DevOps team shift left to earlier stages in the development cycle to enhance their test cases, and accordingly, increase the application quality and minimize operational problems later on in the development cycle.\n\nAI algorithms, as discussed earlier in Chapter 7, and as we will give more examples of later in this book, can detect any application behavior anomalies and automatically try to self-heal to prevent application operations from being disrupted.\n\nThere are many environment-specific tools available for DevOps monitoring, including Nagios, Prometheus, Splunk, Dynatrace, and AWS CloudWatch for AWS cloud environments.\n\nDisaster management\n\nDisaster management is an important yet often overlooked part of the DevOps process. In most cases, application recovery is seen as an extended part of the deployment process. In the cloud, it is generally considered to be an offshoot of configuring availability zones and regions for hosting an application instead of a full-fledged environment challenge.\n\nIn the case of microservices, identifying a disaster is a greater challenge than averting, mitigating, or managing it. Luckily, the CI/CD environment itself can be leveraged to test and simulate disaster scenarios. Moreover, the use of external repositories can be leveraged to recover code down to specific version numbers.\n\nNevertheless, setting up a completely separate set of environment replicas in different geographical locations, setting automatic failover, and load balancers in between can be great ways of maintaining business continuity and an uninterrupted CI/CD pipeline.\n\nUsing IaaC and CaaC tools to automate recovery is extremely helpful in bringing your applications and systems back online in minimal time in case of interruption.\n\nYou still need to define an incident response playbook as part of your DevOps. This playbook should include a detailed plan of what should be executed in each scenario. For example, a response to a natural disaster is likely different from a response to a data breach incident. The playbook needs to have different scenarios and a list of roles and procedures that need to be taken to prevent or minimize system interruptions or data loss.\n\n149\n\n150\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nSummary\n\nFor MSA systems to achieve the goals for which they were created, a certain set of methodologies will need to go hand in hand with developing an MSA system. In this chapter, we discussed a few of the most critical practices to embrace when developing an MSA system: the Agile methodology of development, DevOps processes and practices, and CI/CD pipeline management.\n\nWe also discussed how important it is to set up a DevOps team for managing microservices. We have given examples of tools to use to apply and manage DevOps when building our MSA system.\n\nIn the next chapter, we will take our first step in building an intelligent MSA system. We will talk about Docker, what it is, and why it’s relevant. We will also create isolated and independent virtual environments using Docker and then link these environments (or containers) together to deliver a simple functional part of our MSA system.\n\n9 Building an MSA with Docker Containers\n\nIn the previous chapter, we discussed how to apply the DevOps process of building and running MSA systems, and the importance of aligning the organizational structure with DevOps.\n\nWe also highlighted the importance of embracing automation and adapting agile development methodologies throughout the MSA project life cycle, and throughout the CI/CD operations.\n\nThis chapter will cover what a container is, how to install containers, how to work with them, and how to handle the data flow between containers to build a simple project. We will use Docker as our platform since it is one of the most popular and widely used platforms in the field today.\n\nThe following topics will be covered in this chapter:\n\nWhat are containers anyway, and why use them?\n\n\n\nInstalling Docker\n\nCreating ABC-MSA containers\n\nABC-MSA microservices inter-communication\n\nWhat are containers anyway, and why use them?\n\nA container is defined as an operating system-level virtualization artifact, created by grouping different finite compute resources into a self-contained execution environment.\n\n152\n\nBuilding an MSA with Docker Containers\n\nAs shown in the following figure of a container ship, containers are self-contained units, independent from any other container in the ship. The ship is the engine that is used to carry and transport containers:\n\nFigure 9.1: A container ship\n\nSimilarly, the idea with containers is to create operating system-level virtualization. This means that, from within the kernel, you group different physical machine resources, applications, and I/O functions into a self-contained execution environment. Each of these self-contained resources forms a single container, hence the name container. The Container Engine is similar to the ship in the preceding example, where the container engine is used to carry, run, and transport the containers.\n\nContainers have existed for a long time and can be traced back to Unix’s chroot in the late 1970s and early 1980s, and before we even came to learn about what we call today a hypervisor. A hypervisor is a component that enables us to spin up virtual machines (VMs).\n\nUnix’s chroot evolved later in the 1990s to Linux containers or what we call LXC, and then to Solaris Zones in the early 2000s. These concepts started to evolve with time from cgroups (originally developed by Google) and namespaces (developed by IBM) in to the container engines we see today, such as Docker, Rkt, CRI-O, Containers, Microsoft Hyper-V Containers, and more.\n\nAlthough there are similarities between containers and VMs, both still have a few fundamental differences.\n\nAs shown in the following diagram, containers share the same kernel of the host operating system but isolate and limit the allocated resources, giving us something that feels like a VM but that’s much more lightweight in terms of resources:\n\nWhat are containers anyway, and why use them?\n\nFigure 9.2: VMs versus containers\n\nIn hypervisor virtualization, each VM will have to have its own virtual hardware and its own guest operating system. In addition to all that, there is a great deal of emulation taking place in the hypervisor. Accordingly, each VM needs much more resources compared to what a container needs. Resources include CPU cycles, memory, storage, and more. Moreover, you are likely to have duplicates of the same guest OS deployed on multiple VMs for the VM to deliver the required function, thus even more overhead and waste of resources.\n\nFigure 9.2 shows the hypervisor deployed on top of a host OS. A more common hypervisor virtualization model, however, is deploying the bare-metal hypervisor on the hardware directly. In either case, the overhead is are still significantly higher than deploying containers.\n\nThe lightweight nature of containers enables companies to run many more virtualized environments in the data center compared to VMs. Since containers share resources much more efficiently than VMs, and with finite physical resources in data centers, containers largely increase the capacity of the data center infrastructure, which means containers become a better choice in hosting applications, especially in our case of MSA.\n\nContainer performance is another thing to look at. With containers, I/O virtual drivers’ communication, hardware emulation, and resources overhead are minimal, completely contrary to the case in the hypervisor virtualization environment. Accordingly, containers generally outperform VMs. Containers boot in 1-3 seconds compared to minutes in the case of VMs.\n\nApplications running on a container can directly interact with and access the hardware. In the case of hypervisor virtualization, there is always a hypervisor between the application and the VM (unless a hypervisor bypass is enabled, which has its own limitations).\n\n153\n\n154\n\nBuilding an MSA with Docker Containers\n\nWith containers, you can package the application with all of its dependencies in a contained environment that’s reusable and completely portable to any other platform. That’s one of the most important features of containers. Developers can develop an application on their development servers, ship it to the testing environment, then staging and production, and run the application without having to worry about portability and dependency issues.\n\nImportant note For all the aforementioned reasons, the most popular deployment model of microservices is the container-per-service model. This is where each microservice of the MSA is deployed on a single container dedicated to running that particular application.\n\nThe other important difference between containers and VMs is security. Since containers use the same kernel, multiple containers may very well access the same kernel subsystem outside the boundaries of the container. This means a container could gain access to other containers running on the same physical host. A single application with root access, for example, could access any other container data.\n\nThere are many ways to harden the security of containers, but none of these techniques would help containers match the VM’s total isolation security.\n\nThere are cases, of course, where using VMs would be a better option than using containers. Or in some scenarios, a mix of both VMs and containers would be the most appropriate deployment model. It all depends on the use case, the application, or the system you are deploying in your organization.\n\nIn a multi-tenant environment, where complete workload isolation is necessary, using VMs would be a better choice. Or, if you are trying to build an R&D environment for hosting critical intellectual capital, or highly confidential data or applications, a complete workload isolation will also be necessary. Therefore, in this case, using VMs would be the better option.\n\nFor our MSA example, we need a very lightweight, fast-starting, highly portable, and high-performing virtualization environment to build our MSA system. Hence, containers, with a container-per- microservice deployment model, are the better choice in our scenario. Each of the MSA system’s microservices would be deployed in its own container and would have its own development team, development cycle, QA cycle, updates, run life cycle, and release cycle.\n\nWhat are containers anyway, and why use them?\n\nThe following table summarizes the differences between containers and hypervisor virtualization:\n\nContainers\n\nHypervisor VM’s\n\nResource Usage and Overhead\n\nLightweight, Please overhead, and more efficient use of resources\n\nHigh overhead and resource-intensive\n\nContainer and Application Size\n\nAverages 5-20 MB\n\nMeasured in 100s of MB or GB\n\nPerformance\n\nHigh performance\n\nLower performance\n\nScalability\n\nEasy to scale out/high horizontal scaling\n\nScaling out is harder and consumes resources\n\nBootup Time\n\nVery short startup time (1-3 seconds)\n\nStartup time is in minutes\n\nPortability\n\nSystem-agnostic and highly portable\n\nPortability is limited\n\nDevOps and CI/CD Suitability\n\nEnables more agile DevOps and smoother CI/CD\n\nCould slow down CI/CD operations\n\nHost Hardware Access\n\nApplications access HW directly\n\nNo direct access to HW\n\nSecurity\n\nLess secure; shares the same kernel\n\nMore secure; each VM has its own OS kernel\n\nTable 9.1: Differences between containers and Hypervisor VMs\n\nDespite the many options we currently have in choosing a container engine, Docker is by far the most popular engine used today, to the extent that Docker today is synonymous with containers. That’s the main reason why we have chosen to work with Docker as our container’s engine in this book.\n\nDocker is also ideal for agile DevOps and CI/CD operations. In a CI/CD environment, the time between building a Docker image to the time it is up and running in the production environment is usually around 1-5 minutes in total:\n\n155\n\n156\n\nBuilding an MSA with Docker Containers\n\nFigure 9.3: Docker Engine container virtualization\n\nFigure 9.3 shows Docker Engine installed on the host operating system to enable the containerization of microservices or applications in general.\n\nDocker in itself may not be sufficient to manage all the CI/CD operations. Organizations usually complement Docker by using a clustering technology such as Kubernetes or Marathon to smoothly deploy, manage, and operate the containers within the cluster in which your system is running. However, in this book, we will focus on Docker itself and how to use Docker to build our MSA system.\n\nAlso, to move, test, and deploy containers, we will need to have a repository to save these containers and be able to move them to different environments. Many tools can help with that, with Docker Hub and GitHub being two of the most commonly used repositories. For our project, we will use GitHub as our project repository.\n\nSo far, we have covered what containers are, the difference between containers and VMs, and why we prefer to use containers in MSA. In the next section, we will explain the different components of Docker, how to install Docker, and how to work with Docker’s components to create a system’s microservices.",
      "page_number": 159
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 169-183)",
      "start_page": 169,
      "end_page": 183,
      "detection_method": "topic_boundary",
      "content": "Installing Docker\n\nInstalling Docker\n\nWe will start this section by talking about Docker installation. Then, we will cover the main components of Docker, the purpose of each component, and how these components relate to each other. This section will help us prepare the environment that we will use later for our ABC-MSA demo project.\n\nImportant note To maximize your hands-on learning experience, you need to follow all of our hands-on installation steps. But before doing so, please make sure you have a physical or virtual host available for the Docker installation demo before we dive deeper into this section. A virtual host can be created using virtualization software such as VirtualBox or VMWare.\n\nAlthough you can install Docker on Windows or Mac, in our demo, we will use an Ubuntu Server 22.x Linux environment to install Docker Community Edition (CE). We suggest you use a similar environment to be able to follow our installation steps.\n\nDocker Engine installation\n\nNow that we know the main components of Docker, let’s take a step back and learn how to install Docker and create different Docker images for the ABC-MSA system.\n\nThe best way to install Docker Engine is to follow Docker’s official installation guide from Docker Docs at https://docs.docker.com/engine/install/. Pick your server system platform installation guide from the list.\n\nYou may also want to install Docker Desktop on your workstation. Docker Desktop is available for download from the same installation guide referred to previously.\n\nAfter the installation is completed, verify Docker’s functionality by running the following command:\n\n$ docker --version Docker version 20.10.18, build b40c2f6 $\n\nAnd,\n\n$ docker run hello-world\n\nHello from Docker! This message shows that your installation appears to be working correctly. : :\n\n157\n\n158\n\nBuilding an MSA with Docker Containers\n\nYou may need root privileges to issue the Docker commands successfully.\n\nNow that we have installed Docker, let’s go over the main components of Docker and how to use each.\n\nDocker components\n\nThere are four main components of Docker: the Docker file, the Docker image, the Docker container, and the Docker volume. The following is a brief description of each of these components.\n\nThe Docker file\n\nThe Docker file is a text file that works as a manifest that describes how the Docker image should be built. The Docker file specifies the base image that will be used to create the Docker image. So, for example, if you were to use the latest Ubuntu version as your base Linux image for the container, you would have the following line specified at the top of your Docker file:\n\nFROM ubuntu\n\nNotice that ubuntu is not tagged with any version number, which will instruct Docker to pull the latest version available for that base image. If you prefer to use CentOS version 7.0, for example, you must then tag the base image with the version number, as shown in the following line:\n\nFROM centos:7\n\nThe specific image tag can be found on Docker Hub. Docker Hub is a public repository that stores many free Docker official images for reuse by Docker users. Among many others, base images could be Linux, Windows, Node.js, Redis, Postgres, or other relational DB images.\n\nAfter you specify the base operating system image, you can use the RUN command to run the commands that you would like to execute during the Docker image creation. These are regular shell commands that are usually issued to download and install packages and libraries that will be used in your Docker image.\n\nThe Docker file has to be named Dockerfile for Docker to be able to use it. The following is a simple Dockerfile example:\n\nFigure 9.4: A Docker file (Dockerfile) example\n\nInstalling Docker\n\nThe preceding sample Dockerfile does the following:\n\n1. Uses Ubuntu version 22.10 as the base image to run on the container that will be created later.\n\n2. Fetches the latest packages list.\n\n3.\n\nInstalls Python version 3 and the PIP Python package management system.\n\n4.\n\nInstalls a package called Ansible (Ansible is an automation tool).\n\nThe Docker image\n\nOnce you have finished composing your Dockerfile, you will need to save it as a Dockerfile to be able to use it to create the Docker image.\n\nA Docker image is a binary file that works as a template with a set of instructions on how a Docker container should be created.\n\nPlease note that a Docker image can either be created from the Dockerfile, as we are explaining here, or downloaded from a public or private repository such as Docker Hub or GitHub.\n\nTo build a Docker image, use the following command while pointing at the Dockerfile location. The following example assumes the Dockerfile is located in the user’s home directory:\n\n$ docker build –t packt_demo_image ~/\n\nThe preceding command will build an image called packt_demo_image. This image will be used later to create the container with the specs defined in the Dockerfile.\n\nThe -t option means tty, which attaches a terminal to the container.\n\nTo verify that your image has been created, use the following command:\n\n$ docker image ls\n\nYou can add the -a option to the end of the proceeding command to show all images created on the host machine.\n\nIn CI/CD operations, the images that are built are usually shared in a public or private repository so that they’re available to the project team, or even the public in some cases.\n\nThe Docker container\n\nThe last step is to run a container based on the Docker image you created (or pulled from the image repository). To run a container, use the following command:\n\n$ docker run packt_demo_image\n\n159\n\n160\n\nBuilding an MSA with Docker Containers\n\nTo verify that the container is running, use the following command:\n\n$ docker container ls\n\nThe preceding command will show only the running containers. To show other containers on the host machine, add the -a option to the end of the command.\n\nYou can also use the older version of the preceding command to verify that the container is running:\n\n$ docker ps\n\nThe following diagram shows the relationship between all four Docker components and summarizes the entire process of running a container. First, we create a Dockerfile. Then, we use that file to create the Docker image. The Docker image can then be used to create the Docker container(s) locally, or first uploaded to a private or public repository where others can download and create their Docker container(s):\n\nFigure 9.5: Docker components\n\nDocker containers have a life cycle of their own – they can run for a specific task with no regard for what their previous state is, and once that specific task is completed, the Docker container automatically terminates.\n\nInstalling Docker\n\nIn other cases, containers need to be aware of their previous status. If so, they will need to be persistent to preserve the container data after its termination. That’s when Docker volumes become very handy. Next, we will talk about what a Docker volume is and how it can be created.\n\nThe Docker volume\n\nDocker volumes are a form of storage that a Docker container can be attached to. Containers are attached to volumes to read and write persistent data, which are necessary for the function of the container.\n\nTo elaborate more, consider the Docker container for the Customer Management microservice (customer_management). If you need to create a new customer in the customer_management container, you will need to update the local data store installed in that container. If the container is not persistent, once the container terminates, all data created or changed inside that container will be lost.\n\nTo avoid this problem, we will need to create a Docker volume and attach the container to that volume. The container itself can then run and update whatever data it needs to update in its volume, and then terminate. When it starts the next time, it gets instantiated with all the previous statuses and data it had before the last termination.\n\nTo create a Docker volume for the customer_management container, for example, use the following command:\n\n$ docker volume create customer_management_volume\n\nThe following command will list all volumes created on our host machine and verify the volume we have just created:\n\n$ docker volume ls\n\nOnce we create the volume, Docker mounts a local drive space on the host machine to preserve the container’s data and its mounted filesystem.\n\nTo show more details about the volume, including the volume’s name, the local host and the container’s target mount locations, and the date and time of the volume’s creation, use the docker volume inspect or docker inspect command, as follows:\n\n$ docker volume inspect customer_management_volume [ { \"CreatedAt\": \"2022-10-14T22:24:46Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/customer_ management_volume/_data\",\n\n161\n\n162\n\nBuilding an MSA with Docker Containers\n\n\"Name\": \"customer_management_volume\", \"Options\": {}, \"Scope\": \"local\" } ]\n\nAssuming we have previously created the packt_demo_image image, to create the persistent customer_management container, we will need to attach the container to the volume we have just created using the mount points shown in the docker volume inspect command’s output. The following command will create the container, attach the volume to the container, and then run the container:\n\n$ docker run -itd --mount source=customer_management_ volume,target=/app_data --name customer_management_container packt_demo_image\n\nThe it option in the docker run command is for interactive tty mode, and the d option is for running the container in the background.\n\n/app_data is an absolute path within the container that’s mounted to the local host’s mount point. From the preceding inspect data shown, the /var/lib/docker/volumes/customer_ management_volume/_data mount point is mapped to /app_data in the container.\n\nTo verify that the container is running, use the following command:\n\n$ docker container ls\n\nIf the container terminates for whatever reason, use the -a option at the end of the preceding command to show the available container on the host. You can use the docker container start or docker container stop command, followed by the container’s name, to run or terminate any of the available containers you built on that host.\n\nNow that we have installed Docker Engine and understand the different components of Docker, we will go over how to create the main ABC-MSA containers as microservices and provide an example of how these microservices talk to each other.\n\nCreating ABC-MSA containers\n\nIn our ABC-MSA system, we are adopting a container-per-microservice approach. Therefore, we need to identify the main containers we will build, the components we need for each container in our ABC-MSA system, and then build the necessary Dockerfile(s) to use.\n\nCreating ABC-MSA containers\n\nWe are building our microservice applications using Flask. Flask is a Web Server Gateway Interface (WSGI) micro-framework that enables applications to respond to API calls in a simple, flexible, and scalable manner. We won’t discuss our applications’ code in this book, but the code is available on our GitHub with detailed documentation for your reference.\n\nIn this section, we will explain how we build our ABC-MSA Dockerfile(s), images, and microservices, how we will start to listen to API calls in each container, and how the system’s microservices will be able to communicate with each other.\n\nFor demo purposes, we will use port HTTP/8080 in the container to listen to HTTP API requests. The production environment should use HTTPS/443 and consider the tomcat server for handling all web connections.\n\nThe following is only part of the full system container setup. All the ABC-MSA system’s created files and Docker images can be found in our GitHub repository at https://github.com/ PacktPublishing/Machine-Learning-in-Microservices.\n\nABC-MSA containers\n\nThe following are the services we have previously identified for our ABC-MSA system:\n\n1. API Gateway\n\n2. A frontend web dashboard interface\n\n3. Customer Management\n\n4. Product Management\n\n5. Order Management\n\n6.\n\nInventory Management\n\n7. Courier Management\n\n8. Shipping Management\n\n9. Payment Authorization\n\n10. Notification Management\n\n11. Aggregator: “Product Ordered Qty”\n\n12. Management and Orchestration\n\nWe can code and build each of these services from scratch, but the good news is that we don’t have to. Docker Hub offers a rich library with many Docker images that we can leverage in building our microservices. Docker Hub can be accessed at https://hub.docker.com/.\n\n163\n\n164\n\nBuilding an MSA with Docker Containers\n\nWe will not go over each of these services. Instead, we will focus on the ones that provide different development and deployment approaches. Some of the services are already available through Docker Hub, and some others are similar, so one example of these will suffice. Nevertheless, all the project files will be made available in this book’s GitHub repository.\n\nAPI Gateway\n\nMany open source and commercial API gateways can be pulled from different internet repositories, including Tyk, API Umbrella, WSO2, Apiman, Kong, and Fusio, to name a few. We will use Tyk in our ABC-MSA system since it is easy to use, has comprehensive features including authentication and service discovery, and is 100% an open source product with no feature restrictions.\n\nTo install a Tyk Docker container, just follow the instructions at https://tyk.io/docs/ tyk-oss/ce-docker/.\n\nBy default, the Tyk API gateway listens to TCP port 8080. To verify your installation, issue an API call test to Tyk using the curl command, as follows:\n\n$ curl localhost:8080/hello {\"status\":\"pass\",\"version\":\"4.1.0\",\"description\":\"Tyk GW\"}\n\nIf Tyk has been successfully installed and is running on your host, you should get a dictionary output stating Tyk’s status and the current version, as shown in the preceding command output.\n\nYou can also verify that the Tyk Docker image and container were created successfully using the following commands:\n\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE redis 6.2.7-alpine 48822f443672 3 days ago 25.5MB docker.tyk.io/tyk-gateway/tyk-gateway v4.1.0 0c21a95236de 8 weeks ago 341MB hello-world latest feb5d9fea6a5 12 months ago 13.3kB $ $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac3ac1802647 docker.tyk.io/tyk-gateway/tyk-gateway:v4.1.0\n\nCreating ABC-MSA containers\n\n\"/opt/tyk-gateway/ty…\" 54 minutes ago Up 54 minutes 0.0.0.0:8080->8080/tcp, :::8080->8080/tcp tyk-gateway-docker_tyk-gateway_1 9e0f1ecfb148 redis:6.2.7-alpine \"docker-entrypoint.s…\" 54 minutes ago Up 54 minutes 0.0.0.0:6379->6379/tcp, :::6379->6379/tcp tyk-gateway-docker_tyk-redis_1\n\nWe can see the tyk image details in the preceding command output, as well as the running container and what port it is listening to. We can also see a Redis image and container. This is because Redis is a prerequisite for Tyk and is included in the Tyk installation package.\n\nThe Customer Management microservice as an example\n\nThe Customer Management, Product Management, Order Management, Inventory Management, Courier Management, Shipping Management, Payment Authorization, and Notification Management microservices are all similar in terms of how we can build and deploy the container. In this section, we will learn how to create an image that we can use to create a system microservice. We have picked the Customer Management microservice as an example.\n\nAs mentioned earlier, for these microservices to communicate with the API gateway or any other components in the ABC-MSA system, we need to have Flask installed and running, listening to port HTTP/8080 in the running container.\n\nWe also need an internal data store for our application to use and manage. And since our code will be written in Python, we need to have Python installed as well. All these required components, along with some essential dependency packages, need to be specified in our Dockerfile.\n\nNow, we need to write the Dockerfile required for creating the microservice image that we will use to create the microservice container. Each ABC-MSA container should have its own development cycle and be deployed either using the CI/CD cycle we discussed in Chapter 8 or uploaded manually to the team repository.\n\nThe following is an example of the Dockerfile that’s required for creating the Customer Management image:\n\n# Docker File for \"customer_management\" microservice FROM ubuntu\n\n# Install some dependencies/packages RUN apt-get install -y apt-transport-https RUN apt-get update RUN apt-get install -y net-tools mysql-server python3 pip git build-essential curl wget vim software-properties-common;\n\n165\n\n166\n\nBuilding an MSA with Docker Containers\n\n# Install OpenJDK RUN apt-get update && \\ apt-get install -y default-jdk ENV JAVA_HOME /usr/lib/jvm/java-11-openjdk-amd64/\n\n# Install Flask to run our application and respond to API calls RUN pip install -U flask\n\n# Expose port TCP/8080 to listen the container's application/ flask API calls EXPOSE 8080\n\n# Create the /app_data directory and make it the working directory in the container RUN mkdir /app_data WORKDIR /app_data ENV PATH $PATH:/app_data\n\n# Download the microservice app code from GitHub repo ENV GIT_DISCOVERY_ACROSS_FILESYSTEM 1 RUN git config --global init.defaultBranch main RUN git init RUN git remote add origin https://github.com/mohameosam/abc_ msa.git RUN git config core.sparseCheckout true RUN echo \"/microservices/customer_management/\" > /app_data/. git/info/sparse-checkout RUN git pull origin main\n\n# Initialize the flask app ENV FLASK_APP /app_data/microservices/customer_management/ customer_management_ms.py\n\n# Specify a mount point in the container VOLUME /app_data\n\n# Start mysql & flask services and available bash sheel\n\nCreating ABC-MSA containers\n\nRUN chmod +x /app_data/microservices/customer_management/start_ services CMD /app_data/microservices/customer_management/start_services && bash\n\nThe aforementioned Dockerfile specifies what the Customer Management Docker image should look like. The following are some insights into what each of the lines in the file will do:\n\n1. Specify Ubuntu as the Linux operating system that will be used in the Customer Management container.\n\n2.\n\nInstall some required packages:\n\n MySQL (required for our application)\n\n Python (required for our application)\n\n pip (required to be able to install Flask)\n\n The rest are some other tools needed for troubleshooting (optional)\n\n3.\n\nInstall Flask (required for our application).\n\n4. Expose TCP/HTTP port 8080 for Flask to listen to API calls.\n\n5. Create a working directory in the container to act as the mount point for saving the container’s data.\n\n6. Download the Customer Management application code from our GitHub repository.\n\n7. Set an environment variable to let Flask know what application it will use when responding to API calls.\n\n8. Use our downloaded start_services shell script to start Flask and MySQL in the container.\n\nThe start_services shell script contains the following commands:\n\nflask run -h 0.0.0.0 -p 8080 & usermod -d /var/lib/mysql/ mysql service mysql start\n\nThe first line enables Flask to listen to port 8080 on all the host network interfaces. This is OK in the development and testing environment. In the production environment, however, Flask should only be available on the localhost 127.0.0.1 network interface to limit API access to the local environment. Also, for better security, port HTTPS/443 should be used in API calls instead.\n\nAssuming the Dockerfile has been placed in the current user home directory, we now need to create our Customer Management microservice/container from the Dockerfile:\n\n$ docker build -t abc_msa_customer_management ~/\n\n167\n\n168\n\nBuilding an MSA with Docker Containers\n\nDocker will take a few minutes to finish creating the image. Once all the Dockerfile steps have been completed, you should see the following command as the last line of the docker build command’s output:\n\nSuccessfully tagged abc_msa_customer_management:latest\n\nThis signals a successful completion. Now, we can use the docker image ls command to verify that the abc_msa_customer_management image has been created successfully.\n\nThe last step is creating the container. Since the application will configure and update the MySQL database, we need to create a persistent container to retain all the changes.\n\nSimilar to what we explained earlier, we will use the docker run command to create the Customer Management container, as follows:\n\n$ docker run -itd -p 8003:8080 --mount source=customer_ management_volume,target=/app_data --name customer_management_ container abc_msa_customer_management\n\nThe p option is used to “publish” and map the ports that the container listens to with the ports the host machine listens to. So, the host machine will be listening to port 8003 for HTTP/8080 requests on the container.\n\nWe have chosen 8003 to standardize the way the host listens to the container’s API call requests.\n\nRemember that each container has a TCP stack that is different from the host’s TCP stack. So, the TCP HTTP/8080 port is only local within the container itself, but outside that particular container’s environment, that TCP HTTP/8080 port is different from the TCP HTTP/8080 port available on any other container or on the host machine itself.\n\nTo access that port from outside the realm of the customer_management container, you need to map the customer_management container’s TCP HTTP/8080 port to a specific port on the host machine.\n\nSince we need to map the local TCP HTTP/8080 port of each of the 12 containers we identified earlier, we decided to follow a specific pattern. Map the TCP/80nn port on the host machine to each local TCP HTTP/8080 of each container. Here, nn is the container’s number.\n\nFigure 9.6 shows how some of the ABC-MSA container’s TCP HTTP/8080 ports are mapped on the host machine.\n\nWe don’t have to run all the containers on a single host. The system containers could be scattered across different hosts, depending on many factors, such as how critical the service/application running on the container is, how the system is designed, the desired overall redundancy, and so on:\n\nCreating ABC-MSA containers\n\nFigure 9.6: The container’s local port mappings to the host machine’s TCP stack\n\nNow, verify that the container is running using the following command:\n\n$ docker container ls\n\nThe following command will allow you to connect to the container’s bash shell using the root privilege (a user ID of 0, as specified in the command):\n\n$ docker exec -u 0 -it customer_management_container bash\n\nThat’s all for the Customer Management microservice. In the same manner, we can create the rest of the ABC-MSA containers. We just need to make sure we use appropriate corresponding names for the other microservice’s containers and volumes and map to the right TCP/80nn port number on the host machine.\n\nThe frontend web dashboard interface\n\nThe dashboard is the main component of the user interface (UI) interaction and interacts with all services offered to the user. In our ABC-MSA example, we created a simple cart application where the user can place products in the cart and place an order.\n\n169\n\n170\n\nBuilding an MSA with Docker Containers\n\nThe Dashboard container is built the same way the customer_management container is built, as shown in the previous section. The main difference between both is the additional web server that we will need to have on the Dashboard microservice, and the ports to be exposed on the container. The Dashboard’s Dockerfile should be changed accordingly.\n\nLike all the containers we are building, the container’s local TCP port that listens to API calls is TCP HTTP/8080, and the host-mapped TCP port in the dashboard container case should be TCP/8002.\n\nThe Dashboard container will still need to listen to HTTP/80 for user web UI requests. Unless the host machine is running another application or web page on HTTP/80 port, we should be OK to use that port.\n\nNow, we need to map the HTTP/80 port on the host machine, as shown in the following docker run command:\n\n$ docker run -itd -p 8002:8080 -p 80:80 \\ --mount source=dashboard_volume,target=/app_data \\ --name dashboard_container abc_msa_dashboard\n\nThis command has an additional p option to map the HTTP/80 port on the container with the HTTP/80 port on the host machine. abc_msa_dashboard is the Dashboard microservice image.\n\nManaging your system’s containers\n\nAs you saw in the preceding examples, the docker run command can get lengthy and messy. Docker Compose helps us manage the deployment of containers. With Docker Compose, it is much easier to manage the deployment of the containers, change deployment parameters, include all system containers in a single YAML file, and specify the order of the containers’ deployment and dependencies.\n\nThe following is a sample YAML file for initializing three of the ABC-MSA containers, as we did with the docker run commands earlier, but in a more organized and structured YAML way:\n\n# Docker Compose File abc_msa.yaml version: \"3.9\" services:\n\ncustomer_management_container: image: abc_msa_customer_management ports: - \"8003:8080\" volumes: - customer_management_volume:/app_data\n\nCreating ABC-MSA containers\n\nproduct_management_container: image: abc_msa_product_management ports: - \"8004:8080\" volumes: - product_management_volume:/app_data\n\ndashboard: image: abc_msa_dashboard ports: - \"8002:8080\" - \"80:80\" volumes: - dashboard_volume:/app_data depends_on: - customer_management_container - product_management_container\n\nvolumes: customer_management_volume: product_management_volume: dashboard_volume:\n\nThe following command runs the Docker Compose .yaml file:\n\n$ docker-compose -f abc_msa.yaml up &\n\nThe f option is used to specify the YAML file’s name, and the & option is used to run the containers in the shell’s background.\n\nIn this section, we showed you how to create some of the ABC-MSA images and containers. The ABC-MSA containers are now ready to communicate with each other either directly or, as we will show later in this book, through the API Gateway.\n\nIn the next section, we will learn how we can use the containers we created, how we can issue API calls to them, and what response we should expect.\n\n171",
      "page_number": 169
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 184-197)",
      "start_page": 184,
      "end_page": 197,
      "detection_method": "topic_boundary",
      "content": "172\n\nBuilding an MSA with Docker Containers\n\nABC-MSA microservice inter-communication\n\nIn this section, we will learn how to expose APIs from containers and how containers communicate with API consumers.\n\nThe microservice application code for each container is available in the ABC-MSA project on GitHub. We recommend that you download the code to your local test environment to be able to get some hands-on experience when following the steps we will cover in this section.\n\nThere are two main ways for containers to communicate with each other. One is by using the container’s name in a Docker network, and the other is by using the container’s IP and TCP port. The following are some of the details you need to know about to be able to configure your containers to communicate with each other.\n\nThe Docker network\n\nWhen we have containers running on the same host, containers can communicate with each other on the same host using only container names and without the need to specify the container’s IP address or listening port.\n\nThe concept of using only container names is programmatically very useful, especially in cases where these IPs change dynamically. The names are usually deterministic, and by only specifying the Docker’s container name, you avoid having to apply different layers of system operations to first learn about the container’s TCP/IP details before starting to communicate with the target container.\n\nHowever, there are some prerequisites to enabling container communication through their names only:\n\nThe containers communicating with each other will all need to be on the same host\n\nWe will need to create a Docker network on the host\n\nWe will need to attach the containers to the created Docker network when running the container using the docker run command or by specifying the container’s instantiation details in the docker-compose YAML file\n\nThe following command creates a Docker network on the host machine that can be used for our ABC-MSA system’s inter-microservice communication:\n\n$ docker network create abc_msa_network\n\nThe following command lists the Docker networks configured on the host machine:\n\n$ docker network ls\n\nABC-MSA microservice inter-communication\n\nNow, attach the ABC-MSA containers to the abc_msa_network network by using the --network option in the docker run command, as shown in the following example:\n\n$ docker run -itd -p 8003:8080 \\ --network abc_msa_network --mount \\ source=customer_management_volume,target=/app_data \\ --name customer_management_container abc_msa_customer_ management\n\nUsing Docker networks is very useful in many cases. However, since we are designing our ABC-MSA system so that containers can run independently of their host location, we will be using the container’s IP/TCP communication.\n\nIn the next section, we’ll explain how the ABC-MSA microservices communicate using TCP/IP and go over some examples of how to test the communication and data exchanges.\n\nTCP/IP communication between containers/microservices\n\nSo far, we have installed Docker, built our Docker images and volumes, and started the containers of all our microservices. Now, it is time to understand how these containers interact with each other.\n\nUpon running Docker on the container’s host, the host automatically creates a virtual IP network and assigns an IP address to each running Docker container on that host. That virtual IP network is only internal to the host running the containers and cannot be accessed from anywhere outside that host.\n\nThe container’s host carries at least two IPs. There’s one inside IP that’s internal to the Docker network and that can only be recognized inside that Docker network. Then, there’s an outside IP, which is usually assigned by the Dynamic Host Configuration Protocol (DHCP) server in the organization’s network. The outside IP is necessary for the container’s host to communicate with the outside world.\n\nThe internal Docker network of the container’s host is not visible to any other host in the network. Therefore, for an outside host to communicate with a specific container in the container’s host machine, it will need to use the outside IP of the container’s host machine.\n\nAmong a lot of other information, to get the assigned IP address, as well as the inside and outside listening ports of a specific container in your system, use the docker inspect command, followed by the container’s name.\n\n173\n\n174\n\nBuilding an MSA with Docker Containers\n\nOur demo setup is shown in the following diagram:\n\nFigure 9.7: ABC-MSA container communication\n\nAs you can see, the host machine’s inside IP address is 172.17.0.100, and the outside IP address is 192.168.1.100. The container’s host is listening to the container’s mapped ports (8001 to 8012), as explained earlier.\n\nIf other hosts in the network want to send API calls to one of the ABC-MSA containers, that outside host will need to send the request to the outside IP address of the container’s host, 192.168.1.100, using the mapped port of the container it wants to communicate with.\n\nTo elaborate further, the preceding diagram and the following example show an outside host testing the API response of the Product Management container:\n\n$ curl http://192.168.1.100:8004/ <!DOCTYPE html> <head> <title>PRODUCT MANAGEMENT Microservice</title> </head> <body> <h3>Product Management Microservice Part of ABC-MSA System</ h3> </body>\n\nABC-MSA API calls return a JSON variable for easier data handling. One of the APIs we built for ABC-MSA microservices is service_info. An example of an API call for service_info is as follows:\n\n$ curl http://192.168.1.100:8004/api?func=service_info {\"service_name\": \"product_management\", \"service_descr\": \"ABC- MSA Product Management\"}\n\nIf you are communicating internally from within the Docker network (172.17.0.0), you can communicate directly with the container’s IP and listening ports. Performing the same curl test on the Product Management container from the API Gateway shell would look like this:\n\n$ curl http://172.17.0.4:8080/api?func=service_info {\"service_name\": \"product_management\", \"service_descr\": \"ABC- MSA Product Management\"}\n\nKnowing how to pass API requests and handle the API response is key to developing your MSA system. Please refer to our ABC-MSA code in this book’s GitHub repository for examples of how the API calls are issued and handled across the entire system.\n\nSummary\n\nIn this chapter, we covered the concept of containers, what they are, and how they are different from VMs. Then, we worked with Docker as one of the most popular container platforms available today. We showed you how to install Docker and create Dockerfiles, Docker images, Docker volumes, and Docker containers.\n\nThen, we applied all these concepts by building some of the ABC-MSA microservices with hands-on examples. We built the containers and showed how microservices communicate with each other.\n\nIn the next chapter, we will focus on building an AI microservice in the MSA system. We will discuss some of the most important AI/ML/DL algorithms that should be considered and implemented in an MSA system, and how these algorithms help with a system’s overall stability, performance, and supportability.\n\nSummary\n\n175\n\n10 Building an Intelligent MSA Enterprise System\n\nIn previous chapters, we gradually built the ABC-MSA to demonstrate some of an MSA system’s features, techniques, and traffic patterns.\n\nIn this chapter, we will combine both MSA concepts and AI concepts to build an ABC-Intelligent-MSA system, which is an enhanced version of our ABC-MSA. The intelligent version of the ABC-MSA will use various AI algorithms to enhance the performance and general operations of the original ABC-MSA system.\n\nABC-Intelligent-MSA will be able to examine different traffic patterns and detect potential problems and then self-rectify or self-adjust to try to prevent the problem from taking place before it actually happens.\n\nThe ABC-Intelligent-MSA will be able to self-learn the traffic behavior, API calls, and response patterns, and try to self-heal if a traffic anomaly or problematic pattern is detected for whatever reason.\n\nThe following topics are covered in this chapter:\n\nThe machine learning advantage\n\nBuilding your first AI microservice\n\nThe intelligent MSA system in action\n\nAnalyzing AI service operations\n\nThe machine learning advantage\n\nThere are many areas in our MSA where we can leverage AI to enhance the system’s reliability and operability. We will focus our system on two main potential areas of enhancement. One is to enhance the system response in case of a microservice failure or performance degradation. The second area of enhancement is to add a proactive circuit breaker role.\n\n178\n\nBuilding an Intelligent MSA Enterprise System\n\nAs we discussed in Chapter 3, the circuit breaker pattern is used to prevent a system cascading failure when one of the system’s microservices fails to respond to API consumer requests promptly. Should a microservice fail or perform poorly, our AI will try to take proactive action to fix the problem rather than waiting for the problem to be manually fixed for the system to return to normal operation.\n\nIn Chapter 7, we discussed the advantages of using Machine Learning (ML) and DL in MSA in detail. This chapter will focus on building two AI microservices to enhance our MSA system.\n\nThe first AI microservice is called a Performance Baseline Watchdog (PBW) service. The PBW is an ML microservice that creates a baseline for the expected performance of each microservice in the MSA system under a certain system load. Should the operational performance of the measured microservice fall under the performance baseline by the configurable value of x, the system should send a warning message to the Operation Support System (OSS) or the Network Management System (NMS) and should performance fall by y (which is also configurable), the system then should take predefined action(s) to try to self-rectify and self-heal the MSA system.\n\nThe second AI microservice we will build in this chapter is the Performance Anomaly Detector (PAD) service. The PAD is an ML microservice that takes a holistic view of the entire MSA system. The PAD learns the MSA performance patterns and tries to detect any anomalous behavior. It identifies “problematic patterns,” tries to automatically detect a problem before it happens, and accordingly takes proactive action to fix the faulty area of the system.\n\nBuilding your first AI microservice\n\nBefore we start building our two AI microservices, we need to think about our training and test data first – how we will collect our training data, build the model accordingly, test the model and measure its reliability, and enhance the algorithm’s reliability if needed.\n\nImportant Note The AI services we are building in our MSA system are only a proof of concept to demonstrate the value of implementing AI in MSA systems. Rather, businesses should consider an AI service or model that matches their unique needs, business process, and their deployed MSA system.\n\nWe will also need to simulate the use cases themselves. Simulate a system’s microservice failure or performance degradation, simulate a cascading failure, and we should also be able to simulate some system’s outlier patterns to see how the algorithm would detect and react to pattern anomalies.\n\nTo do all this, let’s first understand how the PBW and PAD microservices fit with the overall system’s operation and how they would normally interact with the different system’s components.\n\nBuilding your first AI microservice\n\nThe anatomy of AI enhancements\n\nThe main role of both the PBW and PAD is to enhance the stability and reliability of our MSA system. It is therefore imperative for both services to constantly watch individual microservices and the overall system performance and then take the necessary action when performance issues are detected.\n\nThe training data is first collected in a controlled environment for a specific training period, where normal, stable system operations and the average user load are simulated and applied. This can be achieved using some of the simulation tools we built, which will be discussed later in this section.\n\nThis training period creates an ideal first baseline that will be the main reference for the AI services to use during actual production time. The collected training data will then be used to build the algorithm. To achieve better and more accurate results, the training data and algorithm can be regularly tuned later when more information about real-time production traffic is collected.\n\nThe simulated load and system operations are tweaked through multiple simulation parameters. These parameters are tweaked regularly to mimic the actual acceptable operational performance. The algorithm tweaks would eventually stop (or become very minor) as the AI algorithms mature. The cycle of onboarding the AI services to the ABC-MSA system is demonstrated in Figure 10.1:\n\nFigure 10.1: AI microservices implementation in ABC-Intelligent-MSA\n\nMore information on the simulation tools and parameters is coming up in the next section.\n\nOnce the AI services are operational, they will start collecting performance stats from each of the system’s microservices through periodic API calls and then compare these performance stats with the expected performance or behavior.\n\nShould an individual microservice or the overall system performance deviate from what the AI expects to see, a system action will be triggered to either warn the system administrators or self-heal whenever possible. Figure 10.2 shows the high-level architecture of the PBW and PAD services:\n\n179\n\n180\n\nBuilding an Intelligent MSA Enterprise System\n\nFigure 10.2: PBW and PAD services in ABC-Intelligent-MSA\n\nThe PBW’s algorithm calculates the expected performance metrics based on the performance stats collected. Collected performance stats include API call response time stats, the failures or failure rate of individual microservices, the API response code, and the load applied on the microservice itself.\n\nPre-defined actions are triggered based on how far the microservice deviates from the calculated performance metric. Based on the configuration of the PBW, the higher the deviation, the more likely a proactive action is to be triggered to try to self-heal. In the case of a slight deviation, however, no healing action is supposed to be trigged; a system warning informing the system administrator is sufficient.\n\nThe following table shows some of the possible system issues that could be encountered during the operations of an ABC-Intelligent-MSA system, and the actions the PBW service would take to try to rectify the problem.\n\nThe list shown in the table is only a sample of potential issues and can, of course, grow as more use cases are considered:\n\nMicroservice Issue\n\nTriggered Action\n\nSlow responsiveness\n\nScale the microservice vertically/horizontally or restart the microservice\n\nIntermittent timeouts\n\nScale vertically/horizontally or restart\n\nAPI call HTTP response errors\n\nCheck Apache, Flask, the JVM, the Docker volume, SQL service, etc. Restart the service if needed\n\nRestart the microservice’s container\n\nService is unresponsive (down)\n\nRestart the microservice’s container\n\nTable 10.1 – Potential ABC-Intelligent-MSA operational issues and the PBW’s self-healing actions\n\nBuilding your first AI microservice\n\nThe healing mechanism can be applied to the MSA system using multiple AI services, not necessarily only using the PBW and PAD that we are implementing in our ABC-Intelligent-MSA. This is just an example.\n\nThe self-healing process\n\nAll of the PBW’s healing actions listed in Table 10.1 should not be taken in isolation from the PAD’s operations, but rather should be carefully coordinated with the PAD’s healing actions. A single issue in a microservice could (although not necessarily) trigger actions from both the PBW and PAD services at the same time and could consequently create an operational conflict.\n\nIn terms of the self-healing process, and to avoid conflict between the system’s AI services when triggering self-healing actions, whenever an action is determined and before it is triggered, the AI service sends an API call to the other AI services first (either directly or through the API gateway), declaring a Self- Healing Lock State in the troubled microservice. Accordingly, all the other AI services in the MSA system will hold off any actions that may have been planned related to that troubled microservice.\n\nDuring the self-healing lock state, the only AI service allowed to work on the troubled microservice is the Healer AI Service, which is the AI service that locked it.\n\nOnce the healer has fixed the problem and detects a normal operation in the affected microservice, the healer then sends another API call to the other AI services in the MSA system declaring that the lock state is over.\n\nIf the healer is unable to self-heal and gives up on resolving the issue, it sends an alarm to the NMS/ OSS and marks that microservice as unhealable for a specific configurable period of time, known as the Unhealable Wait Period (by default, 15 minutes).\n\nThe unhealable wait period allows other AI services to try to heal that microservice and gives the healer a breather to pace out its operation across all other microservices in the MSA system.\n\nTo prevent healers from consuming system resources by slipping into indefinite healing attempts, healers will try to heal the troubled microservices for a specific number of healing attempts, configured through the Maximum Healing Attempts value (four attempts, by default), and will then completely give up trying. If the maximum healing attempts are exhausted, a manual system intervention will be needed to fix the troubled microservice.\n\nSystem administrators can still configure indefinite healing attempts if needed, but this can consume system resources and may not be effective depending on the nature of the problem the MSA system or a specific microservice is experiencing.\n\nIf another AI service can fix the troubled microservice or the microservice is manually fixed, the original healer will automatically clear the unhealable flag of the microservice after the unhealable wait period is over.\n\n181\n\n182\n\nBuilding an Intelligent MSA Enterprise System\n\nIf on the other hand, no other AI service can fix the problem and no manual intervention is taken to fix the microservice, the original healer – and any other healer that may have tried to fix the microservice – will try to heal the microservice again once the unhealable wait period expires if and only if the troubled microservice is not in a self-healing lock state.\n\nThe following visual chart summarizes the self-healing process and may help better explain the entire process.\n\nFigure 10.3: The self-healing process in MSA\n\nIt is important to also understand the main terminology used to explain the self-healing process. The following table shows a summary of the terminology of the main components of our self-healing process:\n\nBuilding your first AI microservice\n\nTerm\n\nDescription\n\nHealer\n\nAn AI service that attempts to heal a troubled microservice.\n\nHealing Action\n\nAn action taken by the healer to try to fix an ongoing system operational issue.\n\nSelf-Healing Lock State\n\nA microservice state in which an attempt is made by the healer to fix the microservice.\n\nIn this state, only one healer (the one that initiated the lock state) is allowed to work on the troubled microservice.\n\nA microservice self-healing lock state is a state visible by the entire MSA system and not a healer-specific state.\n\nRetry Wait Period\n\nThe time the healer for which has to wait when a healing action fails before it retries. The Retry Wait Period is 2 min by default.\n\nUnhealable State\n\nThe state in which the troubled microservice is marked unfixable by a healer after a healer’s failed attempt to fix the troubled microservice.\n\nA microservice unhealable state is a healer-specific state and only visible to the healer that gave up on fixing that troubled microservice. Other healers can still try to fix the troubled microservice.\n\nUnhealable Wait Period\n\nThe time for which the healer has to wait before it starts to make another attempt to fix the troubled microservice. The Unhealable Wait Period is 15 min by default.\n\nMaximum Healing Attempts\n\nThe maximum number of attempts the healer will try after each unhealable wait period, and before the healer totally gives up on the troubled microservice and no longer attempt to fix it. By default, PBW tries 4 healing attempts.\n\nTable 10.2: ABC-Intelligent-MSA operational issues with the self-rectifying actions of the PBW\n\nSo far, we have explained the value of deploying AI services in the MSA system and shown some practical application examples to demonstrate the value of AI in MSA.\n\nIn order to build, run, and tweak AI services in MSA, we need to build certain tools to gather and log system statuses, operational dynamics, and operational statistics. In the following section, we will dive into what these tools are and how to use them.\n\n183\n\n184\n\nBuilding an Intelligent MSA Enterprise System\n\nBuilding the necessary tools\n\nThe purpose of creating project tools is to first be able to build the AI models, then simulate the entire ABC-Intelligent-MSA system, and then collect stats and analyze the system’s operations.\n\nAlthough there may be tools available online that would help us achieve our purpose, instead, we will build simple tools customized specifically for our use cases.\n\nWe created multiple tools to help us collect training and test data, simulate the system and microservices load, and measure the performance of the microservices. All the tools are available in the tools directory in our GitHub repository.\n\nThe tools also help us scrub some of the generated logs and data for analysis and potential future enhancements.\n\nThe following are the main tools we need in our ABC-Intelligent-MSA setup.\n\nAn API traffic simulator\n\nThe API traffic generator/simulator, simulate_api_rqsts.py, helps simulate the API request load for one or more of the system’s microservices.\n\nsimulate_api_rqsts creates multi-threaded API requests across multiple target microservices. API HTTP requests are then sent to each microservice in parallel.\n\nThe API load is measured by requests per minute and API requests can either be uniformly or randomly paced.\n\nThe uniformly paced requests are paced out so that the time between each API call is always the same, so if we are configuring a uniformly paced load of 600 API requests/min, simulate_api_rqsts will send 1 API call every T = 100 ms.\n\nIn the randomly-paced case, each API call is sent after a random period, TR, from the time where the previous call was sent, but so that TR can never be larger or smaller than 95% of T. So if we are configuring a randomly-paced load of 600 API requests/min, TR, in that case, will be equal to a value greater than 5 ms and smaller than 195 ms.\n\nsimulate_api_rqsts will send 1 API call every:\n\n(1-95%)T <= TR <= (1+95%)T (i.e., for 600 requests/min: 5 ms <= TR <= 195 ms)\n\nThe sum of all TRs, however, will still be approximately equal to the configured requests/min. In our example here, the load is 600 API requests/min.\n\nUniformly paced requests are better when you are manually analyzing how a particular microservice responds to the API load, while randomly paced requests are a better representation of a real-time production API request load.\n\nBuilding your first AI microservice\n\nThe microservices performance monitor\n\nThe microservices performance monitor, ms_perfmon.py, is another multithreading tool and is initially used for collecting and building the AI training data during the simulation period of ideal conditions.\n\nms_perfmon sends parallel API calls to each microservice in the system and then logs the API call hyperlink, the date and time at which it was sent, the receiving microservice response time, and the HTTP response code. The following is an example log entry of the collected data in a comma-separated format:\n\nhttp://payment_ms:8080,2022-12-28 15:48:57.271370, 0.010991334915161133,200\n\nEach microservice stat is collected in its own Comma-Separated Values (CSV) log file named after the API link itself (after cleaning up special characters). All the stat files are collected under the perfmon_stats directory in the ms_perfmon working path.\n\nIn real-time operation, both the PBW and PAD perform a similar job to ms_perfmon. They collect their own stats and measure the target microservice’s real-time performance against the baseline and the expected normal behavior.\n\nShould we extend the MSA system’s AI capabilities by including more AI services for different purposes and use cases, which will likely require each AI service to conduct its own performance statistics collection?\n\nDepending on the collection frequency and the type of data collected, as the number of collectors increases, scalability could become an issue. The ms_perfmon function, in that case, can be extended to become the main AI collector for all AI or non-AI services in the MSA system. This setup can help offload the system’s microservices and allow the MSA system to scale better.\n\nFigure 10.4: A collect-once performance stats setup in ABC-Intelligent-MSA\n\n185\n\n186\n\nBuilding an Intelligent MSA Enterprise System\n\nFigure 10.4 shows how ms_perfmon can handle stats collection on behalf of all other services in the MSA system and then act as a proxy and respond to API calls requesting whatever stats are needed for each particular AI (or non-AI) service.\n\nThe response delay simulator\n\nTo simulate a delayed response or a troubled microservice, and solely for simulation and testing purposes, we added a feature in key microservices to simulate a delayed API call response.\n\nThe delay response feature, when enabled in the microservice, has two configurable values – the minimum delay and the maximum delay. When a microservice receives an API call, it will automatically assign a random delay value between the configured minimum delay and maximum delay, and then wait for that time before it responds to the consumer’s API calls.\n\nThe feature is very helpful for simulating a cascading system failure. As will be shown later in this chapter, the response delay feature can also help demonstrate the value of using AI services to enhance the operations of the MSA system compared to using the short circuit traffic pattern previously explained in Chapter 3.\n\nThe response delay is enabled whenever the max delay is configured with a value greater than zero. When the value of max delay is higher than zero, a delay value is assigned to the microservice API’s call response, as shown in the following code snippet:\n\n#Simulate a delay if received an API to do so if delay_max_sec > 0: delay_seconds = round(delay_min_sec + random.random()*(delay_ max_sec-delay_min_sec), 3) #print(\"Adding a delay %s ...\" %delay_seconds) time.sleep(delay_seconds)\n\nThe max and min delay values can be configured using an API call. The following is an example of using curl to send an API call to configure the maximum and minimum delay response in milliseconds:\n\ncurl http://inventory_ms:8080/api/ simulatedelay?min=1500&max=3500\n\nAgain, this feature is only for demo and test purposes. A more secure way of simulating a delay is using secured configuration files or local parameters instead.",
      "page_number": 184
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 198-205)",
      "start_page": 198,
      "end_page": 205,
      "detection_method": "topic_boundary",
      "content": "The intelligent MSA system in action\n\nThe API response error simulator\n\nSimilar to the response delay simulator, this feature is for demo purposes only. The API error simulator feature uses one configurable value – the average HTTP error per hour. When the feature is enabled in the microservice, the microservice will pick a randomly applicable server 500 error and respond to API requests with randomly paced responses that match the configured error rate.\n\nThe error rate can be configured using an API call. The following is an example of using curl to send an API call to configure an API error response rate of 5 HTTP errors per hour:\n\ncurl http://inventory_ms:8080/api/response_err?rate=5\n\nNow, we know the testing and simulation tools available for us to use for training, testing, and simulating production for our MSA system.\n\nIn the next section, we will discuss our ABC-Intelligent-MSA operations – how to initialize the system, how to build and use training and testing data, and how to simulate the system’s production traffic.\n\nThe intelligent MSA system in action\n\nIn the previous sections of this chapter, we discussed how the different system components interact with each other and what tools we use to build the AI algorithms, test the system, and monitor the operations of different components.\n\nIn this section, we will put our ABC-Intelligent-MSA to the test. We will run all system microservices and tools, and see how the different system components actually interact with each other, what results we see, and how we can tweak the system to maintain smooth end-to-end operations.\n\nThe ABC-Intelligent-MSA will first run under an ideal simulation environment (no error simulation and no delays) to collect the training data necessary to build the AI models. Once enough data has been collected, we will then train the models and prepare the system for actual production traffic.\n\nThe system initialization steps, therefore, are as follows:\n\n1. Start the system with no AI services to collect the necessary training data under an ideal operational situation and create an operational baseline.\n\n2. Sanitize the collected data if needed and remove outliers.\n\n3. Train the AI algorithms using the training data collected.\n\n4. Re-initialize the system with all of its AI services.\n\n5. Start production operations. In our example here, we will simulate actual production operations by injecting errors, data delay responses, service failures, and so on.\n\n187\n\n188\n\nBuilding an Intelligent MSA Enterprise System\n\nInitializing the ABC-Intelligent-MSA system\n\nWe start by initializing our MSA system using the system’s Docker compose file, abc_ msa.yaml, and using the docker-compose command as follows,\n\n$ docker-compose -f abc_msa.yaml up &\n\nAs discussed previously in Chapter 9, the preceding docker-compose command is much more convenient than using multiple docker run commands. docker-compose will read the system’s run parameters and configuration from the abc _msa.yaml file, and initialize all the system components accordingly.\n\nIn our example, this will start the analysis and monitoring tools, along with all the regular microservices in the system. Since we are still collecting training data, no AI services need to be initialized yet.\n\nAs shown in Figure 10.2 and Figure 10.4, when we start the AI services (the PBW and PAD), they will need to be able to remotely control (start, stop, and restart) the system’s Docker containers. The PBW and PAD are designed to control the Docker containers using API calls. Therefore, we need to enable Docker Engine first to respond to API calls and for the PBW and PAD to be able to successfully communicate with Docker Engine.\n\nThe following are the steps needed to enable Docker’s API remote management:\n\n1. On your Ubuntu system, use vi, vim, or any other similar tool to edit the /lib/systemd/ system/docker.service file.\n\n2. Look for the ExecStart entry and make the necessary modifications for it to be like the following:\n\nExecStart=/usr/bin/dockerd -H=fd:// -H=tcp://0.0.0.0:2375\n\n3. This will enable Docker Engine to listen to API calls. Make sure you save the file after the modifications.\n\n4. Reload Docker Engine using the following command:\n\nsystemctl daemon-reload\n\n5. To ensure Docker Engine is working properly and responding to API calls, use the following command:\n\ncurl http://localhost:8080/version\n\nNow, the system is running and collecting training data. The longer you run the system, the more training data will be collected, and the more accurate your AI models will be. In our example, we will leave the system running for approximately 48 hours.\n\nThe intelligent MSA system in action\n\nIn the next subsection, we will go over how to run the tools, build training data, collect some of the system performance logs, simulate real-time system operations, and analyze the collected performance data.\n\nBuilding and using the training data\n\nThe ms_perfmon tool will create a separate stat file for each microservice in the <ms_perfmon's working path>/perfmon_stats directory. It is important that we leave the tool running and monitor the system’s performance stats under minimal load conditions.\n\nWe recommend at least 48 hours of training data collection. Ideally, however, data should be collected with seasonality load whenever applicable. In some environments, for example, the system load may increase on weekends over weekdays, during the shopping season, and so on. These situations should be considered in the training data to be able to build a more accurate AI model.\n\nPerformance data is pulled every 10 seconds, and accordingly, with 48h of active monitoring, ms_ perfmon produces 17,280 entries for each microservice.\n\nRegardless of the length of the system’s training period, whenever enough performance data has been collected, the training_data_cleanup.py tool should be run to detect any outliers and sanitize the performance data before using it in our AI services.\n\nThe training_data_cleanup tool scrubs all the performance data files in the <ms_perfmon's working path>/perfmon_stats directory, and automatically creates a scrubbed_stats directory with all the scrubbed data for each microservice. These scrubbed files are the files that we will later use for training the AI services.\n\nWe are now ready to write our Python code for training the PBW:\n\n1. We will use the numpy library for array and scientific data processing, pandas for reading our CSV training data files and testing data, and sklearn to build our AI model:\n\nimport numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression\n\n2. After importing the required libraries, we now need to copy all performance data into a DataFrame object. The following is a code example of this:\n\npayment_ms_stats_df = pd.read_csv('scrubbed_stats/ payment_ms_stats.csv')\n\nThe PBW’s AI model includes the microservice response time, the calculated request failure rate, and the calculated microservice load. The model should calculate the expected response time based on all the preceding parameters.\n\n189\n\n190\n\nBuilding an Intelligent MSA Enterprise System\n\n3.\n\nIn our Python code, we need to point to the data column that needs to be predicted. In our example, that would be the response time. The following is a code snippet for the Payment microservice:\n\npayment_ms_rt = np.array(payment_ms_stats_df['response_ time'])\n\n4. We need now to build our model, but before doing so, we need to load the rest of the performance data column into an array for training and testing processing. We do that by removing the “response time” column (an axis of 1) from the created DataFrame and then loading that DataFrame into an array to be used in our sklearn object, as follows: model_data = payment_ms_stats_df.drop('response_time', axis = 1) model_data = np.array(model_data)\n\n5. The model data need to be split into training data and test data. We split the model data into 80% training and 20% test data as follows:\n\nmodel_data_train, model_data_test, payment_ms_rt_train, payment_ms_rt_test = train_test_split(model_data, payment_ms_rt, test_size = 0.20, shuffle=True)\n\n6. Now, we build the model from the training data:\n\nlr_model = LinearRegression() lr_model.fit(model_data_train, payment_ms_rt_train);\n\n7. Save the data to a CSV file for future use:\n\ntrend_payment_ms_rt_predictions = lr_model. predict(payment_ms_rt) df = payment_ms_rt_df.assign(predicted_payment_ms_rt = trend_payment_ms_rt_predictions) df.to_csv(\"predicted_payment_ms_rt_trend.csv\", mode = 'w', index=False)\n\nNow, we have the training data and the trained model. It is time to use the model for production traffic.\n\nIn the next subsection, we will simulate production operations and describe how that can be applied to our trained MSA system, the ABC-Intelligent-MSA.\n\nAnalyzing AI service operations\n\nSimulating the ABC-Intelligent-MSA’s operation\n\nWe need to reinitialize the system now with the trained model and production traffic. Since no actual production traffic is applied in our example, we need to simulate the production operation with its potential operational challenges, including high traffic loads, service failures, and potential network hiccups.\n\nWe start by reinitializing the ABC-Intelligent-MSA system using docker-compose, as described earlier, but using the abc_intelligent_msa.yaml file:\n\n$ docker-compose -f abc_intelligent_msa.yaml up &\n\nThe main difference between abc_intelligent_msa.yaml and abc _msa.yaml is that the first file includes the initialization of the AI services.\n\nOnce the system is running, the AI tools will start monitoring and collecting the microservice’s performance and trigger healing actions whenever a system problem is detected and metrics exceed the configured performance thresholds.\n\nThe production traffic is ready to be simulated now using the simulate_api_rqsts API traffic simulator and the response delay simulator function discussed earlier.\n\nUsing the API response error simulator, occasional HTTP errors can also be simulated if needed. A more sophisticated simulation would involve injecting HTTP 500 error codes as well, but we will stick to response time performance delays for simplicity.\n\nThe ms_perfmon tool will still be running to collect data for our offline analysis whenever needed.\n\nWe now need to simulate specific production use cases and see how the AI tools will respond and self-heal the entire system. In the next section, we will discuss the operations of the PBW and PAD and look into how both AI services interact with system performance readings and errors.\n\nAnalyzing AI service operations\n\nIn the preceding sections, we started by building our first AI service and covered how to use AI to enhance the MSA system’s operations and resilience, the self-healing process, and the tools we built to generate training data and simulate the ABC-Intelligent-MSA system’s operation.\n\nIn this section, we will examine the system logs and check how the PBW and PAD interact with the system and actually enhance its operations. We will then simulate a cascading system failure and examine how the self-healing process is triggered and handled to bring the MSA system back to normal operation.\n\n191\n\n192\n\nBuilding an Intelligent MSA Enterprise System\n\nThe PBW in action\n\nDuring the training period, the PBW was able to build an AI model and calculate the expected response time of each microservice in the ABC-Intelligent-MSA system. As you can see from the following log sample, under a normal system load, the average response time of the Inventory microservice is about 20 ms:\n\nhttp://inventory_ms:8080,2022-11-23 15:48:25.094675, 0.01450204849243164,200 http://inventory_ms:8080,2022-11-23 15:48:35.816913, 0.0241086483001709,200 http://inventory_ms:8080,2022-11-23 15:48:46.543205, 0.02363872528076172,200 http://inventory_ms:8080,2022-11-23 15:48:57.271370, 0.010991334915161133,200 http://inventory_ms:8080,2022-11-23 15:49:07.983282, 0.021454334259033203,200 http://inventory_ms:8080,2022-11-23 15:49:18.645113, 0.012285232543945312,200 http://inventory_ms:8080,2022-11-23 15:49:29.310656, 0.0245664119720459,200 http://inventory_ms:8080,2022-11-23 15:49:40.010556, 0.013091325759887695,200 http://inventory_ms:8080,2022-11-23 15:49:50.744695, 0.021291017532348633,200 http://inventory_ms:8080,2022-11-23 15:50:01.715555, 0.024635791778564453,200\n\nWe configured the warning threshold for the PBW as 250 ms, and the action threshold as 750 ms. We will now start introducing an API call load to the Inventory microservice using simulate_api_ rqsts and delays using the response delay simulator feature. Then, we will see how the PBW reacts from the PBW action logs.\n\nThe following are the PBW’s performance readings for about 1.5 minutes. As you can see from the readings, the response time is consistently above the 250 ms alarm threshold, but (with the exception of one reading) still below the 750 ms action threshold:\n\nhttp://inventory_ms:8080,2022-11-23 18:24:00.518005, 0.6386377334594727,200 http://inventory_ms:8080,2022-11-23 18:24:11.469172, 0.7164063453674316,200 http://inventory_ms:8080,2022-11-23 18:24:22.203452,\n\nAnalyzing AI service operations\n\n0.7233438491821289,200 http://inventory_ms:8080,2022-11-23 18:24:32.942619, 0.7101089954376221,200 http://inventory_ms:8080,2022-11-23 18:24:43.668907, 0.6982685089111328,200 http://inventory_ms:8080,2022-11-23 18:24:54.777383, 0.8207950115203857,200 http://inventory_ms:8080,2022-11-23 18:25:05.410204, 0.6812236309051514,200 http://inventory_ms:8080,2022-11-23 18:25:16.101344, 0.6544813632965088,200 http://inventory_ms:8080,2022-11-23 18:25:27.072040, 0.7446155548095703,200 http://inventory_ms:8080,2022-11-23 18:25:37.828189, 0.6969136238098145,200\n\nThe readings will have to be consistently above the 750 ms action threshold for the PBW to trigger a healing action. One reading above 750 ms is not enough for an action to be triggered. However, since the readings are constantly above the 250 ms alarm threshold, the PBW is expected to trigger an alarm to the NMS/OSS system.\n\nWe need to verify the PBW’s behavior from the NMS/OSS system or the PBW’s action log. The following is a snippet of the PBW’s action log during the same period from the previous example:\n\n2022-11-23 18:24:00.518005: Alarming high response time (0.6386377334594727) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:24:11.469172: Alarming high response time (0.7164063453674316) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:24:22.203452: Alarming high response time (0.7233438491821289) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:24:32.942619: Alarming high response time (0.7101089954376221) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:24:43.668907: Alarming high response time (0.6982685089111328) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:24:54.777383: Actionable high response time (0.8207950115203857) detected in inventory_ms. No action triggered yet.\n\n193\n\n194\n\nBuilding an Intelligent MSA Enterprise System\n\n2022-11-23 18:25:05.410204: Alarming high response time (0.6812236309051514) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:25:16.101344: Alarming high response time (0.6544813632965088) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:25:27.072040: Alarming high response time (0.7446155548095703) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:25:37.828189: Alarming high response time (0.6969136238098145) detected in inventory_ms. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-23 18:25:48.637317: Alarming high response time (0.6777710914611816) detected in inventory_ms. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-23 18:25:59.327946: Alarming high response time (0.6758050918579102) detected in inventory_ms. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-23 18:26:10.014319: Alarming high response time (0.6641242504119873) detected in inventory_ms. Yellow alarm triggered and sent to NMS/OSS system.\n\nAs you can see from the preceding snippet’s last 4 log entries, after a consistent delay of more than 250 ms, an alarm was triggered and sent to the NMS/OSS system. We need to increase the inventory microservice’s load and response time to see how the PBW will react.\n\nThe following is another snippet of the PBW’s performance log. Only the last 4 log entries in a series of 10 consistent response delay readings are above 750 ms:\n\nhttp://inventory_ms:8080,2022-11-23 18:29:31.852330, 1.326528787612915,200 http://inventory_ms:8080,2022-11-23 18:29:43.196200, 1.4279899597167969,200 http://inventory_ms:8080,2022-11-23 18:30:05.310226, 1.0108487606048584,200 http://inventory_ms:8080,2022-11-23 18:30:16.334608, 1.1380960941314697,200\n\nNormally, we would have configured all healing actions shown in Table 10.1. In our demo system, however, we have configured only one healing action to demo the system self-healing operations in general. We only configured a microservice container to restart if a problem is experienced in the microservice. The response delay simulator feature is therefore a more relevant simulation tool than the other tools we have mentioned earlier.",
      "page_number": 198
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 206-221)",
      "start_page": 206,
      "end_page": 221,
      "detection_method": "topic_boundary",
      "content": "Analyzing AI service operations\n\nIn case of slow performance due to high API call requests volume, the most appropriate healing action would be to try to scale the microservice first and allocate more resources to respond to the high volume of API requests.\n\nWe assume in our simulation that the problem in the Inventory microservice is not necessarily due to the API request load, but rather some unforeseen problem causing the Inventory service to become unstable and unable to handle API calls promptly, so restarting the Inventory microservice could therefore fix the problem.\n\nNow, here is a look at the PBW’s action log during the same period. Please note that prior to the actionably high response time, an alarmingly high response time below 750 ms was previously detected. The response time was higher than 250 ms and below 750 ms:\n\n2022-11-23 18:29:31.852330: Actionable high response time (1.326528787612915) detected in inventory_ms. No action triggered yet. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-23 18:29:43.196200: Actionable high response time (1.4279899597167969) detected in inventory_ms. No action triggered yet. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-23 18:30:05.310226: Actionable high response time (1.0108487606048584) detected in inventory_ms. No action triggered yet. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-23 18:30:16.334608: Actionable high response time (1.1380960941314697) detected in inventory_ms. Red Alarm triggered and sent to NMS/OSS system. 2022-11-23 18:30:16.334608: Self-healing lock state declared for inventory_ms container. 2022-11-23 18:30:16.334608: Self-healing action triggered. Restarting inventory_ms container (inventory_management_ container). 2022-11-23 18:30:21.359377: Verifying inventory_ms operations... 2022-11-23 18:30:22.945823: inventory_ms was successfully restarted 2022-11-23 18:30:23.089051: Self-healing lock state cleared for inventory_ms container.\n\n195\n\n196\n\nBuilding an Intelligent MSA Enterprise System\n\nAs you see from the last 4 entries in the action log, the PBW detected a consistent response time (above 750 ms) and accordingly sent a red alarm to the NMS/OSS system, indicating a critical delay in the Inventory service and the need for a self-healing action to be taken. The PBW then locked the Inventory microservice to avoid clashing with healing actions from other AI services. The PBW then restarted the Inventory microservice by sending a restart API call to Docker Engine, verified that the Inventory microservice was back online, and finally unlocked the Inventory microservice.\n\nTo restart a Docker container through API, you will need to send a POST request as follows:\n\n/containers/<container id or name>/restart\n\nYou can also specify the number of seconds to wait before restarting the container using a t parameter. The following is a container restart POST example to restart the Inventory service container after a 10-second wait time:\n\n/v1.24/containers/inventory_management_container/restart?t=10\n\nFor more information on how to control Docker Engine using API calls, check the Docker Engine API documentation at https://docs.docker.com/engine/api/version-history/.\n\nHowever, was the PBW able to fix the Inventory microservice problem?\n\nLet’s go back now to the PBW’s performance log and see how this self-healing action impacted the Inventory service performance. The following are the log entries just before the healing action was triggered:\n\nhttp://inventory_ms:8080,2022-11-23 18:30:16.334608, 0.1380960941314697,200 http://inventory_ms:8080,2022-11-23 18:30:27.629649, 0.1693825721740723,200 http://inventory_ms:8080,2022-11-23 18:30:38.486793, 0.1700718116760254,200\n\nSure enough, the response time dropped from above 1 s to a maximum of 170 ms. Not as low as it was before the problem appeared, but the Inventory microservice for sure has some breathing room now. The performance issues may very well return if the underlying problem is not attended to and properly fixed.\n\nIn a more advanced AI model, we can train and configure the system to take more sophisticated actions to fully resolve the problem whenever needed, but in this book, we are limited to a specific scope to be able to demonstrate the idea in principle and pave the way for you to develop your own AI models and algorithms for your specific use cases.\n\nAnalyzing AI service operations\n\nWe have demonstrated in this section how the PBW works and how an action is triggered when a microservice performance issue is detected. In the following section, we will go over the PAD AI service and how the PAD takes a rather more holistic view of the entire system.\n\nThe PAD in action\n\nThe best way to demonstrate the operations of the PAD is to simulate a cascading failure and see how the PAD can bring the MSA system back to normal operation.\n\nTo simulate a cascading failure and ensure that the PAD responds to the failure and tries to auto-heal, we will first need to disable the PBW AI service. This will prevent the PBW from triggering a healing action and prevent it from trying to resolve the problem before the PAD’s healing action(s) kick in.\n\nLet’s quickly revisit what we have previously discussed in Chapter 3, an example of how a cascading failure happens.\n\nAs shown in Figure 10.5, under heavy API traffic, a failure to the Inventory microservice could cause the Payment microservice to pile up too many API calls in the queue, waiting for a response from the Inventory service. Eventually, these API calls will consume and exhaust the available resources in the Payment microservice, causing it to fail. A failure in the Payment microservice will produce a similar situation in the Order microservice, and eventually, produce a failure for the Order microservice as well:\n\nFigure 10.5: The Payment microservice is down\n\nFor the PAD to respond with healing actions, each of the PAD’s detected anomaly types has to have healing actions defined for it.\n\nTo successfully simulate the cascading failure, we only defined an action for a cascading failure situation. Otherwise, the PAD would automatically detect the failure in the Inventory service and self-heal it by restarting the Inventory microservice container, preventing a cascading failure from happening to begin with.\n\nWe will start by simulating a high volume of orders for the Order microservice and see how the system is going to respond to this situation in general, and specifically how the PAD will react under the situation.\n\nTo simulate a high volume of order requests, use the following simulate_api_rqsts command to target the Order microservice with a fixed uniformly paced order requests of 100,000 per minute:\n\nsimulate_api_rqsts 100000 http://order_ms:8080/place_order\n\n197\n\n198\n\nBuilding an Intelligent MSA Enterprise System\n\nWe will now shut down the Inventory microservice and examine the PAD action logs. The following is a snippet of the log about a minute after the PAD started to detect a failure in the Inventory microservice.\n\nPlease note that we introduced sudden high-volume traffic into the system. This sudden traffic increase by itself is a traffic pattern anomaly that was picked up by the PAD, but the PAD did not respond to that specific anomaly because no healing action is specifically defined for that anomaly:\n\n2022-11-24 11:39:13.602130: Traffic pattern anomaly detected, (inventory_ms) is likely down. No action is defined. No action triggered yet. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-24 11:39:23.469204: Traffic pattern anomaly detected, (payment_ms) slow API response detected. No action is defined. No action triggered yet. : : 2022-11-24 11:40:26.836405: Traffic pattern anomaly detected, (payment_ms) slow API response detected. No action is defined. No action triggered yet. Yellow alarm triggered and sent to NMS/OSS system.\n\nIn the preceding snippet of the PAD log, the PAD automatically recognized the Inventory service failure since no response traffic was detected from the service. However, no action was taken by the PAD since no healing action was defined for that particular anomaly. Since the anomaly was consistent for more than 1 minute, the PAD sent an alarm to the NMS/OSS system to notify the system admins of the problem.\n\nBecause of the Inventory microservice failure, the Payment microservice started to run out of resources, and the PAD picked up an unusually slow traffic flow from the Payment microservice given the API call request load applied. Accordingly, and as seen in the log, a little over 1 minute later, the PAD started to generate alarms to NMS/OSS.\n\nAs shown in the following PAD log, a few minutes after the Payment microservice anomaly, the Order microservice started acting up, and accordingly, the PAD was able to correlate all these anomalies and detect a potential cascading failure:\n\n2022-11-24 11:47:12.450897: Traffic pattern anomaly detected, (order_ms) slow API response detected. No action is defined. No action triggered yet. Yellow alarm triggered and sent to NMS/ OSS system. 2022-11-24 11:47:12.450897: Traffic pattern anomaly detected, potential cascading failure detected. No action triggered yet. Yellow alarm triggered and sent to NMS/OSS system.\n\nAnalyzing AI service operations\n\nPlease note that the only microservice failure we have so far is the one we manually shut down, the Inventory microservice. Both the Payment and Order microservices are still up and running but, as it seems from the log, may be suffering from resource exhaustion.\n\nThe system is still running so far, and should the Inventory service return back online, the system will automatically recover. The user experience during the heavy load would only be slow performance during the ordering process, but no orders have been denied or failed yet.\n\nBy examining all these previously mentioned PAD action logs, and as the situation stands so far, we are still okay. However, if no action is taken to resolve the Inventory microservice problem, the system will eventually fail and user orders will start to be denied.\n\nThe short circuit traffic pattern discussed in Chapter 3 helps prevent a cascading failure from taking place, but it still cannot resolve the underlying problem. User orders in a traditional short circuit pattern implantation will still be rejected until manual intervention fixes the Inventory microservice.\n\nThat’s where the PAD comes in. Check the following PAD action log!\n\n2022-11-24 11:48:13.638447: Traffic pattern anomaly detected, potential cascading failure detected. (inventory_ms) microservice is likely the root-cause. Red Alarm triggered and sent to NMS/OSS system. 2022-11-24 11:48:13.638447: Self-healing lock state declared for inventory_ms container. 2022-11-24 11:48:13.638447: Self-healing action triggered. Restarting inventory_ms container (inventory_management_ container). 2022-11-24 11:48:18.663912: Verifying inventory_ms operations... 2022-11-24 11:48:20.325807: inventory_ms was successfully restarted 2022-11-24 11:48:20.474590: Self-healing lock state cleared for inventory_ms container.\n\nThe PAD was able to detect the cascading failure before it actually happened, and was able to identify the root cause of the problem. The PAD sent a red alarm to the NMS/OSS system, declared a self-healing lock state on the Inventory service to try to fix the problem’s root cause, was able to successfully restart the Inventory microservice container, and then cleared the self-healing lock on the Inventory service.\n\nLet’s now check the microservices performance logs and ensure that the problem is fixed and that the ABC-Intelligent-MSA system and all of its microservices are running normally.\n\n199\n\n200\n\nBuilding an Intelligent MSA Enterprise System\n\nHere’s the Inventory microservice’s performance log:\n\nhttp://inventory_ms:8080,2022-11-24 11:51:33.132089, 0.033451717535487,200 http://inventory_ms:8080,2022-11-24 11:51:43.894705, 0.035784934718275,200 http://inventory_ms:8080,2022-11-24 11:51:54.809743, 0.027584526453594,200 http://inventory_ms:8080,2022-11-24 11:52:06.155834, 0.028615804809435,200\n\nHere’s the Payment microservice’s performance log:\n\nhttp://payment_ms:8080,2022-11-24 11:54:41.109835, 0.051435877463506,200 http://payment_ms:8080,2022-11-24 11:54:51.924508, 0.102346014326819,200 http://payment_ms:8080,2022-11-24 11:55:03.372841, 0.070163827689135,200 http://payment_ms:8080,2022-11-24 11:55:14.076832, 0.157682760576845,200\n\nHere’s the Order microservice’s performance log:\n\nhttp://order_ms:8080,2022-11-24 11:58:37.135827, 0.209097164508914,200 http://order_ms:8080,2022-11-24 11:58:47.584731, 0.193851625041193,200 http://order_ms:8080,2022-11-24 11:58:58.243759, 0.150628069240741,200 http://order_ms:8080,2022-11-24 11:59:08.961412, 0.138192362340785,200\n\nAs shown for the preceding Inventory, Payment, and Order microservices, all of those microservices are back online with normal performance readings. The system is now back to normal operation and should be able to handle the production load with no issues.\n\nSummary\n\nThis chapter walked us through how we can build AI models to build an intelligent MSA system step by step. We accordingly built two main AI services – the PBW and the PAD – and leveraged these AI services to enhance our MSA demo system, ABC-MSA, to build an intelligent MSA system that we named ABC-Intelligent-MSA.\n\nWe explained the self-healing process design and dynamics in detail, as well as the tools we built to develop AI training data, how to simulate production operations, and how to measure the demo system’s performance. We then put the ABC-Intelligent-MSA to test, simulated a couple of use cases to demonstrate AI functions within the MSA system, and carefully examined the logs of our demo AI services to showcase the value of using AI in MSA.\n\nEverything explained in this chapter is just an example of using AI in an MSA system. Enterprises should consider using AI services that are specifically appropriate for their own MSA system and use cases. These AI tools may very well be available through third parties or built in-house whenever needed.\n\nIn the next chapter, we will discuss the transformation process from a traditional MSA system to an intelligent MSA system – the things to consider in greenfield and brownfield implementations, and how to avoid integration challenges to make the corporate transformation as smooth as possible.\n\nSummary\n\n201\n\n11 Managing the New System’s Deployment – Greenfield versus Brownfield\n\nIn the previous chapters, we discussed building an MSA system and integrating AI algorithms to form an Intelligent MSA. We covered concepts, techniques, and methodologies while accompanying them with examples.\n\nIn this chapter, we will discuss the different greenfield and brownfield deployment considerations, and ways to smoothly deploy the new intelligent MSA system with minimal operational disruptions, to be able to maintain overall system stability and business continuity.\n\nWe will also examine how to overcome general deployment challenges, particularly in brownfield deployments where existing systems are in production, and implement a successful and effective migration plan for the new Intelligent MSA system.\n\nThe following topics will be covered in this chapter:\n\nDeployment strategies\n\nGreenfield versus brownfield deployment\n\nOvercoming deployment challenges\n\n204\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nDeployment strategies\n\nOrganizations utilize various techniques to minimize downtime and ensure a seamless and successful deployment when deploying a new system. Some of the most commonly used deployment strategies organizations follow are Recreate, Ramped, Blue/Green, Canary, A/B Testing, and Shadow deployments:\n\nThe Recreate deployment is a simple, straightforward approach that involves replacing the entire infrastructure at once, similar to the Big Bang migration we discussed in Chapter 3. This approach is best suited for small and simple systems; however, it also means that the system is completely offline during the deployment process, which can lead to significant downtime.\n\nThe Ramped deployment is similar to the Trickle migration we discussed in Chapter 3. The Ramped deployment allows the existing system to remain online during the deployment process. The new system is brought online gradually, and traffic is gradually routed to it, allowing both systems to stay available to users throughout the deployment process. Although this can be effective in small businesses and simple systems, this approach is ideal for larger, more complex systems, where minimizing downtime is a priority.\n\nBlue/Green deployment is a technique that involves maintaining two identical production environments, referred to as “blue” and “green,” and routing traffic to one or the other. This allows for a seamless switchover in case any operational issues are experienced in the newly deployed version. This method is best suited for mission-critical systems since it ensures that there is always a working system available to users at any given time.\n\nCanary deployment is a technique that involves deploying the new system alongside the existing one and routing a small percentage of traffic to the new system. This allows for testing the new system with actual production traffic before rolling it out completely. If problems arise in the new system, the rollout can be reevaluated based on the type of issues encountered; then, the previous system can be reinstated while the problems are being resolved. This approach is often used to deploy changes to critical systems that require high levels of availability.\n\nThe A/B testing deployment is another approach that involves simultaneously running both the old and the new system but testing them with different subsets of users to determine which performs better. This method works best for testing new system features or services.\n\n\n\nIn the Shadow deployment, the new system is deployed to run alongside the existing system. The live production traffic of the old system is then redirected to the new system to test a newly released feature, the system stability under load, or test the new system altogether. This approach is best used in large systems deployed in large organizations.\n\nGreenfield versus brownfield deployment\n\nThe following is a comparison summary of all the preceding deployment strategies:\n\nContinuous Uptime\n\nProduction Traffic Testing\n\nCost\n\nComplexity\n\nRecreate\n\nNo\n\nNo\n\nLow\n\nLow\n\nRamped\n\nYes\n\nNo\n\nLow\n\nLow\n\nBlue/Green\n\nYes\n\nNo\n\nHigh\n\nMedium\n\nCanary\n\nYes\n\nYes\n\nLow\n\nMedium\n\nA/B Testing\n\nYes\n\nYes\n\nLow\n\nHigh\n\nShadow\n\nYes\n\nYes\n\nHigh\n\nHigh\n\nTable 11.1: Comparison of the different deployment strategies\n\nEach of these strategies has its pros and cons, and none of them would be best suited for every case. Organizations must choose the appropriate strategy based on their specific needs and the nature of the changes being deployed.\n\nThe complexity and design of the existing system that’s being upgraded or replaced, as well as its age and operation and the technology stack being used, play significant roles in determining the deployment strategy. In the next section, we will discuss the greenfield and brownfield deployments and their impact on determining the specifics of the deployment approach and plan.\n\nGreenfield versus brownfield deployment\n\nWith our intelligent MSA system ready for deployment, we now need to think in detail about the infrastructure we have or need to acquire to deploy the system in production.\n\nSome of the main questions we need to address are as follows:\n\nWhat are the infrastructure details needed to run the intelligent MSA system?\n\nDo we have the hardware and software resources needed to deploy and run the system efficiently?\n\nCan we leverage our existing infrastructure and applications to deploy the new system?\n\nWhat is the delta between the infrastructure needed and the infrastructure we have in place, and how can we fill that gap?\n\nThe organization’s current infrastructure setup and existing systems (if any) play a crucial role in answering all of the preceding questions – that is, whether the new system is being deployed in a greenfield or brownfield environment.\n\n205\n\n206\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nGreenfield deployment refers to building and deploying a new system or infrastructure from scratch with no previous system or infrastructure in place. Thus, we must build the new system without major constraints, dependencies, integration work, or compatibility issues to consider before building and running the new system.\n\nOn the other hand, a brownfield deployment refers to the process of deploying a new system or infrastructure on a site that is already running with an existing system in place. The site may have existing infrastructure such as servers, applications, network components, and more that may be reused for deploying the new system.\n\nIn short, greenfield deployment is a new start from scratch, whereas brownfield deployment involves building on top of existing systems or infrastructure and possibly dealing with some integration issues, compatibility concerns, and resource constraints.\n\nWhether it is a greenfield or brownfield deployment is often determined by the organization’s situation and how the organization’s business process is being conducted. Nevertheless, it is still important to understand the pros and cons of each deployment type to plan accurately.\n\nImportant Note If cost saving is a major focus in the organization, we should leverage as many existing components as possible from the existing brownfield infrastructure. However, we need to reuse existing components in a way that cannot negatively impact the deployed system’s efficiency, reliability, or functionality.\n\nThe following are some factors to consider when evaluating both deployment types.\n\nFlexibility\n\nBecause in greenfield deployment we are starting from scratch, this gives us the liberty to design, implement, and optimize the new system for the specific needs of the organization without restricting ourselves to any dependent components or existing production systems.\n\nIn brownfield, on the other hand, we must always think of the already running systems and their dependencies before designing or deploying any part of the new system. This in itself can limit the deployment, customization, or optimization of the new system.\n\nScalability\n\nGreenfield implementations offer higher scalability compared to brownfield because, in greenfield implementations, we deploy new infrastructure without any existing constraints that could limit the design or customization of the new system.\n\nGreenfield versus brownfield deployment\n\nThis lack of constraints gives architects and system designers the choice to design the system so that it scales without thinking of underlying technology or existing equipment that may hinder the system’s capabilities or scale.\n\nThe existing infrastructure in the brownfield’s case, however, may have legacy systems that are likely to be reused in part or as a whole whenever possible. Reusing different parts of the legacy systems may impede the new system’s scalability.\n\nMoreover, legacy systems usually allocate more physical space and are more power-consuming than modern systems, which adds more limitations to the overall system scalability.\n\nHaving said that, as we deploy the new system and gradually refresh the existing infrastructure, we will use up-to-date technologies and modern systems that will free up physical space and reduce power requirements. This, in turn, will help us eventually scale the system better.\n\nTechnology stack\n\nIn a greenfield deployment, we have the liberty to leverage the latest technologies, applications, and tools, which can, among many other things, enhance performance, security, and capability, and prolong the system’s overall lifetime.\n\nWith the legacy systems in brownfield environments, older technology, hardware, tools, and systems are used, which can introduce limitations on the system’s supportability, capabilities, scalability, and future expansions and integration.\n\nIntegration\n\nAs explained in other aspects of the comparison, in greenfield environments, all the components of the system are new and are designed and built from the beginning to work together seamlessly. Integration, therefore, is not an issue at all.\n\nIntegrating a new system with an existing IT infrastructure in the case of a brownfield deployment, however, can be challenging, as the two systems may not be fully compatible. Integration efforts may be needed for old and new components to work together, and even then, the new mixed system may later provide operational challenges that can cause unforeseen system mishaps.\n\nCost\n\nFrom an acquisition and CAPEX perspective, building a new system from scratch is higher in cost than building the system from a mix of reused and newly acquired components.\n\nTo set up the new system, a certain level of expertise is required that may not be available in-house. The effort and expertise needed to bring the system up and running will certainly have associated costs. However, it could be argued that this cost can be easily offset by the efforts and expertise needed to integrate the new and old components. Possibly different effort and different expertise, but similar cost.\n\n207\n\n208\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nWhen it comes to OPEX, in a greenfield deployment, we need to consider training costs for the new technologies and systems deployed, as well as potential operational mistakes that can be caused due to the lack of new system hands-on experience. In brownfield deployments, these training costs are usually lower.\n\nPower consumption is typically lower in greenfield deployments, as new systems and technologies are often geared toward power usage optimization.\n\nAnother important OPEX consideration is the potential technical debt in brownfield implementations. Technical debt is the shortcuts the organization takes to get the system up and running. In other words, this involves taking a band-aid approach to resolving an integration or operational issue during the deployment, and achieving short-term results that can be catastrophic in the long term.\n\nTime-to-market\n\nTime-to-market is an interesting aspect of the deployment and can go both ways. Generally speaking, deploying a system in a greenfield environment takes longer than integrating with an already existing functioning system, as is the case in a brownfield environment. But that would be highly dependent on how complex an existing system may be.\n\nIf we are deploying our new system on top of a significantly old or complex disorganized infrastructure, we could argue that deploying a new system fresh from scratch is much more straightforward and a time saver than trying to get both systems integrated successfully.\n\nRisks\n\nThis is another aspect of the deployment that can be debated either way.\n\nWith a lack of experience with the new infrastructure, new technologies, new systems, new tools, and new applications, system operational mishaps are more likely, and the time to resolution could be higher. In contrast, with no backup system in place, there will be no fallback option if the new system does not perform as expected.\n\nBut again, if the old system in the brownfield environment is too complex or disorganized, the risks would be higher in a brownfield deployment due to complexities in integration, potential technical debt, old unsupported components, and more.\n\nStaff onboarding\n\nCompanies that are already using an existing system have a better understanding of how it works and insights into its operations and potential issues, which can make the deployment process and system operations smoother.\n\nIn greenfield deployments, training and accumulated experience are needed before staff can start to become familiar with the system details.\n\nGreenfield versus brownfield deployment\n\nUser adoption\n\nUser adoption of the new system may require adapting to new ways of performing day-to-day tasks, a shift in internal and potentially external business operations, and how the organization deals with internal and external customers. This shift may require a change in organizational culture, which can pose a significant challenge to the successful implementation of the system and reveal operational shortcomings after deployment.\n\nIn a brownfield environment, the updated system capabilities could be incremental or somewhat transparent to the user, which makes user adoption much easier and faster compared to deploying a completely new system, as in the case of a greenfield deployment. A successful and gradual user adoption helps uncover potential design, implementation, and operational deficiencies that can be quickly addressed and fixed.\n\nIn either case, user training is needed, but certainly, in a greenfield, training is more complex and involved over the brownfield case.\n\nThe following table summarizes the comparison between greenfield and brownfield aspects of the deployment:\n\nGreenfield\n\nBrownfield\n\nFlexibility\n\nHigh\n\nLow\n\nScalability\n\nHigh\n\nLow\n\nTechnology Stack\n\nFlexible and optimized\n\nRestrictive\n\nIntegration\n\nMinimal to none\n\nHigh efforts\n\nCost\n\nCAPEX is high but better OPEX CAPEX is lower but higher OPEX\n\nTime-To-Market\n\nUsually longer\n\nUsually shorter\n\nRisks\n\nUsually higher\n\nUsually lower\n\nStaff Onboarding\n\nLonger process\n\nShorter time\n\nUser Adoption\n\nSlow\n\nFast\n\nTable 11.2: Greenfield versus brownfield\n\nIn this section, we discussed the main differences between greenfield and brownfield deployments, the pros and cons of each environment, what to consider, and why.\n\nIn the next section, we will go over how we can overcome deployment challenges in both environments and the deployment best practices in each case.\n\n209\n\n210\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nOvercoming deployment challenges\n\nWe now understand the different aspects of system deployment in greenfield and brownfield environments, as well as the several challenges that can be presented during the system design and implementation. In this section, we will cover some of the concepts, strategies, and approaches for mitigating these challenges to ensure a smooth and successful system deployment.\n\nWe should begin our deployment project with a solid project team that possesses diverse skills and experiences in technical areas of the project, deployment and project management, and vendor management.\n\nIn the absence of in-house experience, outsourcing one or more project experience areas through third parties may be necessary. Partners may include system integrators, and/or equipment vendors, and value-added resellers.\n\nThis experienced team will help conduct thorough planning and research to be able to understand the specific needs of the organization, potential risks, local regulations, and any necessary compliance needs.\n\nCompliance with industry and local regulations is an essential part of the project. Aside from the technical aspects and technologies of the project, a system processing credit cards, for example, will require team members who are experienced in PCI compliance and rules. A healthcare system deployed in the United States, for example, may need members who have experience with HIPAA compliance needs, and so on.\n\nProject management is key to establishing clear team communication and collaboration. The project managers help track the project process, changes, and requirements, and ensure that the timelines and goals are met throughout the project cycles. The project managers also make certain that all stakeholders are properly informed and engaged throughout the different phases of the project.\n\nThe type of project management style or approach is dependent on the organization itself, the timeline, and the implementation details and technologies deployed. Whether it is waterfall, agile, scrum, or something else, the project manager must decide with the team.\n\nAddressing deployment challenges is a task that is pursued within the project cycle. Our focus here is on this aspect of the project cycle, particularly concerning greenfield and brownfield deployments.\n\nBefore any deployment activities, complete visibility of the deployment risks is necessary. Therefore, it is imperative to develop a clear deployment risk plan to be able to identify the risks and mitigate each risk to ensure a successful deployment.\n\nThe following chart illustrates the risk management process for our deployment cycle. The process should start with identifying the risks, determining ways to avoid or minimize them, developing a mitigation plan, and continually testing, monitoring, and reviewing the deployment to update the mitigation plan for any new risks or challenges that arise during implementation:\n\nOvercoming deployment challenges\n\nFigure 11.1: Main steps of overcoming deployment challenges\n\nIn the following subsections, we will go over the main activities of a risk management plan and how each phase of the plan is relevant to our deployment activities.\n\nIdentify deployment risks\n\nTo address the deployment challenges, we start by identifying the potential risks we may encounter when deploying our system. In a greenfield environment, the risks we identified earlier are as follows. The greenfield risk is referred to as GR:\n\nGR1: High CAPEX\n\nGR2: Deployment time\n\nGR3: System failures due to lack of training and experienced staff\n\nGR4: Slow or lack of user adoption\n\n211",
      "page_number": 206
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 222-230)",
      "start_page": 222,
      "end_page": 230,
      "detection_method": "topic_boundary",
      "content": "212\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nIn a brownfield environment, the risks we identified earlier are as follows. The brownfield risk is referred to as BR:\n\nBR1: System capabilities limitation due to potentially low flexibility and scalability, and some reused legacy technologies\n\nBR2: High OPEX\n\nNow that we have identified the potential risks, it’s time to prioritize them based on their likelihood and potential impact on the project. This will help us calculate the risk exposure and plan and allocate proper resources effectively.\n\nPrioritize risks\n\nRisk exposure is the risk probability multiplied by the impact of that risk on the deployment project. The higher the risk exposure, the higher the priority of working on mitigating that risk should be.\n\nFigure 11.2 shows a color-coded risk exposure matrix for the previously identified risks. Please note that the level of risk exposure can greatly vary between organizations based on various factors, such as project complexity and organizational needs, the stability and complexity of the existing system, project requirements, budget, timeline, and more:\n\nFigure 11.2: Risk exposure matrix\n\nWe always prioritize from the top right to the bottom left of the risk mitigation chart. The red zone, where the risk and the likelihood are Medium to High, is where we need to start allocating resources. This is followed by the yellow zone, then the green zone.\n\nSo, we should start with greenfield risk #1 (GR1), how to mitigate high CAPEX risks, then GR3, where we lack experienced staff to deploy the new system, then GR2, where the new system deployment time may be an issue, and then conclude the mitigation of greenfield risks by addressing GR4, where we may face a slow user adoption of the new system.\n\nOvercoming deployment challenges\n\nFor brownfield risk mitigation, we start with brownfield risk #1 (BR1) since it has a higher exposure in the red zone of the matrix, then do BR2.\n\nRisks are not always avoidable. Risks often aren’t. In cases where risk elimination is not possible, the mitigation plan has to address how we can at least minimize the risks to a manageable level.\n\nMany organizations choose to ignore risks with low/very low likelihood and low/very low impact. The process of mitigating risks in this particular case may very well be costly and riskier than the risk itself.\n\nDeveloping and implementing a risk mitigation plan\n\nWe need to develop a mitigation plan to manage the identified risks. This plan should include specific actions that will be taken to mitigate or eliminate the risks, resources involved, and contingencies in case the risks do occur.\n\nWe also need to identify our risk mitigation strategies based on the calculated exposure of each risk. As we will see in the next few subsections, these strategies can include implementing operational safeguards, extra measures using additional system components, testing the system before deployment, training both system users and administrators to ensure they can effectively use the new system, and developing a rollback plan in case of any unexpected deployment issues.\n\nLet’s apply all these to our deployment project’s identified risks.\n\nGR1 – high CAPEX risk\n\nWhen addressing the high CAPEX risk, we need to focus on a few things – first, the project budget, second, the system and project requirements, and third, how to use effective negotiation skills to acquire the necessary infrastructure that would successfully fulfill all of the desired system requirements.\n\nSometimes, of course, budget and time constraints become a barrier to acquiring all of the requirements’ wish list items. Therefore, it is important to prioritize your requirements, especially if you have strong budget constraints.\n\nThe objective is to acquire the infrastructure that would get us all of our wish list requirements; however, at some point, we may need to give up some of the nice-to-have features of the system for the sake of meeting our budget. This is where our negotiation skills become vital. The stronger our negotiation skills, the greater the likelihood of successfully deploying the new system and meeting all the requirements within budget.\n\nIt is useful to go over a few important negotiation techniques that can help us achieve our objective.\n\nStart by conducting thorough research on all vendors that can be involved in providing the infrastructure assets. This includes the technical and business strengths and weaknesses of each potential vendor, their list prices, the quality level of their deployment and operational support, their product roadmap, and their future business outlook.\n\n213\n\n214\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nThen, we can come up with multiple options from our vendor research. Having multiple options gives us more flexibility and bargaining power during our vendor negotiation phase.\n\nOut of all the available options, having a clear Best Alternative To a Negotiated Agreement (BATNA) is critical. BATNA means having the most favorable option we can achieve if the negotiations fail and no agreement is reached with the infrastructure vendors. It is the fallback option that we can rely on if the negotiations do not go as we initially planned with a specific vendor.\n\nHaving a clear and well-defined BATNA is important in displaying strong bargaining power during the negotiation process. A BATNA gives the vendors a sense of how much they can miss out if they do not close the infrastructure acquisition deal.\n\nBATNA also helps us understand when it’s time to walk away from the negotiations if the vendors are not willing to meet our infrastructure needs with the available budget. It is about the alternatives that are available to us and the consequences of choosing or not choosing a specific vendor.\n\nLarger and highly reputable organizations can use leverage. Leverage is the ability to use your reputation, market position, or market size to influence the outcome of the negotiation. Leverage can be very effective in striking a good deal with vendors. Vendors generally strive to gain a foothold in large organizations to create a customer reference or as a way to build more business out of that particular deal.\n\nHaving said all that, it is essential to show a certain level of flexibility during the negotiation. Flexibility will demonstrate our willingness to compromise and is likely to lead to a strong long-term and healthy business relationship with the vendors:\n\nFigure 11.3: Negotiation strategies\n\nFigure 11.3 sums up the negotiation strategies you could follow to minimize budget overrun risks due to infrastructure cost and the overall project CAPEX.\n\nOvercoming deployment challenges\n\nAnother effective way of mitigating CAPEX risks is to use a cloud provider in deploying your new infrastructure, especially during the initial testing and adjustment phases of the project.\n\nUsing containerization and virtualization to build new workloads is another way to help reduce CAPEX risks. However, by doing so, we may introduce some OPEX and other types of deployment risks, especially if the staff is not trained enough on cloud environment deployments.\n\nGR2 – deployment time risks\n\nWhen deploying a new project, especially a project that involves new technologies, techniques, and concepts, the potential of running behind schedule cannot be ignored. There is always a learning curve associated with the overall project cycle.\n\nMoreover, potential frequent changes and scope creeps are also important to account for. This emphasizes the importance of engaging a skilled team from the start even more.\n\nA highly skilled and well-trained project team will help minimize scope creep and time delay risks. In the absence of in-house subject matter experts, hiring an external party as a system integrator and building an adequate training plan for the team become critical when managing time delay risks.\n\nThere may be factors other than team skills that can contribute to project timeline and scope creep risks. As part of our risk mitigation plan, we need to account for all of these factors and make sure they are controlled to consequently control their associated risks.\n\nMSA systems can be very complex and burdensome to deploy. The more complex the system is, the more difficult it can be to predict and manage the deployment timeline.\n\nIn brownfields, the deployment of the new system often depends on the availability and operational stability of the already existing systems. Furthermore, integrating the new system with the existing systems can add another level of complexity and time-consuming tasks, all of which can potentially prolong the overall deployment timeline.\n\nOne way to mitigate the effect of complexity on the overall risk is to avoid forklift and big bang changes. Rather, use a trickle approach by breaking down the deployment into multiple simple phases and stages. Follow the famous Einstein rule, “Everything should be made as simple as possible, but not simpler.” Simplify as much as possible, but not in a way to compromise the system’s functionality or reliability.\n\nAlthough thorough testing is necessary to ensure the new system’s reliability and adherence to the organization’s needs, over-analyzing and over-testing often happen in complex projects. This process can significantly delay the system deployment.\n\nOne way to address the testing delays factor is to consider running the system in production in a Limited Availability (LA) fashion, a beta version, or a pre-launch period of some sort. This LA approach will help us apply real user traffic to the system while we focus on monitoring and making system changes as needed before we transition the system into full-scale production.\n\n215\n\n216\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nGR3 and GR4 – system failure risks and user adoption risks\n\nUser adoption can highly depend on any new user interface/user experience (UI/UX) changes or added complexities. Assuming a well-designed system’s UI/UX, system failure and slow user adoption risks are also dependent on the experience of the project and operation teams, and how familiar the users are with working on the system.\n\nTo normalize the experience and user familiarity risks, it’s important to first include a solid UI/UX design team from the outset of the project, then adopt a top-down approach by securing a strong buy-in from project sponsors and executives.\n\nThe top-down approach will help adopt the organization’s processes and changes necessary to create a cultural shift in conducting the business using the new system. The buy-in can also help enforce a training program for both system users and the project team. This training can significantly help in bridging the gaps between the existing experience of the project team and the experience needed.\n\nBR1 – system capability limitations\n\nBecause we are deploying the new system with some older components being used in the existing system, we are likely to run into integration incompatibilities, limitations in the older hardware and software features, and limitations in scaling in terms of traffic, data loads, or storage.\n\nAdditionally, a system with legacy components may become outdated sooner than anticipated. This can result in obsolete software and hardware components that are no longer usable or valuable, or in vendor out-of-support announcements, which would shorten the life span of the new system.\n\nWhen system components are out of support, vendors can no longer provide part replacements or software updates, or even assist in case any operational issues arise on the out-of-support components. This can severely impact the system’s reliability and jeopardize the organization’s business continuity.\n\nTo mitigate the risk of system capability limitations, we must have clear visibility and understanding of the vendor product map for each reused legacy system component and clear visibility of the component’s dependencies. This understanding shall help us assess the impact of that component on the scalability levels, and future operational reliability and stability of the system being deployed.\n\nBR2 – high OPEX\n\nAs we have previously explained, integrating both new and older systems can introduce many deployment and operational challenges that can make the system much more complex, and introduce technical debt, high maintenance costs, and high operational costs to keep the system running smoothly.\n\nSecurity is also a major concern in brownfield deployments. Technical debt, along with mixing older and new components, may introduce vulnerabilities that were not present in the existing infrastructure, which can lead to costly data corruption, data loss, data recovery, security breaches, and irreversible reputational damage.\n\nOvercoming deployment challenges\n\nTo mitigate these risks, organizations should thoroughly evaluate the impact and have a clear cost-benefit analysis of each reusable component of the existing system before proceeding with the deployment.\n\nFurthermore, having a robust disaster recovery and backup plan in place in case of data loss or corruption is key to mitigating some OPEX risks.\n\nAll the preceding risk examples and their mitigation strategies should be thoroughly discussed as part of the developed risk mitigation plan, with step-by-step guides and documentation.\n\nShould any of these risks take place, especially in the case of brownfields, where an existing system may already be running, a comprehensive rollback plan should be executed immediately. The rollback plan is the next step in how to overcome the deployment challenges. The following subsection takes us through what a rollback plan is and what it entails.\n\nThe rollback plan\n\nRemember Murphy’s Law, which states “Anything that can go wrong will go wrong”?\n\nHow often do we create a solid and well-crafted implementation or migration plan, expecting a smooth change in the system, but then experience unexpected and bizarre behavior during the execution of the plan?\n\nA rule of thumb here is that anything can go wrong during the deployment. We put enough planning and precautions for nothing to go wrong, but unfortunately, things do not always work in our favor. We may still overlook things, system bugs may get triggered, equipment mishaps may happen, and so on.\n\nTherefore, developing a rollback plan is necessary to be able to maintain business continuity. We should have the plan built in a way that it includes clear steps and procedures to move back to the initial system state before the change and resume normal operations quickly.\n\nAdopting a phased deployment approach, as previously discussed, helps to quickly roll back only a portion of the change, which helps us avoid wasting resources, valuable change window time, and efforts invested during the deployment, and in developing the deployment plan.\n\nTest, monitor, and adjust\n\nThe next step in overcoming deployment challenges is to test and validate the system’s requirements and functionality to ensure it meets the performance, functional, and operational requirements.\n\nThe project teams, when under time pressures, often deprioritize system security over system functionality. Data security and data protection, when overlooked, can severely impact the project’s overall deployment and reliability, especially with systems that handle critical user data and have to comply with certain regulations and compliance acts, as in the case of PCI or HIPAA.\n\n217\n\n218\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nTherefore, the test plan has to have a specific section dedicated to system security and compliance testing. Hiring a specialized firm in the area of security and compliance helps minimize the risk of data breaches or other security incidents.\n\nAs we gradually apply test and production load to the system, the test plan should be able to ensure the systems being deployed are scalable and flexible enough to adapt to changing needs and requirements. This is a critical aspect of the test plan during a brownfield deployment since the integration with the existing system may hinder the overall system’s capabilities.\n\nThen, we need to continuously and carefully monitor and review the identified risks throughout the project to ensure that the risk mitigation plan is effective and that any new risks are identified and addressed promptly.\n\nAny newly identified risks will have to be included in the mitigation plan through the project change management process. The newly identified risks and mitigation strategies will need to be communicated to all stakeholders, including the project team, the organization’s management, and the system users.\n\nPost-deployment and pre-production review\n\nOnce the deployment is over, the system is operational, tested, and running in a pre-production or LA manner. Just before closing the deployment project, we need to evaluate the effectiveness of our risk management plan, identify any areas for improvement, and document the outcome of our findings. The outcome can then be integrated into the same project deployment effort or conclude the existing deployment and initiate a new project for that matter.\n\nThe post-deployment assessment will ensure the new system’s continued stability, performance, and reliability.\n\nIn brownfields, we may end up running in a bi-modal approach, where both systems are running at the same time and serving users at the same time but in different ways and at different levels. In this case, we need to consider building specific roles and responsibilities matrices for each system. This helps streamline operations and increase the system’s supportability.\n\nSummary\n\nIn this chapter, we covered greenfield and brownfield deployments, the difference between each, their pros and cons, risk details during the deployment process in general, and the specifics of each risk in each deployment case.\n\nWe also provided examples of the risks associated with greenfield and brownfield deployments, along with the strategies to mitigate these risks, to gain a better understanding of the challenges involved in successfully deploying a new system.\n\nThe topics that were discussed in this chapter act as introductions to what we will be learning in the following chapter. In the next chapter, we will apply some of what we have learned in this chapter and discuss ways to test, monitor, and update our new ABC-Intelligent MSA system.\n\nSummary\n\n219\n\n12 Deploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nIn the previous chapters, we talked in detail about microservices, monolithic architecture, the pros and cons of each architecture, how to transition into MSA, and how to make the MSA system smarter using AI services. We also discussed, in Chapter 11, some of the best practices for deploying the MSA system.\n\nIn this final chapter, we will integrate all the topics and concepts covered throughout the book to understand how we can apply what we have learned through hands-on and practical examples.\n\nBefore we dive into the details, we need to understand what existing system we have in place first.\n\nObviously, every organization is different and has different deployment needs, criteria, and dependencies. Some organizations will deploy in a greenfield, and others in a brownfield. In order to walk you through detailed practical examples and steps for deploying, testing, and operating an intelligent MSA system, we will assume a brownfield environment with an existing monolithic architecture system.\n\nWe will sometimes use our ABC-Monolith as an example of the existing system to illustrate the concepts covered in the chapter. In this chapter, we will cover, the following topics:\n\nOvercoming deployment dependencies\n\nDeploying the MSA system\n\nTesting and tuning the MSA system\n\nThe post-deployment review",
      "page_number": 222
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 231-239)",
      "start_page": 231,
      "end_page": 239,
      "detection_method": "topic_boundary",
      "content": "222\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nOvercoming system dependencies\n\nBefore deploying the ABC-Intelligent-MSA system we built earlier in Chapter 10, it is important to first decide what our deployment strategy should be. Based on the requirements, cost, complexity, and pros and cons of the deployment strategies we discussed in Chapter 11, we believe the best deployment strategy for our ABC system would be a mix between the ramped deployment and canary deployment strategies.\n\nThis deployment strategy will allow us to keep the ABC-Monolith system online and serve users uninterrupted while we deploy the new ABC-Intelligent-MSA system. We will gradually replace older components in ABC-Monolith with the corresponding microservices in the ABC-Intelligent-MSA system. This can be accomplished by routing traffic from the older components to those ABC-Intelligent- MSA system microservices.\n\nAlthough this trickle approach has lower cost, lower complexity, and lower risk than other deployment approaches, we still need to carefully study the incompatibilities, dependencies, and the proper integration between older and newer components.\n\nFurthermore, we will need to evaluate which of our infrastructure and existing system ABC-Monolith’s components can be reused in the new architecture, if any.\n\nReusable ABC-Monolith components and dependencies\n\nWe cannot think of a specific ABC-Monolith code base component that can be reused as is without modification. All of the ABC-Monolith components will have to be either rewritten from scratch or modified to different degrees to be compatible with the ABC-Intelligent-MSA system.\n\nSome of the ABC-Monolith and existing infrastructure components that we know can be reused are the business logic itself, server infrastructure, operating systems, virtualization infrastructure, data storage, network infrastructure, existing monitoring, and network management tools, and some of the software and database licensing. Nevertheless, even these components may need to be updated or upgraded in order to perform the functions of the new system.\n\nIn our system installation, command line, and code examples listed in earlier chapters, we had the most updated Ubuntu, Python, and database versions. In a real-life situation, however, that may not be the case; we will likely have the monolithic application running in an older operating system and have an older Python and/or database version.\n\nThese situations may produce some incompatibilities between the older components and the newer ones. An older Python version, for example, may have some deprecated functions that are no longer valid with the new MSA code base, and hence, will require some updates or upgrades to the existing system. Furthermore, the potentially different technology stack may also produce more dependencies.\n\nOvercoming system dependencies\n\nIn order to minimize these dependencies, we would rather deploy the new system components on a separate server or virtual infrastructure with their own environment, including their own data storage and using their own technology stack. The new environment will have a container engine that will carry all of our ABC-Intelligent-MSA microservices.\n\nIt is important to note that each system is different, and the specific reusable and non-reusable components will vary depending on the existing monolithic system. A thorough analysis and evaluation of the existing system’s components is necessary to determine what can and cannot be reused when migrating to a microservices architecture.\n\nMitigating ABC-Intelligent-MSA deployment risks\n\nSome of the risks discussed in Chapter 11 are relevant to our scenario. However, we still need to determine which CAPEX risks are applicable, examine the risks related to deployment time, potential service disruption, and OPEX, and take specific actions to mitigate these risks.\n\nSince we are using containers on top of virtualized infrastructure in our implementation, CAPEX risks are significantly reduced. As long as the existing infrastructure has the storage and workload capacity to absorb the new ABC system, we are safe. If additional infrastructure resources are needed, we may then need to look into some capacity planning and upgrades to be able to run the system during and after the deployment.\n\nAdopting a trickle migration approach gives us the chance to catch up quickly with any learning curve involved with the new technologies being deployed, which, in return, helps mitigate system failure risks and deployment delays.\n\nThe ramped deployment strategy also helps mitigate other OPEX risks. As we will discuss in this chapter, during the deployment, we can test and monitor the performance of the newly deployed components, identify and resolve any issues, and make necessary adjustments before redirecting all traffic to these new components.\n\nAnother way of mitigating OPEX risks is to establish a robust change management process by establishing a structured and transparent process for managing changes. This includes creating clear guidelines for how changes will be proposed, evaluated, approved, and implemented, as well as communicating the changes to relevant stakeholders.\n\n223\n\n224\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nPart of the change management process is the rollback plan. The rollback plan is essential to bringing back the system to an operational state if a specific technical change is unsuccessful. The following are the steps we need to consider to build a successful rollback plan:\n\n1. Specify some checkpoints for the change where a rollback may be needed. In our example, and should the ACL be used, the ACL would be deployed prior to switching any traffic to the new microservice. During the change (and right after switching some test traffic to the ABC-Intelligent-MSA), a few good checkpoint examples would be as follows: I.\n\n1. Specify some checkpoints for the change where a rollback may be needed. In our example, and should the ACL be used, the ACL would be deployed prior to switching any traffic to the new microservice. During the change (and right after switching some test traffic to the ABC-Intelligent-MSA), a few good checkpoint examples would be as follows: Testing the payment verification communication between ABC-Monolith and the ACL\n\nTesting how the ACL processes the requests\n\nIII. Testing the communication between the ACL and the ABC-Intelligent-MSA system\n\nIV. Testing how the overall end-to-end requests are handled and whether they are processed\n\nas expected\n\nCommon Docker and Linux commands to test and troubleshoot the communication between the ACL, the monolith, and the MSA include the following:\n\n curl, to simulate an API call to the ACL or a specific microservice\n\n netstat, to check whether a specific service is actively listening to connections, what the\n\nlistening port is, and whether there are any active connections\n\n docker inspect, to return detailed JSON information about a specific microservice’s\n\nconfiguration, state, and network settings\n\n docker log, to view the logs of a running container\n\n2. Develop a plan for reversing the change at each of the preceding specified checkpoints.\n\n3. Whenever possible, test the rollback plan in a test or staging environment to ensure it is workable and complete.\n\n4. Know the time by which the rollback plan needs to be completely executed at each of the specified checkpoints, and allocate a reasonable amount of time in your change for it.\n\n5. Monitor the system throughout the change and make adjustments as needed.\n\n6. Have a post-mortem after the change, especially in case of a change failure.\n\n7.\n\nIn case the rollback plan is executed, the team will then need to include in the post-mortem the reasons for the change failure, and how effective the rollback plan was. They need to accordingly make the necessary adjustments to the deployment and rollback plan before scheduling another change.\n\nDeploying the MSA system\n\nBy this point, we should have a clear understanding of the deployment dependencies and risks and be able to determine methods for mitigating them. We are ready now to create a deployment plan and execute it in a manner that minimizes downtime and maintains business continuity.\n\nIn the next section, we will build the ABC-Intelligent-MSA system’s deployment plan in the presence of the running ABC-Monolith system.\n\nDeploying the MSA system\n\nIn Chapter 9 and Chapter 10, we discussed in detail how to install Docker, containers, and other components for our ABC-Intelligent-MSA system. This installation was mostly done in a lab environment with no specific regard to any existing system in the environment. We were basically just simulating a real-life development or staging environment.\n\nIn this section, we will focus rather on how we can take the ABC-Intelligent-MSA system we built, and gradually migrate it into a brownfield production environment where we have the ABC-Monolith system already running in production. The goal is to ramp up the ABC-Intelligent-MSA system’s operations until the system is able to carry the entire existing traffic, then completely phase out the old ABC-Monolith. Everything should be done with minimal operational interruptions.\n\nThe current status by now is that we still have ABC-Monolith running in production, and the ABC-Intelligent-MSA running in the staging environment. The following are detailed broken-down deployment plans with their execution steps.\n\nThe anti-corruption layer\n\nBoth our ABC-Monolith and ABC-Intelligent-MSA systems use the same type of RESTful APIs and the same JSON data formats. Moreover, our demo system is not complicated enough to justify an ACL. We, therefore, won’t be needing an ACL in our migration. However, we developed an ACL in our demo just in case you decide to try it out.\n\nIn case you are interested in trying the ACL, the first step you would need to do is to get the ACL up and running. The ACL will act as a buffer and handle the communication between the ABC-Monolith and the ABC-Intelligent-MSA systems.\n\n225\n\n226\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nFigure 12.1: Deploying with the ACL\n\nACLs are usually a specific custom-built code for the organization’s specific situation, the old system, and the new MSA system. We built the abc_acl ACL for our ABC system. The abc_acl code can be found in our GitHub repository.\n\nIt would make much more sense to deploy all the new components, including abc_acl, on a separate host or virtual workload. In our lab examples, however, and for simplicity, we are building the new system’s containers on the same host that’s running the ABC-Monolith.\n\nWe built the Facade, Adaptor, and Translator components all together as part of the ACL in one microservice. The Facade is created to interface with the ABC-Monolith, the Adaptor to interface with the ABC-Intelligent-MSA, and the translator for input/output data format mappings. Since we are using the same data formats in both the monolith and the MSA systems, the translator code is not doing any processing and is just used as a placeholder.\n\nWe can set up and start the abc_acl microservice the same way we did with other microservices in Chapter 9 and Chapter 10, using the docker build command to build the abc_acl_image image from the Dockerfile, then using the docker run command to create the abc_acl_container container, as follows:\n\n$ docker build -t abc_msa_customer_management ~/\n\nOnce the image is successfully created, use the following command to run the container, and start listening to port TCP/8020 on the host’s IP:\n\n$ docker run -itd -p 8020:8080 --name abc_acl_container abc_ acl_image\n\nDeploying the MSA system\n\nNow the ACL is running, it is time to test it before routing any traffic to it. We can do that using the shell curl command as we did in the previous chapters, or we can use some of the ACL built-in API tools created to verify the connection.\n\nThe following is a curl command issued on the host machine to ensure that the ACL is running successfully:\n\n$ curl http://192.168.1.100:8020/ <!DOCTYPE html> <head> <title>The Anti-Corruption Layer Microservice</title> </head> <body> <p>This is the ACL Microservice Part of ABC System. This ACL is used as part of the process of migrating ABC-Monolith to the new system, ABC-Intelligent-MSA </p> </body>\n\nThe following is an example of another way to test the ACL – more specifically, to test the communication between the ACL and both ABC-Monolith and ABC-Intelligent-MSA systems:\n\n$ curl http://192.168.1.100:8020/api?func=test_com {\"monolith_com_test\": \"Communication SUCCESSFUL\", \"msa_com_ test\": \"Communication SUCCESSFUL\"}\n\nThe first curl command ensures that the ACL is listening to API calls from both ABC-Monolith and ABC-Intelligent-MSA, while the second curl command ensures that the ACL can successfully communicate with both systems.\n\nThe ACL operation is now verified; in the next subsection, we will start migrating the MSA services from the staging (or lab) environment to the actual production environment running the old monolithic system.\n\nIntegrating the MSA system’s services\n\nWith the ACL now up and running and tested successfully, we are ready to start switching specific traffic to specific parts of the ABC-Intelligent-MSA. We will, however, use direct interaction between both the monolith and the MSA system since the ACL is not really needed in our demo example.\n\nPlease note that, depending on the existing monolith structure, design, and system’s code base, this process could either be very straightforward or as complicated as can be. Our deployment strategy requires some code changes in the monolith system to be able to route some parts of the traffic to the new MSA.\n\n227\n\n228\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nFor that reason, we may very well choose, in some systems, to have the MSA completely tested in a staging environment, then put the MSA on an LA period where partial production traffic is passing through the system for deeper testing. Then, once comfortable with the new MSA system’s performance, we can just start forwarding the entire production traffic, and finally, shut down the old monolith.\n\nFigure 12.2 shows a high-level view of the migration before and after status. During the migration, we will route a specific function of ABC-Monolith to one microservice in ABC-Intelligent-MSA. That microservice should be able to replace the corresponding function in the ABC-Monolith system. After we test the operation of that part of the migration, we then move traffic of another monolithic function, then another, and so on, until we end up migrating all of the ABC-Monolith functions to the ABC-Intelligent-MSA system.\n\nFigure 12.2: A high-level view of where we are and where to be\n\nWe can start with a simple microservice such as Notification Management (abc_msa_notify_ user_container). We can route the traffic destined to the notify_user() function in the ABC-Monolith by replacing the function’s code with an API call to abc_msa_notify_user_ container. All user traffic will still flow through the ABC-Monolith, but all user notifications will be processed through the ABC-Intelligent-MSA.\n\nDeploying the MSA system\n\nIn the same manner, the Customer Management microservice (abc_msa_customer_management_ container) should replace the register_customer() function from the ABC-Monolith, and Order Management should replace place_order() and order_status_update() functions, and so on.\n\nAs the system stabilizes, we gradually migrate to other MSA services. That migration cycle is shown in Figure 12.3.\n\nBy following the migration cycle, eventually, all of the ABC-Monolith functions will be replaced with microservices in the ABC-Intelligent-MSA system.\n\nFigure 12.3: The microservices integration testing and tuning cycle\n\nFigure 12.4 shows a snapshot of the system status during the migration process. In the figure, we have the Notification Management, Customer Management, and Order Management microservices successfully migrated, but not any other microservice yet.\n\n229\n\n230\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nFigure 12.4: A system snapshot during the migration process\n\nMonitor the system closely during the monolithic function migration, and use the rollback plan if necessary. Once the last ABC-Monolith function is migrated to the new system, we will need to carefully run an end-to-end test on the ABC-Intelligent-MSA system to ensure the system is running properly and independent of the ABC-Monolith.\n\nTesting all microservice logs and stats is essential in the testing process. We need to have a formal testing process in place every step of the way during the migration process. The test process is described in more detail in the next section.\n\nKeep both systems running for a period of time just in case some overseen issues take place and always be prepared with a contingency plan.\n\nThe final step is shutting down the monolith. If the migration steps were followed and tested correctly, user traffic and system operations should not be impacted. However, complex systems may have a component or more still processing traffic. To avoid business interruptions in this situation, it is best to shut down the monolith during a maintenance window to allow the migration team to analyze any unforeseen issues and create a plan to resolve them.\n\nIn this section, using our ABC system, we explained the MSA system deployment process using an ACL and using a direct monolith-to-microservices approach. We covered the steps to be taken, what to watch for, and how to make the transition to the new system as smooth as possible with minimal system interruption.",
      "page_number": 231
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 240-253)",
      "start_page": 240,
      "end_page": 253,
      "detection_method": "topic_boundary",
      "content": "Testing and tuning the MSA system\n\nIn the next section, we will cover the formal test methodology that should be planned and followed after every microservice migration to ensure system reliability and stability.\n\nTesting and tuning the MSA system\n\nPrior to deploying microservices, a formal testing or QA process should be applied to each microservice to prevent errors during deployment and in production.\n\nThere are a couple of tests that need to be performed on the MSA system microservices before deploying them in the production environment. First, testing the microservice itself as a standalone and before integrating it into any parts of the ABC system – what we refer to it as, unit testing. Second, testing the integration of that microservice into the ABC-Intelligent-MSA system – what we refer to it as integration testing. And third, testing how the microservice functions during an interim mix of operations between the ABC-Monolith and the ABC-Intelligent-MSA systems.\n\nTesting the ABC system functions every time a new microservice is deployed is crucial to ensure a successful migration and that the system is able to properly function and sustain the applied traffic load and user requests.\n\nBuilding structured test cases is an important part of the testing process. Test cases are a set of steps that describe how to test a specific feature or functionality of a system. These test cases should be well defined, easy to understand, and should cover all possible scenarios.\n\nCreating a test case should include the following main steps:\n\n1.\n\nIdentify the requirements of the system and the feature or functionality that we want to test.\n\n2. Write a test case that describes the steps to be taken to test that feature or functionality.\n\n3.\n\nIn the test case, specify any prerequisites that are required to run the test.\n\n4.\n\nIdentify the pass/fail criteria based on the expected outcome of the test case.\n\n5. Run the test case and compare the test results to the expected outcome. Accordingly, and based on the pass/criteria specified, record the result of the test in simple PASS or FAIL terms.\n\nThe following is a simple example of a test case for the Notification Management microservice. The test case verifies that the microservice is actually sending an SMS notification to a registered user’s mobile number. Another test case should also be written to test the microservice’s email Send functionality. We can write as many test cases as needed for each individual microservice, and for the system functionality overall.\n\n231\n\n232\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nTest Case Details\n\nTitle\n\nABC-Intelligent-MSA Notification Management Microservice SMS Functionality\n\nID\n\n002912\n\nDescription\n\nTo ensure that the microservice is actually sending an SMS notification to the specified mobile number\n\nRequirement(s)\n\nAccess to the SMS gateway.\n\nTest Setup\n\nHave access to the receiver test phone +1 (555) 555-5555.\n\nHave access to the Ubuntu test environment.\n\nProcedure\n\nVerify SMS gateway access. Ensure the abc_msa_notify_user_container container is running, or start it as follows:\n\ndocker container start abc_msa_notify_user_container\n\nIssue a shell curl command as follows:\n\ncurl http://192.168.1.100:8010/api?func=send_ sms&num=15555555555&msg=order+received\n\nTest Type\n\nUnit testing\n\nPass/Fail Criteria\n\nTest case passes if you receive the message “order received” on the test phone\n\nTable 12.1 – A sample test case for notification management in ABC-Intelligent-MSA\n\nTesting the AI services of the ABC-Intelligent-MSA system can be more challenging, and the conventional test case approach may not be sufficient. Testing the AI part of the system will require a multi-level approach that would require including the microservice itself in isolation (unit testing), integration testing, functional testing, performance testing, data validation testing, and human-in- the-loop testing. By using all these approaches together in building your test cases, we can ensure that the AI components of the system are functioning as intended and are making accurate predictions and decisions.\n\nIn this section, we covered the importance of building a structured testing process and built a test case example as part of our system’s testing process. We discussed how to create a test case and identify the requirements and expected outcomes.\n\nIn the next section, we will talk about the importance of conducting a post-deployment review after the completion of the ABC-Intelligent-MSA system deployment. The section will also cover the different types of post-deployment reviews, including user feedback reviews.\n\nThe post-deployment review\n\nThe post-deployment review\n\nThe ABC-Intelligent-MSA system is currently running, but it hasn’t been operational for a sufficient amount of time to guarantee its stability and resilience under typical traffic patterns and loads. A post-deployment review is crucial for ensuring the success of the ABC system deployment and its compliance, as well as enhancing its functionality and overall user satisfaction.\n\nDuring the post-deployment review, we will need to monitor the system closely and look for any errors, bugs, or any other operational problems that may happen. Then, we will need to make recommendations for addressing system issues and making necessary improvements to the system to ensure that the system is meeting the user requirements it was created for.\n\nWe need to have special monitoring for the AI services we built in the system to make sure they are performing as they are supposed to and continuously improving themselves and the system’s operations overall. A closer look at the AI services logs that we discussed in Chapter 10 is important to ensure the system’s stability and enhanced performance.\n\nThe following are some of the aspects and criteria that need to be considered when conducting a post-deployment review.\n\nChecking the new system’s performance\n\nWe start by defining performance metrics, which will help us create a baseline for what to expect from the system, in terms of response time, user interactions, network traffic, and so on. We can use tools available on the internet or the ms_perfmon.py we previously discussed in Chapter 10 to measure the performance of the new system and compare that to the monolith’s performance.\n\nThe variance between both the old and the new system’s performances would highly depend on the design, architecture, operational criteria, and infrastructure used in both cases.\n\nIdentifying and fixing system defects\n\nThis goes back to the testing and tuning process discussed earlier, and how the process should be conducted. It is important to point out here that post-deployment, identifying system defects is not yet part of the QA process until they are first documented in the organization’s defect tracking system.\n\nWe are talking here about monitoring the operational aspects of the system and ensuring proper system supportability. The support process may very well lead to filing specific issues found in the system post-deployment. Later, a thorough investigation of customer support cases with their severity levels will need to be conducted to address and fix these issues.\n\nSystem issues can also be identified by gathering customer feedback, as we will discuss in the next couple of sections, as well as from the outcomes of the different audit processes conducted on the system post-deployment.\n\n233\n\n234\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nCompliance\n\nPerform regular maintenance and updates to the system to keep it running smoothly. As briefly discussed in Chapter 8, a considerable part of compliance can be done through automation or commercial tools. The tools will help audit the system for different types of compliances, such as the GDPR, PCI, HIPAA, SCSEM, and so on. The specific compliances that an organization has to comply with will depend on the organization’s business itself, the nature of the system, and what processes and users it is serving.\n\nStart by identifying the relevant regulations and standards that apply to the new system. This may include data privacy regulations, industry-specific standards, and cybersecurity standards.\n\nConduct a risk assessment the way we described in the previous chapter, to identify any potential areas of non-compliance and their associated risks. This may involve reviewing the process of the system’s design, architecture, and data processing. Then, put together a mitigation plan to mitigate the identified risks.\n\nMake sure the organization’s staff are fully aware of compliance, its importance, and the individual roles and responsibilities in that regard. Keeping the staff trained is another aspect of keeping the organization compliant with specific rules, regulations, and specific industry compliances.\n\nThe compliance process is not a one-time thing, the organization has to conduct regular audits to maintain that compliance. Audits may include regularly running specific automated audit tools, and conducting manual system audits by checking system logs, data checks, physical and digital security checks, and so on.\n\nSystem maintenance and updates\n\nJust like your preventive car maintenance, performing regular system maintenance and updates is important to keep the system running smoothly with no sudden unplanned failures.\n\nBy planning, preparing, testing, implementing, monitoring, documenting the process, and taking a proactive role, we can ensure that our newly deployed system is functioning as expected and able to minimize operational interruptions. The following are a few points to consider in the maintenance plan:\n\n1. Put together a regular maintenance plan. This is a must-have for successful and reliable operations. This includes which part of the system needs to be updated, what maintenance activities need to be conducted and how often, prioritizing the maintenance tasks, and determining the resources required.\n\n2. Make sure you have a regular system backup plan in place and have an updated backup before any maintenance work. This is important to bring back the system to its original state in case of any work mishaps.\n\n3. Test any planned work before actually applying the update or the change. Test the change thoroughly in a lab or staging environment to ensure it is functioning as expected.\n\nThe post-deployment review\n\n4. Monitor the system after the updates have been deployed. This includes monitoring performance metrics, running automated checks, checking users’ feedback, and checking system logs.\n\n5. Update your documentation with the changes, and document the maintenance and update outcome. The documentation will help ensure that the maintenance and update process is repeatable for future reference, and help troubleshoot in case of any issues that may happen in the future.\n\nIn the beginning, the maintenance plan may not be as perfect as you may like it to be, but as the process is repeated during the lifetime of the system, the process will eventually get refined to a very accurate level.\n\nUser satisfaction\n\nMonitoring and improving user satisfaction are sometimes underestimated in the success of deploying any new IT system. By gathering feedback from the system’s internal and external customers, analyzing that feedback, prioritizing changes, implementing changes, monitoring progress, and continuously improving, we can ensure that the system meets customer requirements.\n\nThe following is a four-step cycle for ensuring high customer satisfaction post-deployment:\n\nFigure 12.5: The four-step customer satisfaction cycle\n\n1. The first step in monitoring our customer satisfaction is to gather feedback from the system users. The feedback can be collected through surveys, direct customer interaction and visits, phone conversations, and so on.\n\n235\n\n236\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\n2. Analyze the gathered feedback to identify common complaints, common patterns, and specific use cases that may have not been covered during the system testing phase. This will help us understand the strengths of the system and the areas where we need to improve.\n\n3. Prioritize whatever system changes are decided as an outcome of the gathered feedback. This part should have the biggest impact on customer satisfaction. It will show customers that you are addressing their concerns, reacting to their requests, and, sometimes, even being proactive to customer needs. Implement the changes as prioritized. Start with the changes with the highest impact and lowest effort similar to what we discussed in Chapter 11 under the Risk mitigation section and in Figure 11.2.\n\nWe need to continuously regather customer feedback to regularly monitor customer satisfaction progress. This will ensure that the changes being carried out are having the desired customer satisfaction effect.\n\nThe four-step process will help continuously meet customer needs and improve customer satisfaction accordingly. The process helps also constantly improve the system features, supportability, reliability, and stability to enhance the overall user experience.\n\nIn this section, we covered the post-deployment review process, the different aspects, and activities that need to be considered when conducting the review, and how that is essential in the overall success of the ABC-Intelligent-MSA system operations.\n\nSummary\n\nIn this chapter, we discussed, the various steps involved in the successful deployment of the new system. We talked about the importance of overcoming the system deployment dependencies, the importance of building structured test cases, and the steps involved in testing and tuning the system.\n\nBy following the steps outlined in this chapter, organizations can ensure the successful deployment of their MSA system, including overcoming dependencies, integrating with the monolith during the transition phase, testing and tuning the system, and conducting a post-deployment review. The chapter concluded by emphasizing the significance of following a customer satisfaction cycle and having customers engaged in the process of adapting the new system’s operations.\n\nSymbols\n\nDynamic Host Configuration Protocol (DHCP) 173\n\nA\n\nABC-Container Engine 152 ABC-Intelligent-MSA deployment risks\n\nmitigating 223, 224\n\nABC-Intelligent-MSA system\n\ninitializing 188 operations 187 operation, simulating 191 training data, building 189, 190 training data, using 189, 190\n\nABC-Monolith 29 architecture 30 components, reusing 222, 223 current functions 30 database 31, 32 data decomposition 35-38 dependencies, reusing 222, 223 function decomposition 33, 34 requests decomposition 38-40 workflow and current function calls 32, 33\n\nIndex\n\nABC-MSA\n\nenhancements 58, 59 ABC-MSA containers\n\nAPI Gateway 164, 165 creating 162, 163 Customer Management\n\nmicroservice 165-169\n\nfrontend web dashboard interface 169 services 163 system’s containers, managing 170, 171\n\nABC-MSA database access 37 ABC-MSA microservices inter- communication 172\n\nDocker network 172, 173 TCP/IP communication, between\n\ncontainers/microservices 173-175\n\nABC-MSA workflow 39 A/B testing deployment 204 ACID transactions 17 ACL Translator 45 adversarial search 122\n\nmethods 123\n\naggregator communication pattern 53 aggregators 53\n\nversus API gateway 54, 55 versus orchestrators 54, 55\n\n238\n\nAgile development methodology 142, 143 AI microservice building 178 enhancements, anatomy 179-181 self-healing process 181-183 tools, building 184 AI microservice, tools\n\nartificial intelligence\n\nversus deep learning 63-67 versus machine learning 63-67\n\nArtificial Neural Networks (ANNs) 65, 66 artificial neurons 65 Authentication-Authorization- Accounting (AAA) 48\n\nAPI response error simulator 187 API traffic generator/simulator 184 microservices performance\n\nmonitor 185, 186\n\nresponse delay simulator 186\n\nautoencoders 94, 131 Autograd 72 automation 110, 143, 144 Auto-Regression (AR) 86 Auto-Regressive Integrated Moving\n\nAI service operations\n\nAverage (ARIMA) 86\n\nanalyzing 191 PAD operations 197-200 PBW operations 192-196\n\nB\n\nanomaly detection 130 anomaly detection, types collective anomaly 130 contextual anomaly 130 point anomaly 130\n\nAnti-Corruption Layer (ACL) 44, 225-227\n\ncomponents 45 using, to isolate MSA 43-45\n\nAPI gateway\n\nBest Alternative To a Negotiated\n\nAgreement (BATNA) 214\n\nbias 78 big bang migration 44 binary classifier 81 Blue/Green deployment 204 bottleneck 95 brownfield deployment 206 brownfield risk (BR) 212 business continuity (BC) 6\n\ndisadvantages 49 functions 48 using 46-49 versus aggregators 54, 55 versus orchestrators 54, 55\n\nAPI response error simulator 187 API traffic generator/simulator 184 Area Under Curve - Receiver Operating\n\nCharacteristics Curve (AUC-ROC) 120\n\nArea Under the Receiver Operating\n\nCharacteristics Curve (AUROC) 108\n\nC\n\nCanary deployment 204 choreography 18 circuit breaker 57 coefficient of determination 77 collective anomaly 130 Command Query Responsibility Segregation (CQRS) 20-22\n\nComma-Separated Values (CSV) 185\n\n239\n\nCommon Closure Principle 28 Community Edition (CE) 157 compensating transactions 17 concept shifts 120 Configuration-as-a-Code (CaaC) 146 containers 151, 152\n\ndeep learning features 66 libraries, used in Python 67 used, for enhancing models 87-95 used, for implementing system\n\nself-healing 130\n\nuses 153-156 versus VMs 152, 153 contextual anomaly 130 Continuous Integration/Continuous\n\nDevelopment (CI/CD) 23, 141\n\nConvolutional Neural Networks (CNNs) 67 cost function 65 covariate shifts 117 Create-Read-Update-Delete (CRUD) 20\n\nversus CQRS pattern 21\n\nD\n\nversus artificial intelligence 63-67 versus machine learning 63-67\n\nDenial of Service (DoS) 128 density ratio estimation 123 deployment challenges, overcoming 210 modifying, mitigation plan 217, 218 monitoring, identified risks 217, 218 risk mitigation plan, developing 213 risk mitigation plan, implementing 213 risks, identifying 211, 212 risks, prioritizing 212, 213 rollback plan 217 testing system’s requirements and\n\ndata 80\n\nfunctionality 217, 218\n\ncleaning 80 normalizing 80 data components\n\ncyclical 84 irregular 84 seasonality 84 trend 84\n\ndataset shifts 113, 116\n\ndeployment strategies 204\n\nA/B testing deployment 204 Blue/Green deployment 204 Canary deployment 204 comparing 205 Ramped deployment 204 Recreate deployment 204 Shadow Deployment 204\n\nadversarial search 122, 123 causes 117 concept shifts 120 covariate shifts 117 density ratio estimation 123 feature dropping 121 identifying 117-121 prior probability shifts 118, 119 stabilizing 121\n\nDevOps 140\n\napplying, from maintenance 145 applying, from start to operations 145 CI/CD 146, 147 code quality assurance 147, 148 configuration management 145, 146 disaster management 149 in microservices architecture (MSA) 22-25 monitoring 148, 149 source code version control 145\n\n240\n\nDevOps processes\n\nin enterprise MSA system operations 141\n\nF\n\nDevOps team structure 140, 141 differencing 85 dimensionality reduction 95 Distributed Denial of Service (DDoS) 48 Docker\n\nfeature engineering 80 feature extraction 66 feature map 87 fit interface 105, 106 Flask 163\n\ninstalling 157\n\nDocker components 158-162 Docker Compose 170 Docker container 159-161 Docker Docs\n\nG\n\nGeneral Data Protection\n\nRegulation (GDPR) 144\n\nreference link 157, 158\n\nDocker Engine installing 157\n\nDocker Engine API documentation\n\nGitHub 156 greenfield deployment 206 greenfield risk (GR) 211 greenfield deployment, versus\n\nreference link 196\n\nbrownfield deployment 209\n\nDocker file 158 Docker Hub 156\n\nreference link 163 Docker image 159 Docker network 172 Docker volume 161, 162\n\nE\n\ncost 207 flexibility 206 integration 207 risks 208 scalability 206, 207 staff onboarding 208 technology stack 207 time-to-market 208 user adoption 209\n\neager learners 81 enterprise MSA system operations\n\nDevOps processes 141\n\nH\n\nEntity Relationship Diagram (ERD) 31 epics 140 event-driven architecture (EDA) 15, 16 eventual consistency synchronization 22 Exploratory Data Analysis (EDA) 80, 99\n\nHealer AI Service 181 Health Insurance Portability and\n\nAccountability Act (HIPAA) 144, 210\n\nhyperparameters 114, 115\n\nexamples 114 hypervisor 152\n\n241\n\nI\n\nInfrastructure-as-a-Code (IaaC) 146 Integrated (I) 86 integration testing 231\n\nK\n\nmachine learning (ML) 178\n\nadvantage 177, 178 forecasting 84-87 libraries, used in Python 67 need for 25 pattern analysis 84-87 versus artificial intelligence 63-67 versus deep learning 63-67 using, in CI/CD pipeline 25\n\nKeras 71, 72 kernel 87 K-Fold Cross-Validation 106 K-Nearest Neighbors (KNN) algorithm 81 Kubernetes 156\n\nmachine learning model hyperparameters 114 parameters 114\n\nmachine learning systems\n\nL\n\nlabel 65, 74 label shifts 118 Latent Dirichlet Allocation (LDA) 82 lazy learners 81 Limited Availability (LA) approach 215 linear microservices pipeline 4 local transaction 17 logistic function 89 Long Short-Term Memory (LSTM) model 93 loosely coupled monolithic system versus tightly coupled monolithic\n\ncomponents 97-100 interfaces 100 stages 97 Marathon 156 Matplotlib 69\n\nreference link 69\n\nMean Absolute Error (MAE) 108 Mean Squared Error (MSE) 108 Mean Time to Resolution (MTTR) 7 microservices\n\nadvantages 6-9 disadvantages 9, 10\n\nmicroservices aggregators 51-53 microservices architecture (MSA) 3, 125\n\nM\n\nsystem 12, 13\n\nadvantages 11 DevOps 22-25 isolating, with ACL 43-45 need for 4, 5\n\nmachine learning, in MSA enterprise system\n\nsystem decay prediction 125 system load prediction 125 system resource planning 126 system security 126 use cases 125-128\n\nmicroservices circuit breaker 56-58 microservices performance monitor 185, 186\n\nmodel embedding 108 models\n\nenhancing, with deep learning 87-95 issues 93\n\n242\n\nmodel serving 108 monolithic application\n\nP\n\nversus microservices application 14, 15\n\nPandas 70\n\nmonolithic architecture\n\nversus microservices system 13\n\nMoving Average (MA) 86 MSA hybrid model architecture 15, 16 MSA system\n\nreference link 70 parameterization 113 parameters\n\nexamples 114\n\npattern analysis machine learning\n\nanti-corruption layer 225-227 deploying 225 services, integrating 227-230 testing 231, 232 tuning 231, 232\n\nused, for enhancing system supportability 129, 130\n\nused, for enhancing time-to-\n\nresolution (TTR) 129, 130\n\nPayment Card Industry Data Security\n\nmulticlass classification\n\nStandard (PCI DSS) 144\n\nbuilding 80, 81\n\nmulticlass classifiers 81\n\nPayment Card Industry (PCI) 210 Performance Anomaly Detector (PAD) 178\n\noperations 197-200\n\nN\n\nPerformance Baseline Watchdog (PBW) 178\n\noperations 192-196\n\nNano-service anti-pattern 28 Natural Language Processing (NLP) 72, 82 negotiation strategies 214 Network Management System (NMS) 178 non-linear microservices pipeline 5 NumPy 68, 69\n\nreference link 68\n\nO\n\nobject-oriented programming (OOP) 8 Operation Support System (OSS) 178 orchestration 18, 19, 109-112 orchestrators 49-51\n\nbenefits 50 versus aggregators 54, 55 versus API gateway 54, 55\n\npoint anomaly 130 post-deployment assessment 218 post-deployment review 233\n\ncompliance 234 system defects, fixing 233 system defects, identifying 233 system maintenance and updates 234 system’s performance, checking 233 user satisfaction 235, 236 pre-production review 218 Principle Component Analysis (PCA) 95 prior probability shifts 118, 119 Python\n\ndeep learning, used 67 machine learning, used 67\n\nPyTorch 72\n\nreference link 72\n\norganizational structure alignment 138 overfitting 77\n\n243\n\nQ\n\nSciPy 73\n\nreference link 73\n\nquantitative variable 74\n\nR\n\nR2 metrics 76 Ramped deployment 204 Recreate deployment 204 Recurrent Neural Networks (RNNs) 84, 87 regression models building 74-79 regularization 78 ReLU function 88 residual errors 76 response delay simulator 186 risk exposure 212 risk mitigation plan\n\nSelf-Healing Lock State 181 self-healing process 128 sentiment analysis 83 Service Catalog 49 service-driven architecture 15 service-oriented architecture (SOA) 15 serving interface 108, 109 Shadow Deployment 204 sprint 142 Sprint Planning Meetings 142 supervised anomaly detection 131 system dependencies\n\nABC-Intelligent-MSA deployment\n\nrisks, mitigating 223, 224\n\nABC-Monolith components,\n\nreusing 222, 223\n\nBR1, system capability limitations 216 BR2, high OPEX 216 GR1, high CAPEX risk 213-215 GR2, deployment time risks 215 GR3, system failure risks 216 GR4, user adoption risks 216\n\nrobust system\n\ncomponents 111\n\nABC-Monolith dependencies,\n\nreusing 222, 223\n\novercoming 222 system self-healing\n\nimplementing, with deep learning 130\n\nsystem’s microservices identifying 27, 29 system supportability\n\nRoot Mean Squared Error (RMSE) 76, 108 R-Squared (R2) 108\n\nenhancing with pattern analysis machine learning 129, 130\n\nS\n\nT\n\nSafeguard Computer Security Evaluation\n\nMatrix (SCSEM) 144\n\nsaga patterns 17-20, 36 scaling transformations\n\ntypes 103\n\nscikit-learn 73, 74 reference link 73\n\nTanh function 89, 90 technical debt 208 TensorFlow 71, 72 Tensors 71 text sentiment analysis 82-84 The ACL Adapter 45 The ACL Facade 45\n\n244\n\ntightly coupled monolithic system versus loosely coupled monolithic\n\nsystem 12, 13\n\ntime-to-market (TTM) 3 time-to-resolution (TTR)\n\nenhancing, with pattern analysis machine learning 129, 130\n\ntopic modeling 82-84 training data 64 training interface 106-108 transformation 86 transform interface 100-104\n\ntypes 101\n\ntrends\n\nnon-stationary 85 stationary 85\n\ntrickle migration 44 Tyk Docker container installation link 164\n\nU\n\nunderfitting 77 Unhealable Wait Period 181 unit testing 231 UNIX chroot 152 unsupervised anomaly detection 131 user interface/user experience (UI/UX) 216\n\nW\n\nWeb Server Gateway Interface (WSGI) 163 workflows 49",
      "page_number": 240
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 254-261)",
      "start_page": 254,
      "end_page": 261,
      "detection_method": "topic_boundary",
      "content": "Packtpub.com\n\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.\n\nWhy subscribe?\n\nSpend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals\n\n\n\nImprove your learning with Skill Plans built especially for you\n\nGet a free eBook or video every month\n\nFully searchable for easy access to vital information\n\nCopy and paste, print, and bookmark content\n\nDid you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at packtpub.com and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at customercare@packtpub. com for more details.\n\nAt www.packtpub.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.\n\nOther Books You May Enjoy\n\nIf you enjoyed this book, you may be interested in these other books by Packt:\n\nApplying Math with Python - Second Edition\n\nSam Morley\n\nISBN: 9781804618370\n\nBecome familiar with basic Python packages, tools, and libraries for solving mathematical problems\n\nExplore real-world applications of mathematics to reduce a problem in optimization\n\nUnderstand the core concepts of applied mathematics and their application in computer science\n\nFind out how to choose the most suitable package, tool, or technique to solve a problem\n\n\n\nImplement basic mathematical plotting, change plot styles, and add labels to plots using Matplotlib\n\nGet to grips with probability theory with the Bayesian inference and Markov Chain Monte Carlo (MCMC) methods\n\n247\n\nApplied Machine Learning Explainability Techniques\n\nAditya Bhattacharya\n\nISBN: 9781803246154\n\nExplore various explanation methods and their evaluation criteria\n\nLearn model explanation methods for structured and unstructured data\n\nApply data-centric XAI for practical problem-solving\n\nHands-on exposure to LIME, SHAP, TCAV, DALEX, ALIBI, DiCE, and others\n\nDiscover industrial best practices for explainable ML systems\n\nUse user-centric XAI to bring AI closer to non-technical end users\n\nAddress open challenges in XAI using the recommended guidelines\n\n248\n\nPackt is searching for authors like you\n\nIf you’re interested in becoming an author for Packt, please visit authors.packtpub.com and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.\n\nShare Your Thoughts\n\nNow you’ve finished Machine Learning in Microservices, we’d love to hear your thoughts! If you purchased the book from Amazon, please click here to go straight to the Amazon review page for this book and share your feedback or leave a review on the site that you purchased it from.\n\nYour review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.\n\nDownload a free PDF copy of this book\n\nThanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print books everywhere? Is your eBook purchase not compatible with the device of your choice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application.\n\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily\n\nFollow these simple steps to get the benefits:\n\n1. Scan the QR code or visit the link below\n\nhttps://packt.link/free-ebook/9781804617748\n\n2. Submit your proof of purchase\n\n3. That’s it! We’ll send your free PDF and other benefits to your email directly\n\n249",
      "page_number": 254
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 262-269)",
      "start_page": 262,
      "end_page": 269,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 262
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 270-270)",
      "start_page": 270,
      "end_page": 270,
      "detection_method": "topic_boundary",
      "content": "",
      "page_number": 270
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "Machine Learning in Microservices\n\nProductionizing microservices architecture for machine learning solutions\n\nMohamed Abouahmed\n\nOmar Ahmed\n\nBIRMINGHAM—MUMBAI",
      "content_length": 158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "Machine Learning in Microservices\n\nCopyright © 2023 Packt Publishing\n\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.\n\nEvery effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the authors, nor Packt Publishing or its dealers and distributors, will be held liable for any damages caused or alleged to have been caused directly or indirectly by this book.\n\nPackt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.\n\nPublishing Product Manager: Dinesh Chaudhary Content Development Editor: Joseph Sunil Technical Editor: Rahul Limbachiya Copy Editor: Safis Editing Project Coordinator: Farheen Fathima Proofreader: Safis Editing Indexer: Subalakshmi Govindhan Production Designer: Prashant Ghare Marketing Coordinator: Shifa Ansari, Vinishka Kalra\n\nFirst published: February 2023\n\nProduction reference: 1170223\n\nPublished by Packt Publishing Ltd. Livery Place 35 Livery Street Birmingham B3 2PB, UK.\n\nISBN 978-1-80461-774-8\n\nwww.packtpub.com",
      "content_length": 1508,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "To my mother, who endured a lot to shape me into who I am today. To my wife, whose unwavering support has been crucial in my journey. And to my late father, who I deeply miss and who continues to inspire me every step of the way. I love you all, thank you for everything you have done and continue to do for me. This book is a testament to your support and a small token of my appreciation to you all.\n\n– Mohamed Abouahmed\n\nTo my mother and father, who have been an inspiration, for pushing me to become the best version of myself. And to my brother, who has been like a great friend with his continual support and motivation. I wouldn’t be the person I am today without you all in my life. Thank you!\n\n– Omar Ahmed",
      "content_length": 715,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "Contributors\n\nAbout the authors\n\nMohamed Abouahmed is a principal architect and consultant providing a unique combination of technical and commercial expertise to resolve complex and business-critical issues through the design and delivery of innovative, technology-driven systems and solutions. He specializes in network automation solutions and smart system development and deployment.\n\nMohamed’s hands-on experience, underpinned by his strong project management and academic background, which includes a PMP certification, master’s of global management, master of business administration (international business), master of science in computer networking, and BSc in Electronics Engineering, has helped him develop and deliver robust solutions for multiple carriers, service providers, enterprises, and Fortune 200 clients.\n\nOmar Ahmed is a skilled computer engineer with experience at various start-ups and corporations working on a variety of projects, from building scalable enterprise systems to deploying powerful machine learning models.\n\nHe has a bachelor’s degree in computer engineering from the Georgia Institute of Technology and is currently finishing up his master’s in computer science with a specialization in machine learning at the Georgia Institute of Technology.",
      "content_length": 1284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "About the reviewer\n\nSumedh Datar is a senior machine learning engineer with more than 6 years of work experience in the fields of deep learning, machine learning, and software engineering. He has a proven track record of single-handedly delivering end-to-end engineering solutions to real-world problems. He works at the intersection of engineering, research, and product and has developed deep learning-based products from scratch that have been used by a lot of end customers. Currently, Sumedh works in R&D, where he works on applied deep learning with less data, and has been granted several patents and applied for several more. Sumedh studied biomedical engineering focused on computer vision and then went on to pursue a master’s in computer science focused on AI.",
      "content_length": 771,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "Table of Contents\n\nPreface\n\nPart 1: Overview of Microservices Design and Architecture\n\n1\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nWhy microservices? Pros and cons Advantages of microservices Disadvantages of microservices The benefits outweigh the detriments\n\nLoosely versus tightly coupled monolithic systems Service-driven, EDA, and MSA hybrid model architecture ACID transactions\n\n4 6 9 11\n\n12\n\n15 17\n\nSaga patterns Command Query Responsibility Segregation (CQRS)\n\nDevOps in MSA Why ML?\n\nSummary\n\n2\n\nRefactoring Your Monolith\n\nIdentifying the system’s microservices 27 29 The ABC monolith 30 The ABC-Monolith’s current functions 31 The ABC-Monolith’s database 32 The ABC workflow and current function calls\n\nData decomposition Request decomposition Summary\n\nFunction decomposition\n\n33\n\nxiii\n\n3\n\n17\n\n20\n\n22 25\n\n26\n\n27\n\n35 38 41",
      "content_length": 851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "viii\n\nTable of Contents\n\n3\n\nSolving Common MSA Enterprise System Challenges\n\nMSA isolation using an ACL Using an API gateway Service catalogs and orchestrators Microservices aggregators\n\n43 46 49 51\n\nGateways versus orchestrators versus aggregators Microservices circuit breaker ABC-MSA enhancements Summary\n\nPart 2: Overview of Machine Learning Algorithms and Applications\n\n4\n\nKey Machine Learning Algorithms and Concepts\n\nThe differences between artificial intelligence, machine learning, and deep learning Common deep learning and machine learning libraries used in Python Building regression models Building multiclass classification\n\n63\n\n67 74 80\n\nText sentiment analysis and topic modeling Pattern analysis and forecasting in machine learning Enhancing models using deep learning Summary\n\n5\n\nMachine Learning System Design\n\nMachine learning system components 97 100 Fit and transform interfaces 100 Transform 105 Fit\n\nTrain and serve interfaces Training Serving\n\nOrchestration Summary\n\n43\n\n54 56 58 60\n\n63\n\n82\n\n84\n\n87 95\n\n97\n\n106 106 108\n\n109 112",
      "content_length": 1052,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Table of Contents\n\n6\n\nStabilizing the Machine Learning System\n\n113\n\nMachine learning parameterization and dataset shifts The causes of dataset shifts\n\n113 117\n\nIdentifying dataset shifts 117 Handling and stabilizing dataset shifts 121 124 Summary\n\n7\n\nHow Machine Learning and Deep Learning Help in MSA Enterprise Systems\n\n125\n\nMachine learning MSA enterprise system use cases Enhancing system supportability and time-to-resolution (TTR) with pattern analysis machine learning\n\n125\n\n129\n\nImplementing system self-healing with deep learning Summary\n\n130 132\n\nPart 3: Practical Guide to Deploying Machine Learning in MSA Systems\n\n8\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems 137\n\nDevOps and organizational structure alignment DevOps The DevOps team structure\n\nDevOps processes in enterprise MSA system operations The Agile methodology of development Automation\n\n138 138 140\n\n141 142 143\n\nApplying DevOps from the start to operations and maintenance Source code version control Configuration management and everything as a code CI/CD Code quality assurance Monitoring Disaster management\n\n145 145\n\n145 146 147 148 149\n\nSummary\n\n150\n\nix",
      "content_length": 1155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "x\n\nTable of Contents\n\n9\n\nBuilding an MSA with Docker Containers\n\n151\n\nWhat are containers anyway, and why use them? Installing Docker Docker Engine installation Docker components\n\nCreating ABC-MSA containers ABC-MSA containers\n\n151 157 157 158\n\n162 163\n\nManaging your system’s containers\n\nABC-MSA microservice inter- communication The Docker network TCP/IP communication between containers/ microservices\n\nSummary\n\n170\n\n172 172\n\n173\n\n175\n\n10\n\nBuilding an Intelligent MSA Enterprise System\n\n177\n\nThe machine learning advantage Building your first AI microservice The anatomy of AI enhancements The self-healing process Building the necessary tools\n\nThe intelligent MSA system in action 187 188 Initializing the ABC-Intelligent-MSA system\n\n177 178 179 181 184\n\nBuilding and using the training data Simulating the ABC-Intelligent-MSA’s operation\n\nAnalyzing AI service operations The PBW in action The PAD in action\n\nSummary\n\n189\n\n191\n\n191 192 197\n\n201\n\n11\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\n203\n\nDeployment strategies Greenfield versus brownfield deployment Flexibility Scalability Technology stack Integration Cost\n\n204\n\n205 206 206 207 207 207\n\nTime-to-market Risks Staff onboarding User adoption\n\nOvercoming deployment challenges 210 211 Identify deployment risks 212 Prioritize risks\n\n208 208 208 209",
      "content_length": 1336,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "Table of Contents\n\nDeveloping and implementing a risk mitigation plan The rollback plan\n\n213 217\n\nTest, monitor, and adjust Post-deployment and pre-production review\n\n217 218\n\nSummary\n\n219\n\n12\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\n221\n\nOvercoming system dependencies Reusable ABC-Monolith components and dependencies Mitigating ABC-Intelligent-MSA deployment risks\n\nDeploying the MSA system The anti-corruption layer Integrating the MSA system’s services\n\n222\n\n222\n\n223\n\n225 225 227\n\nTesting and tuning the MSA system 231 233 The post-deployment review 233 Checking the new system’s performance 233 Identifying and fixing system defects 234 Compliance 234 System maintenance and updates 235 User satisfaction\n\nSummary\n\n236\n\nIndex\n\n237\n\nOther Books You May Enjoy\n\n246\n\nxi",
      "content_length": 805,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "Preface\n\nMachine Learning (ML) has revolutionized the technology industry and our daily lives in ways that were previously thought impossible. By combining ML algorithms with Microservices Architecture (MSA), organizations are able to create intelligent, robust, flexible, and scalable enterprise systems that can adapt to changing business requirements and improve overall system performance.\n\nThis book is a comprehensive guide that covers different approaches to building intelligent MSA systems and solving common practical challenges faced in system design and operations.\n\nThe first part of the book provides a comprehensive introduction to MSA and its applications. You will learn about common enterprise system architectures, the concepts and value of MSA, and how it differs from traditional enterprise systems. In this part, you will gain an understanding of the design, deployment, and operation of MSA, including the basics of DevOps processes.\n\nThe second part of the book dives into ML and its applications in MSA systems. You will learn about the key ML algorithms and their applications in MSA, including regression models, multiclass classification, text analysis, and Deep Learning (DL). This part provides a comprehensive guide on how to develop the ML model, components, and sub-components, and how to apply them in an MSA system.\n\nThe final part of the book brings together everything covered in the previous parts. It provides a step- by-step guide to designing and developing an intelligent system, with hands-on examples and actual code that can be imported for real-life use cases. You will also learn about the application of DevOps in enterprise MSA systems, including organizational structure alignment, quality assurance testing, and change management.\n\nBy the end of this book, you will have a solid understanding of MSA and its benefits and will be equipped with the skills and knowledge necessary to build your own intelligent MSA system and take the first step toward achieving better business results, operational performance, and business continuity. Whether you are a beginner or an experienced developer, this book is the perfect guide to help you understand and apply MSA and ML in your enterprise systems.\n\nWho this book is for\n\nThis book is ideal for ML solution architects, system and ML developers, and system and solution integrators. These individuals will gain the most from this book as it covers the critical concepts and best practices in ML. The book is written to provide these professionals with the knowledge and skills necessary to implement intelligent MSA solutions.",
      "content_length": 2621,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "xiv\n\nPreface\n\nTo fully benefit from this book, you should have a basic understanding of system architecture and operations. Additionally, a working knowledge of the Python programming language is highly desired. This is because the examples and case studies included in the book are primarily implemented in Python. However, the concepts and best practices covered in this book can be applied to other programming languages and technologies as well. The book is designed to provide a solid foundation in ML, while also helping you to deepen your existing knowledge and skills.\n\nWhat this book covers\n\nChapter 1, Importance of MSA and Machine Learning in Enterprise Systems, provides an introduction to MSA and its role in delivering competitive and reliable enterprise systems. The chapter will compare MSA with traditional monolithic enterprise systems and discuss the benefits and challenges of deploying and operating MSA systems. It will also cover the key concepts of MSA, including service-driven and event-driven architecture and the importance of embracing DevOps in building MSA systems.\n\nChapter 2, Refactoring Your Monolith, focuses on the transition process from a monolithic architecture to an MSA. It emphasizes how to refactor the monolithic system to build a flexible and reliable MSA system. This chapter will explore the steps necessary to transition to MSA, including identifying microservices, breaking down business requirements, and decomposing functions and data. The chapter will provide insights into how to modernize an organization’s enterprise systems through MSA adoption.\n\nChapter 3, Solving Common MSA Enterprise System Challenges, discusses the methodologies of addressing the challenges of maintaining a reliable, durable, and smoothly operating MSA system. The chapter covers topics such as using an Anti-Corruption Layer (ACL) for MSA system isolation, API gateways, service catalogs and orchestrators, a microservices aggregator, and a microservices circuit breaker; the differences between gateways, orchestrators, and aggregators; and other MSA system enhancements.\n\nChapter 4, Key Machine Learning Algorithms and Concepts, provides a comprehensive understanding of the fundamental AI, ML, and DL concepts, to equip you with the necessary knowledge to build and deploy AI models in MSA systems. It covers the differences between these areas and provides an overview of common ML packages and libraries used in Python. The chapter then dives into various applications of ML, including building regression models, multiclass classification, text sentiment analysis and topic modeling, pattern analysis and forecasting, and building enhanced models using DL.\n\nChapter 5, Machine Learning System Design, provides a comprehensive understanding of the design considerations and components involved in building an ML pipeline and equips you with the knowledge necessary to build and deploy a robust and efficient ML system. The chapter covers the main concepts of fit and transform interfaces, train and serve interfaces, and orchestration.",
      "content_length": 3071,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Chapter 6, Stabilizing the Machine Learning System, arms you with a comprehensive understanding of the phenomenon of dataset shifts and how to address them in your ML systems to ensure stable and accurate results. The chapter discusses optimization methods that can be applied to address dataset shifts while maintaining their functional goals. The chapter covers details on the concepts of ML parameterization, the causes of dataset shifts, the methods for identifying dataset shifts, and the techniques for handling and stabilizing dataset shifts.\n\nChapter 7, How Machine Learning and Deep Learning Help in MSA Enterprise Systems, wraps up all the previous chapters by discussing the different use cases where you can apply ML and DL to your intelligent enterprise MSA system. You will learn some possible use cases, such as pattern analysis using a supervised linear regression model and self-healing using DL.\n\nChapter 8, The Role of DevOps in Building Intelligent MSA Systems, teaches you how to apply the concepts of DevOps in building and running an MSA system. The chapter covers the alignment of DevOps with the organizational structure, the DevOps process in enterprise MSA system operations, and the application of DevOps from the start to operations and maintenance.\n\nChapter 9, Building an MSA with Docker Containers, provides an introduction to containers and their use in building a simple project using Docker, a widely used platform in the field. The chapter covers an overview of containers and their purpose, the installation of Docker, the creation of our sample project’s containers, and inter-communication between microservices in the MSA project. The objective is to provide you with a comprehensive understanding of containers and how they can be utilized in the MSA.\n\nChapter 10, Building an Intelligent MSA System, combines the concepts of MSA and Artificial Intelligence (AI) to build a demo Intelligent-MSA system. The system will use various AI algorithms to enhance the performance and operations of the original MSA demo system created earlier in the book. The Intelligent-MSA will be able to detect potential problems in traffic patterns and self-rectify or self-adjust to prevent the problem from occurring. The chapter covers the advantages of using ML, building the first AI microservice, a demonstration of the Intelligent-MSA system in action, and the analysis of AI services’ operations. The goal is to provide you with a comprehensive understanding of how AI can be integrated into an MSA system to enhance its performance and operations.\n\nChapter 11, Managing the New System’s Deployment – Greenfield Versus Brownfield, introduces you to the deployment of Intelligent-MSA systems in greenfield and brownfield deployments. It provides ways to smoothly deploy the new system while maintaining overall system stability and business continuity. The chapter covers deployment strategies, the differences between greenfield and brownfield deployments, and ways to overcome deployment challenges, particularly in brownfield deployments where existing systems are already in production.\n\nPreface\n\nxv",
      "content_length": 3132,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "xvi\n\nPreface\n\nChapter 12, Deploying, Testing, and Operating an Intelligent MSA System, is the final chapter, and it integrates all the concepts covered in the book to provide hands-on and practical examples of deploying an intelligent MSA system. It teaches you how to apply the concepts learned throughout the book to your own deployment needs and criteria. The chapter assumes a brownfield environment with an existing monolithic architecture system and covers overcoming deployment dependencies, deploying the MSA system, testing and tuning the system, and conducting a post-deployment review.\n\nTo get the most out of this book\n\nTo maximize your learning experience, you should have a basic understanding of system architecture, software development concepts, DevOps, and database systems. Previous experience with MySQL and Python is not necessary, but it will help in understanding the concepts in the code examples more efficiently.\n\nSoftware/hardware covered in the book\n\nOperating system requirements\n\nDocker\n\nUbuntu Linux or macOS\n\nPython\n\nLinux, Windows, or macOS\n\nMySQL\n\nUbuntu Linux or macOS\n\nVirtualBox\n\nWindows or macOS\n\nWe recommend you install a Python IDE such as PyCharm to be able to follow the Python examples. PyCharm can be downloaded from https://www.jetbrains.com/lp/pycharm-anaconda/.\n\nVirtualBox is used to build the demo environment and create test virtual machines. VirtualBox can be downloaded from https://www.virtualbox.org/wiki/Downloads.\n\nUbuntu Linux is what we used in the book to install Docker and other utilities. To download the latest Ubuntu version, use the following link: https://ubuntu.com/desktop.\n\nIf you are using the digital version of this book, we advise you to type the code yourself or access the code from the book’s GitHub repository (a link is available in the next section). Doing so will help you avoid any potential errors related to the copying and pasting of code.\n\nDownload the example code files\n\nYou can download the example code files for this book from GitHub at https://github.com/ PacktPublishing/Machine-Learning-in-Microservices. If there’s an update to the code, it will be updated in the GitHub repository.\n\nWe also have other code bundles from our rich catalog of books and videos available at https:// github.com/PacktPublishing/. Check them out!",
      "content_length": 2319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Conventions used\n\nThere are a number of text conventions used throughout this book.\n\nCode in text: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. Here is an example: “Once you are done composing your Dockerfile, you will then need to save it as Dockerfile to be able to use it to create the Docker image.”\n\nA block of code is set as follows:\n\nimport torch model = torch.nn.Sequential( # create a single layer Neural Network torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1) ) loss = torch.nn.MSELoss(reduction='sum')\n\nWhen we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:\n\nimport numpy as np from scipy import linalg a = np.array([[1,4,2], [3,9,7], [8,5,6]]) print(linalg.det(a)) # calculate the matrix determinate 57.0\n\nAny command-line input or output is written as follows:\n\n$ docker --version Docker version 20.10.18, build b40c2f6\n\nBold: Indicates a new term, an important word, or words that you see onscreen. For instance, words in menus or dialog boxes appear in bold. Here is an example: “Select System info from the Administration panel.”\n\nTips or important notes Appear like this.\n\nPreface\n\nxvii",
      "content_length": 1270,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "xviii\n\nPreface\n\nGet in touch\n\nFeedback from our readers is always welcome.\n\nGeneral feedback: If you have questions about any aspect of this book, email us at customercare@ packtpub.com and mention the book title in the subject of your message.\n\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit www.packtpub.com/support/errata and fill in the form.\n\nPiracy: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at copyright@packt.com with a link to the material.\n\nIf you are interested in becoming an author: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit authors.packtpub.com.\n\nShare Your Thoughts\n\nOnce you’ve read Machine Learning in Microservices, we’d love to hear your thoughts! Please click here to go straight to the Amazon review page for this book and share your feedback.\n\nYour review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.",
      "content_length": 1282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Download a free PDF copy of this book\n\nThanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print books everywhere? Is your eBook purchase not compatible with the device of your choice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application.\n\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily\n\nFollow these simple steps to get the benefits:\n\n1. Scan the QR code or visit the link below\n\nhttps://packt.link/free-ebook/9781804617748\n\n2. Submit your proof of purchase\n\n3. That’s it! We’ll send your free PDF and other benefits to your email directly\n\nPreface\n\nxix",
      "content_length": 855,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "Part 1: Overview of Microservices Design and Architecture\n\nWe will start Part 1 by providing a comprehensive introduction to Microservices Architecture (MSA) and its application in enterprise systems. We will learn about common enterprise system architectures, the concepts and value of MSA, and how it differs from traditional enterprise systems. Throughout this part, we will gain an understanding of the design, deployment, and operations of an MSA, including the basics of DevOps processes.\n\nWe will come to understand more details on the use cases in which each enterprise architecture model is best used. Part 1 also examines how to design a basic modular, flexible, scalable, and robust MSA, including how to translate business requirements into microservices. We will acquire in-depth information about the different methodologies used for transitioning into an MSA and the pros and cons of each approach.\n\nThis part discusses the challenges of designing a true MSA and provides the tools and techniques for tackling each of these challenges. We will learn about the enterprise system components used for optimizing system modularity, testability, deployability, and operations.\n\nPart 1 is designed to provide readers with a solid understanding of MSA, its benefits, and how to implement MSA in their own enterprise systems.\n\nThis part comprises the following chapters:\n\nChapter 1, Importance of MSA and Machine Learning in Enterprise Systems\n\nChapter 2, Refactoring Monolith\n\nChapter 3, Solving Common MSA Enterprise System Challenges",
      "content_length": 1543,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "1 Importance of MSA and Machine Learning in Enterprise Systems\n\nIn today’s market, the competition has never been fiercer, and user requirements for IT systems are constantly increasing. To be able to keep up with customer requirements and market demands, the need for a shorter time-to-market (TTM) for IT systems has never been more important, all of which has pushed for agile deployment and the need to streamline the development process and leverage as much code reuse as possible.\n\nMicroservices architecture (MSA) addresses these concerns and tries to deliver a more competitive, reliable, and rapid deployment and update delivery while maintaining an efficient, stable system operation.\n\nIn this chapter, we will learn more details about how microservices help build a modern, flexible, scalable, and resilient enterprise system. The chapter will go over key concepts in MSA and discuss the common enterprise system architectures, how each architecture is different from MSA, why they are different, and what you gain or lose when you adopt one or more architectures over the others.\n\nWe will cover the following areas as we go over the chapter:\n\nWhat MSA is and why\n\nMSA versus monolithic enterprise systems\n\nService-driven architecture, event-driven architecture (EDA), and how to incorporate that in MSA\n\nChallenges of deploying and operating MSA enterprise systems\n\nWhy it is important to embrace DevOps in building MSA",
      "content_length": 1431,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "4\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nWhy microservices? Pros and cons\n\nMicroservices is often likened to MSA. MSA refers to the way in which a complex system is built from a collection of smaller applications, where each application is designed for a specific limited-scope function. These small applications (or services, or microservices) are independently developed and can be independently deployed.\n\nEach microservice has an API interface for communicating with other microservices in the system. The way all these individual microservices are organized together forms the larger system function.\n\nIn order to understand the value of microservices and the challenges one faces in designing an MSA, it is imperative to understand how microservices communicate and interact with each other.\n\nMicroservices can communicate together in a linear or non-linear fashion. In a linear microservices pipeline, each microservice communicates with another microservice, processing data across the system in a sequential manner. The input is always passed to the first microservice, and the output is always generated by the last microservice in the system:\n\nFigure 1.1: Linear microservices pipeline\n\nPractically, however, most existing systems are formed using a non-linear microservices pipeline. In a non-linear microservices pipeline, data is distributed across different functions in the system. You can pass the input to any function in the system, and the output can be generated from any function in the system. You can therefore have multiple pipelines with multiple inputs, serving multiple functions and producing multiple outputs:\n\nFigure 1.2: Non-linear microservices pipeline",
      "content_length": 1710,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "Why microservices? Pros and cons\n\nConsider the following diagram of a simplified order fulfillment process in a typical e-commerce system. Each function within the Placing an Order process represents a microservice. Once an order is placed by a customer, an API call is triggered to the Add/Update Customer Information microservice to save that customer’s information or update it if needed. This microservice sole responsibility is just that: manage customer information based on the data input it receives from the API caller.\n\nAnother API call is issued at the same time to the Verify Payment part of the process. The call will be directed to either the Process PayPal Payment or the Process Credit Card Payment microservice depending on the payment type of the API call. Notice here how the payment verification process is broken down into two different microservices—each is specifically designed and developed for a specific payment function. This enables the flexibility and portability of these microservices to other parts of the system or to another system if needed.\n\nAfter payment is processed, API calls are triggered simultaneously to other microservices in the system to fulfill the order:\n\nFigure 1.3: A non-linear microservices pipeline example – customer order\n\nThe order placement example shows how modular and flexible designing an MSA enterprise system can be. We will often use this example to show some of the advantages and challenges one may face when designing, deploying, and operating an MSA enterprise system.\n\nIt is essential that we go over some of the advantages and disadvantages of building enterprise systems using MSA to help decide whether MSA is a better option for your organization or not.\n\nNote that some of the advantages listed next could also be considered disadvantages in other situations (and vice versa).\n\n5",
      "content_length": 1855,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "6\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nAdvantages of microservices\n\nThere is some significant value to implementing MSA. The following are some of the advantages we see applicable to today’s market.\n\nAutonomy\n\nOne of the biggest advantages of microservices is their autonomy—it is the keystone for many of the other advantages of MSA. And because of their autonomy, microservices have their own technology stack, which means that each system service can be developed with completely different tools, libraries, frameworks, or programming languages than any other system service, yet they integrate with each other smoothly.\n\nMicroservices can be developed and tested independently of any other application within the system, which enables each microservice to have its own life cycle, including quality assurance (QA), change management, upgrades, updates, and so on, which in return greatly minimizes application dependencies.\n\nPortability\n\nMicroservices’ autonomy enables them to be portable across platforms, operating systems, and different systems, all independent of the coding language in which these services were written.\n\nReuse\n\nWhen reusing microservices, you don’t need to reinvent the wheel. Because of their autonomy, microservices can be reused without the need to add additional coding, changes, or testing. Each service can be reused as needed, which largely increases system flexibility and scalability, significantly reduces the development time, cost, and deployment time, and reduces the system’s TTM.\n\nLoosely coupled, highly modular, flexible, and scalable\n\nMicroservices form the main building blocks of an MSA enterprise system. Each block is loosely coupled with the other blocks in the system. Just like Lego blocks, the manner in which these blocks are organized together can form a complex enterprise MSA system building a specific business solution.\n\nThe following diagram shows an example of how we can build three different systems with multiple microservices.\n\nThe diagram shows nine services, and seven out of these services are organized in such a manner to reuse and build three different systems—system A, system B, and system C. This shows how loose coupling enables flexibility in MSA in such a way that you can reuse each service to build a different system function.\n\nYou can build a system with minimal development added to existing microservices either acquired by a third party or previously developed in house. This largely enables rapid system development, new feature releases, very short TTM, and reliable, flexible, and much more stable hot updates and",
      "content_length": 2627,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Why microservices? Pros and cons\n\nupgrades. All of this increases business continuity (BC) and makes the enterprise system much more scalable:\n\nFigure 1.4: Flexibility and modularity in microservices\n\nShorter release cycle and TTM\n\nBecause of the individual and independent services features we previously mentioned, the deployment of microservices becomes much easier and faster to perform. Automation can play a great role in reducing time-of-service testing and deployment, as we will discuss later in this chapter.\n\nFault tolerance and fault isolation\n\nEach microservice has its own separate fault domain. Failures in one microservice will be contained within that microservice, hence it is easier to troubleshoot and faster to fix and bring back the system to full operations.\n\nConsider the order fulfillment example we mentioned earlier; the system can still be functional if the Message/Email Customer microservice—for example—experiences any failures. And because of the nature of the failure and the small fault domain, it will be easy to pinpoint where that failure is and how to fix it. Mean Time to Resolution (MTTR) is therefore significantly reduced, and BC is greatly enhanced.\n\nArchitects are sometimes able to build the system with high embedded tolerance to prevent these failures to begin with or have other backup microservices on standby to take over once a failure is\n\n7",
      "content_length": 1392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "8\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\ndetected in the primary microservice. One of the primary objectives of this book, as we will see later, is to be able to design a system with high enough intelligence to provide the desired high resilience.\n\nWhat software architects have to bear in mind, however, is that, with too many system components in the MSA, too many things can go wrong. Architects and developers, therefore, have to have solid fallback and error handling to manage the system’s resilience.\n\nThe communication between the different microservices, for example, can simply time out for whatever reason; it could be a network issue, a server issue, or too many API calls at the receiving microservices or at the event-handling mechanism developed in the system, overwhelming this system component and causing failures or delayed response.\n\nThere are many data flow streams and data processing points in the system that all need to be synchronized. A single failure, if not taken care of properly by the system, can create system-cascading failures, and accordingly could cause a failure to the entire system.\n\nHow fault tolerance is designed will be a big factor in how system performance and reliability are impacted.\n\nReliability and the Single Responsibility Principle (SRP)\n\nIf you come from the programming world, you are probably familiar with the SRP in object-oriented programming (OOP): A class should have one, and only one, reason to change. Every object, class, or function in the system should have a responsibility over only that functionality of the system, and hence that class, once developed, should only change for the reason it was originally created for. This principle is one of the main drivers of increased system reliability and BC in MSA.\n\nAt the initial phases of developing an MSA enterprise system, and during the phase of developing new microservices from scratch, the MSA enterprise system may not be fully tested or fully matured yet, and reliability may still be building up. When the system matures, changes to individual microservices are minimal—if any— and microservices’ code reliability is, therefore, higher, the operation is more stable, fault domains are contained, fault tolerance is high, and the system’s reliability thus becomes much higher than similar systems with a monolithic architecture. Reliability is highly contingent on how well the system is designed, developed, and deployed.\n\nReducing system development and operational cost\n\nReusing microservices largely reduces the development efforts and time needed to bring the system to life. The more microservices you can reuse, the lower the development time and cost will become.\n\nMicroservices do not have to be developed from scratch; you can purchase already developed microservices that you may need to plug into your MSA enterprise system, cutting the development time significantly.\n\nWhen these microservices are stable and mature, reliability is higher, MTTR is much shorter, and hence system faults are lower and BC is higher. All these factors can play a major role in reducing the development cost, operational cost, and total cost of ownership (TCO).",
      "content_length": 3202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Why microservices? Pros and cons\n\nAutomation and operational orchestration are ideal for microservices; this enables agile development and can also decrease operational costs significantly.\n\nDisadvantages of microservices\n\nMicroservices come with a set of challenges that need to be taken into consideration before considering an MSA in your organization. The good news is that many of these challenges—if not all—can effectively be addressed to have in the end a robust MSA enterprise system.\n\nMentioned here are some of the challenges of microservices, and we will later in this chapter talk about some of the methodologies that help address these challenges.\n\nComplexity\n\nMSA systems contain many components that must work together and communicate together to form the overall solution. The system’s microservices in most cases are built with different frameworks, programming languages, and data structures.\n\nCommunication between microservices has to be in perfect synchronization for the system to properly function. Interface calls could at times overwhelm the microservice itself or the system as a whole, and therefore, system architects and developers have to continuously look for mechanisms to efficiently handle interface calls and try to eliminate dependencies as much as they can.\n\nDesigning the system to handle call loads, data flows, and data synchronization, along with the operational aspects of it, could be a very daunting process and creates layers of complexity that are hard to overlook.\n\nComplexity is one of the main trade-off factors in implementing and running an MSA enterprise system.\n\nInitial cost\n\nMSA systems usually require a large number of resources to be able to handle the individual processing needs of each microservice, the high level of communication between microservices, and the different development and staging environments for developing these microservices.\n\nIf these microservices are being developed from scratch, the initial cost of building an MSA system would therefore be too high. You have to account for the cost of the many individual development environments, the many microservices to develop and test, and the different teams to do all these tasks and integrate all these components. All this adds to the cost of the initial system development.\n\nTight API control\n\nEach microservice has its own API calls to be able to integrate with other microservices in the system. Any change in the API command reference set—such as updates in any API call arguments, deprecated APIs, or changes in the return values—may require a change in how other microservices handle the data flow from and to that updated microservice. This can pose a real challenge.\n\n9",
      "content_length": 2709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "10\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nDevelopers have to either maintain backward compatibility (which can be a big constraint at times) or change the API calls’ code of every other component in the system that interacts with the updated microservice.\n\nSystem architects and developers have therefore to maintain very tight control over API changes in order to maintain system stability.\n\nData structure control and consistency\n\nThe drawback of having independent applications within the enterprise system is that each microservice will have to maintain its own data structure, which creates a challenge in maintaining data consistency across your system.\n\nIf we take the earlier example of customer order fulfillment, the Add/Update Customer Information microservice should have its own database totally independent from any other database in the system. Similarly, the Update Item Inventory microservice should be the microservice responsible for the item information database, the Update Orders Database microservice should have the orders database, and so on.\n\nThe challenge now is that the shipping database will need to be in sync with the customer information database, and the orders database will have to contain some of the customer information. Also, the Message/Email Customer microservice has to have a way to access customer information (or receive customer information through API calls), and so on. In a larger system, the process of keeping data consistent across the different microservices becomes problematic. The more microservices we have, the more complex the data synchronization becomes.\n\nOnce again, designing and developing a system with all that work in mind becomes another burden on the system architects and developers.\n\nPerformance\n\nAs we mentioned earlier, microservices have to communicate with each other to perform the entire system function. This communication, data flows, error handling, and fault-tolerance design—among many other factors—are susceptible to network latency, network congestions, network errors, application data processing time, database processing time, and data synchronization issues. All these factors greatly impact system performance.\n\nPerformance is another major trade-off factor in adopting and running an MSA enterprise system.\n\nSecurity\n\nBecause of microservices’ autonomy and their loose coupling, a high number of data exchanges between the different services is necessary for the MSA to function. This data flow, data storage within each microservice, data processing, the API call itself, and transaction logging all significantly increase the system attack surface and develop considerable security concerns.",
      "content_length": 2709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Why microservices? Pros and cons\n\nOrganizational culture\n\nEach microservice in the MSA has its own development cycle and therefore has its silo of architects, developers, testers, and the entire development and release cycle teams, all to maintain the main objective of microservices: their autonomy.\n\nMSA enterprise systems are built from a large number of microservices and mechanisms to manage the interaction between the different system components. Developers have to therefore have system operational knowledge, and the operational teams need to have development knowledge.\n\nTesting such complex distributed environments that one will have in the MSA system becomes a very daunting process that needs a different set of expertise.\n\nThe traditional organizational structure of one big development team solely focused on development, one QA team only doing basic testing, and so on is no longer sufficient for the way MSA is structured and operated.\n\nAgile development and DevOps methodologies are very well suited for microservices development. You need agile processes to help maintain the fast development and release cycles MSA promises to deliver. You need DevOps teams who are very familiar with the end-to-end process of designing the application itself and how it fits in the big picture, testing the application, testing how it functions within the entire system, the release cycle, and how to monitor the application post release.\n\nAll this requires a cultural shift and significant organizational transformation that can enable DevOps and agile development.\n\nImportant note We rarely see a failure in MSA adoption because of technical limitations; rather, failure in adopting MSA is almost always due to a failure to shift the organization’s culture toward a true DevOps and agile culture.\n\nThe benefits outweigh the detriments\n\nThe main questions you need to answer now are: Is building an MSA worth it? Can we make it happen given the current organizational culture? How long will it take the organization to transform and be ready for MSA? Do we have the luxury of waiting? Can we do both the organizational transformation and the building of the MSA enterprise system at the same time? Do we have the resources and the caliber necessary for the new organizational structure? Is cost an issue, and do I have the budget to cover that?\n\nWell, first of all, if you are planning to build a large enterprise system, and you have the budget and necessary resources for starting this project, building the system as MSA is definitely worth it. All initial costs endured and time spent will eventually be offset by the long-term cost and time-saving benefits of having an MSA system.\n\n11",
      "content_length": 2697,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "12\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nNevertheless, you are still the one to best address all these previous questions. There are overwhelming and compelling advantages to adopting MSA, but as we have seen, this is not a simple undertaking; so, whether an organization is willing to walk that path or not is something it—and only it—can answer.\n\nNow we know what the advantages of deploying an MSA are, and the challenges that come with MSA adoption, we will now go over different enterprise architecture styles, what they are, and the differences between each other.\n\nLoosely versus tightly coupled monolithic systems\n\nTraditional applications back in the day were mostly built using a monolithic architecture, in which the entire application was one big code base. All system components and functions were tightly coupled together to deliver the business solution.\n\nAs shown in the following diagram, system functions are all part of the same code, tightly coupled with centralized governance. Each system function has to be developed within the same framework of the application.\n\nIn an MSA system, however, each function preserves its own anonymity—that is, loosely coupled with decentralized governance, giving each team the ability to work with its own preferred technology stack, with whichever tools, framework, and programming language it desires:\n\nFigure 1.5: Monolithic versus microservices systems",
      "content_length": 1437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Loosely versus tightly coupled monolithic systems\n\nAll functions in the monolithic architecture application are wrapped into the application itself. In the MSA, these functions are developed, packaged, and deployed separately. Therefore, we can run these services in multiple locations’ on-premises infrastructure, in the public cloud, or across both on-premises and the cloud in a hybrid-cloud fashion.\n\nIn monolithic systems, and because of the tight coupling, synchronizing the different system function changes is a development and operational nightmare. If one application (for whatever reason) becomes unstable, it could cause a failure to the entire system, and bringing the system back to a stable point becomes a real pain.\n\nIn the case of microservices, however, since each of these microservices is loosely coupled, changes and troubleshooting are limited to that particular microservice, as long as the microservice interface does not change.\n\nOne large piece of code, in the case of monolithic architecture, is very hard to manage and maintain. It is also hard to understand, especially in large organizations where multiple developers are working together.\n\nIn many cases such as employee turnover, for example, a developer may need to troubleshoot someone else’s code, and when the application is written in a single big piece of code, things tend to be complicated, hard to trace and understand, and hard to reverse engineer and fix. Code maintenance becomes a serious problem, while in the microservices case, this humongous line of code is broken into smaller chunks of code that are easier to read, understand, troubleshoot, and fix, totally independent of the other components of the system.\n\nWhen code changes are needed in monolithic architecture, a single change to part of the code may need changes to many other parts of the application, and accordingly, change updates will likely require a rewrite and a recompile of the entire application.\n\nWe can also reuse and package different applications together in a workflow to form a specific service, as shown previously in Figure 1.4.\n\nIt is just common sense to break down a complex application into multiple modules or microservices, each performing a specific function in the entire ecosystem for better scalability, higher portability, and more efficient development and operations.\n\nFor small, simple, and short-lived systems, monolithic applications may be a better fit for your organization, easier to design and deploy, cheaper to develop, and faster to release. As the business needs grow, MSA becomes a better long-term approach.\n\nSince monolithic systems are tightly coupled, there is no need for API communication between the different system functions; this significantly decreases the security surface of your system, lowering system security risks and increasing the system’s overall performance.\n\nThink of the deployment difference between both monolithic and MSA as the difference between an economy car and a Boeing 787. The car is a better, cheaper, and faster tool for traveling between two cities 50 miles apart, with no need for the security checks you experience in airports before boarding\n\n13",
      "content_length": 3190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "14\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nyour flight. As the distance increases, however, driving the car becomes more hassle. At 5,000 miles, the Boeing 787 is likely to become a better, cheaper, and faster way to get to your destination, and you will likely be willing to put up with the hassle of security checks you have to undergo to be able to board your flight.\n\nThe following is a comparison summary between both monolithic and microservices applications:\n\nMonolithic\n\nMSA\n\nArchitecture\n\nHighly autonomous. System functions are split into independent loosely coupled chunks of smaller code.\n\nNo autonomy. System functions are all tightly coupled into one big piece of code.\n\nPortability\n\nHighly portable\n\nVery limited portability\n\nReuse\n\nHighly reusable\n\nVery limited ability to reuse code\n\nModularity and Scalability\n\nHighly modular and scalable\n\nLimited modularity and hard to scale\n\nInitial TTM\n\nHighly dependent on the readiness of individual system services. The more code reuses, the shorter the TTM is.\n\nLong TTM, especially in large systems. Shorter TTM in small and simple systems.\n\nIf the system microservices are being designed and developed from scratch, TTM is usually longer for monolithic architecture.\n\nRelease Cycle\n\nVery short release cycle, super- fast to deploy changes and patch updates\n\nLong and usually very time- consuming release cycles and patch updates\n\nInitial Cost\n\nUsually high. Depends on the system size.\n\nUsually low. The initial size becomes higher in large enterprise systems.\n\nThe initial cost is offset by operational cost savings.\n\nOperational Cost\n\nLow. Easier to maintain and operate.\n\nHigh. Hard to maintain and operate.\n\nComplexity\n\nHigh\n\nLow\n\nAPI Control\n\nHigh\n\nLow",
      "content_length": 1741,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "Service-driven, EDA, and MSA hybrid model architecture\n\nMonolithic\n\nMSA\n\nData Structure Consistency\n\nDecentralized databases, hence data consistency is harder to maintain\n\nA centralized database, hence easier to maintain data consistency across the system\n\nPerformance\n\nUsually lower\n\nUsually higher\n\nSecurity\n\nMany security concerns\n\nLower security concerns\n\nOrganizational Adoption Hard to adopt depending on the organizational structure. Requires adoption of agile development and DevOps. Organizational transformation may be required and may take a long time to achieve.\n\nEasy to adopt. Minimal organizational transformation needed—if any.\n\nFault Tolerance\n\nUsually higher\n\nUsually lower\n\nTable 1.1: Summary of the differences between monolithic and MSA systems\n\nWe covered in this section the different aspects of a monolithic system; next, we go over service-driven architecture and EDA, and how to combine these architectural styles within MSA to address some of the MSA challenges discussed earlier.\n\nService-driven, EDA, and MSA hybrid model architecture\n\nPeople often get mixed up between MSA and service-driven architecture (aka service-oriented architecture or SOA). Both types of architecture try to break down the monolithic architecture system into smaller services. However, in MSA, the system services decomposition is extremely granular, breaking down the system into very fine specialized independent services. In the SOA, the system services decomposition is instead coarse-grained to the domain level.\n\nAll domains, as shown in the following diagram, share the same centralized database and may actually share other resources in between, creating some level of coupling and system dependencies that are non-existent in MSA. Data storage is a key difference between both architectural styles:\n\n15",
      "content_length": 1816,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "16\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nFigure 1.6: SOA architecture split into functional domains\n\nIn the case of the simplified MSA customer ordering example discussed earlier, there are eight different microservices. A similar implementation in SOA is likely to have all these microservices built together and tightly coupled in a single domain. Other domains within the system could be Cart Handling, Catalog Browsing and Suggestions, and so on.\n\nSOA has a holistic enterprise view, while in a microservice, development looks into the function itself in total isolation of the enterprise system in which the microservice is intended to be used.\n\nEDA is another architectural style that is largely adopted. While MSA’s main focus is on function and SOA emphasizes the domain, EDA instead focuses on system events.\n\nEDA is usually complemented by another main system architecture, such as SOA or MSA. In EDA, services are decoupled at a granularity level determined by its main architecture (MSA or SOA) and then communicate with each other through event-based transactions. In our order placement example, these events could be Order Created, Order Canceled, Order Shipped, and so on.\n\nIn order to maintain event synchronization and data consistency across the enterprise system, these events must be handled by a message broker. The message broker’s sole responsibility is to guarantee the delivery of these events to different services across the system. Therefore, it has to be highly available, highly responsive, fault-tolerant, and scalable and must be able to function under heavy load.\n\nWhen EDA is adopted within the MSA enterprise system, the message broker in that case will be handling events, API calls, and API calls’ responses.\n\nThe message broker has to be able to queue messages when a specific service is down or under heavy load and deliver that message whenever that service becomes available.",
      "content_length": 1942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Service-driven, EDA, and MSA hybrid model architecture\n\nACID transactions\n\nAny system with some form of data storage always needs to ensure the integrity, reliability, and consistency of that data. In MSA, systems store and consume data across the workflow transactions, and for individual services to ensure integrity and reliability for the MSA system as a whole, data stored within the entire system have to comply with a certain set of principles called Atomicity, Consistency, Isolation, and Durability (ACID):\n\nAtomicity: All-or-nothing transactions. Either all transactions in the workflow are successfully executed and committed or they all fail and are canceled.\n\nConsistency: Any data change in one service has to maintain its integrity across the system or be canceled.\n\nIsolation: Each data transaction has its own sovereignty and should not impact or be impacted by other transactions in the system.\n\nDurability: Committed transactions are forever permanent, even in the case of a system failure.\n\nSaga patterns\n\nOne of the main challenges in MSA is distributed transactions, where data flow spans across multiple microservices in the system. This flow of data across the services creates a risk of violating the microservice autonomy. Data has to be managed within the microservice itself in total isolation from any other service in the system.\n\nIf you look at our order placement example again, you find that customer data (or part of it) spans across the different microservices in the example, which could create undesired dependencies in the MSA, and should be avoided at all costs.\n\nWhat if, for whatever reason, the Update Item Inventory service fails, or it just happens that the service reports back that the item is no longer available? The system in that case will need to roll back and update all individual services’ databases to ensure ACID transactions for the workflow.\n\nThe saga pattern manages the entire workflow of transactions. It sees all sets of transactions performed in a specific process as a workflow and ensures that all these transactions in that workflow are either successfully executed and committed or rolled back in case the workflow breaks for whatever reason, to maintain data consistency across the system.\n\nA saga participant service would have a local transaction part of that workflow. A local transaction is a transaction performed within the service itself and produces an event upon execution to trigger the next local transaction in the workflow. These transactions must comply with ACID principles. If one of these local transactions fails, the saga service initiates a set of compensating transactions to roll back any changes caused by the already executed local transactions in the workflow.\n\n17",
      "content_length": 2757,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "18\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nEach local transaction should have corresponding compensating transactions to be executed to roll back actions caused by the local transaction, as shown in the following diagram:\n\nFigure 1.7: Processing of local and compensating transactions\n\nThere are two ways to coordinate transactions’ workflow in a saga service: choreography and orchestration.\n\nIn choreography, saga participant services exchange events without the need for a centralized manager. As in EDA, a message broker is needed to handle event exchanges between services, as illustrated in the following diagram:",
      "content_length": 642,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "Service-driven, EDA, and MSA hybrid model architecture\n\nFigure 1.8: Choreography in a saga service\n\nIn orchestration, a saga pattern-centralized controller is introduced: an orchestrator. The workflow is configured in the orchestrator and the orchestrator sends requests to each saga participant service on which local transaction it needs to execute, receives events from saga participant services, checks the status of each request, and handles any local transaction failures by executing the necessary compensating transactions, as illustrated in the following diagram:\n\n19",
      "content_length": 576,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "20\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nFigure 1.9: Orchestration in a saga service\n\nOrchestrators become the brain of the enterprise system and the single source for all steps that need to be taken to execute a specific system workflow. The orchestrator, therefore, must be implemented in a way to be highly resilient and highly available.\n\nCommand Query Responsibility Segregation (CQRS)\n\nIt is very common in traditional systems, and especially in monolithic applications, to have a common relational database deployed in the backend and accessed by a frontend application. That centralized database is accessed with Create-Read-Update-Delete (CRUD) operations.\n\nIn modern architecture, especially as the application scales, this traditional implementation poses a problem. With multiple CRUD requests being processed on the database, table joins are created with a high likelihood of database locking happening. Table locks introduce latency and resource competition, and greatly impact overall system performance.\n\nComplex queries have a large number of table joins and can lock the tables, preventing any write or update operations on them till the query is done and the database unlocks the tables. Database read operations are typically multiple times more than write operations, and in heavy transaction systems, the problem can multiply.",
      "content_length": 1373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "Service-driven, EDA, and MSA hybrid model architecture\n\nYou can see a comparison of CRUD and CQRS patterns here:\n\nFigure 1.10: CRUD versus CQRS patterns\n\n21",
      "content_length": 156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "22\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nWith CQRS, you simply separate one object into two objects. So, rather than doing both commands and queries on one object, we separate that object into two objects—one for the command, and one for the query. A command is an operation that changes the state of the object, while a query does not change the state of the system but instead returns a result.\n\nIn our case, the object here is the system database, and that database separation could be either physical or logical. Although it is a best practice to have two physical databases for CQRS, you can still use the same physical database for both commands and queries. You can, for example, split the database into two logical views—one for commands and one for queries.\n\nA replica is created from the master database when two physical databases are used in CQRS. The replica will, of course, need to be synchronized with the master for data consistency. The synchronization can be accomplished by implementing EDA where a message broker is handling all system events. The replica subscribes to the message broker, and whenever the master database publishes an event to the message broker, the replica database will synchronize that specific change.\n\nThere will be a delay between the exact time at which the master database was actually changed and when that change is reflected in the replica; the two databases are not 100% consistent during that period of time but will be eventually consistent. In CQRS, this synchronization is called eventual consistency synchronization.\n\nWhen applying CQRS design in MSA, database processing latency is greatly reduced, and hence communication between individual services’ performance is greatly enhanced, resulting in an overall system-enhanced performance.\n\nThe database used can be of any type, depending on the business case of that particular service in the MSA. It may very well be a relational database (RDB), document database, graph database, and so on. A NoSQL database could also be an excellent choice.\n\nWe discussed previously the MSA from a design and architecture perspective. Operating the MSA system is another aspect that the entire organization must consider for a successful business delivery process. In the next section, we discuss DevOps, how it fits into the MSA life cycle, and why it is important for a successful MSA adoption and operation.\n\nDevOps in MSA\n\nDevOps revolves around a set of operational guidelines in the software development and release cycles. The traditional development engineer is no longer living in their confined environment where all the focus is to convert functional specifications into code; rather, they should have an end-to-end awareness of the application.\n\nA DevOps engineer would oversee, understand, and be involved in the entire pipeline from the moment the entire application is planned out, converting business functions into code, building the application, testing it, releasing it, monitoring its operations, and coming back with the feedback necessary for enhancements and updates.",
      "content_length": 3109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "DevOps in MSA\n\nThat does not necessarily mean that a DevOps engineer would be responsible for all development and operational task details. Individual responsibilities within the application team may vary in a way to guarantee a smooth continuous integration and continuous deployment (CI/CD) pipeline of the application:\n\nFigure 1.11: DevOps CI/CD pipeline\n\nOne of the main objectives of DevOps is to speed up the CI/CD pipeline; that’s why there is a lot of emphasis on automation in DevOps. Automation is essential to efficiently perform the pipeline.\n\nAutomation can help at every step of the way. In DevOps, many test cases that are part of your QA plan are automated, which significantly speeds up the QA process. The release management and monitoring of your application are also automated to provide high visibility, continuous learning, and quick fixes whenever needed. All of this will help organizations improve productivity, predictability, and scalability.\n\nDevOps is a holistic view of how the application is developed and managed. It is not a function for only the development team or operational team to adopt; rather, the entire organization should adopt it. It is therefore imperative for the organizational structure and the organization’s vision and goal to all align with the set of procedural and functional changes necessary to shift from the traditional way of developing software.\n\nJust to give you a gist of how traditional and DevOps models differ in terms of application development and release cycles, take a look at the following comparison table:\n\n23",
      "content_length": 1581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "24\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nTraditional\n\nDevOps\n\nPlanning\n\nMonths\n\nDays to weeks\n\nLong time to plan due to the large application size and tight coupling between different application components\n\nVery short planning time since the application is broken down into small individual loosely coupled services\n\nDevelopment\n\nMonths\n\nDays to weeks, and even shorter in the case of patches and fixes\n\nTesting\n\nWeeks to months\n\nDays\n\nMostly manually intensive QA use case testing, which may sometimes jeopardize the reliability of the test’s outcome\n\nMostly automated QA use case execution that brings high reliability to the application\n\nRelease, Deploy\n\nDays\n\nHours\n\nUsually long manual work and more susceptible to human errors\n\nMostly automated\n\nOperate, Monitor Metrics reporting is mostly\n\nmanually pulled and analyzed\n\nMetrics are monitored and analyzed automatically and can even fix the problem in seconds. Moreover, machine learning (ML) tools can be used to enhance operations even further.\n\nTable 1.2: Traditional operational style versus DevOps\n\nIn traditional development environments, you have a big piece of code to write, maintain, and change when needed. Because of the code size, it is only normal to have a long release cycle, and it can only be feasible to deploy patches or new releases when only major changes or high-severity fixes are needed, as illustrated in the following diagram:",
      "content_length": 1436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "DevOps in MSA\n\nFigure 1.12: Traditional development environment versus MSA DevOps\n\nIn MSA, teams are separated based on applications that do not function. That big chunk of code is split into a collection of much smaller code (microservices), and since teams are split to work independently for each team to focus on a specialized microservice, the development and release cycles are much shorter.\n\nSimilarly, in DevOps, the application is broken down into smaller pieces to enable the CI/CD pipeline, which makes DevOps the perfect model that fits MSA.\n\nWhy ML?\n\nUsing ML tools and algorithms in your MSA enterprise system can further enhance and accelerate your DevOps CI/CD pipeline. With ML, you can find patterns in your tests, monitor phases of your pipeline, automatically analyze where the faults may be, and suggest a resolution or automatically fix operational issues whenever possible.\n\nML can greatly shorten your MSA enterprise system’s TTM and make it more intelligent, self-healing, resilient, and supportable.\n\nWe will in this book discuss two aspects of ML: first, we’ll explain in detail how to add CI/CD pipeline intelligence to your MSA enterprise system, and second, we’ll look at how to build an ML enterprise system with MSA in mind:\n\n25",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "26\n\nImportance of MSA and Machine Learning in Enterprise Systems\n\nFigure 1.13: Using ML in CI/CD pipeline\n\nSummary\n\nIn this chapter, we covered the concepts of MSA and how MSA is different from traditional monolithic architecture. By now, you should also have a clear understanding of the advantages of MSA and the challenges organizations may experience when adopting MSA.\n\nWe also covered the key concept of methodologies to consider when designing MSA, such as ACID, the saga pattern, and CQRS. All these concepts are essential to help overcome synchronization challenges and to maintain microservices anonymity.\n\nWe now understand the basics of DevOps and why it is important in MSA design, deployment, and operations, as well as how ML integration in MSA enterprise systems can help enhance system operations.\n\nIn the next chapter, we will go over common methodologies that organizations pursue to transition from running traditional monolithic systems to MSA systems. We will discuss how to break down the existing system into services that form the new MSA enterprise system.",
      "content_length": 1082,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "2 Refactoring Your Monolith\n\nNow we have decided that MSA is the right architectural style for our organization, what’s next?\n\nIn a recent report, 2022 APIs & Microservices Connectivity Report, published by Kong Inc., 75% of organizations have a lack of innovation and technology adoption.\n\nThe need for an IT system that quickly responds to customer and market needs has never been higher. Monolithic applications can no longer respond to high-paced market updates and needs. That’s one main reason for organizations to look to update their IT system, to stay in business.\n\nMSA is a primary enabler for a flexible and reliable enterprise system. Transitioning from a monolithic architecture into MSA is, therefore, becoming essential to modernizing an organization’s IT systems.\n\nWe will discuss, in this chapter, how to break up the business requirements of an existing running monolithic application in to microservices, and the steps necessary to transition toward MSA applications.\n\nWe will cover the following areas as we go over the chapter:\n\n\n\nIdentifying the system’s microservices\n\nThe ABC monolith\n\nFunction decomposition\n\nData decomposition\n\nRequest decomposition\n\nIdentifying the system’s microservices\n\nWhether it is a brownfield or greenfield enterprise system implementation, we still need to break up business requirements into basic functions as granularly as possible. This will later help us identify each microservice and successfully integrate it into our enterprise system.\n\nIn a brownfield system, business and system requirements have already been identified and implemented. They may, however, need to be revisited and updated according to new business criteria, changes, and requirements.",
      "content_length": 1715,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "28\n\nRefactoring Your Monolith\n\nThe objective of refactoring your application into simple services is to form highly granular functions that will eventually be built (or acquired) as microservices. You are very likely to add new functions to your new MSA in addition to some of the functions you will already extract from the monolithic system.\n\nWe, therefore, split the migration process into the following high-level steps:\n\n1. Define the to-be MSA system and the functions needed to build that MSA.\n\n2.\n\nIdentify what existing functions in the current monolithic system are to be reused in the new MSA and implemented as microservices.\n\n3.\n\nIdentify the delta between the existing functions to be reused and the functions needed to get to the to-be MSA system. These are the new functions to be implemented in the new MSA system.\n\n4. From the functions list identified in step 3, identify which functions will be developed as a microservice in-house, and the ones that can be acquired through third parties.\n\nDecomposing the monolith using a function-driven approach is a good starting point; nevertheless, using that approach alone is not enough. Since data stores are centralized in the monolith, data dependencies will still be a big concern in maintaining the microservices’ autonomy.\n\nThe interaction between the different functions in the monolith is another concern. We will need to look into how the function calls are being processed and handled, what data is being shared between these functions, and what data is being returned.\n\nExamining monolithic system functions, data, and function calls (requests) during the refactoring process is essential for maintaining the autonomy of microservices and achieving the desired level of granularity.\n\nImportant note Bear in mind that we must maintain the microservices autonomy principle during the entire monolith decomposition process. Too many microservices would cause a Nano-service anti-pattern effect, while too few would still leave your system with the same issues as a monolithic system.\n\nThe Nano-service anti-pattern creates too many expectations for most systems’ operations, which can in turn further complicate your MSA system and create a lack of stability, decreased reliability, and other system performance issues.\n\nImportant note As a general rule, apply the Common Closure Principle, where microservices that change for the same exact reason are better off packaged together in a single microservice.",
      "content_length": 2477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "The ABC monolith\n\nTo better explain the monolith transformation process to an MSA, in the following sections, we will design a simple hypothetical monolithic system, break up the system using the already mentioned three stages of system decomposition, build the different microservices, and then organize them together to build the MSA.\n\nThe ABC monolith\n\nABC is a simplified hypothetical product-ordering monolithic system built specifically to demonstrate the process and the steps needed in refactoring a monolithic application into an MSA. We will be using this ABC system throughout this book to demonstrate some examples of how to apply the concepts and methodologies.\n\nPlease note that we put the ABC-Monolith system together for demo purposes only and our aim here is not to discuss how the ABC-Monolith can be designed or structured better. We are more focused on the ABC-Monolith system refactoring process itself.\n\nIn the ABC-Monolith, the user can place an order from an existing product catalog and track the order’s shipping status. For simplicity, all sales are final, and products cannot be returned.\n\nThe system will be able to clear the order payment, assign a shipping courier to the order, and track all order and shipping updates.\n\nThe following diagram shows the high-level ABC-Monolith architecture. A user portal is used to add items to the cart, then send the order details to the ABC-Monolith. The ABC-Monolith has different tightly coupled functions with a centralized database, all to process the order from payment to delivery. The user is notified of all order and shipping updates throughout the order fulfillment process.\n\nFigure 2.1: The ABC-Monolith architecture\n\n29",
      "content_length": 1700,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "30\n\nRefactoring Your Monolith\n\nTo further understand the monolith, we will next go over the system As-Is state by discussing the existing monolith’s functions, the monolith database structure, and the workflow of the order placement process. We will close this section by comparing the As-Is to the To-Be state.\n\nThe ABC-Monolith’s current functions\n\nIt is imperative to start by understanding what current functions are implemented in the monolith and what their role is in the overall system. The following table lists the system functions we need to consider later in our system refactoring:\n\nFunction\n\nDescription\n\nplace_order()\n\nA function to create a record with all order information, and mark the order as “pending” awaiting the rest of the order placement process.\n\ncheck_inventory()\n\nTo check the availability of an item in the placed order.\n\nprocess_payment()\n\nVerify the payment of the total order amount. Will return an error code if the payment is not cleared.\n\nupdate_inventory()\n\nOnce an order is verified and the payment is successfully processed, the item inventory should be updated accordingly.\n\ncreate_order()\n\nThe order is now successfully processed; time to change the order status, and kick off the order preparation process (packing, etc.).\n\ncreate_shipping_request()\n\nStarts the order shipping request and notifies the courier with an available order for shipping.\n\norder_status_update()\n\nA function to update the order status with any changes such as preparing, shipping, exception, received, and so on.\n\nshipment_status_update()\n\nA function to update the shipping status with any changes such as, pending pickup, picked up, en route, exception, received, and so on.\n\nnotify_user()\n\nTo notify the user of any changes or updates to the placed order.\n\nregister_customer()\n\nA function that creates customer record information with a full name, address, phone, and other details.\n\nTable 2.1: The ABC-Monolith functions list\n\nIn the preceding table, we focused our description on the role of the function itself regardless of what the parameter passing is, or what the return values are.",
      "content_length": 2109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "The ABC monolith\n\nThe ABC-Monolith’s database\n\nAll the functions identified in the monolith share a centralized database. The following are the database tables being accessed by the functions:\n\nDatabase Table\n\nDescription\n\nCUSTOMER\n\nA table holding all customer information such as name, email, and phone.\n\nITEM\n\nThe product information is in the catalog. Product information includes product name, price, and stock quantity.\n\nORDER\n\nInformation on orders placed.\n\nORDER_ITEM\n\nORDER_STATUS\n\nSTATUS_CODE\n\nA many-to-many relationship normalization table between the ORDER and ITEM tables. The status of each placed order, with a reference to status_ code. Lookup table for order and shipment status codes.\n\nCOURIER\n\nShipping courier information, including courier name, contact, and so on.\n\nSHIPMENT_REQUEST SHIPMENT_REQUEST_STATUS The status of each shipment request, with a reference\n\nA list of all shipping requests for orders placed.\n\nto status_code. Table 2.2: The ABC-Monolith database tables list\n\nThe following is ABC’s Entity Relationship Diagram (ERD). Note that we needed to create the ORDER_ITEM normalization table to break up the many-to-many relationship between both the ORDER and ITEM tables:\n\nFigure 2.2: The ABC-Monolith ERD\n\n31",
      "content_length": 1245,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "32\n\nRefactoring Your Monolith\n\nKeep in mind that some of the monolith’s functions require full read/write access to specific tables with access to all fields in the table, while some other functions need only access to specific fields in the table. This information is important in system refactoring.\n\nIn the following section, we will go over the workflow to identify the ABC-Monolith As-Is state and determine how we can transition into the To-Be state. Along with the workflow information, the function database access requirements will help us refactor the monolith database into individual MSA databases for each microservice.\n\nThe ABC workflow and current function calls\n\nWe know so far what functions are used in the monolith and how the monolith’s database is structured. The next step is to examine the order placement workflow:\n\nFigure 2.3: The ABC-Monolith function requests/workflow\n\nAs shown in the preceding workflow diagram, the individual functions are all executed sequentially. Since it is all one tightly coupled system, there are no synchronization issues expected, and hence no orchestration is needed.\n\nAs we move toward the ABC-MSA, however, the decoupling of services creates the need to have a centralized point for managing the execution of these services in a specific sequence.\n\nShown in the following diagram are ABC’s As-Is and To-Be states. No centralized management in the As-Is state is needed; however, an orchestrator component is introduced in the To-Be state to manage the process flows between the services.",
      "content_length": 1546,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "Function decomposition\n\nEach of the individual services in the To-Be states has a dedicated database, as shown in the diagram. In the As-Is state, on the other hand, the database is centralized.\n\nFigure 2.4: The ABC As-Is and To-Be states\n\nNow that we know how our current ABC-Monolith is structured, and what both the as-is and to-be states are, it is time to start the ABC-Monolith refactoring process to transform into the ABC-MSA.\n\nWe will refactor the monolith in three stages. First, we will decompose the monolith functions and map these functions to microservices. Then, we will decompose the data to see how the individual databases will be designed. Finally, out of the monolith’s workflow, we will analyze the function requests, and build our MSA sagas from there.\n\nFunction decomposition\n\nThe first step in refactoring the ABC-monolith is to create the microservices based on the system functions we previously identified. This is a straightforward mapping between the existing functions and the microservices.\n\nThe key point here is that, by looking only at each function by itself without considering any function calls or data connections, you need to be as granular as possible in your function decomposition.\n\nAt first glance, the notify_user() function is doing too many things for a microservice, displaying a web user message status/update, notifying the user by email, and/or notifying the user by SMS. Each of these functions can have its own rules, design, issues, and concerns. Splitting the notify_user() function into three functions is a better approach from an MSA perspective to achieve the separation of concerns.\n\n33",
      "content_length": 1647,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "34\n\nRefactoring Your Monolith\n\nAccordingly, we split the notify_user() function into one function for handling web messages and notifications, one for handling email notifications, and one for SMS message notifications:\n\nweb_msg_notification()\n\nemail_notification()\n\nsms_notification()\n\nSimilarly, the process_payment() function can also be split into two different, more granular functions, one for handling direct credit card payments and one for handling PayPal payments:\n\nverify_cc_payment()\n\nverify_paypal_payment()\n\nThe following diagram shows how the ABC-Monolith is broken up so far. We haven’t yet looked into how the system’s functions are interacting with each other. The function interactions and the order fulfillment’s overall workflow will be handled at a later stage.\n\nFigure 2.5: The ABC-Monolith function decomposition\n\nAt this point, we are satisfied with the current level of granularity so far and we are ready to examine how the database tables are being accessed to see whether further decomposition is needed.",
      "content_length": 1033,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Data decomposition\n\nData decomposition\n\nDuring this stage, we need to look at how each function is accessing the database and what tables and even which parts of the database tables are accessed.\n\nThe following diagram shows what parts of the database the ABC-Monolith functions access. It is essential to know exactly which tables are accessed by which function and why. This will help us identify database dependencies, in order to later eliminate these dependencies and split the centralized ABC-Monolith database into separate data stores, each data store dedicated to each microservice.\n\nFigure 2.6: ABC-Monolith database access\n\n35",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "36\n\nRefactoring Your Monolith\n\nWe are still bound by the microservice autonomy rule. The challenging part in the diagram and this refactoring phase is the shared tables. Sharing a table between two microservices creates coupling that would clearly violate the autonomy rule. On the other hand, creating multiple copies of the table across different microservices will create serious data consistency issues. So, how do we solve this conundrum?\n\nRemember the saga patterns that we previously discussed in Chapter 1? Saga patterns should be able to solve data consistency issues that arise from having a transaction that spans multiple services. In our example here, we can have duplicates of the ORDER table, for example, across the place_order(), create_order(), and process_payment() services of the ABC-MSA system. A similar approach is taken for check_inventory(), update_inventory(), and so on.\n\nSo, with saga patterns in mind, let’s reexamine the ABC-Monolith database access shown in the preceding diagram, to build a new database access diagram for services in the ABC-MSA system.\n\nThere are two ways to coordinate the data transactions, choreography and orchestration. In choreography, the ABC-MSA saga participant services will have to coordinate data transactions among themselves. In orchestration, a centralized orchestrator performs the coordination process and handles all workflow transactions.\n\nCertainly, we can choose either coordination methodology, but in our example, we would argue that orchestration creates a better decoupling model over choreography. For that reason, and to keep our example simple, we will be using orchestration for our ABC-MSA saga patterns.\n\nThe following diagram shows the ABC-MSA service database access. As you can see in the diagram, there are a few database tables that have been copied across the system. We will, in the next section, use saga patterns to maintain data consistency across the copied tables.",
      "content_length": 1959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Data decomposition\n\nFigure 2.7: ABC-MSA database access\n\nWe notice in other services, such as web_msg_notification, email_notification, and sms_notification, that the database is identical for all three services. This is an indication that creating these three services off the original notify_user() function may not be a good idea anyway. You should only see small database access similarities between these different services, not a completely identical database. In a real scenario, we are better off combining these three services into only one service as it originally was.\n\nSimilarly, in a real-life scenario, the process_payment() function is likely to be mapped to a single service that includes clearing the payment overall, regardless of whether it is a credit card, PayPal, or any other form of payment. For demo purposes, we will split notify_user() and process_payment() into three and two different services respectively.\n\n37",
      "content_length": 940,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "38\n\nRefactoring Your Monolith\n\nSo far, we have been able to build the ABC-MSA’s microservices from the ABC-Monolith functions, identify data access in the monolith, and decompose the monolith into separate microservices, each with its own database. In the next section, we will focus more on how to ensure isolation and separation of concerns for the microservices by looking into how the service requests are orchestrated in the new ABC-MSA system.\n\nRequest decomposition\n\nThe ABC-Monolith function request flow has already been identified and shown in Figure 1.3. We will now see how this flow is going to work in the ABC-MSA.\n\nIn the ABC-MSA, the sagas are programmed and configured in the centralized orchestrator. The orchestrator will initiate separate API calls to each service in the saga, in either a synchronous or asynchronous fashion, depending on the defined workflow, and wait for a response from each API call to determine what other API call(s) to initiate next and how.\n\nThe following diagram shows how the workflow would be in the ABC-MSA. Please note that all API calls in our scenario are being initiated from the orchestrator. As you can see from the sequence number, there are some API calls initiated in parallel, and in some other cases, the orchestrator decides the next course of action based on the response it receives from a previously executed service.\n\nFigure 2.8: The ABC-MSA workflow",
      "content_length": 1416,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "Request decomposition\n\nThe user in the ABC-MSA workflow diagram initiates the order fulfillment process from a web interface, which will kick off the workflow from the orchestrator. Both the place_order and check_inventory services are launched at the same time by the orchestrator. place_order creates the order with all its information and marks its state as pending, waiting for the rest of the workflow to be processed.\n\nThe check_inventory service checks the inventory of items ordered and sends back a true or false response depending on whether the item is available or not. If any of the items ordered are not available, the web_msg_notification, email_notification, and sms_notification services are triggered.\n\nNow, here is the first challenge: all three notification services will require access to the CUSTOMER database in order to get the customer’s name, email address, phone number, and so on. But having one database for all three services creates undesired coupling that would violate the microservices autonomy principle. As we discussed earlier, we should instead create copies of that CUSTOMER database across all services to avoid service coupling. But how do we do that?\n\nFigure 2.9: Maintaining database consistency across MSA\n\n39",
      "content_length": 1253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "40\n\nRefactoring Your Monolith\n\nThe CUSTOMER database is mainly managed by the register_customer service, which is triggered by the orchestrator through the user interface. To be able to maintain data consistency, and as shown in Figure 1.9, the orchestrator will need to simultaneously issue the same transaction on all copies of the CUSTOMER database whenever a record is edited, created, or deleted.\n\nThe orchestrator will need to wait for a success confirmation from all four services, register_customer, email_notification, sms_ notification, and web_ msg_notification, before the workflow is finalized. Now, what if, let’s say, updating the sms_ notification CUSTOMER database fails? You will end up with data inconsistency, which can be a serious issue later on.\n\nThat’s why all saga participants’ local transactions will need to have a set of compensating transactions to ensure a rollback in case of any failures in executing the transaction. In our example, the orchestrator will need to undo updates to the CUSTOMER database for all the other services.\n\nThe following diagram shows how a failure to update the CUSTOMER database should be rolled back using saga patterns.\n\nFigure 2.10: Compensating transactions for registering new customer information and placing an order",
      "content_length": 1282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "Summary\n\nIn this chapter, we were able to go over the main steps of refactoring a monolith into an MSA, the steps necessary, the main things to consider, and the methodology of doing so. The simplified ABC-Monolith system was a good example; however, as systems get more complicated and the workflow gets more involved, data and process synchronization challenges start to arise.\n\nIn Chapter 1, we briefly discussed the challenges and the methodologies to be applied to overcome these challenges. In the next chapter, we will start applying the methodology to the ABC system we are trying to refactor.\n\nIn the next chapter, we will discuss how we can further maintain microservices’ autonomy and MSA stability and overcome some other operational challenges, and the role of API gateways, orchestrators, and microservice aggregators.\n\nSummary\n\n41",
      "content_length": 845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "3 Solving Common MSA Enterprise System Challenges\n\nIn the previous chapter, we learned how to decompose the monolith and refactor it into an MSA enterprise system. We built a simplified system as an example and then refactored the system to demonstrate this process. By doing so, we resolved some of the challenges of running a monolithic system. However, moving toward MSA introduces a completely different set of issues that need to be addressed.\n\nIn this chapter, we will discuss the main challenges introduced in MSA, how to address them, and what specific methodologies we need to apply to maintain the MSA system’s reliability, durability, and smooth operation.\n\nWe will cover the following topics in this chapter:\n\nMSA system isolation using an Anti-Corruption Layer (ACL)\n\nAPI gateways\n\nService catalogs and orchestrators\n\nMicroservices aggregators\n\nMicroservices circuit breaker\n\nGateways versus orchestrators versus aggregators\n\nABC-MSA enhancements\n\nMSA isolation using an ACL\n\nWhen adopting MSA in a brownfield, your migration from the monolithic system to MSA can either be done as a big bang migration or trickle migration.",
      "content_length": 1137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "44\n\nSolving Common MSA Enterprise System Challenges\n\nIn big bang migration, you keep the old monolith system running as-is while building the entire MSA system. Once the MSA system has been completed, tested, and deployed, you can then completely switch to the new MSA system during your organization’s maintenance window, then decommission the old monolith. This type of migration, although usable for some scenarios, is usually not recommended in the case of large enterprise systems.\n\nSwitching users from the old to the new system should be done during the corporate’s off-peak hours or the corporate’s standard migration window. And the sudden switch of users can be a complex and cumbersome process due to the high potential for downtime, potential rollbacks, and risks of unexpected results when applying real traffic to the new system, all of which can impose large time constraints during the migration window.\n\nA common and safer migration approach in our case is trickle migration, where you perform a gradual shift from the old monolithic system to the new MSA system. A common way of doing that is by gradually extracting functions, services, and/or modules out of your monolith and moving them into standalone microservices as part of your new MSA. Gradually, we phase out the existing monolith’s functions and build an MSA system piece by piece.\n\nTo successfully perform a tickle migration, you need what’s called an Anti-Corruption Layer (ACL), which will act as an intermediate layer, a buffer, and a gateway between the old, messy monolith and your new clean MSA. The ACL layer will help temporarily integrate and glue the new extracted services back into the old system, to be able to communicate with old services, databases, and modules without fouling your new MSA system. You can see the ACL architecture in the following diagram:\n\nFigure 3.1: Anti-Corruption Layer (ACL)\n\nThe ACL lifespan is as long as the monolith system’s lifespan. Once the migration has been completed and the monolith has been decommissioned, the ACL will no longer be needed. Therefore, it is recommended that you have the ACL written either as a standalone service or as part of the monolith.",
      "content_length": 2190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "MSA isolation using an ACL\n\nThe ACL has three main components:\n\nThe API component, which allows the ACL to communicate with the MSA system using the same language as the MSA system.\n\nThe ACL Facade, which is the interface that enables the ACL to talk to the monolith using the monolith’s language(s).\n\nThere are two options where the Facade can be placed; one is shown in Figure 3.1, where the Facade has been placed as part of the new standalone ACL microservice. The other option is placing the Facade as a component within the monolith, as shown in Figure 3.2:\n\nFigure 3.2: The Facade’s two implementation options\n\nThe choice would depend on the architects’ and developers’ preference regarding whether they would like to add more glue code inside the monolith itself, or completely isolate any development effort away from the monolith.\n\nThe ACL Adapter, which is a part of the ACL, works between the ACL’s northbound API and the Facade. The main function of the Adapter is to translate between the monolith and the MSA using the ACL Translator interface, as shown in Figure 3.1.\n\nACL is only needed when a trickle migration is adopted. There is no need for implementing an ACL in the big bang migration case. And since, between both migration styles, there are resources to be consumed, as well as advantages, risks, and tradeoffs, MSA project stakeholders will need to decide on which style is more suitable for the project and the organization.\n\nWhether an ACL is implemented or not, MSA systems would still need a component to act as an interface between the MSA system and external clients. Using an API gateway between the MSA and external API calls is considered a good MSA design practice. The next section discusses the roles of the API gateway in MSA systems, and the tradeoffs of having to adopt an API gateway in MSA design.\n\n45",
      "content_length": 1844,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "46\n\nSolving Common MSA Enterprise System Challenges\n\nUsing an API gateway\n\nAs we explained in Chapter 1, microservices can communicate directly with each other without the need for a centralized manager. As the MSA system becomes more mature, the number of microservices gradually increases, and direct communication between microservices can become a large overhead – especially with calls that need multiple round trips between the API consumer and the API provider.\n\nWith the microservices’ autonomy principle, each microservice can use its technology stack and may communicate with a different API contract than the other microservices in the same MSA system. One microservice, for example, may only understand a RESTful API with a JSON data structure, while others may only communicate with Thrift or Avro.\n\nMoreover, the location (IP and listening port) of the active instantiated microservices change dynamically within the MSA system. Therefore, the system will need to have a mechanism to identify the location at which the API consumer can point its calls towards.\n\nThere are also situations where you need to tie in your MSA system to legacy systems such as the mainframe, AS400, and more.\n\nAll of the previous situations require code to be embedded in each microservice in the MSA system. This code will help the microservices understand the legacy and non-REST communication patterns, discover the network location of other microservices in the system, and understand each microservice’s needs in general. Now, how independent and portable would such a set of microservices be?\n\nA better approach to addressing the preceding challenges is to use an API gateway where all system services talk to each other through that gateway. The API gateway receives API calls from the system’s API consumers, then maps the data received into a data structure and a protocol that API providers can understand and process:",
      "content_length": 1920,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "Using an API gateway\n\nFigure 3.3: Moving from services direct communication to API gateway communication\n\n47",
      "content_length": 108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "48\n\nSolving Common MSA Enterprise System Challenges\n\nWith the API gateway, we significantly reduce direct 1-to-1 communication between services. Moreover, we offload the system’s microservices from having multiple translations, mapping code, and Authentication-Authorization-Accounting (AAA) tasks. Rather, we move the responsibility of discovering the location of microservices from the client to the API gateway, which, in turn, further reduces the code overhead and renders microservices as light and independent as possible.\n\nImportant note The MSA system’s availability is as good as the API gateway’s availability. Therefore, it is necessary for the API gateway to be developed, deployed, and managed as a high-performance and highly available mission-critical service.\n\nThe API gateway can be deployed as a standalone service that’s part of the MSA system. The main functions of the gateway are as follows:\n\nMinimize the API calls between microservices, which makes MSA inter-service communication much more efficient.\n\nMinimize API dependencies and breaking changes. In an MSA system with no API gateway, if, for whatever reason, one of the API providers changes its API, a breaking change will likely happen. This means we will need to create a change in every microservice communicating with the API provider. By using an API gateway, a change in the API provider will be limited to the API gateway only, to match the API contract between the provider and the consumers.\n\nTranslate and map between API contracts, which offloads the microservices from embedding translation code in their core function.\n\nRun a service discovery mechanism and offload clients from running that function.\n\nAct as the entry point to the MSA system’s external client calls.\n\nLoad-balance API calls across the different instances of high-availability microservices, and offload microservices in high-traffic situations.\n\nOffer better security by throttling sudden increases in API calls during Distributed Denial of Service (DDoS) and similar attacks.\n\nAuthenticate and authorize users to access different components in the MSA system.\n\nProvide comprehensive analytics to provide deep insights into system metrics and logs, which can help further enhance system design and performance.",
      "content_length": 2272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "Service catalogs and orchestrators\n\nDespite all these advantages and functions of the API gateway, there are still some drawbacks to having an API gateway in your MSA system:\n\nThe most obvious is complexity. The more protocols and API contract data structures we have in the system, the more complex the gateway becomes.\n\nThe MSA’s operation is highly dependent on the API gateway’s performance and availability, which may create an unwanted system performance bottleneck.\n\n\n\nIntroducing an additional intermediary component such as an API gateway in the path of intra-microservice communication increases service response time. And with chatty services, the increased response time can become considerable.\n\nEven with all the functions the API gateway provides, we still need a way to map each user request to specific tasks that the MSA system would need to run to fulfill that request. In the next section, we’ll discuss how the MSA system tasks are mapped to specific user services, and how these tasks are orchestrated in MSA.\n\nService catalogs and orchestrators\n\nOrchestration is one of the most commonly used communication patterns in MSA systems. We briefly discussed this concept in Chapter 1. In this section, we will dive into more details about orchestration.\n\nDetermining the most appropriate communication pattern between the different microservices depends on many factors. Among the factors that will help determine whether choreography or orchestration is the most suited communication pattern for the system, you must consider the number of microservices you have in the system, the level of interactions between the different microservices, the business logic itself, how dynamic business requirements change, and how dynamic system updates are.\n\nOrchestrators act as the central managers controlling all communication between the system’s microservices. They usually interact with the users through a dashboard interface that contains all service catalogs. The Service Catalog is a set of services the MSA system offers to users. Each service in the catalog is linked to a set of workflows. Workflows are the actions the orchestrator will trigger and coordinate between the system’s microservices to deliver the service the user selected from the catalog:\n\n49",
      "content_length": 2279,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "50\n\nSolving Common MSA Enterprise System Challenges\n\nFigure 3.4: Orchestrators in MSA\n\nThe orchestrator’s functions can be extended beyond managing workflows. Orchestrators can also manage the entire life cycle of microservices; this involves provisioning and deploying microservices, configuring the microservice, and performing upgrades, updates, monitoring, performance audits, and shutdowns when needed.\n\nImportant note The orchestrator is the main brain of the MSA system, and it is imperative to have the orchestrator deployed and managed as a high-performing and mission-critical component of the MSA system.\n\nSome of the benefits of running an orchestrator in MSA include the following:\n\nYou have a centralized management platform as a single source of truth for all of your workflows. Thus, you can build complex workflows in a complex MSA without having to worry about how many microservices you have and how they can scale.\n\nAs in the API gateway, you can tie in your legacy systems or part of your old monolith and completely isolate your microservices from having to couple with any other system component. This saves a lot of effort having to build code into the independent microservices and tremendously helps in scaling your MSA.",
      "content_length": 1246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "Microservices aggregators\n\nMicroservices are visible to the orchestrator and hence can be completely managed, audited, and monitored by the orchestrator. This can produce very helpful and insightful analytics that can further enhance the MSA system’s supportability and operations.\n\nThe orchestrator’s visibility can help in troubleshooting any operational issues and identifying problems quickly.\n\nOrchestrators can automatically detect and self-resolute some of the operational problems. Orchestrators can, for example, detect resource starvation and reroute requests to a backup microservice. Orchestrators can automatically vertically or horizontally scale a particular microservice when a problem is detected. Orchestrators can also try to automatically restart the service if the service is not responding.\n\nThe orchestrator solves many of the MSA operational problems, including some of the data synchronization challenges. When scaling the system, however, data synchronization and data consistency become a big challenge for the orchestrator to address by itself. Microservices aggregators help address data synchronization issues when the MSA system scales. In the next section, we will discuss what the aggregator pattern is, what it is used for, and how it works.\n\nMicroservices aggregators\n\nIn Chapter 2, we had to copy some schemas across multiple microservices and use saga patterns through the orchestrator to keep data consistent and preserve the microservice’s autonomy. This solution may be viable in a situation if you have a limited number of microservices within the MSA system. In a large number of microservices systems, copying schemas across different microservices to maintain the synchronization of the individual microservices database doesn’t scale well and can severely impact the system’s overall performance.\n\nConsider an MSA system with 100 microservices and copy schemas across about 20 of those microservices to maintain the microservice’s autonomy. Each time any part of any of the schema’s data is updated, the orchestrator will have to sync those 20 schemas.\n\nMoreover, even if we have all 100 microservices perfectly autonomous, what if one of the user’s operations needs to gather information from those 20 microservices? The orchestrator will have to issue at least 20 different API calls to 20 different microservices to get the information the user is looking for. Not to mention that some of these 20 microservices may need to exchange multiple API calls to send the result back to the user.\n\nTo put things into perspective, let’s revisit the ABC-MSA system we built in Chapter 2. We have the order, product, and inventory microservices. The product microservice is for managing product information, the inventory microservice is for managing the product inventory, and the order microservice is for placing and managing orders.\n\n51",
      "content_length": 2877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "52\n\nSolving Common MSA Enterprise System Challenges\n\nLet’s assume we’re in a situation where a sales analyst is generating a report to check a product’s average quantity purchased per order and the product’s inventory level at the time at which the order was placed, as shown in the following diagram:\n\nFigure 3.5: A sample product order report\n\nThe orchestrator will have to send at least three API calls, one to each of the order, product, and inventory microservices, as shown in the following diagram:\n\nFigure 3.6: A user operation spanning multiple microservices",
      "content_length": 567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "Microservices aggregators\n\nTo minimize dependencies and response time, a better approach for this particular situation is to use an aggregator. This aggregator will collect the different pieces of data from all order, product, and inventory microservices and update its database with the combined information.\n\nThe API gateway or consumer will only need to send one API call to the aggregator to get all the information it needs. The number of API calls is minimized and the overall response time is greatly reduced, especially in cases where the information required is distributed across a large number of microservices:\n\nFigure 3.7: An aggregator communication pattern\n\nThe aggregator communication pattern reduces the number of API calls users could trigger in various operational requests and further enhances the data synchronization’s design and performance, as well as the overall system performance, especially in high-latency networks.\n\nNow, we know the roles of the API gateway, the orchestrator, and the aggregator. In the next section, we will discuss how all these three components interact with each other in the MSA.\n\n53",
      "content_length": 1136,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "54\n\nSolving Common MSA Enterprise System Challenges\n\nGateways versus orchestrators versus aggregators\n\nFrom what we have described so far, there are some overlapping functions between the API gateway, the orchestrator, and the aggregator. In this section, we will answer some of the fundamental questions regarding how all three components interact in a single MSA system:\n\nHow do these three MSA components work together?\n\nCan an API gateway perform the aggregator and orchestrator functions?\n\nWhat are the best practices for deploying all these communication patterns in our MSA?\n\nFirst of all, theoretically speaking, you can have clients interact with the MSA microservices directly without an API gateway. However, this would not be a good practice. By having no gateway in your MSA system, you would need to have most of the gateway functions implemented within each microservice you have in the system.\n\nTo keep microservices as light and autonomous as possible, it is highly recommended to have an API gateway in your MSA system. The API gateway will handle all ingress and egress API traffic from the different types of clients. Clients can be a web dashboard, a mobile application, a tablet, a third-party integration system, and so on.\n\nWhether you add an aggregator or not will highly depend on your business logic and system design. You will only need aggregators if you have client use cases where requests need to span across multiple microservices at the backend.\n\nAggregators can be implemented as part of the gateway itself; however, the best practice is to add an aggregator only whenever it is needed and make it an independent standalone microservice.\n\nAn MSA system can have multiple aggregators, each with specific business logic, and fulfilling a specific collection of data from a different set of microservices.\n\nSimilarly, orchestration patterns can also be implemented in the API gateway; however, you need the API gateway to focus on doing the main functions it was created for and leave orchestration tasks to the orchestrator. The orchestrator is also best deployed as a standalone microservice:",
      "content_length": 2126,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "Gateways versus orchestrators versus aggregators\n\nFigure 3.8: MSA high-level architecture\n\nThe preceding diagram shows the high-level architecture of having all these components working together within the MSA system. Clients always interact with the API gateway, and the API gateway will route the request to the appropriate service within the MSA system.\n\nClient API calls are routed to the appropriate microservice based on the API’s configuration. If the client request is something that is fulfilled by communicating with a single microservice, then the gateway will send that request directly to the microservice. API calls that span multiple microservices and are assigned to a specific aggregator in the system will be forwarded to that particular aggregator. Finally, for API requests that invoke specific workflows, the API gateway will forward those to the orchestrator.\n\nIn this section, we learned how the API gateway, the orchestrator, and the aggregator coexist in the same MSA system. In the next section, we will try to apply the concepts of all these three communication patterns to our previously developed ABC-MSA system.\n\n55",
      "content_length": 1145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "56\n\nSolving Common MSA Enterprise System Challenges\n\nMicroservices circuit breaker\n\nAnother challenge in MSA systems is the stability and assurance of workflow execution. Saga patterns, which we discussed in Chapter 1, are used to ensure that all transactions within a specific workflow are either all successfully executed, or all fail. But is that enough to ensure the reliable execution of microservices?\n\nLet’s consider a scenario where the called microservice is too slow in responding to API calls. Requests get successfully executed, but the microservice’s response times out. The microservice consumer, in turn, may assume an execution failure, and accordingly repeat the operation, which can be very problematic.\n\nAs shown in the following diagram, when a response timeout takes place in the Payment microservice, the Payment microservice will process the payment, but the microservice consumer will assume that the payment has not been processed and may automatically (or upon user request) retry the process. This behavior will cause the payment to be processed multiple times, resulting in multiple charges for the same order, or for the order to be placed multiple times:\n\nFigure 3.9: Payment microservice with too slow of a response time\n\nIn MSA, when microservices get instantiated, they start with limited resources and threads to avoid one particular microservice from hogging all the system’s resources.\n\nWith system resources in mind, consider another scenario, as shown in Figure 3.10, where the Inventory microservice is part of a service workflow and the Payment microservice is neither processing nor responding to API calls for whatever reason. In this case, both the Order and Payment microservices will keep waiting for confirmation from the inventory before they start releasing their resources.",
      "content_length": 1822,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "Microservices circuit breaker\n\nWith the Inventory microservice timing out requests, and under a system heavy load or high order volume, requests start to pile up for the Order and Payment microservices. Eventually, both the Order and Payment microservices start to run out of resources and become unable to respond to requests:\n\nFigure 3.10: Inventory microservice is down\n\nSimilar scenarios in MSA can result in a domino effect, causing a cascading failure to multiple microservices, which, in turn, causes an entire system failure.\n\nA microservice circuit breaker is used to prevent a system cascading failure from happening. A circuit breaker monitors microservice performance using real traffic metrics. It analyses parameters such as response time and successful response rate and then determines the health of the microservice in real time. Should the microservice become unhealthy, the circuit breaker immediately starts responding to the microservice consumers with an error.\n\nA circuit breaker does not prevent the microservice being monitored from failing; rather, it averts a cascading failure from taking place:\n\nFigure 3.11: The Inventory microservice with an inline circuit breaker\n\nWhen the circuit breaker assumes a microservice is unhealthy, it still needs to monitor and evaluate the microservice’s operational performance. The circuit breaker switches to a half-open state, where it only allows a small portion of requests to pass through to the microservice being monitored. Once the circuit breaker detects a healthy microservice response, the circuit breaker switches its state back to a closed state, where API traffic flows back normally to the microservice.\n\nCircuit breakers are not needed for every single microservice in the MSA system. We only need to deploy a circuit breaker on microservices that can cause a cascading failure. Architects will need to study and determine which microservices need circuit breaker protection.\n\n57",
      "content_length": 1959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "58\n\nSolving Common MSA Enterprise System Challenges\n\nA circuit breaker can be deployed as a standalone microservice or be part of the API gateway. Whether circuit breakers are implemented as standalone microservices or part of the API gateway highly depends on the system’s business and operational requirements, as well as the patterns adopted in the architecture itself.\n\nThe circuit breaker pattern, as well as the orchestrator, aggregator, ACL, and API gateway, are all enhancements architects that can be applied to the MSA system for better reliability, resilience, and overall performance. In the next section, we will learn how to apply each of the patterns discussed here to our ABC-MSA system.\n\nABC-MSA enhancements\n\nIn Chapter 2, we refactored our ABC-Monolith into a simple ABC-MSA. The ABC-MSA we designed in Chapter 2 lacked many of the enhancements we are considering in this chapter. It is time to take what we have learned in this chapter and apply that to the ABC-MSA system to enhance its design and operations.\n\nFirst of all, in the ABC-MSA from Chapter 2, the orchestrator was doing both the API gateway functions and the orchestration function. So far, we have learned that combining both the gateway and orchestration functions in one service is not the best option. Therefore, we will add to our ABC-MSA system an API gateway dedicated to ingress and egress API calls, and other API gateway functions we discussed earlier in this chapter, such as authentication, authorization, audit, monitoring, and so on.\n\nThe API gateway will run as a separate standalone microservice serving direct client requests, including the system dashboard and user frontend. The orchestrator will also run as a standalone microservice serving the MSA’s workflows.\n\nThe aggregator(s) will depend on the use cases where multiple ABC-MSA microservices are used to fulfill the user requests.\n\nA simple use case for using an aggregator would be a user checking for an order’s shipping status. The status should include the order information, the products included in the order, and the shipping status of that order.\n\nTo show all this information to the user, we will need to pull information from three different microservices: Order Management, Product Management, and Shipping Management. We will deploy an aggregator as a standalone microservice to pull the data from all these microservices and make them available for user API consumption:",
      "content_length": 2443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "ABC-MSA enhancements\n\nFigure 3.12: The enhanced ABC-MSA architecture\n\nIn the preceding diagram, we added the Management and Orchestration layer as part of the ABC-MSA system. This layer will manage the orchestration workflows and the microservice’s life cycle, including, installation, configuration, instantiation, updates, upgrades, and shutdowns.\n\nWe will also need an ACL to be active during the transition from the ABC-Monolith to the ABC-MSA. The ACL will act as a buffer between both systems to maintain the neatness of the architecture and its operations. Once all the ABC-Monolith functions have been redeployed into the ABC-MSA, both the old ABC-Monolith and the ACL can be decommissioned.\n\n59",
      "content_length": 703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "60\n\nSolving Common MSA Enterprise System Challenges\n\nSummary\n\nIn this chapter, we discussed the different components of the MSA that can be introduced to maintain the system’s stability and enhance its performance.\n\nWe discussed how to use the ACL to protect our new MSA during its transition from the old monolithic system. Then, we covered the roles and functions of the API gateway, the aggregator, and the orchestrator. We also covered some of the drawbacks you may experience when adopting the various communication patterns in MSA.\n\nFinally, we redesigned our ABC-MSA to showcase how these different components can all function together in a typical MSA.\n\nChapter 1 to Chapter 3 covered the basics of the MSA. In the next chapter, we will start discussing, with hands-on examples, key machine learning and deep learning algorithms used in MSA enterprise systems, and go over some programming and tool examples of building machine learning and deep learning algorithms.",
      "content_length": 974,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "Part 2: Overview of Machine Learning Algorithms and Applications\n\nIn this part, we will shift our focus to machine learning. We will learn about the different concepts of machine learning algorithms and how to design and build a machine learning system, maintain the model, and apply machine learning to an intelligent enterprise MSA.\n\nWe will first learn the fundamentals when it comes to identifying the difference between different machine learning models and their use cases. Once we’ve covered the basics, we will start to learn how to design a machine learning system pipeline. Once we have established a machine learning system pipeline, we will learn what data shifts are, how they can impact our system, and how we can identify and address them. Finally, having gone through all the basics, we will start to explore the different use cases for building our very own intelligent enterprise MSA.\n\nBy the end of Part 2, we will have a basic understanding of machine learning and the different algorithms, how to build and maintain a machine learning system, and, finally, the different use cases in which we can use machine learning for our intelligent enterprise MSA.\n\nThis part comprises the following chapters:\n\nChapter 4, Key Machine Learning Algorithms and Concepts\n\nChapter 5, Machine Learning System Design\n\nChapter 6, Stabilizing the Machine Learning System\n\nChapter 7, How Machine Learning and Deep Learning Help in MSA Enterprise Systems",
      "content_length": 1453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "4 Key Machine Learning Algorithms and Concepts\n\nIn the previous chapters, we explored the different concepts of MSA and the role it plays when creating enterprise systems.\n\nIn the coming chapters, we will begin to shift our focus from learning about MSA concepts to learning about key machine learning concepts. We will also learn about the different libraries and packages being used in machine learning models using Python.\n\nWe will cover the following areas in this chapter:\n\nThe differences between artificial intelligence, machine learning, and deep learning\n\nCommon deep learning packages and libraries used in Python\n\nBuilding regression models\n\nBuilding multiclass classification\n\nText sentiment analysis and topic modeling\n\nPattern analysis and forecasting using machine learning\n\nBuilding enhanced models using deep learning\n\nThe differences between artificial intelligence, machine learning, and deep learning\n\nDespite the recent rise in popularity of artificial intelligence and machine learning, the field of artificial intelligence has been around since the 1960s. With different sub-fields emerging, it is important to be able to differentiate between them and understand them and what they entail.",
      "content_length": 1213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "64\n\nKey Machine Learning Algorithms and Concepts\n\nTo start, artificial intelligence is the overarching field that encompasses all the sub-fields we see today, such as machine learning, deep learning, and more. Any system that perceives or receives information from its environment and carries out an action to maximize the reward or achieve its goal is considered to be an artificially intelligent machine.\n\nThis is commonly used today when it comes to robotics. Most of our machines are designed so that they can capture data using their sensors, such as cameras, sonars, or gyroscopes, and use the data captured to respond to a particular task most efficiently. This concept is very similar to how humans function. We use our senses to “capture” information from our environment and based on the information we receive, we carry out certain actions.\n\nArtificial intelligence is an expansive field, but it can be broken into different sub-fields, one we commonly know today as machine learning. What makes machine learning unique is that this field works on creating systems or machines that can continually learn and improve their model without explicitly being programmed.\n\nMachine learning does this by collecting data, also known as training data, and trying to find patterns in the data to make accurate predictions without being programmed to do so. There are many different methods used in machine learning to learn the data and the methods are tailored to the different problems we encounter.\n\nFigure 4.1: Different fields in artificial intelligence",
      "content_length": 1558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "The differences between artificial intelligence, machine learning, and deep learning\n\nMachine learning problems can be broken down into three different tasks: supervised learning, unsupervised learning, and reinforcement learning. For now, we will focus on supervised and unsupervised learning. This distinction is based on the training data that we have. Supervised learning is when we have the input data and the expected output for the particular set of data, which is also called the label. Unsupervised learning, on the other hand, only consists of the input without an expected output.\n\nSupervised learning works by understanding the relationship between the input and output data. One common example of supervised learning is predicting the price of a home in a certain city. We can collect data on existing homes by capturing their specifications and their current prices and then learn the pattern between the characteristics of these homes and their prices. We can then take a home, not in our training set, and test our model by inputting the features of the house into our program and have the model predict the price of the home.\n\nUnsupervised learning works by learning about the structure of the data either using grouping or clustering methods. This method is commonly used for marketing purposes. For example, a store wants to cluster its customers into different groups so that it can efficiently tailor its products to different demographics. It can capture the purchase history of its customers, use that data to learn about purchasing patterns, and suggest certain items or goods that would interest them, thus maximizing its revenue.\n\nBefore we can understand deep learning, which is a sub-field of machine learning, we must first understand what Artificial Neural Networks (ANNs) are. Taking inspiration from neurons in a brain, ANNs are models that comprise a network of fully connected nodes, also known as artificial neurons. They contain a set of inputs, hidden layers connecting the neurons, and also an output node. Each neuron has an input and output, which can be propagated throughout the network. In order to calculate the output of a neuron, we take the weighted sum of all the inputs, multiply it by the weight of the neuron, and then usually add a bias term.\n\nWe continue to perform these actions until we reach the last layer, which is the output neuron. We perform a nonlinear activation function, such as a sigmoid function, to give us the final prediction. We then take the predicted output value and input it in a cost function. This function tells us how well our network is learning. We take this value and backpropagate through our layers back to the first layer, adjusting the weights of the neurons depending on how our network is performing. With this, we can create strong models that can perform tasks such as handwriting recognition, game-playing AI, and much more.\n\nImportant Note A program is considered to be a machine learning model if it can take input data and learn the patterns to make predictions without being explicitly programmed to.\n\n65",
      "content_length": 3099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "66\n\nKey Machine Learning Algorithms and Concepts\n\nFigure 4.2: An ANN\n\nWhile ANNs are capable of performing many tasks, there are significant downsides that limit their use in today’s market:\n\n\n\nIt can be difficult to understand how the model performs. As you add more hidden layers to the network, it becomes complicated to try and debug the network.\n\nTraining the model takes a long time, especially with copious amounts of training data, and can drain hardware resources, as it is difficult to perform all these mathematical operations on a CPU.\n\nThe biggest issue with ANNs is overfitting. As we add more hidden layers, there is a point at which the weights assigned to the neurons will be heavily tailored to our training data. This makes our network perform very poorly when we try to test it with data it has not seen before.\n\nThis is where deep learning comes into play. Deep learning can be categorized by these key features:\n\nThe hierarchical composition of layers: Rather than having only fully connected layers in a network, we can create and combine multiple different layers, consisting of non-linear and linear transformations. These different layers play a role in extracting key features in the data that would be otherwise difficult to find in an ANN.\n\nEnd-to-end learning: The network starts with a method called feature extraction. It looks at the data and finds a way to group redundant information and identify the important features of the data. The network then uses these features to train and predict or classify using fully connected layers.",
      "content_length": 1567,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "Common deep learning and machine learning libraries used in Python\n\nA distributed representation of neurons: With feature extraction, the network can group neurons to encode a bigger feature of the data. Unlike in an ANN, no single neuron encodes everything. This allows the model to reduce the number of parameters it has to learn while still retaining the key elements in the data.\n\nDeep learning is prevalent in computer vision. Due to the advances in the technology of capturing photos and videos, it has become very difficult for ANNs to learn and perform well when it comes to image detection. For starters, when we use an image to train our model, we have to look at every pixel in an image as an input to the model. So, for an image of resolution 256x256, we would be looking at over 65,000 input parameters. Depending on the number of neurons in your fully connected layer, you could be looking at millions of parameters. With the sheer number of parameters, this will be bound to cause overfitting and could take days of training.\n\nWith deep learning, we can create a group of layers called Convolutional Neural Networks (CNNs). These layers are responsible for reducing the number of parameters that we have to learn in our model while still retaining the key features in our data. With these additions, we can learn how to extract certain features and use those to train our model to predict with efficiency and accuracy.\n\nFigure 4.3: A CNN\n\nIn the next section, we will be looking at the different Python libraries used for machine learning and deep learning and their different use cases.\n\nCommon deep learning and machine learning libraries used in Python\n\nNow that we have gone over the concepts of artificial intelligence and machine learning, we can start looking at the programming aspect of implementing these concepts. Many programming languages are used today when it comes to creating machine learning models. Commonly used are MATLAB, R, and Python. Among them, Python has grown to be the most popular programming language in machine learning due to its versatility as a programming language and the extensive number of libraries, which makes creating machine learning models easier. In this section, we will be going over the most commonly used libraries today.\n\n67",
      "content_length": 2290,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "68\n\nKey Machine Learning Algorithms and Concepts\n\nNumPy\n\nNumPy is an essential package when it comes to building machine learning models in Python. You will be mostly working with large, multi-dimensional matrices when building your models. Most of the effort is spent on transforming, splicing, and performing advanced mathematical operations on matrices, and NumPy provides the tools need to perform these actions while retaining speed and efficiency.\n\nFor more information on the different APIs that NumPy offers, you can visit the documentation on its website: https://numpy.org/doc/stable/reference/index.html.\n\nHere, we will look at the example code. This section shows us how we can initialize a NumPy array. In this example, we will create a 3x3 matrix with initialized values of 1 through 9:\n\nimport numpy as np # creates a 3x3 numpy array arr = np.array([[1,2,3],[4,5,6],[7, 8, 9]])\n\nHere, we will print out the results:\n\nprint(arr) [[1 2 3] [4 5 6] [7 8 9]]\n\nNow, we can show how we can splice and extract certain elements from our array.\n\nThis line of code allows us to pull all the values that are in the second column of our array. Keep in mind that in NumPy our arrays and lists are zero-indexed, meaning that the zero index refers to the first element in the array or list:\n\nprint(arr[:,1]) # print the second column of the array [2 5 8]\n\nIn this example, we extract all of the values in the row of 2 in our array:\n\nprint(arr[2,:]) # print the last row of the array [7 8 9]\n\nAnother useful aspect of NumPy arrays is that we can apply mathematical functions to our matrices without having to implement code to perform basic functions. Not only is this much easier but it also is much faster and more efficient.",
      "content_length": 1725,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "Common deep learning and machine learning libraries used in Python\n\nIn this example, we simply perform a multiplication between our matrix and a scalar value of -1:\n\nprint(np.multiply(arr, -1)) # multiplies every element in the array by -1 [[-1 -2 -3] [-4 -5 -6] [-7 -8 -9]]\n\nMatplotlib\n\nIn order to see how your model is learning and performing, it is important to be able to visualize your results and your data. Matplotlib offers a simple way to graph your data, from something as simple as a line plot to more advanced plots, such as contour plots and 3D plots. What makes this library so popular is its seamlessness when working with NumPy.\n\nFor more information on their different functions, you can visit their website: https://matplotlib. org/stable/index.html.\n\nIn this example, we will create a simple line graph. We first initialize two arrays, x and y, and both arrays will contain values from 0 to 9. Then, using Matplotlib’s APIs, we can plot and show our simple graph:\n\nimport matplotlib.pyplot as plt import numpy as np x = np.arange(10) # creates an array from 0-9 y = np.arange(10) plt.plot(x,y) plt.show()\n\nFigure 4.4: A simple line graph using Matplotlib\n\n69",
      "content_length": 1178,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "70\n\nKey Machine Learning Algorithms and Concepts\n\nPandas\n\nWith the recent trend of storing data in CSV files, Pandas has become a staple in the Python community due to its ease and versatility. Pandas is commonly used for data analysis. It stores the data in a tabular format, and it provides users with simple functions to pre-process and manipulate the data to fit their needs. It has also become useful when dealing with time-series data, which is helpful when building forecasting models.\n\nFor more information on the different functions, you can view the documentation on its website: https://pandas.pydata.org/docs/.\n\nIn this example, first, we will simply initialize a DataFrame. This is the data structure used to store our data in a two-dimensional tabular format in Pandas. Usually, we store the data from the files we read from, but it is also possible to create a DataFrame with your own data:\n\nimport pandas as pd data = { \"Number of Bedrooms\": [5, 4, 2, 3], \"Year Build\": [2019, 2017, 2010, 2015], \"Size(Sq ft.)\": [14560, 12487, 9882, 10110], \"Has Garage\": [\"Yes\", \"Yes\", \"No\", \"Yes\"], \"Price\": [305000, 275600, 175000, 235000], } df = pd.DataFrame(data) print(df)\n\nFigure 4.5: Output of our DataFrame\n\nAs with NumPy, we can extract certain columns and rows of our DataFrame. In this code, we can view the first row of our DataFrame:\n\nprint(df.iloc[0]) # view the first entry in the table",
      "content_length": 1402,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Common deep learning and machine learning libraries used in Python\n\nFigure 4.6: Output of the first row of our DataFrame\n\nWith Pandas, we can also extract certain columns from our DataFrame by using the name of the column rather than the index:\n\nprint(df[\"Price\"]) # print all the values in the Prices column\n\nFigure 4.7: Output of all the values in the Price column\n\nTensorFlow and Keras\n\nTensorFlow and Keras are the foundation when it comes to building deep learning models. While both can be used individually, Keras is used as an interface for the TensorFlow framework, allowing users to easily create powerful deep learning models.\n\nTensorFlow, created by Google, functions as the backend when creating machine learning models. It works by creating static data flow graphs that specify how the data moves through the deep learning pipeline. The graph contains nodes and edges, where the nodes represent mathematical operations. It passes this data using multidimensional arrays known as Tensors.\n\nKeras, later to be integrated with TensorFlow, can be viewed as the frontend for designing deep learning models. It was implemented to be user-friendly by allowing users to focus on designing their neural network models without having to deal with a complicated backend. It is similar to object-oriented programming, as it replicates the style of creating objects. Users can freely add different types of layers, activation functions, and more. They can even use prebuilt neural networks for easy training and testing.\n\nIn the following example code, we can see how we can create a simple, hidden two-layer neural network. This block of code allows us to initialize a Sequential model, which consists of a simple stack of layers:\n\nimport tensorflow as tf from tensorflow import keras\n\n71",
      "content_length": 1790,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "72\n\nKey Machine Learning Algorithms and Concepts\n\nfrom tensorflow.keras import layers from keras.models import Sequential model = Sequential() model.add(Flatten(input_shape=[256,256]))\n\nDepending on our application, we can add multiple layers with different configurations, such as the number of nodes, the activation functions, and the kernel regularizer:\n\n#Adding First Hidden Layer model.add(tf.keras.layers.Dense(units=6,kernel_ regularizer='l2',activation=\"leaky_relu\")) #Adding Second Hidden Layer model.add(tf.keras.layers.Dense(units=1,kernel_ regularizer='l2',activation=\"leaky_relu\")) #Adding Output Layer model.add(tf.keras.layers.Dense(units=1,kernel_ regularizer='l2',activation=\"sigmoid\"))\n\nFinally, we can compile our model, which essentially gathers all the different layers and combines them into one simple neural network:\n\n#Compiling ANN model.compile(optimizer='sgd',loss=\"binary_ crossentropy\",metrics=['accuracy'])\n\nPyTorch\n\nPyTorch is another machine learning framework created by Meta, formally known as Facebook. Much like Keras/TensorFlow, it allows the users to create machine learning models. The framework is well suited to Natural Language Processing (NLP) and computer vision problems but can be tailored to most applications. What makes PyTorch unique is its dynamic computational graph. It has a module called Autograd, which allows you to perform automatic differentiation dynamically, compared to TensorFlow, in which it is static. Also, PyTorch is more in line with the Python language, which makes it easier to understand and takes advantage of useful features of Python such as parallel programming. For more information, visit the documentation on their website: https://pytorch.org/docs/ stable/index.html.\n\nIn this section of code, we can create a simple single-layer neural network. Similar to Keras, we can initialize a Sequential model and add layers depending on our needs:\n\nimport torch model = torch.nn.Sequential( # create a single layer Neural",
      "content_length": 1992,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Common deep learning and machine learning libraries used in Python\n\nNetwork torch.nn.Linear(3, 1), torch.nn.Flatten(0, 1) ) loss = torch.nn.MSELoss(reduction='sum')\n\nSciPy\n\nThis library is designed for scientific computing. There are many built-in functions and methods used for linear algebra, optimization, and integration, which are commonly used in machine learning. This library is useful when trying to compute certain statistics and transformations as you build your machine learning model. For more information on the different functions it provides, view the documentation on its website: https://docs.scipy.org/doc/scipy/.\n\nIn this example code, we can create a 3x3 array using NumPy and then we can use SciPy to calculate the determinate:\n\nimport numpy as np from scipy import linalg a = np.array([[1,4,2], [3,9,7], [8,5,6]]) print(linalg.det(a)) # calculate the matrix determinate 57.0\n\nscikit-learn\n\nscikit-learn is a machine learning library that is an extension of SciPy and is built using NumPy and Matplotlib. It contains many prebuilt machine learning models, such as random forests, K-means, and support vector machines. For more information on the different APIs it provides, view the documentation by visiting its website: https://scikit-learn.org/stable/user_guide.html.\n\nIn the following example, we will use an example dataset provided by scikit-learn and build a simple logistic regression model. First, we import all the required libraries and then load the Iris dataset provided by scikit-learn. We can use a handy API from scikit-learn to split our data into training and test datasets:\n\nfrom sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import numpy as np # Load the iris dataset X, y = datasets.load_iris(return_X_y=True)\n\n73",
      "content_length": 1882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "74\n\nKey Machine Learning Algorithms and Concepts\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_ size=0.20, random_state=1) Create linear regression object\n\nWe can then initialize our logistic regression model and simply run the fit function with our training data to train the model. Once we train our model, we can use it to make predictions and then measure its accuracy:\n\n# Create Logistic Regression model model = LogisticRegression() # Train the model using the training sets model.fit(X_train, y_train) # Make predictions using the testing set y_pred = model.predict(X_test) print(accuracy_score(y_test, y_pred))\n\nIn the next few sections, we will start looking at the different models we can build using these libraries. We will understand what makes these models unique, how they are structured, and for what purposes and applications they can best serve our needs.\n\nBuilding regression models\n\nFirst, we will look at regression models. Regression models or regression analysis are modeling techniques used to find the relationship between independent and dependent variables. The output of a regression model is typically a continuous value, also known as a quantitative variable. Some common examples are predicting the price of a home based on its features or predicting the sales of a certain product in a new store based on previous sales information.\n\nBefore building a regression model, we must first understand the data and how it is structured. The majority of regression models involve supervised learning. This consists of features and an output variable, known as a label. This will help the model by adjusting the weights to better fit the data we have observed so far. We usually denote our features as X and our labels as Y to help us understand the mathematical models used to solve regression models:",
      "content_length": 1843,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "Building regression models\n\nFigure 4.8: Example of a supervised learning data structure\n\nTypically, our data is split into two subsets, training and testing sets. The training dataset usually consists of between 70-80% of the original data and the testing dataset contains the rest. This is to allow the model to learn on the training dataset and validate its result on the testing dataset to show its performance. From the results, we can infer how our model is performing on the dataset.\n\nFor a linear regression model to perform effectively, our data must be structured linearly. The model uses this formula to train on and learn about the data:\n\n𝑦𝑦𝑖𝑖 = (cid:3)𝛽𝛽0 + 𝛽𝛽1𝑥𝑥1 + ⋯+ 𝛽𝛽𝑛𝑛𝑥𝑥𝑛𝑛\n\nrepresents the output of the model, or what we usually call the prediction. The In this equation, . The slope, which is also referred prediction is calculated by taking the intercept . When working with to as the weight, is applied to all the features in the data, which represents 0 the data, we usually represent it as a matrix, which makes it easy to understand and easy to work with when using Python:\n\nand the slope\n\n𝑦𝑦 = 𝑋𝑋𝑋𝑋\n\n𝑦𝑦 =\n\n𝑦𝑦1 𝑦𝑦2 . . . 𝑦𝑦𝑛𝑛] [\n\n𝑋𝑋 = [\n\n𝑥𝑥11 ⋯ 𝑥𝑥1𝑐𝑐 ⋮ ⋮ ⋱ 𝑥𝑥𝑟𝑟1 ⋯ 𝑥𝑥𝑟𝑟𝑐𝑐\n\n]\n\n75",
      "content_length": 1202,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "76\n\nKey Machine Learning Algorithms and Concepts\n\n𝛽𝛽0 𝛽𝛽1 . . . 𝛽𝛽𝑛𝑛] [ The number of features describes what type of problem you are solving. If your data only has one feature, it is considered a simple linear regression model. While it can solve straightforward problems, for more advanced data and problems, it can be difficult to map relationships. Therefore, you can ). This allows the model to be create a multiple linear regression model by adding more features ( more robust and find deeper relationships.\n\n𝛽𝛽 =\n\nFigure 4.9: A simple linear regression model\n\nOnce we train our model, we need to learn how to evaluate our model and understand how it performs against the test data. When it comes to linear regression, the two common metrics we use to assess our model are the Root Mean Square Error (RMSE) and the R2 metrics.\n\nThe RMSE is the standard deviation of the residual errors across the predictions. The residual is the measure of the distance from the actual data points to the regression line. The further the average distance of all the points is from the line, the higher the error is. This indicates a weak model, as it’s unable to find the correlation between the data points. This metric can be calculated by using this formula where\n\nis the actual value,\n\nis the predicted value, and\n\nis the number of data points:\n\n𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅 = √\n\n𝑁𝑁 𝑖𝑖=1\n\n∑ (𝑦𝑦𝑖𝑖 − 𝑦𝑦𝑖𝑖̂) 𝑁𝑁\n\n2",
      "content_length": 1385,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "Building regression models\n\nFigure 4.10: Calculating the residual of a linear regression model\n\nR2, also known as the coefficient of determination, measures the proportion of variance in the dependent variables (Y) that can be explained by the independent variables (X). It essentially tells us how well the data fits the model. Unlike the RMSE, which can be an arbitrary number, R2 is given as a percentage, which can be easier to understand. The higher the percentage, the better the correlation of data. Although useful, a higher percentage is not always indicative of a strong model. What determines a good R2 value depends on the application and how the user understands the data. R2 can be calculated by using this formula:\n\n𝑁𝑁 𝑖𝑖=1 𝑁𝑁 𝑖𝑖=1 Many more metrics can evaluate the effectiveness of your regression model, but these two are more than enough to get an understanding of how your model is performing. When building and evaluating your model, it is important to plot and visualize your data and model, as this can identify key points. The plots can help you determine whether your model is overfitting or underfitting.\n\n2\n\n∑ (𝑦𝑦𝑖𝑖 − 𝑦𝑦𝑖𝑖̂) ∑ (𝑦𝑦𝑖𝑖 − 𝑦𝑦𝑖𝑖̂)\n\n2\n\n𝑅𝑅\n\n= 1 −\n\n2\n\nOverfitting occurs when your model is too suited to your training data. Your RMSE will be really low, and you will have a training accuracy of almost 100%. While this seems tempting, it is an indication of a poor model. This can be caused by one of two things: not enough data or too many parameters. As a result, when you test your model on new data it has not seen before, it will perform very poorly due to it not being able to generalize the data.\n\n77",
      "content_length": 1642,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "78\n\nKey Machine Learning Algorithms and Concepts\n\nFigure 4.11: An overfitted linear regression model\n\nTo address overfitting, you can try to increase the amount of training data, or make the model less complex. It also helps to randomly shuffle your data before you split it into the training and testing set. Another important technique is called regularization. While there are many different regularization techniques (L1 or L2 regularization) depending on the model, they all work similarly in that they add bias or noise into the model to prevent overfitting. In the regression equation we previously saw, we can add another term,\n\n, to show that regularization is being applied to our model:\n\n𝑦𝑦𝑖𝑖 = (cid:3)𝛽𝛽0 + 𝛽𝛽1𝑥𝑥1 + ⋯+ 𝛽𝛽𝑛𝑛𝑥𝑥𝑛𝑛 + 𝜖𝜖 On the other end, underfitting occurs when your model is unable to find any meaningful correlation within the data. This is not as common as overfitting since it is easy to find patterns in most data. If this occurs, either your data has too much noise and is severely uncorrelated, your model is too simple and doesn’t have enough parameters, or the model is not effective for the application at hand. It is also useful to debug your code and make sure there are no bugs when it comes to preprocessing your data or setting up your model:",
      "content_length": 1283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Building regression models\n\nFigure 4.12: An under-fitted linear regression model\n\nTherefore, the goal is to find the best-fitting model, between an overfit and an underfit. It takes time and experimentation to find a model that works for your needs, but using the key indicators and metrics discussed here can help guide you in the right direction:\n\nFigure 4.13: A best-fitting linear regression model\n\n79",
      "content_length": 405,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "80\n\nKey Machine Learning Algorithms and Concepts\n\nImportant Note Feature engineering is a critical part of building a comprehensive model. Understanding your data can help determine which features or parameters to include in your model so that you can capture the relationship between the independent and dependent variables without causing overfitting.\n\nThere are some key notes to keep in mind when collecting and working with the data for your model:\n\nNormalize your data: It is possible to have features with very high or low numbers, so to prevent them from overwhelming the model and creating biases, it is imperative to normalize all your data to make it uniform across the features.\n\nClean your data: In the real world, the data we collect isn’t always perfect and can contain missing or egregious data. It is important to deal with these issues because they can cause outliers and impact the model negatively.\n\nUnderstand the data: It is a common practice to perform statistical analysis, also known as Exploratory Data Analysis (EDA), on your data to get a better understanding of how the data can impact your model. This can include plotting graphs, running statical methods, and even using machine learning techniques to reduce the dimensionality of the data, which will be discussed later in the chapter.\n\nIn the next section, we will discuss classification models.\n\nBuilding multiclass classification\n\nUnlike regression models that produce a continuous output, models are considered classification models when they produce a finite output. Some examples include email spam detection, image classification, and speech recognition.\n\nClassification models are considered versatile since they can apply to both supervised and unsupervised learning while regression models are mostly used for supervised learning. There are some regression models (such as logistic regression and support vector machine) that are also considered classification models since they use a threshold to split the output of continuous values into different categories.\n\nUnsupervised learning is a common application used in today’s market. Although supervised learning usually performs better and provides meaningful results since we know the expected output, the majority of the data we collect is unlabeled. It costs companies time and money for human experts to sift through the data and label it. Unsupervised learning helps reduce the cost and time by getting the model to try and determine the labels for the data and extract meaningful information. They can even perform better than humans sometimes.",
      "content_length": 2593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "Building multiclass classification\n\nThe number of categories in the output of a classification model determines what type of model it is. For models with only two outputs (i.e., spam and not spam), this is called a binary classifier, while models with more than two outputs are called multiclass classifiers:\n\nFigure 4.14: Binary and multiclass classifiers\n\nFrom those classifiers, there are two types of learners: lazy learners and eager learners.\n\nLazy learners essentially store the training data and wait until they receive new test data. Once they get the test data, the model classifies the new data based on the already existing data. These types of learners take less time when training since you can continuously add new data without having to retrain the entire model, but take longer when performing classification since they have to go through all the data points. One common type of lazy learner is the K-Nearest Neighbors (KNN) algorithm.\n\nOn the other hand, eager learners work in the opposite way. Whenever new data is added to the model, they have to retrain the model again. Although this takes more time compared to lazy learners, querying the model is much faster since they don’t have to go through all the data points. Some examples of eager learners are decision trees, naïve Bayes, and ANNs.\n\nImportant Note Supervised learning will generally perform better than unsupervised learning since we know what the expected output should be during training, but it is costly to have to collect and label the data, so unsupervised learning excels in this area of training on unlabeled data.\n\nIn the next few sections, we will be looking at a few niche models that can be used for unique problems that most basic classification or regression models can’t solve.\n\n81",
      "content_length": 1780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "82\n\nKey Machine Learning Algorithms and Concepts\n\nText sentiment analysis and topic modeling\n\nA popular field in the machine learning field is topic modeling and text analysis. With a plethora of text on the internet, being able to understand that data and create complex models such as chatbots and translation services has become a hot topic. Interacting with human language using software is called NLP.\n\nDespite the amount of data we can use to train our models, it is a difficult task to create meaningful models. Language itself is complex and contains many grammar rules, especially when trying to translate between languages. Certain powerful techniques can help us when creating NLP models though.\n\nImportant Note Before implementing any NLP models, it is imperative to preprocess the data in some way. Documents and text tend to contain extraneous data, such as stopping words (the/a/and) or random characters, which can affect the model and produce flawed results.\n\nThe first idea we will discuss is topic modeling. This is the process of grouping text or words from documents into different topics or fields. This is useful when you have a document or text and want to classify and group it into a certain genre without having to go through the tedious process of reading documents one by one. There are many different models used for topic modeling:\n\nLatent Semantic Analysis (LSA)\n\nProbabilistic Latent Semantic Analysis (PLSA)\n\nLatent Dirichlet Allocation (LDA)\n\nWe will focus on LDA. LDA uses statistics to find patterns and repeated occurrences of words or phrases and groups them into their topics. It assumes that each document contains a mixture of topics and that each topic contains a mixture of words. LDA first starts the process of going through the documents and keeping a word matrix, where it contains the count of each word in each document:",
      "content_length": 1870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "Text sentiment analysis and topic modeling\n\nFigure 4.15: A word matrix\n\n, that we want to split up the words After creating a word matrix, we determine the number of topics, into and use statistics to find the probability of the words belonging to a certain topic. Using Bayesian statistics, we can then calculate the probability and use that to cluster the words into different topics:\n\nFigure 4.16: An LDA model\n\nAnother rising application in NLP is sentiment analysis. This involves the process of taking words or text and understanding the user’s intent or emotion. This is common today when dealing with online reviews or social media posts. It determines whether a piece of text contains positive, neutral, or negative emotions.\n\nMany different methods and models can solve this problem. The simplest approach is through statistics by using Bayes’ theorem. This formula is used for predictive analysis, as it uses previous words in a text to update the model. The probability can be calculated using this formula:\n\n𝑃𝑃(𝐴𝐴|𝐵𝐵) =\n\n𝑃𝑃(𝐵𝐵|𝐴𝐴)𝑃𝑃(𝐴𝐴) 𝑃𝑃(𝐵𝐵)\n\n83",
      "content_length": 1060,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "84\n\nKey Machine Learning Algorithms and Concepts\n\nDeep learning has become a powerful tool for NLP and can be useful for sentiment analysis. CNNs and Recurrent Neural Networks (RNNs) are two types of deep learning models that can drastically improve models for NLP, especially for sentiment analysis. We will discuss these neural networks and how they perform more later in this chapter.\n\nPattern analysis and forecasting in machine learning\n\nWith the uncertainty of time, being able to predict certain trends and patterns has become a hot topic in today’s industry. Most regression models, while powerful, are not able to make confident time predictions. As a result, some researchers have devised models that take time into consideration when making certain predictions, such as gas prices, stock market, and sales forecasting. Before we go into the different models, we must first understand the different concepts in time-series analysis.\n\nThe first step when dealing with time-series problems is familiarizing yourself with the data. The data usually contains one of four data components:\n\nTrend – The data follows an increasing or decreasing continuous timeline and there are no periodic changes\n\nSeasonality – The data changes in a set periodic timeline\n\nCyclical – The data changes but there is no set periodic timeline\n\nIrregular – The data changes randomly with no pattern",
      "content_length": 1382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Pattern analysis and forecasting in machine learning\n\nFigure 4.17: Different components of time-series data\n\nThese different trends can be split into two different data types:\n\nStationary – Certain attributes of the data, such as mean, variance, and covariance, do not change over time\n\nNon-stationary – Attributes of the data change over time\n\nOften, you will work with non-stationary data, and creating machine learning models using this type of data will generate unreliable results. To resolve this issue, we use certain techniques to change our data into stationary data. They include the following methods:\n\nDifferencing – A mathematical method used to normalize the mean and remove the variance. It can be calculated by using this formula:\n\n𝑦𝑦𝑡𝑡̂ = 𝑦𝑦𝑡𝑡 − 𝑦𝑦𝑡𝑡−1\n\n85",
      "content_length": 773,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "86\n\nKey Machine Learning Algorithms and Concepts\n\nTransformation – Mathematical methods are used to remove the change in variance. Among the transformations, these are three commonly used:\n\n Log transform\n\n Square root\n\n Power transform\n\nFigure 4.18: Differencing non-stationary data\n\nImportant Note Time is already uncertain, and this makes it almost impossible to create a model that can confidently predict future trends. The more we can remove uncertainty in our data, the better our model can find the relationships in our data.\n\nOnce we can transform our data, we can start looking at models to help us with forecasting. Of the different models, the most popular model for time-series analysis is an Auto-Regressive Integrated Moving Average (ARIMA) model. This linear regression model consists of three subcomponents:\n\nAuto-Regression (AR) – A regression model that uses the dependencies of the current time and previous time to make predictions\n\nIntegrated (I) – The process of differencing in order to make the data stationary\n\nMoving Average (MA) – Models between the expected data and the residual error by calculating the MA of the lagged observed data",
      "content_length": 1167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Enhancing models using deep learning\n\nAlong with ARIMA, other machine learning models can be used for time-series problems. Another well-known model is the RNN model. This is a type of deep learning model used for data that has some sort of sequence. We will be going into more detail on how they work in the next section.\n\nEnhancing models using deep learning\n\nEarlier in the chapter, we briefly discussed deep learning and the advantages it brings when enhancing simple machine learning models. In this section, we will go into more information on the different deep learning models.\n\nBefore we can build our model, we will briefly go over the structure of the deep learning models. A simple ANN model usually contains about two to three fully connected layers and is usually strong enough to model most complex linear functions, but as we add more layers, the improvement to the model significantly diminishes, and it is unable to perform more complex applications due to overfitting.\n\nDeep learning allows us to add multiple hidden layers to our ANN while reducing our time to train the model and increasing the performance. We can do this by adding one or both types of hidden layers common in deep learning – a CNN or RNN.\n\nA CNN is mostly applied in the image detection and video recognition field due to how the neural network is structured. The CNN architecture comprises the following key features:\n\nConvolutional layers\n\nActivation layers\n\nPooling layers\n\nThe convolutional layer is the core element in the CNN model. Its primary task is to convolve or group sections of the data using a kernel and produces an output called a feature map. This map contains all the key features extracted from the data, which can then be used for training in the fully connected layer. Each element in the feature map indicates a receptive field, which is used to denote which part of the input is used to map to the output. As you add more convolutional layers, you can extract more features, and this allows your model to adapt to more complex models:\n\n87",
      "content_length": 2052,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "88\n\nKey Machine Learning Algorithms and Concepts\n\nFigure 4.19: Convolutional layer output\n\nIn Figure 4.19, we can see how the feature map is created in the convolutional layer. The layer slides the kernel across the input data, performs the dot operation, and produces an output matrix, which is the result of the convolution function. The size of the kernel and the step size can dictate the output size of the feature map. In this example, we use a kernel size of 2x2 and a stride or step size of 2, which gives us a feature map of size 2x2 based on our input size. The output of the feature map can then be used for more future convolutional layers depending on the requirements of the user.\n\nBefore we use our newly created feature map as an input for another convolutional or fully connected layer, we pass the feature map through an activation layer. It is important to pass your data through some type of nonlinear function during the training process, as this allows your model to map to more complex functions. Many different types of activation functions can be used throughout your model and have their benefits depending on the type of model you are planning to build. Among the many activation functions, these are the most commonly used:\n\nRectified Linear Activation (ReLU)\n\nLogistic (sigmoid)\n\nThe Hyperbolic Tangent (Tanh)\n\nThe ReLU function is the most popular activation function used today. It is very simple and helps the model learn and converge more quickly than most other activation functions. It is calculated using this function:\n\n𝑓𝑓(𝑥𝑥) = {\n\n0,(cid:3)(cid:3)𝑥𝑥 < 0 𝑥𝑥,(cid:3)(cid:3)𝑥𝑥 ≥ 0",
      "content_length": 1615,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "Enhancing models using deep learning\n\nFigure 4.20: ReLU function\n\nThe logistic function is another commonly used activation function. This is the same function used in logistical regression models. This function helps bound the output of the feature map between 0 and 1. While useful, this function is computationally heavy and may slow down the training process. It is calculated using this function:\n\n𝑓𝑓(𝑥𝑥) = (cid:3)\n\n1 1 + 𝑒𝑒\n\n−𝑥𝑥\n\nFigure 4.21: Sigmoid function\n\nThe Tanh function is similar to the sigmoid function in that it bounds the values from the feature map. Rather than bounding it from 0 to 1, it bounds the values from -1 to 1, and it usually performs better than a sigmoid function. The function is calculated using this formula:\n\n89",
      "content_length": 749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "90\n\nKey Machine Learning Algorithms and Concepts\n\n𝑥𝑥\n\n−𝑥𝑥\n\n𝑓𝑓(𝑥𝑥) = (cid:3)\n\n𝑒𝑒 𝑒𝑒\n\n𝑥𝑥\n\n− 𝑒𝑒 + 𝑒𝑒\n\n−𝑥𝑥\n\nFigure 4.22: Tanh function\n\nEach activation has its uses and benefits depending on the task or model at hand. The ReLU function is commonly used in CNN models while sigmoid and Tanh are mostly found in RNN models, but they can be used interchangeably and bring different results.\n\nAfter we run our feature map through an activation layer, we come to the final piece – the pooling layer. As mentioned before, a key element in deep learning is the reduction of parameters. This allows the model to train on fewer parameters while still retaining the important features extracted from our convolutional layers. The pooling layer is responsible for this step of downsizing our parameter size. There are many common pooling functions but the most commonly used is max pooling. This is similar to the convolutional layer, where we use a kernel or filter to slide through our input data and only take the maximum value from each window:",
      "content_length": 1033,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Enhancing models using deep learning\n\nFigure 4.23: Max pooling layer output\n\nIn Figure 4.23, we are using a kernel size of 2x2 with a stride size of 2. Here, we can see our output where only the maximum value from each window is selected.\n\nOther layers and functions can be added to the model to help address certain issues and applications, such as a batch normalization layer, but with these three foundational layers, we can add multiple layers of different sizes and still build a powerful model.\n\nIn the final layer, we feed our output into the fully connected layer. By that time, we are able to extract the important features of the data and still learn more complex models with less time to train as compared to a simple ANN model.\n\nNext, we will go over the RNN architecture. Due to the nature of how the RNN model is structured, it is designed for tasks that need to take into consideration a set of sequence data, in which the data later in the sequence is dependent on earlier data. This model is commonly used for certain fields such as NLP and signal processing.\n\nThe basics of an RNN are built by having a hidden layer in which the output of the layer is fed back into the same hidden layer. This way, the model is able to learn based on previous data and adjust the weights accordingly.\n\n91",
      "content_length": 1306,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "92\n\nKey Machine Learning Algorithms and Concepts\n\nFigure 4.24: RNN architecture\n\nTo better understand how the model works, you can envision a single layer for each data point. Each . We then transfer the weights layer takes in the data point as an input between the layers and then take the total average of all the cost functions from all the layers in the model.\n\nand produces an output\n\nFigure 4.25: An unraveled RNN",
      "content_length": 419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "Enhancing models using deep learning\n\nThis model works for most simple problems. As the sequence increases, it encounters a few issues:\n\nVanishing gradient: This occurs during the training process when the gradient approaches zero. The weights aren’t updated properly as a result and the model performs poorly.\n\nLack of context: The model is unidirectional and cannot look further or previously into the data. Therefore, the model is only able to predict based on data around the current sequence point and is more likely to make a poor prediction based on incorrect context.\n\nThere are different variations of RNNs created to address some of the issues mentioned here. Among them, the most common one used today is the Long Short-Term Memory (LSTM) model. The LSTM model comprise three components:\n\nAn input gate\n\nAn output gate\n\nA forget gate\n\nFigure 4.26: An LSTM neural network\n\nThese gates work by regulating which data points are needed to contextualize the sequence. That way, the model can predict more accurately without being easily manipulated.\n\n93",
      "content_length": 1059,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "94\n\nKey Machine Learning Algorithms and Concepts\n\nThe forget gate is specifically responsible for removing previous data or context that is no longer needed. This gate uses the sigmoid function to determine whether it uses or “forgets” the data.\n\nThe input gate is used to determine whether the new data is relevant to the current sequence or not. This is so that only important data is being used to train the model and not redundant or irrelevant information.\n\nLastly, the output gate’s primary function is to filter the current state’s information and only send relevant information to the next state. As with the other gates, it uses the context from previous states to apply a filter, which helps the model properly contextualize the data.\n\nCNN and RNN models are mostly designed for supervised learning problems. When it comes to unsupervised learning, different models are needed to solve certain problems. Let’s discuss autoencoders.\n\nAutoencoders work by taking the input data, compressing it, and then reconstructing it by decompressing it. While straightforward, it can be used for some advanced applications, such as generating audio or images, or it can be used as an anomaly detector.\n\nThe autoencoder comprises two parts:\n\nAn encoder\n\nA decoder\n\nFigure 4.27: The components of an autoencoder\n\nThe encoder and decoder are usually built with a one-layer ANN. The encoder is responsible for taking the data and compressing or flattening the data. Then, the decoder works on taking the flattened data and trying to reconstruct the input data.",
      "content_length": 1553,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "The hidden layer in the middle of the encode and decoder is usually referred to as the bottleneck. The number of nodes in the hidden layer must be less than those in the encoder and decoder. This forces the model to try and find the pattern or representation in the input data so that it can reconstruct the data with little information. Thus, the cost function is there to calculate and minimize the difference between the input and output data.\n\nOne aspect of autoencoders that is an integral part of deep learning is dimensionality reduction. This is the process of reducing the number of parameters or features used when training your model. As mentioned earlier in this chapter, to build a complex model that can build a deeper representation of the data, it is important to include more features. However, adding too many features can lead to overfitting, so how do we find the best number of features to use in our model?\n\nThere are many models and techniques, such as autoencoders, that can perform dimensionality reduction to help us find the best features to use in our model. Among the different techniques, Principle Component Analysis (PCA) is the most popular. This technique can take an N-dimensional dataset and reduce the number of dimensions in the data using linear algebra. It is a common practice to use a dimensionality reduction technique before using your data to train your model, as this can help to remove noise in the data and avoid overfitting.\n\nSummary\n\nIn this chapter, we discussed what is considered artificial intelligence and the different sub-fields that it contains.\n\nWe discussed the different characteristics of regression and classification models. From there, we also went over the structure of our data and how the model performs when training over our data. We then discussed the different ways of analyzing our model’s performance and how to address the different issues that we can come across when training our model.\n\nWe briefly viewed the different packages and libraries that are used today in machine learning models and their different use cases.\n\nWe also analyzed different topics such as topic modeling and time-series analysis and what they entail. With that, we were able to look at the different methods and techniques used to solve those types of problems.\n\nLastly, we went into deep learning and the different ways it improves on machine learning. We went over the two different types of neural networks – CNNs and RNNs – how they are structured, and their benefits and use cases.\n\nIn the next chapter, we will take what we have learned and start looking into how we can design and build an end-to-end machine learning system and the different components that it contains.\n\nSummary\n\n95",
      "content_length": 2743,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "5 Machine Learning System Design\n\nIn the last chapter, we delved into the different machine learning concepts and the packages and libraries used to create these models. Using that information, we will begin to discuss the design process when building a machine learning pipeline and the different components found in most machine learning pipelines.\n\nWe will cover the following areas in this chapter:\n\nMachine learning system components\n\nFit and transform interfaces\n\nTrain and serve interfaces\n\nOrchestration\n\nMachine learning system components\n\nThere are many moving parts required in order to build a robust machine learning system. Starting from gathering data to deploying your model to the user, each plays a vital role in keeping the system dynamic and scalable. Here, we will briefly discuss the different stages in the machine learning system life cycle and the role they play. These stages can be edited in order to suit the model or application at hand.\n\nThe majority of machine learning systems include the following stages, with some other stages depending on business needs:\n\nData collection\n\nDate preprocessing\n\nModel training",
      "content_length": 1143,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "98\n\nMachine Learning System Design\n\nModel testing\n\nModel serving\n\nRealistically, the majority of the time spent building machine learning systems is spent on the data. This is a key element in the process that can decide the effectiveness of your system since the model is dependent on the data it uses during training. Just like the human body, if you feed the model poor data or not enough data, it will output poor results.\n\nThe first part when it comes to data is the collection process. Understanding the application and the goal of the task can assist in the process of deciding how to collect data and what data to collect. We then determine the target value that we want to predict, such as the price of a home or the presence of a certain disease. These target values can be collected explicitly or implicitly. A target variable is explicit when we can directly determine the value of the variable we are trying to capture, while an implicit target value is found by using contextual data to determine the target value.\n\nDepending on the task, we usually store the data in a database (for either metadata or tabular data) such as MySQL or cloud storage (for images, video, or audio) such as Amazon S3:\n\nFigure 5.1: Data collection",
      "content_length": 1239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Machine learning system components\n\nOnce we set up continuous data collection, we must devise a procedure for cleaning and processing the data. Not everything we collect will be perfect. You will always find missing data and certain outliers, which can negatively impact our model. No matter how intuitive your model is, it will always perform poorly with garbage data.\n\nSome practices to deal with unclean data include removing outliers, normalizing certain features, or imputing missing data depending on the amount of data you have collected. Once the data has gone through the cleaning process, the next step is the feature selection/engineering process.\n\nUnderstanding the different features your data contains plays an important role when your model tries to find the relationship in its data. Exploratory Data Analysis (EDA) is the common process used when it comes to understanding the data you have collected and how the data is structured. This helps when it comes to determining which features to use in your model. As we previously mentioned in Chapter 4, when we include more features in our models, it allows them to map to more complex problems. However, adding too many features can lead to overfitting, so it is important to research the most important features for your model.\n\nWhile most machine learning models can find patterns and relationships in data, the best way of understanding the data you collect is via the experts in the field of the task you are trying to solve. Subject matter experts can provide the best insight into what features to focus on when creating your model. Some unsupervised machine learning models, such as PCA and t-SNE, can group and find features that can provide the most valuable information for your model.\n\nImportant note Having domain knowledge of the problem you are trying to solve is the most effective way of understanding your data and determining which features to use for training your machine learning model.\n\nOnce you have set up the processes to collect and clean the data, the next step is creating and training your model. Thanks to most machine learning libraries, you can import prebuilt models and even use weights from already trained models to use on your own model. Here, it is common practice to use different models and techniques to see which produces the best result, and from there, you can choose the best model and begin to fine-tune it by updating the hyperparameters. This process can take time depending on the amount of data you use.\n\nTesting your model is a critical element in your system’s pipeline. Depending on the application, a poor model can negatively impact your business and give your users a bad experience. To prevent that, you need to determine the different metrics and thresholds that need to be met for the model to be production-ready. If the model can’t meet these expectations, then you need to go back and understand the weaknesses of the model and address them before training again.\n\n99",
      "content_length": 2995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "100\n\nMachine Learning System Design\n\nAfter performing tests and getting solid results from your model, you can now deploy your model to the user application. This varies from application to application. From then, the whole process can start from the beginning, where new data is inserted and follows the machine learning pipeline so it can dynamically grow based on user actions:\n\nFigure 5.2: The machine learning pipeline\n\nIn the following sections, we will look into the details of the different interfaces that constitute our machine learning pipeline.\n\nFit and transform interfaces\n\nNow that we have looked at the entire pipeline process, we will look in detail at the different interfaces that make up the machine learning system. The majority of the systems include the following interfaces:\n\nFit\n\nTransform\n\nTrain\n\nServe\n\nWhen it comes to the data and creating the model, we come across the fit and transform interfaces. We will start by looking at the transform interface.\n\nTransform\n\nThe transform interface is the process of taking in the collected data and preprocessing the data so that the model can train properly and extract meaningful information.",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "Fit and transform interfaces\n\nIt is common for the data we collect to have missing values or outliers, which can cause bias in our model. To remove this bias, we can apply certain techniques that help remove the skew in the data and produce meaningful machine learning models. Some of the following techniques we will learn about fall into the following three types of transformations:\n\nScaling\n\nClipping\n\nLog\n\nLog transformation is the most common and simple transformation technique we can apply to our data. A lot of the time, our data is skewed in one direction, which can introduce bias. To help mitigate the skewed distribution, we can simply apply the log function to our data, and this shifts our data into more of a normal distribution, which allows the data to be more balanced.\n\nWe can perform this transformation by using the following code:\n\nimport numpy as np dataframe_log = np.log(dataframe[\"House Price\"])\n\nFigure 5.3: Performing log transformation on skewed data\n\n101",
      "content_length": 985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "102\n\nMachine Learning System Design\n\nOnce we apply the log transformation, we can start looking at the other transformations. The second transformation we can use is the clipping transformation. The more we make our data follow a normal distribution, the better, but we may encounter outliers that can skew our data. To help reduce the impact that outliers have on our data, we can apply a quantile function. The most common quantile range that people use is the 0.05 and 0.95 percentile. This means that any data below the 0.05 percentile will be rounded up to the lower bound while any data above the 0.95 percentile will be rounded down to the upper bound. This allows us to retain the majority of the data while reducing the impact that outliers have on the model. The upper and lower ranges can also be modified based on what makes sense for the distribution of the data.\n\nThis transformation can be performed using the following code:\n\nfrom sklearn.preprocessing import QuantileTransformer quantile = QuantileTransformer(output_distribution='normal', random_state=0) x_clipped = quantile.fit_transform(\"House Price\")\n\nFigure 5.4: Clipping transformation on data\n\nThe last major transformation technique is scaling transformations. A lot of the time, the data we collect have different types of metrics and values, which can skew our data and confuse our model. For example, one feature measures the revenue of companies in the millions while another feature measures the employee count in the thousands, and when using these features to train the model, the",
      "content_length": 1563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Fit and transform interfaces\n\ndiscrepancy may put more emphasis on one feature over another. To prevent these kinds of problems, we can apply scaling transformations, which can be of the following types:\n\nMinMax\n\nStandard\n\nMax Abs\n\nRobust\n\nUnit Vector\n\nThe MinMax scaler is the simplest scaling transformation. It works best when the data is not distorted. This scales the data between 0 and 1. It can be calculated using this formula:\n\n𝑥𝑥𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 = (𝑥𝑥 − 𝑥𝑥𝑚𝑚𝑚𝑚𝑚𝑚)/ (𝑥𝑥𝑚𝑚𝑠𝑠𝑚𝑚 − 𝑥𝑥𝑚𝑚𝑚𝑚𝑚𝑚)\n\nWe can perform this scaling transformation using the following code:\n\nfrom sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() x_scaled = scaler.fit_transform(\"House Price\")\n\nThe MaxAbs scaler is similar to MinMax but rather than scaling the data between 0 to 1, it scales the data from -1 to 1. This can be calculated using the following formula:\n\nmax⁡(|𝑥𝑥|) 𝑥𝑥\n\n𝑥𝑥𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 =\n\nWe can perform this scaling transformation using the following code:\n\nfrom sklearn.preprocessing import MaxAbsScaler scaler = MaxAbsScaler() x_scaled = scaler.fit_transform(\"House Price\")\n\nThe Standard scaler is another popular scaling transformation. Rather than using the min and max like the MinMax scaler, this scales the data so that the mean is 0 and the standard deviation is 1. This scaler works on the assumption that the data is normally distributed. This can be calculated using the following formula:\n\n𝑥𝑥𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 = 𝑥𝑥 −\n\n𝜇𝜇 𝜎𝜎\n\n103",
      "content_length": 1437,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "104\n\nMachine Learning System Design\n\nWe can perform this scaling transformation using the following code:\n\nfrom sklearn.preprocessing import StandardScaler scaler = StandardScaler() x_scaled = scaler.fit_transform(\"House Price\")\n\nThe MinMax, MaxAbs, and Standard scalers, while powerful, can suffer from outliers and skewed distribution. To remedy this issue, we can use the Robust scaler. Rather than using the mean or max, this scaler works by removing the median from the data and then scaling the data using the interquartile range. This can be calculated using the following formula:\n\n𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝐼𝑡𝑡𝑡𝑡𝐼𝐼 𝑅𝑅𝐼𝐼𝐼𝐼𝑅𝑅𝐼𝐼 (𝐼𝐼𝐼𝐼𝑅𝑅) = 𝐼𝐼3 − 𝐼𝐼1\n\n𝑥𝑥𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠 = (𝑥𝑥 − 𝑄𝑄1)/𝐼𝐼𝑄𝑄𝐼𝐼\n\nWe can perform this scaling transformation using the following code:\n\nfrom sklearn.preprocessing import RobustScalar scaler = RobustScalar() x_scaled = scaler.fit_transform(\"House Price\")\n\nFinally, we have the Unit Vector scaler, also known as a normalizer. While the other scaler functions work based on columns, this scaler normalizes based on rows. It uses the MinMax scaler formula and converts positive values between 0 and 1 and negative values between -1 and 1. There are two ways of performing this scaling:\n\nL1 norm – values in the column are converted so that the sum of their absolute value in the row equals 1\n\nL2 norm – values in the column are squared and added so that the sum of their absolute value in the row is equal to 1\n\nWe can perform this scaling transformation using the following code:\n\nfrom sklearn.preprocessing import Normalizer scaler = Normalizer() x_scaled = scaler.fit_transform(\"House Price\")\n\nThere are many more scaling and transforming techniques, but these are the most commonly used, as they provide stable and consistent results.",
      "content_length": 1754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "Fit and transform interfaces\n\nImportant note Much of the development process takes place in the transformation stage. Understanding how the data is structured and distributed helps dictate which transformation methods you will perform on your data. No matter how advanced your model is, poorly structured data will produce weak models.\n\nFit\n\nNow, we will look at the fit interface. This interface refers to the process of creating the machine learning model that will be used in training. With today’s technology, not much work or effort is needed to create the model used for training in the machine learning pipeline. There are already prebuilt models ready to be imported and used for any type of application.\n\nHere is a small example of creating a KNN classification model using the scikit-learn library.\n\nFirst, we import all the required libraries:\n\nfrom sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KneighborsClassifier from sklearn.datasets import load_iris\n\nWe then import the data, split the data into training and testing batches, and apply a standard scaler transformation:\n\niris = load_iris() X = iris.data y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_ size=0.30) scaler = StandardScaler() scaler.fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test)\n\nWe then initialize a KNN model with k = 3 and then perform training on the model:\n\nclassifier = KNeighborsClassifier(n_neighbors=3) classifier.fit(X_train, y_train)\n\n105",
      "content_length": 1579,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "106\n\nMachine Learning System Design\n\nThe main effort when using the fit interface is setting up the models that will be used for the training phase of the machine learning pipeline. Due to the simplicity of importing multiple prebuilt models, it is common practice to import multiple types of machine learning models and train all of them at once. This way, we are able to test different types of models and determine which one of them performs the best. Once we decide which model to use, we can then start to experiment with different hyperparameters to further fine-tune our model.\n\nTrain and serve interfaces\n\nThe transform and fit interfaces are responsible for preparing the data and setting up our machine learning models for our pipeline. Now that we have preprocessed the data, we need to begin looking at how we can begin the actual training process and take our trained models and deploy them for our clients to use.\n\nTraining\n\nNow that we have preprocessed the data and created our models, we can begin the training process. This stage can vary from time to time depending on the quality of data being trained on or the type of model being used during training.\n\nOnce we preprocess the data, we need to split the dataset into training and testing sets. This is done to prevent overfitting. We need the model to be able to generalize the data, and using all the data for training would defeat the purpose.\n\nA common practice is to split your data into 70% training and 30% testing. This way, the model has enough data to learn the relationships and uses the testing data to self-correct its training process.\n\nThere is a more robust approach to splitting the data, which is called K-Fold Cross-Validation. This process works best in cases where there may not be enough training data. To perform this, we split the data into k number of subsets and then we train on all subsets except for one. We then iterate through this process where a new subset is selected to be the test data. Finally, we measure the performance of the model by averaging the metrics for each iteration. This way, we can train and test using all the data without leaving any important features that may be useful when it comes to learning the data.",
      "content_length": 2231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "Train and serve interfaces\n\nFigure 5.5: K-Cross Validation\n\nOnce we have split the data, now comes the actual training part. This part is as simple as setting up the function used to train the model. This part depends on the type of library you use and the different APIs it offers.\n\nWe can create a simple example using the scikit-learn library:\n\nfrom sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn import metrics\n\ndiabetes = load_diabetes() features = diabetes.data target = diabetes.target\n\nx_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=1) linear_regression = LinearRegression() linear_regression.fit(features, target)\n\n107",
      "content_length": 785,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "108\n\nMachine Learning System Design\n\ny_pred = linear_regression.predict(x_test) print(\"Linear Regression model MSE:\", metrics.mean_squared_ error(y_test, y_pred))\n\nAfter training your model, you must measure its performance. To prevent poor models from being deployed to users, it is a common practice to measure certain metrics and set certain thresholds that need to be met before a model is considered ready for production.\n\nDepending on the type of model you create, certain metrics need to be evaluated. For example, a regression model will typically look at the following metrics:\n\nMean Absolute Error (MAE)\n\nMean Squared Error (MSE)\n\nRoot Mean Squared Error (RMSE)\n\nR-Squared (R2)\n\nFor classification models, you will monitor the following metrics to determine the model’s strength:\n\nAccuracy\n\nPrecision and recall\n\nThe F1-score\n\nThe Area Under the Receiver Operating Characteristics Curve (AUROC)\n\nHaving domain knowledge helps immensely when determining what thresholds are applicable to the model you are training. In some cases, such as with cancer detection models, it is important to avoid false negatives, so it is important to set stricter thresholds for what models can be used confidently.\n\nImportant note Before serving your model, you need to make sure the model is viable for production. Setting up the metric thresholds that the model needs to pass is a fundamental way of validating your models before deploying them. If your model fails to pass these criteria, then there should be a process to redo the data transformation and model training phases until it can pass the thresholds.\n\nServing\n\nWhen it comes to serving our model, this is open and flexible depending on the user’s needs. In most cases, we are deploying our model into one of two types of systems:\n\nModel serving, where we deploy our model as an API\n\nModel embedding, where we deploy our model straight into an application or device",
      "content_length": 1920,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "Orchestration\n\nModel embedding is the simplest way of deploying your model. You create a binary file containing your model and you embed the file into your application code. This simplicity provides the best performance when making predictions, but this comes at a cost. Because you directly embed the file into your application, it is difficult to scale your model since you will have to recreate and reupload the file every time you make an update to your model. As such, this is not considered a recommended practice.\n\nModel serving is the most commonly used method on today’s market. This separation between the application and the model makes it easy for a developer to maintain and update the model without having to change the application itself. You simply create an API service that a user can access to make calls and predictions. Due to the separation, you can continuously update the model without having to redeploy the whole application.\n\nAn alternative to model embedding that includes model serving is creating a microservice that includes the binary file of the model, which could be accessed by other applications:\n\nFigure 5.6: Serving machine learning models\n\nOne of the more intuitive approaches is creating your own package or library that includes all the models that you have trained. That way, you can scale efficiently by allowing multiple applications to access the different models you have created.\n\nEverything we’ve seen so far is what it takes to build a simple machine learning pipeline. While this is doable for most applications, to be dynamic and robust, we need to look at orchestration and what it can offer us to support more advanced applications and problems.\n\nOrchestration\n\nNow that we understand the different interfaces and the roles they play in the machine learning pipeline, the next step is understanding how to wrap everything together into one seamless system. To understand the holistic system, we must first understand automation and orchestration.\n\n109",
      "content_length": 2004,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "110\n\nMachine Learning System Design\n\nAutomation refers to the process of automating small or simple tasks, such as uploading files to a server or deploying an application, without human intervention. Rather than having a person perform these repetitive tasks, we can program our system to handle these simple tasks, thus reducing wasted time and resources.\n\nThis is useful for most systems due to the linear nature of the pipeline. This highlights a common limitation of automation though – the lack of flexibility. Most systems today require a more dynamic process to be able to adapt to certain applications and processes, and automation alone isn’t enough:\n\nFigure 5.7: A linear system pipeline\n\nThis is where orchestration comes into action. Orchestration is the configuration and coordination of automated tasks to create a whole workflow. We can create a system to perform certain jobs or tasks based on a certain set of rules. It takes some planning and understanding to create a comprehensive orchestration workflow since the user determines what actions the system needs to take for certain cases.\n\nA simple example would be deploying an application to users. There can be many moving parts in the system, such as the following:\n\nConnecting to a server\n\nUploading certain files to certain servers\n\nHandling user requests\n\nStoring data or logs in a database\n\nLet’s say that after the recent changes have been deployed, the app has suffered critical errors, which may bring down the application. The system admin could set up rules for recovering and restoring the system, such as rolling back to a stable version. With the system able to self-recover, the developers can spend more time in development rather than dealing with overhead when it comes to recovery.\n\nDepending on certain outcomes, not all tasks may need to be performed. There may be backup actions that need to take place, or different paths that the system needs to go through to maintain a stable workflow. This way, the system can adapt to its environment and self-sustain without much human intervention:",
      "content_length": 2081,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "Orchestration\n\nFigure 5.8: A dynamic system pipeline (orchestration)\n\nThe different tasks in the machine learning system that can be automated are as follows:\n\nGathering and preprocessing the data\n\nTraining the machine learning model\n\nRunning tests and diagnostics on the trained model to evaluate its performance\n\nServing the machine learning model\n\nMonitoring the model in production\n\nWith these automated tasks, the system admin needs to orchestrate the stages of the pipeline to be dynamic and sustainable. The following components help create a robust system:\n\nScheduling: The system must be able to schedule and run different automated tasks in the pipeline individually while maintaining system dependencies.\n\nCI/CD Testing: After model training is complete, it is imperative to do automated testing on your model to measure its performance. If it fails to pass certain metrics, you must repeat the training process from the beginning to address the weaknesses of the model; otherwise, it cannot be deployed to production.\n\nDeployment: Depending on where you will deploy your model to production, setting up an automated process can help reduce the time spent on deployment and still maintain an updated version of the model.\n\n111",
      "content_length": 1237,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "112\n\nMachine Learning System Design\n\nMonitoring: After deploying your model, continuously monitoring the model’s performance in production is needed to maintain the model’s health without it decaying. This will give us an indication of when we need to update our pipeline or our model in order to stay efficient.\n\nImportant note Understanding what your business needs are and how your model functions gives you a good picture of how you want to orchestrate your machine learning pipeline. Setting up backup phases to address certain pitfalls in your system allows it to be more dynamic and adaptable to industry demands.\n\nSummary\n\nIn this chapter, we looked at the different key components that make up a machine learning pipeline.\n\nFrom there, we looked in detail at the interfaces that make up the components. We started with the transform interface, which is responsible for the data aspect of the pipeline. It takes the data and applies different types of data transformation that allow us to maintain clean and stable data, which we can later use in our machine learning model.\n\nAfter our transformation stage, we start creating our model in the fit interface. Here, we can use the prebuilt models that the libraries and packages offer to initialize our models. Due to the ease of creating models, it is a good practice to test different types of models to see which model performs the best based on our data.\n\nOnce we have created our model, we can begin the actual training of our model. We need to split our data into training and test sets to allow our model to understand the relationship in our data. From there, we can measure the different metrics in our model to validate the model’s performance.\n\nOnce we feel comfortable with our model’s performance, we can start to deploy our application to production. There are two major ways of deploying our model, whether it be embedded into our application or deployed as a service for our clients to use.\n\nFinally, wrapping everything together, we learned what orchestration consists of when it comes to machine learning. We learned what concepts need to be considered when orchestrating your machine learning pipeline and how to keep your system dynamic and robust to keep up with everyday demands.\n\nAs time passes and data changes, it is important that we adjust and maintain our models to handle certain situations that may arise in the real world. In the next chapter, we will look at how we can maintain our machine learning models when our data starts to shift and change.",
      "content_length": 2536,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "6 Stabilizing the Machine Learning System\n\nIn the last two chapters, we went over the different concepts in machine learning and how we can create a comprehensive machine learning system pipeline that can work and adapt to our needs.\n\nWhile our pipeline can address our expectations, it is important for us to be able to maintain our system in the face of external factors to which it may be hard for the system to self-adjust.\n\nIn this chapter, we will discuss the phenomenon of dataset shifts and how we can optimize our machine learning system to help address these issues while maintaining its functional goal without having to rebuild our system from scratch.\n\nWe will be going over the following concepts:\n\nMachine learning parameterization and dataset shifts\n\nThe causes of dataset shifts\n\n\n\nIdentifying dataset shifts\n\nHandling and stabilizing dataset shifts\n\nMachine learning parameterization and dataset shifts\n\nMaintaining our machine learning models is an integral part of creating a robust model. As time progresses, our data begins to morph and shift based on our environment, and while most models can detect and self-repair, sometimes, human intervention will be required to guide them back on track.\n\nIn this section, we will briefly go over two main concepts that will help us understand the impact on our model:\n\nParameterization\n\nDataset shifts",
      "content_length": 1364,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "114\n\nStabilizing the Machine Learning System\n\nOur machine learning model is represented by certain specifications that help define the learning process of our model. These include the following:\n\nParameters\n\nHyperparameters\n\nWe will first look at parameters. These specifications are internal within the model. During the training process, these parameters are updated and learned while the model is trying to learn the mapping between the input features and the target values.\n\nMost of the time, these parameters are set to an initial value of either zeros or random values. As the training process happens, the values are continuously updated by an optimization method, such as gradient descent. At the end of the training process, the final weights of the values are what constitute the model itself. These weights can even be used for other models, especially those with similar applications.\n\nSome examples of parameters include the following:\n\nNode weights and bias values for artificial neural networks\n\nCoefficients of linear and logistic regression models\n\nCluster centroids for clustering models\n\nWhile parameters play a core role in determining the performance of a model, they are mostly out of our control since the model itself is what updates the weights. This leads us to hyperparameters.\n\nHyperparameters are parameters that control the learning process of our machine learning model, which, in turn, affects the output weights that our model learns. These values are set from the beginning and stay fixed throughout the learning process.\n\nWe, as users, determine which values to set in the beginning for our model to use during the training process. As a result, it takes time and experience to figure out which values produce the best results. There is effort involved in testing and training multiple variations of hyperparameters to see which performs the best.\n\nThere are many hyperparameters and each model has its own unique set of hyperparameters that the user can modify. These hyperparameters can include the following:\n\nThe split ratio between the training and testing datasets\n\nThe learning rate used in optimization algorithms\n\nThe choice of optimization algorithm\n\nThe batch size\n\nThe number of epochs or iterations\n\nThe number of hidden layers",
      "content_length": 2275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "Machine learning parameterization and dataset shifts\n\nThe number of nodes in each hidden layer\n\nThe choice of cost or loss function\n\nThe choice of activation function\n\nThe number of\n\nclusters\n\nSince there can be many hyperparameters to adjust and many different combinations to try, it can be very time-consuming to test these changes one by one. As discussed in the last chapter, it can be useful to have a section in our pipeline that automates this process by running multiple models with different combinations of hyperparameters to speed up the testing process and find the most optimal combination of hyperparameters.\n\nFigure 6.1: Hyperparameter and parameter tuning\n\nThere may be cases where adjusting our parameters and hyperparameters is not enough for us to prevent our model from degrading.\n\nFor example, let’s say we create a machine learning model with a model accuracy of 85%. This model continues to perform well for some time. We then begin to see our model accuracy deteriorate until it becomes unusable, as the model is unable to properly predict the new test data we collect.\n\n115",
      "content_length": 1099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "116\n\nStabilizing the Machine Learning System\n\nAs we analyze our model, we can begin to see that our training data does not reflect the testing data we have recently collected. Here, we can see that there is a shift between the data distribution for our training and test datasets.\n\nBefore we work on resolving dataset shifts, we must first understand the background of dataset shifts, how they occur, and how we can adjust our machine learning system to help prevent dataset shifts from impacting our model.\n\nMachine learning systems are built under the assumption that the data distribution between the training and test sets is similar. Since the real world is ever-changing, new data distributions emerge and there may be a significant difference between the training and test sets.\n\nThe major difference in data distribution between the training and test sets is considered a dataset shift. This drastic difference will eventually degrade the model, as the model is biased to the training set and is unable to adapt to the test set:\n\nFigure 6.2: Outcome of a machine learning model due to a dataset shift\n\nSome examples of this occurring include a shift in consumer habits, a socioeconomic shift, or a global influence, such as a pandemic. These events can heavily impact the data we collect and observe, which, in turn, can sway our model’s performance.\n\nImportant Note First, try adjusting the hyperparameters of your machine learning model and see whether the newly learned parameters can improve your model significantly. If you still encounter major issues, it may be best to analyze the data and see whether a dataset shift has occurred.",
      "content_length": 1647,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "The causes of dataset shifts\n\nThe causes of dataset shifts\n\nNow that we have learned what dataset shifts are, we can start to investigate the different causes of dataset shifts. While there are many different reasons dataset shifts can occur, we can split them into two categories:\n\nSample selection bias\n\nNon-stationary environments\n\nSample selection bias is self-explanatory in that there is a bias or issue when it comes to labeling or collecting the training data used for the model. Collecting biased data will result in a non-uniform sample selection for the training set. That bias, in essence, will fail to represent the actual sample distribution.\n\nNon-stationary environments are another cause for dataset shifts – we will go into further detail about the different types later in the chapter. Let’s assume that we have a model with a set of input features, , the . This dataset shift is caused , which reflect very much how the ( , )\n\n, a target or output variable\n\n. From there, we can also define the prior probability as\n\nconditional probability as by temporal or spatial changes, defined as real world operates.\n\n, and the joint distribution as\n\n( | )\n\n𝑃𝑃𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦,𝑥𝑥) ≠ 𝑃𝑃𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦,𝑥𝑥)\n\n( )\n\nThis causal effect can lead to different types of shifts:\n\nFor\n\n , giving us a covariate or concept shift\n\nproblems, non-stationary environments can make changes to either\n\n( )\n\nor\n\n\n\nFor\n\n( | )\n\nproblems, a change in\n\nor\n\ncan give us a prior probability or concept shift\n\n( )\n\n( | )\n\nIn the next section, we will look into the different types of shifts and how we can identify them.\n\nIdentifying dataset shifts\n\nAfter looking into the different causes of dataset shifts, we can begin to classify certain shifts into different groups that can help us easily identify the type of dataset shift we are dealing with.\n\nAmong the different dataset shifts we can encounter, we can classify data shifts into these categories:\n\nCovariate shifts\n\nPrior probability shifts\n\nConcept shifts\n\nWe will first look at covariate shifts. This is the most common dataset shift, as a covariate shift occurs when there is a change in the distribution of one or more of the input features of the training or test data. Despite the change, the target value remains the same.\n\n117",
      "content_length": 2272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "118\n\nStabilizing the Machine Learning System\n\nIn mathematical terms, this dataset shift occurs only in X > Y problems. Whenever the input , but the distribution, , this conditional probability of the training and testing dataset stays the same, will cause a covariate shift.\n\n, changes between the training and testing datasets,\n\n𝑝𝑝(𝑥𝑥)\n\n𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥) ≠ 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥)\n\n𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦|𝑥𝑥) = 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦|𝑥𝑥)\n\nFor example, we can create a model that predicts the salary of the employees of a certain city. Let’s say that the majority of the employees in your training set consist of younger individuals. After time passes, the employees get older. If you were to try to predict the salary of the older employees, you would begin to see a significant error. This is due to the model being heavily biased toward the training set, which consisted of mostly younger employees and is unable to find the relationship among the older employees.\n\nFigure 6.3: Covariate dataset shifts\n\nNext, we will be looking into prior probability shifts, also known as label shifts. This is the opposite of a covariate shift, as this shift occurs when the output distribution changes for a given output but the input distribution remains the same.\n\nIn mathematical terms, this occurs only in Y -> X problems. When the prior probability changes,\n\n, but the conditional probability remains the same,\n\n𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦) ≠ 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦) , a prior probability shift occurs:\n\n𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥|𝑦𝑦) = 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥|𝑦𝑦)",
      "content_length": 1491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "Identifying dataset shifts\n\nFigure 6.4: Prior probability shifts\n\nFinally, we will discuss concept shifts, also known as concept drifts. This shift occurs when the distribution of the training data remains the same but the conditional distribution for the output given the training data changes.\n\nIn mathematical terms, this can occur both in X -> Y or Y -> X problems:\n\nFor X -> Y problems, this occurs when the prior probability of the input variables remains , but the conditional the same in the training and testing datasets, ( probability changes,\n\n𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥) = 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥))\n\nFor Y -> X problems, this occurs when the prior probability of the target variables remains , but the conditional the same in the training and testing datasets, probability changes,\n\nFor Y -> X problems, this occurs when the prior probability of the target variables remains , but the conditional .\n\n(𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦) = 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑦𝑦))\n\n(𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥|𝑦𝑦) ≠ 𝑝𝑝𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡(𝑥𝑥|𝑦𝑦))\n\nAs an example, a user’s purchasing behavior is affected due to the economy, but neither our training nor our test data contains any information regarding the economy’s performance. As a result, our model’s performance will degrade.\n\n119",
      "content_length": 1207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "120\n\nStabilizing the Machine Learning System\n\nFigure 6.5: Concept shifts\n\nThis can be a tricky dataset shift since the distribution shift is not related to the data that we train on, but rather external information that our model may not have. Most of the time, these dataset shifts are cyclical and/or seasonal.\n\nImportant Note Visualizing your data and calculating the different probabilities with regard to your data is the best way to help determine and identify which dataset shift you are dealing with. From there, you can decide how you will address your dataset shift.\n\nWhen it comes to identifying most dataset shifts, there is a process that we can follow to help us. It includes the following steps:\n\nPreprocessing the data\n\nCreating random samples of your training and test sets on their own\n\nCombining the random samples into one dataset\n\nCreate a model using one feature at a time while using the origin as the output value\n\nPredicting on the test set and calculating the Area Under Curve – Receiver Operating Characteristics Curve (AUC-ROC)\n\n\n\nIf the AUC-ROC is greater than a certain threshold, for example, 80%, we can classify the data as having experienced a dataset shift",
      "content_length": 1191,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "Handling and stabilizing dataset shifts\n\nFigure 6.6: An example of an AUC-ROC graph (a value close to 1 indicates a strong model)\n\nHandling and stabilizing dataset shifts\n\nNow that we have established the methods for identifying the different types of dataset shifts, we can discuss the different ways of addressing these shifts and stabilizing our machine learning models.\n\nWhile there are many ways to address dataset shifts, we will be looking at the three main methods. They consist of the following:\n\nFeature dropping\n\nAdversarial search\n\nDensity ratio estimation\n\nWe will first look at feature dropping. This is the simplest form of adjusting dataset shifts. As we determine which features are classified as drifting, we can simply drop them from the machine learning model. We can also define a simple rule where any features with a drift value greater than a certain threshold, for example, 80%, can be dropped:\n\n121",
      "content_length": 924,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "122\n\nStabilizing the Machine Learning System\n\nFigure 6.7: Feature Dropping Process\n\nWhile this is a simple change, this is something that needs to be considered carefully. If this feature is considered important when training your machine learning model, then it is worth reconsidering whether this feature needs to be dropped. Also, if the majority of your features pass the threshold for being dropped, you may want to revisit your data as a whole and consider a different approach when addressing your dataset shift.\n\nNext, we will look at adversarial search. This is a technique that requires training a binary classifier to predict whether the sample data is within the training or test datasets. We can then evaluate the performance of the classifier to determine whether there has been a dataset shift. If the performance of our classifier is close to that of a random guess (~50%), we can confidently determine that our training and test dataset distribution is consistent. On the other hand, if our classifier performs better than a random guess, then that will indicate an inconsistency between the distribution of the training and test datasets.\n\nThe adversarial search can be split into three parts:\n\n1. From the original dataset, we will remove the target value column and replace it with a new column that indicates the source of data (train = 0 and test = 1).\n\n2. We will create and train the new classifier with the new dataset. The output of the classifier is the probability that the sample data is part of the test dataset.\n\n3. Finally, we can observe the results and measure the performance of our classifier. If our classifier performance is close to 50%, then this indicates that the model is unable to differentiate whether the data is coming from the training or test set. This can tell us that the data distribution between the training and test datasets is consistent. On the flip side, if our performance is close to 100%, then the model is confident enough to find the difference between the training and test datasets, which then indicates a major difference between the distribution of the training and test datasets.",
      "content_length": 2147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Handling and stabilizing dataset shifts\n\nFigure 6.8: Adversarial search process\n\nUsing adversarial search, we can establish three methods to address the dataset shifts we encounter:\n\nUsing the results, we can use them as sample weights for the training process. The weights correspond to the nature of how the data is distributed. The data that is similar in the actual distribution will be assigned a larger weight while that with inconsistent distribution will be given a lower weight. This will help the model emphasize the data that actually represents the real distribution it is trying to learn.\n\nWe can use only the top-ranked adversarial validation results. Rather than mitigating the weights of inconsistent samples in the testing dataset, we can remove them altogether.\n\nAll data is used for training except for the top-ranked adversarial validation results. This method can address the issues that can arise from the second method by using all the data rather than dropping features. Rather than discarding unimportant data, we can incorporate some of the data in the training data for each fold when using K-fold cross-validation during training. This helps maintain consistency while using all the data.\n\nThe final method used to address dataset shifts is called the density ratio estimation method. This method is still under research and not a commonly used method to address dataset shifts.\n\n123",
      "content_length": 1411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "124\n\nStabilizing the Machine Learning System\n\nWith this approach, we would first estimate the training and test dataset densities separately. Once we have done this, we will then estimate the importance of the dataset by taking the ratio of the estimated densities of the training and test datasets. Using this density ratio, we can use it as the weight for each data entry in our training dataset.\n\nThe reason this method is not preferred and is still under research is that it is computationally expensive, especially for higher dimensional datasets. Even then, the improvements it can bring to addressing dataset shifts are negligible and not worth the effort of pursuing this method.\n\nImportant Note Feature dropping is the easiest and simplest way to address dataset shifts. Consider using this approach before using the adversarial search approach, as that option, while effective, can be a little involved and may require more effort and resources to help mitigate the effect of dataset shifts.\n\nSummary\n\nIn this chapter, we went over the general concepts of dataset shifts and how they can negatively impact our machine learning model.\n\nFrom there, we delved in deeper into what causes these dataset shifts to occur and what different characteristics dataset shifts can exhibit. Using these characteristics, we can better identify the type of dataset shift – whether it was a covariate shift, prior probability shift, or concept shift.\n\nOnce we were able to analyze our data and identify the type of dataset shift, we looked at different methods to help us handle and stabilize these dataset shifts so that we could maintain our machine learning model. We went over some techniques, such as feature searching, adversarial search, and density ratio estimation, that can assist us when dealing with dataset shifts.\n\nUsing these processes and methods, we can prevent our model from suffering from common dataset shifts that occur in the real world and continuously maintain our machine learning model.\n\nNow that we have a firm understanding of machine learning and how to maintain a robust model, we can start looking into how we can incorporate our machine learning models into our Microservices Architecture (MSA).",
      "content_length": 2221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "7 How Machine Learning and Deep Learning Help in MSA Enterprise Systems\n\nIn the previous chapters, we analyzed the different general concepts of artificial intelligence, machine learning, and deep learning, and how they can be used for certain applications and use cases. From there, we looked at how to create an end-to-end machine learning system pipeline and the advantages it brings when establishing a robust system. Finally, we examined the different ways our machine learning model can degrade over time through data shifts, and the different ways we can identify and address them.\n\nHaving a firm understanding of the basics of machine learning, we can now begin to explore the use cases of machine learning in our Microservice Service Architecture (MSA) enterprise. In this chapter, we will go over the different concepts we will be proposing when integrating machine learning in to an MSA enterprise system to establish an intelligent MSA.\n\nMachine learning MSA enterprise system use cases\n\nThe space for adding machine learning to MSA enterprise systems is broad and can be open for many use cases. We can use machine learning for different types of problems that we can encounter in MSA, such as the following:\n\nSystem Load Prediction: This will determine when a service is experiencing higher than usual loads and trigger measures to prevent the system from degrading due to excessive server loads.\n\nSystem Decay Prediction: Similar to system load prediction, this will monitor the microservices and try to predict and determine anomalies in the MSA enterprise, allowing users to act and prevent certain issues from arising and negatively impacting the performance.",
      "content_length": 1677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 147,
      "content": "126\n\nHow Machine Learning and Deep Learning Help in MSA Enterprise Systems\n\nSystem Security: In the age of cybersecurity, it is important to be able to protect your MSA system from targeted attacks. By studying the behavior of your MSA system, the model can predict and detect attacks that could be impacting your system.\n\nSystem Resource Planning: As your system grows and evolves, being able to properly allocate resources and adapt to your system needs is a critical part when establishing your MSA enterprise system. With machine learning, we can learn which services require more resources and how much we need to scale in order to allocate the required resources efficiently and effectively.\n\nFigure 7.1: Use cases of machine learning in MSA\n\nWhile there are many more use cases of machine learning in MSA enterprise systems, most use cases fall under these four categories. Before getting into the implementation of the different models, we need to first get an overview of the different cases and how we need to solve these different problems.\n\nWe can start by looking at system load predictions. This is a common issue that we will encounter when it comes to dealing with services in general. MSA has an advantage compared to monolithic systems, where the resources are dedicated to each microservice, allowing easier maintenance and scalability. As discussed in previous chapters, though, there could be cases where, in MSA, a microservice experiences a high load and, as a result, causes a cascading effect where the failures expand to other microservices.\n\nWith an intelligent MSA, we can train a model using different features, such as the response time, to learn the patterns of the MSA system. Similar to a microservice circuit breaker, this model will be able to swiftly determine whether a microservice is experiencing a heavy load and address the issue before it becomes too late and starts negatively impacting the other microservices.",
      "content_length": 1954,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "Machine learning MSA enterprise system use cases\n\nFigure 7.2: System load prediction model\n\nJust like the system load prediction model, we can build a model to find anomalies within the MSA that could lead to decaying services. Rather than focusing only on the service load for a specific microservice, we can study the entire MSA and learn the different patterns of how it operates at a larger scale.\n\nCertain systems can experience different system loads and bugs over certain times and periods. For example, our service may encounter spikes in requests over certain periods such as holidays and seasonal events, where the user count may drastically increase. Allowing the model to learn and understand the MSA and how it operates over time can prepare the model to better detect anomalies and prevent false positives.\n\nAlso, rather than monitoring separate microservices, we can evaluate clusters of microservices and how they interact with the entire MSA. This way, we can identify certain bottlenecks and bugs that could arise in our MSA.\n\nFigure 7.3: System decay prediction model\n\n127",
      "content_length": 1091,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "128\n\nHow Machine Learning and Deep Learning Help in MSA Enterprise Systems\n\nMachine learning has been thriving in the security field. With more advanced attacks and methods, it has become imperative for users to protect their systems. Machine learning has made it easier for users to create robust models that can analyze and predict attacks before they can even impact their systems, and MSA is no different.\n\nDenial of Service (DoS) is a cyber-attack intended to prevent users from accessing certain services. These attacks are becoming more sophisticated with the advancements in technology. With machine learning, we can train our model to learn about our MSA and simulate DoS attacks such that it can be able to determine whether our MSA is under attack. With that, we can notify the security team or deploy countermeasures to fight back against certain attacks and maintain the integrity of our MSA.\n\nFigure 7.4: System security model\n\nA part of the self-healing process includes resource allocation for certain microservices when your MSA begins to grow and expand. After a time, you may experience a growth in users and as a result, your microservices will have increased request volume. A model may incorrectly identify a problem and offer solutions that wouldn’t address the core problem.\n\nThus, building an advanced model where it can track the gradual growth of the MSA and determine when certain services need more resources can be a critical part of the system’s self-healing process. A successful implementation of the model can greatly improve system reliability as it can properly and efficiently allocate resources more effectively.\n\nFigure 7.5: System resource planning model",
      "content_length": 1694,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "Enhancing system supportability and time-to-resolution (TTR) with pattern analysis machine learning\n\nImportant note The different types of models we can use in our MSA are not mutually exclusive. It is possible, and common, to combine the different use cases to build a more intelligent MSA. Understanding how your MSA operates and determining the different weaknesses it may have makes it easier for the user to determine which models to approach.\n\nWith certain use cases, some models can work better than others due to the nature of the problem. Now that we have looked at the different concepts where we can apply machine learning to our MSA, we can begin to dive deep into the different implementations and models we can use to build our machine learning models in the next few sections.\n\nEnhancing system supportability and time-to-resolution (TTR) with pattern analysis machine learning\n\nBefore we can begin to make our MSA intelligent, we must first understand how our system performs by leveraging machine learning models to learn the common trends and patterns for the performance of our services. From there, we can establish a baseline that can be used as a reference for other advanced models to use.\n\nAs discussed in Chapter 4, supervised learning can occur when we have a labeled test set. For our case, we can mostly use supervised learning because we can easily capture the response time of our services in the MSA and use that as our data label.\n\nFrom there, we have a wide variety of techniques that we can use to create our machine learning model. For simplicity, we can use a linear regression model to predict the expected response time for a particular microservice. Using this output, we can design a system where we can configure a set threshold where, if we detect that our MSA will reach a certain response time, we can notify the developers or initiate a program to resolve the issue before it occurs.\n\nIf we recall from Chapter 6, we discussed data shifts and how they can impact our model. It’s common for MSAs to grow and expand as time passes due to an increase in user counts or seasonal occasions. As a result, we may see a growth in response times and metrics for our MSA. This may falsely trigger an alert notifying us of abnormal response times when, in reality, it accurately depicts the normal behavior of the MSA.\n\n129",
      "content_length": 2357,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "130\n\nHow Machine Learning and Deep Learning Help in MSA Enterprise Systems\n\nTherefore, it is important to continuously collect data and train our model to adapt to expected changes such that it is able to learn how the system grows and to correctly identify changes that are not common to our MSA.\n\nFigure 7.6: Performance baseline system flow\n\nWhile this system is enough for simple problems, we can combine this model output with other advanced models to create a more end-to-end system, where we can understand the health of the MSA and make better decisions. In the next section, we will discuss how we can use deep learning to implement self-healing for our system.\n\nImportant note It’s important to start with a simple model, such as a linear regression model. Once the proof of concept works, you can improve your system by incorporating more advanced models and techniques.\n\nImplementing system self-healing with deep learning\n\nNow that we have determined the baseline for our system, we can use this to our advantage to create a more intelligent MSA, where we can detect anomalies and perform system self-healing. This way, we can be more proactive in resolving issues before they arise and save cost and time.\n\nAnomaly detection is an effective method for identifying any abnormal events or trends that may occur in a system or service. For example, we can use anomaly detection for determining credit card fraud. We can use the user’s purchasing trends and, based on that information, we can determine when the user has been a victim of credit card fraud.\n\nSimilar to credit card fraud detection, we can apply our anomaly detection to our MSA. Before we can go to the different models that we can use to achieve our anomaly detection, let us first understand the different types of anomalies:\n\nPoint Anomaly: This occurs when an individual point is far off from the rest of the data\n\nContextual Anomaly: Data is considered this way when it is not in line with the general data trend due to the context of the data\n\nCollective Anomaly: When a group of related data instances is anomalous with respect to the whole dataset",
      "content_length": 2131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Implementing system self-healing with deep learning\n\nFigure 7.7: Anomalous data\n\nAn anomaly detection model can be done in the following ways:\n\nSupervised Anomaly Detection\n\nUnsupervised Anomaly Detection\n\nA common model we can use for unsupervised learning is an autoencoder. As mentioned in Chapter 4, an autoencoder is a neural network composed of an encoder and a decoder. The general purpose of an autoencoder is to take the data and compress it to a lower dimension similar to PCA. That way, it is able to learn the correlations and patterns between the different data features. Once it learns the patterns, it can feed the compressed data forward to the decoder where it tries to “recreate” the original data with what it has learned in the encoder stage.\n\nWhile experts can study the data to determine what response times are considered an anomaly for a particular MSA, we can leverage machine learning to help us find patterns and relationships that may be hard to see even for an experienced developer.\n\nWith the learned parameters, we can then use this in our supervised regression models to achieve more accurate results when detecting anomalies and prevent false positives from occurring.\n\n131",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "132\n\nHow Machine Learning and Deep Learning Help in MSA Enterprise Systems\n\nFigure 7.8: Self-healing using deep learning\n\nImportant note Labeling data to be used in a supervised machine learning problem can cost time and money. You can leverage unsupervised machine learning models to help you predict and label your unlabeled data. From there, you can feed your newly labeled data into your supervised machine learning problem, thus taking advantage of unsupervised learning. Keep the newly labeled data in mind and make sure it doesn’t negatively impact your supervised machine learning problem.\n\nThese are some of the ways in which we can take advantage of machine learning and deep learning to create an intelligent MSA, where it can detect anomalies in the system and react swiftly. These use cases can be adjusted and enhanced based on the user’s needs and the demands of their MSA by using different models and techniques.\n\nSummary\n\nThis chapter discussed how we can implement machine learning and deep learning in our MSA.\n\nWe first looked into the different use cases for how machine learning can be used to build an intelligent MSA. The uses cases can be grouped into four categories:\n\nSystem Load Prediction\n\nSystem Decay Prediction\n\nSystem Security\n\nSystem Resource Planning",
      "content_length": 1286,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "We discussed each category and what role it plays when looking into creating an intelligent MSA.\n\nFrom there, we started looking into using supervised machine learning to create a pattern analysis model where it can learn our MSA and create a performance baseline model. Using this, we can determine whether our microservice performance is abnormal. We can then use this to either perform actions based on a threshold or use this baseline to build a more advanced model.\n\nAlong with our supervised machine learning model, we can use deep learning to create a more sophisticated model, such as autoencoders, to find anomalies in our MSA. Using the combination of these two models, we can create a set of rules to perform based on certain predictions, such as that our MSA can self-heal with minimal human intervention. This allows us to save time and money when repairing and debugging our MSA.\n\nIn the next chapters, we will be taking what we’ve learned so far and starting to build our own MSA with practical examples and integrating machine learning to create our very own simple intelligent MSA.\n\nSummary\n\n133",
      "content_length": 1112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Part 3: Practical Guide to Deploying Machine Learning in MSA Systems\n\nThe final part of this book will bring everything covered so far to life. It will walk you step by step through the design and development of an intelligent Microservices Architecture (MSA) system, with hands-on examples and actual code that can be imported for real-life use cases. The part will provide an in-depth understanding of how to apply the DevOps process to building and running an intelligent enterprise MSA system, from the very start to operations and maintenance.\n\nThe part starts with the basics of containers, Docker, and how to install and run Docker containers. We will also gain hands-on experience in handling data flows between containers to build a simple project. Additionally, the chapter will cover a practical guide on building specific-purpose AI and how to infuse AI services into an MSA system.\n\nThis part delves into the application of DevOps to enterprise MSA systems, with a focus on organizational structure alignment and how DevOps can impact the MSA and its operations. We will learn how to apply DevOps throughout the project life cycle, from start to operations and change management and maintenance.\n\nThe part also covers how to identify and minimize system dependencies, apply Quality Assurance (QA) testing strategies, build microservice and MSA test cases, and deploy system changes and hot updates. The section will also provide practical examples of how to overcome system dependencies and apply testing strategies effectively.\n\nIn conclusion, the final part of this book will provide you with a comprehensive guide on how to design, develop, and maintain an intelligent enterprise MSA system, with a focus on practical, hands-on experience and real-life use cases. By the end of this part, we will be equipped with the skills and knowledge necessary to build our own intelligent MSA system and take the first step toward achieving better business results, operational performance, and business continuity.",
      "content_length": 2020,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "136\n\nPart 3: Practical Guide to Deploying Machine Learning in MSA Systems\n\nThis part comprises the following chapters:\n\nChapter 8, The Role of DevOps in Building Intelligent MSA Systems\n\nChapter 9, Building an MSA with Docker Containers\n\nChapter 10, Building an Intelligent MSA System\n\nChapter 11, Managing the New System’s Deployment – Greenfield vs. Brownfield\n\nChapter 12, Deploying, Testing, and Operating an Intelligent MSA Systems",
      "content_length": 436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "8 The Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nIn previous chapters, we covered what MSA is and the advantages of MSA over monolithic architecture. Then, we discussed, with examples, how to refactor a monolithic application into an MSA, and then talked about different patterns and techniques to enhance the performance of an MSA system.\n\nWe also discussed the different ML and DL algorithms with hands-on examples, how they can be optimized, and how these ML and DL algorithms can help further enhance the stability, resilience, and supportability of an MSA system in order to build a “smart MSA” or “intelligent MSA” system.\n\nOver the next few chapters, we will further enhance our ABC-MSA system and try to apply what has been learned so far using some hands-on installations and code examples. However, before we do so, we need to discuss the different concepts of DevOps in this chapter, and how to apply the DevOps process to building and running an MSA system.\n\nIn Chapter 1, we briefly talked about DevOps in MSA. In this chapter, we will expand on the subject and dive into the details of the role of DevOps in building intelligent MSA.\n\nThe following topics are covered in this chapter:\n\nDevOps and organizational structure alignment\n\nDevOps processes in enterprise MSA system operations\n\nApplying DevOps from the beginning to operations and maintenance",
      "content_length": 1385,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "138\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nDevOps and organizational structure alignment\n\nIn a traditional software development organization, the software delivery process is matured and built according to how that traditional organization is structured. Typically, we have a business team that defines the core business specifications and requirements, followed by another team of architects that builds how the system is supposed to be structured. In the traditional software model, we also have design engineers who write the functional specs, a development team responsible for writing the code, a QA team to test the code quality, then a release team, an operations team for post-release operations, a support team, and so on.\n\nFigure 8.1: Traditional development structure\n\nWith all these teams involved in the pipeline in the traditional software release cycle, mostly sequential hand-offs between teams, silos, dependencies in between, cross-communication issues, and the possibility of finger-pointing during the process, the release cycle can take weeks or months to finish. For an MSA, this is not acceptable.\n\nImportant note The whole purpose of MSA is to simplify, speed up, and optimize software releases and updates. Applying the traditional methodology to MSA system development just doesn’t work and defeats the purpose of adopting an MSA to begin with.\n\nDevOps\n\nDevOps is one of the major processes adopted in modern software development organizations to help streamline the release process and optimize it so that an organization can make multiple seamless release updates every day with no service interruption whatsoever.",
      "content_length": 1671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "DevOps and organizational structure alignment\n\nDevOps is a combination of processes that allow you to take an application from development to operation smoothly. Enterprises need dedicated and well-defined DevOps processes to manage their solution development, hosting, and operations.\n\nThe primary need of a DevOps team is to implement engineering techniques in managing the operations of applications. While this sounds simple to do, several mundane and random activities are carried out by the operations teams. Streamlining these tasks is the biggest challenge in adopting DevOps.\n\nFigure 8.2: Teams working together in a DevOps fashion\n\nThe primary responsibility of the development team is to build the application. However, they also need to take care of other aspects of the application, such as the application performance, usage analytics, code quality, activity logging, and solving code-level errors.\n\nOn the other hand, the operations team faces a completely different set of problems. Their concerns include managing the availability of the applications, ensuring performance through higher scalability, and improving the monitoring of the solution ecosystem, the allocation of resources, and the overall system analytics. DevOps processes handle all of these concerns for all parties involved in the process.\n\nFigure 8.3: DevOps life cycle\n\n139",
      "content_length": 1359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "140\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nFigure 8.3 is similar to what we discussed in Figure 1.11. One new thing to add here is that the PLAN stage is where the software roadmap is defined and gets broken down into major requirements, called epics. These epics are broken down into a collection of short end user requirements, called user stories. More info on that will come in the next section.\n\nWell, OK then, if an organization is to adopt an MSA, they should embrace a DevOps culture as well. Simple, right? Not quite!\n\nAdopting a DevOps culture within a traditional organizational structure would have many misalignments that are guaranteed to hinder the DevOps cycle. The efficiency and speed of your release cycle will be as fast as the slowest process in your cycle. The software development organization itself has to shift its culture to align with DevOps, not the other way around. Many other methodologies and technologies will need to be adopted as part of the new shift to DevOps. The organizational structure itself may also need to be tweaked to align with the new DevOps methodologies.\n\nThe DevOps team structure\n\nSetting up a DevOps team is the first step toward organizational transformation. However, you cannot expect to have a fully-fledged DevOps team without considering the existing organizational structure and how the organization is aligned with the existing development cycle.\n\nIt is imperative to have an interim phase in which the development and operations teams can function reasonably within the existing traditional organization. Both traditional Dev and Ops teams then slowly morph themselves into a true DevOps structure as the organization modernizes its structure to fit into the new culture.\n\nOne of the recommended approaches in the organizational transformation scenario is to develop a small DevOps team to work as a link between the existing development team and the operations team. The DevOps team’s main objective in this particular case is to cross-function between both Dev and Ops teams to map deliverables in between, slowly familiarize both teams with the new methodology, and start applying basic DevOps methodologies within both teams so that they can be unified in the future.\n\nFigure 8.4: The DevOps team as a link between Dev and Ops during the organizational transition",
      "content_length": 2360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "DevOps processes in enterprise MSA system operations\n\nTeam communication, collaboration, energy, trust, and a solid understanding of the entire development cycle are all paramount to the new DevOps team’s success. Therefore, you must identify the right skills and people who can push the activities of the DevOps team forward. These skills may include, but are not limited to, coding skills, mastering DevOps and Continuous Integration/Continuous Development (CI/CD) tools, and automation.\n\nAs the organizational structure and the teams mature and become more familiar with the new methodologies, merging the old Dev, old Ops, and the interim DevOps teams into a single new DevOps team becomes essential. Staying in the interim stage too long is likely to create even more disruptions than using the traditional development cycle for developing the MSA system.\n\nThe size of the DevOps team can be as small as 3 engineers, and as large as 12, depending on the organization’s size, existing structure, and the effort being put into the organizational transformation. Usually, a number between 3 and 12 is ideal. Having a larger team is likely to create more challenges than benefits and start negatively impacting the team’s overall performance.\n\nBegin the process of transformation in a step-by-step manner, starting with infrastructure codification, the automation of infrastructure provisioning, source code version control, infrastructure monitoring, code build automation, deployment automation, test orchestration, cloud service management, and so on.\n\nWe know now how the organizational structure is relevant and important when embracing DevOps. We still need to understand some other details on the processes that will complement DevOps in order to achieve our goal of developing an efficient, high-quality MSA system with a short time-to- market and seamless updates.\n\nIn the following section, we will examine some other considerations that need to be taken into account when developing an MSA system.\n\nDevOps processes in enterprise MSA system operations\n\nMicroservices development is a fast-paced process and requires all other development processes to run at the same pace. Right from the beginning of the development of the MSA system, source code management and configuration management are needed to provide the correct support to the DevOps team. This is followed by code scans and unit test orchestration in the development environment.\n\nHaving specific standard methodologies and best practices applied among the different team members is essential to manage the efficiency and fast pace of the development cycle. The following discusses what the Agile methodology of development is and how it helps in DevOps operations, and the importance of automation in DevOps.\n\n141",
      "content_length": 2787,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "142\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nThe Agile methodology of development\n\nDefining and accomplishing DevOps processes go hand in hand with adopting a development methodology that can fully support and leverage the power of DevOps. Although there are many ways to apply DevOps methodologies within your organization, the Agile methodology is the one best suited for DevOps.\n\nThe Agile development methodology breaks down the main requirements into small consumable changes – stories and epics. These small, consumable increments help the team achieve short wins throughout the journey of handling the project from start to end.\n\nAs shown in Figure 8.5, the Agile team members meet periodically, typically every week or two, to plan, define, and agree on the epics and stories. These requirements are then put into a backlog and, until the next Agile team meeting, the team members work to deliver the requirements from that backlog:\n\nFigure 8.5: Sprint cycle in Agile development\n\nIn Agile development, the weekly or biweekly recurring meetings are called Sprint Planning Meetings, and the time between these meetings when developers are working on the backlog is called a sprint.\n\nIn order for team members to check on the status of each defined epic and story, they usually meet daily to examine the sprint backlog and refine whatever needs to be refined to ensure timely delivery. This daily meeting is called a Daily Scrum.\n\nThe Agile team handles continuously evolving user stories and requirements within a sprint cycle.",
      "content_length": 1561,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "DevOps processes in enterprise MSA system operations\n\nIn an endeavor to deliver a high-quality product at a fast pace and low cost, Agile teams apply the following principles:\n\nNo blocking time for day-end activities, such as building and deploying the latest code\n\n\n\nImmediate feedback on the code quality and functional quality of the latest code\n\nStrong control, precision monitoring, and continuous improvement of the daily activities of the development team\n\nFaster decision-making for accepting new stories, releasing developed stories, and mitigating risks\n\nA reduced feedback loop with the testers, end users, and customers\n\nRegular review and introspection of the development and delivery processes\n\nA development team abiding by the Agile manifesto and following all the Agile principles should always look for ways to remove unwanted roadblocks from their process model.\n\nThe Agile methodology of development can be applied to develop and deliver all types of software projects; however, it is more suited to the development of microservices-based applications. It is important to view the scope and structure of microservices to align them with Agile and DevOps practices.\n\nOne of the most important pillars of the Agile and DevOps process is the use of on-demand, needs- based resources. This is usually catered to by the use of a cloud-based infrastructure. All the resources required by the Agile teams developing microservices need to be provisioned promptly and in the right quantity or with enough capacity. Cloud infrastructure is best suited to these requirements. Resources can be scaled up and down based on need and demand.\n\nOn-demand cloud workloads needed during the DevOps cycles are not necessarily deployed on the organization’s private infrastructure; they may very well be deployed using a public cloud provider, or they may be deployed in a hybrid cloud fashion.\n\nAutomation\n\nWith the increase in the complexity of the IT infrastructure and MSA adoption and the demand for an Agile development cycle and short time-to-market, the need to streamline the infrastructure management processes becomes the most pressing need for any organization. A big part of managing an MSA’s infrastructure, DevOps, CI/CD, and Agile development is automation.\n\nAutomation provides immense benefits to modern organizations. A few of these benefits include, but are not limited to, the following:\n\nBetter human resource utilization: With automation in place, staff can focus on other activities that may not be automatable, hence optimizing the use of the organization’s workforce, scaling better on other projects, and distributing responsibilities according to the available and required skill sets.\n\n143",
      "content_length": 2717,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "144\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nBetter time-to-market and better business agility: An automated process can certainly save a lot of time that would be otherwise consumed by manual repetitive work and potential dependencies. A job that may traditionally take days can be done in minutes when automation is in place.\n\nHigher reliability and greater business continuity: Complex and time-consuming tasks are simplified into simple keystrokes or mouse clicks. Accordingly, human error is significantly minimized, and operational reliability is largely increased.\n\nBetter compliance: Compliance can be built into automation tools, providing better policy enforcement with minimum effort. Compliance includes industry compliance, best practices, and organizational standards as well. Industry standards may include the General Data Protection Regulation (GDPR), Payment Card Industry Data Security Standard (PCI DSS), Health Insurance Portability and Accountability Act (HIPAA), and Safeguard Computer Security Evaluation Matrix (SCSEM).\n\nAutomation is often used for the fast-paced and high-quality delivery of applications. DevOps is the key process that helps automate various phases of development and delivery. In fact, DevOps is the culture that helps organizations avoid repeated, time-consuming manual steps and efforts. There are various tools, frameworks, and processes within the ambit of DevOps that are needed for successful automation.\n\nMost of the challenges within DevOps and MSA operations cannot be addressed manually – hence, the need for automation in DevOps and MSA is extremely high. Automation is needed in every area of delivery, from the time the microservice is developed to the time the microservice is deployed in the production environment.\n\nFigure 8.6: The four pillars of DevOps",
      "content_length": 1843,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "Applying DevOps from the start to operations and maintenance\n\nIn essence, modern enterprise system development needs DevOps to be able to respond to the dynamic and constantly growing needs of organizations, and DevOps depends heavily on four pillars: MSA, Agile Development, CI/CD, and Automation. These four pillars, as shown in the preceding diagram, play a significant part in DevOps success, and hence, in the success of modern enterprise system development.\n\nMoreover, as we will discuss later in this chapter, AI applications are very hard to test and manage manually, and automation plays a big part in managing the entire DevOps cycle of AI applications.\n\nApplying DevOps from the start to operations and maintenance\n\nEvery step of a microservices rollout requires a corresponding DevOps step. The confluence of the microservices development process with the DevOps process helps empower the Dev and Ops teams. The following is a detailed look at different facets of the DevOps process.\n\nSource code version control\n\nThe Agile teams working on microservices require specific version control to be in place. Three aspects of version control need to be carefully defined for each microservice:\n\nThe setup and management of version control tools, such as Git, SVN, CVS, and Mercurial.\n\nThe version format and nomenclature for the application, such as a format to indicate the application version, the major-change version, the minor-change version, and the build or patch number – for example, version 2.3.11.7.\n\nThe branching strategy for the source code. This is extremely important for microservices development with multiple teams working on separate microservices. Teams need to create separate repositories for each microservice and fork out different branches for each major or minor enhancement.\n\nConfiguration management and everything as a code\n\nConfiguration management is the practice of managing changes systematically across various environments so that the functional and technical performance of the system is at its best. This includes all the environments needed to develop, test, deploy, and run the MSA system components.\n\nWith so many moving parts in an MSA enterprise system, it is essential to identify which parts of the system need their configuration to be maintained and managed. Once these parts have been identified, their configuration will need to be controlled and regularly audited to maintain the overall health of the entire MSA system.\n\n145",
      "content_length": 2482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "146\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nAs the DevOps process matures, and as the MSA system components mature, things become very complex to manage and configure manually, and automation becomes critical for smooth and successful configuration management.\n\nConfiguration management tools can automatically and seamlessly manage the different aspects of the system components. These tools make adjustments as needed during runtime and whenever else, and in accordance with the version of the application, the type of change, and the system load.\n\nOne of the objectives of DevOps is to codify all the aspects of development as well as deployment, including the infrastructure and the configuration. The entire environment can be built from the ground up and quickly provisioned using Infrastructure-as-a-Code (IaaC) and Configuration-as-a-Code (CaaC).\n\nIaaC and CaaC are essential components of configuration management. Both are descriptive files typically written in languages such as Ansible, Terraform, Puppet, Chef, or CloudFormation.\n\nWith IaaC and CaaC, DevOps teams can easily spin up new workloads for different purposes. Workloads can, for example, be configured for testing, specify the properties of each workload based on the test cases involved, and control deviations from the main workload parameters.\n\nCI/CD\n\nAs pointed out earlier in Chapter 1, CI/CD is an integral part of DevOps and plays the most important role in releasing MSA system updates. CI/CD ensures that the code is immediately and periodically built and pushed into the CI/CD pipeline for quick testing and feedback.\n\nAs shown in the following CI/CD pipeline diagram, developers focus primarily on working on the sprint backlog and push the code updates to the team repository, and it gets downloaded from there to the CI server.\n\nFigure 8.7: CI/CD pipeline and process flow",
      "content_length": 1887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "Applying DevOps from the start to operations and maintenance\n\nThe CI server automatically runs preset test cases against the code and then pushes the code to the testers upon it passing all the test cases. Should any of the automated tests fail, the code doesn’t move further along the pipeline, and an error report of all the test failures is sent back to the developers.\n\nIn contrast to the traditional development cycle, in which developers may find out about their code test results days or weeks after their code has been submitted for testing, in CI/CD, developers will get a report of their code problems within minutes. This early visibility into code errors gives developers the chance to immediately work on fixing these errors while working on the original code. Hence, they can continuously enhance the code for release and deployment.\n\nUpon the code successfully passing all CI server tests, the code is tested further by the DevOps team testers. Testers then either push the code to release and deployment if no errors are found or return it for further fixes and enhancements.\n\nThis CI/CD pipeline enables developers to make frequent code merges; do unit testing, integration testing, code scans, and smoke testing; release; and deploy multiple times every single day – something that is not remotely possible using a traditional development cycle.\n\nThe DevOps team needs to identify a tool that can manage the entire CI/CD pipeline. DevOps helps add hooks and steps to include external executables and scripts for performing additional activities during the code build and deployment. Some of the most common and widely used CI/CD tools include Jenkins, Bamboo, and CircleCI.\n\nCode quality assurance\n\nEnsuring high-quality code, both in terms of coding standards and security vulnerabilities, is another important activity within DevOps. This is in addition to ensuring the accuracy of the application’s business logic itself.\n\nCode quality touches upon the concept of static and dynamic analysis of the code. Static analysis of the code is performed on the code itself before it gets executed. It is meant to uncover code smells, dirty code, vulnerable libraries, malicious openings in the code, and violations of code standards or best practices.\n\nDynamic code analysis is performed on the application during or after its execution. It is meant to uncover runtime errors due to the load, unexpected input, or unexpected runtime conditions in general.\n\nMany tools that help perform code scans as part of CI/CD are available. These include, but are not limited to, SonarQube, Fortify SCA, and Raxis.\n\nImportant note Testing AI applications is more challenging than testing regular applications. Certain aspects of AI applications do not exist in regular applications.\n\n147",
      "content_length": 2788,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "148\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nAI applications are non-deterministic – how they will behave in real situations is somewhat uncertain. Accordingly, expecting a specific outcome during AI application testing may not be viable. It may very well happen that the application being tested produces different outcomes with the same input or test criteria.\n\nMost AI applications are as good as their training data quality, which makes AI applications subject to training data bias or unconscious bias. Imagine, for example, you are writing an AI module to predict home prices in any part of the United States, but your training data is 90% from a specific region within a specific state. Your AI model will accordingly be biased toward the area from which 90% of the training data came, so testing the AI application may require running tests against the training data itself. This may sound easy in this home price prediction case, but how would you make sense of other pieces of training data in more complex situations?\n\nLet’s assume that we can accurately test AI/DL applications despite all the training data challenges and their non-deterministic behavior. AI/DL applications constantly learn, train, and change their behavior, so by the time the code is running in production, the application is already learning and changing its behavior. The tests that have been completed a day or a couple of days earlier may not be valid anymore.\n\nThere are, of course, ways to overcome all these challenges. First of all, you will need to curate and validate the training data. You may need to perform both automated and manual tasks to validate the training data, including checking for data biases, data skews, distribution levels, and so on.\n\nWe will also need to test the AI algorithm and how the regression model performs against different sets of test data. The variance and mean square error of the model will also need to be examined and analyzed.\n\nAI application testing tools are available on the market today and grow in number every day. The quality of these tools is constantly improving and can be a huge help to DevOps teams. AI testing tools are usually specialized based on the AI algorithms being used. Examples of different AI test tools include, but are not limited to, Applitools, Sauce Labs, and Testim.\n\nMonitoring\n\nWith the advent of DevOps, standard monitoring has upgraded to continuous monitoring and covers the entire development cycle, from planning to deployment and operations.\n\nMonitoring covers different aspects of the DevOps process and the components needed for the entire application to be developed, tested, deployed, and released, as well as for post-release operations to ensue. This includes infrastructure monitoring and the application itself.\n\nInfrastructure monitoring includes the on-premises infrastructure, virtual cloud environments, networks, communications, and security. Application monitoring, on the other hand, involves performance, scalability, availability, and reliability. Resource monitoring includes the management and distribution of resources across multiple pod replicas within and beyond the physical or virtual workloads.",
      "content_length": 3216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "Applying DevOps from the start to operations and maintenance\n\nDevOps monitoring helps team members respond to any operational issues that arise during the DevOps pre-release or post-release cycles, hence enabling the DevOps team to be able to rectify, readjust, and make any necessary changes during the CI/CD pipeline.\n\nIdeally, monitoring alerts trigger automatic actions to try to respond and fix a problem that has been detected. However, knowing that’s not always possible, manual intervention is usually needed. Monitoring helps the DevOps team shift left to earlier stages in the development cycle to enhance their test cases, and accordingly, increase the application quality and minimize operational problems later on in the development cycle.\n\nAI algorithms, as discussed earlier in Chapter 7, and as we will give more examples of later in this book, can detect any application behavior anomalies and automatically try to self-heal to prevent application operations from being disrupted.\n\nThere are many environment-specific tools available for DevOps monitoring, including Nagios, Prometheus, Splunk, Dynatrace, and AWS CloudWatch for AWS cloud environments.\n\nDisaster management\n\nDisaster management is an important yet often overlooked part of the DevOps process. In most cases, application recovery is seen as an extended part of the deployment process. In the cloud, it is generally considered to be an offshoot of configuring availability zones and regions for hosting an application instead of a full-fledged environment challenge.\n\nIn the case of microservices, identifying a disaster is a greater challenge than averting, mitigating, or managing it. Luckily, the CI/CD environment itself can be leveraged to test and simulate disaster scenarios. Moreover, the use of external repositories can be leveraged to recover code down to specific version numbers.\n\nNevertheless, setting up a completely separate set of environment replicas in different geographical locations, setting automatic failover, and load balancers in between can be great ways of maintaining business continuity and an uninterrupted CI/CD pipeline.\n\nUsing IaaC and CaaC tools to automate recovery is extremely helpful in bringing your applications and systems back online in minimal time in case of interruption.\n\nYou still need to define an incident response playbook as part of your DevOps. This playbook should include a detailed plan of what should be executed in each scenario. For example, a response to a natural disaster is likely different from a response to a data breach incident. The playbook needs to have different scenarios and a list of roles and procedures that need to be taken to prevent or minimize system interruptions or data loss.\n\n149",
      "content_length": 2745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "150\n\nThe Role of DevOps in Building Intelligent MSA Enterprise Systems\n\nSummary\n\nFor MSA systems to achieve the goals for which they were created, a certain set of methodologies will need to go hand in hand with developing an MSA system. In this chapter, we discussed a few of the most critical practices to embrace when developing an MSA system: the Agile methodology of development, DevOps processes and practices, and CI/CD pipeline management.\n\nWe also discussed how important it is to set up a DevOps team for managing microservices. We have given examples of tools to use to apply and manage DevOps when building our MSA system.\n\nIn the next chapter, we will take our first step in building an intelligent MSA system. We will talk about Docker, what it is, and why it’s relevant. We will also create isolated and independent virtual environments using Docker and then link these environments (or containers) together to deliver a simple functional part of our MSA system.",
      "content_length": 977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "9 Building an MSA with Docker Containers\n\nIn the previous chapter, we discussed how to apply the DevOps process of building and running MSA systems, and the importance of aligning the organizational structure with DevOps.\n\nWe also highlighted the importance of embracing automation and adapting agile development methodologies throughout the MSA project life cycle, and throughout the CI/CD operations.\n\nThis chapter will cover what a container is, how to install containers, how to work with them, and how to handle the data flow between containers to build a simple project. We will use Docker as our platform since it is one of the most popular and widely used platforms in the field today.\n\nThe following topics will be covered in this chapter:\n\nWhat are containers anyway, and why use them?\n\n\n\nInstalling Docker\n\nCreating ABC-MSA containers\n\nABC-MSA microservices inter-communication\n\nWhat are containers anyway, and why use them?\n\nA container is defined as an operating system-level virtualization artifact, created by grouping different finite compute resources into a self-contained execution environment.",
      "content_length": 1113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "152\n\nBuilding an MSA with Docker Containers\n\nAs shown in the following figure of a container ship, containers are self-contained units, independent from any other container in the ship. The ship is the engine that is used to carry and transport containers:\n\nFigure 9.1: A container ship\n\nSimilarly, the idea with containers is to create operating system-level virtualization. This means that, from within the kernel, you group different physical machine resources, applications, and I/O functions into a self-contained execution environment. Each of these self-contained resources forms a single container, hence the name container. The Container Engine is similar to the ship in the preceding example, where the container engine is used to carry, run, and transport the containers.\n\nContainers have existed for a long time and can be traced back to Unix’s chroot in the late 1970s and early 1980s, and before we even came to learn about what we call today a hypervisor. A hypervisor is a component that enables us to spin up virtual machines (VMs).\n\nUnix’s chroot evolved later in the 1990s to Linux containers or what we call LXC, and then to Solaris Zones in the early 2000s. These concepts started to evolve with time from cgroups (originally developed by Google) and namespaces (developed by IBM) in to the container engines we see today, such as Docker, Rkt, CRI-O, Containers, Microsoft Hyper-V Containers, and more.\n\nAlthough there are similarities between containers and VMs, both still have a few fundamental differences.\n\nAs shown in the following diagram, containers share the same kernel of the host operating system but isolate and limit the allocated resources, giving us something that feels like a VM but that’s much more lightweight in terms of resources:",
      "content_length": 1773,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "What are containers anyway, and why use them?\n\nFigure 9.2: VMs versus containers\n\nIn hypervisor virtualization, each VM will have to have its own virtual hardware and its own guest operating system. In addition to all that, there is a great deal of emulation taking place in the hypervisor. Accordingly, each VM needs much more resources compared to what a container needs. Resources include CPU cycles, memory, storage, and more. Moreover, you are likely to have duplicates of the same guest OS deployed on multiple VMs for the VM to deliver the required function, thus even more overhead and waste of resources.\n\nFigure 9.2 shows the hypervisor deployed on top of a host OS. A more common hypervisor virtualization model, however, is deploying the bare-metal hypervisor on the hardware directly. In either case, the overhead is are still significantly higher than deploying containers.\n\nThe lightweight nature of containers enables companies to run many more virtualized environments in the data center compared to VMs. Since containers share resources much more efficiently than VMs, and with finite physical resources in data centers, containers largely increase the capacity of the data center infrastructure, which means containers become a better choice in hosting applications, especially in our case of MSA.\n\nContainer performance is another thing to look at. With containers, I/O virtual drivers’ communication, hardware emulation, and resources overhead are minimal, completely contrary to the case in the hypervisor virtualization environment. Accordingly, containers generally outperform VMs. Containers boot in 1-3 seconds compared to minutes in the case of VMs.\n\nApplications running on a container can directly interact with and access the hardware. In the case of hypervisor virtualization, there is always a hypervisor between the application and the VM (unless a hypervisor bypass is enabled, which has its own limitations).\n\n153",
      "content_length": 1948,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "154\n\nBuilding an MSA with Docker Containers\n\nWith containers, you can package the application with all of its dependencies in a contained environment that’s reusable and completely portable to any other platform. That’s one of the most important features of containers. Developers can develop an application on their development servers, ship it to the testing environment, then staging and production, and run the application without having to worry about portability and dependency issues.\n\nImportant note For all the aforementioned reasons, the most popular deployment model of microservices is the container-per-service model. This is where each microservice of the MSA is deployed on a single container dedicated to running that particular application.\n\nThe other important difference between containers and VMs is security. Since containers use the same kernel, multiple containers may very well access the same kernel subsystem outside the boundaries of the container. This means a container could gain access to other containers running on the same physical host. A single application with root access, for example, could access any other container data.\n\nThere are many ways to harden the security of containers, but none of these techniques would help containers match the VM’s total isolation security.\n\nThere are cases, of course, where using VMs would be a better option than using containers. Or in some scenarios, a mix of both VMs and containers would be the most appropriate deployment model. It all depends on the use case, the application, or the system you are deploying in your organization.\n\nIn a multi-tenant environment, where complete workload isolation is necessary, using VMs would be a better choice. Or, if you are trying to build an R&D environment for hosting critical intellectual capital, or highly confidential data or applications, a complete workload isolation will also be necessary. Therefore, in this case, using VMs would be the better option.\n\nFor our MSA example, we need a very lightweight, fast-starting, highly portable, and high-performing virtualization environment to build our MSA system. Hence, containers, with a container-per- microservice deployment model, are the better choice in our scenario. Each of the MSA system’s microservices would be deployed in its own container and would have its own development team, development cycle, QA cycle, updates, run life cycle, and release cycle.",
      "content_length": 2440,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "What are containers anyway, and why use them?\n\nThe following table summarizes the differences between containers and hypervisor virtualization:\n\nContainers\n\nHypervisor VM’s\n\nResource Usage and Overhead\n\nLightweight, Please overhead, and more efficient use of resources\n\nHigh overhead and resource-intensive\n\nContainer and Application Size\n\nAverages 5-20 MB\n\nMeasured in 100s of MB or GB\n\nPerformance\n\nHigh performance\n\nLower performance\n\nScalability\n\nEasy to scale out/high horizontal scaling\n\nScaling out is harder and consumes resources\n\nBootup Time\n\nVery short startup time (1-3 seconds)\n\nStartup time is in minutes\n\nPortability\n\nSystem-agnostic and highly portable\n\nPortability is limited\n\nDevOps and CI/CD Suitability\n\nEnables more agile DevOps and smoother CI/CD\n\nCould slow down CI/CD operations\n\nHost Hardware Access\n\nApplications access HW directly\n\nNo direct access to HW\n\nSecurity\n\nLess secure; shares the same kernel\n\nMore secure; each VM has its own OS kernel\n\nTable 9.1: Differences between containers and Hypervisor VMs\n\nDespite the many options we currently have in choosing a container engine, Docker is by far the most popular engine used today, to the extent that Docker today is synonymous with containers. That’s the main reason why we have chosen to work with Docker as our container’s engine in this book.\n\nDocker is also ideal for agile DevOps and CI/CD operations. In a CI/CD environment, the time between building a Docker image to the time it is up and running in the production environment is usually around 1-5 minutes in total:\n\n155",
      "content_length": 1562,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "156\n\nBuilding an MSA with Docker Containers\n\nFigure 9.3: Docker Engine container virtualization\n\nFigure 9.3 shows Docker Engine installed on the host operating system to enable the containerization of microservices or applications in general.\n\nDocker in itself may not be sufficient to manage all the CI/CD operations. Organizations usually complement Docker by using a clustering technology such as Kubernetes or Marathon to smoothly deploy, manage, and operate the containers within the cluster in which your system is running. However, in this book, we will focus on Docker itself and how to use Docker to build our MSA system.\n\nAlso, to move, test, and deploy containers, we will need to have a repository to save these containers and be able to move them to different environments. Many tools can help with that, with Docker Hub and GitHub being two of the most commonly used repositories. For our project, we will use GitHub as our project repository.\n\nSo far, we have covered what containers are, the difference between containers and VMs, and why we prefer to use containers in MSA. In the next section, we will explain the different components of Docker, how to install Docker, and how to work with Docker’s components to create a system’s microservices.",
      "content_length": 1263,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "Installing Docker\n\nInstalling Docker\n\nWe will start this section by talking about Docker installation. Then, we will cover the main components of Docker, the purpose of each component, and how these components relate to each other. This section will help us prepare the environment that we will use later for our ABC-MSA demo project.\n\nImportant note To maximize your hands-on learning experience, you need to follow all of our hands-on installation steps. But before doing so, please make sure you have a physical or virtual host available for the Docker installation demo before we dive deeper into this section. A virtual host can be created using virtualization software such as VirtualBox or VMWare.\n\nAlthough you can install Docker on Windows or Mac, in our demo, we will use an Ubuntu Server 22.x Linux environment to install Docker Community Edition (CE). We suggest you use a similar environment to be able to follow our installation steps.\n\nDocker Engine installation\n\nNow that we know the main components of Docker, let’s take a step back and learn how to install Docker and create different Docker images for the ABC-MSA system.\n\nThe best way to install Docker Engine is to follow Docker’s official installation guide from Docker Docs at https://docs.docker.com/engine/install/. Pick your server system platform installation guide from the list.\n\nYou may also want to install Docker Desktop on your workstation. Docker Desktop is available for download from the same installation guide referred to previously.\n\nAfter the installation is completed, verify Docker’s functionality by running the following command:\n\n$ docker --version Docker version 20.10.18, build b40c2f6 $\n\nAnd,\n\n$ docker run hello-world\n\nHello from Docker! This message shows that your installation appears to be working correctly. : :\n\n157",
      "content_length": 1820,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "158\n\nBuilding an MSA with Docker Containers\n\nYou may need root privileges to issue the Docker commands successfully.\n\nNow that we have installed Docker, let’s go over the main components of Docker and how to use each.\n\nDocker components\n\nThere are four main components of Docker: the Docker file, the Docker image, the Docker container, and the Docker volume. The following is a brief description of each of these components.\n\nThe Docker file\n\nThe Docker file is a text file that works as a manifest that describes how the Docker image should be built. The Docker file specifies the base image that will be used to create the Docker image. So, for example, if you were to use the latest Ubuntu version as your base Linux image for the container, you would have the following line specified at the top of your Docker file:\n\nFROM ubuntu\n\nNotice that ubuntu is not tagged with any version number, which will instruct Docker to pull the latest version available for that base image. If you prefer to use CentOS version 7.0, for example, you must then tag the base image with the version number, as shown in the following line:\n\nFROM centos:7\n\nThe specific image tag can be found on Docker Hub. Docker Hub is a public repository that stores many free Docker official images for reuse by Docker users. Among many others, base images could be Linux, Windows, Node.js, Redis, Postgres, or other relational DB images.\n\nAfter you specify the base operating system image, you can use the RUN command to run the commands that you would like to execute during the Docker image creation. These are regular shell commands that are usually issued to download and install packages and libraries that will be used in your Docker image.\n\nThe Docker file has to be named Dockerfile for Docker to be able to use it. The following is a simple Dockerfile example:\n\nFigure 9.4: A Docker file (Dockerfile) example",
      "content_length": 1888,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "Installing Docker\n\nThe preceding sample Dockerfile does the following:\n\n1. Uses Ubuntu version 22.10 as the base image to run on the container that will be created later.\n\n2. Fetches the latest packages list.\n\n3.\n\nInstalls Python version 3 and the PIP Python package management system.\n\n4.\n\nInstalls a package called Ansible (Ansible is an automation tool).\n\nThe Docker image\n\nOnce you have finished composing your Dockerfile, you will need to save it as a Dockerfile to be able to use it to create the Docker image.\n\nA Docker image is a binary file that works as a template with a set of instructions on how a Docker container should be created.\n\nPlease note that a Docker image can either be created from the Dockerfile, as we are explaining here, or downloaded from a public or private repository such as Docker Hub or GitHub.\n\nTo build a Docker image, use the following command while pointing at the Dockerfile location. The following example assumes the Dockerfile is located in the user’s home directory:\n\n$ docker build –t packt_demo_image ~/\n\nThe preceding command will build an image called packt_demo_image. This image will be used later to create the container with the specs defined in the Dockerfile.\n\nThe -t option means tty, which attaches a terminal to the container.\n\nTo verify that your image has been created, use the following command:\n\n$ docker image ls\n\nYou can add the -a option to the end of the proceeding command to show all images created on the host machine.\n\nIn CI/CD operations, the images that are built are usually shared in a public or private repository so that they’re available to the project team, or even the public in some cases.\n\nThe Docker container\n\nThe last step is to run a container based on the Docker image you created (or pulled from the image repository). To run a container, use the following command:\n\n$ docker run packt_demo_image\n\n159",
      "content_length": 1887,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "160\n\nBuilding an MSA with Docker Containers\n\nTo verify that the container is running, use the following command:\n\n$ docker container ls\n\nThe preceding command will show only the running containers. To show other containers on the host machine, add the -a option to the end of the command.\n\nYou can also use the older version of the preceding command to verify that the container is running:\n\n$ docker ps\n\nThe following diagram shows the relationship between all four Docker components and summarizes the entire process of running a container. First, we create a Dockerfile. Then, we use that file to create the Docker image. The Docker image can then be used to create the Docker container(s) locally, or first uploaded to a private or public repository where others can download and create their Docker container(s):\n\nFigure 9.5: Docker components\n\nDocker containers have a life cycle of their own – they can run for a specific task with no regard for what their previous state is, and once that specific task is completed, the Docker container automatically terminates.",
      "content_length": 1071,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "Installing Docker\n\nIn other cases, containers need to be aware of their previous status. If so, they will need to be persistent to preserve the container data after its termination. That’s when Docker volumes become very handy. Next, we will talk about what a Docker volume is and how it can be created.\n\nThe Docker volume\n\nDocker volumes are a form of storage that a Docker container can be attached to. Containers are attached to volumes to read and write persistent data, which are necessary for the function of the container.\n\nTo elaborate more, consider the Docker container for the Customer Management microservice (customer_management). If you need to create a new customer in the customer_management container, you will need to update the local data store installed in that container. If the container is not persistent, once the container terminates, all data created or changed inside that container will be lost.\n\nTo avoid this problem, we will need to create a Docker volume and attach the container to that volume. The container itself can then run and update whatever data it needs to update in its volume, and then terminate. When it starts the next time, it gets instantiated with all the previous statuses and data it had before the last termination.\n\nTo create a Docker volume for the customer_management container, for example, use the following command:\n\n$ docker volume create customer_management_volume\n\nThe following command will list all volumes created on our host machine and verify the volume we have just created:\n\n$ docker volume ls\n\nOnce we create the volume, Docker mounts a local drive space on the host machine to preserve the container’s data and its mounted filesystem.\n\nTo show more details about the volume, including the volume’s name, the local host and the container’s target mount locations, and the date and time of the volume’s creation, use the docker volume inspect or docker inspect command, as follows:\n\n$ docker volume inspect customer_management_volume [ { \"CreatedAt\": \"2022-10-14T22:24:46Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/customer_ management_volume/_data\",\n\n161",
      "content_length": 2155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "162\n\nBuilding an MSA with Docker Containers\n\n\"Name\": \"customer_management_volume\", \"Options\": {}, \"Scope\": \"local\" } ]\n\nAssuming we have previously created the packt_demo_image image, to create the persistent customer_management container, we will need to attach the container to the volume we have just created using the mount points shown in the docker volume inspect command’s output. The following command will create the container, attach the volume to the container, and then run the container:\n\n$ docker run -itd --mount source=customer_management_ volume,target=/app_data --name customer_management_container packt_demo_image\n\nThe it option in the docker run command is for interactive tty mode, and the d option is for running the container in the background.\n\n/app_data is an absolute path within the container that’s mounted to the local host’s mount point. From the preceding inspect data shown, the /var/lib/docker/volumes/customer_ management_volume/_data mount point is mapped to /app_data in the container.\n\nTo verify that the container is running, use the following command:\n\n$ docker container ls\n\nIf the container terminates for whatever reason, use the -a option at the end of the preceding command to show the available container on the host. You can use the docker container start or docker container stop command, followed by the container’s name, to run or terminate any of the available containers you built on that host.\n\nNow that we have installed Docker Engine and understand the different components of Docker, we will go over how to create the main ABC-MSA containers as microservices and provide an example of how these microservices talk to each other.\n\nCreating ABC-MSA containers\n\nIn our ABC-MSA system, we are adopting a container-per-microservice approach. Therefore, we need to identify the main containers we will build, the components we need for each container in our ABC-MSA system, and then build the necessary Dockerfile(s) to use.",
      "content_length": 1974,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Creating ABC-MSA containers\n\nWe are building our microservice applications using Flask. Flask is a Web Server Gateway Interface (WSGI) micro-framework that enables applications to respond to API calls in a simple, flexible, and scalable manner. We won’t discuss our applications’ code in this book, but the code is available on our GitHub with detailed documentation for your reference.\n\nIn this section, we will explain how we build our ABC-MSA Dockerfile(s), images, and microservices, how we will start to listen to API calls in each container, and how the system’s microservices will be able to communicate with each other.\n\nFor demo purposes, we will use port HTTP/8080 in the container to listen to HTTP API requests. The production environment should use HTTPS/443 and consider the tomcat server for handling all web connections.\n\nThe following is only part of the full system container setup. All the ABC-MSA system’s created files and Docker images can be found in our GitHub repository at https://github.com/ PacktPublishing/Machine-Learning-in-Microservices.\n\nABC-MSA containers\n\nThe following are the services we have previously identified for our ABC-MSA system:\n\n1. API Gateway\n\n2. A frontend web dashboard interface\n\n3. Customer Management\n\n4. Product Management\n\n5. Order Management\n\n6.\n\nInventory Management\n\n7. Courier Management\n\n8. Shipping Management\n\n9. Payment Authorization\n\n10. Notification Management\n\n11. Aggregator: “Product Ordered Qty”\n\n12. Management and Orchestration\n\nWe can code and build each of these services from scratch, but the good news is that we don’t have to. Docker Hub offers a rich library with many Docker images that we can leverage in building our microservices. Docker Hub can be accessed at https://hub.docker.com/.\n\n163",
      "content_length": 1772,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "164\n\nBuilding an MSA with Docker Containers\n\nWe will not go over each of these services. Instead, we will focus on the ones that provide different development and deployment approaches. Some of the services are already available through Docker Hub, and some others are similar, so one example of these will suffice. Nevertheless, all the project files will be made available in this book’s GitHub repository.\n\nAPI Gateway\n\nMany open source and commercial API gateways can be pulled from different internet repositories, including Tyk, API Umbrella, WSO2, Apiman, Kong, and Fusio, to name a few. We will use Tyk in our ABC-MSA system since it is easy to use, has comprehensive features including authentication and service discovery, and is 100% an open source product with no feature restrictions.\n\nTo install a Tyk Docker container, just follow the instructions at https://tyk.io/docs/ tyk-oss/ce-docker/.\n\nBy default, the Tyk API gateway listens to TCP port 8080. To verify your installation, issue an API call test to Tyk using the curl command, as follows:\n\n$ curl localhost:8080/hello {\"status\":\"pass\",\"version\":\"4.1.0\",\"description\":\"Tyk GW\"}\n\nIf Tyk has been successfully installed and is running on your host, you should get a dictionary output stating Tyk’s status and the current version, as shown in the preceding command output.\n\nYou can also verify that the Tyk Docker image and container were created successfully using the following commands:\n\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE redis 6.2.7-alpine 48822f443672 3 days ago 25.5MB docker.tyk.io/tyk-gateway/tyk-gateway v4.1.0 0c21a95236de 8 weeks ago 341MB hello-world latest feb5d9fea6a5 12 months ago 13.3kB $ $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ac3ac1802647 docker.tyk.io/tyk-gateway/tyk-gateway:v4.1.0",
      "content_length": 1826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "Creating ABC-MSA containers\n\n\"/opt/tyk-gateway/ty…\" 54 minutes ago Up 54 minutes 0.0.0.0:8080->8080/tcp, :::8080->8080/tcp tyk-gateway-docker_tyk-gateway_1 9e0f1ecfb148 redis:6.2.7-alpine \"docker-entrypoint.s…\" 54 minutes ago Up 54 minutes 0.0.0.0:6379->6379/tcp, :::6379->6379/tcp tyk-gateway-docker_tyk-redis_1\n\nWe can see the tyk image details in the preceding command output, as well as the running container and what port it is listening to. We can also see a Redis image and container. This is because Redis is a prerequisite for Tyk and is included in the Tyk installation package.\n\nThe Customer Management microservice as an example\n\nThe Customer Management, Product Management, Order Management, Inventory Management, Courier Management, Shipping Management, Payment Authorization, and Notification Management microservices are all similar in terms of how we can build and deploy the container. In this section, we will learn how to create an image that we can use to create a system microservice. We have picked the Customer Management microservice as an example.\n\nAs mentioned earlier, for these microservices to communicate with the API gateway or any other components in the ABC-MSA system, we need to have Flask installed and running, listening to port HTTP/8080 in the running container.\n\nWe also need an internal data store for our application to use and manage. And since our code will be written in Python, we need to have Python installed as well. All these required components, along with some essential dependency packages, need to be specified in our Dockerfile.\n\nNow, we need to write the Dockerfile required for creating the microservice image that we will use to create the microservice container. Each ABC-MSA container should have its own development cycle and be deployed either using the CI/CD cycle we discussed in Chapter 8 or uploaded manually to the team repository.\n\nThe following is an example of the Dockerfile that’s required for creating the Customer Management image:\n\n# Docker File for \"customer_management\" microservice FROM ubuntu\n\n# Install some dependencies/packages RUN apt-get install -y apt-transport-https RUN apt-get update RUN apt-get install -y net-tools mysql-server python3 pip git build-essential curl wget vim software-properties-common;\n\n165",
      "content_length": 2297,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "166\n\nBuilding an MSA with Docker Containers\n\n# Install OpenJDK RUN apt-get update && \\ apt-get install -y default-jdk ENV JAVA_HOME /usr/lib/jvm/java-11-openjdk-amd64/\n\n# Install Flask to run our application and respond to API calls RUN pip install -U flask\n\n# Expose port TCP/8080 to listen the container's application/ flask API calls EXPOSE 8080\n\n# Create the /app_data directory and make it the working directory in the container RUN mkdir /app_data WORKDIR /app_data ENV PATH $PATH:/app_data\n\n# Download the microservice app code from GitHub repo ENV GIT_DISCOVERY_ACROSS_FILESYSTEM 1 RUN git config --global init.defaultBranch main RUN git init RUN git remote add origin https://github.com/mohameosam/abc_ msa.git RUN git config core.sparseCheckout true RUN echo \"/microservices/customer_management/\" > /app_data/. git/info/sparse-checkout RUN git pull origin main\n\n# Initialize the flask app ENV FLASK_APP /app_data/microservices/customer_management/ customer_management_ms.py\n\n# Specify a mount point in the container VOLUME /app_data\n\n# Start mysql & flask services and available bash sheel",
      "content_length": 1099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "Creating ABC-MSA containers\n\nRUN chmod +x /app_data/microservices/customer_management/start_ services CMD /app_data/microservices/customer_management/start_services && bash\n\nThe aforementioned Dockerfile specifies what the Customer Management Docker image should look like. The following are some insights into what each of the lines in the file will do:\n\n1. Specify Ubuntu as the Linux operating system that will be used in the Customer Management container.\n\n2.\n\nInstall some required packages:\n\n MySQL (required for our application)\n\n Python (required for our application)\n\n pip (required to be able to install Flask)\n\n The rest are some other tools needed for troubleshooting (optional)\n\n3.\n\nInstall Flask (required for our application).\n\n4. Expose TCP/HTTP port 8080 for Flask to listen to API calls.\n\n5. Create a working directory in the container to act as the mount point for saving the container’s data.\n\n6. Download the Customer Management application code from our GitHub repository.\n\n7. Set an environment variable to let Flask know what application it will use when responding to API calls.\n\n8. Use our downloaded start_services shell script to start Flask and MySQL in the container.\n\nThe start_services shell script contains the following commands:\n\nflask run -h 0.0.0.0 -p 8080 & usermod -d /var/lib/mysql/ mysql service mysql start\n\nThe first line enables Flask to listen to port 8080 on all the host network interfaces. This is OK in the development and testing environment. In the production environment, however, Flask should only be available on the localhost 127.0.0.1 network interface to limit API access to the local environment. Also, for better security, port HTTPS/443 should be used in API calls instead.\n\nAssuming the Dockerfile has been placed in the current user home directory, we now need to create our Customer Management microservice/container from the Dockerfile:\n\n$ docker build -t abc_msa_customer_management ~/\n\n167",
      "content_length": 1959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "168\n\nBuilding an MSA with Docker Containers\n\nDocker will take a few minutes to finish creating the image. Once all the Dockerfile steps have been completed, you should see the following command as the last line of the docker build command’s output:\n\nSuccessfully tagged abc_msa_customer_management:latest\n\nThis signals a successful completion. Now, we can use the docker image ls command to verify that the abc_msa_customer_management image has been created successfully.\n\nThe last step is creating the container. Since the application will configure and update the MySQL database, we need to create a persistent container to retain all the changes.\n\nSimilar to what we explained earlier, we will use the docker run command to create the Customer Management container, as follows:\n\n$ docker run -itd -p 8003:8080 --mount source=customer_ management_volume,target=/app_data --name customer_management_ container abc_msa_customer_management\n\nThe p option is used to “publish” and map the ports that the container listens to with the ports the host machine listens to. So, the host machine will be listening to port 8003 for HTTP/8080 requests on the container.\n\nWe have chosen 8003 to standardize the way the host listens to the container’s API call requests.\n\nRemember that each container has a TCP stack that is different from the host’s TCP stack. So, the TCP HTTP/8080 port is only local within the container itself, but outside that particular container’s environment, that TCP HTTP/8080 port is different from the TCP HTTP/8080 port available on any other container or on the host machine itself.\n\nTo access that port from outside the realm of the customer_management container, you need to map the customer_management container’s TCP HTTP/8080 port to a specific port on the host machine.\n\nSince we need to map the local TCP HTTP/8080 port of each of the 12 containers we identified earlier, we decided to follow a specific pattern. Map the TCP/80nn port on the host machine to each local TCP HTTP/8080 of each container. Here, nn is the container’s number.\n\nFigure 9.6 shows how some of the ABC-MSA container’s TCP HTTP/8080 ports are mapped on the host machine.\n\nWe don’t have to run all the containers on a single host. The system containers could be scattered across different hosts, depending on many factors, such as how critical the service/application running on the container is, how the system is designed, the desired overall redundancy, and so on:",
      "content_length": 2464,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "Creating ABC-MSA containers\n\nFigure 9.6: The container’s local port mappings to the host machine’s TCP stack\n\nNow, verify that the container is running using the following command:\n\n$ docker container ls\n\nThe following command will allow you to connect to the container’s bash shell using the root privilege (a user ID of 0, as specified in the command):\n\n$ docker exec -u 0 -it customer_management_container bash\n\nThat’s all for the Customer Management microservice. In the same manner, we can create the rest of the ABC-MSA containers. We just need to make sure we use appropriate corresponding names for the other microservice’s containers and volumes and map to the right TCP/80nn port number on the host machine.\n\nThe frontend web dashboard interface\n\nThe dashboard is the main component of the user interface (UI) interaction and interacts with all services offered to the user. In our ABC-MSA example, we created a simple cart application where the user can place products in the cart and place an order.\n\n169",
      "content_length": 1016,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "170\n\nBuilding an MSA with Docker Containers\n\nThe Dashboard container is built the same way the customer_management container is built, as shown in the previous section. The main difference between both is the additional web server that we will need to have on the Dashboard microservice, and the ports to be exposed on the container. The Dashboard’s Dockerfile should be changed accordingly.\n\nLike all the containers we are building, the container’s local TCP port that listens to API calls is TCP HTTP/8080, and the host-mapped TCP port in the dashboard container case should be TCP/8002.\n\nThe Dashboard container will still need to listen to HTTP/80 for user web UI requests. Unless the host machine is running another application or web page on HTTP/80 port, we should be OK to use that port.\n\nNow, we need to map the HTTP/80 port on the host machine, as shown in the following docker run command:\n\n$ docker run -itd -p 8002:8080 -p 80:80 \\ --mount source=dashboard_volume,target=/app_data \\ --name dashboard_container abc_msa_dashboard\n\nThis command has an additional p option to map the HTTP/80 port on the container with the HTTP/80 port on the host machine. abc_msa_dashboard is the Dashboard microservice image.\n\nManaging your system’s containers\n\nAs you saw in the preceding examples, the docker run command can get lengthy and messy. Docker Compose helps us manage the deployment of containers. With Docker Compose, it is much easier to manage the deployment of the containers, change deployment parameters, include all system containers in a single YAML file, and specify the order of the containers’ deployment and dependencies.\n\nThe following is a sample YAML file for initializing three of the ABC-MSA containers, as we did with the docker run commands earlier, but in a more organized and structured YAML way:\n\n# Docker Compose File abc_msa.yaml version: \"3.9\" services:\n\ncustomer_management_container: image: abc_msa_customer_management ports: - \"8003:8080\" volumes: - customer_management_volume:/app_data",
      "content_length": 2021,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "Creating ABC-MSA containers\n\nproduct_management_container: image: abc_msa_product_management ports: - \"8004:8080\" volumes: - product_management_volume:/app_data\n\ndashboard: image: abc_msa_dashboard ports: - \"8002:8080\" - \"80:80\" volumes: - dashboard_volume:/app_data depends_on: - customer_management_container - product_management_container\n\nvolumes: customer_management_volume: product_management_volume: dashboard_volume:\n\nThe following command runs the Docker Compose .yaml file:\n\n$ docker-compose -f abc_msa.yaml up &\n\nThe f option is used to specify the YAML file’s name, and the & option is used to run the containers in the shell’s background.\n\nIn this section, we showed you how to create some of the ABC-MSA images and containers. The ABC-MSA containers are now ready to communicate with each other either directly or, as we will show later in this book, through the API Gateway.\n\nIn the next section, we will learn how we can use the containers we created, how we can issue API calls to them, and what response we should expect.\n\n171",
      "content_length": 1044,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "172\n\nBuilding an MSA with Docker Containers\n\nABC-MSA microservice inter-communication\n\nIn this section, we will learn how to expose APIs from containers and how containers communicate with API consumers.\n\nThe microservice application code for each container is available in the ABC-MSA project on GitHub. We recommend that you download the code to your local test environment to be able to get some hands-on experience when following the steps we will cover in this section.\n\nThere are two main ways for containers to communicate with each other. One is by using the container’s name in a Docker network, and the other is by using the container’s IP and TCP port. The following are some of the details you need to know about to be able to configure your containers to communicate with each other.\n\nThe Docker network\n\nWhen we have containers running on the same host, containers can communicate with each other on the same host using only container names and without the need to specify the container’s IP address or listening port.\n\nThe concept of using only container names is programmatically very useful, especially in cases where these IPs change dynamically. The names are usually deterministic, and by only specifying the Docker’s container name, you avoid having to apply different layers of system operations to first learn about the container’s TCP/IP details before starting to communicate with the target container.\n\nHowever, there are some prerequisites to enabling container communication through their names only:\n\nThe containers communicating with each other will all need to be on the same host\n\nWe will need to create a Docker network on the host\n\nWe will need to attach the containers to the created Docker network when running the container using the docker run command or by specifying the container’s instantiation details in the docker-compose YAML file\n\nThe following command creates a Docker network on the host machine that can be used for our ABC-MSA system’s inter-microservice communication:\n\n$ docker network create abc_msa_network\n\nThe following command lists the Docker networks configured on the host machine:\n\n$ docker network ls",
      "content_length": 2163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "ABC-MSA microservice inter-communication\n\nNow, attach the ABC-MSA containers to the abc_msa_network network by using the --network option in the docker run command, as shown in the following example:\n\n$ docker run -itd -p 8003:8080 \\ --network abc_msa_network --mount \\ source=customer_management_volume,target=/app_data \\ --name customer_management_container abc_msa_customer_ management\n\nUsing Docker networks is very useful in many cases. However, since we are designing our ABC-MSA system so that containers can run independently of their host location, we will be using the container’s IP/TCP communication.\n\nIn the next section, we’ll explain how the ABC-MSA microservices communicate using TCP/IP and go over some examples of how to test the communication and data exchanges.\n\nTCP/IP communication between containers/microservices\n\nSo far, we have installed Docker, built our Docker images and volumes, and started the containers of all our microservices. Now, it is time to understand how these containers interact with each other.\n\nUpon running Docker on the container’s host, the host automatically creates a virtual IP network and assigns an IP address to each running Docker container on that host. That virtual IP network is only internal to the host running the containers and cannot be accessed from anywhere outside that host.\n\nThe container’s host carries at least two IPs. There’s one inside IP that’s internal to the Docker network and that can only be recognized inside that Docker network. Then, there’s an outside IP, which is usually assigned by the Dynamic Host Configuration Protocol (DHCP) server in the organization’s network. The outside IP is necessary for the container’s host to communicate with the outside world.\n\nThe internal Docker network of the container’s host is not visible to any other host in the network. Therefore, for an outside host to communicate with a specific container in the container’s host machine, it will need to use the outside IP of the container’s host machine.\n\nAmong a lot of other information, to get the assigned IP address, as well as the inside and outside listening ports of a specific container in your system, use the docker inspect command, followed by the container’s name.\n\n173",
      "content_length": 2248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "174\n\nBuilding an MSA with Docker Containers\n\nOur demo setup is shown in the following diagram:\n\nFigure 9.7: ABC-MSA container communication\n\nAs you can see, the host machine’s inside IP address is 172.17.0.100, and the outside IP address is 192.168.1.100. The container’s host is listening to the container’s mapped ports (8001 to 8012), as explained earlier.\n\nIf other hosts in the network want to send API calls to one of the ABC-MSA containers, that outside host will need to send the request to the outside IP address of the container’s host, 192.168.1.100, using the mapped port of the container it wants to communicate with.\n\nTo elaborate further, the preceding diagram and the following example show an outside host testing the API response of the Product Management container:\n\n$ curl http://192.168.1.100:8004/ <!DOCTYPE html> <head> <title>PRODUCT MANAGEMENT Microservice</title> </head> <body> <h3>Product Management Microservice Part of ABC-MSA System</ h3> </body>",
      "content_length": 977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "ABC-MSA API calls return a JSON variable for easier data handling. One of the APIs we built for ABC-MSA microservices is service_info. An example of an API call for service_info is as follows:\n\n$ curl http://192.168.1.100:8004/api?func=service_info {\"service_name\": \"product_management\", \"service_descr\": \"ABC- MSA Product Management\"}\n\nIf you are communicating internally from within the Docker network (172.17.0.0), you can communicate directly with the container’s IP and listening ports. Performing the same curl test on the Product Management container from the API Gateway shell would look like this:\n\n$ curl http://172.17.0.4:8080/api?func=service_info {\"service_name\": \"product_management\", \"service_descr\": \"ABC- MSA Product Management\"}\n\nKnowing how to pass API requests and handle the API response is key to developing your MSA system. Please refer to our ABC-MSA code in this book’s GitHub repository for examples of how the API calls are issued and handled across the entire system.\n\nSummary\n\nIn this chapter, we covered the concept of containers, what they are, and how they are different from VMs. Then, we worked with Docker as one of the most popular container platforms available today. We showed you how to install Docker and create Dockerfiles, Docker images, Docker volumes, and Docker containers.\n\nThen, we applied all these concepts by building some of the ABC-MSA microservices with hands-on examples. We built the containers and showed how microservices communicate with each other.\n\nIn the next chapter, we will focus on building an AI microservice in the MSA system. We will discuss some of the most important AI/ML/DL algorithms that should be considered and implemented in an MSA system, and how these algorithms help with a system’s overall stability, performance, and supportability.\n\nSummary\n\n175",
      "content_length": 1828,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "10 Building an Intelligent MSA Enterprise System\n\nIn previous chapters, we gradually built the ABC-MSA to demonstrate some of an MSA system’s features, techniques, and traffic patterns.\n\nIn this chapter, we will combine both MSA concepts and AI concepts to build an ABC-Intelligent-MSA system, which is an enhanced version of our ABC-MSA. The intelligent version of the ABC-MSA will use various AI algorithms to enhance the performance and general operations of the original ABC-MSA system.\n\nABC-Intelligent-MSA will be able to examine different traffic patterns and detect potential problems and then self-rectify or self-adjust to try to prevent the problem from taking place before it actually happens.\n\nThe ABC-Intelligent-MSA will be able to self-learn the traffic behavior, API calls, and response patterns, and try to self-heal if a traffic anomaly or problematic pattern is detected for whatever reason.\n\nThe following topics are covered in this chapter:\n\nThe machine learning advantage\n\nBuilding your first AI microservice\n\nThe intelligent MSA system in action\n\nAnalyzing AI service operations\n\nThe machine learning advantage\n\nThere are many areas in our MSA where we can leverage AI to enhance the system’s reliability and operability. We will focus our system on two main potential areas of enhancement. One is to enhance the system response in case of a microservice failure or performance degradation. The second area of enhancement is to add a proactive circuit breaker role.",
      "content_length": 1489,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "178\n\nBuilding an Intelligent MSA Enterprise System\n\nAs we discussed in Chapter 3, the circuit breaker pattern is used to prevent a system cascading failure when one of the system’s microservices fails to respond to API consumer requests promptly. Should a microservice fail or perform poorly, our AI will try to take proactive action to fix the problem rather than waiting for the problem to be manually fixed for the system to return to normal operation.\n\nIn Chapter 7, we discussed the advantages of using Machine Learning (ML) and DL in MSA in detail. This chapter will focus on building two AI microservices to enhance our MSA system.\n\nThe first AI microservice is called a Performance Baseline Watchdog (PBW) service. The PBW is an ML microservice that creates a baseline for the expected performance of each microservice in the MSA system under a certain system load. Should the operational performance of the measured microservice fall under the performance baseline by the configurable value of x, the system should send a warning message to the Operation Support System (OSS) or the Network Management System (NMS) and should performance fall by y (which is also configurable), the system then should take predefined action(s) to try to self-rectify and self-heal the MSA system.\n\nThe second AI microservice we will build in this chapter is the Performance Anomaly Detector (PAD) service. The PAD is an ML microservice that takes a holistic view of the entire MSA system. The PAD learns the MSA performance patterns and tries to detect any anomalous behavior. It identifies “problematic patterns,” tries to automatically detect a problem before it happens, and accordingly takes proactive action to fix the faulty area of the system.\n\nBuilding your first AI microservice\n\nBefore we start building our two AI microservices, we need to think about our training and test data first – how we will collect our training data, build the model accordingly, test the model and measure its reliability, and enhance the algorithm’s reliability if needed.\n\nImportant Note The AI services we are building in our MSA system are only a proof of concept to demonstrate the value of implementing AI in MSA systems. Rather, businesses should consider an AI service or model that matches their unique needs, business process, and their deployed MSA system.\n\nWe will also need to simulate the use cases themselves. Simulate a system’s microservice failure or performance degradation, simulate a cascading failure, and we should also be able to simulate some system’s outlier patterns to see how the algorithm would detect and react to pattern anomalies.\n\nTo do all this, let’s first understand how the PBW and PAD microservices fit with the overall system’s operation and how they would normally interact with the different system’s components.",
      "content_length": 2833,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "Building your first AI microservice\n\nThe anatomy of AI enhancements\n\nThe main role of both the PBW and PAD is to enhance the stability and reliability of our MSA system. It is therefore imperative for both services to constantly watch individual microservices and the overall system performance and then take the necessary action when performance issues are detected.\n\nThe training data is first collected in a controlled environment for a specific training period, where normal, stable system operations and the average user load are simulated and applied. This can be achieved using some of the simulation tools we built, which will be discussed later in this section.\n\nThis training period creates an ideal first baseline that will be the main reference for the AI services to use during actual production time. The collected training data will then be used to build the algorithm. To achieve better and more accurate results, the training data and algorithm can be regularly tuned later when more information about real-time production traffic is collected.\n\nThe simulated load and system operations are tweaked through multiple simulation parameters. These parameters are tweaked regularly to mimic the actual acceptable operational performance. The algorithm tweaks would eventually stop (or become very minor) as the AI algorithms mature. The cycle of onboarding the AI services to the ABC-MSA system is demonstrated in Figure 10.1:\n\nFigure 10.1: AI microservices implementation in ABC-Intelligent-MSA\n\nMore information on the simulation tools and parameters is coming up in the next section.\n\nOnce the AI services are operational, they will start collecting performance stats from each of the system’s microservices through periodic API calls and then compare these performance stats with the expected performance or behavior.\n\nShould an individual microservice or the overall system performance deviate from what the AI expects to see, a system action will be triggered to either warn the system administrators or self-heal whenever possible. Figure 10.2 shows the high-level architecture of the PBW and PAD services:\n\n179",
      "content_length": 2131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "180\n\nBuilding an Intelligent MSA Enterprise System\n\nFigure 10.2: PBW and PAD services in ABC-Intelligent-MSA\n\nThe PBW’s algorithm calculates the expected performance metrics based on the performance stats collected. Collected performance stats include API call response time stats, the failures or failure rate of individual microservices, the API response code, and the load applied on the microservice itself.\n\nPre-defined actions are triggered based on how far the microservice deviates from the calculated performance metric. Based on the configuration of the PBW, the higher the deviation, the more likely a proactive action is to be triggered to try to self-heal. In the case of a slight deviation, however, no healing action is supposed to be trigged; a system warning informing the system administrator is sufficient.\n\nThe following table shows some of the possible system issues that could be encountered during the operations of an ABC-Intelligent-MSA system, and the actions the PBW service would take to try to rectify the problem.\n\nThe list shown in the table is only a sample of potential issues and can, of course, grow as more use cases are considered:\n\nMicroservice Issue\n\nTriggered Action\n\nSlow responsiveness\n\nScale the microservice vertically/horizontally or restart the microservice\n\nIntermittent timeouts\n\nScale vertically/horizontally or restart\n\nAPI call HTTP response errors\n\nCheck Apache, Flask, the JVM, the Docker volume, SQL service, etc. Restart the service if needed\n\nRestart the microservice’s container\n\nService is unresponsive (down)\n\nRestart the microservice’s container\n\nTable 10.1 – Potential ABC-Intelligent-MSA operational issues and the PBW’s self-healing actions",
      "content_length": 1703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "Building your first AI microservice\n\nThe healing mechanism can be applied to the MSA system using multiple AI services, not necessarily only using the PBW and PAD that we are implementing in our ABC-Intelligent-MSA. This is just an example.\n\nThe self-healing process\n\nAll of the PBW’s healing actions listed in Table 10.1 should not be taken in isolation from the PAD’s operations, but rather should be carefully coordinated with the PAD’s healing actions. A single issue in a microservice could (although not necessarily) trigger actions from both the PBW and PAD services at the same time and could consequently create an operational conflict.\n\nIn terms of the self-healing process, and to avoid conflict between the system’s AI services when triggering self-healing actions, whenever an action is determined and before it is triggered, the AI service sends an API call to the other AI services first (either directly or through the API gateway), declaring a Self- Healing Lock State in the troubled microservice. Accordingly, all the other AI services in the MSA system will hold off any actions that may have been planned related to that troubled microservice.\n\nDuring the self-healing lock state, the only AI service allowed to work on the troubled microservice is the Healer AI Service, which is the AI service that locked it.\n\nOnce the healer has fixed the problem and detects a normal operation in the affected microservice, the healer then sends another API call to the other AI services in the MSA system declaring that the lock state is over.\n\nIf the healer is unable to self-heal and gives up on resolving the issue, it sends an alarm to the NMS/ OSS and marks that microservice as unhealable for a specific configurable period of time, known as the Unhealable Wait Period (by default, 15 minutes).\n\nThe unhealable wait period allows other AI services to try to heal that microservice and gives the healer a breather to pace out its operation across all other microservices in the MSA system.\n\nTo prevent healers from consuming system resources by slipping into indefinite healing attempts, healers will try to heal the troubled microservices for a specific number of healing attempts, configured through the Maximum Healing Attempts value (four attempts, by default), and will then completely give up trying. If the maximum healing attempts are exhausted, a manual system intervention will be needed to fix the troubled microservice.\n\nSystem administrators can still configure indefinite healing attempts if needed, but this can consume system resources and may not be effective depending on the nature of the problem the MSA system or a specific microservice is experiencing.\n\nIf another AI service can fix the troubled microservice or the microservice is manually fixed, the original healer will automatically clear the unhealable flag of the microservice after the unhealable wait period is over.\n\n181",
      "content_length": 2917,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "182\n\nBuilding an Intelligent MSA Enterprise System\n\nIf on the other hand, no other AI service can fix the problem and no manual intervention is taken to fix the microservice, the original healer – and any other healer that may have tried to fix the microservice – will try to heal the microservice again once the unhealable wait period expires if and only if the troubled microservice is not in a self-healing lock state.\n\nThe following visual chart summarizes the self-healing process and may help better explain the entire process.\n\nFigure 10.3: The self-healing process in MSA\n\nIt is important to also understand the main terminology used to explain the self-healing process. The following table shows a summary of the terminology of the main components of our self-healing process:",
      "content_length": 785,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "Building your first AI microservice\n\nTerm\n\nDescription\n\nHealer\n\nAn AI service that attempts to heal a troubled microservice.\n\nHealing Action\n\nAn action taken by the healer to try to fix an ongoing system operational issue.\n\nSelf-Healing Lock State\n\nA microservice state in which an attempt is made by the healer to fix the microservice.\n\nIn this state, only one healer (the one that initiated the lock state) is allowed to work on the troubled microservice.\n\nA microservice self-healing lock state is a state visible by the entire MSA system and not a healer-specific state.\n\nRetry Wait Period\n\nThe time the healer for which has to wait when a healing action fails before it retries. The Retry Wait Period is 2 min by default.\n\nUnhealable State\n\nThe state in which the troubled microservice is marked unfixable by a healer after a healer’s failed attempt to fix the troubled microservice.\n\nA microservice unhealable state is a healer-specific state and only visible to the healer that gave up on fixing that troubled microservice. Other healers can still try to fix the troubled microservice.\n\nUnhealable Wait Period\n\nThe time for which the healer has to wait before it starts to make another attempt to fix the troubled microservice. The Unhealable Wait Period is 15 min by default.\n\nMaximum Healing Attempts\n\nThe maximum number of attempts the healer will try after each unhealable wait period, and before the healer totally gives up on the troubled microservice and no longer attempt to fix it. By default, PBW tries 4 healing attempts.\n\nTable 10.2: ABC-Intelligent-MSA operational issues with the self-rectifying actions of the PBW\n\nSo far, we have explained the value of deploying AI services in the MSA system and shown some practical application examples to demonstrate the value of AI in MSA.\n\nIn order to build, run, and tweak AI services in MSA, we need to build certain tools to gather and log system statuses, operational dynamics, and operational statistics. In the following section, we will dive into what these tools are and how to use them.\n\n183",
      "content_length": 2062,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "184\n\nBuilding an Intelligent MSA Enterprise System\n\nBuilding the necessary tools\n\nThe purpose of creating project tools is to first be able to build the AI models, then simulate the entire ABC-Intelligent-MSA system, and then collect stats and analyze the system’s operations.\n\nAlthough there may be tools available online that would help us achieve our purpose, instead, we will build simple tools customized specifically for our use cases.\n\nWe created multiple tools to help us collect training and test data, simulate the system and microservices load, and measure the performance of the microservices. All the tools are available in the tools directory in our GitHub repository.\n\nThe tools also help us scrub some of the generated logs and data for analysis and potential future enhancements.\n\nThe following are the main tools we need in our ABC-Intelligent-MSA setup.\n\nAn API traffic simulator\n\nThe API traffic generator/simulator, simulate_api_rqsts.py, helps simulate the API request load for one or more of the system’s microservices.\n\nsimulate_api_rqsts creates multi-threaded API requests across multiple target microservices. API HTTP requests are then sent to each microservice in parallel.\n\nThe API load is measured by requests per minute and API requests can either be uniformly or randomly paced.\n\nThe uniformly paced requests are paced out so that the time between each API call is always the same, so if we are configuring a uniformly paced load of 600 API requests/min, simulate_api_rqsts will send 1 API call every T = 100 ms.\n\nIn the randomly-paced case, each API call is sent after a random period, TR, from the time where the previous call was sent, but so that TR can never be larger or smaller than 95% of T. So if we are configuring a randomly-paced load of 600 API requests/min, TR, in that case, will be equal to a value greater than 5 ms and smaller than 195 ms.\n\nsimulate_api_rqsts will send 1 API call every:\n\n(1-95%)T <= TR <= (1+95%)T (i.e., for 600 requests/min: 5 ms <= TR <= 195 ms)\n\nThe sum of all TRs, however, will still be approximately equal to the configured requests/min. In our example here, the load is 600 API requests/min.\n\nUniformly paced requests are better when you are manually analyzing how a particular microservice responds to the API load, while randomly paced requests are a better representation of a real-time production API request load.",
      "content_length": 2395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "Building your first AI microservice\n\nThe microservices performance monitor\n\nThe microservices performance monitor, ms_perfmon.py, is another multithreading tool and is initially used for collecting and building the AI training data during the simulation period of ideal conditions.\n\nms_perfmon sends parallel API calls to each microservice in the system and then logs the API call hyperlink, the date and time at which it was sent, the receiving microservice response time, and the HTTP response code. The following is an example log entry of the collected data in a comma-separated format:\n\nhttp://payment_ms:8080,2022-12-28 15:48:57.271370, 0.010991334915161133,200\n\nEach microservice stat is collected in its own Comma-Separated Values (CSV) log file named after the API link itself (after cleaning up special characters). All the stat files are collected under the perfmon_stats directory in the ms_perfmon working path.\n\nIn real-time operation, both the PBW and PAD perform a similar job to ms_perfmon. They collect their own stats and measure the target microservice’s real-time performance against the baseline and the expected normal behavior.\n\nShould we extend the MSA system’s AI capabilities by including more AI services for different purposes and use cases, which will likely require each AI service to conduct its own performance statistics collection?\n\nDepending on the collection frequency and the type of data collected, as the number of collectors increases, scalability could become an issue. The ms_perfmon function, in that case, can be extended to become the main AI collector for all AI or non-AI services in the MSA system. This setup can help offload the system’s microservices and allow the MSA system to scale better.\n\nFigure 10.4: A collect-once performance stats setup in ABC-Intelligent-MSA\n\n185",
      "content_length": 1825,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "186\n\nBuilding an Intelligent MSA Enterprise System\n\nFigure 10.4 shows how ms_perfmon can handle stats collection on behalf of all other services in the MSA system and then act as a proxy and respond to API calls requesting whatever stats are needed for each particular AI (or non-AI) service.\n\nThe response delay simulator\n\nTo simulate a delayed response or a troubled microservice, and solely for simulation and testing purposes, we added a feature in key microservices to simulate a delayed API call response.\n\nThe delay response feature, when enabled in the microservice, has two configurable values – the minimum delay and the maximum delay. When a microservice receives an API call, it will automatically assign a random delay value between the configured minimum delay and maximum delay, and then wait for that time before it responds to the consumer’s API calls.\n\nThe feature is very helpful for simulating a cascading system failure. As will be shown later in this chapter, the response delay feature can also help demonstrate the value of using AI services to enhance the operations of the MSA system compared to using the short circuit traffic pattern previously explained in Chapter 3.\n\nThe response delay is enabled whenever the max delay is configured with a value greater than zero. When the value of max delay is higher than zero, a delay value is assigned to the microservice API’s call response, as shown in the following code snippet:\n\n#Simulate a delay if received an API to do so if delay_max_sec > 0: delay_seconds = round(delay_min_sec + random.random()*(delay_ max_sec-delay_min_sec), 3) #print(\"Adding a delay %s ...\" %delay_seconds) time.sleep(delay_seconds)\n\nThe max and min delay values can be configured using an API call. The following is an example of using curl to send an API call to configure the maximum and minimum delay response in milliseconds:\n\ncurl http://inventory_ms:8080/api/ simulatedelay?min=1500&max=3500\n\nAgain, this feature is only for demo and test purposes. A more secure way of simulating a delay is using secured configuration files or local parameters instead.",
      "content_length": 2112,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "The intelligent MSA system in action\n\nThe API response error simulator\n\nSimilar to the response delay simulator, this feature is for demo purposes only. The API error simulator feature uses one configurable value – the average HTTP error per hour. When the feature is enabled in the microservice, the microservice will pick a randomly applicable server 500 error and respond to API requests with randomly paced responses that match the configured error rate.\n\nThe error rate can be configured using an API call. The following is an example of using curl to send an API call to configure an API error response rate of 5 HTTP errors per hour:\n\ncurl http://inventory_ms:8080/api/response_err?rate=5\n\nNow, we know the testing and simulation tools available for us to use for training, testing, and simulating production for our MSA system.\n\nIn the next section, we will discuss our ABC-Intelligent-MSA operations – how to initialize the system, how to build and use training and testing data, and how to simulate the system’s production traffic.\n\nThe intelligent MSA system in action\n\nIn the previous sections of this chapter, we discussed how the different system components interact with each other and what tools we use to build the AI algorithms, test the system, and monitor the operations of different components.\n\nIn this section, we will put our ABC-Intelligent-MSA to the test. We will run all system microservices and tools, and see how the different system components actually interact with each other, what results we see, and how we can tweak the system to maintain smooth end-to-end operations.\n\nThe ABC-Intelligent-MSA will first run under an ideal simulation environment (no error simulation and no delays) to collect the training data necessary to build the AI models. Once enough data has been collected, we will then train the models and prepare the system for actual production traffic.\n\nThe system initialization steps, therefore, are as follows:\n\n1. Start the system with no AI services to collect the necessary training data under an ideal operational situation and create an operational baseline.\n\n2. Sanitize the collected data if needed and remove outliers.\n\n3. Train the AI algorithms using the training data collected.\n\n4. Re-initialize the system with all of its AI services.\n\n5. Start production operations. In our example here, we will simulate actual production operations by injecting errors, data delay responses, service failures, and so on.\n\n187",
      "content_length": 2477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "188\n\nBuilding an Intelligent MSA Enterprise System\n\nInitializing the ABC-Intelligent-MSA system\n\nWe start by initializing our MSA system using the system’s Docker compose file, abc_ msa.yaml, and using the docker-compose command as follows,\n\n$ docker-compose -f abc_msa.yaml up &\n\nAs discussed previously in Chapter 9, the preceding docker-compose command is much more convenient than using multiple docker run commands. docker-compose will read the system’s run parameters and configuration from the abc _msa.yaml file, and initialize all the system components accordingly.\n\nIn our example, this will start the analysis and monitoring tools, along with all the regular microservices in the system. Since we are still collecting training data, no AI services need to be initialized yet.\n\nAs shown in Figure 10.2 and Figure 10.4, when we start the AI services (the PBW and PAD), they will need to be able to remotely control (start, stop, and restart) the system’s Docker containers. The PBW and PAD are designed to control the Docker containers using API calls. Therefore, we need to enable Docker Engine first to respond to API calls and for the PBW and PAD to be able to successfully communicate with Docker Engine.\n\nThe following are the steps needed to enable Docker’s API remote management:\n\n1. On your Ubuntu system, use vi, vim, or any other similar tool to edit the /lib/systemd/ system/docker.service file.\n\n2. Look for the ExecStart entry and make the necessary modifications for it to be like the following:\n\nExecStart=/usr/bin/dockerd -H=fd:// -H=tcp://0.0.0.0:2375\n\n3. This will enable Docker Engine to listen to API calls. Make sure you save the file after the modifications.\n\n4. Reload Docker Engine using the following command:\n\nsystemctl daemon-reload\n\n5. To ensure Docker Engine is working properly and responding to API calls, use the following command:\n\ncurl http://localhost:8080/version\n\nNow, the system is running and collecting training data. The longer you run the system, the more training data will be collected, and the more accurate your AI models will be. In our example, we will leave the system running for approximately 48 hours.",
      "content_length": 2162,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "The intelligent MSA system in action\n\nIn the next subsection, we will go over how to run the tools, build training data, collect some of the system performance logs, simulate real-time system operations, and analyze the collected performance data.\n\nBuilding and using the training data\n\nThe ms_perfmon tool will create a separate stat file for each microservice in the <ms_perfmon's working path>/perfmon_stats directory. It is important that we leave the tool running and monitor the system’s performance stats under minimal load conditions.\n\nWe recommend at least 48 hours of training data collection. Ideally, however, data should be collected with seasonality load whenever applicable. In some environments, for example, the system load may increase on weekends over weekdays, during the shopping season, and so on. These situations should be considered in the training data to be able to build a more accurate AI model.\n\nPerformance data is pulled every 10 seconds, and accordingly, with 48h of active monitoring, ms_ perfmon produces 17,280 entries for each microservice.\n\nRegardless of the length of the system’s training period, whenever enough performance data has been collected, the training_data_cleanup.py tool should be run to detect any outliers and sanitize the performance data before using it in our AI services.\n\nThe training_data_cleanup tool scrubs all the performance data files in the <ms_perfmon's working path>/perfmon_stats directory, and automatically creates a scrubbed_stats directory with all the scrubbed data for each microservice. These scrubbed files are the files that we will later use for training the AI services.\n\nWe are now ready to write our Python code for training the PBW:\n\n1. We will use the numpy library for array and scientific data processing, pandas for reading our CSV training data files and testing data, and sklearn to build our AI model:\n\nimport numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression\n\n2. After importing the required libraries, we now need to copy all performance data into a DataFrame object. The following is a code example of this:\n\npayment_ms_stats_df = pd.read_csv('scrubbed_stats/ payment_ms_stats.csv')\n\nThe PBW’s AI model includes the microservice response time, the calculated request failure rate, and the calculated microservice load. The model should calculate the expected response time based on all the preceding parameters.\n\n189",
      "content_length": 2495,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "190\n\nBuilding an Intelligent MSA Enterprise System\n\n3.\n\nIn our Python code, we need to point to the data column that needs to be predicted. In our example, that would be the response time. The following is a code snippet for the Payment microservice:\n\npayment_ms_rt = np.array(payment_ms_stats_df['response_ time'])\n\n4. We need now to build our model, but before doing so, we need to load the rest of the performance data column into an array for training and testing processing. We do that by removing the “response time” column (an axis of 1) from the created DataFrame and then loading that DataFrame into an array to be used in our sklearn object, as follows: model_data = payment_ms_stats_df.drop('response_time', axis = 1) model_data = np.array(model_data)\n\n5. The model data need to be split into training data and test data. We split the model data into 80% training and 20% test data as follows:\n\nmodel_data_train, model_data_test, payment_ms_rt_train, payment_ms_rt_test = train_test_split(model_data, payment_ms_rt, test_size = 0.20, shuffle=True)\n\n6. Now, we build the model from the training data:\n\nlr_model = LinearRegression() lr_model.fit(model_data_train, payment_ms_rt_train);\n\n7. Save the data to a CSV file for future use:\n\ntrend_payment_ms_rt_predictions = lr_model. predict(payment_ms_rt) df = payment_ms_rt_df.assign(predicted_payment_ms_rt = trend_payment_ms_rt_predictions) df.to_csv(\"predicted_payment_ms_rt_trend.csv\", mode = 'w', index=False)\n\nNow, we have the training data and the trained model. It is time to use the model for production traffic.\n\nIn the next subsection, we will simulate production operations and describe how that can be applied to our trained MSA system, the ABC-Intelligent-MSA.",
      "content_length": 1730,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "Analyzing AI service operations\n\nSimulating the ABC-Intelligent-MSA’s operation\n\nWe need to reinitialize the system now with the trained model and production traffic. Since no actual production traffic is applied in our example, we need to simulate the production operation with its potential operational challenges, including high traffic loads, service failures, and potential network hiccups.\n\nWe start by reinitializing the ABC-Intelligent-MSA system using docker-compose, as described earlier, but using the abc_intelligent_msa.yaml file:\n\n$ docker-compose -f abc_intelligent_msa.yaml up &\n\nThe main difference between abc_intelligent_msa.yaml and abc _msa.yaml is that the first file includes the initialization of the AI services.\n\nOnce the system is running, the AI tools will start monitoring and collecting the microservice’s performance and trigger healing actions whenever a system problem is detected and metrics exceed the configured performance thresholds.\n\nThe production traffic is ready to be simulated now using the simulate_api_rqsts API traffic simulator and the response delay simulator function discussed earlier.\n\nUsing the API response error simulator, occasional HTTP errors can also be simulated if needed. A more sophisticated simulation would involve injecting HTTP 500 error codes as well, but we will stick to response time performance delays for simplicity.\n\nThe ms_perfmon tool will still be running to collect data for our offline analysis whenever needed.\n\nWe now need to simulate specific production use cases and see how the AI tools will respond and self-heal the entire system. In the next section, we will discuss the operations of the PBW and PAD and look into how both AI services interact with system performance readings and errors.\n\nAnalyzing AI service operations\n\nIn the preceding sections, we started by building our first AI service and covered how to use AI to enhance the MSA system’s operations and resilience, the self-healing process, and the tools we built to generate training data and simulate the ABC-Intelligent-MSA system’s operation.\n\nIn this section, we will examine the system logs and check how the PBW and PAD interact with the system and actually enhance its operations. We will then simulate a cascading system failure and examine how the self-healing process is triggered and handled to bring the MSA system back to normal operation.\n\n191",
      "content_length": 2406,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "192\n\nBuilding an Intelligent MSA Enterprise System\n\nThe PBW in action\n\nDuring the training period, the PBW was able to build an AI model and calculate the expected response time of each microservice in the ABC-Intelligent-MSA system. As you can see from the following log sample, under a normal system load, the average response time of the Inventory microservice is about 20 ms:\n\nhttp://inventory_ms:8080,2022-11-23 15:48:25.094675, 0.01450204849243164,200 http://inventory_ms:8080,2022-11-23 15:48:35.816913, 0.0241086483001709,200 http://inventory_ms:8080,2022-11-23 15:48:46.543205, 0.02363872528076172,200 http://inventory_ms:8080,2022-11-23 15:48:57.271370, 0.010991334915161133,200 http://inventory_ms:8080,2022-11-23 15:49:07.983282, 0.021454334259033203,200 http://inventory_ms:8080,2022-11-23 15:49:18.645113, 0.012285232543945312,200 http://inventory_ms:8080,2022-11-23 15:49:29.310656, 0.0245664119720459,200 http://inventory_ms:8080,2022-11-23 15:49:40.010556, 0.013091325759887695,200 http://inventory_ms:8080,2022-11-23 15:49:50.744695, 0.021291017532348633,200 http://inventory_ms:8080,2022-11-23 15:50:01.715555, 0.024635791778564453,200\n\nWe configured the warning threshold for the PBW as 250 ms, and the action threshold as 750 ms. We will now start introducing an API call load to the Inventory microservice using simulate_api_ rqsts and delays using the response delay simulator feature. Then, we will see how the PBW reacts from the PBW action logs.\n\nThe following are the PBW’s performance readings for about 1.5 minutes. As you can see from the readings, the response time is consistently above the 250 ms alarm threshold, but (with the exception of one reading) still below the 750 ms action threshold:\n\nhttp://inventory_ms:8080,2022-11-23 18:24:00.518005, 0.6386377334594727,200 http://inventory_ms:8080,2022-11-23 18:24:11.469172, 0.7164063453674316,200 http://inventory_ms:8080,2022-11-23 18:24:22.203452,",
      "content_length": 1933,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "Analyzing AI service operations\n\n0.7233438491821289,200 http://inventory_ms:8080,2022-11-23 18:24:32.942619, 0.7101089954376221,200 http://inventory_ms:8080,2022-11-23 18:24:43.668907, 0.6982685089111328,200 http://inventory_ms:8080,2022-11-23 18:24:54.777383, 0.8207950115203857,200 http://inventory_ms:8080,2022-11-23 18:25:05.410204, 0.6812236309051514,200 http://inventory_ms:8080,2022-11-23 18:25:16.101344, 0.6544813632965088,200 http://inventory_ms:8080,2022-11-23 18:25:27.072040, 0.7446155548095703,200 http://inventory_ms:8080,2022-11-23 18:25:37.828189, 0.6969136238098145,200\n\nThe readings will have to be consistently above the 750 ms action threshold for the PBW to trigger a healing action. One reading above 750 ms is not enough for an action to be triggered. However, since the readings are constantly above the 250 ms alarm threshold, the PBW is expected to trigger an alarm to the NMS/OSS system.\n\nWe need to verify the PBW’s behavior from the NMS/OSS system or the PBW’s action log. The following is a snippet of the PBW’s action log during the same period from the previous example:\n\n2022-11-23 18:24:00.518005: Alarming high response time (0.6386377334594727) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:24:11.469172: Alarming high response time (0.7164063453674316) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:24:22.203452: Alarming high response time (0.7233438491821289) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:24:32.942619: Alarming high response time (0.7101089954376221) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:24:43.668907: Alarming high response time (0.6982685089111328) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:24:54.777383: Actionable high response time (0.8207950115203857) detected in inventory_ms. No action triggered yet.\n\n193",
      "content_length": 1874,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "194\n\nBuilding an Intelligent MSA Enterprise System\n\n2022-11-23 18:25:05.410204: Alarming high response time (0.6812236309051514) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:25:16.101344: Alarming high response time (0.6544813632965088) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:25:27.072040: Alarming high response time (0.7446155548095703) detected in inventory_ms. No alarm triggered yet. 2022-11-23 18:25:37.828189: Alarming high response time (0.6969136238098145) detected in inventory_ms. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-23 18:25:48.637317: Alarming high response time (0.6777710914611816) detected in inventory_ms. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-23 18:25:59.327946: Alarming high response time (0.6758050918579102) detected in inventory_ms. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-23 18:26:10.014319: Alarming high response time (0.6641242504119873) detected in inventory_ms. Yellow alarm triggered and sent to NMS/OSS system.\n\nAs you can see from the preceding snippet’s last 4 log entries, after a consistent delay of more than 250 ms, an alarm was triggered and sent to the NMS/OSS system. We need to increase the inventory microservice’s load and response time to see how the PBW will react.\n\nThe following is another snippet of the PBW’s performance log. Only the last 4 log entries in a series of 10 consistent response delay readings are above 750 ms:\n\nhttp://inventory_ms:8080,2022-11-23 18:29:31.852330, 1.326528787612915,200 http://inventory_ms:8080,2022-11-23 18:29:43.196200, 1.4279899597167969,200 http://inventory_ms:8080,2022-11-23 18:30:05.310226, 1.0108487606048584,200 http://inventory_ms:8080,2022-11-23 18:30:16.334608, 1.1380960941314697,200\n\nNormally, we would have configured all healing actions shown in Table 10.1. In our demo system, however, we have configured only one healing action to demo the system self-healing operations in general. We only configured a microservice container to restart if a problem is experienced in the microservice. The response delay simulator feature is therefore a more relevant simulation tool than the other tools we have mentioned earlier.",
      "content_length": 2222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "Analyzing AI service operations\n\nIn case of slow performance due to high API call requests volume, the most appropriate healing action would be to try to scale the microservice first and allocate more resources to respond to the high volume of API requests.\n\nWe assume in our simulation that the problem in the Inventory microservice is not necessarily due to the API request load, but rather some unforeseen problem causing the Inventory service to become unstable and unable to handle API calls promptly, so restarting the Inventory microservice could therefore fix the problem.\n\nNow, here is a look at the PBW’s action log during the same period. Please note that prior to the actionably high response time, an alarmingly high response time below 750 ms was previously detected. The response time was higher than 250 ms and below 750 ms:\n\n2022-11-23 18:29:31.852330: Actionable high response time (1.326528787612915) detected in inventory_ms. No action triggered yet. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-23 18:29:43.196200: Actionable high response time (1.4279899597167969) detected in inventory_ms. No action triggered yet. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-23 18:30:05.310226: Actionable high response time (1.0108487606048584) detected in inventory_ms. No action triggered yet. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-23 18:30:16.334608: Actionable high response time (1.1380960941314697) detected in inventory_ms. Red Alarm triggered and sent to NMS/OSS system. 2022-11-23 18:30:16.334608: Self-healing lock state declared for inventory_ms container. 2022-11-23 18:30:16.334608: Self-healing action triggered. Restarting inventory_ms container (inventory_management_ container). 2022-11-23 18:30:21.359377: Verifying inventory_ms operations... 2022-11-23 18:30:22.945823: inventory_ms was successfully restarted 2022-11-23 18:30:23.089051: Self-healing lock state cleared for inventory_ms container.\n\n195",
      "content_length": 1979,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "196\n\nBuilding an Intelligent MSA Enterprise System\n\nAs you see from the last 4 entries in the action log, the PBW detected a consistent response time (above 750 ms) and accordingly sent a red alarm to the NMS/OSS system, indicating a critical delay in the Inventory service and the need for a self-healing action to be taken. The PBW then locked the Inventory microservice to avoid clashing with healing actions from other AI services. The PBW then restarted the Inventory microservice by sending a restart API call to Docker Engine, verified that the Inventory microservice was back online, and finally unlocked the Inventory microservice.\n\nTo restart a Docker container through API, you will need to send a POST request as follows:\n\n/containers/<container id or name>/restart\n\nYou can also specify the number of seconds to wait before restarting the container using a t parameter. The following is a container restart POST example to restart the Inventory service container after a 10-second wait time:\n\n/v1.24/containers/inventory_management_container/restart?t=10\n\nFor more information on how to control Docker Engine using API calls, check the Docker Engine API documentation at https://docs.docker.com/engine/api/version-history/.\n\nHowever, was the PBW able to fix the Inventory microservice problem?\n\nLet’s go back now to the PBW’s performance log and see how this self-healing action impacted the Inventory service performance. The following are the log entries just before the healing action was triggered:\n\nhttp://inventory_ms:8080,2022-11-23 18:30:16.334608, 0.1380960941314697,200 http://inventory_ms:8080,2022-11-23 18:30:27.629649, 0.1693825721740723,200 http://inventory_ms:8080,2022-11-23 18:30:38.486793, 0.1700718116760254,200\n\nSure enough, the response time dropped from above 1 s to a maximum of 170 ms. Not as low as it was before the problem appeared, but the Inventory microservice for sure has some breathing room now. The performance issues may very well return if the underlying problem is not attended to and properly fixed.\n\nIn a more advanced AI model, we can train and configure the system to take more sophisticated actions to fully resolve the problem whenever needed, but in this book, we are limited to a specific scope to be able to demonstrate the idea in principle and pave the way for you to develop your own AI models and algorithms for your specific use cases.",
      "content_length": 2400,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "Analyzing AI service operations\n\nWe have demonstrated in this section how the PBW works and how an action is triggered when a microservice performance issue is detected. In the following section, we will go over the PAD AI service and how the PAD takes a rather more holistic view of the entire system.\n\nThe PAD in action\n\nThe best way to demonstrate the operations of the PAD is to simulate a cascading failure and see how the PAD can bring the MSA system back to normal operation.\n\nTo simulate a cascading failure and ensure that the PAD responds to the failure and tries to auto-heal, we will first need to disable the PBW AI service. This will prevent the PBW from triggering a healing action and prevent it from trying to resolve the problem before the PAD’s healing action(s) kick in.\n\nLet’s quickly revisit what we have previously discussed in Chapter 3, an example of how a cascading failure happens.\n\nAs shown in Figure 10.5, under heavy API traffic, a failure to the Inventory microservice could cause the Payment microservice to pile up too many API calls in the queue, waiting for a response from the Inventory service. Eventually, these API calls will consume and exhaust the available resources in the Payment microservice, causing it to fail. A failure in the Payment microservice will produce a similar situation in the Order microservice, and eventually, produce a failure for the Order microservice as well:\n\nFigure 10.5: The Payment microservice is down\n\nFor the PAD to respond with healing actions, each of the PAD’s detected anomaly types has to have healing actions defined for it.\n\nTo successfully simulate the cascading failure, we only defined an action for a cascading failure situation. Otherwise, the PAD would automatically detect the failure in the Inventory service and self-heal it by restarting the Inventory microservice container, preventing a cascading failure from happening to begin with.\n\nWe will start by simulating a high volume of orders for the Order microservice and see how the system is going to respond to this situation in general, and specifically how the PAD will react under the situation.\n\nTo simulate a high volume of order requests, use the following simulate_api_rqsts command to target the Order microservice with a fixed uniformly paced order requests of 100,000 per minute:\n\nsimulate_api_rqsts 100000 http://order_ms:8080/place_order\n\n197",
      "content_length": 2396,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "198\n\nBuilding an Intelligent MSA Enterprise System\n\nWe will now shut down the Inventory microservice and examine the PAD action logs. The following is a snippet of the log about a minute after the PAD started to detect a failure in the Inventory microservice.\n\nPlease note that we introduced sudden high-volume traffic into the system. This sudden traffic increase by itself is a traffic pattern anomaly that was picked up by the PAD, but the PAD did not respond to that specific anomaly because no healing action is specifically defined for that anomaly:\n\n2022-11-24 11:39:13.602130: Traffic pattern anomaly detected, (inventory_ms) is likely down. No action is defined. No action triggered yet. Yellow alarm triggered and sent to NMS/OSS system. 2022-11-24 11:39:23.469204: Traffic pattern anomaly detected, (payment_ms) slow API response detected. No action is defined. No action triggered yet. : : 2022-11-24 11:40:26.836405: Traffic pattern anomaly detected, (payment_ms) slow API response detected. No action is defined. No action triggered yet. Yellow alarm triggered and sent to NMS/OSS system.\n\nIn the preceding snippet of the PAD log, the PAD automatically recognized the Inventory service failure since no response traffic was detected from the service. However, no action was taken by the PAD since no healing action was defined for that particular anomaly. Since the anomaly was consistent for more than 1 minute, the PAD sent an alarm to the NMS/OSS system to notify the system admins of the problem.\n\nBecause of the Inventory microservice failure, the Payment microservice started to run out of resources, and the PAD picked up an unusually slow traffic flow from the Payment microservice given the API call request load applied. Accordingly, and as seen in the log, a little over 1 minute later, the PAD started to generate alarms to NMS/OSS.\n\nAs shown in the following PAD log, a few minutes after the Payment microservice anomaly, the Order microservice started acting up, and accordingly, the PAD was able to correlate all these anomalies and detect a potential cascading failure:\n\n2022-11-24 11:47:12.450897: Traffic pattern anomaly detected, (order_ms) slow API response detected. No action is defined. No action triggered yet. Yellow alarm triggered and sent to NMS/ OSS system. 2022-11-24 11:47:12.450897: Traffic pattern anomaly detected, potential cascading failure detected. No action triggered yet. Yellow alarm triggered and sent to NMS/OSS system.",
      "content_length": 2476,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "Analyzing AI service operations\n\nPlease note that the only microservice failure we have so far is the one we manually shut down, the Inventory microservice. Both the Payment and Order microservices are still up and running but, as it seems from the log, may be suffering from resource exhaustion.\n\nThe system is still running so far, and should the Inventory service return back online, the system will automatically recover. The user experience during the heavy load would only be slow performance during the ordering process, but no orders have been denied or failed yet.\n\nBy examining all these previously mentioned PAD action logs, and as the situation stands so far, we are still okay. However, if no action is taken to resolve the Inventory microservice problem, the system will eventually fail and user orders will start to be denied.\n\nThe short circuit traffic pattern discussed in Chapter 3 helps prevent a cascading failure from taking place, but it still cannot resolve the underlying problem. User orders in a traditional short circuit pattern implantation will still be rejected until manual intervention fixes the Inventory microservice.\n\nThat’s where the PAD comes in. Check the following PAD action log!\n\n2022-11-24 11:48:13.638447: Traffic pattern anomaly detected, potential cascading failure detected. (inventory_ms) microservice is likely the root-cause. Red Alarm triggered and sent to NMS/OSS system. 2022-11-24 11:48:13.638447: Self-healing lock state declared for inventory_ms container. 2022-11-24 11:48:13.638447: Self-healing action triggered. Restarting inventory_ms container (inventory_management_ container). 2022-11-24 11:48:18.663912: Verifying inventory_ms operations... 2022-11-24 11:48:20.325807: inventory_ms was successfully restarted 2022-11-24 11:48:20.474590: Self-healing lock state cleared for inventory_ms container.\n\nThe PAD was able to detect the cascading failure before it actually happened, and was able to identify the root cause of the problem. The PAD sent a red alarm to the NMS/OSS system, declared a self-healing lock state on the Inventory service to try to fix the problem’s root cause, was able to successfully restart the Inventory microservice container, and then cleared the self-healing lock on the Inventory service.\n\nLet’s now check the microservices performance logs and ensure that the problem is fixed and that the ABC-Intelligent-MSA system and all of its microservices are running normally.\n\n199",
      "content_length": 2464,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "200\n\nBuilding an Intelligent MSA Enterprise System\n\nHere’s the Inventory microservice’s performance log:\n\nhttp://inventory_ms:8080,2022-11-24 11:51:33.132089, 0.033451717535487,200 http://inventory_ms:8080,2022-11-24 11:51:43.894705, 0.035784934718275,200 http://inventory_ms:8080,2022-11-24 11:51:54.809743, 0.027584526453594,200 http://inventory_ms:8080,2022-11-24 11:52:06.155834, 0.028615804809435,200\n\nHere’s the Payment microservice’s performance log:\n\nhttp://payment_ms:8080,2022-11-24 11:54:41.109835, 0.051435877463506,200 http://payment_ms:8080,2022-11-24 11:54:51.924508, 0.102346014326819,200 http://payment_ms:8080,2022-11-24 11:55:03.372841, 0.070163827689135,200 http://payment_ms:8080,2022-11-24 11:55:14.076832, 0.157682760576845,200\n\nHere’s the Order microservice’s performance log:\n\nhttp://order_ms:8080,2022-11-24 11:58:37.135827, 0.209097164508914,200 http://order_ms:8080,2022-11-24 11:58:47.584731, 0.193851625041193,200 http://order_ms:8080,2022-11-24 11:58:58.243759, 0.150628069240741,200 http://order_ms:8080,2022-11-24 11:59:08.961412, 0.138192362340785,200\n\nAs shown for the preceding Inventory, Payment, and Order microservices, all of those microservices are back online with normal performance readings. The system is now back to normal operation and should be able to handle the production load with no issues.",
      "content_length": 1343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": "Summary\n\nThis chapter walked us through how we can build AI models to build an intelligent MSA system step by step. We accordingly built two main AI services – the PBW and the PAD – and leveraged these AI services to enhance our MSA demo system, ABC-MSA, to build an intelligent MSA system that we named ABC-Intelligent-MSA.\n\nWe explained the self-healing process design and dynamics in detail, as well as the tools we built to develop AI training data, how to simulate production operations, and how to measure the demo system’s performance. We then put the ABC-Intelligent-MSA to test, simulated a couple of use cases to demonstrate AI functions within the MSA system, and carefully examined the logs of our demo AI services to showcase the value of using AI in MSA.\n\nEverything explained in this chapter is just an example of using AI in an MSA system. Enterprises should consider using AI services that are specifically appropriate for their own MSA system and use cases. These AI tools may very well be available through third parties or built in-house whenever needed.\n\nIn the next chapter, we will discuss the transformation process from a traditional MSA system to an intelligent MSA system – the things to consider in greenfield and brownfield implementations, and how to avoid integration challenges to make the corporate transformation as smooth as possible.\n\nSummary\n\n201",
      "content_length": 1383,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "11 Managing the New System’s Deployment – Greenfield versus Brownfield\n\nIn the previous chapters, we discussed building an MSA system and integrating AI algorithms to form an Intelligent MSA. We covered concepts, techniques, and methodologies while accompanying them with examples.\n\nIn this chapter, we will discuss the different greenfield and brownfield deployment considerations, and ways to smoothly deploy the new intelligent MSA system with minimal operational disruptions, to be able to maintain overall system stability and business continuity.\n\nWe will also examine how to overcome general deployment challenges, particularly in brownfield deployments where existing systems are in production, and implement a successful and effective migration plan for the new Intelligent MSA system.\n\nThe following topics will be covered in this chapter:\n\nDeployment strategies\n\nGreenfield versus brownfield deployment\n\nOvercoming deployment challenges",
      "content_length": 947,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "204\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nDeployment strategies\n\nOrganizations utilize various techniques to minimize downtime and ensure a seamless and successful deployment when deploying a new system. Some of the most commonly used deployment strategies organizations follow are Recreate, Ramped, Blue/Green, Canary, A/B Testing, and Shadow deployments:\n\nThe Recreate deployment is a simple, straightforward approach that involves replacing the entire infrastructure at once, similar to the Big Bang migration we discussed in Chapter 3. This approach is best suited for small and simple systems; however, it also means that the system is completely offline during the deployment process, which can lead to significant downtime.\n\nThe Ramped deployment is similar to the Trickle migration we discussed in Chapter 3. The Ramped deployment allows the existing system to remain online during the deployment process. The new system is brought online gradually, and traffic is gradually routed to it, allowing both systems to stay available to users throughout the deployment process. Although this can be effective in small businesses and simple systems, this approach is ideal for larger, more complex systems, where minimizing downtime is a priority.\n\nBlue/Green deployment is a technique that involves maintaining two identical production environments, referred to as “blue” and “green,” and routing traffic to one or the other. This allows for a seamless switchover in case any operational issues are experienced in the newly deployed version. This method is best suited for mission-critical systems since it ensures that there is always a working system available to users at any given time.\n\nCanary deployment is a technique that involves deploying the new system alongside the existing one and routing a small percentage of traffic to the new system. This allows for testing the new system with actual production traffic before rolling it out completely. If problems arise in the new system, the rollout can be reevaluated based on the type of issues encountered; then, the previous system can be reinstated while the problems are being resolved. This approach is often used to deploy changes to critical systems that require high levels of availability.\n\nThe A/B testing deployment is another approach that involves simultaneously running both the old and the new system but testing them with different subsets of users to determine which performs better. This method works best for testing new system features or services.\n\n\n\nIn the Shadow deployment, the new system is deployed to run alongside the existing system. The live production traffic of the old system is then redirected to the new system to test a newly released feature, the system stability under load, or test the new system altogether. This approach is best used in large systems deployed in large organizations.",
      "content_length": 2916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Greenfield versus brownfield deployment\n\nThe following is a comparison summary of all the preceding deployment strategies:\n\nContinuous Uptime\n\nProduction Traffic Testing\n\nCost\n\nComplexity\n\nRecreate\n\nNo\n\nNo\n\nLow\n\nLow\n\nRamped\n\nYes\n\nNo\n\nLow\n\nLow\n\nBlue/Green\n\nYes\n\nNo\n\nHigh\n\nMedium\n\nCanary\n\nYes\n\nYes\n\nLow\n\nMedium\n\nA/B Testing\n\nYes\n\nYes\n\nLow\n\nHigh\n\nShadow\n\nYes\n\nYes\n\nHigh\n\nHigh\n\nTable 11.1: Comparison of the different deployment strategies\n\nEach of these strategies has its pros and cons, and none of them would be best suited for every case. Organizations must choose the appropriate strategy based on their specific needs and the nature of the changes being deployed.\n\nThe complexity and design of the existing system that’s being upgraded or replaced, as well as its age and operation and the technology stack being used, play significant roles in determining the deployment strategy. In the next section, we will discuss the greenfield and brownfield deployments and their impact on determining the specifics of the deployment approach and plan.\n\nGreenfield versus brownfield deployment\n\nWith our intelligent MSA system ready for deployment, we now need to think in detail about the infrastructure we have or need to acquire to deploy the system in production.\n\nSome of the main questions we need to address are as follows:\n\nWhat are the infrastructure details needed to run the intelligent MSA system?\n\nDo we have the hardware and software resources needed to deploy and run the system efficiently?\n\nCan we leverage our existing infrastructure and applications to deploy the new system?\n\nWhat is the delta between the infrastructure needed and the infrastructure we have in place, and how can we fill that gap?\n\nThe organization’s current infrastructure setup and existing systems (if any) play a crucial role in answering all of the preceding questions – that is, whether the new system is being deployed in a greenfield or brownfield environment.\n\n205",
      "content_length": 1954,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "206\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nGreenfield deployment refers to building and deploying a new system or infrastructure from scratch with no previous system or infrastructure in place. Thus, we must build the new system without major constraints, dependencies, integration work, or compatibility issues to consider before building and running the new system.\n\nOn the other hand, a brownfield deployment refers to the process of deploying a new system or infrastructure on a site that is already running with an existing system in place. The site may have existing infrastructure such as servers, applications, network components, and more that may be reused for deploying the new system.\n\nIn short, greenfield deployment is a new start from scratch, whereas brownfield deployment involves building on top of existing systems or infrastructure and possibly dealing with some integration issues, compatibility concerns, and resource constraints.\n\nWhether it is a greenfield or brownfield deployment is often determined by the organization’s situation and how the organization’s business process is being conducted. Nevertheless, it is still important to understand the pros and cons of each deployment type to plan accurately.\n\nImportant Note If cost saving is a major focus in the organization, we should leverage as many existing components as possible from the existing brownfield infrastructure. However, we need to reuse existing components in a way that cannot negatively impact the deployed system’s efficiency, reliability, or functionality.\n\nThe following are some factors to consider when evaluating both deployment types.\n\nFlexibility\n\nBecause in greenfield deployment we are starting from scratch, this gives us the liberty to design, implement, and optimize the new system for the specific needs of the organization without restricting ourselves to any dependent components or existing production systems.\n\nIn brownfield, on the other hand, we must always think of the already running systems and their dependencies before designing or deploying any part of the new system. This in itself can limit the deployment, customization, or optimization of the new system.\n\nScalability\n\nGreenfield implementations offer higher scalability compared to brownfield because, in greenfield implementations, we deploy new infrastructure without any existing constraints that could limit the design or customization of the new system.",
      "content_length": 2470,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "Greenfield versus brownfield deployment\n\nThis lack of constraints gives architects and system designers the choice to design the system so that it scales without thinking of underlying technology or existing equipment that may hinder the system’s capabilities or scale.\n\nThe existing infrastructure in the brownfield’s case, however, may have legacy systems that are likely to be reused in part or as a whole whenever possible. Reusing different parts of the legacy systems may impede the new system’s scalability.\n\nMoreover, legacy systems usually allocate more physical space and are more power-consuming than modern systems, which adds more limitations to the overall system scalability.\n\nHaving said that, as we deploy the new system and gradually refresh the existing infrastructure, we will use up-to-date technologies and modern systems that will free up physical space and reduce power requirements. This, in turn, will help us eventually scale the system better.\n\nTechnology stack\n\nIn a greenfield deployment, we have the liberty to leverage the latest technologies, applications, and tools, which can, among many other things, enhance performance, security, and capability, and prolong the system’s overall lifetime.\n\nWith the legacy systems in brownfield environments, older technology, hardware, tools, and systems are used, which can introduce limitations on the system’s supportability, capabilities, scalability, and future expansions and integration.\n\nIntegration\n\nAs explained in other aspects of the comparison, in greenfield environments, all the components of the system are new and are designed and built from the beginning to work together seamlessly. Integration, therefore, is not an issue at all.\n\nIntegrating a new system with an existing IT infrastructure in the case of a brownfield deployment, however, can be challenging, as the two systems may not be fully compatible. Integration efforts may be needed for old and new components to work together, and even then, the new mixed system may later provide operational challenges that can cause unforeseen system mishaps.\n\nCost\n\nFrom an acquisition and CAPEX perspective, building a new system from scratch is higher in cost than building the system from a mix of reused and newly acquired components.\n\nTo set up the new system, a certain level of expertise is required that may not be available in-house. The effort and expertise needed to bring the system up and running will certainly have associated costs. However, it could be argued that this cost can be easily offset by the efforts and expertise needed to integrate the new and old components. Possibly different effort and different expertise, but similar cost.\n\n207",
      "content_length": 2701,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "208\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nWhen it comes to OPEX, in a greenfield deployment, we need to consider training costs for the new technologies and systems deployed, as well as potential operational mistakes that can be caused due to the lack of new system hands-on experience. In brownfield deployments, these training costs are usually lower.\n\nPower consumption is typically lower in greenfield deployments, as new systems and technologies are often geared toward power usage optimization.\n\nAnother important OPEX consideration is the potential technical debt in brownfield implementations. Technical debt is the shortcuts the organization takes to get the system up and running. In other words, this involves taking a band-aid approach to resolving an integration or operational issue during the deployment, and achieving short-term results that can be catastrophic in the long term.\n\nTime-to-market\n\nTime-to-market is an interesting aspect of the deployment and can go both ways. Generally speaking, deploying a system in a greenfield environment takes longer than integrating with an already existing functioning system, as is the case in a brownfield environment. But that would be highly dependent on how complex an existing system may be.\n\nIf we are deploying our new system on top of a significantly old or complex disorganized infrastructure, we could argue that deploying a new system fresh from scratch is much more straightforward and a time saver than trying to get both systems integrated successfully.\n\nRisks\n\nThis is another aspect of the deployment that can be debated either way.\n\nWith a lack of experience with the new infrastructure, new technologies, new systems, new tools, and new applications, system operational mishaps are more likely, and the time to resolution could be higher. In contrast, with no backup system in place, there will be no fallback option if the new system does not perform as expected.\n\nBut again, if the old system in the brownfield environment is too complex or disorganized, the risks would be higher in a brownfield deployment due to complexities in integration, potential technical debt, old unsupported components, and more.\n\nStaff onboarding\n\nCompanies that are already using an existing system have a better understanding of how it works and insights into its operations and potential issues, which can make the deployment process and system operations smoother.\n\nIn greenfield deployments, training and accumulated experience are needed before staff can start to become familiar with the system details.",
      "content_length": 2600,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "Greenfield versus brownfield deployment\n\nUser adoption\n\nUser adoption of the new system may require adapting to new ways of performing day-to-day tasks, a shift in internal and potentially external business operations, and how the organization deals with internal and external customers. This shift may require a change in organizational culture, which can pose a significant challenge to the successful implementation of the system and reveal operational shortcomings after deployment.\n\nIn a brownfield environment, the updated system capabilities could be incremental or somewhat transparent to the user, which makes user adoption much easier and faster compared to deploying a completely new system, as in the case of a greenfield deployment. A successful and gradual user adoption helps uncover potential design, implementation, and operational deficiencies that can be quickly addressed and fixed.\n\nIn either case, user training is needed, but certainly, in a greenfield, training is more complex and involved over the brownfield case.\n\nThe following table summarizes the comparison between greenfield and brownfield aspects of the deployment:\n\nGreenfield\n\nBrownfield\n\nFlexibility\n\nHigh\n\nLow\n\nScalability\n\nHigh\n\nLow\n\nTechnology Stack\n\nFlexible and optimized\n\nRestrictive\n\nIntegration\n\nMinimal to none\n\nHigh efforts\n\nCost\n\nCAPEX is high but better OPEX CAPEX is lower but higher OPEX\n\nTime-To-Market\n\nUsually longer\n\nUsually shorter\n\nRisks\n\nUsually higher\n\nUsually lower\n\nStaff Onboarding\n\nLonger process\n\nShorter time\n\nUser Adoption\n\nSlow\n\nFast\n\nTable 11.2: Greenfield versus brownfield\n\nIn this section, we discussed the main differences between greenfield and brownfield deployments, the pros and cons of each environment, what to consider, and why.\n\nIn the next section, we will go over how we can overcome deployment challenges in both environments and the deployment best practices in each case.\n\n209",
      "content_length": 1910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "210\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nOvercoming deployment challenges\n\nWe now understand the different aspects of system deployment in greenfield and brownfield environments, as well as the several challenges that can be presented during the system design and implementation. In this section, we will cover some of the concepts, strategies, and approaches for mitigating these challenges to ensure a smooth and successful system deployment.\n\nWe should begin our deployment project with a solid project team that possesses diverse skills and experiences in technical areas of the project, deployment and project management, and vendor management.\n\nIn the absence of in-house experience, outsourcing one or more project experience areas through third parties may be necessary. Partners may include system integrators, and/or equipment vendors, and value-added resellers.\n\nThis experienced team will help conduct thorough planning and research to be able to understand the specific needs of the organization, potential risks, local regulations, and any necessary compliance needs.\n\nCompliance with industry and local regulations is an essential part of the project. Aside from the technical aspects and technologies of the project, a system processing credit cards, for example, will require team members who are experienced in PCI compliance and rules. A healthcare system deployed in the United States, for example, may need members who have experience with HIPAA compliance needs, and so on.\n\nProject management is key to establishing clear team communication and collaboration. The project managers help track the project process, changes, and requirements, and ensure that the timelines and goals are met throughout the project cycles. The project managers also make certain that all stakeholders are properly informed and engaged throughout the different phases of the project.\n\nThe type of project management style or approach is dependent on the organization itself, the timeline, and the implementation details and technologies deployed. Whether it is waterfall, agile, scrum, or something else, the project manager must decide with the team.\n\nAddressing deployment challenges is a task that is pursued within the project cycle. Our focus here is on this aspect of the project cycle, particularly concerning greenfield and brownfield deployments.\n\nBefore any deployment activities, complete visibility of the deployment risks is necessary. Therefore, it is imperative to develop a clear deployment risk plan to be able to identify the risks and mitigate each risk to ensure a successful deployment.\n\nThe following chart illustrates the risk management process for our deployment cycle. The process should start with identifying the risks, determining ways to avoid or minimize them, developing a mitigation plan, and continually testing, monitoring, and reviewing the deployment to update the mitigation plan for any new risks or challenges that arise during implementation:",
      "content_length": 3017,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "Overcoming deployment challenges\n\nFigure 11.1: Main steps of overcoming deployment challenges\n\nIn the following subsections, we will go over the main activities of a risk management plan and how each phase of the plan is relevant to our deployment activities.\n\nIdentify deployment risks\n\nTo address the deployment challenges, we start by identifying the potential risks we may encounter when deploying our system. In a greenfield environment, the risks we identified earlier are as follows. The greenfield risk is referred to as GR:\n\nGR1: High CAPEX\n\nGR2: Deployment time\n\nGR3: System failures due to lack of training and experienced staff\n\nGR4: Slow or lack of user adoption\n\n211",
      "content_length": 680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "212\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nIn a brownfield environment, the risks we identified earlier are as follows. The brownfield risk is referred to as BR:\n\nBR1: System capabilities limitation due to potentially low flexibility and scalability, and some reused legacy technologies\n\nBR2: High OPEX\n\nNow that we have identified the potential risks, it’s time to prioritize them based on their likelihood and potential impact on the project. This will help us calculate the risk exposure and plan and allocate proper resources effectively.\n\nPrioritize risks\n\nRisk exposure is the risk probability multiplied by the impact of that risk on the deployment project. The higher the risk exposure, the higher the priority of working on mitigating that risk should be.\n\nFigure 11.2 shows a color-coded risk exposure matrix for the previously identified risks. Please note that the level of risk exposure can greatly vary between organizations based on various factors, such as project complexity and organizational needs, the stability and complexity of the existing system, project requirements, budget, timeline, and more:\n\nFigure 11.2: Risk exposure matrix\n\nWe always prioritize from the top right to the bottom left of the risk mitigation chart. The red zone, where the risk and the likelihood are Medium to High, is where we need to start allocating resources. This is followed by the yellow zone, then the green zone.\n\nSo, we should start with greenfield risk #1 (GR1), how to mitigate high CAPEX risks, then GR3, where we lack experienced staff to deploy the new system, then GR2, where the new system deployment time may be an issue, and then conclude the mitigation of greenfield risks by addressing GR4, where we may face a slow user adoption of the new system.",
      "content_length": 1798,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "Overcoming deployment challenges\n\nFor brownfield risk mitigation, we start with brownfield risk #1 (BR1) since it has a higher exposure in the red zone of the matrix, then do BR2.\n\nRisks are not always avoidable. Risks often aren’t. In cases where risk elimination is not possible, the mitigation plan has to address how we can at least minimize the risks to a manageable level.\n\nMany organizations choose to ignore risks with low/very low likelihood and low/very low impact. The process of mitigating risks in this particular case may very well be costly and riskier than the risk itself.\n\nDeveloping and implementing a risk mitigation plan\n\nWe need to develop a mitigation plan to manage the identified risks. This plan should include specific actions that will be taken to mitigate or eliminate the risks, resources involved, and contingencies in case the risks do occur.\n\nWe also need to identify our risk mitigation strategies based on the calculated exposure of each risk. As we will see in the next few subsections, these strategies can include implementing operational safeguards, extra measures using additional system components, testing the system before deployment, training both system users and administrators to ensure they can effectively use the new system, and developing a rollback plan in case of any unexpected deployment issues.\n\nLet’s apply all these to our deployment project’s identified risks.\n\nGR1 – high CAPEX risk\n\nWhen addressing the high CAPEX risk, we need to focus on a few things – first, the project budget, second, the system and project requirements, and third, how to use effective negotiation skills to acquire the necessary infrastructure that would successfully fulfill all of the desired system requirements.\n\nSometimes, of course, budget and time constraints become a barrier to acquiring all of the requirements’ wish list items. Therefore, it is important to prioritize your requirements, especially if you have strong budget constraints.\n\nThe objective is to acquire the infrastructure that would get us all of our wish list requirements; however, at some point, we may need to give up some of the nice-to-have features of the system for the sake of meeting our budget. This is where our negotiation skills become vital. The stronger our negotiation skills, the greater the likelihood of successfully deploying the new system and meeting all the requirements within budget.\n\nIt is useful to go over a few important negotiation techniques that can help us achieve our objective.\n\nStart by conducting thorough research on all vendors that can be involved in providing the infrastructure assets. This includes the technical and business strengths and weaknesses of each potential vendor, their list prices, the quality level of their deployment and operational support, their product roadmap, and their future business outlook.\n\n213",
      "content_length": 2875,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "214\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nThen, we can come up with multiple options from our vendor research. Having multiple options gives us more flexibility and bargaining power during our vendor negotiation phase.\n\nOut of all the available options, having a clear Best Alternative To a Negotiated Agreement (BATNA) is critical. BATNA means having the most favorable option we can achieve if the negotiations fail and no agreement is reached with the infrastructure vendors. It is the fallback option that we can rely on if the negotiations do not go as we initially planned with a specific vendor.\n\nHaving a clear and well-defined BATNA is important in displaying strong bargaining power during the negotiation process. A BATNA gives the vendors a sense of how much they can miss out if they do not close the infrastructure acquisition deal.\n\nBATNA also helps us understand when it’s time to walk away from the negotiations if the vendors are not willing to meet our infrastructure needs with the available budget. It is about the alternatives that are available to us and the consequences of choosing or not choosing a specific vendor.\n\nLarger and highly reputable organizations can use leverage. Leverage is the ability to use your reputation, market position, or market size to influence the outcome of the negotiation. Leverage can be very effective in striking a good deal with vendors. Vendors generally strive to gain a foothold in large organizations to create a customer reference or as a way to build more business out of that particular deal.\n\nHaving said all that, it is essential to show a certain level of flexibility during the negotiation. Flexibility will demonstrate our willingness to compromise and is likely to lead to a strong long-term and healthy business relationship with the vendors:\n\nFigure 11.3: Negotiation strategies\n\nFigure 11.3 sums up the negotiation strategies you could follow to minimize budget overrun risks due to infrastructure cost and the overall project CAPEX.",
      "content_length": 2040,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "Overcoming deployment challenges\n\nAnother effective way of mitigating CAPEX risks is to use a cloud provider in deploying your new infrastructure, especially during the initial testing and adjustment phases of the project.\n\nUsing containerization and virtualization to build new workloads is another way to help reduce CAPEX risks. However, by doing so, we may introduce some OPEX and other types of deployment risks, especially if the staff is not trained enough on cloud environment deployments.\n\nGR2 – deployment time risks\n\nWhen deploying a new project, especially a project that involves new technologies, techniques, and concepts, the potential of running behind schedule cannot be ignored. There is always a learning curve associated with the overall project cycle.\n\nMoreover, potential frequent changes and scope creeps are also important to account for. This emphasizes the importance of engaging a skilled team from the start even more.\n\nA highly skilled and well-trained project team will help minimize scope creep and time delay risks. In the absence of in-house subject matter experts, hiring an external party as a system integrator and building an adequate training plan for the team become critical when managing time delay risks.\n\nThere may be factors other than team skills that can contribute to project timeline and scope creep risks. As part of our risk mitigation plan, we need to account for all of these factors and make sure they are controlled to consequently control their associated risks.\n\nMSA systems can be very complex and burdensome to deploy. The more complex the system is, the more difficult it can be to predict and manage the deployment timeline.\n\nIn brownfields, the deployment of the new system often depends on the availability and operational stability of the already existing systems. Furthermore, integrating the new system with the existing systems can add another level of complexity and time-consuming tasks, all of which can potentially prolong the overall deployment timeline.\n\nOne way to mitigate the effect of complexity on the overall risk is to avoid forklift and big bang changes. Rather, use a trickle approach by breaking down the deployment into multiple simple phases and stages. Follow the famous Einstein rule, “Everything should be made as simple as possible, but not simpler.” Simplify as much as possible, but not in a way to compromise the system’s functionality or reliability.\n\nAlthough thorough testing is necessary to ensure the new system’s reliability and adherence to the organization’s needs, over-analyzing and over-testing often happen in complex projects. This process can significantly delay the system deployment.\n\nOne way to address the testing delays factor is to consider running the system in production in a Limited Availability (LA) fashion, a beta version, or a pre-launch period of some sort. This LA approach will help us apply real user traffic to the system while we focus on monitoring and making system changes as needed before we transition the system into full-scale production.\n\n215",
      "content_length": 3075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "216\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nGR3 and GR4 – system failure risks and user adoption risks\n\nUser adoption can highly depend on any new user interface/user experience (UI/UX) changes or added complexities. Assuming a well-designed system’s UI/UX, system failure and slow user adoption risks are also dependent on the experience of the project and operation teams, and how familiar the users are with working on the system.\n\nTo normalize the experience and user familiarity risks, it’s important to first include a solid UI/UX design team from the outset of the project, then adopt a top-down approach by securing a strong buy-in from project sponsors and executives.\n\nThe top-down approach will help adopt the organization’s processes and changes necessary to create a cultural shift in conducting the business using the new system. The buy-in can also help enforce a training program for both system users and the project team. This training can significantly help in bridging the gaps between the existing experience of the project team and the experience needed.\n\nBR1 – system capability limitations\n\nBecause we are deploying the new system with some older components being used in the existing system, we are likely to run into integration incompatibilities, limitations in the older hardware and software features, and limitations in scaling in terms of traffic, data loads, or storage.\n\nAdditionally, a system with legacy components may become outdated sooner than anticipated. This can result in obsolete software and hardware components that are no longer usable or valuable, or in vendor out-of-support announcements, which would shorten the life span of the new system.\n\nWhen system components are out of support, vendors can no longer provide part replacements or software updates, or even assist in case any operational issues arise on the out-of-support components. This can severely impact the system’s reliability and jeopardize the organization’s business continuity.\n\nTo mitigate the risk of system capability limitations, we must have clear visibility and understanding of the vendor product map for each reused legacy system component and clear visibility of the component’s dependencies. This understanding shall help us assess the impact of that component on the scalability levels, and future operational reliability and stability of the system being deployed.\n\nBR2 – high OPEX\n\nAs we have previously explained, integrating both new and older systems can introduce many deployment and operational challenges that can make the system much more complex, and introduce technical debt, high maintenance costs, and high operational costs to keep the system running smoothly.\n\nSecurity is also a major concern in brownfield deployments. Technical debt, along with mixing older and new components, may introduce vulnerabilities that were not present in the existing infrastructure, which can lead to costly data corruption, data loss, data recovery, security breaches, and irreversible reputational damage.",
      "content_length": 3062,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "Overcoming deployment challenges\n\nTo mitigate these risks, organizations should thoroughly evaluate the impact and have a clear cost-benefit analysis of each reusable component of the existing system before proceeding with the deployment.\n\nFurthermore, having a robust disaster recovery and backup plan in place in case of data loss or corruption is key to mitigating some OPEX risks.\n\nAll the preceding risk examples and their mitigation strategies should be thoroughly discussed as part of the developed risk mitigation plan, with step-by-step guides and documentation.\n\nShould any of these risks take place, especially in the case of brownfields, where an existing system may already be running, a comprehensive rollback plan should be executed immediately. The rollback plan is the next step in how to overcome the deployment challenges. The following subsection takes us through what a rollback plan is and what it entails.\n\nThe rollback plan\n\nRemember Murphy’s Law, which states “Anything that can go wrong will go wrong”?\n\nHow often do we create a solid and well-crafted implementation or migration plan, expecting a smooth change in the system, but then experience unexpected and bizarre behavior during the execution of the plan?\n\nA rule of thumb here is that anything can go wrong during the deployment. We put enough planning and precautions for nothing to go wrong, but unfortunately, things do not always work in our favor. We may still overlook things, system bugs may get triggered, equipment mishaps may happen, and so on.\n\nTherefore, developing a rollback plan is necessary to be able to maintain business continuity. We should have the plan built in a way that it includes clear steps and procedures to move back to the initial system state before the change and resume normal operations quickly.\n\nAdopting a phased deployment approach, as previously discussed, helps to quickly roll back only a portion of the change, which helps us avoid wasting resources, valuable change window time, and efforts invested during the deployment, and in developing the deployment plan.\n\nTest, monitor, and adjust\n\nThe next step in overcoming deployment challenges is to test and validate the system’s requirements and functionality to ensure it meets the performance, functional, and operational requirements.\n\nThe project teams, when under time pressures, often deprioritize system security over system functionality. Data security and data protection, when overlooked, can severely impact the project’s overall deployment and reliability, especially with systems that handle critical user data and have to comply with certain regulations and compliance acts, as in the case of PCI or HIPAA.\n\n217",
      "content_length": 2700,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "218\n\nManaging the New System’s Deployment – Greenfield versus Brownfield\n\nTherefore, the test plan has to have a specific section dedicated to system security and compliance testing. Hiring a specialized firm in the area of security and compliance helps minimize the risk of data breaches or other security incidents.\n\nAs we gradually apply test and production load to the system, the test plan should be able to ensure the systems being deployed are scalable and flexible enough to adapt to changing needs and requirements. This is a critical aspect of the test plan during a brownfield deployment since the integration with the existing system may hinder the overall system’s capabilities.\n\nThen, we need to continuously and carefully monitor and review the identified risks throughout the project to ensure that the risk mitigation plan is effective and that any new risks are identified and addressed promptly.\n\nAny newly identified risks will have to be included in the mitigation plan through the project change management process. The newly identified risks and mitigation strategies will need to be communicated to all stakeholders, including the project team, the organization’s management, and the system users.\n\nPost-deployment and pre-production review\n\nOnce the deployment is over, the system is operational, tested, and running in a pre-production or LA manner. Just before closing the deployment project, we need to evaluate the effectiveness of our risk management plan, identify any areas for improvement, and document the outcome of our findings. The outcome can then be integrated into the same project deployment effort or conclude the existing deployment and initiate a new project for that matter.\n\nThe post-deployment assessment will ensure the new system’s continued stability, performance, and reliability.\n\nIn brownfields, we may end up running in a bi-modal approach, where both systems are running at the same time and serving users at the same time but in different ways and at different levels. In this case, we need to consider building specific roles and responsibilities matrices for each system. This helps streamline operations and increase the system’s supportability.",
      "content_length": 2204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "Summary\n\nIn this chapter, we covered greenfield and brownfield deployments, the difference between each, their pros and cons, risk details during the deployment process in general, and the specifics of each risk in each deployment case.\n\nWe also provided examples of the risks associated with greenfield and brownfield deployments, along with the strategies to mitigate these risks, to gain a better understanding of the challenges involved in successfully deploying a new system.\n\nThe topics that were discussed in this chapter act as introductions to what we will be learning in the following chapter. In the next chapter, we will apply some of what we have learned in this chapter and discuss ways to test, monitor, and update our new ABC-Intelligent MSA system.\n\nSummary\n\n219",
      "content_length": 779,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "12 Deploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nIn the previous chapters, we talked in detail about microservices, monolithic architecture, the pros and cons of each architecture, how to transition into MSA, and how to make the MSA system smarter using AI services. We also discussed, in Chapter 11, some of the best practices for deploying the MSA system.\n\nIn this final chapter, we will integrate all the topics and concepts covered throughout the book to understand how we can apply what we have learned through hands-on and practical examples.\n\nBefore we dive into the details, we need to understand what existing system we have in place first.\n\nObviously, every organization is different and has different deployment needs, criteria, and dependencies. Some organizations will deploy in a greenfield, and others in a brownfield. In order to walk you through detailed practical examples and steps for deploying, testing, and operating an intelligent MSA system, we will assume a brownfield environment with an existing monolithic architecture system.\n\nWe will sometimes use our ABC-Monolith as an example of the existing system to illustrate the concepts covered in the chapter. In this chapter, we will cover, the following topics:\n\nOvercoming deployment dependencies\n\nDeploying the MSA system\n\nTesting and tuning the MSA system\n\nThe post-deployment review",
      "content_length": 1387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "222\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nOvercoming system dependencies\n\nBefore deploying the ABC-Intelligent-MSA system we built earlier in Chapter 10, it is important to first decide what our deployment strategy should be. Based on the requirements, cost, complexity, and pros and cons of the deployment strategies we discussed in Chapter 11, we believe the best deployment strategy for our ABC system would be a mix between the ramped deployment and canary deployment strategies.\n\nThis deployment strategy will allow us to keep the ABC-Monolith system online and serve users uninterrupted while we deploy the new ABC-Intelligent-MSA system. We will gradually replace older components in ABC-Monolith with the corresponding microservices in the ABC-Intelligent-MSA system. This can be accomplished by routing traffic from the older components to those ABC-Intelligent- MSA system microservices.\n\nAlthough this trickle approach has lower cost, lower complexity, and lower risk than other deployment approaches, we still need to carefully study the incompatibilities, dependencies, and the proper integration between older and newer components.\n\nFurthermore, we will need to evaluate which of our infrastructure and existing system ABC-Monolith’s components can be reused in the new architecture, if any.\n\nReusable ABC-Monolith components and dependencies\n\nWe cannot think of a specific ABC-Monolith code base component that can be reused as is without modification. All of the ABC-Monolith components will have to be either rewritten from scratch or modified to different degrees to be compatible with the ABC-Intelligent-MSA system.\n\nSome of the ABC-Monolith and existing infrastructure components that we know can be reused are the business logic itself, server infrastructure, operating systems, virtualization infrastructure, data storage, network infrastructure, existing monitoring, and network management tools, and some of the software and database licensing. Nevertheless, even these components may need to be updated or upgraded in order to perform the functions of the new system.\n\nIn our system installation, command line, and code examples listed in earlier chapters, we had the most updated Ubuntu, Python, and database versions. In a real-life situation, however, that may not be the case; we will likely have the monolithic application running in an older operating system and have an older Python and/or database version.\n\nThese situations may produce some incompatibilities between the older components and the newer ones. An older Python version, for example, may have some deprecated functions that are no longer valid with the new MSA code base, and hence, will require some updates or upgrades to the existing system. Furthermore, the potentially different technology stack may also produce more dependencies.",
      "content_length": 2868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "Overcoming system dependencies\n\nIn order to minimize these dependencies, we would rather deploy the new system components on a separate server or virtual infrastructure with their own environment, including their own data storage and using their own technology stack. The new environment will have a container engine that will carry all of our ABC-Intelligent-MSA microservices.\n\nIt is important to note that each system is different, and the specific reusable and non-reusable components will vary depending on the existing monolithic system. A thorough analysis and evaluation of the existing system’s components is necessary to determine what can and cannot be reused when migrating to a microservices architecture.\n\nMitigating ABC-Intelligent-MSA deployment risks\n\nSome of the risks discussed in Chapter 11 are relevant to our scenario. However, we still need to determine which CAPEX risks are applicable, examine the risks related to deployment time, potential service disruption, and OPEX, and take specific actions to mitigate these risks.\n\nSince we are using containers on top of virtualized infrastructure in our implementation, CAPEX risks are significantly reduced. As long as the existing infrastructure has the storage and workload capacity to absorb the new ABC system, we are safe. If additional infrastructure resources are needed, we may then need to look into some capacity planning and upgrades to be able to run the system during and after the deployment.\n\nAdopting a trickle migration approach gives us the chance to catch up quickly with any learning curve involved with the new technologies being deployed, which, in return, helps mitigate system failure risks and deployment delays.\n\nThe ramped deployment strategy also helps mitigate other OPEX risks. As we will discuss in this chapter, during the deployment, we can test and monitor the performance of the newly deployed components, identify and resolve any issues, and make necessary adjustments before redirecting all traffic to these new components.\n\nAnother way of mitigating OPEX risks is to establish a robust change management process by establishing a structured and transparent process for managing changes. This includes creating clear guidelines for how changes will be proposed, evaluated, approved, and implemented, as well as communicating the changes to relevant stakeholders.\n\n223",
      "content_length": 2374,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "224\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nPart of the change management process is the rollback plan. The rollback plan is essential to bringing back the system to an operational state if a specific technical change is unsuccessful. The following are the steps we need to consider to build a successful rollback plan:\n\n1. Specify some checkpoints for the change where a rollback may be needed. In our example, and should the ACL be used, the ACL would be deployed prior to switching any traffic to the new microservice. During the change (and right after switching some test traffic to the ABC-Intelligent-MSA), a few good checkpoint examples would be as follows: I.\n\n1. Specify some checkpoints for the change where a rollback may be needed. In our example, and should the ACL be used, the ACL would be deployed prior to switching any traffic to the new microservice. During the change (and right after switching some test traffic to the ABC-Intelligent-MSA), a few good checkpoint examples would be as follows: Testing the payment verification communication between ABC-Monolith and the ACL\n\nTesting how the ACL processes the requests\n\nIII. Testing the communication between the ACL and the ABC-Intelligent-MSA system\n\nIV. Testing how the overall end-to-end requests are handled and whether they are processed\n\nas expected\n\nCommon Docker and Linux commands to test and troubleshoot the communication between the ACL, the monolith, and the MSA include the following:\n\n curl, to simulate an API call to the ACL or a specific microservice\n\n netstat, to check whether a specific service is actively listening to connections, what the\n\nlistening port is, and whether there are any active connections\n\n docker inspect, to return detailed JSON information about a specific microservice’s\n\nconfiguration, state, and network settings\n\n docker log, to view the logs of a running container\n\n2. Develop a plan for reversing the change at each of the preceding specified checkpoints.\n\n3. Whenever possible, test the rollback plan in a test or staging environment to ensure it is workable and complete.\n\n4. Know the time by which the rollback plan needs to be completely executed at each of the specified checkpoints, and allocate a reasonable amount of time in your change for it.\n\n5. Monitor the system throughout the change and make adjustments as needed.\n\n6. Have a post-mortem after the change, especially in case of a change failure.\n\n7.\n\nIn case the rollback plan is executed, the team will then need to include in the post-mortem the reasons for the change failure, and how effective the rollback plan was. They need to accordingly make the necessary adjustments to the deployment and rollback plan before scheduling another change.",
      "content_length": 2767,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "Deploying the MSA system\n\nBy this point, we should have a clear understanding of the deployment dependencies and risks and be able to determine methods for mitigating them. We are ready now to create a deployment plan and execute it in a manner that minimizes downtime and maintains business continuity.\n\nIn the next section, we will build the ABC-Intelligent-MSA system’s deployment plan in the presence of the running ABC-Monolith system.\n\nDeploying the MSA system\n\nIn Chapter 9 and Chapter 10, we discussed in detail how to install Docker, containers, and other components for our ABC-Intelligent-MSA system. This installation was mostly done in a lab environment with no specific regard to any existing system in the environment. We were basically just simulating a real-life development or staging environment.\n\nIn this section, we will focus rather on how we can take the ABC-Intelligent-MSA system we built, and gradually migrate it into a brownfield production environment where we have the ABC-Monolith system already running in production. The goal is to ramp up the ABC-Intelligent-MSA system’s operations until the system is able to carry the entire existing traffic, then completely phase out the old ABC-Monolith. Everything should be done with minimal operational interruptions.\n\nThe current status by now is that we still have ABC-Monolith running in production, and the ABC-Intelligent-MSA running in the staging environment. The following are detailed broken-down deployment plans with their execution steps.\n\nThe anti-corruption layer\n\nBoth our ABC-Monolith and ABC-Intelligent-MSA systems use the same type of RESTful APIs and the same JSON data formats. Moreover, our demo system is not complicated enough to justify an ACL. We, therefore, won’t be needing an ACL in our migration. However, we developed an ACL in our demo just in case you decide to try it out.\n\nIn case you are interested in trying the ACL, the first step you would need to do is to get the ACL up and running. The ACL will act as a buffer and handle the communication between the ABC-Monolith and the ABC-Intelligent-MSA systems.\n\n225",
      "content_length": 2124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "226\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nFigure 12.1: Deploying with the ACL\n\nACLs are usually a specific custom-built code for the organization’s specific situation, the old system, and the new MSA system. We built the abc_acl ACL for our ABC system. The abc_acl code can be found in our GitHub repository.\n\nIt would make much more sense to deploy all the new components, including abc_acl, on a separate host or virtual workload. In our lab examples, however, and for simplicity, we are building the new system’s containers on the same host that’s running the ABC-Monolith.\n\nWe built the Facade, Adaptor, and Translator components all together as part of the ACL in one microservice. The Facade is created to interface with the ABC-Monolith, the Adaptor to interface with the ABC-Intelligent-MSA, and the translator for input/output data format mappings. Since we are using the same data formats in both the monolith and the MSA systems, the translator code is not doing any processing and is just used as a placeholder.\n\nWe can set up and start the abc_acl microservice the same way we did with other microservices in Chapter 9 and Chapter 10, using the docker build command to build the abc_acl_image image from the Dockerfile, then using the docker run command to create the abc_acl_container container, as follows:\n\n$ docker build -t abc_msa_customer_management ~/\n\nOnce the image is successfully created, use the following command to run the container, and start listening to port TCP/8020 on the host’s IP:\n\n$ docker run -itd -p 8020:8080 --name abc_acl_container abc_ acl_image",
      "content_length": 1622,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "Deploying the MSA system\n\nNow the ACL is running, it is time to test it before routing any traffic to it. We can do that using the shell curl command as we did in the previous chapters, or we can use some of the ACL built-in API tools created to verify the connection.\n\nThe following is a curl command issued on the host machine to ensure that the ACL is running successfully:\n\n$ curl http://192.168.1.100:8020/ <!DOCTYPE html> <head> <title>The Anti-Corruption Layer Microservice</title> </head> <body> <p>This is the ACL Microservice Part of ABC System. This ACL is used as part of the process of migrating ABC-Monolith to the new system, ABC-Intelligent-MSA </p> </body>\n\nThe following is an example of another way to test the ACL – more specifically, to test the communication between the ACL and both ABC-Monolith and ABC-Intelligent-MSA systems:\n\n$ curl http://192.168.1.100:8020/api?func=test_com {\"monolith_com_test\": \"Communication SUCCESSFUL\", \"msa_com_ test\": \"Communication SUCCESSFUL\"}\n\nThe first curl command ensures that the ACL is listening to API calls from both ABC-Monolith and ABC-Intelligent-MSA, while the second curl command ensures that the ACL can successfully communicate with both systems.\n\nThe ACL operation is now verified; in the next subsection, we will start migrating the MSA services from the staging (or lab) environment to the actual production environment running the old monolithic system.\n\nIntegrating the MSA system’s services\n\nWith the ACL now up and running and tested successfully, we are ready to start switching specific traffic to specific parts of the ABC-Intelligent-MSA. We will, however, use direct interaction between both the monolith and the MSA system since the ACL is not really needed in our demo example.\n\nPlease note that, depending on the existing monolith structure, design, and system’s code base, this process could either be very straightforward or as complicated as can be. Our deployment strategy requires some code changes in the monolith system to be able to route some parts of the traffic to the new MSA.\n\n227",
      "content_length": 2078,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "228\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nFor that reason, we may very well choose, in some systems, to have the MSA completely tested in a staging environment, then put the MSA on an LA period where partial production traffic is passing through the system for deeper testing. Then, once comfortable with the new MSA system’s performance, we can just start forwarding the entire production traffic, and finally, shut down the old monolith.\n\nFigure 12.2 shows a high-level view of the migration before and after status. During the migration, we will route a specific function of ABC-Monolith to one microservice in ABC-Intelligent-MSA. That microservice should be able to replace the corresponding function in the ABC-Monolith system. After we test the operation of that part of the migration, we then move traffic of another monolithic function, then another, and so on, until we end up migrating all of the ABC-Monolith functions to the ABC-Intelligent-MSA system.\n\nFigure 12.2: A high-level view of where we are and where to be\n\nWe can start with a simple microservice such as Notification Management (abc_msa_notify_ user_container). We can route the traffic destined to the notify_user() function in the ABC-Monolith by replacing the function’s code with an API call to abc_msa_notify_user_ container. All user traffic will still flow through the ABC-Monolith, but all user notifications will be processed through the ABC-Intelligent-MSA.",
      "content_length": 1477,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "Deploying the MSA system\n\nIn the same manner, the Customer Management microservice (abc_msa_customer_management_ container) should replace the register_customer() function from the ABC-Monolith, and Order Management should replace place_order() and order_status_update() functions, and so on.\n\nAs the system stabilizes, we gradually migrate to other MSA services. That migration cycle is shown in Figure 12.3.\n\nBy following the migration cycle, eventually, all of the ABC-Monolith functions will be replaced with microservices in the ABC-Intelligent-MSA system.\n\nFigure 12.3: The microservices integration testing and tuning cycle\n\nFigure 12.4 shows a snapshot of the system status during the migration process. In the figure, we have the Notification Management, Customer Management, and Order Management microservices successfully migrated, but not any other microservice yet.\n\n229",
      "content_length": 883,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "230\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nFigure 12.4: A system snapshot during the migration process\n\nMonitor the system closely during the monolithic function migration, and use the rollback plan if necessary. Once the last ABC-Monolith function is migrated to the new system, we will need to carefully run an end-to-end test on the ABC-Intelligent-MSA system to ensure the system is running properly and independent of the ABC-Monolith.\n\nTesting all microservice logs and stats is essential in the testing process. We need to have a formal testing process in place every step of the way during the migration process. The test process is described in more detail in the next section.\n\nKeep both systems running for a period of time just in case some overseen issues take place and always be prepared with a contingency plan.\n\nThe final step is shutting down the monolith. If the migration steps were followed and tested correctly, user traffic and system operations should not be impacted. However, complex systems may have a component or more still processing traffic. To avoid business interruptions in this situation, it is best to shut down the monolith during a maintenance window to allow the migration team to analyze any unforeseen issues and create a plan to resolve them.\n\nIn this section, using our ABC system, we explained the MSA system deployment process using an ACL and using a direct monolith-to-microservices approach. We covered the steps to be taken, what to watch for, and how to make the transition to the new system as smooth as possible with minimal system interruption.",
      "content_length": 1631,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "Testing and tuning the MSA system\n\nIn the next section, we will cover the formal test methodology that should be planned and followed after every microservice migration to ensure system reliability and stability.\n\nTesting and tuning the MSA system\n\nPrior to deploying microservices, a formal testing or QA process should be applied to each microservice to prevent errors during deployment and in production.\n\nThere are a couple of tests that need to be performed on the MSA system microservices before deploying them in the production environment. First, testing the microservice itself as a standalone and before integrating it into any parts of the ABC system – what we refer to it as, unit testing. Second, testing the integration of that microservice into the ABC-Intelligent-MSA system – what we refer to it as integration testing. And third, testing how the microservice functions during an interim mix of operations between the ABC-Monolith and the ABC-Intelligent-MSA systems.\n\nTesting the ABC system functions every time a new microservice is deployed is crucial to ensure a successful migration and that the system is able to properly function and sustain the applied traffic load and user requests.\n\nBuilding structured test cases is an important part of the testing process. Test cases are a set of steps that describe how to test a specific feature or functionality of a system. These test cases should be well defined, easy to understand, and should cover all possible scenarios.\n\nCreating a test case should include the following main steps:\n\n1.\n\nIdentify the requirements of the system and the feature or functionality that we want to test.\n\n2. Write a test case that describes the steps to be taken to test that feature or functionality.\n\n3.\n\nIn the test case, specify any prerequisites that are required to run the test.\n\n4.\n\nIdentify the pass/fail criteria based on the expected outcome of the test case.\n\n5. Run the test case and compare the test results to the expected outcome. Accordingly, and based on the pass/criteria specified, record the result of the test in simple PASS or FAIL terms.\n\nThe following is a simple example of a test case for the Notification Management microservice. The test case verifies that the microservice is actually sending an SMS notification to a registered user’s mobile number. Another test case should also be written to test the microservice’s email Send functionality. We can write as many test cases as needed for each individual microservice, and for the system functionality overall.\n\n231",
      "content_length": 2550,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "232\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nTest Case Details\n\nTitle\n\nABC-Intelligent-MSA Notification Management Microservice SMS Functionality\n\nID\n\n002912\n\nDescription\n\nTo ensure that the microservice is actually sending an SMS notification to the specified mobile number\n\nRequirement(s)\n\nAccess to the SMS gateway.\n\nTest Setup\n\nHave access to the receiver test phone +1 (555) 555-5555.\n\nHave access to the Ubuntu test environment.\n\nProcedure\n\nVerify SMS gateway access. Ensure the abc_msa_notify_user_container container is running, or start it as follows:\n\ndocker container start abc_msa_notify_user_container\n\nIssue a shell curl command as follows:\n\ncurl http://192.168.1.100:8010/api?func=send_ sms&num=15555555555&msg=order+received\n\nTest Type\n\nUnit testing\n\nPass/Fail Criteria\n\nTest case passes if you receive the message “order received” on the test phone\n\nTable 12.1 – A sample test case for notification management in ABC-Intelligent-MSA\n\nTesting the AI services of the ABC-Intelligent-MSA system can be more challenging, and the conventional test case approach may not be sufficient. Testing the AI part of the system will require a multi-level approach that would require including the microservice itself in isolation (unit testing), integration testing, functional testing, performance testing, data validation testing, and human-in- the-loop testing. By using all these approaches together in building your test cases, we can ensure that the AI components of the system are functioning as intended and are making accurate predictions and decisions.\n\nIn this section, we covered the importance of building a structured testing process and built a test case example as part of our system’s testing process. We discussed how to create a test case and identify the requirements and expected outcomes.\n\nIn the next section, we will talk about the importance of conducting a post-deployment review after the completion of the ABC-Intelligent-MSA system deployment. The section will also cover the different types of post-deployment reviews, including user feedback reviews.",
      "content_length": 2116,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "The post-deployment review\n\nThe post-deployment review\n\nThe ABC-Intelligent-MSA system is currently running, but it hasn’t been operational for a sufficient amount of time to guarantee its stability and resilience under typical traffic patterns and loads. A post-deployment review is crucial for ensuring the success of the ABC system deployment and its compliance, as well as enhancing its functionality and overall user satisfaction.\n\nDuring the post-deployment review, we will need to monitor the system closely and look for any errors, bugs, or any other operational problems that may happen. Then, we will need to make recommendations for addressing system issues and making necessary improvements to the system to ensure that the system is meeting the user requirements it was created for.\n\nWe need to have special monitoring for the AI services we built in the system to make sure they are performing as they are supposed to and continuously improving themselves and the system’s operations overall. A closer look at the AI services logs that we discussed in Chapter 10 is important to ensure the system’s stability and enhanced performance.\n\nThe following are some of the aspects and criteria that need to be considered when conducting a post-deployment review.\n\nChecking the new system’s performance\n\nWe start by defining performance metrics, which will help us create a baseline for what to expect from the system, in terms of response time, user interactions, network traffic, and so on. We can use tools available on the internet or the ms_perfmon.py we previously discussed in Chapter 10 to measure the performance of the new system and compare that to the monolith’s performance.\n\nThe variance between both the old and the new system’s performances would highly depend on the design, architecture, operational criteria, and infrastructure used in both cases.\n\nIdentifying and fixing system defects\n\nThis goes back to the testing and tuning process discussed earlier, and how the process should be conducted. It is important to point out here that post-deployment, identifying system defects is not yet part of the QA process until they are first documented in the organization’s defect tracking system.\n\nWe are talking here about monitoring the operational aspects of the system and ensuring proper system supportability. The support process may very well lead to filing specific issues found in the system post-deployment. Later, a thorough investigation of customer support cases with their severity levels will need to be conducted to address and fix these issues.\n\nSystem issues can also be identified by gathering customer feedback, as we will discuss in the next couple of sections, as well as from the outcomes of the different audit processes conducted on the system post-deployment.\n\n233",
      "content_length": 2810,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "234\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\nCompliance\n\nPerform regular maintenance and updates to the system to keep it running smoothly. As briefly discussed in Chapter 8, a considerable part of compliance can be done through automation or commercial tools. The tools will help audit the system for different types of compliances, such as the GDPR, PCI, HIPAA, SCSEM, and so on. The specific compliances that an organization has to comply with will depend on the organization’s business itself, the nature of the system, and what processes and users it is serving.\n\nStart by identifying the relevant regulations and standards that apply to the new system. This may include data privacy regulations, industry-specific standards, and cybersecurity standards.\n\nConduct a risk assessment the way we described in the previous chapter, to identify any potential areas of non-compliance and their associated risks. This may involve reviewing the process of the system’s design, architecture, and data processing. Then, put together a mitigation plan to mitigate the identified risks.\n\nMake sure the organization’s staff are fully aware of compliance, its importance, and the individual roles and responsibilities in that regard. Keeping the staff trained is another aspect of keeping the organization compliant with specific rules, regulations, and specific industry compliances.\n\nThe compliance process is not a one-time thing, the organization has to conduct regular audits to maintain that compliance. Audits may include regularly running specific automated audit tools, and conducting manual system audits by checking system logs, data checks, physical and digital security checks, and so on.\n\nSystem maintenance and updates\n\nJust like your preventive car maintenance, performing regular system maintenance and updates is important to keep the system running smoothly with no sudden unplanned failures.\n\nBy planning, preparing, testing, implementing, monitoring, documenting the process, and taking a proactive role, we can ensure that our newly deployed system is functioning as expected and able to minimize operational interruptions. The following are a few points to consider in the maintenance plan:\n\n1. Put together a regular maintenance plan. This is a must-have for successful and reliable operations. This includes which part of the system needs to be updated, what maintenance activities need to be conducted and how often, prioritizing the maintenance tasks, and determining the resources required.\n\n2. Make sure you have a regular system backup plan in place and have an updated backup before any maintenance work. This is important to bring back the system to its original state in case of any work mishaps.\n\n3. Test any planned work before actually applying the update or the change. Test the change thoroughly in a lab or staging environment to ensure it is functioning as expected.",
      "content_length": 2929,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "The post-deployment review\n\n4. Monitor the system after the updates have been deployed. This includes monitoring performance metrics, running automated checks, checking users’ feedback, and checking system logs.\n\n5. Update your documentation with the changes, and document the maintenance and update outcome. The documentation will help ensure that the maintenance and update process is repeatable for future reference, and help troubleshoot in case of any issues that may happen in the future.\n\nIn the beginning, the maintenance plan may not be as perfect as you may like it to be, but as the process is repeated during the lifetime of the system, the process will eventually get refined to a very accurate level.\n\nUser satisfaction\n\nMonitoring and improving user satisfaction are sometimes underestimated in the success of deploying any new IT system. By gathering feedback from the system’s internal and external customers, analyzing that feedback, prioritizing changes, implementing changes, monitoring progress, and continuously improving, we can ensure that the system meets customer requirements.\n\nThe following is a four-step cycle for ensuring high customer satisfaction post-deployment:\n\nFigure 12.5: The four-step customer satisfaction cycle\n\n1. The first step in monitoring our customer satisfaction is to gather feedback from the system users. The feedback can be collected through surveys, direct customer interaction and visits, phone conversations, and so on.\n\n235",
      "content_length": 1480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "236\n\nDeploying, Testing, and Operating an Intelligent MSA Enterprise System\n\n2. Analyze the gathered feedback to identify common complaints, common patterns, and specific use cases that may have not been covered during the system testing phase. This will help us understand the strengths of the system and the areas where we need to improve.\n\n3. Prioritize whatever system changes are decided as an outcome of the gathered feedback. This part should have the biggest impact on customer satisfaction. It will show customers that you are addressing their concerns, reacting to their requests, and, sometimes, even being proactive to customer needs. Implement the changes as prioritized. Start with the changes with the highest impact and lowest effort similar to what we discussed in Chapter 11 under the Risk mitigation section and in Figure 11.2.\n\nWe need to continuously regather customer feedback to regularly monitor customer satisfaction progress. This will ensure that the changes being carried out are having the desired customer satisfaction effect.\n\nThe four-step process will help continuously meet customer needs and improve customer satisfaction accordingly. The process helps also constantly improve the system features, supportability, reliability, and stability to enhance the overall user experience.\n\nIn this section, we covered the post-deployment review process, the different aspects, and activities that need to be considered when conducting the review, and how that is essential in the overall success of the ABC-Intelligent-MSA system operations.\n\nSummary\n\nIn this chapter, we discussed, the various steps involved in the successful deployment of the new system. We talked about the importance of overcoming the system deployment dependencies, the importance of building structured test cases, and the steps involved in testing and tuning the system.\n\nBy following the steps outlined in this chapter, organizations can ensure the successful deployment of their MSA system, including overcoming dependencies, integrating with the monolith during the transition phase, testing and tuning the system, and conducting a post-deployment review. The chapter concluded by emphasizing the significance of following a customer satisfaction cycle and having customers engaged in the process of adapting the new system’s operations.",
      "content_length": 2342,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "Symbols\n\nDynamic Host Configuration Protocol (DHCP) 173\n\nA\n\nABC-Container Engine 152 ABC-Intelligent-MSA deployment risks\n\nmitigating 223, 224\n\nABC-Intelligent-MSA system\n\ninitializing 188 operations 187 operation, simulating 191 training data, building 189, 190 training data, using 189, 190\n\nABC-Monolith 29 architecture 30 components, reusing 222, 223 current functions 30 database 31, 32 data decomposition 35-38 dependencies, reusing 222, 223 function decomposition 33, 34 requests decomposition 38-40 workflow and current function calls 32, 33\n\nIndex\n\nABC-MSA\n\nenhancements 58, 59 ABC-MSA containers\n\nAPI Gateway 164, 165 creating 162, 163 Customer Management\n\nmicroservice 165-169\n\nfrontend web dashboard interface 169 services 163 system’s containers, managing 170, 171\n\nABC-MSA database access 37 ABC-MSA microservices inter- communication 172\n\nDocker network 172, 173 TCP/IP communication, between\n\ncontainers/microservices 173-175\n\nABC-MSA workflow 39 A/B testing deployment 204 ACID transactions 17 ACL Translator 45 adversarial search 122\n\nmethods 123\n\naggregator communication pattern 53 aggregators 53\n\nversus API gateway 54, 55 versus orchestrators 54, 55",
      "content_length": 1171,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "238\n\nAgile development methodology 142, 143 AI microservice building 178 enhancements, anatomy 179-181 self-healing process 181-183 tools, building 184 AI microservice, tools\n\nartificial intelligence\n\nversus deep learning 63-67 versus machine learning 63-67\n\nArtificial Neural Networks (ANNs) 65, 66 artificial neurons 65 Authentication-Authorization- Accounting (AAA) 48\n\nAPI response error simulator 187 API traffic generator/simulator 184 microservices performance\n\nmonitor 185, 186\n\nresponse delay simulator 186\n\nautoencoders 94, 131 Autograd 72 automation 110, 143, 144 Auto-Regression (AR) 86 Auto-Regressive Integrated Moving\n\nAI service operations\n\nAverage (ARIMA) 86\n\nanalyzing 191 PAD operations 197-200 PBW operations 192-196\n\nB\n\nanomaly detection 130 anomaly detection, types collective anomaly 130 contextual anomaly 130 point anomaly 130\n\nAnti-Corruption Layer (ACL) 44, 225-227\n\ncomponents 45 using, to isolate MSA 43-45\n\nAPI gateway\n\nBest Alternative To a Negotiated\n\nAgreement (BATNA) 214\n\nbias 78 big bang migration 44 binary classifier 81 Blue/Green deployment 204 bottleneck 95 brownfield deployment 206 brownfield risk (BR) 212 business continuity (BC) 6\n\ndisadvantages 49 functions 48 using 46-49 versus aggregators 54, 55 versus orchestrators 54, 55\n\nAPI response error simulator 187 API traffic generator/simulator 184 Area Under Curve - Receiver Operating\n\nCharacteristics Curve (AUC-ROC) 120\n\nArea Under the Receiver Operating\n\nCharacteristics Curve (AUROC) 108\n\nC\n\nCanary deployment 204 choreography 18 circuit breaker 57 coefficient of determination 77 collective anomaly 130 Command Query Responsibility Segregation (CQRS) 20-22\n\nComma-Separated Values (CSV) 185",
      "content_length": 1691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "239\n\nCommon Closure Principle 28 Community Edition (CE) 157 compensating transactions 17 concept shifts 120 Configuration-as-a-Code (CaaC) 146 containers 151, 152\n\ndeep learning features 66 libraries, used in Python 67 used, for enhancing models 87-95 used, for implementing system\n\nself-healing 130\n\nuses 153-156 versus VMs 152, 153 contextual anomaly 130 Continuous Integration/Continuous\n\nDevelopment (CI/CD) 23, 141\n\nConvolutional Neural Networks (CNNs) 67 cost function 65 covariate shifts 117 Create-Read-Update-Delete (CRUD) 20\n\nversus CQRS pattern 21\n\nD\n\nversus artificial intelligence 63-67 versus machine learning 63-67\n\nDenial of Service (DoS) 128 density ratio estimation 123 deployment challenges, overcoming 210 modifying, mitigation plan 217, 218 monitoring, identified risks 217, 218 risk mitigation plan, developing 213 risk mitigation plan, implementing 213 risks, identifying 211, 212 risks, prioritizing 212, 213 rollback plan 217 testing system’s requirements and\n\ndata 80\n\nfunctionality 217, 218\n\ncleaning 80 normalizing 80 data components\n\ncyclical 84 irregular 84 seasonality 84 trend 84\n\ndataset shifts 113, 116\n\ndeployment strategies 204\n\nA/B testing deployment 204 Blue/Green deployment 204 Canary deployment 204 comparing 205 Ramped deployment 204 Recreate deployment 204 Shadow Deployment 204\n\nadversarial search 122, 123 causes 117 concept shifts 120 covariate shifts 117 density ratio estimation 123 feature dropping 121 identifying 117-121 prior probability shifts 118, 119 stabilizing 121\n\nDevOps 140\n\napplying, from maintenance 145 applying, from start to operations 145 CI/CD 146, 147 code quality assurance 147, 148 configuration management 145, 146 disaster management 149 in microservices architecture (MSA) 22-25 monitoring 148, 149 source code version control 145",
      "content_length": 1803,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "240\n\nDevOps processes\n\nin enterprise MSA system operations 141\n\nF\n\nDevOps team structure 140, 141 differencing 85 dimensionality reduction 95 Distributed Denial of Service (DDoS) 48 Docker\n\nfeature engineering 80 feature extraction 66 feature map 87 fit interface 105, 106 Flask 163\n\ninstalling 157\n\nDocker components 158-162 Docker Compose 170 Docker container 159-161 Docker Docs\n\nG\n\nGeneral Data Protection\n\nRegulation (GDPR) 144\n\nreference link 157, 158\n\nDocker Engine installing 157\n\nDocker Engine API documentation\n\nGitHub 156 greenfield deployment 206 greenfield risk (GR) 211 greenfield deployment, versus\n\nreference link 196\n\nbrownfield deployment 209\n\nDocker file 158 Docker Hub 156\n\nreference link 163 Docker image 159 Docker network 172 Docker volume 161, 162\n\nE\n\ncost 207 flexibility 206 integration 207 risks 208 scalability 206, 207 staff onboarding 208 technology stack 207 time-to-market 208 user adoption 209\n\neager learners 81 enterprise MSA system operations\n\nDevOps processes 141\n\nH\n\nEntity Relationship Diagram (ERD) 31 epics 140 event-driven architecture (EDA) 15, 16 eventual consistency synchronization 22 Exploratory Data Analysis (EDA) 80, 99\n\nHealer AI Service 181 Health Insurance Portability and\n\nAccountability Act (HIPAA) 144, 210\n\nhyperparameters 114, 115\n\nexamples 114 hypervisor 152",
      "content_length": 1317,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "241\n\nI\n\nInfrastructure-as-a-Code (IaaC) 146 Integrated (I) 86 integration testing 231\n\nK\n\nmachine learning (ML) 178\n\nadvantage 177, 178 forecasting 84-87 libraries, used in Python 67 need for 25 pattern analysis 84-87 versus artificial intelligence 63-67 versus deep learning 63-67 using, in CI/CD pipeline 25\n\nKeras 71, 72 kernel 87 K-Fold Cross-Validation 106 K-Nearest Neighbors (KNN) algorithm 81 Kubernetes 156\n\nmachine learning model hyperparameters 114 parameters 114\n\nmachine learning systems\n\nL\n\nlabel 65, 74 label shifts 118 Latent Dirichlet Allocation (LDA) 82 lazy learners 81 Limited Availability (LA) approach 215 linear microservices pipeline 4 local transaction 17 logistic function 89 Long Short-Term Memory (LSTM) model 93 loosely coupled monolithic system versus tightly coupled monolithic\n\ncomponents 97-100 interfaces 100 stages 97 Marathon 156 Matplotlib 69\n\nreference link 69\n\nMean Absolute Error (MAE) 108 Mean Squared Error (MSE) 108 Mean Time to Resolution (MTTR) 7 microservices\n\nadvantages 6-9 disadvantages 9, 10\n\nmicroservices aggregators 51-53 microservices architecture (MSA) 3, 125\n\nM\n\nsystem 12, 13\n\nadvantages 11 DevOps 22-25 isolating, with ACL 43-45 need for 4, 5\n\nmachine learning, in MSA enterprise system\n\nsystem decay prediction 125 system load prediction 125 system resource planning 126 system security 126 use cases 125-128\n\nmicroservices circuit breaker 56-58 microservices performance monitor 185, 186\n\nmodel embedding 108 models\n\nenhancing, with deep learning 87-95 issues 93",
      "content_length": 1522,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "242\n\nmodel serving 108 monolithic application\n\nP\n\nversus microservices application 14, 15\n\nPandas 70\n\nmonolithic architecture\n\nversus microservices system 13\n\nMoving Average (MA) 86 MSA hybrid model architecture 15, 16 MSA system\n\nreference link 70 parameterization 113 parameters\n\nexamples 114\n\npattern analysis machine learning\n\nanti-corruption layer 225-227 deploying 225 services, integrating 227-230 testing 231, 232 tuning 231, 232\n\nused, for enhancing system supportability 129, 130\n\nused, for enhancing time-to-\n\nresolution (TTR) 129, 130\n\nPayment Card Industry Data Security\n\nmulticlass classification\n\nStandard (PCI DSS) 144\n\nbuilding 80, 81\n\nmulticlass classifiers 81\n\nPayment Card Industry (PCI) 210 Performance Anomaly Detector (PAD) 178\n\noperations 197-200\n\nN\n\nPerformance Baseline Watchdog (PBW) 178\n\noperations 192-196\n\nNano-service anti-pattern 28 Natural Language Processing (NLP) 72, 82 negotiation strategies 214 Network Management System (NMS) 178 non-linear microservices pipeline 5 NumPy 68, 69\n\nreference link 68\n\nO\n\nobject-oriented programming (OOP) 8 Operation Support System (OSS) 178 orchestration 18, 19, 109-112 orchestrators 49-51\n\nbenefits 50 versus aggregators 54, 55 versus API gateway 54, 55\n\npoint anomaly 130 post-deployment assessment 218 post-deployment review 233\n\ncompliance 234 system defects, fixing 233 system defects, identifying 233 system maintenance and updates 234 system’s performance, checking 233 user satisfaction 235, 236 pre-production review 218 Principle Component Analysis (PCA) 95 prior probability shifts 118, 119 Python\n\ndeep learning, used 67 machine learning, used 67\n\nPyTorch 72\n\nreference link 72\n\norganizational structure alignment 138 overfitting 77",
      "content_length": 1716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "243\n\nQ\n\nSciPy 73\n\nreference link 73\n\nquantitative variable 74\n\nR\n\nR2 metrics 76 Ramped deployment 204 Recreate deployment 204 Recurrent Neural Networks (RNNs) 84, 87 regression models building 74-79 regularization 78 ReLU function 88 residual errors 76 response delay simulator 186 risk exposure 212 risk mitigation plan\n\nSelf-Healing Lock State 181 self-healing process 128 sentiment analysis 83 Service Catalog 49 service-driven architecture 15 service-oriented architecture (SOA) 15 serving interface 108, 109 Shadow Deployment 204 sprint 142 Sprint Planning Meetings 142 supervised anomaly detection 131 system dependencies\n\nABC-Intelligent-MSA deployment\n\nrisks, mitigating 223, 224\n\nABC-Monolith components,\n\nreusing 222, 223\n\nBR1, system capability limitations 216 BR2, high OPEX 216 GR1, high CAPEX risk 213-215 GR2, deployment time risks 215 GR3, system failure risks 216 GR4, user adoption risks 216\n\nrobust system\n\ncomponents 111\n\nABC-Monolith dependencies,\n\nreusing 222, 223\n\novercoming 222 system self-healing\n\nimplementing, with deep learning 130\n\nsystem’s microservices identifying 27, 29 system supportability\n\nRoot Mean Squared Error (RMSE) 76, 108 R-Squared (R2) 108\n\nenhancing with pattern analysis machine learning 129, 130\n\nS\n\nT\n\nSafeguard Computer Security Evaluation\n\nMatrix (SCSEM) 144\n\nsaga patterns 17-20, 36 scaling transformations\n\ntypes 103\n\nscikit-learn 73, 74 reference link 73\n\nTanh function 89, 90 technical debt 208 TensorFlow 71, 72 Tensors 71 text sentiment analysis 82-84 The ACL Adapter 45 The ACL Facade 45",
      "content_length": 1545,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "244\n\ntightly coupled monolithic system versus loosely coupled monolithic\n\nsystem 12, 13\n\ntime-to-market (TTM) 3 time-to-resolution (TTR)\n\nenhancing, with pattern analysis machine learning 129, 130\n\ntopic modeling 82-84 training data 64 training interface 106-108 transformation 86 transform interface 100-104\n\ntypes 101\n\ntrends\n\nnon-stationary 85 stationary 85\n\ntrickle migration 44 Tyk Docker container installation link 164\n\nU\n\nunderfitting 77 Unhealable Wait Period 181 unit testing 231 UNIX chroot 152 unsupervised anomaly detection 131 user interface/user experience (UI/UX) 216\n\nW\n\nWeb Server Gateway Interface (WSGI) 163 workflows 49",
      "content_length": 640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "Packtpub.com\n\nSubscribe to our online digital library for full access to over 7,000 books and videos, as well as industry leading tools to help you plan your personal development and advance your career. For more information, please visit our website.\n\nWhy subscribe?\n\nSpend less time learning and more time coding with practical eBooks and Videos from over 4,000 industry professionals\n\n\n\nImprove your learning with Skill Plans built especially for you\n\nGet a free eBook or video every month\n\nFully searchable for easy access to vital information\n\nCopy and paste, print, and bookmark content\n\nDid you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at packtpub.com and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at customercare@packtpub. com for more details.\n\nAt www.packtpub.com, you can also read a collection of free technical articles, sign up for a range of free newsletters, and receive exclusive discounts and offers on Packt books and eBooks.",
      "content_length": 1093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "Other Books You May Enjoy\n\nIf you enjoyed this book, you may be interested in these other books by Packt:\n\nApplying Math with Python - Second Edition\n\nSam Morley\n\nISBN: 9781804618370\n\nBecome familiar with basic Python packages, tools, and libraries for solving mathematical problems\n\nExplore real-world applications of mathematics to reduce a problem in optimization\n\nUnderstand the core concepts of applied mathematics and their application in computer science\n\nFind out how to choose the most suitable package, tool, or technique to solve a problem\n\n\n\nImplement basic mathematical plotting, change plot styles, and add labels to plots using Matplotlib\n\nGet to grips with probability theory with the Bayesian inference and Markov Chain Monte Carlo (MCMC) methods",
      "content_length": 763,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "247\n\nApplied Machine Learning Explainability Techniques\n\nAditya Bhattacharya\n\nISBN: 9781803246154\n\nExplore various explanation methods and their evaluation criteria\n\nLearn model explanation methods for structured and unstructured data\n\nApply data-centric XAI for practical problem-solving\n\nHands-on exposure to LIME, SHAP, TCAV, DALEX, ALIBI, DiCE, and others\n\nDiscover industrial best practices for explainable ML systems\n\nUse user-centric XAI to bring AI closer to non-technical end users\n\nAddress open challenges in XAI using the recommended guidelines",
      "content_length": 555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "248\n\nPackt is searching for authors like you\n\nIf you’re interested in becoming an author for Packt, please visit authors.packtpub.com and apply today. We have worked with thousands of developers and tech professionals, just like you, to help them share their insight with the global tech community. You can make a general application, apply for a specific hot topic that we are recruiting an author for, or submit your own idea.\n\nShare Your Thoughts\n\nNow you’ve finished Machine Learning in Microservices, we’d love to hear your thoughts! If you purchased the book from Amazon, please click here to go straight to the Amazon review page for this book and share your feedback or leave a review on the site that you purchased it from.\n\nYour review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.",
      "content_length": 858,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "Download a free PDF copy of this book\n\nThanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print books everywhere? Is your eBook purchase not compatible with the device of your choice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into your application.\n\nThe perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your inbox daily\n\nFollow these simple steps to get the benefits:\n\n1. Scan the QR code or visit the link below\n\nhttps://packt.link/free-ebook/9781804617748\n\n2. Submit your proof of purchase\n\n3. That’s it! We’ll send your free PDF and other benefits to your email directly\n\n249",
      "content_length": 846,
      "extraction_method": "Unstructured"
    }
  ]
}