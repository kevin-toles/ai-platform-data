{
  "metadata": {
    "title": "Data Engineering with AWS Cookbook - Tram Ngoc Pham",
    "author": "Trâm Ngọc Phạm | Gonzalo Herreros González | Viquar Khan | Huda Nofal",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 922,
    "conversion_date": "2025-12-19T17:25:58.417419",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Data Engineering with AWS Cookbook - Tram Ngoc Pham.pdf",
    "extraction_method": "Unstructured"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Segment 1 (pages 2-24)",
      "start_page": 2,
      "end_page": 24,
      "detection_method": "synthetic",
      "content": "dealers and distributors will be held liable for any damages caused\n\nor alleged to have been caused directly or indirectly by this book.\n\nPackt Publishing has endeavored to provide trademark information\n\nabout all of the companies and products mentioned in this book by\n\nthe appropriate use of capitals. However, Packt Publishing cannot\n\nguarantee the accuracy of this information.\n\nGroup Product Manager: Apeksha Shetty\n\nPublishing Product Manager: Nilesh Kowadkar\n\nBook Project Manager: Urvi Sharma\n\nSenior Editor: Rohit Singh\n\nTechnical Editor: Kavyashree K S\n\nCopy Editor: Sa\u0000s Editing\n\nProofreader: Rohit Singh\n\nIndexer: Manju Arasan\n\nProduction Designer: Shankar Kalbhor\n\nSenior DevRel Marketing Executive: Nivedita Singh\n\nFirst published: November 2024\n\nProduction reference: 1301024\n\nPublished by Packt Publishing Ltd.\n\nGrosvenor House\n\n11 St Paul’s Square\n\nBirmingham\n\nB3 1RB, UK.\n\nISBN 978-1-80512-728-4\n\nwww.packtpub.com\n\nOceanofPDF.com\n\nTo my mother, Ngoc Truong, for her love and sacri\u0000ces, and for\n\nexemplifying the power of determination. To my family members\n\nand friends, who always oﬀer support and kindness throughout\n\nmy life journey.\n\n– Trâm Ngọc Phạm\n\nOceanofPDF.com\n\nContributors\n\nAbout the authors\n\nTrâm Ngọc Phạm is a senior data architect with over a decade of\n\nhands-on experience working in the big data and AI \u0000eld, from\n\nplaying a lead role in tailoring cloud data platforms to BI and\n\nanalytics use cases for enterprises in Vietnam. While working as a\n\nSenior Data and Analytics consultant for the AWS Professional\n\nServices team, she specialized in guiding \u0000nance and telco\n\ncompanies across Southeast Asian countries to build enterprise-\n\nscale data platforms and drive analytics use cases that utilized AWS\n\nservices and big data tools.\n\nGonzalo Herreros González is a principal data architect. He holds a\n\nbachelor’s degree in computer science and a master’s degree in data\n\nanalytics. He has experience of over a decade in big data and two\n\ndecades of soware development, both in AWS and on-premises.\n\nPreviously, he worked at MasterCard where he achieved the \u0000rst\n\nPCI-DSS Hadoop cluster in the world. More recently, he worked at\n\nAWS for over 6 years, building data pipelines for the internal\n\nnetwork data, and later, as an architect in the AWS Glue service\n\nteam, building transforms for AWS Glue Studio and helping large\n\ncustomers succeed with AWS data services.\n\nOceanofPDF.com\n\nContributors\n\nAbout the authors\n\nTrâm Ngọc Phạm is a senior data architect with over a decade of\n\nhands-on experience working in the big data and AI \u0000eld, from\n\nplaying a lead role in tailoring cloud data platforms to BI and\n\nanalytics use cases for enterprises in Vietnam. While working as a\n\nSenior Data and Analytics consultant for the AWS Professional\n\nServices team, she specialized in guiding \u0000nance and telco\n\ncompanies across Southeast Asian countries to build enterprise-\n\nscale data platforms and drive analytics use cases that utilized AWS\n\nservices and big data tools.\n\nGonzalo Herreros González is a principal data architect. He holds a\n\nbachelor’s degree in computer science and a master’s degree in data\n\nanalytics. He has experience of over a decade in big data and two\n\ndecades of soware development, both in AWS and on-premises.\n\nPreviously, he worked at MasterCard where he achieved the \u0000rst\n\nPCI-DSS Hadoop cluster in the world. More recently, he worked at\n\nAWS for over 6 years, building data pipelines for the internal\n\nnetwork data, and later, as an architect in the AWS Glue service\n\nteam, building transforms for AWS Glue Studio and helping large\n\ncustomers succeed with AWS data services.\n\nViquar Khan is a senior data architect at AWS Professional Services\n\nand brings over 20 years of expertise in \u0000nance and data analytics,\n\nempowering global \u0000nancial institutions to harness the full\n\npotential of AWS technologies. He designs cutting-edge,\n\ncustomized data solutions tailored to complex industry needs. A\n\npolyglot developer skilled in Java, Scala, Python, and other\n\nlanguages, Viquar has excelled in various technical roles. As an\n\nexpert group member of JSR368 (JavaTM Message Service 2.1), he\n\nhas shaped industry standards and actively contributes to open\n\nsource projects such as Apache Spark and Terraform. His technical\n\ninsights have reached and bene\u0000ted over 6.7 million users on Stack\n\nOver\u0000ow.\n\nHuda Nofal is a seasoned data engineer with over 7 years of\n\nexperience at Amazon, where she has played a key role in helping\n\ninternal business teams achieve their data goals. With deep\n\nexpertise in AWS services, she has successfully designed and\n\nimplemented data pipelines that power critical decision-making\n\nprocesses across various organizations. Huda’s work primarily\n\nfocuses on leveraging Redshi, Glue, data lakes, and Lambda to\n\ncreate scalable, eﬃcient data solutions.\n\nOceanofPDF.com\n\nAbout the reviewers\n\nSaransh Arora is a seasoned data engineer with more than 6 years of\n\nexperience in the \u0000eld. He has developed pro\u0000ciency in Python,\n\nJava, Spark, SQL, and various data engineering tools, enabling him\n\nto address a wide range of data challenges. He has expertise in data\n\norchestration, management, and analysis, with a strong emphasis on\n\nleveraging big data technologies to generate actionable insights.\n\nSaransh also possesses signi\u0000cant experience in machine learning\n\nand predictive analytics. Currently serving as a data engineer at\n\nAWS, he is dedicated to driving innovation and delivering business\n\nvalue. As an expert in data engineering, Saransh has also been\n\nworking on the integration of generative AI into data engineering\n\npractices.\n\nHaymang Ahuja specializes in ETL development, cloud computing,\n\nbig data technologies, and cutting-edge AI. He is adept at creating\n\nrobust data pipelines and delivering high-performance data\n\nsolutions, backed by strong soware development skills and\n\npro\u0000ciency in programming languages such as Python and SQL.\n\nHis expertise includes big data technologies such as Spark, Apache\n\nHudi, Air\u0000ow, Kylin, HDFS, and HBase. With a combination of\n\ntechnical knowledge, problem-solving skills, and a commitment to\n\nleveraging emerging technologies, he helps organizations achieve\n\ntheir strategic objectives and stay competitive in the dynamic digital\n\nlandscape.\n\nOceanofPDF.com\n\nTable of Contents\n\nPreface\n\n1\n\nManaging Data Lake Storage\n\nTechnical requirements\n\nControlling access to S3 buckets\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nStorage types in S3 for optimized storage costs\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nEnforcing encryption on S3 buckets\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nSetting up retention policies for your objects\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nVersioning your data\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nReplicating your data\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nMonitoring your S3 bucket\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n2\n\nSharing Your Data Across Environments and Accounts\n\nTechnical requirements\n\nCreating read-only replicas for RDS\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nRedshift live data sharing among your clusters\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSynchronizing Glue Data Catalog to a different account\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nEnforcing fine-grained permissions on S3 data sharing using Lake Formation\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nSharing your S3 data temporarily using a presigned URL\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nReal-time sharing of S3 data\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nSharing read-only access to your CloudWatch data with another AWS account\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n3\n\nIngesting and Transforming Your Data with AWS Glue\n\nTechnical requirements\n\nCreating ETL jobs visually using AWS Glue Studio\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nParameterizing jobs to make them more flexible and reusable\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nHandling job failures and reruns for partial results\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nProcessing data incrementally using bookmarks and bounded execution\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nHandling a high quantity of small files in your job\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nReusing libraries in your Glue job\n\nGetting ready\n\nHow to do it...\n\nHow it works…\n\nThere’s more...\n\nSee also\n\nUsing data lake formats to store your data\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nOptimizing your catalog data retrieval using pushdown filters and indexes\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nRunning pandas code using AWS Glue for Ray\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n4\n\nA Deep Dive into AWS Orchestration Frameworks\n\nTechnical requirements\n\nDefining a simple workflow using AWS Glue workflows\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nSetting up event-driven orchestration with Amazon EventBridge\n\nGetting ready\n\nHow to do it…\n\nCreating a data workflow using AWS Step Functions\n\nHow to do it…\n\nSee also\n\nManaging data pipelines with MWAA\n\nHow to do it…\n\nSee also\n\nMonitoring your pipeline’s health\n\nHow to do it…\n\nSetting up a pipeline using AWS Glue to ingest data from a JDBC database into a catalog table\n\nHow to do it…\n\n5\n\nRunning Big Data Workloads with Amazon EMR\n\nTechnical requirements\n\nRunning jobs using AWS EMR serverless\n\nGetting ready\n\nHow to do it…",
      "page_number": 2
    },
    {
      "number": 2,
      "title": "Segment 2 (pages 25-47)",
      "start_page": 25,
      "end_page": 47,
      "detection_method": "synthetic",
      "content": "How it works…\n\nThere’s more…\n\nSee also\n\nRunning your AWS EMR cluster on EKS\n\nGetting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...\n\nSee also\n\nUsing the AWS Glue catalog from another account\n\nGetting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...\n\nSee also\n\nMaking your cluster highly available\n\nGetting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...\n\nSee also\n\nScaling your cluster based on workload\n\nGetting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...\n\nSee also\n\nCustomizing the cluster nodes easily using bootstrap actions\n\nGetting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...\n\nSee also\n\nTuning Apache Spark resource usage\n\nGetting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...\n\nSee also\n\nCode development on EMR using Workspaces\n\nHow to do it...\n\nThere’s more...\n\nSee also\n\nMonitoring your cluster\n\nGetting ready\n\nHow to do it...\n\nThere’s more\n\nSee also\n\nProtecting your cluster from security vulnerabilities\n\nGetting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...\n\nSee also\n\n6\n\nGoverning Your Platform\n\nTechnical requirements\n\nApplying a data quality check on Glue tables\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nAutomating the discovery and reporting of sensitive data on your S3 buckets\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nEstablishing a tagging strategy for AWS resources\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nBuilding your distributed data community with Amazon DataZone following data mesh principles\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nHandling security-sensitive data (PII and PHI)\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nEnsuring S3 compliance with AWS Config\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n7\n\nData Quality Management\n\nTechnical requirements\n\nCreating data quality for ETL jobs in AWS Glue Studio notebooks\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nUnit testing your data quality using Deequ\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nSchema management for ETL pipelines\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nBuilding unit test functions for ETL pipelines\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nBuilding data cleaning and profiling jobs with DataBrew\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n8\n\nDevOps – Defining IaC and Building CI/CD Pipelines\n\nTechnical requirements\n\nSetting up a code deployment pipeline using CDK and AWS CodePipeline\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nSetting up a CDK pipeline to deploy on multiple accounts and regions\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nRunning code in a CloudFormation deployment\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nProtecting resources from accidental deletion\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nDeploying a data pipeline using Terraform\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nReverse-engineering IaC\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nIntegrating AWS Glue and Git version control\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n9\n\nMonitoring Data Lake Cloud Infrastructure\n\nTechnical requirements\n\nAdditional information\n\nAutomatically setting CloudWatch log group retention to reduce cost\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nCreating custom dashboards to monitor Data Lake services\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nSetting up System Manager to remediate non- compliance with AWS Config rules\n\nGetting ready\n\nHow to do it…\n\nThere’s more…\n\nSee also\n\nUsing AWS config to automate non- compliance S3 server access logging policy\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nTracking AWS Data Lake cost per analytics workload\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n10\n\nBuilding a Serving Layer with AWS Analytics Services\n\nTechnical requirements\n\nUsing Redshift workload management (WLM) to manage workload priority\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nQuerying large historical data with Redshift Spectrum\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nCreating a VPC endpoint to a Redshift cluster\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nAccessing a Redshift cluster using JDBC to query data\n\nGetting ready\n\nHow to do it…\n\nThere’s more…\n\nSee also\n\nUsing AWS SDK for pandas, the Redshift Data API, and Lambda to execute SQL statements\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nUsing the AWS SDK for Python to manage Amazon QuickSight\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n11\n\nMigrating to AWS – Steps, Strategies, and Best Practices for Modernizing Your Analytics and Big Data Workloads\n\nTechnical requirements\n\nReviewing the steps and processes for migrating an on-premises platform to AWS\n\nGetting ready\n\nHow to do it…\n\nChoosing your AWS analytics stack – the re- platforming approach\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nPicking the correct migration approach for your workload\n\nGetting ready\n\nHow to do it…\n\nPlanning for prototyping and testing\n\nGetting ready\n\nHow to do it…\n\nConverting ETL processes with big data frameworks\n\nGetting ready\n\nHow to do it…\n\nDefining and executing your migration process with Hadoop\n\nGetting ready\n\nHow to do it…\n\nMigrating the existing Hadoop security authentication and authorization processes\n\nGetting ready\n\nHow to do it…\n\n12\n\nHarnessing the Power of AWS for Seamless Data Warehouse Migration\n\nTechnical requirements\n\nCreating SCT migration assessment report with AWS SCT\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nExtracting data with AWS DMS\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nLive example – migrating an Oracle database from a local laptop to AWS RDS using AWS SCT\n\nGetting ready\n\nHow to do it…\n\nLeveraging AWS Snow Family for large-scale data migration\n\nThe Snow Family Large Data Migration Manager as a service\n\nGetting ready\n\nHow to do it…\n\nSee also\n\n13\n\nStrategizing Hadoop Migrations – Cost, Data, and Workflow Modernization with AWS\n\nTechnical requirements\n\nCalculating total cost of ownership (TCO) using AWS TCO calculators\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nConducting a Hadoop migration assessment using the TCO simulator\n\nHadoop to Amazon EMR TCO simulator\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nSelecting how to store your data\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nMigrating on-premises HDFS data using AWS DataSync\n\nGetting ready\n\nHow to do it...",
      "page_number": 25
    },
    {
      "number": 3,
      "title": "Segment 3 (pages 48-65)",
      "start_page": 48,
      "end_page": 65,
      "detection_method": "synthetic",
      "content": "See also\n\nMigrating the Hive Metastore to AWS\n\nGetting ready\n\nHow to do it…\n\nMigrating and running Apache Oozie workflows on Amazon EMR\n\nGetting ready\n\nHow to do it…\n\nMigrating an Oozie database to the Amazon RDS MySQL\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nSetting up networking – establishing a secure connection to your EMR cluster\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nPerforming a seamless HBase migration to AWS\n\nGetting ready\n\nHow to do it…\n\nMigrating HBase to DynamoDB on AWS\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nIndex\n\nOther Books You May Enjoy\n\nOceanofPDF.com\n\nPreface\n\nHello and welcome! In today’s rapidly evolving data landscape,\n\nmanaging, migrating, and governing large-scale data systems are\n\namong the top priorities for data engineers. is book serves as a\n\ncomprehensive guide to help you navigate these essential tasks, with\n\na focus on three key pillars of modern data engineering:\n\nHadoop and data warehouse migration: Organizations are increasingly\n\nmoving from traditional Hadoop clusters and on-premises data\n\nwarehouses to more scalable, cloud-based data platforms. is book\n\nwalks you through the best practices, methodologies, and how to use the\n\ntools for migrating large-scale data systems, ensuring data consistency,\n\nminimal downtime, and scalable performance.\n\nData lake operations: Building and maintaining a data lake in today’s\n\nmulti-cloud, big data environment is complex and demands a strong\n\noperational strategy. is book covers how to ingest, transform, and\n\nmanage data at scale using AWS services such as S3, Glue, and Athena.\n\nYou will learn how to structure and maintain a robust data lake\n\narchitecture that supports the varied needs of data analysts, data\n\nscientists, and business users alike.\n\nData lake governance: Managing and governing your data lake involves\n\nmore than just operational eﬃciency; it requires stringent security\n\nprotocols, data quality controls, and compliance measures. With the\n\nexplosion of data, it’s more important than ever to have clear governance\n\nframeworks in place. is book delves into the best practices for\n\nimplementing governance strategies using services such as AWS Lake\n\nFormation, Glue, and other AWS security frameworks. You’ll also learn\n\nabout setting up policies that ensure your data lake is compliant with\n\nindustry regulations while maintaining scalability and \u0000exibility.\n\nis cookbook is tailored to data engineers who are looking to\n\nimplement best practices and take their cloud data platforms to the\n\nnext level. roughout this book, you’ll \u0000nd practical examples,\n\ndetailed recipes, and real-world scenarios from the authors’\n\nexperience of working with complex data environments across\n\ndiﬀerent industries.\n\nBy the end of this journey, you will have a thorough understanding\n\nof how to migrate, operate, and govern your data platforms at scale,\n\nall while aligning with industry best practices and modern\n\ntechnological advancements.\n\nSo, let’s dive in and build the future of data engineering together!\n\nWho this book is for\n\nis book is designed for data engineers, data platform engineers,\n\nand cloud practitioners who are actively involved in building and\n\nmanaging data infrastructure in the cloud. If you’re involved in\n\ndesigning, building, or overseeing data solutions on AWS, this book\n\nwill be ideal as it provides proven strategies for addressing\n\nchallenges in large-scale data environments. Data engineers and big\n\ndata professionals aiming to enhance their understanding of AWS\n\nfeatures for optimizing their work\u0000ow, even if they’re new to the\n\nplatform, will \u0000nd value. Basic familiarity with AWS security (users\n\nand roles) and command shell is recommended. is book will\n\nprovide you with practical guidance, hands-on recipes, and\n\nadvanced techniques for tackling real-world challenges.\n\nWhat this book covers\n\nChapter 1, Managing Data Lake Storage, covers the fundamentals of\n\nmanaging S3 buckets. We’ll focus on implementing robust security\n\nmeasures through data encryption and access control, managing\n\ncosts by optimizing storage tiers and applying retention policies,\n\nand utilizing monitoring techniques to ensure timely issue\n\nresolution. Additionally, we’ll cover other essential aspects of S3\n\nbucket management.\n\nChapter 2, Sharing Your Data Across Environments and Accounts,\n\npresents methods for securely and eﬃciently sharing data across\n\ndiﬀerent environments and accounts. We will explore strategies for\n\nload distribution and collaborative analysis using Redshi data\n\nsharing and RDS replicas. We will implement \u0000ne-grained access\n\ncontrol with Lake Formation and manage Glue data sharing\n\nthrough both Lake Formation and Resource Access Manager\n\n(RAM). Additionally, we will discuss real-time sharing via event-\n\ndriven services, temporary data sharing with S3, and sharing\n\noperational data from CloudWatch.\n\nChapter 3, Ingesting and Transforming Your Data with AWS Glue,\n\nexplores diﬀerent features of AWS Glue when building data\n\npipelines and data lakes. It covers the multiple tools and engines\n\nprovided for the diﬀerent kinds of users, from visual jobs with little\n\nor no code to managed notebooks and jobs using the diﬀerent data\n\nhandling APIs provided.\n\nChapter 4, A Deep Dive into AWS Orchestration Frameworks,\n\nexplores the essential services and techniques for managing data\n\nwork\u0000ows and pipelines on AWS. You’ll learn how to de\u0000ne a\n\nsimple work\u0000ow using AWS Glue Work\u0000ows, set up event-driven\n\norchestration with Amazon EventBridge, and create data work\u0000ows\n\nwith AWS Step Functions. We also cover managing data pipelines\n\nusing Amazon MWAA, monitoring their health, and setting up a\n\ndata ingestion pipeline with AWS Glue to bring data from a JDBC\n\ndatabase into a catalog table.\n\nChapter 5, Running Big Data Workloads with Amazon EMR, teaches\n\nhow to make the most of your AWS EMR clusters and explore the\n\nservice features that enable them to be customizable, eﬃcient,\n\nscalable, and robust.\n\nChapter 6, Governing Your Platform, presents the key aspects of data\n\ngovernance within AWS. is includes data protection techniques\n\nsuch as data masking in Redshi and classifying sensitive\n\ninformation using Maice. We will also cover ensuring data quality\n\nwith Glue quality checks. Additionally, we will discuss resource\n\ngovernance to enforce best practices and maintain a secure,\n\ncompliant infrastructure using AWS Con\u0000g and resource tagging.\n\nChapter 7, Data Quality Management, covers how to use AWS Glue\n\nDeequ and AWS DataBrew to automate data quality checks and\n\nmaintain high standards across your datasets. You will learn how to\n\nde\u0000ne and enforce data quality rules and monitor data quality\n\nmetrics. is chapter also provides practical examples and recipes\n\nfor integrating these tools into your data work\u0000ows, ensuring that\n\nyour data is accurate, complete, and reliable for analysis.\n\nChapter 8, DevOps – De\u0000ning IaC and Building CI/CD Pipelines,\n\nexplores multiple ways to automate AWS services and CI/CD\n\ndeployment pipelines, the pros and cons of each tool, and examples\n\nof common data product deployments to illustrate DevOps best\n\npractices.\n\nChapter 9, Monitoring Data Lake Cloud Infrastructure, provides a\n\ncomprehensive guide to the day-to-day operations of a cloud-based\n\ndata platform. It covers key topics such as monitoring, logging, and\n\nalerting using AWS services such as CloudWatch, CloudTrail, and\n\nX-Ray. You will learn how to set up dashboards to monitor the\n\nhealth and performance of your data platform, troubleshoot issues,\n\nand ensure high availability and reliability. is chapter also\n\ndiscusses best practices for cost management and scaling operations\n\nto meet changing demands, making it an essential resource for\n\nanyone responsible for the ongoing maintenance and optimization\n\nof a data platform.\n\nChapter 10, Building a Serving Layer with AWS Analytics Services,\n\nguides you through the process of building an eﬃcient serving layer\n\nusing AWS Redshi, Athena, and QuickSight. e serving layer is\n\nwhere your data becomes accessible to end-users for analysis and\n\nreporting. In this chapter, you will learn how to load data from your\n\ndata lake into Redshi, query it using Redshi Spectrum and\n\nAthena, and visualize it using QuickSight. is chapter also covers\n\nbest practices for managing diﬀerent QuickSight environments and\n\nmigrating assets between them. By the end of this chapter, you will\n\nhave the knowledge to create a powerful and user-friendly analytics\n\nlayer that meets the needs of your organization.\n\nChapter 11, Migrating to AWS – Steps, Strategies, and Best Practices\n\nfor Modernizing Your Analytics and Big Data Workloads, presents a\n\ntheoretical framework for migrating data and workloads to AWS. It\n\nexplores key concepts, strategies, and best practices for planning\n\nand executing a successful migration. You’ll learn about various\n\nmigration approaches—rehosting, replatforming, and refactoring—\n\nand how to choose the best option for your organization’s needs.\n\ne chapter also addresses critical challenges and considerations,\n\nsuch as data security, compliance, and minimizing downtime,\n\npreparing you to navigate the complexities of cloud migration with\n\ncon\u0000dence.\n\nChapter 12, Harnessing the Power of AWS for Seamless Data\n\nWarehouse Migration, explores the key strategies for eﬃciently\n\nmigrating data warehouses to AWS. You’ll learn how to generate a\n\nmigration assessment report using the AWS Schema Conversion\n\nTool (SCT), extract and transfer data with AWS Database Migration\n\nService (DMS), and handle large-scale migrations with the AWS\n\nSnow Family. You’ll also learn how to streamline your data\n\nmigration, ensuring minimal disruption and maximum eﬃciency\n\nwhile transitioning to the cloud.\n\nChapter 13, Strategizing Hadoop Migrations – Cost, Data, and\n\nWork\u0000ow Modernization with AWS, guides you through essential\n\nrecipes for migrating your on-premises Hadoop ecosystem to AWS,\n\ncovering a range of critical tasks. You’ll learn about cost analysis\n\nusing the AWS Total Cost of Ownership (TCO) calculators and the\n\nHadoop Migration Assessment tool. You’ll also learn how to choose\n\nthe right storage solution, migrate HDFS data using AWS DataSync,\n\nand transition key components such as the Hive Metastore and\n\nApache Oozie work\u0000ows to AWS EMR. We also cover setting up a\n\nsecure network connection to your EMR cluster, seamless HBase\n\nmigration to AWS, and transitioning HBase to DynamoDB.\n\nTo get the most out of this book\n\nTo follow the recipes in this book, you will need the following:\n\nSoware/hardware covered in the book\n\nOS requirements\n\nAWS CLI\n\nWindows, macOS X,\n\nand Linux (any)\n\nAccess to AWS services such as EMR,\n\nGlue, Redshi, QuickSight, and Lambda\n\nPython (for scripting and SDK usage)\n\nIn addition to these requirements, you will also need a basic\n\nknowledge of data engineer terminology.\n\nIf you are using the digital version of this book, we advise you to\n\ntype the code yourself or access the code via the GitHub repository\n\n(link available in the next section). Doing so will help you avoid any\n\npotential errors related to the copying and pasting of code.\n\nDownload the example code files\n\nYou can download the example code \u0000les for this book from\n\nGitHub at https://github.com/PacktPublishing/Data-Engineering-\n\nwith-AWS-Cookbook. In case there’s an update to the code, it will\n\nbe updated on the existing GitHub repository.\n\nWe also have other code bundles from our rich catalog of books and\n\nvideos available at https://github.com/PacktPublishing/. Check\n\nthem out!\n\nConventions used\n\nere are a number of text conventions used throughout this book.\n\nCode in text: Indicates code words in text, database table names,\n\nfolder names, \u0000lenames, \u0000le extensions, pathnames, dummy URLs,\n\nuser input, and Twitter handles. Here is an example: “Make sure you\n\nreplace <your_bucket_name> with the actual name of your S3\n\nbucket.”\n\nA block of code is set as follows:\n\n{ Sid: DenyListBucketFolder, Action: [s3:*], Effect: Deny, Resource: [arn:aws:s3:::<bucket-name>/<folder- name>/*] }\n\nAny command-line input or output is written as follows:\n\nCREATE DATASHARE datashare_name;\n\nBold: Indicates a new term, an important word, or words that you\n\nsee onscreen. For example, words in menus or dialog boxes appear\n\nin the text like this. Here is an example: “Choose Policies from the\n\nnavigation pane on the le and choose Create policy.”\n\nTIPS OR IMPORTANT NOTES\n\nAppear like this.\n\nSections\n\nIn this book, you will \u0000nd several headings that appear frequently\n\n(Getting ready, How to do it..., How it works..., ere’s more..., and\n\nSee also).\n\nTo give clear instructions on how to complete a recipe, use these\n\nsections as follows:\n\nGetting ready\n\nis section tells you what to expect in the recipe and describes how\n\nto set up any soware or any preliminary settings required for the\n\nrecipe.\n\nHow to do it…\n\nis section contains the steps required to follow the recipe.\n\nHow it works…\n\nis section usually consists of a detailed explanation of what\n\nhappened in the previous section.\n\nThere’s more…\n\nis section consists of additional information about the recipe in\n\norder to make you more knowledgeable about the recipe.\n\nSee also\n\nis section provides helpful links to other useful information for\n\nthe recipe.\n\nGet in touch\n\nFeedback from our readers is always welcome.\n\nGeneral feedback: If you have questions about any aspect of this\n\nbook, mention the book title in the subject of your message and\n\nemail us at customercare@packtpub.com.\n\nErrata: Although we have taken every care to ensure the accuracy of\n\nour content, mistakes do happen. If you have found a mistake in\n\nthis book, we would be grateful if you would report this to us.\n\nPlease visit www.packtpub.com/support/errata, select your book,\n\nclick on the Errata Submission Form link, and enter the details.\n\nPiracy: If you come across any illegal copies of our works in any\n\nform on the Internet, we would be grateful if you would provide us\n\nwith the location address or website name. Please contact us at\n\ncopyright@packt.com with a link to the material.\n\nIf you are interested in becoming an author: If there is a topic that\n\nyou have expertise in and you are interested in either writing or\n\ncontributing to a book, please visit authors.packtpub.com.\n\nShare Your Thoughts\n\nOnce you’ve read Data Engineering with AWS Cookbook, we’d love\n\nto hear your thoughts! Please click here to go straight to the\n\nAmazon review page for this book and share your feedback.\n\nYour review is important to us and the tech community and will\n\nhelp us make sure we’re delivering excellent quality content.\n\nDownload a free PDF copy of this book\n\nanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print\n\nbooks everywhere?\n\nIs your eBook purchase not compatible with the device of your\n\nchoice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF\n\nversion of that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste\n\ncode from your favorite technical books directly into your\n\napplication.\n\ne perks don’t stop there, you can get exclusive access to discounts,\n\nnewsletters, and great free content in your inbox daily\n\nFollow these simple steps to get the bene\u0000ts:\n\n1. Scan the QR code or visit the link below\n\nhttps://packt.link/free-ebook/9781805127284\n\n2. Submit your proof of purchase\n\n3. at’s it! We’ll send your free PDF and other bene\u0000ts to your email\n\ndirectly\n\nOceanofPDF.com\n\n1 Managing Data Lake Storage\n\nAmazon Simple Storage Service (Amazon S3) is a highly scalable\n\nand secure cloud storage service. It allows you to store and retrieve\n\nany amount of data at any time from anywhere in the world. S3\n\nbuckets aim to help enterprises and individuals achieve their data\n\nbackup and delivery needs and serve a variety of use cases,\n\nincluding but not limited to web and mobile applications, big data\n\nanalytics, data lakes, and data backup and archiving.\n\nIn this chapter, we will learn how to keep data secure in S3 buckets\n\nand con\u0000gure your buckets in a way that best serves your use case\n\nfrom performance and cost perspectives.\n\ne following recipes will be covered in this chapter:\n\nControlling access to S3 buckets\n\nStorage types in S3 for optimized storage costs\n\nEnforcing encryption of S3 buckets\n\nSetting up retention policies for your objects\n\nVersioning your data\n\nReplicating your data\n\nMonitoring your S3 buckets",
      "page_number": 48
    },
    {
      "number": 4,
      "title": "Segment 4 (pages 66-84)",
      "start_page": 66,
      "end_page": 84,
      "detection_method": "synthetic",
      "content": "Technical requirements\n\ne recipes in this chapter assume you have an S3 bucket with\n\nadmin permission. If you don’t have admin permission to the\n\nbucket, you will need to con\u0000gure the permission for each recipe as\n\nneeded.\n\nYou can \u0000nd the code \u0000les for this chapter in this book’s GitHub\n\nrepository: https://github.com/PacktPublishing/Data-Engineering-\n\nwith-AWS-Cookbook/tree/main/Chapter01.\n\nControlling access to S3 buckets\n\nControlling access to S3 buckets through policies and IAM roles is\n\ncrucial for maintaining the security and integrity of your objects\n\nand data stored in Amazon S3. By de\u0000ning granular permissions\n\nand access controls, you can ensure that only authorized users or\n\nservices have the necessary privileges to interact with your S3\n\nresources. You can restrict permissions according to your\n\nrequirements by precisely de\u0000ning who can access your data, what\n\nactions they can take, and under what conditions. is \u0000ne-grained\n\naccess control helps protect sensitive data, prevent unauthorized\n\nmodi\u0000cations, and mitigate the risk of accidental or malicious\n\nactions.\n\nAWS Identity and Access Management (IAM) allows you to create\n\nan entity referred to as an IAM identity, which is granted speci\u0000c\n\nactions on your AWS account. is entity can be a person or an\n\napplication. You can create this identity as an IAM role, which is\n\ndesigned to be attached to any entity that needs it. Alternatively, you\n\ncan create IAM users, which represent individual people and are\n\nusually used for granting long-term access to speci\u0000c users. IAM\n\nusers can be grouped into an IAM group, allowing permissions to\n\nbe assigned at the group level and inherited by all member users.\n\nIAM policies are sets of permissions that can be attached to the\n\nIAM identity to grant speci\u0000c access rights.\n\nIn this recipe, we will learn how to create a policy so that we can\n\nview all the buckets in the account, give read access to one speci\u0000c\n\nbucket content, and then give write access to one of its folders.\n\nGetting ready\n\nFor this recipe, you need to have an IAM user, role, or group to\n\nwhich you want to grant access. You also need to have an S3 bucket\n\nwith a folder to grant access to.\n\nTo learn how to create IAM identities, go to\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id.html.\n\nHow to do it…\n\n1. Sign in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the IAM console.\n\n2. Choose Policies from the navigation pane on the le and choose Create\n\npolicy.\n\n3. Choose the JSON tab to provide the policy in JSON format and replace\n\nthe existing JSON with this policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowListBuckets\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListAllMyBuckets\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowBucketListing\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetBucketLocation\" ], \"Resource\": [ \"arn:aws:s3:::<bucket-name>\" ] }, { \"Sid\": \"AllowFolderAccess\",\n\n\"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::<bucket- name>/<folder-name>/*\" ] } ] }\n\n4. Provide a policy name and, optionally, a description of the policy in the\n\nrespective \u0000elds.\n\n5. Click on Create Policy.\n\nNow, you can attach this policy to an IAM role, user, or group.\n\nHowever, exercise caution and ensure access is granted only as\n\nnecessary; avoid providing admin access policies to regular users.\n\nHow it works…\n\nAn IAM policy comprises three key elements:\n\nEffect: is speci\u0000es whether the policy allows or denies access\n\nAction: is details the speci\u0000c actions being allowed or denied\n\nResource: is identi\u0000es the resources to which the actions apply\n\nA single statement can apply multiple actions to multiple resources.\n\nIn this recipe, we’ve de\u0000ned three statements:\n\ne AllowListBuckets statement gives access to list all buckets in\n\nthe AWS account\n\ne AllowBucketListing statement gives access to list the content\n\nof a speci\u0000c S3 bucket\n\ne AllowFolderAccess gives access to upload, download, and\n\ndelete objects from a speci\u0000c folder\n\nThere’s more…\n\nIf you want to make sure that no access is given to a speci\u0000c bucket\n\nor object in your bucket, you can use a deny statement, as shown\n\nhere:\n\n{ \"Sid\":\"DenyListBucketFolder\", \"Action\":[ \"s3:*\" ], \"Effect\":\"Deny\", \"Resource\":[ \"arn:aws:s3:::<bucket-name>/<folder- name>/*\" }\n\nInstead of using an IAM policy to set up permissions to your\n\nbucket, you can use S3 bucket policies. ese can be located in the\n\nPermission tab of the bucket. Bucket policies can be used when\n\nyou’re trying to set up access at the bucket level, regardless of the\n\nIAM role or user.\n\nSee also\n\nAWS provides a set of policies that are managed and administered by\n\nAWS, all of which can be used for many common use cases. You can\n\nlearn more about these policies at\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam-\n\nawsmanpol.html.\n\nTo learn how to set up cross-account access to S3 buckets, go to\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/example-\n\nwalkthroughs-managing-access-example2.html.\n\nStorage types in S3 for optimized storage costs\n\nAmazon S3 oﬀers diﬀerent tiers or classes of storage that allow you\n\nto optimize for cost and performance based on your access pattern\n\nand data requirements. e default storage class for S3 buckets is S3\n\nStandard, which oﬀers high availability and low latency. For less\n\nfrequently accessed data, S3 Standard-IA and S3 One Zone-IA can\n\nbe used. For rare access, Amazon S3 oﬀers archiving classes called\n\nGlacier, which are the lowest-cost classes. If you’re not sure how\n\nfrequently your data will be accessed, S3 Intelligent-Tiering would\n\nbe optimal for you as it will automatically move objects between the\n\nclasses based on the access patterns. However, be aware that\n\nadditional costs may be incurred when you’re moving objects to a\n\nhigher-cost storage class.\n\nese storage classes provide users with the \u0000exibility to choose the\n\nright trade-oﬀ between storage costs and access performance based\n\non their speci\u0000c data storage and retrieval requirements. You can\n\nchoose the storage class based on your access patterns, durability\n\nrequirements, and budget considerations. Con\u0000guring storage\n\nclasses at the object level allows for a mix of storage classes within\n\nthe same bucket. Objects from diverse storage classes, including S3\n\nStandard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-\n\nIA, can coexist in a single bucket.\n\nIn this recipe, we will learn how to enforce the S3 Intelligent-\n\nTiering storage class for an S3 bucket through a bucket policy.\n\nGetting ready\n\nFor this recipe, you only need to have an S3 bucket for which you\n\nwill enforce the storage class.\n\nHow to do it…\n\n1. Open the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the S3 service.\n\n2. Locate and select the S3 bucket on which you want to enable S3\n\nIntelligent-Tiering and navigate to the Permissions tab.\n\n3. Under the Bucket Policy section, click on Edit.\n\n4. In the bucket policy editor, add the following statement. Make sure you\n\nreplace <your_bucket_name> with the actual name of your S3\n\nbucket:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"EnableIntelligentTiering\", \"Effect\": \"Deny\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::<your-bucket- name>/*\", \"Condition\": { \"StringNotEquals\": { \"s3:x-amz-storage-class\": \"INTELLIGENT_TIERING\" } } } ] }\n\n5. Save the bucket policy by clicking on Save changes.\n\nHow it works…\n\ne policy will ensure that objects are stored via the Intelligent-\n\nTiering class by allowing the PUT operation to be used on the\n\nbucket for all users (Principal: *), but only if the storage class is\n\nset to INTELLIGENT_TIERING. You can do this by choosing it from\n\nthe storage class list in the Object properties section. If you’re using\n\nthe console or the S3 API, add the x-amz-storage-class:\n\nINTELLIGENT_TIERING header. Use the -storage-class\n\nINTELLIGENT_TIERING parameter when using the AWS CLI.\n\nThere’s more…\n\nIntelligent-Tiering will place newly uploaded objects in the S3\n\nStandard class (Frequent Access class). If the object hasn’t been\n\naccessed in 30 consecutive days, it will be moved to the Infrequent\n\nAccess tier; if it hasn’t been accessed in 90 consecutive days, it will\n\nbe moved to the Archive Instant Access tier. For further cost\n\nsavings, you can enable INTELLIGENT_TIERING to move your\n\nobject to the Archive Access tier and Deep Archive Access tier if\n\nthey have not been accessed for a longer period. To do this, follow\n\nthese steps:\n\n1. Navigate to the Properties tab for the bucket.\n\n2. Scroll down to Intelligent-Tiering Archive con\u0000gurations and click on\n\nCreate con\u0000guration.\n\n3. Name the con\u0000guration and specify whether you want to enable it for all\n\nobjects in the bucket or on a subset based on a \u0000lter and/or tags.\n\n4. Under Status, click on Enable to enable the con\u0000guration directly aer\n\nyou create it.\n\n5. Under Archive rule actions, enable the Archive Access tier and specify\n\nthe number of days in which the objects should be moved to this class if\n\nthey’re not being accessed. e value must be between 90 and 730 days.\n\nSimilarly, enable the Deep Archive Access tier and set the number of\n\ndays to a minimum of 180 days. It’s also possible to enable only one of\n\nthese classes:\n\nFigure 1.1 – Intelligent-Tiering Archive rule action\n\n6. Click on Create to create the con\u0000guration.\n\nSee also\n\nA detailed comparison of storage classes:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-\n\nintro.html\n\nS3 storage classes pricing: https://aws.amazon.com/s3/pricing/\n\nEnforcing encryption on S3 buckets\n\nAmazon S3 encryption increases the level of security and privacy of\n\nyour data; it helps ensure that only authorized parties can read it.\n\nEven if an unauthorized person gains logical or physical access to\n\nthat data, the data is unreadable if they don’t get a hold of the key to\n\nunencrypt it.\n\nS3 supports encrypting data both at transit (as it travels to and from\n\nS3) and at rest (while it’s stored on disks in S3 data centers).\n\nFor protecting data at rest, you have two options. e \u0000rst is server-\n\nside encryption (SSE), in which Amazon S3 will be handling the\n\nheavy encryption operation on the server side in AWS. By default,\n\nAmazon S3 encrypts your data using SSE-S3. However, you can\n\nchange this to SSE-KMS, which uses KMS keys for encryption, or to\n\nSSE-C, where you can provide and manage your own encryption\n\nkey. Alternatively, you can encrypt your data using client-side\n\nencryption, where Amazon S3 doesn’t play any role in the\n\nencryption process rather; you are responsible for all the encryption\n\noperations.\n\nIn this recipe, we’ll learn how to enforce SSE-KMS server-side\n\nencryption using customer-managed keys.\n\nGetting ready\n\nFor this recipe, you need to have a KMS key in the same region as\n\nyour bucket to use for encryption. KMS provides a managed key for\n\nS3 (aws/s3) that can be utilized for encryption. However, if you\n\ndesire greater control over the key properties, such as modifying its\n\npolicies or performing key rotation, you can create a customer-\n\nmanaged key. To do so, follow these steps:\n\n1. Sign in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the AWS Key Management Service (AWS KMS)\n\nservice.\n\n2. In the navigation pane, choose Customer managed keys and click on\n\nCreate key.\n\n3. For Key type, choose Symmetric, while for Key usage, choose Encrypt\n\nand decrypt. Click on Next:\n\nFigure 1.2 – KMS conﬁguration\n\n4. Click on Next.\n\n5. Type an Alias value for the KMS key. is will be the display name.\n\nOptionally, you can provide Description and Tags key-value pairs for the\n\nkey.\n\n6. Click on Next. Optionally, you can provide Key administrators to\n\nadminister the key. Click on Finish to create the key.\n\nHow to do it…\n\n1. Sign in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the S3 service.\n\n2. In the Buckets list, choose the name of the bucket that you want to\n\nchange the encryption for and navigate to the Properties tab.\n\n3. Click on Edit in the Default encryption section.\n\n4. For Encryption type, choose Server-side encryption with AWS Key\n\nManagement Service keys (SSE-KMS).\n\n5. For AWS KMS key, you can select Enter AWS KMS key ARN to enter\n\nthe key you have created or browse it using Choose from your AWS\n\nKMS keys.\n\n6. Keep Bucket Key enabled and save your changes:\n\nFigure 1.3 – Changing the default encryption\n\nHow it works…\n\nBy changing the default encryption for your bucket, all newly\n\nuploaded objects to your bucket, which don’t have an encryption\n\nsetting, will be encrypted using the KMS you have provided.\n\nAlready existing objects in your bucket will not be aﬀected.\n\nEnabling the bucket key leads to cost savings in KMS service calls\n\nassociated with the encryption or decryption of individual objects.\n\nis is achieved by KMS generating a key at the bucket level rather\n\nthan generating a separate KMS key for each encrypted object. S3\n\nuses this bucket-level key to generate distinct data keys for objects\n\nwithin the bucket, thereby eliminating the need for additional KMS\n\nrequests to complete encryption operations.\n\nThere’s more…\n\nBy following this recipe, you can encrypt your objects with SSE-\n\nKMS but only if they don’t have encryption con\u0000gured. You can\n\nenforce your objects to have an SSE-KMS encryption setting in the\n\nPUT operation using a bucket policy, as shown here:\n\n1. Navigate to the bucket’s Permissions tab.\n\n2. Go to the Bucket Policy section and click on Edit.\n\n3. Paste the following policy. Make sure you replace <your-bucket-\n\nname> with the actual name of your S3 bucket and <your-kms-\n\nkey-arn> with the Amazon Resource Name (ARN) of your KMS key:\n\n{ \"Version\": \"2012-10-17\", \"Id\": \"EnforceSSE-KMS\", \"Statement\": [ { \"Sid\": \"DenyNonKmsEncrypted\", \"Effect\": \"Deny\", \"Principal\": \"*\", \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::<your- bucket-name>/*\", \"Condition\": {\n\n\"StringNotEquals\": { \"s3:x-amz-server-side- encryption\": \"aws:kms\" } } }, { \"Sid\": \"AllowKmsEncrypted\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::<your- bucket-name>/*\", \"Condition\": { \"StringEquals\": { \"s3:x-amz-server-side- encryption\": \"aws:kms\", \"s3:x-amz-server-side- encryption-aws-kms-key-id\": \"<your-kms-key- arn>\" } } } ] }\n\n4. Save your changes.\n\nis policy contains two statements. e \u0000rst statement\n\n(DenyNonKmsEncrypted) denies the s3:PutObject action for\n\nany request that does not include SSE-KMS encryption. e second\n\nstatement (AllowKmsEncrypted) only allows the s3:PutObject\n\naction when the request includes SSE-KMS encryption and the\n\nspeci\u0000ed KMS key.\n\nSee also\n\nSSE-C:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEn\n\ncryptionCustomerKeys.html\n\nClient-side encryption:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSi\n\ndeEncryption.html\n\nEnforcing encryption in transit with TLS1.2 or higher with Amazon S3:\n\nhttps://aws.amazon.com/blogs/storage/enforcing-encryption-in-transit-\n\nwith-tls1-2-or-higher-with-amazon-s3/\n\nEncrypting existing S3 objects:\n\nhttps://aws.amazon.com/blogs/storage/encrypting-existing-amazon-s3-\n\nobjects-with-the-aws-cli/\n\nSetting up retention policies for your objects\n\nAmazon S3’s storage lifecycle allows you to manage the lifecycle of\n\nobjects in an S3 bucket based on prede\u0000ned rules. e lifecycle\n\nmanagement feature consists of two main actions: transitions and\n\nexpiration. Transitions involve automatically moving objects\n\nbetween diﬀerent storage classes based on a de\u0000ned duration. is\n\nhelps in optimizing costs by storing less frequently accessed data in\n\na cheaper storage class. Expiration, on the other hand, allows users\n\nto set rules to automatically delete objects from an S3 bucket. ese\n\nrules can be based on a speci\u0000ed duration. Additionally, you can\n\napply a combination of transitions and expiration actions to objects.\n\nAmazon S3’s storage lifecycle provides \u0000exibility and ease of\n\nmanagement for users and it helps organizations optimize storage\n\ncosts while ensuring that data is stored according to its relevance\n\nand access patterns.\n\nIn this recipe, we will learn how to set up a lifecycle policy to\n\narchive objects in S3 Glacier aer a certain period and then expire\n\nthem.\n\nGetting ready\n\nTo complete this recipe, you need to have a Glacier vault, which is a\n\nseparate storage container that can be used to store archives,\n\nindependent from S3. You can create one by following these steps:\n\n1. Open the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the Glacier service.\n\n2. Click on Create vault to start creating a new Glacier vault.",
      "page_number": 66
    },
    {
      "number": 5,
      "title": "Segment 5 (pages 85-105)",
      "start_page": 85,
      "end_page": 105,
      "detection_method": "synthetic",
      "content": "3. Provide a unique and descriptive name for your vault in the Vault name\n\n\u0000eld.\n\n4. Optionally, you can choose to receive noti\u0000cations for events by clicking\n\nTurn on noti\u0000cations under the Event noti\u0000cations section.\n\n5. Click on Create to create the vault.\n\nHow to do it…\n\n1. Open the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the S3 service.\n\n2. Select the desired bucket for which you want to con\u0000gure the lifecycle\n\npolicy and navigate to the Management tab.\n\n3. In the le panel, select Lifecycle and click on Create lifecycle rule.\n\n4. Under Rule name, name the lifecycle rule to identify it.\n\n5. Under Choose a rule scope, you can choose Apply to all objects in the\n\nbucket or Limit the scope of this rule using one or more \u0000lters to specify\n\nthe objects for which the rule will be applied. You can use one of the\n\nfollowing \u0000lters or a combination of them:\n\nFilter objects based on pre\u0000xes (for example, logs)\n\nFilter objects based on tags; you can add multiple key-value pair\n\ntags to \u0000lter on\n\nFilter objects based on object size by setting Specify minimum\n\nobject size and/or Specify maximum object size and specifying\n\nthe size value and unit\n\ne following screenshot shows a rule that’s been restricted to a\n\nset of objects based on a pre\u0000x:\n\nFigure 1.4 – Lifecycle rule conﬁguration\n\n6. Under Lifecycle rule actions, select the following options:\n\nMove current versions of objects between storage classes. en,\n\nchoose one of the Glacier classes and set Days aer object\n\ncreation in which the object will be transitioned (for example,\n\n60 days).\n\nExpire current versions of objects. en, set Days aer object\n\ncreation in which the object will expire. Choose a value higher\n\nthan the one you set for transitioning the object to Glacier (for\n\nexample, 100).\n\nReview the transition and expiration actions you have set and\n\nclick on Create rule to apply the lifecycle policy to the bucket:\n\nFigure 1.5 – Reviewing the lifecycle rule\n\nNOTE\n\nIt may take some time for the lifecycle rule to be applied to all the selected\n\nobjects, depending on the size of the bucket and the number of objects. The\n\nrule will aﬀect existing ﬁles, not just new ones, so ensure that no applications\n\nare accessing ﬁles that will be archived or deleted as they will no longer be\n\naccessible via direct S3 retrieval.\n\nHow it works…\n\nAer you save the lifecycle rule, Amazon S3 will periodically\n\nevaluate it to \u0000nd objects that meet the criteria speci\u0000ed in the\n\nlifecycle rule. In this recipe, the object will remain in its default\n\nstorage type for the speci\u0000ed period (for example, 60 days) aer\n\nwhich it will automatically be moved to the Glacier storage class.\n\nis transition is handled transparently, and the object’s metadata\n\nand properties remain unchanged. Once the objects are transitioned\n\nto Glacier, they are stored in a Glacier vault and become part of the\n\nGlacier storage infrastructure. Objects will then remain in Glacier\n\nfor the remaining period of expiry (for example, 40 days), aer\n\nwhich they will expire and be permanently deleted from your S3\n\nbucket.\n\nPlease note that once the objects have expired, they will be queued\n\nfor deletion, so it might take a few days aer the object reaches the\n\nend of its lifetime for it to be deleted.\n\nThere’s more…\n\nLifecycle con\u0000guration can be speci\u0000ed as an XML when using the\n\nS3 API or AWS console, which can be helpful if you are planning on\n\nusing the same lifecycle rules on multiple buckets. You can read\n\nmore on setting this up at\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-\n\nlifecycle-rules.html.\n\nSee also\n\nSetting up noti\u0000cations for events related to your lifecycle rule:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-\n\ncon\u0000gure-noti\u0000cation.html\n\nSupported lifecycle transitions and related constraints:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-\n\ntransition-general-considerations.html\n\nVersioning your data\n\nAmazon S3 versioning refers to maintaining multiple variants of an\n\nobject at the same time in the same bucket. Versioning provides you\n\nwith an additional layer of protection by giving you a way to recover\n\nfrom unintended overwrites and accidental deletions as well as\n\napplication failures.\n\nS3 Object Versioning is not enabled by default and has to be\n\nexplicitly enabled for each bucket. Once enabled, versioning cannot\n\nbe disabled and can only be suspended. When versioning is\n\nenabled, you will be able to preserve, retrieve, and restore any\n\nversion of an object stored in the bucket using the version ID. Every\n\nversion of an object is the whole object, not the delta from the\n\nprevious version, and you can set permissions at the version level.\n\nSo, you can set diﬀerent permissions for diﬀerent versions of the\n\nsame object.\n\nIn this recipe, we’ll learn how to delete the current version of an\n\nobject to make the previous one the current version.\n\nGetting ready\n\nFor this recipe, you need to have a version-enabled bucket with an\n\nobject that has at least two versions.\n\nYou can enable versioning for your bucket by going to the bucket’s\n\nProperties tab, editing the Bucket Versioning area, and setting it to\n\nEnable:\n\nFigure 1.6 – Enabling bucket versioning\n\nYou can create a new version of an object by simply uploading a \u0000le\n\nwith the same name to the versioning-enabled bucket.\n\nIt’s important to note that enabling versioning for a bucket is\n\nirreversible. Once versioning is enabled, it will be applied to all\n\nexisting and future objects in that bucket. So, before enabling\n\nversioning, make sure that your application or work\u0000ow is\n\ncompatible with object versioning.\n\nEnabling versioning for the \u0000rst time will take time to take eﬀect, so\n\nwe recommend waiting 15 minutes before performing any write\n\noperation on objects in the bucket.\n\nHow to do it…\n\n1. Sign in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the S3 service.\n\n2. In the Buckets list, select the S3 bucket that contains the object for which\n\nyou want to set the previous version as the current one.\n\n3. In the Objects tab, click on Show versions. Here, you can view all your\n\nobject versions:\n\nFigure 1.7 – Object versions\n\n4. Select the current version of the object that you want to delete. It’s the\n\ntop-most version with the latest modi\u0000ed date.\n\n5. Click on the Delete button and write permanently delete as\n\nprompted on the next screen.\n\nAer deleting the current version, the previous version will\n\nautomatically become the latest version:\n\nFigure 1.8 – Object versions after version deletion\n\n6. Verify that the previous version is now the latest version by checking the\n\nLast modi\u0000ed timestamps or verifying this through object listing,\n\nmetadata, or download.\n\nHow it works…\n\nOnce you enable bucket versioning, each object in the bucket will\n\nhave a version ID that uniquely identi\u0000es the object in the bucket,\n\nand the non-version-enabled buckets will have their version IDs set\n\nto null for their objects. e older versions of an object become\n\nnon-current but continue to exist and remain accessible. When you\n\ndelete the current version of the object, it will be permanently\n\nremoved and the S3 versioning mechanism will automatically\n\npromote the previous version as the current one aer deletion. If\n\nyou delete an object without specifying the version ID, Amazon S3\n\ndoesn’t delete it permanently; instead, it inserts a delete marker into\n\nit and it becomes the current object version. However, you can still\n\nrestore its previous versions:\n\nFigure 1.9 – Object with a delete marker\n\nThere’s more…\n\nS3 rates apply to every version of an object that’s stored and\n\nrequested, so keeping non-current versions of objects can increase\n\nyour storage cost. You can use lifecycle rules to archive the non-\n\ncurrent versions or permanently delete them aer a certain period\n\nand keep the bucket clean from unnecessary object versions.\n\nFollow these steps to add a lifecycle rule to delete non-current\n\nversions aer a certain period:\n\n1. Go to the bucket’s Management tab and click on the Lifecycle\n\ncon\u0000guration.\n\n2. Click on the Add lifecycle rule button to create a new rule.\n\n3. Provide a unique name for the rule.\n\n4. Under Apply rule to, select the appropriate resources (for example, the\n\nentire bucket or speci\u0000c pre\u0000xes).\n\n5. Set the action to Permanently delete non-current versions.\n\n6. Specify Days aer objects become noncurrent in which the delete will be\n\nexecuted. Optionally, you can specify Number of newer versions to\n\nretain, which means it will keep the said number of versions for the\n\nobject and all others will be deleted when they are eligible for deletion\n\nbased on the speci\u0000ed period.\n\n7. Click on Save to save the lifecycle rule.\n\nSee also\n\nYou can prevent accidental or malicious deletion of objects in a\n\nversioned bucket by enabling Multi-Factor Authentication (MFA)\n\nDelete. You can learn how to implement this at\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorA\n\nuthenticationDelete.html.\n\nSetting permissions at the version level:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/VersionedObj\n\nectPermissionsandACLs.html.\n\nReplicating your data\n\nAWS S3 replication is an automatic asynchronous process that\n\ninvolves copying objects to one or multiple destination buckets.\n\nReplication can be con\u0000gured across buckets in the same AWS\n\nregion with Same-Region Replication, which can be useful for\n\nscenarios such as isolating diﬀerent workloads, segregating data for\n\ndiﬀerent teams, or achieving compliance requirements. Replication\n\ncan also be con\u0000gured for buckets across diﬀerent AWS regions\n\nwith Cross-Region Replication (CRR), which helps in reducing\n\nlatency for accessing data, especially for enterprises with a large\n\nnumber of locations, by maintaining multiple copies of the objects\n\nin diﬀerent geographies or diﬀerent regions. It provides compliance\n\nand data redundancy for improved performance, availability, and\n\ndisaster recovery capabilities.\n\nIn this recipe, we’ll learn how to set up replication between two\n\nbuckets in diﬀerent AWS regions and the same AWS account.\n\nGetting ready\n\nYou need to have an S3 bucket in the destination AWS region to act\n\nas a target for the replication. Also, S3 versioning must be enabled\n\nfor both the source and destination buckets.\n\nHow to do it…\n\n1. Sign in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the S3 service.\n\n2. In the Buckets list, choose the source bucket you want to replicate.\n\n3. Go to the Management tab and select Create replication rule under\n\nReplication rules.\n\n4. Under Replication rule name in the Replication rule con\u0000guration\n\nsection, give your rule a unique name.\n\n5. Under Status, either keep it Enabled for the rule to take eﬀect once you\n\nsave it or change it to Disabled to enable it later as required:\n\nFigure 1.10 – Replication rule conﬁguration\n\n6. If this is the \u0000rst replication rule for the bucket, Priority will be set to 0.\n\nSubsequent rules that are added will be assigned higher priorities. When\n\nmultiple rules share the same destination, the rule with the highest\n\npriority takes precedence during execution, typically the one created last.\n\nIf you wish to control the priority for each rule, you can achieve this by\n\nsetting the rule using XML. For guidance on how to con\u0000gure this, refer\n\nto the See also section.\n\n7. In the Source bucket section, you have the option to replicate all objects\n\nin the bucket by selecting Apply to all objects in the bucket or you can\n\nnarrow it down to speci\u0000c objects by selecting Limit the scope of this\n\nrule using one or more \u0000lters and specifying a Pre\u0000x value (for example,\n\nlogs_ or logs/) to \u0000lter objects. Additionally, you have the option to\n\nreplicate objects based on their tags. Simply choose Add tag and input\n\nkey-value pairs. is process can be repeated so that you can include\n\nmultiple tags:\n\nFigure 1.11 – Source bucket conﬁguration\n\n8. Under Destination, select Choose a bucket in this account and enter or\n\nbrowse for the destination bucket name.\n\n9. Under IAM role, select Choose from existing IAM roles, then choose\n\nCreate new role from the drop-down list.\n\n10. Under Destination storage class, you can select Change the storage class\n\nfor the replicated objects and choose one of the storage classes to be set\n\nfor the replicated objects in the destination bucket.\n\n11. Click on Save to save your changes.\n\nHow it works…\n\nBy adding this replication rule, you grant the source bucket\n\npermission to replicate objects to the destination bucket in the said\n\nregion. Once the replication process is complete, the destination\n\nbucket will contain a copy of the objects from the source bucket.\n\ne objects in the destination bucket will have the same ownership,\n\npermissions, and metadata as the source objects. When you enable\n\nreplication to your bucket, several background processes occur to\n\nfacilitate this process. S3 continuously monitors changes to objects\n\nin your source bucket. Once a change is detected, S3 generates a\n\nreplication request for the corresponding objects and initiates the\n\nprocess of transferring the data from the source to the destination\n\nbucket.\n\nThere’s more…\n\nere are additional options that you can enable while setting the\n\nreplication rule under Additional replication options. e\n\nReplication metrics option enables you to monitor the replication\n\nprogress with S3 Replication metrics. It does this by tracking bytes\n\npending, operations pending, and replication latency. e\n\nReplication Time Control (RTC) option can be bene\u0000cial if you\n\nhave a strict service-level agreement (SLA) for data replication as it\n\nwill ensure that approximately 99% of your objects will be replicated\n\nwithin a 15-minute timeframe. It also enables replication metrics to\n\nnotify you of any instances of delayed object replication. e Delete\n\nmarker replication option will replicate object versions with a delete\n\nmarker. Finally, the Replica modi\u0000cation sync option will replicate\n\nthe metadata changes of objects.\n\nSee also\n\nReplicating buckets in diﬀerent AWS accounts:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-\n\nwalkthrough-2.html\n\nReplication con\u0000guration:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-\n\nadd-con\u0000g.html\n\nMonitoring your S3 bucket\n\nEnabling and monitoring S3 metrics allows you to proactively\n\nmanage your S3 resources, optimize performance, ensure\n\nappropriate security and compliance measures are in place, identify\n\ncost-saving opportunities, and ensure the operational readiness of\n\nyour S3 infrastructure. S3 oﬀers various methods for monitoring\n\nyour buckets, including S3 server access logs, CloudTrail,\n\nCloudWatch metrics, and S3 event noti\u0000cations. S3 server access\n\nlogs can be enabled to log each request made to the bucket.\n\nCloudTrail captures actions taken on S3 or API calls on the bucket,\n\nallowing you to monitor and audit actions, including object-level\n\noperations such as uploads, downloads, and deletions. CloudWatch\n\nmetrics track speci\u0000c metrics for your buckets and allow you to set\n\nup alarms so that you receive noti\u0000cations when certain thresholds\n\nare met. S3 event noti\u0000cations enable you to set up noti\u0000cations for\n\nspeci\u0000c S3 events and con\u0000gure actions in response to those events.\n\nIn this recipe, we will cover enabling CloudTrail for your S3 buckets\n\nand con\u0000guring CloudWatch metrics to monitor high-volume data\n\ntransfer based on these logs.\n\nGetting ready\n\nTo proceed with this recipe, you need to enable CloudTrail so that it\n\ncan log S3 data events and insights. Follow these steps:\n\n1. Open the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the CloudTrail service.\n\n2. Click on Trails in the le navigation pane and click on Create trail to\n\ncreate a new trail.\n\n3. Provide a name for the trail in the Trail name \u0000eld.\n\n4. For Storage location, you need to provide an S3 bucket for storing\n\nCloudTrail logs. You can select Use existing S3 bucket or Create new S3\n\nbucket.\n\n5. Optionally, you can enable Log \u0000le SSE-KMS encryption and choose the\n\nKMS key.\n\n6. Under CloudWatch Logs, choose Yes for Send CloudTrail events to\n\nCloudWatch Logs.\n\n7. Con\u0000gure the CloudWatch Logs settings as per your requirements. For\n\nexample, you can select an existing CloudWatch Logs group or create a\n\nnew one:\n\nFigure 1.12 – Enabling CloudWatch Logs\n\n8. For Role name, choose to create a new one and give it a name.\n\n9. Review the other trail settings, such as log \u0000le validation and tags, make\n\nadjustments if needed, and click on Next.\n\n10. Under the Events section, enable Data events and Insight events in\n\naddition to Management events, which is already enabled.\n\n11. Under Management events, select Read and Write:\n\nFigure 1.13 – Conﬁguring Events\n\n12. Under Data events, choose S3 for Data event type and Log all events for\n\nthe Log selector template.\n\n13. Under Insights events, select API call rate and API error rate.\n\n14. Click on Next and then click on Create trail to create the trail.\n\nOnce the trail has been created, CloudTrail will start capturing S3\n\ndata events and storing the logs in the speci\u0000ed S3 bucket.\n\nSimultaneously, the logs will be sent to the CloudWatch Logs group\n\nspeci\u0000ed during trail creation.\n\nHow to do it…\n\n1. Open the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the CloudWatch console.\n\n2. Go to Log groups from the navigation pane on the le and select the\n\nCloudTrail log group you just created.\n\n3. Click on Create Metric Filter from the Action drop-down list.\n\n4. Provide { ($.eventName = CopyObject) ||\n\n($.eventName = PutObject) || ($.eventName =\n\nCompleteMultipartUpload) &&\n\n$.request.bytes_transferred > 500000000} as the \u0000lter\n\npattern. is \u0000lter pattern will capture events related to copying or\n\nuploading objects to S3 that are larger than 500 MB. e threshold value\n\nshould be set based on your bucket access patterns.\n\nYou can test your pattern by specifying one of the log \u0000les or\n\nproviding a custom log in the Test pattern section. en, you\n\ncan click on Test pattern and validate the result:",
      "page_number": 85
    },
    {
      "number": 6,
      "title": "Segment 6 (pages 106-124)",
      "start_page": 106,
      "end_page": 124,
      "detection_method": "synthetic",
      "content": "Figure 1.14 – Filter pattern\n\n5. Click on Next.\n\n6. Under the Create \u0000lter name \u0000eld, specify a \u0000lter name.\n\n7. Under the Metric Details section, specify a Metric namespace value (for\n\nexample, S3Metrics) and provide a name for the metric itself under\n\nMetric name (for example, HighVolumeTransfers).\n\n8. Set Unit to Count for your metric and set Metric value to 1 to indicate\n\nthat a transfer event has occurred. Finally, set Default value to 0:\n\nFigure 1.15 – Metric details\n\n9. Click on Create metric \u0000lter.\n\nHow it works…\n\nBy enabling CloudTrail in your AWS account and ensuring that the\n\nlogs are delivered to CloudWatch, the S3 API activities can be\n\naccessed and analyzed within your AWS environment. By creating a\n\nmetric \u0000lter with a customized \u0000lter pattern that matches S3\n\ntransfer events, relevant information from the CloudTrail logs can\n\nbe extracted. Once the metric \u0000lter is created, CloudWatch\n\ngenerates a custom metric based on the \u0000lter’s con\u0000guration. is\n\nmetric represents the occurrence of high-volume S3 transfers. You\n\ncan then view this metric in the CloudWatch console, where you\n\ncan gain insights into your S3 transfer activity and take the\n\nnecessary actions.\n\nThere’s more…\n\nOnce your metric has been created, you can create alarms based on\n\nthe metric’s value to notify you when a high volume of S3 transfers\n\nhas been detected.\n\nTo create an alarm for the metric you have created based on high-\n\nvolume S3 transfers from CloudTrail logs on CloudWatch, follow\n\nthese steps:\n\n1. Go to the CloudWatch console and select the Alarms tab.\n\n2. Click on Create Alarm. In the Create Alarm wizard, select the metric\n\nyou created. You can \u0000nd it by navigating the namespace and \u0000nding the\n\nmetric name you con\u0000gured earlier.\n\n3. Under the Metric section, set Statistic to Sum and Period to 15 minutes.\n\nis can be changed as per your needs:\n\nFigure 1.16 – Metric statistics\n\n4. Under the Conditions section, Set reshold type to Static and choose\n\nGreater than for the alarm condition. is indicates a high volume\n\ntransfer on your bucket, as per your observations. Optionally, you can\n\nchoose how many data points within the evaluation period must be\n\nbreached to cause the alarm to go to the alarm state by expanding\n\nAdditional con\u0000guration. is will help you avoid false positives caused\n\nby transient spikes in the metric values:\n\nFigure 1.17 – Metric conditions\n\n5. Click on Next.\n\n6. Under the Noti\u0000cation section, choose In alarm to send a noti\u0000cation\n\nwhen the metric is in the alarm state. Choose Create a new topic,\n\nprovide a name for it and the email endpoints that will receive the SNS\n\nnoti\u0000cation, and click on Create topic or choose an existing SNS topic if\n\nyou have one that’s been con\u0000gured. You can con\u0000gure other actions to\n\nbe executed if the alarm state is triggered, such as executing a Lambda\n\nfunction or performing automated scaling actions:\n\nFigure 1.18 – Metric notiﬁcation settings\n\n7. Provide a name for the alarm so that it can be identi\u0000ed with ease.\n\n8. Review the alarm settings and click on Create Alarm to create the alarm.\n\nOnce the alarm has been created, it will start monitoring the metric\n\nfor high-volume S3 transfers based on the de\u0000ned conditions. If the\n\nthreshold is breached for the speci\u0000ed duration (there are more\n\nthan 150 data transfer requests of more than 500 MB within 45\n\nminutes), the alarm state will be triggered, and an SNS noti\u0000cation\n\nwill be sent. is allows you to receive timely noti\u0000cations and take\n\nappropriate remedial actions in case of high-volume S3 transfers,\n\nensuring that any potential issues are addressed proactively.\n\nSee also\n\nS3 monitoring tools:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/monitoring-\n\nautomated-manual.html\n\nLogging options for S3:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-\n\nS3.html\n\nOceanofPDF.com\n\n2 Sharing Your Data Across Environments and Accounts\n\nData sharing plays a pivotal role in today’s data-driven world,\n\nenabling organizations to unlock the full potential of their data\n\nresources. It encourages collaboration and drives innovation.\n\nSharing data securely and eﬃciently is a fundamental requirement\n\nfor businesses across various industries. is chapter explores\n\nvarious AWS solutions designed to facilitate the secure sharing of\n\ndata with both internal and external stakeholders. ese solutions\n\nrecognize the signi\u0000cance of data sharing and provide robust\n\noptions to meet diverse sharing needs.\n\nWe will start by discussing methods for sharing database resources\n\nby creating read-only replicas for RDS, which ensures reliable\n\nrelational data sharing and reduces the operational load on the\n\nprimary RDS instance. Similarly, we will explore Redshi live data\n\nsharing that allows multiple clusters to access the same data,\n\nenhancing collaborative analysis and distributing the load across\n\ndiﬀerent clusters.\n\nNext, we will leverage Lake Formation to synchronize the Glue Data\n\nCatalog across diﬀerent AWS accounts, providing a uni\u0000ed view of\n\nmetadata for data discovery. We’ll also examine enforcing \u0000ne-\n\ngrained permissions on Simple Storage Service (S3) data using Lake\n\nFormation to enhance data lake security.\n\nFor dynamic data sharing, we will cover real-time S3 data sharing\n\nusing SNS and Lambda for immediate access, as well as temporary\n\nsharing using presigned URLs. Finally, we’ll discuss sharing\n\noperational data by providing read-only access to CloudWatch data\n\nwith other AWS accounts.\n\nis chapter contains the following recipes:\n\nCreating read-only replicas for RDS\n\nRedshi live data sharing among your clusters\n\nSynchronizing Glue Data Catalog to a diﬀerent account\n\nEnforcing \u0000ne-grained permissions on S3 data sharing using Lake\n\nFormation\n\nSharing your S3 data temporarily using a presigned URL\n\nReal-time sharing of S3 data\n\nSharing read-only access to your CloudWatch data with another AWS\n\naccount\n\nTechnical requirements\n\nMany recipes in this chapter presume the availability of an S3\n\nbucket, Glue databases and tables, and an IAM role/user, in\n\naddition to the one you are utilizing to implement the recipes, to\n\nserve as the grantee for data sharing. Furthermore, certain recipes\n\nassume the presence of two AWS accounts to streamline the process\n\nof sharing data with other AWS accounts.\n\nYou can \u0000nd the code \u0000les for this chapter on GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter02.\n\nCreating read-only replicas for RDS\n\nAmazon Relational Database Service (RDS) provides a functionality\n\nto establish read-only replicas, replicating your database for\n\nmultiple users’ access and eﬃcient data sharing. ese replicas can\n\nreside in the same region as, or a diﬀerent one from, the primary\n\none. is proves bene\u0000cial when encountering heavy read loads that\n\ncould potentially impact RDS performance; replicas can\n\naccommodate the heavy read loads without impacting the primary\n\ninstance’s performance. Additionally, you can create a replica in a\n\nregion that is closer to your users, which can also help in the event\n\nof primary region disruption where the replica can be promoted to\n\na standalone instance, thus allowing seamless operation\n\ncontinuation and disaster recovery.\n\nRDS oﬀers two primary replication modes for read replicas:\n\nIn single-primary mode, a single primary RDS instance manages both\n\nread and write operations, while one or more read replicas can be added\n\nto oﬄoad read traﬃc, making it ideal for applications with high read and\n\nlow write loads.\n\nIn contrast, multi-primary mode features multiple primary instances\n\nthat handle both read and write operations, with synchronous\n\nreplication among them. is setup ensures that if one primary instance\n\nfails, the others can seamlessly take over, making it suitable for\n\napplications with high read and write demands that require high\n\navailability.\n\nIn this recipe, we will learn how to create a read-only replica for\n\nRDS.\n\nGetting ready\n\nFor this recipe, you need to have an RDS instance that can be of the\n\nMySQL, PostgreSQL, or MariaDB type, with automatic backup\n\nenabled. You can enable this by modifying your RDS instance and\n\nsetting the backup retention period to a value greater than zero.\n\nHow to do it…\n\n1. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the RDS service.\n\n2. Select Databases from the le navigation pane, locate your RDS, and\n\nselect Create read replica from the Actions menu.\n\n3. Enter a unique name for your replica under the DB instance identi\u0000er\n\n\u0000eld.\n\n4. In the Instance con\u0000guration section, choose a class for your replica that\n\nmatches the primary RDS class or a bigger one that aligns with your\n\nrequirements.\n\n5. Choose the destination region for your replica. You can choose the same\n\nregion as your primary replica or a diﬀerent one.\n\n6. When con\u0000guring storage, it’s advisable to allocate a type and size\n\nequivalent to or larger than what is allocated to the primary RDS. Under\n\nStorage autoscaling, you can check the box next to Enable storage\n\nautoscaling and choose the maximum value that the replica can auto-\n\nscale to.\n\nFigure 2.1 – Conﬁguring storage for the replica\n\n7. In the Availability section, select from three options according to your\n\nrequirements:\n\nMulti-AZ DB Cluster, which deploys one primary DB and two\n\nreadable standby instances across diﬀerent availability zones\n\nMulti-AZ DB Instance, which launches one primary instance\n\nand one standby DB in separate availability zones\n\nSingle DB instance, which creates a single DB instance\n\n8. For the Connectivity section, choose the protocol used to communicate\n\nwith your database, IPv4 or Dual-stack mode, which will allow\n\ncommunications from both IPv6 and IPv4 protocols. Ensure that the\n\nreplica is assigned to the same DB subnet group as the primary RDS.\n\nDetermine whether the replica will allow public access (this is usually\n\ndisabled in production settings) and specify the security group\n\ngoverning access. Lastly, set the port number for replica connections.\n\nFigure 2.2 – Conﬁguring connectivity for the replica\n\n9. Select the preferred authentication method for the replica.\n\n10. Con\u0000gure additional settings as required in the Additional con\u0000guration\n\nsection. Note that encryption settings must match those of the primary\n\nRDS.\n\n11. Click on Create read replica.\n\nHow it works…\n\nWhile con\u0000guring the read replica, you need to make several key\n\ndecisions to align with your performance, availability, and security\n\nrequirements. First, choose the DB instance class and storage to\n\nmatch or exceed the speci\u0000cations of the primary RDS, especially if\n\nyou anticipate a high load on the replica. Next, decide on the region\n\nfor your replica. Placing the replica in the same region as the\n\nprimary RDS results in lower latency for data replication, while\n\nplacing it in a diﬀerent region enhances disaster recovery\n\ncapabilities but introduces higher latency and potentially higher\n\ndata transfer costs. For availability, you must strike a balance\n\nbetween higher availability options and associated costs. For\n\nconnectivity, you can choose between IPv4, which supports\n\ncommunication over the traditional IP version 4, or dual-stack\n\nmode, which allows communication over both IPv4 and IPv6,\n\noﬀering greater \u0000exibility. Once the replica creation is initiated,\n\nAWS generates a snapshot of the primary RDS that it uses to create\n\nthe replica. e duration of replica creation depends on the data\n\nsize of the primary RDS. Upon completion, the replica becomes\n\navailable and the snapshot is deleted. Asynchronous replication\n\ngoverns data transfer from the primary RDS to the read replica.\n\nData changes in the primary RDS are logged in the binary log,\n\nperiodically retrieved by the replication process, and transmitted to\n\nthe read replica. Replica lag, a potential delay between the primary\n\nand replica RDS instances, may occur due to various factors\n\nincluding network latency, workload on the primary instance, and\n\nthe magnitude of data changes.\n\nThere’s more…\n\nFor enhanced scalability, it’s possible to create up to \u0000ve replicas per\n\ndatabase and implement load balancing among them via AWS\n\nRoute 53 to evenly distribute the workload between them.\n\nFurthermore, you can establish read replicas from these replicas;\n\nhowever, it’s important to note that this may result in increased lag\n\ndue to dual-level synchronization. In the event of disruption in the\n\nprimary RDS, replicas can be manually promoted to standalone\n\ninstances, ensuring seamless operational continuity and facilitating\n\ndisaster recovery.\n\nSee also\n\nHow can I distribute read requests across multiple Amazon RDS read\n\nreplicas?: https://repost.aws/knowledge-center/requests-rds-read-\n\nreplicas\n\nRedshift live data sharing among your clusters\n\nRedshi data sharing is a feature provided by Amazon Redshi. It\n\nenables the sharing of data within and across AWS accounts,\n\nallowing diﬀerent teams or users to access the same data sets for\n\ntheir analytics or reporting needs. Datashare re\u0000ects updates in the\n\nproducer cluster in real time with the consumers ensuring that all\n\nusers have access to the most up-to-date information without the\n\nneed to create and maintain duplicate copies of the same data.\n\nIn this recipe, we will see how to share Redshi data between two\n\nRedshi clusters in diﬀerent AWS accounts.\n\nGetting ready\n\nis recipe assumes that you have two encrypted Redshi clusters\n\nusing Ra3 node types. Also, the tables to be shared from the\n\nproducer cluster should not have interleaved sort keys. If you want\n\nto follow the recipe with clusters in the same account, you can skip\n\nthe authorization and association steps. Instead, directly grant usage\n\nof the datashare to the cluster namespace rather than to the AWS\n\naccount.\n\nHow to do it…\n\ne following actions must be performed in the Redshi producer\n\ncluster:\n\n1. Create the datashare:\n\nI. Connect to your Redshi cluster using a super user.\n\nII. Create a datashare using the following command:\n\nCREATE DATASHARE datashare_name;\n\nAdd PUBLICACCESSIBLE = TRUE to the command if\n\nyour consumer cluster has public access enabled.\n\nIII. Add a schema to the datashare:\n\nALTER DATASHARE datashare_name ADD SCHEMA schema_name;\n\nIV. Add tables under the schema that you have added to the\n\ndatashare:\n\nALTER DATASHARE datashare_name ADD TABLE schema_name.table_name;\n\nYou can add all the tables within the schema using the\n\ncommand that follows:\n\nALTER DATASHARE datashare_name ADD ALL TABLES in schema schema_name;\n\nTo add all new tables in the schema, add SET\n\nINCLUDENEW = TRUE; to the previous command.\n\nV. Repeat the previous two steps for all the schemas you want to\n\nshare.\n\nVI. Grant permission for the consumer cluster AWS account:\n\nGRANT USAGE ON DATASHARE datashare_name TO ACCOUNT 'account_ID';",
      "page_number": 106
    },
    {
      "number": 7,
      "title": "Segment 7 (pages 125-143)",
      "start_page": 125,
      "end_page": 143,
      "detection_method": "synthetic",
      "content": "2. Authorize the datashare:\n\nI. Log in to AWS Management Console\n\n(https://console.aws.amazon.com/console/home?\n\nnc2=h_ct&src=header-signin) and navigate to the Redshi\n\nservice.\n\nII. From the navigation menu on the le, go to Datashares.\n\nIII. Go to the In my account tab and open the datashare you have\n\ncreated.\n\nIV. In the Data consumers section, choose the consumer you gave\n\naccess to and then click on Authorize.\n\nFigure 2.3 – Authorizing a consumer\n\nActions in the consumer Redshi cluster include the following:\n\n1. Associate the datashare with consumer clusters:\n\nI. Log in to AWS Management Console and navigate to the\n\nRedshi service.\n\nII. From the navigation menu on the le, go to Datashares.\n\nIII. Choose the From other accounts tab. Select the datashare you\n\nhave created in the producer cluster and click on Associate.\n\nFigure 2.4 – Associating the datashare\n\nIV. Under Association type, select Speci\u0000c AWS Regions and namespaces,\n\nthen choose Add Region.\n\nFigure 2.5 – Choosing the association type\n\nV. Select the region your cluster resides in, then select Add speci\u0000c cluster\n\nnamespaces, select one or more cluster namespaces, and click on Add\n\nAWS Region. Alternatively, you can choose to associate the datashare\n\nwith all the clusters in your account by choosing the entire AWS account\n\nor all clusters in a speci\u0000c region by choosing Entire AWS account or\n\nAdd all cluster namespaces.\n\nFigure 2.6 – Deﬁning the AWS region to associate\n\nVI. Click on Associate.\n\nFigure 2.7 – Association with speciﬁc AWS region\n\n2. Create a database for the datashare:\n\nI. Connect to your Redshi cluster.\n\nII. Create a local database that references the datashare, specifying\n\nthe namespace and account ID:\n\nCREATE DATABASE consumer_database_name FROM DATASHARE datashare_name OF ACCOUNT 'producer_account_ID' NAMESPACE 'producer_cluster_namespace';\n\nIII. You can now query the tables as follows:\n\nSELECT * FROM consumer_database_name.schema_name.table _name;\n\nHow it works…\n\nFirst, you initiate the recipe by creating the datashare within the\n\nproducer cluster and including the relevant databases and tables for\n\nthe consumer’s access. Aerward, you grant the consumer cluster’s\n\nAWS account access to the datashare, initially resulting in a pending\n\nauthorization status. Upon authorization, the status shis to\n\nAuthorized. At this point, the consumer perceives the data share’s\n\nstatus as Available (with action required within the Amazon\n\nRedshi console). To complete the process and activate the\n\ndatashare in both the producer and consumer clusters, you\n\nassociate the datashare with the consumer cluster.\n\nis enables the consumer to create a database associated with the\n\ndatashare. is enables direct querying of the tables in the data\n\nshare using\n\nconsumer_database_name.schema_name.table_name.\n\nThere’s more…\n\nYou can extend the same datashare to multiple consumers by\n\nproviding access to each consumer cluster’s namespace if it’s within\n\nthe same AWS account, or to its AWS account if it belongs to a\n\ndiﬀerent account. You can achieve this by executing separate\n\nRedshi GRANT commands on the datashare.\n\nTo address security concerns, it’s possible to share your views\n\nwithout sharing the underlying tables. Sharing a view follows the\n\nsame procedure as sharing a table.\n\nSee also\n\nManaging existing datashares:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/manage-datashare-\n\nexisting-console.html\n\nConsumer cluster administrator actions:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/consumer-cluster-\n\nadmin.html\n\nAmazon Redshi database encryption:\n\nhttps://docs.aws.amazon.com/redshi/latest/mgmt/working-with-db-\n\nencryption.html\n\nSynchronizing Glue Data Catalog to a different account\n\nAs a centralized repository, a data lake allows you to store all your\n\nstructured and unstructured data at any scale without needing\n\nupfront data schema de\u0000nitions. AWS Lake Formation simpli\u0000es\n\ndata lake management on AWS by acting as a layer on top of\n\nexisting data storage services such as S3. It automatically catalogs\n\ndata, manages access, and integrates with various AWS services\n\nsuch as Glue and Athena. One of its key features is its integration\n\nwith AWS Glue. Lake formation in conjunction with AWS Resource\n\nAccess Manager (RAM) oﬀers a secure and simple method for the\n\nsharing of the AWS Glue Data Catalog across multiple AWS\n\naccounts. is promotes collaboration and data accessibility across\n\ndiﬀerent teams or organizations, allowing them to make the most of\n\ntheir data lakes and extract valuable insights from their shared\n\nresources. However, you need to carefully plan and test your data-\n\nsharing process. Before sharing the catalog, conduct a\n\ncomprehensive analysis of existing applications and work\u0000ows to\n\nidentify any dependencies on speci\u0000c catalog resources or\n\npermissions that may require adjustments. Existing applications\n\nmay slow down due to the increased read load from the shared\n\naccount. Ensure that permissions are set up correctly so that your\n\napplications can access the intended resources, whether they are\n\noriginal or shared, and avoid unauthorized exposure.\n\nTo ensure that your resources can handle the additional load,\n\nconsider conducting load testing and monitoring performance\n\nmetrics, adjusting resource allocation as necessary. Be aware that\n\ndelays in synchronization may lead to data consistency issues. To\n\nmitigate these risks, consider sharing the data within the same\n\nregion to reduce latency or select regions that have low latency\n\nbetween them.\n\nIn this recipe, we will learn how to share the Glue Data Catalog with\n\nanother AWS account using Lake Formation.\n\nGetting ready\n\nis recipe assumes the existence of two AWS accounts and a Glue\n\ndatabase within one of those accounts, which you intend to share\n\nwith the other. Ensure that you perform this recipe with a user or\n\nrole with administrative privileges in AWS Lake Formation by\n\nattaching the AWSLakeFormationDataAdmin managed policy to its\n\npermissions. To be able to use Lake Formation to control access to\n\nthe Glue Data Catalog, you need to switch to the Lake Formation\n\npermission model. If you already have Glue tables permission\n\nmanaged by IAM identities, list down these permissions, and once\n\nyou \u0000nish the outlined steps, grant the same permissions using the\n\nLake Formation:\n\n1. Revoke the super access from the IAMAllowedPrincipals principle, an\n\nautomatically generated entity containing IAM users and roles\n\nauthorized to access your Data Catalog resources in accordance with\n\nyour IAM policies.\n\nI. Navigate to Lake Formation and select Data Lake permissions\n\nunder Permissions in the navigation pane on the le.\n\nII. Search for the permission for the IAMAllowedPrincipals\n\nprincipal on your table. Select the permission and click on\n\nRevoke.\n\nFigure 2.8 – Revoking the IAMAllowedPrincipals permission\n\n2. Disable the default setting of controlling access to the new tables added\n\nto the databases only through IAM access control for the databases you\n\nshare:\n\nI. Go to Databases under Data Catalog in the le navigation pane\n\non the Lake Formation service.\n\nII. Select the database you will be sharing and click on Edit.\n\nDisable Use only IAM access control for new tables in this\n\ndatabase and Use only IAM access control for new databases\n\nunder Default permissions.\n\nIII. Repeat these steps for all the databases you will be sharing.\n\nFigure 2.9 – Disabling Data Catalog’s default settings\n\n3. Stop new tables created in the Data Catalog from having a default super\n\npermission to IAMAllowedPrincipals:\n\nI. Choose Data Catalog settings from Administration in the Lake\n\nFormation navigation pane.\n\nII. Clear both Use only IAM access control for new databases and\n\nUse only IAM access control for new tables in this database\n\nunder the Default permissions for newly created databases and\n\ntables section.\n\nIII. Click on Save.\n\n4. Register the S3 location for the Glue database or tables that you will\n\nshare with Lake Formation:\n\nI. In the Lake Formation navigation pane, go to Data Lake\n\nlocations under Register and Ingest.\n\nII. Choose Register location, and then choose Browse to select an\n\nAmazon S3 path for the database or tables you intend to share.\n\nIII. Repeat these steps for the diﬀerent S3 locations you have for the\n\ndatabase or tables you intend to share.\n\nHow to do it…\n\nLet’s consider that the account hosting the Glue database is referred\n\nto as Account A, and the account with which we intend to share the\n\ndatabase is identi\u0000ed as Account B.\n\nIn Account A, follow these steps:\n\n1. Log in to the AWS console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the AWS Glue service.\n\n2. From the navigation pane on the le, go to Catalog settings under Data\n\nCatalog service.\n\n3. Add the following policy statement to Permissions and save it (make\n\nsure to replace aws_region and aws_account_id with your\n\nspeci\u0000c values):\n\n{ \"Version\" : \"2012-10-17\", \"Statement\" : [ { \"Effect\" : \"Allow\", \"Principal\" : { \"Service\" : \"ram.amazonaws.com\" }, \"Action\" : \"glue:ShareResource\", \"Resource\" : [ \"arn:aws:glue:<aws_region>:\n\n<aws_account_id>:table/*\", \"arn:aws:glue:<aws_region>: <aws_account_id>:database/*\", \"arn:aws:glue:<aws_region>: <aws_account_id>:catalog\" ] } ] }\n\n4. Navigate to the Lake Formation service. From the navigation pane on\n\nthe le, choose Data lake permissions under Permissions.\n\n5. In Data permissions, click on Grant.\n\n6. For principles, choose External accounts, then type Account B’s ID and\n\npress Enter.\n\n7. Under LF-Tags or catalog resources, choose Named Data Catalog\n\nresources. Choose all the databases you want to share under Databases:\n\nFigure 2.10 – Deﬁning the resources to be shared\n\nI. To grant access to the tables within those databases, choose the list of\n\ntables under Tables. You can optionally add Data \u0000lters. In the Table\n\npermissions section, under Table permissions, choose Super to grant all\n\npermissions or choose speci\u0000c permissions from the list. You can\n\noptionally choose Grantable permissions, which will allow Account B to\n\ngrant permissions to the shared resources.\n\nII. To grant access to the database itself, do not select any tables. In the\n\nDatabase permissions section, choosing the permissions is like setting\n\nthe table permissions – you have to choose the level of permissions and\n\noptionally grantable permissions.\n\nIII. Click on Grant.\n\nFigure 2.11 – Deﬁning the table permission to be granted\n\nIn Account B, follow these steps:\n\n1. Log in to the AWS console and navigate to the AWS RAM service.\n\n2. In the le navigation pane, choose Resource shares under Shared with\n\nme.\n\n3. You should see a resource with a pending status from Account A. Click\n\non it and choose Accept resource share.\n\nFigure 2.12 – Accepting a resource share\n\nYou should be able to see the database and tables in Account B lake\n\nformation as if they were created in Account A.\n\nHow it works…\n\nTo begin the sharing process, you created a resource share\n\nspecifying the account to share with and the level of permissions\n\nthat will be granted to the recipient account. is triggers the\n\nsharing work\u0000ow and sends a sharing request to the recipient\n\naccount. Next, in the recipient account, you accepted the invitation\n\nfor the shared Glue databases. e recipient account can access the\n\nshared Glue database using their own Lake Formation console.\n\nThere’s more…\n\nTo be able to query the shared catalog tables in Account B, you have\n\nto create resource links that point to shared databases and tables. If\n\nyou are sharing all tables within a database, you can create a\n\nresource link for the database, which will allow you to query all the\n\ntables within it. However, if only speci\u0000c tables from a database are\n\nshared, you must create resource links for each of those individual\n\ntables. Follow the next steps to create a resource link for the tables.\n\nIf you need to create a resource link for the database, you can follow\n\na similar process by navigating to the Databases section instead of\n\nTables.\n\n1. Navigate to Lake Formation with a user that has the glue:CreateTable\n\npermission.\n\n2. In the navigation pane, choose Tables, and then choose Create table.\n\n3. In the Table details section, choose Resource link.\n\n4. Give a name under Resource link name and then choose the local\n\ndatabase that the table will be added to. Choose the shared table that you\n\nare creating the link for. Finally, for the table’s region under Shared\n\ntable’s region, choose the same region of the shared table and click on\n\nCreate.\n\nFigure 2.13 – Creating a table from a resource link\n\nSee also\n\nUpdate a resource share in AWS RAM:\n\nhttps://docs.aws.amazon.com/ram/latest/userguide/working-with-\n\nsharing-update.html\n\nGranting permissions on a database or table shared with your account:\n\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/regranting-\n\nshared-resources.html\n\nEnforcing fine-grained permissions on S3 data sharing using Lake Formation\n\nAWS Lake Formation simpli\u0000es the process of setting up and\n\nmanaging a data lake by providing capabilities for data ingestion,\n\norganization, and access control. You can arrange your S3 data into\n\ntables within the Lake Formation Data Catalog and implement \u0000ne-\n\ngrained access control on them. is means you can enforce precise\n\naccess permissions at both the table and column levels, ensuring\n\nthat sensitive data is safeguarded. Fine-grained access control\n\nempowers you to exercise precise control over who can view or\n\nmodify speci\u0000c tables and columns within the Lake Formation Data\n\nCatalog. Consequently, you can provide varying levels of access to\n\ndiﬀerent tables or columns, aligning with the speci\u0000c requirements\n\nof diﬀerent users or roles.\n\nIn this recipe, we will use Lake Formation to establish a table for an\n\nS3 dataset and provide granular access controls for it.\n\nGetting ready\n\nTo follow this recipe, you should have an S3 dataset that you want\n\nto grant access to, as well as an IAM role to grant access for. It’s also\n\npreferable that this role has access to Amazon Athena to query the\n\ntable.\n\ne recipe must be implemented with a user or role that has admin\n\naccess to Lake Formation.\n\nHow to do it…\n\n1. Create a table for your S3 location:\n\nI. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?\n\nnc2=h_ct&src=header-signin) and navigate to the Lake\n\nFormation service.\n\nII. In the Lake Formation console, select Databases under Data\n\nCatalog from the navigation pane on the le.\n\nIII. Click on Create database.\n\nIV. Give a name and description for the database under the\n\nrespective \u0000elds and optionally add an S3 location if all the\n\ntables in your database will fall into a common location.",
      "page_number": 125
    },
    {
      "number": 8,
      "title": "Segment 8 (pages 144-167)",
      "start_page": 144,
      "end_page": 167,
      "detection_method": "synthetic",
      "content": "Figure 2.14 – Creating a Lake Formation database\n\nV. Click on Create database.\n\nVI. To create the table automatically using a crawler, go to Crawlers under\n\nData catalog from the navigation pane on the le.\n\nVII. You will be directed to the Glue crawler; click on Create crawler.\n\nVIII. Give a name and description for your crawler and click on Next.\n\nIX. Choose Not yet for Is your data already mapped to Glue tables? under\n\nthe Data source con\u0000guration.\n\nX. Under Data sources, click on Add a data source. For the location of S3\n\ndata, choose In this account and add the S3 path. Click on Add an S3\n\nlocation.\n\nFigure 2.15 – Adding an S3 data source\n\nXI. Optionally, you can add a classi\u0000er for your data under the Custom\n\nclassi\u0000ers section.\n\nXII. Click on Next.\n\nXIII. Click on Create new IAM role, give a name to the role, and click on\n\nCreate | Next.\n\nXIV. Under Target database, choose the database you have created.\n\nXV. Optionally, provide a Table name pre\u0000x.\n\nXVI. You can schedule your crawler or keep it on demand.\n\nXVII. Click on Next and then click on Create crawler.\n\nXVIII. Now click on Run crawler.\n\nXIX. Once the run is done, you should see that one table was\n\nchanged:\n\nFigure 2.16 – Crawler completed a run\n\nXX. You can view the table by going to Lake Formation and clicking\n\non Tables under Data catalog from the navigation pane on the le.\n\n2. Create an LF-Tag for data classi\u0000cation:\n\nI. Navigate to LF-Tags and permissions under Permissions in the\n\nle navigation pane and click on Add LF-Tag.\n\nII. Give a key and list of comma-separated values or a single value\n\nto the tag, click on Add, and then click on Add LF-Tag.\n\nFigure 2.17 – Adding an LF-Tag\n\n3. Assign the tag to your table:\n\nI. Navigate to Tables under Data Catalog in the le navigation\n\npane and open your table.\n\nII. Under the Schema section, click on Edit schema.\n\nIII. Select the columns you want to give access to, click on Edit LF-\n\nTags, then click on Assign new LF-Tag. Choose the LF-Tag you\n\ncreated under Assigned keys, select one of the values of the tag,\n\nand click on Save. You can assign diﬀerent values to the tag for\n\nthe remaining columns in your table.\n\nFigure 2.18 – Assigning tags to columns\n\nIV. Click on Save as new version.\n\n4. Give permission to the IAM role:\n\nI. Navigate to Data lake permissions under Permissions in the\n\nnavigation pane on the le.\n\nII. Click on Grant.\n\nIII. Under the Principals section, choose IAM users and roles and\n\nchoose the IAM users and roles you want to give access to.\n\nIV. Under LF-Tags or catalog resources, choose Resources matched\n\nby LF-Tags (recommended). Choose the key to the tag you\n\ncreated and the values that you are allowing this user access to.\n\nFigure 2.19 – Deﬁning the resource to set up access for\n\nV. Under Table permissions, choose Select and click on Grant.\n\nFigure 2.20 – Deﬁning table permissions\n\n5. Revoke IAMAllowedPrincipals access:\n\nI. Navigate to Lake Formation and select Data lake permissions\n\nunder Permissions in the navigation pane on the le.\n\nII. Search for the permission for IAMAllowedPrincipals principle\n\non your table.\n\nIII. Select the permission and click on Revoke.\n\nFigure 2.21 – Revoking IAMAllowedPrincipals access\n\nHow it works…\n\nWe initiated this recipe by registering our S3 location with the data\n\nlake, followed by the creation of a database in AWS Lake Formation\n\nto host our table. To automate the table creation process, we\n\nemployed AWS Glue Crawler, con\u0000guring it to scan a designated S3\n\nlocation. e information gathered by the crawler was utilized to\n\nde\u0000ne the table’s structure, and added to the Lake Formation Data\n\nCatalog, which serves as a logical representation of the data.\n\nMoving forward, we created an LF-Tag for tagging speci\u0000c columns\n\nwith prede\u0000ned values. Subsequently, we granted select access to a\n\nparticular IAM user or role based on a speci\u0000c tag value, providing\n\naccess to a de\u0000ned set of columns. To align with these access\n\npermissions, it was essential to revoke super access from the\n\nIAMAllowedPrincipals principle, an automatically generated entity\n\nthat includes IAM users and roles with access to Data Catalog\n\nresources as per IAM policies. With these access controls, when\n\nusers attempt to access the table, Lake Formation will grant\n\ntemporary credentials, granting them access to the speci\u0000ed data.\n\nThere’s more…\n\nYou can choose to set up tags at the database level, and all\n\nunderlying tables will inherit these same tags. Similarly, tags can be\n\nde\u0000ned at the table level, and all associated columns will also inherit\n\nthese tags. Additionally, when granting access to users, you have the\n\n\u0000exibility to utilize a combination of various tags. Leveraging LF-\n\nTags provides an eﬃcient means of managing permissions at scale.\n\nAlternatively, if you prefer using named resources rather than LF-\n\nTags-based resources, you can implement data \u0000lters on your tables.\n\nis allows you to precisely specify the rows and columns that can\n\nbe accessed following the outlined steps:\n\n1. Navigate to Data \u0000lters under Data Catalog in the le navigation pane\n\nand click on Create new \u0000lter.\n\n2. Give a name for your data \u0000lter and choose the target database and table\n\nto which the \u0000lter will be applied.\n\n3. Under Column-level access, choose Include columns, then list the\n\ncolumns you want to give access to. Alternatively, you can choose to\n\nexclude columns and list the columns you don’t want to give access to.\n\n4. Under Row-level access, choose Filter rows, and then write down your\n\ncondition the same way as you would add it in a where clause in SQL\n\nquery and click on Create data \u0000lter.\n\nFigure 2.22 – Deﬁning data ﬁlter constraints\n\nNow you can use this \u0000lter when you de\u0000ne the table you are giving\n\naccess to under Named Data Catalog resource.\n\nSee also\n\nData \u0000ltering and cell-level security in Lake Formation:\n\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/data-\n\n\u0000ltering.html\n\nViewing the resources that an LF-Tag is assigned to:\n\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/TBAC-view-tag-\n\nresources.html\n\nSharing your S3 data temporarily using a presigned URL\n\nS3 presigned URLs provide a secure and controlled way to grant\n\ntime-limited access to S3 objects. You can specify an expiration time\n\nwhen generating the URL, aer which the URL becomes invalid.\n\nPresigned URLs can have speci\u0000c permissions associated with\n\nthem; you can generate URLs that allow only speci\u0000c actions (such\n\nas read, write, or both) on individual objects. is provides granular\n\ncontrol over which operations can be performed on the object while\n\nminimizing the exposure of your AWS credentials, as you can avoid\n\nthe need to embed AWS access credentials directly into your\n\napplication or to share your AWS access keys.\n\nIn this recipe, we will learn how to create a presigned URL to\n\ndownload an S3 object.\n\nGetting ready\n\nis recipe assumes that you have an S3 bucket with an object that\n\nyou will be creating a presigned URL for. Also, you need to perform\n\nthese steps using a user who has access to download the object as\n\nthe URL will inherit the same credentials.\n\nHow to do it…\n\n1. Log in to the AWS console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to S3.\n\n2. Navigate to the bucket that has the object you want to create a presigned\n\nURL for. Select the object to be shared from the Objects list.\n\n3. From the Object actions menu, choose Share with a presigned URL.\n\n4. Specify the Time interval until the presigned URL expires value and\n\nthen Number of hours or Number of minutes for the URL to be valid.\n\ne interval must be up to seven hours only; however, if you are doing\n\nthe setup through the CLI, you can set the time interval up to 12 hours.\n\n5. Choose Create presigned URL.\n\nFigure 2.23 – A presigned URL’s expiration time\n\n6. You will get con\u0000rmation that the URL was created and it will\n\nautomatically be copied to your clipboard. You can copy it again using\n\nthe Copy the presigned URL button.\n\nHow it works…\n\nBy clicking on Create presigned URL, you have initiated the process\n\nof creating a time-limited URL that allows temporary access to the\n\nselected object. You can share the presigned URL with the intended\n\nusers, applications, or services. ey can use this URL to access the\n\nS3 object for the duration speci\u0000ed in the expiration time you set.\n\nThere’s more…\n\nAnyone in possession of the generated presigned URL will have the\n\ncapability to download your S3 object. However, you can control\n\nwho can utilize this URL for downloading by limiting access based\n\non IP address ranges. To implement this access restriction,\n\npresigned URLs inherit the access permissions of the user or role\n\nthat created them. erefore, you can achieve this by attaching an\n\nIAM policy to the user or role responsible for generating the\n\npresigned URL, restricting its access to a speci\u0000c IP or IP range.\n\nConsequently, the presigned URL will only be accessible within that\n\nde\u0000ned IP range.\n\nHere’s an illustrative IAM policy. Ensure that you substitute IP-\n\naddress and s3-bucket-name with the actual IP address you\n\nintend to use and the S3 bucket containing the \u0000les to be shared,\n\nrespectively. Also, ensure that there are no con\u0000icting policies in\n\nplace that grant broader access:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"IPAllow\", \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": \"arn:aws:s3:::<s3-bucket- name>/*\", \"Condition\": { \"IpAddress\": { \"aws:SourceIp\": \"IP-address\" } } } ] }\n\nSee also\n\nUploading objects with presigned URLs:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/PresignedUrl\n\nUploadObject.html\n\nReal-time sharing of S3 data\n\nReal-time sharing of S3 data ensures immediate responsiveness to\n\nchanges in data, facilitating seamless communication between\n\nvarious components of a system. By setting up an S3 bucket to\n\ninvoke a Lambda function upon the occurrence of any new event,\n\nperforming the required processing, and publishing the data\n\nthrough noti\u0000cations to an SNS topic, this method provides an\n\neﬀective and scalable means for broadcasting events to multiple\n\nsubscribers. It oﬀers a dynamic and responsive approach to sharing\n\ninformation in real time across diﬀerent systems or applications.\n\nis approach proves particularly advantageous in scenarios where\n\nthe timely distribution of information is crucial for maintaining up-\n\nto-date and synchronized systems.\n\nIn this recipe, we will learn the process of triggering a Lambda\n\nfunction when new \u0000les are created in an S3 bucket, subsequently\n\nprocessing the \u0000les and broadcasting them as SNS noti\u0000cations.\n\nGetting ready\n\nFor this recipe, you need to have an S3 bucket whose content will be\n\nshared.\n\nHow to do it…\n\n1. Create an SNS topic:\n\nI. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?\n\nnc2=h_ct&src=header-signin), navigate to SNS service, and\n\nclick on Create topic.\n\nII. Under Details, choose Standard type, give a name and\n\noptionally a display name for the topic, and click on Create\n\ntopic.\n\nIII. In the Subscription tab, click on Create subscription.\n\nIV. Select Email under Protocol, provide your email under\n\nEndpoint, and click on Create subscription.\n\nFigure 2.24 – Setting up an email subscription to SNS\n\nYou should receive an email asking you to con\u0000rm the subscription.\n\n2. Create an IAM role for the Lambda function:\n\nI. Create a new IAM policy using the following permissions\n\n(make sure to replace sns-topic-arn and s3-bucket-\n\narn with your speci\u0000c values):\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"sns:Publish\" ], \"Resource\": [ \"<sns-topic-arn>\", \"<s3-bucket-arn>/*\" ] } ] }\n\nII. Create a role for Lambda service, attaching the policy from the\n\nprevious step and the AWSLambdaBasicExecutionRole\n\nmanaged policy.\n\n3. Create a Lambda function:\n\nI. Navigate to the Lambda service.\n\nII. Click on Create function, select Author from scratch, and write\n\na function name. Under Runtime, choose Python 3.11, and\n\nunder Permissions, expand Change default execution role,\n\nselect Use an existing role, and choose the role you created in\n\nthe previous step. Finally, click on Create function.\n\nFigure 2.25 – Creating a Python Lambda function\n\n4. Create an S3 event:\n\nI. Navigate to the S3 service.\n\nII. Open the S3 bucket whose \u0000les you need to publish and go to\n\nthe Properties tab.\n\nIII. Under the Event noti\u0000cation section, click on Create event\n\nnoti\u0000cation.\n\nIV. Give an event name and optionally add a pre\u0000x (for example, a\n\nfolder name) for the \u0000les to be shared.\n\nV. Under Event types, select All object create events under Object\n\ncreation.\n\nVI. Under the Destination section, choose Lambda function, select\n\nthe Lambda function created in the previous step, and click on\n\nSave changes.\n\nFigure 2.26 – Conﬁguring the S3 event for Lambda\n\n5. Update the Lambda code:\n\nI. Navigate back to your Lambda function; you should see an S3\n\ntrigger added to your function.\n\nII. In the Code tab, update the code with the following script.\n\nMake sure to update the TopicArn and Subject \u0000elds with\n\nyour own values:\n\nimport json import boto3 import logging logger = logging.getLogger() logger.setLevel(logging.INFO) def publish_to_sns(message): try: sns_client = boto3.client('sns') # add any processing required on the file or extract certain fields before publishing the message. sns_client.publish( TopicArn = 'yourSNSTopicArn', Message = message, Subject = 'emailSubject' ) logging.info('Published SNS message successfully') except ClientError: logging.info('Failed to publish SNS due to: {}'.format(str(ClientError))) return False return True def lambda_handler(event, context): # retrieve S3 file name s3_event = event['Records'][0]['s3'] object_key = s3_event['object'] ['key']\n\nbucket_name = s3_event['bucket'] ['name'] s3_client = boto3.client('s3') # retrieve file from S3 by its object key response = s3_client.get_object(Bucket=bucket_name, Key=object_key) file_content = response['Body'].read().decode('utf-8') # publish S3 file content to SNS publish_to_sns(file_content) return { 'statusCode': 200 }\n\nIII. Click on Deploy.\n\n6. Test: Test the process by uploading a \u0000le to your S3 bucket. is should\n\ntrigger your Lambda function and you should receive an email with the\n\nnoti\u0000cation from SNS.\n\nHow it works…\n\ne IAM role we established provides Lambda with the necessary\n\npermissions to access and read S3 \u0000les within our designated\n\nbucket. It also allows Lambda to publish messages to our SNS topic\n\nand generate log \u0000les in CloudWatch. An S3 event has been\n\ncon\u0000gured to trigger the Lambda function whenever an object is\n\nadded to the bucket, covering various methods such as object\n\nputting or copying. Upon triggering, the Lambda function retrieves\n\nthe S3 \u0000le’s location, reads its content, processes the information,\n\nand subsequently publishes it to the speci\u0000ed SNS topic. All\n\nsubscribers connected to the SNS topic are then noti\u0000ed of the\n\npublished message.\n\nThere’s more…\n\nYou have the \u0000exibility to incorporate multiple subscribers into\n\nyour SNS topic, including AWS services within your AWS account,\n\nor extending to various AWS accounts. Moreover, if your intention\n\nis to directly share S3 events with consumers, you have the option to\n\ncon\u0000gure the S3 event noti\u0000cation destination to be an SNS topic.\n\nis allows for the subscription of multiple consumers to the topic,\n\nproviding them with the choice to utilize it as is or integrate it into\n\nAWS services for customized processing.\n\nSee also\n\nEvent noti\u0000cation types and destinations:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/noti\u0000cation-\n\nhow-to-event-types-and-destinations.html\n\nSharing read-only access to your CloudWatch data with another AWS account",
      "page_number": 144
    },
    {
      "number": 9,
      "title": "Segment 9 (pages 168-187)",
      "start_page": 168,
      "end_page": 187,
      "detection_method": "synthetic",
      "content": "CloudWatch cross-account sharing in AWS is valuable for\n\ncentralizing monitoring and management. It enables organizations\n\nto share monitoring data, including alarms, dashboards, and logs,\n\nacross AWS accounts while maintaining resource isolation. is\n\nfacilitates consolidated reporting, collaboration, troubleshooting,\n\nand cost management. Cross-account sharing allows for customized\n\naccess control and \u0000ne-grained permissions, and is scalable for\n\ngrowing organizations. It simpli\u0000es the sharing of critical\n\nmonitoring data, enhancing operational eﬃciency and visibility\n\nwithout compromising security.\n\nIn this recipe, we will learn how to share read-only access to\n\nCloudWatch data with another AWS account.\n\nGetting ready\n\nTo follow this recipe, you need to have two AWS accounts, one of\n\nwhich must have the CloudWatch log group to be shared. e other\n\naccount will share it and use it for monitoring.\n\nHow to do it…\n\nIn the source AWS account, follow these steps:\n\n1. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the CloudWatch service.\n\n2. From the navigation pane on the le, choose Settings.\n\n3. Go to the Enable account switching section then select Con\u0000gure for the\n\nShare your CloudWatch data option.\n\nFigure 2.27 – Enabling the sharing of CloudWatch data\n\n4. In the Sharing section, click on Add account, then type the monitoring\n\naccount ID and hit Enter.\n\nFigure 2.28 – Sharing to a speciﬁc AWS account\n\n5. For Permissions, choose Provide read-only access to your CloudWatch\n\nmetrics, dashboards, and alarms and check both the Include\n\nCloudWatch automatic dashboards and Include X-Ray read-only access\n\nfor ServiceLens checkboxes.\n\nFigure 2.29 – Conﬁguring permissions\n\n6. Click on Launch CloudFormation template from the Create\n\nCloudFormation stack section.\n\n7. Type Confirm on the con\u0000rmation screen and click on Launch\n\nTemplate.\n\n8. On the Quick create stack page, select I acknowledge that AWS\n\nCloudFormation might create IAM resources with custom names. en\n\nclick on Create stack.\n\nIn the destination AWS account, follow these steps:\n\n1. Log in to the AWS Management Console and navigate to the\n\nCloudWatch service.\n\n2. From the navigation pane on the le, choose Settings.\n\n3. Go to the Enable account switching section, then select Con\u0000gure for\n\nthe View cross-account cross-region option.\n\n4. In the Enable account selector section, select Show selector in the\n\nconsole.\n\n5. Select Custom account selector, and then enter the owner account ID\n\nand a label to identify it.\n\nFigure 2.30 – Conﬁguring the source AWS account\n\n6. Click on Enable.\n\nHow it works…\n\nIn the source AWS account, we have granted read-only access to the\n\ndestination AWS account (monitoring account), enabling it to\n\ncreate dashboards using widgets from the source account, view\n\nautomatically generated dashboards, and access X-Ray data with\n\nread-only permissions for ServiceLens in the source account. e\n\nLake Formation template will create an IAM role with policies that\n\nprovide the speci\u0000ed access levels and establish a trust relationship\n\nwith the destination AWS account.\n\nIn the destination AWS account, we have enabled the ability to view\n\nshared CloudWatch data from the source AWS account. is access\n\nis con\u0000gured through a custom account selector, which oﬀers a\n\ndrop-down list of accounts to choose from based on the labels\n\nassigned to these accounts. is streamlined access management\n\nensures a seamless experience when logging into the destination\n\nAWS account.\n\nThere’s more…\n\nIn the destination AWS account, upon accessing CloudWatch, you\n\nwill encounter a View data feature that presents a drop-down menu.\n\nis menu facilitates seamless switching between viewing data from\n\nyour own account and the source account. By selecting the source\n\naccount based on its designated label, you can proceed to create\n\ndashboards or set up alerts using metrics generated in the source\n\naccount, just as you would for your own account.\n\nFigure 2.31 – Selecting the account of CloudWatch data\n\nSee also\n\nCross-account cross-region dashboards:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cl\n\noudwatch_xaxr_dashboard.html\n\nOceanofPDF.com\n\n3 Ingesting and Transforming Your Data with AWS Glue\n\nIn data engineering, implementing integration between systems to\n\nextract, transform, and load (ETL) data is frequently what\n\nconsumes the most time and cost. AWS Glue is a serverless data\n\nintegration service that provides diﬀerent engines and tools to build\n\nETL jobs in a simple and scalable way, paying for what you use.\n\nGlue is comprised of many components and features to serve many\n\nkinds of data products and users, including a Hive-compatible\n\nmetastore and multiple engines for diﬀerent needs, from single-\n\nnode Glue Python shell jobs to distributed clusters that auto-scale\n\nusing Glue for Spark, and the latest addition: Glue for Ray. Each of\n\nthose engines has connectors for common data storage systems.\n\nGlue also oﬀers on-demand clusters via interactive sessions, which\n\ncan be used for interactive development and analysis via Jupyter\n\nnotebooks (either provided by Glue or your own). Finally, Glue\n\nStudio oﬀers a visual environment where you can create visual jobs\n\nand build ETL pipelines with little or no coding.\n\nis chapter includes the following recipes:\n\nCreating ETL jobs visually using AWS Glue Studio\n\nParameterizing jobs to make them more \u0000exible and reusable\n\nHandling job failures and reruns for partial results\n\nProcessing data incrementally using bookmarks and bounded execution\n\nHandling a high quantity of small \u0000les in your job\n\nReusing libraries in your Glue job\n\nUsing data lake formats to store your data\n\nOptimizing your catalog data retrieval using pushdown \u0000lters and\n\nindexes\n\nRunning pandas code using AWS Glue for Ray\n\nTechnical requirements\n\nMany recipes in this chapter require you to have a bash shell or\n\nequivalent available with the AWS CLI installed with access to AWS.\n\nCheck the AWS documentation installation guide:\n\nhttps://docs.aws.amazon.com/cli/latest/userguide/getting-started-\n\ninstall.html.\n\nIf using Microso Windows, you can enable Windows Subsystem\n\nfor Linux (WSL) to get a bash shell. e instructions will also\n\nassume you have con\u0000gured default credentials and the default\n\nregion as the one you intend to use, using aws configure or an\n\nAWS CLI pro\u0000le.\n\ne easier way to experience the recipes is using a test account, on\n\nwhich you can use an admin user to access the console and the\n\ncommand line. Otherwise, you’ll have to add the required\n\npermissions to create and use Glue components.\n\nA Glue job requires a Simple Storage Service (S3) bucket to store\n\nscripts, temporary \u0000les, and, potentially, data; you will create a\n\nbucket for this purpose. None of the recipes use or generate large\n\namounts of data that would incur high usage costs in the short\n\nterm; it is up to you to clean up the \u0000les or even delete the bucket\n\nonce you no longer want to keep the example data.\n\nDe\u0000ne some variables in the shell, making sure the region\n\ncon\u0000gured on the AWS CLI is the region you intend to use later on\n\nthe console:\n\nACCOUNT_ID=\"$(aws sts get-caller-identity --query \\ 'Account' --output text)\" AWS_REGION=\"$(aws configure get region)\" GLUE_BUCKET =\"glue-recipes-$ACCOUNT_ID\" GLUE_ROLE=AWSGlueServiceRole-Recipe GLUE_ROLE_ARN=arn:aws:iam::${ACCOUNT_ID}:role/$GLU E_ROLE\n\ne variables will be lost once you close the shell, but you can rerun\n\nthe previous code de\u0000nition block if a recipe requires one or more\n\nof them (this will be speci\u0000ed in the Getting ready section of each\n\nrecipe that requires it).\n\nCreate a bucket in the region using the default con\u0000guration:\n\naws s3api create-bucket --create-bucket- configuration \\ LocationConstraint=$AWS_REGION --bucket $GLUE_BUCKET\n\ne default permissions block public access but allow\n\nreading/writing to roles in the same account, which have\n\npermission to use S3.\n\nAer each recipe, you can wipe the bucket objects if you no longer\n\nneed them.\n\nIn addition, all Glue jobs require a role that Glue can assume at\n\nruntime. By following the convention of naming it starting with\n\nAWSGlueServiceRole, it doesn’t require an explicit\n\niam:passRole permission. Create it using the following command\n\nlines:\n\naws iam create-role --role-name $GLUE_ROLE -- assume-\\ role-policy-document '{\"Version\": \"2012-10- 17\",\"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"Service\": \"glue.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}]}' aws iam attach-role-policy --policy-arn \\ arn:aws:iam::aws:policy/service- role/AWSGlueServiceRole \\ --role-name $GLUE_ROLE\n\naws iam put-role-policy --role-name $GLUE_ROLE \\ --policy-name S3Access --policy-document '{\"Version\": \"2012-10-17\", \"Statement\": [{\"Effect\": \"Allow\", \"Action\": [\"s3:*\"], \"Resource\": [\"arn:aws:s3:::'$GLUE_BUCKET'\", \"arn:aws:s3:::'$GLUE_BUCKET'/*\", \"arn:aws:s3:::aws-glue-*\",\"arn:aws:s3:::aws-glue- */*\"]}]}'\n\ne wildcard in the bucket name allows it to accommodate a\n\nvariable bucket name.\n\nYou can \u0000nd the script and code \u0000les on GitHub:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter03.\n\nCreating ETL jobs visually using AWS Glue Studio\n\nTypical ETL tasks consist of moving data from one data storage to\n\nanother, with some simple transformations in the process. For such\n\ncases, building a job using a visual data diagram allows users\n\nwithout coding skills to develop such a pipeline, using their\n\nknowledge of the business and the data. ese kinds of jobs are also\n\neasier to maintain and update, thus reducing the total cost of\n\nownership (TCO).\n\nOne of the multiple types of jobs that AWS Glue Studio allows for\n\ncreating is a visual job. is allows the user to de\u0000ne the pipeline as\n\na graph of nodes, and then the code is generated automatically so\n\nthat it runs like a regular script job.\n\nGetting ready\n\nCreate a bucket and a role as indicated in the Technical requirements\n\nsection.\n\nTo follow this recipe, you require a CSV \u0000le with headers and some\n\ndata, which can be your own data or the sales_sample.csv \u0000le\n\nprovided on the code repository:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/blob/main/Chapter03/Recipe1/sales_sample.csv.\n\nUpload the sample CSV \u0000le to the bucket created and copy the S3\n\nuploaded \u0000le URL; you will need it in step 2 of the How to do it…\n\nsection of this recipe. Here’s an example:\n\naws s3 cp Recipe1/sales_sample.csv s3://$GLUE_BUCKET/\n\nHow to do it…\n\n1. In the AWS console, navigate to Glue, select Visual ETL in the le menu,\n\nand create a job using the button with the same name. It will open Glue\n\nStudio where you can start building your visual job. As you add\n\ncomponents, the one selected automatically becomes the parent of the\n\none added. You can also set the parent manually by selecting it and\n\nchoosing a parent in the properties.\n\n2. Add an S3 source node. en, on the right-hand side, navigate to Data\n\nsource properties - S3. In the S3 URL \u0000eld, enter the path to the input\n\nCSV \u0000le you loaded in the Getting ready section (Note: Normally, you\n\nwould enter an input directory so that it picks up many \u0000les recursively.):\n\nFigure 3.1 – New visual job prompting to add the source node\n\n3. Select CSV for Data format and then choose Infer schema.\n\n4. Change the tab on the bottom panel to Output schema. e CSV\n\nheaders have been used to de\u0000ne the \u0000elds; however, all of them are of\n\ntype String.\n\n5. Using the top le + button, add a Change Schema transform as a child of\n\nthe S3 source. On the Transform tab, change the type of some of the\n\ncolumns as in the example (the conversion has to be valid; otherwise, the\n\ndata will be lost):\n\nFigure 3.2 – Change Schema sample conﬁguration\n\n6. Add an S3 target node (in the Add node menu, select the Target tab) as a\n\nchild of the Change Schema node. Select parquet for Format and enter a\n\ntarget S3 location under the bucket created for recipes; for instance,\n\ns3:// glue-recipes-<account id>/visual_recipe/table/.\n\n7. Select the Create a table option in Data Catalog, and on subsequent runs,\n\nupdate the schema and add new partitions, choose a database, and enter\n\na table name of your choice as shown:\n\nFigure 3.3 – S3 target node properties\n\n8. Edit the job name at the top, and in the Job details tab, select\n\nAWSGlueServiceRole-Recipe as the IAM Role value. Save and\n\nrun the job; you can monitor it on the Runs tab. It should only take\n\nabout a minute for such a small \u0000le.\n\n9. Navigate to Amazon Athena in the console and select the database you\n\nstored the table in. e table should be listed and using the action menu\n\nfor the table. Select Preview table, which will create a SQL SELECT\n\nstatement you can run to visualize the table content. If you haven’t\n\npreviously used Athena on this account and region, it will ask you to\n\nselect a temporary path S3 to store the query results.\n\nHow it works…\n\nAs you de\u0000ne your nodes and their con\u0000guration, the editor builds\n\nthe job pipeline with the components, from which it will generate\n\nthe code that the job uses at runtime. You can view the generated\n\ncode in the Script tab:\n\nFigure 3.4 – Script tab with the autogenerated code\n\nIf you export the job de\u0000nition (for instance, using the AWS CLI),\n\nyou will notice that the JSON produced is like a script Glue job, but\n\nit has a property that stores the diagram and the node properties:\n\ncodeGenConfigurationNodes. When it gets saved, it generates a\n\nPython script on the speci\u0000ed destination. Visual jobs only generate\n\nPySpark code, not Scala. If you edit the code in the Script tab, you\n\nget a warning that the job will become a script job, which means\n\nyou can no longer edit it visually.\n\nThere’s more…\n\nCheck the AWS Glue documentation for a list of visual\n\ncomponents, available at\n\nhttps://docs.aws.amazon.com/glue/latest/ug/edit-jobs-\n\ntransforms.html.\n\nData Preview is a key element of data pipeline development; it\n\nshows you how the sample data looks aer each node of the\n\npipeline has processed it. at way, you can gain con\u0000dence and\n\nmake sure you are handling it as intended. Bear in mind that\n\nbehind the scenes it uses a tiny interactive session Glue cluster,\n\nwhich has a cost. You can stop the preview and then disable the\n\nautomatic start if you are not going to use it (for instance, if you\n\ndon’t have data yet) to save on cost.\n\nIf you need to troubleshoot the job and Data Preview is not enough,\n\nyou can always copy the script code and use it in a separate script\n\njob to troubleshoot, comment out some parts, print partial results\n\nand schemas, and so on. at way, you can make code changes that\n\nhelp you troubleshoot without losing the visual job if you convert it.\n\nSee also\n\nYou can create your own visual components for Visual Studio. See how\n\nat https://docs.aws.amazon.com/glue/latest/ug/custom-visual-\n\ntransform.html.\n\ne catalog source node can use parameters. See more about job\n\nparameters in general in the Parameterizing jobs to make them more\n\n\u0000exible and reusable recipe.\n\nVisual jobs can also bene\u0000t from Glue bookmarks; see how they work in\n\nthe Processing data incrementally using bookmarks and bounded\n\nexecution recipe.\n\nParameterizing jobs to make them more flexible and reusable\n\nA job without any parameters normally does the same task in each\n\nrun, with speci\u0000c data sources and destinations. Using parameters,\n\nyou can reuse the same job on diﬀerent data sources or destinations,\n\nboth to run recurring jobs on new data or to reuse the same logic\n\nfor diﬀerent purposes, such as data transformation or cleaning.\n\nFor instance, the same type of data comes from various sources but\n\nneeds the same processing in a centralized data store.\n\nGlue allows you to de\u0000ne your parameters for your own purposes,\n\nwhich you then can use in your script. You can set default values on\n\nthe job and then override them as needed for each run when\n\nstarting a job run manually using the console, the AWS CLI, or an\n\nAPI such as boto3 or the Java SDK.\n\nGetting ready\n\nFor this recipe, you need to follow the instructions in the Technical\n\nrequirements section at the beginning of the chapter to create a role\n\nfor Glue.\n\nHow to do it…\n\n1. Create a new job in the Glue console by selecting ETL jobs in the le\n\nmenu and then, using the Script Editor button, give it a name and select\n\nthe AWSGlueServiceRole-Recipe role in the Job details tab.\n\nLeave Spark as the engine and Python as the language (see the ere’s\n\nmore… section later to learn about how it could be done in Scala).\n\n2. First, we will de\u0000ne the parameter and assign it a default value. In the\n\nJob detail tab, scroll down and open the Advanced properties section,\n\nadd a new parameter, and enter --DATE as the key and TODAY as the\n\nvalue. is is a special placeholder value; otherwise, we expect on that\n\nparameter a date string in the format yyyy-mm-dd, such as 2000-\n\n01-01. e key name is arbitrary but needs to start with a double\n\nhyphen:\n\nFigure 3.5 – Job parameters job conﬁguration\n\n3. Replace the sample code provided with the following code block:\n\nimport sys from datetime import datetime from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job args = getResolvedOptions(sys.argv, ['JOB_NAME', 'DATE']) data_date = args[\"DATE\"] date_format = '%Y-%m-%d' if data_date == 'TODAY': data_date = datetime.today().strftime(date_format) print(f\"Running job for date: '{data_date}'\")\n\n4. Run the job using the Run button and then switch to the Runs tab to see\n\nyour run progress. Select the job run and wait for it to \u0000nish. In the job\n\nrun details, use the Output logs link to view the output logs on Amazon\n\nCloudWatch. ere should be a message indicating that the job is using\n\nthe current date.\n\n5. Now, instead of using the Run button, select in the Job actions\n\ndropdown the Run with parameters option. Open the job parameters,\n\nenter a date in the format we have de\u0000ned for the parameter (yyyy-\n\nmm-dd), and run the job:",
      "page_number": 168
    },
    {
      "number": 10,
      "title": "Segment 10 (pages 188-208)",
      "start_page": 188,
      "end_page": 208,
      "detection_method": "synthetic",
      "content": "Figure 3.6 – Overriding the parameter for this run\n\nNotice the value you pass when running this way is used just for\n\nthis run and does not alter the default parameter value (TODAY) on\n\nthe job.\n\nIn the output log for this run, you should now see a message\n\nreferencing the date entered. If the parameter does not follow the\n\nformat, the job run will fail, and you will get a corresponding error.\n\nHow it works…\n\nIn the script you pasted in step 3 when creating the job, it uses the\n\nGlue getResolvedOptions API to extract the speci\u0000ed\n\nparameters from the command-line arguments Glue used to run\n\nthe script. You could use another library or your own code to parse\n\nthe parameters.\n\ne script then checks for the value of the DATE argument. For the\n\nspecial value of TODAY, it replaces this keyword with the current\n\nsystem date.\n\nIn step 5, you triggered the run overriding the DATE argument with\n\na speci\u0000c date instead of the default value. is is useful if you need\n\nto rerun a speci\u0000c date or you need to backload if the job hasn’t\n\nbeen running in a timely manner each day.\n\nOnce the parameter value is stored in a variable in the script, you\n\ncan use it as needed. For instance, you could use it to build a\n\npushdown predicate when reading a table or an S3 path or the name\n\nof the table to write into.\n\nThere’s more…\n\nIf Scala is the language you use to script your Glue jobs, then the\n\ncode to extract the parameter is slightly diﬀerent in syntax but\n\nworks the same way. e main method receives a string array\n\n(named sysArgs in this example), which is then parsed by\n\ngetResolvedOptions:\n\nimport com.amazonaws.services.glue.util.GlueArgParser val args = GlueArgParser.getResolvedOptions(sysArgs, Seq(\"JOB_NAME\",\" DATE \").toArray) val region = args(\"DATE\")\n\nYou can trigger multiple instances of a job, each using diﬀerent\n\nparameters. In the job run history, you can review the speci\u0000c\n\nparameters that were used for each run.\n\nIf you need to trigger a job with multiple parameters, you might\n\nwant to trigger them in parallel since they are doing diﬀerent\n\nthings, such as handling diﬀerent data. e total run cost would be\n\nthe same, but you can get them all completed faster that way. To\n\nallow that parallelism, make sure you have enough concurrent runs\n\nallowed on the Job details con\u0000guration:\n\nFigure 3.7 – Concurrency conﬁg in the Job details tab\n\nSee also\n\nIf you invoke a job by other mechanisms such as boto3 (for instance,\n\nfrom AWS Lambda) or the AWS CLI, you also have the option to use\n\nspeci\u0000c parameters that override the default for that run. Refer to the\n\nAPI documentation:\n\nhttps://docs.aws.amazon.com/cli/latest/reference/glue/start-\n\njob-run.html\n\nhttps://boto3.amazonaws.com/v1/documentation/api/latest/refe\n\nrence/services/glue/client/start_job_run.html\n\nHandling job failures and reruns for partial results\n\nWhen building ETL pipelines (with Glue or in general), it’s\n\nimportant to consider diﬀerent scenarios that could make the job\n\nfail and how to deal with it. Ideally, we want the recovery to be\n\nautomatic, at least for transient issues, but regardless of the recovery\n\nmethod, the most important aspect is that the jobs don’t result in\n\npermanent data loss or duplication due to the issue. In traditional\n\ndatabases, this is solved using transactions, but in the case of big\n\ndata ETL, that is rarely an option or would cause too much\n\noverhead.\n\nIn this recipe, you will see how to deal with job failures and\n\nresulting partial results.\n\nGetting ready\n\nis recipe requires a bash shell with the AWS CLI installed and\n\ncon\u0000gured and the GLUE_ROLE_ARN and GLUE_BUCKET\n\nenvironment variables set, as indicated in the Technical\n\nrequirements section at the beginning of the chapter.\n\nHow to do it…\n\n1. Create a job script as a local \u0000le running the following multiline bash\n\ncommand. It will execute when you enter the line with just EOF:\n\ncat <<EOF > RetryRecipeJob.py from pyspark.context import SparkContext from pyspark.sql.functions import * from awsglue.context import GlueContext from awsglue.dynamicframe import DynamicFrame glueContext = GlueContext(SparkContext()) spark = glueContext.spark_session s3_output_path =\n\n\"s3://${GLUE_BUCKET}/retry_recipe\" # The first attempt is just an hexadecimal number # the retries have a suffix with the retry number is_retry = \"_attempt_\" in \\ spark.conf.get(\"spark.glue.JOB_RUN_ID\") df = spark.range(1 << 10, numPartitions=4) # Simulate the retry works ok if is_retry: df = df.withColumn(\"fail\", lit(False)) else: df = df.withColumn(\"fail\", expr( f\"case when id=10 then true else false end\")) # Introduce a failure when flagged fail_udf = udf(lambda fail: 1/0 if fail else fail) df = df.withColumn(\"fail\", fail_udf(df.fail)) failDf = DynamicFrame.fromDF(df, glueContext, \"\") glueContext.write_dynamic_frame.from_options( frame=failDf, connection_type='s3', format='csv', connection_options={\"path\": s3_output_path} ) EOF\n\n2. Upload the script to the S3 bucket and delete the local copy:\n\naws s3 cp RetryRecipeJob.py s3://$GLUE_BUCKET rm RetryRecipeJob.py\n\n3. Create a job, making sure to use \\ only at the lines indicated:\n\naws glue create-job --name RetryRecipe --role \\ $GLUE_ROLE_ARN --number-of-workers 2 -- worker-type \\ \"G.1X\" --glue-version 4.0 --command '{\"Name\": \"glueetl\", \"ScriptLocation\": \"s3://'$GLUE_BUCKET'/RetryRecipeJob.py\"}' \\ --max-retries 1 --default-arguments \\ '{\"--job-language\":\"python\", \"--TempDir\": \"s3://'$GLUE_BUCKET'/tmp/\"}'\n\n4. Run the job:\n\naws glue start-job-run --job-name RetryRecipe\n\n5. Open the AWS console, navigate to Glue, and select ETL Jobs on the le\n\nmenu. On the table of jobs, it should list the RetryRecipe job (if not,\n\nmake sure you are in the same region where you are using the command\n\nline). Select the job name to view the details and then select the Runs\n\ntab. Refresh until you see two attempts complete, the \u0000rst one failing and\n\nthe second succeeding:\n\nFigure 3.8 – Runs tab showing both attempts\n\n6. Check the output folder by running the following in the command line:\n\naws s3 ls s3://${GLUE_BUCKET}/retry_recipe/\n\n7. Examine the \u0000les listed. Notice there are two sets of four \u0000les; one of\n\nthem has one \u0000le that is much smaller. is is the set from the \u0000rst run\n\nthat produced incomplete results due to an error. e retry worked but\n\ndidn’t clean the previous run’s partial results. Let’s solve that.\n\n8. Back in the console, open the Script tab for the job. Near the end of the\n\nscript, locate the\n\nglueContext.write_dynamic_frame.from_options line.\n\nJust before it, add a new line with the following content:\n\nglueContext.purge_s3_path(s3_output_path, {\"retentionPeriod\": 0})\n\n9. Save the script change and run the job. Open the Runs tab again and\n\nrefresh to see the new job run, again with an error and a retry.\n\n10. Once the retry has succeeded, in the command line, again list the \u0000les as\n\ndone in step 6. Now, there is only one step of four \u0000les with similar sizes.\n\n11. Clean the \u0000les and the job if no longer needed:\n\naws glue delete-job --job-name RetryRecipe aws s3 rm --recursive \\ s3://${GLUE_BUCKET}/retry_recipe/\n\nHow it works…\n\nIn this recipe, you created a Glue job with one retry con\u0000gured,\n\nwhich means that if the job fails (regardless of the reason), then it\n\nwill immediately retry the same run, using the same con\u0000guration\n\nand parameters. In most cases, the job fails because an uncaught\n\nexception was raised from the script.\n\ne script creates a dataset of 1,024 rows divided into four\n\npartitions (Spark memory partitions, not to be confused with table\n\npartitions). en, it checks the job ID to detect whether this is the\n\n\u0000rst run or a retry. e retry uses the name job ID but appends the\n\nnumber of retry attempts starting with 1. If it’s the \u0000rst run, it marks\n\nthe row with the ID 10 so that the user-de\u0000ned function (UDF) that\n\nis applied later simulates a runtime error.\n\nis arti\u0000cial failure simulates a transient error that can succeed on\n\nretry; for instance, a call to an external system or some missing\n\nexternal data that was delayed and the job cannot be completed\n\nwithout.\n\ne exception the UDF caused is propagated, and aer Spark\n\nexhausts the task retries, it causes the script action to raise an\n\nexception and the job fails. Unfortunately, the failed job has le\n\nincomplete results; most tasks succeeded, and the one that failed\n\nproduced a partial output \u0000le (up to row 10, which the UDF failed).\n\nen, the job run retry succeeded, but because of the \u0000le naming\n\nbased on timestamp, it didn’t override the previous attempt’s \u0000les,\n\nresulting in most of the data being duplicated.\n\nIn step 8, you added a line that purges the output path, to make sure\n\nprevious attempts are cleaned before writing the data. is is useful\n\nnot only on retries but also on reruns. For instance, imagine the\n\ncode has a mistake doing calculations and has to be run again to\n\ncorrect the resulting data.\n\nThere’s more…\n\nIn a Glue job, there are multiple levels of retries. First, Spark will\n\nretry a failed task three times by default. It will also retry stages if\n\nthe cause is missing partial data; once Spark gives up and fails the\n\nSpark job because the exception is not captured and handled in\n\nyour script, then Glue will check whether retries are con\u0000gured on\n\nthe job.\n\ne issue experienced with the partial writes occurs because Glue\n\nby default uses a direct committer, which means the \u0000les are written\n\ndirectly into the destination folder. is is not the default on\n\nApache Spark in general, which historically assumes it runs on\n\nHadoop Distributed File System (HDFS), where it can write onto a\n\ntemporary folder and then do atomic renames once all \u0000les are\n\ngenerated correctly; this is not possible on S3, where renaming a \u0000le\n\nmeans copying it and thus has a high cost. Glue writes \u0000les directly\n\non the destination path, with the drawback that it can leave partial\n\nresults.\n\nIf using a DataFrame to write \u0000les, instead of calling purge, you\n\ncan just call: .mode(\"overwrite\"); in fact, by default, it will\n\nrefuse to write if it detects the output folder exists to prevent this\n\nsituation. If you want it to behave like the DynamicFrame write, you\n\ncan specify the \"append\" mode.\n\nWhen writing partitioned data, things get more complicated since\n\nyou just want to overwrite the partitions the job is writing and not\n\nthe ones created by previous runs (or other jobs). Fortunately, Spark\n\nprovides an option to do exactly that:\n\nspark.conf.set(\"spark.sql.sources.partitionOverwr iteMode\", \"dynamic\")\n\nSee also\n\nIn this example, you have seen how to do simple automatic retries. For\n\nmore sophisticated retries such as exponential backoﬀ, you can invoke\n\nthe Glue job as part of an AWS Step Functions or an Amazon Managed\n\nWork\u0000ows for Apache Air\u0000ow (MWAA) work\u0000ow:\n\nhttps://docs.aws.amazon.com/mwaa/latest/userguide/what-is-\n\nmwaa.html.\n\ne open source s3a implementation provides a more sophisticated\n\nway of writing \u0000les that provides eﬃcient writing without partial writes;\n\nit’s called the Magic Committer. Previously, it had the drawback that it\n\nrequired an external system for consistency, but now that S3 is\n\nconsistent, that dependency is no longer needed:\n\nhttps://hadoop.apache.org/docs/r3.1.1/hadoop-aws/tools/hadoop-\n\naws/committers.html#e_Magic_Committer.\n\nProcessing data incrementally using bookmarks and bounded execution\n\nData pipelines oen need to process data as it gets continuously\n\ngenerated and the ETL pipelines have to run on a regular basis. For\n\nsuch cases where the use (and extra cost) of streaming is not\n\njusti\u0000ed (for instance, if the data is uploaded once a day), using\n\nbookmarks is a simple way of keeping track of which \u0000les are\n\nalready processed and which are new since the last run. With\n\nbookmarks, you can run a scheduled job on a regular basis and\n\nprocess only new data added since the last run.\n\nIn addition, Glue provides an optional feature called bounded\n\nexecution; with it, a limited amount of data (size or \u0000les) is\n\nhandled in each bookmarked run. is allows the job to run in a\n\ntimely fashion and predictably with a volume of data that has been\n\ntested and not run into issues with memory, disk, or latency. is\n\ncan be useful if you are backloading a large amount of data or new\n\ndata arrives in bursts.\n\nGetting ready\n\nis recipe requires a bash shell with the AWS CLI installed and\n\ncon\u0000gured and the GLUE_ROLE_ARN and GLUE_BUCKET\n\nenvironment variables set, as indicated in the Technical\n\nrequirements section at the beginning of the chapter.\n\nIn addition, run the following code to set up two environment\n\nvariables, pointing each to a S3 URL for the job source and\n\ndestination, on the bucket for recipes:\n\nRECIPE_S3_SRC=s3://$GLUE_BUCKET/bookmarkrecipe/in put/ RECIPE_S3_DST=s3://$GLUE_BUCKET/bookmarkrecipe/ou tput/\n\nFor this recipe, you’ll generate synthetic data. To do so, run the\n\nfollowing bash commands in the shell, from a directory where you\n\ncan write. e script will create 10 tiny JSON \u0000les locally; update\n\nthem to the path speci\u0000ed by the RECIPE_S3_SRC variable you just\n\nset, and \u0000nally delete the local \u0000les:\n\nmkdir ./bookmark_recipe_data/ for i in 1 2 3 4 5 6 7 8 9 10;do echo '{\"file_number\": '$i'}' > \\ ./bookmark_recipe_data/$i.json done aws s3 sync ./bookmark_recipe_data/ $RECIPE_S3_SRC rm ./bookmark_recipe_data/*.json rmdir ./bookmark_recipe_data/\n\nHow to do it…\n\n1. Create a job script as a local \u0000le running the following multiline shell\n\ncommand. It will execute when you enter the line with just EOF:\n\ncat <<EOF > BookmarksRecipeJob.py import sys from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job args = getResolvedOptions(sys.argv, ['JOB_NAME']) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args['JOB_NAME'], args) dynf = glueContext.create_dynamic_frame_from_options( connection_type = \"s3\", connection_options = { \"paths\": [\"$RECIPE_S3_SRC\"], \"boundedFiles\": 5 }, transformation_ctx=\"json_source\", format = \"json\" ) glueContext.write_dynamic_frame.from_options( frame=dynf.repartition(1), connection_type='s3', format='json', transformation_ctx=\"csv_dst\", connection_options={\"path\": \"$RECIPE_S3_DST\" } )\n\njob.commit() EOF\n\n2. Upload the script to the S3 bucket and delete the local copy:\n\naws s3 cp BookmarksRecipeJob.py s3://$GLUE_BUCKET rm BookmarksRecipeJob.py\n\n3. Create a job with the following command:\n\naws glue create-job --name BookmarksRecipe -- role \\ $GLUE_ROLE_ARN --glue-version 4.0 --command\\ '{\"Name\": \"glueetl\", \"ScriptLocation\":\n\n\"s3://'$GLUE_BUCKET'/BookmarksRecipeJob.py\"}'\\ --default-arguments '{\"--job- language\":\"python\", \"--TempDir\": \"s3://'$GLUE_BUCKET'/tmp/\", \"--job-bookmark-option\": \"job-bookmark- enable\"}'\n\n4. Run the job:\n\nJOB_RUN_ID=$(aws glue start-job-run --job-name \\ BookmarksRecipe --output text)\n\n5. Get the run details; in the response, check the JobRunState job run\n\nto check the progress until it completes in a couple of minutes. If the\n\n\u0000nal state is ERROR or FAILED, it will give you an error message with\n\nthe cause:\n\naws glue get-job-run --job-name BookmarksRecipe \\ --run-id $JOB_RUN_ID\n\n6. Retrieve the output \u0000les to a local directory and visualize the content. It\n\nshould have 5 rows taken from 5 of the 10 \u0000les in the source:\n\naws s3 sync $RECIPE_S3_DST bookmarks_recipe_result cut -b 1- bookmarks_recipe_result/* | sort\n\n7. Repeat steps 4, 5, and 6. If you try to start the job too soon aer the\n\nprevious run is \u0000nished, you might get a concurrency error. Wait for a\n\nfew seconds and retry. Now, step 6 prints the lines from the 10 \u0000les; this\n\nshows each input \u0000le was processed once, either on the \u0000rst or the\n\nsecond run.\n\n8. Wipe the local directory and delete the job if no longer needed:\n\nrm bookmarks_recipe_result/* rmdir bookmarks_recipe_result aws glue delete-job --job-name BookmarksRecipe\n\nHow it works…\n\ne \u0000rst step created the code to use Glue S3 bookmarks.\n\nBookmarks need three things:\n\ne init() method called on the job object.\n\ne read operation to specify a transformation_ctx parameter.\n\nEach source must have a diﬀerent one to track the bookmarks separately.\n\nCalling commit() on the job when the data is processed successfully.\n\nUntil that method is called, the bookmarks are not updated; there are no\n\npartial bookmark updates if some of the \u0000les are processed correctly but\n\nsome fail.\n\ne code uses repartition(1) so that the output is generated\n\nonto a single output \u0000le for convenience. On a real job with large\n\ndata, such a low number could cause a bottleneck.\n\nen, the script is uploaded and used on a job where bookmarks\n\nare enabled; note the job argument con\u0000gured on creation: --job-\n\nbookmark-option.\n\nWhen you run the job again, it will pick up the remaining \u0000les not\n\nalready bookmarked. So, aer two runs, there are two output \u0000les,\n\neach with the rows of \u0000ve input \u0000les (in this example, each source\n\n\u0000le has just one row, for easier traceability).\n\nYou can run the job once again and see if it produces an empty \u0000le\n\nbecause there are no new \u0000les. You could avoid generating empty\n\n\u0000les by checking in the code for dynf.count() > 0, before\n\nwriting the output.\n\nIn this example, the script used bounding based on \u0000les, but you\n\ncan do bounding based on data size. is is a better option if the\n\ndata \u0000les have very diﬀerent sizes.\n\nWhen using bookmarks, don’t change the default job currency of 1.\n\nIn a job using bookmarks, if there are multiple concurrent runs, at\n\ncommit time only one can succeed, and the others will fail because\n\nthey will detect the concurrent bookmark update.\n\nThere’s more…\n\nAs listed before, a transformation_ctx parameter on the source\n\nis a requirement for bookmarks to work. You can use this to your\n\nadvantage and combine the same job sources that need\n\nbookmarking and sources that do not. For instance, if you process\n\norders and need to join with customers, you want to bookmark the\n\norders but not the customers, so each run can join with all existing\n\ncustomers and not just new ones. You can also use bookmarks on\n\ncatalog tables, which will keep track of all \u0000les on all partitions.\n\ne --job-bookmark-option argument has a third option other\n\nthan enable or disable. You can pause the bookmark, which\n\nmeans the job will use the bookmark to read but won’t update it.\n\nYou can use this option for testing/troubleshooting.\n\nIt is possible to reset the bookmarks of a job or rewind to the state\n\nin which a previous job run le it. is is useful if you need to\n\nreprocess data due to some issue.\n\nGlue also has bookmarks for Java Database Connectivity (JDBC)\n\nsources. For that kind of bookmark, it needs one or more\n\nnumerically monotonic (always increasing or decreasing on\n\nupdates) columns to be speci\u0000ed so that Glue can remember the\n\nlast number processed and in the next run take the records with a\n\nlarger number (or smaller, if con\u0000gured that way).\n\nFor instance, if you have a sequence column, you can use that to\n\nbookmark records already processed versus new ones created, but it\n\nwon’t detect updates unless the bookmark column is updated as\n\nwell, using a timestamp column as bookmark.\n\nYou can read more about Glue bookmarks at\n\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-\n\ncontinuations.html.\n\nSee also\n\nYou can also use bookmarks in a visual job. See how to create one in the\n\nCreating ETL jobs visually using AWS Glue Studio recipe.\n\nBookmarks control the input data being processed at least once, but you\n\nalso must make sure the job output is tolerant to errors and doesn’t\n\ngenerate duplicates. Learn more in the Handling job failures and reruns\n\nfor partial results recipe.\n\nHandling a high quantity of small files in your job\n\nFrequently, when ingesting data, the source data is not optimized\n\nand it comes in tiny \u0000les, maybe because it was produced at short\n\nintervals or by many sources such as diﬀerent sensors sending their\n\nindividual reports. Apache Spark was designed as a big data tool\n\nand it struggles when handling such cases, causing ineﬃciency\n\nwhen processing too many partitions and also causing memory\n\nissues on the driver when building a plan.\n\nTo handle data eﬃciently, we want to consolidate small \u0000les to make\n\nthe reading more eﬃcient, especially if using a columnar format\n\nsuch as Parquet; as a rule of thumb, at least 100 MB bytes on each\n\n\u0000le. e simple way to control that is to repartition/coalesce the\n\ndata to the target number of output \u0000les, but that oen requires a\n\ncostly shuﬄe operation.\n\nIn this recipe, you will see a simple and eﬀective way provided by\n\nGlue to group small \u0000les at reading time.\n\nGetting ready\n\nis recipe requires a bash shell with the AWS CLI installed and\n\ncon\u0000gured. e GLUE_ROLE_ARN and GLUE_BUCKET environment\n\nvariables need to be set, as per the Technical requirements section at\n\nthe beginning of the chapter.\n\nTo demonstrate this feature, we need data with lots of small \u0000les. To\n\nprepare such an input dataset, execute the following bash\n\ncommands. Make sure you have at least 50 MB of disk space free in\n\nthe local directory. It will generate 10k tiny CSV gzipped \u0000les. It will\n\ntake a few minutes, printing a dot each time it has completed 10k\n\n\u0000les:\n\necho placeholder > template_file.csv gzip template_file.csv mkdir smallfiles_recipe_input # Generate the files locally for i in {1..10000}; do cp template_file.csv.gz smallfiles_recipe_input/$i.csv.gz; if (($i % 1000 == 0)); then echo -n .; fi done echo # Upload to s3 and cleanup the local files rm template_file.csv.gz S3_INPUT_URL=s3://$GLUE_BUCKET/smallfiles_input/ aws s3 sync smallfiles_recipe_input $S3_INPUT_URL rm smallfiles_recipe_input/*.csv.gz rmdir smallfiles_recipe_input S3_OUTPUT_URL=s3://$GLUE_BUCKET/smallfiles_output /\n\nHow to do it…\n\n1. Create a job script as a local \u0000le by running the following bash\n\ncommand:\n\ncat <<EOF > GroupingFilesRecipeJob.py from pyspark.context import SparkContext from awsglue.context import GlueContext glueContext = GlueContext(SparkContext()) dynf = glueContext.create_dynamic_frame.from_options( connection_type=\"s3\", connection_options={ \"paths\": [\"$S3_INPUT_URL\"], \"useS3ListImplementation\": True, \"groupFiles\": \"inPartition\", \"groupSize\": \"100000\" }, format=\"csv\" ) writer=dynf.toDF().write.format(\"parquet\") writer.mode(\"overwrite\").save(\"$S3_OUTPUT_URL\" ) EOF\n\n2. Upload the job Python script to S3:\n\naws s3 cp GroupingFilesRecipeJob.py s3://$GLUE_BUCKET rm GroupingFilesRecipeJob.py\n\n3. Create a job:\n\naws glue create-job --name GroupingFilesRecipe \\ --role $GLUE_ROLE_ARN --number-of-workers 2 \\",
      "page_number": 188
    },
    {
      "number": 11,
      "title": "Segment 11 (pages 209-228)",
      "start_page": 209,
      "end_page": 228,
      "detection_method": "synthetic",
      "content": "--glue-version 4.0 --command '{\"Name\": \"gluestreaming\", \"ScriptLocation\":\n\n\"s3://'$GLUE_BUCKET'/GroupingFilesRecipeJob.py \"}'\\ --worker-type \"G.025X\" --default-arguments \\ '{\"--job-language\":\"python\", \"--TempDir\": \"s3://'$GLUE_BUCKET'/tmp/\"}'\n\n4. Run the job:\n\nJOB_RUN_ID=$(aws glue start-job-run --job-name \\ GroupingFilesRecipe --output text)\n\n5. Check the JobRunState job run until it completes (should be less\n\nthan 10 minutes). If the job fails for some reason, it will give you an\n\nerror message:\n\naws glue get-job-run --job-name GroupingFilesRecipe \\ --run-id $JOB_RUN_ID\n\n6. List the output \u0000les; the 10K tiny \u0000les have been consolidated into just 5:\n\naws s3 ls $S3_OUTPUT_URL\n\n7. Remove the job and the S3 \u0000les, if no longer needed:\n\naws glue delete-job --job-name GroupingFilesRecipe aws s3 rm --recursive $S3_INPUT_URL aws s3 rm --recursive $S3_OUTPUT_URL\n\nHow it works…\n\ne script speci\u0000es in the source that the \u0000les should be grouped\n\ninto 100k bytes on each partition. In this case, there is only one\n\npartition, which is the input directory. Each gzipped \u0000le is 50 bytes\n\n(notice the data is so small that it takes more space compressed, but\n\nthat is what Glue sees at planning time).\n\nerefore, we have 100k / 50 = 2000 \u0000les per group, so 10k \u0000les\n\ngrouped result in 5 Spark partitions overall (not to be confused with\n\nthe S3 partitions, which the grouping parameter refers to).\n\nen, you created a Glue job that uses the script uploaded to S3.\n\nNotice something peculiar? It is de\u0000ned as Spark Streaming so that\n\nthe job can use the smallest node size G.025X (a quarter of a DPU).\n\nis kind of node is intended for streaming because it is normally\n\ntoo small for ETL jobs in terms of memory, but in this case, it is\n\nenough because this job script requires minimum memory for\n\nplanning.\n\ne outcome was \u0000ve Parquet \u0000les since the \u0000les were grouped into\n\n\u0000ve Spark partitions. Notice here the number of partitions is\n\nmaintained when converting from DynamicFrame to DataFrame.\n\nUnfortunately, the conversion caused a small delay since\n\nDynamicFrame couldn’t \u0000gure out the schema on the \u0000y and had to\n\ndo a two-pass to \u0000rst determine it for DataFrame and then the\n\nactual processing. is wouldn’t have been the case if writing\n\ndirectly from DynamicFrame.\n\nThere’s more…\n\nIf you con\u0000gure the Spark UI logs and visualize the execution, you\n\nwill see there is no shuﬄe of data. e \u0000les are assigned to diﬀerent\n\npartitions from the source.\n\nWith such small \u0000les, most of the time is not spent doing the actual\n\nreading and writing but listing the \u0000les from s3, which is not\n\nre\u0000ected in the Spark tasks. e job uses the\n\nuseS3ListImplementation option, which lists \u0000les directly\n\nusing the AWS Java SDK instead of the generic HDFS that Spark\n\nuses.\n\nis grouping feature will kick in automatically if GlueContext\n\ndetects the source has more than 50k \u0000les. If you do not want this\n\noptimization for some reason, you can disable the setting as\n\n\"groupFiles\": \"none\".\n\nNot all formats support this feature; check the documentation for\n\nfurther details: https://docs.aws.amazon.com/glue/latest/dg/aws-\n\nglue-programming-etl-format.html.\n\nSee also\n\nFor DataFrame, in Spark 3.5 or later, you can specify the desired\n\npartitions with spark.sql.files.maxPartitionNum and it will\n\ntry to group \u0000les to honor that. On older versions, there are very limited\n\ncapabilities to group \u0000les.\n\nGlue for Ray is better suited for handling small \u0000les; learn more about\n\nthis engine in the Running pandas code using AWS Glue for Ray recipe.\n\nReusing libraries in your Glue job\n\nSpark provides a rich data framework that can be extended with\n\nadditional plugins, libraries, and Python modules. As you build\n\nmore jobs, you would likely reuse your own code, whether it’s UDFs\n\nto process data when it’s not possible to do the same using the Spark\n\nfunctions or some pipeline code you want to reuse; for instance, a\n\nfunction with some transformations that you do regularly.\n\nIn this recipe, you will see how you can reuse Python code on Glue\n\nfor Spark jobs.\n\nGetting ready\n\nis recipe requires a bash shell with the AWS CLI installed and\n\ncon\u0000gured and the GLUE_ROLE_ARN and GLUE_BUCKET\n\nenvironment variables set, as indicated in the Technical\n\nrequirements section at the beginning of the chapter.\n\nHow to do it...\n\n1. e following bash commands will create a Python module and con\u0000g\n\n\u0000le:\n\nmkdir my_module cat <<EOF > my_module/__init__.py from random import randint def do_some_calculation(a): return randint(1, 10) + a def get_config_value(): with open('/tmp/my_config') as f: lines = f.readlines() return lines[0].strip() EOF zip -r my_module my_module echo \"recipe_example_value\" > my_config\n\n2. Upload both \u0000les to S3 and clean up:\n\nRECIPE_S3_PATH=s3://$GLUE_BUCKET/reuse_module aws s3 cp my_module.zip $RECIPE_S3_PATH/ rm my_module.zip rm my_module/__init__.py rmdir my_module aws s3 cp my_config $RECIPE_S3_PATH/ rm my_config\n\n3. Prepare the job script:\n\ncat <<EOF > ReuseLibrariesRecipe.py from pyspark.sql import SparkSession from pyspark.sql.functions import lit, udf from my_module import do_some_calculation,\n\nget_config_value spark = SparkSession.builder.getOrCreate() df = spark.range(1 << 4).toDF(\"id\") df = df.withColumn(\"config_val\", lit(get_config_value())) calc_udf = udf(do_some_calculation) df = df.withColumn(\"calc\", calc_udf(df[\"id\"])) df.repartition(1).write.csv(\"$RECIPE_S3_PATH/o ut\") EOF aws s3 cp ReuseLibrariesRecipe.py $RECIPE_S3_PATH/ rm ReuseLibrariesRecipe.py\n\n4. Create a Glue job; be careful to use \\ only in the lines indicated:\n\naws glue create-job --name ReuseLibraryRecipe --role\\ $GLUE_ROLE_ARN --glue-version 4.0 --command \\ '{\"Name\": \"glueetl\", \"ScriptLocation\": \"'$RECIPE_S3_PATH'/ReuseLibrariesRecipe.py\"}' \\ --number-of-workers 2 --worker-type \"G.1X\" \\ --default-arguments '{\"--job- language\":\"python\", \"--extra-py- files\":\"'$RECIPE_S3_PATH'/my_module.zip\", \"-- extra-files\": \"'$RECIPE_S3_PATH'/my_config\", \"--TempDir\": \"'$RECIPE_S3_PATH'/tmp/\"}'\n\n5. Run the job:\n\nJOB_RUN_ID=$(aws glue start-job-run --job-name \\ ReuseLibraryRecipe --output text)\n\n6. Check the JobRunState job run until it completes (should be less\n\nthan 10 minutes). If the job fails for some reason, it will give you an\n\nerror message:\n\naws glue get-job-run --job-name ReuseLibraryRecipe \\ --run-id $JOB_RUN_ID\n\n7. Show the result \u0000le content:\n\naws s3 sync $RECIPE_S3_PATH/out/ . cat part-*-c000.csv rm part-*-c000.csv\n\n8. Remove the job if no longer needed:\n\naws glue delete-job --job-name ReuseLibraryRecipe\n\nHow it works…\n\nIn the \u0000rst step, you created a basic Python module. e module\n\nhas two functions: one that simulates a calculation providing a\n\nrandom value and another that reads a local text \u0000le and provides\n\nthe context.\n\nWhen a job is created, it is con\u0000gured with the S3 path to the text\n\n\u0000le as an extra \u0000le and the module as an extra Python \u0000le. At\n\nruntime, the text \u0000le will be deployed on the /tmp directory of each\n\nnode of the cluster, and each node will download, extract the ZIP\n\n\u0000le with the module, and make it available in the Python runtime.\n\nWhen the job runs, the code creates a DataFrame with the help of\n\nthe Python functions and stores it on S3 as a CSV \u0000le, with three\n\ncolumns:\n\nA sequential number for each row\n\nA column containing the text provided by the function – this function is\n\ncalled on the driver and then used as a constant for all rows\n\ne third column is the result of calling the do_some_calculation\n\nmodule on the row ID using a UDF, and the result is a random number\n\nbetween 1 and 10 added to the row ID\n\nThere’s more...\n\nIn this recipe, the code was reused directly on the driver and\n\ndistributed in the form of UDFs. Notice this is for demonstration\n\npurposes, but using Python UDFs results in performance\n\ndegradation when running with a signi\u0000cant amount of data.\n\nInstead of a ZIP \u0000le, you can use --extra-py-files with\n\nindividual Python \u0000les, which then you can import into the code\n\nusing the \u0000lename. If you want to use wheel \u0000les (created by you or\n\nfrom a Python repository), then you instead need to use the --\n\nadditional-python-modules parameter.\n\nReusing Java/Scala code is similar, but instead, you specify the\n\nlocation of the JAR \u0000les using the --extra-jars parameter.\n\nSee also\n\nYou can reuse code in the same way on Glue Studio visual jobs when you\n\nuse the custom code node to extend the capabilities. See an example of a\n\nvisual job in the Creating ETL jobs visually using AWS Glue Studio recipe.\n\nVisual jobs also allow creating your own components to reuse code; see\n\nan example at https://aws.amazon.com/es/blogs/big-data/create-your-\n\nown-reusable-visual-transforms-for-aws-glue-studio/.\n\nUsing data lake formats to store your data\n\nHistorically, big data technologies on the Hadoop ecosystem have\n\ntaken some trade-oﬀs to scale to volumes that traditional databases\n\ncannot handle. In the case of Apache Hive, which became the\n\nstandard Hadoop SQL database, the external tables just point to\n\n\u0000les on some object storage such as HDFS or S3, and then jobs\n\naccess those \u0000les without a central system coordinating access or\n\ntransactions. is is still how the standard tables work on the Glue\n\ncatalog.\n\nAs a result, the atomicity, consistency, isolation, and durability\n\n(ACID) properties of RDBMSs were relaxed to allow for scalability\n\nin use cases where write concurrency or the lack of transactions is\n\nnot an issue, such as historical append-only tables.\n\nIn recent years, the desire has been to bring back those ACID\n\nproperties while keeping the data on a scalable object store for\n\ncheap and virtually in\u0000nite scalability, with many clients and\n\nengines using the data in a distributed way.\n\ne result is the growing popularity of so-called “data lake table\n\nformats,” which de\u0000ne tables that are no longer just plain data \u0000les\n\nbut have a structure of metadata that can handle transactions, keep\n\ntrack of changes and versions, and allow time travel.\n\ne most popular formats are Apache Iceberg, Apache Hudi, and\n\nDelta Lake.\n\nIceberg’s diﬀerentiating feature is that it allows dynamic\n\npartitioning. In this recipe, you will see an example of how to easily\n\nenable and use Iceberg in your Glue jobs.\n\nGetting ready\n\nis recipe requires a bash shell with the AWS CLI installed and\n\ncon\u0000gured. e GLUE_ROLE_ARN, ACCOUNT_ID, and\n\nGLUE_BUCKET environment variables must be set, as indicated in\n\nthe Technical requirements section.\n\nHow to do it…\n\n1. Create a job script as a local \u0000le running the following multiline bash\n\ncommand. It will execute when you enter the line with just EOF:\n\ncat <<EOF > IcebergRecipe.scala import org.apache.spark.sql.SparkSession import org.apache.spark.sql.functions. {col,lit,rand} object GlueApp { def main(sysArgs: Array[String]) { val spark = (SparkSession.builder .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.\\ extensions.IcebergSparkSessionExtensions\") .config(\"spark.sql.catalog.iceberg\",\n\n\"org.apache.iceberg.spark.SparkCatalog\")\n\n.config(\"spark.sql.catalog.iceberg.warehouse\", \"s3://$GLUE_BUCKET/iceberg\")\n\n.config(\"spark.sql.catalog.iceberg.catalog-\\ impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") .config(\"spark.sql.catalog.iceberg.io- impl\",\n\n\"org.apache.iceberg.aws.s3.S3FileIO\") .getOrCreate()) val db = \"iceberg_recipe_db\" spark.sql(\"CREATE DATABASE IF NOT EXISTS iceberg.\" + db) val df = (spark.range(1 << 10).toDF(\"id\")\n\n.withColumn(\"value1\", rand()) .withColumn(\"region\", lit(\"region1\")) ) df.writeTo(\"iceberg.\" + db + \".icetable\").\n\npartitionedBy(col(\"region\")).createOrReplace() } } EOF\n\n2. Upload the script to S3:\n\naws s3 cp IcebergRecipe.scala s3://$GLUE_BUCKET rm IcebergRecipe.scala\n\n3. Create a job:\n\naws glue create-job --name IcebergRecipe -- role \\ $GLUE_ROLE_ARN --glue-version 4.0 --worker- type \\ \"G.1X\" --number-of-workers 2 --default- arguments \\ '{\"--job-language\":\"scala\", \"--class\": \"GlueApp\", \"--datalake-formats\": \"iceberg\", \"--TempDir\": \"s3://'$GLUE_BUCKET'/tmp/\"}' --command '{\"Name\": \"glueetl\",\"ScriptLocation\": \"s3://'$GLUE_BUCKET'/IcebergRecipe.scala\"}'\n\n4. Run the job:\n\nJOB_RUN_ID=$(aws glue start-job-run --job-name\n\n\\ IcebergRecipe --output text)\n\n5. Check the job status until it succeeds:\n\naws glue get-job-run --job-name IcebergRecipe \\ --run-id $JOB_RUN_ID\n\n6. On the AWS console, navigate to Athena and run the following query to\n\nshow the table content (it might ask to select an S3 output location \u0000rst):\n\nSELECT * FROM iceberg_recipe_db.icetable LIMIT 20;\n\nFigure 3.9 – Iceberg table sample in Athena\n\n7. If you no longer need them, delete the table created on the catalog (this\n\ndoesn’t delete the data on S3), the catalog policy, and the job:\n\naws glue delete-database --name iceberg_recipe_db aws glue delete-job --job-name IcebergRecipe\n\nHow it works…\n\nIn the \u0000rst step, you de\u0000ned the script (in this case, using Scala) and\n\nthen created a job to run it. Notice the code in this recipe is pure\n\nSpark; there is no Glue API-speci\u0000c code. is shows you can bring\n\nstandard Spark code and run in Glue without changes.\n\ne job code de\u0000nes a special Spark catalog con\u0000guration, using the\n\nproperties starting with spark.sql.catalog, and then an\n\narbitrary name for the catalog. In this example named iceberg,\n\nyou can use your own name as long as when you reference the table\n\nin the code, you use the same catalog pre\u0000x so that Spark knows it\n\nneeds to use that con\u0000guration for that table.\n\nLater in the script, it inserts the sample DataFrame created into a\n\ntable named iceberg.iceberg_recipe_db.icetable. e \u0000rst\n\npart is the catalog to match the con\u0000guration name with the\n\ndatabase and table names. Iceberg will create a directory using the\n\ndatabase and table names (adding the.db suﬃx for the database).\n\nen, it will create a matching database and table in the catalog\n\nwith the S3 location.\n\nTo enable the Iceberg framework, all that was required was to add\n\nthe --datalake-formats=iceberg argument. You can enable\n\nHudi or Delta in the same way, specifying hudi or delta\n\nrespectively or multiple of them separated by a comma.\n\nEach framework has its own particularities, con\u0000guration, and way\n\nto expose the table in the Glue catalog, but the way to enable them\n\nin a Glue job is the same.\n\nThere’s more…\n\ne updates on the table are transactional, so you will always see a\n\nconsistent view. When there are multiple writers on the same table,\n\nthings get more complicated; therefore, it’s best to have a single job\n\nwriting on the table.\n\nIceberg and the other frameworks have optimistic lock detection,\n\nwhich means they can detect when another writer has made\n\nchanges concurrently and rectify it. In addition, the frameworks\n\nhave the option to con\u0000gure an external system such as DynamoDB\n\nto hold locks and do pessimistic locking so that other writers wait.\n\nis makes sense when the optimistic locking has many collisions\n\nand must constantly redo work due to con\u0000icts.\n\nBefore Glue supported these frameworks natively, the way to add\n\nthem was by subscribing to a marketplace connector. is option is\n\nnot only more laborious but adds a runtime dependency to the us-\n\neast-1 Elastic Container Registry (ECR) repository to download\n\nthe container at runtime. erefore, this option is no longer\n\nrecommended for data lake formats.\n\nYou also have the option to add the JARs yourself to the job as extra\n\nlibraries, which allows you to use the latest version of the\n\nframework.\n\nSee also\n\nWhen creating a visual job in Glue Studio, the support for data lake\n\nformats is added automatically as needed. See how to create a visual job\n\nin the Creating ETL jobs visually using AWS Glue Studio recipe.\n\nIf you want to read incrementally from one of these formats, you need to\n\ntrack the snapshots read or use a streaming source. If this incremental\n\ndata doesn’t need to be updated or queried, it might be more eﬀective for\n\ndoing incremental ingestion to use Glue bookmarks; see how in the\n\nProcessing data incrementally using bookmarks and bounded execution\n\nrecipe.\n\nOptimizing your catalog data retrieval using pushdown filters and\n\nindexes\n\ne AWS Glue Data Catalog is a key component in a big data cloud\n\narchitecture. It doesn’t hold data but acts as an Apache Hive-\n\ncompatible metastore, de\u0000ning table metadata that acts as a layer of\n\nabstraction. It shows clients how to locate and interpret the data\n\nstored in a system such as Amazon S3.\n\nIn the traditional Hive-compatible catalog tables, the catalog doesn’t\n\nkeep track of data \u0000les. It just points to a directory pre\u0000x, and then\n\nthe client will list the pre\u0000x to get a list of \u0000les currently present\n\nthere. e way these kinds of tables can scale is by using partitions,\n\neach one corresponding to a pre\u0000x on S3, to avoid listing all \u0000les for\n\nany query. A partitioned table de\u0000nes one or more columns as\n\npartition columns.\n\nFor instance, if you have a table with the year, month, and day\n\npartition columns as strings, then the data for each day will be\n\nplaced under a speci\u0000c pre\u0000x, which when following the Hive\n\nconventions contains the partition values; for instance:\n\ns3://mybucket/mytable/year=2023/month=08/day=01/.\n\nis way, a client tool that only needs data from that day just needs\n\nto read that speci\u0000c data and not all the \u0000les from the table, which\n\nmight potentially have years of data.\n\nIn this recipe, you will see how to bene\u0000t from partitions, even in\n\ncases where a table contains a massive number of them.\n\nGetting ready\n\nis recipe assumes you have created the AWSGlueServiceRole-\n\nRecipe role and an S3 bucket with your account ID of glue-\n\nrecipes-<your accountid>, as indicated in the Technical\n\nrequirements section at the beginning of this chapter.\n\nIn the bash shell, add a policy to allow the role to run notebooks:\n\nGLUE_ROLE=AWSGlueServiceRole-Recipe aws iam put-role-policy --role-name $GLUE_ROLE \\ --policy-name GlueSessions --policy-document '{\"Version\": \"2012-10-17\", \"Statement\": [{\"Effect\": \"Allow\", \"Action\":[\"glue:*Session\", \"glue:RunStatement\", \"iam:PassRole\"], \"Resource\":[\"*\"]}]}'\n\nHow to do it…\n\n1. Log in to the AWS console, navigate the Glue service, and on the le\n\nmenu select Notebooks. en, choose the Notebook button to create it.\n\n2. On the popup, leave the Spark (Python) and Start fresh default options\n\nin the AWSGlueServiceRole-Recipe IAM role and complete the\n\ncreation.\n\n3. Aer a few seconds, it will load Jupyter and open a notebook with some\n\nsample cells already \u0000lled in. e sample code might change, but there\n\nshould always be a cell to help you set up your job. is setup cell does\n\nsome con\u0000guration (the lines starting with %) and then sets up the\n\nGlueContext and SparkSession objects with the names\n\nglueContext and spark, respectively (should the name of these\n\nobjects change, you need to update the code provided accordingly). To\n\nreduce costs, you can change %number_of_workers down to 2.\n\nen, run this cell by selecting it and using the Run button on\n\nthe toolbar or by using the Shi + Enter shortcut. Below the cell,\n\nyou will see the initialization progress until it con\u0000rms the\n\nsession has been created:\n\nFigure 3.10 – Glue Studio Notebook\n\n4. Now, add a new cell to the notebook; you can use the plus (+) button on\n\nthe top le. Inside, enter the following code and replace the bucket name\n\non the \u0000rst line with your own. en, run the cell and wait until it\n\ncompletes:\n\nimport boto3 from pyspark.sql.functions import rand from awsglue.dynamicframe import DynamicFrame account_id = boto3.client('sts').get_caller_identity( ) [\"Account\"] s3_bucket = f\"glue-recipes-{account_id}\" s3_path =",
      "page_number": 209
    },
    {
      "number": 12,
      "title": "Segment 12 (pages 229-249)",
      "start_page": 229,
      "end_page": 249,
      "detection_method": "synthetic",
      "content": "f\"s3://{s3_bucket}/pushdown_recipe_table/\" database = \"default\" table_name = \"pushdown_recipe_table\" df = spark.range(1 << 8).withColumn(\"value\", rand()) sink = glueContext.getSink(connection_type=\"s3\", path=s3_path, enableUpdateCatalog=True, updateBehavior=\"UPDATE_IN_DATABASE\", partitionKeys=[\"id\"]) sink.setFormat(\"avro\") sink.setCatalogInfo(catalogDatabase=database,\n\ncatalogTableName=table_name) sink.writeFrame(DynamicFrame.fromDF(df, glueContext))\n\n5. Now, add a cell with the following code and run it. It will take about a\n\nminute to print the result with the count:\n\nglueContext.create_dynamic_frame.from_catalog( database=database, table_name=table_name).count()\n\n6. Now, add a cell with this alternative code and run it. It should just take a\n\nfew seconds to complete:\n\nglueContext.create_dynamic_frame.from_catalog( database=database, table_name=table_name, push_down_predicate=\"id in (3, 6, 9)\", additional_options={ \"catalogPartitionPredicate\":\"id < 10\"\n\n} ).count()\n\n7. Add another cell to run this code to get the same result:\n\nspark.sql(f\"SELECT * FROM {database}. {table_name} \" \"WHERE id IN (3, 6, 9)\").count()\n\n8. Extend the le menu, select Tables, and then search for\n\npushdown_recipe_table. Observe the schema with the ID as the\n\npartition column and switch to the Partitions tab. It contains 32\n\npartitions. Switch to the Indexes tab and select Add index. Give it the\n\nname main and select the id column. If steps 6 and 7 had been slow\n\ndue to the high number of partitions, adding this index would\n\nsigni\u0000cantly speed up queries on the id column:\n\nFigure 3.11 – Table indexes with the one index added\n\n9. Add a new cell and run it with this code to remove the table:\n\nboto3.client('glue').delete_table( DatabaseName=database,Name=table_name)\n\nHow it works…\n\nIn this recipe, instead of using a Glue job, you used a Glue\n\ninteractive session. is is an equivalent of a cluster, which you can\n\ncon\u0000gure using magics instead of the Job details page. e magics\n\ncan be line magics (starting with %), where the magic parameters\n\nare set on the same line (for instance, %glue_version 3.0), and\n\ncell magics (starting with %%), where the whole cell is used for the\n\nmagic (for instance, %%configure to add arguments). You can run\n\nthe %help cell to get a listing of magics and their uses.\n\nFor simplicity, it’s common to use the same role for the notebook\n\nand the session, but it’s possible to use a diﬀerent use using the\n\n%iam_role magic.\n\ne interactive session starts the moment you run any code (any\n\nline that is not a magic or a comment). Changes in the session\n\ncon\u0000guration (for example, number of workers) won’t take eﬀect\n\nuntil the kernel or the notebook is restarted.\n\nNotice that while a cell is running, it has an asterisk at the le of it.\n\nOnly one cell can run at a time; you can queue multiple cell\n\nexecutions.\n\nen, you ran a cell that created simple data with a sequential ID\n\nand a random value, which was stored as a partitioned table with\n\nthe ID as the partition column. erefore, the table has just 32\n\npartitions; a real table could have thousands of them.\n\nen, you ran multiple queries on the table. In the \u0000rst case, any\n\noperation will have to read all the \u0000les, even if you \u0000lter out the data\n\nlater because you only need a subset. e other queries leveraged\n\nthe table partition. e performance diﬀerence should be noticeable\n\neven for such a small table (note the cluster is also tiny with just two\n\nnodes, one of which is the driver).\n\nFinally, you added an index to the table. In such a small table, it will\n\nbe added instantly but won’t make a real diﬀerence at query time\n\nsince most of the time is spent checking \u0000les. In a table with tens of\n\nthousands of partitions or more, the index will make a big\n\ndiﬀerence in retrieving partitions quickly.\n\nThere’s more…\n\nIn this example, the sample DataFrame created with sample data is\n\nconverted to a DynamicFrame to create a table. It’s also possible to\n\ncreate a table directly from the DataFrame using saveAsTable(),\n\nbut using the DynamicFrame writer is more eﬃcient in adding\n\npartitions and can update the schema automatically as needed.\n\nNotice the diﬀerence in the usage of DynamicFrame and SparkSQL\n\nAPIs to read the partitioned table. In the case of SparkSQL, the\n\nengine uses the query criteria to push down the \u0000lters applicable to\n\nthe partitions and only read the related data. On the other hand, in\n\nthe case of create_dynamic_frame.from_catalog, the\n\npartition \u0000lters are explicitly indicated in two possible ways:\n\npush_down_predicate allows using rich SQL \u0000lters, which are\n\napplied on the partitions retrieved from the server. So, it avoids checking\n\n\u0000les not needed but can still spend a lot of time listing table partitions if\n\nthe table has many.\n\ncatalogPartitionPredicate applies the \u0000lter on the server\n\nside, so it scales better, but it’s more limited in the kind of \u0000lters you can\n\nbuild – basically, equals and greater/smaller than conditions, of which\n\nyou can add multiple but not make them optional (no OR clause). In\n\naddition, this predicate can use a table index like the one you created in\n\nthe recipe , if it \u0000lters on the index columns.\n\nBoth types of predicates can be combined to bene\u0000t from the\n\neﬃciency of catalogPartitionPredicate and then apply richer\n\n\u0000lters to narrow further the results by using\n\npush_down_predicate.\n\nCheck the Glue documentation for details and examples about\n\nsyntax and usage:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-\n\nprogramming-etl-partitions.html.\n\nSee also\n\nis recipe is based on traditional Hive-style tables (using directories).\n\nWhile this is a simple and eﬃcient way to partition data, it has a big\n\nlimitation: changing the partitioning means you need to rebuild the\n\ntable. Instead, you could use Apache Iceberg, which provides dynamic\n\npartitions that allow evolving the partitioning without rebuilding the\n\ntable or impacting the users. See the Using data lake formats to store your\n\ndata recipe for the recommended way to use Apache Iceberg in Glue.\n\nWhen adding partitions using the DynamicFrame writer, at the time of\n\nthis writing, it updates the main table schema to make sure it matches\n\n(whether is needed or not); this can keep increasing the number of table\n\nversions and eventually hit the account limit.\n\nere is an open-source tool provided by AWS to clean up old\n\nversions:\n\nhttps://github.com/aws-samples/aws-glue-table-versions-\n\ncleanup-utility.\n\nRunning pandas code using AWS Glue for Ray\n\ne pandas library is a highly popular Python library for data\n\nmanipulation and analysis, based on the well-established numpy\n\nlibrary, handling data in a table-like format. It is so well established\n\namong Python analysts and data scientists, that it has become a de\n\nfacto standard to the point that other libraries implement their\n\ninterfaces so that they can run existing pandas code. is is oen\n\ndone to overcome pandas’ limitations, namely being a single\n\nprocess memory-based library, which limits scalability.\n\nOne such pandas-compatible library is Modin. It can run pandas\n\ncode by just changing the imports while being able to scale by using\n\nan engine such as Dask or Ray. In this recipe, you will see how to\n\nrun pandas code on Glue for Ray using Modin.\n\nGetting ready\n\nis recipe requires a bash shell with the AWS CLI installed and\n\ncon\u0000gured. e GLUE_ROLE_ARN and GLUE_BUCKET environment\n\nvariables need to be set, as indicated in the Technical requirements\n\nsection at the beginning of the chapter.\n\nMake sure that Glue for Ray is available in the AWS region you are\n\nusing.\n\nHow to do it…\n\n1. Create a job script as a local \u0000le running the following multiline bash\n\ncommand. It will execute when you enter the line with just EOF:\n\ncat <<EOF > RayModinRecipeJob.py import ray import modin.pandas as pd import numpy as np\n\nray.init() s3_path = \"s3://$GLUE_BUCKET/ray_recipe_output\" num_samples = 10000 df = pd.DataFrame({ 'id' : range(num_samples), 'value' : np.random.randn(num_samples), }) result = df[df['value'] > 0] print(f\"Std Dev: {df['value'].std()}\") print(f\"Out of {num_samples}, {result.shape[0]} will\\ be saved after filtering\") result.to_parquet(s3_path) EOF\n\n2. Upload the script to S3:\n\naws s3 cp RayModinRecipeJob.py s3://$GLUE_BUCKET rm RayModinRecipeJob.py\n\n3. Create a job, making sure to only use \\ in the lines indicated:\n\naws glue create-job --name RayModinRecipeJob \\ --role $GLUE_ROLE_ARN --command '{\"Name\": \"glueray\", \"Runtime\": \"Ray2.4\", \"ScriptLocation\": \"s3://'$GLUE_BUCKET'/RayModinRecipeJob.py\"}'\\ --number-of-workers 1 --worker-type \"Z.2X\"\\ --glue-version 4.0 --default-arguments\\ '{\"--pip-install\": \"modin,s3fs\"}'\n\n4. Run the job:\n\nJOB_RUN_ID=$(aws glue start-job-run --job-name \\ RayModinRecipeJob --output text)\n\n5. Check the JobRunState job run until it completes (should be less\n\nthan 10 minutes). If the job fails for some reason, it will give you an\n\nerror message:\n\naws glue get-job-run --job-name RayModinRecipeJob \\ --run-id $JOB_RUN_ID\n\n6. Print the job using stdout and observe the messages printed from the\n\ncode:\n\naws s3 cp \\ s3://$GLUE_BUCKET/jobs/RayModinRecipeJob/\\ $JOB_RUN_ID/job-result/stdout /tmp cat /tmp/stdout && echo -e \"\\n\" && rm /tmp/stdout\n\n7. List the \u0000les generated by the job:\n\naws s3 ls s3://$GLUE_BUCKET/ray_recipe_output/\n\n8. Remove the job if no longer needed:\n\naws glue delete-job --job-name RayModinRecipeJob\n\nHow it works…\n\nIf you have run other Glue recipes that use script jobs, you might\n\nhave noticed the process is almost identical to Glue for Ray. First,\n\nyou created a Python script, uploaded it to S3, then created a job\n\nspecifying glueray as the command. is command requires the\n\nversion of Ray to be speci\u0000ed and uses diﬀerent instance types as\n\nother types of Glue jobs. e job speci\u0000es that the Modin and s3fs\n\nlibraries need to be installed.\n\nRunning the job and checking the status was identical to other\n\nkinds of Glue jobs.\n\nGlue stores the job logs, including stdout and stderr, under the\n\npath where the script was located. is allowed you to easily check\n\nthe result of the print statements.\n\nLogs are also available on CloudWatch, as usual.\n\ne script generated a DataFrame with 1,000 rows and a column of\n\nvalues based on a standard normal distribution, then the code\n\n\u0000ltered only the positive values and saved them as Parquet \u0000les on\n\nS3.\n\nThere’s more…\n\nWhen using Glue for Ray, the smallest instance at the time of this\n\nwriting is two DPUs. However, unlike Glue for Spark, you can use a\n\nsingle node since the Ray driver can also do work. Modin\n\nautomatically detected the Ray framework. If the job had more\n\nnodes assigned, it would have used them for distributing the data\n\nand processing.\n\nIn this example, we assumed that a requirement was to be able to\n\nreuse existing pandas code (or existing pandas skills); if you want\n\nmore control of the distributed processing, you can use the Ray\n\nframework directly with the APIs it provides.\n\nAs Ray gains popularity, other libraries are adding support for it,\n\nsuch as the popular machine learning (ML) library pytorch or the\n\nversatile AWS library awsdatawrangler, which provides\n\nintegration to AWS data services such as S3, DynamoDB, Redshi,\n\nthe Glue Catalog, or OpenSearch.\n\nSee also\n\nIf you want to avoid doing any coding at all, you can refer to the Creating\n\nETL jobs visually using AWS Glue Studio recipe. At the time of this\n\nwriting, visual jobs are only able to generate Glue for Spark code.\n\nTo reuse existing pandas code, you also have the option to use the\n\nrecently added Pandas API on Spark. Unfortunately, this API has more\n\nlimitations and discrepancies in terms of pandas compatibility. See\n\ndetails on the Apache site:\n\nhttps://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_\n\nspark/index.html.\n\nOceanofPDF.com\n\n4 A Deep Dive into AWS Orchestration Frameworks\n\nWelcome to this chapter on orchestration frameworks in the AWS\n\necosystem. In this chapter, you will gain valuable insights into\n\nsetting up and managing orchestration frameworks using AWS\n\nservices. Speci\u0000cally, we will explore four key AWS oﬀerings: AWS\n\nGlue work\u0000ows, Amazon Managed Work\u0000ows for Apache Air\u0000ow\n\n(MWAA), AWS Step Functions, and Amazon EventBridge.\n\nIn distributed systems, coordinating tasks across diﬀerent services\n\nand components can be challenging. Building complex architecture\n\nand implementing work\u0000ows with code can lead to spaghetti code\n\nthat’s hard to maintain and debug. roughout this chapter, you will\n\n\u0000nd practical examples and guidance on creating, updating, and\n\nimplementing rollback strategies based on metrics emitted by your\n\nwork\u0000ows. By the end of this chapter, you’ll be well-equipped to\n\nharness the power of these AWS orchestration tools to optimize\n\nyour work\u0000ow management and automation processes.\n\ne following recipes will be covered in this chapter:\n\nDe\u0000ning a simple work\u0000ow using AWS Glue work\u0000ows\n\nSetting up event-driven orchestration with Amazon EventBridge\n\nCreating a data work\u0000ow using AWS Step Functions\n\nManaging data pipelines with MWAA\n\nMonitoring your pipeline’s health\n\nSetting up a pipeline using AWS Glue to ingest data from a JDBC\n\ndatabase into a catalog table\n\nTechnical requirements\n\nFor the recipes in this chapter, you’ll need an active AWS account\n\nwith appropriate permissions to create and manage the following:\n\nGlue work\u0000ows\n\nGlue triggers\n\nStep Functions\n\nMWAA environment\n\nAWS roles and permissions\n\nCloudWatch\n\nYou can \u0000nd the code \u0000les for this chapter on GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter04.\n\nDefining a simple workflow using AWS Glue workflows\n\nIn this recipe, we will explore the world of AWS Glue work\u0000ows.\n\nAWS Glue work\u0000ows are powerful tool for orchestrating complex\n\nextract, transform, and load (ETL) processes on AWS. By\n\ncombining jobs, crawlers, and triggers, work\u0000ows can automate\n\ndata pipelines, ensuring data is consistently processed and delivered\n\nto its intended destinations. is makes them ideal for a variety of\n\ndata-driven applications, from data warehousing and analytics to\n\nmachine learning and real-time data processing.\n\nImagine a retail company that needs to load sales data from\n\nmultiple sources into its data lake on Amazon S3. e data comes in\n\nraw CSV format, and the goal is to transform this data into a\n\nstructured format that can be used for reporting and analysis. e\n\ndata is updated daily, and you need an automated pipeline to clean,\n\ntransform, and store this data.\n\nIn this scenario, you can leverage an AWS Glue work\u0000ow, crawler,\n\njob, and trigger to automate the process.\n\nGetting ready\n\nTo build a simple AWS Glue work\u0000ow that reads CSV data from S3\n\nand writes it out as partitioned Parquet, you need an S3 bucket with\n\nsource CSV data.\n\nIf you have an existing S3 bucket, then use it, or else create a new S3\n\nbucket. To create an S3 bucket, \u0000rst log in to the AWS Management\n\nConsole and navigate to the S3 service. Click on Create bucket and\n\nchoose a unique name using lowercase letters, numbers, periods,\n\nand hyphens. Select your desired name. Finally, aer reviewing your\n\ncon\u0000guration, click on Create bucket to \u0000nalize.\n\nHow to do it…\n\ne high-level steps to build a simple AWS Glue work\u0000ow for CSV-\n\nto-Parquet conversion are as follows:\n\n1. Prepare your data: Store your CSV data in an S3 bucket. is will be the\n\ninput source for your Glue job.\n\n2. Create an IAM role: Create an IAM role with the necessary permissions\n\nfor your Glue crawler and Glue job to access S3 and the Glue Data\n\nCatalog.\n\n3. Create a Glue database: A Glue database is like a container that holds\n\ntables and their schemas within the AWS Glue Data Catalog. It helps\n\norganize your metadata, making it easier to manage and query your\n\ndata.\n\nIn the AWS Glue Data Catalog, create a database to organize the\n\ntables that your crawler will generate.\n\n4. Create an AWS Glue crawler: Glue crawlers automatically discover and\n\ncatalog your data stored in various locations such as S3, databases, and\n\ndata lakes. ey extract schema information and create table de\u0000nitions,\n\nmaking your data readily available for analysis and querying.\n\nCon\u0000gure the crawler to read the CSV data from your S3\n\nbucket. e crawler will infer the schema of your data and create\n\ncorresponding tables in the AWS Glue Data Catalog, within the\n\ndatabase you created.\n\n5. Create an AWS Glue job: Glue jobs are the heart of your ETL processes\n\nin AWS Glue. ey run your scripts (written in Python or Scala) to\n\nextract data from sources, transform it according to your needs, and\n\nload it into target destinations:\n\nI. Use a Python or Scala script to de\u0000ne your ETL logic.\n\nII. In the script, read the CSV data from the S3 bucket using the\n\ntable created by the crawler.\n\nIII. Perform any necessary transformations on the data.\n\nIV. Write the transformed data to a new location in the S3 bucket\n\nin Parquet format, partitioning the data as needed.\n\n6. Create an AWS Glue work\u0000ow: Work\u0000ows provide a visual\n\nrepresentation of your data processing steps in AWS Glue. ey allow\n\nyou to connect various components, de\u0000ne dependencies, and monitor\n\nthe execution \u0000ow, making it easier to manage complex ETL processes:\n\nI. Create a Glue work\u0000ow to orchestrate the crawler and job.\n\nII. Add the crawler and job as nodes in the work\u0000ow.\n\nIII. De\u0000ne the dependencies between the nodes (for example, the\n\njob should run aer the crawler has \u0000nished).\n\n7. Create an AWS Glue trigger: Triggers act like “event listeners” for your\n\nGlue work\u0000ows. ey initiate your data processing tasks based on\n\nschedules, data arrival events, or completion of other jobs, making your\n\ndata pipelines dynamic and responsive:\n\nI. Con\u0000gure the trigger to start the Glue work\u0000ow based on a\n\nschedule or an event (for example, new \u0000les arriving in the S3\n\nbucket).\n\nNow let’s go through the detailed explanation of each step:\n\n1. Prepare CSV data in S3:\n\nI. Ensure you have CSV data stored in an S3 bucket. For this\n\nexample, let’s assume your CSV \u0000les are stored in\n\ns3://your-folder-name/data/.\n\nII. Download the sample_data.csv \u0000le from GitHub\n\n(https://github.com/PacktPublishing/Data-Engineering-with-\n\nAWS-Cookbook/blob/main/Chapter04/Recipe1/glue-\n\nwork\u0000ow/sample.csv).\n\nIII. Upload it to the S3 bucket (s3://your-folder-\n\nname/data/). It should present the following content:\n\nFigure 4.1 – The content of the sample_data.csv ﬁle\n\n2. Create an IAM role for AWS Glue:\n\nI. Choose an IAM role or create one that allows AWS Glue to\n\naccess your S3 data. To create an IAM role and policy in AWS,\n\n\u0000rst, navigate to the IAM service in the AWS Management\n\nConsole and open the IAM console at\n\nhttps://console.aws.amazon.com/iam/.\n\nII. Create the policy with the following steps:\n\ni. In the navigation pane, click on Policies.\n\nii. Click on the Create policy button.\n\niii. Select the JSON tab and paste the policy document\n\n(available at https://github.com/PacktPublishing/Data-\n\nEngineering-with-AWS-\n\nCookbook/blob/main/Chapter04/Recipe1/glue-\n\nwork\u0000ow/glue-policy.json).\n\niv. en, create the AWSGlueServiceRole role. Select\n\nRole | Glue and then select the policy name you\n\ncreated in the previous steps.\n\nNOTE\n\nDon’t forget to update your S3 bucket name and create the same folder\n\nstructure under your S3 bucket.\n\nIn this policy, we have given the following permissions:\n\nGlueAccess:\n\nAllows most common Glue actions. You can further re\u0000ne this\n\nlist based on your speci\u0000c needs.\n\ne policy allows these actions on all Glue resources\n\n(\"Resource\": \"*\").\n\nS3Access:\n\nGrants permissions to get, put, and delete objects, list the\n\nbucket, and get the location for the speci\u0000ed bucket (my-\n\ndata-bucket).\n\nReplace my-data-bucket with your actual bucket name.\n\nYou can extend the \"Resource\" list to include other buckets\n\nor paths within buckets as necessary.\n\n3. Create an AWS Glue database:\n\nI. Open the AWS Glue console at\n\nhttps://console.aws.amazon.com/glue/.\n\nII. Create a database with the help of the following steps:\n\ni. Navigate to the Databases section.\n\nii. Click on Add database.\n\niii. Name your database glue_workshop.\n\n4. Create an AWS Glue crawler:\n\nI. Open the AWS Glue console at\n\nhttps://console.aws.amazon.com/glue/.\n\nII. Create a Glue crawler with the help of the following steps:\n\ni. Navigate to the Crawlers section.\n\nii. Click on Add crawler.\n\niii. Name your crawler csvCrawler.\n\niv. Set the data store to S3 and specify the S3 path\n\n(s3://your-folder/data/).\n\nv. Con\u0000gure the IAM role to use\n\nAWSGlueServiceRole.",
      "page_number": 229
    },
    {
      "number": 13,
      "title": "Segment 13 (pages 250-271)",
      "start_page": 250,
      "end_page": 271,
      "detection_method": "synthetic",
      "content": "vi. Set the frequency to Run on demand.\n\nvii. Create or select a database (created in step 3) where the\n\ncrawler results will be stored, for example,\n\naws_workshop.\n\nReview and create the crawler.\n\nNOTE\n\nPlease ensure that the IAM role used by the AWS Glue crawler is the same as\n\nthe one that has the necessary permissions for creating tables in the\n\ndatabase. Use an already created role with these permissions.\n\n5. Run the crawler:\n\nI. Select the crawler, csvCrawler, and click on Run crawler.\n\nII. Wait for the crawler to complete. It will create a table in the\n\nspeci\u0000ed database.\n\nFigure 4.2 – Crawler output\n\n6. Create an AWS Glue job:\n\nI. Open the AWS Glue console at\n\nhttps://console.aws.amazon.com/glue/.\n\nII. Create a Glue job as follows:\n\ni. Download the csv_to_parquet.py \u0000le from your\n\nGitHub for upload. You can \u0000nd scripts on following\n\nGitHub path\n\nhttps://github.com/PacktPublishing/Data-\n\nEngineering-with-AWS-\n\nCookbook/blob/main/Chapter04/Recipe1/glue-\n\nwork\u0000ow/csv_to_parquet.py.\n\nii. Go to the Jobs section in AWS Glue.\n\niii. Click on Author code with a script editor and then on\n\nScript editor.\n\niv. en select Spark, choose Upload scripts, and add the\n\ncsv_to_parquet.py script you downloaded from\n\nGitHub.\n\nv. Click on Create script to complete the setup.\n\nFigure 4.3 – Glue ETL job\n\nvi. Fill in the following job properties:\n\nName: CsvToParquetJob (double-click, update the Glue\n\nJob name, and save)\n\nNow go to Job detail tab then click IAM Role:\n\nAWSGlueServiceRole (select from dropdown)\n\nType: Spark (select from dropdown)\n\nScript \u0000le name: s3://aws-glue-assets-accountNo-\n\nus-east-1/scripts/\n\nSpark UI logs path: s3://aws-glue-assets-\n\naccountNo-us-east-1/sparkHistoryLogs/\n\nTemporary directory: s3://your-folder/temp/\n\nvii. Aer starting the job, monitor its progress in the AWS Glue console.\n\nUpon completion, your Parquet data should be available in the speci\u0000ed\n\nS3 location, partitioned according to the keys you speci\u0000ed.\n\nNOTE\n\n1. In the previous script csv_to_parquet.py code don’t forget to\n\nupdate following:\n\ndatabase with the Glue database name you created as\n\nglue_workshop.\n\ntable_name with the table your crawler created (since we are using\n\nsample.csv for crawling data, your table name will be sample).\n\noutput_s3_path with your S3 bucket and folder locations. The\n\nscript path is created by the job by default if you want to change and add\n\nyour S3 bucket and path update it.\n\n2. Always monitor AWS costs associated with running AWS Glue crawlers and\n\njobs and storing data in S3. For large datasets, converting data from CSV to\n\nParquet can be cost-eﬀective, as Parquet is a columnar storage format\n\noptimized for analytics. Ensure you’ve set up appropriate data retention,\n\nbackup, and lifecycle policies for your S3 buckets.\n\n7. Con\u0000gure the Glue job:\n\nI. To set the Glue job script path, click on Job details, and in the\n\ndropdown, choose your S3 path: s3://your-\n\nfolder_name/scripts/csv_to_parquet.py.\n\nII. Enable job bookmarking if you want to keep track of processed\n\ndata.\n\nIII. Choose an appropriate worker type and number of workers\n\nbased on your data size and complexity.\n\nNOTE\n\nStandard workers are general-purpose workers suitable for a wide range of\n\nETL tasks; we will use the same for our recipe.\n\nFigure 4.4 – Glue ETL job conﬁguration\n\nIV. In Advanced property, select Job parameters and add the following\n\nvalues:\n\n--enable-metrics true\n\n--enable-job-insights =true\n\n--enable-observability-metrics =true\n\n8. Create and con\u0000gure the Glue work\u0000ow:\n\nI. Work\u0000ow creation:\n\ni. Navigate to the AWS Glue console.\n\nii. Click on the Work\u0000ows menu on the le and then\n\nclick on the Add work\u0000ow button.\n\niii. On the next screen, type in glueworkflow as the\n\nwork\u0000ow name and click on the Add work\u0000ow button.\n\ne work\u0000ow is created.\n\nII. Trigger con\u0000guration:\n\ni. Select the work\u0000ow and click on the Add trigger link.\n\nii. On the Add trigger popup, select the Add new tab.\n\nType in startcrawler as the trigger name and\n\nselect On demand for the trigger type. (You select the\n\nOn demand trigger type because you will start the\n\nwork\u0000ow manually in this workshop.) Click on the\n\nAdd button. e trigger is added to the work\u0000ow.\n\nIII. Node addition:\n\ni. Click on the Add node link to con\u0000gure what you want\n\nto run aer the trigger.\n\nii. On the pop-up screen, select the Crawlers tab. Select\n\ncsvCrawler (created in step 4 Create an AWS Glue\n\ncrawler ) and click on the Add button.\n\niii. e crawler node is added as the next step to the\n\ntrigger. Next, select the Add trigger option under the\n\nAction menu to add another trigger.\n\niv. On the pop-up screen, select the Add new tab. Type in\n\nstartjob as the name. Select Event for the trigger\n\ntype. Select Start aer ANY watched event for the\n\ntrigger logic. Finally, click on the Add button. e\n\ntrigger is added.\n\nIV. Trigger and job con\u0000guration:\n\ni. Select the Start job trigger and select Add jobs crawlers\n\nto the Watch option under the Action menu.\n\nii. On the pop-up screen, select the Crawlers tab. Select\n\nSUCCEEDED for the Crawler event to watch \u0000eld.\n\nFinally, click on the Add button.\n\niii. e startjob trigger is now con\u0000gured to run when\n\nthe crawler \u0000nishes execution successfully. Click on the\n\nAdd node icon next to startjob to con\u0000gure what job or\n\ncrawler the statjob trigger will invoke.\n\niv. On the pop-up screen, select the Jobs tab. Select\n\ncsvtoparquetjob and click on the Add button. e\n\nwork\u0000ow is now con\u0000gured end to end. It will \u0000rst run\n\nthe crawler and then the job.\n\nV. Work\u0000ow execution:\n\ni. Select gluework\u0000ow and click on the Run option under\n\nthe Action menu.\n\nii. e work\u0000ow execution will start with the status of\n\nRunning. Wait till the status changes to Completed.\n\nYou can see the crawler run in the work\u0000ow has added\n\nthe table customers under the glue-workshop\n\ndatabase.\n\nVI. Veri\u0000cation:\n\ni. Con\u0000rm that the crawler has successfully created the\n\ntable de\u0000nition for the data in the glue-workshop\n\ndatabase.\n\nFigure 4.5 – Glue workﬂow UI\n\nat’s a high-level overview of creating a simple AWS Glue\n\nwork\u0000ow. Adjust the steps based on the speci\u0000cs of your use case,\n\nsuch as the complexity of the transformations needed or additional\n\nsources and sinks.\n\nOnce the Glue work\u0000ow is \u0000nished, you can see the job’s status is\n\nshown as successful:\n\nFigure 4.6 – Glue workﬂow status\n\nYou can also check the results in the S3 bucket; go to the S3 bucket\n\nand explore the data:\n\nFigure 4.7 – Results of the ETL job in the S3 bucket\n\nWhen you open the folder, you will see the Parquet \u0000le:\n\nFigure 4.8 – Parquet ﬁle in the S3 bucket\n\nSee also\n\nOverview of work\u0000ows in AWS Glue:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/work\u0000ows_overview.html\n\nAWS Glue Immersion day – Introduction: https://catalog.us-east-\n\n1.prod.workshops.aws/workshops/ee59d21b-4cb8-4b3d-a629-\n\n24537cf37bb5/en-US/intro\n\nSetting up event-driven orchestration with Amazon EventBridge\n\nCreating a work\u0000ow that reacts to speci\u0000c events to trigger AWS\n\nGlue components on-demand typically requires the combination of\n\nAWS Glue with other AWS services, such as AWS Lambda and\n\nAmazon EventBridge (formerly known as Amazon CloudWatch\n\nEvents).\n\nNow, we want our work\u0000ow to react to the addition of a new \u0000le in\n\nan S3 bucket to trigger an AWS Glue job.\n\nLet’s understand when this is required. A media company processes\n\ndaily uploads of large CSV \u0000les to an S3 bucket. Each time a new\n\nCSV \u0000le is uploaded, the company needs to trigger an ETL pipeline\n\nthat extracts data from the CSV, processes it, and stores the\n\ninformation in a data lake for analysis. is work\u0000ow is fully\n\nautomated and event-driven, ensuring immediate processing upon\n\n\u0000le arrival.\n\nGetting ready\n\nBefore proceeding with this recipe, ensure you have completed the\n\nsteps in the De\u0000ning a simple work\u0000ow using AWS Glue work\u0000ows\n\nrecipe earlier in this chapter.\n\nHow to do it…\n\n1. Set up an S3 bucket event noti\u0000cation: Ensure you have an S3 bucket\n\nwhere \u0000les will be uploaded. Please use the same bucket we created in\n\nthe De\u0000ning a simple work\u0000ow using AWS Glue work\u0000ows recipe.\n\n2. Create an IAM role and policy for Glue: Please use the same policy and\n\nrole we created in the De\u0000ning a simple work\u0000ow using AWS Glue\n\nwork\u0000ows recipe.\n\n3. Create an AWS Glue database and crawler:\n\nI. Create Glue database:\n\ni. Go to the AWS Glue console at\n\nhttps://console.aws.amazon.com/glue/.\n\nii. Navigate to Databases and click on Add database.\n\niii. Name your database csv_database.\n\niv. Click on Create.\n\nNOTE\n\nPlease use the same database we created in the Deﬁning a simple workﬂow\n\nusing AWS Glue workﬂows recipe.\n\nII. Create Glue crawler:\n\ni. Go to the Crawlers section in the Glue console.\n\nii. Click on Add crawler.\n\niii. Name your crawler csvCrawler.\n\niv. For the data store, choose S3 and specify the S3 path\n\n(s3://your-bucket/csv-data/).\n\nv. Con\u0000gure the IAM role to use glue-sample-role.\n\nvi. Set the frequency to Run on demand.\n\nvii. Create or select the csv_database database where the\n\ncrawler results will be stored.\n\nviii. Review and create the crawler.\n\nix. Do not run the crawler yet.\n\nNOTE\n\nPlease use the same crawler we created in the Deﬁning a simple workﬂow\n\nusing AWS Glue workﬂows recipe.\n\n4. Create AWS Glue job:\n\nI. Prepare the Glue ETL script: Save the script presented in the\n\nDe\u0000ning a simple work\u0000ow using AWS Glue work\u0000ows recipe as\n\ncsv_to_parquet.py and upload it to your S3 bucket.\n\nII. Create the Glue job:\n\ni. Go to the Jobs section in the Glue console.\n\nii. Click on Add Job.\n\niii. Name the job CsvToParquetJob.\n\niv. Choose the glue-sample-role IAM role.\n\nv. For the ETL language, select Python.\n\nvi. For the script, choose A new script to be authored by\n\nyou.\n\nvii. Set the script \u0000lename to s3://your-\n\nbucket/scripts/csv_to_parquet.py.\n\nviii. Set the temporary directory to s3://your-\n\nbucket/temp/.\n\nix. Click on Next and con\u0000gure the job properties as\n\nneeded.\n\nx. Click on Save.\n\nNOTE\n\nPlease use the same job we created in the Deﬁning a simple workﬂow using\n\nAWS Glue workﬂows recipe.\n\n5. Create and con\u0000gure the Glue work\u0000ow:\n\nI. Create work\u0000ow:\n\ni. Go to the Work\u0000ows section in the Glue console.\n\nii. Click on Add work\u0000ow.\n\niii. Name your work\u0000ow glueWorkflow.\n\niv. Click on Create.\n\nII. Add crawler to the work\u0000ow:\n\ni. In the Glue console, select your glueWorkflow\n\nwork\u0000ow.\n\nii. Click on Add trigger and choose Add new.\n\niii. Name the trigger csvCrawlerTrigger.\n\niv. Set the type to On-demand.\n\nv. In the Actions section, add an action to start the\n\ncsvCrawler crawler.\n\nvi. Save the trigger.\n\nIII. Add job to the work\u0000ow:\n\ni. In the Glue console, select your glueWorkflow\n\nwork\u0000ow.\n\nii. Click on Add trigger and choose Add new.\n\niii. Name the trigger csvToParquetJobTrigger.\n\niv. Set the type to Event-based.\n\nv. Under Conditions, add a condition:\n\nLogical operator: EQUALS\n\nState: SUCCEEDED\n\nCrawler name: csvCrawler\n\nvi. In the Actions section, add an action to start the job,\n\nCsvToParquetJob.\n\nvii. Save the trigger.\n\nNOTE\n\nPlease use the same workﬂow we created in the Deﬁning a simple workﬂow\n\nusing AWS Glue workﬂows recipe.\n\n6. Create an EventBridge rule to trigger the work\u0000ow:\n\nI. Create EventBridge rule:\n\ni. Go to the Amazon EventBridge console at\n\nhttps://console.aws.amazon.com/events/.\n\nii. Click on Create rule.\n\niii. Name your rule S3FileUploadRule].\n\niv. Select Rule with an event pattern and click on the Next\n\nbutton.\n\nFigure 4.9 – EventBridge rule\n\nv. Choose Event source on next page and select AWS events or\n\nEventBridge partner events.\n\nFigure 4.10 – EventBridge rule event source\n\nvi. Now choose Custom pattern JSON editor:\n\nFigure 4.11 – EventBridge patterns\n\nvii. en, on the next page, set Creation method as Custom pattern (JSON\n\neditor) and add the following JSON in the input box:\n\n{ \"source\": [\"aws.s3\"], \"detail-type\": [\"Object Created\"], \"detail\": {\n\n\"bucket\": { \"name\": [\"your-bucket\"] }, \"object\": { \"key\": [{\"prefix\": \"data/\"}] } }}\n\nNOTE\n\nReplace your-bucket with your bucket name.\n\nviii. Click on Save.\n\nix. In the Targets section, click on Add target and select AWS Lambda\n\nfunction.\n\nx. Choose the Lambda function you created to start the Glue work\u0000ow\n\n(TriggerGlueWork\u0000ow).\n\nxi. Click on Create.\n\n7. Create AWS Lambda function to start Glue work\u0000ow:\n\nI. Create Lambda function:\n\ni. Go to the AWS Lambda console at\n\nhttps://console.aws.amazon.com/lambda/.\n\nii. Click on Create function.\n\niii. Choose Author from scratch.\n\niv. Enter a function name, for example,\n\nTriggerGlueWorkflow-lambda.\n\nv. Choose the Python 3.x runtime.\n\nvi. Choose or create an execution role and ensure it uses\n\nlambda-glue-trigger-role (choose Use an\n\nexisting role in lambda). You need to go to IAM, open\n\nlambda-glue-trigger-role, and add\n\nAWSGlueConsoleFullAccess.\n\nFigure 4.12 – Lambda Glue trigger role\n\nvii. Click on Create function.\n\nII. Add Lambda function code: Replace the default Lambda function code\n\nwith the following:\n\nimport json import boto3 def lambda_handler(event, context): glue = boto3.client('glue') # Start the Glue workflow workflow_name = 'glueworkflow' response = glue.start_workflow_run(Name=workflow_name) return {\n\n'statusCode': 200, 'body': json.dumps('Workflow started: {}'.format(response['RunId'])) }\n\nIII. Add S3 trigger to Lambda function:\n\ni. In the Lambda function, go to the Con\u0000guration tab.\n\nii. Click on Add trigger.\n\niii. Select S3.\n\niv. Choose the bucket name (your-bucket).\n\nv. Set the event type to All object create events.\n\nvi. Set the pre\u0000x to csv-data/ (or whatever path you want to\n\nmonitor).\n\nvii. Click on Add.",
      "page_number": 250
    },
    {
      "number": 14,
      "title": "Segment 14 (pages 272-293)",
      "start_page": 272,
      "end_page": 293,
      "detection_method": "synthetic",
      "content": "Figure 4.13 – Lambda function\n\nUpon uploading the \u0000le to the S3 bucket, a Lambda function will be\n\ntriggered to initiate a Glue work\u0000ow using Boto3.\n\nCreating a data workflow using AWS Step Functions\n\nAWS Step Functions is a serverless work\u0000ow orchestration service\n\nthat enables you to connect and manage various AWS services\n\nwithin a streamlined work\u0000ow. is allows for the rapid\n\ndevelopment and updating of applications.\n\nStep Functions employs a state machine approach, where each step\n\nin the work\u0000ow is represented by a state. ese states are de\u0000ned\n\nusing the Amazon States Language, a JSON-based format. By\n\nchaining together diﬀerent states, you can create complex\n\nwork\u0000ows that integrate services such as AWS Lambda, AWS\n\nFargate, and Amazon SageMaker to build powerful applications.\n\nIn this recipe, we’ll create a simple AWS Step Functions work\u0000ow\n\nthat uses two AWS Lambda functions: one that gets data and one\n\nthat processes data.\n\nHow to do it…\n\n1. Create two Lambda functions:\n\nI. Go to the AWS Management Console.\n\nII. Navigate to Lambda by searching for Lambda in the services\n\nsearch bar.\n\nIII. Create a new Lambda function\n\nIV. In the Lambda Console, click on Create function.\n\nV. Select Author from scratch.\n\nVI. Enter the function name (for example, GetDataFunction).\n\nVII. For Runtime, select Python 3.x from the dropdown menu.\n\nVIII. Under Permissions, choose or create a role with necessary\n\nLambda execution permissions (for example, basic permissions\n\nto write logs to CloudWatch).\n\nIX. In the Function code section, you can edit the default Python\n\ncode and replace it with following code same step repeat for\n\nProcessDataFunction.\n\nX. Once you have written the code and con\u0000gured the settings,\n\nclick on Deploy to save and activate the function.\n\ne \u0000rst Lambda function is GetDataFunction. Use the\n\nPython 3.x runtime for this:\n\ndef lambda_handler(event, context): # Sample data fetching logic data = { \"message\": \"Hello from GetDataFunction\" } return data\n\ne second Lambda function is ProcessDataFunction, again\n\nusing the Python 3.x runtime:\n\ndef lambda_handler(event, context): # Sample data processing logic processed_data = event['message'].upper() return { \"processedMessage\": processed_data }\n\n2. De\u0000ne a state machine in Amazon States Language:\n\nNow, create a state machine with the help of the following JSON\n\nthat \u0000rst invokes GetDataFunction and then\n\nProcessDataFunction:\n\n{ \"Comment\": \"A simple AWS Step Functions state machine that invokes two Lambda functions.\", \"StartAt\": \"GetDataState\", \"States\": { \"GetDataState\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:Get DataFunction\", \"Next\": \"ProcessDataState\" }, \"ProcessDataState\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:Pro cessDataFunction\", \"End\": true } }}\n\nNOTE\n\nReplace REGION and ACCOUNT_ID with your AWS Region and account\n\nID, respectively.\n\n3. Create the Step Functions state machine:\n\nI. Download stepfunction.json from GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-\n\nAWS-Cookbook/tree/main/Chapter04/Recipe3/stepfunction.\n\nII. Open the AWS Management Console and navigate to step\n\nfunction or navigate to https://us-east-\n\n1.console.aws.amazon.com/states.\n\nIII. Click on the Create state machine button and select Blank.\n\nIV. Click on the Action dropdown and select Import de\u0000nition.\n\nSelect your \u0000le and upload it. Your step function will be ready\n\naer upload.\n\nV. e other option is to navigate to the AWS Step Functions\n\ndashboard and create a new state machine. Paste the Amazon\n\nStates Language JSON from the previous step into the state\n\nmachine de\u0000nition. Ensure your AWS Step Functions role has\n\npermission to invoke Lambda functions.\n\nYou can view the JSON data and its corresponding visual\n\nrepresentation in the Work\u0000ow studio.\n\nFigure 4.14 – Step function with JSON code\n\n4. Execute the state machine: Once the state machine is created, you can\n\nstart executing the step function (click on Start execution, enter {}, and\n\nthen click on the Start execution button).\n\nFigure 4.15 – Start step function execution\n\nis will run GetDataFunction, take its output, and pass it as an\n\ninput to ProcessDataFunction.\n\nAWS Step Functions oﬀers a clear visual representation of your\n\nwork\u0000ow, making it easier to follow the execution process. As the\n\nstate machine runs, each step is highlighted in real time, allowing\n\nyou to track progress visually. Additionally, for each step in the\n\nwork\u0000ow, you can view detailed input and output data, helping you\n\nunderstand how the information \u0000ows and changes throughout the\n\nexecution.\n\n5. Work\u0000ow Studio in AWS Step Functions: Work\u0000ow Studio is a visual\n\ntool that allows users to design work\u0000ows by dragging and dropping\n\nAWS services onto a canvas. is graphical interface provides an easy\n\nway for those who might not be familiar with Amazon States Language\n\n(ASL) or prefer visual design overwriting code.\n\nHere’s a basic overview of how to use Work\u0000ow Studio:\n\nI. Navigate to AWS Step Functions:\n\ni. Open the AWS Management Console.\n\nii. Navigate to the Step Functions service.\n\nII. Create a new state machine:\n\ni. Click on Create state machine, select Blank, and click\n\non Create.\n\nii. Choose Design with Work\u0000ow Studio.\n\nIII. Design your work\u0000ow:\n\ni. You’ll be provided with a blank canvas. On the right\n\nside of the interface, there’s a palette of AWS services\n\nand \u0000ow control elements.\n\nii. Simply drag and drop services or elements onto the\n\ncanvas.\n\niii. For each service or element, con\u0000gure its parameters\n\nusing the properties panel on the right side.\n\nIV. Connect the elements:\n\ni. Click on the output connector (a small circle) of an\n\nelement and drag it to the input connector of another\n\nelement to establish a transition between them.\n\nii. You can also add error catchers, retries, and other \u0000ow\n\ncontrol mechanisms.\n\nV. Deploy the work\u0000ow:\n\ni. Once you’ve designed the work\u0000ow visually, click on\n\nNext. Work\u0000ow Studio will automatically generate the\n\nASL de\u0000nition for your work\u0000ow.\n\nii. Provide a name for the state machine.\n\niii. De\u0000ne or select an IAM role for the state machine to\n\nuse.\n\niv. Click on Create state machine.\n\nVI. Execute and monitor the work\u0000ow: Aer creating your\n\nwork\u0000ow, you can initiate its execution to test its functionality.\n\nAWS Step Functions oﬀers a visual representation of the\n\nwork\u0000ow, allowing you to observe the progress as each step is\n\nexecuted. While we’ve previously shown how to create a Step\n\nFunction using JSON, you can also utilize a more intuitive\n\ndrag-and-drop interface for a simpli\u0000ed work\u0000ow creation\n\nprocess.\n\nSee also\n\nAWS Step Functions: https://aws.amazon.com/step-functions/\n\nUsing Amazon States Language to de\u0000ne Step Functions work\u0000ows:\n\nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-\n\namazon-states-language.html\n\nCreating a work\u0000ow with Work\u0000ow Studio in Step Functions:\n\nhttps://docs.aws.amazon.com/step-functions/latest/dg/work\u0000ow-studio-\n\nuse.html\n\nNew – AWS Step Functions Work\u0000ow Studio – A Low-Code Visual Tool\n\nfor Building State Machines: https://aws.amazon.com/blogs/aws/new-\n\naws-step-functions-work\u0000ow-studio-a-low-code-visual-tool-for-\n\nbuilding-state-machines/\n\nManaging data pipelines with MWAA\n\nMWAA is a fully managed service provided by AWS that simpli\u0000es\n\nthe deployment and operation of Apache Air\u0000ow, an open source\n\nwork\u0000ow automation platform. Apache Air\u0000ow is widely used for\n\norchestrating complex data work\u0000ows, scheduling batch jobs, and\n\nmanaging data pipelines. MWAA takes the power of Apache\n\nAir\u0000ow and makes it easier to use, maintain, and scale in the AWS\n\ncloud environment.\n\nMWAA is commonly used to orchestrate complex data pipelines.\n\nUsers can de\u0000ne and schedule tasks to transform, process, and\n\nmove data between various AWS services, databases, and external\n\nsystems. Another good use case is that MWAA simpli\u0000es the\n\nmanagement of ETL work\u0000ows. Users can easily schedule and\n\nautomate data extraction, transformation, and loading tasks,\n\nensuring data accuracy and consistency. MWAA also supports\n\nbatch processing and batch jobs, such as data aggregation, report\n\ngeneration, and data synchronization, which can be eﬃciently\n\nmanaged and scheduled using MWAA.\n\nHow to do it…\n\n1. Setting up your MWAA environment: Before you can start orchestrating\n\ndata pipelines with MWAA, you need to set up your environment:\n\nI. Create IAM roles and policies:\n\ni. Go to the IAM console at\n\nhttps://console.aws.amazon.com/iam/.\n\nii. Navigate to Policies and click on Create policy.\n\niii. Select the JSON tab and paste the following policy\n\ndocument:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"glue:*\",\n\n\"s3:GetObject\", \"s3:PutObject\", \"s3:ListBucket\",\n\n\"logs:CreateLogGroup\",\n\n\"logs:CreateLogStream\",\n\n\"logs:PutLogEvents\",\n\n\"cloudwatch:PutMetricData\" ], \"Resource\": [ \"arn:aws:s3::your- mwaa-bucket\", \"arn:aws:s3::your- mwaa-bucket/*\",\n\n\"arn:aws:logs:*:*:*\",\n\n\"arn:aws:glue:*:*:catalog\",\n\n\"arn:aws:glue:*:*:database/*\",\n\n\"arn:aws:glue:*:*:table/*\",\n\n\"arn:aws:glue:*:*:connection/*\",\n\n\"arn:aws:glue:*:*:job/*\",\n\n\"arn:aws:glue:*:*:crawler/*\",\n\n\"arn:aws:glue:*:*:workflow/*\",\n\n\"arn:aws:glue:*:*:trigger/*\",\n\n\"arn:aws:glue:*:*:classifier/*\",\n\n\"arn:aws:cloudwatch:*:*:metric/*\" ] } ]}\n\nNOTE\n\nUpdate your S3 bucket.\n\n2. Create an IAM role for MWAA:\n\nI. Go to the IAM console, navigate to Roles, and click on Create\n\nrole.\n\nII. Select AWS service and then choose MWAA.\n\nIII. Click on Next: Permissions.\n\nIV. Attach the policy you just created.\n\nV. Click on Next: Tags, optionally add tags, then click on Next:\n\nReview.\n\nVI. Name your role mwaa-service-role.\n\nVII. Click on Create role.\n\n3. Create MWAA environment:\n\nI. Go to the MWAA console at https://us-east-\n\n1.console.aws.amazon.com/mwaa/home?region=us-east-\n\n1#home?landingPageCheck=1.\n\nII. Click on Create environment.\n\nIII. Enter the environment details, such as name and description.\n\nIV. For DAG folder S3 Path, enter s3://your-mwaa-\n\nbucket/dag.\n\nV. For Plugins \u0000le S3 Path, enter s3://your-mwaa-\n\nbucket/plugins (optional). You can \u0000nd details about\n\nplugins on AWS Documents\n\n(https://docs.aws.amazon.com/mwaa/latest/userguide/con\u0000guri\n\nng-dag-import-plugins.html).\n\nFigure 4.16 – DAG conﬁguration\n\nVI. For Requirements \u0000le S3 Path, enter s3://your-mwaa-\n\nbucket/requirements.txt (optional). You can \u0000nd details about\n\nreuirments.txt in AWS documents\n\n(https://docs.aws.amazon.com/mwaa/latest/userguide/best-practices-\n\ndependencies.html).\n\nVII. Select the mwaa-service-role you created earlier in the Create\n\nIAM roles and policies section at the beginning of step 1.\n\nFigure 4.17 – Add role in conﬁguration\n\nVIII. Con\u0000gure networking settings (VPC, subnets, and security groups).\n\nFigure 4.18 – Networking conﬁguration\n\nIX. Select Create new VPC. Wait for a few minutes. Once the VPC is\n\ncreated, select New; you will see that a subnet will be automatically\n\nassigned. Now, choose VPC security group and Public network (internet\n\naccess).\n\nNOTE\n\nBefore you start MWAA, create an S3 bucket that will store your Directed\n\nAcyclic Graphs (DAGs). This bucket will also store MWAA logs and metrics.\n\nX. Logging con\u0000guration: Specify the S3 pre\u0000x where you want logs to be\n\nstored in your S3 bucket and enable the checkbox in the Amazon\n\nManaged Work\u0000ows for Apache Air\u0000ow (MWAA) UI.\n\nXI. Air\u0000ow con\u0000guration options: If needed, override the default\n\nairflow.cfg settings in the CFG \u0000le.\n\nXII. Permissions: Add permissions using the following two steps:\n\ni. Attach an execution role that allows MWAA to access necessary\n\nresources (download the policy from GitHub and attach it to\n\nyour role).\n\nii. Attach a task role if your tasks need to access other AWS\n\nservices.\n\niii. Click on Create environment. is can take a few minutes.\n\nFigure 4.19 – MWAA environment created\n\nXIII. Access the Air\u0000ow UI: Once the environment is created, you can access\n\nthe Air\u0000ow UI through the link provided in the MWAA console.\n\nFigure 4.20 – MWAA UI\n\nXIV. Once your UI is visible, download dag-sample.py from GitHub\n\n(https://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/blob/main/Chapter04/Recipe4/mwaa/dag-sample.py) and\n\nupload it into the dags folder in your S3 bucket. Your uploaded DAGs\n\nwill be visible in a few minutes:\n\nFigure 4.21 – MWAA sample DAG\n\nXV. Clicking on the MWAA DAG will display a graph view. Explore the user\n\ninterface by interacting with all available options.\n\nFigure 4.22 – MWAA DAG\n\nNow, we will look into MWAA and its integration with AWS Glue:\n\n1. Create an S3 bucket for MWAA:\n\nI. Use the S3 bucket we created in the previous steps to upload\n\nyour DAGs (please use the steps from Create MWAA\n\nenvironment).\n\ni. Create a folder for dags in your S3 bucket.\n\nii. Upload your Air\u0000ow DAGs to the dags folder.\n\niii. Upload any custom plugins to the plugins folder.\n\niv. Upload the requirements.txt \u0000le to the root of\n\nthe bucket.\n\n2. Create an Air\u0000ow DAG to trigger AWS Glue jobs:\n\nI. Create a DAG: Save the following DAG as mwaa-glue-\n\ndag.py and upload it to your S3 bucket in the dags folder.\n\nLet’s discuss the code:\n\ni. Introduction and initial setup: We start by importing\n\nthe required libraries: boto3 for interacting with AWS\n\nservices, airflow for orchestrating tasks, and time\n\nfor delays between task checks. en, we initialize an\n\nAWS Glue client using boto3, specifying the AWS\n\nRegion:\n\nimport boto3 from airflow import DAG from airflow.operators.python_operator import PythonOperator from airflow.utils.dates import days_ago from botocore.exceptions import ClientError import time # Initialize AWS Glue client glue_client = boto3.client('glue', region_name='us-east-1')\n\nii. Starting the Glue crawler: e start_crawler\n\nfunction initiates the Glue crawler by calling\n\nglue_client.start_crawler(). It includes\n\nerror handling using ClientError in case of\n\nfailures:\n\n# Function to start Glue crawler def start_crawler(crawler_name): try: response = glue_client.start_crawler(Name=craw\n\nler_name) print(f\"Started crawler: {crawler_name}\") except ClientError as e: print(f\"Error starting crawler: {e}\") raise as e\n\niii. Checking Glue crawler status: is function\n\ncontinuously checks the status of the crawler and waits\n\nuntil it completes. e time.sleep(60) function\n\nintroduces a delay of 1 minute between status checks to\n\nprevent rapid polling:\n\n# Function to check the status of Glue crawler def check_crawler_status(crawler_name): while True: response = glue_client.get_crawler(Name=crawle r_name) crawler_status = response['Crawler']['State'] if crawler_status == 'READY': print(f\"Crawler {crawler_name} has completed.\") break elif crawler_status == 'RUNNING': print(f\"Crawler {crawler_name} is still running...\") time.sleep(60) # Wait\n\nfor 1 minute before checking again else: raise Exception(f\"Crawler {crawler_name} encountered an error: {crawler_status}\")\n\niv. Starting the Glue ETL job: e start_glue_job\n\nfunction triggers a Glue ETL job using the\n\nstart_job_run() method and returns\n\nJobRunId, which is used later to monitor the job’s\n\nprogress:\n\n# Function to start Glue ETL job def start_glue_job(job_name): try: response = glue_client.start_job_run(JobName=j ob_name) job_run_id = response['JobRunId'] print(f\"Started Glue job: {job_name} with JobRunId: {job_run_id}\") return job_run_id except ClientError as e: print(f\"Error starting Glue job: {e}\") raise\n\nv. Checking Glue ETL job status: is function monitors\n\nthe status of the Glue ETL job using JobRunId.\n\nSimilar to the crawler status check, the function loops\n\nuntil the job either succeeds or fails:",
      "page_number": 272
    },
    {
      "number": 15,
      "title": "Segment 15 (pages 294-313)",
      "start_page": 294,
      "end_page": 313,
      "detection_method": "synthetic",
      "content": "# Function to check Glue job status def check_glue_job_status(job_name, job_run_id): while True: response = glue_client.get_job_run(JobName=job _name, RunId=job_run_id) job_status = response['JobRun']['JobRunState'] if job_status == 'SUCCEEDED': print(f\"Glue job {job_name} succeeded.\") break elif job_status in ['STARTING', 'RUNNING']: print(f\"Glue job {job_name} is still {job_status}...\") time.sleep(60) # Wait for 1 minute before checking again else: raise Exception(f\"Glue job {job_name} failed with status: {job_status}\")\n\nvi. De\u0000ning the DAG: e DAG de\u0000nes the work\u0000ow\n\nusing Air\u0000ow, including task dependencies and\n\nscheduling. Default arguments such as the start date\n\nand retry count are speci\u0000ed for the DAG:\n\n# Define the DAG default_args = { 'owner': 'airflow', 'start_date': days_ago(1), 'retries': 1\n\n}with DAG(\n\ndag_id='glue_crawler_etl_workflow', default_args=default_args, description='A simple MWAA DAG to trigger Glue Crawler and ETL Job', schedule_interval=None, # Manual trigger for this workshop catchup=False ) as dag:\n\nvii. Adding tasks to the DAG: Each step of the work\u0000ow is\n\nrepresented as a task using Air\u0000ow’s\n\nPythonOperator\n\n(https://air\u0000ow.apache.org/docs/apache-\n\nair\u0000ow/stable/howto/operator/python.html). Tasks\n\nsuch as starting the crawler, checking its status, starting\n\nthe Glue ETL job, and monitoring the job’s status are\n\nadded to the DAG:\n\n# Task 1: Start Glue Crawler start_crawler_task = PythonOperator( task_id='start_crawler',\n\npython_callable=start_crawler, op_kwargs={'crawler_name': 'csvToCatalogCrawler'} ) # Task 2: Check Glue Crawler Status check_crawler_task = PythonOperator(\n\ntask_id='check_crawler_status',\n\npython_callable=check_crawler_statu s, op_kwargs={'crawler_name': 'csvToCatalogCrawler'} ) # Task 3: Start Glue ETL Job start_glue_job_task = PythonOperator( task_id='start_glue_job',\n\npython_callable=start_glue_job, op_kwargs={'job_name': 'CsvToParquetJob'} ) # Task 4: Check Glue ETL Job Status check_glue_job_task = PythonOperator(\n\ntask_id='check_glue_job_status',\n\npython_callable=check_glue_job_stat us, op_kwargs={ 'job_name': 'CsvToParquetJob', 'job_run_id': '{{ ti.xcom_pull(task_ids=\"start_glue_j ob\") }}' } )\n\nviii. De\u0000ning task dependencies: Task dependencies are\n\nde\u0000ned using >>, creating a sequential \u0000ow where the\n\ntasks are executed one aer the other. is ensures that\n\nthe Glue crawler runs \u0000rst, followed by the ETL job\n\naer the successful completion of the crawler:\n\n# Task dependencies: DAG Flow start_crawler_task >> check_crawler_task >> start_glue_job_task >> check_glue_job_task\n\n3. Monitor and handle failures:\n\nI. Set up CloudWatch alarms:\n\ni. Go to the Amazon CloudWatch console at\n\nhttps://console.aws.amazon.com/cloudwatch/.\n\nii. Navigate to Alarms and click on Create alarm.\n\niii. Select the Glue metrics to monitor (for example,\n\nJobRunTime).\n\niv. Set the threshold and con\u0000gure noti\u0000cations.\n\nII. Add retries and failure handling in the DAG: Update your\n\nAir\u0000ow DAG to include retries and failure handling\n\nIII. Update your existing Glue job with CloudWatch metrics and\n\nretry:\n\ni. Importing libraries and setting up the environment:\n\nis section imports the necessary libraries for the\n\nAWS Glue job. Key libraries include awsglue for\n\nworking with Glue, pyspark for handling Spark, and\n\nboto3 for interacting with AWS services such as\n\nCloudWatch:\n\nimport sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job import boto3\n\nii. Initializing the Spark and Glue context: is code\n\nblock sets up the Spark and Glue context, initializing\n\nthe job using parameters passed from the command\n\nline. e GlueContext object allows access to Glue\n\nfeatures within Spark:\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME']) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args['JOB_NAME'], args)\n\niii. Reading data from the Glue Data Catalog: Here, we\n\nspecify the source database and table in the Glue Data\n\ncatalog from which the data will be read, and the\n\ndestination S3 path where the transformed data will be\n\nwritten:\n\n# Read the data from the Glue Data catalog database_name = \"glue-workshop\" table_name = \"data\" output_s3_path = \"s3://mwaa-env- data-bucket-vk/parquet-data/\"\n\niv. Sending metrics to CloudWatch: is function sends\n\ncustom metrics to Amazon CloudWatch. e\n\nput_metric_data method allows us to track job\n\nsuccess or failure by pushing metric values:\n\ndef put_metric_data(metric_name, value): cloudwatch = boto3.client('cloudwatch') cloudwatch.put_metric_data( Namespace='GlueIngestion', MetricData=[ { 'MetricName': metric_name, 'Value': value, 'Unit': 'Count' }, ] )\n\nv. Creating a DynamicFrame: is part of the code reads\n\ndata from the Glue Data Catalog as a DynamicFrame.\n\nGlue uses DynamicFrames to handle semi-structured\n\ndata eﬃciently:\n\n# Create a DynamicFrame from the table dynamic_frame = glueContext.create_dynamic_frame.fr om_catalog(\n\ndatabase=database_name, table_name=table_name )\n\nvi. Converting to a DataFrame and writing to S3: e\n\nDynamicFrame is converted to a Spark DataFrame,\n\nwhich is then written to Amazon S3 as partitioned\n\nParquet \u0000les, using country, state, and city as\n\npartition keys:\n\n# Convert DynamicFrame to DataFrame data_frame = dynamic_frame.toDF() # Write DataFrame to S3 as partitioned Parquet files data_frame.write.mode(\"overwrite\"). partitionBy( \"country\", \"state\", \"city\").parquet(output_s3_path)\n\nvii. Adding retry logic for data processing: A retry\n\nmechanism is added to ensure the job can retry up to\n\nthree times in case of failure. If the job succeeds, it\n\nsends a success metric to CloudWatch:\n\n# Maximum number of retries max_retries = 3 retries = 0 success = False while retries < max_retries and not success: try: # Create a DynamicFrame from the table dynamic_frame = glueContext.create_dynamic_frame.fr\n\nom_catalog( database=database_name, table_name=table_name ) # Convert DynamicFrame to DataFrame data_frame = dynamic_frame.toDF() # Write DataFrame to S3 as partitioned Parquet files\n\ndata_frame.write.mode(\"overwrite\"). partitionBy( \"country\", \"state\", \"city\").parquet(output_s3_path) success = True\n\nput_metric_data('JobSuccess', 1)\n\nviii. Handling errors and sending failure metrics: If an error\n\noccurs, the job will retry aer a 30-second wait. If the\n\nmaximum retries are reached, a failure metric is\n\nsent to CloudWatch, and an exception is raised:\n\nexcept Exception as e: retries += 1 if retries < max_retries: time.sleep(30) # Wait for 30 seconds before retrying else:\n\nput_metric_data('JobFailure', 1) raise e\n\nix. Committing the job: e job.commit() call\n\n\u0000nalizes the Glue job, ensuring that all changes are\n\nsaved and the job is properly completed:\n\njob.commit()\n\nOn the MWAA UI, your MWAA DAG will be visible in a few\n\nminutes as Auto-refresh is enabled:\n\nFigure 4.23 – MWAA DAGs\n\nOn clicking on DAG, you can see Orchestration in the Flow graph:\n\nFigure 4.24 – MWAA DAG\n\nSee also\n\nTo learn more about Apache Air\u0000ow and MWAA, you can refer to the\n\nfollowing links:\n\nhttps://air\u0000ow.apache.org/docs/apache-\n\nair\u0000ow/stable/index.html\n\nhttps://docs.aws.amazon.com/mwaa/latest/userguide/what-is-\n\nmwaa.html\n\nWhat Is Amazon Managed Work\u0000ows for Apache Air\u0000ow?:\n\nhttps://docs.aws.amazon.com/mwaa/latest/userguide/what-is-\n\nmwaa.html\n\nApache Air\u0000ow tutorial: https://air\u0000ow.apache.org/docs/apache-\n\nair\u0000ow/stable/tutorial/index.html\n\nInstalling custom plugins:\n\nhttps://docs.aws.amazon.com/mwaa/latest/userguide/con\u0000guring-dag-\n\nimport-plugins.html\n\nInstalling Python dependencies:\n\nhttps://docs.aws.amazon.com/mwaa/latest/userguide/working-dags-\n\ndependencies.html\n\nMonitoring your pipeline’s health\n\nTo create a robust monitoring and alerting system for your AWS\n\nGlue pipeline, you can use Amazon CloudWatch to emit success\n\nmetrics, create alarms, and monitor the health of your pipeline.\n\nWe’ll use CloudWatch Metrics, CloudWatch alarms, and Amazon\n\nSNS for noti\u0000cations.\n\nHow to do it…\n\n1. Modify the AWS Glue job to emit success metrics:\n\nI. Update the Glue ETL script: Modify your\n\ncsv_to_parquet.py script to include the following code to\n\nemit custom metrics:\n\nimport sys import boto3 # Emit success metric to CloudWatch cloudwatch = boto3.client('cloudwatch') cloudwatch.put_metric_data( Namespace='GluePipelineMetrics', MetricData=[ { 'MetricName': 'JobSuccess', 'Dimensions': [ { 'Name': 'JobName', 'Value': args['JOB_NAME'] }, ], 'Value': 1, 'Unit': 'Count' }, ])\n\nII. Upload the updated script to your S3 bucket using the following\n\nAWS CLI command:\n\naws s3 cp csv_to_parquet.py s3://your- bucket/scripts/csv_to_parquet.py\n\n2. Create CloudWatch alarms for metrics:\n\nI. Create an SNS topic for noti\u0000cations:\n\ni. Go to the Amazon SNS console at\n\nhttps://console.aws.amazon.com/sns/.\n\nii. Click on Topic and then click on Create topic.\n\niii. Choose Standard and enter a name for your topic, for\n\nexample, GluePipelineAlerts.\n\niv. Click on Create topic.\n\nv. Copy the topic ARN (for example,\n\narn:aws:sns:region:account-\n\nid:GluePipelineAlerts).\n\nvi. Click on Create subscription.\n\nvii. Set Protocol as Email and enter your email address.\n\nviii. Click on Create subscription.\n\nII. Create CloudWatch alarms:\n\ni. Go to the Amazon CloudWatch console at\n\nhttps://console.aws.amazon.com/cloudwatch/.\n\nii. Navigate to Alarms and click on Create alarm.\n\niii. Click on Select metric in cloudWatch.\n\niv. Choose Glue from the list of services.\n\nv. en, select CloudWatch Metrics to view the available\n\nmetrics for your Glue jobs.\n\nvi. Choose CsvToParquetJob.\n\nvii. Click on Select metric and proceed to con\u0000gure the\n\nalarm.\n\nviii. Con\u0000gure the threshold type to Static and set the\n\ncondition to reshold < 1 for 1 consecutive period(s).\n\nix. Set the period to match your Glue job’s frequency (for\n\nexample, 1 hour).\n\nx. Click on Next.\n\nxi. Con\u0000gure the actions as follows:\n\nChoose In alarm and select Send noti\u0000cation\n\nto.\n\nSelect your SNS topic, GluePipelineAlerts\n\n(created previously in Create an SNS topic for\n\nnoti\u0000cations).\n\nxii. Click on Next.\n\nxiii. Name your alarm (for example,\n\nGlueJobSuccessAlarm).\n\nxiv. Click on Create alarm.\n\nYou can \u0000nd all alerts in the CloudWatch UI:\n\nFigure 4.25 – Cloudwatch alarms\n\n3. Monitor queue size metrics: To monitor the size of queues that are part\n\nof your data pipeline, you might be using Amazon SQS or other queue\n\nservices. Here, we’ll focus on monitoring an SQS queue:\n\nI. Create SQS queue (if not already created):\n\ni. Go to the Amazon SQS console at\n\nhttps://console.aws.amazon.com/sqs/.\n\nii. Click on Create queue.\n\niii. Enter the queue name (for example,\n\nGluePipelineQueue).\n\niv. Con\u0000gure the queue settings as required.\n\nv. Click on Create queue.\n\nII. Monitor SQS queue metrics:\n\ni. Go to the Amazon CloudWatch console at\n\nhttps://console.aws.amazon.com/cloudwatch/.\n\nii. Navigate to Metrics.\n\niii. Choose Metrics and select the SQS namespace.\n\niv. Select the metrics for your queue (for example,\n\nApproximateNumberOfMessagesVisible).\n\nv. Create a dashboard to monitor these metrics by\n\nclicking on Add to dashboard.\n\nvi. Con\u0000gure the dashboard with relevant widgets to\n\nmonitor your queue size.\n\nIII. Create CloudWatch alarms for SQS queue:\n\ni. Go to the Amazon CloudWatch console at\n\nhttps://console.aws.amazon.com/cloudwatch/.\n\nii. Navigate to Alarms and click on Create alarm.\n\niii. Click on Select metric and choose the SQS namespace.\n\niv. Select the\n\nApproximateNumberOfMessagesVisible\n\nmetric for your queue.\n\nv. Click on Select metric and proceed to con\u0000gure the\n\nalarm.\n\nvi. Con\u0000gure the threshold type to Static and set the\n\ncondition (for example, reshold > 100 for 5\n\nconsecutive period(s)).\n\nvii. Set the period (for example, 1 minute).\n\nviii. Click on Next.\n\nix. Con\u0000gure the actions as follows:\n\nChoose In alarm and select Send noti\u0000cation\n\nto.\n\nSelect your SNS topic, GluePipelineAlerts.\n\nx. Click on Next.\n\nxi. Name your alarm (for example,\n\nSQSQueueSizeAlarm).\n\nxii. Click on Create alarm.\n\nYou can \u0000nd all alerts in CloudWatch UI:\n\nFigure 4.26 – CloudWatch alarm SQS\n\nTo monitor AWS Glue job performance eﬀectively, setting up\n\nCloudWatch alarms with SNS for AWS Glue metrics provides a\n\nproactive way to track and manage your data pipeline. By\n\ncon\u0000guring alarms for key Glue job metrics (for example, job\n\nfailures or long runtimes) and integrating them with SNS, you can\n\nreceive instant noti\u0000cations when thresholds are exceeded. is\n\nensures that any issues in your pipeline are addressed promptly,\n\nmaintaining the reliability and health of your data processes.\n\nSetting up a pipeline using AWS Glue to ingest data from a JDBC database into a catalog table\n\nCreating a full pipeline using AWS Glue to ingest data from a\n\nrelational database on a regular basis involves setting up the\n\nnecessary components such as a Glue job, Glue crawler, and a retry\n\nmechanism to handle transient errors. In this recipe, we are going\n\nto use the AWS Glue job with EventBridge and Step Functions\n\nwork\u0000ow. We will read data from a relational database and store it\n\nin an S3 bucket.\n\nHow to do it…\n\n1. Set up your environment:\n\nI. Use your existing S3 bucket or create a new one. (To create a\n\nnew S3 bucket, navigate to the S3 service in the AWS\n\nManagement Console, click on Create bucket, and specify a\n\nunique name. Choose the region and con\u0000gure settings such as\n\nversioning or encryption as needed, then click on Create.)\n\nII. Create an RDS MySQL instance (please use the following link\n\nand follow the given instructions:\n\nhttps://aws.amazon.com/getting-started/hands-on/create-\n\nmysql-db/).\n\nIII. Once the database is created, download mysql-\n\nscripts.txt, connect it to your RDS (MySQL) instance,\n\nand run it.\n\nFigure 4.27 – MySQL editor with query and results\n\n2. Create IAM roles and policies:\n\nI. Create an IAM policy for Glue:\n\ni. Go to the IAM console at\n\nhttps://console.aws.amazon.com/iam/.\n\nii. Navigate to Policies and click on Create policy.\n\niii. Select the JSON tab and paste the policy document.\n\nPlease use the same policy and role we created in the\n\nDe\u0000ning a simple work\u0000ow using AWS Glue work\u0000ows\n\nrecipe.\n\nII. Create an IAM role for Glue:\n\ni. Go to the IAM console, navigate to Roles, and click on\n\nCreate role.\n\nii. Select AWS service and then choose Glue.\n\niii. Click on Next: Permissions.\n\niv. Attach the policy you just created (use the same policy\n\nas in De\u0000ning a simple work\u0000ow using AWS Glue\n\nwork\u0000ows).\n\nv. Click on Next: Tags, optionally add tags, then click on\n\nNext: Review.\n\nvi. Name your role AWSGlueServiceRole. Use the\n\nsame role as in De\u0000ning a simple work\u0000ow using AWS\n\nGlue work\u0000ows.\n\nvii. Click on Create role.\n\n3. Create an AWS Glue database:\n\nI. Open the AWS Glue console at\n\nhttps://console.aws.amazon.com/glue/.\n\nII. Create a database with the help of the following steps:\n\ni. Navigate to the Databases section.",
      "page_number": 294
    },
    {
      "number": 16,
      "title": "Segment 16 (pages 314-331)",
      "start_page": 314,
      "end_page": 331,
      "detection_method": "synthetic",
      "content": "ii. Click on Add database.\n\niii. Name your database relational_database.\n\n4. Create an AWS Glue connection:\n\nI. Create Glue connection:\n\ni. Go to the AWS Glue console at\n\nhttps://console.aws.amazon.com/glue/.\n\nii. Navigate to Connections and click on Add connection.\n\niii. Choose the connection type as JDBC and click on\n\nNext.\n\niv. Fill in the connection details, including the JDBC\n\nURL, username, and password for your relational\n\ndatabase.\n\nv. Name your connection glue-rds-Jdbc-\n\nconnection.\n\nTest your connection using Test Connection:\n\nFigure 4.28 – Testing your connection\n\nWhen the connection is successful, you will see the following\n\nmessage:\n\nFigure 4.29 – JDBC connection test\n\nvi. Click on Finish.\n\n5. Create AWS Glue crawler:\n\nI. Create Glue crawler:\n\ni. Go to the Crawlers section in the Glue console.\n\nii. Click on Add crawler.\n\niii. Name your crawler RelationalDataCrawler.\n\niv. For the data store, choose JDBC and select your\n\nconnection, glue-rds-Jdbc-connection.\n\nv. Include the EMPLOYEE_DB/EMPLOYEE% path and\n\nclick on the Add datasource button.\n\nvi. Specify the database and table name to crawl.\n\nvii. Con\u0000gure the IAM role to use\n\nAWSGlueServiceRole.\n\nviii. Set the frequency to run on demand.\n\nix. Create or select the relational_database\n\ndatabase where the crawler results will be stored.\n\nx. Review and create the crawler.\n\n6. Create an AWS Glue job with the retry mechanism:\n\nI. Download the jdbc_to_s3.py script from GitHub and\n\nupload it to your S3 bucket:\n\ni. Importing libraries and de\u0000ning the metric function:\n\nis section imports the necessary libraries such as\n\nawsglue, pyspark, boto3, and time. e\n\nput_metric_data function allows us to send\n\ncustom metrics (for example, job success or failure) to\n\nAmazon CloudWatch to monitor job performance:\n\nimport sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job import boto3 import time def put_metric_data(metric_name, value): cloudwatch = boto3.client('cloudwatch')\n\ncloudwatch.put_metric_data( Namespace='GlueIngestion', MetricData=[ { 'MetricName': metric_name, 'Value': value, 'Unit': 'Count' }, ] )\n\nii. Setting up the Glue and Spark context: is block\n\ninitializes the Spark and Glue contexts. e\n\ngetResolvedOptions function fetches the job\n\nname passed as a command-line argument, and Job is\n\ninitialized for tracking and managing the Glue job:\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME']) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args['JOB_NAME'], args)\n\niii. De\u0000ning the data source and destination: database\n\nand table_name specify the Glue Data Catalog from\n\nwhich the data is pulled. output_s3_path points\n\nto the Amazon S3 bucket where the ingested data will\n\nbe written in the Parquet format:\n\ndatabase = \"relational_data\" table_name = \"your_table_name\" output_s3_path = \"s3://your-glue- data-bucket/ingested-data/\"\n\niv. Implementing retry logic for data ingestion: e retry\n\nmechanism allows the job to retry up to three times if\n\nit encounters an issue. A DynamicFrame is created\n\nfrom the Glue Data Catalog, which enables handling\n\nsemi-structured data:\n\nmax_retries = 3 retries = 0 success = False while retries < max_retries and not success: try: dynamic_frame = glueContext.create_dynamic_frame.fr om_catalog( database=database, table_name=table_name,\n\ntransformation_ctx=\"dynamic_frame\" )\n\nv. Converting data and writing to S3: e DynamicFrame\n\nis converted into a Spark DataFrame for easier\n\nmanipulation. e DataFrame is written to S3 in the\n\nParquet format, partitioned by the speci\u0000ed columns.\n\nOn success, a metric is sent to CloudWatch:\n\ndata_frame = dynamic_frame.toDF()\n\ndata_frame.write.mode(\"overwrite\"). parquet(output_s3_path) success = True put_metric_data('JobSuccess', 1)\n\nvi. Handling exceptions and sending failure metrics: If an\n\nexception occurs, the retry count is incremented, and\n\nthe process waits for 30 seconds before trying again. If\n\nthe retries are exhausted, a failure metric is sent to\n\nCloudWatch, and an exception is raised:\n\nexcept Exception as e: retries += 1 if retries < max_retries: time.sleep(30) # Wait for 30 seconds before retrying else:\n\nput_metric_data('JobFailure', 1) raise e\n\nvii. Finalizing the Glue job: e job.commit() method\n\ncommits the job, marking its completion in the AWS\n\nGlue environment, ensuring that all job metadata is\n\n\u0000nalized:\n\njob.commit()\n\nII. Create the Glue job:\n\ni. Go to the Jobs section in the Glue console.\n\nii. Click on Add Job.\n\niii. Name the job JdbcToS3Job.\n\niv. Choose the GlueServiceRole IAM role.\n\nv. For the ETL language, select Python.\n\nvi. For the script, choose A new script to be authored by\n\nyou.\n\nvii. Set the script \u0000lename to s3://your-glue-\n\ndata-bucket/scripts/jdbc_to_s3.py.\n\nviii. Set the temporary directory to s3://your-glue-\n\ndata-bucket/temp/.\n\nix. Click on Next and con\u0000gure the job properties as\n\nneeded.\n\nx. Click on Save.\n\n7. Create an AWS Step Functions state machine:\n\nI. Download and use the following state machine JSON de\u0000nition\n\nto coordinate the work\u0000ow:\n\ni. Initial setup and work\u0000ow start: e state machine\n\nstarts with the StartAt directive, which speci\u0000es the\n\n\u0000rst step, \"StartCrawler\". \"StartCrawler\" is\n\na Task state that uses the glue:startCrawler\n\nresource to start an AWS Glue crawler named\n\n\"RelationalDataCrawler\":\n\n{ \"Comment\": \"A description of my state machine\", \"StartAt\": \"StartCrawler\", \"States\": { \"StartCrawler\": { \"Type\": \"Task\", \"Resource\":\n\n\"arn:aws:states::aws- sdk:glue:startCrawler\", \"Parameters\": { \"Name\": \"RelationalDataCrawler\" },Next\": \"WaitForCrawler\" },\n\nii. Adding a wait step: e \"WaitForCrawler\" state\n\nis a Wait state that pauses the work\u0000ow for 60\n\nseconds before proceeding to the next step. is\n\nensures the state machine allows the Glue crawler\n\nenough time to start and process the data:\n\n\"WaitForCrawler\": { \"Type\": \"Wait\", \"Seconds\": 60, \"Next\": \"CheckCrawlerStatus\" },\n\niii. Checking the crawler status: e\n\n\"CheckCrawlerStatus\" state is another Task\n\nstate – this time, using the glue:getCrawler\n\nresource to retrieve the current status of the crawler.\n\ne crawler’s status is critical to determine whether it\n\nhas \u0000nished, is still running, or encountered an error:\n\n\"CheckCrawlerStatus\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:states:::aws- sdk:glue:getCrawler\", \"Parameters\": { \"Name\":\n\n\"RelationalDataCrawler\" }, \"Next\": \"CrawlerComplete?\" },\n\niv. Conditional check for crawler completion: e\n\n\"CrawlerComplete?\" state is a Choice state that\n\nevaluates the status of the crawler. If the status is\n\n\"READY\", it means the crawler has \u0000nished and the\n\nwork\u0000ow proceeds to the next task,\n\n\"StartGlueJob\". If not, it defaults back to the\n\n\"WaitForCrawler\" state to wait and check again:\n\n\"CrawlerComplete?\": { \"Type\": \"Choice\", \"Choices\": [ { \"Variable\": \"$.Crawler.State\", \"StringEquals\": \"READY\", \"Next\": \"StartGlueJob\" } ], \"Default\": \"WaitForCrawler\" },\n\nv. Starting the Glue job: e \"StartGlueJob\" state is\n\nthe \u0000nal Task that triggers the AWS Glue job using\n\nthe glue:startJobRun resource. e job named\n\n\"JdbcToS3Job\" is started to process and load the\n\ndata, and the state machine ends aer this step\n\n(\"End\": true). is state machine orchestrates a\n\nsimple data processing pipeline using AWS Glue. It\n\n\u0000rst starts a Glue crawler to scan the data, waits for the\n\ncrawler to complete, and then triggers a Glue ETL job\n\nto process the ingested data. By using Task, Wait,\n\nand Choice states, the work\u0000ow is controlled and\n\nmonitored eﬃciently:\n\n\"StartGlueJob\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:states:::aws- sdk:glue:startJobRun\", \"Parameters\": { \"JobName\": \"JdbcToS3Job\" }, \"End\": true } }}\n\nII. Create the state machine:\n\ni. Go to the AWS Step Functions console at\n\nhttps://console.aws.amazon.com/states/.\n\nii. Click on Create state machine.\n\niii. Choose Author with code snippets.\n\niv. Name your state machine\n\nGlueIngestionStateMachine.\n\nv. Paste the state machine de\u0000nition into the editor.\n\nvi. Choose the step-functions-role execution\n\nrole.\n\nvii. Click on Create state machine.\n\n8. Create an EventBridge rule to trigger the state machine:\n\nI. Create an EventBridge rule:\n\ni. Go to the Amazon EventBridge console at\n\nhttps://console.aws.amazon.com/events/.\n\nii. Click on Create rule.\n\niii. Name your rule S3FileUploadRule.\n\niv. Choose Event Source and select Event Pattern.\n\nv. Click on Edit and specify the following JSON event\n\npattern:\n\n{ \"source\": [\"aws.s3\"], \"detail-type\": [\"Object Created\"], \"detail\": { \"bucket\": { \"name\": [\"your-bucket\"] }, \"object\": { \"key\": [{\"prefix\": \"csv- data/\"}] } } }\n\nNOTE\n\nReplace your-bucket with your bucket name.\n\nvi. Click on Save.\n\nvii. In the Targets section, click on Add target\n\nIn AWS Step Functions Studio, you will \u0000nd the following visual\n\nrepresentations:\n\nFigure 4.30 – Step Functions workﬂow\n\nOnce your execution completes, you can see all the steps in green,\n\nand if you click on a step and de\u0000nition, you can see all inputs,\n\noutputs, de\u0000nitions, events, and so on.\n\nFigure 4.31 – Step Functions with Glue\n\nFinally, you can go into your S3 bucket and validate the data created\n\ninside the ingetion-data folder:\n\nFigure 4.32 – Results in the S3 bucket\n\nIn conclusion, setting up a pipeline using AWS Glue to ingest data\n\nfrom a JDBC database into a catalog table streamlines the process of\n\nextracting, transforming, and loading data into a centralized data\n\ncatalog. is enables eﬃcient data management, making it easier to\n\nquery, analyze, and integrate with other AWS services for scalable\n\nanalytics solutions.\n\nOceanofPDF.com\n\n5 Running Big Data Workloads with Amazon EMR\n\nAmazon EMR is a managed service that allows running big data\n\nframeworks such as Apache Spark or Apache Hive on the Apache\n\nHadoop ecosystem. It provides clusters for data applications to\n\nhandle large amounts of data in a distributed and scalable way.\n\nEMR removes the complexity of having to deploy, con\u0000gure, and\n\ncoordinate all these open source frameworks and tools to work\n\ntogether, so you can just start using them. Each version of EMR lists\n\nall the speci\u0000c frameworks and the speci\u0000c versions it provides.\n\nUnlike other AWS-managed services, EMR allows you to have full\n\ncontrol and visibility of your cluster: which hardware to run, which\n\nEC2 image to use, what to install, and even root access to the cluster\n\n(except when you run on EMR serverless mode). e recipes in this\n\nchapter will help you learn the EMR capabilities and how to make\n\nthe best use of them.\n\nis chapter includes the following recipes:\n\nRunning jobs using AWS EMR serverless\n\nRunning your AWS EMR cluster on EKS\n\nUsing the AWS Glue catalog from another account\n\nMaking your cluster highly available\n\nScaling your cluster based on workload\n\nCustomizing the cluster nodes easily using bootstrap actions\n\nTuning Apache Spark resource usage\n\nCode development on EMR using Workspaces\n\nMonitoring your cluster\n\nProtecting your cluster from security vulnerabilities\n\nTechnical requirements\n\ne recipes in this chapter assume that you have a Bash shell or\n\nequivalent available with the AWS CLI installed\n\n(https://docs.aws.amazon.com/cli/latest/userguide/getting-started-\n\ninstall.html) with access to AWS. If using Microso Windows, you\n\ncan enable WSL (https://learn.microso.com/en-\n\nus/windows/wsl/install) or install Git (https://git-\n\nscm.com/downloads) to use the Bash shell that it brings.\n\nMost commands are too long to list in the book line width. To \u0000t\n\nthem in, lines have been broken using the \\ character, which tells\n\nthe shell that the command continues in the next line. Be careful to\n\nrespect the spaces (or lack of) on the line breaks.\n\nIt is also assumed that you have con\u0000gured the default credentials\n\nand region using aws configure or using an AWS CLI pro\u0000le.\n\nYou need to have permission to manage EMR clusters. Also, most\n\nrecipes will expect you to have a user with access to the AWS EMR\n\nconsole using a web browser. It’s easier if you have a test account on\n\nwhich you can use an admin user to access the console and the\n\ncommand line; otherwise, you’ll need to grant permissions as\n\nneeded.\n\nMost recipes create an EMR cluster that requires a subnet and an S3\n\ndirectory to store logs. For simplicity, it’s easier if the subnet is\n\npublic and the default security groups haven’t been customized, or if\n\nthey have, that they are secured. If you want to run on a private\n\nsubnet, you will have to meet some further requirements (see\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-\n\nclusters-in-a-vpc.html#emr-vpc-private-subnet).\n\nFor the log’s path, choose a bucket in the account and region or\n\ncreate one.\n\ne recipes that run AWS CLI commands will indicate the speci\u0000c\n\nvariables that needs to be set. You can de\u0000ne them as needed when\n\nyou open the shell or export them on your user pro\u0000le so that they\n\nare available on every shell you create.\n\nMany recipes will ask you to de\u0000ne the following variables in your\n\nshell, so it’s better if you have them de\u0000ned beforehand. Pick or\n\ncreate an S3 bucket in the region you intend to use:\n\nSUBNET=<enter the subnet for EMR to use here> # Don't use a trailing / in the S3 urls S3_SCRIPTS_URL=<enter an s3:// path where to store scripts> S3_LOGS_URL=<enter an s3:// path where to store logs>\n\nIt is recommended that you set the Bash \u0000ag to warn you if a\n\nvariable is missing:\n\nset -u\n\ne recipes try to be frugal and create small clusters that suﬃce for\n\nthe demonstration while using commonly available nodes; if that\n\ninstance type doesn’t happen to be available in your region, please\n\nreplace them with the most similar instance type available at the\n\ntime.\n\nSome recipes will expect you to have a keypair .pem \u0000le, created in\n\nthe region, which you can do in the EC2 console\n\n(https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/create-\n\nkey-pairs.html#having-ec2-create-your-key-pair).\n\nWhen the keypair is created, it will download a .pem \u0000le with the\n\nname of the key. Change the permissions so only your user can read",
      "page_number": 314
    },
    {
      "number": 17,
      "title": "Segment 17 (pages 332-351)",
      "start_page": 332,
      "end_page": 351,
      "detection_method": "synthetic",
      "content": "it (for instance, by running cmhod 400 mykey.pem). Create a\n\nvariable in the shell with the key name:\n\nKEYNAME=<just the key name, no .pem extension>\n\nYou can \u0000nd the code \u0000les for this chapter on GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter05.\n\nRunning jobs using AWS EMR serverless\n\nEMR serverless was created for the common case where the user\n\njust wants to run Spark and Hive jobs without having to worry\n\nabout the type of nodes, capacity, and con\u0000guration.\n\nFor such cases, EMR serverless really simpli\u0000es the operation, since\n\nit does not require a cluster to be con\u0000gured or maintained. You do\n\nnot have to worry about which kind of EC2 is the right one or\n\nwhether it is going to be present (and available in enough capacity)\n\nfor your chosen region and Availability Zone. e main trade-oﬀ is\n\nthat you can no longer ssh into nodes to do low-level\n\nadministration and troubleshooting.\n\nIn this recipe, you will see how simple it is to run a Spark\n\napplication using EMR serverless.\n\nGetting ready\n\nTo test serverless, you will need a sample script to run. e\n\nfollowing script is a basic example that accesses the Glue catalog\n\nfrom the EMR serverless job. In the shell, run the following\n\ncommand to create a Python \u0000le with the script that follows:\n\ncat <<EOF > testEMRServerless.py from pyspark.sql import SparkSession spark = (SparkSession.builder.config( \"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.\\ AWSGlueDataCatalogHiveClientFactory\") .enableHiveSupport() .getOrCreate()) spark.sql(\"SHOW TABLES\").show() EOF\n\nUpload the \u0000le to S3 in a bucket on the same region as you intend to\n\nuse Glue (otherwise, you would need a VPC for EMR serverless to\n\ndownload it).\n\nHow to do it…\n\n1. Open the AWS console on EMR and select EMR Serverless in the le\n\nmenu.\n\n2. Use the Get Started button to set up EMR Studio so it logs you in and\n\nopens the application creation screen. If you had previously set EMR\n\nStudio up in the region, then select Manage application, and then Create\n\napplication in the applications screen.\n\n3. In the application creation screen, enter the name SparkTest. Make\n\nsure that the type is Spark and leave the rest as defaults.\n\n4. Select Create and start application at the bottom.\n\n5. In the Applications list, use the SparkTest link to view the details,\n\nthen use the refresh button until the status becomes Started.\n\n6. Now that the serverless application is started, you can submit jobs. Select\n\nSubmit job run on the lower side of the Application details screen.\n\nFigure 5.1 – An EMR Studio application is created\n\n7. Select a runtime role. e dropdown will show the ones that are suitable\n\nfor EMR Serverless and allow you to create a new one. Choose one of\n\nthose.\n\nAt the time of this writing, the role is created without\n\npermission to use the Glue catalog. To allow this, use the link in\n\nthe creation con\u0000rmation message that appears to open a new\n\ntab. From the Add permissions dropdown, select Create inline\n\npolicy using this JSON policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"glue:CreateDatabase\", \"glue:UpdateDatabase\", \"glue:DeleteDatabase\", \"glue:GetDatabase\", \"glue:GetDatabases\", \"glue:CreateTable\", \"glue:UpdateTable\", \"glue:DeleteTable\", \"glue:GetTable\", \"glue:GetTables\", \"glue:GetTableVersions\", \"glue:CreatePartition\", \"glue:BatchCreatePartition\", \"glue:UpdatePartition\", \"glue:DeletePartition\", \"glue:BatchDeletePartition\",\n\n\"glue:GetPartition\", \"glue:GetPartitions\", \"glue:BatchGetPartition\",\n\n\"glue:CreateUserDefinedFunction\",\n\n\"glue:UpdateUserDefinedFunction\",\n\n\"glue:DeleteUserDefinedFunction\", \"glue:GetUserDefinedFunction\", \"glue:GetUserDefinedFunctions\" ], \"Resource\": [ \"arn:aws:glue:*:*:database/*\", \"arn:aws:glue:*:*:table/*/*\", \"arn:aws:glue:*:*:catalog\" ] } ] }\n\n8. In the script location \u0000eld, enter the S3 URL of the Python \u0000le that was\n\nuploaded earlier, as mentioned in the Getting ready section of this recipe.\n\n9. Once you \u0000nish editing the \u0000eld, it should detect it is a Python script\n\nand remove or empty the Main class \u0000eld.\n\n10. Leave the rest as default and submit it. e job will be listed in the Job\n\nruns tab and the status will change to Scheduled. Aer a minute or so, it\n\nshould change to Running. You can use the refresh button to refresh the\n\nstatus. If you have made a mistake, such as lacking permissions, you\n\ncould clone the job in the table to submit it again quickly.\n\n11. When the job ends successfully, you can view it from the application\n\nscreen listing the jobs and the logs (stdout and stderr). In this\n\nexample, opening the stdout log should print the list of the tables in\n\nthe catalog default database on the current region.\n\nFigure 5.2 – The Spark application runs\n\nHow it works…\n\nWhen the job is submitted, EMR creates a cluster taking care of all\n\nthe con\u0000guration, instance selection, and capacity. is means that\n\nwe only have to provide the PySpark script or the executable JAR if\n\nwe are using Scala or Java.\n\nIt’s possible to avoid this scheduling delay by enabling the pre-\n\ninitialized capacity. With that con\u0000guration, you can set a\n\nminimum capacity to be available when the application is started,\n\nso it’s ready to handle jobs promptly. en EMR will scale the\n\ncapacity as needed.\n\nSpark can use the Glue catalog because, by default, the AWS Glue\n\nData Catalog as metastore con\u0000guration setting is enabled for the\n\njobs. is in turn sets a Spark property that tells Spark to use Glue.\n\nAs you have seen, in EMR serverless, you can submit Spark jobs in\n\nan equivalent way to how you would start it using spark-submit\n\nin a traditional EMR cluster or other Spark environment. You can\n\nread more about it in the Spark documentation:\n\nhttps://spark.apache.org/docs/latest/submitting-applications.html.\n\nIf instead of Python, you wrote your Spark job using Scala or Java,\n\nthen instead of the path to the script on S3, you would specify the\n\nS3 path to the JAR with the job and dependencies, and then on the\n\nSpark properties tab you would specify the main class using --\n\nclass, just like you would do if you were using spark-submit.\n\nYou can experiment with this by selecting the Submit sample job\n\noption in the application screen, which creates a job that runs a\n\nsample PI calculation using the JAR provided by EMR.\n\nThere’s more…\n\nInside the job run, you can easily access SparkUI, both while\n\nrunning and aer the job has completed.\n\nYou can streamline running the apps if you con\u0000gure the app to\n\nauto-start when a job is submitted. You can also avoid the startup\n\nlag by adding provisioned nodes (at minimum a driver and an\n\nexecutor).\n\nIn addition, you can con\u0000gure the application to auto-stop if it has\n\nbeen idle for a con\u0000gured amount of time.\n\nFor more information, you can visit\n\nhttps://docs.aws.amazon.com/emr/latest/EMR-Serverless-\n\nUserGuide/application-capacity.html.\n\nSee also\n\nAWS Glue is an alternative solution to run Spark jobs in a managed\n\nenvironment, with value features such as bookmarks and incremental\n\nprocessing. You can experience these features with the Processing data\n\nincrementally using bookmarks and bounded execution recipe in Chapter\n\n3, Ingesting and Transforming Your Data with AWS Glue.\n\nRunning your AWS EMR cluster on EKS\n\nEMR oﬀers the option of running a fully managed Spark cluster\n\nleveraging Kubernetes.\n\nInstead of running a full Hadoop cluster with YARN and HDFS,\n\nEMR on Elastic Kubernetes Service (EKS) is a lightweight solution\n\nto run Spark applications with a reduced start time, better resource\n\nutilization, and better availability. It allows running a cluster with\n\nnodes from diﬀerent Availability Zones or even AWS Outposts (on\n\nthe customer data center).\n\nGetting ready\n\nYou need to have installed both the AWS and EKS CLI client tools.\n\nYou can follow the AWS instructions to install them on your\n\nmachine depending on your OS:\n\nAWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/getting-\n\nstarted-install.html\n\nEKS CLI: https://github.com/weaveworks/eksctl\n\nYou can verify that both tools are installed by running the following\n\ncommands:\n\naws --version eksctl version\n\nIf you haven’t already set up the credentials and region to use by\n\nrunning aws configure in the commands, we’ll assume that the\n\nregion you want to work with is the default region set.\n\nHow to do it...\n\n1. On the command line, run the following command:\n\neksctl create cluster --name emr-recipe2 \\ --instance-types=m5.xlarge --managed\n\n2. Wait for a few minutes while the two CloudFormation templates are\n\ndeployed: one for the cluster and another for the managed node group\n\nfor the cluster.\n\nTIP:\n\nYou can see progress details on the AWS CloudFormation console.\n\nYou might see an error such as kubectl not found. If the Kubernetes\n\nclient is available, it will be used to check the cluster, but this is not required.\n\n3. Check the capacity status by running the following:\n\neksctl get nodegroup --cluster emr-recipe2\n\nNotice that these nodes are being charged, so if you are not\n\ngoing to complete the recipe (including the cleanup) at this time,\n\nyou can manually adjust this capacity using eksctl scale\n\nnodegroup. See the ere’s more… section of this recipe for how\n\nto automate scaling.\n\n4. Allow the EMR cluster to be used as a Kubernetes namespace:\n\neksctl create iamidentitymapping --cluster emr-recipe2\\ --namespace default --service-name \"emr- containers\"\n\n5. Create an EMR virtual cluster using the EKS cluster:\n\naws emr-containers create-virtual-cluster \\ --name vc_emr_recipe2 --container-provider \\ '{\"id\": \"emr-recipe2\", \"type\": \"EKS\", \"info\": {\"eksInfo\": {\"namespace\": \"default\"}}}'\n\n6. De\u0000ne variables with the cluster id and role arn.\n\nVIRTUAL_CLUSTER_ID=$(aws emr-containers \\ list-virtual-clusters --output text --query \\ \"sort_by(virtualClusters, &createdAt)[-1].id\") ROLE_ARN=$(eksctl get \\ iamidentitymapping --cluster emr-recipe2 | \\ grep emr-containers | cut -f 1)\n\n7. Prepare the job con\u0000guration (type the following without break lines\n\ninside quoted strings):\n\ncat > emr-recipe2.json << EOF { \"name\": \"EmrRecipe2-Pi\", \"virtualClusterId\": \"$VIRTUAL_CLUSTER_ID\", \"executionRoleArn\": \"$ROLE_ARN\", \"releaseLabel\": \"emr-6.10.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"local:///usr/lib/spark/examples/jars/spark- examples.jar\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=1 --conf spark.executor.memory=1G --conf spark.driver.memory=1G --class org.apache.spark.examples.SparkPi\" } } } EOF\n\n8. Launch the job:\n\naws emr-containers start-job-run --cli-input- json \\ file://./emr-recipe2.json\n\n9. Check the job run status. You can run this multiple times to get the\n\nstatus:\n\naws emr-containers list-job-runs \\ --virtual-cluster-id $VIRTUAL_CLUSTER_ID\n\n10. Clean up to avoid incurring further costs:\n\neksctl delete cluster --name emr-recipe2 aws emr-containers delete-virtual-cluster --id \\ ${VIRTUAL_CLUSTER_ID}\n\nHow it works...\n\nWhen you ran the eksctl create command, it created and\n\ndeployed a CloudFormation template that creates a cluster in EKS.\n\ne --managed option enabled the cluster to work with managed\n\nnode group, meaning that nodes of the speci\u0000ed type will be\n\nallocated and released as needed. It is recommended to use at least\n\nm5.xlarge or instances with more memory. Spark is likely to fail\n\nto run on smaller instances. is feature doesn’t have an extra cost;\n\nyou pay for what you use.\n\nYou can optionally download the Kubernetes client\n\n(https://docs.aws.amazon.com/eks/latest/userguide/install-\n\nkubectl.html). You can check the current usage by running the\n\nfollowing:\n\nkubectl get nodes\n\nFor simplicity, we used the\n\nAWSServiceRoleForAmazonEMRContainers role, which\n\nprovides the minimum permissions and trusts emr-containers\n\n(the name of EMR on EKS) to assume it. is is enough to run a\n\njob, but in a more realistic example, you would need your own role\n\nwith the same AmazonEMRContainersServiceRolePolicy and\n\ntrust relation, but with additional permissions to write to S3,\n\nCloudWatch, and possibly others depending on the job.\n\nWhen the job was submitted, it allocated nodes from the node\n\ngroup, retrieved the image indicated, and started one as the driver.\n\nerefore, with this method, Spark can only run on cluster mode,\n\nmeaning that the driver also runs in the remote cluster and not a\n\nlocal process (like you can do in normal EMR).\n\nThere’s more...\n\nOn the AWS console, you can navigate the EMR, select virtual\n\nclusters under EKS, and view the details of the run in the SparkUI.\n\nIt should look like this:\n\nFigure 5.3 – The SparkUI job run\n\nNotice that we didn’t select the Spark version directly. is is\n\nselected by the EMR version of the selected image. is way, each\n\njob in the virtual cluster can use a diﬀerent version if needed. is\n\nallows for more \u0000exibility than using a traditional EMR cluster.\n\nRemember that this way of running jobs allows you to ssh into the\n\nnodes (like a regular EMR cluster does). For that, you need to set up\n\na keypair ssh keys on EC2 and then use them when creating the\n\nEKS cluster by adding the --ssh-access and --ssh-public-\n\nkey parameters.\n\nIn this recipe, the provision of nodes was automated by using the\n\ndefault desired nodes, but you can use one of the autoscaling\n\nmethods to allow the job to adapt to the demand and grow and\n\nshrink as you submit workloads\n\n(https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.htm\n\nl).\n\nSee also\n\nUnless you have a speci\u0000c reason for using EKS (such as optimizing a\n\npool of capacity) to run a Spark job like in this recipe, it’s easier to use\n\nEMR serverless and that way simplify the capacity management. See\n\nhow to do this in the Running jobs using AWS EMR serverless recipe.\n\nUsing the AWS Glue catalog from another account\n\nApache Hive is the traditional SQL solution in the Hadoop\n\necosystem. Since its early versions, it has decoupled the catalog\n\nmetastore from the query engine to store information about the\n\ntables and schemas. is means that multiple tools have added\n\nsupport to integrate with the Hive metastore over the years such as\n\nSpark, Presto/Trino, Impala, or the Hive server itself.\n\nWhen AWS Glue was released, one of the value propositions that it\n\ncould provide was a Hive-compatible store, which could massively\n\nscale and provide fault tolerance out of the box.\n\nWhen you run EMR on EC2, you can run a Hive metastore or use\n\nthe Glue catalog as the metastore just by checking the\n\ncorresponding box in the cluster con\u0000guration screen (or setting\n\nthe equivalent con\u0000guration if doing programmatically). When\n\nGlue is set as the catalog metastore, all the tools in the cluster that\n\nare compatible with Hive will use it to retrieve and store\n\ninformation about the databases and tables. By default, it will use\n\nthe catalog on the same account (and region) as EMR is running.\n\nIn this recipe, you’ll see how to enable the Glue metastore and use\n\none from a diﬀerent account in the same region, as well as how to\n\nrun a PySpark script that prints the tables in the\n\nCrossCatalogRecipeDB database of the catalog.\n\nGetting ready\n\nFor this recipe, you ideally need two accounts: one running EMR\n\nand the other holding the catalog to use. You can also do it on the\n\nsame account if you don’t have multiple accounts; in that case, you\n\ncan skip the permissions step.\n\nIn the Bash shell, de\u0000ne the environment variables with the account\n\nIDs:\n\nEMR_ACCOUNT=<id of the account running EMR> CATALOG_ACCOUNT=<id of the account holding the\n\ncatalog> REGION=<id of the account holding the catalog>\n\ne recipe assumes that the AWS CLI credentials are for the EMR\n\naccount, while the catalog account will be accessed using the AWS\n\nconsole with a Lake Formation admin user (or a user that can create\n\none).\n\nis recipe assumes that you have de\u0000ned the SUBNET,\n\nS3_SCRIPTS_URL, and S3_LOGS_URL variables in the shell (see\n\nthe Technical requirements section at the beginning of the chapter\n\nfor more information).\n\nHow to do it...\n\n1. Using a text editor, create a text \u0000le named PrintTables.py with the\n\nfollowing content:\n\nfrom pyspark.sql import SparkSession spark = SparkSession.builder.enableHiveSupport( ).getOrCreate spark.sql(\"SHOW TABLES FROM cross_catalog_recipe\" ).show()\n\n2. Upload the script you created to S3:\n\naws s3 cp PrintTables.py $S3_SCRIPTS_URL/\n\n3. For this recipe, we are using plain IAM permissions and not Lake\n\nFormation. Let us verify whether it is disabled while you create the test\n\ndatabase and table.\n\nOn the AWS console, log in using the account you speci\u0000ed in\n\nthe CATALOG_ACCOUNT variable in Getting ready and go to AWS\n\nLake Formation. If this is the \u0000rst time you are using it, you will\n\nbe asked to assign an administrator.\n\nNavigate to Settings on the le menu and ensure that IAM\n\npermissions are enabled for new tables and databases (you can\n\nchange it back aer you create a database and table in the\n\nfollowing steps if you want to).\n\nFigure 5.4 – Lake Formation settings\n\n4. Navigate to Glue in the console and select Databases in the le menu,\n\nthen choose Create and name it cross_catalog_recipe. Leave\n\nthe rest as default.\n\n5. In the le menu, select Tables and then Add Table. Enter\n\ncross_account_table as the name and select\n\ncross_catalog_recipe as the database. Enter any S3 path\n\nlocation; it doesn’t matter, as we are not really going to read the table\n\ncontent. Continue to the next steps leaving everything as default until\n\nyou complete the table creation.\n\n6. Go back to the Bash shell and run the following command to generate\n\nthe policy based on your variables:\n\ncat << EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::${EMR_ACCOUNT}:role/EMR_EC2_Defa ultRole\" }, \"Action\": [ \"glue:GetDatabase\", \"glue:GetPartition\",\"glue:GetTables\", \"glue:GetPartitions\", \"glue:BatchGetPartition\", \"glue:GetDatabases\", \"glue:GetTable\", \"glue:GetUserDefinedFunction\", \"glue:GetUserDefinedFunctions\"], \"Resource\" : [ \"arn:aws:glue:$REGION:$CATALOG_ACCOUNT:catalog \", \"arn:aws:glue:$REGION:$CATALOG_ACCOUNT:databas e\\ /default\", \"arn:aws:glue:$REGION:$CATALOG_ACCOUNT:databas e\\ /cross_catalog_recipe\",\n\n\"arn:aws:glue:$REGION:$CATALOG_ACCOUNT:table\\ /cross_catalog_recipe/*\", \"arn:aws:glue:$REGION:$CATALOG_ACCOUNT:databas e\\ /global_temp\"] } ] } EOF\n\n7. Copy the JSON object printed by the previous step (all the lines from the\n\none starting with { to the one ending with }). In the AWS console,\n\nunder Glue, select Settings in the le menu, paste the policy in the\n\nPermissions textbox, and save the changes. If your policy is not empty,\n\nsave the existing policy to restore it aer completing the recipe, or use\n\nanother account or region. If you have mixed up the account IDs, it will\n\nreject the policy.\n\n8. Go back into the Bash shell and run the following command (note that\n\nthe last seven lines don’t have a \\ at the end because they are inside a\n\nmultiline string):\n\nMETASTORE_CLASS=\"com.amazonaws.glue.catalog.\\ metastore.AWSGlueDataCatalogHiveClientFactory\" aws emr create-cluster --name CrossAccountCatalog \\ --release-label emr-6.11.0 --instance-count 2 \\ --log-uri $S3_LOGS_URL --instance- type=m5.xlarge \\ --applications Name=Spark --use-default-roles \\ --ec2-attributes SubnetId=${SUBNET} --auto- terminate\\",
      "page_number": 332
    },
    {
      "number": 18,
      "title": "Segment 18 (pages 352-372)",
      "start_page": 352,
      "end_page": 372,
      "detection_method": "synthetic",
      "content": "--steps Type=Spark,Name=SparkPi,ActionOnFailure=\\ TERMINATE_CLUSTER,Args=[$S3_SCRIPTS_URL/\\ PrintTables.py] --configurations '[{\"Classification\": \"spark-hive-site\", \"Properties\": {\"hive.metastore.client.factory.class\": \"'$METASTORE_CLASS'\",\"hive.metastore.glue.cata logid\": \"'$CATALOG_ACCOUNT'\"}}]'\n\n9. If all is correct, the previous command will return a JSON with a cluster\n\nID and ARN. You can check periodically for the cluster status like this\n\n(use your ID):\n\naws emr describe-cluster --cluster-id j- XXXXXXXXX \\ | grep '\"State\"'\n\n10. Once the cluster is in the terminated state, retrieve the stdout of the\n\nstep (the Spark job) by running the following command (again, enter\n\nyour cluster ID instead of j-XXXXXXXXX). It should con\u0000rm that it has\n\ndownloaded a single \u0000le:\n\naws s3 cp $S3_LOGS_URL/j-XXXXXXXXX/steps/ . \\ --exclude '*' --include '*stdout.gz' -- recursive\n\n11. Print the contents of the zipped \u0000le (replace XXX with the path indicated\n\nby the previous command):\n\nzcat s-XXXXXXXXXXXXXXXXX/stdout.gz\n\nYou should get con\u0000rmation that Spark was able to list the tables\n\nin the catalog from the other account:\n\nFigure 5.5 – Job stdout listing tables\n\n12. e next step pertains to cleanup. If you no longer need them, delete the\n\ntable, catalog, and policy in the catalog account. You can also delete the\n\nlogs and the script \u0000le from S3:\n\naws s3 rm --recursive $S3_LOGS_URL aws s3 rm $S3_SCRIPTS_URL/PrintTables.py\n\nHow it works...\n\nIn this recipe, you \u0000rst created a trivial Spark script to demonstrate\n\nthat it can view the tables in a database on a diﬀerent account. en\n\nyou con\u0000gured the catalog account to allow EMR on the other\n\naccount to use a new database that was created for this purpose.\n\ne reason you were asked to check that only IAM permissions are\n\nset is for you to be able to use a simple catalog policy to give access\n\nwith a resource rules. When a database or table is created using an\n\nIAM only con\u0000guration, Lake Formation adds a special permission\n\nautomatically, which means that it delegates on the catalog IAM\n\npolicy instead of using what Lake Formation grants.\n\nYou can go to Lake Formation on the catalog account, select Data\n\nLake permissions on the le menu, and type\n\nIAMAllowedPrincipals in the table search box. is will show\n\nyou the databases and tables that are secured by the catalog policy\n\nrules. You can add this special principal permission to existing\n\ndatabases or tables that were created without IAM only enabled to\n\nachieve the same result.\n\nReview the policy that you added to the catalog. Notice that it gives\n\ntypical catalog permissions to multiple resources and that the\n\ncross_catalog_recipe database is listed twice, once as a\n\ndatabase resource and another to allow access to all tables in it\n\nusing a wildcard. In addition, the policy gives access to the default\n\nand global_temp databases. is is done for historical reasons\n\nbecause Spark will check those databases, for instance, to check\n\nwhether there are UDFs registered. is depends on the Spark\n\nversion.\n\nen you created a small cluster. Notice that there are two\n\nproperties in the con\u0000guration. e \u0000rst one enables the Glue\n\ncatalog client for the Spark Hive metastore; it is the equivalent of\n\nchecking the option to use the Glue catalog for Spark table\n\nmetadata in the UI:\n\nFigure 5.6 – EMR basic conﬁguration options\n\ne second property, hive.metastore.glue.catalogid, tells\n\nthe client to use another account catalog instead of using the one\n\nfrom the current account.\n\nFinally, by checking the output of the step, you can verify that Spark\n\ncan list the tables.\n\nThere’s more...\n\nYou can go ahead and create a table with real data and use it.\n\nHowever, notice that the catalog policy just gives you access to the\n\nmetadata. Spark will still need permission to access the actual data,\n\nnormally on S3. For that, you need to create a policy in the S3\n\nbucket granting access to the EMR role.\n\nLake Formation oﬀers the option to manage S3 locations, so when\n\naccess is granted to a user by Lake Formation, it also includes access\n\nto S3 (Lake Formation creates temporary credentials for S3 for that\n\nspeci\u0000c data). More information can be found in the oﬃcial\n\ndocumentation: https://docs.aws.amazon.com/lake-\n\nformation/latest/dg/access-control-underlying-data.html#data-\n\nlocation-permissions.\n\nSee also\n\nAn alternative way to access another account’s catalog (while retaining\n\naccess to the account’s catalog) is to enable a special feature available on\n\nEMR and Glue. By con\u0000guring the\n\naws.glue.catalog.separator=\"/\" property in the spark-\n\nhive-site cluster con\u0000guration, you can now reference a database in\n\nanother account (of course, you will still need to be granted permission\n\non the catalog). For instance, you can use the following command:\n\nSELECT * FROM `111122223333/default.mytable`;\n\nFor more information, you can refer to\n\nhttps://repost.aws/knowledge-center/query-glue-data-catalog-\n\ncross-account.\n\nMaking your cluster highly available\n\nHistorically, AWS EMR clusters were used for batch workloads and\n\ntorn down aerward; errors in the worker nodes would be handled\n\nby YARN and HDFS’ innate resiliency. ere was still the single\n\npoint of failure of the primary node (previously called master), in\n\nthe unlikely case that the whole batch process would need to be\n\nretried.\n\nHadoop has added full High Availability (HA) since the early days,\n\nas it was intended to run on permanent on-premise clusters that\n\nwould be shared by many users and teams.\n\nSince EMR 5.23, it allows running with multiple primary nodes.\n\nEMR takes care of the tedious process of correctly con\u0000guring\n\nHadoop to be HA. Over time, it has also improved the process of\n\nautomatically replacing a primary node and recon\u0000guring the\n\nsystem, so the cluster can graciously survive the failure of a single\n\nnode with minimal or no disruption to the cluster users.\n\nHA is important in cases where delays have a business impact, such\n\nas real-time systems or services where the users interact directly\n\nwith the cluster and an outage would directly impact productivity.\n\nNonetheless, this HA support has its limitations, and it is good to\n\nknow them. First, the cluster can only run on a single Availability\n\nZone (you will see later some ideas to mitigate this). Just because\n\nHadoop is HA does not mean every service, for instance a Presto\n\nserver, in the cluster is automatically HA.\n\nIn this recipe, you will see how straightforward it is to run a failure-\n\ntolerant cluster on EMR.\n\nGetting ready\n\nis recipe assumes that you have de\u0000ned the SUBNET variable and\n\nset up KEYNAME in the shell, as well as having the.pem key in the\n\ncurrent shell directory (see the Technical requirements section at the\n\nbeginning of the chapter for more information).\n\nHow to do it...\n\n1. Add the resource placement policy to the default EMR role. is role\n\nwill have automatically been created if you have run the other recipes in\n\nthis chapter in the same account and region. If the command fails\n\nbecause the role doesn’t exist, you can execute step 2 (which will fail) so\n\nit creates the role, then come back to execute the following command,\n\nand then continue again with step 2:\n\naws iam attach-role-policy --role-name \\ EMR_DefaultRole --policy-arn \\ arn:aws:iam::aws:policy/\\ AmazonElasticMapReducePlacementGroupPolicy\n\n2. In the shell, run the following command to create an HA cluster. If\n\neverything is correct, you should just see the cluster ID printed:\n\nCLUSTER_ID=$(aws emr create-cluster --release- label \\ emr-6.15.0 --use-default-roles --applications \\ Name=HBase --use-default-roles --ec2- attributes \\ SubnetId=$SUBNET,KeyName=$KEYNAME --instance- groups \\ InstanceGroupType=MASTER,InstanceCount=3,Insta nceType\\ =m5.xlarge InstanceGroupType=CORE,InstanceCount=1\\ ,InstanceType=m5.xlarge --placement-group- configs \\ InstanceRole=MASTER | grep ClusterArn \\ | grep -o 'j-.*[^\",]'); echo $CLUSTER_ID\n\n3. Check the status of the cluster by running the following command until\n\nit prints that the cluster is WAITING and both instance groups are\n\nRUNNING:\n\naws emr describe-cluster --cluster-id $CLUSTER_ID\n\nIf the status returned is TERMINATED_WITH_ERRORS, it’s\n\nprobably because the subnet and key name speci\u0000ed are not\n\nvalid for the region.\n\n4. By default, the security group created by EMR doesn’t allow SSH access.\n\nFirst, you need to \u0000nd the cluster ID (in case there are other clusters):\n\naws emr describe-cluster --cluster-id $CLUSTER_ID \\ | grep 'MasterSecurityGroup' | grep -o 'sg-.* [^\",]'\n\n5. Open the AWS console, search for the Security Groups feature in the top\n\nbar, and open it. In the list, \u0000nd the one matching the ID from the\n\nprevious step (it should be named ElasticMapReduce-master).\n\n6. Open the security group using the ID link. en, select Edit inbound\n\nrules | Add rule, and in the new rule on the dropdowns, select SSH and\n\nMy IP. Save the rule changes.\n\nFigure 5.7 – A security group’s new SSH rule\n\n7. Log in to the server using the .pem keypair. e command assumes that\n\nyou have the .pem \u0000le available in the current directory and with read\n\npermission just for you:\n\naws emr ssh --cluster-id $CLUSTER_ID\\ --key-pair-file $KEYNAME.pem\n\nIf the command results in a Connection timed out message, it\n\nmeans that the previous step wasn’t completed correctly or that\n\nthe IP detected by the browser is not the one that the shell is\n\nreally using to access AWS. In that case, you will need an\n\nalternative method to \u0000nd the IP to allow, for instance, running\n\nthe following in the shell:\n\ncurl ifconfig.me\n\n8. In the EMR shell that you just logged in to, run the following commands\n\nto con\u0000rm that the cluster is con\u0000gured in HA, with two or three servers\n\ndepending on the service:\n\ngrep -A 1 zookeeper.quorum \\ /etc/hadoop/conf/core-site.xml grep qjournal /etc/hadoop/conf/hdfs-site.xml grep -A 1 yarn.resourcemanager.address \\ /etc/hadoop/conf/yarn-site.xml\n\n9. Exit the EMR shell to go back to your local one:\n\nexit\n\n10. Remove the termination protection and terminate the cluster:\n\naws emr modify-cluster-attributes --cluster-id \\ $CLUSTER_ID --no-termination-protected aws emr terminate-clusters --cluster-id $CLUSTER_ID\n\n11. Con\u0000rm that the cluster is terminating:\n\naws emr describe-cluster --cluster-id $CLUSTER_ID \\ grep '\"State\"'\n\n12. If you don’t need to ssh to other clusters, edit the security group again to\n\nremove the SSH inbound rule that you added in step 6.\n\nHow it works...\n\nAt the beginning of the recipe, you added the\n\nAmazonElasticMapReducePlacementGroupPolicy policy to\n\nthe default role, which was enabled using the --placement-\n\ngroup-configs option on the master node group. Currently, the\n\nonly strategy is SPREAD, which is used by default. is means that\n\nthe three nodes requested for the MASTER instance group will be\n\nimplemented by EC2 by ensuring that the instances cannot be\n\ncollocated on the same physical machine. is would reduce\n\nreliability, since Hadoop can only survive the loss of one master\n\nnode without any service loss.\n\nis is because both ZooKeeper and the HDFS journal nodes work\n\non a quorum-based system that requires a majority to make\n\ndecisions. On the other hand, other services such as the YARN\n\nResourceManager, HBase region server, and others work on an\n\nactive/passive model, but still rely on ZooKeeper to make that\n\nselection and failover. In step 9, you saw the con\u0000guration for these\n\nservices and how they con\u0000gure multiple servers, so clients can\n\nfailover if one is down.\n\nIn this case, the cluster was created with a keypair, so you could ssh\n\nand check the con\u0000guration to highlight the key diﬀerences. You\n\ncan compare it with the one of a single master on your own.\n\nFinally, notice that to terminate the cluster, you \u0000rst had to disable\n\nthe termination protection \u0000ag despite not creating the cluster with\n\nthe explicit --termination-protected \u0000ag. is is because\n\nwhen a multimaster node is created programmatically, it assumes\n\nthat it is meant to be long-term and protects it from accidental\n\ndeletion and potential data loss.\n\nSince EMR 7, if the master capacity is not speci\u0000ed, it will use\n\nmultimaster by default.\n\nThere’s more...\n\nNotice that despite spreading the master nodes across diﬀerent\n\nhardware, the cluster still has a single subnet speci\u0000ed. is means\n\nthat a single VPC and a single region and Availability Zone are\n\nused.\n\nAs a result, the cluster won’t survive if the AWS AZ goes down. If\n\nyou need to protect against such extreme cases, you will need to use\n\nyour own measures. Examples include running multiple redundant\n\nclusters on diﬀerent AZs (or even regions) and de\u0000ning one as the\n\nprimary and one as the backup, or another strategy that allows\n\ncoordinating multiple clusters as needed.\n\nAs an exercise, you can ssh to one of the masters and issue the\n\nfollowing command:\n\nsudo shutdown now\n\nYou can then list the instances and observe how EMR detects that\n\nthe host is no longer available and replaces it.\n\nOen, the HA cluster will depend on having an HA catalog. e\n\neasiest way to achieve that is to con\u0000gure EMR to use Glue as the\n\nHive metastore; it provides out-of-the-box scalability and AZ fail\n\ntolerance. However, if you need to use a Hive metastore, create a\n\ndatabase on AWS Aurora and then con\u0000gure Hive to use it.\n\nSee also\n\nAlways consider whether you really need a long-lived EMR cluster and\n\nwhether there are any alternatives; for instance, instead of storing\n\npermanent data on HDFS, you could store it on S3. Instead of using a\n\nPresto or Trino server, you could use Athena. Instead of getting capacity\n\nfrom YARN, use serverless EMR, and so on.\n\nTo run workloads, see the Running jobs using AWS EMR\n\nserverless recipe.\n\nTo learn more about the Glue catalog, see the Optimizing your\n\ncatalog data retrieval using pushdown \u0000lters and indexes recipe\n\nin Chapter 3, Ingesting and Transforming Your Data with AWS\n\nGlue\n\nScaling your cluster based on workload\n\ne main bene\u0000t of running on the cloud compared to on-premises\n\nis the access to virtually endless capacity. When running EMR\n\nworkloads, you don’t want to just have resources available but also\n\nto only pay for them when needed to be cost-eﬀective.\n\nIn this recipe, you will see how EMR can eﬀortlessly allow you to\n\nscale your cluster capacity based on the workload.\n\nGetting ready\n\nis recipe assumes that you have set up the SUBNET environment\n\nvariable as indicated in the Technical requirements section at the\n\nbeginning of this chapter.\n\nHow to do it...\n\n1. Create a cluster with autoscale and idle timeout (make sure you use \\\n\nonly at the end of the lines indicated; the second command will print the\n\ncluster ID):\n\nCLUSTER_ID=$(aws emr create-cluster --name AutoScale\\ --release-label emr-7.1.0 --use-default-roles \\ --ec2-attributes SubnetId=${SUBNET} \\\n\n--auto-termination-policy IdleTimeout=900 \\ --applications Name=Spark --instance-groups \\ '[{\"InstanceCount\":1,\"InstanceGroupType\":\"MAST ER\", \"Name\":\"MASTER\",\"InstanceType\":\"m5.xlarge\"}, {\"InstanceCount\":1,\"InstanceGroupType\":\"CORE\", \"Name\":\"CORE\",\"InstanceType\":\"m5.xlarge\"}, {\"InstanceCount\":1,\"InstanceGroupType\":\"TASK\", \"Name\":\"TASK\",\"InstanceType\":\"m5.xlarge\"}]' \\ --managed-scaling-policy '{\"ComputeLimits\": {\"UnitType\":\"Instances\",\"MinimumCapacityUnits\" :1, \"MaximumCapacityUnits\":10, \"MaximumOnDemandCapacityUnits\":10, \"MaximumCoreCapacityUnits\":2}}' \\ | grep ClusterArn | grep -o 'j-.*[^\",]') echo $CLUSTER_ID\n\n2. Print the cluster capacity details. is will show that there is one node\n\nallocated for each group and run multiple times until it gets the\n\nRUNNING status:\n\naws emr describe-cluster --cluster-id $CLUSTER_ID \\ | grep -B 3 -A 5 \"RunningInstanceCount\"\n\n3. Run a Spark example app to force the cluster to scale up:\n\naws emr add-steps --cluster-id $CLUSTER_ID -- steps \\ Type=Spark,Name=SparkPi,ActionOnFailure=\\ TERMINATE_CLUSTER,Args=-- class,org.apache.spark.\\ examples.SparkPi,/usr/lib/spark/examples/jars/\n\n\\ spark-examples.jar,10000\n\n4. Run the following command multiple times to see how the cluster\n\nresizes until it hits the maximum capacity allowed (11 nodes in total).\n\nNotice that it \u0000rst changes RequestedInstanceCount, then\n\neventually scales up to that, and then scales down again when no longer\n\nneeded. In the rare case that the step \u0000nishes before the scaling kicks in,\n\nyou can repeat step 3 and this step as needed:\n\naws emr describe-cluster --cluster-id $CLUSTER_ID \\ | grep -B 2 -A 3 \"RequestedInstanceCount\"\n\n5. You can wait for the step to \u0000nish and see it scale back down to the\n\nminimum capacity. Terminate the cluster when you are done observing\n\nit:\n\naws emr terminate-clusters --cluster-id $CLUSTER_ID\n\nHow it works...\n\nIn the \u0000rst step, you created a cluster, which included the three\n\nkinds of group types:\n\nMASTER to mainly run YARN ResourceManager and the HDFS\n\nNameNode\n\nCORE nodes running HDFS and YARN\n\nTASK nodes that just run YARN\n\ne distinctive feature here is that you used managed scaling, which\n\nonly requires you to tell it how much to scale and lets EMR take\n\ncare of applying it.\n\nIn addition, you de\u0000ned an auto termination policy of 15 minutes.\n\nis means that if the cluster is idle for that period, it will be shut\n\ndown automatically. is is a good practice for clusters that do not\n\nhave auto-shutdown when all steps are complete, which avoids\n\nleaving clusters forgotten and building up charges over time. e\n\ncriteria of what constitutes an idle cluster are more sophisticated in\n\nnewer versions (for further details, see the documentation at\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-\n\nauto-termination-policy.html).\n\nen you added the SparkPI estimation but with 10,000 partitions\n\nso that it generated enough load to force the cluster to scale up.\n\nWhen the task completes, the cluster will scale back to the initial\n\ncapacity.\n\nThere’s more...\n\nFor this recipe, you de\u0000ned a maximum of 10 units of on-demand\n\ncapacity (you can also use cheaper spot instances for part or all), of\n\nwhich a maximum of two will be core nodes. at is because this\n\nexample is pure computing, so it makes sense to add most of them\n\nas TASK nodes, so it has more resources for Spark.\n\nSince EMR 5.10, the scaling behavior has been optimized to\n\nterminate nodes when the instance approaches the next hour to\n\navoid incurring another hour of cost. You can change back to the\n\nold behavior by using --scale-down-behavior\n\n\"TERMINATE_AT_TASK_COMPLETION\". See further details in the\n\ndocumentation:\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-\n\nscaledown-behavior.html.\n\nRemember that managed scaling is based on YARN usage. With\n\ncluster services that are not based on YARN, such as Trino or\n\nHBase, it cannot work correctly. See the next section for\n\nalternatives.\n\nSee also\n\nIn this recipe, you have seen how to use managed scaled, which is an\n\neﬀortless way of doing eﬃcient scaling, just setting the limits. However,\n\nthere are cases where you might want to take control and build your own\n\nscaling rules. Check the documentation for guidance on how to build\n\nsuch rules:\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-\n\nautomatic-scaling.html.\n\nCustomizing the cluster nodes easily using bootstrap actions\n\nEMR oﬀers the freedom to control all aspects of the cluster nodes.\n\nYou can provide your own Amazon Machine Images (AMIs) and\n\nrun your own choice of OS (within reason) and con\u0000guration.\n\nHowever, if you create or even branch your AMIs, you will create a\n\nmaintenance burden for the future such as patching for\n\nvulnerabilities or \u0000xes. In practice, you rarely need to deviate so\n\nmuch as to justify this eﬀort. In most cases, you just need to do\n\nsmall customizations, such as installing a system package, changing\n\nsome OS con\u0000guration, or running some special service.\n\nFor such cases, bootstrap actions provide a straightforward way of\n\nrunning these customizations just by running a shell script as root\n\nduring setup. In most cases, this means that you can run the same\n\nactions on newer versions of the OS and EMR, reducing the\n\noperational burden. If new nodes are added to the cluster, they will\n\nalso run the same bootstrap actions to ensure consistency.\n\nIn this recipe, let us say that we will run an application in the cluster\n\nthat requires using utilities from the popular Python scikit-\n\nlearn machine learning library. Instead of asking applications to\n\ninstall it or having it preinstalled in a container or image, you will\n\nuse a bootstrap action to launch an EMR cluster that comes with\n\nthe package installed and ready to use.\n\nGetting ready\n\nis recipe assumes that you have de\u0000ned the SUBNET,\n\nS3_SCRIPTS_URL and S3_LOGS_URL variables in the shell (see the\n\nTechnical requirements section at the beginning of this chapter for\n\nmore information). For this recipe, it is essential that the S3 URLs\n\npoint to a bucket in the same region as EMR.\n\nHow to do it...\n\n1. Create the bootstrap script \u0000le:\n\ncat > recipe_bootstrap.sh << EOF #!/bin/bash set –x /usr/bin/pip3 install -U scikit-learn EOF\n\n2. Copy the \u0000le onto S3:\n\naws s3 cp recipe_bootstrap.sh $S3_SCRIPTS_URL/\n\n3. Create a cluster that uses the bootstrap script you just uploaded. Take\n\ncare to only use the \\ character at the end of the lines indicated:\n\nBOOTSTRAP_SCRIPT=$S3_SCRIPTS_URL/recipe_bootst rap.sh CLUSTER_ID=$(aws emr create-cluster \\ --name BootstrapRecipe --log-uri $S3_LOGS_URL \\ --auto-terminate --release-label \"emr-6.15.0\" \\ --instance-type=m5.xlarge --instance-count 2 \\ --applications Name=Spark --use-default-roles\n\n\\ --ec2-attributes SubnetId=$SUBNET --bootstrap- action\\ Name=PythonDeps,Path=$BOOTSTRAP_SCRIPT -- steps \\ '[{\"Name\": \"Validate bootstrapaction\", \"ActionOnFailure\":\"TERMINATE_CLUSTER\", \"Jar\": \"command-runner.jar\",\"Properties\":\"\",\"Args\": [\"/usr/bin/python3\",\"-c\",\"exec(\\\"import sklearn'\\ '\\\\nprint(sklearn.__version__)\\\")\"], \"Type\":\"CUSTOM_JAR\"}]')\n\n4. You can use the AWS console, navigate to EMR, select Clusters on the\n\nle menu, and then select the one named BootstrapRecipe. Explore the\n\ndiﬀerent tabs, including Bootstrap Actions and Steps. If all goes\n\ncorrectly, the cluster will start and then run the step con\u0000gured, which\n\nwill check the package version and terminate the cluster automatically.\n\n5. Once the cluster has completed the bootstrap, run the step, and\n\ncompleted successfully, check the step log to verify that the bootstrap\n\nstep did its job and the package was installed. It might take a few\n\nminutes aer the step is executed before the log becomes available. In\n\nthe Steps tab, open the stdout link. Once it becomes available, it should\n\ndisplay the version of the library installed by the bootstrap script (for\n\ninstance, 1.0.2). If something has gone wrong, the stderr log will show\n\nthe error.",
      "page_number": 352
    },
    {
      "number": 19,
      "title": "Segment 19 (pages 373-392)",
      "start_page": 373,
      "end_page": 392,
      "detection_method": "synthetic",
      "content": "Figure 5.8 – EMR steps details\n\n6. If you no longer need it, remove the S3 \u0000les from the shell:\n\naws s3 rm $S3_SCRIPTS_URL/recipe_bootstrap.sh aws s3 rm --recursive $S3_LOGS_URL\n\nHow it works...\n\nIn step 3, you created a basic EMR cluster (using as little\n\ncon\u0000guration as possible) with the minimum number of nodes: a\n\nprimary node (formerly known as master) and a core node. e\n\ndiﬀerence between a core and a task node is that the former is used\n\nfor the HDFS \u0000lesystem, as well as for computing. ese nodes use\n\nthe modest m5.xlarge type to reduce cost.\n\nNotice that you didn’t have to specify any roles. We used the default\n\nones provided by EMR, which serve for the common cases. Also,\n\nyou didn’t specify any image or OS, which means that we have let\n\nEMR use the latest version of Amazon Linux 2 and thus don’t have\n\nto worry about patching it for vulnerabilities.\n\ne subnet implicitly determines the VPC that will be used; in this\n\ncase, it sets no security con\u0000guration or ssh access, since there was\n\nno need to access the cluster nodes to complete the recipe.\n\ne cluster instructions included a bootstrap action to call the\n\nscript that you put on S3 following steps 1 and 2. A bootstrap action\n\nis a reference to a script that gets executed as root once the OS is\n\nready but before services in the cluster are started. is allows the\n\nbootstrap script to do OS con\u0000guration and tuning. Once EMR has\n\ncompleted the provisioning of nodes, the bootstrap, along with the\n\ninstallation of the con\u0000gured apps (in this case just Spark), EMR\n\nstarts executing the steps de\u0000ned (if any).\n\nIn this recipe, the Spark step cluster runs the simple shell script that\n\nruns Python code to import the package that was installed on the\n\nOS by the bootstrap script. e step is con\u0000gured to terminate the\n\ncluster if it fails; this will mark the cluster as terminated with errors.\n\nIf there is no issue, the script prints the version of the package,\n\nshowing that the package is installed. Notice that the step only runs\n\non the primary node, but the bootstrap script is applied on all\n\nnodes; therefore, we could use this package’s utilities in a PySpark-\n\ndistributed script.\n\nFinally, because the cluster was started with the --auto-\n\nterminate \u0000ag, it shuts itself down aer completing the steps.\n\nOtherwise, the cluster would be le running until it is manually\n\nshut down or hits the idle timeout (if con\u0000gured).\n\nThere’s more...\n\nRemember that the intent of this feature is not to con\u0000gure the\n\nEMR applications such as Spark conf, as such con\u0000guration would\n\nbe overridden when the apps are installed and when using the\n\napplication options provided by EMR.\n\nIn the recipe, you checked the step output log. However, there are\n\nalso stdout and sterr logs for the bootstrap action. Go check\n\nthem to see the bootstrap script running. You could print messages\n\nin the script and then check in these logs. Notice that there is one\n\nfor each node in the cluster (two in this case). For instance, you can\n\nuse: s3://{S3_LOGS_URL}/<your cluster id>/node/<node\n\nid>/bootstrap-actions/1/.\n\nIt’s possible to pass parameters to the script. at way, you can reuse\n\na bootstrap script and make it con\u0000gurable to your needs. For\n\ninstance, you could indicate the set of packages that the script needs\n\nto install based on the cluster purpose. You can pass shell\n\narguments to the script using an Args parameter aer the script\n\npath. For instance, here, the script will receive the arg1 and arg2\n\narguments, which the script can access using Bash syntax ($1 and\n\n$2):\n\n--boostrap-action \\ Name=PythonDeps,Path=\"$BOOTRAP_SCRIPT}\",Args= [args1,arg2]\n\nFinally, notice that the script runs without user variables such as\n\nPATH. at’s why we used full paths to run the pip and Python\n\ninterpreter.\n\nSee also\n\nBootstrap actions run in all nodes before services are started. In cases\n\nwhere you need to do some setup once the services are running such as\n\nuploading \u0000les to HDFS, instead of a bootstrap action, add a command\n\ntype step (before any other steps), which can also run a script (but unlike\n\nbootstrap, only on the master). See how to do this in the documentation:\n\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-\n\ncommandrunner.html.\n\nTuning Apache Spark resource usage\n\nApache Spark is probably the most-used framework on EMR. It\n\nworks by running YARN containers for executor instances and\n\nanother one for the Application Manager (AM), which oen also\n\nacts as the Spark driver.\n\nIn traditional on-premises clusters, the cluster has many resources\n\nthat are shared by many cluster users, so you use as few resources as\n\npossible for the job at hand. On EMR, it is simple to run clusters on\n\ndemand, which are \u0000t for purpose, and then shut them down when\n\nthe job is complete. at way, the cluster size, nodes, and\n\ncon\u0000guration can be optimized for the speci\u0000c job and negative\n\ninteractions between users, such as resource starvation or\n\nsaturation, can be avoided.\n\nIn such dedicated clusters, you want your application, such as\n\nApache Spark, to make the most of the hardware provided since it\n\ndoesn’t have to share it with other users. at’s why EMR added the\n\nmaximizeResourceAllocation con\u0000guration option for Spark.\n\nIt is enabled by default and indicates to EMR to automatically\n\ncon\u0000gure the Spark executors’ resource usage based on the cluster\n\ncapacity. at includes cores and memory, and, unless\n\nspark.dynamicAllocation.enabled is explicitly enabled, it\n\nwill also calculate the number of instances to use.\n\nHowever, it’s possible when using EMR on EC2 that the cluster can\n\nhave nodes of diﬀerent sizes. at’s why since EMR versions 5.32.0\n\nand 6.5.0, a spark.yarn.heterogeneousExecutors.enabled\n\n\u0000ag was added and enabled by default. is feature adjusts the\n\ncontainers requested by Spark to run executors dynamically.\n\nWhile these con\u0000guration features provide good defaults and easy\n\ncon\u0000guration, they might not be optimal, and you might want to\n\ntune them yourself; you’ll see how in this recipe.\n\nGetting ready\n\nis recipe assumes that you have de\u0000ned the SUBNET and\n\nS3_LOGS_URL variables in the shell (see the Technical requirements\n\nsection at the beginning of the chapter for more information).\n\nHow to do it...\n\n1. In a shell, run the following command. If all is correct, you should get a\n\nJSON printed with id and arn of the cluster that was just created:\n\naws emr create-cluster --name SparkResourcesRecipe \\ --release-label emr-6.15.0 --auto-terminate \\ --instance-type=m5.2xlarge --log-uri $S3_LOGS_URL \\ --instance-count 2 --applications Name=Spark \\ --use-default-roles --ec2-attributes \\ SubnetId=$SUBNET --steps Type=Spark,Name=SparkPi\\ ,ActionOnFailure=TERMINATE_CLUSTER,Args=\\ --deploy-mode,cluster,-- class,org.apache.spark.\\ examples.SparkPi,/usr/lib/spark/examples/jars/ \\ spark-examples.jar,12\n\n2. Go to the AWS EMR console and select Clusters under EMR on EC2 on\n\nthe le menu. Select the cluster that was just created (with the ID\n\nmatching the one returned in step 1). In the Steps tab, there is a Spark\n\napplication to be executed once the cluster is ready. It should \u0000nish in\n\nless than a minute and aer that, the cluster will shut down. Note that we\n\ndidn’t specify any resources for Spark to use.\n\n3. Once the previous step has been completed, on the cluster summary\n\nsection of the page, select the YARN timeline server link. Aer a few\n\nseconds, it will open a new tab with the YARN application history. If you\n\ndon’t see the SparkPl application listed, there might be a bit of a delay in\n\npublishing the logs. Refresh the browser page until it appears.\n\nFigure 5.9 – The cluster summary\n\n4. Click on the application ID, which is named in the format:\n\napplication_<timestamp>_0001. is will take you to the table\n\nof attempts. ere should be only one; click on the attempt ID link. Now,\n\nyou should see the containers that this app was allocated in. ere\n\nshould be two. e one ending in 000001 is the driver and the other is\n\nthe executor:\n\nFigure 5.10 – The application container list\n\n5. Click on each container ID. It will take you to the container details.\n\nNotice that each one has a diﬀerent memory allocated, as indicated in\n\nthe Resource line.\n\n6. Now go back to the Cluster summary page tab and use the Spark History\n\nServer link. Aer a few seconds, a new tab will open wherein the\n\nSparkPI application should be listed. Use the link on the app ID to open\n\nSparkUI.\n\n7. In SparkUI, navigate to the Executors tab. e executor is given six cores\n\nand over 7 GB of storage memory, which is half of the JVM heap size by\n\ndefault.\n\n8. You can use the options to control some of the aspects such as the cores,\n\nmemory, and overhead; the system algorithm will try to adapt, but that\n\noen leads to confusing results. Now, you are going to start a cluster\n\nwhere you will disable the automation logic and run it with custom\n\nexecutor resources:\n\naws emr create-cluster --name SparkResourcesRecipe2 \\ --release-label emr-6.15.0 --applications Name=Spark\\ --instance-type=m5.2xlarge --instance-count 2\\ --use-default-roles --ec2-attributes \\ SubnetId=${SUBNET} --log-uri $S3_LOGS_URL \\ --auto-terminate --configurations \\ '[{\"Classification\":\"spark- defaults\",\"Properties\": {\"spark.yarn.heterogeneousExecutors.enabled\":\" false\"}} ]' --steps Type=Spark,Name=SparkPi,ActionOnFailure\\ =TERMINATE_CLUSTER,Args=[--deploy- mode,cluster,\\ --executor-cores,4,--executor-memory,6G,-- conf,\\ 'spark.yarn.executor.memoryOverhead=1G',\\ --class,org.apache.spark.examples.SparkPi,\\ /usr/lib/spark/examples/jars/spark- examples.jar,12]\n\n9. Check the Spark resources like in steps 4 and 5. Notice the diﬀerences in\n\nthe core node resource usage; now you have three executors on the\n\ninstance using 21 GB of RAM and 12 Spark cores (YARN only allocates\n\none per executor, this is explained in the next paragraph).\n\nFigure 5.11 – Spark executors’ details\n\n10. Clean up the log \u0000les if you don’t want to keep them:\n\naws s3 rm --recursive $S3_LOGS_URL\n\nHow it works...\n\nIn step 1, you created an EMR on EC2 cluster with default resource\n\npolicies and the minimum number of nodes: a primary node and a\n\ncore node (the diﬀerence between primary and task is that the latter\n\ndoesn’t run the HDFS \u0000lesystem). e cluster was created with a\n\nstep already set up and con\u0000gured to shut itself down automatically\n\nto save on cost. It’s also possible to create the cluster, leave it\n\nrunning (remove the --auto-terminate \u0000ag), and add steps later.\n\ne cluster was created using YARN and Spark con\u0000gurations \u0000t for\n\nthe kind of node used. In addition, the\n\nheterogeneousExecutors feature dynamically adjusts to try to\n\n\u0000t a single executor per node, even if nodes have diﬀerent sizes or\n\nare added aer the cluster is con\u0000gured (for instance, if you add a\n\ntask group of a diﬀerent type of instance).\n\nWhen the Spark application was executed, it \u0000rst allocated the AM\n\ncontainer. In this recipe, that container also runs the Spark driver.\n\nis is because of --deploy-mode cluster. en it\n\nautomatically created an executor for the instance trying to use the\n\nrest of the available memory.\n\nIn YARN, by default, it allocates the memory available but doesn’t\n\nenforce vcores, which are sharable thanks to CPU threads. Spark\n\ncores are really threads, so they don’t need to match the cores that\n\nYARN has assigned, since it won’t enforce vcore usage by default.\n\nIf you check the SparkUI Environment tab, you will see that EMR\n\nset the spark.executor.memory property to 4,743M for the\n\ncluster, but that was dynamically overridden by the\n\nheterogeneousExecutors feature to use a JVM of nearly 15 GB\n\n(you can know that because by default, half of the heap is used for\n\nstorage, which you can see in the Executors tab).\n\nere is also a discrepancy with the container size allocated on\n\nYARN, which is 16,896M. at diﬀerence is called the memory\n\noverhead and is used for memory outside of the heap such as the\n\nstack, I/O buﬀers, unmanaged memory, or Python daemons used\n\nfor Python UDFs. e driver also has some memory allocated by\n\nYARN outside of the JVM. If a process exceeds the memory\n\nallocated, YARN will kill it, but it will give you a message suggesting\n\nincreasing the overhead.\n\nYou might have noticed that the two containers on YARN amount\n\nto less than 20 GB of RAM, while m5.2xlarge has 32 GB. Bear in\n\nmind that YARN cannot allocate all the memory on the machine\n\nfor containers, since there are other processes such as Hadoop itself\n\nthat need memory. It depends on the kind of node but, in general,\n\nabout 80% of the memory is used for containers (which is\n\nadjustable in the EMR con\u0000guration but is risky to lower).\n\nIn the end, the automatic allocation used less than 20 GB of the 24\n\nGB available in the worker node and six out of the eight cores. It\n\nwas very conservative.\n\nIn step 7, you started a cluster but this time with the\n\nheterogeneousExecutors feature disabled. In this case, it was\n\ndisabled at the cluster level. However, it is also possible to disable it\n\nin the Spark submit options, like any other Spark con\u0000guration\n\nproperty.\n\nOnce that feature was disabled, you could control the number of\n\ncores, the size of the heap, and the overhead used exactly – 1 G in\n\nthis example, which is plenty for a job that doesn’t require\n\nunmanaged memory use. e only thing that was automatic in this\n\ncase is that by default, EMR is con\u0000gured to maximize resources\n\n(assuming that the cluster is not shared and that you want the best\n\nperformance for your app), so it allocated as many executors as it\n\ncould \u0000t. Each executor container was con\u0000gured with 6 GB heap\n\nand 1 GB overhead for a total of 7 GB. 2.5 GB was used by the\n\ndriver/AM, so that leaves room for three executors.\n\nAlso, notice that each executor is con\u0000gured with four Spark cores,\n\nso in total, the executors will use 12 when the machine has eight\n\nvcores and there are other processes running. is kind of\n\novercommitting can help maximize CPU usage when the job is low\n\non computing and high on I/O. Otherwise, it might hurt\n\nperformance a bit due to the CPU context switching. In this case, it\n\nwas done for demonstration purposes, since the job is purely doing\n\ncomputing so it won’t bene\u0000t from this core con\u0000guration.\n\nThere’s more...\n\nere is another element of \u0000ne-tuning that you can do if you don’t\n\nrequire the Spark driver to be failure-tolerant. If your job has a plan\n\nthat is more complex than the trivial example in this recipe, it is\n\nlikely that you will require multiple GB of memory, while the\n\nprimary node memory is underused at the same time (especially as\n\nyou move to larger instances).\n\nIf you run Spark without the --deploy-mode cluster argument\n\n(which we used in this recipe), the Spark driver will run on the\n\nprimary node, and the \u0000rst container is just a small AM using less\n\nthan 1 GB of memory (you can reduce that further with\n\ncon\u0000guration if the job doesn’t need many containers to manage).\n\nis can free up worker node resources for executors.\n\nNow that you know how you can \u0000ne-tune the resource usage.\n\nInstead of trial and error, it’s better to get system-level information\n\nin addition to SparkUI. For that, you can use an application named\n\nGanglia, which you can enable in the cluster application list and\n\nthen access via a browser once you set up a ssh tunnel to access it\n\n(see the documentation at\n\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/view_Gangl\n\nia.html). With Ganglia, you can get detailed information about\n\nsystem CPU and IO usage both overall and per host.\n\nYou might also want to try to run clusters with multiple types of\n\ncore and task nodes without disabling\n\nspark.yarn.heterogeneousExecutors.enabled and see how\n\nthe algorithm reacts and adjusts. is depends on the EMR version,\n\nbut it has a component of trial and error whereby the algorithm\n\nrequests containers that fail to allocate or are discarded.\n\nSee also\n\nIf you run a job without special requirements and want to simplify\n\nresource usage, you can just use EMR serverless or EMR on EKS. If this\n\nis the case for you, see the corresponding recipes:\n\nRunning jobs using AWS EMR serverless\n\nRunning your AWS EMR cluster on EKS\n\nCode development on EMR using Workspaces\n\nDeveloping data processing code on complex distributed\n\nframeworks is much more productive when it is done in an\n\ninteractive way by using representative data and seeing the results of\n\nthe transformations done on each step. is has led to an increase\n\nin the popularity of languages that can be interpreted interactively,\n\nsuch as Python or Scala.\n\nWhile you can do some interactive development via a shell, as the\n\ncode becomes larger, it stops being practical. e productive way to\n\ndo this is via a notebook with cells, where each cell holds and\n\nexecutes a block of code, but the variables are common to the\n\nnotebook so the work you do in one cell is visible to the others. at\n\nway, you can develop and test a small piece of code at a time and see\n\nthe results.\n\nEMR has traditionally supported this style of development with\n\nApache Zeppelin, which can be installed on the cluster to run\n\nmultiple types of notebooks including Spark or Bash, with multiple\n\nlanguages such as Python or Scala. Accessing the notebooks\n\nrequires opening a port to the primary node (or setting up a proxy\n\nsuch as Apache Knox).\n\ne rise in popularity of Python brought its preferred notebook\n\nenvironment, Jupyter, to EMR, which was added as an alternative to\n\nZeppelin.\n\nEMR Workspaces (previously called notebooks) put the Jupyter\n\nnotebooks at the center of the service, instead of a cluster that runs\n\nJupyter. is service consists of a JupyterLab app, which can be\n\nlinked to existing or new clusters. In this recipe, you will see how to\n\ndo this.\n\nHow to do it...\n\n1. In a shell, create the role that you will use for EMR Studio:\n\nROLE_NAME=EMR_Notebooks_RecipeRole echo '{\"Version\":\"2012-10-17\",\"Statement\": [{\"Effect\": \"Allow\",\"Principal\":{\"Service\": \"elasticmapreduce.amazonaws.com\"},\"Action\": \"sts:AssumeRole\"}]}' > role-assume.json aws iam create-role --role-name $ROLE_NAME \\ --assume-role-policy-document file://role-\n\nassume.json aws iam attach-role-policy --policy-arn \\ arn:aws:iam::aws:policy/service-role/\\ AmazonElasticMapReduceRole --role-name $ROLE_NAME aws iam attach-role-policy --role-name $ROLE_NAME \\ --policy-arn arn:aws:iam::aws:policy/service- role/\\ AmazonElasticMapReduceEditorsRole aws iam attach-role-policy --policy-arn \\ arn:aws:iam::aws:policy/AmazonS3FullAccess \\ --role-name $ROLE_NAME rm role-assume.json\n\n2. Open the AWS console in a browser and navigate to EMR. On the le\n\nmenu, under EMR Studio, select Studios and then Create Studio.\n\n3. Choose the Custom option, name it RecipeStudio and choose\n\nEMR_Notebooks_RecipeRole (you created this in the \u0000rst step).You can\n\nlet it create an S3 bucket or choose an existing one.\n\n4. Enter RecipeStudio_Workspace in the Workspace name \u0000eld,\n\nextend the Networking and security section, and select a VPC and\n\nsubnets (by default, each region has one VPC with three subnets).\n\nChoose Create Studio.\n\n5. Select Workspaces (Notebooks) on the le, choose the one that was just\n\ncreated, and then choose Quick Launch. It will open a new tab with\n\nJupyterLab.\n\n6. In JupyterLab, the le bar shows the \u0000le explorer with a notebook \u0000le\n\nnamed like the workspace by default. Double-click on the name to open\n\nit; it will prompt you to select a kernel. Choose PySpark.\n\n7. Now we need a cluster that Spark can run in. Open the second tab on\n\nthe le bar; it will show the Compute panel:\n\nFigure 5.12 – JupyterLab showing the compute conﬁguration\n\n8. Choose the EMR on EC2 cluster type and expand the Advanced\n\ncon\u0000guration section. Enter the name RecipeCluster, pick a release\n\nand a bucket to store logs, and two m5.xlarge nodes or similar. For\n\nthis kind of cluster, it is especially important to have an auto termination\n\non idle setup, since it is easy to forget about the cluster and build up\n\ncharges. Choose Create cluster and wait until it is ready. It will give you\n\nthe cluster ID, which starts with j-, and then you can go to EMR and\n\nmonitor the progress.\n\n9. Once it is running, it should show a message and update the EMR on\n\nEC2 cluster dropdown. In that dropdown, choose RecipeCluster |\n\nAttach.\n\n10. Once the action is completed, you will have a cluster powering the\n\nPySpark notebook. To prove this, on the \u0000rst cell, type spark and use\n\nthe Run button on the bar (or use the Shi + Enter shortcut). e icon\n\non the le of the cell will become an asterisk to indicate that it’s in\n\nprocess. Once it \u0000nishes, it will show the YARN ID of the Spark\n\napplication running on the EMR cluster.\n\nFigure 5.13 – JupyterLab showing the notebook executor results\n\n11. Stop the EMR cluster. At the time of writing this book, you cannot do it\n\nfrom JupyterLab. Go to the EMR console, choose Clusters under EMR\n\non EC2 in the le menu, select the cluster named RecipeCluster, and\n\nterminate it.\n\n12. Go back to the Workshop screen, select the RecipeWorkspace row, and\n\nchoose Delete in the Action menu (or Stop if you want to keep it). You\n\nmight also want to empty any temporary \u0000les on S3, depending on\n\nwhich S3 bucket you chose in step 3.\n\nThere’s more...\n\nIn the \u0000rst step, you created a role to use on EMR Studio. In this\n\nexample, it has unrestricted access to all the S3 buckets in the\n\naccount. In a production system, it is better to narrow down the\n\npermissions to the bucket and path that will be used by Studio.\n\nen you created an EMR Studio. is is something you normally\n\nonly need to do once and can then reuse for all your Workspaces.\n\nAerward, you created a Workspace. is was previously called a\n\nnotebook but has since been renamed, as now it provides an\n\nenvironment where you can have multiple notebooks and\n\nintegrations with other tools such as Git or Presto (if you have them\n\ninstalled on the attached cluster).\n\nNext, you launched the Workspace. In practice, it starts\n\nautomatically when you create it. When using Launch with options\n\ninstead of the quick launch, you could have speci\u0000ed a cluster to\n\nattach to. e cluster must already be running in one of the subnets\n\ncon\u0000gured on Studio and have the JupyterEnterpriseGateway\n\napplication installed (so the Workspace can attach to it). en, if",
      "page_number": 373
    },
    {
      "number": 20,
      "title": "Segment 20 (pages 393-414)",
      "start_page": 393,
      "end_page": 414,
      "detection_method": "synthetic",
      "content": "you want to use Spark, you would need Spark, Hadoop, and Livy\n\ninstalled. Normally, it is easier to just create a \u0000t-for-purpose cluster\n\ninside the Workspace as you did in the recipe and then terminate it\n\nor let it expire. You can create a cluster template so you do not have\n\nto con\u0000gure it every time.\n\nBear in mind that it is possible to share the Workspace with other\n\nusers to enable collaboration on the development, although\n\nworking independently is oen desirable.\n\nSee also\n\nIf you only use PySpark in your notebooks, you have the option to use a\n\nGlue Notebook instead, which provides an integrated Jupyter notebook\n\nand cluster with minimum con\u0000guration and a cluster created on\n\ndemand. You can see an example of Glue notebook usage in the\n\nOptimizing your catalog data retrieval using pushdown \u0000lters and indexes\n\nrecipe in Chapter 3, Ingesting and Transforming Your Data with AWS\n\nGlue.\n\nIf you are a heavy user of SageMaker, you can link your SageMaker\n\nnotebooks with EMR or even easier to use a Glue kernel and run a\n\nserverless cluster. You can learn more about this at\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-\n\nemr-spark-glue.html.\n\nMonitoring your cluster\n\ne convenience of using EMR to create \u0000t-for-purpose,\n\ndiscardable clusters has signi\u0000cantly reduced the maintenance\n\nneeds for Hadoop clusters compared to long-lived, multitenant on-\n\npremises clusters.\n\nHowever, there is still a need to monitor how the cluster is doing in\n\ndetail, in cases where you need to optimize the use or troubleshoot\n\nan issue. For instance, you might wonder what the limiting factor to\n\nperformance is: is it the CPU, memory, network, disk, or something\n\nelse?\n\nIn this recipe, you will see how to go deep into the cluster metrics\n\nand monitoring tools that it provides out of the box.\n\nGetting ready\n\nTo carry out this recipe, you need to set up the SUBNET,\n\nS3_LOGS_URL, and KEYNAME shell environment variables (see the\n\nTechnical requirements section at the beginning of this chapter to\n\nlearn how to set them up).\n\nTo complete the recipe, you will need a SOCKS5 proxy in your\n\nbrowser to access the cluster. Follow the instructions depending on\n\nyour browser, so that when you access a **.amazonaws.com URL,\n\nit uses the localhost proxy on port 8157 that you’ll create. You can\n\nlearn more at\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-\n\nconnect-master-node-proxy.html.\n\nFigure 5.14 – A proxy proﬁle\n\nFigure 5.15 – Auto-switch in SwitchyOmega\n\nHow to do it...\n\n1. Create a tiny cluster that will keep running so we can monitor it:\n\nCLUSTER_ID=$(aws emr create-cluster --name \\ MonitorRecipe --release-label emr-6.15.0 \\ --instance-type=m5.xlarge --instance-count 2 \\ --use-default-roles --use-default-roles \\ --ec2-attributes SubnetId=$SUBNET,KeyName=$KEYNAME \\\n\n--applications Name=Spark Name=Ganglia --steps \\ Type=Spark,Name=SparkPi,ActionOnFailure=\\ TERMINATE_CLUSTER,Args=--deploy-mode,cluster,\\ --class,org.apache.spark.examples.SparkPi\\ ,/usr/lib/spark/examples/jars/spark- examples.jar\\ ,100000 --log-uri $S3_LOGS_URL \\ --auto-termination-policy IdleTimeout=3600 \\ | grep ClusterArn | grep -o 'j-.*[^\",]') echo $CLUSTER_ID\n\n2. Keep running this command until you get the public DNS name:\n\naws emr describe-cluster --cluster-id $CLUSTER_ID \\ | grep 'Dns'\n\n3. Open the AWS console, navigate to EMR, and \u0000nd the cluster with the\n\nsame ID as the one printed in step 1 (on the same region you are using\n\nthe AWS CLI in). Notice that just by using the console, you can do basic\n\nmonitoring tasks conveniently. You can open the historical YARN and\n\nSpark UIs in the summary, check the list and status of nodes on the\n\nInstances tab, and view several metrics on the Monitoring tab. For\n\ninstance, in this case, the app that is running uses two YARN containers:\n\na Spark driver and an executor. e app does not move any data but\n\ncauses signi\u0000cant system load and memory usage.\n\nFigure 5.16 – EMR cluster status metrics\n\n4. To dive deeper and explore timely metrics, you’ll need to access the\n\ncluster. For that, you need to allow SSH access \u0000rst. In the cluster page,\n\nswitch to the Properties tab, expand the EC2 Security groups section,\n\nand open the link under Primary node EMR managed security group.\n\n5. On the security group page, select Edit inbound rules | Add rule. In the\n\nnew role on the dropdowns, select SSH and My IP. Save the rule\n\nchanges.\n\nFigure 5.17 – Choosing a new SSH rule\n\n6. Run the following command to enable port forwarding on your machine\n\ntoward the primary cluster node:\n\naws emr socks --cluster-id $CLUSTER_ID\\\n\n--key-pair-file $KEYNAME.pem\n\nIf the command results in Connection timed out, it means that\n\nthe previous step wasn’t completed correctly or that the IP that\n\nwas detected by the browser is not the one that the shell is really\n\nusing to access AWS. In that case, you would need an alternative\n\nmethod to \u0000nd the IP to allow, for instance, running the\n\nfollowing in the shell:\n\ncurl ifconfig.me\n\n7. In your browser, switch the proxy manually if you did not de\u0000ne\n\nautomated rules in the Getting ready section of this chapter. Navigate to\n\nthe master DNS name retrieved in step 2 and specify the 8088 port, for\n\ninstance: http://ec2-X-X-X-X.compute.amazonaws.com:8088.\n\n8. is should open the live YARN UI. It looks like the screen that you see\n\non the YARN timeline server summary link. e diﬀerence is that this\n\none has far more details about the cluster status. Under Cluster Metrics,\n\nyou can see the resources that are available and assigned. In the\n\nfollowing screenshot, under Cluster Node Metrics, you can see the status\n\nof the nodes. ere should be just one active; you can click on the\n\nnumber list of each state and see details of the nodes (for instance, why a\n\nnode is unhealthy).\n\nFigure 5.18 – The YARN application list\n\n9. e cluster creation included the Ganglia application. Ganglia can\n\nprovide you with low-level details of the cluster usage globally and per\n\nnode, such as I/O, disk, or CPU usage. On the browser with the proxy\n\nenabled, enter the master node DNS with the /ganglia location. For\n\ninstance, you might enter the following: http://ec2-X-X-X-X.compute-\n\n1.amazonaws.com/ganglia/.\n\n10. Under the time range selector, you should see metrics per node instead\n\nof aggregated. Compare the metrics of the two nodes and note the\n\ndiﬀerence between the primary and the core node (where the Spark\n\napplication ran) CPU usage.\n\nFigure 5.19 – Ganglia cluster metrics\n\n11. Once you have explored the metrics on your own, stop the proxy in the\n\ncommand line using the Ctrl + C key shortcut, remove the rule you\n\nadded in step 5, terminate the cluster, and clean up the logs:\n\naws emr terminate-clusters –cluster-id $CLUSTER_ID aws s3 rm --recursive $S3_LOGS_URL\n\nThere’s more\n\nIn this recipe, you created a tiny cluster with a Spark step to\n\ngenerate some workload. e cluster also has an automatic idle\n\nauto-shutdown of 30 minutes. is is good practice to limit\n\nexpenses in case you forget to terminate the cluster.\n\nen, you created a ssh tunnel using the AWS CLI tool. Notice that\n\nthe command indicates the openssh command it is running ssh -\n\no StrictHostKeyChecking=no -o\n\nServerAliveInterval=10 -ND 8157 -I yourkey.pem\n\nhadoop@yourserver.com.\n\nis command tells ssh to accept the server key without asking for\n\ncon\u0000rmation, to keep the connection alive, and to do dynamic port\n\nforwarding from the local 8157 port. If you wanted to use a\n\ndiﬀerent local port for some reason, you would need to install and\n\nrun the ssh command yourself. is port acts as a socks5 proxy.\n\nRemember that you can always ssh into the primary node, copy and\n\npaste the .pem \u0000le content, and then use that \u0000le to ssh to other\n\nhosts on the cluster (you can get the internal names of the other\n\nnodes using one of the tools that you have seen in this recipe). You\n\nhave the option of using OS-speci\u0000c tools such as iostat or\n\nvmstat.\n\nSee also\n\nInstead of monitoring the cluster actively, in most cases, you will want to\n\nreceive an alert when something is abnormal (for instance, if you have\n\nunhealthy YARN nodes). You have the metrics that you see on the\n\nconsole monitor screen on CloudWatch, but the challenge is that they\n\nare cluster-speci\u0000c, so you would need to create these speci\u0000c alarms on\n\nthe same script that automates the cluster creation. You can learn more\n\nin the documentation:\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR\n\n_ViewingMetrics.html\n\nProtecting your cluster from security vulnerabilities\n\nHistorically, each version of EMR was provided with an AMI, on\n\nwhich it was tested. In addition, on provisioning the image, it\n\nautomatically checks the repository for security updates. You can\n\nonly disable that behavior using repo-upgrade-on-boot=NONE.\n\nRemember that when using a custom AMI, you must take the\n\nresponsibility of keeping the image patched. However, patches that\n\naﬀect the kernel require a restart to be installed. us, in the past,\n\nyou had two options:\n\nUpgrade to a newer version of EMR, which means taking upgrades in all\n\ncomponents and services. is could cause your application to need to\n\nbe retested and would potentially require a migration.\n\nIndicate EMR to use a newer AMI and risk running the services on an\n\nimage on which it has never been tested.\n\nat has changed since EMR 5.36 and 6.6, if using EMR 5 or 6,\n\nrespectively. From those versions onwards, you can let EMR\n\nupgrade the AMI automatically with a patched AmazonLinux2 OS\n\nwithout having to do any testing or upgrading yourself.\n\nGetting ready\n\nis recipe assumes that you have de\u0000ned the SUBNET variable in\n\nthe shell (see the Technical requirements section at the beginning of\n\nthis chapter for more information).\n\nHow to do it...\n\n1. Create a cluster that will terminate as soon it runs:\n\nCLUSTER_ID=$(aws emr create-cluster --release- label \\ emr-6.15.0 --auto-terminate --instance-type=\\ m5.xlarge --instance-count 2 --use-default- roles \\ --use-default-roles --ec2-attributes \\ SubnetId=$SUBNET | grep ClusterArn \\ | grep -o 'j-.*[^\",]')\n\n2. Check which Amazon Linux version was used:\n\naws emr describe-cluster --cluster-id $CLUSTER_ID\\ | grep OS\n\n3. List the EMR version details; at the end, there is the\n\nAvailableOSReleases section. Notice that the \u0000rst OS label listed\n\n(the most recent one) is the one that was returned in the previous step:\n\naws emr describe-release-label --release- label\\ emr-6.15.0\n\n4. Stop the cluster to avoid further charges:\n\naws emr terminate-clusters –cluster-id $CLUSTER_ID\n\nHow it works...\n\ne cluster in this example had no steps and was set to auto-\n\nterminate when it had completed all tasks. So, it terminated as soon\n\nas it was started. e point here was to prove that the cluster\n\nautomatically used the most up-to-date version of AmazonLinux2\n\nfor EMR.\n\ne cluster was created with a newer version than the minimum\n\nrequired (6.11 versus 6.6) and you did not specify the --os-\n\nrelease-label \u0000ag. us, EMR was free to automatically select\n\nthe latest veri\u0000ed release for EMR with all the security patches. At\n\nthe time of writing this book, the latest version automatically\n\nselected for 6.11 is AmazonLinux 2 2.0.20230628.0.\n\nis not only provides automatic security but also reduces the EMR\n\nprovisioning time, since the number of patches that must be applied\n\non start is signi\u0000cantly reduced compared to an old image.\n\nThere’s more...\n\nYou can \u0000nd the details of the OS release picked by EMR on the\n\nAmazonLinux2 release notes at\n\nhttps://docs.aws.amazon.com/AL2/latest/relnotes/relnotes-al2.html.\n\nSee also\n\nIn this case, you have seen how short-lived clusters get OS security\n\npatches. However, that does not cover applications that need patching,\n\nsince in most cases, patches for applications are not security-related. If\n\nthat is the case, you will need to consider upgrading to a version of EMR\n\nor doing custom patching using bootstrap action; see the Customizing\n\nthe cluster nodes easily using bootstrap actions recipe for reference.\n\nOceanofPDF.com\n\n6 Governing Your Platform\n\nEnsuring data integrity, compliance, and security is fundamental in\n\ndata engineering, providing a reliable foundation for accurate\n\nanalysis and informed decision-making. Data governance involves\n\nestablishing clear policies and standards to govern data throughout\n\nits life cycle. On the resource management front, governance\n\nensures that best practices and policies are not only de\u0000ned but also\n\nactively applied and enforced. Automation is essential in this\n\nprocess, ensuring consistent adherence to governance policies and\n\nfacilitating smooth implementation of policy updates.\n\nis chapter explores data governance techniques, such as data\n\nmasking, implementing quality checks, and classifying data to\n\nidentify sensitive information. Additionally, it explores resource\n\ngovernance, ensuring eﬀective implementation and enforcement of\n\nbest practices and policies across the AWS environment.\n\ne following recipes will be covered in this chapter:\n\nApplying a data quality check on Glue tables\n\nAutomating the discovery and reporting of sensitive data on your S3\n\nbuckets\n\nEstablishing a tagging strategy for AWS resources\n\nBuilding your distributed data community with Amazon DataZone\n\nfollowing data mesh principles\n\nHandling security-sensitive data (PII and PHI)\n\nEnsuring S3 compliance with AWS Con\u0000g\n\nTechnical requirements\n\nSeveral recipes in this chapter require having S3 buckets. Glue\n\ntables and Redshi clusters are used in the recipes as well.\n\nYou can \u0000nd the code \u0000les for this chapter on GitHub repository\n\nusing the following link: https://github.com/PacktPublishing/Data-\n\nEngineering-with-AWS-Cookbook/tree/main/Chapter06.\n\nApplying a data quality check on Glue tables\n\nGlue Data Quality is crucial for maintaining the integrity and\n\nreliability of data within the AWS Glue environment. It ensures that\n\ndata conforms to speci\u0000ed quality standards, allowing organizations\n\nto trust and rely on the accuracy of their data-driven insights and\n\ndecision-making processes. Implementing data quality checks helps\n\nidentify and rectify issues, such as missing values, inconsistencies,\n\nand inaccuracies in datasets, promoting data reliability and\n\nreducing the risk of making decisions based on \u0000awed information.\n\nAWS Data Quality allows you to enforce quality checks on your\n\ndata on transit and rest using Data Quality De\u0000nition Language\n\n(DQDL). is allows you to proactively apply data quality rules to\n\nyour jobs and tables, helping to identify potential issues early.\n\nAdditionally, you can enforce rules on multiple tables and con\u0000gure\n\nactions or alarms based on detected quality issues, preventing larger\n\nproblems, such as making decisions based on inaccurate data.\n\nIn this recipe, we will learn how to implement data quality checks at\n\nrest on a Glue table.\n\nGetting ready\n\nFor this recipe, you need to have a Glue table and an IAM role with\n\nthe following policies (make sure to replace bucket_name,\n\naws_region_id, and aws_account_id for the Glue source with\n\nyour own values):\n\nis is the Glue Data Quality rule recommendation policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowGlueRuleRecommendationRunActions\", \"Effect\": \"Allow\", \"Action\": [\n\n\"glue:GetDataQualityRuleRecommendationRun\", \"glue:PublishDataQuality\",\n\n\"glue:CreateDataQualityRuleset\",\n\n\"glue:GetDataQualityRulesetEvaluationRun\", \"glue:GetDataQualityRuleset\" ], \"Resource\": \"arn:aws:glue: <aws_region_id>: <aws_account_id>:dataQualityRuleset/*\" }, { \"Sid\": \"AllowS3GetObjectToRunRuleRecommendationTask\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\" ], \"Resource\": [ \"arn:aws:s3:::aws-glue-*\" ] } ] }\n\nis is the Glue Catalog policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowCatalogPermissions\", \"Effect\": \"Allow\", \"Action\": [ \"glue:GetPartitions\",\n\n\"glue:GetTable\" ], \"Resource\": [ \"*\" ] } ]\n\nis is the CloudWatch and S3 policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowPublishingCloudwatchLogs\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogStream\", \"logs:CreateLogGroup\", \"logs:PutLogEvents\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowCloudWatchPutMetricDataToPublishTaskMetr ics\", \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:PutMetricData\" ], \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"cloudwatch:namespace\":\n\n\"Glue Data Quality\" } } } ] }\n\nis is the S3 policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::<bucket_name>\", \"arn:aws:s3:::<bucket_name>/*\" ] } ] }\n\nHow to do it…\n\n1. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the Glue service.\n\n2. From the navigation pane on the le, go to Tables and open the table you\n\nneed to set data quality rules for.\n\n3. Go to the Data quality tab and click on Create data quality rules.\n\n4. Choose the IAM role you have created and, if required, change the\n\nvalues of Task timeout value and Number of workers (which refers to the\n\nnumber of worker nodes allocated to execute the rule). Optionally, add a\n\n\u0000lter on the table that the task will run on and click on Recommended\n\nrules.\n\n5. Aer a few minutes, the task will \u0000nish scanning the table and will\n\ngenerate a set of recommended data quality rules. Click on Insert rule\n\nrecommendation, review the rules, and if you \u0000nd any rules to be useful\n\nfor your table, select it and click on Add selected rules.\n\n6. In the Ruleset editor, you can de\u0000ne your own rule using DDQL.\n\n7. Click on Save ruleset. A prompt will appear where you can enter a\n\nruleset name and, optionally, a description and tags for the rule. Aer\n\nthat, click again on Save ruleset to complete the process.\n\n8. In the Data quality section, select your ruleset and click on Run. Select\n\nthe IAM role you have created and select the run frequency for your\n\nruleset. Keep the Publish run metrics to Amazon CloudWatch option\n\nselected and click on Run.\n\n9. Once the run is completed, select the ruleset, and from the Actions\n\ndrop-down list, select Download results and review it.\n\nHow it works…\n\nUpon the initial establishment of a data quality ruleset, Glue Data\n\nQuality scanned our table, formulating recommended rules based\n\non the data within each column of the input table. ese rules serve\n\nas guidelines for pinpointing potential boundaries where data\n\n\u0000ltration can be applied to uphold quality standards. We have\n\nspeci\u0000ed the set of rules to be enforced on our table, initiated a\n\nmanual run that produced a resultset indicating the status of each\n\nrule—whether it has been successfully passed or not, and the Data\n\nquality snapshot section will show the trend of your data quality\n\nscore for the last 10 runs.\n\nThere’s more…\n\nYou can set up alerts on data quality issues to take proactive actions,\n\nusing EventBridge to send noti\u0000cations to a channel of your choice,\n\nsuch as an SNS topic or Lambda. Follow the outlined steps to\n\nachieve this:\n\n1. Navigate to the EventBridge service.\n\n2. Select Rules under Buses from the navigation pane on the le and click\n\non Create rule.\n\n3. Give a name for the rule and optionally a description. Under Rule type,\n\nselect Rule with an event pattern and click on Next.\n\n4. Under Event source, select AWS events or EventBridge partner events.\n\n5. Choose Use pattern form for Creation method.\n\n6. Select AWS service for Event source and then select Glue Data Quality.\n\nUnder Event type, select Data Quality Evaluation Results Available and\n\nthen choose Speci\u0000c state(s), select Failed, and click on Next.\n\n7. Choose a target for the rule and create it.\n\nSee also\n\nData Quality De\u0000nition Language (DQDL) reference:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/dqdl.html#dqdl-syntax-\n\nrule-structure\n\nData Quality for ETL jobs in AWS Glue Studio notebooks:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/data-quality-gs-studio-\n\nnotebooks.html\n\nAutomating the discovery and reporting of sensitive data on your S3 buckets\n\ne identi\u0000cation of sensitive data within Amazon S3 is essential\n\nfor maintaining data security and compliance in cloud\n\nenvironments. Given that S3 oen contains extensive datasets,\n\nincluding PII, pinpointing the locations of sensitive data within S3\n\nbuckets is critical for the precise implementation of security\n\nmeasures, the application of relevant access controls, and the\n\neﬀective enforcement of data protection policies. AWS Macie\n\nprovides an automated solution for the discovery, classi\u0000cation, and\n\nprotection of sensitive data through the utilization of machine",
      "page_number": 393
    },
    {
      "number": 21,
      "title": "Segment 21 (pages 415-435)",
      "start_page": 415,
      "end_page": 435,
      "detection_method": "synthetic",
      "content": "learning algorithms. is proactive approach aids in mitigating the\n\nrisks associated with data breaches and ensures compliance with\n\nregulatory standards. It enables organizations to respond promptly\n\nto potential threats, constructing a resilient infrastructure to\n\nsafeguard the con\u0000dentiality and integrity of their data in the cloud.\n\nMacie is particularly useful for AWS-based environments with\n\nlarge, unstructured datasets due to its scalability, automated data\n\ndiscovery, and seamless integration with other AWS services. It\n\nprovides built-in data identi\u0000ers, while also allowing the creation of\n\ncustom ones to meet speci\u0000c needs.\n\nIn this recipe, we will learn how to create AWS Macie’s job to\n\nidentify sensitive data in our S3 bucket, speci\u0000cally email addresses.\n\nGetting ready\n\nFor this recipe, you need to have an S3 dataset with email addresses\n\n(dummy data) and other potential PII data (such as credit card\n\ninformation), and you have to enable AWS Macie if you are using it\n\nfor the \u0000rst time following the outlined steps:\n\n1. Log in to the AWS Management Console and navigate to the AWS Macie\n\nservice.\n\n2. Click on Get started.\n\n3. Click on Enable Macie.\n\nHow to do it…\n\n1. Log in to the AWS Management Console and navigate to the AWS Macie\n\nservice.\n\n2. From the le navigation pane, choose Jobs under the S3 buckets section.\n\n3. Click on Create job. You will get a con\u0000rmation message saying that\n\ncreating a job is not included in the free trial; acknowledge the message\n\nby clicking on Yes.\n\n4. Under Choose S3 buckets, click on Select speci\u0000c buckets, choose your\n\nS3 bucket, and click on Next as shown:\n\nFigure 6.1 – Selecting an S3 bucket for Macie’s job\n\n5. On the Review S3 buckets page, your S3 bucket will be listed with its\n\nestimated cost. Review the values and click on Next.\n\n6. In the Re\u0000ne the scope page, under Sensitive data discovery options, you\n\nchoose either to have a scheduled job where you can set up the update\n\nfrequency for the job or have a one-time job that will run the job only\n\nonce. Optionally, you can set up a criterion for the object the job will run\n\non by expanding the Additional settings section and choosing Object\n\ncriteria.\n\nFigure 6.2 – Setting up the job run frequency\n\n7. For Select managed data identi\u0000ers, under Managed data identi\u0000er\n\noptions, you can either choose to use all of the managed data identi\u0000ers\n\nMacie provides by clicking on Recommended, you can choose to select\n\nthe speci\u0000c identi\u0000ers you want to apply, or even choose not to apply any\n\nidenti\u0000ers by clicking on Custom. For this recipe, we will go with the\n\nrecommended option to identify any potential PII data. Click on Next.\n\nFigure 6.3 – Choosing the managed data identiﬁers\n\n8. Managed data identi\u0000ers will not detect email addresses; so, we will\n\ncreate a custom identi\u0000er for that by clicking on Manage custom\n\nidenti\u0000ers under Custom data identi\u0000ers and do the following:\n\nI. Click on Create.\n\nII. Give a name and description to your identi\u0000er.\n\nIII. Enter (?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.\n\n[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\n\n\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\n\n\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\n\n\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-\n\nz0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\n\n|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-\n\n9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4]\n\n[0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*\n\n[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\n\n\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\n\n\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\]) in the Regular\n\nexpression \u0000eld, which will be used to identify the email\n\naddresses.\n\nIV. Optionally, you can choose to add keywords and ignore words,\n\nwhich are words that Macie will ignore in the text even if they\n\nmatch the pattern, and the maximum match distance, which is\n\nthe number of characters that can appear between the keyword\n\nto ignore and the sensitive data. If the distance between the\n\nkeyword and the sensitive data is within this speci\u0000ed limit,\n\nMacie will ignore the match.\n\nV. For Severity, you can keep the default value of using the\n\nMinimum severity setting for any number of matches or use a\n\ncustom setting to determine severity.\n\nVI. Optionally, add tags to label your identi\u0000er and click on Submit.\n\nVII. Go back to the main page and click on the refresh mark to\n\nre\u0000ect the new identi\u0000er, select it, and click on Next.\n\n9. Under Select allow lists, you can optionally create an allow list to ignore\n\nspeci\u0000c text or text patterns by clicking on Manage allow lists.\n\n10. Under the Review and create page, review all the values you have\n\nprovided and click on Submit.\n\n11. e job will be in running status for some time depending on the data\n\nsize. Once it moves to completed status, click on Show results and then\n\nShow \u0000ndings, which will give you a report of the identi\u0000ed sensitive\n\ndata within your S3 bucket.\n\nHow it works…\n\nWe kickstarted the job creation process by de\u0000ning the targeted\n\ndata scope for Macie to scan, specifying a particular S3 bucket.\n\nMoving forward with the job con\u0000guration, we chose to leverage all\n\nof Macie’s managed data identi\u0000ers and introduced our custom data\n\nidenti\u0000er since the existing ones lack a pattern for recognizing\n\nemail addresses. To address this, we de\u0000ned a regular expression:\n\nthe \u0000rst part (before @) matches the local part of email addresses,\n\nallowing both alphanumeric and special characters, which can be\n\nenclosed in quotes or not. e second part matches the domain,\n\nsupporting both standard domain names with subdomains or an IP\n\naddress. Aer submitting the job, Macie scanned the data within\n\nour designated bucket to locate sensitive data in accordance with\n\nour chosen data identi\u0000ers and its machine learning algorithms.\n\nUpon completion, it gave us a comprehensive report detailing the\n\nidenti\u0000ed sensitive data.\n\nThere’s more…\n\nYou can ensure that sensitive data discovered by Macie is promptly\n\nhandled by con\u0000guring alerts and remediation actions. You can set\n\nup an EventBridge rule to be triggered when Macie discovers\n\nsensitive data, which in turn triggers a Lambda function that\n\nremediates it, such as by encrypting the data or deleting it. For\n\nalerts, you can con\u0000gure SNS topics to notify you of Macie’s\n\n\u0000ndings.\n\nSee also\n\nProcessing Macie \u0000ndings with Amazon EventBridge:\n\nhttps://docs.aws.amazon.com/macie/latest/user/\u0000ndings-monitor-\n\nevents-eventbridge.html\n\nStoring and retaining sensitive data discovery results with Amazon Macie:\n\nhttps://docs.aws.amazon.com/macie/latest/user/discovery-results-\n\nrepository-s3.html\n\nEstablishing a tagging strategy for AWS resources\n\nEstablishing a comprehensive resource tagging strategy is pivotal in\n\nthe eﬃcient organization, management, and optimization of AWS\n\nresources. It simpli\u0000es the identi\u0000cation and oversight of resources\n\nwhile facilitating precise cost attribution to designated projects,\n\nteams, or business units. To implement an eﬀective tagging strategy,\n\nit is imperative to assign metadata labels, structured as key-value\n\npairs, to diverse AWS resources based on speci\u0000c attributes or\n\ncriteria. is strategic approach is fundamental for achieving\n\noperational eﬃciency, cost-eﬀectiveness, and governance within the\n\ncloud environment. Subsequently, employing proactive and reactive\n\nmeasures aligning with the established tagging strategy ensures the\n\nsystematic tagging of resources.\n\nIn this recipe, we will learn how to proactively tag resources (EC2,\n\nS3, Lambda) using Lambda function.\n\nGetting ready\n\nis recipe assumes you have AWS CLI installed and con\u0000gured\n\nwith the IAM pro\u0000le, and the IAM role used in Lambda must have\n\nthe following outlined policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowLambdaTagging\", \"Effect\": \"Allow\", \"Action\": [ \"lambda:TagResource\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowEC2Tagging\", \"Effect\": \"Allow\", \"Action\": [\n\n\"ec2:CreateTags\", \"ec2:DescribeInstances\", \"ec2:DescribeVolumes\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowS3Tagging\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetBucketTagging\", \"s3:PutBucketTagging\" ], \"Resource\": \"arn:aws:s3:::*\" }, { \"Sid\": \"AllowCloudTrailLookup\", \"Effect\": \"Allow\", \"Action\": [ \"cloudtrail:LookupEvents\" ], \"Resource\": \"*\" } ] }\n\nAdditionally, CloudTrail must be enabled. You can follow this guide\n\nto enable it:\n\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudt\n\nrail-create-a-trail-using-the-console-\u0000rst-time.html.\n\nHow to do it…\n\n1. Create a Lambda function with the help of the following steps:\n\nI. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?\n\nnc2=h_ct&src=header-signin) and navigate to the Lambda\n\nservice.\n\nII. Click on Create function.\n\nIII. Select Author from scratch in the Basic information section,\n\ngive a name for the function, set the Runtime option to Python\n\n3.11, and select x86_64 as Architecture.\n\nIV. Click on Create function.\n\nV. In the code editor, replace the existing code with the following\n\ncode, which only handles tagging Lambda functions (you can\n\nrefer to the GitHub code for the full code to handle S3 and EC2\n\ntagging):\n\nimport json import boto3 import logging from botocore.exceptions import ClientError logger = logging.getLogger() logger.setLevel(logging.INFO) def get_user_name(event): if 'userIdentity' in event['detail']: if event['detail'] ['userIdentity']['type'] == 'AssumedRole': user_name = str('UserName: '\n\n+ event['detail']['userIdentity'] ['principalId'].split(':')[1] + ', Role: ' + event['detail']['userIdentity'] ['sessionContext']['sessionIssuer'] ['userName'] + ' (role)') elif event['detail'] ['userIdentity']['type'] == 'IAMUser': user_name = event['detail'] ['userIdentity']['userName'] elif event['detail'] ['userIdentity']['type'] == 'Root': user_name = 'root' else: logging.info('Could not determine username (unknown iam userIdentity) ') user_name = '' else: logging.info('Could not determine username (no userIdentity data in cloudtrail') user_name = '' return user_name def lambda_handler(event, context): client = boto3.client('cloudtrail') resource_type = event[\"detail\"] [\"eventSource\"] user_name=get_user_name(event) if resource_type == \"lambda.amazonaws.com\": resource_arn = event[\"resources\"][0] resource_name = event[\"detail\"] [\"configurationItem\"][\"configuration\"] [\"functionName\"] try:\n\nclient = boto3.client('lambda') client.tag_resource( Resource=resource_arn, Tags={'Created_by': user_name} ) logging.info(f\"Lambda function {resource_name} tagged with username : {user_name}\") except ClientError as e: logging.error(f\"Error tagging Lambda function {resource_name}: {e}\")\n\nVI. Click on Deploy.\n\nVII. Go to the Con\u0000guration tab and open the IAM role under\n\nExecution Role.\n\nVIII. Click on Add permissions.\n\n2. Create EventBridge rule with the next steps:\n\nI. Navigate to the EventBridge service and click on Create rule.\n\nII. Give a name and optionally a description for the rule, keep\n\nEvent bus as default, select Rule with an event pattern, and then\n\nclick on Next.\n\nIII. Under Creation method, select Custom pattern (JSON editor),\n\npaste the following rule, and click on Next:\n\n{ \"source\": [\"aws.s3\", \"aws.lambda\", \"aws.ec2\"],\n\n\"detail-type\": [\"AWS API Call via CloudTrail\"], \"detail\": { \"eventSource\": [\"s3.amazonaws.com\", \"lambda.amazonaws.com\", \"ec2.amazonaws.com\"], \"eventName\": [\"CreateBucket\", \"CreateFunction\", \"RunInstances\"] } }\n\nIV. Under Target 1, choose AWS service, select Lambda function,\n\nthen select the function you have created, and then click on\n\nNext and Create rule.\n\nIf you go back to your Lambda function, an EventBridge rule\n\ntrigger will be added.\n\nHow it works…\n\nWe enabled CloudTrail to log all write actions to our services,\n\nincluding the creation of Lambda functions, S3 buckets, and EC2\n\ninstances. en, we set up an EventBridge rule that listens for these\n\nCloudTrail events related to the creation of these resources. is\n\nrule triggers a Lambda function, which checks the type of event it\n\nreceives. Based on the event type, the Lambda function retrieves the\n\ncurrent tags for the resource and appends a tag with the user\n\nidentity that created the resource, as obtained from the CloudTrail\n\nevent.\n\nThere’s more…\n\nYou can have additional tags added to your resources that you can\n\nget from CloudTrail events, such as the created time or AWS region,\n\nand you can add other taggable resources to the process.\n\nSee also\n\nBuilding your tagging strategy:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/tagging-best-\n\npractices/building-your-tagging-strategy.html\n\nBuilding your distributed data community with Amazon DataZone following data mesh principles\n\nData mesh is a decentralized approach to data architecture, aiming\n\nto address the challenges of traditional centralized models by\n\ndistributing responsibility for data and treating it as a product,\n\nwhich shis the responsibility from a central team to domain-\n\nspeci\u0000c teams, leveraging domain expertise. Key principles in data\n\nmesh involve the following:\n\nDomain-oriented decentralization where each domain will be\n\nresponsible for creating their data as a product and making it available to\n\nothers\n\nSelf-serve data infrastructure that empowers each domain to\n\nindependently create, manage, and utilize their own data pipelines\n\nwithout relying on a central team\n\nA federated computational governance model where each domain is\n\nresponsible for the governance of its data, ensuring compliance with\n\nglobal standards while maintaining domain-speci\u0000c requirements\n\nAmazon DataZone, based on data mesh principles, provides a\n\nmanaged platform for data governance and access control.\n\nIn this recipe, we will learn how to use DataZone to set up a\n\ndomain, publish Glue tables, and make it accessible to others.\n\nGetting ready\n\nTo complete this recipe, you need to have a Glue table that is\n\nmanaged by Data Lake (Data Lake permission mode, not hybrid\n\nmode), which means the S3 location of the table must be registered\n\non Lake Formation and permission to IAMAllowedPrincipals\n\nmust be revoked (refer to the Getting ready section of the\n\nSynchronizing Glue Data Catalog to a diﬀerent account recipe in\n\nChapter 2, Sharing Your Data Across Environments and Accounts).\n\nHow to do it…\n\nis recipe involves diﬀerent tasks, as discussed under each\n\nsubsection.\n\nCreating a domain with the next steps\n\n1. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the DataZone service.\n\n2. Click on Create domain.\n\n3. Enter a name and optionally a description for your domain.\n\n4. Under the Quick setup section, select Set-up this account for data\n\nconsumption and publishing, which will enable Data Lake and Data\n\nWarehouse blueprints. It will create default environment pro\u0000les that are\n\nopen to all users.\n\n5. Click on Create domain. Once your domain is created, you can click on\n\nOpen data portal to start building the catalog.\n\nFigure 6.4 – Domain summary\n\nPublishing data to your domain\n\n1. Create a project:\n\nI. Open the data portal of your domain.\n\nII. Click on Create project.\n\nIII. Enter a name and optionally a description for your project.\n\nFigure 6.5 – Creating a project\n\n2. Create environment:\n\nI. On your project page, click on CREATE ENVIRONMENT.\n\nII. Enter a name and optionally a description for your\n\nenvironment.\n\nIII. Under Environment pro\u0000le, choose DataLakeProfile.\n\nIV. Click on CREATE ENVIRONMENT.\n\nFigure 6.6 – Creating an environment for the project\n\n3. Create data source:\n\nI. Go to the Data tab in the portal.\n\nII. Enter a name and optionally a description for your data source.\n\nIII. Under Data Source type, choose AWS Glue.\n\nIV. Select the environment you created in the previous step in the\n\nSelect an environment section.\n\nV. Under Data Selection, write down the name of the Glue\n\ndatabase you need to add, and under Table selection criteria,\n\nchoose Include * to include all the tables in your database and\n\nclick on Next.\n\nFigure 6.7 – Selecting a data source type\n\nVI. Under the Publishing setting section, select No for Publish assets to the\n\ncatalog, which will allow us to review and edit the assets before\n\npublishing them. Select automated business name generation and click\n\non Next.\n\nFigure 6.8 – Publishing setting\n\nVII. Under Run preference, you can choose to run the data source on\n\ndemand or to run it based on a schedule, and then click on Next.\n\nVIII. Review all the details you have provided and click on Create.\n\nIX. Click on RUN to trigger the data source. Once the run is complete, your\n\nGlue database tables will be added as assets.\n\nX. Open each asset created on your data source and review the business\n\nname that was generated for you and the business attribute names in the\n\nSchema tab. Accept or edit them if required. You can also provide\n\nmetadata to your assets such as a README \u0000le and glossary terms.\n\nOnce done with the changes, click on PUBLISH ASSET.\n\nXI. To allow DataZone to grant access to consumers on your behalf, grant\n\nthe DataZone access role, <AmazonDataZoneGlueAccess-\n\n<region>-<domainId>, the following permissions through Lake\n\nFormation, as shown in Figure 6.9:\n\nDatabase permissions\n\nTable permissions\n\nFigure 6.9 – DataZone Lake Formation permissions\n\nConsuming data from the domain\n\n1. Log in to the AWS Management Console using the consumer user or\n\nrole and navigate to the DataZone service.\n\n2. Create a new project and environment for the consumer following the\n\nsame steps done for the producer.\n\n3. In the search bar, search for the table/asset you have added as a\n\nproducer, add a justi\u0000cation for subscribing to the data under the\n\nReason for request \u0000eld, and click on SUBSCRIBE.\n\nApproving consumer access request",
      "page_number": 415
    },
    {
      "number": 22,
      "title": "Segment 22 (pages 436-454)",
      "start_page": 436,
      "end_page": 454,
      "detection_method": "synthetic",
      "content": "1. Navigate to the DataZone portal and choose the project with the\n\npublished data asset.\n\n2. Go to the Data tab and click on Incoming requests from the navigation\n\npane on the le. You should be able to see the request under\n\nREQUESTED – click on View request and approve it.\n\nHow it works…\n\nWe initiated a domain through the quick setup, allowing DataZone\n\nto automate the creation of IAM roles and facilitate domain access\n\nto AWS resources such as Glue, Athena, and Redshi. As\n\npublishers, we con\u0000gured a collaborative project for managing asset\n\naccess and ownership. Within the project, we created an\n\nenvironment using a prede\u0000ned template pro\u0000le to host our\n\nresources. DataZone background processes generated IAM roles, S3\n\nsuﬃxes, Athena workgroups, and AWS Glue databases,\n\nstreamlining Data Lake operations. We proceeded to create a data\n\nsource that de\u0000ned the data to bring, adding assets visible only to\n\nproject members until published to the catalog for wider discovery.\n\nAs a subscriber, we established our own project and environment,\n\nsubscribing to assets that notify publishers for approval. Once\n\napproved by publishers, assets were then accessible for querying in\n\nAthena within the environment.\n\nThere’s more…\n\nAer publishing an asset, you can still edit its business or technical\n\nvalues, which will create a new version of your asset that you will\n\nhave to publish to make it discoverable.\n\nSee also\n\nSecurity in DataZone:\n\nhttps://docs.aws.amazon.com/datazone/latest/userguide/security.html\n\nMonitoring Amazon DataZone:\n\nhttps://docs.aws.amazon.com/datazone/latest/userguide/monitoring-\n\noverview.html\n\nHandling security-sensitive data (PII and PHI)\n\nRedshi data tokenization is a crucial aspect of data security and\n\nprivacy within AWS Redshi to protect the con\u0000dentiality of\n\ncritical data. Tokenization involves substituting sensitive\n\ninformation with unique identi\u0000ers or tokens, preserving the\n\nformat and length of the original data without revealing the actual\n\nsensitive information. Tokenization is necessary to mitigate the risk\n\nof data breaches, comply with regulatory requirements, and\n\nmaintain customer trust by ensuring their personal information is\n\nsecure. Leveraging AWS Lambda user-de\u0000ned functions (UDFs) for\n\ndata tokenization in Redshi provides a scalable and eﬃcient\n\nsolution for protecting the data.\n\nIn this recipe, we will learn how to create a Lambda UDF to use in\n\nRedshi for tokenization.\n\nGetting ready\n\nTo complete this recipe, you must have a Redshi cluster with a\n\ntable in which you need to tokenize its data. You need to have a\n\nCloud9 environment as well, please create one following the\n\noutlined steps:\n\n1. Log in to the AWS Management Console and navigate to the Cloud9\n\nservice.\n\n2. Click on Create environment.\n\n3. Give a name and a description for the environment.\n\n4. Select New EC2 instance, then select the t2.micro instance and\n\nAmazon Linux 2 for the Platform setting.\n\n5. Keep everything else set to default and click on Create.\n\nHow to do it…\n\n1. Create a secret key in Secrets Manager:\n\nI. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?\n\nnc2=h_ct&src=header-signin) and navigate to Secret manager.\n\nII. Click on Store new secret.\n\nIII. Choose Other type of secret for Secret type. Under Key/value\n\npairs, type secret_key and the value you would like to use\n\nfor the key. It should be a long, complex value, at least 32\n\ncharacters of mixed uppercase and lowercase characters, digits,\n\nand special characters. en, click on Next.\n\nIV. Give a name for the secret and then click on Next. Review the\n\ndetails and click on Store.\n\n2. Create a Lambda layer:\n\nI. Navigate to Cloud9 service.\n\nII. Open a Linux environment and run the following commands.\n\nReplace aws_region with the region you will use to create\n\nthe Lambda function:\n\nsudo amazon-linux-extras install python3.8 curl -O https://bootstrap.pypa.io/get- pip.py python3.8 get-pip.py --user mkdir python python3.8 -m pip install pyffx -t python/ zip -r layer.zip python aws lambda publish-layer-version -- layer-name pyffx-layer --zip-file fileb://layer.zip --compatible-runtimes python3.8 --region <aws_region>\n\n3. Create a Lambda function:\n\nI. Navigate to the Lambda service.\n\nII. Click on Create function.\n\nIII. Select Author from scratch.\n\nIV. In the Basic information section, give a name for the function,\n\nset the Runtime setting as Python 3.11, and select x86_64 as\n\nArchitecture.\n\nV. Click on Create function.\n\nVI. Under Layers in your Lambda function’s Code tab, select Add a\n\nlayer.\n\nVII. Select Custom layers, choose pyffx-layer created in the\n\nprevious step, and select the latest version you have created.\n\nVIII. In the code editor, replace the existing code with the following\n\ncode:\n\nimport boto3 import json import numbers import pyffx from botocore.exceptions import ClientError, BotoCoreError def get_secret(): try: client = boto3.client('secretsmanager') response = client.get_secret_value(SecretId='Redshi\n\nftTokenizationSecretKey') secret = json.loads(response['SecretString']) return secret['secret_key'] except (ClientError, BotoCoreError) as e: return json.dumps({'success': False, 'message': f'Failed to retrieve secret key: {e}'}) def encrypt_data(data, secret_key): alphabet = '0123456789abcdefghijklmnopqrstuvwxyz' try: if isinstance(data, numbers.Number): e = pyffx.Integer(secret_key.encode(), length=len(str(data))) else: e = pyffx.String(secret_key.encode(), alphabet=alphabet, length=len(data)) encrypted_text = e.encrypt(data) return encrypted_text except Exception as e: return json.dumps({'success': False, 'message': f'Failed to encrypt text: {e}'}) def lambda_handler(event, context): secret_key = get_secret() return_value = dict() response = [] for argument in event['arguments']: msg = argument[0] encrypted_text = encrypt_data(msg, secret_key)\n\nresponse.append(json.dumps(encrypted_tex t)) return_value['success'] = True return_value['results'] = response return json.dumps(return_value) )\n\nIX. Click on Deploy.\n\n4. Create an IAM role for Redshi:\n\nI. Navigate to the IAM service.\n\nII. Select Policies under Access management from the navigation\n\npane on the le, and click on Create Policy.\n\nIII. Choose the JSON tab and in the Policy editor, replace the code\n\nwith the following code (make sure to add your Lambda\n\nfunction ARN, which you can get from the function overview),\n\nand click on Next:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Invoke\", \"Effect\": \"Allow\", \"Action\": [ \"lambda:InvokeFunction\" ], \"Resource\": \" <lambda_function_arn>\" }\n\n] }\n\nIV. Give a policy name and optionally a description, and then click\n\non Create policy.\n\nV. Select Roles under Access management from the navigation\n\npane on the le, and click on Create Role.\n\nVI. Under Trusted entity type, choose AWS service.\n\nVII. Under Use case, choose Redshi - Customizable and click on\n\nNext.\n\nVIII. Select the policy you have created and click on Next.\n\nIX. Enter a role name and optionally a description, and click on\n\nCreate role.\n\n5. Associate the IAM role with the Redshi cluster:\n\nI. Navigate to the Redshi service.\n\nII. Open your Redshi cluster and go to the Properties tab.\n\nIII. Under the Cluster permissions, select Associate IAM roles from\n\nthe Manage IAM roles list. en, select the IAM role you have\n\ncreated in the previous step and click on Associate IAM roles.\n\n6. Create a UDF in Redshi:\n\nI. Connect to your Redshi cluster.\n\nII. Use the following commands to create the UDF:\n\nCREATE OR REPLACE EXTERNAL FUNCTION\n\nPII_tokenize_str (value varchar) RETURNS varchar STABLE LAMBDA 'your_lambda_function_name' IAM_ROLE 'your_redshift_role_arn'; CREATE OR REPLACE EXTERNAL FUNCTION PII_tokenize_int (value int) RETURNS varchar STABLE LAMBDA 'your_lambda_function_name' IAM_ROLE 'your_redshift_role_arn';\n\nHow it works…\n\nWe developed a Lambda function capable of encrypting text or\n\nintegers using the pyffx library, which implements format-\n\npreserving encryption (FPE). is ensures that the encrypted data\n\nretains the same structure, type, and length as the original input.\n\nWe used Secrets Manager to securely store our secret key, which is\n\nused by pyffx internally to generate a pseudo-random\n\npermutation that is applied to the input text. We provided the\n\nalphabet for encrypting strings, which is used by pyffx to de\u0000ne\n\nwhich characters should appear in the encrypted text. For numeric\n\ndata identi\u0000ed using the Numbers library, no alphabet is needed, as\n\npyffx will keep the numeric representation of the input. Since\n\nLambda doesn’t include the pyffx library by default, we addressed\n\nthis by installing and packaging it within a Lambda layer, which has\n\nbeen added to the code. To enable Redshi to invoke this Lambda\n\nfunction, we established an IAM role providing the necessary\n\npermissions, which has been associated with the Redshi cluster.\n\nWithin the Redshi cluster, we created a function that references\n\nthe Lambda function, utilizing the IAM role we established. As a\n\nresult, Redshi is now equipped to call the function and encrypt the\n\ndata.\n\nThere’s more…\n\nIf you want to be able to reverse the tokenization, you can create a\n\ndecrypt function by following the same steps from How to do it…\n\nbut in the Lambda function; replace the encrypt_data function\n\nwith a decrypt_data function and update the Lambda handler\n\nusing the following code (refer to DataDecryptionLambda.py\n\n\u0000le in GitHub for the full Lambda function code):\n\ndef decrypt_data(token, is_number, secret_key): alphabet = '0123456789abcdefghijklmnopqrstuvwxyz' try: if is_number: d = pyffx.Integer(secret_key.encode(), length=len(str(token))) else: d = pyffx.String(secret_key.encode(), alphabet=alphabet, length=len(token)) decrypted_text = d.decrypt(token) return decrypted_text except Exception as e:\n\nreturn { 'success': False, 'error': str(e) } def lambda_handler(event, context): secret_key = get_secret() return_value = dict() response = [] for argument in event['arguments']: token = argument[0] is_number = argument[1] try: result = decrypt_data(token, is_number, secret_key) response.append(json.dumps(result)) except Exception as e: return { 'success': False, 'error': str(e) } return_value['success'] = True return_value['results'] = response return json.dumps(return_value)\n\nSee also\n\ne pyﬃx code repository: https://github.com/emulbreh/pyﬀx\n\nEnsuring S3 compliance with AWS Config\n\nEnsuring that your AWS resources are con\u0000gured according to your\n\nspeci\u0000cations and best practices is crucial. AWS Con\u0000g facilitates\n\nthe governance of your resources by continuously evaluating your\n\nresources against your prede\u0000ned rules. It provides a\n\ncomprehensive view of your resources, enabling you to monitor and\n\ntake corrective actions if any resource deviates from these rules and\n\nbecomes non-compliant.\n\nIn Chapter 1, Managing Data Lake Storage, we discussed the\n\nimportance of encryption, life cycle policies, and access control for\n\nS3 buckets. In this recipe, we will learn how to implement AWS\n\nCon\u0000g rules to verify the compliance of S3 buckets with these\n\nstandards.\n\nGetting ready\n\nFor this recipe, you’ll need S3 buckets that you will monitor for\n\ncompliance, and you must enable AWS CloudTrail, as AWS Con\u0000g\n\nrelies on CloudTrail logs to track and record resource\n\ncon\u0000gurations.\n\nHow to do it…\n\n1. Set up the rules:\n\nI. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?\n\nnc2=h_ct&src=header-signin) and navigate to the AWS Con\u0000g\n\nservice.\n\nII. Click on Get started.\n\nIII. Under Recording strategy, select Speci\u0000c resource types and\n\nchoose AWS S3 Bucket for Resource type. For the Frequency\n\nsetting, select Daily.\n\nIV. Under Data governance, choose Create AWS Con\u0000g service-\n\nlinked role.\n\nV. Under Delivery method, either select Choose a bucket from\n\nyour account if you have an existing bucket for storing con\u0000g\n\ndata or select Create a bucket to create a new one. Specify the\n\nbucket name and an optional pre\u0000x for storing the data, which\n\nis helpful if you are using an existing bucket.\n\nVI. Click on Next. On the Rule page, under AWS Managed Rules,\n\nselect s3-bucket-level-public-access-\n\nprohibited and s3-bucket-server-side-\n\nencryption-enabled.\n\nVII. Select Next. Review the setting and click on Con\u0000rm.\n\nFigure 6.10 – Reviewing conﬁg settings\n\n2. Enable remediation:\n\nI. From the navigation pane on the le of your AWS Con\u0000g\n\ndashboard, select Rules.\n\nII. Open one rule at a time, and from the Actions drop-down list,\n\nselect Manage remediation.\n\nIII. Choose either Automatic remediation for AWS Con\u0000g to take\n\naction automatically on your behalf or Manual remediation if\n\nyou want to review the non-compliant bucket before\n\nremediating it.\n\nIV. Under Remediation action details, choose the following\n\nsettings:\n\nFor s3-bucket-level-public-access-\n\nprohibited, choose\n\nAWSConfigRemediation-\n\nConfigureS3BucketPublicAccessBlock\n\nFor s3-bucket-server-side-encryption-\n\nenabled choose aws-\n\nenables3bucketencryption and Bucketname\n\nunder Resource ID parameter\n\nV. Click on Save changes.\n\nHow it works…\n\nWe enabled AWS Con\u0000g speci\u0000cally to track S3 buckets and\n\nselected two managed rules to check if the buckets are publicly\n\naccessible and if server-side encryption is enabled. AWS Con\u0000g\n\ngenerated a dashboard displaying the resources in the account,\n\nfocusing on S3 buckets in this recipe. It showed the compliance\n\nstatus of your S3 buckets based on the rules set, scanning them\n\ndaily and \u0000agging compliant buckets accordingly while highlighting\n\nnon-compliant ones for remediation. Integration with AWS Systems\n\nManager Automation documents allowed us to de\u0000ne remediation\n\nactions for each rule to rectify non-compliant buckets. If you\n\nidentify a non-compliant bucket within a rule, you can easily\n\nremediate it by selecting it and clicking on the Remediate button.\n\nFigure 6.11 – S3 bucket remediation\n\nAdditionally, AWS Con\u0000g maintains a historical record of\n\ncon\u0000guration changes for your S3 buckets, providing valuable audit\n\ntrails. You can review these changes by accessing the S3 bucket via\n\nResource Inventory in the dashboard and examining Resource\n\nTimeline, which details CloudTrail events and con\u0000guration\n\nchanges.\n\nFigure 6.12 – S3 bucket historical events\n\nThere’s more…\n\nIf you require a custom rule that isn’t available among AWS-\n\nmanaged rules, you can create it using AWS Lambda. Let’s create a\n\nrule to monitor whether our S3 buckets have life cycle policies\n\ncon\u0000gured to either delete objects aer a speci\u0000ed time or archive\n\nthem:\n\n1. Create a Python Lambda function:\n\nI. Create a Python function from scratch with a new IAM role.\n\nII. Add the following policies for the function IAM role:\n\nAWSConfigRulesExecutionRole AWS-\n\nmanaged policy\n\nRun the following command in AWS CLI or AWS\n\nCloudShell to create a resource-based policy that will\n\nallow Con\u0000g to invoke our function:\n\naws lambda add-permission -- function-name your_function_arn \\ --principal config.amazonaws.com \\ --statement-id AllowConfigInvoke \\ --action lambda:InvokeFunction \\ --source-account your_aws_account_id\n\nCreate an inline policy with the following permission:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\",\n\n\"Action\": [\n\n\"s3:GetLifecycleConfiguration\" ], \"Resource\": [ \"*\" ] } ] }\n\nIII. Add the following code, which AWS Con\u0000g will invoke for each\n\nbucket:\n\nimport boto3 import json from botocore.exceptions import ClientError def lambda_handler(event, context): invoking_event = json.loads(event['invokingEvent']) configuration_item = invoking_event['configurationItem'] # Extract the bucket name from the configurationItem bucket_name = configuration_item['configuration'] ['name'] s3_client = boto3.client('s3') compliance_type = 'NON_COMPLIANT' try: lifecycle_policy = s3_client.get_bucket_lifecycle_configura tion(Bucket=bucket_name) for rule in lifecycle_policy.get('Rules', []):\n\nif ('Transitions' in rule and any('StorageClass' in transition and transition.get('StorageClass', '') in ['GLACIER', 'GLACIER_DEEP_ARCHIVE'] for transition in rule['Transitions'])) or \\\n\n('NoncurrentVersionTransition' in rule and any('StorageClass' in transition and transition.get('StorageClass', '') in ['GLACIER', 'GLACIER_DEEP_ARCHIVE'] for transition in rule.get('NoncurrentVersionTransition', []))) or \\ ('Expiration' in rule and 'Days' in rule.get('Expiration', {})) or \\\n\n('NoncurrentVersionExpiration' in rule and 'NoncurrentDays' in rule.get('NoncurrentVersionExpiration', {})): compliance_type = 'COMPLIANT' message = 'The bucket has a lifecycle policy with expiration after specified days.' break else: message = 'The bucket lifecycle policy does not archive or delete objects.' except ClientError as e: message = f\"Error getting lifecycle configuration for bucket {bucket_name}: {e}\" evaluation_result = {",
      "page_number": 436
    },
    {
      "number": 23,
      "title": "Segment 23 (pages 455-477)",
      "start_page": 455,
      "end_page": 477,
      "detection_method": "synthetic",
      "content": "'ComplianceResourceType': 'AWS::S3::Bucket', 'ComplianceResourceId': bucket_name, 'ComplianceType': compliance_type, 'OrderingTimestamp': configuration_item['configurationItemCap tureTime'], 'Annotation': message } config_client = boto3.client('config') response = config_client.put_evaluations( Evaluations=[evaluation_result], ResultToken=event['resultToken'] ) return response\n\n2. Set up a Con\u0000g custom rule:\n\nI. From the le navigation pane of AWS con\u0000g, select Rules.\n\nII. Select Add rule, choose Create custom Lambda rule for the\n\nRule type, and click on Next.\n\nIII. Provide a name for the rule and enter the ARN of the Lambda\n\nfunction under Lambda function ARN.\n\nIV. Enable Turn on detective evaluation under Evaluation mode,\n\nselect When con\u0000guration changes for Trigger type, and choose\n\nResources for Scope of change. Select AWS resource under\n\nResource category and specify AWS S3 bucket for Resource\n\ntype.\n\nFigure 6.13 – Custom rule evaluation mode\n\nV. Select Next, review the details, and click on Save.\n\nYou will be able to track this rule along with the managed con\u0000g\n\nrules.\n\nSee also\n\nConformance Packs:\n\nhttps://docs.aws.amazon.com/con\u0000g/latest/developerguide/conformance\n\npacks.html\n\nOceanofPDF.com\n\n7 Data Quality Management\n\nUnreliable data can lead to incorrect insights, misguided business\n\ndecisions, and a signi\u0000cant loss of resources. As organizations treat\n\ndata as a product and rely more on data freshness, data engineers\n\nand analysts must implement robust data quality control\n\nmechanisms to ensure the data’s accuracy, completeness,\n\nconsistency, and reliability to maintain high data quality standards.\n\nIn this chapter, we will explore various methods and tools available\n\non AWS for maintaining data quality. We’ll provide step-by-step\n\nrecipes to help you implement these tools eﬀectively in your data\n\nengineering work\u0000ows. e recipes will guide you through practical\n\nexamples, starting with data quality control using AWS DataBrew,\n\nDeequ, and Glue. Before diving into the chapter, it is important to\n\nwork with your stakeholders to build a data quality control\n\nframework and an SLA for your data quality. When you lead a data\n\nquality project, besides identifying the data owners, you need to\n\nwork with the data owners to create a process to manage and ensure\n\nthe data quality. For this chapter, we won’t explore the theoretical\n\ndetails of building the data quality scorecards or de\u0000ning the\n\nmetrics and process for a data quality project. It would be bene\u0000cial\n\nfor you to explore the book DMBoK – Data Management Body of\n\nKnowledge for supplementary knowledge on how to work with\n\nstakeholders on running a data quality project.\n\nWe will cover the following recipes in this chapter:\n\nCreating data quality for ETL jobs in AWS Glue Studio notebooks\n\nUnit testing your data quality using Deequ\n\nSchema management for ETL pipelines\n\nBuilding unit test functions for ETL pipelines\n\nBuilding data cleaning and pro\u0000ling jobs with DataBrew\n\nTechnical requirements\n\nYou can \u0000nd the code \u0000les for this chapter on GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter07.\n\ne dataset for this chapter is available at\n\nhttps://github.com/tidyverse/dplyr/blob/main/data-\n\nraw/starwars.csv.\n\nCreating data quality for ETL jobs in AWS Glue Studio notebooks\n\nFrom the Applying a data quality check on Glue tables recipe in\n\nChapter 6, Governing Your Platform, we learned how to set a ruleset\n\nfor the Glue pipeline. In this recipe, we will dive deeper into how to\n\nuse Glue Studio notebooks to build a Data Quality template. Using\n\nGlue Studio is useful because you can see the output along with the\n\ndataset that you are testing. We will also introduce how to use\n\ncaching and produce row-level and rule-level outputs. e row-level\n\noutput would be suitable for using data quality rule violations for\n\neach of the records.\n\nGetting ready\n\nBefore proceeding with this recipe, go through the Applying a data\n\nquality check on Glue tables recipe in Chapter 6, Governing Your\n\nPlatform, and ensure that you have basic knowledge of how Glue\n\nworks as covered in Chapter 3, Ingesting and Transforming Your\n\nData with AWS Glue. In this recipe, we will provide the code to run\n\nthe quality check scenarios, so you do not need to follow the steps\n\nto create ruleset steps from Chapter 6.\n\nIn this recipe, we will assume that you already have the relevant\n\nIAM for Glue and a dataset against which to run Data Quality. e\n\npublic dataset we will use as an example is starwars.csv that is\n\nrenamed as star_wars_characters.csv.\n\nHow to do it…\n\n1. Head to the Glue console and click on Author using an interactive code\n\nnotebook:\n\nFigure 7.1 – Clicking on the script editor to edit the code\n\n2. Instead of using the console to create a ruleset as we did in Chapter 6, in\n\nthis recipe, we will examine how to use the Glue Studio notebook. e\n\n\u0000rst step is to import the sample data quality notebook from Recipe1\n\nin this chapter’s GitHub folder and select the relevant IAM role that you\n\ncreated in Chapter 3 or Chapter 6.\n\nFigure 7.2 – Uploading the sample notebook from GitHub\n\n3. e next step is to set up an interactive session and import relevant\n\nlibraries. For this recipe, the code is edited.\n\nFigure 7.3 – Setting up a Glue session\n\n4. For this recipe, we will create rules to compare two DataFrames with\n\neach other as shown. We will call these two DataFrames main and\n\nreference.\n\nFigure 7.4 – Creating two DataFrames for evaluating\n\n5. e following code in this recipe will go through various data quality\n\ncheck scenarios by creating a ruleset in the Data Quality De\u0000nition\n\nLanguage (DQDL) format. We will \u0000rst go through the rules that will be\n\nused for comparing multiple DataFrames. Depending on your\n\norganization, you need to set your own rules. e ruleset de\u0000nes the\n\nfollowing rules:\n\nRowCount = 42: Checks that the number of rows in the\n\ndataset is 42\n\nColumnCount = 14: Checks that the number of columns in\n\nthe dataset is 42\n\nReferentialIntegrity \"homeworld\"\n\n\"reference.homeworld\" = 1: Checks the data integrity\n\nwith the homeworld column in the reference dataset\n\nReferentialIntegrity \"species\"\n\n\"reference.species\" = 1: Checks the data integrity\n\nwith the species column in the reference dataset\n\nFigure 7.5 – Setting up data quality rules\n\n6. For the previous rules to work, you need to add the data frame that is\n\nrequired for the data quality evaluation. In this case, the reference\n\ndataset will be used as an additional source:\n\nEvaluateDataQuality_additional_sources = { \"reference\": df_reference\n\n, }\n\n7. Next, we will validate the datasets against the ruleset. We will add the\n\nfollowing options:\n\nChange the option\n\nenableDataQualityResultsPublishing': False.\n\nis option should not be True while using Glue Studio\n\nNotebook because you do not have a Glue job to run within a\n\nnotebook. us, if this option is set to True, it will produce an\n\nerror.\n\nSet observations.scope\": \"ALL\". Specify the scope of\n\nthe observations that will be collected during the execution of\n\nthe AWS Glue job. e ALL value indicates that all observations\n\nwill be collected.\n\nUse cache input to improve the performance of the evaluation.\n\nis is useful if the input data is large or the assessment needs\n\nto be run multiple times by enabling the CACHE_INPUT\n\noption.\n\nFigure 7.6 – Using the EvaluateDataQuality function to evaluate\n\ntwo DataFrames\n\n8. Instead of producing only a single dataset, we will also use\n\nSelectFromCollection to produce rowLevelOutcomes and\n\nruleOutcomes, as shown from cell 21 and onward in the notebook.\n\nYou can use the notebook to reference how these two methods’ outputs\n\nwould be diﬀerent from each other.\n\nFigure 7.7 – A sample of result of ruleOutcomes\n\nFor more information on the output of Glue Data Quality, please\n\ncheck the Applying a data quality check on Glue tables recipe in\n\nChapter 6, Governing Your Platform.\n\nHow it works…\n\nWhen the CACHE_INPUT option is enabled, the input\n\nDynamicFrame is cached in memory during the \u0000rst execution of\n\nthe data quality evaluation. Subsequent executions of the data\n\nquality evaluation on the same input DynamicFrame will use the\n\ncached data, which can signi\u0000cantly reduce the processing time.\n\nis caching mechanism can be particularly useful in the following\n\nscenarios:\n\nWhen running the data quality evaluation multiple times on the same\n\ninput data, such as during iterative development or testing\n\nWhen the input data is large and loading it from the source every time\n\ncan be time-consuming\n\nWhen you want to optimize the performance of the data quality\n\nevaluation process and reduce the overall processing time\n\ne key diﬀerence between rowLevelOutcomes and\n\nruleOutcomes is that rowLevelOutcomes contains the original\n\ninput data with additional columns that indicate the data quality\n\nevaluation results at the row level.\n\nis output is useful when you want to identify speci\u0000c rows that\n\nfailed the data quality checks and understand the reasons for the\n\nfailures.\n\nRemember, ruleOutcomes contains the overall data quality\n\nevaluation results at the rule level. For each rule in the data quality\n\nruleset, it provides the following information:\n\nRule: e name of the data quality rule\n\nOutcome: e pass/fail status of the rule\n\nFailureReason: e reason why the rule failed (if applicable)\n\nEvaluatedMetrics: Any metrics or statistics calculated as part of\n\nthe rule evaluation\n\nis output is useful when you want to understand the overall\n\nperformance of the data quality rules and identify which rules are\n\nfailing, along with the reasons for the failures.\n\nThere’s more…\n\nEvaluateDataQuality.DATA_QUALITY_RULE_OUTCOMES_KEY\n\nis used to access the rule-level outcomes from the output of the\n\nEvaluateDataQuality transform in AWS Glue. It provides access\n\nto the overall data quality evaluation results at the rule level. is\n\nincludes information such as the following:\n\ne name of the data quality rule\n\ne pass/fail status of the rule\n\ne reason for rule failure (if applicable)\n\nAny metrics or statistics calculated as part of the rule evaluation\n\ne following code snippet will use the assert statement to evaluate\n\nthe check if there are any failed data quality rules and then raise an\n\nexception error. is code is useful when integrated into an Extract,\n\nTransform, Load (ETL) pipeline to ensure the pipeline passes the\n\ndata quality check. To turn the code from Glue Studio into an ETL\n\njob, click on the Script tab. You will see the job.commit() line\n\nadded:\n\nassert EvaluateDataQuality_output[EvaluateDataQuality.DA TA_QUALITY_RULE_OUTCOMES_KEY].filter(lambda x: x[\"Outcome\"] == \"Failed\").count() == 0, \"The job failed due to failing DQ rules\"\n\nIn the Glue Studio notebook , you can save the output to S3 with the\n\nfollowing code:\n\nglueContext.write_dynamic_frame.from_options( frame = rowLevelOutcomes_data, connection_type = \"s3\", connection_options = {\"path\": \"s3://sample-target/dq_outcome\"}, format = \"csv\")\n\nSee also\n\nMeasuring the performance of AWS Glue Data Quality for ETL pipelines:\n\nhttps://aws.amazon.com/blogs/big-data/measure-performance-of-aws-\n\nglue-data-quality-for-etl-pipelines/\n\nEvaluating data quality for ETL jobs in AWS Glue Studio:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/tutorial-data-quality.html\n\nUnit testing your data quality using Deequ\n\nAmazon Deequ is an open source data quality library developed\n\ninternally at Amazon. e purpose of Deequ is to unit test data\n\nbefore feeding it to analytics use cases. Several analytics products\n\nsuch as DataBrew and Glue Data Quality were built upon the\n\nDeequ library to help serve the needs of data engineers and data\n\nscientists. See the Deequ GitHub page\n\n(https://github.com/awslabs/deequ) for more information.\n\nIn the previous recipe, we learned about Glue Data Quality. ere\n\nare several key considerations when choosing between AWS Glue\n\nData Quality and Deequ:\n\nManaged service versus open source library: AWS Glue Data Quality is a\n\nfully managed service built on top of the open source Deequ framework.\n\nDeequ is an open source library that you can use to implement data\n\nquality checks in your applications. Also, since Deequ is an open source\n\nlibrary, there are metrics that might be available on Deequ but are not\n\n(yet) available on AWS Glue Data Quality, such as RatioOfSums (at the\n\ntime of writing this book).\n\nDeployment and integration: AWS Glue Data Quality mainly focuses on\n\nthe Glue pipeline. In contrast, Deequ is an open source library that can\n\nbe used in any environment that supports the JVM, which allows users\n\nto have more \u0000exibility in deployment scenarios such as if you need to\n\ncheck for data quality in an on-prem/hybrid environment. If you’re\n\nlooking for a fully managed and scalable data quality solution that\n\nintegrates well with other AWS services, AWS Glue Data Quality would\n\nbe the better option.\n\nGetting ready\n\nDeequ depends on Java 8. Deequ version 2.x only runs with Spark\n\n3.1. Deequ metrics are computed using Apache Spark running on\n\nGlue or EMR. Scala or Python can be used over Spark for this\n\npurpose. As Deequ is an open source library, the update of new\n\nmetrics might not be the same across the supported language. us,\n\nthere might be metrics that are available in Scala prior to being\n\navailable in Python. In this recipe, we will use Scala to demonstrate\n\nexamples of how Deequ and Scala work together in a Sagemaker\n\nendpoint.\n\nHow to do it…\n\n1. Click on Author code with a script editor in the console:\n\nFigure 7.8 – Clicking on the script editor in the console\n\n2. Choose Spark engine and create a script.\n\n3. In the next screen, type DQ-test-scala under Name and choose the\n\nrelevant IAM role, Glue 3.0 as Glue version, and Scala as Language, as\n\nshown:\n\nFigure 7.9 – Setting up the Glue pipeline with Scala option\n\n4. Scroll to the Advance option. Under the Libraries section, add the\n\nDeequ package to the JARs path. e Deequ package is available on the\n\nMVN repository at\n\nhttps://mvnrepository.com/artifact/com.amazon.deequ/deequ. Make\n\nsure that you select the version that is compatible with your Spark\n\nversion.\n\nFigure 7.10 – Importing the S3 bucket to the dependent JARs\n\npath\n\n5. In the Script section, paste the following script:\n\nimport com.amazonaws.services.glue.GlueContext import com.amazonaws.services.glue.util.GlueArgParser import com.amazonaws.services.glue.util.Job import org.apache.spark.SparkContext import org.apache.spark.SparkConf import scala.collection.JavaConverters._ import\n\ncom.amazonaws.services.glue.log.GlueLogger import scala.util.matching.Regex import com.amazon.deequ.{VerificationSuite, VerificationResult} import com.amazon.deequ.VerificationResult.checkResul tsAsDataFrame import com.amazon.deequ.checks.{Check, CheckLevel} import com.amazon.deequ.constraints.ConstrainableData Types object GlueApp { def main(sysArgs: Array[String]) { val spark: SparkContext = new SparkContext() val glueContext: GlueContext = new GlueContext(spark) val args = GlueArgParser.getResolvedOptions(sysArgs, Seq(\"JOB_NAME\").toArray) Job.init(args(\"JOB_NAME\"), glueContext, args.asJava) val datasource0 = glueContext.getCatalogSource(database = \"dq- test\", tableName = \"test\", redshiftTmpDir = \"\", transformationContext = \"datasource0\").getDynamicFrame() //Deequ start val dataset: DataFrame = dframe.toDF() val verificationResult : VerificationResult = { VerificationSuite() .onData(df) .addCheck( Check(CheckLevel.Error, \"Customer Code Check\")\n\n.isComplete(\"CUSTOMER_CODE\") .isUnique(\"CUSTOMER_CODE\")) .addCheck( Check(CheckLevel.Error, \"Mobile Check\") .isComplete(\"PHONE_1\") .hasPattern(\"PHONE_1\",\"\"\"^[0][\\d] {9}$\"\"\".r, _>=0.9) .isUnique(\"PHONE_1\")) .addCheck( Check(CheckLevel.Error, \"Tax code Check\") .isComplete(\"TAX_ID\") .isUnique(\"TAX_ID\") .satisfies(\"length(`TAX_ID`) = 10 or length(`TAX_ID`) = 14\", \"is 10 or 14 digits\", Check.IsOne, None)) .run() // retrieve successfully computed metrics as a Spark data frame val resultDataFrame = checkResultsAsDataFrame(spark, verificationResult) glueContext.getSinkWithFormat( connectionType = \"s3\", options = JsonOptions(Map(\"path\" -> \"s3://dq-outputs/results/\")), format = \"csv\" ).writeDynamicFrame(resultDataFrame) Job.commit()\n\nHow it works…\n\nWhen you build a Glue pipeline, you can de\u0000ne which stage of the\n\npipeline Deequ will be used to capture data quality metrics.\n\nDeequ has three main components:\n\nMetrics computation: Deequ computes data quality metrics, that is,\n\nstatistics such as completeness, maximum, or correlation.\n\nConstraint veri\u0000cation: As a user, you focus on de\u0000ning a set of data\n\nquality constraints to be veri\u0000ed. Deequ takes care of deriving the\n\nrequired set of metrics and verifying the constraints.\n\nConstraint suggestion: You can choose to de\u0000ne your custom data\n\nquality constraints, or use the automated constraint suggestion methods\n\nthat pro\u0000le the data to infer useful constraints with\n\nConstraintSuggestionRunner.\n\ne code example in step 5 showcases how to use constraint\n\nveri\u0000cation.\n\nThere’s more…\n\ne following table shows some examples of Deequ rules that you\n\ncan use to check your data:\n\nFigure 7.11 – Example of some common data quality checks\n\nSee also\n\nDeequ GitHub: https://github.com/awslabs/deequ/tree/master\n\nProgramming AWS Glue ETL scripts in Scala:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-\n\nscala.html\n\nUsing Scala to program AWS Glue ETL scripts:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/glue-etl-scala-using.html\n\nBuilding a serverless data quality and analysis framework with Deequ and\n\nAWS Glue: https://aws.amazon.com/blogs/big-data/building-a-\n\nserverless-data-quality-and-analysis-framework-with-deequ-and-aws-\n\nglue/\n\nSchema management for ETL pipelines\n\nIn this recipe, we will learn how to perform schema validation using\n\nApache Spark and AWS Glue. Schema validation is essential to\n\nensure the consistency and integrity of your data as it moves\n\nthrough various stages of the data pipeline. By validating schemas,\n\nyou can prevent data quality issues and ensure that downstream\n\napplications receive data in the expected format.\n\nWithout schema validation, once the data reaches Redshi or\n\nAthena, it will cause data corruption errors from, for example,\n\nduplicate columns or using a wrong datatype. Schema-on-read is a\n\nfeature of the modern data lake, which contrasts with the schema-\n\non-write that is traditionally used in on-prem data warehouses. In\n\nthe data lake environment, when the data moves through layers of\n\nthe data lake, you typically need to de\u0000ne the schema and store the\n\nde\u0000ned scheme either in a JSON con\u0000g \u0000le or in a database that the\n\nETL pipeline could later use to verify the schema of a \u0000le. Using\n\nSpark’s Infer schema is not always a good option, especially when\n\ndealing with columns representing currency. us, this recipe will\n\nintroduce to you how to manage your schema and what is the\n\nbene\u0000t of using Glue Schema Registry.\n\nGetting ready\n\nis recipe assumes that you have a Glue pipeline along with a Glue\n\ncatalog set up from previous recipes.\n\nHow to do it…\n\n1. Head to the Glue console and click on Schemas under Data Catalog:",
      "page_number": 455
    },
    {
      "number": 24,
      "title": "Segment 24 (pages 478-496)",
      "start_page": 478,
      "end_page": 496,
      "detection_method": "synthetic",
      "content": "Figure 7.12 – Selecting schemas under stream schema registries\n\n2. Click on Create schema, \u0000ll in Schema name and Registry, and select\n\nJSON as Data format, as shown:\n\nFigure 7.13 – Adding the new schema name, registry, and data\n\nformat\n\n3. In the \u0000rst schema version, add in your schema. Make sure you read the\n\nCreate Schema article\n\n(https://docs.aws.amazon.com/glue/latest/webapi/API_CreateSchema.ht\n\nml) so that you adhere to the syntax. Here is one example of a schema in\n\nJSON format:\n\n{ \"$schema\": \"http://json-schema.org/draft- 07/schema#\", \"type\": \"object\", \"properties\": { \"firstname\": { \"type\": \"string\" }, \"ID\": { \"type\": \"integer\" } } }\n\n4. Click on Create schema and choose Version 1.\n\n5. Once you have created a schema, you can use SchemaReference in\n\nthe request syntax when calling the CreateTable or UpdateTable\n\nAPIs, or through the Glue Data Catalog console under the relevant\n\ndatabase, as shown:\n\nFigure 7.14 – Choosing a schema from Glue\n\nHow it works…\n\ne AWS Glue Schema Registry allows you to centrally discover,\n\ncontrol, and evolve data schemas. It helps enforce schema\n\ncompatibility checks when registering new schema versions.\n\nGlue Schema Registry also helps to manage the schema evolution.\n\nWhen a new version of a schema is registered, Schema Registry will\n\ncheck the compatibility based on the con\u0000gured mode. If the new\n\nversion is compatible, it will be registered as a new version. You can\n\nalso specify which version of the schema you want a table to have.\n\nis allows you to evolve your schemas over time without breaking\n\ndownstream applications. Schema evolution comes with the\n\nfollowing modes:\n\nNONE: Any change is allowed\n\nBACKWARD: New \u0000elds can be added, but existing \u0000elds cannot be\n\nremoved or renamed\n\nFORWARD: Existing \u0000elds can be removed or renamed, but new \u0000elds\n\ncannot be added\n\nFULL: Changes must be backward- and forward-compatible\n\nThere’s more…\n\nSchema Registry is a specialized product for schema management.\n\nIt can integrate with streaming data applications such as Apache\n\nKaa, Amazon MSK, Amazon Kinesis Data Streams, Apache Flink,\n\nand AWS Lambda, providing schema management capabilities for\n\nthese platforms. Data Catalog can also reference schemas stored in\n\nSchema Registry when creating or updating AWS Glue tables or\n\npartitions.\n\nTo expand the architecture further, investigate an architecture of\n\nETL framework where you integrate a control table to keep track of\n\nGlue job run status and a technical and business metadata\n\ndictionary.\n\nSee also\n\nAWS Glue Schema Registry – AWS Glue:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/schema-registry.html\n\nIntegrating with AWS Glue Schema Registry:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/schema-registry-\n\nintegrations.html\n\nBuilding unit test functions for ETL pipelines\n\nIn this recipe, we will learn how to build some unit test functions\n\nfor your ETL pipeline to help identify and \u0000x issues at an early stage\n\nof your pipeline development. By incorporating unit tests, you can\n\ncatch errors early in the development process, leading to more\n\nrobust and reliable data work\u0000ows. is recipe is particularly useful\n\nfor data engineers who need to validate the functionality of their\n\nETL jobs and ensure data integrity before deploying them to\n\nproduction.\n\ne goal of this recipe is to introduce some code snippets of\n\nfunctions to test Glue Jobs in a unit testing context that you can use\n\nto integrate into your Glue pipeline or your company’s internal\n\nlibraries.\n\nHow to do it…\n\n1. You should create a \u0000le name, such as unit_test.py, that is separate\n\nfrom your ETL code. is \u0000le will contain various functions for unit\n\ntesting.\n\n2. Import the relevant libraries. ese are the libraries that we will use for\n\nthis recipe:\n\nimport logging import os import subprocess import sys from functools import reduce from itertools import zip_longest from pyspark.sql import DataFrame, Row, SparkSession from pandas.testing import assert_frame_equal from pyspark import SparkConf, SparkContext from pyspark.sql.types import ArrayType, MapType from pyspark.testing import assertDataFrameEqual, assertSchemaEqual\n\n3. In this code, we will raise an exception error as follows:\n\nclass DataFrameNotEqualError(Exception): \"\"\"The created dataframes are not equal\"\"\" pass\n\n4. We will create a GlueUnitTest class, as shown, to compare schemas\n\nand row data:\n\nclass GlueUnitTest: @staticmethod def assert_df_equality(df1, df2, ignore_col_order=True, ignore_row_order=True, ignore_nullable=True): \"\"\" Check two PySpark dataframes, and\n\nraise an exception if they are not equal Handles list/dict style columns, and prints detailed error messages on failure. Do not use this to compare a PySpark dataframe to a Python dict in a test. Instead, collect() the dataframe rows and do a comparison in Python. :param df1: The 1st dataframe to check :param df2: The 2nd dataframe to check :param ignore_col_order: If true, the columns will be sorted prior to checking the schema :param ignore_row_order: If true, the rows will be sorted prior to checking the data :param ignore_nullable: If true, differences in whether or not the schema allows None/null will be ignored. \"\"\" transforms = [] if ignore_col_order: transforms.append(lambda df: df.select(sorted(df.columns))) if ignore_row_order: transforms.append(lambda df: df.sort(df.columns)) df1 = reduce(lambda acc, fn: fn(acc), transforms, df1) df2 = reduce(lambda acc, fn: fn(acc), transforms, df2)\n\nGlueUnitTest.assert_df_schema_equality(df1, df2)\n\nGlueUnitTest.assert_df_data_equality(df1, df2) @staticmethod\n\ndef assert_df_data_equality(df1, df2): \"\"\" compare two PySpark dataframes and return if rows are different and the percentage of difference :param df1: the first dataframe :param df2: the second dataframe \"\"\" try: assertDataFrameEqual(df1, df2) except AssertionError as e: raise AssertionError(f\"Dataframes are not equal: {e}\") from e @staticmethod def assert_df_schema_equality(df1, df2): \"\"\" Assert that two pyspark dataframe schemas are equal. this function only compares two schemas , column names, datatypes and nullable property :param df1: the first dataframe :param df2: the second dataframe \"\"\" try:\n\nassertSchemaEqual(actual=df1.schema, expected=df2.schema) except AssertionError as e: raise AssertionError(f\"Schema are not equal: {e}\") from e\n\n5. In your Glue Studio notebook, you can add an additional\n\nunit_test.py \u0000le with the following code:\n\n%%configure\n\n{ \"--extra-files\": \"s3://path/to/additional/unit_test.py/\" }\n\nHow it works…\n\nere are three types of testing that you can implement for your\n\npipelines to ensure their accuracy and reliability:\n\nUnit test that tests individual unit code: e standard unit test libraries\n\nare Pytest, uniitest, or scalatest if you write your code in\n\nScala. Unit testing your pipeline is diﬀerent from unit testing your data.\n\nis is why we have two diﬀerent recipes for unit testing in this chapter.\n\nIntegration test that tests for components of your pipeline: It is not\n\nrecommended to build a pipeline with too many components. e best\n\npractice is to standardize your pipeline into a single and modular action\n\nand use an orchestration tool such as Glue Work\u0000ow or Step Functions\n\nto orchestrate the steps of these actions.\n\nEnd-to-end testing whereby you test the entire pipeline from start to\n\n\u0000nish: You will need to adapt the knowledge from Chapter 8, DevOps –\n\nDe\u0000ning IaC and Building CI/CD Pipelines, where you set up a code\n\ndeployment pipeline for CI/CD work\u0000ow.\n\nIn this recipe’s code snippets, we went through two unit test\n\nscenarios that helped you evaluate your DataFrame before passing it\n\nto the next layer of the data lake as follows:\n\nComparing whether the information of two data frames is identical\n\nComparing whether the schema of two data frames is identical\n\nEach organization will have diﬀerent test scenarios to meet your\n\ndata quality needs. You can build a comprehensive testing\n\nframework with Deequ and Spark’s UDF. You can then add it to\n\nyour pipeline as extra \u0000les.\n\nThere’s more…\n\nTo take this a step further, you should write unit test cases and\n\nautomate the running of these test cases with AWS CodePipeline.\n\nPlease see Chapter 8, DevOps – De\u0000ning IaC and Building CI/CD\n\nPipelines, for more details.\n\nAs an example of writing a unit test, you would need to build mock\n\ndata and pass it into a test case, as shown here:\n\nimport pytest tableName = \"marketing_data\" dbName = \"marketing\" # Does the table exist? def test_tableExists(): assert tableExists(tableName, dbName) is True\n\nSee also\n\nUsing Python libraries with AWS Glue:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-\n\npython-libraries.html\n\nPySpark Testing:\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.testin\n\ng.html\n\nCon\u0000guring AWS Glue interactive sessions for Jupyter and AWS Glue\n\nStudio notebooks:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/interactive-sessions-\n\nmagics.html\n\nBuilding data cleaning and profiling jobs with DataBrew\n\nAWS Glue DataBrew is a no-code data preparation tool that\n\nsimpli\u0000es data pro\u0000ling, cleansing, and validation, making it an\n\nexcellent choice for data engineers looking to automate data quality\n\nchecks. In this recipe, we’ll use DataBrew to perform data pro\u0000ling\n\nand PII detection.\n\nGetting ready\n\nis recipe assumes that you have a dataset in S3 for testing out\n\nDataBrew.\n\nHow to do it…\n\n1. Navigate to the AWS DataBrew console, click on Projects | Create\n\nproject, and \u0000nally click on Provide project name.\n\nFigure 7.15 – Clicking on Create Project\n\n2. Select the appropriate dataset that you would like to use for building the\n\nDataBrew project:\n\nFigure 7.16 – Selecting a relevant dataset\n\n3. In the Permission section, select the appropriate name under Role name,\n\nthen click on Create project. If you have not created an IAM role for\n\nDataBrew, you can click on Create new IAM role and then click on\n\nCreate project.\n\nFigure 7.17 – Selecting an IAM role\n\n4. You will need to wait a few minutes for the console to start. Once the\n\nconsole \u0000nishes loading, click on RECIPE, then click on Add step:\n\nFigure 7.18 – Adding steps to your recipe\n\n5. Next, we will see how to build some steps in the recipe. Assume that we\n\nhave a PII column of 10 number and we only want to keep the \u0000rst\n\nnumber and the last 5 numbers for analytics purposes. First, search for\n\nREDACT VALUES:\n\nFigure 7.19 – Choosing the REDACT VALUES option\n\n6. Next, choose the value for Source columns, Value to redact, Starting\n\nposition, and Redact symbol.\n\nFigure 7.20 – Selecting rules for Redact Values\n\n7. en click on Preview changes to preview the eﬀect. Once you are happy\n\nwith the result, click on Apply.\n\nFigure 7.21 – Previewing output\n\n8. You can also use Substitute values to create new values from your\n\nexisting record. In the following example, you can use shuﬄe value to\n\nshuﬄe customers’ \u0000rst or last names. e Name of columns to group by\n\nwith \u0000eld could be used in a similar way to the GROUP_BY command in\n\nSQL when you want the data to shuﬄe only within the Group by\n\nvalue.\n\nFigure 7.22 – Creating rules for substitute values\n\n9. e following screenshot shows an example of a recipe with multiple\n\nsteps, including re-arranging columns to rename, \u0000ltering value, and\n\nmasking data. Depending on your needs, you can also add hashing and\n\nencryption for your dataset. Once you \u0000nish with the recipe steps, click\n\non Publish.\n\nFigure 7.23 – Reviewing the steps and publishing\n\n10. Once you publish a recipe, you can head to the Job tab and \u0000ll in the job\n\nname, dataset, and previously created recipe, as shown. Once you \u0000ll in\n\nthe necessary details, click on either Create job or Create and run job.",
      "page_number": 478
    },
    {
      "number": 25,
      "title": "Segment 25 (pages 497-517)",
      "start_page": 497,
      "end_page": 517,
      "detection_method": "synthetic",
      "content": "Figure 7.24 – Creating a recipe job\n\nHow it works…\n\nAWS DataBrew simpli\u0000es data quality control by providing a visual\n\ninterface for data pro\u0000ling and transformation. e steps outlined\n\nin this recipe show how to leverage AWS Glue DataBrew to\n\nautomate data quality checks by creating a recipe. A recipe could be\n\nadjusted to be reused in multiple datasets, such as creating a recipe\n\nto mask PII data for columns with ID.\n\nThere’s more…\n\nPro\u0000ling helps you understand the state of your data by generating\n\nstatistics and identifying potential issues. It is advised to run a\n\npro\u0000le job \u0000rst to understand your dataset before applying a ruleset.\n\nClick on Create job on the Pro\u0000le jobs tab to start your pro\u0000ling job\n\nfor the dataset.\n\nFigure 7.25 – Clicking on Create job to automate the DataBrew\n\nrecipe\n\nAs discussed in the Unit testing your data quality using Deequ\n\nrecipe, Data Quality is all built upon the Deequ library. When\n\nchoosing between AWS Glue DataBrew and Deequ for your data\n\nworkloads, there are a few key factors to consider:\n\nData transformation capabilities: AWS Glue DataBrew is a visual data\n\npreparation tool that allows you to easily transform and clean your data,\n\nwhile Deequ is more of a data quality checking tool.\n\nEase of use: AWS Glue DataBrew provides a user-friendly, no-code\n\ninterface that makes it easy for non-technical users to prepare and\n\ntransform data. On the other hand, Deequ requires more technical\n\nexpertise, as it is a library that needs to be integrated into your data\n\nprocessing pipelines.\n\nIntegration with other AWS services: Deequ can be used with a variety\n\nof data processing frameworks, including Apache Spark. It can also be\n\nused within your Glue data pipeline with DataBrew. However, it would\n\nbe a separate step and would need an orchestration tool such as Air\u0000ow\n\nor Step function to trigger the Glue pipeline and DataBrew together.\n\nSee also\n\nGROUP_BY: https://docs.aws.amazon.com/databrew/latest/dg/recipe-\n\nactions.GROUP_BY.html\n\nAWS Glue DataBrew adds binning, skewness, binarization, and transpose\n\ntransformations for pre-processing data for machine learning and\n\nanalytics: https://aws.amazon.com/about-aws/whats-new/2021/03/aws-\n\nglue-databrew-adds-binning-skewness-binarization-transformations-\n\npre-processing-data/\n\nOceanofPDF.com\n\n8 DevOps – Defining IaC and Building CI/CD Pipelines\n\ne idea of combining soware development with production\n\noperations has been around for decades, but it’s since the rise of\n\ncloud computing that it has been established as a best practice,\n\naiming to shorten the soware development cycles from\n\ndevelopment to deployment while improving their reliability and\n\nlowering operational costs.\n\nis methodology involves many aspects, including team\n\norganization, culture, and processes. In this chapter, we will focus\n\non implementing two of the key technical components for\n\nsuccessfully implementing DevOps on AWS:\n\nInfrastructure as Code (IaC): IaC ful\u0000lls the key role of eliminating or\n\nreducing manual operations by coding the deployment and maintenance\n\nof infrastructure and services. is reduces human errors, improves\n\nscalability, and reduces the maintenance burden in general. In recent\n\nyears, AWS Cloud Development Kit (CDK) has become the reference\n\nIaC for AWS. It allows you to code the AWS infrastructure on one of the\n\nmultiple languages it supports, aer which it’s converted into an AWS\n\nCloudFormation template. We will also cover the main alternative,\n\nTerraform, which aims to be a cloud-agnostic tool for IaC.\n\nContinuous integration/continuous deployment (CI/CD): is is the set\n\nof techniques that takes care of reliably deploying code to production\n\nsystems. e aim is that once the developer code has been merged into\n\nthe code repository, a pipeline takes care of all the deployment\n\nautomatically. is includes not only deploying the code alongside any\n\ninfrastructure required but also following a deployment process that\n\nnormally includes tests, stages, and potentially manual approvals (full\n\nCI/CD is when there are none).\n\ne traditional tool to implement CI/CD pipelines is Jenkins.\n\nHowever, here, we will use AWS CodePipeline since it is easier\n\nto set up and is well-supported by CDK.\n\ne following recipes will be covered in this chapter:\n\nSetting up a code deployment pipeline using CDK and AWS\n\nCodePipeline\n\nSetting up a CDK pipeline to deploy on multiple accounts and regions\n\nRunning code in a CloudFormation deployment\n\nProtecting resources from accidental deletion\n\nDeploying a data pipeline using Terraform\n\nReverse-engineering IaC\n\nIntegrating AWS Glue and Git version control\n\nTechnical requirements\n\ne recipes in this chapter assume you have a bash shell or\n\nequivalent available with the AWS CLI installed (refer to the\n\ninstructions at\n\nhttps://docs.aws.amazon.com/cli/latest/userguide/getting-started-\n\ninstall.html) with access to AWS. If you’re using Microso\n\nWindows, you can enable WSL (https://learn.microso.com/en-\n\nus/windows/wsl/install) or install Git (https://git-\n\nscm.com/downloads) to use the bash shell it brings.\n\nCon\u0000gure the default region and user credentials, ensuring you\n\nhave enough permissions to use the diﬀerent services. You can use\n\naws configure or an AWS CLI pro\u0000le. Alternatively, you can use\n\nenvironment variables to provide the credentials:\n\nAWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and optionally\n\nAWS_SESSION_TOKEN.\n\nFor the recipes that need CDK, you need to install NPM \u0000rst by\n\ndownloading it from https://nodejs.org/en/download (in Linux, it’s\n\nhighly dependent on the GNU Libc version that your OS uses, so\n\nyou might need to use an older NPM version or compile it yourself)\n\nand then, in the command line, run npm install -g aws-cdk\n\nand then cdk --version to verify it’s working. More information\n\nand details can be found in the AWS documentation:\n\nhttps://docs.aws.amazon.com/cdk/v2/guide/cli.html.\n\nYou can \u0000nd the code \u0000les for this chapter on GitHub:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter08.\n\nSetting up a code deployment pipeline using CDK and AWS CodePipeline\n\nIn this recipe, you will create a CDK project that de\u0000nes a\n\ndeployment pipeline and a service infrastructure stack with a\n\nsimple Glue Shell job as an example. is recipe uses Python for\n\nboth the Glue Shell script and the CDK script.\n\ne pipeline will monitor the CodeCommit repository to\n\nautomatically deploy both changes to the pipeline itself and the\n\nsolution (the Glue job in this case).\n\nGetting ready\n\nFor this recipe, you’ll require the AWS CLI, CDK, Python 3.8+, and\n\nGit to be installed in your system, as well as a command-line\n\ninterface to invoke them. Check the Technical requirements section\n\nfor indications and guidance on installing the CLI and CDK. You\n\ncan install Python and Git from their respective websites\n\n(www.python.org and git-scm.com). On Windows, make sure that\n\nwhen you install Python, you enable the option to add it to PATH so\n\nthat it can be easily run from the command line. You can verify the\n\nPython version by running python --version.\n\nHow to do it…\n\n1. Bootstrap the CDK infrastructure on the previously con\u0000gured account\n\nand region. is is required so that you can deploy the CDK project\n\nlater:\n\ncdk bootstrap\n\n2. Prepare the CDK Python project by creating a directory for the project\n\nand preparing the Python virtual environment:\n\nmkdir cdk-deployment-recipe cd cdk-deployment-recipe cdk init --language=python source .venv/bin/activate echo git-remote-codecommit >> requirements.txt pip install -r requirements.txt\n\n3. Create the Git CodeDeploy repo where the project will be stored:\n\naws codecommit create-repository --repository- name \\ cdk-deployment-recipe --repository- description \\ \"CDK deployment pipeline recipe\" git remote add codecommit codecommit::$(aws \\ configure get region | \\ sed 's/\\s//g')://cdk-deployment-recipe\n\n4. Create a script for the Glue job script:\n\nmkdir glue echo \"print('Hello from GlueShell')\" >\\ ./glue/GlueShellScript.py\n\n5. Override the default CDK script and add the imports:\n\nCDK_FILE=\\ cdk_deployment_recipe/cdk_deployment_recipe_st ack.py cat > $CDK_FILE << EOF from aws_cdk import ( Stack, Stage, aws_codecommit as codecommit, aws_glue as glue, aws_iam as iam, aws_s3 as s3, aws_s3_deployment as s3_deploy ) from aws_cdk.pipelines import ( CodePipeline, CodePipelineSource, ShellStep ) from constructs import Construct EOF\n\n6. Add the deployment stack to the script. Ensure that you respect the\n\nindentation:\n\ncat >> $CDK_FILE << EOF class CdkDeploymentRecipeStack(Stack): def __init__(self, scope: Construct,\n\nconstruct_id: str, **kwargs): super().__init__(scope, construct_id, **kwargs) repo = codecommit.Repository.\\\n\nfrom_repository_name(self,\"DeployRecipeRepo\", repository_name=\"cdk-deployment- recipe\") git_source = \\ CodePipelineSource.code_commit(repo, \"master\") pipeline = CodePipeline(self, \"Pipeline\", pipeline_name=\"RecipePipeline\", synth=ShellStep(\"Synth\", input=git_source, commands=[ \"npm install -g aws-cdk\", \"python -m pip install -r requirements.txt\", \"cdk synth\"] ) ) pipeline.add_stage(GlueStage(self, \"prod\")) class GlueStage(Stage): def __init__(self, scope: Construct, construct_id: str, **kwargs): super().__init__(scope, construct_id, **kwargs) GlueAppStack(self, \"GlueAppStack\", stage=self.stage_name) EOF\n\n7. Add the \u0000rst part of the Glue stack to the script:\n\ncat >> $CDK_FILE << EOF\n\nclass GlueAppStack(Stack): def __init__(self, scope: Construct, construct_id: str, stage: str, **kwargs): super().__init__(scope, construct_id, **kwargs) bucket_name = f\"deployment-recipe- {self.account}-{stage}\" bucket = s3.Bucket(self, id=\"GlueBucket\", bucket_name=bucket_name) deployment = s3_deploy.BucketDeployment(self, \"DeployCode\", destination_bucket=bucket, sources= [s3_deploy.Source.asset(\"./glue\")]) role_name = \\ f\"AWSGlueServiceRole-CdkRecipe- {stage}\" job_role = iam.Role(self, id=role_name, role_name=role_name, managed_policies=[\n\niam.ManagedPolicy.from_managed_policy_arn(self , \"glue-service\", \"arn:aws:iam::aws:policy\\ /service-role/AWSGlueServiceRole\") ],\n\nassumed_by=iam.ServicePrincipal(\n\n\"glue.amazonaws.com\") )\n\njob_role.add_to_policy(iam.PolicyStatement(\n\neffect=iam.Effect.ALLOW, resources=[ f'arn:aws:s3::: {bucket_name}', f'arn:aws:s3::: {bucket_name}/*' ], actions=[ 's3:ListBucket', 's3:GetObject', 's3:PutObject' ] ) ) EOF\n\n8. Complete the stack de\u0000nition:\n\ncat >> $CDK_FILE << EOF job = glue.CfnJob( self, \"glue_CDK_job\", command = glue.CfnJob.JobCommandProperty( name = \"pythonshell\", python_version= '3.9', script_location = \\ f's3://{bucket_name}/GlueShellScript.py' ), role= job_role.role_arn, name= \"deployment_recipe_glueshell\", glue_version=\"3.0\" ) EOF\n\n9. Test the CDK script to ensure it’s valid. If it isn’t, review the previous\n\nsteps while paying attention to the Python indentation, especially on\n\nnew statements:\n\ncdk synth\n\n10. Commit the changes and push them to the project via the Git repo:\n\ngit add * git commit -m \"Added cdk and Glue code\" git push --set-upstream codecommit master\n\n11. Deploy the pipeline and the stack:\n\ncdk deploy\n\n12. Open the AWS console, navigate to CodePipeline in the same region you\n\nhave the AWS CLI con\u0000gured to use, and locate the RecipePipeline\n\npipeline. If it hasn’t completed yet, wait until all steps up to the prod\n\nstage are green.\n\nNote that you can retry a failed action, but in most cases, you\n\nwill need to make a correction to the CDK code and push to the\n\nrepository to make corrections:\n\nFigure 8.1 – Pipeline prod stage\n\n13. Now, you can navigate to Glue on the AWS console, \u0000nd the\n\ndeployment_recipe_glueshell job, select it, and run it. Once it completes\n\nsuccessfully, in the Run details tab, using the Output link, you can view\n\nthe message that the script printed.\n\n14. If you don’t want to keep it, remove the stack, which will delete the\n\nresources:\n\ncdk destroy\n\nHow it works…\n\nIn this recipe, you created a CDK project and the sample Glue\n\nscript. Because the language of choice was Python, it needed some\n\nCDK dependencies. You downloaded these via pip based on the\n\nrequirement.txt \u0000le, loaded the Python virtual environment\n\ncreated by CDK, and activated them by running source\n\n.venv/bin/activate. is allowed Python to install the speci\u0000c\n\ndependencies just for this project and not globally so that other\n\nCDK projects could use diﬀerent versions. Other languages have\n\ntheir own way of using CDK and managing dependencies.\n\ne cdk bootstrap command deployed the CDKToolkit\n\nCloudFormation stack on the con\u0000gured region and account. is\n\nis a one-time resource setup and is required to be able to deploy\n\nCDK stacks onto that region and account; you only need to\n\nbootstrap again if you’re upgrading the project’s CDK version (the\n\ntool will detect that and remind you if needed).\n\nen, you created a Git repository on AWS CodeCommit to store\n\nthe project. is is not required by CDK since you could run the\n\nCDK project locally, but this is best practice to ensure the\n\ninfrastructure gets treated like any other code – that is, it gets peer-\n\nreviewed, stored in a version management system, and\n\nautomatically deployed.\n\nAer updating the code, you pushed the changes to the Git remote\n\nserver and deployed the CDK project. is was needed because the\n\npipeline doesn’t exist yet. Once the pipeline is in place, it will\n\nmonitor the repository project to trigger the pipeline on any\n\ncommit that’s done. e pipeline has a stage to self-update, so if you\n\nhave manually deployed changes that have not been pushed to the\n\nGit server, they will be undone. Remember to only invoke cdk\n\ndeploy the very \u0000rst time you create the pipeline.\n\nTo do the deployment (either when you do it manually or with the\n\npipeline), CDK generates a CloudFormation stack or changeset, as\n\nneeded.\n\nOnce the pipeline completes the deployment, the Glue job is ready\n\nto use. All the work to create a bucket, upload the script, and create\n\na role was taken care of by the CDK stack. In the same way, the\n\nstack could have deployed other AWS components or services, such\n\nas an AWS Lambda, an RDS database, or an EMR cluster.\n\nThere’s more…\n\nis recipe combines using CDK and AWS CodePipeline. If you\n\nwanted to use the traditional tool for pipelines instead, such as\n\nJenkins, you would need a Jenkins task that also monitors the Git\n\nrepository and then runs CDK in a project, where the main stack\n\nwould be GlueAppStack instead of CdkDeploymentRecipeStack,\n\nwhich is needed for CodePipeline.\n\nIf something fails in the pipeline, you can retry the failed stages –\n\nfor instance, if the issue is that it couldn’t deploy because there was a\n\ncon\u0000ict with some existing resource (such as the S3 bucket) and you\n\nwere able to solve it manually without code changes.\n\nAt the bottom of each stage in the pipeline, the Git commit is listed\n\nwith a message stating that it has been deployed or is currently\n\nbeing deployed. is means that in most cases, you can roll back the\n\nchanges by rolling back the changes in Git.\n\nSee also\n\nFor a more advanced use case of this recipe involving multiple stages\n\nand regions, please refer to the Setting up a CDK pipeline to deploy on\n\nmultiple accounts and regions recipe.\n\nIf you’re going to need to use multiple cloud vendors, consider using\n\nTerraform for your Glue stack on the pipeline instead of CDK. See the\n\nDeploying a data pipeline using Terraform recipe for an example of how\n\nto use that cloud-vendor-agnostic tool.\n\nSetting up a CDK pipeline to deploy on multiple accounts and regions\n\nOnce your pipeline reaches a certain complexity, it’s good practice\n\nto deploy it to diﬀerent environments using diﬀerent accounts or at\n\nleast diﬀerent regions; this separates the environments (especially\n\nwhen using diﬀerent accounts), which reduces the blast radius if\n\nsomething goes wrong and helps with security separation. For\n\ninstance, you can allow developers to log in to pre-production\n\naccounts for troubleshooting or investigation, but only\n\nadministrators can access production accounts. Another reason to\n\ndo this is to avoid having diﬀerent environments competing for\n\nservice quotas, which in most cases are per account and region.\n\nDeploying on multiple regions is oen used to serve customers in\n\nthat area or if your service needs to survive a regional disaster.\n\nGetting ready\n\nis recipe is an extension of the Setting up a code deployment\n\npipeline using CDK and AWS CodePipeline recipe; therefore, you\n\nneed to follow the Getting ready instructions there to set up the\n\ntools required for this recipe.\n\nIn addition, when using a multi-account deployment, you need the\n\nCDK infrastructure stack to trust the role that’s used by the\n\npipeline. To do so for each account involved (even the main one for\n\nconsistency), you need to do an upgraded CDK bootstrap to add\n\nthis trust to the pipeline. If that account and region have already\n\nbeen CDK bootstrapped, the command will just make the updates\n\nneeded to add that trust.\n\nAs indicated in the Technical requirements section, you are expected\n\nto have AWS credentials con\u0000gured by default, either using the\n\ncon\u0000guration \u0000les or environment variables. When you need to\n\naccess the alternative account(s), if you use the con\u0000guration\n\noption, you can update the ~/.aws/config and\n\n~/.aws/credentials \u0000le to add a pro\u0000le for each account so that\n\nyou can specify diﬀerent credentials and con\u0000gurations for each\n\npro\u0000le. At this point, you can reference them in the commands.\n\nPlease check the AWS documentation on CLI pro\u0000les for more\n\ndetails.\n\nIf you’re using environment variables, just override the variables\n\nwhile you run the command and don’t specify any pro\u0000le (omit the\n\n--profile parameter).\n\nDecide on the accounts and regions you will deploy as part of this\n\nrecipe. Ideally, you should use two accounts and regions to fully\n\nexplore this recipe. For each pair of accounts and regions, run the\n\nfollowing bootstrap command, replacing my values with your own:\n\ncdk bootstrap 1234567890/us-west-1 --profile myprofile \\ --trust 11111111111 --cloudformation-execution- policies \\ arn:aws:iam::aws:policy/AdministratorAccess\n\nHere, 1234567890 and us-west-1 are the account and region\n\nwhere the stack will be deployed, and myprofile is the AWS CLI\n\npro\u0000le with credentials for the 1234567890 account. In this case, it\n\nassumes the account is dedicated to the pipeline deployments and it\n\ngives full-trust administrator access. If the account is shared for\n\nother purposes, the best practice is to grant the minimum required\n\npermissions.\n\nHow to do it…\n\n1. Complete steps 2 to 7 of the Setting up a code deployment pipeline using\n\nCDK and AWS CodePipeline recipe. If you have the resulting project aer\n\ncompleting that recipe, it’s okay if you run the last step to destroy the\n\nstack. Move into the project directory:\n\ncd cdk-deployment-recipe\n\n2. Edit the application \u0000le to specify an account and region for the main\n\nstack – that is, the one that hosts the pipeline. Use a text editor to open\n\nthe \u0000le (for example, vi app.py) and uncomment one of the lines\n\nthat assigns the env variable either using the AWS CLI defaults or by\n\nspecifying your own account and region, like so:\n\nenv=cdk.Environment( account=os.getenv('CDK_DEFAULT_ACCOUNT'), region=os.getenv('CDK_DEFAULT_REGION')),\n\n3. Edit the main CDK \u0000le,\n\ncdk_deployment_recipe/cdk_deployment_recipe_stac\n\nk.py, in the CodePipeline constructor editor and add a new parameter\n\ncalled cross_account_keys=True so that it looks like this:\n\npipeline = CodePipeline(self, \"Pipeline\",\n\npipeline_name=\"RecipePipeline\", cross_account_keys=True, synth=ShellStep(\"Synth\", ...........\n\n4. In the \u0000le, just aer pipeline creation, replace the line containing\n\naddStage with one line for each of the accounts and regions you want\n\nto deploy, specifying an env argument for each one. For instance, if you\n\nwant to deploy the GlueStack to 1234567890 with the us-east-1\n\nregion set to \"integration\" and 0987654321 with the same\n\nregion but set to \"production\", you must do the following (make\n\nsure you respect the Python indentation so that both lines are aligned\n\nwith the pipeline variable assignment):\n\npipeline.add_stage(GlueStage(self, \"integration\", env={\"region\":\"us-east- 1\",\"account\":\"1234567890\"})) pipeline.add_stage(GlueStage(self, \"prod2\", env={\"region\":\"us-east- 1\",\"account\":\"0987654321\"}))\n\n5. Test whether the CDK script is valid. If not, review the previous steps\n\nwhile paying attention to the Python indentation being aligned:\n\ncdk synth",
      "page_number": 497
    },
    {
      "number": 26,
      "title": "Segment 26 (pages 518-537)",
      "start_page": 518,
      "end_page": 537,
      "detection_method": "synthetic",
      "content": "6. Commit the changes and push them to Git:\n\ngit add * git commit -m \"Made the pipeline multiregion\" git push --force --set-upstream codecommit master\n\n7. Deploy the pipeline and the stack:\n\ncdk deploy --all\n\nIf you get an “invalid principals” error, revise that the accounts\n\nand regions you speci\u0000ed in step 4 match the bootstrap you did\n\nin the Getting ready section.\n\n8. Open the AWS console, navigate to CodePipeline in the same region\n\nwhere you have AWS CLI con\u0000gured by default, and locate the\n\nRecipePipeline pipeline. If it hasn’t been deployed yet, wait until all steps\n\nup to the prod stage are green.\n\n9. Note that you can retry a failed action, but in most cases, you will need\n\nto make a correction to the CDK code and push it to the repository to\n\nmake corrections.\n\n10. Now, navigate to Glue in the console and go to the accounts in one of the\n\nregions you chose. Review and optionally run the\n\ndeployment_recipe_glueshell Glue shell job.\n\n11. If you no longer need it, remove the stack (which deletes the pipeline as\n\nwell):\n\ncdk destroy --all\n\nHow it works…\n\nis recipe extends the previous recipe, Setting up a code\n\ndeployment pipeline using CDK and AWS CodePipeline, so the\n\nconcepts and explanations there apply here as well. We’ll focus on\n\nthe diﬀerences when deploying on multiple accounts and regions.\n\nWhen CodePipeline detects that multiple accounts are involved, the\n\npipeline is considered by CDK as multi-account. is requires an\n\nexplicit environment con\u0000guration, even if you use environment\n\nvariables to take it from the AWS CLI defaults. Also, the pipeline\n\nwill work while assuming roles, even for the current account, thus\n\nthe need for the special bootstrap you did in the Getting ready\n\nsection for each of the accounts and regions involved. is trusts\n\nthe pipeline to do the deployment.\n\nIn the CodePipeline code, you changed the cross_account_keys\n\n\u0000ag to True. is is required so that you can deploy artifacts cross-\n\naccount using trust policies. It creates a KMS key that can be shared\n\nbetween the accounts to encrypt the artifact bucket. In older\n\nversions, this was always done by default, but then it was disabled to\n\navoid the cost of maintaining the key (typically $1/month). is\n\nisn’t needed when the pipeline deploys just in the same account, so\n\nit doesn’t need to impersonate roles.\n\nen, in the main script, you added additional deployments of the\n\nGlue stack for multiple accounts and regions. Note how simple this\n\nis compared to manually propagating changes across environments,\n\neven if you’re using CloudFormation changesets.\n\nFinally, you deployed the project like any other CDK stack.\n\nThere’s more…\n\nCDK does most of the security setup using defaults. You can\n\nexamine the changes and the CloudFormation templates regarding\n\nroles and permissions. It is possible to con\u0000gure the pipeline so that\n\nit uses speci\u0000c roles that already exist. Alternatively, you can create\n\nroles in the pipeline with more \u0000ne-grained control or have a role\n\nname prede\u0000ned.\n\nCheck the CDK documentation for more options. Each new version\n\nprovides both more automation and more customization if you\n\nneed it.\n\nIn this example, we used the s3_deployer utility, which is an easy\n\nway to put \u0000les from the repository into S3. So, in the step where it\n\nbuilds the artifacts, it also deploys them to S3. At the time of\n\nwriting, it doesn’t optimize the build for reuse between the diﬀerent\n\nenvironments and causes more builds than needed.\n\nSee also\n\nIf you want to learn more about Glue jobs and the diﬀerent usages for\n\nbuilding data pipelines, you can check the recipes dedicated exclusively\n\nto AWS Glue in Chapter 3, Ingesting and Transforming Your Data with\n\nAWS Glue.\n\nRunning code in a CloudFormation deployment\n\nBefore the availability of more sophisticated code-based solutions\n\nsuch as CDK or Terraform, AWS CloudFormation was the standard\n\nto automate AWS deployments for more than a decade. It started as\n\na purely template solution but then introduced the ability to call\n\nfunctions with custom resource deployment.\n\nWhile CloudFormation templates are not a fully \u0000edged IaC\n\nsolution, since it was the only AWS deployment solution for years,\n\nthey are still present on many projects. Oen, the eﬀort of\n\nmigrating to a tool such as CDK doesn’t justify the bene\u0000t, so\n\ninstead, it makes sense to use CloudFormation’s advanced features\n\nand automation infrastructure, despite its limitations.\n\nIn this recipe, you’ll learn how to use CloudFormation custom\n\nresources to run your own code on deployment, which overcomes\n\nthe limitations of templates. In this case, the code will be used to set\n\nup a \u0000le on S3, but the same concept can be used for any of the\n\nfeatures of the AWS SDKs.\n\nGetting ready\n\nFor this recipe, you just need the AWS CLI installed and con\u0000gured,\n\na text editor, and access to the AWS console to view the results. See\n\nthe Technical requirements section for guidance on setting up the\n\ncommand line.\n\nHow to do it…\n\n1. Using a text editor, create a \u0000le named recipe3_template.json\n\nwith the following JSON content. ere are line breaks to \u0000t the code on\n\nthe page. When you enter it in the \u0000le, make sure you keep it as valid\n\nJSON by not introducing line breaks within the string double quotes,\n\n\"\":\n\n{ \"Resources\": { \"S3Bucket\": { \"Type\": \"AWS::S3::Bucket\", \"Properties\": { \"BucketName\": { \"Fn::Sub\": \"recipe-deploy- action-${AWS::AccountId}\" } } }, \"DeployLambdaRole\": { \"Type\": \"AWS::IAM::Role\", \"Properties\": { \"AssumeRolePolicyDocument\": { \"Statement\": [\n\n{ \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"lambda.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] }, \"ManagedPolicyArns\": [ \"arn:aws:iam::aws:policy/service- role/AWSLambdaBasicExecutionRole\" ] } }, \"DeployerPolicy\": { \"Type\": \"AWS::IAM::ManagedPolicy\", \"Properties\": { \"Path\": \"/\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:PutObject\", \"Resource\": { \"Fn::Sub\": \"arn:aws:s3:::${S3Bucket}/*\" } }, { \"Effect\": \"Allow\", \"Action\": \"s3:DeleteObject\", \"Resource\": { \"Fn::Sub\":\n\n\"arn:aws:s3:::${S3Bucket}/*\" } } ] }, \"Roles\": [ { \"Ref\": \"DeployLambdaRole\" } ] } }, \"DeployLambda\": { \"DependsOn\": [ \"S3Bucket\", \"DeployLambdaRole\", \"DeployerPolicy\" ], \"Type\": \"AWS::Lambda::Function\", \"Properties\": { \"FunctionName\": \"DeployActionLambda\", \"Description\": \"Intended to run during deploment\", \"Code\": { \"ZipFile\": { \"Fn::Sub\": \"import boto3\\nimport cfnresponse\\n\\ndef handler(event, context):\\n print(f'Event:')\\n print(event)\\n bucket_name = '${S3Bucket}'\\n file_path = 'config/setup.txt'\\n\\n if event['RequestType'] == \\\"Create\\\":\\n print('The bucket ${S3Bucket}')\\n print(f'Using bucket {bucket_name}')\\n s3 = boto3.resource('s3')\\n config = ('''property1=value1\\n property2=value2\\n''')\\n s3.Object(\\n\n\nbucket_name=bucket_name, \\n key=file_path\\n ).put(Body=config)\\n elif event['RequestType'] == \\\"Delete\\\":\\n print(f'Deleting file: s3://'\\n '{bucket_name}/{file_path}')\\n boto3.client('s3').delete_object(\\n Bucket=bucket_name,\\n Key=file_path\\n )\\n cfnresponse.send(event, context, cfnresponse.SUCCESS, {})\\n\" } }, \"Handler\": \"index.handler\", \"Role\": { \"Fn::GetAtt\": [ \"DeployLambdaRole\", \"Arn\" ] }, \"Runtime\": \"python3.9\", \"Timeout\": 5 } }, \"DeployRun\": { \"Type\": \"AWS::CloudFormation::CustomResource\", \"DependsOn\": \"DeployLambda\", \"Version\": \"1.0\", \"Properties\": { \"ServiceToken\": { \"Fn::GetAtt\": [ \"DeployLambda\", \"Arn\" ] } } }\n\n}, \"Outputs\": { \"RecipeBucketName\": { \"Description\": \"Name of the bucket created by the recipe\", \"Value\": { \"Ref\": \"S3Bucket\" } } } }\n\n2. Validate the template by running the following command:\n\naws cloudformation validate-template \\ --template-body file://recipe3_template.json\n\n3. Check for any errors and address the issue in the template. If everything\n\nis okay, it will return a JSON advising the capabilities required:\n\n{ \"Parameters\": [], \"Capabilities\": [ \"CAPABILITY_IAM\" ], \"CapabilitiesReason\": \"The following resource(s) require capabilities: [AWS::IAM::Role]\" }\n\n4. Deploy the stack and wait until it completes successfully:\n\naws cloudformation deploy --template-file\\ recipe3_template.json --stack-name RecipeCF-\n\nAction\\ --capabilities CAPABILITY_IAM\n\n5. Check the bucket that was created based on the con\u0000gured account and\n\nthat the text \u0000le was created and updated by the stack deployment:\n\nBUCKET_NAME=\"recipe-deploy-action-$(aws sts get-\\ caller-identity --query 'Account' --output text)\" aws s3 ls --recursive $BUCKET_NAME\n\n6. Optionally, navigate to the AWS console to check the RecipeCF-Action\n\nstack. Explore the Resources and Events tabs.\n\n7. Delete the stack. is will remove all resources that were created by the\n\nstack, including the S3 object and bucket:\n\naws cloudformation delete-stack \\ --stack-name RecipeCF-Action\n\n8. e delete-stack command triggers the deletion process but\n\ndoesn’t return anything immediately. You can check the deletion’s\n\nprogress by checking the last events until the command errors because\n\nthe stack no longer exists. e deletion can sometimes take a couple of\n\nminutes:\n\naws cloudformation describe-stack-events \\ --stack-name RecipeCF-Action --max-items 3\n\nHow it works…\n\nWhen you deployed the CloudFormation template, it applied the\n\nFn::Sub and Fn::GetAtt functions, resolving the actual values at\n\ndeployment time. en, it proceeded to create the resources,\n\nconsidering the dependencies de\u0000ned. e last to be created was the\n\nDeployRun custom resource, which causes DeployLambda to run\n\non stack creation and deletion.\n\nis Lambda de\u0000nes the Python code inline for convenience,\n\nwithin a call to Fn::Sub so that it can dynamically insert the name\n\nof the bucket using ${S3Bucket}. In addition, the code is de\u0000ned\n\nto be deployed as a ZIP \u0000le via the ZipFile property, so it\n\nautomatically includes module dependencies, which are detected by\n\nCloudFormation using the top import lines.\n\ne code creates a text \u0000le on S3 with some con\u0000guration for an\n\napplication.\n\nTo create the template via the command line, you had to specify the\n\n--capabilities argument with a value of CAPABILITY_IAM,\n\nwhich con\u0000rms that accepting the template might create resources\n\nand aﬀect permissions. is is the equivalent of the con\u0000rmation\n\ncheck box when you create the stack via the console.\n\nWhen the stack is deleted, it deletes the resources in the inverse\n\norder of deployment. To delete the DeployRun custom resource, it\n\ninvokes the Lambda, indicating in the parameters that it is a\n\ndeletion run. e code then deletes the \u0000le it created on deployment\n\n(so that the bucket can be deleted by CloudFormation) and executes\n\nthe callback provided in the parameters to specify that the deletion\n\nis complete. If this call isn’t made, the deletion will be stuck in\n\nprogress until it eventually times out.\n\nThere’s more…\n\nCloudFormation also supports using the YAML format, which is\n\nmore compact but also stricter with indentation and spaces. You\n\ncan convert a template from one format into the other using the\n\ntemplate editor in the CloudFormation console.\n\nCustom resources also allow you to specify an SNS ARN instead of\n\na Lambda, which can be used to notify listeners that the stack has\n\nbeen deployed and can take asynchronous actions.\n\nTIP\n\nWhile you develop deployment actions, you will likely make mistakes and\n\nneed to make several attempts since deleting and redeploying each time isn’t\n\npractical.\n\nInstead, you can use the AWS Lambda console to open the speciﬁc Lambda\n\nand do test invocations while passing a similar JSON to what\n\nCloudFormation uses until you’re happy with the code. Then, you can do a\n\nfull redeployment to validate.\n\nSee also\n\nCloudFormation templates have limitations because you need to follow\n\nthe template structure instead of doing your own custom code, and thus\n\nare not ideal for complex scenarios. When starting a new project, it’s\n\npreferable to use CDK, even if you don’t need complex logic yet. See the\n\nrecipes in this chapter on using CDK – that is, Setting up a code\n\ndeployment pipeline using CDK and AWS CodePipeline and Setting up a\n\nCDK pipeline to deploy on multiple accounts and regions.\n\nProtecting resources from accidental deletion\n\nIn general, automation is the best way to avoid mistakes that can\n\nresult in loss of service, data, or both. However, there are cases\n\nwhere human intervention is needed to address exceptional\n\nsituations. In a situation of urgency, the operator might need to\n\nbuild an ad hoc script quickly or take manual action under\n\npressure.\n\nSome AWS resources allow resource protection to prevent costly\n\nmistakes that can lead to serious or even irreversible damage. In this\n\nrecipe, you’ll learn how to protect RDS databases, DynamoDB\n\ntables, and CloudFormation stacks from accidental deletion.\n\nGetting ready\n\nTo complete this recipe, you need a bash command line with the\n\nAWS CLI, as indicated in the Technical requirements section at the\n\nbeginning of this chapter.\n\nHow to do it…\n\n1. Create a simple RDS database. is will return the full con\u0000guration:\n\naws rds create-db-instance --db-instance-class \\ db.t3.micro --db-instance-identifier \\ recipe-db-protected --deletion-protection -- engine \\ postgres --no-publicly-accessible \\ --allocated-storage 20 --master-username postgres \\ --master-user-password Password1\n\n2. Try to delete the instance – it should refuse to carry out the action:\n\naws rds delete-db-instance --db-instance- identifier \\ recipe-db-protected --delete-automated- backups \\ --skip-final-snapshot\n\n3. Disable the deletion protection:\n\naws rds modify-db-instance --db-instance- identifier \\ recipe-db-protected --no-deletion-protection\n\n4. Repeat step 2. It should delete the database instance.\n\n5. Create a DynamoDB table:\n\naws dynamodb create-table --table-name\\ recipe-protected --deletion-protection- enabled\\ --key-schema AttributeName=id,KeyType=HASH\\ --attribute-definitions\\ AttributeName=id,AttributeType=S\\ --billing-mode PAY_PER_REQUEST\n\n6. Try to delete the table – it should refuse:\n\naws dynamodb delete-table --table-name \\ recipe-protected\n\n7. Disable the deletion protection on the table:\n\naws dynamodb update-table --table-name \\ recipe-protected --no-deletion-protection- enabled\n\n8. Repeat step 6. Now, it should delete the table. You can verify that it’s been\n\ndeleted:\n\naws dynamodb list-tables | grep recipe- protected\n\n9. De\u0000ne your variables and create the CloudFormation template:\n\nCFN_TEMPLATE_FILE=cfn_sample.yaml STACK_NAME=CfnProtectRecipe cat > $CFN_TEMPLATE_FILE << EOF AWSTemplateFormatVersion: '2010-09-09' Resources: CfnProtectionSample:\n\nType: 'AWS::DynamoDB::Table' Properties: BillingMode: PAY_PER_REQUEST AttributeDefinitions: - AttributeName: id AttributeType: S KeySchema: - AttributeName: id KeyType: HASH EOF\n\n10. Deploy the template just created:\n\naws cloudformation deploy --template-file \\ $CFN_TEMPLATE_FILE --stack-name $STACK_NAME\n\nProtect the stack from deletion:\n\naws cloudformation update-termination- protection \\ --stack-name $STACK_NAME \\ --enable-termination-protection\n\n11. Try to delete the protected stack:\n\naws cloudformation delete-stack\\ --stack-name $STACK_NAME\n\n12. Disable deletion protection on the stack:\n\naws cloudformation update-termination- protection\\ --stack-name $STACK_NAME\\ --no-enable-termination-protection\n\n13. Repeat step 11. It should con\u0000rm the stack was deleted correctly.\n\nHow it works…\n\nIn this recipe, you created examples for three kinds of AWS data-\n\nrelated resources. For each of the three cases – RDS, DynamoDB,\n\nand CloudFormation – you veri\u0000ed that when protected, they\n\ncannot be deleted by issuing the delete command, even if your\n\nuser has permission; deletion protection had to be explicitly\n\ndisabled \u0000rst.\n\nFor RDS and DynamoDB, the resources were created with the\n\nprotection already enabled, while in the case of CloudFormation, it\n\nrequired a separate call.\n\nThere’s more…\n\nis recipe covered RDS, DynamoDB, and CloudFormation, the\n\nservices that support this kind of protection at the time of writing.\n\nIn the case of S3, the bucket cannot be deleted if there are still\n\nobjects. Note that by enabling versioning or replication, you can\n\nmitigate the damage of accidental S3 deletion.\n\nis still leaves out important systems such as Redshi databases or\n\nKinesis streams. In those cases (and in general), the best practice is\n\napplying the principle of least privilege. is means that both for\n\nautomation and manual access (both via the console and the\n\ncommand line), the user role should be as restricted as possible to\n\naccomplish its purpose, which also serves to help reduce mistakes.\n\nFor instance, the operations team could have access to read-only\n\nroles, but to get an admin role, they might require approval, review,\n\nor a two-person process.\n\nSee also\n\nIdeally, you want to have environments where deployments and changes\n\nare only made by pipelines and not manually. With that automation in\n\nplace and highly restricted manual intervention on production, the\n\naccidental protections seen in this recipe could be considered\n\nunnecessary. To learn more about deployment automation, see the\n\nSetting up a code deployment pipeline using CDK and AWS CodePipeline\n\nand Deploying a data pipeline using Terraform recipe.\n\nDeploying a data pipeline using Terraform\n\nIn this recipe, you will see the Terraform cloud-agnostic IaC\n\nautomation tool. Agnostic doesn’t mean you can abstract yourself\n\nfrom the underlying vendor but that the tool has support for\n\nmultiple cloud providers, so you can apply the knowledge of the\n\ntool to other providers as well as build multi-cloud projects. But you\n\nstill need to know about the diﬀerent services on each cloud\n\nprovider and how to use them. See the oﬃcial vendor\n\ndocumentation for further details: https://www.terraform.io/.\n\ne trade-oﬀ of its multi-cloud support is that because it’s not\n\nprovided directly by each vendor, it may take some time until\n\nTerraform supports new features.\n\nGetting ready\n\nTo complete this recipe, you need a bash command line with the\n\nAWS CLI set up, as indicated in the Technical requirements section\n\nat the beginning of this chapter.\n\nYou also need the Terraform executable in your system. You must\n\nput it in the system’s PATH so that it can be executed easily. Check\n\nthe HashiCorp website for instructions on how to download the\n\nexecutable for your OS and architecture:\n\nhttps://developer.hashicorp.com/terraform/install.\n\nYou also need a user who can log in to the AWS console and use\n\nAWS Glue.\n\nHow to do it…\n\n1. Create a directory for the Terraform project and change to it:\n\nmkdir terraform_recipe && cd terraform_recipe\n\n2. Create a \u0000le with con\u0000guration variables (note that the \u0000rst \\ is to break\n\nthe line and the second one is to escape the $ sign so that bash doesn’t\n\ntry to replace it):\n\ncat > var.tf << EOF variable \"region\" { description = \"AWS region\" type = string default = \"us-east-1\" } data \"aws_caller_identity\" \"current\" {} locals { bucket_name = \"terraform-recipe-\\ \\${data.aws_caller_identity.current.account_id }\" } variable \"script_file\" { type = string default = \"ShellScriptRecipe.py\" } EOF\n\n3. Create a placeholder script for the Glue Shell job:\n\necho 'print(\"Running Glue Shell job\")' > \\ ShellScriptRecipe.py\n\n4. Create the main Terraform project \u0000le so that you can deploy the script\n\nto S3 (here, EOF has quotes around it, so it is written literally and doesn’t\n\nhave to escape via $):\n\ncat > main.tf << 'EOF' provider \"aws\" {",
      "page_number": 518
    },
    {
      "number": 27,
      "title": "Segment 27 (pages 538-560)",
      "start_page": 538,
      "end_page": 560,
      "detection_method": "synthetic",
      "content": "region = var.region } resource \"aws_s3_bucket\" \"bucket\" { bucket = local.bucket_name } resource \"aws_s3_object\" \"script\" { bucket = local.bucket_name key = \"scripts/${var.script_file}\" source = var.script_file depends_on = [ aws_s3_bucket.bucket ] } EOF\n\n5. Append a Glue Shell job and a new role with minimum permissions it\n\ncan use to run to the main \u0000le (notice >> to append instead of\n\noverwriting):\n\ncat >> main.tf << EOF resource \"aws_iam_role\" \"glue\" { name = \"AWSGlueServiceRoleTerraformRecipe\" managed_policy_arns = [ \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAcces s\", \"arn:aws:iam::aws:policy/CloudWatchAgentServer Policy\" ] assume_role_policy = jsonencode( { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"sts:AssumeRole\", \"Principal\": {\n\n\"Service\": \"glue.amazonaws.com\" }, \"Effect\": \"Allow\", \"Sid\": \"\" } ] } ) } resource \"aws_glue_job\" \"shell_job\" { name = \"TerraformRecipeShellJob\" role_arn = aws_iam_role.glue.arn command { name = \"pythonshell\" python_version = \"3.9\" script_location = \"s3://\\${local.bucket_name}\\ /scripts/\\${var.script_file}\" } } EOF\n\n6. Initialize the Terraform project. is can take a minute or so since it\n\nneeds to download the AWS provider:\n\nterraform init\n\n7. Validate that the \u0000les that were generated in the previous steps are valid,\n\nas well as the AWS CLI credentials and con\u0000guration:\n\nterraform plan\n\nIf all is correct, the plan summary should look like this:\n\nPlan: 4 to add, 0 to change, 0 to destroy.\n\n8. Deploy the stack with the apply command. Using the region\n\nvariable, set the name of your region – for example, us-east-2:\n\nterraform apply -var region=us-east-2\n\n9. In the AWS console, navigate to AWS Glue on the chosen region. Use the\n\nETL Jobs menu and then select TerraformRecipeShellJobs from the job\n\ntable. Use the Run button and then open the Runs tab. Once it completes\n\nsuccessfully, you can use the Output link to view the message the job\n\nprinted:\n\nFigure 8.2 – AWS Glue job deployed by Terraform\n\n10. Back on the command line, clean up if you don’t want to keep the\n\ndeployment.\n\nYou need to specify the same region you used on deployment,\n\nlike so:\n\nterraform destroy -var region=us-east-2\n\nHow it works…\n\nIn addition to the trivial Python script, you created two \u0000les with\n\nthe .tf (Terraform Format) extension. ese are text \u0000les that\n\nresemble JSON but it’s not (notice that there are no commas at the\n\nend of the lines and the object de\u0000nition is also diﬀerent). In\n\naddition, this format allows you to easily reference other objects\n\nfrom the same or diﬀerent \u0000les as you don’t need to import other\n\n\u0000les to reference them.\n\ne names of the \u0000les are arbitrary. Terraform will pick up any \u0000les\n\nwith the extension and load them together; as shown in this recipe,\n\nit’s common practice to have a \u0000le that acts as a\n\nvariable/con\u0000guration reference so that it’s easy to \u0000nd what can be\n\ncon\u0000gured.\n\nInside the main \u0000le, the variables can be used either with a direct\n\nreference using the var/local pre\u0000x or inside a string with a\n\nsyntax like bash: ${variable-name}. e bucket name is de\u0000ned\n\nin locals instead of variable so that it can dynamically retrieve\n\nthe account ID and make the bucket unique for your account.\n\nVariables have default values that can be overridden at deployment\n\ntime, as you did by specifying the region when applying and\n\ndestroying the stack.\n\nIf you peruse the Terraform \u0000le, you will see that it’s intuitive to\n\nunderstand how it de\u0000nes the components. An important element\n\nis the provider \"aws\" section, which tells Terraform that the\n\nproject needs the libraries for AWS resources.\n\nWhen you ran terraform init, it downloaded the AWS provider\n\nand put it under the project’s hidden .terraform folder.\n\nBy running the AWS Glue job successfully, you proved that the job\n\nwas deployed with all the required con\u0000gurations: it could \u0000nd and\n\nload the script you created locally and was able to impersonate the\n\nIAM role created for it.\n\nThere’s more…\n\nIn this recipe, you used terraform plan to validate the\n\ndeployment on the actual AWS account. If you want to check if the\n\ncon\u0000g is valid but don’t have access to an account (for instance,\n\nbecause you are oﬄine), you can run terraform validate\n\ninstead to make sure the syntax is correct. e terraform fmt\n\ncommand can be used to format the .tf \u0000les to conform to the\n\nTerraform conventions and recommendations.\n\nIn addition to AWS, Terraform has support for many providers for\n\nthe popular cloud vendors, as well as generic ones such as the one\n\nfor Kubernetes. ere is a registry where they are clearly labeled,\n\nindicating whether they are provided by HashiCorp, a partner, or\n\nthe community. You can use multiple providers on the same project.\n\nTo keep track of deployments, Terraform creates a \u0000le named\n\nterraform.tfstat (as well as a backup when it needs to update\n\nit). e \u0000le is synchronized with the cloud before deployment. On a\n\nproduction project, it is bad practice to have it locally since it can be\n\nlost or get out of sync if diﬀerent people have their own local copy.\n\nYou can con\u0000gure Terraform to save that \u0000le in a shared place (for\n\nexample, S3) and use locks with a tool such as DynamoDB so that\n\nthe status \u0000le is free from con\u0000icts or loss.\n\nSee also\n\nInstead of writing your own Terraform code, you could generate it from\n\nexisting resources. See the Reverse-engineering IaC recipe.\n\ne main alternative to Terraform is the frameworks and tools provided\n\nby each speci\u0000c cloud vendor. For AWS, that’s CloudFormation and\n\nCDK. To learn more about them, take a look at the Setting up a code\n\ndeployment pipeline using CDK and AWS CodePipeline, Setting up a CDK\n\npipeline to deploy on multiple accounts and regions, and Running code in a\n\nCloudFormation deployment recipes in this chapter.\n\nTo learn more about data pipelines using AWS Glue, refer to Chapter 3,\n\nIngesting and Transforming Your Data with AWS Glue.\n\nReverse-engineering IaC\n\nOen, we aren’t creating a brand-new data infrastructure but\n\nextending existing resources that we want to automate. So, future\n\nchanges are automatically tracked and deployed using IaC. You may\n\nalso need some help building your IaC and would rather create the\n\nresources using the AWS console than generate the corresponding\n\ncode. You can then use this as the basis instead of writing the code\n\nfrom scratch.\n\nIn this recipe, you will learn how Terraformer can reverse engineer\n\nexisting resources to generate Terraform code. is tool is relatively\n\nnew and still under development, so it’s likely to help you but not do\n\nthe full work for you.\n\nIf you wish to use other IaC tools instead of Terraform, there are\n\nreverse-engineering alternatives that you can check in the See also\n\nsection at the end of this recipe.\n\nGetting ready\n\nTo complete this recipe, you need a bash command line with the\n\nAWS CLI set up, as indicated in the Technical requirements section\n\nat the beginning of this chapter.\n\nYou also need the Terraform executable in our system in the\n\nsystem’s PATH so that it can be executed just using its name. Check\n\nthe HashiCorp indications and download the executable matching\n\nthe OS and architecture from their website:\n\nhttps://developer.hashicorp.com/terraform/install. In addition,\n\nyou’ll need the Terraformer executable, which you can download as\n\na binary or install with a package manager. Check the project on\n\nGitHub for instructions:\n\nhttps://github.com/GoogleCloudPlatform/terraformer.\n\nDISCLAIMER\n\nDespite the project being hosted in the Google Cloud community repository,\n\nit is Apache licensed open source and it’s not aﬃliated nor provided by\n\nAlphabet Inc.\n\nHow to do it…\n\n1. Create the Glue job that you’ll reverse engineer later to Terraform code:\n\naws glue create-job --name recipe-shell- reveng\\ --role arn:aws:iam::$(aws sts get-caller- identity\\ --query 'Account' --output text)\\ :role/SomeRoleForGlue --command \\\n\n'{\"Name\": \"pythonshell\", \"PythonVersion\":\"3.9\", \"ScriptLocation\": \"s3://somebucket/yourscript.py\"}'\n\n2. View the job you just created. You’ll see that many properties have been\n\n\u0000lled with defaults:\n\naws glue get-job --job-name recipe-shell- reveng\n\n3. Initialize an empty Terraform project for AWS:\n\nmkdir recipe_reveng && cd recipe_reveng echo \"provider \"aws\" {}\" > provider.tf terraform init\n\n4. Import the Glue job you created in step 1 into the Terraform project:\n\nterraformer import aws --path-pattern=.\\ --compact=true --resources=glue --filter\\ \"Name=name;Value=recipe-shell-reveng\"\n\n5. Edit the resources \u0000le using a text editor – for instance, vi\n\nresources.tf. Make the following changes and save them:\n\nRemove number_of_workers = \"0\"\n\nChange the number of retries to 1 with max_retries =\n\n\"1\"\n\n6. Test the Terraform project with the changes and address any errors:\n\nterraform plan\n\nAt the time of writing, the latest version of Terraformer is\n\nv0.8.24 and still references the old Terraform AWS provider.\n\nCheck for the following error:\n\nError: Failed to load plugin schemas Error while loading schemas for plugin components: Failed to obtain provider schema: Could not load the schema for provider registry.terraform.io/-/aws: failed to instantiate provider \"registry.terraform.io/-/aws\" to obtain schema: unavailable provider \"registry.terraform.io/-/aws\"..\n\nIf you can see this error, you need to address it by upgrading the\n\nprovider:\n\nterraform state replace-provider \\ registry.terraform.io/-/aws \\ registry.terraform.io/hashicorp/aws\n\nIf you’re running the code on Microso Windows, you might get\n\nthe following error:\n\nError: Failed to read state file The state file could not be read: read terraform.tfstate: The process cannot access the file because another process has locked a portion of the file.\n\nis is a known issue regarding handling \u0000le locks locally. You\n\ncan work around this by adding -lock=false to the failed\n\ncommand, like so:\n\nterraform plan -lock=false\n\n7. Deploy the changes you made in step 5 and con\u0000rm that the retries is\n\nnow set to 1:\n\nterraform apply aws glue get-job --job-name recipe-shell- reveng\n\n8. Delete the Glue Shell job using Terraform:\n\nterraform destroy\n\nHow it works…\n\nFirst, you created a very simple Glue job to reverse engineer. Note\n\nthat this job can’t run since it references a non-existing AWS role\n\nand S3 path – it is just for demonstration purposes. To learn more\n\nabout Glue, check out Chapter 3, Ingesting and Transforming Your\n\nData with AWS Glue.\n\nen you initialized a minimal Terraform project to download the\n\nAWS provider under the .terraform directory, which\n\nTerraformer requires to access the account. It is possible to put the\n\nprovider on a shared path, so it’s shared by projects.\n\nUsing Terraformer, you imported the job, and underneath it used\n\nthe AWS CLI to access the account using the default credentials and\n\ncon\u0000guration, such as region. at generated multiple \u0000les in the\n\ncurrent directory. is is due to the --path-pattern= parameter,\n\nwhich places the import \u0000les in the current directory instead of\n\nmultiple subdirectories. Note that --compact=true indicates all\n\nthe Terraform resources are imported into the resources.tf \u0000le.\n\nIn addition, the import statement overwrote the provider.tf \u0000le\n\nto pin the AWS provider version, while Terraform 1.0 or later has a\n\ncombability promise. It is good practice to specify the version for\n\nbetter stability. e ~> operator indicates that the build version (the\n\nlast of the three numbers) can change but not the others. is\n\nallows you to make \u0000xes to the version with a very low risk of\n\nintroducing incompatibilities.\n\nIt’s also noteworthy that it generated a variables.tf \u0000le,\n\nindicating that the con\u0000guration is stored locally on the\n\nterraform.tfstate \u0000le. For a production project, this should be\n\nreplaced with a shared durable repository such as S3.\n\nIf you had to replace the AWS provider, it upgraded the\n\nterraform.tfstate \u0000le as needed to bring it up to the Terraform\n\nversion you’re using. e state \u0000le is a JSON \u0000le, so you can open it\n\nand explore its contents.\n\nYou used Terraform to deploy a change to the Glue job and \u0000nally\n\ndelete it by destroying the stack. Both commands demonstrated that\n\nonce you completed the import, you can continue working as a\n\nregular Terraform project. Running Terraformer again would\n\noverride any changes you made.\n\nThere’s more…\n\nAs you’ve seen, reverse engineering is not trivial and you’ll likely\n\nneed to make adjustments, such as removing the number of\n\nworkers on a Glue Shell job, since it doesn’t have that concept. You\n\nshould use the generated project as a template and revise all the\n\ncode that’s produced for potential mistakes and inconsistencies. In\n\naddition, many resource types are still not available, so you would\n\nhave to add that part of the code manually.\n\nWhen you use the Terraformer import command, you can specify\n\nmultiple resources so that they are generated on the same project.\n\nHowever, the \u0000lters for resources must be speci\u0000c to the resource\n\ntype. Alternatively, you can import all without \u0000lters and then\n\nmanually delete what you don’t need.\n\nSee also\n\nTo learn how to create a Terraform project from scratch, see the\n\nDeploying a data pipeline using Terraform recipe.\n\nIf you’d rather use CDK, CloudFormation, or AWS CLI commands\n\ninstead of Terraform, there are alternative tools to help you generate\n\ncode automatically:\n\nFormer 2 (www.former2.com): is connects to your AWS\n\naccount, detects resources, and generates code for multiple\n\nlanguages, including CDK and CloudFormation. It requires you\n\nto provide AWS credentials, so it’s best practice to provide a\n\ntemporary session with read-only limited privileges. e tool is\n\nunder development and doesn’t work for all kinds of resources.\n\nDISCLAIMER\n\nThe Former 2 tool is provided “as-is” without any guarantees, by a reputable\n\ndeveloper (AWS ambassador since 2018).\n\nConsole Recorder for AWS: From the creator of Former 2, it’s no longer\n\nunder development, so it is missing new services but it’s very easy to use.\n\nIt provides a plugin for the main web browsers, which listens to the\n\nactions you do on the AWS console and then can provide the equivalent\n\ncode for many languages, including CDK, CloudFormation, boto3, and\n\nthe AWS CLI.\n\nIntegrating AWS Glue and Git version control\n\nAWS Glue is a serverless data integration service that oﬀers\n\ndiﬀerent engines and tools for diﬀerent personas involved in data\n\nengineering. Git is the industry standard source code version\n\ncontrol system. Integrating both enables version handling on Glue\n\njobs and improves DevOps in general.\n\nIn this recipe, you’ll learn how to save and retrieve the status of a\n\nGlue job on a Git repository provided by AWS CodeCommit. is\n\nfeature is supported for other kinds of jobs, including notebooks,\n\nand is also available directly on the AWS console. See the ere’s\n\nmore… section for further details.\n\nGetting ready\n\nTo complete this recipe, you need a command line bash with the\n\nAWS CLI set up, as indicated in the Technical requirements section\n\nat the beginning of this chapter. e AWS user needs permission to\n\nuse AWS CodeCommit.\n\nHow to do it…\n\n1. Set up a placeholder Python script on S3 for the Glue job:\n\nBUCKET_NAME=\"recipe-glue-git-\\ $(aws sts get-caller-identity --query 'Account' \\ --output text)\" AWS_REGION=\"$(aws configure get region)\" aws s3api create-bucket --bucket $BUCKET_NAME \\ --create-bucket-configuration \\ LocationConstraint=$AWS_REGION\n\necho \"print('Running Shell job')\" > ShellScript.py aws s3 cp ShellScript.py s3://$BUCKET_NAME rm ShellScript.py\n\n2. Create a Glue Shell job, referencing the script you uploaded in Step 1:\n\naws glue create-job --name recipe-glue-git- job\\ --role arn:aws:iam::$(aws sts get-caller- identity\\ --query 'Account' --output text)}role/RoleForGlue\\ --command '{\"Name\" : \"pythonshell\",'\\ '\"PythonVersion\": \"3.9\", \"ScriptLocation\":'\\ '\"s3://'$BUCKET_NAME'/ShellScript.py\"}'\n\n3. Create a CodeCommit Git repository and push the job onto the main\n\nbranch:\n\naws codecommit create-repository \\ --repository-name RecipeGlueGit aws glue update-source-control-from-job --job- name \\ recipe-glue-git-job --provider AWS_CODE_COMMIT \\ --repository-name RecipeGlueGit --branch-name main\n\n4. Delete the job script and verify it’s no longer in the bucket:\n\naws s3 rm s3://$BUCKET_NAME/ShellScript.py aws s3 ls s3://$BUCKET_NAME/\n\n5. Recover the script from Git to S3 so that the job can use it:\n\naws glue update-job-from-source-control --job- name \\ recipe-glue-git-job --provider AWS_CODE_COMMIT \\ --repository-name RecipeGlueGit --branch- name main aws s3 ls s3://$BUCKET_NAME/\n\n6. Download the restored script and verify that it contains the simple\n\nprint statement you created it with in step 1:\n\naws s3 cp s3://$BUCKET_NAME/ShellScript.py . cat ShellScript.py\n\n7. Finally, run the cleanup commands:\n\nrm ShellScript.py aws glue delete-job --job-name recipe-glue- git-job aws s3 rm s3://$BUCKET_NAME/ShellScript.py aws s3api delete-bucket --bucket $BUCKET_NAME\n\nHow it works…\n\nIn the \u0000rst two steps, you created a sample Glue Shell job with a\n\nminimal script. Please note that the role that we used wasn’t created\n\nin this recipe, so the job can’t be run. However, this isn’t a\n\nrequirement for this recipe.\n\nen, you created a CodeCommit Git repository using the default\n\nmain branch, where you asked Glue to save the job. is created a\n\ncommit on the branch with two \u0000les placed under a directory\n\nmatching the name of the job: a JSON \u0000le with the job de\u0000nition\n\nand the Python script. It is possible to con\u0000gure the folder on Git\n\nunder which these \u0000les are saved.\n\nNotice how simple this was compared to having to con\u0000gure a Git\n\nclient, credentials, and connectivity; it’s all integrated using your\n\nIAM user permissions.\n\nTo demonstrate versioning, you simulated a mistake where the\n\nscript on S3 was deleted and then recovered from the latest version\n\nof the Git repository.\n\nThere’s more…\n\nis recipe used the command line to do all the actions to\n\ndemonstrate it can be automated, as well as to make things easier\n\nfor users not familiar with AWS Glue.\n\nHowever, normally, this Git integration is used via the AWS console\n\nwhile the user is developing a data solution. is is especially the\n\ncase for AWS Studio to develop visual jobs, where the user de\u0000nes\n\nthe ETL visually instead of writing a script \u0000le.\n\ne Glue console provides an action to push and pull changes from\n\nGit that can be used in all kinds of Glue jobs, including managed\n\nnotebooks. When you use this action in the console, the repository’s\n\ncon\u0000guration is stored on the Version Control tab:\n\nFigure 8.3 – Glue Studio version control actions\n\nIf you wish to use one of the other Git repositories supported\n\ninstead of CodeCommit, such as GitHub or GitLab, you must\n\nspecify further details and authentication mechanisms, depending\n\non each vendor’s requirements.\n\nIn this recipe, you used the default main branch for simplicity; in\n\npractice, development would typically be done on a diﬀerent\n\nbranch. en, once a version is deemed suitable for production, it\n\nwould be merged onto the branch that re\u0000ects production, possibly\n\nwith an automated process that monitors that branch for commits\n\nand triggers a pipeline to get those changes tested and deployed.\n\nAt the time of writing, branches must already exist for Glue to be\n\nable to use them. You can create one easily if you have the Git client\n\non a checkout project by running git branch <new branch\n\nname> or by using the AWS CLI on CodeCommit, branching from\n\nanother branch, like so:\n\naws codecommit get-branch --repository-name <your repo> \\ --branch-name <branch to branch from> aws codecommit create-branch --repository-name <your repo>\\ --branch-name dev --commit-id <from the first command>\n\nSee also\n\nWhen the Glue job is pushed into the repository, you can have a\n\ndeployment pipeline automatically trigger and run a CI/CD process. e\n\nSetting up a code deployment pipeline using CDK and AWS CodePipeline\n\nrecipe shows how to do this.\n\nTo learn more about Glue and its capabilities, see the recipes dedicated\n\nto this AWS data integration service in Chapter 3, Ingesting and\n\nTransforming Your Data with AWS Glue.\n\nOceanofPDF.com\n\n9 Monitoring Data Lake Cloud Infrastructure\n\nIn this chapter, we will discuss the essential aspects of tracking and\n\nmonitoring your data lake infrastructure. A data lake, oen a\n\nrepository for vast amounts of structured and unstructured data, is\n\na critical component of any data-driven organization. However,\n\nwithout eﬀective monitoring, the data lake can quickly become a\n\ndata swamp, leading to ineﬃciencies, increased costs, and potential\n\ncompliance risks. e recipes covered in this chapter are designed\n\nto address common challenges and ensure your data lake remains\n\nan asset rather than a liability.\n\nis chapter includes the following recipes:\n\nAutomatically setting CloudWatch log group retention to reduce cost\n\nCreating custom dashboards to monitor Data Lake services\n\nSetting up System Manager to remediate non-compliance with AWS\n\nCon\u0000g rules\n\nUsing AWS con\u0000g to automate non-compliance S3 server access logging\n\npolicy\n\nTracking AWS Data Lake cost per analytics workload\n\nBy the end of this chapter, you will have acquired the knowledge\n\nand skills necessary to monitor your data platform eﬀectively. is\n\nwill help you maintain a robust, eﬃcient, cost-eﬀective data lake\n\noperation.\n\nTechnical requirements\n\ne code \u0000les for this chapter are available on GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter09.\n\nAdditional information\n\nBefore you start this chapter, you need to understand the following\n\nconcepts:\n\nLogs: Logs are vital for diagnosing issues, auditing activities, and\n\nmonitoring the health of your data lake. You can subscribe to speci\u0000c\n\nevents within your log \u0000les, such as errors and warnings, to stay\n\ninformed about critical occurrences. is proactive approach enables\n\nyou to address issues before they escalate, ensuring the smooth\n\noperation of your data lake services.\n\nAlarms: Alarms are used to monitor speci\u0000c metrics and trigger actions\n\nbased on prede\u0000ned thresholds. For example, you can set an alarm for\n\nCPU utilization to notify the development team when usage exceeds a\n\ncertain limit. Additionally, alarms can automate responses, such as\n\nlaunching new instances to handle increased load, thereby maintaining\n\nperformance and avoiding service disruptions.\n\nEventBridge: It contains events, rules, and targets. Amazon EventBridge\n\nallows you to manage and respond to events across your AWS\n\nenvironment. It comprises three main components:\n\nEvents: Any signi\u0000cant occurrence in your system, such as an\n\nAPI call, console sign-in, auto-scaling state change, EC2\n\ninstance state change, or EBS volume creation, is included here.\n\nRules: ey de\u0000ne the conditions under which events should\n\ntrigger actions. For example, you might create a rule that\n\ntriggers when an EC2 instance changes state.\n\nTargets: ese are the actions that are executed when an event\n\nmatches a rule. Common targets include AWS Lambda\n\nfunctions, Amazon Simple Noti\u0000cation Service (SNS), and\n\nAmazon Simple Queue Service (SQS).\n\nBy con\u0000guring EventBridge with appropriate rules and targets,\n\nyou can automate responses to various events, enhancing the\n\nresilience and eﬃciency of your data lake infrastructure.\n\nAutomatically setting CloudWatch log group retention to reduce cost\n\nAmazon CloudWatch collects metrics, logs, and events from your\n\nresources by default. ese logs could then be used to build",
      "page_number": 538
    },
    {
      "number": 28,
      "title": "Segment 28 (pages 561-581)",
      "start_page": 561,
      "end_page": 581,
      "detection_method": "synthetic",
      "content": "dashboards, alarms, and alerts. By default, Amazon CloudWatch\n\nLogs stores your log data inde\u0000nitely, which can add up the cost,\n\nparticularly when you use detailed monitoring instead of basic\n\nmonitoring. Using Lambda to automatically check log groups\n\nwithin the regions of your services and data lake can help you save\n\non storage costs.\n\nGetting ready\n\nBefore reducing logging costs, you need to have a strategy. Good\n\nlogging leads to good monitoring. A sizable number enables\n\nhumans and machines to analyze information. It would be best to\n\nhave a logging strategy that can answer questions such as “who did\n\nwhat and when?” without including sensitive information such as\n\npasswords or secrets before trying to reduce the number of logs.\n\nHow to do it…\n\n1. In the Home console, click on IAM:\n\nFigure 9.1 – Selecting IAM in the portal\n\n2. Click on Policies and then Create policy, as shown:\n\nFigure 9.2 – Creating a policy for the Lambda service\n\n3. Select Lambda from the Select a service drop-down menu:\n\nFigure 9.3 – Selecting Lambda\n\n4. On the next page, select JSON formatting:\n\nFigure 9.4 – Creating a policy as JSON\n\n5. Add the following Lambda IAM policy in JSON formatting:\n\n\"Version\": \"2012-10-17\", \"Statement\": [\n\n{ \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:PutLogEvents\", \"logs:DescribeLogGroups\", \"logs:PutRetentionPolicy\" ], \"Resource\": \"*\" } ] }\n\n6. Name the policy, for example, lambda-logging. en click on\n\nCreate policy:\n\nFigure 9.5 – Naming the Lambda policy\n\n7. Next, click on Create role from the IAM console:\n\nFigure 9.6 – Selecting roles in the IAM portal\n\n8. Select AWS Service and then Lambda under the Service or use case\n\ndropdown, and then click on Next:\n\nFigure 9.7 – Selecting the Lambda service for roles\n\n9. Select the lambda-logging policy that we created in step 6 and click\n\non Next:\n\nFigure 9.8 – Adding the lambda-logging policy to roles\n\n10. Fill in Role details and click on Create role. For best supervision, click on\n\nAdd tag and add the purpose of creating the policy:\n\nFigure 9.9 – Clicking on Create role\n\n11. Select Services | Lambda as shown:\n\nFigure 9.10 – Selecting the Lambda service\n\n12. en click on the Create a function button:\n\nFigure 9.11 – Selecting the Create a function button\n\n13. Fill in basic information, as shown in Figure 9.12, and choose the IAM\n\nrole that was created in step 10. en click on Next:\n\nFigure 9.12 – Filling in the function name, runtime, and execution\n\nrole\n\n14. Go to the code editor and paste the following Python code:\n\nimport boto3 from botocore.config import Config import logging import os # Set the number of retention days retention_days = 30 # logging LOGGER = logging.getLogger() LOGGER.setLevel(logging.INFO) logging.getLogger('boto3').setLevel(logging.CR ITICAL) logging.getLogger('botocore').setLevel(logging .CRITICAL) regions = [item.strip() for item in os.environ['AVAILABLE_REGION'].split(\",\") if item]\n\ndef lambda_handler(event, context): LOGGER.info(f\"start checking= {regions}\") if not regions: return {'statusCode': 200, 'body': 'No regions found '} for region in regions: client = boto3.client('logs', region_name= region) response = client.describe_log_groups() nextToken = response.get('nextToken', None) log_groups = response['logGroups'] # Continue to fetch log groups if nextToken is present while nextToken is not None: response = client.describe_log_groups(nextToken=nextToken ) nextToken = response.get('nextToken', None) log_groups += response['logGroups'] for group in log_groups: if 'retentionInDays' in group.keys(): print(group['logGroupName'], group['retentionInDays'], region) else: print(\"Retention Not found for \", group['logGroupName'], region) set_retention = client.put_retention_policy(\n\nlogGroupName=group['logGroupName'],\n\nretentionInDays=retention_days ) LOGGER.info(f\"PutRetention result {set_retention}\") return {'statusCode': 200, 'body': 'completed.'}\n\n15. Go to the CloudWatch console and navigate to the Rules section:\n\nFigure 9.13 – Clicking on Rules under the Events section\n\n16. Click on Create rule:\n\nFigure 9.14 – Clicking on Create rule\n\n17. Fill in the Name and Description \u0000elds, as shown, and click on Next:\n\nFigure 9.15 – Filling in the rule details\n\n18. When you click on Next, you will be led to the page where you can select\n\nthe event pattern. Select CloudWatch Logs under AWS Service and AWS\n\nAPI Call via CloudTrail under Event Type as shown here:\n\nFigure 9.16 – Selecting the AWS service and Event type options\n\n19. In the Speci\u0000c operations \u0000eld, add CreateLogGroup:\n\nFigure 9.17 – Filling in the Speciﬁc operation(s) box\n\n20. On the Create rule page, select the Lambda function that you created to\n\nlink the CloudWatch Events rule with the Lambda function:\n\nFigure 9.18 – Selecting Lambda as the target service\n\n21. Con\u0000gure tags if needed. en, under Review and create, review all the\n\nprevious steps and then click on Create rule:\n\nFigure 9.19 – Creating the CloudWatch rule\n\nHow it works…\n\ne Lambda function will scan all log groups within the region and\n\napply the 30-day retention rule. CloudWatch will trigger the\n\nLambda function to run every \u0000ve days. ere are two required\n\nparameters, logGroupName and retentionInDays, which de\u0000ne\n\nthe name of the log group you want to target and the number of\n\ndays you are trying to retain a log.\n\nIn the Event pattern, once you are used to the manual select, you\n\ncan re-use the JSON and alter it per use case. e JSON event\n\npattern that was created through steps 8-10 is as follows:\n\n{ \"source\": [\"aws.logs\"], \"detail-type\": [\"AWS API Call via CloudTrail\"], \"detail\": { \"eventSource\": [\"logs.amazonaws.com\"], \"eventName\": [\"CreateLogGroup\"] } }\n\nThere’s more…\n\nAnother architecture option is to combine Event Bridge with\n\nLambda. is architecture creates an event-based Lambda trigger to\n\napply the retention rules right away aer the log groups are created,\n\nsuch as in the reference link. Alternatively, you can also set the S3\n\nlife cycle policy archive log data to S3.\n\nSee also\n\nput_retention_policy:\n\nhttps://boto3.amazonaws.com/v1/documentation/api/latest/reference/se\n\nrvices/logs/client/put_retention_policy.html#\n\nReduce log-storage costs by automating retention settings in Amazon\n\nCloudWatch: https://aws.amazon.com/blogs/infrastructure-and-\n\nautomation/reduce-log-storage-costs-by-automating-retention-settings-\n\nin-amazon-cloudwatch/\n\nFilter pattern syntax for metric \u0000lters, subscription \u0000lters, and \u0000lter log\n\nevents:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAn\n\ndPatternSyntax.html\n\nCreating custom dashboards to monitor Data Lake services\n\nCloudWatch dashboards allow you to create interactive\n\nvisualizations of your data, giving you a consolidated view of the\n\nhealth and status of your AWS assets. CloudWatch provides several\n\npre-built dashboards for various services, but you can also create\n\ncustom dashboards to meet your speci\u0000c needs.\n\nIn the daily operation, you should have a high-level dashboard built\n\nfrom CloudWatch log groups metrics to help you understand the\n\ngeneral performance and perform drill-down on services if needed,\n\nsuch as creating a high-level dashboard on ETL performance and\n\nthen drilling down to Glue job run monitoring if required for\n\nfurther investigation.\n\nGetting ready\n\nTo proceed with this recipe, you need a CloudWatch log group for\n\nthe services for which you want to build a dashboard.\n\nHow to do it…\n\n1. Open the AWS console and select the CloudWatch service, which will\n\npresent you with a screen that is similar to the following:\n\nFigure 9.20 – Selecting the CloudWatch log group that you want\n\nto monitor\n\n2. Click on Dashboards. Here, you will see two options: Custom\n\ndashboards and Automatic dashboards. For this recipe, we will click on\n\nCreate dashboard next to Custom Dashboards.\n\nFigure 9.21 – Selecting Create dashboard\n\n3. On the next page, select the widget that is relevant to your use case. e\n\npopular ones are as follows:\n\nLine chart, which helps you to see the trend over time\n\nAlarm status, which helps you see a set of alarms\n\nBar chart, which helps you compare categories of data\n\nFigure 9.22 – Selecting the type of chart that is appropriate for\n\nyour dashboard\n\n4. Select whether you want to monitor metrics or log insights, which allow\n\nyou to query log groups. You can click on Choose a sample query to see\n\na sample query that you can run in the CloudWatch console.\n\nFigure 9.23 – Creating a query for your dashboard\n\nHow it works…\n\nAmazon CloudWatch dashboards rely on the underlying structure\n\nof log groups and streams to organize and present data. A log group\n\nacts as a container for log streams that share the same properties,\n\noen grouped by application, system component, or similar criteria.\n\nese log groups can be associated with speci\u0000c AWS resources.\n\nWhen creating a dashboard, users can select speci\u0000c log groups and\n\nstreams and then apply metric \u0000lters to transform the raw log data\n\ninto numerical metrics. ese metrics can be visualized in various\n\nforms, such as graphs, charts, or tables within the dashboard. is\n\nprocess allows users to distill large volumes of log data into\n\nmeaningful insights.\n\nThere’s more…",
      "page_number": 561
    },
    {
      "number": 29,
      "title": "Segment 29 (pages 582-599)",
      "start_page": 582,
      "end_page": 599,
      "detection_method": "synthetic",
      "content": "Aer you create a dashboard, you can migrate it across multiple\n\nenvironments or accounts by simply copying the source of the\n\nwidget to a new environment using the Copy source option, as\n\nshown. Remember to check that the properties part aligns with the\n\nproperties of the new environment.\n\nFigure 9.24 – Copying the source of a widget to migrate the\n\ndashboard to another environment\n\nFor S3 monitoring, S3 Storage Lens provides metrics and dashboard\n\nability to allow you to understand your storage usage. Some use\n\ncases include observing bucket costs across organizations, life cycle\n\nrules, and incomplete multiple upload parts that are over seven days\n\nold. ere are free and advanced metrics for building the\n\ndashboard.\n\nDepending on your use case, there would be a lot of metrics to\n\nchoose from, such as ActiveJobCount and ActiveWork\u0000owCount,\n\nwhich help you understand Glue resource utilization or YARN\n\nutilization metrics for EMR. Before choosing which logs to monitor,\n\nyour team must decide on relevant use case logs with the goals of\n\nmonitoring and investigation in mind. Selecting the relevant\n\nmetrics should help give you an overview of your service and\n\nsystem health with an option to drill down further if needed. ere\n\nare \u0000ve key areas to monitor:\n\nPerformance such as bottlenecks and hotspots\n\nSecurity such as assessments and violations\n\nCon\u0000guration such as change history and violations\n\nCost such as drive and control business spending\n\nFault tolerance such as reliability and availability\n\nCloudwatch comes with standard metrics such as CPU and network\n\nutilization for instances, disk read/write operations for Amazon\n\nEBS volumes, and memory and disk activity for managed database\n\n(Amazon RDS) instances. You can also create custom metrics that\n\nare speci\u0000c to the function of your instance and have them\n\nregistered in CloudWatch. For example, if you are running an\n\nHTTP server on the instance, you could publish a statistic on\n\nservice memory usage.\n\nSee also\n\nMonitoring and optimizing the Data Lake environment:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/building-data-\n\nlakes/monitoring-optimizing-data-lake-environment.html\n\nMaximizing the value of your cloud-enabled enterprise Data Lake:\n\nhttps://aws.amazon.com/blogs/apn/maximizing-the-value-of-your-\n\ncloud-enabled-enterprise-data-lake-by-tracking-critical-metrics/\n\nCost and usage analysis – AWS Well-Architected Labs:\n\nhttps://wellarchitectedlabs.com/\n\nMonitoring workload resources:\n\nhttps://docs.aws.amazon.com/wellarchitected/latest/reliability-\n\npillar/monitor-workload-resources.html\n\nSetting up System Manager to remediate non-compliance with AWS Config rules\n\nIn this recipe, we will learn how to set up AWS System Manager\n\n(SSM) so that we can use this setting to automate non-compliance\n\nremediation in the next chapter. AWS SSM automation simpli\u0000es\n\nmaintenance and deployment tasks for AWS services such as S3.\n\nFor a data engineer, one of the common challenges is ensuring that\n\nall resources adhere to organizational policies and regulatory\n\nrequirements. Due to the length of the recipe, it will be broken into\n\ntwo parts: setting up SSM in this recipe and using this setup in the\n\nUsing AWS con\u0000g to automate non-compliance S3 server access\n\nlogging policy recipe later.\n\nAWS SSM is a comprehensive suite of tools designed to help\n\nautomate and streamline various management tasks across your\n\nAWS infrastructure. It provides a secure, remote management\n\nsolution for con\u0000guring your managed instances, ensuring that they\n\nremain compliant with your organization’s standards and policies.\n\nis automation not only enhances eﬃciency but also reduces the\n\nrisk of manual errors, making it an invaluable tool for maintaining\n\nthe health and security of your AWS environment.\n\nGetting ready\n\nEnsure that you enable AWS Con\u0000g and AWS SSM in your account.\n\nHow to do it…\n\n1. In the AWS IAM console, navigate to Roles and click on Create role.\n\n2. Under Select type of trusted entity, choose AWS service and System\n\nManager.\n\n3. Under Role name, search for the\n\nSSMServiceRoleForAutomation policy on the Attached\n\npermissions policy page.\n\nFigure 9.25 – Selecting the SSM role\n\n4. You need to add S3 permissions that SSM automation would use, such as\n\nthe following policy. You need to tailor it to your organization’s security\n\nrequirements:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [\n\n\"s3:PutEncryptionConfiguration\",\n\n\"s3:PutBucketLogging\", \"s3:GetBucketLogging\" ], \"Resource\": \"arn:aws:s3:::*\" } ] }\n\n5. Go to Trust Relationships and edit the trust entities as shown:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"ssm.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\", \"Condition\": { \"StringEquals\": { \"aws:SourceAccount\": \"YOUR-ACCOUNT-ID\" }, \"ArnLike\": { \"aws:SourceArn\": \"arn:aws:ssm:*:YOUR-ACCOUNT-ID:automation- execution/*\" } } } ] }\n\nSave and proceed to the next recipe for monitoring non-\n\ncompliant AWS Con\u0000g rules.\n\nThere’s more…\n\nAWS SSM Explorer is an operations dashboard presenting\n\ninformation about your resources. is dashboard consolidates\n\noperations data views from various AWS accounts and across\n\ndiﬀerent AWS regions. By doing so, Explorer oﬀers insights into the\n\ndistribution of operational issues, their trends over a period, and\n\ntheir diﬀerentiation by various categories. It aids in understanding\n\nthe overall functional health and potential areas that may require\n\nattention.\n\nSee also\n\nUse IAM to con\u0000gure roles for automation:\n\nhttps://docs.aws.amazon.com/systems-\n\nmanager/latest/userguide/automation-setup-iam.html\n\nUsing AWS config to automate non- compliance S3 server access logging policy\n\nIn the Creating custom dashboards to monitor Data Lake services\n\nrecipe, we learned that the S3 Storage Lens provides a general\n\ndashboard to observe your S3 activities. One of the best practices\n\nfor more comprehensive monitoring and auditing of your bucket is\n\nenabling S3 server access logging. is feature gives you detailed\n\nrecords of the requests made to the buckets, which is helpful in\n\nscenarios wherein you need to detect potential security weaknesses\n\nand incidents. is recipe will teach you to use AWS Con\u0000g and\n\nAWS SSM to enforce this feature.\n\nYou can use the idea in this recipe to create more enforcement not\n\nonly for S3 but also for other resources in your Data Lake.\n\nGetting ready\n\nEnsure that you enable AWS Con\u0000g and AWS SSM in your account.\n\nYou also need to \u0000nish setting up the AWS SSM role as covered in\n\nthe Setting up System Manager to remediate non-compliant AWS\n\nCon\u0000g rules recipe.\n\nBesides that, you need to identify the scope of your AWS Con\u0000g in\n\nthe general settings as shown:\n\nFigure 9.26 – Selecting the recording strategy for AWS conﬁg\n\nHow to do it…\n\n1. Select AWS Con\u0000g service; if this is your \u0000rst time starting with AWS\n\nCon\u0000g, you can select 1-click setup as shown:.\n\nFigure 9.27 – Selecting 1-click setup\n\n2. In Step 2 – Rules, under AWS Managed Rules, select s3-bucket-logging-\n\nenabled and then click on Review and create:\n\nFigure 9.28 – Selecting relevant rule name in AWS Managed Rules\n\nHead to the Rules part in the AWS Con\u0000g console, where you can\n\nsee the buckets that are not compliant under Detective compliance:\n\nFigure 9.29 – The list of buckets that do not align with the rules\n\n3. Select Manage remediation to con\u0000gure automated remediation actions:\n\nFigure 9.30 – Selecting Manage remediation to resolve the non-\n\ncompliance\n\n4. Select Automatic remediation, and in the Remediation action details\n\nsection, choose AWS-Con\u0000gureS3BucketLogging. More options exist,\n\nsuch as Creating a Jira ticket or Publishing an SNS noti\u0000cation. In this\n\nrecipe, we will select AWS-Con\u0000gureS3BucketLogging.\n\nFigure 9.31 – Editing the Remediation action section\n\n5. Choose BucketName under Resource ID parameter:\n\nFigure 9.32 – Selecting BucketName for remediation action\n\n6. In the Parameters \u0000eld, enter the values shown in the following image\n\nand then save the changes:\n\nFigure 9.33 – Parameters for remediation action\n\nHow it works…\n\nAWS Con\u0000g rules can verify speci\u0000c settings within your AWS\n\nenvironment, such as whether Amazon S3 buckets have logging\n\nenabled. ese rules employ AWS Lambda functions to conduct\n\ncompliance assessments, returning either compliant or non-\n\ncompliant statuses for the inspected resources. If a resource is non-\n\ncompliant, it can be corrected through a remediation action linked\n\nto the AWS Con\u0000g rule. e auto-remediation feature of AWS\n\nCon\u0000g rules allows this corrective action to be triggered\n\nautomatically, immediately addressing any detected non-\n\ncompliance.\n\nThere’s more…\n\nWith AWS server access logs enabled, you can use Athena to\n\nanalyze these logs.\n\nIn addition to using prede\u0000ned AWS Con\u0000g rules, you have the\n\n\u0000exibility to create custom AWS Con\u0000g rules to enforce your own\n\ncorporate security policies. ese custom rules are linked to an\n\nAWS Lambda function that you develop and manage. When a\n\ncustom rule is triggered, it executes the associated Lambda\n\nfunction, enabling you to enforce speci\u0000c con\u0000gurations tailored to\n\nyour organization’s needs. Furthermore, AWS Con\u0000g provides real-\n\ntime noti\u0000cations whenever a resource is miscon\u0000gured or violates\n\nthe de\u0000ned security policies, allowing for prompt remediation and\n\nenhancing the security posture of your Data Lake infrastructure.\n\nSee also\n\nSetting up AWS Con\u0000g with the console:\n\nhttps://docs.aws.amazon.com/con\u0000g/latest/developerguide/gs-\n\nconsole.html\n\nRemediating non-compliant AWS Con\u0000g rules with AWS SSM\n\nautomation runbooks: https://aws.amazon.com/blogs/mt/remediate-\n\nnoncompliant-aws-con\u0000g-rules-with-aws-systems-manager-\n\nautomation-runbooks/\n\nAnalyzing Amazon S3 server access logs using Athena:\n\nhttps://repost.aws/knowledge-center/analyze-logs-athena\n\nAmazon S3 server access log format:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html\n\nTracking AWS Data Lake cost per analytics workload\n\nIn an enterprise data lake, multiple teams oen use the data lake to\n\nrun several business campaigns. Tying the cost to the business value\n\nit provides is crucial to assess the return on investment later.\n\nIt is essential to have a cost allocation strategy. ere are multiple\n\nways to build the systems, such as implementing cost allocation\n\ntagging to re\u0000ect business units that utilize the resources as well as\n\nclosely monitoring the cost through set budgets, and building\n\nCloud Intelligence dashboards such as Cost and Usage report and\n\nCost Intelligence Dashboards.\n\nis recipe will look at how to track cost per analytics workload.\n\nFirst, we will look at how to create tags and then at how to bulk-add\n\ntags for multiple resources using Tag Editor. Finally, we will use the\n\ncost categories.\n\nGetting ready\n\nEnsure that you can access AWS billing and cost services and\n\ndiscuss with stakeholders to create a strategy for monitoring costs,\n\nsuch as using a tagging strategy.\n\nHow to do it…\n\n1. First, tag your resources in a key-value pair. e Name tag is oen used\n\nin the AWS GUI to identify resources. Each tag should have a single\n\nvalue. For example, if you want to remember that a particular redshi\n\ncluster is for a marketing test environment, you should tag as shown in\n\nthe following \u0000gure instead of combining the tag such as Name:\n\nmarketing-test.\n\nFigure 9.34 – Tagging an example of a key-value pair\n\n2. Select Resource Groups & Tag Editor in the console.\n\nFigure 9.35 – Selecting the resource group and tag editor in AWS\n\nportal\n\n3. Go to Tag Editor under Tagging and choose the AWS regions that you\n\ndeployed the resources to. In the Tag key, choose department and tag\n\nvalue as marketing in the Tags section. en click on the Search\n\nresources button. In this example, we created a project named\n\ntokenization; thus, we will \u0000lter the resources search box as Tag:\n\nproject: tokenization and hit the Enter key.\n\nFigure 9.36 – The tag editor for resource groups\n\n4. Select all the resources. en click on Manage tags of selected resources.\n\nFigure 9.37 – Managing the tags of selected resources\n\n5. In the Edit tags of all selected resources section as shown next, you can\n\nadd additional tags besides the pre-existing ones. Add the\n\ncost_center tag and then click on Review and apply tag changes to\n\nbulk-apply this to the resources that you previously tagged for the\n\nproject tokenization.",
      "page_number": 582
    },
    {
      "number": 30,
      "title": "Segment 30 (pages 600-622)",
      "start_page": 600,
      "end_page": 622,
      "detection_method": "synthetic",
      "content": "Figure 9.38 – Applying changes after editing tags\n\n6. In the following steps, we will start creating cost categories. First, you\n\nneed to go to Cost allocation tags and activate the tags that you want to\n\ncategorize such as the name, department, or cost_center tags,\n\nwhich we created in the previous steps. Click on the tag you want to\n\nactivate, then click on the Activate button. Please note that sometimes\n\ntags take time to show up in Cost allocation tags.\n\nFigure 9.39 – Creating cost allocation tags\n\n7. Go to the cost category. In the Cost category details section, give a name\n\nfor your categorization such as Cost per environment to re\u0000ect\n\ncategorization per environment usage.\n\nFigure 9.40 – Naming the cost category\n\n8. Select the lookback period to apply the rules to the previous date.\n\nFigure 9.41 – Selecting the lookback period\n\n9. On the De\u0000ne category rules page, select Inherited value, then choose\n\nCost Allocation Tag as Dimension and Name as Tag key.\n\nFigure 9.42 – Deﬁning the category rules\n\n10. Click on Create cost category and wait for the category to \u0000nish\n\nprocessing. e longer of a lookback period it has, the longer it will take\n\nto process the category.\n\nFigure 9.43 – Creating a cost category\n\nHow it works…\n\nCost allocation tags allow you to categorize your AWS resources as\n\nEC2 instances, EMR clusters, Glue jobs, and so on based on the\n\nworkload they are processing. In addition to that, you can de\u0000ne a\n\ntag per BusinessUnit, Workload, and Department, and then use\n\nAWS Tag Editor or SDK/CLI to apply these tags to relevant\n\nresources in your Data Lake.\n\nThere’s more…\n\nOrganizing your costs with cost categories is a foundational step for\n\nyour organization to visualize these categories and apply split\n\ncharges. You can further create cost visualizations, such as building\n\na QuickSight dashboard or using cost explorer in Cost\n\nManagement. To use cost explorer for dashboard visualization,\n\nsimply use the Cost category option in the Report parameters.\n\nMake sure to save it to the report library for quick access later.\n\nSplit charges let you divide a single cost category’s expenses across\n\nmultiple target values, either proportionally, by \u0000xed percentages, or\n\nas an even split. For example, you might have a general Project cost\n\ncategory. Still, you can apply split charges to allocate speci\u0000c\n\nportions of this category to multiple teams based on their actual\n\nresource utilization or business objectives.\n\nSee also\n\nUser-de\u0000ned tag restrictions:\n\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/allocatio\n\nn-tag-restrictions.html\n\nCost modeling Data Lakes for beginners:\n\nhttps://d1.awsstatic.com/whitepapers/cost-modeling-data-lakes.pdf\n\nBest practices for tagging AWS resources:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/tagging-best-\n\npractices/tagging-best-practices.html\n\nOceanofPDF.com\n\n10 Building a Serving Layer with AWS Analytics Services\n\nIn this chapter, we will explore how to manage a serving layer with\n\nAmazon Redshi and QuickSight. e consumption layer focuses\n\non building solutions for data users to access, deriving data insights,\n\nand building dashboards to demonstrate the insights. Analysts must\n\nquery this data quickly and eﬃciently to generate insights and\n\nreports. However, due to the sheer volume and complexity of the\n\ndata, traditional querying methods are slow and cumbersome.\n\nus, implementing a robust consumption layer can address this\n\nchallenge by enabling fast, eﬃcient access and querying capabilities,\n\nthus empowering your analytics team to derive actionable insights\n\nwithout delay.\n\nis chapter will walk you through the \u0000rst step of managing a\n\nserving layer, from loading the data to Redshi, connecting client\n\napplications using a VPC endpoint, querying using Redshi\n\nServerless, and using AWS SDK to manage QuickSight.\n\ne recipes in this chapter are as follows:\n\nUsing Redshi workload management (WLM) to manage workload\n\npriority\n\nQuerying large historical data with Redshi Spectrum\n\nCreating a VPC endpoint to a Redshi cluster\n\nAccessing a Redshi cluster using JDBC to query data\n\nUsing AWS SDK for pandas, the Redshi Data API, and Lambda to\n\nexecute SQL statements\n\nUsing the AWS SDK for Python to manage Amazon QuickSight\n\nTechnical requirements\n\nBefore going ahead with the recipes in this chapter, it would be\n\nuseful to have an understanding of the data lake architecture and a\n\nbasic knowledge of how data warehouses and data ingestion using\n\nGlue work.\n\ne code \u0000les for this chapter are available on GitHub:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter10.\n\nUsing Redshift workload management (WLM) to manage workload priority\n\nAmazon Redshi’s WLM feature is designed to help manage and\n\nprioritize queries and other database operations. By enabling and\n\ncon\u0000guring WLM, users can ensure critical queries receive the\n\nnecessary resources without being stalled by other less urgent\n\nprocesses. is recipe will guide you through setting up WLM,\n\nincluding the con\u0000guration of automatic WLM, to manage\n\nworkloads in your Redshi environment eﬃciently.\n\nGetting ready\n\nBefore con\u0000guring WLM in Amazon Redshi, you must have\n\nadministrative access to your Redshi cluster and the AWS\n\nManagement Console. Ensure your Redshi cluster is operational\n\nand you have familiarized yourself with the basic concepts of how\n\nRedshi handles queries and operations.\n\nHow to do it…\n\n1. Log in to the Redshi management console.\n\n2. Navigate to your cluster, click on Con\u0000gurations as shown, and click on\n\nWorkload management:\n\nFigure 10.1 – Clicking on Workload management on the\n\nconﬁguration\n\n3. On the Workload management screen, you can see the default parameter\n\ngroup that comes with the cluster creation. In this step, we will create a\n\ncustomized parameter group to help us:\n\nFigure 10.2 – Clicking on Create in the Parameter groups section\n\n4. In the pop-up menu, \u0000ll in the Parameter group name and Description\n\n\u0000elds:\n\nFigure 10.3 – Creating a parameter group\n\n5. Aer you create the new WLM feature, there will be a new parameter\n\ngroup. On the right-hand side, you have two options: either using\n\nautomatic WLM or creating workload queues. e default, Automatic\n\nWLM, is enabled. Click on Switch WLM mode to change the mode to\n\nManual WLM:\n\nFigure 10.4 – Switching WLM to manual WLM\n\n6. By switching to manual WLM, you will have full control of memory and\n\nconcurrency for your queues:\n\nFigure 10.5 – Switching from automatic WLM to manual WLM\n\n7. In the Modify workload queues screen, you can create or modify\n\nexisting queues to de\u0000ne workload priorities and resource allocation. On\n\nthis page, you can also set parameters to monitor each queue, such as\n\nmemory allocation, concurrency, and timeout settings. e following\n\nscreenshot is an example of a queue where you can con\u0000gure memory\n\nand concurrency, create a monitoring rule, and use wildcards for the\n\nquery group:\n\nFigure 10.6 – Conﬁguring workload queues\n\nHow it works…\n\nAmazon Redshi WLM is designed to eﬃciently handle multiple\n\nworkloads by ensuring that short, fast-running queries do not get\n\ndelayed by longer, resource-intensive queries. By prioritizing\n\nqueries, Redshi WLM helps maintain optimal performance for\n\ncritical workloads and user groups.\n\nSome workloads or user groups may require higher performance\n\nthan others. Redshi allows you to set priorities for diﬀerent\n\nworkloads using a system of queues. Each queue can be assigned a\n\npriority level, which is then inherited by all queries associated with\n\nthat queue. e priority levels, from highest to lowest, are the\n\nfollowing:\n\nHIGHEST\n\nHIGH\n\nNORMAL\n\nLOW\n\nLOWEST\n\nTo manage the prioritization of queries, you can map user groups\n\nand query groups to speci\u0000c queues. is ensures that queries from\n\nimportant users or critical workloads are processed with the\n\nappropriate priority. For instance, you might assign executive\n\nreports to a queue with the highest priority, while routine data loads\n\ncould be assigned to a low-priority queue. Redshi allows up to 50\n\nconcurrent queries, so your concurrent queue needs to be a\n\nsummary of 50. For an example JSON template for WLM con\u0000g,\n\nplease access the following GitHub repo:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/blob/main/Chapter10/Recipe1.\n\nThere’s more…\n\nImplementing eﬀective WLM in Redshi goes beyond the basic\n\ncon\u0000guration. Here are additional tips and advanced techniques:\n\nUse short query acceleration (SQA): SQA is a feature in Amazon\n\nRedshi that prioritizes short-running queries over longer-running\n\nqueries. is can help improve query performance and reduce query\n\nwait times. e default SQA maximum runtime is dynamic but can be\n\nmanually set to a \u0000xed value between 1-20 seconds. SQA is set\n\nautomatically in automatic WLM to reduce overall query latency or\n\nthrough clicking on the Enable short query acceleration for queries\n\nwhose maximum runtime is checkbox to a value of your choice:\n\nFigure 10.7 – Enabling SQA\n\nCustom WLM rules: Create custom rules to manage speci\u0000c scenarios,\n\nsuch as automatically aborting long-running queries or reallocating\n\nresources during peak times. You can further monitor query\n\nperformance and resource utilization using system tables such as\n\nSTL_WLM_QUERY, STL_QUERY, and STV_WLM_QUERY_STATE.\n\nFor this recipe, it might be helpful to know queries to get users grouped\n\ntogether. You can use the pg_group table from the system catalog\n\ntable:\n\nSelect * from pg_group where groname='reporting_users'\n\nSee also\n\nCon\u0000guring the WLM parameter using the AWS CLI:\n\nhttps://docs.aws.amazon.com/redshi/latest/mgmt/workload-mgmt-\n\ncon\u0000g.html#Con\u0000guring-the-wlm-json-con\u0000guration-Parameter\n\nWLM dynamic and static con\u0000guration properties:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/cm-c-wlm-dynamic-\n\nproperties.html\n\nWLM queue assignment rules:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/cm-c-wlm-queue-\n\nassignment-rules.html\n\nQuerying large historical data with Redshift Spectrum\n\nAmazon Redshi Spectrum is a feature of Amazon Redshi that\n\nallows you to query exabytes of data stored in Amazon S3 directly\n\nwithout prior loading to Redshi tables. is can be useful for\n\nvarious reasons, such as querying historical datasets that have\n\nexpanded to multiple years or having multiple Redshi workgroups\n\nto query the same dataset. You can directly query scalar data or\n\nnested data formats stored in Amazon S3.\n\nBy using Redshi Spectrum, you will launch a cluster that is\n\nindependent of your existing cluster.\n\nGetting ready\n\nTo use Amazon Redshi Spectrum, you need to have the following:\n\nAn SQL client: is cluster is independent of your existing Redshi\n\ncluster.\n\nAt least three subnets: Each subnet should be associated with diﬀerent\n\nAvailability Zones (AZs) for your Redshi Serverless workspace. Make\n\nsure you understand how the subnet mask and subnet Classless Inter-\n\nDomain Routing (CIDR) block work prior to creating these subnets.\n\nFundamental knowledge of networking is required. Please see this link\n\nfor more information:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/subnet-sizing.html.\n\nS3 bucket: e data that you want to query from the S3 bucket must be\n\nin the same region as your Amazon Redshi cluster.\n\nAn Identity and Access Management (IAM) role for Redshi: To create\n\nan IAM role for Redshi, follow the instructions at\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/c-spectrum-iam-\n\npolicies.html.\n\nIn this recipe, we will use a default-namespace namespace, so\n\nmake sure your default-namespace namespace has the relevant\n\nIAM roles attached. You can check for the same under Security and\n\nencryption in your default-namespace namespace:\n\nFigure 10.8 – Creating an IAM role for Redshift\n\nHow to do it…\n\n1. Select Redshi Serverless from the Amazon Redshi console.\n\n2. Select Create workgroup and follow the next page to \u0000ll in the\n\nworkgroup name, capacity, network and security, and namespace. In\n\nNetwork and security, choose the subnets that you created for Redshi\n\nSpectrum. For the capacity, as this is a test recipe, make sure you only\n\nuse the smallest Redshi Processing Unit (RPU) value, which is 8 RPUs:\n\nFigure 10.9 – Selecting Create workgroup\n\n3. When you go to the Redshi console, there is a default namespace\n\navailable. Make sure your default-namespace namespace already\n\nhas the relevant permission to access Glue and S3, as mentioned in the\n\nGetting ready section of this recipe:\n\nFigure 10.10 – Choosing a relevant namespace\n\n4. Once your Amazon Redshi Spectrum namespace is created, click on\n\nQuery editor v2 as shown:\n\nFigure 10.11 – Selecting Query editor v2\n\n5. When you \u0000rst start using the query editor, you can choose Federated\n\nuser to start with. Once you select it, you will see the data that exists in\n\nyour Glue data catalog:\n\nFigure 10.12 – Selecting Federated user\n\n6. e next step is to create an external schema and external tables. You can\n\nchoose from various external databases, such as an Amazon Athena\n\nData Catalog, an AWS Glue Data Catalog, or an Apache Hive Metastore:\n\ncreate external schema spectrum_data from data catalog database 'sample_db' iam_role 'arn:aws:iam::xxxxxxxx:role/xxxxx' create external table if not exists;\n\n7. Once you have created an external schema, you can create an external\n\ntable using the following SQL statement. e LOCATION clause\n\nspeci\u0000es the location of the data in Amazon S3. e bucket_name\n\nparameter is the name of the S3 bucket where the data is stored:\n\nCREATE EXTERNAL TABLE\n\nspectrum_data.your_table_name ( first_and_last_name VARCHAR, email VARCHAR, id BIGINT, id_2 BIGINT, gender VARCHAR, country VARCHAR, fax VARCHAR ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION 's3://sample-test-wf09/' TABLE PROPERTIES ('skip.header.line.count'='1');LOCATION 's3://bucket_name/prefix/';\n\n8. Once you have created the external table, you can use standard SQL\n\nqueries to query the data in Amazon S3. For example, the following SQL\n\nstatement will query the top three rows in the table you just created:\n\nselect top 3 * from spectrum_data.your_table_name\n\nHow it works…\n\nAmazon Redshi Spectrum is a feature of Amazon Redshi that\n\nallows you to run SQL queries on data stored directly in Amazon\n\nS3. Redshi Spectrum supports the same data format as Athena.\n\ne process begins with creating an external schema and an\n\nexternal table to reference the data stored in Amazon S3. Once the\n\nexternal table is set up, you can run SQL queries on this data\n\ndirectly from Amazon Redshi. For more information on the\n\nCREATE EXTERNAL TABLE statement command, refer to the\n\nCreating external tables for Redshi Spectrum blog\n\n(https://docs.aws.amazon.com/redshi/latest/dg/c-spectrum-\n\nexternal-tables.html). Once you create an external table, you can\n\nuse SQL to query data similar to how you would use Athena.\n\nTo summarize, the following table compares Redshi Spectrum,\n\nAmazon Athena, and S3 Select:\n\nCOMPARISON\n\nAWS\n\nS3 SELECT\n\nAMAZON\n\nATHENA\n\nREDSHIFT\n\nSPECTRUM\n\nMain purpose\n\nServerless\n\nBuilt-in S3\n\nServerless\n\nSQL\n\nfeature for\n\nSQL querying\n\nquerying\n\nsimple\n\nof S3 data\n\nof S3 data\n\nqueries\n\nAllowed queries\n\nSQL, based\n\nA limited\n\nSQL, based\n\non Presto\n\nsubset of\n\non\n\nSQL\n\nPostgreSQL\n\ncommands.\n\nS3 select has\n\na maximum",
      "page_number": 600
    },
    {
      "number": 31,
      "title": "Segment 31 (pages 623-644)",
      "start_page": 623,
      "end_page": 644,
      "detection_method": "synthetic",
      "content": "COMPARISON\n\nAWS\n\nATHENA\n\nManaged/serverless Serverless\n\nTable con\u0000g\n\nVirtual\n\ntables (for\n\nexample,\n\nusing AWS\n\nGlue)\n\nUse case\n\nAd hoc\n\nquerying;\n\ninteractive\n\nS3 SELECT\n\nlength of 256\n\nKB for SQL\n\nexpressions.\n\nServerless\n\nN/A\n\nRetrieving a\n\nsubset of\n\ndata from a\n\nAMAZON\n\nREDSHIFT\n\nSPECTRUM\n\nManaged\n\nservice\n\nrequires\n\nAmazon\n\nRedshi\n\ncluster\n\nVirtual tables,\n\nmanually\n\ncon\u0000gured\n\nwith external\n\ntables\n\nEnterprise\n\nreporting;\n\nbusiness\n\nCOMPARISON\n\nAWS\n\nS3 SELECT\n\nAMAZON\n\nATHENA\n\nREDSHIFT\n\nSPECTRUM\n\nanalytics;\n\nsingle object;\n\nintelligence\n\nexploring\n\nsimple\n\n(BI); complex\n\ndata in S3\n\nquerying of\n\nanalytical use\n\nan S3 object\n\ncases from\n\napps that\n\nrequire a low\n\nlatency\n\nTable 10.1 – Comparison between Athena, Redshift Spectrum,\n\nand S3 Select\n\nThere’s more…\n\nAmazon Redshi Spectrum also supports federated query access,\n\nwhich allows you to query data stored in Amazon Aurora directly\n\nfrom the Amazon Redshi query editor. To use federated query\n\naccess, follow these steps:\n\n1. You need to create a database user in Amazon Redshi that has the\n\nSUPERUSER role. You then need to create a database user in Amazon\n\nAurora that has the same name and password as the database user in\n\nAmazon Redshi. e LOCATION clause speci\u0000es the location of the\n\ndata in Amazon Aurora. e HOST and PORT clauses specify the\n\nhostname and port of the Amazon Aurora database. e DATABASE,\n\nSCHEMA, and TABLE clauses specify the name of the database, schema,\n\nand table, respectively, in Amazon Aurora. Once you have created the\n\ndatabase users, you can use the following SQL statement to create a\n\nfederated table in Amazon Redshi:\n\nCREATE TABLE table_name ( column_name1 data_type1, column_name2 data_type2, ... ) LOCATION ( TYPE = aurora HOST = 'aurora_host_name' PORT = 'aurora_port' DATABASE = 'aurora_database_name' SCHEMA = 'aurora_schema_name' TABLE = 'aurora_table_name' )\n\n2. Once you have created the federated table, you can use standard SQL\n\nqueries to query the data in Amazon Aurora directly from the Amazon\n\nRedshi query editor. For example, the following SQL statement will\n\nquery the table_name table and return the column_name1 and\n\ncolumn_name2 columns:\n\nSELECT column_name1, column_name2 FROM table_name;\n\n3. To estimate the cost of a query executed using Redshi Spectrum, you\n\ncan use the SVL_S3QUERY_SUMMARY system view. e\n\ns3_scanned_bytes column is the number of bytes scanned; it\n\ndivides it by 1024^4 to convert it to bytes. Multiply by 2.5 to account\n\nfor the cost per GB of data scanned in Amazon Redshi Spectrum:\n\nSELECT s3_scanned_bytes / 1024^4 * 2.5 AS spectrum_cost FROM SVL_S3QUERY_SUMMARY WHERE query = <your_query_id>;\n\n4. e STL_ALERT_EVENT_LOG table monitors performance thresholds\n\nthat are exceeded. You can join STL_ALERT_EVENT_LOG with\n\nSTL_QUERY to get detailed information about queries that trigger\n\nperformance alerts. e following SQL example demonstrates how to\n\n\u0000lter the results based on a speci\u0000c job run ID (to be replaced from your\n\nsystem):\n\nSELECT A.*, B.* FROM STL_ALERT_EVENT_LOG A LEFT JOIN STL_QUERY B ON A.QUERY = B.QUERY WHERE A.PID IN (SELECT PID FROM STL_QUERY WHERE QUERYTXT LIKE '%<job_run_id>%' GROUP BY 1 );\n\n5. To further optimize the performance of Amazon Redshi Spectrum\n\nqueries, it’s essential to pre-\u0000lter the data before joining it with other\n\ntables and ensure that \u0000ltering is done on partition keys. is practice\n\nhelps in reducing the data scanned. Consider the following example\n\nwhere a temporary table is created to pre-\u0000lter the data:\n\nI. First, create a temporary table to pre-\u0000lter data from an external\n\nsource:\n\nCREATE TEMP TABLE TEMP_HITS_A DISTKEY (MARKER_ID) COMPOUND SORTKEY (MARKER_ID) AS (SELECT MARKER_ID, COUNT(1) AS TOTAL_HITS FROM EXTERNAL_SCHEMA.SOURCE_TABLE_A WHERE ENTITY_ID = 101 AND MARKET_ID = 1 AND EVENT_DAY AND IS_EVENT = 1 GROUP BY MARKER_ID);\n\nII. en, create another temporary table to pre-\u0000lter data from a\n\ndiﬀerent source:\n\nCREATE TEMP TABLE TEMP_HITS_B DISTKEY (MARKER_ID) COMPOUND SORTKEY (MARKER_ID) AS SELECT MARKER_ID, COUNT(1) AS TOTAL_HITS FROM DATA_WAREHOUSE.SOURCE_TABLE_B WHERE REGION_ID = 1 AND MARKET_ID = 1 AND EVENT_DAY AND IS_EVENT = 2024-04- 20'::DATE GROUP BY MARKER_ID;\n\nIII. Lastly, join the pre-\u0000ltered temporary tables and calculate the\n\ndiﬀerence:\n\nSELECT A.MARKER_ID, A.TOTAL_HITS AS HITS_A, B.TOTAL_HITS AS HITS_B, A.TOTAL_HITS - B.TOTAL_HITS AS HIT_DIFFERENCE FROM TEMP_HITS_A A LEFT OUTER JOIN TEMP_HITS_B B ON A.MARKER_ID = B.MARKER_ID;\n\nSee also\n\nBest Practices for Amazon Redshi Spectrum | AWS Big Data Blog:\n\nhttps://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-\n\nredshi-spectrum/\n\nHow do I calculate the query charges in Amazon Redshi Spectrum?\n\nhttps://repost.aws/knowledge-center/redshi-spectrum-query-charges\n\nSVL_S3QUERY_SUMMARY:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/r_SVL_S3QUERY_SU\n\nMMARY.html\n\nMonitoring queries and workloads with Amazon Redshi Serverless:\n\nhttps://docs.aws.amazon.com/redshi/latest/mgmt/serverless-\n\nmonitoring.html\n\nImproving Amazon Redshi Spectrum query performance:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/c-spectrum-external-\n\nperformance.html\n\nIssues of overlapping CIDR ranges for subnets:\n\nhttps://repost.aws/questions/QUjrakcRqgRTGwD4pD0K8v8Q/overlappi\n\nng-cidr-ranges-for-subnets\n\nCreating a VPC endpoint to a Redshift cluster\n\nis recipe will guide you in creating a Redshi VPC endpoint to\n\nensure secure, private connectivity between your Redshi cluster\n\nand client applications within your VPC. A VPC is an isolated\n\nvirtual network within the cloud that provides private access to\n\nyour resources. By creating this endpoint, you enable direct, secure\n\naccess to your Redshi cluster from BI tools such as Tableau\n\nOnline, Qlik Sense, and Looker without exposing your data cluster\n\nto a public IP address or routing traﬃc through the internet.\n\nere are two types of VPC endpoints:\n\nInterface endpoint\n\nGateway endpoint\n\nIn this recipe, we will demo two concepts:\n\nShow you step-by-step how to create an S3 gateway endpoint so that the\n\nRedshi cluster can communicate with S3. You can use it later when you\n\ncreate a Glue connection.\n\nCreate a Redshi-managed VPC endpoint so that you can further\n\nintegrate your BI application. is scenario would be useful when you\n\nhave your Redshi cluster in a VPC that is diﬀerent from your BI\n\napplications.\n\nIt’s important to note that Redshi-managed VPC endpoints are\n\nonly available for Redshi clusters that are running on the RA3\n\nnode type and have either cluster relocation or Multi-AZ enabled.\n\nAdditionally, the cluster or workgroup must be available within the\n\nvalid port ranges, and the VPC endpoint quota limits the number of\n\nendpoints.\n\nIn the code, AWS Systems Manager (SSM) will be used to help\n\nautomate operational tasks across your AWS resources. e\n\nParameter Store capability within SSM is useful for storing sensitive\n\ndata such as database connection details, credentials, and\n\ncon\u0000guration settings in a secure and structured manner, with\n\noptional encryption using AWS Key Management Service (KMS).\n\nGetting ready\n\nBefore starting, ensure you have the following prerequisites:\n\nAn active AWS account with an IAM role that has permission to manage\n\nRedshi, VPC, Glue, and SSM\n\nKnowledge of SSM\n\nAn active Redshi cluster\n\nHow to do it…\n\n1. In the Glue service console, select Data connections and then Create\n\nconnection:\n\nFigure 10.13 – Selecting Data connections and creating a\n\nconnection\n\n2. is step assumes that you have an existing Redshi cluster that you\n\ncreated in the previous recipe. Select Amazon Redshi as the data\n\nsource, then follow the \u0000ow of Step 2, Step 3, and Step 4, as highlighted\n\non the le, to \u0000nish the setup:\n\nFigure 10.14 – Selecting Amazon Redshift as a data source\n\n3. In Step 2, you need to select the relevant database instances, database\n\nname, and credential type. e best practice is to use AWS Secrets\n\nManager for storing credentials. If you con\u0000gure your Redshi cluster\n\nusing username and password, you can head to the Redshi console,\n\nselect Namespace con\u0000guration, and then Edit admin credentials to\n\nchange the password:\n\nFigure 10.15 – Heading to Namespace conﬁguration to ﬁnd out\n\nyour admin credentials or to change your password\n\n4. In Step 3, set properties; if you want to reuse your connection later on in\n\nthe Jupyter notebook, you should create a name without a space. For this\n\nrecipe, we will use a connection name of redshift_serverless\n\ninstead of the default name. Later on, if you want to reuse the\n\nconnection in the Glue session, you should use the %connections\n\nredshift_serverless magic cell:\n\nFigure 10.16 – Editing the connection name\n\n5. In Step 4, when you click on Create connection, you will see a Redshi\n\nconnection successfully created message. However, at this step, your\n\nconnection won’t work. You can \u0000nd out why by clicking on Test\n\nconnection:\n\nFigure 10.17 – Testing connection of the Redshift connection\n\ncreated\n\n6. To test the connection, you need to select the relevant IAM role. You will\n\nreceive an InvalidInputException: VPC S3 endpoint validation failed\n\nerror message:\n\nFigure 10.18 – Error message prior to creating VPC endpoint\n\n7. Head to the VPC service, click on Endpoints, and then click on Create\n\nendpoint.\n\n8. In Endpoint settings, select AWS services, and for the service name,\n\nselect com.amazonaws.us-east-1.s3. Make sure you select the\n\none marked as the Gateway type:\n\nFigure 10.19 – Selecting an S3 gateway in Services\n\n9. Select the VPC that you want to create the endpoint and click on Create.\n\nOnce you create the endpoint, you can go back and test the connection,\n\nand you will see that aer creating the endpoint, your test connection\n\nwill be successfully connected:\n\nFigure 10.20 – Successfully created Redshift connection\n\nHow it works…\n\nCreating connections to Amazon Redshi from AWS Glue requires\n\nan Amazon S3 VPC gateway endpoint. is is to ensure secure and\n\nprivate communication between the AWS Glue job and the Redshi\n\ncluster without exposing the data to the public internet.\n\nTo set up a VPC endpoint for Amazon S3, you would need to do the\n\nfollowing:\n\n1. Create a VPC endpoint for the S3 service in the same VPC as your\n\nRedshi cluster.\n\n2. When you create the VPC endpoint, you can customize the level of\n\naccess by con\u0000guring the route tables to allow the AWS Glue job to\n\naccess the S3 bucket through the VPC endpoint.\n\nThere’s more…\n\nFor secure, reliable connectivity, you can further integrate an AWS\n\nnetwork \u0000rewall, an AWS shield, and a Network Load Balancer\n\n(NLB) instance to enhance security and performance. Additionally,\n\nyou can employ the AWS Cloud Development Kit (AWS CDK) to\n\nde\u0000ne cloud infrastructure as code (IaC).\n\nAn NLB is not required with a Redshi VPC endpoint, but it can be\n\nused to distribute incoming traﬃc to multiple Redshi clusters or\n\nworkgroups. If you have multiple Redshi clusters or workgroups\n\nthat need to be accessed by the same client tool, you can use an NLB\n\nto distribute incoming traﬃc to diﬀerent Redshi endpoints. is\n\ncan ensure that traﬃc is distributed evenly across the diﬀerent\n\nRedshi clusters or workgroups. To set up an NLB with a Redshi\n\nVPC endpoint, you need to create a load balancer and con\u0000gure it\n\nto distribute traﬃc to diﬀerent Redshi endpoints. You will also\n\nneed to con\u0000gure security groups for the Redshi clusters or\n\nworkgroups to allow traﬃc from the load balancer. It’s important to\n\nnote that the NLB will need to be in the same VPC as the Redshi\n\nclusters or workgroups, and the security groups for the Redshi\n\nclusters or workgroups will need to be con\u0000gured to allow traﬃc\n\nfrom the load balancer.\n\ne script that automates the creation of a Redshi VPC endpoint,\n\nenabling secure, private connectivity for your Redshi cluster\n\nwithin the VPC, can be found in the following GitHub repo:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/blob/main/Chapter10/Recipe3.\n\ne steps involve the following:\n\n1. Initializing Boto3 clients and de\u0000ning environment variables.\n\n2. Fetching necessary parameters from AWS SSM.\n\n3. Checking if the VPC endpoint exists or if the cluster type is RA3.\n\n4. Creating a managed VPC endpoint if one doesn’t already exist.\n\ne script uses create_endpoint_access() to create a VPC\n\nendpoint with the provided cluster ID, subnet, and security group.\n\nIt will wait for 400 seconds aer initiating the creation process to\n\nallow the endpoint to be fully set up, then use an API call in\n\ndescribe_endpoint_access() to get the newly created\n\nendpoint details. e VPC endpoint details are then stored back in\n\nSSM for future reference.\n\nSee also\n\nRedshi-managed VPC endpoints:\n\nhttps://docs.aws.amazon.com/redshi/latest/mgmt/managing-cluster-\n\ncross-vpc.html\n\nGateway endpoints for Amazon S3:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-\n\ns3.html\n\nDiﬀerent types of VPC endpoints: https://tutorialsdojo.com/vpc-\n\ninterface-endpoint-vs-gateway-endpoint-in-aws/\n\nEnhance data security and governance for Amazon Redshi Spectrum\n\nwith VPC endpoints: https://aws.amazon.com/blogs/big-data/enhance-\n\ndata-security-and-governance-for-amazon-redshi-spectrum-with-vpc-\n\nendpoints/\n\nAccessing a Redshift cluster using JDBC to query data\n\nRedshi has many connection options depending on the use case\n\nand downstream requirements. In this recipe, we will focus on how\n\nto connect Glue to Redshi using JDBC. e main bene\u0000ts of using\n\na JDBC connection are the following:\n\nYou can connect to Redshi using a wide variety of tools\n\nDepending on your use case, you can access Redshi data within your\n\nETL job without going through the Glue crawler, which will help you\n\nsave on cost\n\nGetting ready\n\nTo connect with Amazon Redshi through JDBC, you’ll need the\n\nfollowing:\n\nAWS Glue notebook: Make sure you know how to use Glue notebooks.\n\nRedshi cluster: You should have a working Redshi cluster with the\n\nnecessary data. If you don’t have one, create it before proceeding.\n\nAccess credentials: You should have access credentials to your cluster.\n\nCon\u0000gure security group and VPC endpoint: You should complete the\n\nsetup of relevant security rules to allow inbound and outbound traﬃc.\n\nPlease follow the steps mentioned at\n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-\n\nconnect-redshi-home.html. You also need to create a VPC endpoint to\n\nconnect your application. Please see the Creating a VPC endpoint to a\n\nRedshi cluster recipe to create a managed VPC endpoint or reuse the\n\nredshift_serverless connection using the %connections\n\nsyntax. For more information, please see\n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-\n\nconnect-redshi-home.html.\n\nSample data: Have sample data to query. For this recipe, we will use a\n\nusers table in Redshi, as shown:\n\nCREATE TABLE public.users ( user_id integer identity(1, 1) ENCODE az64, first_name character varying(50) ENCODE lzo, last_name character varying(50) ENCODE\n\nlzo, email character varying(100) ENCODE lzo, created_at timestamp without time zone DEFAULT ('now':: text):: timestamp with time zone ENCODE az64 ) DISTSTYLE AUTO;\n\nHow to do it…\n\n1. Open the AWS Management Console and type Redshift in the search\n\nbar at the top. Click on the result under Services to access the Amazon\n\nRedshi management console:\n\nFigure 10.21 – Selecting Amazon Redshift\n\n2. On the Redshi page, click on Provisioned clusters dashboard and then\n\nclick on the cluster you would like to connect:\n\nFigure 10.22 – Selecting Amazon Redshift cluster to connect\n\n3. On the right side, you will see options to connect to Redshi, such as\n\nEndpoint, JDBC URL, and ODBC URL:\n\nFigure 10.23 – Copying the JDBC URL\n\nDepending on your application usage, you could consider a JDBC\n\nor Open Database Connectivity (ODBC) endpoint. e following\n\ncode is an example of how to use Glue to connect to the Redshi\n\ncluster using the JDBC URL. us, for this recipe, you will need the\n\nJDBC URL as shown:\n\nFigure 10.24 – Noting down Redshift JDBC URL\n\n4. Next, we need to enter the code in the Glue crawler.\n\nI. We will start with installing the necessary libraries:\n\nfrom awsglue.context import GlueContext from awsglue.dynamicframe import DynamicFrame, DynamicFrameWriter from awsglue.job import Job\n\nfrom awsglue.transforms import * from awsglue.utils import getResolvedOptions from awsglue.context import GlueContext from awsglue.dynamicframe import DynamicFrame from pyspark.context import SparkContext from pyspark.sql import * import sys import boto3\n\nII. en, we create a Spark session:\n\nsc = SparkContext.getOrCreate() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext)\n\nIII. Set up your Redshi credentials:\n\nredshift_properties = { \"user\": \"your_username\", \"password\": \"your_password\", \"driver\": \"com.amazon.redshift.jdbc.Driver\" } # Define Redshift connection URL redshift_url = \"\"\n\nIV. Read the Redshi table into a Spark DataFrame:\n\ndf = spark.read.jdbc(url=redshift_url, table=\"your_table\", properties=redshift_properties)\n\nV. Create a temporary view for running SQL queries:\n\ndf.createOrReplaceTempView(\"users\") spark.sql(\"\"\" INSERT INTO public.users (first_name,",
      "page_number": 623
    },
    {
      "number": 32,
      "title": "Segment 32 (pages 645-663)",
      "start_page": 645,
      "end_page": 663,
      "detection_method": "synthetic",
      "content": "last_name, email) SELECT 'John' AS first_name, 'Doe' AS last_name, 'john.doe@example.com' AS email UNION ALL SELECT 'Jane', 'Smith', 'jane.smith@example.com' UNION ALL SELECT 'Bob', 'Johnson', 'bob.johnson@example.com' \"\"\")\n\nVI. Write it back to Redshi:\n\ndf.write.jdbc(url=redshift_url, table=\"your_data\", mode=\"overwrite\", properties=redshift_properties) \"\"\")\n\nThere’s more…\n\nYou need to analyze requirements clearly to understand whether\n\nyou need a Redshi cluster or if Redshi Spectrum would be\n\nsuﬃcient. We will learn how to use Redshi Spectrum and the pros\n\nand cons of using Redshi Spectrum in the next chapter.\n\nAmazon Redshi provides several system tables that can be used to\n\nmonitor and troubleshoot the performance of your cluster. ese\n\ntables are divided into six categories: STL, STV, SVV, SYS, SVCS,\n\nand SVL. When you migrate to Redshi Serverless, some SVV\n\nviews do not directly apply because they are designed to work with\n\nthe architecture and management features speci\u0000c to provisioned\n\nclusters. Instead, Redshi Serverless uses SYS system views.\n\nFor this recipe, we have created a Glue notebook on GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/blob/main/Chapter10/Recipe4.ipynb.\n\nSee also\n\nTop 10 performance tuning techniques for Amazon Redshi:\n\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-\n\ntechniques-for-amazon-redshi/\n\nBest practices to optimize your Amazon Redshi and MicroStrategy\n\ndeployment: https://aws.amazon.com/blogs/big-data/best-practices-to-\n\noptimize-your-amazon-redshi-and-microstrategy-deployment/\n\nHow to retain system tables’ data spanning multiple Amazon Redshi\n\nclusters and run cross-cluster diagnostic queries:\n\nhttps://aws.amazon.com/blogs/big-data/how-to-retain-system-tables-\n\ndata-spanning-multiple-amazon-redshi-clusters-and-run-cross-\n\ncluster-diagnostic-queries/\n\nTypes of system tables and views:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/cm_chap_system-\n\ntables.html#c_types-of-system-tables-and-views\n\nHow can I troubleshoot high or full disk usage with Amazon Redshi?:\n\nhttps://repost.aws/knowledge-center/redshi-high-disk-usage\n\nUsing AWS SDK for pandas, the Redshift Data API, and Lambda to execute SQL statements\n\ne AWS SDK for pandas library (previously known as AWS Data\n\nWrangler) is a powerful tool that provides a pandas interface to\n\nvarious AWS services, including Glue, Redshi, Athena, and more\n\nusing pandas syntax. is library is handy for data scientists, and\n\nanalysts already using the pandas library and want to interact with\n\nAWS data and analytics services. One of the use cases of AWS SDK\n\nfor pandas is to simplify the querying and manipulating data stored\n\nin AWS data stores.\n\nIn this recipe, we will demonstrate how to use AWS SDK for pandas\n\nand the Redshi Data API. In this \u0000ow, we will demonstrate a\n\nLambda function with two steps:\n\n1. Execute an SQL statement to retrieve the data.\n\n2. Save the DataFrame to S3.\n\nGetting ready\n\nBefore starting, ensure you have the following prerequisites:\n\nAWS Lambda set up with the necessary execution role permissions to\n\ninteract with Redshi and S3. Your Lambda instance needs to be at the\n\nsame VPC subnet as your Redshi instance unless you set the VPC\n\nendpoint. is recipe assumes that you created a VPC endpoint.\n\nAble to use AWS Lambda managed layers. Check out\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.html for\n\nmore information.\n\nHave an Amazon Redshi cluster and associate it with an AWS IAM\n\nrole.\n\nHow to do it…\n\n1. Create a policy in IAM named LambdaRedshiftDataAPIRole and\n\nadd the following permission. Make sure to change the Resource\n\nvalue so that it is relevant to the Redshi cluster that you want the\n\nLambda instance to interact with:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogStream\", \"redshift-data:GetStatementResult\", \"redshift:GetClusterCredentials\", \"redshift-data:DescribeStatement\", \"logs:CreateLogGroup\", \"logs:PutLogEvents\", \"redshift-data:ExecuteStatement\", \"redshift-data:ListStatements\"\n\n], \"Resource\": \"*\" } ] }\n\n2. Log in to the Lambda service in your AWS portal. Click on the Create a\n\nfunction button to create a new Lambda function:\n\nFigure 10.25 – Clicking on Create a function\n\n3. Fill in the details shown in the following screenshot. Make sure you\n\ncreate a new role or choose an existing relevant IAM role in the Change\n\ndefault execution role section. If you create a new role, make sure to note\n\nit down for the next step. As of the time of writing this book, AWS Data\n\nWrangler version 2.13.0 works with Python version 3.9, so make sure\n\nyou use Python version 3.9 when creating the function. Fill in the\n\ninformation and then click on the Create function button:\n\nFigure 10.26 – Filling in the Runtime and Function name ﬁelds\n\n4. Aer the function is successfully created, go to the Lambda | Roles\n\nsection in IAM and attach the policy we created in step 1:\n\nFigure 10.27 – Attaching policies to your Lambda function\n\n5. Go back to the created Lambda function, scroll down to the Layers\n\nsection, and click on Add a layer:\n\nFigure 10.28 – Adding a layer to your Lambda function\n\n6. Add a layer under Specify an ARN. You should select the Amazon\n\nResource Name (ARN) that matches the runtime setting. Make sure to\n\n\u0000ll in your Lambda region in the ARN. Aer that, click on Add to add\n\nthe layer:\n\nFigure 10.29 – Verifying the ARN then clicking on Add\n\n7. Remember to change the Timeout and Memory values so that they \u0000t\n\nyour use case. In this exercise, the timeout will be 10 minutes (maximum\n\nallowed timeout):\n\nFigure 10.30 – Making sure to edit the timeout and memory\n\n8. Paste the following script in the Code tab. Edit the SQL to \u0000t your use\n\ncase:\n\nimport logging import awswrangler as wr import json statement = \"\"\" UNLOAD (SELECT * FROM redshift. {table_name}') TO '{S3_location}' IAM_ROLE '{'arn:aws:iam:your-account- id:role/your-role' }' PARQUET PARALLEL ON PARTITION BY (product_category)' \"\"\" def lambda_handler(event, context): logger.info(json.dumps(event)) query_id = event['Input'].get('query_id') con_redshift = wr.data_api.redshift.connect( workgroup_name=\"aws-sdk-pandas\", database=\"test_redshift\", secret_arn=\"arn:aws:secretsmanager:us- east-1:your-account-id:secret:your-secret- name\", ) wr.data_api.redshift.read_sql_query( sql=statement, con=con_redshift, return { 'statusCode': 200, 'body': json.dumps(\"finished\") }\n\nHow it works…\n\nWith Lambda, you create a serverless, highly available computing\n\narchitecture that can be further elevated by integrating with\n\nEventBridge for event-driven architecture (EDA) or Simple\n\nNoti\u0000cation Service (SNS) for error noti\u0000cation. It can help to\n\ntrigger a stored procedure based on speci\u0000c events or conditions.\n\nYou need to carefully plan your trigger and outcome to \u0000t the use\n\ncase’s goal and outcome.\n\ne Data API enables you to access Redshi data without\n\nmaintaining a persistent connection. is API can execute queries\n\nfrom any application or service that invokes HTTPS requests,\n\nincluding AWS Lambda and web applications. e Data API\n\nreturns the result asynchronously, so ideally, for a complex job, you\n\nshould integrate with Step Functions to get the job status using the\n\nDescribeStatement API and get the GetStatementResult\n\nAPI to see if the execution is \u0000nished. To use in a web application, it\n\nis better to parse the response to JSON instead of a pandas\n\nDataFrame.\n\nIn a real-world application, the S3 bucket that stores the DataFrame\n\nfrom Lambda execution could be the storage for the analytical\n\nworkbench where you store relevant data for your team to explore.\n\nDue to the limitations of Lambda timeout and memory, you should\n\nconsider appropriate tools such as Glue or SageMaker notebooks if\n\nthe dataset is large.\n\nThere’s more…\n\nWhen working with AWS Lambda, you need to pay attention to the\n\nselected Python version. To take a step further to integrate into a\n\nweb application, you can add an API gateway to leverage\n\nWebSocket API and REST API capability for your application.\n\nYou need to make sure your Python version in runtime matches\n\nwith AWS SDK for pandas. If you make a mistake while choosing\n\nthe runtime version, you can go to the runtime settings to change\n\nthe version.\n\nSee also\n\nManaging Lambda dependencies with layers:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.html\n\nAWS Lambda Layer: https://aws-sdk-\n\npandas.readthedocs.io/en/2.14.0/install.html#aws-lambda-layer\n\nStored procedure limitations:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/stored-procedure-\n\nconstraints.html\n\nSecurity and privileges for stored procedures:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/stored-procedure-\n\nsecurity-and-privileges.html\n\nCon\u0000gure Lambda function timeout:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/con\u0000guration-\n\ntimeout.html\n\nUsing the AWS SDK for Python to manage Amazon QuickSight\n\ne AWS SDK for Python (Boto3) could be used to create,\n\ncon\u0000gure, and manage Amazon QuickSight. ere are various use\n\ncases for it, from quickly granting permission to exporting\n\nQuickSight assets from one account to another account. e\n\nfollowing script in this recipe will demonstrate how to update\n\npermissions for data sources, datasets, analyses, dashboards, and\n\nthemes in Amazon QuickSight. It grants a speci\u0000c user or role the\n\nnecessary permissions to access and manage these resources.\n\nGetting ready\n\nBefore starting, ensure you have the following prerequisites:\n\nAn active AWS account with a QuickSight subscription\n\nPython 3 and Jupyter Notebook installed on your system\n\nBoto3 and PyYAML libraries installed (pip install boto3\n\nPyYAML)\n\nAWS credentials and pro\u0000le con\u0000gured\n\nHow to do it…\n\n1. From your AWS CLI, retrieve the Amazon QuickSight Principal ARN:\n\naws quicksight list-users --region <aws- region> --aws-account-id <account-id> -- namespace <namespace-name>\n\n2. In your Jupyter notebook, start with import boto3, set up the client,\n\nand input your Principal ARN. Replace the principal value with the\n\nvalue from step 1:\n\nimport boto3 client = boto3.client( \"quicksight\", ) principal = 'arn:aws:quicksight:<aws-region>: <account-id>:user/<namespace- name>/<quicksight-user-name>'\n\n3. Input the recipe code from the Using AWS SDK for pandas, the Redshi\n\nData API, and Lambda to execute SQL statements recipe; please check\n\nthe How it works… section.\n\nHow it works…\n\ne AWS SDK for QuickSight provides a set of APIs that you can\n\nuse to manage your QuickSight resources programmatically. ese\n\nAPIs allow you to perform various tasks, such as creating and\n\ndeleting users, managing datasets and analyses, and con\u0000guring\n\npermissions and security settings. In this recipe, we went through\n\nseveral important terms such as dataset, analysis, , and dashboard.\n\nLet’s elaborate more on these terms and how the API of these\n\ngroups works:\n\nA dataset is a collection of data used to create visualizations and\n\ndashboards. Datasets can be shared across users and groups and can be\n\nused to create multiple analyses and dashboards. With the API, you can\n\nspecify the data source and con\u0000gure data refresh schedules. You can\n\nalso retrieve a list of datasets in your QuickSight account and search for\n\ndatasets based on various criteria, such as dataset name or data source.\n\nAn analysis is a workspace where you can create and customize\n\nvisualizations and dashboards. It contains a dataset, which is the source\n\nof the data used in the visualizations, and a set of visualizations that you\n\ncan arrange and customize to suit your needs. You can create new\n\nvisualizations, add \u0000lters, remove visualizations, and apply formatting to\n\nthe data.\n\nA template is a reusable set of metadata that de\u0000nes the structure and\n\nsettings of an analysis or dashboard. It includes the dataset,\n\nvisualizations, \u0000lters, and formatting that you have de\u0000ned in the\n\nanalysis or dashboard and can be reused to create a new dashboard.\n\nA dashboard is a read-only version of your data visualization.\n\nDashboards can be scheduled to refresh at regular intervals and can be\n\ndelivered to users via email or other communication channels. ey can\n\nalso be embedded in external websites and applications, allowing you to\n\nshare your data with a wider audience.\n\nWhen you use the AWS SDK for QuickSight, you can use the\n\nprogramming interface to manage users, datasets, analyses,\n\ntemplates, and dashboards. is solution can help you to manage\n\nthe access of your visualization in a programmatic way. It is a good\n\nway to apply DevOps practices such as version control and\n\ncontinuous integration / continuous deployment (CI/CD) to your\n\nQuickSight artifacts.\n\nThere’s more…\n\nWith the boto3 library for QuickSight, you can transfer dashboard\n\nownership and create a CI/CD pipeline to move the dashboard\n\nfrom one environment to another or from one region to another\n\nregion.\n\nBesides the previously mentioned API, two frequently useful APIs\n\ncan help you quickly create and download your QuickSight\n\ntemplate:\n\ne create_template API helps you create a template from an\n\nexisting analysis or template.\n\ne describe_template API allows you to retrieve metadata about\n\na speci\u0000c template, including its name, ARN, creation and last updated\n\ntimes, description, sheets, source entity ARN, status, theme ARN, and\n\nversion number. is API is useful for getting information about a\n\ntemplate that you have created or that has been shared with you.\n\nPlease note that not all visuals are supported over APIs, so you\n\nshould check the API Reference Index page in the See also section\n\nfor more details.\n\nSee also\n\nData Types:\n\nhttps://docs.aws.amazon.com/quicksight/latest/APIReference/API_Type\n\ns.html\n\nAPI Reference Index:\n\nhttps://docs.aws.amazon.com/quicksight/latest/APIReference/API_Refe\n\nrence.html\n\nBoto3 documentation – create_template:\n\nhttps://boto3.amazonaws.com/v1/documentation/api/1.26.85/reference/\n\nservices/quicksight/client/create_template.html\n\nBoto3 documentation – describe_template:\n\nhttps://boto3.amazonaws.com/v1/documentation/api/1.26.93/reference/\n\nservices/quicksight/client/describe_template.html\n\nOceanofPDF.com\n\n11 Migrating to AWS – Steps, Strategies, and Best Practices for Modernizing Your Analytics and Big Data Workloads\n\nIn this chapter, we’ll embark on the journey of migrating a data\n\nwarehouse from an on-premises environment to AWS, a\n\ntransformative move that brings numerous bene\u0000ts and a few\n\nchallenges. is migration process represents a crucial step for\n\norganizations seeking to leverage the scalability, \u0000exibility, and cost-\n\neﬀectiveness of cloud computing.\n\nWe’ll explore the key considerations, strategies, and best practices\n\nfor a successful transition, including data migration methods,\n\nsecurity concerns, performance optimization, and leveraging AWS-\n\nspeci\u0000c services such as Amazon Redshi.\n\nOur focus will be on ensuring a seamless migration with minimal\n\ndisruption, while also optimizing the data warehouse to harness the\n\nfull potential of AWS’s cloud capabilities. is process not only\n\ninvolves physically transferring data but also a strategic shi in data\n\nmanagement and operations, aiming to enhance accessibility,\n\nanalytics, and overall business intelligence (BI) in the cloud\n\nenvironment.\n\ne following recipes will be covered in this chapter:\n\nReviewing the steps and processes for migrating an on-premises\n\nplatform to AWS\n\nChoosing your AWS analytics stack – the re-platforming approach\n\nPicking the correct migration approach for your workload\n\nPlanning for prototyping and testing\n\nConverting ETL processes with big data frameworks\n\nDe\u0000ning and executing your migration process with Hadoop\n\nMigrating the existing Hadoop security authentication and\n\nauthorization processes\n\nTechnical requirements\n\nBefore initiating a data warehouse migration to AWS, it’s essential\n\nto set a robust foundation by addressing a set of technical\n\nprerequisites. Initially, a comprehensive infrastructure assessment is\n\nnecessary so that you can evaluate the existing data warehouse’s\n\nspeci\u0000cations, network setup, and data volume to ensure the AWS\n\nenvironment is provisioned appropriately. Compatibility is\n\nimportant; hence, in-depth analysis to ensure data types, schemas,\n\nand procedures align with AWS services such as Amazon Redshi\n\nis crucial. Employing tools such as AWS Schema Conversion Tool\n\n(SCT) for assessment and conversion planning can signi\u0000cantly aid\n\nin this assessment and conversion planning process. e key\n\nconsiderations are as follows:\n\nNetwork readiness must be ensured. For this, you may potentially\n\nrequire a high-bandwidth, secure, and possibly dedicated connection\n\nsuch as AWS Direct Connect to manage the substantial data transfer\n\nprocess.\n\nSecurity considerations are non-negotiable, demanding a thorough\n\nreview and planning for encryption, compliance, and governance\n\nstandards. A detailed resource inventory, including databases, ETL jobs,\n\nand connected applications, should be documented and accompanied by\n\na performance baseline of the current system to measure post-migration\n\neﬃciency.\n\nA robust backup and recovery strategy is imperative, ensuring data\n\nintegrity during and aer the transition. Operational readiness, which\n\ninvolves setting up monitoring, logging, and maintenance mechanisms\n\nin AWS, will ensure smooth, ongoing operations.\n\nAddressing these prerequisites diligently sets the stage for a\n\nsuccessful, seamless migration where you can leverage AWS’s\n\nscalability, performance, and extensive feature set.\n\nReviewing the steps and processes for migrating an on-premises",
      "page_number": 645
    },
    {
      "number": 33,
      "title": "Segment 33 (pages 664-686)",
      "start_page": 664,
      "end_page": 686,
      "detection_method": "synthetic",
      "content": "platform to AWS\n\nMigrating from an on-premises data warehouse to AWS is a\n\nstrategic move that can yield signi\u0000cant bene\u0000ts such as cost\n\nsavings, enhanced scalability, and improved performance. is\n\nrecipe provides a comprehensive guide to the migration process\n\nwhile covering key considerations, strategies, best practices, and\n\nactionable steps to ensure a successful transition.\n\nGetting ready\n\nMigration consists of \u0000ve stages, as shown in Figure 11.1:\n\nFigure 11.1 – The ﬁve phases of migration\n\nLet’s take a closer look at these phases in detail:\n\n1. Assessment or opportunity evaluation: Begin by taking stock of your\n\ncurrent on-premises environment. is involves cataloging your\n\ninfrastructure (servers, storage, and networking), applications, data, and\n\nthe dependencies between them. Understand your business goals and\n\nperformance requirements to identify which workloads are suitable for\n\nmigration and which strategy aligns best with your objectives.\n\n2. Planning or portfolio discovery and planning: is phase is where the\n\nblueprint for your migration takes shape. Develop a comprehensive\n\nmigration plan that outlines the chosen strategy, detailed timeline,\n\nresource allocation (both personnel and \u0000nancial), and a thorough risk\n\nassessment. A well-craed plan is your compass, guiding you through\n\nthe complexities of the migration process.\n\n3. Migration or application design and migration: is is the heart of the\n\nprocess and is where you execute your plan. It can involve various\n\nstrategies:\n\nRehosting (li-and-shi): is involves moving your\n\napplications and data to AWS with minimal changes. It’s the\n\nquickest approach but may not fully leverage cloud bene\u0000ts.\n\nReplatforming: is involves making minor modi\u0000cations to\n\nyour applications so that you can take advantage of cloud-native\n\nfeatures such as managed databases or autoscaling.\n\nRefactoring/re-architecting: is is a more comprehensive\n\napproach where you redesign your applications so that they’re\n\ncloud-native, oen while utilizing microservices and serverless\n\ntechnologies.\n\n4. Testing and validation: Aer migrating your workloads, rigorous testing\n\nis paramount. is involves functional testing to ensure applications\n\nbehave as expected, performance testing to validate that they meet your\n\nrequirements under load, and security testing to identify and mitigate\n\nany vulnerabilities.\n\n5. Optimization and cutover: Once testing is complete, it’s time to optimize\n\nyour AWS resources for cost eﬃciency and performance. is can\n\ninvolve right-sizing instances, using reserved instances or savings plans,\n\nand employing autoscaling to adapt to varying workloads. Finally, you’ll\n\ntransition your production environment to AWS, either through a\n\nphased approach or a complete cutover.\n\nYou can read more at https://aws.amazon.com/blogs/apn/tips-for-\n\nbecoming-an-aws-migration-consulting-and-delivery-competency-\n\npartner/.\n\nHow to do it…\n\nFollow these steps to perform a detailed migration:\n\n1. Discovery and planning (assessment phase):\n\nInfrastructure inventory: Start by documenting all your\n\ninfrastructure components, including servers, storage systems,\n\nnetwork equipment, soware licenses, and any other essential\n\nassets.\n\nApplication inventory: List all your applications, their\n\ndependencies, and how they interact with infrastructure and\n\ndata.\n\nData assessment: Analyze your data in terms of volume, type,\n\nstructure, and sensitivity. is will help you determine the\n\noptimal migration and storage strategy.\n\nBusiness case development: Clearly de\u0000ne the objectives for the\n\nmigration, whether they’re cost reduction, scalability,\n\nperformance improvements, or faster time to market.\n\nCloud-readiness assessment: Evaluate the organization’s\n\nreadiness for cloud adoption by considering technical skills,\n\nsecurity practices, and governance protocols.\n\n2. Migration planning (planning phase):\n\nMigration strategy selection: Choose the most appropriate\n\nstrategy (rehosting, replatforming, or refactoring) based on the\n\nworkload’s requirements\n\nAWS account structure design: Set up your AWS accounts\n\naccording to your organizational needs, such as separate\n\naccounts for production, development, and testing\n\nenvironments\n\nNetwork architecture design: Plan your virtual private cloud\n\n(VPC), including subnets, security groups, and routing\n\ncon\u0000gurations\n\nResource provisioning: Estimate the AWS resources that are\n\nrequired (EC2 instances, RDS databases, S3 buckets, and so on)\n\nbased on your workload and capacity needs\n\nCost estimation: Use the AWS Pricing Calculator to predict\n\ncosts and create a budget for your cloud infrastructure\n\nMigration timeline: Develop a timeline outlining key tasks,\n\ndependencies, and deadlines\n\nRisk assessment: Identify potential risks such as downtime, data\n\nloss, or security vulnerabilities, and put mitigation plans in\n\nplace\n\nCommunication plan: Create a communication strategy to keep\n\nstakeholders informed and involved during the migration\n\nprocess\n\n3. Migration execution (migration phase):\n\nAWS environment setup: Build your AWS environment by\n\nsetting up VPCs, subnets, and security groups. Establish\n\nnetwork connectivity (such as Direct Connect or VPN)\n\nbetween your on-premises infrastructure and AWS.\n\nData migration:\n\nUse AWS Database Migration Service (DMS) to move\n\ndatabases from on-premises to AWS (for example,\n\nOracle to Amazon RDS)\n\nImplement AWS DataSync for fast and secure \u0000le\n\ntransfers to Amazon S3 or EFS\n\nFor large-scale transfers, consider AWS Snowball\n\nApplication migration:\n\nRehosting: Use AWS Server Migration Service (SMS)\n\nor AWS Application Migration Service (MGS) to\n\nreplicate VMs from your data center to AWS\n\nReplatforming: Shi applications to AWS-managed\n\nservices, such as Amazon RDS, ElastiCache, or\n\nAmazon MQ\n\nRefactoring: Redesign applications so that they\n\nleverage cloud-native services such as microservices,\n\ncontainers (Amazon ECS or EKS), or serverless\n\nfunctions (AWS Lambda)\n\n4. Testing and validation (testing phase):\n\nFunctional testing: Develop and run test cases to ensure that all\n\napplication features and business processes function as\n\nexpected in AWS\n\nPerformance testing: Simulate real-world workloads to verify\n\nthat your applications meet performance benchmarks in the\n\ncloud\n\nSecurity testing: Conduct vulnerability scans and penetration\n\ntests to detect and \u0000x potential security risks\n\nData validation: Con\u0000rm data accuracy and integrity between\n\nthe on-premises and AWS environments\n\n5. Optimization and cutover (\u0000nal phase):\n\nResource optimization: Continuously monitor AWS resource\n\nusage (CPU, memory, and storage) and adjust it to optimize\n\nperformance and cost-eﬃciency\n\nCost optimization: Regularly review your AWS billing to\n\nidentify cost-saving opportunities, such as Reserved Instances\n\nor Savings Plans\n\nPerformance tuning: Fine-tune applications and databases to\n\nenhance performance in the AWS cloud\n\nCutover: ere are two cutover approaches we can use:\n\nPhased migration: Gradually migrate applications and\n\ndata in stages, allowing for validation and testing at\n\neach phase.\n\nBig bang migration: Migrate all applications and data\n\nin one go. While faster, this method carries more risk.\n\nBy precisely following these steps and understanding the \u0000ve key\n\nphrases of migration, you can con\u0000dently and successfully\n\ntransition your on-premises platform to AWS.\n\nChoosing your AWS analytics stack – the re-platforming approach\n\nIn this recipe, we’ll focus on the re-platforming approach, which\n\ninvolves migrating to a new platform while making modi\u0000cations to\n\nyour architecture so that you can take advantage of cloud bene\u0000ts.\n\nWe’ll examine the rich ecosystem of AWS services that can power\n\nyour modern cloud data warehouse.\n\nGetting ready\n\nBefore you initiate your migration, ensure you have the following in\n\nplace:\n\nClear business objectives: Clearly de\u0000ne your motivations for migrating\n\nto AWS. Are you seeking cost savings, scalability, improved\n\nperformance, or increased agility? ese goals will guide your decision-\n\nmaking throughout the migration process.\n\nAn AWS account: If you don’t already have one, you’ll need to create an\n\nAWS account. is will be the foundation for all your cloud resources.\n\nNetwork connectivity: Establish secure connectivity between your on-\n\npremises data center and AWS. is can be achieved through AWS\n\nDirect Connect (dedicated private network connection) or a site-to-site\n\nVPN.\n\nInventory and assessment:\n\nInfrastructure: Catalog your existing on-premises data\n\nwarehouse infrastructure (servers, storage, and network\n\ndevices)\n\nApplications: Identify and document all applications that\n\ninteract with your data warehouse\n\nData: Analyze your data assets (volume, type, structure, and\n\nquality) to understand the scope of the migration\n\nDependencies: Map the relationships between applications,\n\ndata, and infrastructure to identify potential migration\n\nchallenges\n\nPerformance requirements: Determine your desired\n\nperformance metrics for the cloud data warehouse\n\nSkills and resources: Ensure you have the necessary technical expertise\n\nin-house or have access to external consultants with experience in AWS\n\ndata warehousing technologies.\n\nWhy re-platform on AWS?\n\nRe-platforming on AWS oﬀers a compelling middle ground\n\nbetween the speed of rehosting and the full transformation of\n\nrefactoring. Here’s why it’s a popular choice:\n\nReduced operational overhead: AWS takes care of infrastructure\n\nmanagement, allowing you to focus on your data and analytics\n\nScalability and elasticity: You can dynamically scale your resources up or\n\ndown to match demand, paying only for what you use\n\nManaged services: You can leverage AWS’s fully managed services, such\n\nas Amazon Redshi, to oﬄoad maintenance tasks and accelerate\n\ninnovation\n\nPerformance and cost optimization: You can choose from a variety of\n\ncompute and storage options to \u0000ne-tune performance and cost-\n\neﬀectiveness\n\nA rich ecosystem of services: You can integrate with a wide range of\n\nAWS services for data ingestion, transformation, analytics, and\n\nvisualization purposes\n\nThe AWS analytics ecosystem\n\nAWS oﬀers a comprehensive suite of services tailored for building a\n\nmodern data warehouse:\n\nAmazon Redshi: A fast, fully managed, petabyte-scale data warehouse\n\noptimized for analytics. It oﬀers excellent performance for complex\n\nqueries and supports a wide range of data types and SQL functions.\n\nAmazon S3: A scalable, high-durability object storage service for storing\n\nvast amounts of structured and unstructured data. It’s a cost-eﬀective\n\noption for data lakes and can be queried directly by Redshi using\n\nRedshi Spectrum.\n\nAWS Glue: A serverless data integration service that simpli\u0000es\n\ndiscovering, preparing, and combining data for analytics and machine\n\nlearning. Glue oﬀers crawlers for schema discovery, extract, transform,\n\nand load (ETL) capabilities, as well as a data catalog.\n\nAmazon Athena: A serverless interactive query service that allows you to\n\nanalyze data in S3 using standard SQL. Athena is a great option for ad\n\nhoc queries and exploration.\n\nAWS Lake Formation: is is a fully managed service that simpli\u0000es the\n\nprocess of creating, securing, and managing data lakes. It automates data\n\ningestion, cataloging, and access control, allowing you to organize,\n\nsecure, and analyze large-scale data eﬃciently in a centralized repository.\n\nBy integrating with other AWS services, Lake Formation enables\n\nseamless data governance and analytics in a scalable, cost-eﬀective\n\nmanner.\n\nAWS DynamoDB: is is a serverless, NoSQL database service that\n\nenables you to develop modern applications at any scale.\n\nAWS Lambda: AWS Lambda is a serverless computing service that lets\n\nyou run code without the need to provision or manage servers. You pay\n\nonly for the compute time you consume. is eliminates the need for\n\nupfront infrastructure costs and allows you to focus on building\n\napplications.\n\nAWS Managed Work\u0000ows for Apache Air\u0000ow (MWAA): is is a fully\n\nmanaged service that simpli\u0000es the deployment and operation of Apache\n\nAir\u0000ow, an open source work\u0000ow orchestration platform. It allows you\n\nto build, schedule, and monitor data pipelines and work\u0000ows without\n\nthe need to manage the underlying infrastructure.\n\nAmazon Kinesis: is is a fully managed service that processes and\n\nanalyzes real-time streaming data at scale. It can capture and process\n\nmillions of events per second from various sources, making it ideal for\n\napplications such as clickstream analysis, IoT data processing, and real-\n\ntime analytics.\n\nAmazon EMR: is service provides the cloud big data platform for\n\nprocessing vast amounts of data using open source tools.\n\nAmazon QuickSight: A scalable, serverless BI service for creating\n\nvisualizations, dashboards, and reports from your data warehouse data.\n\nIf you’re aiming to re-platform your analytics stack and wish to\n\nleverage the AWS analytics ecosystem, here’s a guide on choosing\n\nthe right tools and services:\n\n1. Data ingestion and integration:\n\nAWS Glue: is is a serverless data integration service that\n\nsimpli\u0000es the process of discovering, preparing, and combining\n\ndata for analytics. It automates many of the tasks involved in\n\nETL, allowing you to focus on building data-driven\n\napplications.\n\nAWS Data Migration Service (DMS): is service helps migrate\n\non-premises databases to AWS easily and securely.\n\nAmazon Kinesis or MSK: For real-time streaming data.\n\n2. Data storage:\n\nAmazon S3: Scalable storage for data lakes and analytics. S3’s\n\nintegrations and data protection features make it ideal for a data\n\nlake.\n\nAmazon Redshi: A petabyte-scale data warehouse service. For\n\ncomplex analytics workloads, you can analyze all your data\n\nusing your existing BI tools.\n\nAmazon RDS and Aurora: For relational data storage needs.\n\n3. Data processing and analysis:\n\nAWS Glue: With this service, you can visually clean and\n\nnormalize data\n\nAmazon Elastic MapReduce (EMR): You can process large\n\namounts of data with popular distributed frameworks such as\n\nApache Spark and Hadoop\n\nAWS Lambda: You can run code in response to events without\n\nprovisioning servers, which can be useful for lightweight data\n\nprocessing tasks\n\n4. Data querying:\n\nAmazon Athena: A serverless interactive query service that\n\nallows you to analyze data in Amazon S3 using SQL\n\nAmazon Redshi Spectrum: With this service, you can examine\n\nhuge datasets in S3 without having to load or transform them\n\n5. Machine learning and advanced analytics:\n\nAmazon SageMaker: Build, train, and deploy machine learning\n\nmodels at scale\n\nAmazon Comprehend and Recognition: For natural language\n\nprocessing (NLP) and video/image analysis, respectively\n\n6. Data visualization and BI:\n\nAmazon QuickSight: A BI service with native machine learning\n\nintegrations\n\nIntegration with third-party tools: AWS supports integration\n\nwith popular BI tools such as Tableau, Looker, and more\n\n7. Data security:\n\nAmazon Macie: Discover, classify, and protect sensitive data\n\nAWS Key Management Service (KMS): Create and manage\n\ncryptographic keys\n\nAWS Identity and Access Management (IAM): Manage user\n\npermissions and access\n\n8. Data governance and cataloging:\n\nAWS Glue Catalog: A central metadata repository integrated\n\nwith a wide range of AWS services\n\nLake Formation: Simpli\u0000es and automates many of the complex\n\ntasks associated with setting up, securing, and managing data\n\nlakes\n\n9. Monitoring and Management:\n\nAmazon CloudWatch: Monitor resources and applications\n\nAWS CloudTrail: Provides governance, compliance, operational\n\nauditing, and risk auditing\n\n10. Optimization and cost management:\n\nAWS Cost Explorer: View and analyze your costs and usage\n\nAWS Trusted Advisor: Oﬀers real-time guidance to provision\n\nresources while following AWS best practices\n\nHow to do it…\n\nWhen re-platforming, consider these steps:\n\n1. Assess: Understand your current analytics stack and identify areas of\n\nimprovement.\n\n2. Select AWS services: Based on your needs, pick the right services from\n\nthe AWS analytics ecosystem.\n\n3. Data migration: Move your data to AWS, either through batch transfers\n\nor real-time streams.\n\n4. Re-architect and develop: Modify or redesign your analytics work\u0000ows\n\nand processes so that they \u0000t within the AWS ecosystem.\n\n5. Testing: Ensure that the new system works correctly and meets\n\nperformance standards.\n\n6. Deployment and monitoring: Roll out the new system and continuously\n\nmonitor its performance, optimizing as necessary.\n\nAWS analytics services architecture\n\nHarnessing the power of AWS’s comprehensive data analytics\n\nservices, the following diagram outlines a scalable and \u0000exible\n\nsolution for processing, storing, and analyzing massive datasets\n\neﬃciently.\n\ne following mind map diagram shows a use case map for\n\nanalytics services:\n\nFigure 11.2 – Analytics service mind map\n\nA modern data analytics architecture on AWS involves integrating\n\nvarious AWS services so that you can ingest, store, process, analyze,\n\nand visualize data in a scalable, secure, and cost-eﬀective manner.\n\nHere’s a breakdown of the components that are typically involved in\n\nsuch a reference architecture:\n\n1. Data ingestion: Data comes from multiple sources, such as applications,\n\ndatabases, logs, and third-party systems. AWS provides several services\n\nfor handling diﬀerent types of data ingestion:\n\nAmazon Kinesis or AWS Managed Streaming for Apache Kaa\n\n(MSK): For real-time streaming data such as IoT sensor data or\n\nlog \u0000les\n\nAWS DMS: For migrating or replicating databases to AWS\n\nAWS IoT Core: For ingesting data from IoT devices\n\nAmazon AppFlow: For integrating data from SaaS applications\n\nsuch as Salesforce, ServiceNow, or Google Analytics\n\nAWS DataSync: For moving large amounts of unstructured\n\ndata, such as \u0000les and logs\n\nAWS Snowball: For transferring petabyte-scale data from on-\n\npremises environments\n\n2. Data lake storage: Aer data ingestion, the raw and processed data is\n\nstored in a scalable, durable, and secure data lake.\n\nAmazon S3: Serves as the primary data lake storage platform. It\n\nprovides cost-eﬀective, scalable storage with high durability.\n\nAWS Lake Formation: Helps in building, managing, and\n\nsecuring the data lake on S3.\n\n3. Data cataloging and governance: Metadata management, data\n\ncataloging, and data governance are crucial for managing a data lake at\n\nscale:\n\nAWS Glue: For automatic schema discovery, data cataloging,\n\nand ETL work\u0000ows\n\nAWS Glue DataBrew: For data preparation and cleaning\n\nwithout writing code\n\nAWS Lake Formation: For uni\u0000ed governance, which helps\n\nmanage security, access controls, and audit logs centrally\n\n4. Data processing and transformation: Once the data has been ingested\n\nand cataloged, various processing methods can be applied, depending on\n\nthe data type and latency requirements:\n\nAWS Glue: For running ETL jobs that transform and load data\n\ninto a format suitable for analysis\n\nAmazon EMR: For big data processing using Apache Spark,\n\nHive, and other open source frameworks\n\nAmazon Redshi Spectrum: Enables querying directly from the\n\ndata lake using SQL\n\nAWS Lambda: For serverless data processing, especially for\n\nreal-time or event-driven processing\n\n5. Data warehousing: For structured, high-performance, and scalable\n\nquerying, data is oen transformed into a data warehouse optimized for\n\nanalytics:\n\nAmazon Redshi: AWS’s fully managed data warehouse service,\n\noptimized for querying large datasets and integrating with the\n\ndata lake using Redshi Spectrum\n\nAmazon RDS/Aurora: For structured data storage with\n\nrelational databases such as MySQL, PostgreSQL, and Aurora\n\n6. Data analytics: With data ingested, processed, and stored, analytics can\n\nbe performed using various tools to derive insights and perform\n\nmachine learning:\n\nAmazon Athena: An interactive query service for analyzing\n\ndata in S3 using standard SQL\n\nAmazon OpenSearch: For full-text search, log analytics, and\n\nreal-time monitoring\n\nAWS Kinesis Data Analytics: For real-time analytics on\n\nstreaming data\n\nAWS QuickSight: AWS’s BI tool for generating dashboards and\n\nreports\n\n7. Machine learning and AI: Data science and machine learning work\u0000ows\n\nare essential for predictive analytics, recommendation systems, and\n\nanomaly detection:\n\nAmazon SageMaker: A fully managed machine learning service\n\nfor building, training, and deploying machine learning models\n\nAWS AI services: Pre-built AI services such as Amazon\n\nRekognition (image recognition), Amazon Comprehend\n\n(natural language processing), and Amazon Forecast (time\n\nseries forecasting)\n\n8. Security, governance, and monitoring: Security and governance play a\n\ncritical role in modern analytics architecture, ensuring that data is\n\nprotected and compliant with industry standards:\n\nAWS IAM: For controlling access to services and resources\n\nAWS KMS: For encrypting data at rest and in transit\n\nAWS CloudTrail: For monitoring and logging all activities on\n\nAWS accounts for security audits\n\nAmazon CloudWatch: For monitoring the performance of\n\napplications, services, and infrastructure\n\nAWS Con\u0000g: For monitoring compliance and security posture\n\n9. Data visualization and BI: Visualizing insights is key for decision-\n\nmaking processes, and AWS oﬀers a variety of services to accomplish\n\nthis:\n\nAmazon QuickSight: AWS’s scalable BI tool that oﬀers machine\n\nlearning and AI-driven insights, visualizations, and dashboards\n\nird-party tools: Integration with popular BI tools such as\n\nTableau, Power BI, or Looker\n\n10. Cost management: With all services in place, managing costs is critical\n\nto ensure the analytics platform remains eﬃcient:\n\nAWS Cost Explorer: For visualizing and understanding costs\n\nAWS Trusted Advisor: Provides real-time guidance to help\n\noptimize the AWS infrastructure in terms of cost, performance,\n\nsecurity, and fault tolerance\n\nChoosing your architecture\n\ne right architecture for your AWS data warehouse will depend on\n\nyour speci\u0000c needs and use cases. Here are a few popular\n\narchitectural patterns:\n\nFigure 11.3 – AWS data warehouse beneﬁts\n\nis Redshi-centric architecture leverages Amazon Redshi’s\n\nmassively parallel processing (MPP) capabilities to process and\n\nanalyze large datasets eﬃciently while supporting advanced BI and\n\nreporting needs.\n\nLet’s explore the diverse range of AWS architectures that are\n\navailable to support your unique cloud computing needs:\n\nRedshi-centric architecture: is is ideal for high-performance\n\nanalytics workloads. All your data is loaded into Redshi, and you use\n\nits built-in capabilities for analysis.\n\nIn a Redshi-centric architecture, data from various operational\n\nsystems, marketing data sources, and other systems is initially\n\nmoved to Amazon S3. AWS Glue is then used to perform ETL\n\nprocesses, transforming and cleansing the data before it’s loaded\n\ninto Amazon Redshi. Redshi acts as the central data\n\nwarehouse, serving as a producer that consolidates and stores\n\nthis processed data. e consolidated data is then made\n\naccessible to multiple consumers, including Consumer 1,\n\nConsumer 2, and Consumer 3, each leveraging the data for\n\nanalytics, reporting, and BI purposes. is setup ensures a\n\nstreamlined, scalable, and eﬃcient data pipeline that supports\n\ndiverse analytical needs across the organization:\n\nFigure 11.4 – AWS Redshift-centric architecture diagram\n\nData lakehouse architecture: Combines the best of data lakes and data\n\nwarehouses. Raw data is stored in S3, and Glue is used for ETL and\n\ntransformation. You can use Redshi, Athena, or both to query the data.\n\ne initial layer, known as the landing layer, serves as the entry\n\npoint for all source \u0000les in their native formats. Subsequently,\n\nthese \u0000les undergo conversion and are stored in a standardized\n\nParquet format within the raw layer. e stage layer maintains\n\nhistorical records of dimensional tables using Slowly Changing\n\nDimension Type 2 (SCD2), which is facilitated by Apache Hudi\n\nin Amazon S3 and AWS Glue jobs. AWS Glue orchestrates ETL\n\ntasks across layers, ensuring seamless data movement, cleansing,\n\nvalidation, and transformation. In the presentation layer, data is\n\nmeticulously re\u0000ned so that it aligns with business requirements",
      "page_number": 664
    },
    {
      "number": 34,
      "title": "Segment 34 (pages 687-708)",
      "start_page": 687,
      "end_page": 708,
      "detection_method": "synthetic",
      "content": "through AWS Glue jobs. Finally, the data warehouse layer,\n\npowered by Amazon Redshi, houses the curated and cleansed\n\ndata, accessible either through direct copying via AWS Glue or\n\nby creating Spectrum tables linked to the S3 location.\n\nServerless analytics architecture: Leverages Athena and S3 for a fully\n\nserverless data lake. Glue can be used for ETL if needed. is is a cost-\n\neﬀective option for ad hoc or intermittent workloads.\n\nAWS data mesh architecture: is is a decentralized approach to data\n\nmanagement that empowers business domains to own and manage their\n\ndata. It contrasts with traditional data management, where data is\n\ncentralized and managed by a dedicated team. In data mesh, each\n\ndomain is responsible for the entire life cycle of its data, from ingestion\n\nto consumption. is includes ingesting data from various sources,\n\nprocessing it to make it usable, and serving it to consumers.\n\nData mesh is similar to microservices in that it breaks down data\n\nmanagement into smaller, independent units. is makes it\n\nmore scalable and \u0000exible than traditional data management\n\napproaches. It also makes it easier for business domains to\n\ninnovate and experiment with new data use cases.\n\nAWS Lake Formation and AWS Glue are two services that can\n\nbe used to implement a data mesh architecture on AWS. Lake\n\nFormation is a service that makes it easy to create and manage\n\naccess and permissions for the data lake. AWS Glue is a\n\nserverless ETL service that can be used to extract, transform,\n\nand load data into a data lake.\n\nData mesh is a relatively new concept, but it’s gaining traction as\n\norganizations look for ways to improve their data management\n\npractices. AWS Lake Formation and AWS Glue are two services\n\nthat can be used to implement a data mesh architecture on AWS.\n\nHere are some key considerations when you’re choosing your\n\narchitecture:\n\nWorkload types: Are your workloads primarily batch processing,\n\ninteractive querying, or a mix of both?\n\nData volume: How much data do you need to store and process?\n\nData variety: Do you need to handle structured, semi-structured, or\n\nunstructured data?\n\nPerformance requirements: What are your latency requirements for\n\nqueries and reporting?\n\nCost optimization: What is your budget, and how can you balance cost\n\nwith performance?\n\ne key considerations for data migration services and tools are as\n\nfollows:\n\nSchema conversion: Use the AWS SCT to convert your on-premises data\n\nwarehouse schema into a format compatible with Redshi.\n\nData migration: Use AWS DMS, S3 Transfer Acceleration, or other tools\n\nto move your data to S3.\n\nETL with Glue: Develop ETL jobs in Glue to transform and load the data\n\ninto Redshi (or prepare it for querying with Athena).\n\nOptimize and validate: Fine-tune your Redshi cluster (or Athena\n\nqueries) for performance. Validate data integrity and functionality.\n\nBy carefully considering your requirements and choosing the right\n\narchitecture, you can use the full power of the AWS analytics\n\necosystem to build a scalable, \u0000exible, and cost-eﬀective data\n\nwarehouse that meets your business needs.\n\nSee also\n\nModern Data Analytics Reference Architecture on AWS Diagram:\n\nhttps://docs.aws.amazon.com/architecture-diagrams/latest/modern-\n\ndata-analytics-on-aws/modern-data-analytics-on-aws.html\n\nUse a reusable ETL framework in your AWS lake house architecture:\n\nhttps://aws.amazon.com/blogs/architecture/use-a-reusable-etl-\n\nframework-in-your-aws-lake-house-architecture/\n\nDesign a data mesh architecture using AWS Lake Formation and AWS\n\nGlue: https://aws.amazon.com/blogs/big-data/design-a-data-mesh-\n\narchitecture-using-aws-lake-formation-and-aws-glue/\n\nServerless data lake centric analytics architecture:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-serverless-data-\n\nanalytics-pipeline/serverless-data-lake-centric-analytics-\n\narchitecture.html\n\nPicking the correct migration approach for your workload\n\nChoosing the right migration approach depends on various factors,\n\nincluding the type of workload, existing infrastructure,\n\nperformance needs, future scalability requirements, and budget\n\nconstraints. e choice becomes even more nuanced when you’re\n\nconsidering speci\u0000c workloads, such as data warehouses or Hadoop\n\nclusters. is recipe will guide you through the process of picking\n\nthe correct migration approach for diﬀerent workloads.\n\nIn this recipe, we’ll explore several migration approaches and\n\ndiscuss the factors to consider when you’re choosing the best\n\napproach for your workload. We’ll also provide a step-by-step guide\n\nto planning and executing a migration.\n\nEach migration strategy comes with its pros and cons. e choice\n\ndepends on the current state of your systems, the nature of the\n\nworkload, the desired bene\u0000ts, and any other constraints. A\n\nthorough assessment of the existing environment, coupled with a\n\nclear understanding of business and technical objectives, will guide\n\nthe decision-making process.\n\nGetting ready\n\nSelecting the optimal migration strategy is crucial. Here are some\n\ncommon approaches and when to consider them.\n\nData warehouse migration to AWS oﬀers several approaches,\n\nallowing you to choose the one that best \u0000ts your needs:\n\nLi and shi: Li and shi, also known as rehosting, is a migration\n\nstrategy that involves moving workloads from on-premises\n\ninfrastructure to the cloud without making any signi\u0000cant changes to\n\nthe application code or architecture. is approach is oen used for\n\napplications that are running well on-premises but that could bene\u0000t\n\nfrom the scalability, elasticity, and cost-eﬀectiveness of the cloud:\n\nWhen to use: Your current on-premises data warehouse meets\n\nperformance and scalability requirements, but you desire the\n\nbene\u0000ts of cloud infrastructure, such as operational eﬃciency,\n\nelasticity, and cost savings\n\nAdvantages: Faster migration, lower initial eﬀort, and minimal\n\nchanges to existing systems\n\nExample: Migrating an on-premises Teradata or Oracle data\n\nwarehouse to Amazon Redshi without major transformations\n\nLi and rewrite/re-architect: Li and rewrite/re-architect is a cloud\n\nmigration strategy that involves rewriting or re-architecting applications\n\nto take full advantage of the cloud’s capabilities. is approach is oen\n\nused for applications that aren’t running eﬃciently on-premises or that\n\nneed to be modernized to meet new business requirements:\n\nWhen to use: Your current data warehouse has inherent\n\nlimitations, or you wish to leverage cloud-native features,\n\noptimized performance, and scalability\n\nAdvantages: Fully utilizing cloud capabilities, improved\n\nperformance, and potentially reduced operational costs in the\n\nlong run\n\nExample: Transforming an on-premises data warehouse into a\n\nserverless architecture using Amazon Athena and S3\n\nHybrid: A hybrid cloud environment combines on-premises\n\ninfrastructure with a public cloud, such as AWS. is approach allows\n\norganizations to take advantage of the bene\u0000ts of both environments,\n\nsuch as the scalability and elasticity of the cloud, while still maintaining\n\ncontrol over their sensitive data on-premises:\n\nWhen to use: You need to maintain some components on-\n\npremises due to data sovereignty, latency, or other business\n\nrequirements, but you also want to leverage the cloud for\n\nscalability and \u0000exibility\n\nAdvantages: You can balance on-premises control and cloud\n\nscalability\n\nExample: Keeping a subset of sensitive data on-premises while\n\nbursting compute-intensive analytics tasks to Amazon Redshi\n\nWhen choosing a migration approach, it’s important to consider the\n\nfollowing factors:\n\ne size and complexity of the workload: e size and complexity of the\n\nworkload will determine the time and eﬀort required to migrate it to the\n\ncloud. Larger and more complex workloads will typically require more\n\ntime and eﬀort to migrate than smaller and simpler workloads.\n\ne skills and experience of the team: e skills and experience of the\n\nteam will determine how much work they can do on their own and how\n\nmuch help they will need from external consultants. A team with strong\n\ncloud skills and experience will be able to migrate workloads more\n\nquickly and eﬃciently than a team with limited cloud skills and\n\nexperience.\n\ne desired timeframe for migration: e desired timeframe for\n\nmigration will determine the urgency with which the migration needs to\n\nbe completed. If the migration needs to be completed quickly, then a li-\n\nand-shi approach may be the best option. If the migration can be\n\ncompleted more slowly, then a refactoring or re-platforming approach\n\nmay be a better option.\n\nBudget: e budget for the migration will determine the resources that\n\nare available to complete the migration. A larger budget will allow for\n\nmore resources to be allocated to the migration, which may allow for a\n\nmore complex migration approach to be used.\n\nRisk tolerance: e risk tolerance of the organization will determine how\n\nmuch risk is acceptable when migrating workloads to the cloud. A risk-\n\naverse organization may prefer a li-and-shi approach, while a more\n\nrisk-tolerant organization may be willing to consider a more complex\n\nmigration approach.\n\nHow to do it…\n\nOnce you’ve chosen a migration approach, you can start planning\n\nand executing the migration. e following is a step-by-step guide\n\nto planning and executing a migration:\n\n1. De\u0000ne the scope of the migration: e \u0000rst step is to de\u0000ne the scope of\n\nthe migration, which includes identifying the workloads that will be\n\nmigrated, the target cloud environment, and the timeframe for the\n\nmigration.\n\n2. Assess the readiness of the organization for migration: e next step is to\n\nassess the readiness of the organization for migration. is includes\n\nassessing the skills and experience of the team, the availability of\n\nresources, and the organizational culture.\n\n3. Design the migration plan: e design of the migration plan will include\n\ndetailed speci\u0000cations for the migration, such as the timeline for each\n\nstep, the tools and technologies that will be used, and the procedures for\n\ntesting and cutover.\n\n4. Execute the migration: e execution of the migration will involve\n\ncarrying out the steps in the migration plan. is includes migrating the\n\nworkloads to the cloud, testing the migrated workloads, and cutting over\n\nto the cloud environment.\n\n5. Monitor and optimize the migration: Once the migration is complete, it’s\n\nimportant to monitor the migrated workloads to ensure that they’re\n\nperforming as expected.\n\nPlanning for prototyping and testing\n\nWhen planning for a migration, especially a signi\u0000cant migration,\n\nsuch as moving to the cloud, prototyping and testing are critical\n\nphases. ese steps allow organizations to validate assumptions,\n\nidentify potential issues, and ensure smooth operations when the\n\nmigration is executed at scale.\n\nGetting ready\n\nPrototyping and testing are crucial phases in the on-premises to\n\nAWS cloud migration process. ese phases ensure that the\n\nmigration is well-planned, executed seamlessly, and meets the\n\norganization’s objectives.\n\nPrototyping involves creating a small-scale replica of the on-\n\npremises environment in the AWS cloud. is allows for thorough\n\ntesting and validation of the migration plan before it’s applied to the\n\nentire production environment. Here are the bene\u0000ts of\n\nprototyping:\n\nIdentifying and resolving potential issues early in the migration process\n\nValidating the migration plan and ensuring it aligns with the\n\norganization’s requirements\n\nGaining hands-on experience with AWS services and tools\n\nTesting is an ongoing process that encompasses both functional and\n\nnon-functional testing. Functional testing veri\u0000es that the migrated\n\napplications are working as expected, while non-functional testing\n\nassesses aspects such as performance, security, and scalability. Here\n\nare some of the bene\u0000ts of testing:\n\nYou can ensure that the migrated applications are fully compatible with\n\nthe AWS environment\n\nYou can identify and address performance bottlenecks or security\n\nvulnerabilities\n\nYou can optimize the applications for maximum eﬃciency and\n\nscalability in the cloud\n\nBy incorporating prototyping and testing into the on-premises to\n\nAWS cloud migration process, organizations can minimize risks,\n\nenhance the migration’s success rate, and achieve a smooth\n\ntransition to the cloud.\n\nHow to do it…\n\nPlanning for prototyping and testing from on-premises to AWS\n\ninvolves several key steps. Here’s how to approach it:\n\n1. De\u0000ne the objective:\n\nUnderstand the primary goals of the migration: Performance\n\nenhancement, cost-saving, scalability, and so on\n\nClearly state what you hope to achieve with the prototype:\n\nValidate migration tools, test performance, ensure data\n\nintegrity, and so on\n\n2. Pick a prototype scope:\n\nRepresentative subset: Choose a subset of your systems/data\n\nthat’s a good representative of the whole. is should include\n\nvarious data types, applications, and workloads.\n\nComplexity: Include both simple and complex components to\n\nensure a comprehensive test.\n\nSize: e dataset should be large enough to simulate real-world\n\nscenarios but not so vast that it becomes cumbersome.\n\n3. Develop a prototype plan:\n\nSetup: De\u0000ne the tools (for example, AWS SCT and DMS) and\n\nresources needed\n\nExecution: Outline the step-by-step process of migration for the\n\nprototype\n\nValidation: Describe how you’ll validate the success of the\n\nmigration prototype\n\n4. Execute the prototype migration:\n\nMigrate data: Use the chosen tools to migrate the data\n\nCon\u0000gure environment: Ensure the target environment has\n\nbeen set up similarly to the planned \u0000nal setup\n\nDocument: Keep detailed notes of any challenges or issues that\n\nyou faced\n\n5. Test the prototype:\n\nFunctionality tests: Ensure all applications and services are\n\nworking as expected post-migration\n\nPerformance tests: Compare the performance of the prototype\n\nsystem to the original and your performance goals\n\nSecurity and compliance tests: Ensure that data remains secure\n\nand that all compliance requirements are met post-migration\n\nFailover and recovery tests: Check the reliability and resilience\n\nof the new setup\n\n6. Gather feedback: Involve end users and stakeholders. eir feedback on\n\napplication performance, accessibility, and any potential issues is\n\ninvaluable.\n\n7. Re\u0000ne the migration process: Based on feedback and test results, re\u0000ne\n\nyour migration strategy. Address any issues or challenges that were\n\nnoted during the prototyping phase.\n\n8. Plan for mass migration:\n\nIterate: If major issues are found during the prototype phase,\n\nconsider running additional prototypes or tests before\n\ncommitting to a mass migration.\n\nScale up: Once con\u0000dent in the prototype’s success, plan out the\n\nmigration for the entire system. Use the prototype as a\n\nblueprint.\n\nBack up: Ensure you have backups of everything before the\n\nmass migration.\n\nHere are a few best practices:\n\nIterative approach: It’s okay to run multiple prototypes if necessary. It’s\n\nbetter to identify issues now than during the mass migration.\n\nStakeholder communication: Keep all stakeholders in the loop. eir\n\ninsights can be invaluable and ensure everyone is aligned on\n\nexpectations.\n\nDocumentation: Maintain detailed documentation throughout the\n\nprototyping and testing phase. is will be invaluable during the mass\n\nmigration.\n\nSafety \u0000rst: Always ensure data integrity and security. Any signs of data\n\nloss or potential breaches should be addressed immediately.\n\nHere are a few key takeaways:\n\nPrototyping is crucial: It oﬀers a glimpse into the potential challenges\n\nand successes of a mass migration\n\nDetailed planning: Detailed and careful planning for the prototype phase\n\nincreases the chances of a successful mass migration\n\nFeedback and iteration: Using feedback and iterating on the migration\n\nstrategy ensures fewer issues during the actual migration\n\nBy following this guide, you’ll be well-prepared to tackle the\n\nchallenges of migration with a well-tested prototype, minimizing\n\nrisks and ensuring a smoother transition.\n\nConverting ETL processes with big data frameworks\n\nAs the volume and complexity of data continue to grow, traditional\n\nETL processes are struggling to keep pace. Big data frameworks,\n\nsuch as Apache Hadoop, Apache Spark, and AWS, oﬀer a powerful\n\nsolution for migrating and managing big data workloads, enabling\n\norganizations to process, analyze, and extract valuable insights\n\neﬀectively from their vast data repositories.\n\nGetting ready\n\nLet’s discover how AWS can help you overcome the limitations of\n\ntraditional ETL processes and unlock new possibilities for data\n\nanalysis:\n\nChallenges of traditional ETL in big data: Traditional ETL processes face\n\nseveral limitations in handling the massive scale and complexity of big\n\ndata:\n\nScalability: Traditional ETL tools aren’t designed to handle the\n\nmassive scale of big data, leading to performance bottlenecks\n\nand slow processing times\n\nFlexibility: Traditional ETL processes are oen rigid and\n\nin\u0000exible, making it diﬃcult to adapt to the ever-changing\n\nnature of big data\n\nReal-time processing: Traditional ETL processes are batch-\n\noriented, making it challenging to handle real-time data\n\nstreams and perform real-time analytics\n\nIntroducing big data frameworks for ETL migration: Big data\n\nframeworks such as Apache Hadoop, Apache Spark, and AWS oﬀer\n\nseveral advantages for migrating ETL processes:\n\nScalability: Big data frameworks are designed to handle massive\n\ndatasets horizontally by distributing data across multiple nodes,\n\nenabling scalability and parallel processing\n\nFlexibility: Big data frameworks provide \u0000exible data processing\n\ncapabilities, allowing for data ingestion, transformation, and\n\nanalysis in a variety of formats and structures\n\nReal-time processing: Big data frameworks enable real-time\n\ndata processing and analytics, making it possible to respond to\n\ndata changes and events in real time\n\nMigrating ETL processes to big data frameworks on AWS: Converting\n\nETL processes into a big data framework oen involves transitioning\n\nfrom traditional, monolithic ETL tools to scalable, distributed\n\nprocessing systems, such as those provided by big data platforms. e\n\ngoal is to handle larger volumes of data more eﬃciently, ensure\n\nscalability, and reduce processing times.\n\nHow to do it…\n\nWhen migrating ETL processes to a big data framework, you’d\n\ntypically follow these steps:\n\n1. Assessment and planning:\n\nIdentify current ETL processes: Understand current data\n\nsources, transformations, and load processes.\n\nDetermine volume and velocity: Estimate data volume and the\n\nrate of incoming data to choose the right big data tools.\n\nSet goals: What do you want to achieve with the migration? It\n\ncould be faster processing, handling larger datasets, cost\n\nsavings, and so on.\n\n2. Choose a big data framework:\n\nApache Hadoop: An open source framework for distributed\n\nstorage and processing. Its ecosystem (for example, Hive for\n\nSQL-like operations and Pig for scripting) can aid in ETL\n\nprocesses.\n\nApache Spark: It oﬀers fast, in-memory data processing and is\n\nparticularly suited for ETL tasks. It supports various languages,\n\nincluding Python, Scala, and Java.\n\n3. Redesign ETL for scalability:\n\nParallel processing: Instead of sequential processing, design\n\nETL jobs so that they run in parallel, distributing the workload\n\nacross clusters\n\nOptimize transformations: Some transformations might be\n\noptimized or restructured to suit the distributed nature of big\n\ndata frameworks\n\nIncremental loads: Rather than full loads, consider incremental\n\napproaches, processing only new or changed data\n\n4. Data ingestion tools: Use tools such as Apache Kaa for real-time data\n\nstreaming or Apache Flume and Sqoop for batch data ingestion from\n\nvarious sources.\n\n5. Data transformation:\n\nLeverage big data tools: Use Spark’s DataFrame or Dataset API\n\nfor transformations. If you’re using Hadoop, tools such as Hive\n\nand Pig can help.\n\nUser-de\u0000ned functions (UDFs): For complex transformations,\n\nyou can write UDFs tailored to your needs.\n\n6. Data loading:\n\nStorage options: Depending on the nature and usage of the data,\n\nchoose storage options such as Hadoop Distributed File System\n\n(HDFS), cloud storage (for example, Amazon S3), or NoSQL\n\ndatabases\n\nBatch versus real time: Depending on your needs, you can load\n\ndata in batches (using tools such as Sqoop) or in real time\n\n(using Kaa or Spark Streaming)\n\n7. Optimization and performance tuning:\n\nTune cluster con\u0000guration: Adjust con\u0000gurations such as\n\nmemory allocation, number of executors, and so on for optimal\n\nperformance\n\nPartitioning and bucketing: Partitioning divides data into\n\nsubsets, whereas bucketing divides data based on column\n\nvalues, optimizing query performance\n\n8. Testing and validation:\n\nEnd-to-end testing: Ensure that the new ETL process captures\n\ndata correctly, transforms it as expected, and loads it without\n\nissues\n\nPerformance testing: Con\u0000rm that the new process meets the\n\nperformance goals that were set at the beginning\n\n9. Monitoring and maintenance:\n\nMonitoring tools: Use tools such as Apache Ambari or\n\nCloudera Manager to monitor health, performance, and failures\n\nAutomate failover and recovery: Ensure that if nodes fail or\n\nprocesses crash, the system can recover automatically without\n\ndata loss\n\n10. Documentation: Ensure every part of the new ETL process is well-\n\ndocumented, from data sources and transformations to load processes\n\nand optimizations.\n\nMigrating traditional ETL processes to a big data framework is a\n\nsigni\u0000cant endeavor that can oﬀer immense bene\u0000ts in terms of\n\nscalability, performance, and \u0000exibility. While the process requires\n\ncareful planning, design, and testing, the result can lead to more\n\neﬃcient and cost-eﬀective data processing and analysis.\n\nDefining and executing your migration process with Hadoop\n\nDe\u0000ning and executing a Hadoop migration process from on-\n\npremises to AWS involves a complex transition that requires careful\n\nplanning and execution. is process encompasses transferring\n\ndata, applications, and infrastructure to AWS cloud-based Hadoop\n\nclusters. In this recipe, we’ll explore the essential steps and\n\nconsiderations involved in successfully migrating your Hadoop\n\necosystem to AWS, helping you unlock the bene\u0000ts of cloud\n\nscalability, \u0000exibility, and cost-eﬃciency.\n\nGetting ready\n\nBefore embarking on your migration journey, it’s essential to lay a\n\nstrong foundation with the following prerequisites and technical\n\nrequirements:\n\nPrerequisites:\n\nClear objectives: De\u0000ne your migration goals and expected\n\noutcomes. What are you trying to achieve with this migration\n\n(cost savings, improved scalability, increased agility, and so on)?\n\nStakeholder alignment: Ensure buy-in and collaboration from\n\nkey stakeholders across IT, business, and management.\n\nAWS account: Ensure you have an active AWS account with the\n\nnecessary permissions to create and manage resources.\n\nSkilled team: Assemble a team with expertise in AWS services,\n\nnetworking, security, and your existing on-premises\n\ninfrastructure.\n\nTechnical requirements:\n\nInventory and assessment: oroughly document your current\n\non-premises environment, including servers, applications,\n\ndatabases, storage, and network con\u0000gurations. Identify\n\ndependencies and performance requirements.\n\nMigration strategy: Choose the most suitable migration\n\napproach for each application or workload.\n\nRehost (li and shi): Migrate applications without major\n\nchanges, oen using tools such as AWS SMS:\n\nReplatform: Make minor modi\u0000cations to take\n\nadvantage of cloud-native features\n\nRefactor/rearchitect: Redesign applications so that you\n\ncan fully leverage AWS services and optimize for the\n\ncloud\n\nConnectivity: Establish secure and reliable network\n\nconnectivity between your on-premises environment and AWS\n\nusing options such as AWS Direct Connect or a VPN.\n\nMigration tools: Select and con\u0000gure appropriate AWS\n\nmigration tools, such as AWS Migration Hub, DMS, and\n\nApplication Discovery Service.\n\nHow to do it…\n\nWhen de\u0000ning migration processes for a Hadoop framework,\n\nfollow these steps:\n\n1. Planning phase:\n\nAssessment:\n\nAnalyze existing Hadoop infrastructure, data size,\n\nworkloads, and dependencies\n\nEvaluate AWS services for compatibility and cost-\n\neﬀectiveness (for example, EMR, S3, EC2, and Glue)\n\nConsider performance, scalability, security, and\n\ngovernance requirements\n\nDesign:\n\nChoose appropriate AWS services and architecture (for\n\nexample, multi-cluster EMR, hybrid setup, and so on)\n\nPlan data transfer and storage strategies (for example,\n\nS3 buckets and data partitioning)\n\nAddress network connectivity and security measures\n\nDetermine monitoring and logging tools for post-\n\nmigration analysis\n\nCreate a detailed migration timeline and rollback plan\n\n2. Execution phase:\n\nAWS environment setup:\n\nCreate AWS accounts and con\u0000gure IAM roles for\n\naccess control\n\nSet up the necessary VPCs, subnets, security groups,\n\nand network con\u0000gurations",
      "page_number": 687
    },
    {
      "number": 35,
      "title": "Segment 35 (pages 709-731)",
      "start_page": 709,
      "end_page": 731,
      "detection_method": "synthetic",
      "content": "Provision EMR clusters with appropriate hardware and\n\nsoware con\u0000gurations\n\nData migration:\n\nTransfer data from on-premises Hadoop storage to S3\n\nusing tools such as DistCp or S3DistCp\n\nOptimize transfer speed and validate data integrity\n\nduring the transfer\n\nJob migration:\n\nRefactor or modify Hadoop jobs so that they run on\n\nEMR while considering any syntax or API changes\n\nTest modi\u0000ed jobs with sample data in the AWS\n\nenvironment\n\nTesting and validation:\n\noroughly test migrated workloads on EMR to ensure\n\nfunctionality and performance\n\nValidate data integrity and consistency post-migration\n\nMonitor resource usage and costs for optimization\n\n3. Post-migration phase:\n\nMonitoring and optimization:\n\nContinuously monitor EMR clusters for performance,\n\nresource usage, and costs\n\nImplement cost optimization strategies (for example,\n\nspot instances, auto-scaling, and so on)\n\nMaintenance and security:\n\nApply security patches and updates to AWS services\n\nand EMR clusters\n\nRegularly review security con\u0000gurations and access\n\ncontrols\n\nTroubleshooting: Address any issues that arise while leveraging\n\nAWS support and documentation\n\nData security: Encrypt sensitive data in transit and at rest while\n\nadhering to compliance requirements\n\nCost optimization: Leverage AWS cost management tools and\n\nexplore spot instances and reserved pricing\n\nPerformance optimization: Optimize EMR con\u0000gurations and\n\nconsider caching and data partitioning techniques\n\nGovernance: Implement data access control and auditing\n\nmechanisms for compliance\n\nNOTE\n\nEvery migration is unique, so tailor your approach so that it ﬁts your speciﬁc\n\nrequirements and challenges. Don’t hesitate to seek assistance from AWS\n\nexperts or certiﬁed partners if needed. With the right strategy and execution,\n\nyour AWS migration can be a smooth and successful journey.\n\nMigrating the existing Hadoop security authentication and authorization processes\n\nMigrating your security domain from on-premises to the AWS\n\ncloud is a pivotal transformation that can enhance the resilience,\n\nscalability, and manageability of your security infrastructure. is\n\nprocess involves transitioning critical security components and\n\nprotocols to the cloud environment, ensuring that your\n\norganization can harness the power of AWS while maintaining\n\nrobust security measures. In this context, we’ll explore the crucial\n\nsteps and considerations involved in successfully migrating your\n\nsecurity domain to AWS, enabling you to safeguard your digital\n\nassets eﬀectively in the cloud.\n\ne steps to migrate security authentication and authorization from\n\nHadoop to AWS are as follows:\n\n1. Understand your current setup: Identify the security mechanisms\n\ncurrently in place in your Hadoop environment. is could include\n\nKerberos for authentication, Apache Ranger or Apache Sentry for\n\nauthorization, and encryption methods.\n\n2. Perform authentication and authorization: Assess your Hadoop\n\necosystem’s components (such as HDFS, Hive, Spark, and so on) and\n\ntheir security con\u0000gurations:\n\nAuthentication mechanisms: Identify the methods being used\n\n(Kerberos, LDAP, and so on)\n\nAuthorization mechanisms: Determine how permissions are\n\ngranted and enforced (HDFS ACLs, Ranger, and so on)\n\nIntegrations: Map any external systems that interact with\n\nHadoop security\n\n3. Choose AWS services:\n\nIAM: Centrally manages users, groups, and permissions for\n\nAWS services and resources\n\nS3 access points: Create access points with distinct permissions\n\nfor S3 buckets, simplifying access management\n\nLake Formation: Optionally, create a secure data lake with \u0000ne-\n\ngrained access control and auditing\n\nEncryption:\n\nUnderstand how encryption is handled in AWS using\n\nservices such as AWS KMS and AWS Certi\u0000cate\n\nManager.\n\nPlan how to migrate data securely. AWS oﬀers services\n\nsuch as AWS DataSync, AWS Transfer Family, and\n\nAWS Snowball for large-scale data migrations.\n\nEnsure data is encrypted during transit and at rest in\n\nAWS.\n\n4. Integrate with AWS security services:\n\nIntegrate your Hadoop applications with AWS security services.\n\nis might require some recon\u0000guration or code changes.\n\nUse AWS IAM for managing users and permissions. You can\n\ncreate IAM roles and policies that align with your existing\n\nHadoop permissions.\n\n5. Testing and validation:\n\noroughly test the security con\u0000gurations in a staging\n\nenvironment before moving to production\n\nValidate that authentication and authorization work as expected\n\nand that data is accessed securely\n\nUtilize AWS CloudTrail and AWS Con\u0000g for monitoring and\n\nauditing security con\u0000gurations\n\nContinuously monitor for any security threats or vulnerabilities\n\n6. Migrate authentication:\n\nAWS credential providers: Con\u0000gure Hadoop so that it uses\n\nAWS credentials for S3 access via the following methods:\n\nAccess keys and secret keys\n\nIAM roles for EC2 instances\n\nTemporary security credentials\n\nInstance pro\u0000le credentials provider: For EC2 instances, use\n\nIAM roles for authentication\n\n7. Migrate authorization:\n\nIAM policies: De\u0000ne permissions for S3 buckets and objects\n\nusing IAM policies\n\nS3 access points: Simplify permission management with access\n\npoints for speci\u0000c use cases\n\nLake Formation: For a data lake, manage permissions centrally\n\nwith Lake Formation\n\n8. Map existing permissions:\n\nTranslate existing Hadoop permissions into IAM policies or S3\n\naccess points\n\nConsider using tools such as Apache Ranger for policy\n\nmanagement across environments\n\n9. Migrate data:\n\nTransfer data to S3, preserving existing permissions or mapping\n\nthem to IAM policies\n\nEncrypt sensitive data at rest and in transit\n\n10. Others:\n\nEncryption: Use S3 encryption features for data protection\n\nAuditing: Enable AWS CloudTrail for logging and auditing of\n\nS3 access\n\nird-party tools: Consider tools for managing authentication\n\nand authorization across Hadoop and AWS (for example,\n\nApache Ranger and Cloudera Navigator)\n\nAccess patterns: Analyze access patterns to optimize S3 access\n\npolicies and access points\n\nCompliance requirements: Adhere to regulatory or industry-\n\nspeci\u0000c compliance requirements\n\nGetting ready\n\nTo ensure a successful migration, it’s essential to understand the\n\ntechnical and security requirements outlined here.\n\noroughly document your on-premises Hadoop security setup,\n\nincluding the following aspects:\n\nAuthentication methods: How users and services authenticate (for\n\nexample, Kerberos, LDAP, and Active Directory).\n\nAuthorization mechanisms: How access to data and resources is\n\ncontrolled (for example, Sentry and Ranger).\n\nData encryption: Any encryption methods in place for data at rest and in\n\ntransit.\n\nAWS account and IAM: An active AWS account with necessary IAM\n\npermissions to create and manage resources. Ensure you have familiarity\n\nwith IAM roles and policies for \u0000ne-grained access control.\n\nTarget AWS services: Knowledge of relevant AWS services for Hadoop\n\nmigration (for example, EMR, S3, Glue, and Lake Formation). You must\n\nalso understand how security integrates with these services.\n\ne technical requirements are as follows:\n\nNetwork connectivity: Secure network connectivity between your on-\n\npremises environment and AWS (for example, Direct Connect and\n\nVPN)\n\nMigration tools: Tools such as AWS SCT, DMS, and DataSync for data\n\nand metadata transfer\n\ne AWS CLI or SDK: For interacting with AWS services\n\nprogrammatically\n\nBackup and recovery: Ensure proper backups of your on-premises\n\nHadoop security con\u0000gurations and data\n\nHow to do it…\n\nWhen migrating existing Hadoop security authentication and\n\nauthorization, you’d typically follow these steps:\n\n1. Planning and design:\n\nMap security controls: Identify the equivalent AWS security\n\nservices and con\u0000gurations to replicate your on-premises\n\nsecurity model\n\nAuthentication: Consider AWS IAM, AWS SSO, or integrating\n\nwith existing identity providers using SAML or OpenID\n\nConnect\n\nAuthorization: Evaluate AWS Lake Formation for \u0000ne-grained\n\naccess control on data stored in S3, or continue using tools such\n\nas Ranger if you’re migrating them to EMR\n\nEncryption: Utilize server-side encryption with Amazon S3 or\n\nKMS for data at rest and in transit\n\nProvide a migration strategy:\n\nPhased approach: Migrate components gradually,\n\nstarting with non-critical data and workloads\n\nParallel testing: Run your migrated environment in\n\nparallel with the on-premises setup for testing and\n\nvalidation purposes\n\n2. Implementation:\n\nIAM:\n\nCreate IAM roles: Create IAM roles for users, services,\n\nand applications, granting appropriate permissions\n\nCon\u0000gure authentication: Set up authentication\n\nmechanisms such as IAM users, federated identities, or\n\nSSO integration\n\nAWS Lake Formation:\n\nFine-grained control: Set up Lake Formation to\n\nmanage permissions on data stored in S3 data lakes\n\nRanger (optional): If you’re using Ranger, migrate it to\n\nyour EMR cluster, and con\u0000gure your policies\n\nEncryption:\n\nS3 encryption: Enable server-side encryption (SSE-S3\n\nor SSE-KMS) for S3 buckets that store sensitive data\n\nEMR encryption: Con\u0000gure encryption for EBS\n\nvolumes attached to EMR instances\n\nIn-transit encryption: Ensure secure communication\n\nchannels between your on-premises environment and\n\nAWS\n\n3. Testing and validation:\n\norough testing: Test authentication and authorization\n\nmechanisms in the AWS environment\n\nSecurity review: Conduct security assessments and penetration\n\ntesting to identify vulnerabilities\n\n4. Cutover and go-live:\n\nUser communication: Communicate the migration plan and\n\nany changes in authentication or access procedures to users\n\nGradual cutover: Gradually shi workloads to the AWS\n\nenvironment, monitoring them closely for any issues\n\nRollback plan: Have a well-de\u0000ned rollback plan in case of\n\nunforeseen problems during the cutover\n\n5. Post-migration:\n\nMonitoring and auditing: Continuously monitor access logs and\n\nsecurity events\n\nRe\u0000nement: Re\u0000ne security con\u0000gurations and policies based\n\non real-world usage patterns and evolving threats\n\n6. Decommission on-premises: Once you’re con\u0000dent in the AWS setup,\n\ndecommission the on-premises Hadoop security infrastructure.\n\nOceanofPDF.com\n\n12 Harnessing the Power of AWS for Seamless Data Warehouse Migration\n\nIn the ever-evolving landscape of data management, the migration\n\nof your data warehouse to the cloud is an essential step toward\n\nunlocking scalability, agility, and cost eﬃciency. As we embark on\n\nthis journey, we’ll examine the powerful tools that AWS oﬀers to\n\nstreamline and optimize this transition. In this chapter, we’ll explore\n\nthree key services: the AWS Schema Conversion Tool (SCT) and\n\nAWS Database Migration Service (DMS), as well as the AWS Snow\n\nFamily. Each of these tools plays a crucial role in ensuring a\n\nseamless and successful migration of your valuable data assets to\n\nthe AWS cloud.\n\nWe will cover the following recipes:\n\nCreating SCT migration assessment report with AWS SCT\n\nExtracting Data with AWS DMS\n\nLive example – migrating an Oracle database from a local laptop to AWS\n\nRDS using AWS SCT\n\nLeveraging AWS Snow Family for large-scale data migration\n\nTechnical requirements\n\nMake sure you have an active AWS account. If you don’t already\n\nhave one, sign up for an AWS account at\n\nhttps://aws.amazon.com/resources/create-account/ before\n\nproceeding. You can access the code for this project in the GitHub\n\nrepository at https://github.com/PacktPublishing/Data-\n\nEngineering-with-AWS-Cookbook/tree/main/Chapter12.\n\nCreating SCT migration assessment report with AWS SCT\n\nAWS SCT is a valuable utility for automating the migration of\n\ndatabase schemas between diﬀerent database engines. Whether\n\nyou’re moving from an on-premises database to a cloud-based\n\nsolution or converting between diﬀerent types of databases, SCT\n\nsimpli\u0000es the process by analyzing the source schema and\n\ngenerating a compatible schema for the target database. SCT\n\nautomates much of the process. It analyzes the source database\n\nschema and converts it into a format compatible with the target\n\ndatabase as shown, ensuring a smooth transition with minimal\n\nmanual intervention.\n\nUnderstand the data migration journey with the following \u0000ow\n\ndiagram outlining the use of AWS SCT tools:\n\nFigure 12.1 – SCT for data migration\n\nis recipe demonstrates how to migrate an on-premises database\n\n(Source) to AWS (Target) using the AWS SCT. We’ll focus on\n\nconverting the database schema and generating comprehensive\n\nreports to streamline the migration process.\n\nFigure 12.2 – SCT ﬂow for data migration\n\nWhen you run AWS SCT, it assesses your source database and\n\nprovides a detailed report on what can be automatically converted\n\nand what needs manual intervention.\n\nGetting ready\n\nBefore you begin generating your migration assessment report,\n\nensure you have the following in place:\n\nAWS account:\n\nActive AWS account: You need an active AWS account to use\n\nAWS SCT, as it’s an AWS service.\n\nSource and target database connectivity:\n\nEnsure that your source and target databases are reachable. You\n\nneed to con\u0000gure network settings (for example, VPC and\n\n\u0000rewall rules) to allow SCT to connect to both the source and\n\ntarget databases.\n\nSupported databases include Oracle, Microso SQL Server,\n\nMySQL, MariaDB, PostgreSQL, Amazon Aurora, and many\n\nothers.\n\nAWS SCT installation:\n\nDownload and install the AWS SCT application on your local\n\nmachine (Windows, Linux, or macOS). It requires Java Runtime\n\nEnvironment (JRE) to run.\n\nEnsure that your machine meets these system requirements:\n\nAt least 4 GB of RAM\n\nAt least 500 MB of disk space\n\nAWS IAM role and permissions:\n\nCreate or use an AWS Identity and Access Management (IAM)\n\nrole that has the required permissions for SCT to access\n\nresources in AWS, such as AWS Glue, Amazon Relational\n\nDatabase Service (RDS), and Amazon S3.\n\ne IAM role should have permissions for AWS Glue, Amazon\n\nS3, Amazon Redshi, or any other target service you are using.\n\nSchema conversion project setup:\n\nDe\u0000ne your source and target database types to con\u0000gure the\n\ncorrect conversion project within SCT.\n\nMake sure SCT is compatible with both the source and target\n\ndatabase versions.\n\nAWS DMS: If you are planning to use AWS DMS for data migration aer\n\nschema conversion, ensure that the AWS DMS endpoints are properly\n\nset up, and that your replication instance has access to both the source\n\nand target databases.\n\nSuﬃcient disk space for SCT project \u0000les: SCT generates project \u0000les\n\nduring the schema conversion process, which can require signi\u0000cant\n\nstorage space depending on the size of the schema and data to be\n\nconverted.\n\nSource database privileges: Ensure that the user accounts for both the\n\nsource and target databases have the necessary privileges to access\n\nschemas, tables, and other database objects for schema conversion.\n\nSource and target databases:\n\nDatabase details: Have the connection details (hostname/IP,\n\nport, username, and password) ready for both your source and\n\ntarget databases.\n\nCompatibility: Ensure that the source and target database\n\nplatforms are supported by AWS SCT. Consult the AWS SCT\n\ndocumentation\n\n(https://docs.aws.amazon.com/SchemaConversionTool/latest/u\n\nserguide/CHAP_Welcome.html) for the latest compatibility\n\ninformation.\n\nDatabase access: Make sure you have appropriate access\n\npermissions to the source and target databases to run queries,\n\nextract schemas, and potentially modify data (if necessary).\n\nSoware and drivers:\n\nAWS SCT installation: Download and install the latest version\n\nof AWS SCT on a machine that has network access to both your\n\nsource and target databases:\n\nDownload for Windows:\n\nhttps://s3.amazonaws.com/publicsctdownload/Windo\n\nws/aws-schema-conversion-tool-1.0.latest.zip\n\nDownload for Linux:\n\nhttps://s3.amazonaws.com/publicsctdownload/Fedora/\n\naws-schema-conversion-tool-1.0.latest.zip\n\nDownload for Ubuntu:\n\nhttps://s3.amazonaws.com/publicsctdownload/Ubuntu\n\n/aws-schema-conversion-tool-1.0.latest.zip\n\nJDBC drivers: Install the JDBC drivers for both your source and\n\ntarget database platforms. ese drivers are essential for AWS\n\nSCT to connect to and interact with your databases:\n\nDownload the JDBC driver for your Oracle database:\n\nhttps://www.oracle.com/jdbc\n\nDownload the PostgreSQL driver:\n\nhttps://jdbc.postgresql.org/download/postgresql-\n\n42.2.19.jar\n\nFor other JDBC drivers, please check the database\n\ndriver page. All supported database drivers are listed in\n\nthe AWS SCT user guide:\n\nhttps://docs.aws.amazon.com/SchemaConversionTool/\n\nlatest/userguide/CHAP_Installing.html#CHAP_Install\n\ning.JDBCDrivers\n\nNetwork connectivity: Verify that the machine running\n\nAWS SCT can connect to both your source and target\n\ndatabases over the network. is may involve adjusting\n\n\u0000rewall rules or security group settings.\n\nSchema understanding:\n\nSource schema: Have a good understanding of your source\n\ndatabase schema, including tables, columns, relationships, data\n\ntypes, and any custom objects or stored procedures.\n\nTarget schema (optional): If you’re migrating to a speci\u0000c target\n\ndatabase, familiarize yourself with its schema as well to\n\nanticipate potential mapping challenges.\n\nHow to do it…\n\nWith all the necessary setup in place, we are now ready to begin\n\ncon\u0000guring the AWS SCT tool.\n\nConfiguring JDBC drivers in AWS SCT\n\nCon\u0000guring JDBC drivers in AWS SCT involves several steps.\n\nJDBC drivers are essential for AWS SCT to connect to your source\n\nand target databases. Here’s a general guide to help you con\u0000gure\n\nJDBC drivers in AWS SCT global settings:\n\n1. Download the JDBC drivers: First, you need to download the\n\nappropriate JDBC drivers for your source and target databases. ese\n\ndrivers are typically available on the database vendors’ websites. For\n\nexample, if you are converting a schema from an Oracle database, you\n\nwould download the Oracle JDBC driver from Oracle’s website.\n\n2. Launch AWS SCT: Open the AWS SCT on your computer. Once you\n\nhave installed and opened SCT, you will see the following screen:\n\nFigure 12.3 – Opening SCT after installation\n\n3. Access Global Settings: In the AWS SCT, go to Global Settings on the\n\nSettings menu. is is usually found in the top menu bar. Locate the\n\ndriver con\u0000guration. Inside Global Settings, look for a tab or section\n\nrelated to JDBC drivers. is might be labeled something such as JDBC\n\nDrivers, Driver Con\u0000guration, or similar.\n\n4. Add the JDBC drivers: In the JDBC drivers section, you will likely see\n\ndiﬀerent categories for diﬀerent database types (for example, Oracle,\n\nMySQL, and PostgreSQL). Click on the Add or Install button next to the\n\ndatabase type for which you downloaded the JDBC driver. Navigate to\n\nthe location where you downloaded the JDBC driver \u0000le, select it, and\n\nthen open or install it as prompted.\n\n5. Verify or test the driver: Aer adding the driver, there might be an\n\noption to test the driver to ensure that it’s correctly installed and\n\nfunctional. Use this feature to verify the setup.\n\n6. Save the Con\u0000guration: Once the drivers are added and tested, save your\n\ncon\u0000guration settings.\n\n7. Restart AWS SCT (if required): In some cases, you may need to restart\n\nAWS SCT for the changes to take eﬀect.\n\n8. Proceed with database connection: Now, when setting up a new\n\nconnection to a database in AWS SCT, it should use the JDBC driver\n\nyou’ve con\u0000gured.\n\nCHECKING THE COMPATIBILITY\n\nEnsure the JDBC driver version is compatible with your version of AWS SCT.\n\nUse AWS SCT documentation and your database’s documentation for any\n\nspeciﬁc instructions or requirements.\n\n9. Add source and target connections: Adding source and target\n\nconnections in AWS SCT is a critical step in the database migration\n\nprocess. is allows AWS SCT to analyze your source database and\n\ncreate a corresponding schema in your target database. Here’s how you\n\ncan add these connections:\n\nI. Adding a source connection: Open AWS SCT and launch it on\n\nyour computer:\n\nFigure 12.4 – AWS SCT\n\ni. Create a new project or open an existing one: If you haven’t already,\n\ncreate a new project by going to File | New project or open an existing\n\nproject.\n\nii. Initiate adding a source connection: Go to Database | Add source or\n\nsimply click on the Add source button if it’s available on your toolbar.\n\niii. Choose the source database type: Select the type of your source database\n\n(for example, Oracle, Microso SQL Server, and MySQL) from the list\n\nprovided.\n\niv. Enter connection details: Fill in the necessary details such as server\n\nname, port, database name, user, and password. ese details are speci\u0000c\n\nto your database environment.\n\nFigure 12.5 – Adding data source in SCT\n\nOnce you add the data source, it will be visible in the SCT editor.\n\nTest the connection: Click on the Test connection button to ensure that\n\nAWS SCT can successfully connect to your source database.\n\nTroubleshoot any connection issues if the test fails.\n\nFinish and save: Once the connection is successful, click on OK or Save\n\nto store the source database connection.\n\nII. Adding a target connection:",
      "page_number": 709
    },
    {
      "number": 36,
      "title": "Segment 36 (pages 732-749)",
      "start_page": 732,
      "end_page": 749,
      "detection_method": "synthetic",
      "content": "i. Initiate adding a target connection: Go to Database | Add target\n\nor use the Add target button.\n\nii. Select the target database type: Choose the type of your target\n\ndatabase. is could be an AWS service such as Amazon RDS,\n\nAmazon Redshi, Aurora, or another database type.\n\niii. Input target connection details: Just like with the source, input\n\nthe required details for your target database: server name, port,\n\ndatabase name, user, and password.\n\niv. Test the target connection: Use the Test connection feature to\n\nverify connectivity to the target database.\n\nv. Save the target connection: Once the test is successful, save the\n\nconnection by clicking on Save.\n\nNOTE\n\nMake sure you meet the following conditions:\n\nJDBC drivers: Ensure you have the correct JDBC drivers installed for both\n\nthe source and target databases before attempting to connect.\n\nNetwork accessibility: Make sure that your source and target databases\n\nare accessible from where AWS SCT is running. This might involve\n\nconﬁguring network settings or VPNs.\n\nSecurity and permissions: Verify that the user credentials used for both\n\nsource and target have the necessary permissions for the migration\n\nprocess.\n\nFirewall and security groups: Adjust ﬁrewall settings or security groups\n\nas needed to allow AWS SCT to access the databases.\n\nOnce you have successfully added both the source and target\n\nconnections, you can proceed with the schema conversion process\n\nand any other migration activities using AWS SCT.\n\nMapping source and target schemas\n\nMapping source and target schemas in the AWS SCT is a crucial\n\nstep in the database migration process. is process involves\n\naligning the schemas of your source database with those in your\n\ntarget database, which helps ensure a smooth and accurate transfer\n\nof data. Here’s how you can map source and target schemas in AWS\n\nSCT:\n\n1. Open your project in AWS SCT: Start by opening your project in the\n\nAWS SCT where you have already established the source and target\n\nconnections.\n\nI. Access the schema mapping interface: Look for an option to\n\nview or manage your schemas, oen found under a menu\n\nlabeled something such as Schema or Database.\n\nII. Select your source schema: In the schema view, you’ll see a list\n\nof schemas from your source database. Select the schema you\n\nwant to migrate to.\n\nIII. Initiate the mapping process: Right-click on the selected\n\nschema and look for an option to map it to a target schema.\n\nis might be labeled as Map to target or something similar.\n\nIV. Choose the corresponding target schema:\n\ni. A list of available target schemas (from your target\n\ndatabase) will be displayed. Select the schema in the\n\ntarget database that corresponds to or will host the\n\nschema from the source database.\n\nii. If the target schema does not exist, you may have an\n\noption to create a new schema.\n\nV. Con\u0000gure schema mapping options: Depending on the\n\ncomplexity of your database, you might have additional options\n\nto con\u0000gure during the mapping. is can include data type\n\nmappings, key conversions, and so on.\n\ni. Review and adjust mappings: Carefully review the\n\nproposed schema mappings. AWS SCT might provide\n\nsuggestions or automatic mappings, but it’s essential to\n\nverify that these are correct and adjust as necessary.\n\nii. Apply and save the mappings: Once you are satis\u0000ed\n\nwith the mappings, apply and save them.\n\niii. Repeat for additional schemas: If you have multiple\n\nschemas to migrate, repeat this process for each one.\n\n2. Generate an assessment report:\n\nI. In AWS SCT, go to the View menu and select Main view.\n\nII. In the le panel, choose the schema objects you want to include\n\nin the report. You can select individual objects, schemas, or the\n\nentire database.\n\nIII. Go to the View menu and select Assessment Report view.\n\ne report will provide a summary of the following:\n\nConversion success: Objects converted successfully\n\nAction items: Objects requiring manual intervention or code\n\nchanges\n\nComplexity estimates: Eﬀort levels for addressing action items\n\nRecommendations: Suggestions for optimization and best\n\npractices\n\n3. Save the report: You have the following options to save the report:\n\nExport options: You can save the assessment report as a PDF or\n\na CSV \u0000le\n\nPDF format: is provides a comprehensive, formatted report\n\nsuitable for sharing\n\nCSV format: is allows you to analyze the report data in a\n\nspreadsheet\n\n4. Review the assessment report: In a multi-server project, the assessment\n\nreport provides consolidated views of all the mapped schemas and\n\ntargets from a single source database.\n\ne following screenshot illustrates a sample assessment report\n\nfrom a multi-server project. is report consolidates\n\ninformation from a single source database, mapping various\n\nschemas to diﬀerent target databases such as Aurora PostgreSQL\n\nand Amazon Redshi. e summary section oﬀers a\n\ncomprehensive overview of all target databases, allowing you to\n\nquickly select a speci\u0000c server or scroll through detailed\n\ninformation for each.\n\nFigure 12.6 – Viewing report in SCT\n\ne details report is grouped by each target database. You can save\n\nthis report in CSV and PDF format for viewing outside AWS SCT\n\nor sharing with others.\n\nFigure 12.7 – Report in SCT\n\nFigure 12.8 – Report details in SCT\n\n5. Convert schemas: When you’re ready to convert your schemas, complete\n\nthe following steps:\n\nI. Select the desired objects from the source tree (right-click).\n\nII. Choose Convert schema.\n\nIII. AWS SCT doesn’t apply the converted code to the target\n\ndatabase directly. You can view and edit the converted code\n\ninside AWS SCT or save it to a SQL script.\n\nIV. To view or edit the converted code inside AWS SCT, choose an\n\nobject in the source tree. e converted code is displayed in the\n\nlower-center panel.\n\nV. To save the converted code to a SQL script, choose the intended\n\nschema on the target tree (right-click), and choose Save as SQL.\n\nVI. To apply the converted code to the database, choose the\n\nintended schema on the target tree (right-click), and choose\n\nApply to database. is option is only available for non-virtual\n\ntargets.\n\nSee also\n\nWhat is the AWS SCT?:\n\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/\n\nCHAP_Source.htm\n\nCategory: AWS SCT:\n\nhttps://aws.amazon.com/blogs/database/category/database/aws-schema-\n\nconversion-tool/\n\nExtracting data with AWS DMS\n\nAWS DMS is a versatile tool designed to simplify database\n\nmigrations and data transformations. While it’s primarily known for\n\ndatabase migrations, it’s also a powerful tool for extracting data\n\nfrom your source databases and loading it into other destinations,\n\nsuch as Amazon S3, for further processing, analysis, or archival.\n\nIn this recipe, you’ll learn how to use AWS DMS to extract data and\n\nmigrate it to the cloud, along with converting database schemas\n\nusing AWS SCT. DMS handles the extraction and migration of your\n\ndatabase data, while SCT helps convert the schema when moving\n\nbetween diﬀerent database engines. By the end of this recipe, you’ll\n\nhave a streamlined approach to converting database structures and\n\nextracting data eﬃciently, ensuring a smooth transition to AWS\n\nservices such as Amazon RDS.\n\nIn this recipe, we will explore how to extract data from an on-\n\npremises database and migrate it to AWS using DMS. is process\n\nis divided into two parts:\n\nAWS SCT for database schema conversion (we already covered it in the\n\nprevious recipe, Creating SCT migration assessment report with AWS\n\nSCT)\n\nAWS DMS for data migration\n\ne following diagram illustrates how on-premises data can be\n\nmigrated to the AWS cloud using an internet gateway, AWS SCT,\n\nand AWS DMS, ultimately landing in an Amazon RDS instance:\n\nFigure 12.9 – AWS DMS architecture\n\nLet’s get started!\n\nGetting ready\n\nBefore diving into data extraction with AWS DMS, ensure you have\n\nthe following in place:\n\nActive AWS account: You’ll need an active AWS account with suﬃcient\n\npermissions to create and manage DMS resources. If you don’t have an\n\naccount, you can create one for free at https://aws.amazon.com.\n\nSource database:\n\nSupported database: Verify that your source database is\n\ncompatible with AWS DMS. DMS supports a wide range of\n\ndatabase engines, including popular options such as MySQL,\n\nPostgreSQL, Oracle, Microso SQL Server, and more. You can\n\n\u0000nd the full list of supported engines in the AWS DMS\n\ndocumentation (https://docs.aws.amazon.com/dms/).\n\nConnectivity: Ensure that your DMS replication instance can\n\nconnect to your source database. is may involve adjusting\n\nnetwork settings or security groups to allow inbound\n\nconnections.\n\nCredentials: Have valid database credentials (username and\n\npassword) with suﬃcient permissions to read the data you want\n\nto extract.\n\nAmazon S3 bucket: Create an Amazon S3 bucket in the same AWS\n\nregion as your DMS replication instance to store the extracted data.\n\nPermissions: Ensure that your DMS replication instance has\n\npermission to write to the S3 bucket.\n\nIAM roles: To securely interact with your resources, you’ll need to create\n\nand con\u0000gure two IAM roles:\n\nDMS replication instance role: is role allows your DMS\n\nreplication instance to access your source and target resources.\n\nAttach the following managed policies to this role:\n\nAmazonS3FullAccess: is grants full access to the S3\n\nbucket for storing extracted data.\n\nAmazonDMSReplicationInstance: is provides the\n\nnecessary permissions for DMS to operate.\n\nDMS task role: If you’re accessing your source database through\n\na VPC, create a custom IAM role and attach policies that grant\n\naccess to your VPC and the source database.\n\nHow to do it…\n\n1. Create a DMS replication instance: An AWS DMS replication instance is\n\na key component in the DMS that performs the actual data migration\n\ntasks. It provides the necessary compute and memory resources for AWS\n\nDMS to migrate data from the source database to the target database.\n\ne replication instance handles the migration process, including\n\nextracting data, transforming it (if needed), and loading it into the\n\ntarget. e steps are as follows:\n\nI. Open the AWS DMS console at https://us-east-\n\n1.console.aws.amazon.com/dms/v2/home?region=us-east-\n\n1#\u0000rstRun.\n\nII. Click on Create replication instance.\n\nFigure 12.10 – DMS replication instance\n\nIII. Provide a name, choose an instance class based on your data volume,\n\nand select your VPC (if applicable):\n\nParameter\n\nValue\n\nName\n\nOracle-to-aurora-replication-\n\ninstance\n\nDescription\n\nOracle to Aurora DMS replication\n\ninstance\n\nInstance Class\n\ndms.c4.xlarge\n\nReplication engine\n\nLatest\n\nversion\n\nVPC\n\nYour VPC id\n\nParameter\n\nValue\n\nAllocated storage (GB)\n\nLeave default\n\nMulti-AZ\n\nUnchecked\n\nPublicly accessible\n\nUnchecked\n\nTable 12.1 – Replication instance conﬁguration\n\nI. Select the Publicly accessible option if your source database is in a\n\ndiﬀerent network.\n\nII. Assign the DMS replication instance role you created.\n\n1. Create a source endpoint: AWS DMS source endpoints are con\u0000gurations\n\nthat de\u0000ne the source database systems from which you want to migrate\n\ndata. ese endpoints specify the details of your on-premises or cloud-\n\nbased databases, such as the database type, hostname, port number, and\n\nauthentication credentials. It’s easy to create your own source endpoint.\n\nIn the DMS console, navigate to Endpoints and click on Create\n\nendpoint.\n\nFigure 12.11 – DMS endpoints\n\n3. Select your source database engine: When using AWS DMS to migrate\n\ndata, the \u0000rst critical step is selecting your source database engine. e\n\nsource database is the system from which you’re migrating data to AWS.\n\nAWS DMS supports a wide range of both commercial and open source\n\ndatabase engines, allowing for migrations between heterogeneous or\n\nhomogeneous systems. Here’s what to do:\n\nI. Provide the endpoint identi\u0000er, server name or IP address, port,\n\nusername, and password.\n\nII. Test the connection to ensure it’s successful.\n\n4. Create target endpoint: In AWS DMS, the target endpoint refers to the\n\ndestination database where your data will be migrated. It is the system or\n\nenvironment that receives the data from the source database during the\n\nmigration process. Here are the steps:\n\nI. Click on Create endpoint again.\n\nII. Select S3 as the target endpoint type.\n\nIII. Provide the endpoint identi\u0000er and S3 bucket name for storing\n\nextracted data.\n\nIV. Specify the output format (for example, CSV and Parquet) and\n\ncon\u0000gure any data format or compression settings:\n\nFigure 12.12 – Create endpoints\n\nV. Once the information has been entered, click on Run test under Test\n\nendpoint connection (optional). When the status turns to successful,\n\nclick on Create endpoint.\n\n5. Create DMS task:\n\nI. Go to Tasks and click on Create task.\n\nII. Provide a task identi\u0000er and select the replication instance,\n\nsource endpoint, and target endpoint you created.\n\nIII. Choose Migrate existing data as the migration type.\n\nIV. Select the tables or schemas you want to extract.\n\nV. Optionally, con\u0000gure task settings such as logging, error\n\nhandling, and validation.\n\nFigure 12.13 – DMS task\n\n6. Start task:\n\nI. Review the task settings and click on Start task.\n\nII. AWS DMS will initiate the extraction process, transferring data\n\nfrom your source database to the S3 bucket.",
      "page_number": 732
    },
    {
      "number": 37,
      "title": "Segment 37 (pages 751-770)",
      "start_page": 751,
      "end_page": 770,
      "detection_method": "synthetic",
      "content": "smooth transition of both data and schema, while maintaining high\n\navailability and data integrity. As you plan your migration, AWS\n\nDMS empowers you to scale your data workloads eﬀortlessly,\n\noptimizing performance and cost in the cloud.\n\nSee also\n\nMigrating an On-Premises Oracle Database to Amazon Aurora MySQL:\n\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-on-\n\npremoracle2aurora.html\n\nLive example – migrating an Oracle database from a local laptop to AWS RDS using AWS SCT\n\nIn this recipe, we will walk through the process of migrating an\n\nOracle database installed on your local laptop to AWS RDS using\n\nAWS SCT. is involves installing Oracle on your laptop, setting up\n\nthe AWS RDS Oracle instance, using SCT for schema conversion,\n\nand completing the migration process with AWS DMS.\n\nGetting ready\n\nBefore diving into data extraction with AWS DMS, ensure you have\n\nthe following in place:\n\nAWS account with the necessary permissions (RDS, DMS, SCT, and\n\nnetworking)\n\nProper networking con\u0000gurations to ensure connectivity between your\n\nlocal machine and the AWS RDS instance\n\nHow to do it…\n\nInstalling the Oracle database on your laptop\n\n1. Download the Oracle database installer:\n\nI. Visit the Oracle website\n\n(https://www.oracle.com/database/technologies/oracle-\n\ndatabase-soware-downloads.html).\n\nII. Download the database soware (choose the version suitable\n\nfor your OS such as Windows or macOS).\n\n2. Install Oracle database: Follow the installation instructions provided by\n\nOracle to install the database on your laptop. You’ll need to do the\n\nfollowing:\n\nI. Set up a System Identi\u0000er (SID) for your Oracle instance.\n\nII. Con\u0000gure the listener for database connections (usually port\n\n1521).\n\nIII. Create a database username and password for access (such as\n\nsystem or another DBA account).\n\n3. Verify Oracle database: Aer installation, verify that the Oracle Database\n\nis running correctly by logging into the database using Oracle SQL\n\nDeveloper. For detailed steps on how to con\u0000gure SQL Developer, you\n\ncan visit https://docs.oracle.com/en/cloud/paas/exadata-express-\n\ncloud/csdbp/connect-sql-developer.html#GUID-00D45398-2BF3-48D5-\n\nB0E9-11979D5EAFFC.\n\nSetting up an AWS RDS Oracle instance\n\n1. Create an RDS instance: Log into your AWS Management Console, go to\n\nRDS Dashboard, and click on Create Database.\n\n2. Select Oracle: Choose Oracle as the database engine. Select the desired\n\nversion of Oracle, keeping it compatible with your local installation.\n\n3. Con\u0000gure instance settings: Con\u0000gure your database settings:\n\nDB instance class: Choose the appropriate instance size (for\n\nexample, db.t3.medium)\n\nStorage: Choose the storage size (based on your data size)\n\nMaster username/password: Set up the admin credentials for\n\nthe RDS instance\n\n4. Security and network:\n\nI. Ensure the RDS instance is accessible by con\u0000guring the VPC\n\nand Security Group options to allow incoming traﬃc on port\n\n1521.\n\nII. If you’re migrating from your laptop, you’ll need to con\u0000gure\n\nthe laptop’s IP address in the security group to allow access.\n\n5. Launch RDS instance: Click on Create database and wait for the RDS\n\ninstance to be created. Note down the endpoint of your RDS Oracle\n\ninstance for later use.\n\nInstalling and setting up AWS SCT\n\n1. Download and install SCT:\n\nI. Download SCT from the AWS SCT download page\n\n(https://aws.amazon.com/dms/schema-conversion-tool/) for\n\nyour OS and install it on your laptop.\n\nII. You need to follow the detailed steps given in the previous\n\nrecipe, Creating SCT migration assessment report with AWS\n\nSCT.\n\n2. Launch SCT: Open SCT and create a new project. Go to File | New\n\nProject, name your project, and select Oracle as the source database.\n\n3. Connect to Oracle on Laptop:\n\nI. Add a connection to your local Oracle database:\n\nEndpoint: localhost\n\nPort: 1521\n\nUsername: System or another user with DBA privileges\n\nSID: Use the SID or service name of your local Oracle\n\ndatabase\n\nII. Test the connection to ensure it is working correctly.\n\n4. Connect to AWS RDS Oracle:\n\nI. Add a target database connection to your RDS instance:\n\nEndpoint: Use the RDS endpoint from the AWS\n\nconsole\n\nPort: 1521\n\nUsername/Password: Use the credentials you set\n\nduring RDS instance creation\n\nII. Test the connection to ensure it is properly con\u0000gured.\n\nSchema conversion with SCT\n\n1. Convert the schema:\n\nI. Aer setting up connections to both your local Oracle and RDS\n\nOracle, SCT will display the schemas in your local Oracle\n\ndatabase.\n\nII. Select the schema(s) or objects you wish to migrate (tables,\n\nviews, procedures, and so on).\n\nIII. Right-click and select Convert schema. SCT will attempt to\n\nconvert the schema into a format compatible with the AWS\n\nRDS Oracle database.\n\n2. Analyze compatibility: Review the compatibility report generated by\n\nSCT. If there are issues with converting certain features, SCT will\n\nhighlight them. You may need to manually edit some parts of the\n\nschema if they are incompatible with RDS.\n\n3. Apply the schema to RDS:\n\nI. Once the conversion is complete, right-click on the converted\n\nschema and select Apply to database.\n\nII. SCT will then apply the converted schema to your AWS RDS\n\nOracle instance, creating tables, indexes, and other schema\n\nobjects in the cloud.\n\nData migration with AWS DMS\n\n1. Set up a DMS replication instance: Go to the AWS DMS Console and\n\nclick on Create replication instance. is instance serves as the engine\n\nfor migrating your data between the source (local Oracle) and target\n\n(RDS Oracle).\n\nFor all con\u0000guration steps, please refer to the detailed steps in\n\nthe Extracting data with AWS DMS recipe.\n\n2. Create source endpoint: In the DMS console, create a source endpoint\n\nusing the connection details of your local Oracle database:\n\nEndpoint type: Source\n\nDatabase engine: Oracle\n\nServer name: Your laptop’s IP address or localhost (if\n\ncon\u0000gured)\n\nPort: 1521\n\nUsername/Password: Oracle DB credentials\n\n3. Create target endpoint: Create a target endpoint pointing to your AWS\n\nRDS Oracle instance:\n\nEndpoint type: Target\n\nDatabase engine: Oracle\n\nServer name: RDS Endpoint.\n\nPort: 1521\n\nUsername/Password: RDS admin credentials.\n\n4. Create a migration task: Create a new migration task in DMS to migrate\n\ndata from your local Oracle database to your RDS Oracle instance:\n\nI. Choose whether to perform a full load or continuous data\n\nreplication.\n\nII. Select the tables or schemas to migrate.\n\n5. Start the migration task: Start the task and monitor its progress through\n\nthe DMS console. DMS will transfer the data from your local Oracle\n\ndatabase to the AWS RDS Oracle instance.\n\nVerification and post-migration steps\n\n1. Verify schema and data:\n\nI. Once the migration task is complete, use an Oracle SQL client\n\nsuch as SQL*Plus or Oracle SQL Developer to log into your\n\nAWS RDS Oracle instance.\n\nII. Verify that all schema objects (tables, indexes, and so on) have\n\nbeen created and that the data has been migrated successfully.\n\n2. Update application con\u0000guration: If your application was pointing to\n\nyour local Oracle database, update it to point to the new AWS RDS\n\nOracle endpoint.\n\n3. Monitoring and tuning: Monitor your RDS instance performance and\n\n\u0000ne-tune parameters such as storage auto-scaling, backup policies, and\n\nperformance metrics using Amazon CloudWatch.\n\nBy following these detailed steps, you have successfully migrated an\n\nOracle database from your local laptop to AWS RDS using AWS\n\nSCT for schema conversion and AWS DMS for data migration. is\n\napproach minimizes downtime and ensures a smooth migration to\n\nthe cloud.\n\nLeveraging AWS Snow Family for large-scale data migration\n\nIn the dynamic landscape of data management and migration,\n\nbusinesses oen face the challenge of moving massive volumes of\n\ndata securely and eﬃciently. AWS oﬀers a suite of physical devices\n\nunder the Snow Family, designed to facilitate large-scale data\n\ntransfers.\n\nis recipe explores the scenarios in which AWS Snow Family\n\nproducts should be considered for data migration. Additionally, we\n\nwill delve into the Snow Family Large Data Migration Manager, a\n\nvaluable tool for managing migrations of 500 TB or more.\n\nAWS Snow Family comprises a range of physical devices designed\n\nto transfer large amounts of data to and from AWS. ese devices\n\ninclude Snowcone, Snowball Edge, and Snowmobile, each tailored\n\nfor diﬀerent scales and types of data migration.\n\nHere’s a comparison of the key features of Snowball, Snowball Edge,\n\nand Snowmobile:\n\nSnowball:\n\nCapacity: 80 TB\n\nNetworking: 10 GB+ network connection\n\nIdeal for: Medium-sized data transfers\n\nSnowball Edge:\n\nCapacity: 100 TB\n\nNetworking: 10 GB+ network connection\n\nIdeal for: Medium to large-scale data transfers and edge\n\ncomputing\n\nSnowmobile:\n\nCapacity: Exabyte-scale\n\nIdeal for: Extremely large data transfers\n\nThe Snow Family Large Data Migration Manager as a service\n\nis service simpli\u0000es the process of migrating petabytes of data\n\nfrom your on-premises data centers to AWS. It eliminates the need\n\nfor manual tracking and provides a centralized platform to manage\n\nmultiple Snowball Edge or Snowmobile jobs simultaneously.\n\nLet’s talk about choosing the right device:\n\nConsider data size: Choose Snowball for medium-sized data, Snowball\n\nEdge for medium to large, and Snowmobile for exabyte-scale\n\nEvaluate edge computing requirements: Snowball Edge oﬀers both\n\nstorage and compute capabilities\n\nPrioritize security: Snowmobile provides the highest level of security\n\nBy understanding these diﬀerences, you can select the most\n\nappropriate Snow Family device for your data transfer needs.\n\nA typical Snowball import job \u0000ow is as follows:\n\nFigure 12.15 – Snow family job ﬂow\n\nLet’s walk through the process of ordering AWS Snow Family\n\ndevices:\n\n1. Request: In the AWS web console, order one or more snowballs to your\n\nlocation.\n\n2. Setup:\n\nI. Install the Snowball client app on a workstation.\n\nII. Do an import speed test.\n\nIII. Connect the Snowball to your local network.\n\n3. Load it:\n\nI. Download the job manifest \u0000le from the AWS web console.\n\nII. Connect the Snowball client to the snowball device.\n\nIII. Start the import process.\n\n4. Ship it:\n\nI. Power oﬀ and close the snowball device.\n\nII. Arrange for UPS pickup. Shipping is prepaid.\n\nIII. UPS provides a tracking number.\n\n5. S3 import:\n\nI. AWS loads the Snowball contents into your S3 bucket.\n\nII. S3 access is controlled via the IAM role and bucket policy.\n\nSubsteps involved in using the Large Data Migration Manager\n\ninclude the following:\n\n1. Create a data migration plan:\n\nI. Access the AWS Snow Family Management Console.\n\nII. Provide details about your data migration goals, including the\n\nfollowing:\n\nTotal data size to be migrated\n\nDesired migration timeframe\n\nNumber of Snow devices you plan to use\n\ne Large Data Migration Manager will analyze your input and\n\ngenerate a projected schedule for your migration project. It will\n\nalso recommend a job ordering schedule to optimize the process\n\nand meet your deadlines.\n\n2. Create and monitor jobs:\n\nI. Based on the recommended schedule, create individual jobs for\n\neach Snow device.\n\nII. Specify the source and destination locations for your data.\n\nIII. Track the status of each job in real time through the console.\n\nYou can monitor progress, identify any issues, and receive\n\nnoti\u0000cations about important events.\n\n3. Manage and track devices:\n\nI. e service provides tools to manage the Snow devices\n\nassociated with your migration plan.\n\nII. Track the physical location of devices, their status (for example,\n\nin transit, at AWS, and so on), and any maintenance updates.\n\n4. Optimize and adjust:\n\nI. e Large Data Migration Manager allows you to adjust your\n\nplan as needed.\n\nII. If your data size or timeframe changes, you can modify the plan\n\nand the service will update the projected schedule and job\n\nordering recommendations.\n\nGetting ready\n\nLet’s get started with the AWS Snow Family! First, here’s what you’ll\n\nneed to have in place:\n\nAWS account:\n\nActive account: An active AWS account is required to create\n\nand manage Snow Family jobs\n\nPermissions: Ensure your IAM user or role has the necessary\n\npermissions to create Snow Family jobs, interact with S3\n\nbuckets, and use other relevant AWS services (for example,\n\nKMS for encryption)\n\nData preparation:\n\nIdentify data: Determine the data you want to transfer and its\n\nlocation (on-premises or in the cloud)\n\nEstimate size: Estimate the total data size accurately to order the\n\nappropriate Snow Family device or cluster\n\nData format: Ensure your data is in a format compatible with\n\nAWS S3 or other supported data formats\n\nNetwork and logistics:\n\nNetwork connectivity: Ensure your location has adequate\n\nnetwork connectivity to support data transfer to/from the\n\nSnowball device\n\nShipping address: Provide a valid shipping address that can\n\nreceive and ship large packages\n\nPhysical space: Allocate suﬃcient space to accommodate the\n\nSnowball device, especially for larger Snowball Edge or\n\nSnowmobile options\n\nSoware and tools:\n\nAWS Snow Family client: Download and install the appropriate\n\nAWS Snow Family client soware for your operating system\n\nData transfer tools: If you’re not using the Snow Family client,\n\nensure you have compatible tools for data transfer (for example,\n\nS3Cmd and AWS s3 sync)\n\nHow to do it…\n\n1. Plan your migration: Before starting your migration, ensure you have\n\nthe following prepared:\n\nI. Estimate data size: Measure how much data you need to\n\ntransfer. If it’s 500 TB or more, AWS will recommend using\n\nmultiple Snowball or Snowball Edge devices, or even\n\nSnowmobile for petabyte-scale transfers.\n\nII. Identify source data: Determine where the data is currently\n\nstored (on-premises, data center, and so on) and the size of each\n\ndataset.\n\nIII. Decide target location: Decide where you want the data stored\n\nin AWS, typically in Amazon S3 or another AWS service.\n\nIV. Choose your Snow Family device:\n\nSnowball: Ideal for data transfers ranging from 10 TB\n\nto multiple petabytes\n\nSnowball Edge: Includes both storage and local\n\ncompute capabilities, making it suitable for edge\n\nprocessing before migration\n\nSnowmobile: e best option for migrations of 100 PB\n\nor more, typically used in large-scale data center\n\nmigrations\n\n2. Create a Large Data Migration Job in AWS Snow Family Console:\n\nI. Log in to the AWS Snow Family Console: Go to the AWS Snow\n\nFamily section in the AWS Management Console.\n\nII. Create a new job: Click on Create Job to start the migration\n\nprocess.\n\nIII. Select job type: Choose between Import into Amazon S3 (if\n\nyou’re transferring data to AWS) or Export from Amazon S3 (if\n\nyou’re moving data from AWS to your location).\n\nFigure 12.16 – AWS snow creates new job\n\nIV. Choose the Snowball device: Under Device options, choose Snowball\n\nEdge devices, which are designed for large data migrations (such as\n\nSnowball Edge Storage Optimized or Compute Optimized).\n\nV. Select the Large Data Migration option: In the Job options section, you\n\nwill be asked to specify your use case. Choose Large Data Migration to\n\nindicate that you’re using Snowball to transfer large volumes of data (for\n\ndata migrations of 500 TB or more, choose Large Data Migration\n\nManager).\n\nVI. De\u0000ne job details:\n\nS3 bucket: Select the S3 bucket where you want to transfer the\n\ndata.\n\nEncryption: Select or create an AWS KMS key for encryption.\n\nis key ensures that all data is encrypted while in transit.\n\nAdd IAM: Create and add IM policy to the job. Please check the\n\nsample policy in the following GitHub path:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-\n\nAWS-Cookbook/blob/main/Chapter12/snowball-policy.json.\n\nShipping details: Provide the shipping address for where AWS\n\nwill deliver the Snowball or Snowball Edge devices.\n\nFigure 12.17 – AWS job details\n\n3. AWS ships the devices to you:\n\nI. Receive Snowball devices: AWS will ship the required number\n\nof Snowball or Snowball Edge devices to your location based on\n\nyour data size. You can track the shipment via the AWS console.\n\nII. Unpack and set up devices: Follow the instructions provided\n\nwith the devices to set them up in your data center. You will\n\nneed to plug them into your network.\n\n4. Load data onto Snowball devices:\n\nI. Install the AWS Snowball Client:\n\ni. AWS provides a Snowball Client that allows you to\n\ntransfer data to the Snowball device using a\n\nCommand-Line Interface (CLI).\n\nii. Download the client from the AWS Snow Family\n\ndocumentation\n\n(https://docs.aws.amazon.com/snowball/).\n\nII. Connect to Snowball: Use the Snowball Client to connect to the\n\ndevice. You will need the job ID and unlock code (provided in\n\nthe AWS console).\n\nIII. Start data transfer: Begin loading data onto the Snowball using\n\nthe CLI or APIs. For large datasets, consider segmenting the\n\ndata to load it in parallel across multiple devices. Here’s an\n\nexample command for transferring data:\n\nbash snowball cp /path/to/data snowball://snowball_device/data -- recursive\n\nIV. Verify data transfer: Ensure all data has been transferred\n\nsuccessfully. Use checksums or other data validation techniques\n\nto verify data integrity.\n\n5. Ship devices back to AWS\n\nI. Pack the Snowball devices: Aer completing the data transfer,\n\npack the devices securely using the provided packaging\n\nmaterials.\n\nII. Return shipment: AWS provides pre-paid shipping labels.\n\nAttach them to the Snowball devices and schedule a pickup or\n\ndrop them oﬀ at the speci\u0000ed shipping carrier.\n\nIII. Track the return: Track the shipment via the AWS console to\n\nmonitor the device’s return to AWS data centers.\n\n6. Data import to Amazon S3:\n\nI. AWS receives the devices: Once AWS receives the devices, they\n\nwill automatically begin transferring the data to the speci\u0000ed\n\nAmazon S3 bucket.\n\nII. Monitor progress: Use the AWS Snow Family Console to\n\nmonitor the progress of the data transfer. You can check\n\nwhether the import is complete or whether any issues have\n\noccurred.\n\nIII. Validation: AWS will validate the integrity of the data and\n\nnotify you once the transfer is successful.\n\n7. Post-migration operations",
      "page_number": 751
    },
    {
      "number": 38,
      "title": "Segment 38 (pages 771-793)",
      "start_page": 771,
      "end_page": 793,
      "detection_method": "synthetic",
      "content": "I. Access data in Amazon S3: Once the data transfer is complete,\n\nyou can access your data in Amazon S3. is data can now be\n\nused for further processing, analysis, or storage.\n\nII. Edge compute results (optional): If you used Snowball Edge for\n\nedge computing, you can also access the results of any compute\n\ntasks that were performed locally during the migration.\n\nIII. Data life cycle management: Con\u0000gure S3 storage classes, life\n\ncycle policies, and versioning to optimize your storage costs.\n\nMigrating 500 TB or more of data using the AWS Snow Family is a\n\nsecure, eﬃcient way to handle large-scale data migrations where\n\nnetwork transfer is impractical. By leveraging the Large Data\n\nMigration Manager, you can streamline the process of managing\n\nmultiple Snowball devices, ensuring that your data transfer is\n\nseamless and secure. Whether you’re moving to Amazon S3,\n\nperforming edge compute tasks, or managing long-term storage,\n\nAWS Snow Family provides the tools needed to perform large-scale\n\ndata migrations with minimal impact on your operations.\n\nKEY CONSIDERATIONS\n\nHere are a few key considerations:\n\nPlanning: Accurate data estimation is crucial to avoid ordering too few\n\nor too many devices\n\nData preparation: Ensure your data is organized and ready for transfer\n\nbefore the devices arrive\n\nSecurity: Use strong encryption keys and IAM roles to protect your data\n\nduring transit and in the cloud\n\nNetwork: Adequate network bandwidth is essential for eﬃcient data\n\ntransfer\n\nTracking: Use the Large Data Migration Manager’s dashboard to\n\nmonitor the progress of your migration and address any issues promptly\n\nSee also\n\nAWS Snowball Edge Primer:\n\nhttps://explore.skillbuilder.aws/learn/course/external/view/elearning/45/\n\naws-snowball-edge-getting-started\n\nAWS Snowball Edge Logistics and Planning:\n\nhttps://explore.skillbuilder.aws/learn/course/external/view/elearning/11\n\n5/aws-snowball-edge-logistics-and-planning\n\nOceanofPDF.com\n\n13 Strategizing Hadoop Migrations – Cost, Data, and Workflow Modernization with AWS\n\nMigrating large-scale data and machine learning processes to cloud\n\nplatforms such as Amazon Web Services (AWS) brings multiple\n\nbene\u0000ts. AWS and similar services oﬀer a wide variety of on-\n\ndemand computing resources, cost-eﬀective and durable storage\n\nsolutions, and managed environments that are up to date and user-\n\nfriendly for big data applications. is setup allows data engineers,\n\ndevelopers, data scientists, and IT staﬀ to concentrate on data\n\npreparation and insight extraction.\n\nTools such as Amazon Elastic MapReduce (EMR), AWS Glue, and\n\nAmazon Simple Storage Service (S3) facilitate the separation and\n\nindependent scaling of computing and storage resources while\n\nensuring a cohesive, robust, and well-managed environment. is\n\nsigni\u0000cantly reduces many of the challenges associated with on-\n\npremises methods. Adopting this cloud-based approach results in\n\nquicker, more \u0000exible, user-friendly, and cost-eﬀective solutions for\n\nbig data and data lake projects.\n\nIn this chapter, we’ll guide you through various strategies for\n\nmigration, covering how to transfer your cluster data, catalog\n\nmetadata, extract, transform, and load (ETL) jobs, and work\u0000ow\n\nservices. You’ll also discover methods to incorporate quality checks\n\nto ensure your migration is successful.\n\ne recipes we’ll explore include the following:\n\nCalculating total cost of ownership (TCO) using AWS TCO calculators\n\nConducting a Hadoop migration assessment using the TCO simulator\n\nSelecting how to store your data\n\nMigrating on-premises HDFS data using AWS DataSync\n\nMigrating the Hive Metastore to AWS\n\nMigrating and running Apache Oozie work\u0000ows on Amazon EMR\n\nMigrating an Oozie database to the Amazon RDS MySQL\n\nSetting up networking – establishing a secure connection to your EMR\n\ncluster\n\nPerforming a seamless HBase migration to AWS\n\nMigrating HBase to DynamoDB on AWS\n\nGaining insight into these areas will equip you with a\n\ncomprehensive understanding of the crucial factors to consider\n\nduring cloud migration planning. Additionally, you’ll learn about\n\nthe necessary modi\u0000cations to adapt your existing applications for a\n\ncloud-native architecture.\n\nTechnical requirements\n\nMake sure you have an active AWS account. If you don’t already\n\nhave one, sign up for an AWS account at\n\nhttps://aws.amazon.com/resources/create-account/ before\n\nproceeding.\n\nYou can access the code for this project in the GitHub repository:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter13.\n\nCalculating total cost of ownership (TCO) using AWS TCO calculators\n\nNavigating the \u0000nancial landscape of cloud migration can be\n\ncomplex. AWS total cost of ownership (TCO) calculators simplify\n\nthis process by oﬀering an estimate of your potential cost savings\n\nwhen transitioning from on-premises infrastructure to AWS cloud\n\nservices. ese tools allow you to compare the expenses associated\n\nwith maintaining your current infrastructure to the projected costs\n\nof running similar workloads on AWS.\n\nBy factoring in variables such as server costs, storage, networking,\n\nsoware licenses, and even personnel expenses, TCO calculators\n\npaint a comprehensive picture of your current IT expenditure. ey\n\nthen contrast this with the potential costs of AWS, considering\n\nvarious pricing models, usage patterns, and optimizations available\n\nin the cloud.\n\nWhile a TCO calculator provides a useful starting point, it’s crucial\n\nto consider it as an estimate. Your actual costs will depend on your\n\nunique con\u0000guration and how you use the service and\n\ncon\u0000guration. Nonetheless, these tools serve as a crucial guide in\n\nyour cloud migration journey, helping you make informed\n\ndecisions based on potential cost savings and a clearer\n\nunderstanding of the \u0000nancial implications of moving to AWS.\n\nIn this recipe, we aim to guide you through the process of\n\nevaluating the cost of migrating an on-premises Hadoop cluster to\n\nAmazon EMR using the AWS TCO calculator. By following a step-\n\nby-step approach, you will learn how to input your current\n\ninfrastructure details, compare on-premises costs with AWS\n\nservices, and assess potential savings.\n\nIn this migration from an on-premises Hadoop cluster to Amazon\n\nEMR, we assume a 10-server Hadoop setup with 200 TB of Hadoop\n\nDistributed File System (HDFS) storage. e EMR cluster is\n\ncon\u0000gured with r5.2xlarge instances for master, core, and task\n\nnodes. Amazon S3 will replace HDFS for data storage, with 200 TB\n\nof data transferred during the initial migration. Additionally, 500\n\nGB of Amazon Elastic Block Store (EBS) storage is allocated per\n\nEMR instance for local storage needs.\n\nGetting ready\n\nTo facilitate the migration decision-making process, ensure that you\n\ncollect the following metrics from your current Hadoop clusters:\n\nTotal count of physical CPUs\n\nCPU clock speed and core counts\n\nTotal memory capacity\n\nHDFS storage volume (excluding replication)\n\nMaximum aggregate network throughput\n\nUtilization graphs span at least one week for the resources\n\nCurrent Hadoop cluster inventory: Detailed information on the existing\n\nHadoop cluster, including the following:\n\nHardware speci\u0000cations (servers, storage, and network)\n\nSoware versions (Hadoop distribution, related tools, and so\n\non)\n\nCurrent utilization and performance metrics\n\nAWS cost estimation tools: Access to AWS Pricing Calculator, AWS Cost\n\nExplorer, or similar tools to estimate the cost of running equivalent\n\nworkloads on AWS\n\nTCO analysis framework: A methodology or tool to compare on-\n\npremises costs (hardware, soware, maintenance, and personnel) against\n\nestimated AWS costs\n\nHow to do it…\n\nHere’s a step-by-step guide on how to use the TCO calculator to\n\nassess the cost of moving your Hadoop cluster to AWS:\n\n1. Access the AWS TCO calculator:\n\nI. Navigate to https://calculator.aws/#/.\n\nII. Click on Create estimate.\n\nFigure 13.1 – The pricing calculator\n\n2. Input on-premises Cloudera Hadoop details:\n\nI. Select the workload: Choose Hadoop (for example, Cloudera or\n\nHortonworks) as the current workload.\n\nII. Add server details: Let’s assume you have 10 physical servers\n\nrunning your Cloudera Hadoop cluster. Enter the following\n\nserver details in the calculator:\n\nNumber of servers: 10\n\nCPU cores: 64 cores per server\n\nMemory: 256 GB RAM per server\n\nStorage: 20 TB per server (SSD or HDD)\n\nUtilization: Choose High if your servers are running at\n\nhigh capacity\n\nIII. Storage details: Let’s assume that the HDFS storage on your\n\nHadoop cluster amounts to 200 TB in total. Specify the type of\n\nstorage used for Hadoop, whether SSD or HDD.\n\nIV. Network costs: Estimate the network bandwidth used for data\n\ntransfers between your cluster nodes and other systems. In this\n\nexample, assume a bandwidth usage of 10 Gbps.\n\n3. Add additional on-premises costs:\n\nI. Data center costs: For electricity, cooling, and physical space,\n\nassume the following estimates:\n\nPower and cooling: $100 per month per server\n\nData center space: $500 per month for rack space\n\nII. Labor costs: If you have an IT team managing the Hadoop\n\ncluster, input labor costs. For example:\n\nIT staﬀ: 2 full-time employees, each with a salary of\n\n$100,000/year\n\nCloudera licensing: $50,000/year for Cloudera\n\nManager licenses\n\nIII. Hardware refresh costs: If you refresh hardware every 4-5 years,\n\ninput an average of $10,000 per server for hardware\n\nreplacement.\n\n4. Con\u0000gure the AWS cloud settings for EMR:\n\nI. Amazon EMR cluster setup: Let’s estimate that you will use\n\nAmazon EMR with r5.2xlarge instances for your new\n\nsetup:\n\nNumber of instances: You need 10 Amazon Elastic\n\nCompute Cloud (EC2) instances, similar to your\n\ncurrent on-premises setup\n\nInstance type: Select r5.2xlarge (8 vCPUs, 64 GB\n\nof RAM)\n\nAdd a con\u0000guration for one master node, six core nodes,\n\nand three task nodes\n\nII. Amazon S3 for storage: You plan to move your HDFS storage\n\n(200 TB) to Amazon S3. Add the estimated cost of S3 storage\n\nbased on your data size. In this example, you’ll be using\n\nstandard S3 storage for 200 TB of data.\n\nIII. EBS storage for EMR nodes: Specify the amount of EBS storage\n\nneeded for the EMR nodes, assuming 500 GB per instance.\n\nIV. Network and data transfer costs: Estimate the costs of\n\ntransferring your data from on-premises to AWS using AWS\n\nDirect Connect or public internet. Assume you’ll transfer 200\n\nTB of data initially.\n\nV. Additional AWS services: You may use AWS Glue for ETL\n\nprocessing or Amazon CloudWatch for monitoring. Add these\n\ncosts based on your projected usage.\n\nVI. Auto-scaling for EMR: If you plan to use auto-scaling, con\u0000gure\n\nyour scaling policy (for example, scaling up to 20 instances\n\nduring peak times).\n\nOnce you’ve entered all the required information, the TCO\n\ncalculator will generate the estimated total cost:\n\nFigure 13.2 – The pricing calculator estimate summary\n\n5. Run the TCO calculation: Aer inputting the required details for both\n\nyour on-premises Cloudera Hadoop cluster and your Amazon EMR\n\ncon\u0000guration, click on Calculate TCO:\n\nI. View the results: e AWS TCO calculator will provide a\n\ncomparison of the cost of running your Hadoop cluster on-\n\npremises versus running it on Amazon EMR. You will see a\n\nbreakdown of the costs, such as the following:\n\nCompute costs (for EMR EC2 instances)\n\nStorage costs (for S3 and EBS)\n\nNetworking costs (for data transfer)\n\nLabor and licensing savings (since Cloudera licensing\n\nand IT staﬀ costs may decrease)\n\n6. Review and analyze the results:\n\nI. Estimated savings: e TCO report will show how much you\n\ncould potentially save by moving to Amazon EMR. For\n\nexample, it might show that your on-premises Cloudera cluster\n\ncosts $500,000/year, while running Amazon EMR could cost\n\n$300,000/year, representing a 40% savings.\n\nII. Download the report: You can export the results as a PDF or\n\nExcel \u0000le to share with stakeholders:\n\nFigure 13.3 – The pricing calculator estimate summary\n\nSummary:\n\nYou have a 10-node on-premises Cloudera Hadoop cluster with 200 TB\n\nof data and a total cost of $500,000/year\n\nMoving to Amazon EMR with r5.2xlarge instances, S3 storage for 200\n\nTB, and auto-scaling con\u0000gured, the estimated annual cost is around\n\n$300,000/year\n\nis migration could potentially save you around 40% in operational\n\nand hardware costs\n\nBy following these steps and inputting similar values into the AWS\n\nTCO calculator, you can create your own estimates for migrating\n\nfrom Cloudera Hadoop to Amazon EMR. is hands-on approach\n\nhelps in understanding the potential cost bene\u0000ts of moving\n\nworkloads to the cloud.\n\nSee also\n\nCreate and con\u0000gure an estimate: https://docs.aws.amazon.com/pricing-\n\ncalculator/latest/userguide/create-con\u0000gure-estimate.html#create-\n\nestimate\n\nConducting a Hadoop migration assessment using the TCO simulator\n\nUsing AWS’s TCO assessment tool to evaluate the cost of migrating\n\nyour current Hadoop cluster to AWS is an eﬀective approach to\n\nunderstanding and comparing the expenses involved. e TCO\n\nassessment helps you get a comprehensive view of the \u0000nancial\n\nimpact of the migration. Here’s how you can use the AWS TCO\n\nassessment for your Hadoop cluster migration:\n\nFigure 13.4 – The TCO cost-saving summary\n\nHadoop to Amazon EMR TCO simulator\n\ne AWS ProServe Hadoop Migration Delivery Kit (HMDK) TCO\n\ntool is designed to assist organizations in migrating from on-\n\npremises Hadoop clusters to Amazon EMR. is tool helps in\n\nassessing the TCO by simulating the resource usage of future EMR\n\nclusters based on historical Hadoop job data. It provides insights\n\ninto workload patterns, job timelines, and resource utilization,\n\nenabling organizations to design cost-eﬀective and optimized EMR\n\nclusters.\n\nFor a more in-depth analysis of migration, AWS professional\n\nservices oﬀer a valuable asset: the Hadoop migration assessment\n\nTCO tool. is tool is now integrated into the AWS ProServe\n\nHMDK. We’ll explore this further in this recipe.\n\nis step-by-step recipe provides a comprehensive approach to\n\nassessing and optimizing the migration of Hadoop workloads to\n\nAmazon EMR using the TCO simulator tool.\n\nGetting ready\n\nBefore using the TCO simulator, ensure that you have the following\n\nprerequisites in place:\n\nAccess to Hadoop cluster logs: You need access to the YARN logs from\n\nyour existing Hadoop cluster. ese logs provide essential data on job\n\nexecution and resource usage, which the TCO tool will analyze.\n\nPython environment: A Python environment is required to run the log\n\ncollector and analyzer scripts. e tool supports execution through both\n\nnative Python environments and Docker containers. You can download\n\nit from Python’s oﬃcial website (https://www.python.org/downloads/).\n\nAWS account: An active AWS account with necessary permissions to\n\ncreate and manage EMR clusters and to use services, such as Amazon\n\nQuickSight for visualization.\n\nExcel: e \u0000nal TCO calculations are done using an Excel template with\n\nmacros, so you need a working installation of Excel that supports macro\n\nexecution.\n\nTechnical requirements are as follows:\n\nYARN log collector: is tool collects logs from the Hadoop YARN\n\nResource Manager. e logs are securely transported using HTTPS and\n\nconverted into CSV format, which serves as input for further analysis.\n\nYARN log analyzer: Aer collecting the logs, this analyzer processes the\n\ndata to extract key metrics such as job timelines, user activity, and\n\nresource usage. ese metrics are visualized using Amazon QuickSight\n\ndashboards.\n\nOptimized TCO calculator: is calculator processes the aggregated log\n\ndata to estimate the costs of running equivalent workloads on Amazon\n\nEMR. e calculator uses inputs such as instance types, storage costs,\n\nand resource allocation to provide detailed cost estimates.\n\nHow to do it…\n\n1. Set up the environment:\n\nI. Clone the repository: Start by cloning the TCO simulator\n\nrepository from GitHub using the following command:\n\ngit clone https://github.com/awslabs/migration- hadoop-to-emr-tco-simulator.git\n\nII. Install Python dependencies: Navigate to the cloned directory\n\nand install the required Python packages. If using a virtual\n\nenvironment, you can do the following:\n\ncd migration-hadoop-to-emr-tco-simulator\n\nUnder the migration-hadoop-to-emr-tco-\n\nsimulator folder, you will \u0000nd\n\nrequirements.txt:\n\npip install -r requirements.txt\n\nIII. Set up Docker (optional): If you prefer using Docker, ensure\n\nDocker is installed on your system. You can follow the\n\ninstructions from Docker’s oﬃcial installation guide:\n\nhttps://www.docker.com/get-started/.\n\n2. Collect YARN logs:\n\nI. Run the YARN log collector: Use the provided script to collect\n\nlogs from your Hadoop cluster. Ensure you have the necessary\n\npermissions to access the YARN ResourceManager API. e\n\ncommand looks like this:\n\npython yarn-log-collector.py --resource- manager <YARN_RESOURCE_MANAGER_ENDPOINT> --output-dir ./logs\n\nYou can \u0000nd details about commands and how to run\n\nthem in the README \u0000le at\n\nhttps://github.com/awslabs/migration-hadoop-to-emr-\n\ntco-simulator/tree/main/yarn-log-collector.\n\nII. Convert logs to CSV: e script will convert the collected JSON\n\nlogs into CSV format, ready for analysis.\n\n3. Analyze logs:\n\nI. Run the log analyzer: Use the yarn-log-analysis script\n\nto analyze the collected logs. is will generate detailed metrics\n\nthat you can use to understand your current Hadoop workload.\n\npython yarn-log-analysis.py --input-dir ./logs --output-dir ./analysis\n\nII. Visualize with Amazon QuickSight: Set up Amazon QuickSight\n\nusing the provided CloudFormation template to visualize the\n\nanalysis results.\n\n4. Run the TCO calculator:\n\nI. Aggregate log data: Use the tco-input-generator.py\n\nscript to aggregate the log data:\n\npython tco-input-generator.py --input- dir ./analysis --output-file tco- input.csv\n\nII. Calculate TCO in Excel: Open the provided Excel template,\n\nenable macros, and input your data. Use the green cells to enter\n\nparameters such as instance types, data size, and other relevant\n\ncon\u0000gurations.\n\n5. Interpret results: Use the insights from the QuickSight dashboards and\n\nExcel-based TCO estimates to design your future EMR architecture.\n\nAdjust your cluster design based on workload types, peak usage times,\n\nand resource needs to optimize for both performance and cost.\n\nSee also\n\nAmazon EMR pricing: https://aws.amazon.com/emr/pricing/\n\nIntroducing the AWS ProServe Hadoop Migration Delivery Kit TCO tool:\n\nhttps://aws.amazon.com/blogs/big-data/introducing-the-aws-proserve-\n\nhadoop-migration-delivery-kit-tco-tool/\n\nHadoop Migration Assessment (TCO-Simulator):\n\nhttps://github.com/awslabs/migration-hadoop-to-emr-tco-simulator\n\nSelecting how to store your data\n\nWhen migrating from an on-premises Hadoop cluster to AWS, one\n\nof the crucial decisions to be made is selecting the appropriate\n\nstorage solution for your data. Amazon S3 and HDFS both oﬀer\n\nrobust data storage capabilities, but they diﬀer in their architecture,\n\nfeatures, and use cases. is recipe will help you navigate this choice\n\nby comparing S3 and HDFS, examining their technical\n\nrequirements, and oﬀering guidance on how to make an informed\n\ndecision based on your speci\u0000c needs.\n\nChoosing between Amazon S3 and Hadoop HDFS depends on your\n\nspeci\u0000c use case, performance requirements, and long-term goals.\n\nAmazon S3 oﬀers unmatched scalability and integration with AWS\n\nservices, making it ideal for cloud-native workloads and data lakes.\n\nHDFS, on the other hand, is well suited for high-throughput big\n\ndata processing within a Hadoop ecosystem. By carefully evaluating\n\nyour needs against the capabilities of each storage solution, you can\n\nselect the most appropriate technology for your environment.\n\nHere’s a brief overview of HDFS and S3:\n\nAWS presents Amazon S3 as an object storage service. S3 boasts\n\nindustry-leading attributes, including remarkable scalability, data\n\navailability, security, and performance. is comprehensive feature set\n\nempowers customers to securely store and safeguard vast volumes of\n\ndata, catering to a wide spectrum of use cases. Amazon S3 is best for\n\ncloud-native applications, multi-tenant environments, and when\n\nleveraging other AWS analytics and machine learning services.\n\nHDFS serves as a distributed \u0000lesystem speci\u0000cally craed for the\n\neﬃcient management of extensive data sets spread across multiple\n\nnodes. Within the Apache Hadoop ecosystem, HDFS plays a pivotal role,\n\nensuring robust and high-throughput access to application data while\n\ngracefully managing failures. HDFS is ideal for on-premises\n\ndeployments, traditional data warehousing, and when using a Hadoop-\n\ncentric processing tool.\n\nWhen deciding between Amazon S3 and Hadoop HDFS for storing\n\nyour data, it’s crucial to consider various factors, such as data size,\n\naccess patterns, cost, performance, scalability, and your speci\u0000c use\n\ncase requirements. Here’s a step-by-step guide to help you make an\n\ninformed choice.\n\nIn an HDFS versus S3 comparison, it’s vital to recognize that each\n\nstorage system possesses its unique strengths and weaknesses,\n\ntailored to distinct scenarios and objectives. HDFS excels in\n\nenvironments prioritizing data locality and replication, while S3\n\nstands out due to its exceptional scalability, durability, and seamless\n\nintegration with various AWS services:\n\nUnderstand the basics of Amazon S3 and Hadoop HDFS:\n\nAmazon S3:\n\nType: Object storage service\n\nScalability: Highly scalable, designed for\n\n99.999999999% durability\n\nAccess: Accessible via HTTP/HTTPS; supports a wide\n\nrange of APIs\n\nCost: Pay-as-you-go pricing; no upfront costs; diﬀerent\n\nstorage classes (Standard, Intelligent-Tiering, Glacier)\n\ndepending on access frequency\n\nPerformance: Optimized for high availability and\n\nscalability, with the ability to manage large volumes of",
      "page_number": 771
    },
    {
      "number": 39,
      "title": "Segment 39 (pages 794-815)",
      "start_page": 794,
      "end_page": 815,
      "detection_method": "synthetic",
      "content": "data\n\nHard limits:\n\nNumber of buckets: You can have up to\n\n100,000 buckets per AWS account\n\nObject size: Individual objects can be up to 5\n\nTB in size\n\nNumber of objects per bucket: ere is no\n\nstrict limit on the number of objects per\n\nbucket, but performance and scalability might\n\nbe aﬀected by extremely large numbers\n\nQuotas:\n\nStorage capacity: e amount of storage you\n\ncan use depends on your AWS account plan\n\nand usage. AWS provides \u0000exible storage\n\noptions, including Standard, Infrequent\n\nAccess (IA), Glacier, and Glacier Deep\n\nArchive.\n\nData transfer: ere are limits on the amount\n\nof data you can transfer in and out of S3.\n\nese limits can vary based on your region\n\nand usage patterns.\n\nAPI requests: e number of API requests you\n\ncan make to S3 is subject to quotas. ese\n\nquotas are generally adjusted based on your\n\nusage patterns.\n\nHadoop HDFS:\n\nType: Distributed \u0000lesystem designed for large-scale\n\ndata processing\n\nScalability: Scales across multiple nodes; provides high\n\nthroughput\n\nAccess: Typically accessed within a Hadoop ecosystem;\n\nsupports POSIX-like \u0000le operations\n\nCost: Costs are tied to the underlying hardware,\n\nincluding storage devices and network infrastructure\n\nPerformance: Optimized for batch processing with\n\nhigh throughput; can be con\u0000gured for low-latency\n\naccess depending on the hardware\n\nHard limits:\n\nCluster size: e number of nodes in your\n\nHDFS cluster will determine the overall\n\nstorage capacity and processing power. As you\n\nadd more nodes, your HDFS cluster can scale\n\nto handle larger datasets.\n\nData volume: While HDFS can handle\n\nmassive amounts of data, there are practical\n\nlimits based on the hardware resources of\n\nyour cluster. Factors such as disk space,\n\nnetwork bandwidth, and processing power\n\nwill in\u0000uence how much data your HDFS\n\ncluster can eﬀectively store and process.\n\nFile size: Individual \u0000les in HDFS can be very\n\nlarge, but there might be limitations based on\n\nthe underlying \u0000lesystem and hardware.\n\nQuotas: HDFS doesn’t have prede\u0000ned quotas like\n\nAWS S3. However, there are practical considerations\n\nand limitations that can aﬀect your HDFS cluster’s\n\nperformance and capacity:\n\nStorage capacity: e total storage capacity of\n\nyour HDFS cluster is determined by the\n\ncombined storage of all nodes. While there’s\n\nno \u0000xed limit, you might need to add more\n\nnodes to accommodate larger datasets.\n\nNetwork bandwidth: e network bandwidth\n\nbetween nodes in your HDFS cluster can\n\nimpact data transfer speeds. If your network is\n\ncongested, it can aﬀect performance.\n\nProcessing power: e processing power of\n\nyour HDFS nodes will in\u0000uence how quickly\n\ndata can be processed and analyzed. If your\n\nworkloads are demanding, you might need to\n\nupgrade your hardware.\n\nAssess your use case requirements:\n\nData size and growth:\n\nAmazon S3: Ideal for storing vast amounts of data that\n\ncan scale in\u0000nitely without concern for the underlying\n\ninfrastructure.\n\nHDFS: Suitable for large datasets that are part of a\n\nHadoop ecosystem. However, scaling requires adding\n\nmore hardware nodes.\n\nData access patterns:\n\nAmazon S3: Best for scenarios where data is accessed\n\ninfrequently or requires global access. Suitable for\n\nobject storage, backups, logs, and data lakes.\n\nHDFS: Ideal for high-throughput data processing\n\ntasks, such as big data analytics, where data is\n\nprocessed in large batches within a cluster.\n\nPerformance requirements:\n\nAmazon S3: is provides scalable performance for\n\nread and write operations but is typically slower than\n\nHDFS for high-speed, sequential access required by\n\nsome big data workloads\n\nHDFS: is is optimized for sequential reads and\n\nwrites, making it highly eﬃcient for data processing\n\ntasks such as Spark and MapReduce jobs in an EMR\n\nHadoop cluster, where large-scale data is processed in\n\nparallel.\n\nCost considerations:\n\nAmazon S3: is is cost-eﬀective for storing large\n\nvolumes of data with diﬀerent storage classes to\n\noptimize costs based on access patterns.\n\nHDFS: Costs are higher if you need to manage and\n\nscale the physical hardware and associated\n\ninfrastructure. However, HDFS can be more cost-\n\neﬀective for on-premises deployments if you already\n\nhave the hardware.\n\nScalability and management:\n\nAmazon S3: Automatically scales to accommodate\n\ngrowing data volumes without manual intervention\n\nHDFS: is requires manual management to scale by\n\nadding more nodes to the cluster, which increases\n\noperational complexity\n\nConsider data security and compliance:\n\nAmazon S3 security:\n\nis oﬀers advanced security features such as\n\nencryption at rest (Server-Side Encryption with\n\nAmazon S3-Managed Keys (SSE-S3) and Server-Side\n\nEncryption with AWS Key Management Service-\n\nManaged Keys (SSE-KMS)) and in transit (Secure\n\nSockets Layer/Transport Layer Security (SSL/TLS))\n\nis supports \u0000ne-grained access control with Identity\n\nand Access Management (IAM) policies, bucket\n\npolicies, and Access Control List (ACLs)\n\nCompliance certi\u0000cations such as Health Insurance\n\nPortability and Accountability Act (HIPAA), General\n\nData Protection Regulation (GDPR), and others make\n\nit suitable for storing sensitive data\n\nHDFS security:\n\nis provides encryption at rest using Hadoop’s\n\ntransparent encryption features\n\nis supports Kerberos for authentication and Apache\n\nRanger for \u0000ne-grained access control\n\nSecurity depends on the underlying environment;\n\nadditional con\u0000gurations may be needed for\n\ncompliance\n\nEvaluate integration with your ecosystem:\n\nAmazon S3:\n\nis integrates seamlessly with AWS services such as\n\nAWS Lambda, Amazon Athena, Amazon EMR, and\n\nAWS Glue\n\nSuitable for building serverless architectures, data\n\nlakes, and integrating with cloud-native applications\n\nis supports native integration with big data tools via\n\nS3 connectors (for example, Spark and Hive)\n\nHDFS:\n\nTight integration with the Hadoop ecosystem and tools\n\nsuch as Apache Hive, Apache HBase, and Apache\n\nSpark\n\nBest for environments where Hadoop is central to data\n\nprocessing, and you rely on its ecosystem for analytics\n\nMatch the solution to your speci\u0000c use case:\n\nUse cases for Amazon S3:\n\nData lakes: Store structured and unstructured data for\n\nanalytics and reporting\n\nBackup and archive: Cost-eﬀective storage for long-\n\nterm backups and archival\n\nWeb content hosting: Serve static content such as\n\nimages, videos, and HTML \u0000les globally\n\nBig data analytics: Store raw data for processing using\n\nAWS services such as Amazon EMR or Athena\n\nUse cases for HDFS:\n\nBig data processing: Ideal for running Hadoop-based\n\nanalytics, including MapReduce, Spark jobs, and\n\nmachine learning workloads, providing scalable and\n\nhigh-throughput storage optimized for processing\n\nlarge datasets in a distributed environment\n\nBatch processing: High-throughput data processing for\n\nETL jobs and large-scale data transformations\n\nData warehousing: Used as a storage layer in Hadoop-\n\nbased data warehouses\n\nMake your decision:\n\nChoose Amazon S3 if the following is the case:\n\nYou need highly scalable, durable, and globally\n\naccessible storage\n\nYour data access patterns are varied, and you require\n\n\u0000exibility in storage classes\n\nYou want to minimize infrastructure management and\n\nfocus on cloud-native solutions\n\nChoose HDFS if the following is the case:\n\nYou are operating within a Hadoop ecosystem and\n\nneed high-throughput access to large datasets\n\nYou prefer or require an on-premises solution with\n\ncontrol over the underlying hardware\n\nYour use case involves large-scale batch processing\n\nwith speci\u0000c performance requirements\n\nGetting ready\n\ne technical requirements for this recipe are as follows:\n\nAWS account: An active AWS account to provide and manage S3\n\nbuckets and EMR clusters\n\nNetwork connectivity: Secure network connectivity between your on-\n\npremises environment and AWS, if you need to transfer data\n\nData transfer tools: Tools such as AWS DataSync or S3 Transfer\n\nAcceleration for eﬃcient data movement\n\nHow to do it…\n\nHere’s a step-by-step guide to help you make an informed choice.\n\nLet’s start with the steps to implement Amazon S3 storage:\n\n1. Set up Amazon S3 bucket:\n\nI. In the AWS Management Console, navigate to S3 and create a\n\nbucket.\n\nII. De\u0000ne the bucket region, con\u0000gure permissions, and set up\n\npolicies as per your security requirements.\n\n2. Transfer data to S3:\n\nI. Use the AWS Command Line Interface (CLI) or S3 console to\n\nupload data.\n\nCommand example: aws s3 cp\n\n/path/to/data s3://your-bucket-name/\n\n--recursive\n\nII. For large-scale data transfer, use AWS DataSync or AWS\n\nSnowball to migrate data eﬃciently from on-premises systems.\n\n3. Enable life cycle policies:\n\nI. De\u0000ne storage policies to automatically move older data to\n\ncheaper storage classes, such as S3 Glacier, or delete unneeded\n\nobjects.\n\nII. Con\u0000gure versioning and replication as needed for backup and\n\ndisaster recovery.\n\n4. Integrate with AWS services:\n\nI. Set up AWS Glue, Athena, or EMR to interact with the data\n\nstored in S3 for analysis and processing.\n\nII. Use Amazon S3 Select to retrieve speci\u0000c data directly from S3\n\nobjects, reducing data transfer and processing overhead.\n\n5. Monitor and manage storage:\n\nI. Use Amazon CloudWatch and S3 Storage Lens to monitor\n\nusage, performance, and storage costs.\n\nII. Set up alerts for storage thresholds or unexpected data transfers.\n\ne steps to implement HDFS storage on Amazon EMR are as\n\nfollows:\n\n1. Set up an EMR cluster with HDFS:\n\nI. In the AWS Management Console, navigate to EMR and create\n\na cluster (https://console.aws.amazon.com/emr).\n\nII. Choose Hadoop as the framework and con\u0000gure the cluster\n\nwith HDFS.\n\nIII. Select your instance types (for example, r5.2xlarge for\n\nmaster and core nodes) and con\u0000gure the number of nodes\n\nbased on your processing requirements.\n\n2. Con\u0000gure HDFS on EMR:\n\nI. HDFS is automatically con\u0000gured as the default storage for the\n\nEMR cluster.\n\nII. Data stored on HDFS will be local to the cluster and will not\n\npersist aer cluster termination unless backed up.\n\n3. Upload data to HDFS:\n\nI. Aer the cluster is running, Secure Shell (SSH) into the Master\n\nNode.\n\nII. Use the following command to upload data to HDFS:\n\nbash\n\nhadoop fs -put /local/data/path /hdfs/path\n\nIII. For large datasets, use AWS DataSync\n\n(https://docs.aws.amazon.com/datasync/) or DistCp\n\n(https://docs.aws.amazon.com/prescriptive-\n\nguidance/latest/patterns/migrate-data-from-an-on-premises-\n\nhadoop-environment-to-amazon-s3-using-distcp-with-aws-\n\nprivatelink-for-amazon-s3.html) to eﬃciently migrate data to\n\nHDFS.\n\n4. Managing and monitoring HDFS:\n\nI. Use the Hadoop ResourceManager and HDFS NameNode\n\ndashboards in EMR to monitor disk space, replication status,\n\nand node health.\n\nII. Adjust the replication factor for fault tolerance (default is 3):\n\nbash hadoop fs -setrep -R 3 /hdfs/path\n\n5. Job execution on HDFS:\n\nI. Submit your data processing jobs (for example, MapReduce and\n\nSpark) on the EMR cluster using the EMR console or via the\n\ncommand line.\n\nII. e jobs will read and write data directly from and to HDFS.\n\n6. Backup and data persistence: As HDFS data is tied to the EMR cluster,\n\nback up your data to Amazon S3 before shutting down the cluster:\n\nbash hadoop distcp hdfs:///path s3://your-s3- bucket/\n\nBy following these action steps, you can successfully implement and\n\nmanage either HDFS or Amazon S3 based on your storage needs.\n\nSee also\n\nHDFS con\u0000guration:\n\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-\n\ncon\u0000g.html\n\nMigrating on-premises HDFS data using AWS DataSync\n\nMigrating large datasets from an on-premises HDFS environment\n\nto Amazon S3 can be complex, but AWS DataSync simpli\u0000es and\n\naccelerates this process. In this recipe, you’ll learn how to use AWS\n\nDataSync to seamlessly transfer data from Hadoop HDFS to\n\nAmazon S3, ensuring a secure and cost-eﬀective migration.\n\nAWS DataSync automates the tasks involved in data transfers, such\n\nas managing encryption, handling scripts, optimizing networks,\n\nand ensuring data integrity. It supports one-time migrations,\n\nongoing work\u0000ows, and automatic replication for disaster recovery,\n\noﬀering transfer speeds up to 10 times faster than open source\n\ntools.\n\nAWS DataSync supports the following for HDFS:\n\nCopying \u0000les and folders between Hadoop clusters and AWS storage\n\nDataSync agents running external to the cluster\n\nTransferring over internet, Direct Connect, or VPN\n\nEnd-to-end data validation\n\nIncremental transfers, \u0000ltering, and scheduling\n\nKerberos authentication\n\nHadoop in-\u0000ight encryption\n\nHadoop at-rest encryption transparent data encryption (TDE) when\n\nusing simple authentication\n\nGetting ready\n\nTo successfully execute DataSync, the following prerequisites must\n\nbe met:\n\nEnsure that you have an AWS account\n\nYour Hadoop cluster should be operational, and you should have\n\nadministrative access to it\n\nNetwork connectivity between your Hadoop environment and AWS\n\nDataSync agent: Installation and con\u0000guration of the DataSync agent on\n\nan on-premises server with access to the HDFS cluster\n\nNetwork connectivity: Ensure network connectivity between the\n\nDataSync agent and your AWS VPC\n\nS3 bucket: An S3 bucket to store the migrated data\n\nIAM role: IAM role for DataSync with permissions to access the S3\n\nbucket and perform data transfer operations\n\nDataSync IAM sample policy\n\nHow to do it...\n\nTo move data from Hadoop HDFS to Amazon S3 using AWS\n\nDataSync, you’ll go through a series of steps that involve setting up\n\nDataSync, con\u0000guring your Hadoop environment, and executing\n\nthe data transfer. Here are the comprehensive steps to walk you\n\nthrough the process:\n\n1. Create a DataSync IAM policy: Create an IAM role with a policy that\n\ngrants DataSync the necessary permissions to access your S3 bucket. e\n\npolicy should look something like this:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\",\n\n\"s3:GetObject\", \"s3:ListBucket\", \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::your-s3-bucket-name\", \"arn:aws:s3:::your-s3-bucket-name/*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"datasync:StartTaskExecution\", \"datasync:ListTasks\", \"datasync:DescribeTaskExecution\" ], \"Resource\": \"*\" } ]\n\n2. Deploy and con\u0000gure the DataSync agent: Deploy the DataSync agent as\n\na virtual machine (VM) in your environment. e agent is available in\n\nvarious formats, such as Open Virtual Appliance (OVA) for VMware,\n\nHyper-V, and AWS EC2.\n\nI. Open the AWS DataSync console\n\nhttps://console.aws.amazon.com/datasync/.\n\nII. In the le navigation pane, choose Agents, and then choose\n\nCreate agent. e following page will open:\n\nFigure 13.5 – Creating a DataSync agent\n\n3. Activate the agent: Aer installing the agent, an activating agent is\n\nneeded. To activate the agent and secure the connection between an\n\nagent and the DataSync service, several network ports should be opened\n\nby the \u0000rewall. For example, if your Hadoop network was inside the on-\n\npremises \u0000rewall, open the outbound traﬃc (Transmission Control\n\nProtocol (TCP) ports 1024–1064) from the DataSync agent to the\n\nVirtual Private Cloud (VPC) endpoint. You also had to open the TCP\n\nport 443 for the entire subnet where the VPC endpoint is located\n\nbecause dynamic IP is assigned to Elastic Network Interface (ENI) for\n\ndata transmission of DataSync. For more detailed network requirements,\n\nyou can refer to the network requirement document\n\n(https://docs.aws.amazon.com/datasync/latest/userguide/agent-\n\nrequirements.html):\n\nOnce deployed, activate the agent using the AWS Management\n\nConsole by entering the agent’s IP address. is step registers the\n\nagent with your AWS account.\n\n4. Con\u0000gure network settings: Ensure the agent can connect to both your\n\nHDFS cluster and the AWS DataSync service by con\u0000guring the\n\nnecessary network settings, such as Domain Name System (DNS) and\n\nrouting.\n\nFigure 13.6 – DataSync agent running\n\n5. Create a DataSync task:\n\nI. De\u0000ne source location (HDFS):\n\ni. In the AWS Management Console, navigate to the\n\nDataSync service and create a new task.\n\nii. Select HDFS as the source location and enter the\n\nrequired details, such as the HDFS NameNode address\n\nand the path to the data you want to migrate.\n\nII. De\u0000ne destination location (Amazon S3):\n\ni. Select Amazon S3 as the destination location.\n\nii. Choose the S3 bucket you created earlier and specify\n\nthe target directory where the data will be stored.\n\nIII. Con\u0000gure data transfer settings:\n\ni. Set transfer options, including bandwidth throttling,\n\ntask scheduling, and data integrity checks.\n\nii. Enable encryption and con\u0000gure other security\n\nsettings to protect your data during transfer.\n\n6. Start and monitor the data transfer:\n\nI. Start the DataSync task:\n\ni. Start the task manually or schedule it to run at a\n\nspeci\u0000c time.\n\nii. Monitor the progress of the data transfer through the\n\nAWS Management Console, where you can view logs\n\nand statistics.\n\nFigure 13.7 – DataSync data transfer monitor\n\nII. Verify data integrity:\n\ni. Once the transfer is complete, verify that all data has been\n\naccurately copied to Amazon S3.\n\nii. Check for any errors or discrepancies in the transfer logs and\n\nresolve them as needed.\n\n7. Verify data transfer:\n\nI. Aer the transfer is complete, verify the data in your S3 bucket\n\nto ensure everything is transferred correctly and completely.\n\nII. Perform any necessary data validation or integrity checks.\n\n8. Post-transfer cleanup and optimization:\n\nI. If it was a one-time transfer, you might want to delete the\n\nDataSync task or agent.\n\nII. For recurring transfers, review the performance and make any\n\nnecessary adjustments.\n\n9. Security and compliance:\n\nI. Ensure that your data transfer complies with your organization’s\n\ndata security and compliance policies.\n\nII. Consider encrypting data both in transit and at rest.\n\nMigrating data from HDFS to Amazon S3 using AWS DataSync is a\n\nstraightforward process that signi\u0000cantly reduces the complexity\n\nand time required for large-scale data transfers. By following the\n\nsteps outlined in this recipe, you can eﬃciently and securely move\n\nyour data to the cloud, taking advantage of Amazon S3’s scalability,\n\ndurability, and integration with other AWS services.\n\nSee also\n\nAWS DataSync FAQs: https://aws.amazon.com/datasync/faqs/\n\nPrerequisites to start running the labs: https://cloudone-\n\ndatamigr.awsworkshop.io/20_prerequisites.html\n\nMigrating the Hive Metastore to AWS\n\ne Hive Metastore is a crucial component within Hadoop, acting\n\nas a centralized storehouse for metadata related to Hive tables,\n\nschemas, and partitions. When transitioning your Hadoop cluster\n\nto the AWS cloud, you can opt to either establish a dedicated Hive\n\nMetastore on AWS or utilize the managed AWS Glue Data Catalog.\n\nMigrating to the AWS Glue Data Catalog provides bene\u0000ts such as\n\nschema versioning and eﬃcient integration with Amazon EMR,\n\nespecially for transient clusters. is migration ensures high\n\navailability, fault tolerance, and better data governance.\n\nWhen it comes to data discovery and management, two data\n\ncatalogs are among the most popular choices:\n\nHive Metastore: is repository houses essential information about Hive\n\ntables and their underlying data structures, including partition names\n\nand data types. Hive, an open source data warehousing and analytics\n\ntool built on Hadoop, can be deployed on platforms such as EMR.\n\nAWS Glue Data Catalog: A fully managed service by AWS, the Glue\n\nData Catalog oﬀers a blend of \u0000exibility and reliability. It’s particularly\n\nwell suited for those starting with metastore creation or management, as\n\nit reduces the need for dedicated resources and hands-on con\u0000guration.\n\nIt’s designed for high availability and fault tolerance, ensuring data\n\ndurability through replication and automatic scaling based on usage. It\n\nalso provides granular control over features such as encryption and\n\naccess management.\n\nChoose migration methods as follows:\n\nDirect migration:\n\nUse AWS Glue ETL jobs to extract metadata from the Hive\n\nMetastore and load it into the Glue Data Catalog\n\nStraightforward for smaller datasets",
      "page_number": 794
    },
    {
      "number": 40,
      "title": "Segment 40 (pages 816-833)",
      "start_page": 816,
      "end_page": 833,
      "detection_method": "synthetic",
      "content": "Federation:\n\nConnect AWS Glue to your existing Hive Metastore for\n\nseamless access without full migration\n\nIdeal for maintaining on-premises metastore or gradual\n\nmigration\n\nird-party tools:\n\nExplore tools such as AWS Schema Conversion Tool (SCT) for\n\nspeci\u0000c migration scenarios\n\nGetting ready\n\nBefore you begin the migration process of your Hive Metastore to\n\nthe AWS Glue Data Catalog, ensure that you have met the following\n\nprerequisites:\n\nAWS account: You need an active AWS account with the necessary\n\npermissions to create and manage AWS Glue, Amazon S3, and related\n\nresources.\n\nExisting Hive Metastore: Ensure you have access to your existing Hive\n\nMetastore, which contains the metadata for your Hive tables and\n\nschemas.\n\nAmazon S3 bucket: Create an Amazon S3 bucket in your AWS account\n\nto store any data that needs to be persisted during the migration.\n\nAWS Glue Data Catalog: Optionally, set up the AWS Glue Data Catalog,\n\nwhich will serve as your centralized metadata repository in the cloud.\n\nis managed service provides a scalable and reliable alternative to a\n\nself-managed Hive Metastore.\n\nNetwork connectivity: Ensure that your on-premises Hadoop cluster,\n\nwhere the Hive Metastore resides, has network connectivity to AWS\n\nservices, including AWS Glue and Amazon S3.\n\nAccess and permissions: Verify that you have the appropriate IAM roles\n\nand policies in place, allowing the migration tool to access the Hive\n\nMetastore, the AWS Glue Data Catalog, and Amazon S3.\n\nBy ful\u0000lling these prerequisites, you’ll be well prepared to migrate\n\nyour Hive Metastore to the AWS Glue Data Catalog, leveraging the\n\nbene\u0000ts of a fully managed, cloud-native metadata repository.\n\nHow to do it…\n\nHere is a step-by-step guide to help you implement metadata\n\nfederation using AWS Glue and Hive Metastore:\n\n1. Create an AWS Glue connection to the Hive Metastore:\n\nI. Set up a JDBC connection in AWS Glue to the Hive Metastore\n\ndatabase (MySQL, PostgreSQL, and so on): Create a Glue\n\nconnection that allows Glue to access your Hive Metastore\n\nusing JDBC:\n\nbash\n\naws glue create-connection --name hive- metastore-connection \\ --connection-input '{ \"Name\": \"hive-metastore-connection\", \"Description\": \"JDBC connection to Hive Metastore\", \"ConnectionType\": \"JDBC\", \"ConnectionProperties\": { \"JDBC_CONNECTION_URL\": \"jdbc:mysql://<hive-metastore- host>:3306/hive_metastore\", \"USERNAME\": \"hiveuser\", \"PASSWORD\": \"hivepassword\" }, \"PhysicalConnectionRequirements\": { \"AvailabilityZone\": \"us-east- 1a\", \"SubnetId\": \"subnet- 0bb1c79de3EXAMPLE\" } }'\n\nNOTE TO REPLACE\n\n<hive-metastore-host>: The host or IP where your Hive\n\nMetastore database is located\n\nhiveuser: Your Hive Metastore database username\n\nhivepassword: Your Hive Metastore database password\n\nII. Verify the connection to ensure it’s set up correctly:\n\nbash\n\naws glue get-connections --name hive- metastore-connection\n\n2. Con\u0000gure AWS Glue to use the connection:\n\nI. Create an AWS Glue job that will access the Hive Metastore\n\nusing the connection you just set up:\n\nbash aws glue create-job --name GlueFederationJob \\ --role arn:aws:iam::<your-account- id>:role/GlueServiceRole \\ --command '{\"Name\": \"glueetl\", \"ScriptLocation\": \"s3://your- bucket/scripts/hive-metadata- federation.py\"}' \\ --connections '{\"Connections\": [\"hive- metastore-connection\"]}' \\ --default-arguments '{ \"--TempDir\": \"s3://your- bucket/temp/\", \"--enable-glue-datacatalog\": \"true\" }'\n\nLet’s break this down:\n\nScriptLocation: e S3 location of your ETL\n\nscript, which we will de\u0000ne next\n\n--connections: is speci\u0000es the JDBC\n\nconnection to your Hive Metastore\n\n--enable-glue-datacatalog: is enables the\n\nuse of the AWS Glue Data Catalog as the metadata\n\nrepository\n\nII. Create the ETL script that reads from the Hive Metastore and\n\nperforms metadata federation. Save this script in the S3 bucket\n\nspeci\u0000ed in ScriptLocation.\n\nYou can \u0000nd the full ETL script (hive-metadata-\n\nfederation.py) here:\n\nhttps://github.com/PacktPublishing/Data-Engineering-\n\nwith-AWS-Cookbook/tree/main/Chapter13.\n\n# Reading from the Hive Metastore datasource = glueContext.create_dynamic_frame.from_ca talog( database=\"hive_database_name\", table_name=\"hive_table_name\" ) # Writing data to S3 (you can modify the output path or format) glueContext.write_dynamic_frame.from_opt ions( frame=datasource, connection_type=\"s3\", connection_options={\"path\": \"s3://your-bucket/output-path\"}, format=\"parquet\" ) job.commit()\n\nLet’s break this down:\n\ndatabase and table_name: Replace with your\n\nHive Metastore’s database and table names\n\noutput-path: Specify the S3 bucket where you\n\nwant to store the output\n\n3. Set up AWS Glue crawlers (optional): If you want AWS Glue to\n\nautomatically crawl your Hive Metastore tables and update the Glue\n\nData Catalog, then perform the following steps:\n\nI. Create a Glue crawler that crawls the data in your Hive\n\nMetastore:\n\nbash aws glue create-crawler --name hive- crawler \\ --role arn:aws:iam::<your-account- id>:role/GlueServiceRole \\ --database-name hive_database \\ --targets '{ \"JdbcTargets\": [{ \"ConnectionName\": \"hive- metastore-connection\", \"Path\": \"hive_metastore\" }] }'\n\nII. Run the crawler to populate the Glue Data Catalog with\n\nmetadata from your Hive Metastore:\n\nbash aws glue start-crawler --name hive- crawler\n\n4. Run the AWS Glue job: Once the Glue job and the script are ready, run\n\nthe job to begin metadata federation:\n\nbash aws glue start-job-run --job-name GlueFederationJob\n\n5. Monitor job execution: You can monitor the job’s progress either in the\n\nAWS Glue console or by using the following command:\n\nbash aws glue get-job-runs --job-name GlueFederationJob\n\nis will give you the status of your Glue job, including whether\n\nit succeeded or failed.\n\n6. Verify the federated metadata: Once the Glue job \u0000nishes, do the\n\nfollowing:\n\nI. Check the AWS Glue Data Catalog: You will see the federated\n\nmetadata from your Hive Metastore in the Glue Data Catalog.\n\nII. Validate the results: Make sure that all your Hive tables,\n\nschemas, and partitions are correctly federated and accessible\n\nvia the Glue Data Catalog.\n\nBy following these steps, you have successfully federated your Hive\n\nMetastore metadata with the AWS Glue Data Catalog. is allows\n\nyou to leverage AWS Glue’s scalable, fully managed capabilities\n\nwithout needing to fully migrate your Hive Metastore. e Glue\n\nData Catalog now acts as a centralized metadata store, facilitating\n\neﬃcient analytics work\u0000ows and seamless access across AWS\n\nservices, such as Amazon Athena, EMR, and Redshi.\n\nMigrating and running Apache Oozie workflows on Amazon EMR\n\nApache Oozie is a popular work\u0000ow scheduler for Hadoop\n\necosystems, orchestrating complex data processing tasks and\n\ndependencies. When migrating from your on-premises Hadoop\n\ncluster to Amazon EMR, you can seamlessly continue using Oozie\n\nor explore alternative AWS services for work\u0000ow orchestration.\n\nIf your migration strategy is “li and shi” and your ETL scripts are\n\nset up to interact with HDFS for both input and output, then your\n\nexisting scripts – including those for Hive, EMR, and Spark –\n\nshould operate eﬀectively in EMR without signi\u0000cant modi\u0000cations.\n\nHowever, if you’ve chosen to re-architect your system during the\n\nmove to AWS and switch to using Amazon S3 as your persistent\n\nstorage layer instead of HDFS, you’ll need to update your scripts.\n\ney must be adapted to work with Amazon S3 (using the s3://\n\nprotocol) via Elastic MapReduce File System (EMRFS).\n\nIn addition to migrating your Hive and Spark scripts, if Apache\n\nOozie is your chosen tool for orchestrating ETL job work\u0000ows, it’s\n\nessential to also strategize for its migration. Let’s explore the\n\navailable options for this process and the methodologies for\n\nmigrating to Oozie.\n\nGetting ready\n\nBefore you begin migrating Apache Oozie to Amazon EMR, ensure\n\nthat you meet the following prerequisites:\n\nActive AWS account: You’ll need an active AWS account with the\n\nnecessary permissions to create and manage EMR clusters and related\n\nresources\n\nOozie work\u0000ows: Access to your existing Oozie work\u0000ows, including\n\nwork\u0000ow de\u0000nitions (XML \u0000les), coordinator de\u0000nitions (if applicable),\n\nand any associated scripts or con\u0000gurations\n\nEMR cluster: An EMR cluster con\u0000gured with the desired Hadoop\n\nversion and components, including Oozie\n\nS3 bucket: An S3 bucket to store your Oozie work\u0000ow de\u0000nitions and\n\nany required libraries or dependencies\n\nNetwork connectivity: Ensure network connectivity between your EMR\n\ncluster and any external systems or data sources required by your Oozie\n\nwork\u0000ows\n\nIAM roles: Con\u0000gure IAM roles with appropriate permissions for EMR\n\nto access S3 and other AWS services used by your work\u0000ows\n\nExisting Oozie work\u0000ows ready for migration\n\nHow to do it…\n\ne following crucial phase in your migration process involves\n\nthorough testing and validation. is ensures that your cluster setup\n\nis functioning correctly and that the data migration was accurate\n\nand complete:\n\n1. Launch an Elastic MapReduce (EMR) cluster:\n\nI. Use the AWS Management Console, Command Line Interface\n\n(CLI), or Soware Development Kit (SDK) to create an EMR\n\ncluster.\n\nII. Choose a compatible EMR release that supports Oozie (for\n\nexample, emr-5.30.0 or later).\n\nIII. Optionally, customize soware settings and instance types\n\nbased on workload requirements.\n\nYou can launch an EMR cluster using the AWS Management\n\nConsole, AWS CLI, or AWS SDK. Here is a CLI example:\n\naws emr create-cluster --name \"Oozie Cluster\" \\ --release-label emr-5.30.0 \\ --applications Name=Hadoop Name=Hive Name=Pig \\ --use-default-roles \\ --instance-type m5.xlarge \\ --instance-count 3\n\nRelease version: Choose an EMR release that supports Oozie\n\n(for example, emr-5.30.0 or later)\n\nInstance types: Customize instance types and the number of\n\ninstances based on your workload requirements\n\n2. Install Oozie:\n\nI. Oozie isn’t installed by default on EMR. Use bootstrap actions\n\nduring cluster creation to install Oozie manually. Follow the\n\nAWS documentation\n\n(https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-\n\noozie.html) for speci\u0000c installation steps and con\u0000guration\n\noptions.\n\nII. Create a bootstrap action script to install Oozie and its\n\ndependencies. Here’s an example script:\n\n!/bin/bash sudo yum install -y oozie oozie-client sudo service oozie start\n\nIII. Add this script during the EMR cluster creation:\n\naws emr create-cluster --name \"Oozie Cluster with Bootstrap\" \\ --release-label emr-5.30.0 \\ --applications Name=Hadoop Name=Hive Name=Pig \\ --bootstrap-actions Path=s3://your- bucket/path-to-bootstrap.sh \\ --use-default-roles \\ --instance-type m5.xlarge \\ --instance-count 3\n\nIV. Follow the AWS documentation\n\n(https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-\n\noozie.html) to con\u0000gure Oozie properly, including setting up\n\noozie-site.xml and ensuring that the Oozie server is\n\nrunning on the master node of the cluster.\n\n3. Prepare work\u0000ows:\n\nI. Review work\u0000ows for compatibility with EMR’s Hadoop\n\ndistribution and con\u0000guration.\n\nII. Address any version-speci\u0000c diﬀerences or dependencies.\n\nIII. Test work\u0000ows in a staging environment before migrating to\n\nproduction.\n\n4. Transfer data:\n\nI. Move the data required for work\u0000ows to S3 or other accessible\n\nstorage within EMR.\n\nII. Ensure proper permissions and access from the cluster.\n\ni. Move data to S3: Transfer the datasets required by your\n\nwork\u0000ows to Amazon S3, making them accessible\n\nfrom the EMR cluster:\n\naws s3 cp /local-data-path s3://your-bucket/data-path -- recursive\n\nii. Permissions: Ensure that the EMR cluster has the\n\ncorrect IAM role permissions to access the S3 bucket\n\nwhere your data is stored.\n\n5. Modify the default EMR role:\n\nI. Go to the IAM console: https://console.aws.amazon.com/iam/.\n\nII. In the le sidebar, select Roles.\n\nIII. Search for the role named EMR_EC2_DefaultRole.\n\nIV. Under the role’s Permissions tab, choose Add inline policy to\n\nde\u0000ne custom permissions for S3 access.\n\nYou can de\u0000ne the permissions using an IAM policy that grants\n\nthe EMR cluster access to speci\u0000c S3 buckets. Here is a sample\n\npolicy that grants read and write access to S3:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:ListBucket\" ], \"Resource\": [ \"arn:aws:s3:::your-bucket- name\", \"arn:aws:s3:::your-bucket- name/*\" ] }, {\n\n\"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:DeleteObject\" ], \"Resource\": \"arn:aws:s3:::your- bucket-name/*\" } ] }\n\nNOTE\n\nPlease update your-bucket-name with the bucket you are planning to\n\nuse.\n\n6. Submit work\u0000ows:\n\nI. Use Oozie’s command-line tools (oozie job -submit) or\n\nweb user interface (UI) to submit work\u0000ows to the EMR cluster.\n\nII. Monitor work\u0000ow execution through Oozie’s web UI or logs:\n\ni. Submit work\u0000ows: Use Oozie’s command-line tools or\n\nweb UI to submit work\u0000ows to your EMR cluster.\n\nHere’s an example of submitting a work\u0000ow using the\n\ncommand line:\n\noozie job -oozie http://<EMR- MasterNode-DNS>:11000/oozie -config /path/to/job.properties -run\n\nii. Monitor execution: Monitor the execution of your\n\nwork\u0000ows through Oozie’s web UI or by reviewing logs\n\ngenerated in the cluster.\n\nALTERNATIVE OPTIONS TO MOVE OUT OF OOZIE\n\nConsider managed work\u0000ow services or AWS Step Functions for\n\nsimpliﬁed orchestration if appropriate for your workloads.\n\nMigrating an Oozie database to the Amazon RDS MySQL\n\nApache Oozie, a widely used work\u0000ow scheduler in the Hadoop\n\necosystem, orchestrates a variety of Hadoop jobs, including Hive,\n\nPig, Sqoop, Spark, DistCp, Linux shell actions, and more. It stands\n\nout in the Hadoop community for its scalability and reliability.\n\nOozie operates with two key components: work\u0000ow jobs, which\n\nallow you to map out work\u0000ow steps in the form of Directed\n\nAcyclic Graphs (DAGs), and the Oozie Coordinator, designed for\n\nscheduling these work\u0000ow jobs based on events or timed triggers.\n\nUsing XML de\u0000nitions, Oozie enables the creation of work\u0000ows\n\nand has been available on Amazon EMR since the 5.0.0 release.\n\nLike Hive, Oozie also relies on a Metastore database, a crucial\n\naspect to consider during migration. When moving Oozie\n\nwork\u0000ows to EMR, it’s essential to transfer both the work\u0000ow\n\nde\u0000nition \u0000les and the Metastore database.\n\nis recipe provides a step-by-step walkthrough of migrating your\n\nOozie database to Amazon Relational Database Service (RDS) for\n\nMySQL. You’ll learn how this migration enhances scalability,\n\nreliability, and maintenance by automating tasks such as backups,\n\npatching, and recovery. By moving to RDS, you can seamlessly\n\nintegrate your Oozie work\u0000ows with AWS services, reduce\n\nmanagement overhead, and create more eﬃcient and secure data\n\npipelines. Follow along to learn how to make this transition and\n\nreap the bene\u0000ts of a managed database service for your Oozie\n\nwork\u0000ows.\n\nGetting ready\n\nBefore migrating your Apache Oozie database to Amazon RDS\n\nMySQL, ensure you have the following prerequisites in place:\n\nAWS account: An active AWS account with permissions to create and\n\nmanage RDS instances and associated resources.\n\nExisting Oozie database: Access to your current Oozie database, which\n\ncould be running on MySQL or another supported database system.\n\nMySQL client: Install a MySQL client on your local machine to interact\n\nwith both your current Oozie database and the RDS MySQL instance.\n\nDatabase dump: Ensure you can export a dump of your existing Oozie\n\ndatabase. is is typically done using tools such as mysqldump for\n\nMySQL databases.\n\nVPC and security groups: Basic understanding of AWS VPCs and\n\nsecurity groups, as they will be needed to con\u0000gure your RDS instance’s\n\nnetwork access.\n\nBackup plan: A backup strategy in place before migrating, ensuring that\n\nyou have a fallback option in case of any issues during the migration\n\nprocess.\n\nAmazon RDS MySQL instance: You will need to create an Amazon RDS\n\nMySQL instance where the Oozie database will be migrated.\n\nIAM roles and permissions: Ensure your AWS account has IAM roles\n\nand permissions to create, manage, and access RDS instances.\n\nNetwork con\u0000guration: Ensure your RDS instance is con\u0000gured to allow\n\nconnections from your current Oozie environment and any\n\nmanagement machines.\n\nDatabase migration tools: Install database migration tools such as\n\nmysqldump for exporting your database and a mysql client for\n\nimporting it into RDS.\n\nHow to do it…\n\n1. Setting up the Amazon RDS MySQL instance: Create a RDS instance\n\nwith the following steps:\n\nI. Go to the AWS Management Console and navigate to the RDS\n\nservice.\n\nII. Click on Create database and choose MySQL as the database\n\nengine.\n\nIII. Select Standard create and con\u0000gure your instance details, such\n\nas database instance class, storage, and VPC settings.\n\nIV. Set the database name, username, and password. is will be\n\nused later for migration.\n\nHere’s an example using AWS CLI:\n\naws rds create-db-instance \\ --db-instance-identifier oozie-db-instance \\ --db-instance-class db.m5.large \\ --engine mysql \\ --allocated-storage 20 \\ --master-username admin \\ --master-user-password yourpassword \\ --vpc-security-group-ids sg-xxxxxxx \\ --availability-zone us-west-2a \\ --db-name oozie\n\n2. Con\u0000gure security groups: Ensure that your RDS instance is accessible\n\nfrom your current Oozie environment. Modify the security group\n\nassociated with the RDS instance to allow incoming MySQL traﬃc (port\n\n3306) from your IP range.",
      "page_number": 816
    },
    {
      "number": 41,
      "title": "Segment 41 (pages 834-853)",
      "start_page": 834,
      "end_page": 853,
      "detection_method": "synthetic",
      "content": "3. Migrating an Oozie database to the Amazon RDS: For the migration of\n\nan Oozie database to the Amazon RDS MySQL engine, follow these\n\nsteps, which will lead you through the export and import procedure:\n\nI. Access the Oozie server node in your on-premises cluster, go to\n\nthe directory containing oozie-setup.sh, and run the\n\nspeci\u0000ed Oozie command to export the Metastore database:\n\n./oozie-setup.sh export /<path>/<oozie- exported-db>.zip\n\nII. In EMR, the oozie-setup.sh \u0000le is in the\n\n/usr/lib/oozie/bin/ directory.\n\nIII. Next, upload the exported database ZIP \u0000le to Amazon S3,\n\nfrom which you can import the database using the following\n\ncommand:\n\naws s3 cp <oozie-exported-db>.zip s3://<bucket-name-path>/<oozie-exported- db>.zip\n\nIV. en, SSH into the EMR master node using Putty to download\n\nthe \u0000le:\n\naws s3 cp s3://<bucket-name- path>/<oozie-exported-db>.zip <oozie- exported-db>.zip\n\nV. Following that, it’s necessary to establish the Oozie database in\n\nAmazon RDS and assign the necessary permissions. Access\n\nyour database with root privileges and run the following\n\ncommands in the MySQL prompt:\n\nmysql> create database oozie default character set utf8; mysql> grant all privileges on oozie.* to 'oozie'@'localhost' identified by 'oozie'; mysql> grant all privileges on oozie.* to 'oozie'@'%' identified by 'oozie';\n\nVI. Once you’ve successfully set up the database with all the\n\nessential permissions, the next step involves importing the\n\ndatabase \u0000le. is can be done using the same oozie-\n\nsetup.sh utility, as demonstrated in the following command:\n\n./oozie-setup.sh import <oozie-exported- db>.zip\n\n4. Update Oozie con\u0000guration: Once the database is prepared with all the\n\nimported metadata, it’s necessary to update the Oozie con\u0000guration in\n\nEMR to redirect it to the new Amazon RDS database. To do this, apply\n\nthe following modi\u0000cations to the oozie-site.xml con\u0000guration\n\n\u0000le:\n\n<property>\n\n<name>oozie.service.JPAService.jdbc.driver</na me> <value>com.mysql.jdbc.Driver</value> </property> <property>\n\n<name>oozie.service.JPAService.jdbc.url</name> <value>jdbc:mysql://<amazon-rds- host>:3306/oozie</value> </property>\n\n<property>\n\n<name>oozie.service.JPAService.jdbc.username</ name> <value><mysql-db-username></value> </property> <property>\n\n<name>oozie.service.JPAService.jdbc.password</ name> <value><mysql-db-password></value> </property>\n\nNOTE\n\nEnsure that you substitute the placeholders with original values in the\n\nprevious code with your speciﬁc database connection details:\n\n<amazon-rds-host>\n\n<mysql-db-username>\n\n<mysql-db-password>\n\nTo apply the updates to Oozie, the ﬁnal step involves restarting\n\nthe Oozie service. This can be done using the following\n\ncommand:\n\nsudo restart oozie\n\nThe outlined steps assist in transferring the Oozie Metastore\n\ndatabase to a remote database on EMR RDS. Following this, your\n\nnext task is to relocate all workﬂow deﬁnition ﬁles to EMR.\n\n5. Migrating Oozie work\u0000ow de\u0000nitions:\n\nFor each work\u0000ow de\u0000nition, you will have a set of essential\n\n\u0000les, including the following:\n\njob.properties\n\nworkflow.xml\n\ncoordinator.xml\n\nAny other related dependent \u0000les\n\nTo back up these \u0000les, you can archive them into a single ZIP\n\n\u0000le. Upload this archive to Amazon S3, and then transfer it to\n\nEMR using the aws s3 cp command to integrate it into your\n\nsystem. Remember to update the work\u0000ow con\u0000guration \u0000le to\n\nalign with the EMR connection.\n\nSubsequently, you can submit your jobs to EMR in the same\n\nmanner as you did in your on-premises setup. e Hue interface is\n\navailable for monitoring your Oozie work\u0000ows on EMR.\n\nSee also\n\nConverting Oozie work\u0000ows to AWS Step Functions with AWS Schema\n\nConversion Tool:\n\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/\n\nbig-data-oozie.html\n\nSetting up networking – establishing a secure connection to your EMR cluster\n\nAmazon EMR provides a managed Hadoop framework in the\n\ncloud, enabling powerful big data processing and analytics.\n\nHowever, ensuring the security of your EMR cluster is crucial to\n\nprotect sensitive data and prevent unauthorized access. is recipe\n\nwill guide you through the essential considerations and steps to\n\nestablish a secure connection to your EMR cluster, safeguarding\n\nyour valuable information.\n\nIn this recipe, we introduce a more secure method for engineers to\n\naccess Amazon EMR cluster instances located in a private subnet,\n\nalong with the traditional approach of using a bastion host or jump\n\nserver with open SSH inbound ports.\n\nGetting ready\n\nBefore setting up a secure network connection, ensure you have the\n\nfollowing prerequisites in place:\n\nActive AWS account: You’ll need an active AWS account with the\n\nnecessary permissions to create and manage EMR clusters and related\n\nresources\n\nEMR cluster: An existing or newly created EMR cluster in your AWS\n\naccount\n\nSSH client: An SSH client installed on your local machine (for example,\n\nPuTTY or OpenSSH)\n\nKey pair: An SSH key pair generated within your AWS account for\n\nsecure authentication\n\nTo ensure secure communication with your EMR cluster, carefully\n\ncon\u0000gure the following network settings:\n\nPublic subnet: If you need to access the EMR cluster from the internet,\n\nensure it’s launched in a public subnet with appropriate security groups\n\nto allow inbound SSH traﬃc (typically on port 22)\n\nPrivate subnet: If your cluster is in a private subnet, consider using a\n\nbastion host or AWS Systems Manager Session Manager for secure\n\naccess\n\nSecurity groups: Con\u0000gure security groups to restrict inbound and\n\noutbound traﬃc to your EMR cluster based on your speci\u0000c needs\n\nIAM roles: Assign appropriate IAM roles to your EMR instances and\n\nusers to control access to AWS services and resources\n\nHow to do it…\n\nYou have three options for establishing a secure connection to your\n\nEMR cluster, each oﬀering diﬀerent levels of security and \u0000exibility.\n\nWe will discuss each method in detail.\n\nMethod 1 – SSH connection (for clusters in public subnets)\n\n1. Retrieve master node public DNS:\n\nI. Open the Amazon EMR console.\n\nII. Navigate to your cluster details and locate the Master public\n\nDNS \u0000eld.\n\n2. Connect using SSH:\n\nI. Open your SSH client.\n\nII. Enter the master node’s public DNS as the hostname.\n\nIII. Specify the appropriate username (for example, hadoop or\n\nec2-user, depending on your EMR con\u0000guration).\n\nIV. Select the private key \u0000le associated with the key pair you\n\ncreated for the cluster.\n\nV. Click on Connect or initiate the SSH connection.\n\nMethod 2 – bastion host (for clusters in private subnets)\n\n1. Launch bastion host:\n\nI. Create an EC2 instance (bastion host) in a public subnet within\n\nthe same VPC as your EMR cluster.\n\nII. Con\u0000gure security groups to allow inbound SSH access from\n\nyour IP address and outbound SSH access to the EMR master\n\nnode’s private IP.\n\n2. Connect to bastion host: Use SSH to connect to the bastion host’s public\n\nIP address using its associated key pair.\n\n3. Connect to EMR master node: From the bastion host, use SSH to\n\nconnect to the EMR master node’s private IP address using the EMR\n\ncluster’s key pair.\n\nMethod 3 – AWS Systems Manager Session Manager (recommended for private subnets)\n\nAWS Systems Manager oﬀers a consolidated UI, enabling you to\n\nmonitor and control your Amazon EC2 instances eﬀectively. Within\n\nthis, Session Manager enhances security and auditability, for\n\ninstance, management.\n\nWhen integrated with IAM, Systems Manager facilitates centralized\n\naccess control to your EMR cluster. However, by default, Systems\n\nManager lacks the necessary permissions to execute actions on\n\ncluster instances. To enable this functionality, you must assign an\n\nIAM role to the instance that carries the required access\n\npermissions. Before beginning this process, it’s important to\n\nestablish an IAM service role for your cluster’s EC2 instances,\n\nensuring it adheres to the principle of least privilege in its access\n\npolicy. e steps are as follows:\n\n1. Enable Session Manager:\n\nI. Ensure Session Manager is enabled for your EMR cluster (it’s\n\noen enabled by default for newer EMR versions).\n\nII. Con\u0000gure an IAM role with necessary permissions for Session\n\nManager access.\n\nIII. Connect using Session Manager:\n\ni. Open the AWS Systems Manager console.\n\nii. Navigate to Session Manager and click on Start session.\n\niii. Select your EMR master node instance from the list.\n\niv. A browser-based shell session will open, providing\n\nsecure access to your EMR cluster without needing\n\nSSH keys or open ports.\n\nIV. Create an IAM service role speci\u0000cally for EMR cluster EC2\n\ninstances (known as the Amazon EMR role for EC2) and\n\nassociate it with the AWS-managed Systems Manager core\n\ninstance policy (AmazonSSMManagedInstanceCore):\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [\n\n\"ssm:DescribeInstanceProperties\", \"ssm:DescribeSessions\",\n\n\"ec2:describeInstances\", \"ssm:GetConnectionStatus\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"ssm:StartSession\" ], \"Resource\": [\n\n\"arn:aws:ec2:${Region}:${Account- Id}:instance/*\" ], \"Condition\": { \"StringEquals\": { \"ssm:resourceTag/ClusterType\": [ \"QACluster\" ] } } } ] }\n\nNow, attach the least privilege policy to the IAM principal (role\n\nor user).\n\nYou can set up the AWS Systems Manager Agent (SSM Agent)\n\non your Amazon EMR cluster nodes using bootstrap actions.\n\ne SSM Agent allows Session Manager to control and con\u0000gure\n\nthese nodes.\n\nUsing Session Manager doesn’t incur extra costs for managing\n\nAmazon EC2 instances, though other features might have\n\nadditional charges (check the Systems Manager pricing page for\n\ndetails). e agent receives and executes instructions from the\n\nSession Manager service in the AWS cloud according to user\n\nrequests. By installing the Systems Manager plugin on your local\n\nmachine, you can use dynamic port forwarding. Also, IAM\n\npolicies help in centrally controlling access to the EMR cluster.\n\n2. Con\u0000guring SSM Agent on an EMR cluster: To set up the SSM Agent on\n\nyour cluster, follow these steps:\n\nI. When you are launching the EMR cluster, go to the Bootstrap\n\nActions section and select Add bootstrap action. Choose\n\nCustom action.\n\nII. Include a bootstrap action that executes a script from Amazon\n\nS3. is script will install and set up the SSM Agent on your\n\nAmazon EMR cluster instances.\n\nIII. e SSM Agent requires a localhost entry in the host’s \u0000le. is\n\nis necessary for redirecting traﬃc from your local computer to\n\nthe EMR cluster instance when you’re using dynamic port\n\nforwarding.\n\nIV. e bootstrap script is as follows:\n\n#!/bin/bash ## Name: SSM Agent Installer Script ## Description: Installs SSM Agent on EMR cluster EC2 instances and update hosts file ## sudo yum install -y https://s3.amazonaws.com/ec2-downloads- windows/SSMAgent/latest/linux_amd64/amaz on-ssm-agent.rpm sudo status amazon-ssm-agent >>/tmp/ssm- status.log ## Update hosts file echo \"\\n ########### localhost mapping check ########### \\n\" > /tmp/localhost.log lhost=`sudo cat /etc/hosts | grep localhost | grep '127.0.0.1' | grep -v '^#'` v_ipaddr=`hostname --ip-address` lhostmapping=`sudo cat /etc/hosts | grep $v_ipaddr | grep -v '^#'` if [ -z \"${lhostmapping}\" ]; then echo \"\\n ########### IP address to localhost mapping NOT defined in hosts files. add now ########### \\n \" >> /tmp/localhost.log sudo echo \"${v_ipaddr} localhost\" >>/etc/hosts else echo \"\\n IP address to localhost mapping already defined in hosts file \\n\" >> /tmp/localhost.log fi\n\necho \"\\n ########### IP Address to localhost mapping check complete and below is the content ########### \" >> /tmp/localhost.log sudo cat /etc/hosts >> /tmp/localhost.log echo \"\\n ########### Exit script ########### \" >> /tmp/localhost.log\n\nV. In the Security options section, navigate to Permissions and\n\nselect Custom. en, for the EMR role, choose the IAM role\n\nthat you previously created.\n\nVI. Once the cluster has successfully launched, go to the Session\n\nManager console and select Managed instances. en, choose\n\nyour cluster instance. From the Actions menu, click on Start\n\nsession.\n\ne following \u0000gure shows AWS Systems Manager:\n\nFigure 13.8 – AWS Systems Manager\n\nVII. To access web UIs of Hadoop applications, such as YARN Resource\n\nManager and the Spark Job Server, on the Amazon EMR primary node,\n\nyou can establish a secure tunnel between your computer and the\n\nprimary node using Session Manager.\n\n3. Install AWS CLI: Please use the instructions at\n\nhttps://docs.aws.amazon.com/cli/v1/userguide/cli-chap-install.html to\n\nset up AWS CLI. Once done, run the following command:\n\naws ssm start-session --target \"Your Instance ID\" --document-name AWS- StartPortForwardingSession --parameters \"portNumber\"=[\"8080\"],\"localPortNumber\"= [\"8158\"]\n\nIn an environment with a multi-tenant Amazon EMR cluster,\n\nyou can limit access to the cluster instances using speci\u0000c\n\nAmazon EC2 tags.\n\nFor instance, in the example code provided, an IAM principal\n\n(either an IAM user or role) is permitted to initiate a session on\n\nany instance (as indicated by the resource Amazon Resource\n\nName (ARN): arn: aws:ec2:::instance/*) under the\n\ncondition that the instance is tagged as TestCluster (denoted\n\nby ssm:resourceTag/ClusterType: TestCluster).\n\nIf the IAM principal attempts to start a session on an instance\n\nthat either lacks a tag or has a tag diﬀerent from ClusterType:\n\nTestCluster, the outcome will display a message indicating\n\nthat they are not authorized to perform the\n\nssm:StartSession action:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ssm:DescribeInstanceProperties\", \"ssm:DescribeSessions\", \"ec2:describeInstances\", \"ssm:GetConnectionStatus\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"ssm:StartSession\" ], \"Resource\": [ \"arn:aws:ec2:${Region}:${Account- Id}:instance/*\" ], \"Condition\": { \"StringEquals\": { \"aws:username\": \"${aws:username}\" }, \"StringLike\": { \"ssm:resourceTag/ClusterType\": [\n\n\"TestCluster\" ] } } } ] }\n\nYou can modify the default login settings to limit root access in user\n\nsessions. Normally, sessions start with ssm-user, but you can opt\n\nto use an operating system account’s credentials by tagging an IAM\n\nuser or role with SSMSessionRunAs or specifying a username.\n\nis is enabled through updates to Session Manager preferences. An\n\nexample con\u0000guration allows the appdev2 IAM user to start\n\nsessions as ec2-user rather than the standard ssm-user.\n\nSee also\n\nBest Practices for Securing Amazon EMR:\n\nhttps://aws.amazon.com/blogs/big-data/best-practices-for-securing-\n\namazon-emr/\n\nAWS Systems Manager: https://aws.amazon.com/systems-manager/\n\nPerforming a seamless HBase migration to AWS\n\nHBase provides a Hadoop-based solution for managing large-scale,\n\nsparse datasets. is non-relational, distributed database excels at\n\nrandom read/write access to data with high volume and variety,\n\nmaking it a valuable tool for big data applications.\n\nMigrating an on-premises Apache HBase database to AWS allows\n\norganizations to leverage the scalability, \u0000exibility, and cost-\n\neﬀectiveness of the cloud. AWS provides several options for\n\nmigrating to HBase, including moving to HBase on Amazon S3\n\n(running on Amazon EMR), or migrating to a fully managed\n\nservice such as Amazon DynamoDB. is recipe outlines the\n\ndetailed steps to perform this migration.\n\nGetting ready\n\nBefore beginning the migration process, ensure the following\n\nprerequisites are met:\n\nAWS account: Ensure you have an AWS account with permissions to\n\ncreate and manage resources such as Amazon EMR, S3, and DynamoDB\n\nExisting HBase cluster: Access to your on-premises HBase cluster,\n\nincluding the HBase tables, con\u0000guration \u0000les, and data\n\nData backup: Back up your HBase data to ensure that you have a fallback\n\noption in case of any issues during migration\n\nAWS CLI and SDK: Install the AWS CLI and AWS SDK if you plan to\n\nautomate or script parts of the migration process\n\nNetwork connectivity: Ensure there is secure and reliable network\n\nconnectivity between your on-premises environment and AWS\n\ne technical requirements are as follows:\n\nAmazon S3 bucket: Create an Amazon S3 bucket where you will store\n\nHBase snapshots or data \u0000les during the migration process\n\nAmazon EMR cluster: If migrating to HBase on Amazon EMR, ensure\n\nyou can provision an EMR cluster with HBase and Hadoop installed\n\nHBase compatibility: Ensure that the version of HBase you are migrating\n\nfrom is compatible with the version of HBase available on Amazon EMR\n\nor that necessary adjustments are planned\n\nIAM roles and permissions: Set up IAM roles with permissions to access\n\nS3, DynamoDB (if applicable), and other AWS services required during\n\nthe migration process\n\nHow to do it…\n\ne migration process involves exporting data from your on-\n\npremises HBase cluster, transferring it to AWS, and then importing\n\nit into the target AWS service. e detailed steps, including sample\n\ncommands and code snippets, are as follows:\n\n1. Migrate HBase on EMR: Export HBase data:\n\nI. Take a snapshot:\n\ni. Create a snapshot of your HBase tables to capture the\n\ncurrent state of your data.\n\nii. Use the HBase shell to create snapshots:\n\necho \"snapshot 'tableName', 'snapshotName'\" | hbase shell\n\niii. Replace tableName with your HBase table’s name\n\nand snapshotName with your desired snapshot\n\nname.\n\nII. Export the snapshot:\n\ni. Use the ExportSnapshot utility to export the\n\nsnapshot to your on-premises \u0000lesystem or directly to\n\nan S3 bucket:\n\nhbase org.apache.hadoop.hbase.snapshot.Ex portSnapshot \\ -snapshot snapshotName \\ -copy-to hdfs://namenode:8020/path- to-backup\n\nii. Utilize the ExportSnapshot tool provided by\n\nHBase to extract data from the snapshot. is tool\n\nconverts the snapshot data into sequence \u0000les, which\n\ncan be processed further.\n\n2. Transfer data to Amazon S3:\n\nI. Copy data to S3: If you have exported the snapshot to a local\n\n\u0000lesystem, use the AWS CLI to copy the snapshot \u0000les to\n\nAmazon S3:\n\naws s3 cp /local-snapshot-path/ s3://your-s3-bucket/snapshot-path/ -- recursive\n\nNOTE\n\nReplace /local-snapshot-path/ with the local directory containing\n\nthe snapshot ﬁles and -s3-bucket/snapshot-path/ with your S3\n\nbucket and path.\n\n3. Set up Amazon EMR cluster:\n\nI. Launch the EMR cluster: Launch an EMR cluster con\u0000gured\n\nwith HBase and Hadoop using the AWS Management Console:\n\naws emr create-cluster --name \"HBase Cluster\" \\ --release-label emr-6.3.0 \\ --applications Name=HBase Name=Hadoop \\ --use-default-roles \\ --instance-type m5.xlarge \\ --instance-count 3\n\nII. Con\u0000gure HBase on EMR: Once the cluster is up, connect to\n\nthe master node and con\u0000gure HBase to restore from the\n\nsnapshot stored in S3.\n\n4. Import HBase data to EMR:",
      "page_number": 834
    },
    {
      "number": 42,
      "title": "Segment 42 (pages 854-876)",
      "start_page": 854,
      "end_page": 876,
      "detection_method": "synthetic",
      "content": "I. Restore snapshot to HBase on EMR: Use the\n\nCloneSnapshot utility to restore the snapshot to HBase on\n\nEMR:\n\nhbase org.apache.hadoop.hbase.snapshot.CloneSn apshot \\ -snapshot snapshotName \\ -restore-to tableName\n\n5. Validate and test the migration:\n\nI. Verify data integrity: Perform checks to ensure that all data has\n\nbeen migrated successfully and that the data integrity is\n\nmaintained.\n\nII. Run tests: Run tests on the migrated data to con\u0000rm that your\n\napplications can interact with it as expected in the new\n\nenvironment.\n\nMigrating HBase to DynamoDB on AWS\n\nMigrating from HBase to DynamoDB can provide signi\u0000cant\n\nadvantages in terms of scalability, managed infrastructure, and cost-\n\neﬀectiveness. HBase is a powerful, distributed, NoSQL database\n\noen used in on-premises or Hadoop-based environments.\n\nHowever, managing an HBase cluster can be complex and resource-\n\nintensive. Amazon DynamoDB oﬀers a fully managed, serverless\n\nalternative with built-in replication, scalability, and integration with\n\nother AWS services, making it a strong candidate for workloads that\n\nneed high availability and elasticity.\n\nSuppose a media streaming company that stores user session logs\n\nand interaction data in an HBase cluster on an on-premises Hadoop\n\nenvironment is looking to migrate to a cloud-based solution. e\n\ncompany needs the ability to handle sudden spikes in traﬃc (such\n\nas during new content releases), while also reducing the overhead of\n\nmanaging their own HBase clusters. Migrating to Amazon\n\nDynamoDB provides the elasticity to handle these spikes, along\n\nwith automatic scaling and reduced operational complexity.\n\nGetting ready\n\nBefore beginning the migration process, ensure the following\n\nprerequisites are met:\n\nAWS account: Ensure you have an AWS account with appropriate\n\npermissions to create and manage DynamoDB, IAM, and data migration\n\nservices\n\nHBase data analysis: Review your existing HBase data model, including\n\ntables, columns, and key structures, as these will need to be restructured\n\nin DynamoDB\n\nAWS CLI installed: Install and con\u0000gure the AWS CLI to interact with\n\nAWS services\n\nDynamoDB table design: Understand DynamoDB’s table structure,\n\nprimary keys, and global/local secondary indexes, as they diﬀer from\n\nHBase’s schema\n\nHow to do it…\n\n1. ExportHBase snapshot: First, you need to create and export a snapshot\n\nof your HBase tables. To create a snapshot in HBase, please follow the\n\nsteps as explained in the previous recipe, Seamless HBase migration to\n\nAWS, under Migrate HBase on EMR (step 1).\n\n2. Transfer data to Amazon S3: Next, you need to transfer the exported\n\nsnapshot data to an Amazon S3 bucket. Please follow the same steps as\n\nexplained previously.\n\n3. Set up AWS Glue for data transformation: Amazon DynamoDB and\n\nHBase have diﬀerent data models, so you may need to transform the\n\ndata before importing it into DynamoDB. AWS Glue is a suitable service\n\nfor this task:\n\nI. Create a Glue crawler: Set up a Glue crawler to catalog the data\n\nyou uploaded to S3. is will automatically infer the schema\n\nand make the data accessible for ETL processes. Use the AWS\n\nManagement Console or CLI to create the crawler:\n\naws glue create-crawler \\ --name your-crawler-name \\ --role your-glue-role \\ --database-name your-database-name \\ --targets S3Targets=[{Path=s3://your-s3- bucket/snapshot-path/}]\n\nII. Run the Glue ETL job: Aer the crawler has created the\n\nmetadata catalog, use a Glue ETL job to transform and load the\n\ndata into DynamoDB. You can use Python scripts in AWS Glue\n\nto perform this transformation.\n\nYou can \u0000nd the full code on the following path:\n\nhttps://github.com/PacktPublishing/Data-Engineering-\n\nwith-AWS-\n\nCookbook/blob/main/Chapter13/dynamodb-import-\n\nglue-job.py.\n\ndynamic_frame = glueContext.create_dynamic_frame.from_ca talog(database = \"your-database-name\", table_name = \"your-table-name\") # Transformation logic here glueContext.write_dynamic_frame.from_opt ions(frame = dynamic_frame, connection_type = \"dynamodb\", connection_options = {\"dynamodb.output.tableName\": \"your- dynamodb-table-name\"})\n\n4. Import data into Amazon DynamoDB: Finally, load the transformed\n\ndata into your DynamoDB table:\n\nI. Create a DynamoDB table: Ensure you have a DynamoDB table\n\ncreated that matches the schema requirements of your\n\ntransformed data. An example CLI command to create a\n\nDynamoDB table is as follows:\n\nbash\n\naws dynamodb create-table \\ --table-name your-dynamodb-table-name \\ --attribute-definitions AttributeName=PrimaryKey,AttributeType=S \\ --key-schema AttributeName=PrimaryKey,KeyType=HASH \\ --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5\n\nII. Load data into DynamoDB: Use AWS Glue, as shown in the\n\npreceding script, to load the transformed data directly into\n\nDynamoDB.\n\nIII. Verify the data import: Aer the ETL job completes, verify the\n\ndata in DynamoDB using the AWS Management Console, CLI,\n\nor by running queries against your DynamoDB table.\n\n5. Validate and optimize:\n\nI. Validation:\n\ni. Run queries and validate that all the data has been\n\nsuccessfully imported into DynamoDB.\n\nii. Check for data integrity and consistency across your\n\nnew DynamoDB tables.\n\nII. Optimization:\n\ni. Adjust DynamoDB read/write capacity based on your\n\napplication’s needs.\n\nii. Set up indexes and optimize the table design for\n\neﬃcient querying.\n\niii. Use Amazon CloudWatch and other AWS tools to\n\nmonitor the performance and health.\n\nMigrating from HBase to Amazon DynamoDB oﬀers operational\n\nsimplicity, scalability, and cost bene\u0000ts. By following this recipe, you\n\ncan eﬀectively move your data from an on-premises HBase cluster\n\nto a fully managed DynamoDB environment. is transition not\n\nonly reduces infrastructure management but also improves your\n\napplication’s availability and elasticity on AWS.\n\nSee also\n\nComparing the Use of Amazon DynamoDB and Apache HBase for\n\nNoSQL:\n\nhttps://d1.awsstatic.com/whitepapers/AWS_Comparing_the_Use_of_Dy\n\nnamoDB_and_HBase_for_NoSQL.pdf\n\nMigrating and restoring Apache HBase tables on Apache HBase on\n\nAmazon S3: https://docs.aws.amazon.com/whitepapers/latest/migrate-\n\napache-hbase-s3/migrating-and-restoring-apache-hbase-tables-on-\n\napache-hbase-on-amazon-s3.html\n\nIntroduction to Apache HBase Snapshots:\n\nhttps://blog.cloudera.com/introduction-to-apache-hbase-snapshots/\n\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/\n\nOceanofPDF.com\n\nIndex\n\nAs this ebook edition doesn't have \u0000xed pagination, the page numbers\n\nbelow are hyperlinked for reference only, based on the printed edition\n\nof this book.\n\nA\n\nAccess Control List (ACLs) 453\n\nAmazon AppFlow 381\n\nAmazon Athena 378, 379, 382\n\ncomparing, with Redshi Spectrum and S3 Select 345\n\nAmazon CloudWatch 380, 383\n\nreference link 314\n\nAmazon CloudWatch Events 119\n\nAmazon Comprehend 379\n\nAmazon DataZone\n\nconsumer access request, approving 224\n\ndata, consuming from domain 223\n\ndata, publishing to domain 219-222\n\ndomain, creating 219\n\nAmazon DataZone, with data mesh principles\n\nused, for building distributed data community 218-224\n\nAmazon Elastic Block Store (EBS) 441\n\nAmazon Elastic Compute Cloud (EC2) instances 443\n\nAmazon Elastic MapReduce (EMR) 378, 379, 382\n\nApache Oozie, migrating and running on 467-471\n\nAmazon EMR TCO simulator\n\nHadoop migration assessment to 446\n\nAmazon EventBridge 300\n\nevent-driven orchestration, setting up with 119-127\n\nAmazon Kinesis 378-381\n\nAmazon Machine Images (AMIs) 183\n\nAmazon Macie 380\n\nAmazon Managed Work\u0000ows for Apache Air\u0000ow (MWAA) 82\n\ndata pipelines, managing with 132-146\n\nAmazon OpenSearch 382\n\nAmazon QuickSight 378, 379, 383\n\nAmazon RDS/Aurora 379, 382\n\nAmazon RDS MySQL\n\nApache Oozie database, migrating to 471-475\n\nAmazon Recognition 379\n\nAmazon Redshi 377, 379, 382\n\nsystem tables 361\n\nAmazon Redshi Spectrum 341, 379, 382\n\nlarge historical data, querying 342-345\n\nrequisites 341\n\nAmazon Resource Name (ARN) 10, 365, 481\n\nAmazon SageMaker 379, 383\n\nAmazon Simple Storage Service (Amazon S3) 1, 378, 379, 382, 449\n\ndata, replicating 19-22\n\ndata, versioning 15-19\n\nenabling and monitoring 22-29\n\nencryption, enforcing 7-11\n\nstorage types, for optimized storage costs 4-7\n\nAmazon States Language (ASL) 131\n\nAmazon Web Services (AWS) 439\n\nHive Metastore, migrating to 462-467\n\nseamless HBase migration, performing to 482-484\n\nanalysis 369\n\nApache Kaa 381\n\nApache Oozie\n\ndatabase, migrating to Amazon RDS MySQL 471-475\n\nmigrating and running, on Amazon EMR 467-471\n\nApache Spark\n\nresource usage, tuning 187-192\n\nApplication Manager (AM) 187\n\natomicity, consistency, isolation, and durability (ACID) 94\n\nAvailability Zones (AZs) 341\n\nAWS AI services 383\n\nAWS analytics ecosystem 377, 378\n\nAWS analytics services architecture 380\n\ncost management 383\n\ndata analytics 382\n\ndata cataloging and governance 382\n\ndata ingestion 381\n\ndata lake storage 382\n\ndata processing and transformation 382\n\ndata visualization and BI 383\n\ndata warehousing 382\n\nmachine learning and AI 383\n\nsecurity, governance, and monitoring 383\n\nAWS Application Migration Service (MGS) 375\n\nAWS architectures\n\nAWS data mesh architecture 386\n\nconsiderations 386\n\ndata lakehouse architecture 385\n\nRedshi-centric architecture 384, 385\n\nserverless analytics architecture 385\n\nAWS CLI\n\nreference link 167\n\nAWS Cloud Development Kit (AWS CDK) 355\n\nAWS CloudTrail 380, 383\n\nAWS CodePipeline\n\nused, for setting up code deployment pipeline 266-272\n\nAWS Con\u0000g 383\n\nreference link 327\n\nS3 compliance, ensuring with 230-236\n\nusing, to automate non-compliance S3 server access logging\n\npolicy 322-326\n\nAWS Con\u0000g rules\n\nSystem Manager, setting up to remediate non-compliance with\n\n319-321\n\nAWS Cost Explorer 380, 383\n\nAWS Database Migration Service (DMS) 375\n\nAWS Data Lake\n\ncost per analytics workload, tracking 327-333\n\nAWS data mesh architecture 386\n\nAWS Data Migration Service (DMS) 379\n\nAWS DataSync 381\n\nreference link 456\n\nused, for migrating on-premises HDFS data 457-462\n\nAWS data warehouse\n\nbene\u0000ts 384\n\nAWS DMS tool 381\n\nused, for data migration 428\n\nused, for extracting data 417-424\n\nAWS DynamoDB 378\n\nAWS EMR clusters\n\nhigh availability 177-180\n\nrunning, on EKS 167-170\n\nAWS EMR serverless\n\nused, for running jobs 163-167\n\nAWS Glue 295, 378, 382\n\nintegrating 295-298\n\nused, for pipeline setup to ingest data from JDBC database 150-\n\n160\n\nused, for running pandas code for Ray 103-106\n\nAWS Glue Catalog 380\n\nusing, from another account 171-176\n\nAWS Glue DataBrew 382, 379\n\nAWS Glue Data Catalog 462\n\nAWS Glue Data Quality\n\nversus Deequ 244\n\nAWS Glue Data Quality, performance for ETL pipelines\n\nreference link 244\n\nAWS Glue Schema Registry\n\nreferences 252\n\nAWS Glue Studio\n\nused, for creating ETL visual jobs 71-75\n\nAWS Glue work\u0000ows\n\nused, for de\u0000ning simple work\u0000ow 108-119\n\nAWS Identity and Access Management (IAM) 380, 383\n\nAWS IoT Core 381\n\nAWS Key Management Service (KMS) 349, 380, 383, 434\n\nAWS Kinesis Data Analytics 382\n\nAWS Lake Formation 378, 382\n\nAWSLakeFormationDataAdmin 42\n\nAWS Lambda 378, 379, 382\n\nAWS Managed Streaming 381\n\nAWS Managed Work\u0000ows for Apache Air\u0000ow (MWAA) 378\n\nAWS QuickSight 382\n\nAWS RDS Oracle instance\n\nsetting up 426\n\nAWS resources\n\ntagging strategy, establishing 214-218\n\nAWS Schema Conversion Tool (SCT) 372\n\ninstalling and setting up 426, 427\n\nJDBC drivers, con\u0000guring 409-412\n\nSCT migration assessment report, creating with 406-409\n\nsource and target schemas, mapping 412-417\n\nused, for migrating Oracle database from local laptop to AWS\n\nRDS 425\n\nAWS SDK for pandas\n\nusing, to execute SQL statements 362-367\n\nAWS SDK for Python 368\n\nusing, to manage Amazon QuickSight 368\n\nAWS Server Migration Service (SMS) 375\n\nAWS Snowball 381\n\nAWS Snow Family\n\nlarge data migration manager, as service 430-437\n\nleveraging, for large-scale data migration 429, 430\n\nAWS SSM Explorer 321\n\nAWS Step Functions 471\n\ndata work\u0000ow, creating with 127-132\n\nAWS Systems Manager Session Manager 477-482\n\nAWS Systems Manager (SSM) 349\n\nAWS TCO calculators 441\n\nused, for calculating total cost of ownership (TCO) 440-445\n\nAWS Trusted Advisor 380, 383\n\nAWS Well-Architected Labs\n\nreference link 318\n\nB\n\nbastion host 477\n\nbig bang migration 376\n\nbig data frameworks\n\nETL processes, converting with 392-395\n\nbookmarks and bounded execution\n\nused, for processing data incrementally 83-87\n\nbootstrap actions\n\nused, for customizing cluster 183-187\n\nboto3 library 369\n\nbusiness intelligence (BI) 346, 371\n\nC\n\ncatalog data retrieval\n\noptimizing, with pushdown \u0000lters and indexes 98-102\n\ncatalog table\n\ndata ingestion, from JDBC database to 150-160\n\ncdk bootstrap command 272\n\nCDK pipeline\n\nsetting up, to deploy multiple accounts and regions 273-276\n\nCDKToolkit 272\n\nClassless Inter-Domain Routing (CIDR) 341\n\nclient-side encryption 8\n\nCloud Development Kit (CDK) 265\n\nused, for setting up code deployment pipeline 267-272\n\ncloud-enabled enterprise Data Lake\n\nreference link 318\n\nCloudFormation deployment\n\ncode, running 277-282\n\nCloudWatch data\n\nread-only access, sharing to 64-67\n\nCloudWatch log group retention\n\nsetting, automatically to reduce cost 300-314\n\ncluster\n\ncustomizing, with bootstrap actions 183-187\n\nmonitoring 196-201\n\nprotecting, from security vulnerabilities 201, 202\n\nscaling, based on workload 181-183\n\ncode deployment pipeline\n\nsetting up, with AWS CodePipeline 266-272\n\nsetting up, with CDK 266-272\n\nCommand-Line Interface (CLI) 436, 455, 468\n\nConsole Recorder for AWS 294\n\nconsumer access request\n\napproving 224\n\ncontinuous integration/continuous deployment (CI/CD) 265, 369\n\nCross-Region Replication (CRR) 19\n\ncustom dashboards\n\ncreating, to monitor Data Lake services 314-318\n\ncustom WLM rules 340\n\nD\n\ndashboard 369\n\ndata\n\nconsuming, from domain 223\n\nextracting, with AWS DMS 417-424\n\nprocessing, incrementally with bookmarks and bounded\n\nexecution 83-87\n\npublishing, to domain 219-222\n\nreplicating 19-21\n\nstoring, considerations 449-457\n\nstoring, with data lake formats 94-97\n\nversioning 15-19\n\nDatabase Migration Service (DMS) 405\n\nDataBrew\n\nused, for building data cleaning and pro\u0000ling jobs 256-263\n\ndata cleaning\n\nbuilding, with DataBrew 256-263\n\ndata ingestion, from JDBC database\n\nAWS Glue, used for setting up pipeline for 150-160\n\nData Lake environment\n\nreference link 318\n\ndata lake formats\n\nusing, to store data 94-97\n\ndata lakehouse architecture 385\n\nData Lake services\n\ncustom dashboards, creating to monitor 314-318\n\ndata migration services, and tools\n\nconsiderations 386\n\ndata pipelines\n\ndeploying, with Terraform 285-290\n\nhealth, monitoring 146-150\n\nmanaging, with MWAA 132-146\n\ndata quality\n\ncreating, for ETL jobs in AWS Glue Studio notebooks 238-243\n\nunit testing, with Deequ 244-248\n\nData Quality De\u0000nition Language (DQDL) 206, 240\n\ndataset 369\n\ndata work\u0000ow\n\ncreating, with AWS Step Functions 127-132\n\nDeequ\n\ncomponents 248\n\nexamples 248\n\nused, for unit testing data quality 244-248\n\nversus AWS Glue Data Quality 244\n\nDeequ GitHub page\n\nreference link 244\n\nDirected Acyclic Graphs (DAGs) 471\n\nDistCp\n\nreference link 456\n\ndistributed data community\n\nbuilding, with Amazon DataZone following data mesh principles\n\n218-224\n\ndomain\n\ncreating 219\n\ndata, consuming from 223\n\ndata, publishing to 219-222\n\nDomain Name System (DNS) 460\n\nDynamoDB, on AWS\n\nHBase, migrating to 485-487\n\nE\n\nEKS CLI\n\nreference link 167\n\nElastic Container Registry (ECR) 97\n\nElastic Kubernetes Service (EKS)\n\nAWS EMR cluster, running on 167-170\n\nElastic MapReduce (EMR) 439\n\nElastic MapReduce File System (EMRFS) 467\n\nElastic Network Interface (ENI) 460\n\nEMR cluster\n\nsecure connection, establishing to 475, 476",
      "page_number": 854
    },
    {
      "number": 43,
      "title": "Segment 43 (pages 877-899)",
      "start_page": 877,
      "end_page": 899,
      "detection_method": "synthetic",
      "content": "EMR cluster, secure network connection\n\nAWS Systems Manager Session Manager 477-482\n\nbastion host 477\n\nSSH connection 476\n\nEMR, with Workspaces\n\ncode development on 192-196\n\nEMR Workspaces 193\n\nETL jobs\n\nparameters, making more \u0000exible and reusable 75-78\n\nETL jobs, in AWS Glue Studio notebooks\n\ndata quality, creating for 238-243\n\nETL pipelines\n\nschema management for 249-252\n\nunit test functions, building 252-255\n\nETL processes\n\nconverting, with big data frameworks 392-395\n\nETL visual jobs\n\ncreating, with AWS Glue Studio 71-75\n\nEvent Bridge, with Lambda\n\ncombining, architecture option 314\n\nevent-driven architecture (EDA) 367\n\nevent-driven orchestration\n\nsetting up, with Amazon EventBridge 120-127\n\nextract, transform, load (ETL) 69, 108, , 243, 335, 378, 439\n\nF\n\nfederated query access\n\nusing 346-348\n\n\u0000ne-grained permissions, on S3 data sharing\n\nenforcing, with Lake Formation 49-57\n\nformat-preserving encryption (FPE) 228\n\nFormer 2\n\nreference link 294\n\nG\n\nGanglia 192\n\nGeneral Data Protection Regulation (GDPR) 453\n\nGit version control\n\nintegrating 295-298\n\nGlue Data Catalog\n\nsynchronizing, to diﬀerent account 41-47\n\nGlue Data Quality 206\n\nGlue job\n\nlibraries, reusing in 91-93\n\nretry stages 82\n\nGlue tables\n\ndata quality check, applying on 206-209\n\nH\n\nHadoop\n\nmigration process, de\u0000ning and executing 395-398\n\nsecurity authentication and authorization processes, migrating\n\n398-403\n\nHadoop Distributed File System (HDFS) 82, 394, 441, 450\n\nHadoop migration assessment\n\nconducting, with TCO simulator 446-449\n\ndata, storing considerations 449-457\n\nto Amazon EMR TCO simulator 446\n\nHadoop Migration Delivery Kit (HMDK) 446\n\nHBase\n\nmigrating, to DynamoDB on AWS 485-487\n\nHealth Insurance Portability and Accountability Act (HIPAA) 453\n\nHigh Availability (HA) 177\n\nHive Metastore 462\n\nmigrating, to AWS 462-467\n\nhybrid cloud environment 388\n\nI\n\nIaC reverse-engineering 291-294\n\nIAMAllowedPrincipals principle 42\n\nIAM only con\u0000guration\n\nusing 175\n\nIdentity and Access Management (IAM) 2, 407, 453\n\nfor Redshi 341\n\nInfrastructure as Code (IaC) 265, 355\n\nInfrequent Access (IA) 450\n\nIntelligent-Tiering 6\n\nJ\n\nJava Database Connectivity (JDBC) 87, 335\n\ndrivers, con\u0000guring, in AWS SCT tool 409-412\n\nJava Runtime Environment (JRE) 407\n\njob failures and reruns\n\nhandling, for partial results 78-82\n\njobs\n\nrunning, with AWS EMR serverless 163-166\n\nK\n\nKey Management Service (AWS KMS) 8\n\nKubernetes client\n\ndownload link 170\n\nL\n\nLake Formation 380\n\nused, for enforcing \u0000ne-grained permission on S3 data sharing\n\n49-56\n\nLambda\n\nusing, to execute SQL statements 362-367\n\nlarge historical data\n\nquerying, with Redshi Spectrum 342-345\n\nlarge-scale data migration\n\nAWS Snow Family, leveraging for 429, 430\n\nlogging costs\n\nreducing, by setting automatically CloudWatch log group\n\nretention 300-314\n\nM\n\nmachine learning (ML) 106\n\nmanaged work\u0000ow services 471\n\nmassively parallel processing (MPP) 384\n\nmaster 177\n\nmaster nodes 180\n\nmemory overhead 191\n\nmigration approach\n\nselecting, for workload 387-389\n\nmigration methods\n\nselecting 463\n\nmigration process\n\nde\u0000ning and executing, with Hadoop 395-398\n\nmigrations\n\ntypes 424\n\nMulti-Factor Authentication (MFA) Delete 19\n\nmulti-primary mode 32\n\nN\n\nnatural language processing (NLP) 379\n\nNetwork Load Balancer (NLB) 355\n\nnon-compliance S3 server access logging policy\n\nautomating, with AWS con\u0000g 322-326\n\nnotebooks 193\n\nO\n\non-premise platform to AWS, migration\n\ndiscovery and planning (assessment phase) 373, 374\n\nmigration execution (migration phase) 373-375\n\nmigration planning (planning phase) 373-375\n\noptimization and cutover 374-376\n\ntesting and validation (testing phase) 374, 375\n\non-premises HDFS data\n\nmigrating, with AWS DataSync 457-462\n\non-premises platform to AWS, migration\n\nstages 373-376\n\nOozie Coordinator 471\n\nOpen Database Connectivity (ODBC) endpoint 359\n\nOpen Virtual Appliance (OVA) 459\n\nOracle database\n\nAWS RDS Oracle instance, setting up 426\n\nAWS SCT, installing and setting up 426, 427\n\ndata migration, with AWS DMS 428\n\ninstalling, on laptop 425\n\nmigrating, from local laptop to AWS RDS with AWS SCT 425\n\nschema conversion, with SCT 427\n\nveri\u0000cation and post-migration steps 428, 429\n\nP\n\npandas code\n\nrunning, with AWS Glue for Ray 103-106\n\npending authorization status 40\n\nphased migration 376\n\npresigned URL\n\nused, for sharing S3 data 57, 58\n\nprimary replication modes, RDS read replicas\n\nmulti-primary mode 32\n\nsingle-primary mode 32\n\npro\u0000ling jobs\n\nbuilding, with DataBrew 256-263\n\nprototyping and testing\n\nplanning for 390-392\n\npushdown \u0000lters and indexes\n\nused, for optimizing catalog data retrieval 98-102\n\nput_retention_policy\n\nreference link 314\n\nPySpark Testing\n\nreference link 256\n\nPython libraries, with AWS Glue\n\nreference link 256\n\nR\n\nRatioOfSums 244\n\nRay\n\nAWS Glue, used for running pandas code for 103-106\n\nread-only replicas\n\ncreating, for RDS 32-35\n\nre-architecting 373, 388\n\nRedshi-centric architecture 384, 385\n\nRedshi cluster\n\naccessing, with JDBC to query data 356-360\n\nVPC endpoint, creating to 350-355\n\nRedshi Data API\n\nusing, to execute SQL statements 362-367\n\nRedshi Processing Unit (RPU) 342\n\nRedshi producer cluster\n\nlive data sharing 36-40\n\nRedshi Spectrum 382\n\ncomparing, with Amazon Athena and S3 Select 345\n\nRedshi workload management (WLM)\n\nusing, to manage workload priority 336-340\n\nrefactoring 373\n\nrehosting 373, 388\n\nRelational Database Service (RDS) 407, 471\n\nread-only replicas, creating for 32-35\n\nreplatforming 373, 376\n\nconsiderations 380\n\non AWS 377\n\nResource Access Manager (RAM) 41\n\nresource protection\n\nfrom accidental deletion 282-285\n\nretention policies, for objects\n\nsetting up 11-15\n\nS\n\nS3 bucket 341\n\naccess, controlling to 2-4\n\nS3 compliance\n\nensuring, with AWS Con\u0000g 230-236\n\nS3 data\n\nreal-time sharing 59-64\n\nS3 data sharing\n\nLake Formation, used for enforcing \u0000ne-grained permissions on\n\n49-56\n\npresigned URL, using for 57, 58\n\ns3_deployer utility 276\n\nS3 logging options\n\nreference link 30\n\nS3 monitoring tools\n\nreference link 30\n\nS3 Select\n\ncomparing, with Redshi Spectrum and Amazon Athena 345\n\nSame-Region Replication 19\n\nSchema Conversion Tool (SCT) 405, 463\n\nschema management\n\nfor ETL pipelines 249-252\n\nSCT migration assessment report\n\ncreating, with AWS SCT 406-409\n\nseamless HBase migration\n\nperforming, to AWS 482-484\n\nSecure Shell (SSH) 456\n\nSecure Sockets Layer/Transport Layer Security (SSL/TLS) 453\n\nsecure tunnel 480\n\nsecurity-sensitive data (PII and PHI)\n\nhandling 224-229\n\nsensitive data, on S3 buckets\n\ndiscovery and report, automating of 210-213\n\nserverless analytics architecture 385\n\nserver-side encryption (SSE) 8\n\nServer-Side Encryption with Amazon S3-Managed Keys (SSE-S3) 8,\n\n453\n\nServer-Side Encryption with AWS Key Management Service-\n\nManaged Keys (SSE-KMS) 453\n\nservice-level agreement (SLA) 22\n\nshort query acceleration (SQA) 340\n\nSimple Noti\u0000cation Service (SNS) 300, 367\n\nSimple Queue Service (SQS) 300\n\nSimple Storage Service (S3) 335, 439\n\nsimple work\u0000ow\n\nde\u0000ning, with AWS Glue work\u0000ows 108-119\n\nsingle-primary mode 32\n\nSlowly Changing Dimension Type 2 (SCD2) 385\n\nsmall \u0000les, in job\n\nhigh quantity, handling of 87-90\n\nSOCKS5 proxy 196\n\nSoware Development Kit (SDK) 468\n\nSparkUI job run 171\n\nSQL client 341\n\nSQL statements\n\nexecuting, with AWS SDK for pandas 362-367\n\nexecuting, with Lambda 362-367\n\nexecuting, with Redshi Data API 362-367\n\nSSH connection 476\n\nstandard workers 115\n\nSUBNET variable 177\n\nSUPERUSER role 346\n\nSwitchyOmega\n\nauto-switch 197\n\nSystem Identi\u0000er (SID) 425\n\nSystem Manager (SSM)\n\nsetting up, to remediate non-compliance with AWS Con\u0000g rules\n\n319-321\n\nSystems Manager Agent (SSM Agent) 478\n\nT\n\nTCO simulator\n\nused, for conducting Hadoop migration assessment 446-449\n\ntemplate 369\n\nTerraform\n\nused, for deploying data pipeline 285-290\n\nterraform fmt command 290\n\ntotal cost of ownership (TCO) 71\n\ncalculating, with AWS TCO calculators 440-445\n\ntransformation_ctx parameter 86\n\nTransmission Control Protocol (TCP) ports 460\n\ntransparent data encryption (TDE) 457\n\nU\n\nunit test 244\n\nunit test functions\n\nbuilding, for ETL pipelines 252-255\n\nuser-de\u0000ned function (UDF) 81, 224\n\nV\n\nvirtual machine (VM) 459\n\nVirtual Private Cloud (VPC) 374, 460\n\nVPC endpoints 349\n\ncreating, to Redshi cluster 350-355\n\nW\n\nwork\u0000ow jobs 471\n\nworkload\n\ncorrect migration approach, selecting for 387-389\n\nworkload priority\n\nmanaging, with Redshi workload management (WLM) 336-340\n\nY\n\nYARN application list 199\n\nOceanofPDF.com\n\npacktpub.com\n\nSubscribe to our online digital library for full access to over 7,000\n\nbooks and videos, as well as industry leading tools to help you plan\n\nyour personal development and advance your career. For more\n\ninformation, please visit our website.\n\nWhy subscribe?\n\nSpend less time learning and more time coding with practical eBooks\n\nand Videos from over 4,000 industry professionals\n\nImprove your learning with Skill Plans built especially for you\n\nGet a free eBook or video every month\n\nFully searchable for easy access to vital information\n\nCopy and paste, print, and bookmark content\n\nDid you know that Packt oﬀers eBook versions of every book\n\npublished, with PDF and ePub \u0000les available? You can upgrade to\n\nthe eBook version at packtpub.com and as a print book customer,\n\nyou are entitled to a discount on the eBook copy. Get in touch with\n\nus at customercare@packtpub.com for more details.\n\nAt www.packtpub.com, you can also read a collection of free\n\ntechnical articles, sign up for a range of free newsletters, and receive\n\nexclusive discounts and oﬀers on Packt books and eBooks.\n\nOther Books You May Enjoy\n\nIf you enjoyed this book, you may be interested in these other\n\nbooks by Packt:\n\nData Engineering with Databricks Cookbook\n\nPulkit Chadha\n\nISBN: 978-1-83763-335-7\n\nPerform data loading, ingestion, and processing with Apache Spark\n\nDiscover data transformation techniques and custom user-de\u0000ned\n\nfunctions (UDFs) in Apache Spark\n\nManage and optimize Delta tables with Apache Spark and Delta Lake\n\nAPIs\n\nUse Spark Structured Streaming for real-time data processing\n\nOptimize Apache Spark application and Delta table query performance\n\nImplement DataOps and DevOps practices on Databricks\n\nOrchestrate data pipelines with Delta Live Tables and Databricks\n\nWork\u0000ows\n\nImplement data governance policies with Unity Catalog\n\nData Engineering with Google Cloud Platform\n\nAdi Wijaya\n\nISBN: 978-1-83508-011-5\n\nLoad data into BigQuery and materialize its output\n\nFocus on data pipeline orchestration using Cloud Composer\n\nFormulate Air\u0000ow jobs to orchestrate and automate a data warehouse\n\nEstablish a Hadoop data lake, generate ephemeral clusters, and execute\n\njobs on the Dataproc cluster\n\nHarness Pub/Sub for messaging and ingestion for event-driven systems\n\nApply Data\u0000ow to conduct ETL on streaming data\n\nImplement data governance services on Google Cloud\n\nPackt is searching for authors like you\n\nIf you’re interested in becoming an author for Packt, please visit\n\nauthors.packtpub.com and apply today. We have worked with\n\nthousands of developers and tech professionals, just like you, to\n\nhelp them share their insight with the global tech community. You\n\ncan make a general application, apply for a speci\u0000c hot topic that we\n\nare recruiting an author for, or submit your own idea.\n\nShare Your Thoughts",
      "page_number": 877
    },
    {
      "number": 44,
      "title": "Segment 44 (pages 900-919)",
      "start_page": 900,
      "end_page": 919,
      "detection_method": "synthetic",
      "content": "Now you’ve \u0000nished Data Engineering with AWS Cookbook, we’d\n\nlove to hear your thoughts! If you purchased the book from\n\nAmazon, please click here to go straight to the Amazon review page\n\nfor this book and share your feedback or leave a review on the site\n\nthat you purchased it from.\n\nYour review is important to us and the tech community and will\n\nhelp us make sure we’re delivering excellent quality content.\n\nDownload a free PDF copy of this book\n\nanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print\n\nbooks everywhere?\n\nIs your eBook purchase not compatible with the device of your\n\nchoice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF\n\nversion of that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste\n\ncode from your favorite technical books directly into your\n\napplication.\n\ne perks don’t stop there, you can get exclusive access to discounts,\n\nnewsletters, and great free content in your inbox daily\n\nFollow these simple steps to get the bene\u0000ts:\n\n1. Scan the QR code or visit the link below\n\nhttps://packt.link/free-ebook/9781805127284\n\n2. Submit your proof of purchase\n\n3. at’s it! We’ll send your free PDF and other bene\u0000ts to your email\n\ndirectly\n\nOceanofPDF.com\n\nContents\n\n1. Data Engineering with AWS Cookbook 2. Contributors 3. About the authors 4. About the reviewers 5. Preface\n\n1. Who this book is for 2. What this book covers 3. To get the most out of this book 4. Download the example code files 5. Conventions used 6. Sections\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also 7. Get in touch 8. Share Your Thoughts 9. Download a free PDF copy of this book\n\n6. Chapter 1: Managing Data Lake Storage\n\n1. Technical requirements 2. Controlling access to S3 buckets\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Storage types in S3 for optimized storage costs\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n4. Enforcing encryption on S3 buckets\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Setting up retention policies for your objects\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n6. Versioning your data 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n7. Replicating your data 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n8. Monitoring your S3 bucket\n\n1. Getting ready 2. How to do it…\n\n3. How it works… 4. There’s more… 5. See also\n\n7. Chapter 2: Sharing Your Data Across Environments and Accounts\n\n1. Technical requirements 2. Creating read-only replicas for RDS\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Redshift live data sharing among your clusters\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more…\n\n4. Synchronizing Glue Data Catalog to a different account\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Enforcing fine-grained permissions on S3 data sharing using Lake Formation 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n6. Sharing your S3 data temporarily using a presigned URL\n\n1. Getting ready\n\n2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n7. Real-time sharing of S3 data\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n8. Sharing read-only access to your CloudWatch data with another AWS account 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n8. Chapter 3: Ingesting and Transforming Your Data with AWS Glue\n\n1. Technical requirements 2. Creating ETL jobs visually using AWS Glue Studio\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Parameterizing jobs to make them more flexible and reusable\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more…\n\n5. See also\n\n4. Handling job failures and reruns for partial results\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Processing data incrementally using bookmarks and bounded execution 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n6. Handling a high quantity of small files in your job\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n7. Reusing libraries in your Glue job\n\n1. Getting ready 2. How to do it... 3. How it works… 4. There’s more... 5. See also\n\n8. Using data lake formats to store your data\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n9. Optimizing your catalog data retrieval using pushdown filters and indexes 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n10. Running pandas code using AWS Glue for Ray\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n9. Chapter 4: A Deep Dive into AWS Orchestration Frameworks\n\n1. Technical requirements 2. Defining a simple workflow using AWS Glue workflows\n\n1. Getting ready 2. How to do it… 3. See also\n\n3. Setting up event-driven orchestration with Amazon EventBridge\n\n1. Getting ready 2. How to do it…\n\n4. Creating a data workflow using AWS Step Functions\n\n1. How to do it… 2. See also\n\n5. Managing data pipelines with MWAA\n\n1. How to do it… 2. See also\n\n6. Monitoring your pipeline’s health\n\n1. How to do it…\n\n7. Setting up a pipeline using AWS Glue to ingest data from a JDBC database into a catalog table\n\n1. How to do it…\n\n10. Chapter 5: Running Big Data Workloads with Amazon EMR\n\n1. Technical requirements 2. Running jobs using AWS EMR serverless\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Running your AWS EMR cluster on EKS\n\n1. Getting ready 2. How to do it... 3. How it works... 4. There’s more... 5. See also\n\n4. Using the AWS Glue catalog from another account\n\n1. Getting ready 2. How to do it... 3. How it works... 4. There’s more... 5. See also\n\n5. Making your cluster highly available\n\n1. Getting ready 2. How to do it... 3. How it works... 4. There’s more... 5. See also\n\n6. Scaling your cluster based on workload\n\n1. Getting ready 2. How to do it...\n\n3. How it works... 4. There’s more... 5. See also\n\n7. Customizing the cluster nodes easily using bootstrap actions\n\n1. Getting ready 2. How to do it... 3. How it works... 4. There’s more... 5. See also\n\n8. Tuning Apache Spark resource usage\n\n1. Getting ready 2. How to do it... 3. How it works... 4. There’s more... 5. See also\n\n9. Code development on EMR using Workspaces\n\n1. How to do it... 2. There’s more... 3. See also\n\n10. Monitoring your cluster 1. Getting ready 2. How to do it... 3. There’s more 4. See also\n\n11. Protecting your cluster from security vulnerabilities\n\n1. Getting ready 2. How to do it... 3. How it works... 4. There’s more... 5. See also\n\n11. Chapter 6: Governing Your Platform\n\n1. Technical requirements 2. Applying a data quality check on Glue tables\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Automating the discovery and reporting of sensitive data on your S3 buckets 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n4. Establishing a tagging strategy for AWS resources\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Building your distributed data community with Amazon DataZone following data mesh principles\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n6. Handling security-sensitive data (PII and PHI)\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more…\n\n5. See also\n\n7. Ensuring S3 compliance with AWS Config\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n12. Chapter 7: Data Quality Management\n\n1. Technical requirements 2. Creating data quality for ETL jobs in AWS Glue Studio notebooks\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Unit testing your data quality using Deequ\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n4. Schema management for ETL pipelines\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Building unit test functions for ETL pipelines\n\n1. How to do it… 2. How it works… 3. There’s more…\n\n4. See also\n\n6. Building data cleaning and profiling jobs with DataBrew\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n13. Chapter 8: DevOps – Defining IaC and Building CI/CD Pipelines\n\n1. Technical requirements 2. Setting up a code deployment pipeline using CDK and AWS CodePipeline 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Setting up a CDK pipeline to deploy on multiple accounts and regions\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n4. Running code in a CloudFormation deployment\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Protecting resources from accidental deletion\n\n1. Getting ready\n\n2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n6. Deploying a data pipeline using Terraform\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n7. Reverse-engineering IaC 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n8. Integrating AWS Glue and Git version control\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n14. Chapter 9: Monitoring Data Lake Cloud Infrastructure\n\n1. Technical requirements\n\n1. Additional information\n\n2. Automatically setting CloudWatch log group retention to reduce cost\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Creating custom dashboards to monitor Data Lake services\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n4. Setting up System Manager to remediate non-compliance with AWS Config rules\n\n1. Getting ready 2. How to do it… 3. There’s more… 4. See also\n\n5. Using AWS config to automate non-compliance S3 server access logging policy 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n6. Tracking AWS Data Lake cost per analytics workload\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n15. Chapter 10: Building a Serving Layer with AWS Analytics Services\n\n1. Technical requirements 2. Using Redshift workload management (WLM) to manage workload priority 1. Getting ready\n\n2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Querying large historical data with Redshift Spectrum\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n4. Creating a VPC endpoint to a Redshift cluster\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Accessing a Redshift cluster using JDBC to query data\n\n1. Getting ready 2. How to do it… 3. There’s more… 4. See also\n\n6. Using AWS SDK for pandas, the Redshift Data API, and Lambda to execute SQL statements\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n7. Using the AWS SDK for Python to manage Amazon QuickSight\n\n1. Getting ready 2. How to do it…\n\n3. How it works… 4. There’s more… 5. See also\n\n16. Chapter 11: Migrating to AWS – Steps, Strategies, and Best Practices for Modernizing Your Analytics and Big Data Workloads\n\n1. Technical requirements 2. Reviewing the steps and processes for migrating an on- premises platform to AWS\n\n1. Getting ready 2. How to do it…\n\n3. Choosing your AWS analytics stack – the re-platforming approach\n\n1. Getting ready 2. How to do it… 3. See also\n\n4. Picking the correct migration approach for your workload\n\n1. Getting ready 2. How to do it…\n\n5. Planning for prototyping and testing\n\n1. Getting ready 2. How to do it…\n\n6. Converting ETL processes with big data frameworks\n\n1. Getting ready 2. How to do it…\n\n7. Defining and executing your migration process with Hadoop\n\n1. Getting ready 2. How to do it…\n\n8. Migrating the existing Hadoop security authentication and authorization processes\n\n1. Getting ready\n\n2. How to do it…\n\n17. Chapter 12: Harnessing the Power of AWS for Seamless Data Warehouse Migration\n\n1. Technical requirements 2. Creating SCT migration assessment report with AWS SCT\n\n1. Getting ready 2. How to do it… 3. See also\n\n3. Extracting data with AWS DMS\n\n1. Getting ready 2. How to do it… 3. See also\n\n4. Live example – migrating an Oracle database from a local laptop to AWS RDS using AWS SCT\n\n1. Getting ready 2. How to do it…\n\n5. Leveraging AWS Snow Family for large-scale data migration\n\n1. The Snow Family Large Data Migration Manager as a service\n\n2. Getting ready 3. How to do it… 4. See also\n\n18. Chapter 13: Strategizing Hadoop Migrations – Cost, Data, and Workflow Modernization with AWS\n\n1. Technical requirements 2. Calculating total cost of ownership (TCO) using AWS TCO calculators\n\n1. Getting ready 2. How to do it… 3. See also\n\n3. Conducting a Hadoop migration assessment using the TCO simulator\n\n1. Hadoop to Amazon EMR TCO simulator 2. Getting ready 3. How to do it… 4. See also\n\n4. Selecting how to store your data\n\n1. Getting ready 2. How to do it… 3. See also\n\n5. Migrating on-premises HDFS data using AWS DataSync\n\n1. Getting ready 2. How to do it... 3. See also\n\n6. Migrating the Hive Metastore to AWS\n\n1. Getting ready 2. How to do it…\n\n7. Migrating and running Apache Oozie workflows on Amazon EMR\n\n1. Getting ready 2. How to do it…\n\n8. Migrating an Oozie database to the Amazon RDS MySQL\n\n1. Getting ready 2. How to do it… 3. See also\n\n9. Setting up networking – establishing a secure connection to your EMR cluster 1. Getting ready 2. How to do it… 3. See also\n\n10. Performing a seamless HBase migration to AWS\n\n1. Getting ready\n\n2. How to do it…\n\n11. Migrating HBase to DynamoDB on AWS\n\n1. Getting ready 2. How to do it… 3. See also\n\n19. Index\n\n1. Why subscribe?\n\n20. Other Books You May Enjoy\n\n1. Packt is searching for authors like you 2. Share Your Thoughts 3. Download a free PDF copy of this book\n\nLandmarks\n\n1. Cover 2. Table of Contents 3. Index\n\nOceanofPDF.com",
      "page_number": 900
    },
    {
      "number": 45,
      "title": "Segment 45 (pages 920-922)",
      "start_page": 920,
      "end_page": 922,
      "detection_method": "synthetic",
      "content": "",
      "page_number": 920
    }
  ],
  "pages": [
    {
      "page_number": 2,
      "content": "Data Engineering with AWS Cookbook\n\nCopyright © 2024 Packt Publishing\n\nAll rights reserved. No part of this book may be reproduced, stored\n\nin a retrieval system, or transmitted in any form or by any means,\n\nwithout the prior written permission of the publisher, except in the\n\ncase of brief quotations embedded in critical articles or reviews.\n\ne authors acknowledge the use of cutting-edge AI, such as\n\nChatGPT, with the sole aim of enhancing the language and clarity\n\nwithin the book, thereby ensuring a smooth reading experience for\n\nreaders. It’s important to note that the content itself has been craed\n\nby the authors and edited by a professional publishing team.\n\nEvery eﬀort has been made in the preparation of this book to ensure\n\nthe accuracy of the information presented. However, the\n\ninformation contained in this book is sold without warranty, either\n\nexpress or implied. Neither the authors nor Packt Publishing or its",
      "content_length": 936,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 3,
      "content": "dealers and distributors will be held liable for any damages caused\n\nor alleged to have been caused directly or indirectly by this book.\n\nPackt Publishing has endeavored to provide trademark information\n\nabout all of the companies and products mentioned in this book by\n\nthe appropriate use of capitals. However, Packt Publishing cannot\n\nguarantee the accuracy of this information.\n\nGroup Product Manager: Apeksha Shetty\n\nPublishing Product Manager: Nilesh Kowadkar\n\nBook Project Manager: Urvi Sharma\n\nSenior Editor: Rohit Singh\n\nTechnical Editor: Kavyashree K S\n\nCopy Editor: Sa\u0000s Editing\n\nProofreader: Rohit Singh\n\nIndexer: Manju Arasan\n\nProduction Designer: Shankar Kalbhor\n\nSenior DevRel Marketing Executive: Nivedita Singh\n\nFirst published: November 2024\n\nProduction reference: 1301024\n\nPublished by Packt Publishing Ltd.",
      "content_length": 826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 4,
      "content": "Grosvenor House\n\n11 St Paul’s Square\n\nBirmingham\n\nB3 1RB, UK.\n\nISBN 978-1-80512-728-4\n\nwww.packtpub.com\n\nOceanofPDF.com",
      "content_length": 119,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 5,
      "content": "To my mother, Ngoc Truong, for her love and sacri\u0000ces, and for\n\nexemplifying the power of determination. To my family members\n\nand friends, who always oﬀer support and kindness throughout\n\nmy life journey.",
      "content_length": 205,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 6,
      "content": "– Trâm Ngọc Phạm\n\nOceanofPDF.com",
      "content_length": 32,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 7,
      "content": "Contributors\n\nAbout the authors\n\nTrâm Ngọc Phạm is a senior data architect with over a decade of\n\nhands-on experience working in the big data and AI \u0000eld, from\n\nplaying a lead role in tailoring cloud data platforms to BI and\n\nanalytics use cases for enterprises in Vietnam. While working as a\n\nSenior Data and Analytics consultant for the AWS Professional\n\nServices team, she specialized in guiding \u0000nance and telco\n\ncompanies across Southeast Asian countries to build enterprise-\n\nscale data platforms and drive analytics use cases that utilized AWS\n\nservices and big data tools.\n\nGonzalo Herreros González is a principal data architect. He holds a\n\nbachelor’s degree in computer science and a master’s degree in data\n\nanalytics. He has experience of over a decade in big data and two\n\ndecades of soware development, both in AWS and on-premises.\n\nPreviously, he worked at MasterCard where he achieved the \u0000rst\n\nPCI-DSS Hadoop cluster in the world. More recently, he worked at\n\nAWS for over 6 years, building data pipelines for the internal\n\nnetwork data, and later, as an architect in the AWS Glue service",
      "content_length": 1107,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 8,
      "content": "team, building transforms for AWS Glue Studio and helping large\n\ncustomers succeed with AWS data services.\n\nOceanofPDF.com",
      "content_length": 122,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 9,
      "content": "Contributors\n\nAbout the authors\n\nTrâm Ngọc Phạm is a senior data architect with over a decade of\n\nhands-on experience working in the big data and AI \u0000eld, from\n\nplaying a lead role in tailoring cloud data platforms to BI and\n\nanalytics use cases for enterprises in Vietnam. While working as a\n\nSenior Data and Analytics consultant for the AWS Professional\n\nServices team, she specialized in guiding \u0000nance and telco\n\ncompanies across Southeast Asian countries to build enterprise-\n\nscale data platforms and drive analytics use cases that utilized AWS\n\nservices and big data tools.\n\nGonzalo Herreros González is a principal data architect. He holds a\n\nbachelor’s degree in computer science and a master’s degree in data\n\nanalytics. He has experience of over a decade in big data and two\n\ndecades of soware development, both in AWS and on-premises.\n\nPreviously, he worked at MasterCard where he achieved the \u0000rst\n\nPCI-DSS Hadoop cluster in the world. More recently, he worked at\n\nAWS for over 6 years, building data pipelines for the internal\n\nnetwork data, and later, as an architect in the AWS Glue service\n\nteam, building transforms for AWS Glue Studio and helping large\n\ncustomers succeed with AWS data services.",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 10,
      "content": "Viquar Khan is a senior data architect at AWS Professional Services\n\nand brings over 20 years of expertise in \u0000nance and data analytics,\n\nempowering global \u0000nancial institutions to harness the full\n\npotential of AWS technologies. He designs cutting-edge,\n\ncustomized data solutions tailored to complex industry needs. A\n\npolyglot developer skilled in Java, Scala, Python, and other\n\nlanguages, Viquar has excelled in various technical roles. As an\n\nexpert group member of JSR368 (JavaTM Message Service 2.1), he\n\nhas shaped industry standards and actively contributes to open\n\nsource projects such as Apache Spark and Terraform. His technical\n\ninsights have reached and bene\u0000ted over 6.7 million users on Stack\n\nOver\u0000ow.\n\nHuda Nofal is a seasoned data engineer with over 7 years of\n\nexperience at Amazon, where she has played a key role in helping\n\ninternal business teams achieve their data goals. With deep\n\nexpertise in AWS services, she has successfully designed and\n\nimplemented data pipelines that power critical decision-making\n\nprocesses across various organizations. Huda’s work primarily\n\nfocuses on leveraging Redshi, Glue, data lakes, and Lambda to\n\ncreate scalable, eﬃcient data solutions.\n\nOceanofPDF.com",
      "content_length": 1219,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 11,
      "content": "About the reviewers\n\nSaransh Arora is a seasoned data engineer with more than 6 years of\n\nexperience in the \u0000eld. He has developed pro\u0000ciency in Python,\n\nJava, Spark, SQL, and various data engineering tools, enabling him\n\nto address a wide range of data challenges. He has expertise in data\n\norchestration, management, and analysis, with a strong emphasis on\n\nleveraging big data technologies to generate actionable insights.\n\nSaransh also possesses signi\u0000cant experience in machine learning\n\nand predictive analytics. Currently serving as a data engineer at\n\nAWS, he is dedicated to driving innovation and delivering business\n\nvalue. As an expert in data engineering, Saransh has also been\n\nworking on the integration of generative AI into data engineering\n\npractices.\n\nHaymang Ahuja specializes in ETL development, cloud computing,\n\nbig data technologies, and cutting-edge AI. He is adept at creating\n\nrobust data pipelines and delivering high-performance data\n\nsolutions, backed by strong soware development skills and\n\npro\u0000ciency in programming languages such as Python and SQL.\n\nHis expertise includes big data technologies such as Spark, Apache\n\nHudi, Air\u0000ow, Kylin, HDFS, and HBase. With a combination of\n\ntechnical knowledge, problem-solving skills, and a commitment to\n\nleveraging emerging technologies, he helps organizations achieve",
      "content_length": 1344,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 12,
      "content": "their strategic objectives and stay competitive in the dynamic digital\n\nlandscape.\n\nOceanofPDF.com",
      "content_length": 98,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 13,
      "content": "Table of Contents\n\nPreface\n\n1\n\nManaging Data Lake Storage\n\nTechnical requirements\n\nControlling access to S3 buckets\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nStorage types in S3 for optimized storage costs\n\nGetting ready",
      "content_length": 249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 14,
      "content": "How to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nEnforcing encryption on S3 buckets\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nSetting up retention policies for your objects\n\nGetting ready\n\nHow to do it…\n\nHow it works…",
      "content_length": 252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 15,
      "content": "There’s more…\n\nSee also\n\nVersioning your data\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nReplicating your data\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also",
      "content_length": 208,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 16,
      "content": "Monitoring your S3 bucket\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n2\n\nSharing Your Data Across Environments and Accounts\n\nTechnical requirements\n\nCreating read-only replicas for RDS\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…",
      "content_length": 271,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 17,
      "content": "See also\n\nRedshift live data sharing among your clusters\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSynchronizing Glue Data Catalog to a different account\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nEnforcing fine-grained permissions on S3 data sharing using Lake Formation",
      "content_length": 318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 18,
      "content": "Getting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nSharing your S3 data temporarily using a presigned URL\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nReal-time sharing of S3 data\n\nGetting ready\n\nHow to do it…",
      "content_length": 254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 19,
      "content": "How it works…\n\nThere’s more…\n\nSee also\n\nSharing read-only access to your CloudWatch data with another AWS account\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n3\n\nIngesting and Transforming Your Data with AWS Glue\n\nTechnical requirements\n\nCreating ETL jobs visually using AWS Glue Studio",
      "content_length": 312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 20,
      "content": "Getting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nParameterizing jobs to make them more flexible and reusable\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nHandling job failures and reruns for partial results\n\nGetting ready",
      "content_length": 268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 21,
      "content": "How to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nProcessing data incrementally using bookmarks and bounded execution\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nHandling a high quantity of small files in your job\n\nGetting ready\n\nHow to do it…",
      "content_length": 275,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 22,
      "content": "How it works…\n\nThere’s more…\n\nSee also\n\nReusing libraries in your Glue job\n\nGetting ready\n\nHow to do it...\n\nHow it works…\n\nThere’s more...\n\nSee also\n\nUsing data lake formats to store your data\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…",
      "content_length": 252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 23,
      "content": "See also\n\nOptimizing your catalog data retrieval using pushdown filters and indexes\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nRunning pandas code using AWS Glue for Ray\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n4",
      "content_length": 270,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 24,
      "content": "A Deep Dive into AWS Orchestration Frameworks\n\nTechnical requirements\n\nDefining a simple workflow using AWS Glue workflows\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nSetting up event-driven orchestration with Amazon EventBridge\n\nGetting ready\n\nHow to do it…\n\nCreating a data workflow using AWS Step Functions\n\nHow to do it…\n\nSee also",
      "content_length": 331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 25,
      "content": "Managing data pipelines with MWAA\n\nHow to do it…\n\nSee also\n\nMonitoring your pipeline’s health\n\nHow to do it…\n\nSetting up a pipeline using AWS Glue to ingest data from a JDBC database into a catalog table\n\nHow to do it…\n\n5\n\nRunning Big Data Workloads with Amazon EMR\n\nTechnical requirements\n\nRunning jobs using AWS EMR serverless\n\nGetting ready\n\nHow to do it…",
      "content_length": 358,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 26,
      "content": "How it works…\n\nThere’s more…\n\nSee also\n\nRunning your AWS EMR cluster on EKS\n\nGetting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...\n\nSee also\n\nUsing the AWS Glue catalog from another account\n\nGetting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...",
      "content_length": 266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 27,
      "content": "See also\n\nMaking your cluster highly available\n\nGetting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...\n\nSee also\n\nScaling your cluster based on workload\n\nGetting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...\n\nSee also\n\nCustomizing the cluster nodes easily using bootstrap actions",
      "content_length": 300,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 28,
      "content": "Getting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...\n\nSee also\n\nTuning Apache Spark resource usage\n\nGetting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...\n\nSee also\n\nCode development on EMR using Workspaces\n\nHow to do it...\n\nThere’s more...",
      "content_length": 262,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 29,
      "content": "See also\n\nMonitoring your cluster\n\nGetting ready\n\nHow to do it...\n\nThere’s more\n\nSee also\n\nProtecting your cluster from security vulnerabilities\n\nGetting ready\n\nHow to do it...\n\nHow it works...\n\nThere’s more...\n\nSee also\n\n6\n\nGoverning Your Platform",
      "content_length": 248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 30,
      "content": "Technical requirements\n\nApplying a data quality check on Glue tables\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nAutomating the discovery and reporting of sensitive data on your S3 buckets\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also",
      "content_length": 285,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 31,
      "content": "Establishing a tagging strategy for AWS resources\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nBuilding your distributed data community with Amazon DataZone following data mesh principles\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also",
      "content_length": 283,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 32,
      "content": "Handling security-sensitive data (PII and PHI)\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nEnsuring S3 compliance with AWS Config\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n7\n\nData Quality Management",
      "content_length": 254,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 33,
      "content": "Technical requirements\n\nCreating data quality for ETL jobs in AWS Glue Studio notebooks\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nUnit testing your data quality using Deequ\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nSchema management for ETL pipelines",
      "content_length": 308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 34,
      "content": "Getting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nBuilding unit test functions for ETL pipelines\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nBuilding data cleaning and profiling jobs with DataBrew\n\nGetting ready\n\nHow to do it…\n\nHow it works…",
      "content_length": 273,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 35,
      "content": "There’s more…\n\nSee also\n\n8\n\nDevOps – Defining IaC and Building CI/CD Pipelines\n\nTechnical requirements\n\nSetting up a code deployment pipeline using CDK and AWS CodePipeline\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nSetting up a CDK pipeline to deploy on multiple accounts and regions\n\nGetting ready",
      "content_length": 327,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 36,
      "content": "How to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nRunning code in a CloudFormation deployment\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nProtecting resources from accidental deletion\n\nGetting ready\n\nHow to do it…\n\nHow it works…",
      "content_length": 260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 37,
      "content": "There’s more…\n\nSee also\n\nDeploying a data pipeline using Terraform\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nReverse-engineering IaC\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also",
      "content_length": 231,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 38,
      "content": "Integrating AWS Glue and Git version control\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n9\n\nMonitoring Data Lake Cloud Infrastructure\n\nTechnical requirements\n\nAdditional information\n\nAutomatically setting CloudWatch log group retention to reduce cost\n\nGetting ready\n\nHow to do it…\n\nHow it works…",
      "content_length": 322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 39,
      "content": "There’s more…\n\nSee also\n\nCreating custom dashboards to monitor Data Lake services\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nSetting up System Manager to remediate non- compliance with AWS Config rules\n\nGetting ready\n\nHow to do it…\n\nThere’s more…\n\nSee also",
      "content_length": 284,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 40,
      "content": "Using AWS config to automate non- compliance S3 server access logging policy\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nTracking AWS Data Lake cost per analytics workload\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n10",
      "content_length": 272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 41,
      "content": "Building a Serving Layer with AWS Analytics Services\n\nTechnical requirements\n\nUsing Redshift workload management (WLM) to manage workload priority\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nQuerying large historical data with Redshift Spectrum\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…",
      "content_length": 331,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 42,
      "content": "See also\n\nCreating a VPC endpoint to a Redshift cluster\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nAccessing a Redshift cluster using JDBC to query data\n\nGetting ready\n\nHow to do it…\n\nThere’s more…\n\nSee also\n\nUsing AWS SDK for pandas, the Redshift Data API, and Lambda to execute SQL statements",
      "content_length": 322,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 43,
      "content": "Getting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\nUsing the AWS SDK for Python to manage Amazon QuickSight\n\nGetting ready\n\nHow to do it…\n\nHow it works…\n\nThere’s more…\n\nSee also\n\n11\n\nMigrating to AWS – Steps, Strategies, and Best Practices for Modernizing Your Analytics and Big Data Workloads",
      "content_length": 312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 44,
      "content": "Technical requirements\n\nReviewing the steps and processes for migrating an on-premises platform to AWS\n\nGetting ready\n\nHow to do it…\n\nChoosing your AWS analytics stack – the re- platforming approach\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nPicking the correct migration approach for your workload\n\nGetting ready\n\nHow to do it…\n\nPlanning for prototyping and testing\n\nGetting ready",
      "content_length": 379,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 45,
      "content": "How to do it…\n\nConverting ETL processes with big data frameworks\n\nGetting ready\n\nHow to do it…\n\nDefining and executing your migration process with Hadoop\n\nGetting ready\n\nHow to do it…\n\nMigrating the existing Hadoop security authentication and authorization processes\n\nGetting ready\n\nHow to do it…\n\n12\n\nHarnessing the Power of AWS for Seamless Data Warehouse Migration",
      "content_length": 367,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 46,
      "content": "Technical requirements\n\nCreating SCT migration assessment report with AWS SCT\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nExtracting data with AWS DMS\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nLive example – migrating an Oracle database from a local laptop to AWS RDS using AWS SCT\n\nGetting ready\n\nHow to do it…",
      "content_length": 307,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 47,
      "content": "Leveraging AWS Snow Family for large-scale data migration\n\nThe Snow Family Large Data Migration Manager as a service\n\nGetting ready\n\nHow to do it…\n\nSee also\n\n13\n\nStrategizing Hadoop Migrations – Cost, Data, and Workflow Modernization with AWS\n\nTechnical requirements\n\nCalculating total cost of ownership (TCO) using AWS TCO calculators\n\nGetting ready\n\nHow to do it…",
      "content_length": 365,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 48,
      "content": "See also\n\nConducting a Hadoop migration assessment using the TCO simulator\n\nHadoop to Amazon EMR TCO simulator\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nSelecting how to store your data\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nMigrating on-premises HDFS data using AWS DataSync\n\nGetting ready\n\nHow to do it...",
      "content_length": 308,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 49,
      "content": "See also\n\nMigrating the Hive Metastore to AWS\n\nGetting ready\n\nHow to do it…\n\nMigrating and running Apache Oozie workflows on Amazon EMR\n\nGetting ready\n\nHow to do it…\n\nMigrating an Oozie database to the Amazon RDS MySQL\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nSetting up networking – establishing a secure connection to your EMR cluster\n\nGetting ready",
      "content_length": 351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 50,
      "content": "How to do it…\n\nSee also\n\nPerforming a seamless HBase migration to AWS\n\nGetting ready\n\nHow to do it…\n\nMigrating HBase to DynamoDB on AWS\n\nGetting ready\n\nHow to do it…\n\nSee also\n\nIndex\n\nOther Books You May Enjoy\n\nOceanofPDF.com",
      "content_length": 225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 51,
      "content": "Preface\n\nHello and welcome! In today’s rapidly evolving data landscape,\n\nmanaging, migrating, and governing large-scale data systems are\n\namong the top priorities for data engineers. is book serves as a\n\ncomprehensive guide to help you navigate these essential tasks, with\n\na focus on three key pillars of modern data engineering:\n\nHadoop and data warehouse migration: Organizations are increasingly\n\nmoving from traditional Hadoop clusters and on-premises data\n\nwarehouses to more scalable, cloud-based data platforms. is book\n\nwalks you through the best practices, methodologies, and how to use the\n\ntools for migrating large-scale data systems, ensuring data consistency,\n\nminimal downtime, and scalable performance.\n\nData lake operations: Building and maintaining a data lake in today’s\n\nmulti-cloud, big data environment is complex and demands a strong\n\noperational strategy. is book covers how to ingest, transform, and\n\nmanage data at scale using AWS services such as S3, Glue, and Athena.\n\nYou will learn how to structure and maintain a robust data lake\n\narchitecture that supports the varied needs of data analysts, data\n\nscientists, and business users alike.\n\nData lake governance: Managing and governing your data lake involves\n\nmore than just operational eﬃciency; it requires stringent security\n\nprotocols, data quality controls, and compliance measures. With the\n\nexplosion of data, it’s more important than ever to have clear governance",
      "content_length": 1454,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 52,
      "content": "frameworks in place. is book delves into the best practices for\n\nimplementing governance strategies using services such as AWS Lake\n\nFormation, Glue, and other AWS security frameworks. You’ll also learn\n\nabout setting up policies that ensure your data lake is compliant with\n\nindustry regulations while maintaining scalability and \u0000exibility.\n\nis cookbook is tailored to data engineers who are looking to\n\nimplement best practices and take their cloud data platforms to the\n\nnext level. roughout this book, you’ll \u0000nd practical examples,\n\ndetailed recipes, and real-world scenarios from the authors’\n\nexperience of working with complex data environments across\n\ndiﬀerent industries.\n\nBy the end of this journey, you will have a thorough understanding\n\nof how to migrate, operate, and govern your data platforms at scale,\n\nall while aligning with industry best practices and modern\n\ntechnological advancements.\n\nSo, let’s dive in and build the future of data engineering together!",
      "content_length": 982,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 53,
      "content": "Who this book is for\n\nis book is designed for data engineers, data platform engineers,\n\nand cloud practitioners who are actively involved in building and\n\nmanaging data infrastructure in the cloud. If you’re involved in\n\ndesigning, building, or overseeing data solutions on AWS, this book\n\nwill be ideal as it provides proven strategies for addressing\n\nchallenges in large-scale data environments. Data engineers and big\n\ndata professionals aiming to enhance their understanding of AWS\n\nfeatures for optimizing their work\u0000ow, even if they’re new to the\n\nplatform, will \u0000nd value. Basic familiarity with AWS security (users\n\nand roles) and command shell is recommended. is book will\n\nprovide you with practical guidance, hands-on recipes, and\n\nadvanced techniques for tackling real-world challenges.",
      "content_length": 800,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 54,
      "content": "What this book covers\n\nChapter 1, Managing Data Lake Storage, covers the fundamentals of\n\nmanaging S3 buckets. We’ll focus on implementing robust security\n\nmeasures through data encryption and access control, managing\n\ncosts by optimizing storage tiers and applying retention policies,\n\nand utilizing monitoring techniques to ensure timely issue\n\nresolution. Additionally, we’ll cover other essential aspects of S3\n\nbucket management.\n\nChapter 2, Sharing Your Data Across Environments and Accounts,\n\npresents methods for securely and eﬃciently sharing data across\n\ndiﬀerent environments and accounts. We will explore strategies for\n\nload distribution and collaborative analysis using Redshi data\n\nsharing and RDS replicas. We will implement \u0000ne-grained access\n\ncontrol with Lake Formation and manage Glue data sharing\n\nthrough both Lake Formation and Resource Access Manager\n\n(RAM). Additionally, we will discuss real-time sharing via event-\n\ndriven services, temporary data sharing with S3, and sharing\n\noperational data from CloudWatch.\n\nChapter 3, Ingesting and Transforming Your Data with AWS Glue,\n\nexplores diﬀerent features of AWS Glue when building data\n\npipelines and data lakes. It covers the multiple tools and engines",
      "content_length": 1230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 55,
      "content": "provided for the diﬀerent kinds of users, from visual jobs with little\n\nor no code to managed notebooks and jobs using the diﬀerent data\n\nhandling APIs provided.\n\nChapter 4, A Deep Dive into AWS Orchestration Frameworks,\n\nexplores the essential services and techniques for managing data\n\nwork\u0000ows and pipelines on AWS. You’ll learn how to de\u0000ne a\n\nsimple work\u0000ow using AWS Glue Work\u0000ows, set up event-driven\n\norchestration with Amazon EventBridge, and create data work\u0000ows\n\nwith AWS Step Functions. We also cover managing data pipelines\n\nusing Amazon MWAA, monitoring their health, and setting up a\n\ndata ingestion pipeline with AWS Glue to bring data from a JDBC\n\ndatabase into a catalog table.\n\nChapter 5, Running Big Data Workloads with Amazon EMR, teaches\n\nhow to make the most of your AWS EMR clusters and explore the\n\nservice features that enable them to be customizable, eﬃcient,\n\nscalable, and robust.\n\nChapter 6, Governing Your Platform, presents the key aspects of data\n\ngovernance within AWS. is includes data protection techniques\n\nsuch as data masking in Redshi and classifying sensitive\n\ninformation using Maice. We will also cover ensuring data quality\n\nwith Glue quality checks. Additionally, we will discuss resource\n\ngovernance to enforce best practices and maintain a secure,\n\ncompliant infrastructure using AWS Con\u0000g and resource tagging.",
      "content_length": 1360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 56,
      "content": "Chapter 7, Data Quality Management, covers how to use AWS Glue\n\nDeequ and AWS DataBrew to automate data quality checks and\n\nmaintain high standards across your datasets. You will learn how to\n\nde\u0000ne and enforce data quality rules and monitor data quality\n\nmetrics. is chapter also provides practical examples and recipes\n\nfor integrating these tools into your data work\u0000ows, ensuring that\n\nyour data is accurate, complete, and reliable for analysis.\n\nChapter 8, DevOps – De\u0000ning IaC and Building CI/CD Pipelines,\n\nexplores multiple ways to automate AWS services and CI/CD\n\ndeployment pipelines, the pros and cons of each tool, and examples\n\nof common data product deployments to illustrate DevOps best\n\npractices.\n\nChapter 9, Monitoring Data Lake Cloud Infrastructure, provides a\n\ncomprehensive guide to the day-to-day operations of a cloud-based\n\ndata platform. It covers key topics such as monitoring, logging, and\n\nalerting using AWS services such as CloudWatch, CloudTrail, and\n\nX-Ray. You will learn how to set up dashboards to monitor the\n\nhealth and performance of your data platform, troubleshoot issues,\n\nand ensure high availability and reliability. is chapter also\n\ndiscusses best practices for cost management and scaling operations\n\nto meet changing demands, making it an essential resource for\n\nanyone responsible for the ongoing maintenance and optimization\n\nof a data platform.",
      "content_length": 1395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 57,
      "content": "Chapter 10, Building a Serving Layer with AWS Analytics Services,\n\nguides you through the process of building an eﬃcient serving layer\n\nusing AWS Redshi, Athena, and QuickSight. e serving layer is\n\nwhere your data becomes accessible to end-users for analysis and\n\nreporting. In this chapter, you will learn how to load data from your\n\ndata lake into Redshi, query it using Redshi Spectrum and\n\nAthena, and visualize it using QuickSight. is chapter also covers\n\nbest practices for managing diﬀerent QuickSight environments and\n\nmigrating assets between them. By the end of this chapter, you will\n\nhave the knowledge to create a powerful and user-friendly analytics\n\nlayer that meets the needs of your organization.\n\nChapter 11, Migrating to AWS – Steps, Strategies, and Best Practices\n\nfor Modernizing Your Analytics and Big Data Workloads, presents a\n\ntheoretical framework for migrating data and workloads to AWS. It\n\nexplores key concepts, strategies, and best practices for planning\n\nand executing a successful migration. You’ll learn about various\n\nmigration approaches—rehosting, replatforming, and refactoring—\n\nand how to choose the best option for your organization’s needs.\n\ne chapter also addresses critical challenges and considerations,\n\nsuch as data security, compliance, and minimizing downtime,\n\npreparing you to navigate the complexities of cloud migration with\n\ncon\u0000dence.",
      "content_length": 1395,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 58,
      "content": "Chapter 12, Harnessing the Power of AWS for Seamless Data\n\nWarehouse Migration, explores the key strategies for eﬃciently\n\nmigrating data warehouses to AWS. You’ll learn how to generate a\n\nmigration assessment report using the AWS Schema Conversion\n\nTool (SCT), extract and transfer data with AWS Database Migration\n\nService (DMS), and handle large-scale migrations with the AWS\n\nSnow Family. You’ll also learn how to streamline your data\n\nmigration, ensuring minimal disruption and maximum eﬃciency\n\nwhile transitioning to the cloud.\n\nChapter 13, Strategizing Hadoop Migrations – Cost, Data, and\n\nWork\u0000ow Modernization with AWS, guides you through essential\n\nrecipes for migrating your on-premises Hadoop ecosystem to AWS,\n\ncovering a range of critical tasks. You’ll learn about cost analysis\n\nusing the AWS Total Cost of Ownership (TCO) calculators and the\n\nHadoop Migration Assessment tool. You’ll also learn how to choose\n\nthe right storage solution, migrate HDFS data using AWS DataSync,\n\nand transition key components such as the Hive Metastore and\n\nApache Oozie work\u0000ows to AWS EMR. We also cover setting up a\n\nsecure network connection to your EMR cluster, seamless HBase\n\nmigration to AWS, and transitioning HBase to DynamoDB.\n\nTo get the most out of this book\n\nTo follow the recipes in this book, you will need the following:",
      "content_length": 1335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 59,
      "content": "Soware/hardware covered in the book\n\nOS requirements\n\nAWS CLI\n\nWindows, macOS X,\n\nand Linux (any)\n\nAccess to AWS services such as EMR,\n\nGlue, Redshi, QuickSight, and Lambda\n\nPython (for scripting and SDK usage)\n\nIn addition to these requirements, you will also need a basic\n\nknowledge of data engineer terminology.\n\nIf you are using the digital version of this book, we advise you to\n\ntype the code yourself or access the code via the GitHub repository\n\n(link available in the next section). Doing so will help you avoid any\n\npotential errors related to the copying and pasting of code.\n\nDownload the example code files\n\nYou can download the example code \u0000les for this book from\n\nGitHub at https://github.com/PacktPublishing/Data-Engineering-\n\nwith-AWS-Cookbook. In case there’s an update to the code, it will\n\nbe updated on the existing GitHub repository.",
      "content_length": 858,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 60,
      "content": "We also have other code bundles from our rich catalog of books and\n\nvideos available at https://github.com/PacktPublishing/. Check\n\nthem out!\n\nConventions used\n\nere are a number of text conventions used throughout this book.\n\nCode in text: Indicates code words in text, database table names,\n\nfolder names, \u0000lenames, \u0000le extensions, pathnames, dummy URLs,\n\nuser input, and Twitter handles. Here is an example: “Make sure you\n\nreplace <your_bucket_name> with the actual name of your S3\n\nbucket.”\n\nA block of code is set as follows:\n\n{ Sid: DenyListBucketFolder, Action: [s3:*], Effect: Deny, Resource: [arn:aws:s3:::<bucket-name>/<folder- name>/*] }\n\nAny command-line input or output is written as follows:\n\nCREATE DATASHARE datashare_name;\n\nBold: Indicates a new term, an important word, or words that you\n\nsee onscreen. For example, words in menus or dialog boxes appear",
      "content_length": 872,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 61,
      "content": "in the text like this. Here is an example: “Choose Policies from the\n\nnavigation pane on the le and choose Create policy.”\n\nTIPS OR IMPORTANT NOTES\n\nAppear like this.\n\nSections\n\nIn this book, you will \u0000nd several headings that appear frequently\n\n(Getting ready, How to do it..., How it works..., ere’s more..., and\n\nSee also).\n\nTo give clear instructions on how to complete a recipe, use these\n\nsections as follows:\n\nGetting ready\n\nis section tells you what to expect in the recipe and describes how\n\nto set up any soware or any preliminary settings required for the\n\nrecipe.\n\nHow to do it…\n\nis section contains the steps required to follow the recipe.\n\nHow it works…",
      "content_length": 672,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 62,
      "content": "is section usually consists of a detailed explanation of what\n\nhappened in the previous section.\n\nThere’s more…\n\nis section consists of additional information about the recipe in\n\norder to make you more knowledgeable about the recipe.\n\nSee also\n\nis section provides helpful links to other useful information for\n\nthe recipe.\n\nGet in touch\n\nFeedback from our readers is always welcome.\n\nGeneral feedback: If you have questions about any aspect of this\n\nbook, mention the book title in the subject of your message and\n\nemail us at customercare@packtpub.com.\n\nErrata: Although we have taken every care to ensure the accuracy of\n\nour content, mistakes do happen. If you have found a mistake in\n\nthis book, we would be grateful if you would report this to us.\n\nPlease visit www.packtpub.com/support/errata, select your book,\n\nclick on the Errata Submission Form link, and enter the details.",
      "content_length": 888,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 63,
      "content": "Piracy: If you come across any illegal copies of our works in any\n\nform on the Internet, we would be grateful if you would provide us\n\nwith the location address or website name. Please contact us at\n\ncopyright@packt.com with a link to the material.\n\nIf you are interested in becoming an author: If there is a topic that\n\nyou have expertise in and you are interested in either writing or\n\ncontributing to a book, please visit authors.packtpub.com.\n\nShare Your Thoughts\n\nOnce you’ve read Data Engineering with AWS Cookbook, we’d love\n\nto hear your thoughts! Please click here to go straight to the\n\nAmazon review page for this book and share your feedback.\n\nYour review is important to us and the tech community and will\n\nhelp us make sure we’re delivering excellent quality content.\n\nDownload a free PDF copy of this book\n\nanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print\n\nbooks everywhere?",
      "content_length": 938,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 64,
      "content": "Is your eBook purchase not compatible with the device of your\n\nchoice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF\n\nversion of that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste\n\ncode from your favorite technical books directly into your\n\napplication.\n\ne perks don’t stop there, you can get exclusive access to discounts,\n\nnewsletters, and great free content in your inbox daily\n\nFollow these simple steps to get the bene\u0000ts:\n\n1. Scan the QR code or visit the link below\n\nhttps://packt.link/free-ebook/9781805127284\n\n2. Submit your proof of purchase\n\n3. at’s it! We’ll send your free PDF and other bene\u0000ts to your email\n\ndirectly",
      "content_length": 687,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 65,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 66,
      "content": "1 Managing Data Lake Storage\n\nAmazon Simple Storage Service (Amazon S3) is a highly scalable\n\nand secure cloud storage service. It allows you to store and retrieve\n\nany amount of data at any time from anywhere in the world. S3\n\nbuckets aim to help enterprises and individuals achieve their data\n\nbackup and delivery needs and serve a variety of use cases,\n\nincluding but not limited to web and mobile applications, big data\n\nanalytics, data lakes, and data backup and archiving.\n\nIn this chapter, we will learn how to keep data secure in S3 buckets\n\nand con\u0000gure your buckets in a way that best serves your use case\n\nfrom performance and cost perspectives.\n\ne following recipes will be covered in this chapter:\n\nControlling access to S3 buckets\n\nStorage types in S3 for optimized storage costs\n\nEnforcing encryption of S3 buckets\n\nSetting up retention policies for your objects\n\nVersioning your data\n\nReplicating your data\n\nMonitoring your S3 buckets",
      "content_length": 951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 67,
      "content": "Technical requirements\n\ne recipes in this chapter assume you have an S3 bucket with\n\nadmin permission. If you don’t have admin permission to the\n\nbucket, you will need to con\u0000gure the permission for each recipe as\n\nneeded.\n\nYou can \u0000nd the code \u0000les for this chapter in this book’s GitHub\n\nrepository: https://github.com/PacktPublishing/Data-Engineering-\n\nwith-AWS-Cookbook/tree/main/Chapter01.\n\nControlling access to S3 buckets\n\nControlling access to S3 buckets through policies and IAM roles is\n\ncrucial for maintaining the security and integrity of your objects\n\nand data stored in Amazon S3. By de\u0000ning granular permissions\n\nand access controls, you can ensure that only authorized users or\n\nservices have the necessary privileges to interact with your S3\n\nresources. You can restrict permissions according to your\n\nrequirements by precisely de\u0000ning who can access your data, what\n\nactions they can take, and under what conditions. is \u0000ne-grained\n\naccess control helps protect sensitive data, prevent unauthorized\n\nmodi\u0000cations, and mitigate the risk of accidental or malicious\n\nactions.",
      "content_length": 1093,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 68,
      "content": "AWS Identity and Access Management (IAM) allows you to create\n\nan entity referred to as an IAM identity, which is granted speci\u0000c\n\nactions on your AWS account. is entity can be a person or an\n\napplication. You can create this identity as an IAM role, which is\n\ndesigned to be attached to any entity that needs it. Alternatively, you\n\ncan create IAM users, which represent individual people and are\n\nusually used for granting long-term access to speci\u0000c users. IAM\n\nusers can be grouped into an IAM group, allowing permissions to\n\nbe assigned at the group level and inherited by all member users.\n\nIAM policies are sets of permissions that can be attached to the\n\nIAM identity to grant speci\u0000c access rights.\n\nIn this recipe, we will learn how to create a policy so that we can\n\nview all the buckets in the account, give read access to one speci\u0000c\n\nbucket content, and then give write access to one of its folders.\n\nGetting ready\n\nFor this recipe, you need to have an IAM user, role, or group to\n\nwhich you want to grant access. You also need to have an S3 bucket\n\nwith a folder to grant access to.\n\nTo learn how to create IAM identities, go to\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id.html.\n\nHow to do it…",
      "content_length": 1218,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 69,
      "content": "1. Sign in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the IAM console.\n\n2. Choose Policies from the navigation pane on the le and choose Create\n\npolicy.\n\n3. Choose the JSON tab to provide the policy in JSON format and replace\n\nthe existing JSON with this policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowListBuckets\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListAllMyBuckets\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowBucketListing\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\", \"s3:GetBucketLocation\" ], \"Resource\": [ \"arn:aws:s3:::<bucket-name>\" ] }, { \"Sid\": \"AllowFolderAccess\",",
      "content_length": 672,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 70,
      "content": "\"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:PutObject\", \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::<bucket- name>/<folder-name>/*\" ] } ] }\n\n4. Provide a policy name and, optionally, a description of the policy in the\n\nrespective \u0000elds.\n\n5. Click on Create Policy.\n\nNow, you can attach this policy to an IAM role, user, or group.\n\nHowever, exercise caution and ensure access is granted only as\n\nnecessary; avoid providing admin access policies to regular users.\n\nHow it works…\n\nAn IAM policy comprises three key elements:\n\nEffect: is speci\u0000es whether the policy allows or denies access\n\nAction: is details the speci\u0000c actions being allowed or denied\n\nResource: is identi\u0000es the resources to which the actions apply",
      "content_length": 729,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 71,
      "content": "A single statement can apply multiple actions to multiple resources.\n\nIn this recipe, we’ve de\u0000ned three statements:\n\ne AllowListBuckets statement gives access to list all buckets in\n\nthe AWS account\n\ne AllowBucketListing statement gives access to list the content\n\nof a speci\u0000c S3 bucket\n\ne AllowFolderAccess gives access to upload, download, and\n\ndelete objects from a speci\u0000c folder\n\nThere’s more…\n\nIf you want to make sure that no access is given to a speci\u0000c bucket\n\nor object in your bucket, you can use a deny statement, as shown\n\nhere:\n\n{ \"Sid\":\"DenyListBucketFolder\", \"Action\":[ \"s3:*\" ], \"Effect\":\"Deny\", \"Resource\":[ \"arn:aws:s3:::<bucket-name>/<folder- name>/*\" }\n\nInstead of using an IAM policy to set up permissions to your\n\nbucket, you can use S3 bucket policies. ese can be located in the",
      "content_length": 808,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 72,
      "content": "Permission tab of the bucket. Bucket policies can be used when\n\nyou’re trying to set up access at the bucket level, regardless of the\n\nIAM role or user.\n\nSee also\n\nAWS provides a set of policies that are managed and administered by\n\nAWS, all of which can be used for many common use cases. You can\n\nlearn more about these policies at\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam-\n\nawsmanpol.html.\n\nTo learn how to set up cross-account access to S3 buckets, go to\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/example-\n\nwalkthroughs-managing-access-example2.html.\n\nStorage types in S3 for optimized storage costs\n\nAmazon S3 oﬀers diﬀerent tiers or classes of storage that allow you\n\nto optimize for cost and performance based on your access pattern\n\nand data requirements. e default storage class for S3 buckets is S3\n\nStandard, which oﬀers high availability and low latency. For less\n\nfrequently accessed data, S3 Standard-IA and S3 One Zone-IA can\n\nbe used. For rare access, Amazon S3 oﬀers archiving classes called\n\nGlacier, which are the lowest-cost classes. If you’re not sure how",
      "content_length": 1117,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 73,
      "content": "frequently your data will be accessed, S3 Intelligent-Tiering would\n\nbe optimal for you as it will automatically move objects between the\n\nclasses based on the access patterns. However, be aware that\n\nadditional costs may be incurred when you’re moving objects to a\n\nhigher-cost storage class.\n\nese storage classes provide users with the \u0000exibility to choose the\n\nright trade-oﬀ between storage costs and access performance based\n\non their speci\u0000c data storage and retrieval requirements. You can\n\nchoose the storage class based on your access patterns, durability\n\nrequirements, and budget considerations. Con\u0000guring storage\n\nclasses at the object level allows for a mix of storage classes within\n\nthe same bucket. Objects from diverse storage classes, including S3\n\nStandard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-\n\nIA, can coexist in a single bucket.\n\nIn this recipe, we will learn how to enforce the S3 Intelligent-\n\nTiering storage class for an S3 bucket through a bucket policy.\n\nGetting ready\n\nFor this recipe, you only need to have an S3 bucket for which you\n\nwill enforce the storage class.\n\nHow to do it…",
      "content_length": 1133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 74,
      "content": "1. Open the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the S3 service.\n\n2. Locate and select the S3 bucket on which you want to enable S3\n\nIntelligent-Tiering and navigate to the Permissions tab.\n\n3. Under the Bucket Policy section, click on Edit.\n\n4. In the bucket policy editor, add the following statement. Make sure you\n\nreplace <your_bucket_name> with the actual name of your S3\n\nbucket:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"EnableIntelligentTiering\", \"Effect\": \"Deny\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::<your-bucket- name>/*\", \"Condition\": { \"StringNotEquals\": { \"s3:x-amz-storage-class\": \"INTELLIGENT_TIERING\" } } } ] }",
      "content_length": 761,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 75,
      "content": "5. Save the bucket policy by clicking on Save changes.\n\nHow it works…\n\ne policy will ensure that objects are stored via the Intelligent-\n\nTiering class by allowing the PUT operation to be used on the\n\nbucket for all users (Principal: *), but only if the storage class is\n\nset to INTELLIGENT_TIERING. You can do this by choosing it from\n\nthe storage class list in the Object properties section. If you’re using\n\nthe console or the S3 API, add the x-amz-storage-class:\n\nINTELLIGENT_TIERING header. Use the -storage-class\n\nINTELLIGENT_TIERING parameter when using the AWS CLI.\n\nThere’s more…\n\nIntelligent-Tiering will place newly uploaded objects in the S3\n\nStandard class (Frequent Access class). If the object hasn’t been\n\naccessed in 30 consecutive days, it will be moved to the Infrequent\n\nAccess tier; if it hasn’t been accessed in 90 consecutive days, it will\n\nbe moved to the Archive Instant Access tier. For further cost\n\nsavings, you can enable INTELLIGENT_TIERING to move your\n\nobject to the Archive Access tier and Deep Archive Access tier if\n\nthey have not been accessed for a longer period. To do this, follow\n\nthese steps:\n\n1. Navigate to the Properties tab for the bucket.",
      "content_length": 1185,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 76,
      "content": "2. Scroll down to Intelligent-Tiering Archive con\u0000gurations and click on\n\nCreate con\u0000guration.\n\n3. Name the con\u0000guration and specify whether you want to enable it for all\n\nobjects in the bucket or on a subset based on a \u0000lter and/or tags.\n\n4. Under Status, click on Enable to enable the con\u0000guration directly aer\n\nyou create it.\n\n5. Under Archive rule actions, enable the Archive Access tier and specify\n\nthe number of days in which the objects should be moved to this class if\n\nthey’re not being accessed. e value must be between 90 and 730 days.\n\nSimilarly, enable the Deep Archive Access tier and set the number of\n\ndays to a minimum of 180 days. It’s also possible to enable only one of\n\nthese classes:",
      "content_length": 708,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 77,
      "content": "Figure 1.1 – Intelligent-Tiering Archive rule action\n\n6. Click on Create to create the con\u0000guration.\n\nSee also\n\nA detailed comparison of storage classes:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-\n\nintro.html\n\nS3 storage classes pricing: https://aws.amazon.com/s3/pricing/",
      "content_length": 299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 78,
      "content": "Enforcing encryption on S3 buckets\n\nAmazon S3 encryption increases the level of security and privacy of\n\nyour data; it helps ensure that only authorized parties can read it.\n\nEven if an unauthorized person gains logical or physical access to\n\nthat data, the data is unreadable if they don’t get a hold of the key to\n\nunencrypt it.\n\nS3 supports encrypting data both at transit (as it travels to and from\n\nS3) and at rest (while it’s stored on disks in S3 data centers).\n\nFor protecting data at rest, you have two options. e \u0000rst is server-\n\nside encryption (SSE), in which Amazon S3 will be handling the\n\nheavy encryption operation on the server side in AWS. By default,\n\nAmazon S3 encrypts your data using SSE-S3. However, you can\n\nchange this to SSE-KMS, which uses KMS keys for encryption, or to\n\nSSE-C, where you can provide and manage your own encryption\n\nkey. Alternatively, you can encrypt your data using client-side\n\nencryption, where Amazon S3 doesn’t play any role in the\n\nencryption process rather; you are responsible for all the encryption\n\noperations.\n\nIn this recipe, we’ll learn how to enforce SSE-KMS server-side\n\nencryption using customer-managed keys.\n\nGetting ready",
      "content_length": 1186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 79,
      "content": "For this recipe, you need to have a KMS key in the same region as\n\nyour bucket to use for encryption. KMS provides a managed key for\n\nS3 (aws/s3) that can be utilized for encryption. However, if you\n\ndesire greater control over the key properties, such as modifying its\n\npolicies or performing key rotation, you can create a customer-\n\nmanaged key. To do so, follow these steps:\n\n1. Sign in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the AWS Key Management Service (AWS KMS)\n\nservice.\n\n2. In the navigation pane, choose Customer managed keys and click on\n\nCreate key.\n\n3. For Key type, choose Symmetric, while for Key usage, choose Encrypt\n\nand decrypt. Click on Next:",
      "content_length": 747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 80,
      "content": "Figure 1.2 – KMS conﬁguration\n\n4. Click on Next.\n\n5. Type an Alias value for the KMS key. is will be the display name.\n\nOptionally, you can provide Description and Tags key-value pairs for the\n\nkey.\n\n6. Click on Next. Optionally, you can provide Key administrators to\n\nadminister the key. Click on Finish to create the key.\n\nHow to do it…\n\n1. Sign in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the S3 service.\n\n2. In the Buckets list, choose the name of the bucket that you want to\n\nchange the encryption for and navigate to the Properties tab.\n\n3. Click on Edit in the Default encryption section.\n\n4. For Encryption type, choose Server-side encryption with AWS Key\n\nManagement Service keys (SSE-KMS).\n\n5. For AWS KMS key, you can select Enter AWS KMS key ARN to enter\n\nthe key you have created or browse it using Choose from your AWS\n\nKMS keys.\n\n6. Keep Bucket Key enabled and save your changes:",
      "content_length": 976,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 81,
      "content": "Figure 1.3 – Changing the default encryption\n\nHow it works…\n\nBy changing the default encryption for your bucket, all newly\n\nuploaded objects to your bucket, which don’t have an encryption\n\nsetting, will be encrypted using the KMS you have provided.\n\nAlready existing objects in your bucket will not be aﬀected.\n\nEnabling the bucket key leads to cost savings in KMS service calls\n\nassociated with the encryption or decryption of individual objects.\n\nis is achieved by KMS generating a key at the bucket level rather\n\nthan generating a separate KMS key for each encrypted object. S3",
      "content_length": 581,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 82,
      "content": "uses this bucket-level key to generate distinct data keys for objects\n\nwithin the bucket, thereby eliminating the need for additional KMS\n\nrequests to complete encryption operations.\n\nThere’s more…\n\nBy following this recipe, you can encrypt your objects with SSE-\n\nKMS but only if they don’t have encryption con\u0000gured. You can\n\nenforce your objects to have an SSE-KMS encryption setting in the\n\nPUT operation using a bucket policy, as shown here:\n\n1. Navigate to the bucket’s Permissions tab.\n\n2. Go to the Bucket Policy section and click on Edit.\n\n3. Paste the following policy. Make sure you replace <your-bucket-\n\nname> with the actual name of your S3 bucket and <your-kms-\n\nkey-arn> with the Amazon Resource Name (ARN) of your KMS key:\n\n{ \"Version\": \"2012-10-17\", \"Id\": \"EnforceSSE-KMS\", \"Statement\": [ { \"Sid\": \"DenyNonKmsEncrypted\", \"Effect\": \"Deny\", \"Principal\": \"*\", \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::<your- bucket-name>/*\", \"Condition\": {",
      "content_length": 965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 83,
      "content": "\"StringNotEquals\": { \"s3:x-amz-server-side- encryption\": \"aws:kms\" } } }, { \"Sid\": \"AllowKmsEncrypted\", \"Effect\": \"Allow\", \"Principal\": \"*\", \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::<your- bucket-name>/*\", \"Condition\": { \"StringEquals\": { \"s3:x-amz-server-side- encryption\": \"aws:kms\", \"s3:x-amz-server-side- encryption-aws-kms-key-id\": \"<your-kms-key- arn>\" } } } ] }\n\n4. Save your changes.\n\nis policy contains two statements. e \u0000rst statement\n\n(DenyNonKmsEncrypted) denies the s3:PutObject action for\n\nany request that does not include SSE-KMS encryption. e second\n\nstatement (AllowKmsEncrypted) only allows the s3:PutObject",
      "content_length": 640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 84,
      "content": "action when the request includes SSE-KMS encryption and the\n\nspeci\u0000ed KMS key.\n\nSee also\n\nSSE-C:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEn\n\ncryptionCustomerKeys.html\n\nClient-side encryption:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSi\n\ndeEncryption.html\n\nEnforcing encryption in transit with TLS1.2 or higher with Amazon S3:\n\nhttps://aws.amazon.com/blogs/storage/enforcing-encryption-in-transit-\n\nwith-tls1-2-or-higher-with-amazon-s3/\n\nEncrypting existing S3 objects:\n\nhttps://aws.amazon.com/blogs/storage/encrypting-existing-amazon-s3-\n\nobjects-with-the-aws-cli/\n\nSetting up retention policies for your objects\n\nAmazon S3’s storage lifecycle allows you to manage the lifecycle of\n\nobjects in an S3 bucket based on prede\u0000ned rules. e lifecycle\n\nmanagement feature consists of two main actions: transitions and\n\nexpiration. Transitions involve automatically moving objects",
      "content_length": 923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 85,
      "content": "between diﬀerent storage classes based on a de\u0000ned duration. is\n\nhelps in optimizing costs by storing less frequently accessed data in\n\na cheaper storage class. Expiration, on the other hand, allows users\n\nto set rules to automatically delete objects from an S3 bucket. ese\n\nrules can be based on a speci\u0000ed duration. Additionally, you can\n\napply a combination of transitions and expiration actions to objects.\n\nAmazon S3’s storage lifecycle provides \u0000exibility and ease of\n\nmanagement for users and it helps organizations optimize storage\n\ncosts while ensuring that data is stored according to its relevance\n\nand access patterns.\n\nIn this recipe, we will learn how to set up a lifecycle policy to\n\narchive objects in S3 Glacier aer a certain period and then expire\n\nthem.\n\nGetting ready\n\nTo complete this recipe, you need to have a Glacier vault, which is a\n\nseparate storage container that can be used to store archives,\n\nindependent from S3. You can create one by following these steps:\n\n1. Open the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the Glacier service.\n\n2. Click on Create vault to start creating a new Glacier vault.",
      "content_length": 1206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 86,
      "content": "3. Provide a unique and descriptive name for your vault in the Vault name\n\n\u0000eld.\n\n4. Optionally, you can choose to receive noti\u0000cations for events by clicking\n\nTurn on noti\u0000cations under the Event noti\u0000cations section.\n\n5. Click on Create to create the vault.\n\nHow to do it…\n\n1. Open the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the S3 service.\n\n2. Select the desired bucket for which you want to con\u0000gure the lifecycle\n\npolicy and navigate to the Management tab.\n\n3. In the le panel, select Lifecycle and click on Create lifecycle rule.\n\n4. Under Rule name, name the lifecycle rule to identify it.\n\n5. Under Choose a rule scope, you can choose Apply to all objects in the\n\nbucket or Limit the scope of this rule using one or more \u0000lters to specify\n\nthe objects for which the rule will be applied. You can use one of the\n\nfollowing \u0000lters or a combination of them:\n\nFilter objects based on pre\u0000xes (for example, logs)\n\nFilter objects based on tags; you can add multiple key-value pair\n\ntags to \u0000lter on\n\nFilter objects based on object size by setting Specify minimum\n\nobject size and/or Specify maximum object size and specifying",
      "content_length": 1204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 87,
      "content": "the size value and unit\n\ne following screenshot shows a rule that’s been restricted to a\n\nset of objects based on a pre\u0000x:\n\nFigure 1.4 – Lifecycle rule conﬁguration\n\n6. Under Lifecycle rule actions, select the following options:\n\nMove current versions of objects between storage classes. en,\n\nchoose one of the Glacier classes and set Days aer object",
      "content_length": 353,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 88,
      "content": "creation in which the object will be transitioned (for example,\n\n60 days).\n\nExpire current versions of objects. en, set Days aer object\n\ncreation in which the object will expire. Choose a value higher\n\nthan the one you set for transitioning the object to Glacier (for\n\nexample, 100).\n\nReview the transition and expiration actions you have set and\n\nclick on Create rule to apply the lifecycle policy to the bucket:\n\nFigure 1.5 – Reviewing the lifecycle rule\n\nNOTE",
      "content_length": 464,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 89,
      "content": "It may take some time for the lifecycle rule to be applied to all the selected\n\nobjects, depending on the size of the bucket and the number of objects. The\n\nrule will aﬀect existing ﬁles, not just new ones, so ensure that no applications\n\nare accessing ﬁles that will be archived or deleted as they will no longer be\n\naccessible via direct S3 retrieval.\n\nHow it works…\n\nAer you save the lifecycle rule, Amazon S3 will periodically\n\nevaluate it to \u0000nd objects that meet the criteria speci\u0000ed in the\n\nlifecycle rule. In this recipe, the object will remain in its default\n\nstorage type for the speci\u0000ed period (for example, 60 days) aer\n\nwhich it will automatically be moved to the Glacier storage class.\n\nis transition is handled transparently, and the object’s metadata\n\nand properties remain unchanged. Once the objects are transitioned\n\nto Glacier, they are stored in a Glacier vault and become part of the\n\nGlacier storage infrastructure. Objects will then remain in Glacier\n\nfor the remaining period of expiry (for example, 40 days), aer\n\nwhich they will expire and be permanently deleted from your S3\n\nbucket.\n\nPlease note that once the objects have expired, they will be queued\n\nfor deletion, so it might take a few days aer the object reaches the\n\nend of its lifetime for it to be deleted.",
      "content_length": 1300,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 90,
      "content": "There’s more…\n\nLifecycle con\u0000guration can be speci\u0000ed as an XML when using the\n\nS3 API or AWS console, which can be helpful if you are planning on\n\nusing the same lifecycle rules on multiple buckets. You can read\n\nmore on setting this up at\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-\n\nlifecycle-rules.html.\n\nSee also\n\nSetting up noti\u0000cations for events related to your lifecycle rule:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-\n\ncon\u0000gure-noti\u0000cation.html\n\nSupported lifecycle transitions and related constraints:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-\n\ntransition-general-considerations.html\n\nVersioning your data\n\nAmazon S3 versioning refers to maintaining multiple variants of an\n\nobject at the same time in the same bucket. Versioning provides you\n\nwith an additional layer of protection by giving you a way to recover\n\nfrom unintended overwrites and accidental deletions as well as\n\napplication failures.",
      "content_length": 977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 91,
      "content": "S3 Object Versioning is not enabled by default and has to be\n\nexplicitly enabled for each bucket. Once enabled, versioning cannot\n\nbe disabled and can only be suspended. When versioning is\n\nenabled, you will be able to preserve, retrieve, and restore any\n\nversion of an object stored in the bucket using the version ID. Every\n\nversion of an object is the whole object, not the delta from the\n\nprevious version, and you can set permissions at the version level.\n\nSo, you can set diﬀerent permissions for diﬀerent versions of the\n\nsame object.\n\nIn this recipe, we’ll learn how to delete the current version of an\n\nobject to make the previous one the current version.\n\nGetting ready\n\nFor this recipe, you need to have a version-enabled bucket with an\n\nobject that has at least two versions.\n\nYou can enable versioning for your bucket by going to the bucket’s\n\nProperties tab, editing the Bucket Versioning area, and setting it to\n\nEnable:",
      "content_length": 935,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 92,
      "content": "Figure 1.6 – Enabling bucket versioning\n\nYou can create a new version of an object by simply uploading a \u0000le\n\nwith the same name to the versioning-enabled bucket.\n\nIt’s important to note that enabling versioning for a bucket is\n\nirreversible. Once versioning is enabled, it will be applied to all\n\nexisting and future objects in that bucket. So, before enabling\n\nversioning, make sure that your application or work\u0000ow is\n\ncompatible with object versioning.\n\nEnabling versioning for the \u0000rst time will take time to take eﬀect, so\n\nwe recommend waiting 15 minutes before performing any write\n\noperation on objects in the bucket.",
      "content_length": 626,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 93,
      "content": "How to do it…\n\n1. Sign in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the S3 service.\n\n2. In the Buckets list, select the S3 bucket that contains the object for which\n\nyou want to set the previous version as the current one.\n\n3. In the Objects tab, click on Show versions. Here, you can view all your\n\nobject versions:\n\nFigure 1.7 – Object versions\n\n4. Select the current version of the object that you want to delete. It’s the\n\ntop-most version with the latest modi\u0000ed date.\n\n5. Click on the Delete button and write permanently delete as\n\nprompted on the next screen.",
      "content_length": 646,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 94,
      "content": "Aer deleting the current version, the previous version will\n\nautomatically become the latest version:\n\nFigure 1.8 – Object versions after version deletion\n\n6. Verify that the previous version is now the latest version by checking the\n\nLast modi\u0000ed timestamps or verifying this through object listing,\n\nmetadata, or download.\n\nHow it works…\n\nOnce you enable bucket versioning, each object in the bucket will\n\nhave a version ID that uniquely identi\u0000es the object in the bucket,\n\nand the non-version-enabled buckets will have their version IDs set\n\nto null for their objects. e older versions of an object become\n\nnon-current but continue to exist and remain accessible. When you\n\ndelete the current version of the object, it will be permanently\n\nremoved and the S3 versioning mechanism will automatically\n\npromote the previous version as the current one aer deletion. If\n\nyou delete an object without specifying the version ID, Amazon S3",
      "content_length": 938,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 95,
      "content": "doesn’t delete it permanently; instead, it inserts a delete marker into\n\nit and it becomes the current object version. However, you can still\n\nrestore its previous versions:\n\nFigure 1.9 – Object with a delete marker\n\nThere’s more…\n\nS3 rates apply to every version of an object that’s stored and\n\nrequested, so keeping non-current versions of objects can increase\n\nyour storage cost. You can use lifecycle rules to archive the non-\n\ncurrent versions or permanently delete them aer a certain period\n\nand keep the bucket clean from unnecessary object versions.\n\nFollow these steps to add a lifecycle rule to delete non-current\n\nversions aer a certain period:",
      "content_length": 657,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 96,
      "content": "1. Go to the bucket’s Management tab and click on the Lifecycle\n\ncon\u0000guration.\n\n2. Click on the Add lifecycle rule button to create a new rule.\n\n3. Provide a unique name for the rule.\n\n4. Under Apply rule to, select the appropriate resources (for example, the\n\nentire bucket or speci\u0000c pre\u0000xes).\n\n5. Set the action to Permanently delete non-current versions.\n\n6. Specify Days aer objects become noncurrent in which the delete will be\n\nexecuted. Optionally, you can specify Number of newer versions to\n\nretain, which means it will keep the said number of versions for the\n\nobject and all others will be deleted when they are eligible for deletion\n\nbased on the speci\u0000ed period.\n\n7. Click on Save to save the lifecycle rule.\n\nSee also\n\nYou can prevent accidental or malicious deletion of objects in a\n\nversioned bucket by enabling Multi-Factor Authentication (MFA)\n\nDelete. You can learn how to implement this at\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorA\n\nuthenticationDelete.html.\n\nSetting permissions at the version level:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/VersionedObj\n\nectPermissionsandACLs.html.",
      "content_length": 1146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 97,
      "content": "Replicating your data\n\nAWS S3 replication is an automatic asynchronous process that\n\ninvolves copying objects to one or multiple destination buckets.\n\nReplication can be con\u0000gured across buckets in the same AWS\n\nregion with Same-Region Replication, which can be useful for\n\nscenarios such as isolating diﬀerent workloads, segregating data for\n\ndiﬀerent teams, or achieving compliance requirements. Replication\n\ncan also be con\u0000gured for buckets across diﬀerent AWS regions\n\nwith Cross-Region Replication (CRR), which helps in reducing\n\nlatency for accessing data, especially for enterprises with a large\n\nnumber of locations, by maintaining multiple copies of the objects\n\nin diﬀerent geographies or diﬀerent regions. It provides compliance\n\nand data redundancy for improved performance, availability, and\n\ndisaster recovery capabilities.\n\nIn this recipe, we’ll learn how to set up replication between two\n\nbuckets in diﬀerent AWS regions and the same AWS account.\n\nGetting ready\n\nYou need to have an S3 bucket in the destination AWS region to act\n\nas a target for the replication. Also, S3 versioning must be enabled\n\nfor both the source and destination buckets.",
      "content_length": 1163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 98,
      "content": "How to do it…\n\n1. Sign in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the S3 service.\n\n2. In the Buckets list, choose the source bucket you want to replicate.\n\n3. Go to the Management tab and select Create replication rule under\n\nReplication rules.\n\n4. Under Replication rule name in the Replication rule con\u0000guration\n\nsection, give your rule a unique name.\n\n5. Under Status, either keep it Enabled for the rule to take eﬀect once you\n\nsave it or change it to Disabled to enable it later as required:\n\nFigure 1.10 – Replication rule conﬁguration\n\n6. If this is the \u0000rst replication rule for the bucket, Priority will be set to 0.\n\nSubsequent rules that are added will be assigned higher priorities. When",
      "content_length": 781,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 99,
      "content": "multiple rules share the same destination, the rule with the highest\n\npriority takes precedence during execution, typically the one created last.\n\nIf you wish to control the priority for each rule, you can achieve this by\n\nsetting the rule using XML. For guidance on how to con\u0000gure this, refer\n\nto the See also section.\n\n7. In the Source bucket section, you have the option to replicate all objects\n\nin the bucket by selecting Apply to all objects in the bucket or you can\n\nnarrow it down to speci\u0000c objects by selecting Limit the scope of this\n\nrule using one or more \u0000lters and specifying a Pre\u0000x value (for example,\n\nlogs_ or logs/) to \u0000lter objects. Additionally, you have the option to\n\nreplicate objects based on their tags. Simply choose Add tag and input\n\nkey-value pairs. is process can be repeated so that you can include\n\nmultiple tags:",
      "content_length": 849,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 100,
      "content": "Figure 1.11 – Source bucket conﬁguration\n\n8. Under Destination, select Choose a bucket in this account and enter or\n\nbrowse for the destination bucket name.\n\n9. Under IAM role, select Choose from existing IAM roles, then choose\n\nCreate new role from the drop-down list.\n\n10. Under Destination storage class, you can select Change the storage class\n\nfor the replicated objects and choose one of the storage classes to be set\n\nfor the replicated objects in the destination bucket.\n\n11. Click on Save to save your changes.",
      "content_length": 519,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 101,
      "content": "How it works…\n\nBy adding this replication rule, you grant the source bucket\n\npermission to replicate objects to the destination bucket in the said\n\nregion. Once the replication process is complete, the destination\n\nbucket will contain a copy of the objects from the source bucket.\n\ne objects in the destination bucket will have the same ownership,\n\npermissions, and metadata as the source objects. When you enable\n\nreplication to your bucket, several background processes occur to\n\nfacilitate this process. S3 continuously monitors changes to objects\n\nin your source bucket. Once a change is detected, S3 generates a\n\nreplication request for the corresponding objects and initiates the\n\nprocess of transferring the data from the source to the destination\n\nbucket.\n\nThere’s more…\n\nere are additional options that you can enable while setting the\n\nreplication rule under Additional replication options. e\n\nReplication metrics option enables you to monitor the replication\n\nprogress with S3 Replication metrics. It does this by tracking bytes\n\npending, operations pending, and replication latency. e\n\nReplication Time Control (RTC) option can be bene\u0000cial if you\n\nhave a strict service-level agreement (SLA) for data replication as it\n\nwill ensure that approximately 99% of your objects will be replicated",
      "content_length": 1306,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 102,
      "content": "within a 15-minute timeframe. It also enables replication metrics to\n\nnotify you of any instances of delayed object replication. e Delete\n\nmarker replication option will replicate object versions with a delete\n\nmarker. Finally, the Replica modi\u0000cation sync option will replicate\n\nthe metadata changes of objects.\n\nSee also\n\nReplicating buckets in diﬀerent AWS accounts:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-\n\nwalkthrough-2.html\n\nReplication con\u0000guration:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-\n\nadd-con\u0000g.html\n\nMonitoring your S3 bucket\n\nEnabling and monitoring S3 metrics allows you to proactively\n\nmanage your S3 resources, optimize performance, ensure\n\nappropriate security and compliance measures are in place, identify\n\ncost-saving opportunities, and ensure the operational readiness of\n\nyour S3 infrastructure. S3 oﬀers various methods for monitoring\n\nyour buckets, including S3 server access logs, CloudTrail,\n\nCloudWatch metrics, and S3 event noti\u0000cations. S3 server access\n\nlogs can be enabled to log each request made to the bucket.",
      "content_length": 1102,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 103,
      "content": "CloudTrail captures actions taken on S3 or API calls on the bucket,\n\nallowing you to monitor and audit actions, including object-level\n\noperations such as uploads, downloads, and deletions. CloudWatch\n\nmetrics track speci\u0000c metrics for your buckets and allow you to set\n\nup alarms so that you receive noti\u0000cations when certain thresholds\n\nare met. S3 event noti\u0000cations enable you to set up noti\u0000cations for\n\nspeci\u0000c S3 events and con\u0000gure actions in response to those events.\n\nIn this recipe, we will cover enabling CloudTrail for your S3 buckets\n\nand con\u0000guring CloudWatch metrics to monitor high-volume data\n\ntransfer based on these logs.\n\nGetting ready\n\nTo proceed with this recipe, you need to enable CloudTrail so that it\n\ncan log S3 data events and insights. Follow these steps:\n\n1. Open the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the CloudTrail service.\n\n2. Click on Trails in the le navigation pane and click on Create trail to\n\ncreate a new trail.\n\n3. Provide a name for the trail in the Trail name \u0000eld.\n\n4. For Storage location, you need to provide an S3 bucket for storing\n\nCloudTrail logs. You can select Use existing S3 bucket or Create new S3\n\nbucket.",
      "content_length": 1244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 104,
      "content": "5. Optionally, you can enable Log \u0000le SSE-KMS encryption and choose the\n\nKMS key.\n\n6. Under CloudWatch Logs, choose Yes for Send CloudTrail events to\n\nCloudWatch Logs.\n\n7. Con\u0000gure the CloudWatch Logs settings as per your requirements. For\n\nexample, you can select an existing CloudWatch Logs group or create a\n\nnew one:\n\nFigure 1.12 – Enabling CloudWatch Logs\n\n8. For Role name, choose to create a new one and give it a name.\n\n9. Review the other trail settings, such as log \u0000le validation and tags, make\n\nadjustments if needed, and click on Next.",
      "content_length": 548,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 105,
      "content": "10. Under the Events section, enable Data events and Insight events in\n\naddition to Management events, which is already enabled.\n\n11. Under Management events, select Read and Write:\n\nFigure 1.13 – Conﬁguring Events\n\n12. Under Data events, choose S3 for Data event type and Log all events for\n\nthe Log selector template.\n\n13. Under Insights events, select API call rate and API error rate.\n\n14. Click on Next and then click on Create trail to create the trail.",
      "content_length": 459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 106,
      "content": "Once the trail has been created, CloudTrail will start capturing S3\n\ndata events and storing the logs in the speci\u0000ed S3 bucket.\n\nSimultaneously, the logs will be sent to the CloudWatch Logs group\n\nspeci\u0000ed during trail creation.\n\nHow to do it…\n\n1. Open the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the CloudWatch console.\n\n2. Go to Log groups from the navigation pane on the le and select the\n\nCloudTrail log group you just created.\n\n3. Click on Create Metric Filter from the Action drop-down list.\n\n4. Provide { ($.eventName = CopyObject) ||\n\n($.eventName = PutObject) || ($.eventName =\n\nCompleteMultipartUpload) &&\n\n$.request.bytes_transferred > 500000000} as the \u0000lter\n\npattern. is \u0000lter pattern will capture events related to copying or\n\nuploading objects to S3 that are larger than 500 MB. e threshold value\n\nshould be set based on your bucket access patterns.\n\nYou can test your pattern by specifying one of the log \u0000les or\n\nproviding a custom log in the Test pattern section. en, you\n\ncan click on Test pattern and validate the result:",
      "content_length": 1121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 107,
      "content": "Figure 1.14 – Filter pattern\n\n5. Click on Next.\n\n6. Under the Create \u0000lter name \u0000eld, specify a \u0000lter name.\n\n7. Under the Metric Details section, specify a Metric namespace value (for\n\nexample, S3Metrics) and provide a name for the metric itself under",
      "content_length": 251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 108,
      "content": "Metric name (for example, HighVolumeTransfers).\n\n8. Set Unit to Count for your metric and set Metric value to 1 to indicate\n\nthat a transfer event has occurred. Finally, set Default value to 0:\n\nFigure 1.15 – Metric details\n\n9. Click on Create metric \u0000lter.\n\nHow it works…",
      "content_length": 272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 109,
      "content": "By enabling CloudTrail in your AWS account and ensuring that the\n\nlogs are delivered to CloudWatch, the S3 API activities can be\n\naccessed and analyzed within your AWS environment. By creating a\n\nmetric \u0000lter with a customized \u0000lter pattern that matches S3\n\ntransfer events, relevant information from the CloudTrail logs can\n\nbe extracted. Once the metric \u0000lter is created, CloudWatch\n\ngenerates a custom metric based on the \u0000lter’s con\u0000guration. is\n\nmetric represents the occurrence of high-volume S3 transfers. You\n\ncan then view this metric in the CloudWatch console, where you\n\ncan gain insights into your S3 transfer activity and take the\n\nnecessary actions.\n\nThere’s more…\n\nOnce your metric has been created, you can create alarms based on\n\nthe metric’s value to notify you when a high volume of S3 transfers\n\nhas been detected.\n\nTo create an alarm for the metric you have created based on high-\n\nvolume S3 transfers from CloudTrail logs on CloudWatch, follow\n\nthese steps:\n\n1. Go to the CloudWatch console and select the Alarms tab.\n\n2. Click on Create Alarm. In the Create Alarm wizard, select the metric\n\nyou created. You can \u0000nd it by navigating the namespace and \u0000nding the\n\nmetric name you con\u0000gured earlier.",
      "content_length": 1221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 110,
      "content": "3. Under the Metric section, set Statistic to Sum and Period to 15 minutes.\n\nis can be changed as per your needs:\n\nFigure 1.16 – Metric statistics\n\n4. Under the Conditions section, Set reshold type to Static and choose\n\nGreater than for the alarm condition. is indicates a high volume\n\ntransfer on your bucket, as per your observations. Optionally, you can\n\nchoose how many data points within the evaluation period must be\n\nbreached to cause the alarm to go to the alarm state by expanding\n\nAdditional con\u0000guration. is will help you avoid false positives caused\n\nby transient spikes in the metric values:",
      "content_length": 608,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 111,
      "content": "Figure 1.17 – Metric conditions\n\n5. Click on Next.\n\n6. Under the Noti\u0000cation section, choose In alarm to send a noti\u0000cation\n\nwhen the metric is in the alarm state. Choose Create a new topic,\n\nprovide a name for it and the email endpoints that will receive the SNS\n\nnoti\u0000cation, and click on Create topic or choose an existing SNS topic if\n\nyou have one that’s been con\u0000gured. You can con\u0000gure other actions to\n\nbe executed if the alarm state is triggered, such as executing a Lambda\n\nfunction or performing automated scaling actions:",
      "content_length": 533,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 112,
      "content": "Figure 1.18 – Metric notiﬁcation settings\n\n7. Provide a name for the alarm so that it can be identi\u0000ed with ease.\n\n8. Review the alarm settings and click on Create Alarm to create the alarm.\n\nOnce the alarm has been created, it will start monitoring the metric\n\nfor high-volume S3 transfers based on the de\u0000ned conditions. If the\n\nthreshold is breached for the speci\u0000ed duration (there are more\n\nthan 150 data transfer requests of more than 500 MB within 45\n\nminutes), the alarm state will be triggered, and an SNS noti\u0000cation",
      "content_length": 526,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 113,
      "content": "will be sent. is allows you to receive timely noti\u0000cations and take\n\nappropriate remedial actions in case of high-volume S3 transfers,\n\nensuring that any potential issues are addressed proactively.\n\nSee also\n\nS3 monitoring tools:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/monitoring-\n\nautomated-manual.html\n\nLogging options for S3:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-\n\nS3.html\n\nOceanofPDF.com",
      "content_length": 439,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 114,
      "content": "2 Sharing Your Data Across Environments and Accounts\n\nData sharing plays a pivotal role in today’s data-driven world,\n\nenabling organizations to unlock the full potential of their data\n\nresources. It encourages collaboration and drives innovation.\n\nSharing data securely and eﬃciently is a fundamental requirement\n\nfor businesses across various industries. is chapter explores\n\nvarious AWS solutions designed to facilitate the secure sharing of\n\ndata with both internal and external stakeholders. ese solutions\n\nrecognize the signi\u0000cance of data sharing and provide robust\n\noptions to meet diverse sharing needs.\n\nWe will start by discussing methods for sharing database resources\n\nby creating read-only replicas for RDS, which ensures reliable\n\nrelational data sharing and reduces the operational load on the\n\nprimary RDS instance. Similarly, we will explore Redshi live data\n\nsharing that allows multiple clusters to access the same data,\n\nenhancing collaborative analysis and distributing the load across\n\ndiﬀerent clusters.\n\nNext, we will leverage Lake Formation to synchronize the Glue Data\n\nCatalog across diﬀerent AWS accounts, providing a uni\u0000ed view of",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 115,
      "content": "metadata for data discovery. We’ll also examine enforcing \u0000ne-\n\ngrained permissions on Simple Storage Service (S3) data using Lake\n\nFormation to enhance data lake security.\n\nFor dynamic data sharing, we will cover real-time S3 data sharing\n\nusing SNS and Lambda for immediate access, as well as temporary\n\nsharing using presigned URLs. Finally, we’ll discuss sharing\n\noperational data by providing read-only access to CloudWatch data\n\nwith other AWS accounts.\n\nis chapter contains the following recipes:\n\nCreating read-only replicas for RDS\n\nRedshi live data sharing among your clusters\n\nSynchronizing Glue Data Catalog to a diﬀerent account\n\nEnforcing \u0000ne-grained permissions on S3 data sharing using Lake\n\nFormation\n\nSharing your S3 data temporarily using a presigned URL\n\nReal-time sharing of S3 data\n\nSharing read-only access to your CloudWatch data with another AWS\n\naccount\n\nTechnical requirements",
      "content_length": 905,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 116,
      "content": "Many recipes in this chapter presume the availability of an S3\n\nbucket, Glue databases and tables, and an IAM role/user, in\n\naddition to the one you are utilizing to implement the recipes, to\n\nserve as the grantee for data sharing. Furthermore, certain recipes\n\nassume the presence of two AWS accounts to streamline the process\n\nof sharing data with other AWS accounts.\n\nYou can \u0000nd the code \u0000les for this chapter on GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter02.\n\nCreating read-only replicas for RDS\n\nAmazon Relational Database Service (RDS) provides a functionality\n\nto establish read-only replicas, replicating your database for\n\nmultiple users’ access and eﬃcient data sharing. ese replicas can\n\nreside in the same region as, or a diﬀerent one from, the primary\n\none. is proves bene\u0000cial when encountering heavy read loads that\n\ncould potentially impact RDS performance; replicas can\n\naccommodate the heavy read loads without impacting the primary\n\ninstance’s performance. Additionally, you can create a replica in a\n\nregion that is closer to your users, which can also help in the event\n\nof primary region disruption where the replica can be promoted to\n\na standalone instance, thus allowing seamless operation\n\ncontinuation and disaster recovery.",
      "content_length": 1311,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 117,
      "content": "RDS oﬀers two primary replication modes for read replicas:\n\nIn single-primary mode, a single primary RDS instance manages both\n\nread and write operations, while one or more read replicas can be added\n\nto oﬄoad read traﬃc, making it ideal for applications with high read and\n\nlow write loads.\n\nIn contrast, multi-primary mode features multiple primary instances\n\nthat handle both read and write operations, with synchronous\n\nreplication among them. is setup ensures that if one primary instance\n\nfails, the others can seamlessly take over, making it suitable for\n\napplications with high read and write demands that require high\n\navailability.\n\nIn this recipe, we will learn how to create a read-only replica for\n\nRDS.\n\nGetting ready\n\nFor this recipe, you need to have an RDS instance that can be of the\n\nMySQL, PostgreSQL, or MariaDB type, with automatic backup\n\nenabled. You can enable this by modifying your RDS instance and\n\nsetting the backup retention period to a value greater than zero.\n\nHow to do it…\n\n1. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-",
      "content_length": 1116,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 118,
      "content": "signin) and navigate to the RDS service.\n\n2. Select Databases from the le navigation pane, locate your RDS, and\n\nselect Create read replica from the Actions menu.\n\n3. Enter a unique name for your replica under the DB instance identi\u0000er\n\n\u0000eld.\n\n4. In the Instance con\u0000guration section, choose a class for your replica that\n\nmatches the primary RDS class or a bigger one that aligns with your\n\nrequirements.\n\n5. Choose the destination region for your replica. You can choose the same\n\nregion as your primary replica or a diﬀerent one.\n\n6. When con\u0000guring storage, it’s advisable to allocate a type and size\n\nequivalent to or larger than what is allocated to the primary RDS. Under\n\nStorage autoscaling, you can check the box next to Enable storage\n\nautoscaling and choose the maximum value that the replica can auto-\n\nscale to.",
      "content_length": 826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 119,
      "content": "Figure 2.1 – Conﬁguring storage for the replica\n\n7. In the Availability section, select from three options according to your\n\nrequirements:",
      "content_length": 139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 120,
      "content": "Multi-AZ DB Cluster, which deploys one primary DB and two\n\nreadable standby instances across diﬀerent availability zones\n\nMulti-AZ DB Instance, which launches one primary instance\n\nand one standby DB in separate availability zones\n\nSingle DB instance, which creates a single DB instance\n\n8. For the Connectivity section, choose the protocol used to communicate\n\nwith your database, IPv4 or Dual-stack mode, which will allow\n\ncommunications from both IPv6 and IPv4 protocols. Ensure that the\n\nreplica is assigned to the same DB subnet group as the primary RDS.\n\nDetermine whether the replica will allow public access (this is usually\n\ndisabled in production settings) and specify the security group\n\ngoverning access. Lastly, set the port number for replica connections.",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 121,
      "content": "Figure 2.2 – Conﬁguring connectivity for the replica\n\n9. Select the preferred authentication method for the replica.\n\n10. Con\u0000gure additional settings as required in the Additional con\u0000guration\n\nsection. Note that encryption settings must match those of the primary\n\nRDS.\n\n11. Click on Create read replica.",
      "content_length": 306,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 122,
      "content": "How it works…\n\nWhile con\u0000guring the read replica, you need to make several key\n\ndecisions to align with your performance, availability, and security\n\nrequirements. First, choose the DB instance class and storage to\n\nmatch or exceed the speci\u0000cations of the primary RDS, especially if\n\nyou anticipate a high load on the replica. Next, decide on the region\n\nfor your replica. Placing the replica in the same region as the\n\nprimary RDS results in lower latency for data replication, while\n\nplacing it in a diﬀerent region enhances disaster recovery\n\ncapabilities but introduces higher latency and potentially higher\n\ndata transfer costs. For availability, you must strike a balance\n\nbetween higher availability options and associated costs. For\n\nconnectivity, you can choose between IPv4, which supports\n\ncommunication over the traditional IP version 4, or dual-stack\n\nmode, which allows communication over both IPv4 and IPv6,\n\noﬀering greater \u0000exibility. Once the replica creation is initiated,\n\nAWS generates a snapshot of the primary RDS that it uses to create\n\nthe replica. e duration of replica creation depends on the data\n\nsize of the primary RDS. Upon completion, the replica becomes\n\navailable and the snapshot is deleted. Asynchronous replication\n\ngoverns data transfer from the primary RDS to the read replica.\n\nData changes in the primary RDS are logged in the binary log,\n\nperiodically retrieved by the replication process, and transmitted to",
      "content_length": 1453,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 123,
      "content": "the read replica. Replica lag, a potential delay between the primary\n\nand replica RDS instances, may occur due to various factors\n\nincluding network latency, workload on the primary instance, and\n\nthe magnitude of data changes.\n\nThere’s more…\n\nFor enhanced scalability, it’s possible to create up to \u0000ve replicas per\n\ndatabase and implement load balancing among them via AWS\n\nRoute 53 to evenly distribute the workload between them.\n\nFurthermore, you can establish read replicas from these replicas;\n\nhowever, it’s important to note that this may result in increased lag\n\ndue to dual-level synchronization. In the event of disruption in the\n\nprimary RDS, replicas can be manually promoted to standalone\n\ninstances, ensuring seamless operational continuity and facilitating\n\ndisaster recovery.\n\nSee also\n\nHow can I distribute read requests across multiple Amazon RDS read\n\nreplicas?: https://repost.aws/knowledge-center/requests-rds-read-\n\nreplicas\n\nRedshift live data sharing among your clusters",
      "content_length": 995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 124,
      "content": "Redshi data sharing is a feature provided by Amazon Redshi. It\n\nenables the sharing of data within and across AWS accounts,\n\nallowing diﬀerent teams or users to access the same data sets for\n\ntheir analytics or reporting needs. Datashare re\u0000ects updates in the\n\nproducer cluster in real time with the consumers ensuring that all\n\nusers have access to the most up-to-date information without the\n\nneed to create and maintain duplicate copies of the same data.\n\nIn this recipe, we will see how to share Redshi data between two\n\nRedshi clusters in diﬀerent AWS accounts.\n\nGetting ready\n\nis recipe assumes that you have two encrypted Redshi clusters\n\nusing Ra3 node types. Also, the tables to be shared from the\n\nproducer cluster should not have interleaved sort keys. If you want\n\nto follow the recipe with clusters in the same account, you can skip\n\nthe authorization and association steps. Instead, directly grant usage\n\nof the datashare to the cluster namespace rather than to the AWS\n\naccount.\n\nHow to do it…\n\ne following actions must be performed in the Redshi producer\n\ncluster:",
      "content_length": 1089,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 125,
      "content": "1. Create the datashare:\n\nI. Connect to your Redshi cluster using a super user.\n\nII. Create a datashare using the following command:\n\nCREATE DATASHARE datashare_name;\n\nAdd PUBLICACCESSIBLE = TRUE to the command if\n\nyour consumer cluster has public access enabled.\n\nIII. Add a schema to the datashare:\n\nALTER DATASHARE datashare_name ADD SCHEMA schema_name;\n\nIV. Add tables under the schema that you have added to the\n\ndatashare:\n\nALTER DATASHARE datashare_name ADD TABLE schema_name.table_name;\n\nYou can add all the tables within the schema using the\n\ncommand that follows:\n\nALTER DATASHARE datashare_name ADD ALL TABLES in schema schema_name;\n\nTo add all new tables in the schema, add SET\n\nINCLUDENEW = TRUE; to the previous command.\n\nV. Repeat the previous two steps for all the schemas you want to\n\nshare.\n\nVI. Grant permission for the consumer cluster AWS account:\n\nGRANT USAGE ON DATASHARE datashare_name TO ACCOUNT 'account_ID';",
      "content_length": 935,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 126,
      "content": "2. Authorize the datashare:\n\nI. Log in to AWS Management Console\n\n(https://console.aws.amazon.com/console/home?\n\nnc2=h_ct&src=header-signin) and navigate to the Redshi\n\nservice.\n\nII. From the navigation menu on the le, go to Datashares.\n\nIII. Go to the In my account tab and open the datashare you have\n\ncreated.\n\nIV. In the Data consumers section, choose the consumer you gave\n\naccess to and then click on Authorize.\n\nFigure 2.3 – Authorizing a consumer\n\nActions in the consumer Redshi cluster include the following:\n\n1. Associate the datashare with consumer clusters:\n\nI. Log in to AWS Management Console and navigate to the\n\nRedshi service.\n\nII. From the navigation menu on the le, go to Datashares.\n\nIII. Choose the From other accounts tab. Select the datashare you\n\nhave created in the producer cluster and click on Associate.",
      "content_length": 836,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 127,
      "content": "Figure 2.4 – Associating the datashare\n\nIV. Under Association type, select Speci\u0000c AWS Regions and namespaces,\n\nthen choose Add Region.\n\nFigure 2.5 – Choosing the association type\n\nV. Select the region your cluster resides in, then select Add speci\u0000c cluster\n\nnamespaces, select one or more cluster namespaces, and click on Add\n\nAWS Region. Alternatively, you can choose to associate the datashare\n\nwith all the clusters in your account by choosing the entire AWS account",
      "content_length": 471,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 128,
      "content": "or all clusters in a speci\u0000c region by choosing Entire AWS account or\n\nAdd all cluster namespaces.\n\nFigure 2.6 – Deﬁning the AWS region to associate\n\nVI. Click on Associate.",
      "content_length": 173,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 129,
      "content": "Figure 2.7 – Association with speciﬁc AWS region\n\n2. Create a database for the datashare:\n\nI. Connect to your Redshi cluster.\n\nII. Create a local database that references the datashare, specifying\n\nthe namespace and account ID:\n\nCREATE DATABASE consumer_database_name FROM DATASHARE datashare_name OF ACCOUNT 'producer_account_ID' NAMESPACE 'producer_cluster_namespace';\n\nIII. You can now query the tables as follows:\n\nSELECT * FROM consumer_database_name.schema_name.table _name;\n\nHow it works…\n\nFirst, you initiate the recipe by creating the datashare within the\n\nproducer cluster and including the relevant databases and tables for\n\nthe consumer’s access. Aerward, you grant the consumer cluster’s\n\nAWS account access to the datashare, initially resulting in a pending\n\nauthorization status. Upon authorization, the status shis to\n\nAuthorized. At this point, the consumer perceives the data share’s\n\nstatus as Available (with action required within the Amazon\n\nRedshi console). To complete the process and activate the",
      "content_length": 1025,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 130,
      "content": "datashare in both the producer and consumer clusters, you\n\nassociate the datashare with the consumer cluster.\n\nis enables the consumer to create a database associated with the\n\ndatashare. is enables direct querying of the tables in the data\n\nshare using\n\nconsumer_database_name.schema_name.table_name.\n\nThere’s more…\n\nYou can extend the same datashare to multiple consumers by\n\nproviding access to each consumer cluster’s namespace if it’s within\n\nthe same AWS account, or to its AWS account if it belongs to a\n\ndiﬀerent account. You can achieve this by executing separate\n\nRedshi GRANT commands on the datashare.\n\nTo address security concerns, it’s possible to share your views\n\nwithout sharing the underlying tables. Sharing a view follows the\n\nsame procedure as sharing a table.\n\nSee also\n\nManaging existing datashares:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/manage-datashare-\n\nexisting-console.html\n\nConsumer cluster administrator actions:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/consumer-cluster-",
      "content_length": 1019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 131,
      "content": "admin.html\n\nAmazon Redshi database encryption:\n\nhttps://docs.aws.amazon.com/redshi/latest/mgmt/working-with-db-\n\nencryption.html\n\nSynchronizing Glue Data Catalog to a different account\n\nAs a centralized repository, a data lake allows you to store all your\n\nstructured and unstructured data at any scale without needing\n\nupfront data schema de\u0000nitions. AWS Lake Formation simpli\u0000es\n\ndata lake management on AWS by acting as a layer on top of\n\nexisting data storage services such as S3. It automatically catalogs\n\ndata, manages access, and integrates with various AWS services\n\nsuch as Glue and Athena. One of its key features is its integration\n\nwith AWS Glue. Lake formation in conjunction with AWS Resource\n\nAccess Manager (RAM) oﬀers a secure and simple method for the\n\nsharing of the AWS Glue Data Catalog across multiple AWS\n\naccounts. is promotes collaboration and data accessibility across\n\ndiﬀerent teams or organizations, allowing them to make the most of\n\ntheir data lakes and extract valuable insights from their shared\n\nresources. However, you need to carefully plan and test your data-\n\nsharing process. Before sharing the catalog, conduct a\n\ncomprehensive analysis of existing applications and work\u0000ows to",
      "content_length": 1221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 132,
      "content": "identify any dependencies on speci\u0000c catalog resources or\n\npermissions that may require adjustments. Existing applications\n\nmay slow down due to the increased read load from the shared\n\naccount. Ensure that permissions are set up correctly so that your\n\napplications can access the intended resources, whether they are\n\noriginal or shared, and avoid unauthorized exposure.\n\nTo ensure that your resources can handle the additional load,\n\nconsider conducting load testing and monitoring performance\n\nmetrics, adjusting resource allocation as necessary. Be aware that\n\ndelays in synchronization may lead to data consistency issues. To\n\nmitigate these risks, consider sharing the data within the same\n\nregion to reduce latency or select regions that have low latency\n\nbetween them.\n\nIn this recipe, we will learn how to share the Glue Data Catalog with\n\nanother AWS account using Lake Formation.\n\nGetting ready\n\nis recipe assumes the existence of two AWS accounts and a Glue\n\ndatabase within one of those accounts, which you intend to share\n\nwith the other. Ensure that you perform this recipe with a user or\n\nrole with administrative privileges in AWS Lake Formation by\n\nattaching the AWSLakeFormationDataAdmin managed policy to its\n\npermissions. To be able to use Lake Formation to control access to",
      "content_length": 1298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 133,
      "content": "the Glue Data Catalog, you need to switch to the Lake Formation\n\npermission model. If you already have Glue tables permission\n\nmanaged by IAM identities, list down these permissions, and once\n\nyou \u0000nish the outlined steps, grant the same permissions using the\n\nLake Formation:\n\n1. Revoke the super access from the IAMAllowedPrincipals principle, an\n\nautomatically generated entity containing IAM users and roles\n\nauthorized to access your Data Catalog resources in accordance with\n\nyour IAM policies.\n\nI. Navigate to Lake Formation and select Data Lake permissions\n\nunder Permissions in the navigation pane on the le.\n\nII. Search for the permission for the IAMAllowedPrincipals\n\nprincipal on your table. Select the permission and click on\n\nRevoke.",
      "content_length": 748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 134,
      "content": "Figure 2.8 – Revoking the IAMAllowedPrincipals permission\n\n2. Disable the default setting of controlling access to the new tables added\n\nto the databases only through IAM access control for the databases you\n\nshare:\n\nI. Go to Databases under Data Catalog in the le navigation pane\n\non the Lake Formation service.\n\nII. Select the database you will be sharing and click on Edit.\n\nDisable Use only IAM access control for new tables in this\n\ndatabase and Use only IAM access control for new databases\n\nunder Default permissions.\n\nIII. Repeat these steps for all the databases you will be sharing.",
      "content_length": 593,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 135,
      "content": "Figure 2.9 – Disabling Data Catalog’s default settings\n\n3. Stop new tables created in the Data Catalog from having a default super\n\npermission to IAMAllowedPrincipals:\n\nI. Choose Data Catalog settings from Administration in the Lake\n\nFormation navigation pane.\n\nII. Clear both Use only IAM access control for new databases and\n\nUse only IAM access control for new tables in this database\n\nunder the Default permissions for newly created databases and\n\ntables section.\n\nIII. Click on Save.\n\n4. Register the S3 location for the Glue database or tables that you will\n\nshare with Lake Formation:\n\nI. In the Lake Formation navigation pane, go to Data Lake\n\nlocations under Register and Ingest.\n\nII. Choose Register location, and then choose Browse to select an\n\nAmazon S3 path for the database or tables you intend to share.",
      "content_length": 819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 136,
      "content": "III. Repeat these steps for the diﬀerent S3 locations you have for the\n\ndatabase or tables you intend to share.\n\nHow to do it…\n\nLet’s consider that the account hosting the Glue database is referred\n\nto as Account A, and the account with which we intend to share the\n\ndatabase is identi\u0000ed as Account B.\n\nIn Account A, follow these steps:\n\n1. Log in to the AWS console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the AWS Glue service.\n\n2. From the navigation pane on the le, go to Catalog settings under Data\n\nCatalog service.\n\n3. Add the following policy statement to Permissions and save it (make\n\nsure to replace aws_region and aws_account_id with your\n\nspeci\u0000c values):\n\n{ \"Version\" : \"2012-10-17\", \"Statement\" : [ { \"Effect\" : \"Allow\", \"Principal\" : { \"Service\" : \"ram.amazonaws.com\" }, \"Action\" : \"glue:ShareResource\", \"Resource\" : [ \"arn:aws:glue:<aws_region>:",
      "content_length": 915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 137,
      "content": "<aws_account_id>:table/*\", \"arn:aws:glue:<aws_region>: <aws_account_id>:database/*\", \"arn:aws:glue:<aws_region>: <aws_account_id>:catalog\" ] } ] }\n\n4. Navigate to the Lake Formation service. From the navigation pane on\n\nthe le, choose Data lake permissions under Permissions.\n\n5. In Data permissions, click on Grant.\n\n6. For principles, choose External accounts, then type Account B’s ID and\n\npress Enter.\n\n7. Under LF-Tags or catalog resources, choose Named Data Catalog\n\nresources. Choose all the databases you want to share under Databases:",
      "content_length": 544,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 138,
      "content": "Figure 2.10 – Deﬁning the resources to be shared\n\nI. To grant access to the tables within those databases, choose the list of\n\ntables under Tables. You can optionally add Data \u0000lters. In the Table\n\npermissions section, under Table permissions, choose Super to grant all\n\npermissions or choose speci\u0000c permissions from the list. You can\n\noptionally choose Grantable permissions, which will allow Account B to\n\ngrant permissions to the shared resources.\n\nII. To grant access to the database itself, do not select any tables. In the\n\nDatabase permissions section, choosing the permissions is like setting\n\nthe table permissions – you have to choose the level of permissions and\n\noptionally grantable permissions.",
      "content_length": 709,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 139,
      "content": "III. Click on Grant.\n\nFigure 2.11 – Deﬁning the table permission to be granted\n\nIn Account B, follow these steps:\n\n1. Log in to the AWS console and navigate to the AWS RAM service.\n\n2. In the le navigation pane, choose Resource shares under Shared with\n\nme.\n\n3. You should see a resource with a pending status from Account A. Click\n\non it and choose Accept resource share.",
      "content_length": 373,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 140,
      "content": "Figure 2.12 – Accepting a resource share\n\nYou should be able to see the database and tables in Account B lake\n\nformation as if they were created in Account A.\n\nHow it works…\n\nTo begin the sharing process, you created a resource share\n\nspecifying the account to share with and the level of permissions\n\nthat will be granted to the recipient account. is triggers the\n\nsharing work\u0000ow and sends a sharing request to the recipient\n\naccount. Next, in the recipient account, you accepted the invitation\n\nfor the shared Glue databases. e recipient account can access the\n\nshared Glue database using their own Lake Formation console.\n\nThere’s more…\n\nTo be able to query the shared catalog tables in Account B, you have\n\nto create resource links that point to shared databases and tables. If\n\nyou are sharing all tables within a database, you can create a\n\nresource link for the database, which will allow you to query all the\n\ntables within it. However, if only speci\u0000c tables from a database are\n\nshared, you must create resource links for each of those individual\n\ntables. Follow the next steps to create a resource link for the tables.\n\nIf you need to create a resource link for the database, you can follow",
      "content_length": 1204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 141,
      "content": "a similar process by navigating to the Databases section instead of\n\nTables.\n\n1. Navigate to Lake Formation with a user that has the glue:CreateTable\n\npermission.\n\n2. In the navigation pane, choose Tables, and then choose Create table.\n\n3. In the Table details section, choose Resource link.\n\n4. Give a name under Resource link name and then choose the local\n\ndatabase that the table will be added to. Choose the shared table that you\n\nare creating the link for. Finally, for the table’s region under Shared\n\ntable’s region, choose the same region of the shared table and click on\n\nCreate.",
      "content_length": 589,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 142,
      "content": "Figure 2.13 – Creating a table from a resource link\n\nSee also\n\nUpdate a resource share in AWS RAM:\n\nhttps://docs.aws.amazon.com/ram/latest/userguide/working-with-",
      "content_length": 162,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 143,
      "content": "sharing-update.html\n\nGranting permissions on a database or table shared with your account:\n\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/regranting-\n\nshared-resources.html\n\nEnforcing fine-grained permissions on S3 data sharing using Lake Formation\n\nAWS Lake Formation simpli\u0000es the process of setting up and\n\nmanaging a data lake by providing capabilities for data ingestion,\n\norganization, and access control. You can arrange your S3 data into\n\ntables within the Lake Formation Data Catalog and implement \u0000ne-\n\ngrained access control on them. is means you can enforce precise\n\naccess permissions at both the table and column levels, ensuring\n\nthat sensitive data is safeguarded. Fine-grained access control\n\nempowers you to exercise precise control over who can view or\n\nmodify speci\u0000c tables and columns within the Lake Formation Data\n\nCatalog. Consequently, you can provide varying levels of access to\n\ndiﬀerent tables or columns, aligning with the speci\u0000c requirements\n\nof diﬀerent users or roles.\n\nIn this recipe, we will use Lake Formation to establish a table for an\n\nS3 dataset and provide granular access controls for it.",
      "content_length": 1139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 144,
      "content": "Getting ready\n\nTo follow this recipe, you should have an S3 dataset that you want\n\nto grant access to, as well as an IAM role to grant access for. It’s also\n\npreferable that this role has access to Amazon Athena to query the\n\ntable.\n\ne recipe must be implemented with a user or role that has admin\n\naccess to Lake Formation.\n\nHow to do it…\n\n1. Create a table for your S3 location:\n\nI. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?\n\nnc2=h_ct&src=header-signin) and navigate to the Lake\n\nFormation service.\n\nII. In the Lake Formation console, select Databases under Data\n\nCatalog from the navigation pane on the le.\n\nIII. Click on Create database.\n\nIV. Give a name and description for the database under the\n\nrespective \u0000elds and optionally add an S3 location if all the\n\ntables in your database will fall into a common location.",
      "content_length": 867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 145,
      "content": "Figure 2.14 – Creating a Lake Formation database\n\nV. Click on Create database.\n\nVI. To create the table automatically using a crawler, go to Crawlers under\n\nData catalog from the navigation pane on the le.\n\nVII. You will be directed to the Glue crawler; click on Create crawler.\n\nVIII. Give a name and description for your crawler and click on Next.",
      "content_length": 350,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 146,
      "content": "IX. Choose Not yet for Is your data already mapped to Glue tables? under\n\nthe Data source con\u0000guration.\n\nX. Under Data sources, click on Add a data source. For the location of S3\n\ndata, choose In this account and add the S3 path. Click on Add an S3\n\nlocation.",
      "content_length": 259,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 148,
      "content": "Figure 2.15 – Adding an S3 data source\n\nXI. Optionally, you can add a classi\u0000er for your data under the Custom\n\nclassi\u0000ers section.\n\nXII. Click on Next.\n\nXIII. Click on Create new IAM role, give a name to the role, and click on\n\nCreate | Next.\n\nXIV. Under Target database, choose the database you have created.\n\nXV. Optionally, provide a Table name pre\u0000x.\n\nXVI. You can schedule your crawler or keep it on demand.\n\nXVII. Click on Next and then click on Create crawler.\n\nXVIII. Now click on Run crawler.\n\nXIX. Once the run is done, you should see that one table was\n\nchanged:\n\nFigure 2.16 – Crawler completed a run\n\nXX. You can view the table by going to Lake Formation and clicking\n\non Tables under Data catalog from the navigation pane on the le.\n\n2. Create an LF-Tag for data classi\u0000cation:",
      "content_length": 793,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 149,
      "content": "I. Navigate to LF-Tags and permissions under Permissions in the\n\nle navigation pane and click on Add LF-Tag.\n\nII. Give a key and list of comma-separated values or a single value\n\nto the tag, click on Add, and then click on Add LF-Tag.\n\nFigure 2.17 – Adding an LF-Tag\n\n3. Assign the tag to your table:\n\nI. Navigate to Tables under Data Catalog in the le navigation\n\npane and open your table.\n\nII. Under the Schema section, click on Edit schema.\n\nIII. Select the columns you want to give access to, click on Edit LF-\n\nTags, then click on Assign new LF-Tag. Choose the LF-Tag you\n\ncreated under Assigned keys, select one of the values of the tag,",
      "content_length": 645,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 150,
      "content": "and click on Save. You can assign diﬀerent values to the tag for\n\nthe remaining columns in your table.\n\nFigure 2.18 – Assigning tags to columns\n\nIV. Click on Save as new version.\n\n4. Give permission to the IAM role:\n\nI. Navigate to Data lake permissions under Permissions in the\n\nnavigation pane on the le.\n\nII. Click on Grant.\n\nIII. Under the Principals section, choose IAM users and roles and\n\nchoose the IAM users and roles you want to give access to.\n\nIV. Under LF-Tags or catalog resources, choose Resources matched\n\nby LF-Tags (recommended). Choose the key to the tag you",
      "content_length": 578,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 151,
      "content": "created and the values that you are allowing this user access to.\n\nFigure 2.19 – Deﬁning the resource to set up access for\n\nV. Under Table permissions, choose Select and click on Grant.",
      "content_length": 185,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 152,
      "content": "Figure 2.20 – Deﬁning table permissions\n\n5. Revoke IAMAllowedPrincipals access:\n\nI. Navigate to Lake Formation and select Data lake permissions\n\nunder Permissions in the navigation pane on the le.\n\nII. Search for the permission for IAMAllowedPrincipals principle\n\non your table.\n\nIII. Select the permission and click on Revoke.",
      "content_length": 328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 153,
      "content": "Figure 2.21 – Revoking IAMAllowedPrincipals access\n\nHow it works…\n\nWe initiated this recipe by registering our S3 location with the data\n\nlake, followed by the creation of a database in AWS Lake Formation\n\nto host our table. To automate the table creation process, we\n\nemployed AWS Glue Crawler, con\u0000guring it to scan a designated S3\n\nlocation. e information gathered by the crawler was utilized to\n\nde\u0000ne the table’s structure, and added to the Lake Formation Data\n\nCatalog, which serves as a logical representation of the data.\n\nMoving forward, we created an LF-Tag for tagging speci\u0000c columns\n\nwith prede\u0000ned values. Subsequently, we granted select access to a\n\nparticular IAM user or role based on a speci\u0000c tag value, providing\n\naccess to a de\u0000ned set of columns. To align with these access",
      "content_length": 796,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 154,
      "content": "permissions, it was essential to revoke super access from the\n\nIAMAllowedPrincipals principle, an automatically generated entity\n\nthat includes IAM users and roles with access to Data Catalog\n\nresources as per IAM policies. With these access controls, when\n\nusers attempt to access the table, Lake Formation will grant\n\ntemporary credentials, granting them access to the speci\u0000ed data.\n\nThere’s more…\n\nYou can choose to set up tags at the database level, and all\n\nunderlying tables will inherit these same tags. Similarly, tags can be\n\nde\u0000ned at the table level, and all associated columns will also inherit\n\nthese tags. Additionally, when granting access to users, you have the\n\n\u0000exibility to utilize a combination of various tags. Leveraging LF-\n\nTags provides an eﬃcient means of managing permissions at scale.\n\nAlternatively, if you prefer using named resources rather than LF-\n\nTags-based resources, you can implement data \u0000lters on your tables.\n\nis allows you to precisely specify the rows and columns that can\n\nbe accessed following the outlined steps:\n\n1. Navigate to Data \u0000lters under Data Catalog in the le navigation pane\n\nand click on Create new \u0000lter.\n\n2. Give a name for your data \u0000lter and choose the target database and table\n\nto which the \u0000lter will be applied.",
      "content_length": 1280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 155,
      "content": "3. Under Column-level access, choose Include columns, then list the\n\ncolumns you want to give access to. Alternatively, you can choose to\n\nexclude columns and list the columns you don’t want to give access to.\n\n4. Under Row-level access, choose Filter rows, and then write down your\n\ncondition the same way as you would add it in a where clause in SQL\n\nquery and click on Create data \u0000lter.",
      "content_length": 390,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 156,
      "content": "Figure 2.22 – Deﬁning data ﬁlter constraints",
      "content_length": 44,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 157,
      "content": "Now you can use this \u0000lter when you de\u0000ne the table you are giving\n\naccess to under Named Data Catalog resource.\n\nSee also\n\nData \u0000ltering and cell-level security in Lake Formation:\n\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/data-\n\n\u0000ltering.html\n\nViewing the resources that an LF-Tag is assigned to:\n\nhttps://docs.aws.amazon.com/lake-formation/latest/dg/TBAC-view-tag-\n\nresources.html\n\nSharing your S3 data temporarily using a presigned URL\n\nS3 presigned URLs provide a secure and controlled way to grant\n\ntime-limited access to S3 objects. You can specify an expiration time\n\nwhen generating the URL, aer which the URL becomes invalid.\n\nPresigned URLs can have speci\u0000c permissions associated with\n\nthem; you can generate URLs that allow only speci\u0000c actions (such\n\nas read, write, or both) on individual objects. is provides granular\n\ncontrol over which operations can be performed on the object while\n\nminimizing the exposure of your AWS credentials, as you can avoid\n\nthe need to embed AWS access credentials directly into your\n\napplication or to share your AWS access keys.",
      "content_length": 1089,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 158,
      "content": "In this recipe, we will learn how to create a presigned URL to\n\ndownload an S3 object.\n\nGetting ready\n\nis recipe assumes that you have an S3 bucket with an object that\n\nyou will be creating a presigned URL for. Also, you need to perform\n\nthese steps using a user who has access to download the object as\n\nthe URL will inherit the same credentials.\n\nHow to do it…\n\n1. Log in to the AWS console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to S3.\n\n2. Navigate to the bucket that has the object you want to create a presigned\n\nURL for. Select the object to be shared from the Objects list.\n\n3. From the Object actions menu, choose Share with a presigned URL.\n\n4. Specify the Time interval until the presigned URL expires value and\n\nthen Number of hours or Number of minutes for the URL to be valid.\n\ne interval must be up to seven hours only; however, if you are doing\n\nthe setup through the CLI, you can set the time interval up to 12 hours.\n\n5. Choose Create presigned URL.",
      "content_length": 1018,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 159,
      "content": "Figure 2.23 – A presigned URL’s expiration time\n\n6. You will get con\u0000rmation that the URL was created and it will\n\nautomatically be copied to your clipboard. You can copy it again using\n\nthe Copy the presigned URL button.\n\nHow it works…",
      "content_length": 236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 160,
      "content": "By clicking on Create presigned URL, you have initiated the process\n\nof creating a time-limited URL that allows temporary access to the\n\nselected object. You can share the presigned URL with the intended\n\nusers, applications, or services. ey can use this URL to access the\n\nS3 object for the duration speci\u0000ed in the expiration time you set.\n\nThere’s more…\n\nAnyone in possession of the generated presigned URL will have the\n\ncapability to download your S3 object. However, you can control\n\nwho can utilize this URL for downloading by limiting access based\n\non IP address ranges. To implement this access restriction,\n\npresigned URLs inherit the access permissions of the user or role\n\nthat created them. erefore, you can achieve this by attaching an\n\nIAM policy to the user or role responsible for generating the\n\npresigned URL, restricting its access to a speci\u0000c IP or IP range.\n\nConsequently, the presigned URL will only be accessible within that\n\nde\u0000ned IP range.\n\nHere’s an illustrative IAM policy. Ensure that you substitute IP-\n\naddress and s3-bucket-name with the actual IP address you\n\nintend to use and the S3 bucket containing the \u0000les to be shared,\n\nrespectively. Also, ensure that there are no con\u0000icting policies in\n\nplace that grant broader access:",
      "content_length": 1265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 161,
      "content": "{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"IPAllow\", \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": \"arn:aws:s3:::<s3-bucket- name>/*\", \"Condition\": { \"IpAddress\": { \"aws:SourceIp\": \"IP-address\" } } } ] }\n\nSee also\n\nUploading objects with presigned URLs:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/PresignedUrl\n\nUploadObject.html\n\nReal-time sharing of S3 data\n\nReal-time sharing of S3 data ensures immediate responsiveness to\n\nchanges in data, facilitating seamless communication between\n\nvarious components of a system. By setting up an S3 bucket to\n\ninvoke a Lambda function upon the occurrence of any new event,",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 162,
      "content": "performing the required processing, and publishing the data\n\nthrough noti\u0000cations to an SNS topic, this method provides an\n\neﬀective and scalable means for broadcasting events to multiple\n\nsubscribers. It oﬀers a dynamic and responsive approach to sharing\n\ninformation in real time across diﬀerent systems or applications.\n\nis approach proves particularly advantageous in scenarios where\n\nthe timely distribution of information is crucial for maintaining up-\n\nto-date and synchronized systems.\n\nIn this recipe, we will learn the process of triggering a Lambda\n\nfunction when new \u0000les are created in an S3 bucket, subsequently\n\nprocessing the \u0000les and broadcasting them as SNS noti\u0000cations.\n\nGetting ready\n\nFor this recipe, you need to have an S3 bucket whose content will be\n\nshared.\n\nHow to do it…\n\n1. Create an SNS topic:\n\nI. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?\n\nnc2=h_ct&src=header-signin), navigate to SNS service, and\n\nclick on Create topic.",
      "content_length": 995,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 163,
      "content": "II. Under Details, choose Standard type, give a name and\n\noptionally a display name for the topic, and click on Create\n\ntopic.\n\nIII. In the Subscription tab, click on Create subscription.\n\nIV. Select Email under Protocol, provide your email under\n\nEndpoint, and click on Create subscription.\n\nFigure 2.24 – Setting up an email subscription to SNS",
      "content_length": 346,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 164,
      "content": "You should receive an email asking you to con\u0000rm the subscription.\n\n2. Create an IAM role for the Lambda function:\n\nI. Create a new IAM policy using the following permissions\n\n(make sure to replace sns-topic-arn and s3-bucket-\n\narn with your speci\u0000c values):\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"sns:Publish\" ], \"Resource\": [ \"<sns-topic-arn>\", \"<s3-bucket-arn>/*\" ] } ] }\n\nII. Create a role for Lambda service, attaching the policy from the\n\nprevious step and the AWSLambdaBasicExecutionRole\n\nmanaged policy.\n\n3. Create a Lambda function:\n\nI. Navigate to the Lambda service.",
      "content_length": 655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 165,
      "content": "II. Click on Create function, select Author from scratch, and write\n\na function name. Under Runtime, choose Python 3.11, and\n\nunder Permissions, expand Change default execution role,\n\nselect Use an existing role, and choose the role you created in\n\nthe previous step. Finally, click on Create function.\n\nFigure 2.25 – Creating a Python Lambda function\n\n4. Create an S3 event:\n\nI. Navigate to the S3 service.\n\nII. Open the S3 bucket whose \u0000les you need to publish and go to\n\nthe Properties tab.\n\nIII. Under the Event noti\u0000cation section, click on Create event\n\nnoti\u0000cation.\n\nIV. Give an event name and optionally add a pre\u0000x (for example, a\n\nfolder name) for the \u0000les to be shared.",
      "content_length": 680,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 166,
      "content": "V. Under Event types, select All object create events under Object\n\ncreation.\n\nVI. Under the Destination section, choose Lambda function, select\n\nthe Lambda function created in the previous step, and click on\n\nSave changes.\n\nFigure 2.26 – Conﬁguring the S3 event for Lambda\n\n5. Update the Lambda code:\n\nI. Navigate back to your Lambda function; you should see an S3\n\ntrigger added to your function.",
      "content_length": 398,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 167,
      "content": "II. In the Code tab, update the code with the following script.\n\nMake sure to update the TopicArn and Subject \u0000elds with\n\nyour own values:\n\nimport json import boto3 import logging logger = logging.getLogger() logger.setLevel(logging.INFO) def publish_to_sns(message): try: sns_client = boto3.client('sns') # add any processing required on the file or extract certain fields before publishing the message. sns_client.publish( TopicArn = 'yourSNSTopicArn', Message = message, Subject = 'emailSubject' ) logging.info('Published SNS message successfully') except ClientError: logging.info('Failed to publish SNS due to: {}'.format(str(ClientError))) return False return True def lambda_handler(event, context): # retrieve S3 file name s3_event = event['Records'][0]['s3'] object_key = s3_event['object'] ['key']",
      "content_length": 807,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 168,
      "content": "bucket_name = s3_event['bucket'] ['name'] s3_client = boto3.client('s3') # retrieve file from S3 by its object key response = s3_client.get_object(Bucket=bucket_name, Key=object_key) file_content = response['Body'].read().decode('utf-8') # publish S3 file content to SNS publish_to_sns(file_content) return { 'statusCode': 200 }\n\nIII. Click on Deploy.\n\n6. Test: Test the process by uploading a \u0000le to your S3 bucket. is should\n\ntrigger your Lambda function and you should receive an email with the\n\nnoti\u0000cation from SNS.\n\nHow it works…\n\ne IAM role we established provides Lambda with the necessary\n\npermissions to access and read S3 \u0000les within our designated\n\nbucket. It also allows Lambda to publish messages to our SNS topic\n\nand generate log \u0000les in CloudWatch. An S3 event has been\n\ncon\u0000gured to trigger the Lambda function whenever an object is\n\nadded to the bucket, covering various methods such as object\n\nputting or copying. Upon triggering, the Lambda function retrieves",
      "content_length": 982,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 169,
      "content": "the S3 \u0000le’s location, reads its content, processes the information,\n\nand subsequently publishes it to the speci\u0000ed SNS topic. All\n\nsubscribers connected to the SNS topic are then noti\u0000ed of the\n\npublished message.\n\nThere’s more…\n\nYou have the \u0000exibility to incorporate multiple subscribers into\n\nyour SNS topic, including AWS services within your AWS account,\n\nor extending to various AWS accounts. Moreover, if your intention\n\nis to directly share S3 events with consumers, you have the option to\n\ncon\u0000gure the S3 event noti\u0000cation destination to be an SNS topic.\n\nis allows for the subscription of multiple consumers to the topic,\n\nproviding them with the choice to utilize it as is or integrate it into\n\nAWS services for customized processing.\n\nSee also\n\nEvent noti\u0000cation types and destinations:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/noti\u0000cation-\n\nhow-to-event-types-and-destinations.html\n\nSharing read-only access to your CloudWatch data with another AWS account",
      "content_length": 986,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 170,
      "content": "CloudWatch cross-account sharing in AWS is valuable for\n\ncentralizing monitoring and management. It enables organizations\n\nto share monitoring data, including alarms, dashboards, and logs,\n\nacross AWS accounts while maintaining resource isolation. is\n\nfacilitates consolidated reporting, collaboration, troubleshooting,\n\nand cost management. Cross-account sharing allows for customized\n\naccess control and \u0000ne-grained permissions, and is scalable for\n\ngrowing organizations. It simpli\u0000es the sharing of critical\n\nmonitoring data, enhancing operational eﬃciency and visibility\n\nwithout compromising security.\n\nIn this recipe, we will learn how to share read-only access to\n\nCloudWatch data with another AWS account.\n\nGetting ready\n\nTo follow this recipe, you need to have two AWS accounts, one of\n\nwhich must have the CloudWatch log group to be shared. e other\n\naccount will share it and use it for monitoring.\n\nHow to do it…\n\nIn the source AWS account, follow these steps:\n\n1. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the CloudWatch service.",
      "content_length": 1131,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 171,
      "content": "2. From the navigation pane on the le, choose Settings.\n\n3. Go to the Enable account switching section then select Con\u0000gure for the\n\nShare your CloudWatch data option.\n\nFigure 2.27 – Enabling the sharing of CloudWatch data\n\n4. In the Sharing section, click on Add account, then type the monitoring\n\naccount ID and hit Enter.\n\nFigure 2.28 – Sharing to a speciﬁc AWS account\n\n5. For Permissions, choose Provide read-only access to your CloudWatch\n\nmetrics, dashboards, and alarms and check both the Include\n\nCloudWatch automatic dashboards and Include X-Ray read-only access\n\nfor ServiceLens checkboxes.",
      "content_length": 602,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 172,
      "content": "Figure 2.29 – Conﬁguring permissions\n\n6. Click on Launch CloudFormation template from the Create\n\nCloudFormation stack section.\n\n7. Type Confirm on the con\u0000rmation screen and click on Launch\n\nTemplate.\n\n8. On the Quick create stack page, select I acknowledge that AWS\n\nCloudFormation might create IAM resources with custom names. en\n\nclick on Create stack.\n\nIn the destination AWS account, follow these steps:\n\n1. Log in to the AWS Management Console and navigate to the\n\nCloudWatch service.\n\n2. From the navigation pane on the le, choose Settings.\n\n3. Go to the Enable account switching section, then select Con\u0000gure for\n\nthe View cross-account cross-region option.",
      "content_length": 668,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 173,
      "content": "4. In the Enable account selector section, select Show selector in the\n\nconsole.\n\n5. Select Custom account selector, and then enter the owner account ID\n\nand a label to identify it.\n\nFigure 2.30 – Conﬁguring the source AWS account\n\n6. Click on Enable.\n\nHow it works…",
      "content_length": 266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 174,
      "content": "In the source AWS account, we have granted read-only access to the\n\ndestination AWS account (monitoring account), enabling it to\n\ncreate dashboards using widgets from the source account, view\n\nautomatically generated dashboards, and access X-Ray data with\n\nread-only permissions for ServiceLens in the source account. e\n\nLake Formation template will create an IAM role with policies that\n\nprovide the speci\u0000ed access levels and establish a trust relationship\n\nwith the destination AWS account.\n\nIn the destination AWS account, we have enabled the ability to view\n\nshared CloudWatch data from the source AWS account. is access\n\nis con\u0000gured through a custom account selector, which oﬀers a\n\ndrop-down list of accounts to choose from based on the labels\n\nassigned to these accounts. is streamlined access management\n\nensures a seamless experience when logging into the destination\n\nAWS account.\n\nThere’s more…\n\nIn the destination AWS account, upon accessing CloudWatch, you\n\nwill encounter a View data feature that presents a drop-down menu.\n\nis menu facilitates seamless switching between viewing data from\n\nyour own account and the source account. By selecting the source\n\naccount based on its designated label, you can proceed to create",
      "content_length": 1241,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 175,
      "content": "dashboards or set up alerts using metrics generated in the source\n\naccount, just as you would for your own account.\n\nFigure 2.31 – Selecting the account of CloudWatch data\n\nSee also\n\nCross-account cross-region dashboards:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cl\n\noudwatch_xaxr_dashboard.html\n\nOceanofPDF.com",
      "content_length": 334,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 176,
      "content": "3 Ingesting and Transforming Your Data with AWS Glue\n\nIn data engineering, implementing integration between systems to\n\nextract, transform, and load (ETL) data is frequently what\n\nconsumes the most time and cost. AWS Glue is a serverless data\n\nintegration service that provides diﬀerent engines and tools to build\n\nETL jobs in a simple and scalable way, paying for what you use.\n\nGlue is comprised of many components and features to serve many\n\nkinds of data products and users, including a Hive-compatible\n\nmetastore and multiple engines for diﬀerent needs, from single-\n\nnode Glue Python shell jobs to distributed clusters that auto-scale\n\nusing Glue for Spark, and the latest addition: Glue for Ray. Each of\n\nthose engines has connectors for common data storage systems.\n\nGlue also oﬀers on-demand clusters via interactive sessions, which\n\ncan be used for interactive development and analysis via Jupyter\n\nnotebooks (either provided by Glue or your own). Finally, Glue\n\nStudio oﬀers a visual environment where you can create visual jobs\n\nand build ETL pipelines with little or no coding.\n\nis chapter includes the following recipes:\n\nCreating ETL jobs visually using AWS Glue Studio",
      "content_length": 1185,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 177,
      "content": "Parameterizing jobs to make them more \u0000exible and reusable\n\nHandling job failures and reruns for partial results\n\nProcessing data incrementally using bookmarks and bounded execution\n\nHandling a high quantity of small \u0000les in your job\n\nReusing libraries in your Glue job\n\nUsing data lake formats to store your data\n\nOptimizing your catalog data retrieval using pushdown \u0000lters and\n\nindexes\n\nRunning pandas code using AWS Glue for Ray\n\nTechnical requirements\n\nMany recipes in this chapter require you to have a bash shell or\n\nequivalent available with the AWS CLI installed with access to AWS.\n\nCheck the AWS documentation installation guide:\n\nhttps://docs.aws.amazon.com/cli/latest/userguide/getting-started-\n\ninstall.html.\n\nIf using Microso Windows, you can enable Windows Subsystem\n\nfor Linux (WSL) to get a bash shell. e instructions will also\n\nassume you have con\u0000gured default credentials and the default\n\nregion as the one you intend to use, using aws configure or an\n\nAWS CLI pro\u0000le.",
      "content_length": 991,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 178,
      "content": "e easier way to experience the recipes is using a test account, on\n\nwhich you can use an admin user to access the console and the\n\ncommand line. Otherwise, you’ll have to add the required\n\npermissions to create and use Glue components.\n\nA Glue job requires a Simple Storage Service (S3) bucket to store\n\nscripts, temporary \u0000les, and, potentially, data; you will create a\n\nbucket for this purpose. None of the recipes use or generate large\n\namounts of data that would incur high usage costs in the short\n\nterm; it is up to you to clean up the \u0000les or even delete the bucket\n\nonce you no longer want to keep the example data.\n\nDe\u0000ne some variables in the shell, making sure the region\n\ncon\u0000gured on the AWS CLI is the region you intend to use later on\n\nthe console:\n\nACCOUNT_ID=\"$(aws sts get-caller-identity --query \\ 'Account' --output text)\" AWS_REGION=\"$(aws configure get region)\" GLUE_BUCKET =\"glue-recipes-$ACCOUNT_ID\" GLUE_ROLE=AWSGlueServiceRole-Recipe GLUE_ROLE_ARN=arn:aws:iam::${ACCOUNT_ID}:role/$GLU E_ROLE\n\ne variables will be lost once you close the shell, but you can rerun\n\nthe previous code de\u0000nition block if a recipe requires one or more\n\nof them (this will be speci\u0000ed in the Getting ready section of each\n\nrecipe that requires it).",
      "content_length": 1253,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 179,
      "content": "Create a bucket in the region using the default con\u0000guration:\n\naws s3api create-bucket --create-bucket- configuration \\ LocationConstraint=$AWS_REGION --bucket $GLUE_BUCKET\n\ne default permissions block public access but allow\n\nreading/writing to roles in the same account, which have\n\npermission to use S3.\n\nAer each recipe, you can wipe the bucket objects if you no longer\n\nneed them.\n\nIn addition, all Glue jobs require a role that Glue can assume at\n\nruntime. By following the convention of naming it starting with\n\nAWSGlueServiceRole, it doesn’t require an explicit\n\niam:passRole permission. Create it using the following command\n\nlines:\n\naws iam create-role --role-name $GLUE_ROLE -- assume-\\ role-policy-document '{\"Version\": \"2012-10- 17\",\"Statement\": [{ \"Effect\": \"Allow\", \"Principal\": {\"Service\": \"glue.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}]}' aws iam attach-role-policy --policy-arn \\ arn:aws:iam::aws:policy/service- role/AWSGlueServiceRole \\ --role-name $GLUE_ROLE",
      "content_length": 985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 180,
      "content": "aws iam put-role-policy --role-name $GLUE_ROLE \\ --policy-name S3Access --policy-document '{\"Version\": \"2012-10-17\", \"Statement\": [{\"Effect\": \"Allow\", \"Action\": [\"s3:*\"], \"Resource\": [\"arn:aws:s3:::'$GLUE_BUCKET'\", \"arn:aws:s3:::'$GLUE_BUCKET'/*\", \"arn:aws:s3:::aws-glue-*\",\"arn:aws:s3:::aws-glue- */*\"]}]}'\n\ne wildcard in the bucket name allows it to accommodate a\n\nvariable bucket name.\n\nYou can \u0000nd the script and code \u0000les on GitHub:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter03.\n\nCreating ETL jobs visually using AWS Glue Studio\n\nTypical ETL tasks consist of moving data from one data storage to\n\nanother, with some simple transformations in the process. For such\n\ncases, building a job using a visual data diagram allows users\n\nwithout coding skills to develop such a pipeline, using their\n\nknowledge of the business and the data. ese kinds of jobs are also\n\neasier to maintain and update, thus reducing the total cost of\n\nownership (TCO).\n\nOne of the multiple types of jobs that AWS Glue Studio allows for\n\ncreating is a visual job. is allows the user to de\u0000ne the pipeline as",
      "content_length": 1134,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 181,
      "content": "a graph of nodes, and then the code is generated automatically so\n\nthat it runs like a regular script job.\n\nGetting ready\n\nCreate a bucket and a role as indicated in the Technical requirements\n\nsection.\n\nTo follow this recipe, you require a CSV \u0000le with headers and some\n\ndata, which can be your own data or the sales_sample.csv \u0000le\n\nprovided on the code repository:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/blob/main/Chapter03/Recipe1/sales_sample.csv.\n\nUpload the sample CSV \u0000le to the bucket created and copy the S3\n\nuploaded \u0000le URL; you will need it in step 2 of the How to do it…\n\nsection of this recipe. Here’s an example:\n\naws s3 cp Recipe1/sales_sample.csv s3://$GLUE_BUCKET/\n\nHow to do it…\n\n1. In the AWS console, navigate to Glue, select Visual ETL in the le menu,\n\nand create a job using the button with the same name. It will open Glue\n\nStudio where you can start building your visual job. As you add\n\ncomponents, the one selected automatically becomes the parent of the",
      "content_length": 1016,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 182,
      "content": "one added. You can also set the parent manually by selecting it and\n\nchoosing a parent in the properties.\n\n2. Add an S3 source node. en, on the right-hand side, navigate to Data\n\nsource properties - S3. In the S3 URL \u0000eld, enter the path to the input\n\nCSV \u0000le you loaded in the Getting ready section (Note: Normally, you\n\nwould enter an input directory so that it picks up many \u0000les recursively.):\n\nFigure 3.1 – New visual job prompting to add the source node\n\n3. Select CSV for Data format and then choose Infer schema.\n\n4. Change the tab on the bottom panel to Output schema. e CSV\n\nheaders have been used to de\u0000ne the \u0000elds; however, all of them are of\n\ntype String.",
      "content_length": 671,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 183,
      "content": "5. Using the top le + button, add a Change Schema transform as a child of\n\nthe S3 source. On the Transform tab, change the type of some of the\n\ncolumns as in the example (the conversion has to be valid; otherwise, the\n\ndata will be lost):\n\nFigure 3.2 – Change Schema sample conﬁguration\n\n6. Add an S3 target node (in the Add node menu, select the Target tab) as a\n\nchild of the Change Schema node. Select parquet for Format and enter a\n\ntarget S3 location under the bucket created for recipes; for instance,\n\ns3:// glue-recipes-<account id>/visual_recipe/table/.\n\n7. Select the Create a table option in Data Catalog, and on subsequent runs,\n\nupdate the schema and add new partitions, choose a database, and enter\n\na table name of your choice as shown:",
      "content_length": 752,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 184,
      "content": "Figure 3.3 – S3 target node properties\n\n8. Edit the job name at the top, and in the Job details tab, select\n\nAWSGlueServiceRole-Recipe as the IAM Role value. Save and\n\nrun the job; you can monitor it on the Runs tab. It should only take\n\nabout a minute for such a small \u0000le.\n\n9. Navigate to Amazon Athena in the console and select the database you\n\nstored the table in. e table should be listed and using the action menu\n\nfor the table. Select Preview table, which will create a SQL SELECT\n\nstatement you can run to visualize the table content. If you haven’t\n\npreviously used Athena on this account and region, it will ask you to\n\nselect a temporary path S3 to store the query results.\n\nHow it works…",
      "content_length": 702,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 185,
      "content": "As you de\u0000ne your nodes and their con\u0000guration, the editor builds\n\nthe job pipeline with the components, from which it will generate\n\nthe code that the job uses at runtime. You can view the generated\n\ncode in the Script tab:\n\nFigure 3.4 – Script tab with the autogenerated code\n\nIf you export the job de\u0000nition (for instance, using the AWS CLI),\n\nyou will notice that the JSON produced is like a script Glue job, but\n\nit has a property that stores the diagram and the node properties:\n\ncodeGenConfigurationNodes. When it gets saved, it generates a\n\nPython script on the speci\u0000ed destination. Visual jobs only generate\n\nPySpark code, not Scala. If you edit the code in the Script tab, you\n\nget a warning that the job will become a script job, which means\n\nyou can no longer edit it visually.\n\nThere’s more…",
      "content_length": 805,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 186,
      "content": "Check the AWS Glue documentation for a list of visual\n\ncomponents, available at\n\nhttps://docs.aws.amazon.com/glue/latest/ug/edit-jobs-\n\ntransforms.html.\n\nData Preview is a key element of data pipeline development; it\n\nshows you how the sample data looks aer each node of the\n\npipeline has processed it. at way, you can gain con\u0000dence and\n\nmake sure you are handling it as intended. Bear in mind that\n\nbehind the scenes it uses a tiny interactive session Glue cluster,\n\nwhich has a cost. You can stop the preview and then disable the\n\nautomatic start if you are not going to use it (for instance, if you\n\ndon’t have data yet) to save on cost.\n\nIf you need to troubleshoot the job and Data Preview is not enough,\n\nyou can always copy the script code and use it in a separate script\n\njob to troubleshoot, comment out some parts, print partial results\n\nand schemas, and so on. at way, you can make code changes that\n\nhelp you troubleshoot without losing the visual job if you convert it.\n\nSee also\n\nYou can create your own visual components for Visual Studio. See how\n\nat https://docs.aws.amazon.com/glue/latest/ug/custom-visual-\n\ntransform.html.",
      "content_length": 1145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 187,
      "content": "e catalog source node can use parameters. See more about job\n\nparameters in general in the Parameterizing jobs to make them more\n\n\u0000exible and reusable recipe.\n\nVisual jobs can also bene\u0000t from Glue bookmarks; see how they work in\n\nthe Processing data incrementally using bookmarks and bounded\n\nexecution recipe.\n\nParameterizing jobs to make them more flexible and reusable\n\nA job without any parameters normally does the same task in each\n\nrun, with speci\u0000c data sources and destinations. Using parameters,\n\nyou can reuse the same job on diﬀerent data sources or destinations,\n\nboth to run recurring jobs on new data or to reuse the same logic\n\nfor diﬀerent purposes, such as data transformation or cleaning.\n\nFor instance, the same type of data comes from various sources but\n\nneeds the same processing in a centralized data store.\n\nGlue allows you to de\u0000ne your parameters for your own purposes,\n\nwhich you then can use in your script. You can set default values on\n\nthe job and then override them as needed for each run when\n\nstarting a job run manually using the console, the AWS CLI, or an\n\nAPI such as boto3 or the Java SDK.\n\nGetting ready",
      "content_length": 1146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 188,
      "content": "For this recipe, you need to follow the instructions in the Technical\n\nrequirements section at the beginning of the chapter to create a role\n\nfor Glue.\n\nHow to do it…\n\n1. Create a new job in the Glue console by selecting ETL jobs in the le\n\nmenu and then, using the Script Editor button, give it a name and select\n\nthe AWSGlueServiceRole-Recipe role in the Job details tab.\n\nLeave Spark as the engine and Python as the language (see the ere’s\n\nmore… section later to learn about how it could be done in Scala).\n\n2. First, we will de\u0000ne the parameter and assign it a default value. In the\n\nJob detail tab, scroll down and open the Advanced properties section,\n\nadd a new parameter, and enter --DATE as the key and TODAY as the\n\nvalue. is is a special placeholder value; otherwise, we expect on that\n\nparameter a date string in the format yyyy-mm-dd, such as 2000-\n\n01-01. e key name is arbitrary but needs to start with a double\n\nhyphen:\n\nFigure 3.5 – Job parameters job conﬁguration\n\n3. Replace the sample code provided with the following code block:",
      "content_length": 1054,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 189,
      "content": "import sys from datetime import datetime from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job args = getResolvedOptions(sys.argv, ['JOB_NAME', 'DATE']) data_date = args[\"DATE\"] date_format = '%Y-%m-%d' if data_date == 'TODAY': data_date = datetime.today().strftime(date_format) print(f\"Running job for date: '{data_date}'\")\n\n4. Run the job using the Run button and then switch to the Runs tab to see\n\nyour run progress. Select the job run and wait for it to \u0000nish. In the job\n\nrun details, use the Output logs link to view the output logs on Amazon\n\nCloudWatch. ere should be a message indicating that the job is using\n\nthe current date.\n\n5. Now, instead of using the Run button, select in the Job actions\n\ndropdown the Run with parameters option. Open the job parameters,\n\nenter a date in the format we have de\u0000ned for the parameter (yyyy-\n\nmm-dd), and run the job:",
      "content_length": 968,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 190,
      "content": "Figure 3.6 – Overriding the parameter for this run\n\nNotice the value you pass when running this way is used just for\n\nthis run and does not alter the default parameter value (TODAY) on\n\nthe job.\n\nIn the output log for this run, you should now see a message\n\nreferencing the date entered. If the parameter does not follow the\n\nformat, the job run will fail, and you will get a corresponding error.\n\nHow it works…\n\nIn the script you pasted in step 3 when creating the job, it uses the\n\nGlue getResolvedOptions API to extract the speci\u0000ed\n\nparameters from the command-line arguments Glue used to run\n\nthe script. You could use another library or your own code to parse\n\nthe parameters.\n\ne script then checks for the value of the DATE argument. For the\n\nspecial value of TODAY, it replaces this keyword with the current\n\nsystem date.\n\nIn step 5, you triggered the run overriding the DATE argument with\n\na speci\u0000c date instead of the default value. is is useful if you need\n\nto rerun a speci\u0000c date or you need to backload if the job hasn’t\n\nbeen running in a timely manner each day.",
      "content_length": 1080,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 191,
      "content": "Once the parameter value is stored in a variable in the script, you\n\ncan use it as needed. For instance, you could use it to build a\n\npushdown predicate when reading a table or an S3 path or the name\n\nof the table to write into.\n\nThere’s more…\n\nIf Scala is the language you use to script your Glue jobs, then the\n\ncode to extract the parameter is slightly diﬀerent in syntax but\n\nworks the same way. e main method receives a string array\n\n(named sysArgs in this example), which is then parsed by\n\ngetResolvedOptions:\n\nimport com.amazonaws.services.glue.util.GlueArgParser val args = GlueArgParser.getResolvedOptions(sysArgs, Seq(\"JOB_NAME\",\" DATE \").toArray) val region = args(\"DATE\")\n\nYou can trigger multiple instances of a job, each using diﬀerent\n\nparameters. In the job run history, you can review the speci\u0000c\n\nparameters that were used for each run.\n\nIf you need to trigger a job with multiple parameters, you might\n\nwant to trigger them in parallel since they are doing diﬀerent\n\nthings, such as handling diﬀerent data. e total run cost would be\n\nthe same, but you can get them all completed faster that way. To",
      "content_length": 1120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 192,
      "content": "allow that parallelism, make sure you have enough concurrent runs\n\nallowed on the Job details con\u0000guration:\n\nFigure 3.7 – Concurrency conﬁg in the Job details tab\n\nSee also\n\nIf you invoke a job by other mechanisms such as boto3 (for instance,\n\nfrom AWS Lambda) or the AWS CLI, you also have the option to use\n\nspeci\u0000c parameters that override the default for that run. Refer to the\n\nAPI documentation:\n\nhttps://docs.aws.amazon.com/cli/latest/reference/glue/start-\n\njob-run.html\n\nhttps://boto3.amazonaws.com/v1/documentation/api/latest/refe\n\nrence/services/glue/client/start_job_run.html\n\nHandling job failures and reruns for partial results\n\nWhen building ETL pipelines (with Glue or in general), it’s\n\nimportant to consider diﬀerent scenarios that could make the job\n\nfail and how to deal with it. Ideally, we want the recovery to be\n\nautomatic, at least for transient issues, but regardless of the recovery",
      "content_length": 908,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 193,
      "content": "method, the most important aspect is that the jobs don’t result in\n\npermanent data loss or duplication due to the issue. In traditional\n\ndatabases, this is solved using transactions, but in the case of big\n\ndata ETL, that is rarely an option or would cause too much\n\noverhead.\n\nIn this recipe, you will see how to deal with job failures and\n\nresulting partial results.\n\nGetting ready\n\nis recipe requires a bash shell with the AWS CLI installed and\n\ncon\u0000gured and the GLUE_ROLE_ARN and GLUE_BUCKET\n\nenvironment variables set, as indicated in the Technical\n\nrequirements section at the beginning of the chapter.\n\nHow to do it…\n\n1. Create a job script as a local \u0000le running the following multiline bash\n\ncommand. It will execute when you enter the line with just EOF:\n\ncat <<EOF > RetryRecipeJob.py from pyspark.context import SparkContext from pyspark.sql.functions import * from awsglue.context import GlueContext from awsglue.dynamicframe import DynamicFrame glueContext = GlueContext(SparkContext()) spark = glueContext.spark_session s3_output_path =",
      "content_length": 1053,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 194,
      "content": "\"s3://${GLUE_BUCKET}/retry_recipe\" # The first attempt is just an hexadecimal number # the retries have a suffix with the retry number is_retry = \"_attempt_\" in \\ spark.conf.get(\"spark.glue.JOB_RUN_ID\") df = spark.range(1 << 10, numPartitions=4) # Simulate the retry works ok if is_retry: df = df.withColumn(\"fail\", lit(False)) else: df = df.withColumn(\"fail\", expr( f\"case when id=10 then true else false end\")) # Introduce a failure when flagged fail_udf = udf(lambda fail: 1/0 if fail else fail) df = df.withColumn(\"fail\", fail_udf(df.fail)) failDf = DynamicFrame.fromDF(df, glueContext, \"\") glueContext.write_dynamic_frame.from_options( frame=failDf, connection_type='s3', format='csv', connection_options={\"path\": s3_output_path} ) EOF\n\n2. Upload the script to the S3 bucket and delete the local copy:\n\naws s3 cp RetryRecipeJob.py s3://$GLUE_BUCKET rm RetryRecipeJob.py\n\n3. Create a job, making sure to use \\ only at the lines indicated:",
      "content_length": 942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 195,
      "content": "aws glue create-job --name RetryRecipe --role \\ $GLUE_ROLE_ARN --number-of-workers 2 -- worker-type \\ \"G.1X\" --glue-version 4.0 --command '{\"Name\": \"glueetl\", \"ScriptLocation\": \"s3://'$GLUE_BUCKET'/RetryRecipeJob.py\"}' \\ --max-retries 1 --default-arguments \\ '{\"--job-language\":\"python\", \"--TempDir\": \"s3://'$GLUE_BUCKET'/tmp/\"}'\n\n4. Run the job:\n\naws glue start-job-run --job-name RetryRecipe\n\n5. Open the AWS console, navigate to Glue, and select ETL Jobs on the le\n\nmenu. On the table of jobs, it should list the RetryRecipe job (if not,\n\nmake sure you are in the same region where you are using the command\n\nline). Select the job name to view the details and then select the Runs\n\ntab. Refresh until you see two attempts complete, the \u0000rst one failing and\n\nthe second succeeding:\n\nFigure 3.8 – Runs tab showing both attempts\n\n6. Check the output folder by running the following in the command line:",
      "content_length": 903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 196,
      "content": "aws s3 ls s3://${GLUE_BUCKET}/retry_recipe/\n\n7. Examine the \u0000les listed. Notice there are two sets of four \u0000les; one of\n\nthem has one \u0000le that is much smaller. is is the set from the \u0000rst run\n\nthat produced incomplete results due to an error. e retry worked but\n\ndidn’t clean the previous run’s partial results. Let’s solve that.\n\n8. Back in the console, open the Script tab for the job. Near the end of the\n\nscript, locate the\n\nglueContext.write_dynamic_frame.from_options line.\n\nJust before it, add a new line with the following content:\n\nglueContext.purge_s3_path(s3_output_path, {\"retentionPeriod\": 0})\n\n9. Save the script change and run the job. Open the Runs tab again and\n\nrefresh to see the new job run, again with an error and a retry.\n\n10. Once the retry has succeeded, in the command line, again list the \u0000les as\n\ndone in step 6. Now, there is only one step of four \u0000les with similar sizes.\n\n11. Clean the \u0000les and the job if no longer needed:\n\naws glue delete-job --job-name RetryRecipe aws s3 rm --recursive \\ s3://${GLUE_BUCKET}/retry_recipe/\n\nHow it works…\n\nIn this recipe, you created a Glue job with one retry con\u0000gured,\n\nwhich means that if the job fails (regardless of the reason), then it",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 197,
      "content": "will immediately retry the same run, using the same con\u0000guration\n\nand parameters. In most cases, the job fails because an uncaught\n\nexception was raised from the script.\n\ne script creates a dataset of 1,024 rows divided into four\n\npartitions (Spark memory partitions, not to be confused with table\n\npartitions). en, it checks the job ID to detect whether this is the\n\n\u0000rst run or a retry. e retry uses the name job ID but appends the\n\nnumber of retry attempts starting with 1. If it’s the \u0000rst run, it marks\n\nthe row with the ID 10 so that the user-de\u0000ned function (UDF) that\n\nis applied later simulates a runtime error.\n\nis arti\u0000cial failure simulates a transient error that can succeed on\n\nretry; for instance, a call to an external system or some missing\n\nexternal data that was delayed and the job cannot be completed\n\nwithout.\n\ne exception the UDF caused is propagated, and aer Spark\n\nexhausts the task retries, it causes the script action to raise an\n\nexception and the job fails. Unfortunately, the failed job has le\n\nincomplete results; most tasks succeeded, and the one that failed\n\nproduced a partial output \u0000le (up to row 10, which the UDF failed).\n\nen, the job run retry succeeded, but because of the \u0000le naming\n\nbased on timestamp, it didn’t override the previous attempt’s \u0000les,\n\nresulting in most of the data being duplicated.",
      "content_length": 1349,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 198,
      "content": "In step 8, you added a line that purges the output path, to make sure\n\nprevious attempts are cleaned before writing the data. is is useful\n\nnot only on retries but also on reruns. For instance, imagine the\n\ncode has a mistake doing calculations and has to be run again to\n\ncorrect the resulting data.\n\nThere’s more…\n\nIn a Glue job, there are multiple levels of retries. First, Spark will\n\nretry a failed task three times by default. It will also retry stages if\n\nthe cause is missing partial data; once Spark gives up and fails the\n\nSpark job because the exception is not captured and handled in\n\nyour script, then Glue will check whether retries are con\u0000gured on\n\nthe job.\n\ne issue experienced with the partial writes occurs because Glue\n\nby default uses a direct committer, which means the \u0000les are written\n\ndirectly into the destination folder. is is not the default on\n\nApache Spark in general, which historically assumes it runs on\n\nHadoop Distributed File System (HDFS), where it can write onto a\n\ntemporary folder and then do atomic renames once all \u0000les are\n\ngenerated correctly; this is not possible on S3, where renaming a \u0000le\n\nmeans copying it and thus has a high cost. Glue writes \u0000les directly\n\non the destination path, with the drawback that it can leave partial\n\nresults.",
      "content_length": 1289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 199,
      "content": "If using a DataFrame to write \u0000les, instead of calling purge, you\n\ncan just call: .mode(\"overwrite\"); in fact, by default, it will\n\nrefuse to write if it detects the output folder exists to prevent this\n\nsituation. If you want it to behave like the DynamicFrame write, you\n\ncan specify the \"append\" mode.\n\nWhen writing partitioned data, things get more complicated since\n\nyou just want to overwrite the partitions the job is writing and not\n\nthe ones created by previous runs (or other jobs). Fortunately, Spark\n\nprovides an option to do exactly that:\n\nspark.conf.set(\"spark.sql.sources.partitionOverwr iteMode\", \"dynamic\")\n\nSee also\n\nIn this example, you have seen how to do simple automatic retries. For\n\nmore sophisticated retries such as exponential backoﬀ, you can invoke\n\nthe Glue job as part of an AWS Step Functions or an Amazon Managed\n\nWork\u0000ows for Apache Air\u0000ow (MWAA) work\u0000ow:\n\nhttps://docs.aws.amazon.com/mwaa/latest/userguide/what-is-\n\nmwaa.html.\n\ne open source s3a implementation provides a more sophisticated\n\nway of writing \u0000les that provides eﬃcient writing without partial writes;\n\nit’s called the Magic Committer. Previously, it had the drawback that it\n\nrequired an external system for consistency, but now that S3 is",
      "content_length": 1239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 200,
      "content": "consistent, that dependency is no longer needed:\n\nhttps://hadoop.apache.org/docs/r3.1.1/hadoop-aws/tools/hadoop-\n\naws/committers.html#e_Magic_Committer.\n\nProcessing data incrementally using bookmarks and bounded execution\n\nData pipelines oen need to process data as it gets continuously\n\ngenerated and the ETL pipelines have to run on a regular basis. For\n\nsuch cases where the use (and extra cost) of streaming is not\n\njusti\u0000ed (for instance, if the data is uploaded once a day), using\n\nbookmarks is a simple way of keeping track of which \u0000les are\n\nalready processed and which are new since the last run. With\n\nbookmarks, you can run a scheduled job on a regular basis and\n\nprocess only new data added since the last run.\n\nIn addition, Glue provides an optional feature called bounded\n\nexecution; with it, a limited amount of data (size or \u0000les) is\n\nhandled in each bookmarked run. is allows the job to run in a\n\ntimely fashion and predictably with a volume of data that has been\n\ntested and not run into issues with memory, disk, or latency. is\n\ncan be useful if you are backloading a large amount of data or new\n\ndata arrives in bursts.\n\nGetting ready",
      "content_length": 1158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 201,
      "content": "is recipe requires a bash shell with the AWS CLI installed and\n\ncon\u0000gured and the GLUE_ROLE_ARN and GLUE_BUCKET\n\nenvironment variables set, as indicated in the Technical\n\nrequirements section at the beginning of the chapter.\n\nIn addition, run the following code to set up two environment\n\nvariables, pointing each to a S3 URL for the job source and\n\ndestination, on the bucket for recipes:\n\nRECIPE_S3_SRC=s3://$GLUE_BUCKET/bookmarkrecipe/in put/ RECIPE_S3_DST=s3://$GLUE_BUCKET/bookmarkrecipe/ou tput/\n\nFor this recipe, you’ll generate synthetic data. To do so, run the\n\nfollowing bash commands in the shell, from a directory where you\n\ncan write. e script will create 10 tiny JSON \u0000les locally; update\n\nthem to the path speci\u0000ed by the RECIPE_S3_SRC variable you just\n\nset, and \u0000nally delete the local \u0000les:\n\nmkdir ./bookmark_recipe_data/ for i in 1 2 3 4 5 6 7 8 9 10;do echo '{\"file_number\": '$i'}' > \\ ./bookmark_recipe_data/$i.json done aws s3 sync ./bookmark_recipe_data/ $RECIPE_S3_SRC rm ./bookmark_recipe_data/*.json rmdir ./bookmark_recipe_data/\n\nHow to do it…",
      "content_length": 1072,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 202,
      "content": "1. Create a job script as a local \u0000le running the following multiline shell\n\ncommand. It will execute when you enter the line with just EOF:\n\ncat <<EOF > BookmarksRecipeJob.py import sys from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job args = getResolvedOptions(sys.argv, ['JOB_NAME']) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args['JOB_NAME'], args) dynf = glueContext.create_dynamic_frame_from_options( connection_type = \"s3\", connection_options = { \"paths\": [\"$RECIPE_S3_SRC\"], \"boundedFiles\": 5 }, transformation_ctx=\"json_source\", format = \"json\" ) glueContext.write_dynamic_frame.from_options( frame=dynf.repartition(1), connection_type='s3', format='json', transformation_ctx=\"csv_dst\", connection_options={\"path\": \"$RECIPE_S3_DST\" } )",
      "content_length": 922,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 203,
      "content": "job.commit() EOF\n\n2. Upload the script to the S3 bucket and delete the local copy:\n\naws s3 cp BookmarksRecipeJob.py s3://$GLUE_BUCKET rm BookmarksRecipeJob.py\n\n3. Create a job with the following command:\n\naws glue create-job --name BookmarksRecipe -- role \\ $GLUE_ROLE_ARN --glue-version 4.0 --command\\ '{\"Name\": \"glueetl\", \"ScriptLocation\":\n\n\"s3://'$GLUE_BUCKET'/BookmarksRecipeJob.py\"}'\\ --default-arguments '{\"--job- language\":\"python\", \"--TempDir\": \"s3://'$GLUE_BUCKET'/tmp/\", \"--job-bookmark-option\": \"job-bookmark- enable\"}'\n\n4. Run the job:\n\nJOB_RUN_ID=$(aws glue start-job-run --job-name \\ BookmarksRecipe --output text)\n\n5. Get the run details; in the response, check the JobRunState job run\n\nto check the progress until it completes in a couple of minutes. If the\n\n\u0000nal state is ERROR or FAILED, it will give you an error message with\n\nthe cause:",
      "content_length": 856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 204,
      "content": "aws glue get-job-run --job-name BookmarksRecipe \\ --run-id $JOB_RUN_ID\n\n6. Retrieve the output \u0000les to a local directory and visualize the content. It\n\nshould have 5 rows taken from 5 of the 10 \u0000les in the source:\n\naws s3 sync $RECIPE_S3_DST bookmarks_recipe_result cut -b 1- bookmarks_recipe_result/* | sort\n\n7. Repeat steps 4, 5, and 6. If you try to start the job too soon aer the\n\nprevious run is \u0000nished, you might get a concurrency error. Wait for a\n\nfew seconds and retry. Now, step 6 prints the lines from the 10 \u0000les; this\n\nshows each input \u0000le was processed once, either on the \u0000rst or the\n\nsecond run.\n\n8. Wipe the local directory and delete the job if no longer needed:\n\nrm bookmarks_recipe_result/* rmdir bookmarks_recipe_result aws glue delete-job --job-name BookmarksRecipe\n\nHow it works…\n\ne \u0000rst step created the code to use Glue S3 bookmarks.\n\nBookmarks need three things:\n\ne init() method called on the job object.\n\ne read operation to specify a transformation_ctx parameter.\n\nEach source must have a diﬀerent one to track the bookmarks separately.",
      "content_length": 1070,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 205,
      "content": "Calling commit() on the job when the data is processed successfully.\n\nUntil that method is called, the bookmarks are not updated; there are no\n\npartial bookmark updates if some of the \u0000les are processed correctly but\n\nsome fail.\n\ne code uses repartition(1) so that the output is generated\n\nonto a single output \u0000le for convenience. On a real job with large\n\ndata, such a low number could cause a bottleneck.\n\nen, the script is uploaded and used on a job where bookmarks\n\nare enabled; note the job argument con\u0000gured on creation: --job-\n\nbookmark-option.\n\nWhen you run the job again, it will pick up the remaining \u0000les not\n\nalready bookmarked. So, aer two runs, there are two output \u0000les,\n\neach with the rows of \u0000ve input \u0000les (in this example, each source\n\n\u0000le has just one row, for easier traceability).\n\nYou can run the job once again and see if it produces an empty \u0000le\n\nbecause there are no new \u0000les. You could avoid generating empty\n\n\u0000les by checking in the code for dynf.count() > 0, before\n\nwriting the output.\n\nIn this example, the script used bounding based on \u0000les, but you\n\ncan do bounding based on data size. is is a better option if the\n\ndata \u0000les have very diﬀerent sizes.",
      "content_length": 1190,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 206,
      "content": "When using bookmarks, don’t change the default job currency of 1.\n\nIn a job using bookmarks, if there are multiple concurrent runs, at\n\ncommit time only one can succeed, and the others will fail because\n\nthey will detect the concurrent bookmark update.\n\nThere’s more…\n\nAs listed before, a transformation_ctx parameter on the source\n\nis a requirement for bookmarks to work. You can use this to your\n\nadvantage and combine the same job sources that need\n\nbookmarking and sources that do not. For instance, if you process\n\norders and need to join with customers, you want to bookmark the\n\norders but not the customers, so each run can join with all existing\n\ncustomers and not just new ones. You can also use bookmarks on\n\ncatalog tables, which will keep track of all \u0000les on all partitions.\n\ne --job-bookmark-option argument has a third option other\n\nthan enable or disable. You can pause the bookmark, which\n\nmeans the job will use the bookmark to read but won’t update it.\n\nYou can use this option for testing/troubleshooting.\n\nIt is possible to reset the bookmarks of a job or rewind to the state\n\nin which a previous job run le it. is is useful if you need to\n\nreprocess data due to some issue.",
      "content_length": 1199,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 207,
      "content": "Glue also has bookmarks for Java Database Connectivity (JDBC)\n\nsources. For that kind of bookmark, it needs one or more\n\nnumerically monotonic (always increasing or decreasing on\n\nupdates) columns to be speci\u0000ed so that Glue can remember the\n\nlast number processed and in the next run take the records with a\n\nlarger number (or smaller, if con\u0000gured that way).\n\nFor instance, if you have a sequence column, you can use that to\n\nbookmark records already processed versus new ones created, but it\n\nwon’t detect updates unless the bookmark column is updated as\n\nwell, using a timestamp column as bookmark.\n\nYou can read more about Glue bookmarks at\n\nhttps://docs.aws.amazon.com/glue/latest/dg/monitor-\n\ncontinuations.html.\n\nSee also\n\nYou can also use bookmarks in a visual job. See how to create one in the\n\nCreating ETL jobs visually using AWS Glue Studio recipe.\n\nBookmarks control the input data being processed at least once, but you\n\nalso must make sure the job output is tolerant to errors and doesn’t\n\ngenerate duplicates. Learn more in the Handling job failures and reruns\n\nfor partial results recipe.",
      "content_length": 1106,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 208,
      "content": "Handling a high quantity of small files in your job\n\nFrequently, when ingesting data, the source data is not optimized\n\nand it comes in tiny \u0000les, maybe because it was produced at short\n\nintervals or by many sources such as diﬀerent sensors sending their\n\nindividual reports. Apache Spark was designed as a big data tool\n\nand it struggles when handling such cases, causing ineﬃciency\n\nwhen processing too many partitions and also causing memory\n\nissues on the driver when building a plan.\n\nTo handle data eﬃciently, we want to consolidate small \u0000les to make\n\nthe reading more eﬃcient, especially if using a columnar format\n\nsuch as Parquet; as a rule of thumb, at least 100 MB bytes on each\n\n\u0000le. e simple way to control that is to repartition/coalesce the\n\ndata to the target number of output \u0000les, but that oen requires a\n\ncostly shuﬄe operation.\n\nIn this recipe, you will see a simple and eﬀective way provided by\n\nGlue to group small \u0000les at reading time.\n\nGetting ready\n\nis recipe requires a bash shell with the AWS CLI installed and\n\ncon\u0000gured. e GLUE_ROLE_ARN and GLUE_BUCKET environment",
      "content_length": 1098,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 209,
      "content": "variables need to be set, as per the Technical requirements section at\n\nthe beginning of the chapter.\n\nTo demonstrate this feature, we need data with lots of small \u0000les. To\n\nprepare such an input dataset, execute the following bash\n\ncommands. Make sure you have at least 50 MB of disk space free in\n\nthe local directory. It will generate 10k tiny CSV gzipped \u0000les. It will\n\ntake a few minutes, printing a dot each time it has completed 10k\n\n\u0000les:\n\necho placeholder > template_file.csv gzip template_file.csv mkdir smallfiles_recipe_input # Generate the files locally for i in {1..10000}; do cp template_file.csv.gz smallfiles_recipe_input/$i.csv.gz; if (($i % 1000 == 0)); then echo -n .; fi done echo # Upload to s3 and cleanup the local files rm template_file.csv.gz S3_INPUT_URL=s3://$GLUE_BUCKET/smallfiles_input/ aws s3 sync smallfiles_recipe_input $S3_INPUT_URL rm smallfiles_recipe_input/*.csv.gz rmdir smallfiles_recipe_input S3_OUTPUT_URL=s3://$GLUE_BUCKET/smallfiles_output /\n\nHow to do it…",
      "content_length": 1000,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 210,
      "content": "1. Create a job script as a local \u0000le by running the following bash\n\ncommand:\n\ncat <<EOF > GroupingFilesRecipeJob.py from pyspark.context import SparkContext from awsglue.context import GlueContext glueContext = GlueContext(SparkContext()) dynf = glueContext.create_dynamic_frame.from_options( connection_type=\"s3\", connection_options={ \"paths\": [\"$S3_INPUT_URL\"], \"useS3ListImplementation\": True, \"groupFiles\": \"inPartition\", \"groupSize\": \"100000\" }, format=\"csv\" ) writer=dynf.toDF().write.format(\"parquet\") writer.mode(\"overwrite\").save(\"$S3_OUTPUT_URL\" ) EOF\n\n2. Upload the job Python script to S3:\n\naws s3 cp GroupingFilesRecipeJob.py s3://$GLUE_BUCKET rm GroupingFilesRecipeJob.py\n\n3. Create a job:\n\naws glue create-job --name GroupingFilesRecipe \\ --role $GLUE_ROLE_ARN --number-of-workers 2 \\",
      "content_length": 800,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 211,
      "content": "--glue-version 4.0 --command '{\"Name\": \"gluestreaming\", \"ScriptLocation\":\n\n\"s3://'$GLUE_BUCKET'/GroupingFilesRecipeJob.py \"}'\\ --worker-type \"G.025X\" --default-arguments \\ '{\"--job-language\":\"python\", \"--TempDir\": \"s3://'$GLUE_BUCKET'/tmp/\"}'\n\n4. Run the job:\n\nJOB_RUN_ID=$(aws glue start-job-run --job-name \\ GroupingFilesRecipe --output text)\n\n5. Check the JobRunState job run until it completes (should be less\n\nthan 10 minutes). If the job fails for some reason, it will give you an\n\nerror message:\n\naws glue get-job-run --job-name GroupingFilesRecipe \\ --run-id $JOB_RUN_ID\n\n6. List the output \u0000les; the 10K tiny \u0000les have been consolidated into just 5:\n\naws s3 ls $S3_OUTPUT_URL\n\n7. Remove the job and the S3 \u0000les, if no longer needed:\n\naws glue delete-job --job-name GroupingFilesRecipe aws s3 rm --recursive $S3_INPUT_URL aws s3 rm --recursive $S3_OUTPUT_URL",
      "content_length": 866,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 212,
      "content": "How it works…\n\ne script speci\u0000es in the source that the \u0000les should be grouped\n\ninto 100k bytes on each partition. In this case, there is only one\n\npartition, which is the input directory. Each gzipped \u0000le is 50 bytes\n\n(notice the data is so small that it takes more space compressed, but\n\nthat is what Glue sees at planning time).\n\nerefore, we have 100k / 50 = 2000 \u0000les per group, so 10k \u0000les\n\ngrouped result in 5 Spark partitions overall (not to be confused with\n\nthe S3 partitions, which the grouping parameter refers to).\n\nen, you created a Glue job that uses the script uploaded to S3.\n\nNotice something peculiar? It is de\u0000ned as Spark Streaming so that\n\nthe job can use the smallest node size G.025X (a quarter of a DPU).\n\nis kind of node is intended for streaming because it is normally\n\ntoo small for ETL jobs in terms of memory, but in this case, it is\n\nenough because this job script requires minimum memory for\n\nplanning.\n\ne outcome was \u0000ve Parquet \u0000les since the \u0000les were grouped into\n\n\u0000ve Spark partitions. Notice here the number of partitions is\n\nmaintained when converting from DynamicFrame to DataFrame.\n\nUnfortunately, the conversion caused a small delay since\n\nDynamicFrame couldn’t \u0000gure out the schema on the \u0000y and had to\n\ndo a two-pass to \u0000rst determine it for DataFrame and then the",
      "content_length": 1312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 213,
      "content": "actual processing. is wouldn’t have been the case if writing\n\ndirectly from DynamicFrame.\n\nThere’s more…\n\nIf you con\u0000gure the Spark UI logs and visualize the execution, you\n\nwill see there is no shuﬄe of data. e \u0000les are assigned to diﬀerent\n\npartitions from the source.\n\nWith such small \u0000les, most of the time is not spent doing the actual\n\nreading and writing but listing the \u0000les from s3, which is not\n\nre\u0000ected in the Spark tasks. e job uses the\n\nuseS3ListImplementation option, which lists \u0000les directly\n\nusing the AWS Java SDK instead of the generic HDFS that Spark\n\nuses.\n\nis grouping feature will kick in automatically if GlueContext\n\ndetects the source has more than 50k \u0000les. If you do not want this\n\noptimization for some reason, you can disable the setting as\n\n\"groupFiles\": \"none\".\n\nNot all formats support this feature; check the documentation for\n\nfurther details: https://docs.aws.amazon.com/glue/latest/dg/aws-\n\nglue-programming-etl-format.html.\n\nSee also",
      "content_length": 976,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 214,
      "content": "For DataFrame, in Spark 3.5 or later, you can specify the desired\n\npartitions with spark.sql.files.maxPartitionNum and it will\n\ntry to group \u0000les to honor that. On older versions, there are very limited\n\ncapabilities to group \u0000les.\n\nGlue for Ray is better suited for handling small \u0000les; learn more about\n\nthis engine in the Running pandas code using AWS Glue for Ray recipe.\n\nReusing libraries in your Glue job\n\nSpark provides a rich data framework that can be extended with\n\nadditional plugins, libraries, and Python modules. As you build\n\nmore jobs, you would likely reuse your own code, whether it’s UDFs\n\nto process data when it’s not possible to do the same using the Spark\n\nfunctions or some pipeline code you want to reuse; for instance, a\n\nfunction with some transformations that you do regularly.\n\nIn this recipe, you will see how you can reuse Python code on Glue\n\nfor Spark jobs.\n\nGetting ready\n\nis recipe requires a bash shell with the AWS CLI installed and\n\ncon\u0000gured and the GLUE_ROLE_ARN and GLUE_BUCKET\n\nenvironment variables set, as indicated in the Technical\n\nrequirements section at the beginning of the chapter.",
      "content_length": 1133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 215,
      "content": "How to do it...\n\n1. e following bash commands will create a Python module and con\u0000g\n\n\u0000le:\n\nmkdir my_module cat <<EOF > my_module/__init__.py from random import randint def do_some_calculation(a): return randint(1, 10) + a def get_config_value(): with open('/tmp/my_config') as f: lines = f.readlines() return lines[0].strip() EOF zip -r my_module my_module echo \"recipe_example_value\" > my_config\n\n2. Upload both \u0000les to S3 and clean up:\n\nRECIPE_S3_PATH=s3://$GLUE_BUCKET/reuse_module aws s3 cp my_module.zip $RECIPE_S3_PATH/ rm my_module.zip rm my_module/__init__.py rmdir my_module aws s3 cp my_config $RECIPE_S3_PATH/ rm my_config\n\n3. Prepare the job script:\n\ncat <<EOF > ReuseLibrariesRecipe.py from pyspark.sql import SparkSession from pyspark.sql.functions import lit, udf from my_module import do_some_calculation,",
      "content_length": 822,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 216,
      "content": "get_config_value spark = SparkSession.builder.getOrCreate() df = spark.range(1 << 4).toDF(\"id\") df = df.withColumn(\"config_val\", lit(get_config_value())) calc_udf = udf(do_some_calculation) df = df.withColumn(\"calc\", calc_udf(df[\"id\"])) df.repartition(1).write.csv(\"$RECIPE_S3_PATH/o ut\") EOF aws s3 cp ReuseLibrariesRecipe.py $RECIPE_S3_PATH/ rm ReuseLibrariesRecipe.py\n\n4. Create a Glue job; be careful to use \\ only in the lines indicated:\n\naws glue create-job --name ReuseLibraryRecipe --role\\ $GLUE_ROLE_ARN --glue-version 4.0 --command \\ '{\"Name\": \"glueetl\", \"ScriptLocation\": \"'$RECIPE_S3_PATH'/ReuseLibrariesRecipe.py\"}' \\ --number-of-workers 2 --worker-type \"G.1X\" \\ --default-arguments '{\"--job- language\":\"python\", \"--extra-py- files\":\"'$RECIPE_S3_PATH'/my_module.zip\", \"-- extra-files\": \"'$RECIPE_S3_PATH'/my_config\", \"--TempDir\": \"'$RECIPE_S3_PATH'/tmp/\"}'\n\n5. Run the job:\n\nJOB_RUN_ID=$(aws glue start-job-run --job-name \\ ReuseLibraryRecipe --output text)",
      "content_length": 970,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 217,
      "content": "6. Check the JobRunState job run until it completes (should be less\n\nthan 10 minutes). If the job fails for some reason, it will give you an\n\nerror message:\n\naws glue get-job-run --job-name ReuseLibraryRecipe \\ --run-id $JOB_RUN_ID\n\n7. Show the result \u0000le content:\n\naws s3 sync $RECIPE_S3_PATH/out/ . cat part-*-c000.csv rm part-*-c000.csv\n\n8. Remove the job if no longer needed:\n\naws glue delete-job --job-name ReuseLibraryRecipe\n\nHow it works…\n\nIn the \u0000rst step, you created a basic Python module. e module\n\nhas two functions: one that simulates a calculation providing a\n\nrandom value and another that reads a local text \u0000le and provides\n\nthe context.\n\nWhen a job is created, it is con\u0000gured with the S3 path to the text\n\n\u0000le as an extra \u0000le and the module as an extra Python \u0000le. At\n\nruntime, the text \u0000le will be deployed on the /tmp directory of each",
      "content_length": 857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 218,
      "content": "node of the cluster, and each node will download, extract the ZIP\n\n\u0000le with the module, and make it available in the Python runtime.\n\nWhen the job runs, the code creates a DataFrame with the help of\n\nthe Python functions and stores it on S3 as a CSV \u0000le, with three\n\ncolumns:\n\nA sequential number for each row\n\nA column containing the text provided by the function – this function is\n\ncalled on the driver and then used as a constant for all rows\n\ne third column is the result of calling the do_some_calculation\n\nmodule on the row ID using a UDF, and the result is a random number\n\nbetween 1 and 10 added to the row ID\n\nThere’s more...\n\nIn this recipe, the code was reused directly on the driver and\n\ndistributed in the form of UDFs. Notice this is for demonstration\n\npurposes, but using Python UDFs results in performance\n\ndegradation when running with a signi\u0000cant amount of data.\n\nInstead of a ZIP \u0000le, you can use --extra-py-files with\n\nindividual Python \u0000les, which then you can import into the code\n\nusing the \u0000lename. If you want to use wheel \u0000les (created by you or\n\nfrom a Python repository), then you instead need to use the --\n\nadditional-python-modules parameter.",
      "content_length": 1176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 219,
      "content": "Reusing Java/Scala code is similar, but instead, you specify the\n\nlocation of the JAR \u0000les using the --extra-jars parameter.\n\nSee also\n\nYou can reuse code in the same way on Glue Studio visual jobs when you\n\nuse the custom code node to extend the capabilities. See an example of a\n\nvisual job in the Creating ETL jobs visually using AWS Glue Studio recipe.\n\nVisual jobs also allow creating your own components to reuse code; see\n\nan example at https://aws.amazon.com/es/blogs/big-data/create-your-\n\nown-reusable-visual-transforms-for-aws-glue-studio/.\n\nUsing data lake formats to store your data\n\nHistorically, big data technologies on the Hadoop ecosystem have\n\ntaken some trade-oﬀs to scale to volumes that traditional databases\n\ncannot handle. In the case of Apache Hive, which became the\n\nstandard Hadoop SQL database, the external tables just point to\n\n\u0000les on some object storage such as HDFS or S3, and then jobs\n\naccess those \u0000les without a central system coordinating access or\n\ntransactions. is is still how the standard tables work on the Glue\n\ncatalog.\n\nAs a result, the atomicity, consistency, isolation, and durability\n\n(ACID) properties of RDBMSs were relaxed to allow for scalability",
      "content_length": 1200,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 220,
      "content": "in use cases where write concurrency or the lack of transactions is\n\nnot an issue, such as historical append-only tables.\n\nIn recent years, the desire has been to bring back those ACID\n\nproperties while keeping the data on a scalable object store for\n\ncheap and virtually in\u0000nite scalability, with many clients and\n\nengines using the data in a distributed way.\n\ne result is the growing popularity of so-called “data lake table\n\nformats,” which de\u0000ne tables that are no longer just plain data \u0000les\n\nbut have a structure of metadata that can handle transactions, keep\n\ntrack of changes and versions, and allow time travel.\n\ne most popular formats are Apache Iceberg, Apache Hudi, and\n\nDelta Lake.\n\nIceberg’s diﬀerentiating feature is that it allows dynamic\n\npartitioning. In this recipe, you will see an example of how to easily\n\nenable and use Iceberg in your Glue jobs.\n\nGetting ready\n\nis recipe requires a bash shell with the AWS CLI installed and\n\ncon\u0000gured. e GLUE_ROLE_ARN, ACCOUNT_ID, and\n\nGLUE_BUCKET environment variables must be set, as indicated in\n\nthe Technical requirements section.",
      "content_length": 1098,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 221,
      "content": "How to do it…\n\n1. Create a job script as a local \u0000le running the following multiline bash\n\ncommand. It will execute when you enter the line with just EOF:\n\ncat <<EOF > IcebergRecipe.scala import org.apache.spark.sql.SparkSession import org.apache.spark.sql.functions. {col,lit,rand} object GlueApp { def main(sysArgs: Array[String]) { val spark = (SparkSession.builder .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.\\ extensions.IcebergSparkSessionExtensions\") .config(\"spark.sql.catalog.iceberg\",\n\n\"org.apache.iceberg.spark.SparkCatalog\")\n\n.config(\"spark.sql.catalog.iceberg.warehouse\", \"s3://$GLUE_BUCKET/iceberg\")\n\n.config(\"spark.sql.catalog.iceberg.catalog-\\ impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") .config(\"spark.sql.catalog.iceberg.io- impl\",\n\n\"org.apache.iceberg.aws.s3.S3FileIO\") .getOrCreate()) val db = \"iceberg_recipe_db\" spark.sql(\"CREATE DATABASE IF NOT EXISTS iceberg.\" + db) val df = (spark.range(1 << 10).toDF(\"id\")",
      "content_length": 952,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 222,
      "content": ".withColumn(\"value1\", rand()) .withColumn(\"region\", lit(\"region1\")) ) df.writeTo(\"iceberg.\" + db + \".icetable\").\n\npartitionedBy(col(\"region\")).createOrReplace() } } EOF\n\n2. Upload the script to S3:\n\naws s3 cp IcebergRecipe.scala s3://$GLUE_BUCKET rm IcebergRecipe.scala\n\n3. Create a job:\n\naws glue create-job --name IcebergRecipe -- role \\ $GLUE_ROLE_ARN --glue-version 4.0 --worker- type \\ \"G.1X\" --number-of-workers 2 --default- arguments \\ '{\"--job-language\":\"scala\", \"--class\": \"GlueApp\", \"--datalake-formats\": \"iceberg\", \"--TempDir\": \"s3://'$GLUE_BUCKET'/tmp/\"}' --command '{\"Name\": \"glueetl\",\"ScriptLocation\": \"s3://'$GLUE_BUCKET'/IcebergRecipe.scala\"}'\n\n4. Run the job:\n\nJOB_RUN_ID=$(aws glue start-job-run --job-name",
      "content_length": 724,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 223,
      "content": "\\ IcebergRecipe --output text)\n\n5. Check the job status until it succeeds:\n\naws glue get-job-run --job-name IcebergRecipe \\ --run-id $JOB_RUN_ID\n\n6. On the AWS console, navigate to Athena and run the following query to\n\nshow the table content (it might ask to select an S3 output location \u0000rst):\n\nSELECT * FROM iceberg_recipe_db.icetable LIMIT 20;\n\nFigure 3.9 – Iceberg table sample in Athena",
      "content_length": 392,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 224,
      "content": "7. If you no longer need them, delete the table created on the catalog (this\n\ndoesn’t delete the data on S3), the catalog policy, and the job:\n\naws glue delete-database --name iceberg_recipe_db aws glue delete-job --job-name IcebergRecipe\n\nHow it works…\n\nIn the \u0000rst step, you de\u0000ned the script (in this case, using Scala) and\n\nthen created a job to run it. Notice the code in this recipe is pure\n\nSpark; there is no Glue API-speci\u0000c code. is shows you can bring\n\nstandard Spark code and run in Glue without changes.\n\ne job code de\u0000nes a special Spark catalog con\u0000guration, using the\n\nproperties starting with spark.sql.catalog, and then an\n\narbitrary name for the catalog. In this example named iceberg,\n\nyou can use your own name as long as when you reference the table\n\nin the code, you use the same catalog pre\u0000x so that Spark knows it\n\nneeds to use that con\u0000guration for that table.\n\nLater in the script, it inserts the sample DataFrame created into a\n\ntable named iceberg.iceberg_recipe_db.icetable. e \u0000rst\n\npart is the catalog to match the con\u0000guration name with the\n\ndatabase and table names. Iceberg will create a directory using the\n\ndatabase and table names (adding the.db suﬃx for the database).",
      "content_length": 1210,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 225,
      "content": "en, it will create a matching database and table in the catalog\n\nwith the S3 location.\n\nTo enable the Iceberg framework, all that was required was to add\n\nthe --datalake-formats=iceberg argument. You can enable\n\nHudi or Delta in the same way, specifying hudi or delta\n\nrespectively or multiple of them separated by a comma.\n\nEach framework has its own particularities, con\u0000guration, and way\n\nto expose the table in the Glue catalog, but the way to enable them\n\nin a Glue job is the same.\n\nThere’s more…\n\ne updates on the table are transactional, so you will always see a\n\nconsistent view. When there are multiple writers on the same table,\n\nthings get more complicated; therefore, it’s best to have a single job\n\nwriting on the table.\n\nIceberg and the other frameworks have optimistic lock detection,\n\nwhich means they can detect when another writer has made\n\nchanges concurrently and rectify it. In addition, the frameworks\n\nhave the option to con\u0000gure an external system such as DynamoDB\n\nto hold locks and do pessimistic locking so that other writers wait.\n\nis makes sense when the optimistic locking has many collisions\n\nand must constantly redo work due to con\u0000icts.",
      "content_length": 1174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 226,
      "content": "Before Glue supported these frameworks natively, the way to add\n\nthem was by subscribing to a marketplace connector. is option is\n\nnot only more laborious but adds a runtime dependency to the us-\n\neast-1 Elastic Container Registry (ECR) repository to download\n\nthe container at runtime. erefore, this option is no longer\n\nrecommended for data lake formats.\n\nYou also have the option to add the JARs yourself to the job as extra\n\nlibraries, which allows you to use the latest version of the\n\nframework.\n\nSee also\n\nWhen creating a visual job in Glue Studio, the support for data lake\n\nformats is added automatically as needed. See how to create a visual job\n\nin the Creating ETL jobs visually using AWS Glue Studio recipe.\n\nIf you want to read incrementally from one of these formats, you need to\n\ntrack the snapshots read or use a streaming source. If this incremental\n\ndata doesn’t need to be updated or queried, it might be more eﬀective for\n\ndoing incremental ingestion to use Glue bookmarks; see how in the\n\nProcessing data incrementally using bookmarks and bounded execution\n\nrecipe.\n\nOptimizing your catalog data retrieval using pushdown filters and",
      "content_length": 1156,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 227,
      "content": "indexes\n\ne AWS Glue Data Catalog is a key component in a big data cloud\n\narchitecture. It doesn’t hold data but acts as an Apache Hive-\n\ncompatible metastore, de\u0000ning table metadata that acts as a layer of\n\nabstraction. It shows clients how to locate and interpret the data\n\nstored in a system such as Amazon S3.\n\nIn the traditional Hive-compatible catalog tables, the catalog doesn’t\n\nkeep track of data \u0000les. It just points to a directory pre\u0000x, and then\n\nthe client will list the pre\u0000x to get a list of \u0000les currently present\n\nthere. e way these kinds of tables can scale is by using partitions,\n\neach one corresponding to a pre\u0000x on S3, to avoid listing all \u0000les for\n\nany query. A partitioned table de\u0000nes one or more columns as\n\npartition columns.\n\nFor instance, if you have a table with the year, month, and day\n\npartition columns as strings, then the data for each day will be\n\nplaced under a speci\u0000c pre\u0000x, which when following the Hive\n\nconventions contains the partition values; for instance:\n\ns3://mybucket/mytable/year=2023/month=08/day=01/.\n\nis way, a client tool that only needs data from that day just needs\n\nto read that speci\u0000c data and not all the \u0000les from the table, which\n\nmight potentially have years of data.",
      "content_length": 1234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 228,
      "content": "In this recipe, you will see how to bene\u0000t from partitions, even in\n\ncases where a table contains a massive number of them.\n\nGetting ready\n\nis recipe assumes you have created the AWSGlueServiceRole-\n\nRecipe role and an S3 bucket with your account ID of glue-\n\nrecipes-<your accountid>, as indicated in the Technical\n\nrequirements section at the beginning of this chapter.\n\nIn the bash shell, add a policy to allow the role to run notebooks:\n\nGLUE_ROLE=AWSGlueServiceRole-Recipe aws iam put-role-policy --role-name $GLUE_ROLE \\ --policy-name GlueSessions --policy-document '{\"Version\": \"2012-10-17\", \"Statement\": [{\"Effect\": \"Allow\", \"Action\":[\"glue:*Session\", \"glue:RunStatement\", \"iam:PassRole\"], \"Resource\":[\"*\"]}]}'\n\nHow to do it…\n\n1. Log in to the AWS console, navigate the Glue service, and on the le\n\nmenu select Notebooks. en, choose the Notebook button to create it.\n\n2. On the popup, leave the Spark (Python) and Start fresh default options\n\nin the AWSGlueServiceRole-Recipe IAM role and complete the\n\ncreation.",
      "content_length": 1023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 229,
      "content": "3. Aer a few seconds, it will load Jupyter and open a notebook with some\n\nsample cells already \u0000lled in. e sample code might change, but there\n\nshould always be a cell to help you set up your job. is setup cell does\n\nsome con\u0000guration (the lines starting with %) and then sets up the\n\nGlueContext and SparkSession objects with the names\n\nglueContext and spark, respectively (should the name of these\n\nobjects change, you need to update the code provided accordingly). To\n\nreduce costs, you can change %number_of_workers down to 2.\n\nen, run this cell by selecting it and using the Run button on\n\nthe toolbar or by using the Shi + Enter shortcut. Below the cell,\n\nyou will see the initialization progress until it con\u0000rms the\n\nsession has been created:",
      "content_length": 755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 230,
      "content": "Figure 3.10 – Glue Studio Notebook\n\n4. Now, add a new cell to the notebook; you can use the plus (+) button on\n\nthe top le. Inside, enter the following code and replace the bucket name\n\non the \u0000rst line with your own. en, run the cell and wait until it\n\ncompletes:\n\nimport boto3 from pyspark.sql.functions import rand from awsglue.dynamicframe import DynamicFrame account_id = boto3.client('sts').get_caller_identity( ) [\"Account\"] s3_bucket = f\"glue-recipes-{account_id}\" s3_path =",
      "content_length": 484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 231,
      "content": "f\"s3://{s3_bucket}/pushdown_recipe_table/\" database = \"default\" table_name = \"pushdown_recipe_table\" df = spark.range(1 << 8).withColumn(\"value\", rand()) sink = glueContext.getSink(connection_type=\"s3\", path=s3_path, enableUpdateCatalog=True, updateBehavior=\"UPDATE_IN_DATABASE\", partitionKeys=[\"id\"]) sink.setFormat(\"avro\") sink.setCatalogInfo(catalogDatabase=database,\n\ncatalogTableName=table_name) sink.writeFrame(DynamicFrame.fromDF(df, glueContext))\n\n5. Now, add a cell with the following code and run it. It will take about a\n\nminute to print the result with the count:\n\nglueContext.create_dynamic_frame.from_catalog( database=database, table_name=table_name).count()\n\n6. Now, add a cell with this alternative code and run it. It should just take a\n\nfew seconds to complete:\n\nglueContext.create_dynamic_frame.from_catalog( database=database, table_name=table_name, push_down_predicate=\"id in (3, 6, 9)\", additional_options={ \"catalogPartitionPredicate\":\"id < 10\"",
      "content_length": 968,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 232,
      "content": "} ).count()\n\n7. Add another cell to run this code to get the same result:\n\nspark.sql(f\"SELECT * FROM {database}. {table_name} \" \"WHERE id IN (3, 6, 9)\").count()\n\n8. Extend the le menu, select Tables, and then search for\n\npushdown_recipe_table. Observe the schema with the ID as the\n\npartition column and switch to the Partitions tab. It contains 32\n\npartitions. Switch to the Indexes tab and select Add index. Give it the\n\nname main and select the id column. If steps 6 and 7 had been slow\n\ndue to the high number of partitions, adding this index would\n\nsigni\u0000cantly speed up queries on the id column:\n\nFigure 3.11 – Table indexes with the one index added\n\n9. Add a new cell and run it with this code to remove the table:\n\nboto3.client('glue').delete_table( DatabaseName=database,Name=table_name)\n\nHow it works…",
      "content_length": 812,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 233,
      "content": "In this recipe, instead of using a Glue job, you used a Glue\n\ninteractive session. is is an equivalent of a cluster, which you can\n\ncon\u0000gure using magics instead of the Job details page. e magics\n\ncan be line magics (starting with %), where the magic parameters\n\nare set on the same line (for instance, %glue_version 3.0), and\n\ncell magics (starting with %%), where the whole cell is used for the\n\nmagic (for instance, %%configure to add arguments). You can run\n\nthe %help cell to get a listing of magics and their uses.\n\nFor simplicity, it’s common to use the same role for the notebook\n\nand the session, but it’s possible to use a diﬀerent use using the\n\n%iam_role magic.\n\ne interactive session starts the moment you run any code (any\n\nline that is not a magic or a comment). Changes in the session\n\ncon\u0000guration (for example, number of workers) won’t take eﬀect\n\nuntil the kernel or the notebook is restarted.\n\nNotice that while a cell is running, it has an asterisk at the le of it.\n\nOnly one cell can run at a time; you can queue multiple cell\n\nexecutions.\n\nen, you ran a cell that created simple data with a sequential ID\n\nand a random value, which was stored as a partitioned table with\n\nthe ID as the partition column. erefore, the table has just 32\n\npartitions; a real table could have thousands of them.",
      "content_length": 1319,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 234,
      "content": "en, you ran multiple queries on the table. In the \u0000rst case, any\n\noperation will have to read all the \u0000les, even if you \u0000lter out the data\n\nlater because you only need a subset. e other queries leveraged\n\nthe table partition. e performance diﬀerence should be noticeable\n\neven for such a small table (note the cluster is also tiny with just two\n\nnodes, one of which is the driver).\n\nFinally, you added an index to the table. In such a small table, it will\n\nbe added instantly but won’t make a real diﬀerence at query time\n\nsince most of the time is spent checking \u0000les. In a table with tens of\n\nthousands of partitions or more, the index will make a big\n\ndiﬀerence in retrieving partitions quickly.\n\nThere’s more…\n\nIn this example, the sample DataFrame created with sample data is\n\nconverted to a DynamicFrame to create a table. It’s also possible to\n\ncreate a table directly from the DataFrame using saveAsTable(),\n\nbut using the DynamicFrame writer is more eﬃcient in adding\n\npartitions and can update the schema automatically as needed.\n\nNotice the diﬀerence in the usage of DynamicFrame and SparkSQL\n\nAPIs to read the partitioned table. In the case of SparkSQL, the\n\nengine uses the query criteria to push down the \u0000lters applicable to\n\nthe partitions and only read the related data. On the other hand, in",
      "content_length": 1312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 235,
      "content": "the case of create_dynamic_frame.from_catalog, the\n\npartition \u0000lters are explicitly indicated in two possible ways:\n\npush_down_predicate allows using rich SQL \u0000lters, which are\n\napplied on the partitions retrieved from the server. So, it avoids checking\n\n\u0000les not needed but can still spend a lot of time listing table partitions if\n\nthe table has many.\n\ncatalogPartitionPredicate applies the \u0000lter on the server\n\nside, so it scales better, but it’s more limited in the kind of \u0000lters you can\n\nbuild – basically, equals and greater/smaller than conditions, of which\n\nyou can add multiple but not make them optional (no OR clause). In\n\naddition, this predicate can use a table index like the one you created in\n\nthe recipe , if it \u0000lters on the index columns.\n\nBoth types of predicates can be combined to bene\u0000t from the\n\neﬃciency of catalogPartitionPredicate and then apply richer\n\n\u0000lters to narrow further the results by using\n\npush_down_predicate.\n\nCheck the Glue documentation for details and examples about\n\nsyntax and usage:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-\n\nprogramming-etl-partitions.html.\n\nSee also",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 236,
      "content": "is recipe is based on traditional Hive-style tables (using directories).\n\nWhile this is a simple and eﬃcient way to partition data, it has a big\n\nlimitation: changing the partitioning means you need to rebuild the\n\ntable. Instead, you could use Apache Iceberg, which provides dynamic\n\npartitions that allow evolving the partitioning without rebuilding the\n\ntable or impacting the users. See the Using data lake formats to store your\n\ndata recipe for the recommended way to use Apache Iceberg in Glue.\n\nWhen adding partitions using the DynamicFrame writer, at the time of\n\nthis writing, it updates the main table schema to make sure it matches\n\n(whether is needed or not); this can keep increasing the number of table\n\nversions and eventually hit the account limit.\n\nere is an open-source tool provided by AWS to clean up old\n\nversions:\n\nhttps://github.com/aws-samples/aws-glue-table-versions-\n\ncleanup-utility.\n\nRunning pandas code using AWS Glue for Ray\n\ne pandas library is a highly popular Python library for data\n\nmanipulation and analysis, based on the well-established numpy\n\nlibrary, handling data in a table-like format. It is so well established\n\namong Python analysts and data scientists, that it has become a de\n\nfacto standard to the point that other libraries implement their",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 237,
      "content": "interfaces so that they can run existing pandas code. is is oen\n\ndone to overcome pandas’ limitations, namely being a single\n\nprocess memory-based library, which limits scalability.\n\nOne such pandas-compatible library is Modin. It can run pandas\n\ncode by just changing the imports while being able to scale by using\n\nan engine such as Dask or Ray. In this recipe, you will see how to\n\nrun pandas code on Glue for Ray using Modin.\n\nGetting ready\n\nis recipe requires a bash shell with the AWS CLI installed and\n\ncon\u0000gured. e GLUE_ROLE_ARN and GLUE_BUCKET environment\n\nvariables need to be set, as indicated in the Technical requirements\n\nsection at the beginning of the chapter.\n\nMake sure that Glue for Ray is available in the AWS region you are\n\nusing.\n\nHow to do it…\n\n1. Create a job script as a local \u0000le running the following multiline bash\n\ncommand. It will execute when you enter the line with just EOF:\n\ncat <<EOF > RayModinRecipeJob.py import ray import modin.pandas as pd import numpy as np",
      "content_length": 1002,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 238,
      "content": "ray.init() s3_path = \"s3://$GLUE_BUCKET/ray_recipe_output\" num_samples = 10000 df = pd.DataFrame({ 'id' : range(num_samples), 'value' : np.random.randn(num_samples), }) result = df[df['value'] > 0] print(f\"Std Dev: {df['value'].std()}\") print(f\"Out of {num_samples}, {result.shape[0]} will\\ be saved after filtering\") result.to_parquet(s3_path) EOF\n\n2. Upload the script to S3:\n\naws s3 cp RayModinRecipeJob.py s3://$GLUE_BUCKET rm RayModinRecipeJob.py\n\n3. Create a job, making sure to only use \\ in the lines indicated:\n\naws glue create-job --name RayModinRecipeJob \\ --role $GLUE_ROLE_ARN --command '{\"Name\": \"glueray\", \"Runtime\": \"Ray2.4\", \"ScriptLocation\": \"s3://'$GLUE_BUCKET'/RayModinRecipeJob.py\"}'\\ --number-of-workers 1 --worker-type \"Z.2X\"\\ --glue-version 4.0 --default-arguments\\ '{\"--pip-install\": \"modin,s3fs\"}'\n\n4. Run the job:",
      "content_length": 840,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 239,
      "content": "JOB_RUN_ID=$(aws glue start-job-run --job-name \\ RayModinRecipeJob --output text)\n\n5. Check the JobRunState job run until it completes (should be less\n\nthan 10 minutes). If the job fails for some reason, it will give you an\n\nerror message:\n\naws glue get-job-run --job-name RayModinRecipeJob \\ --run-id $JOB_RUN_ID\n\n6. Print the job using stdout and observe the messages printed from the\n\ncode:\n\naws s3 cp \\ s3://$GLUE_BUCKET/jobs/RayModinRecipeJob/\\ $JOB_RUN_ID/job-result/stdout /tmp cat /tmp/stdout && echo -e \"\\n\" && rm /tmp/stdout\n\n7. List the \u0000les generated by the job:\n\naws s3 ls s3://$GLUE_BUCKET/ray_recipe_output/\n\n8. Remove the job if no longer needed:\n\naws glue delete-job --job-name RayModinRecipeJob\n\nHow it works…",
      "content_length": 727,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 240,
      "content": "If you have run other Glue recipes that use script jobs, you might\n\nhave noticed the process is almost identical to Glue for Ray. First,\n\nyou created a Python script, uploaded it to S3, then created a job\n\nspecifying glueray as the command. is command requires the\n\nversion of Ray to be speci\u0000ed and uses diﬀerent instance types as\n\nother types of Glue jobs. e job speci\u0000es that the Modin and s3fs\n\nlibraries need to be installed.\n\nRunning the job and checking the status was identical to other\n\nkinds of Glue jobs.\n\nGlue stores the job logs, including stdout and stderr, under the\n\npath where the script was located. is allowed you to easily check\n\nthe result of the print statements.\n\nLogs are also available on CloudWatch, as usual.\n\ne script generated a DataFrame with 1,000 rows and a column of\n\nvalues based on a standard normal distribution, then the code\n\n\u0000ltered only the positive values and saved them as Parquet \u0000les on\n\nS3.\n\nThere’s more…\n\nWhen using Glue for Ray, the smallest instance at the time of this\n\nwriting is two DPUs. However, unlike Glue for Spark, you can use a\n\nsingle node since the Ray driver can also do work. Modin",
      "content_length": 1148,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 241,
      "content": "automatically detected the Ray framework. If the job had more\n\nnodes assigned, it would have used them for distributing the data\n\nand processing.\n\nIn this example, we assumed that a requirement was to be able to\n\nreuse existing pandas code (or existing pandas skills); if you want\n\nmore control of the distributed processing, you can use the Ray\n\nframework directly with the APIs it provides.\n\nAs Ray gains popularity, other libraries are adding support for it,\n\nsuch as the popular machine learning (ML) library pytorch or the\n\nversatile AWS library awsdatawrangler, which provides\n\nintegration to AWS data services such as S3, DynamoDB, Redshi,\n\nthe Glue Catalog, or OpenSearch.\n\nSee also\n\nIf you want to avoid doing any coding at all, you can refer to the Creating\n\nETL jobs visually using AWS Glue Studio recipe. At the time of this\n\nwriting, visual jobs are only able to generate Glue for Spark code.\n\nTo reuse existing pandas code, you also have the option to use the\n\nrecently added Pandas API on Spark. Unfortunately, this API has more\n\nlimitations and discrepancies in terms of pandas compatibility. See\n\ndetails on the Apache site:\n\nhttps://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_\n\nspark/index.html.",
      "content_length": 1232,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 242,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 243,
      "content": "4 A Deep Dive into AWS Orchestration Frameworks\n\nWelcome to this chapter on orchestration frameworks in the AWS\n\necosystem. In this chapter, you will gain valuable insights into\n\nsetting up and managing orchestration frameworks using AWS\n\nservices. Speci\u0000cally, we will explore four key AWS oﬀerings: AWS\n\nGlue work\u0000ows, Amazon Managed Work\u0000ows for Apache Air\u0000ow\n\n(MWAA), AWS Step Functions, and Amazon EventBridge.\n\nIn distributed systems, coordinating tasks across diﬀerent services\n\nand components can be challenging. Building complex architecture\n\nand implementing work\u0000ows with code can lead to spaghetti code\n\nthat’s hard to maintain and debug. roughout this chapter, you will\n\n\u0000nd practical examples and guidance on creating, updating, and\n\nimplementing rollback strategies based on metrics emitted by your\n\nwork\u0000ows. By the end of this chapter, you’ll be well-equipped to\n\nharness the power of these AWS orchestration tools to optimize\n\nyour work\u0000ow management and automation processes.\n\ne following recipes will be covered in this chapter:\n\nDe\u0000ning a simple work\u0000ow using AWS Glue work\u0000ows\n\nSetting up event-driven orchestration with Amazon EventBridge",
      "content_length": 1163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 244,
      "content": "Creating a data work\u0000ow using AWS Step Functions\n\nManaging data pipelines with MWAA\n\nMonitoring your pipeline’s health\n\nSetting up a pipeline using AWS Glue to ingest data from a JDBC\n\ndatabase into a catalog table\n\nTechnical requirements\n\nFor the recipes in this chapter, you’ll need an active AWS account\n\nwith appropriate permissions to create and manage the following:\n\nGlue work\u0000ows\n\nGlue triggers\n\nStep Functions\n\nMWAA environment\n\nAWS roles and permissions\n\nCloudWatch\n\nYou can \u0000nd the code \u0000les for this chapter on GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter04.\n\nDefining a simple workflow using AWS Glue workflows",
      "content_length": 679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 245,
      "content": "In this recipe, we will explore the world of AWS Glue work\u0000ows.\n\nAWS Glue work\u0000ows are powerful tool for orchestrating complex\n\nextract, transform, and load (ETL) processes on AWS. By\n\ncombining jobs, crawlers, and triggers, work\u0000ows can automate\n\ndata pipelines, ensuring data is consistently processed and delivered\n\nto its intended destinations. is makes them ideal for a variety of\n\ndata-driven applications, from data warehousing and analytics to\n\nmachine learning and real-time data processing.\n\nImagine a retail company that needs to load sales data from\n\nmultiple sources into its data lake on Amazon S3. e data comes in\n\nraw CSV format, and the goal is to transform this data into a\n\nstructured format that can be used for reporting and analysis. e\n\ndata is updated daily, and you need an automated pipeline to clean,\n\ntransform, and store this data.\n\nIn this scenario, you can leverage an AWS Glue work\u0000ow, crawler,\n\njob, and trigger to automate the process.\n\nGetting ready\n\nTo build a simple AWS Glue work\u0000ow that reads CSV data from S3\n\nand writes it out as partitioned Parquet, you need an S3 bucket with\n\nsource CSV data.",
      "content_length": 1138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 246,
      "content": "If you have an existing S3 bucket, then use it, or else create a new S3\n\nbucket. To create an S3 bucket, \u0000rst log in to the AWS Management\n\nConsole and navigate to the S3 service. Click on Create bucket and\n\nchoose a unique name using lowercase letters, numbers, periods,\n\nand hyphens. Select your desired name. Finally, aer reviewing your\n\ncon\u0000guration, click on Create bucket to \u0000nalize.\n\nHow to do it…\n\ne high-level steps to build a simple AWS Glue work\u0000ow for CSV-\n\nto-Parquet conversion are as follows:\n\n1. Prepare your data: Store your CSV data in an S3 bucket. is will be the\n\ninput source for your Glue job.\n\n2. Create an IAM role: Create an IAM role with the necessary permissions\n\nfor your Glue crawler and Glue job to access S3 and the Glue Data\n\nCatalog.\n\n3. Create a Glue database: A Glue database is like a container that holds\n\ntables and their schemas within the AWS Glue Data Catalog. It helps\n\norganize your metadata, making it easier to manage and query your\n\ndata.\n\nIn the AWS Glue Data Catalog, create a database to organize the\n\ntables that your crawler will generate.\n\n4. Create an AWS Glue crawler: Glue crawlers automatically discover and\n\ncatalog your data stored in various locations such as S3, databases, and",
      "content_length": 1240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 247,
      "content": "data lakes. ey extract schema information and create table de\u0000nitions,\n\nmaking your data readily available for analysis and querying.\n\nCon\u0000gure the crawler to read the CSV data from your S3\n\nbucket. e crawler will infer the schema of your data and create\n\ncorresponding tables in the AWS Glue Data Catalog, within the\n\ndatabase you created.\n\n5. Create an AWS Glue job: Glue jobs are the heart of your ETL processes\n\nin AWS Glue. ey run your scripts (written in Python or Scala) to\n\nextract data from sources, transform it according to your needs, and\n\nload it into target destinations:\n\nI. Use a Python or Scala script to de\u0000ne your ETL logic.\n\nII. In the script, read the CSV data from the S3 bucket using the\n\ntable created by the crawler.\n\nIII. Perform any necessary transformations on the data.\n\nIV. Write the transformed data to a new location in the S3 bucket\n\nin Parquet format, partitioning the data as needed.\n\n6. Create an AWS Glue work\u0000ow: Work\u0000ows provide a visual\n\nrepresentation of your data processing steps in AWS Glue. ey allow\n\nyou to connect various components, de\u0000ne dependencies, and monitor\n\nthe execution \u0000ow, making it easier to manage complex ETL processes:\n\nI. Create a Glue work\u0000ow to orchestrate the crawler and job.\n\nII. Add the crawler and job as nodes in the work\u0000ow.",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 248,
      "content": "III. De\u0000ne the dependencies between the nodes (for example, the\n\njob should run aer the crawler has \u0000nished).\n\n7. Create an AWS Glue trigger: Triggers act like “event listeners” for your\n\nGlue work\u0000ows. ey initiate your data processing tasks based on\n\nschedules, data arrival events, or completion of other jobs, making your\n\ndata pipelines dynamic and responsive:\n\nI. Con\u0000gure the trigger to start the Glue work\u0000ow based on a\n\nschedule or an event (for example, new \u0000les arriving in the S3\n\nbucket).\n\nNow let’s go through the detailed explanation of each step:\n\n1. Prepare CSV data in S3:\n\nI. Ensure you have CSV data stored in an S3 bucket. For this\n\nexample, let’s assume your CSV \u0000les are stored in\n\ns3://your-folder-name/data/.\n\nII. Download the sample_data.csv \u0000le from GitHub\n\n(https://github.com/PacktPublishing/Data-Engineering-with-\n\nAWS-Cookbook/blob/main/Chapter04/Recipe1/glue-\n\nwork\u0000ow/sample.csv).\n\nIII. Upload it to the S3 bucket (s3://your-folder-\n\nname/data/). It should present the following content:",
      "content_length": 1021,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 249,
      "content": "Figure 4.1 – The content of the sample_data.csv ﬁle\n\n2. Create an IAM role for AWS Glue:\n\nI. Choose an IAM role or create one that allows AWS Glue to\n\naccess your S3 data. To create an IAM role and policy in AWS,\n\n\u0000rst, navigate to the IAM service in the AWS Management\n\nConsole and open the IAM console at\n\nhttps://console.aws.amazon.com/iam/.\n\nII. Create the policy with the following steps:\n\ni. In the navigation pane, click on Policies.\n\nii. Click on the Create policy button.",
      "content_length": 480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 250,
      "content": "iii. Select the JSON tab and paste the policy document\n\n(available at https://github.com/PacktPublishing/Data-\n\nEngineering-with-AWS-\n\nCookbook/blob/main/Chapter04/Recipe1/glue-\n\nwork\u0000ow/glue-policy.json).\n\niv. en, create the AWSGlueServiceRole role. Select\n\nRole | Glue and then select the policy name you\n\ncreated in the previous steps.\n\nNOTE\n\nDon’t forget to update your S3 bucket name and create the same folder\n\nstructure under your S3 bucket.\n\nIn this policy, we have given the following permissions:\n\nGlueAccess:\n\nAllows most common Glue actions. You can further re\u0000ne this\n\nlist based on your speci\u0000c needs.\n\ne policy allows these actions on all Glue resources\n\n(\"Resource\": \"*\").\n\nS3Access:\n\nGrants permissions to get, put, and delete objects, list the\n\nbucket, and get the location for the speci\u0000ed bucket (my-\n\ndata-bucket).",
      "content_length": 837,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 251,
      "content": "Replace my-data-bucket with your actual bucket name.\n\nYou can extend the \"Resource\" list to include other buckets\n\nor paths within buckets as necessary.\n\n3. Create an AWS Glue database:\n\nI. Open the AWS Glue console at\n\nhttps://console.aws.amazon.com/glue/.\n\nII. Create a database with the help of the following steps:\n\ni. Navigate to the Databases section.\n\nii. Click on Add database.\n\niii. Name your database glue_workshop.\n\n4. Create an AWS Glue crawler:\n\nI. Open the AWS Glue console at\n\nhttps://console.aws.amazon.com/glue/.\n\nII. Create a Glue crawler with the help of the following steps:\n\ni. Navigate to the Crawlers section.\n\nii. Click on Add crawler.\n\niii. Name your crawler csvCrawler.\n\niv. Set the data store to S3 and specify the S3 path\n\n(s3://your-folder/data/).\n\nv. Con\u0000gure the IAM role to use\n\nAWSGlueServiceRole.",
      "content_length": 830,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 252,
      "content": "vi. Set the frequency to Run on demand.\n\nvii. Create or select a database (created in step 3) where the\n\ncrawler results will be stored, for example,\n\naws_workshop.\n\nReview and create the crawler.\n\nNOTE\n\nPlease ensure that the IAM role used by the AWS Glue crawler is the same as\n\nthe one that has the necessary permissions for creating tables in the\n\ndatabase. Use an already created role with these permissions.\n\n5. Run the crawler:\n\nI. Select the crawler, csvCrawler, and click on Run crawler.\n\nII. Wait for the crawler to complete. It will create a table in the\n\nspeci\u0000ed database.",
      "content_length": 585,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 253,
      "content": "Figure 4.2 – Crawler output\n\n6. Create an AWS Glue job:\n\nI. Open the AWS Glue console at\n\nhttps://console.aws.amazon.com/glue/.\n\nII. Create a Glue job as follows:\n\ni. Download the csv_to_parquet.py \u0000le from your\n\nGitHub for upload. You can \u0000nd scripts on following\n\nGitHub path\n\nhttps://github.com/PacktPublishing/Data-\n\nEngineering-with-AWS-\n\nCookbook/blob/main/Chapter04/Recipe1/glue-\n\nwork\u0000ow/csv_to_parquet.py.\n\nii. Go to the Jobs section in AWS Glue.\n\niii. Click on Author code with a script editor and then on\n\nScript editor.",
      "content_length": 531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 254,
      "content": "iv. en select Spark, choose Upload scripts, and add the\n\ncsv_to_parquet.py script you downloaded from\n\nGitHub.\n\nv. Click on Create script to complete the setup.\n\nFigure 4.3 – Glue ETL job\n\nvi. Fill in the following job properties:\n\nName: CsvToParquetJob (double-click, update the Glue\n\nJob name, and save)\n\nNow go to Job detail tab then click IAM Role:\n\nAWSGlueServiceRole (select from dropdown)",
      "content_length": 396,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 255,
      "content": "Type: Spark (select from dropdown)\n\nScript \u0000le name: s3://aws-glue-assets-accountNo-\n\nus-east-1/scripts/\n\nSpark UI logs path: s3://aws-glue-assets-\n\naccountNo-us-east-1/sparkHistoryLogs/\n\nTemporary directory: s3://your-folder/temp/\n\nvii. Aer starting the job, monitor its progress in the AWS Glue console.\n\nUpon completion, your Parquet data should be available in the speci\u0000ed\n\nS3 location, partitioned according to the keys you speci\u0000ed.\n\nNOTE\n\n1. In the previous script csv_to_parquet.py code don’t forget to\n\nupdate following:\n\ndatabase with the Glue database name you created as\n\nglue_workshop.\n\ntable_name with the table your crawler created (since we are using\n\nsample.csv for crawling data, your table name will be sample).\n\noutput_s3_path with your S3 bucket and folder locations. The\n\nscript path is created by the job by default if you want to change and add\n\nyour S3 bucket and path update it.\n\n2. Always monitor AWS costs associated with running AWS Glue crawlers and\n\njobs and storing data in S3. For large datasets, converting data from CSV to\n\nParquet can be cost-eﬀective, as Parquet is a columnar storage format",
      "content_length": 1130,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 256,
      "content": "optimized for analytics. Ensure you’ve set up appropriate data retention,\n\nbackup, and lifecycle policies for your S3 buckets.\n\n7. Con\u0000gure the Glue job:\n\nI. To set the Glue job script path, click on Job details, and in the\n\ndropdown, choose your S3 path: s3://your-\n\nfolder_name/scripts/csv_to_parquet.py.\n\nII. Enable job bookmarking if you want to keep track of processed\n\ndata.\n\nIII. Choose an appropriate worker type and number of workers\n\nbased on your data size and complexity.\n\nNOTE\n\nStandard workers are general-purpose workers suitable for a wide range of\n\nETL tasks; we will use the same for our recipe.",
      "content_length": 613,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 257,
      "content": "Figure 4.4 – Glue ETL job conﬁguration",
      "content_length": 38,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 258,
      "content": "IV. In Advanced property, select Job parameters and add the following\n\nvalues:\n\n--enable-metrics true\n\n--enable-job-insights =true\n\n--enable-observability-metrics =true\n\n8. Create and con\u0000gure the Glue work\u0000ow:\n\nI. Work\u0000ow creation:\n\ni. Navigate to the AWS Glue console.\n\nii. Click on the Work\u0000ows menu on the le and then\n\nclick on the Add work\u0000ow button.\n\niii. On the next screen, type in glueworkflow as the\n\nwork\u0000ow name and click on the Add work\u0000ow button.\n\ne work\u0000ow is created.\n\nII. Trigger con\u0000guration:\n\ni. Select the work\u0000ow and click on the Add trigger link.\n\nii. On the Add trigger popup, select the Add new tab.\n\nType in startcrawler as the trigger name and\n\nselect On demand for the trigger type. (You select the\n\nOn demand trigger type because you will start the\n\nwork\u0000ow manually in this workshop.) Click on the\n\nAdd button. e trigger is added to the work\u0000ow.",
      "content_length": 877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 259,
      "content": "III. Node addition:\n\ni. Click on the Add node link to con\u0000gure what you want\n\nto run aer the trigger.\n\nii. On the pop-up screen, select the Crawlers tab. Select\n\ncsvCrawler (created in step 4 Create an AWS Glue\n\ncrawler ) and click on the Add button.\n\niii. e crawler node is added as the next step to the\n\ntrigger. Next, select the Add trigger option under the\n\nAction menu to add another trigger.\n\niv. On the pop-up screen, select the Add new tab. Type in\n\nstartjob as the name. Select Event for the trigger\n\ntype. Select Start aer ANY watched event for the\n\ntrigger logic. Finally, click on the Add button. e\n\ntrigger is added.\n\nIV. Trigger and job con\u0000guration:\n\ni. Select the Start job trigger and select Add jobs crawlers\n\nto the Watch option under the Action menu.\n\nii. On the pop-up screen, select the Crawlers tab. Select\n\nSUCCEEDED for the Crawler event to watch \u0000eld.\n\nFinally, click on the Add button.\n\niii. e startjob trigger is now con\u0000gured to run when\n\nthe crawler \u0000nishes execution successfully. Click on the",
      "content_length": 1029,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 260,
      "content": "Add node icon next to startjob to con\u0000gure what job or\n\ncrawler the statjob trigger will invoke.\n\niv. On the pop-up screen, select the Jobs tab. Select\n\ncsvtoparquetjob and click on the Add button. e\n\nwork\u0000ow is now con\u0000gured end to end. It will \u0000rst run\n\nthe crawler and then the job.\n\nV. Work\u0000ow execution:\n\ni. Select gluework\u0000ow and click on the Run option under\n\nthe Action menu.\n\nii. e work\u0000ow execution will start with the status of\n\nRunning. Wait till the status changes to Completed.\n\nYou can see the crawler run in the work\u0000ow has added\n\nthe table customers under the glue-workshop\n\ndatabase.\n\nVI. Veri\u0000cation:\n\ni. Con\u0000rm that the crawler has successfully created the\n\ntable de\u0000nition for the data in the glue-workshop\n\ndatabase.",
      "content_length": 740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 261,
      "content": "Figure 4.5 – Glue workﬂow UI\n\nat’s a high-level overview of creating a simple AWS Glue\n\nwork\u0000ow. Adjust the steps based on the speci\u0000cs of your use case,\n\nsuch as the complexity of the transformations needed or additional\n\nsources and sinks.\n\nOnce the Glue work\u0000ow is \u0000nished, you can see the job’s status is\n\nshown as successful:\n\nFigure 4.6 – Glue workﬂow status\n\nYou can also check the results in the S3 bucket; go to the S3 bucket\n\nand explore the data:",
      "content_length": 458,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 262,
      "content": "Figure 4.7 – Results of the ETL job in the S3 bucket\n\nWhen you open the folder, you will see the Parquet \u0000le:\n\nFigure 4.8 – Parquet ﬁle in the S3 bucket\n\nSee also\n\nOverview of work\u0000ows in AWS Glue:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/work\u0000ows_overview.html\n\nAWS Glue Immersion day – Introduction: https://catalog.us-east-\n\n1.prod.workshops.aws/workshops/ee59d21b-4cb8-4b3d-a629-\n\n24537cf37bb5/en-US/intro",
      "content_length": 412,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 263,
      "content": "Setting up event-driven orchestration with Amazon EventBridge\n\nCreating a work\u0000ow that reacts to speci\u0000c events to trigger AWS\n\nGlue components on-demand typically requires the combination of\n\nAWS Glue with other AWS services, such as AWS Lambda and\n\nAmazon EventBridge (formerly known as Amazon CloudWatch\n\nEvents).\n\nNow, we want our work\u0000ow to react to the addition of a new \u0000le in\n\nan S3 bucket to trigger an AWS Glue job.\n\nLet’s understand when this is required. A media company processes\n\ndaily uploads of large CSV \u0000les to an S3 bucket. Each time a new\n\nCSV \u0000le is uploaded, the company needs to trigger an ETL pipeline\n\nthat extracts data from the CSV, processes it, and stores the\n\ninformation in a data lake for analysis. is work\u0000ow is fully\n\nautomated and event-driven, ensuring immediate processing upon\n\n\u0000le arrival.\n\nGetting ready\n\nBefore proceeding with this recipe, ensure you have completed the\n\nsteps in the De\u0000ning a simple work\u0000ow using AWS Glue work\u0000ows\n\nrecipe earlier in this chapter.",
      "content_length": 1007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 264,
      "content": "How to do it…\n\n1. Set up an S3 bucket event noti\u0000cation: Ensure you have an S3 bucket\n\nwhere \u0000les will be uploaded. Please use the same bucket we created in\n\nthe De\u0000ning a simple work\u0000ow using AWS Glue work\u0000ows recipe.\n\n2. Create an IAM role and policy for Glue: Please use the same policy and\n\nrole we created in the De\u0000ning a simple work\u0000ow using AWS Glue\n\nwork\u0000ows recipe.\n\n3. Create an AWS Glue database and crawler:\n\nI. Create Glue database:\n\ni. Go to the AWS Glue console at\n\nhttps://console.aws.amazon.com/glue/.\n\nii. Navigate to Databases and click on Add database.\n\niii. Name your database csv_database.\n\niv. Click on Create.\n\nNOTE\n\nPlease use the same database we created in the Deﬁning a simple workﬂow\n\nusing AWS Glue workﬂows recipe.\n\nII. Create Glue crawler:\n\ni. Go to the Crawlers section in the Glue console.\n\nii. Click on Add crawler.",
      "content_length": 851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 265,
      "content": "iii. Name your crawler csvCrawler.\n\niv. For the data store, choose S3 and specify the S3 path\n\n(s3://your-bucket/csv-data/).\n\nv. Con\u0000gure the IAM role to use glue-sample-role.\n\nvi. Set the frequency to Run on demand.\n\nvii. Create or select the csv_database database where the\n\ncrawler results will be stored.\n\nviii. Review and create the crawler.\n\nix. Do not run the crawler yet.\n\nNOTE\n\nPlease use the same crawler we created in the Deﬁning a simple workﬂow\n\nusing AWS Glue workﬂows recipe.\n\n4. Create AWS Glue job:\n\nI. Prepare the Glue ETL script: Save the script presented in the\n\nDe\u0000ning a simple work\u0000ow using AWS Glue work\u0000ows recipe as\n\ncsv_to_parquet.py and upload it to your S3 bucket.\n\nII. Create the Glue job:\n\ni. Go to the Jobs section in the Glue console.\n\nii. Click on Add Job.\n\niii. Name the job CsvToParquetJob.",
      "content_length": 826,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 266,
      "content": "iv. Choose the glue-sample-role IAM role.\n\nv. For the ETL language, select Python.\n\nvi. For the script, choose A new script to be authored by\n\nyou.\n\nvii. Set the script \u0000lename to s3://your-\n\nbucket/scripts/csv_to_parquet.py.\n\nviii. Set the temporary directory to s3://your-\n\nbucket/temp/.\n\nix. Click on Next and con\u0000gure the job properties as\n\nneeded.\n\nx. Click on Save.\n\nNOTE\n\nPlease use the same job we created in the Deﬁning a simple workﬂow using\n\nAWS Glue workﬂows recipe.\n\n5. Create and con\u0000gure the Glue work\u0000ow:\n\nI. Create work\u0000ow:\n\ni. Go to the Work\u0000ows section in the Glue console.\n\nii. Click on Add work\u0000ow.\n\niii. Name your work\u0000ow glueWorkflow.\n\niv. Click on Create.",
      "content_length": 679,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 267,
      "content": "II. Add crawler to the work\u0000ow:\n\ni. In the Glue console, select your glueWorkflow\n\nwork\u0000ow.\n\nii. Click on Add trigger and choose Add new.\n\niii. Name the trigger csvCrawlerTrigger.\n\niv. Set the type to On-demand.\n\nv. In the Actions section, add an action to start the\n\ncsvCrawler crawler.\n\nvi. Save the trigger.\n\nIII. Add job to the work\u0000ow:\n\ni. In the Glue console, select your glueWorkflow\n\nwork\u0000ow.\n\nii. Click on Add trigger and choose Add new.\n\niii. Name the trigger csvToParquetJobTrigger.\n\niv. Set the type to Event-based.\n\nv. Under Conditions, add a condition:\n\nLogical operator: EQUALS\n\nState: SUCCEEDED\n\nCrawler name: csvCrawler",
      "content_length": 636,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 268,
      "content": "vi. In the Actions section, add an action to start the job,\n\nCsvToParquetJob.\n\nvii. Save the trigger.\n\nNOTE\n\nPlease use the same workﬂow we created in the Deﬁning a simple workﬂow\n\nusing AWS Glue workﬂows recipe.\n\n6. Create an EventBridge rule to trigger the work\u0000ow:\n\nI. Create EventBridge rule:\n\ni. Go to the Amazon EventBridge console at\n\nhttps://console.aws.amazon.com/events/.\n\nii. Click on Create rule.\n\niii. Name your rule S3FileUploadRule].\n\niv. Select Rule with an event pattern and click on the Next\n\nbutton.",
      "content_length": 518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 269,
      "content": "Figure 4.9 – EventBridge rule\n\nv. Choose Event source on next page and select AWS events or\n\nEventBridge partner events.\n\nFigure 4.10 – EventBridge rule event source\n\nvi. Now choose Custom pattern JSON editor:",
      "content_length": 209,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 270,
      "content": "Figure 4.11 – EventBridge patterns\n\nvii. en, on the next page, set Creation method as Custom pattern (JSON\n\neditor) and add the following JSON in the input box:\n\n{ \"source\": [\"aws.s3\"], \"detail-type\": [\"Object Created\"], \"detail\": {",
      "content_length": 233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 271,
      "content": "\"bucket\": { \"name\": [\"your-bucket\"] }, \"object\": { \"key\": [{\"prefix\": \"data/\"}] } }}\n\nNOTE\n\nReplace your-bucket with your bucket name.\n\nviii. Click on Save.\n\nix. In the Targets section, click on Add target and select AWS Lambda\n\nfunction.\n\nx. Choose the Lambda function you created to start the Glue work\u0000ow\n\n(TriggerGlueWork\u0000ow).\n\nxi. Click on Create.\n\n7. Create AWS Lambda function to start Glue work\u0000ow:\n\nI. Create Lambda function:\n\ni. Go to the AWS Lambda console at\n\nhttps://console.aws.amazon.com/lambda/.\n\nii. Click on Create function.\n\niii. Choose Author from scratch.\n\niv. Enter a function name, for example,\n\nTriggerGlueWorkflow-lambda.\n\nv. Choose the Python 3.x runtime.",
      "content_length": 681,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 272,
      "content": "vi. Choose or create an execution role and ensure it uses\n\nlambda-glue-trigger-role (choose Use an\n\nexisting role in lambda). You need to go to IAM, open\n\nlambda-glue-trigger-role, and add\n\nAWSGlueConsoleFullAccess.\n\nFigure 4.12 – Lambda Glue trigger role\n\nvii. Click on Create function.\n\nII. Add Lambda function code: Replace the default Lambda function code\n\nwith the following:\n\nimport json import boto3 def lambda_handler(event, context): glue = boto3.client('glue') # Start the Glue workflow workflow_name = 'glueworkflow' response = glue.start_workflow_run(Name=workflow_name) return {",
      "content_length": 591,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 273,
      "content": "'statusCode': 200, 'body': json.dumps('Workflow started: {}'.format(response['RunId'])) }\n\nIII. Add S3 trigger to Lambda function:\n\ni. In the Lambda function, go to the Con\u0000guration tab.\n\nii. Click on Add trigger.\n\niii. Select S3.\n\niv. Choose the bucket name (your-bucket).\n\nv. Set the event type to All object create events.\n\nvi. Set the pre\u0000x to csv-data/ (or whatever path you want to\n\nmonitor).\n\nvii. Click on Add.",
      "content_length": 418,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 274,
      "content": "Figure 4.13 – Lambda function\n\nUpon uploading the \u0000le to the S3 bucket, a Lambda function will be\n\ntriggered to initiate a Glue work\u0000ow using Boto3.\n\nCreating a data workflow using AWS Step Functions\n\nAWS Step Functions is a serverless work\u0000ow orchestration service\n\nthat enables you to connect and manage various AWS services\n\nwithin a streamlined work\u0000ow. is allows for the rapid\n\ndevelopment and updating of applications.",
      "content_length": 425,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 275,
      "content": "Step Functions employs a state machine approach, where each step\n\nin the work\u0000ow is represented by a state. ese states are de\u0000ned\n\nusing the Amazon States Language, a JSON-based format. By\n\nchaining together diﬀerent states, you can create complex\n\nwork\u0000ows that integrate services such as AWS Lambda, AWS\n\nFargate, and Amazon SageMaker to build powerful applications.\n\nIn this recipe, we’ll create a simple AWS Step Functions work\u0000ow\n\nthat uses two AWS Lambda functions: one that gets data and one\n\nthat processes data.\n\nHow to do it…\n\n1. Create two Lambda functions:\n\nI. Go to the AWS Management Console.\n\nII. Navigate to Lambda by searching for Lambda in the services\n\nsearch bar.\n\nIII. Create a new Lambda function\n\nIV. In the Lambda Console, click on Create function.\n\nV. Select Author from scratch.\n\nVI. Enter the function name (for example, GetDataFunction).\n\nVII. For Runtime, select Python 3.x from the dropdown menu.\n\nVIII. Under Permissions, choose or create a role with necessary\n\nLambda execution permissions (for example, basic permissions",
      "content_length": 1054,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 276,
      "content": "to write logs to CloudWatch).\n\nIX. In the Function code section, you can edit the default Python\n\ncode and replace it with following code same step repeat for\n\nProcessDataFunction.\n\nX. Once you have written the code and con\u0000gured the settings,\n\nclick on Deploy to save and activate the function.\n\ne \u0000rst Lambda function is GetDataFunction. Use the\n\nPython 3.x runtime for this:\n\ndef lambda_handler(event, context): # Sample data fetching logic data = { \"message\": \"Hello from GetDataFunction\" } return data\n\ne second Lambda function is ProcessDataFunction, again\n\nusing the Python 3.x runtime:\n\ndef lambda_handler(event, context): # Sample data processing logic processed_data = event['message'].upper() return { \"processedMessage\": processed_data }\n\n2. De\u0000ne a state machine in Amazon States Language:\n\nNow, create a state machine with the help of the following JSON\n\nthat \u0000rst invokes GetDataFunction and then",
      "content_length": 913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 277,
      "content": "ProcessDataFunction:\n\n{ \"Comment\": \"A simple AWS Step Functions state machine that invokes two Lambda functions.\", \"StartAt\": \"GetDataState\", \"States\": { \"GetDataState\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:Get DataFunction\", \"Next\": \"ProcessDataState\" }, \"ProcessDataState\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:Pro cessDataFunction\", \"End\": true } }}\n\nNOTE\n\nReplace REGION and ACCOUNT_ID with your AWS Region and account\n\nID, respectively.\n\n3. Create the Step Functions state machine:\n\nI. Download stepfunction.json from GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-\n\nAWS-Cookbook/tree/main/Chapter04/Recipe3/stepfunction.",
      "content_length": 719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 278,
      "content": "II. Open the AWS Management Console and navigate to step\n\nfunction or navigate to https://us-east-\n\n1.console.aws.amazon.com/states.\n\nIII. Click on the Create state machine button and select Blank.\n\nIV. Click on the Action dropdown and select Import de\u0000nition.\n\nSelect your \u0000le and upload it. Your step function will be ready\n\naer upload.\n\nV. e other option is to navigate to the AWS Step Functions\n\ndashboard and create a new state machine. Paste the Amazon\n\nStates Language JSON from the previous step into the state\n\nmachine de\u0000nition. Ensure your AWS Step Functions role has\n\npermission to invoke Lambda functions.\n\nYou can view the JSON data and its corresponding visual\n\nrepresentation in the Work\u0000ow studio.",
      "content_length": 716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 279,
      "content": "Figure 4.14 – Step function with JSON code\n\n4. Execute the state machine: Once the state machine is created, you can\n\nstart executing the step function (click on Start execution, enter {}, and\n\nthen click on the Start execution button).\n\nFigure 4.15 – Start step function execution\n\nis will run GetDataFunction, take its output, and pass it as an\n\ninput to ProcessDataFunction.\n\nAWS Step Functions oﬀers a clear visual representation of your\n\nwork\u0000ow, making it easier to follow the execution process. As the\n\nstate machine runs, each step is highlighted in real time, allowing\n\nyou to track progress visually. Additionally, for each step in the\n\nwork\u0000ow, you can view detailed input and output data, helping you\n\nunderstand how the information \u0000ows and changes throughout the\n\nexecution.",
      "content_length": 789,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 280,
      "content": "5. Work\u0000ow Studio in AWS Step Functions: Work\u0000ow Studio is a visual\n\ntool that allows users to design work\u0000ows by dragging and dropping\n\nAWS services onto a canvas. is graphical interface provides an easy\n\nway for those who might not be familiar with Amazon States Language\n\n(ASL) or prefer visual design overwriting code.\n\nHere’s a basic overview of how to use Work\u0000ow Studio:\n\nI. Navigate to AWS Step Functions:\n\ni. Open the AWS Management Console.\n\nii. Navigate to the Step Functions service.\n\nII. Create a new state machine:\n\ni. Click on Create state machine, select Blank, and click\n\non Create.\n\nii. Choose Design with Work\u0000ow Studio.\n\nIII. Design your work\u0000ow:\n\ni. You’ll be provided with a blank canvas. On the right\n\nside of the interface, there’s a palette of AWS services\n\nand \u0000ow control elements.\n\nii. Simply drag and drop services or elements onto the\n\ncanvas.\n\niii. For each service or element, con\u0000gure its parameters\n\nusing the properties panel on the right side.",
      "content_length": 980,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 281,
      "content": "IV. Connect the elements:\n\ni. Click on the output connector (a small circle) of an\n\nelement and drag it to the input connector of another\n\nelement to establish a transition between them.\n\nii. You can also add error catchers, retries, and other \u0000ow\n\ncontrol mechanisms.\n\nV. Deploy the work\u0000ow:\n\ni. Once you’ve designed the work\u0000ow visually, click on\n\nNext. Work\u0000ow Studio will automatically generate the\n\nASL de\u0000nition for your work\u0000ow.\n\nii. Provide a name for the state machine.\n\niii. De\u0000ne or select an IAM role for the state machine to\n\nuse.\n\niv. Click on Create state machine.\n\nVI. Execute and monitor the work\u0000ow: Aer creating your\n\nwork\u0000ow, you can initiate its execution to test its functionality.\n\nAWS Step Functions oﬀers a visual representation of the\n\nwork\u0000ow, allowing you to observe the progress as each step is\n\nexecuted. While we’ve previously shown how to create a Step\n\nFunction using JSON, you can also utilize a more intuitive\n\ndrag-and-drop interface for a simpli\u0000ed work\u0000ow creation\n\nprocess.",
      "content_length": 1013,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 282,
      "content": "See also\n\nAWS Step Functions: https://aws.amazon.com/step-functions/\n\nUsing Amazon States Language to de\u0000ne Step Functions work\u0000ows:\n\nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-\n\namazon-states-language.html\n\nCreating a work\u0000ow with Work\u0000ow Studio in Step Functions:\n\nhttps://docs.aws.amazon.com/step-functions/latest/dg/work\u0000ow-studio-\n\nuse.html\n\nNew – AWS Step Functions Work\u0000ow Studio – A Low-Code Visual Tool\n\nfor Building State Machines: https://aws.amazon.com/blogs/aws/new-\n\naws-step-functions-work\u0000ow-studio-a-low-code-visual-tool-for-\n\nbuilding-state-machines/\n\nManaging data pipelines with MWAA\n\nMWAA is a fully managed service provided by AWS that simpli\u0000es\n\nthe deployment and operation of Apache Air\u0000ow, an open source\n\nwork\u0000ow automation platform. Apache Air\u0000ow is widely used for\n\norchestrating complex data work\u0000ows, scheduling batch jobs, and\n\nmanaging data pipelines. MWAA takes the power of Apache\n\nAir\u0000ow and makes it easier to use, maintain, and scale in the AWS\n\ncloud environment.\n\nMWAA is commonly used to orchestrate complex data pipelines.\n\nUsers can de\u0000ne and schedule tasks to transform, process, and",
      "content_length": 1146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 283,
      "content": "move data between various AWS services, databases, and external\n\nsystems. Another good use case is that MWAA simpli\u0000es the\n\nmanagement of ETL work\u0000ows. Users can easily schedule and\n\nautomate data extraction, transformation, and loading tasks,\n\nensuring data accuracy and consistency. MWAA also supports\n\nbatch processing and batch jobs, such as data aggregation, report\n\ngeneration, and data synchronization, which can be eﬃciently\n\nmanaged and scheduled using MWAA.\n\nHow to do it…\n\n1. Setting up your MWAA environment: Before you can start orchestrating\n\ndata pipelines with MWAA, you need to set up your environment:\n\nI. Create IAM roles and policies:\n\ni. Go to the IAM console at\n\nhttps://console.aws.amazon.com/iam/.\n\nii. Navigate to Policies and click on Create policy.\n\niii. Select the JSON tab and paste the following policy\n\ndocument:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"glue:*\",",
      "content_length": 929,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 284,
      "content": "\"s3:GetObject\", \"s3:PutObject\", \"s3:ListBucket\",\n\n\"logs:CreateLogGroup\",\n\n\"logs:CreateLogStream\",\n\n\"logs:PutLogEvents\",\n\n\"cloudwatch:PutMetricData\" ], \"Resource\": [ \"arn:aws:s3::your- mwaa-bucket\", \"arn:aws:s3::your- mwaa-bucket/*\",\n\n\"arn:aws:logs:*:*:*\",\n\n\"arn:aws:glue:*:*:catalog\",\n\n\"arn:aws:glue:*:*:database/*\",\n\n\"arn:aws:glue:*:*:table/*\",\n\n\"arn:aws:glue:*:*:connection/*\",\n\n\"arn:aws:glue:*:*:job/*\",\n\n\"arn:aws:glue:*:*:crawler/*\",\n\n\"arn:aws:glue:*:*:workflow/*\",\n\n\"arn:aws:glue:*:*:trigger/*\",",
      "content_length": 500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 285,
      "content": "\"arn:aws:glue:*:*:classifier/*\",\n\n\"arn:aws:cloudwatch:*:*:metric/*\" ] } ]}\n\nNOTE\n\nUpdate your S3 bucket.\n\n2. Create an IAM role for MWAA:\n\nI. Go to the IAM console, navigate to Roles, and click on Create\n\nrole.\n\nII. Select AWS service and then choose MWAA.\n\nIII. Click on Next: Permissions.\n\nIV. Attach the policy you just created.\n\nV. Click on Next: Tags, optionally add tags, then click on Next:\n\nReview.\n\nVI. Name your role mwaa-service-role.\n\nVII. Click on Create role.\n\n3. Create MWAA environment:\n\nI. Go to the MWAA console at https://us-east-\n\n1.console.aws.amazon.com/mwaa/home?region=us-east-\n\n1#home?landingPageCheck=1.\n\nII. Click on Create environment.",
      "content_length": 663,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 286,
      "content": "III. Enter the environment details, such as name and description.\n\nIV. For DAG folder S3 Path, enter s3://your-mwaa-\n\nbucket/dag.\n\nV. For Plugins \u0000le S3 Path, enter s3://your-mwaa-\n\nbucket/plugins (optional). You can \u0000nd details about\n\nplugins on AWS Documents\n\n(https://docs.aws.amazon.com/mwaa/latest/userguide/con\u0000guri\n\nng-dag-import-plugins.html).",
      "content_length": 351,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 287,
      "content": "Figure 4.16 – DAG conﬁguration\n\nVI. For Requirements \u0000le S3 Path, enter s3://your-mwaa-\n\nbucket/requirements.txt (optional). You can \u0000nd details about",
      "content_length": 150,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 288,
      "content": "reuirments.txt in AWS documents\n\n(https://docs.aws.amazon.com/mwaa/latest/userguide/best-practices-\n\ndependencies.html).\n\nVII. Select the mwaa-service-role you created earlier in the Create\n\nIAM roles and policies section at the beginning of step 1.\n\nFigure 4.17 – Add role in conﬁguration\n\nVIII. Con\u0000gure networking settings (VPC, subnets, and security groups).",
      "content_length": 362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 289,
      "content": "Figure 4.18 – Networking conﬁguration\n\nIX. Select Create new VPC. Wait for a few minutes. Once the VPC is\n\ncreated, select New; you will see that a subnet will be automatically",
      "content_length": 176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 290,
      "content": "assigned. Now, choose VPC security group and Public network (internet\n\naccess).\n\nNOTE\n\nBefore you start MWAA, create an S3 bucket that will store your Directed\n\nAcyclic Graphs (DAGs). This bucket will also store MWAA logs and metrics.\n\nX. Logging con\u0000guration: Specify the S3 pre\u0000x where you want logs to be\n\nstored in your S3 bucket and enable the checkbox in the Amazon\n\nManaged Work\u0000ows for Apache Air\u0000ow (MWAA) UI.\n\nXI. Air\u0000ow con\u0000guration options: If needed, override the default\n\nairflow.cfg settings in the CFG \u0000le.\n\nXII. Permissions: Add permissions using the following two steps:\n\ni. Attach an execution role that allows MWAA to access necessary\n\nresources (download the policy from GitHub and attach it to\n\nyour role).\n\nii. Attach a task role if your tasks need to access other AWS\n\nservices.\n\niii. Click on Create environment. is can take a few minutes.\n\nFigure 4.19 – MWAA environment created",
      "content_length": 905,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 291,
      "content": "XIII. Access the Air\u0000ow UI: Once the environment is created, you can access\n\nthe Air\u0000ow UI through the link provided in the MWAA console.\n\nFigure 4.20 – MWAA UI\n\nXIV. Once your UI is visible, download dag-sample.py from GitHub\n\n(https://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/blob/main/Chapter04/Recipe4/mwaa/dag-sample.py) and\n\nupload it into the dags folder in your S3 bucket. Your uploaded DAGs\n\nwill be visible in a few minutes:\n\nFigure 4.21 – MWAA sample DAG\n\nXV. Clicking on the MWAA DAG will display a graph view. Explore the user\n\ninterface by interacting with all available options.",
      "content_length": 616,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 292,
      "content": "Figure 4.22 – MWAA DAG\n\nNow, we will look into MWAA and its integration with AWS Glue:\n\n1. Create an S3 bucket for MWAA:\n\nI. Use the S3 bucket we created in the previous steps to upload\n\nyour DAGs (please use the steps from Create MWAA\n\nenvironment).\n\ni. Create a folder for dags in your S3 bucket.\n\nii. Upload your Air\u0000ow DAGs to the dags folder.\n\niii. Upload any custom plugins to the plugins folder.\n\niv. Upload the requirements.txt \u0000le to the root of\n\nthe bucket.\n\n2. Create an Air\u0000ow DAG to trigger AWS Glue jobs:\n\nI. Create a DAG: Save the following DAG as mwaa-glue-\n\ndag.py and upload it to your S3 bucket in the dags folder.\n\nLet’s discuss the code:",
      "content_length": 658,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 293,
      "content": "i. Introduction and initial setup: We start by importing\n\nthe required libraries: boto3 for interacting with AWS\n\nservices, airflow for orchestrating tasks, and time\n\nfor delays between task checks. en, we initialize an\n\nAWS Glue client using boto3, specifying the AWS\n\nRegion:\n\nimport boto3 from airflow import DAG from airflow.operators.python_operator import PythonOperator from airflow.utils.dates import days_ago from botocore.exceptions import ClientError import time # Initialize AWS Glue client glue_client = boto3.client('glue', region_name='us-east-1')\n\nii. Starting the Glue crawler: e start_crawler\n\nfunction initiates the Glue crawler by calling\n\nglue_client.start_crawler(). It includes\n\nerror handling using ClientError in case of\n\nfailures:\n\n# Function to start Glue crawler def start_crawler(crawler_name): try: response = glue_client.start_crawler(Name=craw",
      "content_length": 877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 294,
      "content": "ler_name) print(f\"Started crawler: {crawler_name}\") except ClientError as e: print(f\"Error starting crawler: {e}\") raise as e\n\niii. Checking Glue crawler status: is function\n\ncontinuously checks the status of the crawler and waits\n\nuntil it completes. e time.sleep(60) function\n\nintroduces a delay of 1 minute between status checks to\n\nprevent rapid polling:\n\n# Function to check the status of Glue crawler def check_crawler_status(crawler_name): while True: response = glue_client.get_crawler(Name=crawle r_name) crawler_status = response['Crawler']['State'] if crawler_status == 'READY': print(f\"Crawler {crawler_name} has completed.\") break elif crawler_status == 'RUNNING': print(f\"Crawler {crawler_name} is still running...\") time.sleep(60) # Wait",
      "content_length": 754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 295,
      "content": "for 1 minute before checking again else: raise Exception(f\"Crawler {crawler_name} encountered an error: {crawler_status}\")\n\niv. Starting the Glue ETL job: e start_glue_job\n\nfunction triggers a Glue ETL job using the\n\nstart_job_run() method and returns\n\nJobRunId, which is used later to monitor the job’s\n\nprogress:\n\n# Function to start Glue ETL job def start_glue_job(job_name): try: response = glue_client.start_job_run(JobName=j ob_name) job_run_id = response['JobRunId'] print(f\"Started Glue job: {job_name} with JobRunId: {job_run_id}\") return job_run_id except ClientError as e: print(f\"Error starting Glue job: {e}\") raise\n\nv. Checking Glue ETL job status: is function monitors\n\nthe status of the Glue ETL job using JobRunId.\n\nSimilar to the crawler status check, the function loops\n\nuntil the job either succeeds or fails:",
      "content_length": 831,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 296,
      "content": "# Function to check Glue job status def check_glue_job_status(job_name, job_run_id): while True: response = glue_client.get_job_run(JobName=job _name, RunId=job_run_id) job_status = response['JobRun']['JobRunState'] if job_status == 'SUCCEEDED': print(f\"Glue job {job_name} succeeded.\") break elif job_status in ['STARTING', 'RUNNING']: print(f\"Glue job {job_name} is still {job_status}...\") time.sleep(60) # Wait for 1 minute before checking again else: raise Exception(f\"Glue job {job_name} failed with status: {job_status}\")\n\nvi. De\u0000ning the DAG: e DAG de\u0000nes the work\u0000ow\n\nusing Air\u0000ow, including task dependencies and\n\nscheduling. Default arguments such as the start date\n\nand retry count are speci\u0000ed for the DAG:\n\n# Define the DAG default_args = { 'owner': 'airflow', 'start_date': days_ago(1), 'retries': 1",
      "content_length": 814,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 297,
      "content": "}with DAG(\n\ndag_id='glue_crawler_etl_workflow', default_args=default_args, description='A simple MWAA DAG to trigger Glue Crawler and ETL Job', schedule_interval=None, # Manual trigger for this workshop catchup=False ) as dag:\n\nvii. Adding tasks to the DAG: Each step of the work\u0000ow is\n\nrepresented as a task using Air\u0000ow’s\n\nPythonOperator\n\n(https://air\u0000ow.apache.org/docs/apache-\n\nair\u0000ow/stable/howto/operator/python.html). Tasks\n\nsuch as starting the crawler, checking its status, starting\n\nthe Glue ETL job, and monitoring the job’s status are\n\nadded to the DAG:\n\n# Task 1: Start Glue Crawler start_crawler_task = PythonOperator( task_id='start_crawler',\n\npython_callable=start_crawler, op_kwargs={'crawler_name': 'csvToCatalogCrawler'} ) # Task 2: Check Glue Crawler Status check_crawler_task = PythonOperator(",
      "content_length": 814,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 298,
      "content": "task_id='check_crawler_status',\n\npython_callable=check_crawler_statu s, op_kwargs={'crawler_name': 'csvToCatalogCrawler'} ) # Task 3: Start Glue ETL Job start_glue_job_task = PythonOperator( task_id='start_glue_job',\n\npython_callable=start_glue_job, op_kwargs={'job_name': 'CsvToParquetJob'} ) # Task 4: Check Glue ETL Job Status check_glue_job_task = PythonOperator(\n\ntask_id='check_glue_job_status',\n\npython_callable=check_glue_job_stat us, op_kwargs={ 'job_name': 'CsvToParquetJob', 'job_run_id': '{{ ti.xcom_pull(task_ids=\"start_glue_j ob\") }}' } )\n\nviii. De\u0000ning task dependencies: Task dependencies are\n\nde\u0000ned using >>, creating a sequential \u0000ow where the\n\ntasks are executed one aer the other. is ensures that",
      "content_length": 719,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 299,
      "content": "the Glue crawler runs \u0000rst, followed by the ETL job\n\naer the successful completion of the crawler:\n\n# Task dependencies: DAG Flow start_crawler_task >> check_crawler_task >> start_glue_job_task >> check_glue_job_task\n\n3. Monitor and handle failures:\n\nI. Set up CloudWatch alarms:\n\ni. Go to the Amazon CloudWatch console at\n\nhttps://console.aws.amazon.com/cloudwatch/.\n\nii. Navigate to Alarms and click on Create alarm.\n\niii. Select the Glue metrics to monitor (for example,\n\nJobRunTime).\n\niv. Set the threshold and con\u0000gure noti\u0000cations.\n\nII. Add retries and failure handling in the DAG: Update your\n\nAir\u0000ow DAG to include retries and failure handling\n\nIII. Update your existing Glue job with CloudWatch metrics and\n\nretry:\n\ni. Importing libraries and setting up the environment:\n\nis section imports the necessary libraries for the\n\nAWS Glue job. Key libraries include awsglue for\n\nworking with Glue, pyspark for handling Spark, and",
      "content_length": 934,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 300,
      "content": "boto3 for interacting with AWS services such as\n\nCloudWatch:\n\nimport sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job import boto3\n\nii. Initializing the Spark and Glue context: is code\n\nblock sets up the Spark and Glue context, initializing\n\nthe job using parameters passed from the command\n\nline. e GlueContext object allows access to Glue\n\nfeatures within Spark:\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME']) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args['JOB_NAME'], args)\n\niii. Reading data from the Glue Data Catalog: Here, we\n\nspecify the source database and table in the Glue Data\n\ncatalog from which the data will be read, and the\n\ndestination S3 path where the transformed data will be\n\nwritten:",
      "content_length": 928,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 301,
      "content": "# Read the data from the Glue Data catalog database_name = \"glue-workshop\" table_name = \"data\" output_s3_path = \"s3://mwaa-env- data-bucket-vk/parquet-data/\"\n\niv. Sending metrics to CloudWatch: is function sends\n\ncustom metrics to Amazon CloudWatch. e\n\nput_metric_data method allows us to track job\n\nsuccess or failure by pushing metric values:\n\ndef put_metric_data(metric_name, value): cloudwatch = boto3.client('cloudwatch') cloudwatch.put_metric_data( Namespace='GlueIngestion', MetricData=[ { 'MetricName': metric_name, 'Value': value, 'Unit': 'Count' }, ] )\n\nv. Creating a DynamicFrame: is part of the code reads\n\ndata from the Glue Data Catalog as a DynamicFrame.\n\nGlue uses DynamicFrames to handle semi-structured\n\ndata eﬃciently:\n\n# Create a DynamicFrame from the table dynamic_frame = glueContext.create_dynamic_frame.fr om_catalog(",
      "content_length": 844,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 302,
      "content": "database=database_name, table_name=table_name )\n\nvi. Converting to a DataFrame and writing to S3: e\n\nDynamicFrame is converted to a Spark DataFrame,\n\nwhich is then written to Amazon S3 as partitioned\n\nParquet \u0000les, using country, state, and city as\n\npartition keys:\n\n# Convert DynamicFrame to DataFrame data_frame = dynamic_frame.toDF() # Write DataFrame to S3 as partitioned Parquet files data_frame.write.mode(\"overwrite\"). partitionBy( \"country\", \"state\", \"city\").parquet(output_s3_path)\n\nvii. Adding retry logic for data processing: A retry\n\nmechanism is added to ensure the job can retry up to\n\nthree times in case of failure. If the job succeeds, it\n\nsends a success metric to CloudWatch:\n\n# Maximum number of retries max_retries = 3 retries = 0 success = False while retries < max_retries and not success: try: # Create a DynamicFrame from the table dynamic_frame = glueContext.create_dynamic_frame.fr",
      "content_length": 909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 303,
      "content": "om_catalog( database=database_name, table_name=table_name ) # Convert DynamicFrame to DataFrame data_frame = dynamic_frame.toDF() # Write DataFrame to S3 as partitioned Parquet files\n\ndata_frame.write.mode(\"overwrite\"). partitionBy( \"country\", \"state\", \"city\").parquet(output_s3_path) success = True\n\nput_metric_data('JobSuccess', 1)\n\nviii. Handling errors and sending failure metrics: If an error\n\noccurs, the job will retry aer a 30-second wait. If the\n\nmaximum retries are reached, a failure metric is\n\nsent to CloudWatch, and an exception is raised:\n\nexcept Exception as e: retries += 1 if retries < max_retries: time.sleep(30) # Wait for 30 seconds before retrying else:\n\nput_metric_data('JobFailure', 1) raise e\n\nix. Committing the job: e job.commit() call\n\n\u0000nalizes the Glue job, ensuring that all changes are",
      "content_length": 818,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 304,
      "content": "saved and the job is properly completed:\n\njob.commit()\n\nOn the MWAA UI, your MWAA DAG will be visible in a few\n\nminutes as Auto-refresh is enabled:\n\nFigure 4.23 – MWAA DAGs\n\nOn clicking on DAG, you can see Orchestration in the Flow graph:\n\nFigure 4.24 – MWAA DAG\n\nSee also",
      "content_length": 272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 305,
      "content": "To learn more about Apache Air\u0000ow and MWAA, you can refer to the\n\nfollowing links:\n\nhttps://air\u0000ow.apache.org/docs/apache-\n\nair\u0000ow/stable/index.html\n\nhttps://docs.aws.amazon.com/mwaa/latest/userguide/what-is-\n\nmwaa.html\n\nWhat Is Amazon Managed Work\u0000ows for Apache Air\u0000ow?:\n\nhttps://docs.aws.amazon.com/mwaa/latest/userguide/what-is-\n\nmwaa.html\n\nApache Air\u0000ow tutorial: https://air\u0000ow.apache.org/docs/apache-\n\nair\u0000ow/stable/tutorial/index.html\n\nInstalling custom plugins:\n\nhttps://docs.aws.amazon.com/mwaa/latest/userguide/con\u0000guring-dag-\n\nimport-plugins.html\n\nInstalling Python dependencies:\n\nhttps://docs.aws.amazon.com/mwaa/latest/userguide/working-dags-\n\ndependencies.html\n\nMonitoring your pipeline’s health\n\nTo create a robust monitoring and alerting system for your AWS\n\nGlue pipeline, you can use Amazon CloudWatch to emit success\n\nmetrics, create alarms, and monitor the health of your pipeline.",
      "content_length": 902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 306,
      "content": "We’ll use CloudWatch Metrics, CloudWatch alarms, and Amazon\n\nSNS for noti\u0000cations.\n\nHow to do it…\n\n1. Modify the AWS Glue job to emit success metrics:\n\nI. Update the Glue ETL script: Modify your\n\ncsv_to_parquet.py script to include the following code to\n\nemit custom metrics:\n\nimport sys import boto3 # Emit success metric to CloudWatch cloudwatch = boto3.client('cloudwatch') cloudwatch.put_metric_data( Namespace='GluePipelineMetrics', MetricData=[ { 'MetricName': 'JobSuccess', 'Dimensions': [ { 'Name': 'JobName', 'Value': args['JOB_NAME'] }, ], 'Value': 1, 'Unit': 'Count' }, ])\n\nII. Upload the updated script to your S3 bucket using the following\n\nAWS CLI command:",
      "content_length": 670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 307,
      "content": "aws s3 cp csv_to_parquet.py s3://your- bucket/scripts/csv_to_parquet.py\n\n2. Create CloudWatch alarms for metrics:\n\nI. Create an SNS topic for noti\u0000cations:\n\ni. Go to the Amazon SNS console at\n\nhttps://console.aws.amazon.com/sns/.\n\nii. Click on Topic and then click on Create topic.\n\niii. Choose Standard and enter a name for your topic, for\n\nexample, GluePipelineAlerts.\n\niv. Click on Create topic.\n\nv. Copy the topic ARN (for example,\n\narn:aws:sns:region:account-\n\nid:GluePipelineAlerts).\n\nvi. Click on Create subscription.\n\nvii. Set Protocol as Email and enter your email address.\n\nviii. Click on Create subscription.\n\nII. Create CloudWatch alarms:\n\ni. Go to the Amazon CloudWatch console at\n\nhttps://console.aws.amazon.com/cloudwatch/.\n\nii. Navigate to Alarms and click on Create alarm.\n\niii. Click on Select metric in cloudWatch.\n\niv. Choose Glue from the list of services.",
      "content_length": 877,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 308,
      "content": "v. en, select CloudWatch Metrics to view the available\n\nmetrics for your Glue jobs.\n\nvi. Choose CsvToParquetJob.\n\nvii. Click on Select metric and proceed to con\u0000gure the\n\nalarm.\n\nviii. Con\u0000gure the threshold type to Static and set the\n\ncondition to reshold < 1 for 1 consecutive period(s).\n\nix. Set the period to match your Glue job’s frequency (for\n\nexample, 1 hour).\n\nx. Click on Next.\n\nxi. Con\u0000gure the actions as follows:\n\nChoose In alarm and select Send noti\u0000cation\n\nto.\n\nSelect your SNS topic, GluePipelineAlerts\n\n(created previously in Create an SNS topic for\n\nnoti\u0000cations).\n\nxii. Click on Next.\n\nxiii. Name your alarm (for example,\n\nGlueJobSuccessAlarm).\n\nxiv. Click on Create alarm.\n\nYou can \u0000nd all alerts in the CloudWatch UI:",
      "content_length": 740,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 309,
      "content": "Figure 4.25 – Cloudwatch alarms\n\n3. Monitor queue size metrics: To monitor the size of queues that are part\n\nof your data pipeline, you might be using Amazon SQS or other queue\n\nservices. Here, we’ll focus on monitoring an SQS queue:\n\nI. Create SQS queue (if not already created):\n\ni. Go to the Amazon SQS console at\n\nhttps://console.aws.amazon.com/sqs/.\n\nii. Click on Create queue.\n\niii. Enter the queue name (for example,\n\nGluePipelineQueue).\n\niv. Con\u0000gure the queue settings as required.\n\nv. Click on Create queue.\n\nII. Monitor SQS queue metrics:",
      "content_length": 549,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 310,
      "content": "i. Go to the Amazon CloudWatch console at\n\nhttps://console.aws.amazon.com/cloudwatch/.\n\nii. Navigate to Metrics.\n\niii. Choose Metrics and select the SQS namespace.\n\niv. Select the metrics for your queue (for example,\n\nApproximateNumberOfMessagesVisible).\n\nv. Create a dashboard to monitor these metrics by\n\nclicking on Add to dashboard.\n\nvi. Con\u0000gure the dashboard with relevant widgets to\n\nmonitor your queue size.\n\nIII. Create CloudWatch alarms for SQS queue:\n\ni. Go to the Amazon CloudWatch console at\n\nhttps://console.aws.amazon.com/cloudwatch/.\n\nii. Navigate to Alarms and click on Create alarm.\n\niii. Click on Select metric and choose the SQS namespace.\n\niv. Select the\n\nApproximateNumberOfMessagesVisible\n\nmetric for your queue.\n\nv. Click on Select metric and proceed to con\u0000gure the\n\nalarm.\n\nvi. Con\u0000gure the threshold type to Static and set the\n\ncondition (for example, reshold > 100 for 5\n\nconsecutive period(s)).",
      "content_length": 924,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 311,
      "content": "vii. Set the period (for example, 1 minute).\n\nviii. Click on Next.\n\nix. Con\u0000gure the actions as follows:\n\nChoose In alarm and select Send noti\u0000cation\n\nto.\n\nSelect your SNS topic, GluePipelineAlerts.\n\nx. Click on Next.\n\nxi. Name your alarm (for example,\n\nSQSQueueSizeAlarm).\n\nxii. Click on Create alarm.\n\nYou can \u0000nd all alerts in CloudWatch UI:\n\nFigure 4.26 – CloudWatch alarm SQS\n\nTo monitor AWS Glue job performance eﬀectively, setting up\n\nCloudWatch alarms with SNS for AWS Glue metrics provides a",
      "content_length": 500,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 312,
      "content": "proactive way to track and manage your data pipeline. By\n\ncon\u0000guring alarms for key Glue job metrics (for example, job\n\nfailures or long runtimes) and integrating them with SNS, you can\n\nreceive instant noti\u0000cations when thresholds are exceeded. is\n\nensures that any issues in your pipeline are addressed promptly,\n\nmaintaining the reliability and health of your data processes.\n\nSetting up a pipeline using AWS Glue to ingest data from a JDBC database into a catalog table\n\nCreating a full pipeline using AWS Glue to ingest data from a\n\nrelational database on a regular basis involves setting up the\n\nnecessary components such as a Glue job, Glue crawler, and a retry\n\nmechanism to handle transient errors. In this recipe, we are going\n\nto use the AWS Glue job with EventBridge and Step Functions\n\nwork\u0000ow. We will read data from a relational database and store it\n\nin an S3 bucket.\n\nHow to do it…\n\n1. Set up your environment:\n\nI. Use your existing S3 bucket or create a new one. (To create a\n\nnew S3 bucket, navigate to the S3 service in the AWS\n\nManagement Console, click on Create bucket, and specify a",
      "content_length": 1107,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 313,
      "content": "unique name. Choose the region and con\u0000gure settings such as\n\nversioning or encryption as needed, then click on Create.)\n\nII. Create an RDS MySQL instance (please use the following link\n\nand follow the given instructions:\n\nhttps://aws.amazon.com/getting-started/hands-on/create-\n\nmysql-db/).\n\nIII. Once the database is created, download mysql-\n\nscripts.txt, connect it to your RDS (MySQL) instance,\n\nand run it.",
      "content_length": 411,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 314,
      "content": "Figure 4.27 – MySQL editor with query and results\n\n2. Create IAM roles and policies:\n\nI. Create an IAM policy for Glue:\n\ni. Go to the IAM console at\n\nhttps://console.aws.amazon.com/iam/.\n\nii. Navigate to Policies and click on Create policy.",
      "content_length": 240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 315,
      "content": "iii. Select the JSON tab and paste the policy document.\n\nPlease use the same policy and role we created in the\n\nDe\u0000ning a simple work\u0000ow using AWS Glue work\u0000ows\n\nrecipe.\n\nII. Create an IAM role for Glue:\n\ni. Go to the IAM console, navigate to Roles, and click on\n\nCreate role.\n\nii. Select AWS service and then choose Glue.\n\niii. Click on Next: Permissions.\n\niv. Attach the policy you just created (use the same policy\n\nas in De\u0000ning a simple work\u0000ow using AWS Glue\n\nwork\u0000ows).\n\nv. Click on Next: Tags, optionally add tags, then click on\n\nNext: Review.\n\nvi. Name your role AWSGlueServiceRole. Use the\n\nsame role as in De\u0000ning a simple work\u0000ow using AWS\n\nGlue work\u0000ows.\n\nvii. Click on Create role.\n\n3. Create an AWS Glue database:\n\nI. Open the AWS Glue console at\n\nhttps://console.aws.amazon.com/glue/.\n\nII. Create a database with the help of the following steps:\n\ni. Navigate to the Databases section.",
      "content_length": 900,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 316,
      "content": "ii. Click on Add database.\n\niii. Name your database relational_database.\n\n4. Create an AWS Glue connection:\n\nI. Create Glue connection:\n\ni. Go to the AWS Glue console at\n\nhttps://console.aws.amazon.com/glue/.\n\nii. Navigate to Connections and click on Add connection.\n\niii. Choose the connection type as JDBC and click on\n\nNext.\n\niv. Fill in the connection details, including the JDBC\n\nURL, username, and password for your relational\n\ndatabase.\n\nv. Name your connection glue-rds-Jdbc-\n\nconnection.\n\nTest your connection using Test Connection:\n\nFigure 4.28 – Testing your connection",
      "content_length": 580,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 317,
      "content": "When the connection is successful, you will see the following\n\nmessage:\n\nFigure 4.29 – JDBC connection test\n\nvi. Click on Finish.\n\n5. Create AWS Glue crawler:\n\nI. Create Glue crawler:\n\ni. Go to the Crawlers section in the Glue console.\n\nii. Click on Add crawler.\n\niii. Name your crawler RelationalDataCrawler.\n\niv. For the data store, choose JDBC and select your\n\nconnection, glue-rds-Jdbc-connection.\n\nv. Include the EMPLOYEE_DB/EMPLOYEE% path and\n\nclick on the Add datasource button.\n\nvi. Specify the database and table name to crawl.\n\nvii. Con\u0000gure the IAM role to use\n\nAWSGlueServiceRole.",
      "content_length": 592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 318,
      "content": "viii. Set the frequency to run on demand.\n\nix. Create or select the relational_database\n\ndatabase where the crawler results will be stored.\n\nx. Review and create the crawler.\n\n6. Create an AWS Glue job with the retry mechanism:\n\nI. Download the jdbc_to_s3.py script from GitHub and\n\nupload it to your S3 bucket:\n\ni. Importing libraries and de\u0000ning the metric function:\n\nis section imports the necessary libraries such as\n\nawsglue, pyspark, boto3, and time. e\n\nput_metric_data function allows us to send\n\ncustom metrics (for example, job success or failure) to\n\nAmazon CloudWatch to monitor job performance:\n\nimport sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job import boto3 import time def put_metric_data(metric_name, value): cloudwatch = boto3.client('cloudwatch')",
      "content_length": 913,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 319,
      "content": "cloudwatch.put_metric_data( Namespace='GlueIngestion', MetricData=[ { 'MetricName': metric_name, 'Value': value, 'Unit': 'Count' }, ] )\n\nii. Setting up the Glue and Spark context: is block\n\ninitializes the Spark and Glue contexts. e\n\ngetResolvedOptions function fetches the job\n\nname passed as a command-line argument, and Job is\n\ninitialized for tracking and managing the Glue job:\n\nargs = getResolvedOptions(sys.argv, ['JOB_NAME']) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args['JOB_NAME'], args)\n\niii. De\u0000ning the data source and destination: database\n\nand table_name specify the Glue Data Catalog from\n\nwhich the data is pulled. output_s3_path points\n\nto the Amazon S3 bucket where the ingested data will\n\nbe written in the Parquet format:\n\ndatabase = \"relational_data\" table_name = \"your_table_name\" output_s3_path = \"s3://your-glue- data-bucket/ingested-data/\"",
      "content_length": 943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 320,
      "content": "iv. Implementing retry logic for data ingestion: e retry\n\nmechanism allows the job to retry up to three times if\n\nit encounters an issue. A DynamicFrame is created\n\nfrom the Glue Data Catalog, which enables handling\n\nsemi-structured data:\n\nmax_retries = 3 retries = 0 success = False while retries < max_retries and not success: try: dynamic_frame = glueContext.create_dynamic_frame.fr om_catalog( database=database, table_name=table_name,\n\ntransformation_ctx=\"dynamic_frame\" )\n\nv. Converting data and writing to S3: e DynamicFrame\n\nis converted into a Spark DataFrame for easier\n\nmanipulation. e DataFrame is written to S3 in the\n\nParquet format, partitioned by the speci\u0000ed columns.\n\nOn success, a metric is sent to CloudWatch:\n\ndata_frame = dynamic_frame.toDF()\n\ndata_frame.write.mode(\"overwrite\"). parquet(output_s3_path) success = True put_metric_data('JobSuccess', 1)",
      "content_length": 876,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 321,
      "content": "vi. Handling exceptions and sending failure metrics: If an\n\nexception occurs, the retry count is incremented, and\n\nthe process waits for 30 seconds before trying again. If\n\nthe retries are exhausted, a failure metric is sent to\n\nCloudWatch, and an exception is raised:\n\nexcept Exception as e: retries += 1 if retries < max_retries: time.sleep(30) # Wait for 30 seconds before retrying else:\n\nput_metric_data('JobFailure', 1) raise e\n\nvii. Finalizing the Glue job: e job.commit() method\n\ncommits the job, marking its completion in the AWS\n\nGlue environment, ensuring that all job metadata is\n\n\u0000nalized:\n\njob.commit()\n\nII. Create the Glue job:\n\ni. Go to the Jobs section in the Glue console.\n\nii. Click on Add Job.\n\niii. Name the job JdbcToS3Job.\n\niv. Choose the GlueServiceRole IAM role.\n\nv. For the ETL language, select Python.",
      "content_length": 828,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 322,
      "content": "vi. For the script, choose A new script to be authored by\n\nyou.\n\nvii. Set the script \u0000lename to s3://your-glue-\n\ndata-bucket/scripts/jdbc_to_s3.py.\n\nviii. Set the temporary directory to s3://your-glue-\n\ndata-bucket/temp/.\n\nix. Click on Next and con\u0000gure the job properties as\n\nneeded.\n\nx. Click on Save.\n\n7. Create an AWS Step Functions state machine:\n\nI. Download and use the following state machine JSON de\u0000nition\n\nto coordinate the work\u0000ow:\n\ni. Initial setup and work\u0000ow start: e state machine\n\nstarts with the StartAt directive, which speci\u0000es the\n\n\u0000rst step, \"StartCrawler\". \"StartCrawler\" is\n\na Task state that uses the glue:startCrawler\n\nresource to start an AWS Glue crawler named\n\n\"RelationalDataCrawler\":\n\n{ \"Comment\": \"A description of my state machine\", \"StartAt\": \"StartCrawler\", \"States\": { \"StartCrawler\": { \"Type\": \"Task\", \"Resource\":",
      "content_length": 851,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 323,
      "content": "\"arn:aws:states::aws- sdk:glue:startCrawler\", \"Parameters\": { \"Name\": \"RelationalDataCrawler\" },Next\": \"WaitForCrawler\" },\n\nii. Adding a wait step: e \"WaitForCrawler\" state\n\nis a Wait state that pauses the work\u0000ow for 60\n\nseconds before proceeding to the next step. is\n\nensures the state machine allows the Glue crawler\n\nenough time to start and process the data:\n\n\"WaitForCrawler\": { \"Type\": \"Wait\", \"Seconds\": 60, \"Next\": \"CheckCrawlerStatus\" },\n\niii. Checking the crawler status: e\n\n\"CheckCrawlerStatus\" state is another Task\n\nstate – this time, using the glue:getCrawler\n\nresource to retrieve the current status of the crawler.\n\ne crawler’s status is critical to determine whether it\n\nhas \u0000nished, is still running, or encountered an error:\n\n\"CheckCrawlerStatus\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:states:::aws- sdk:glue:getCrawler\", \"Parameters\": { \"Name\":",
      "content_length": 870,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 324,
      "content": "\"RelationalDataCrawler\" }, \"Next\": \"CrawlerComplete?\" },\n\niv. Conditional check for crawler completion: e\n\n\"CrawlerComplete?\" state is a Choice state that\n\nevaluates the status of the crawler. If the status is\n\n\"READY\", it means the crawler has \u0000nished and the\n\nwork\u0000ow proceeds to the next task,\n\n\"StartGlueJob\". If not, it defaults back to the\n\n\"WaitForCrawler\" state to wait and check again:\n\n\"CrawlerComplete?\": { \"Type\": \"Choice\", \"Choices\": [ { \"Variable\": \"$.Crawler.State\", \"StringEquals\": \"READY\", \"Next\": \"StartGlueJob\" } ], \"Default\": \"WaitForCrawler\" },\n\nv. Starting the Glue job: e \"StartGlueJob\" state is\n\nthe \u0000nal Task that triggers the AWS Glue job using\n\nthe glue:startJobRun resource. e job named\n\n\"JdbcToS3Job\" is started to process and load the\n\ndata, and the state machine ends aer this step\n\n(\"End\": true). is state machine orchestrates a\n\nsimple data processing pipeline using AWS Glue. It\n\n\u0000rst starts a Glue crawler to scan the data, waits for the",
      "content_length": 977,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 325,
      "content": "crawler to complete, and then triggers a Glue ETL job\n\nto process the ingested data. By using Task, Wait,\n\nand Choice states, the work\u0000ow is controlled and\n\nmonitored eﬃciently:\n\n\"StartGlueJob\": { \"Type\": \"Task\", \"Resource\": \"arn:aws:states:::aws- sdk:glue:startJobRun\", \"Parameters\": { \"JobName\": \"JdbcToS3Job\" }, \"End\": true } }}\n\nII. Create the state machine:\n\ni. Go to the AWS Step Functions console at\n\nhttps://console.aws.amazon.com/states/.\n\nii. Click on Create state machine.\n\niii. Choose Author with code snippets.\n\niv. Name your state machine\n\nGlueIngestionStateMachine.\n\nv. Paste the state machine de\u0000nition into the editor.\n\nvi. Choose the step-functions-role execution\n\nrole.\n\nvii. Click on Create state machine.\n\n8. Create an EventBridge rule to trigger the state machine:",
      "content_length": 786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 326,
      "content": "I. Create an EventBridge rule:\n\ni. Go to the Amazon EventBridge console at\n\nhttps://console.aws.amazon.com/events/.\n\nii. Click on Create rule.\n\niii. Name your rule S3FileUploadRule.\n\niv. Choose Event Source and select Event Pattern.\n\nv. Click on Edit and specify the following JSON event\n\npattern:\n\n{ \"source\": [\"aws.s3\"], \"detail-type\": [\"Object Created\"], \"detail\": { \"bucket\": { \"name\": [\"your-bucket\"] }, \"object\": { \"key\": [{\"prefix\": \"csv- data/\"}] } } }\n\nNOTE\n\nReplace your-bucket with your bucket name.\n\nvi. Click on Save.",
      "content_length": 530,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 327,
      "content": "vii. In the Targets section, click on Add target\n\nIn AWS Step Functions Studio, you will \u0000nd the following visual\n\nrepresentations:\n\nFigure 4.30 – Step Functions workﬂow\n\nOnce your execution completes, you can see all the steps in green,\n\nand if you click on a step and de\u0000nition, you can see all inputs,\n\noutputs, de\u0000nitions, events, and so on.",
      "content_length": 345,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 328,
      "content": "Figure 4.31 – Step Functions with Glue\n\nFinally, you can go into your S3 bucket and validate the data created\n\ninside the ingetion-data folder:\n\nFigure 4.32 – Results in the S3 bucket\n\nIn conclusion, setting up a pipeline using AWS Glue to ingest data\n\nfrom a JDBC database into a catalog table streamlines the process of\n\nextracting, transforming, and loading data into a centralized data\n\ncatalog. is enables eﬃcient data management, making it easier to",
      "content_length": 456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 329,
      "content": "query, analyze, and integrate with other AWS services for scalable\n\nanalytics solutions.\n\nOceanofPDF.com",
      "content_length": 104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 330,
      "content": "5 Running Big Data Workloads with Amazon EMR\n\nAmazon EMR is a managed service that allows running big data\n\nframeworks such as Apache Spark or Apache Hive on the Apache\n\nHadoop ecosystem. It provides clusters for data applications to\n\nhandle large amounts of data in a distributed and scalable way.\n\nEMR removes the complexity of having to deploy, con\u0000gure, and\n\ncoordinate all these open source frameworks and tools to work\n\ntogether, so you can just start using them. Each version of EMR lists\n\nall the speci\u0000c frameworks and the speci\u0000c versions it provides.\n\nUnlike other AWS-managed services, EMR allows you to have full\n\ncontrol and visibility of your cluster: which hardware to run, which\n\nEC2 image to use, what to install, and even root access to the cluster\n\n(except when you run on EMR serverless mode). e recipes in this\n\nchapter will help you learn the EMR capabilities and how to make\n\nthe best use of them.\n\nis chapter includes the following recipes:\n\nRunning jobs using AWS EMR serverless\n\nRunning your AWS EMR cluster on EKS",
      "content_length": 1043,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 331,
      "content": "Using the AWS Glue catalog from another account\n\nMaking your cluster highly available\n\nScaling your cluster based on workload\n\nCustomizing the cluster nodes easily using bootstrap actions\n\nTuning Apache Spark resource usage\n\nCode development on EMR using Workspaces\n\nMonitoring your cluster\n\nProtecting your cluster from security vulnerabilities\n\nTechnical requirements\n\ne recipes in this chapter assume that you have a Bash shell or\n\nequivalent available with the AWS CLI installed\n\n(https://docs.aws.amazon.com/cli/latest/userguide/getting-started-\n\ninstall.html) with access to AWS. If using Microso Windows, you\n\ncan enable WSL (https://learn.microso.com/en-\n\nus/windows/wsl/install) or install Git (https://git-\n\nscm.com/downloads) to use the Bash shell that it brings.\n\nMost commands are too long to list in the book line width. To \u0000t\n\nthem in, lines have been broken using the \\ character, which tells\n\nthe shell that the command continues in the next line. Be careful to\n\nrespect the spaces (or lack of) on the line breaks.",
      "content_length": 1034,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 332,
      "content": "It is also assumed that you have con\u0000gured the default credentials\n\nand region using aws configure or using an AWS CLI pro\u0000le.\n\nYou need to have permission to manage EMR clusters. Also, most\n\nrecipes will expect you to have a user with access to the AWS EMR\n\nconsole using a web browser. It’s easier if you have a test account on\n\nwhich you can use an admin user to access the console and the\n\ncommand line; otherwise, you’ll need to grant permissions as\n\nneeded.\n\nMost recipes create an EMR cluster that requires a subnet and an S3\n\ndirectory to store logs. For simplicity, it’s easier if the subnet is\n\npublic and the default security groups haven’t been customized, or if\n\nthey have, that they are secured. If you want to run on a private\n\nsubnet, you will have to meet some further requirements (see\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-\n\nclusters-in-a-vpc.html#emr-vpc-private-subnet).\n\nFor the log’s path, choose a bucket in the account and region or\n\ncreate one.\n\ne recipes that run AWS CLI commands will indicate the speci\u0000c\n\nvariables that needs to be set. You can de\u0000ne them as needed when\n\nyou open the shell or export them on your user pro\u0000le so that they\n\nare available on every shell you create.",
      "content_length": 1233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 333,
      "content": "Many recipes will ask you to de\u0000ne the following variables in your\n\nshell, so it’s better if you have them de\u0000ned beforehand. Pick or\n\ncreate an S3 bucket in the region you intend to use:\n\nSUBNET=<enter the subnet for EMR to use here> # Don't use a trailing / in the S3 urls S3_SCRIPTS_URL=<enter an s3:// path where to store scripts> S3_LOGS_URL=<enter an s3:// path where to store logs>\n\nIt is recommended that you set the Bash \u0000ag to warn you if a\n\nvariable is missing:\n\nset -u\n\ne recipes try to be frugal and create small clusters that suﬃce for\n\nthe demonstration while using commonly available nodes; if that\n\ninstance type doesn’t happen to be available in your region, please\n\nreplace them with the most similar instance type available at the\n\ntime.\n\nSome recipes will expect you to have a keypair .pem \u0000le, created in\n\nthe region, which you can do in the EC2 console\n\n(https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/create-\n\nkey-pairs.html#having-ec2-create-your-key-pair).\n\nWhen the keypair is created, it will download a .pem \u0000le with the\n\nname of the key. Change the permissions so only your user can read",
      "content_length": 1123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 334,
      "content": "it (for instance, by running cmhod 400 mykey.pem). Create a\n\nvariable in the shell with the key name:\n\nKEYNAME=<just the key name, no .pem extension>\n\nYou can \u0000nd the code \u0000les for this chapter on GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter05.\n\nRunning jobs using AWS EMR serverless\n\nEMR serverless was created for the common case where the user\n\njust wants to run Spark and Hive jobs without having to worry\n\nabout the type of nodes, capacity, and con\u0000guration.\n\nFor such cases, EMR serverless really simpli\u0000es the operation, since\n\nit does not require a cluster to be con\u0000gured or maintained. You do\n\nnot have to worry about which kind of EC2 is the right one or\n\nwhether it is going to be present (and available in enough capacity)\n\nfor your chosen region and Availability Zone. e main trade-oﬀ is\n\nthat you can no longer ssh into nodes to do low-level\n\nadministration and troubleshooting.\n\nIn this recipe, you will see how simple it is to run a Spark\n\napplication using EMR serverless.",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 335,
      "content": "Getting ready\n\nTo test serverless, you will need a sample script to run. e\n\nfollowing script is a basic example that accesses the Glue catalog\n\nfrom the EMR serverless job. In the shell, run the following\n\ncommand to create a Python \u0000le with the script that follows:\n\ncat <<EOF > testEMRServerless.py from pyspark.sql import SparkSession spark = (SparkSession.builder.config( \"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.\\ AWSGlueDataCatalogHiveClientFactory\") .enableHiveSupport() .getOrCreate()) spark.sql(\"SHOW TABLES\").show() EOF\n\nUpload the \u0000le to S3 in a bucket on the same region as you intend to\n\nuse Glue (otherwise, you would need a VPC for EMR serverless to\n\ndownload it).\n\nHow to do it…\n\n1. Open the AWS console on EMR and select EMR Serverless in the le\n\nmenu.\n\n2. Use the Get Started button to set up EMR Studio so it logs you in and\n\nopens the application creation screen. If you had previously set EMR",
      "content_length": 951,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 336,
      "content": "Studio up in the region, then select Manage application, and then Create\n\napplication in the applications screen.\n\n3. In the application creation screen, enter the name SparkTest. Make\n\nsure that the type is Spark and leave the rest as defaults.\n\n4. Select Create and start application at the bottom.\n\n5. In the Applications list, use the SparkTest link to view the details,\n\nthen use the refresh button until the status becomes Started.\n\n6. Now that the serverless application is started, you can submit jobs. Select\n\nSubmit job run on the lower side of the Application details screen.\n\nFigure 5.1 – An EMR Studio application is created",
      "content_length": 637,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 337,
      "content": "7. Select a runtime role. e dropdown will show the ones that are suitable\n\nfor EMR Serverless and allow you to create a new one. Choose one of\n\nthose.\n\nAt the time of this writing, the role is created without\n\npermission to use the Glue catalog. To allow this, use the link in\n\nthe creation con\u0000rmation message that appears to open a new\n\ntab. From the Add permissions dropdown, select Create inline\n\npolicy using this JSON policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"glue:CreateDatabase\", \"glue:UpdateDatabase\", \"glue:DeleteDatabase\", \"glue:GetDatabase\", \"glue:GetDatabases\", \"glue:CreateTable\", \"glue:UpdateTable\", \"glue:DeleteTable\", \"glue:GetTable\", \"glue:GetTables\", \"glue:GetTableVersions\", \"glue:CreatePartition\", \"glue:BatchCreatePartition\", \"glue:UpdatePartition\", \"glue:DeletePartition\", \"glue:BatchDeletePartition\",",
      "content_length": 868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 338,
      "content": "\"glue:GetPartition\", \"glue:GetPartitions\", \"glue:BatchGetPartition\",\n\n\"glue:CreateUserDefinedFunction\",\n\n\"glue:UpdateUserDefinedFunction\",\n\n\"glue:DeleteUserDefinedFunction\", \"glue:GetUserDefinedFunction\", \"glue:GetUserDefinedFunctions\" ], \"Resource\": [ \"arn:aws:glue:*:*:database/*\", \"arn:aws:glue:*:*:table/*/*\", \"arn:aws:glue:*:*:catalog\" ] } ] }\n\n8. In the script location \u0000eld, enter the S3 URL of the Python \u0000le that was\n\nuploaded earlier, as mentioned in the Getting ready section of this recipe.\n\n9. Once you \u0000nish editing the \u0000eld, it should detect it is a Python script\n\nand remove or empty the Main class \u0000eld.\n\n10. Leave the rest as default and submit it. e job will be listed in the Job\n\nruns tab and the status will change to Scheduled. Aer a minute or so, it\n\nshould change to Running. You can use the refresh button to refresh the\n\nstatus. If you have made a mistake, such as lacking permissions, you\n\ncould clone the job in the table to submit it again quickly.",
      "content_length": 979,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 339,
      "content": "11. When the job ends successfully, you can view it from the application\n\nscreen listing the jobs and the logs (stdout and stderr). In this\n\nexample, opening the stdout log should print the list of the tables in\n\nthe catalog default database on the current region.\n\nFigure 5.2 – The Spark application runs\n\nHow it works…\n\nWhen the job is submitted, EMR creates a cluster taking care of all\n\nthe con\u0000guration, instance selection, and capacity. is means that\n\nwe only have to provide the PySpark script or the executable JAR if\n\nwe are using Scala or Java.\n\nIt’s possible to avoid this scheduling delay by enabling the pre-\n\ninitialized capacity. With that con\u0000guration, you can set a\n\nminimum capacity to be available when the application is started,\n\nso it’s ready to handle jobs promptly. en EMR will scale the\n\ncapacity as needed.\n\nSpark can use the Glue catalog because, by default, the AWS Glue\n\nData Catalog as metastore con\u0000guration setting is enabled for the",
      "content_length": 967,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 340,
      "content": "jobs. is in turn sets a Spark property that tells Spark to use Glue.\n\nAs you have seen, in EMR serverless, you can submit Spark jobs in\n\nan equivalent way to how you would start it using spark-submit\n\nin a traditional EMR cluster or other Spark environment. You can\n\nread more about it in the Spark documentation:\n\nhttps://spark.apache.org/docs/latest/submitting-applications.html.\n\nIf instead of Python, you wrote your Spark job using Scala or Java,\n\nthen instead of the path to the script on S3, you would specify the\n\nS3 path to the JAR with the job and dependencies, and then on the\n\nSpark properties tab you would specify the main class using --\n\nclass, just like you would do if you were using spark-submit.\n\nYou can experiment with this by selecting the Submit sample job\n\noption in the application screen, which creates a job that runs a\n\nsample PI calculation using the JAR provided by EMR.\n\nThere’s more…\n\nInside the job run, you can easily access SparkUI, both while\n\nrunning and aer the job has completed.\n\nYou can streamline running the apps if you con\u0000gure the app to\n\nauto-start when a job is submitted. You can also avoid the startup\n\nlag by adding provisioned nodes (at minimum a driver and an\n\nexecutor).",
      "content_length": 1224,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 341,
      "content": "In addition, you can con\u0000gure the application to auto-stop if it has\n\nbeen idle for a con\u0000gured amount of time.\n\nFor more information, you can visit\n\nhttps://docs.aws.amazon.com/emr/latest/EMR-Serverless-\n\nUserGuide/application-capacity.html.\n\nSee also\n\nAWS Glue is an alternative solution to run Spark jobs in a managed\n\nenvironment, with value features such as bookmarks and incremental\n\nprocessing. You can experience these features with the Processing data\n\nincrementally using bookmarks and bounded execution recipe in Chapter\n\n3, Ingesting and Transforming Your Data with AWS Glue.\n\nRunning your AWS EMR cluster on EKS\n\nEMR oﬀers the option of running a fully managed Spark cluster\n\nleveraging Kubernetes.\n\nInstead of running a full Hadoop cluster with YARN and HDFS,\n\nEMR on Elastic Kubernetes Service (EKS) is a lightweight solution\n\nto run Spark applications with a reduced start time, better resource\n\nutilization, and better availability. It allows running a cluster with\n\nnodes from diﬀerent Availability Zones or even AWS Outposts (on\n\nthe customer data center).",
      "content_length": 1075,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 342,
      "content": "Getting ready\n\nYou need to have installed both the AWS and EKS CLI client tools.\n\nYou can follow the AWS instructions to install them on your\n\nmachine depending on your OS:\n\nAWS CLI: https://docs.aws.amazon.com/cli/latest/userguide/getting-\n\nstarted-install.html\n\nEKS CLI: https://github.com/weaveworks/eksctl\n\nYou can verify that both tools are installed by running the following\n\ncommands:\n\naws --version eksctl version\n\nIf you haven’t already set up the credentials and region to use by\n\nrunning aws configure in the commands, we’ll assume that the\n\nregion you want to work with is the default region set.\n\nHow to do it...\n\n1. On the command line, run the following command:\n\neksctl create cluster --name emr-recipe2 \\ --instance-types=m5.xlarge --managed\n\n2. Wait for a few minutes while the two CloudFormation templates are\n\ndeployed: one for the cluster and another for the managed node group",
      "content_length": 898,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 343,
      "content": "for the cluster.\n\nTIP:\n\nYou can see progress details on the AWS CloudFormation console.\n\nYou might see an error such as kubectl not found. If the Kubernetes\n\nclient is available, it will be used to check the cluster, but this is not required.\n\n3. Check the capacity status by running the following:\n\neksctl get nodegroup --cluster emr-recipe2\n\nNotice that these nodes are being charged, so if you are not\n\ngoing to complete the recipe (including the cleanup) at this time,\n\nyou can manually adjust this capacity using eksctl scale\n\nnodegroup. See the ere’s more… section of this recipe for how\n\nto automate scaling.\n\n4. Allow the EMR cluster to be used as a Kubernetes namespace:\n\neksctl create iamidentitymapping --cluster emr-recipe2\\ --namespace default --service-name \"emr- containers\"\n\n5. Create an EMR virtual cluster using the EKS cluster:\n\naws emr-containers create-virtual-cluster \\ --name vc_emr_recipe2 --container-provider \\ '{\"id\": \"emr-recipe2\", \"type\": \"EKS\", \"info\": {\"eksInfo\": {\"namespace\": \"default\"}}}'",
      "content_length": 1023,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 344,
      "content": "6. De\u0000ne variables with the cluster id and role arn.\n\nVIRTUAL_CLUSTER_ID=$(aws emr-containers \\ list-virtual-clusters --output text --query \\ \"sort_by(virtualClusters, &createdAt)[-1].id\") ROLE_ARN=$(eksctl get \\ iamidentitymapping --cluster emr-recipe2 | \\ grep emr-containers | cut -f 1)\n\n7. Prepare the job con\u0000guration (type the following without break lines\n\ninside quoted strings):\n\ncat > emr-recipe2.json << EOF { \"name\": \"EmrRecipe2-Pi\", \"virtualClusterId\": \"$VIRTUAL_CLUSTER_ID\", \"executionRoleArn\": \"$ROLE_ARN\", \"releaseLabel\": \"emr-6.10.0-latest\", \"jobDriver\": { \"sparkSubmitJobDriver\": { \"entryPoint\": \"local:///usr/lib/spark/examples/jars/spark- examples.jar\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=1 --conf spark.executor.memory=1G --conf spark.driver.memory=1G --class org.apache.spark.examples.SparkPi\" } } } EOF\n\n8. Launch the job:",
      "content_length": 868,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 345,
      "content": "aws emr-containers start-job-run --cli-input- json \\ file://./emr-recipe2.json\n\n9. Check the job run status. You can run this multiple times to get the\n\nstatus:\n\naws emr-containers list-job-runs \\ --virtual-cluster-id $VIRTUAL_CLUSTER_ID\n\n10. Clean up to avoid incurring further costs:\n\neksctl delete cluster --name emr-recipe2 aws emr-containers delete-virtual-cluster --id \\ ${VIRTUAL_CLUSTER_ID}\n\nHow it works...\n\nWhen you ran the eksctl create command, it created and\n\ndeployed a CloudFormation template that creates a cluster in EKS.\n\ne --managed option enabled the cluster to work with managed\n\nnode group, meaning that nodes of the speci\u0000ed type will be\n\nallocated and released as needed. It is recommended to use at least\n\nm5.xlarge or instances with more memory. Spark is likely to fail\n\nto run on smaller instances. is feature doesn’t have an extra cost;\n\nyou pay for what you use.",
      "content_length": 893,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 346,
      "content": "You can optionally download the Kubernetes client\n\n(https://docs.aws.amazon.com/eks/latest/userguide/install-\n\nkubectl.html). You can check the current usage by running the\n\nfollowing:\n\nkubectl get nodes\n\nFor simplicity, we used the\n\nAWSServiceRoleForAmazonEMRContainers role, which\n\nprovides the minimum permissions and trusts emr-containers\n\n(the name of EMR on EKS) to assume it. is is enough to run a\n\njob, but in a more realistic example, you would need your own role\n\nwith the same AmazonEMRContainersServiceRolePolicy and\n\ntrust relation, but with additional permissions to write to S3,\n\nCloudWatch, and possibly others depending on the job.\n\nWhen the job was submitted, it allocated nodes from the node\n\ngroup, retrieved the image indicated, and started one as the driver.\n\nerefore, with this method, Spark can only run on cluster mode,\n\nmeaning that the driver also runs in the remote cluster and not a\n\nlocal process (like you can do in normal EMR).\n\nThere’s more...\n\nOn the AWS console, you can navigate the EMR, select virtual\n\nclusters under EKS, and view the details of the run in the SparkUI.",
      "content_length": 1109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 347,
      "content": "It should look like this:\n\nFigure 5.3 – The SparkUI job run\n\nNotice that we didn’t select the Spark version directly. is is\n\nselected by the EMR version of the selected image. is way, each\n\njob in the virtual cluster can use a diﬀerent version if needed. is\n\nallows for more \u0000exibility than using a traditional EMR cluster.\n\nRemember that this way of running jobs allows you to ssh into the\n\nnodes (like a regular EMR cluster does). For that, you need to set up\n\na keypair ssh keys on EC2 and then use them when creating the\n\nEKS cluster by adding the --ssh-access and --ssh-public-\n\nkey parameters.\n\nIn this recipe, the provision of nodes was automated by using the\n\ndefault desired nodes, but you can use one of the autoscaling\n\nmethods to allow the job to adapt to the demand and grow and",
      "content_length": 794,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 348,
      "content": "shrink as you submit workloads\n\n(https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.htm\n\nl).\n\nSee also\n\nUnless you have a speci\u0000c reason for using EKS (such as optimizing a\n\npool of capacity) to run a Spark job like in this recipe, it’s easier to use\n\nEMR serverless and that way simplify the capacity management. See\n\nhow to do this in the Running jobs using AWS EMR serverless recipe.\n\nUsing the AWS Glue catalog from another account\n\nApache Hive is the traditional SQL solution in the Hadoop\n\necosystem. Since its early versions, it has decoupled the catalog\n\nmetastore from the query engine to store information about the\n\ntables and schemas. is means that multiple tools have added\n\nsupport to integrate with the Hive metastore over the years such as\n\nSpark, Presto/Trino, Impala, or the Hive server itself.\n\nWhen AWS Glue was released, one of the value propositions that it\n\ncould provide was a Hive-compatible store, which could massively\n\nscale and provide fault tolerance out of the box.",
      "content_length": 1007,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 349,
      "content": "When you run EMR on EC2, you can run a Hive metastore or use\n\nthe Glue catalog as the metastore just by checking the\n\ncorresponding box in the cluster con\u0000guration screen (or setting\n\nthe equivalent con\u0000guration if doing programmatically). When\n\nGlue is set as the catalog metastore, all the tools in the cluster that\n\nare compatible with Hive will use it to retrieve and store\n\ninformation about the databases and tables. By default, it will use\n\nthe catalog on the same account (and region) as EMR is running.\n\nIn this recipe, you’ll see how to enable the Glue metastore and use\n\none from a diﬀerent account in the same region, as well as how to\n\nrun a PySpark script that prints the tables in the\n\nCrossCatalogRecipeDB database of the catalog.\n\nGetting ready\n\nFor this recipe, you ideally need two accounts: one running EMR\n\nand the other holding the catalog to use. You can also do it on the\n\nsame account if you don’t have multiple accounts; in that case, you\n\ncan skip the permissions step.\n\nIn the Bash shell, de\u0000ne the environment variables with the account\n\nIDs:\n\nEMR_ACCOUNT=<id of the account running EMR> CATALOG_ACCOUNT=<id of the account holding the",
      "content_length": 1163,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 350,
      "content": "catalog> REGION=<id of the account holding the catalog>\n\ne recipe assumes that the AWS CLI credentials are for the EMR\n\naccount, while the catalog account will be accessed using the AWS\n\nconsole with a Lake Formation admin user (or a user that can create\n\none).\n\nis recipe assumes that you have de\u0000ned the SUBNET,\n\nS3_SCRIPTS_URL, and S3_LOGS_URL variables in the shell (see\n\nthe Technical requirements section at the beginning of the chapter\n\nfor more information).\n\nHow to do it...\n\n1. Using a text editor, create a text \u0000le named PrintTables.py with the\n\nfollowing content:\n\nfrom pyspark.sql import SparkSession spark = SparkSession.builder.enableHiveSupport( ).getOrCreate spark.sql(\"SHOW TABLES FROM cross_catalog_recipe\" ).show()\n\n2. Upload the script you created to S3:\n\naws s3 cp PrintTables.py $S3_SCRIPTS_URL/",
      "content_length": 821,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 351,
      "content": "3. For this recipe, we are using plain IAM permissions and not Lake\n\nFormation. Let us verify whether it is disabled while you create the test\n\ndatabase and table.\n\nOn the AWS console, log in using the account you speci\u0000ed in\n\nthe CATALOG_ACCOUNT variable in Getting ready and go to AWS\n\nLake Formation. If this is the \u0000rst time you are using it, you will\n\nbe asked to assign an administrator.\n\nNavigate to Settings on the le menu and ensure that IAM\n\npermissions are enabled for new tables and databases (you can\n\nchange it back aer you create a database and table in the\n\nfollowing steps if you want to).\n\nFigure 5.4 – Lake Formation settings\n\n4. Navigate to Glue in the console and select Databases in the le menu,\n\nthen choose Create and name it cross_catalog_recipe. Leave\n\nthe rest as default.\n\n5. In the le menu, select Tables and then Add Table. Enter\n\ncross_account_table as the name and select",
      "content_length": 907,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 352,
      "content": "cross_catalog_recipe as the database. Enter any S3 path\n\nlocation; it doesn’t matter, as we are not really going to read the table\n\ncontent. Continue to the next steps leaving everything as default until\n\nyou complete the table creation.\n\n6. Go back to the Bash shell and run the following command to generate\n\nthe policy based on your variables:\n\ncat << EOF { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::${EMR_ACCOUNT}:role/EMR_EC2_Defa ultRole\" }, \"Action\": [ \"glue:GetDatabase\", \"glue:GetPartition\",\"glue:GetTables\", \"glue:GetPartitions\", \"glue:BatchGetPartition\", \"glue:GetDatabases\", \"glue:GetTable\", \"glue:GetUserDefinedFunction\", \"glue:GetUserDefinedFunctions\"], \"Resource\" : [ \"arn:aws:glue:$REGION:$CATALOG_ACCOUNT:catalog \", \"arn:aws:glue:$REGION:$CATALOG_ACCOUNT:databas e\\ /default\", \"arn:aws:glue:$REGION:$CATALOG_ACCOUNT:databas e\\ /cross_catalog_recipe\",",
      "content_length": 922,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 353,
      "content": "\"arn:aws:glue:$REGION:$CATALOG_ACCOUNT:table\\ /cross_catalog_recipe/*\", \"arn:aws:glue:$REGION:$CATALOG_ACCOUNT:databas e\\ /global_temp\"] } ] } EOF\n\n7. Copy the JSON object printed by the previous step (all the lines from the\n\none starting with { to the one ending with }). In the AWS console,\n\nunder Glue, select Settings in the le menu, paste the policy in the\n\nPermissions textbox, and save the changes. If your policy is not empty,\n\nsave the existing policy to restore it aer completing the recipe, or use\n\nanother account or region. If you have mixed up the account IDs, it will\n\nreject the policy.\n\n8. Go back into the Bash shell and run the following command (note that\n\nthe last seven lines don’t have a \\ at the end because they are inside a\n\nmultiline string):\n\nMETASTORE_CLASS=\"com.amazonaws.glue.catalog.\\ metastore.AWSGlueDataCatalogHiveClientFactory\" aws emr create-cluster --name CrossAccountCatalog \\ --release-label emr-6.11.0 --instance-count 2 \\ --log-uri $S3_LOGS_URL --instance- type=m5.xlarge \\ --applications Name=Spark --use-default-roles \\ --ec2-attributes SubnetId=${SUBNET} --auto- terminate\\",
      "content_length": 1120,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 354,
      "content": "--steps Type=Spark,Name=SparkPi,ActionOnFailure=\\ TERMINATE_CLUSTER,Args=[$S3_SCRIPTS_URL/\\ PrintTables.py] --configurations '[{\"Classification\": \"spark-hive-site\", \"Properties\": {\"hive.metastore.client.factory.class\": \"'$METASTORE_CLASS'\",\"hive.metastore.glue.cata logid\": \"'$CATALOG_ACCOUNT'\"}}]'\n\n9. If all is correct, the previous command will return a JSON with a cluster\n\nID and ARN. You can check periodically for the cluster status like this\n\n(use your ID):\n\naws emr describe-cluster --cluster-id j- XXXXXXXXX \\ | grep '\"State\"'\n\n10. Once the cluster is in the terminated state, retrieve the stdout of the\n\nstep (the Spark job) by running the following command (again, enter\n\nyour cluster ID instead of j-XXXXXXXXX). It should con\u0000rm that it has\n\ndownloaded a single \u0000le:\n\naws s3 cp $S3_LOGS_URL/j-XXXXXXXXX/steps/ . \\ --exclude '*' --include '*stdout.gz' -- recursive\n\n11. Print the contents of the zipped \u0000le (replace XXX with the path indicated\n\nby the previous command):\n\nzcat s-XXXXXXXXXXXXXXXXX/stdout.gz",
      "content_length": 1018,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 355,
      "content": "You should get con\u0000rmation that Spark was able to list the tables\n\nin the catalog from the other account:\n\nFigure 5.5 – Job stdout listing tables\n\n12. e next step pertains to cleanup. If you no longer need them, delete the\n\ntable, catalog, and policy in the catalog account. You can also delete the\n\nlogs and the script \u0000le from S3:\n\naws s3 rm --recursive $S3_LOGS_URL aws s3 rm $S3_SCRIPTS_URL/PrintTables.py\n\nHow it works...\n\nIn this recipe, you \u0000rst created a trivial Spark script to demonstrate\n\nthat it can view the tables in a database on a diﬀerent account. en\n\nyou con\u0000gured the catalog account to allow EMR on the other\n\naccount to use a new database that was created for this purpose.\n\ne reason you were asked to check that only IAM permissions are\n\nset is for you to be able to use a simple catalog policy to give access\n\nwith a resource rules. When a database or table is created using an\n\nIAM only con\u0000guration, Lake Formation adds a special permission",
      "content_length": 968,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 356,
      "content": "automatically, which means that it delegates on the catalog IAM\n\npolicy instead of using what Lake Formation grants.\n\nYou can go to Lake Formation on the catalog account, select Data\n\nLake permissions on the le menu, and type\n\nIAMAllowedPrincipals in the table search box. is will show\n\nyou the databases and tables that are secured by the catalog policy\n\nrules. You can add this special principal permission to existing\n\ndatabases or tables that were created without IAM only enabled to\n\nachieve the same result.\n\nReview the policy that you added to the catalog. Notice that it gives\n\ntypical catalog permissions to multiple resources and that the\n\ncross_catalog_recipe database is listed twice, once as a\n\ndatabase resource and another to allow access to all tables in it\n\nusing a wildcard. In addition, the policy gives access to the default\n\nand global_temp databases. is is done for historical reasons\n\nbecause Spark will check those databases, for instance, to check\n\nwhether there are UDFs registered. is depends on the Spark\n\nversion.\n\nen you created a small cluster. Notice that there are two\n\nproperties in the con\u0000guration. e \u0000rst one enables the Glue\n\ncatalog client for the Spark Hive metastore; it is the equivalent of\n\nchecking the option to use the Glue catalog for Spark table\n\nmetadata in the UI:",
      "content_length": 1320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 357,
      "content": "Figure 5.6 – EMR basic conﬁguration options\n\ne second property, hive.metastore.glue.catalogid, tells\n\nthe client to use another account catalog instead of using the one\n\nfrom the current account.\n\nFinally, by checking the output of the step, you can verify that Spark\n\ncan list the tables.\n\nThere’s more...\n\nYou can go ahead and create a table with real data and use it.\n\nHowever, notice that the catalog policy just gives you access to the\n\nmetadata. Spark will still need permission to access the actual data,\n\nnormally on S3. For that, you need to create a policy in the S3\n\nbucket granting access to the EMR role.",
      "content_length": 618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 358,
      "content": "Lake Formation oﬀers the option to manage S3 locations, so when\n\naccess is granted to a user by Lake Formation, it also includes access\n\nto S3 (Lake Formation creates temporary credentials for S3 for that\n\nspeci\u0000c data). More information can be found in the oﬃcial\n\ndocumentation: https://docs.aws.amazon.com/lake-\n\nformation/latest/dg/access-control-underlying-data.html#data-\n\nlocation-permissions.\n\nSee also\n\nAn alternative way to access another account’s catalog (while retaining\n\naccess to the account’s catalog) is to enable a special feature available on\n\nEMR and Glue. By con\u0000guring the\n\naws.glue.catalog.separator=\"/\" property in the spark-\n\nhive-site cluster con\u0000guration, you can now reference a database in\n\nanother account (of course, you will still need to be granted permission\n\non the catalog). For instance, you can use the following command:\n\nSELECT * FROM `111122223333/default.mytable`;\n\nFor more information, you can refer to\n\nhttps://repost.aws/knowledge-center/query-glue-data-catalog-\n\ncross-account.\n\nMaking your cluster highly available",
      "content_length": 1062,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 359,
      "content": "Historically, AWS EMR clusters were used for batch workloads and\n\ntorn down aerward; errors in the worker nodes would be handled\n\nby YARN and HDFS’ innate resiliency. ere was still the single\n\npoint of failure of the primary node (previously called master), in\n\nthe unlikely case that the whole batch process would need to be\n\nretried.\n\nHadoop has added full High Availability (HA) since the early days,\n\nas it was intended to run on permanent on-premise clusters that\n\nwould be shared by many users and teams.\n\nSince EMR 5.23, it allows running with multiple primary nodes.\n\nEMR takes care of the tedious process of correctly con\u0000guring\n\nHadoop to be HA. Over time, it has also improved the process of\n\nautomatically replacing a primary node and recon\u0000guring the\n\nsystem, so the cluster can graciously survive the failure of a single\n\nnode with minimal or no disruption to the cluster users.\n\nHA is important in cases where delays have a business impact, such\n\nas real-time systems or services where the users interact directly\n\nwith the cluster and an outage would directly impact productivity.\n\nNonetheless, this HA support has its limitations, and it is good to\n\nknow them. First, the cluster can only run on a single Availability\n\nZone (you will see later some ideas to mitigate this). Just because",
      "content_length": 1305,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 360,
      "content": "Hadoop is HA does not mean every service, for instance a Presto\n\nserver, in the cluster is automatically HA.\n\nIn this recipe, you will see how straightforward it is to run a failure-\n\ntolerant cluster on EMR.\n\nGetting ready\n\nis recipe assumes that you have de\u0000ned the SUBNET variable and\n\nset up KEYNAME in the shell, as well as having the.pem key in the\n\ncurrent shell directory (see the Technical requirements section at the\n\nbeginning of the chapter for more information).\n\nHow to do it...\n\n1. Add the resource placement policy to the default EMR role. is role\n\nwill have automatically been created if you have run the other recipes in\n\nthis chapter in the same account and region. If the command fails\n\nbecause the role doesn’t exist, you can execute step 2 (which will fail) so\n\nit creates the role, then come back to execute the following command,\n\nand then continue again with step 2:\n\naws iam attach-role-policy --role-name \\ EMR_DefaultRole --policy-arn \\ arn:aws:iam::aws:policy/\\ AmazonElasticMapReducePlacementGroupPolicy\n\n2. In the shell, run the following command to create an HA cluster. If\n\neverything is correct, you should just see the cluster ID printed:",
      "content_length": 1175,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 361,
      "content": "CLUSTER_ID=$(aws emr create-cluster --release- label \\ emr-6.15.0 --use-default-roles --applications \\ Name=HBase --use-default-roles --ec2- attributes \\ SubnetId=$SUBNET,KeyName=$KEYNAME --instance- groups \\ InstanceGroupType=MASTER,InstanceCount=3,Insta nceType\\ =m5.xlarge InstanceGroupType=CORE,InstanceCount=1\\ ,InstanceType=m5.xlarge --placement-group- configs \\ InstanceRole=MASTER | grep ClusterArn \\ | grep -o 'j-.*[^\",]'); echo $CLUSTER_ID\n\n3. Check the status of the cluster by running the following command until\n\nit prints that the cluster is WAITING and both instance groups are\n\nRUNNING:\n\naws emr describe-cluster --cluster-id $CLUSTER_ID\n\nIf the status returned is TERMINATED_WITH_ERRORS, it’s\n\nprobably because the subnet and key name speci\u0000ed are not\n\nvalid for the region.\n\n4. By default, the security group created by EMR doesn’t allow SSH access.\n\nFirst, you need to \u0000nd the cluster ID (in case there are other clusters):",
      "content_length": 942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 362,
      "content": "aws emr describe-cluster --cluster-id $CLUSTER_ID \\ | grep 'MasterSecurityGroup' | grep -o 'sg-.* [^\",]'\n\n5. Open the AWS console, search for the Security Groups feature in the top\n\nbar, and open it. In the list, \u0000nd the one matching the ID from the\n\nprevious step (it should be named ElasticMapReduce-master).\n\n6. Open the security group using the ID link. en, select Edit inbound\n\nrules | Add rule, and in the new rule on the dropdowns, select SSH and\n\nMy IP. Save the rule changes.\n\nFigure 5.7 – A security group’s new SSH rule\n\n7. Log in to the server using the .pem keypair. e command assumes that\n\nyou have the .pem \u0000le available in the current directory and with read\n\npermission just for you:\n\naws emr ssh --cluster-id $CLUSTER_ID\\ --key-pair-file $KEYNAME.pem\n\nIf the command results in a Connection timed out message, it\n\nmeans that the previous step wasn’t completed correctly or that\n\nthe IP detected by the browser is not the one that the shell is\n\nreally using to access AWS. In that case, you will need an",
      "content_length": 1022,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 363,
      "content": "alternative method to \u0000nd the IP to allow, for instance, running\n\nthe following in the shell:\n\ncurl ifconfig.me\n\n8. In the EMR shell that you just logged in to, run the following commands\n\nto con\u0000rm that the cluster is con\u0000gured in HA, with two or three servers\n\ndepending on the service:\n\ngrep -A 1 zookeeper.quorum \\ /etc/hadoop/conf/core-site.xml grep qjournal /etc/hadoop/conf/hdfs-site.xml grep -A 1 yarn.resourcemanager.address \\ /etc/hadoop/conf/yarn-site.xml\n\n9. Exit the EMR shell to go back to your local one:\n\nexit\n\n10. Remove the termination protection and terminate the cluster:\n\naws emr modify-cluster-attributes --cluster-id \\ $CLUSTER_ID --no-termination-protected aws emr terminate-clusters --cluster-id $CLUSTER_ID\n\n11. Con\u0000rm that the cluster is terminating:\n\naws emr describe-cluster --cluster-id $CLUSTER_ID \\ grep '\"State\"'\n\n12. If you don’t need to ssh to other clusters, edit the security group again to\n\nremove the SSH inbound rule that you added in step 6.",
      "content_length": 982,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 364,
      "content": "How it works...\n\nAt the beginning of the recipe, you added the\n\nAmazonElasticMapReducePlacementGroupPolicy policy to\n\nthe default role, which was enabled using the --placement-\n\ngroup-configs option on the master node group. Currently, the\n\nonly strategy is SPREAD, which is used by default. is means that\n\nthe three nodes requested for the MASTER instance group will be\n\nimplemented by EC2 by ensuring that the instances cannot be\n\ncollocated on the same physical machine. is would reduce\n\nreliability, since Hadoop can only survive the loss of one master\n\nnode without any service loss.\n\nis is because both ZooKeeper and the HDFS journal nodes work\n\non a quorum-based system that requires a majority to make\n\ndecisions. On the other hand, other services such as the YARN\n\nResourceManager, HBase region server, and others work on an\n\nactive/passive model, but still rely on ZooKeeper to make that\n\nselection and failover. In step 9, you saw the con\u0000guration for these\n\nservices and how they con\u0000gure multiple servers, so clients can\n\nfailover if one is down.\n\nIn this case, the cluster was created with a keypair, so you could ssh\n\nand check the con\u0000guration to highlight the key diﬀerences. You\n\ncan compare it with the one of a single master on your own.",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 365,
      "content": "Finally, notice that to terminate the cluster, you \u0000rst had to disable\n\nthe termination protection \u0000ag despite not creating the cluster with\n\nthe explicit --termination-protected \u0000ag. is is because\n\nwhen a multimaster node is created programmatically, it assumes\n\nthat it is meant to be long-term and protects it from accidental\n\ndeletion and potential data loss.\n\nSince EMR 7, if the master capacity is not speci\u0000ed, it will use\n\nmultimaster by default.\n\nThere’s more...\n\nNotice that despite spreading the master nodes across diﬀerent\n\nhardware, the cluster still has a single subnet speci\u0000ed. is means\n\nthat a single VPC and a single region and Availability Zone are\n\nused.\n\nAs a result, the cluster won’t survive if the AWS AZ goes down. If\n\nyou need to protect against such extreme cases, you will need to use\n\nyour own measures. Examples include running multiple redundant\n\nclusters on diﬀerent AZs (or even regions) and de\u0000ning one as the\n\nprimary and one as the backup, or another strategy that allows\n\ncoordinating multiple clusters as needed.\n\nAs an exercise, you can ssh to one of the masters and issue the\n\nfollowing command:",
      "content_length": 1138,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 366,
      "content": "sudo shutdown now\n\nYou can then list the instances and observe how EMR detects that\n\nthe host is no longer available and replaces it.\n\nOen, the HA cluster will depend on having an HA catalog. e\n\neasiest way to achieve that is to con\u0000gure EMR to use Glue as the\n\nHive metastore; it provides out-of-the-box scalability and AZ fail\n\ntolerance. However, if you need to use a Hive metastore, create a\n\ndatabase on AWS Aurora and then con\u0000gure Hive to use it.\n\nSee also\n\nAlways consider whether you really need a long-lived EMR cluster and\n\nwhether there are any alternatives; for instance, instead of storing\n\npermanent data on HDFS, you could store it on S3. Instead of using a\n\nPresto or Trino server, you could use Athena. Instead of getting capacity\n\nfrom YARN, use serverless EMR, and so on.\n\nTo run workloads, see the Running jobs using AWS EMR\n\nserverless recipe.\n\nTo learn more about the Glue catalog, see the Optimizing your\n\ncatalog data retrieval using pushdown \u0000lters and indexes recipe\n\nin Chapter 3, Ingesting and Transforming Your Data with AWS\n\nGlue",
      "content_length": 1062,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 367,
      "content": "Scaling your cluster based on workload\n\ne main bene\u0000t of running on the cloud compared to on-premises\n\nis the access to virtually endless capacity. When running EMR\n\nworkloads, you don’t want to just have resources available but also\n\nto only pay for them when needed to be cost-eﬀective.\n\nIn this recipe, you will see how EMR can eﬀortlessly allow you to\n\nscale your cluster capacity based on the workload.\n\nGetting ready\n\nis recipe assumes that you have set up the SUBNET environment\n\nvariable as indicated in the Technical requirements section at the\n\nbeginning of this chapter.\n\nHow to do it...\n\n1. Create a cluster with autoscale and idle timeout (make sure you use \\\n\nonly at the end of the lines indicated; the second command will print the\n\ncluster ID):\n\nCLUSTER_ID=$(aws emr create-cluster --name AutoScale\\ --release-label emr-7.1.0 --use-default-roles \\ --ec2-attributes SubnetId=${SUBNET} \\",
      "content_length": 904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 368,
      "content": "--auto-termination-policy IdleTimeout=900 \\ --applications Name=Spark --instance-groups \\ '[{\"InstanceCount\":1,\"InstanceGroupType\":\"MAST ER\", \"Name\":\"MASTER\",\"InstanceType\":\"m5.xlarge\"}, {\"InstanceCount\":1,\"InstanceGroupType\":\"CORE\", \"Name\":\"CORE\",\"InstanceType\":\"m5.xlarge\"}, {\"InstanceCount\":1,\"InstanceGroupType\":\"TASK\", \"Name\":\"TASK\",\"InstanceType\":\"m5.xlarge\"}]' \\ --managed-scaling-policy '{\"ComputeLimits\": {\"UnitType\":\"Instances\",\"MinimumCapacityUnits\" :1, \"MaximumCapacityUnits\":10, \"MaximumOnDemandCapacityUnits\":10, \"MaximumCoreCapacityUnits\":2}}' \\ | grep ClusterArn | grep -o 'j-.*[^\",]') echo $CLUSTER_ID\n\n2. Print the cluster capacity details. is will show that there is one node\n\nallocated for each group and run multiple times until it gets the\n\nRUNNING status:\n\naws emr describe-cluster --cluster-id $CLUSTER_ID \\ | grep -B 3 -A 5 \"RunningInstanceCount\"\n\n3. Run a Spark example app to force the cluster to scale up:\n\naws emr add-steps --cluster-id $CLUSTER_ID -- steps \\ Type=Spark,Name=SparkPi,ActionOnFailure=\\ TERMINATE_CLUSTER,Args=-- class,org.apache.spark.\\ examples.SparkPi,/usr/lib/spark/examples/jars/",
      "content_length": 1129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 369,
      "content": "\\ spark-examples.jar,10000\n\n4. Run the following command multiple times to see how the cluster\n\nresizes until it hits the maximum capacity allowed (11 nodes in total).\n\nNotice that it \u0000rst changes RequestedInstanceCount, then\n\neventually scales up to that, and then scales down again when no longer\n\nneeded. In the rare case that the step \u0000nishes before the scaling kicks in,\n\nyou can repeat step 3 and this step as needed:\n\naws emr describe-cluster --cluster-id $CLUSTER_ID \\ | grep -B 2 -A 3 \"RequestedInstanceCount\"\n\n5. You can wait for the step to \u0000nish and see it scale back down to the\n\nminimum capacity. Terminate the cluster when you are done observing\n\nit:\n\naws emr terminate-clusters --cluster-id $CLUSTER_ID\n\nHow it works...\n\nIn the \u0000rst step, you created a cluster, which included the three\n\nkinds of group types:\n\nMASTER to mainly run YARN ResourceManager and the HDFS\n\nNameNode\n\nCORE nodes running HDFS and YARN\n\nTASK nodes that just run YARN",
      "content_length": 956,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 370,
      "content": "e distinctive feature here is that you used managed scaling, which\n\nonly requires you to tell it how much to scale and lets EMR take\n\ncare of applying it.\n\nIn addition, you de\u0000ned an auto termination policy of 15 minutes.\n\nis means that if the cluster is idle for that period, it will be shut\n\ndown automatically. is is a good practice for clusters that do not\n\nhave auto-shutdown when all steps are complete, which avoids\n\nleaving clusters forgotten and building up charges over time. e\n\ncriteria of what constitutes an idle cluster are more sophisticated in\n\nnewer versions (for further details, see the documentation at\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-\n\nauto-termination-policy.html).\n\nen you added the SparkPI estimation but with 10,000 partitions\n\nso that it generated enough load to force the cluster to scale up.\n\nWhen the task completes, the cluster will scale back to the initial\n\ncapacity.\n\nThere’s more...\n\nFor this recipe, you de\u0000ned a maximum of 10 units of on-demand\n\ncapacity (you can also use cheaper spot instances for part or all), of\n\nwhich a maximum of two will be core nodes. at is because this\n\nexample is pure computing, so it makes sense to add most of them\n\nas TASK nodes, so it has more resources for Spark.",
      "content_length": 1267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 371,
      "content": "Since EMR 5.10, the scaling behavior has been optimized to\n\nterminate nodes when the instance approaches the next hour to\n\navoid incurring another hour of cost. You can change back to the\n\nold behavior by using --scale-down-behavior\n\n\"TERMINATE_AT_TASK_COMPLETION\". See further details in the\n\ndocumentation:\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-\n\nscaledown-behavior.html.\n\nRemember that managed scaling is based on YARN usage. With\n\ncluster services that are not based on YARN, such as Trino or\n\nHBase, it cannot work correctly. See the next section for\n\nalternatives.\n\nSee also\n\nIn this recipe, you have seen how to use managed scaled, which is an\n\neﬀortless way of doing eﬃcient scaling, just setting the limits. However,\n\nthere are cases where you might want to take control and build your own\n\nscaling rules. Check the documentation for guidance on how to build\n\nsuch rules:\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-\n\nautomatic-scaling.html.\n\nCustomizing the cluster nodes easily using bootstrap actions",
      "content_length": 1050,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 372,
      "content": "EMR oﬀers the freedom to control all aspects of the cluster nodes.\n\nYou can provide your own Amazon Machine Images (AMIs) and\n\nrun your own choice of OS (within reason) and con\u0000guration.\n\nHowever, if you create or even branch your AMIs, you will create a\n\nmaintenance burden for the future such as patching for\n\nvulnerabilities or \u0000xes. In practice, you rarely need to deviate so\n\nmuch as to justify this eﬀort. In most cases, you just need to do\n\nsmall customizations, such as installing a system package, changing\n\nsome OS con\u0000guration, or running some special service.\n\nFor such cases, bootstrap actions provide a straightforward way of\n\nrunning these customizations just by running a shell script as root\n\nduring setup. In most cases, this means that you can run the same\n\nactions on newer versions of the OS and EMR, reducing the\n\noperational burden. If new nodes are added to the cluster, they will\n\nalso run the same bootstrap actions to ensure consistency.\n\nIn this recipe, let us say that we will run an application in the cluster\n\nthat requires using utilities from the popular Python scikit-\n\nlearn machine learning library. Instead of asking applications to\n\ninstall it or having it preinstalled in a container or image, you will\n\nuse a bootstrap action to launch an EMR cluster that comes with\n\nthe package installed and ready to use.\n\nGetting ready",
      "content_length": 1362,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 373,
      "content": "is recipe assumes that you have de\u0000ned the SUBNET,\n\nS3_SCRIPTS_URL and S3_LOGS_URL variables in the shell (see the\n\nTechnical requirements section at the beginning of this chapter for\n\nmore information). For this recipe, it is essential that the S3 URLs\n\npoint to a bucket in the same region as EMR.\n\nHow to do it...\n\n1. Create the bootstrap script \u0000le:\n\ncat > recipe_bootstrap.sh << EOF #!/bin/bash set –x /usr/bin/pip3 install -U scikit-learn EOF\n\n2. Copy the \u0000le onto S3:\n\naws s3 cp recipe_bootstrap.sh $S3_SCRIPTS_URL/\n\n3. Create a cluster that uses the bootstrap script you just uploaded. Take\n\ncare to only use the \\ character at the end of the lines indicated:\n\nBOOTSTRAP_SCRIPT=$S3_SCRIPTS_URL/recipe_bootst rap.sh CLUSTER_ID=$(aws emr create-cluster \\ --name BootstrapRecipe --log-uri $S3_LOGS_URL \\ --auto-terminate --release-label \"emr-6.15.0\" \\ --instance-type=m5.xlarge --instance-count 2 \\ --applications Name=Spark --use-default-roles",
      "content_length": 950,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 374,
      "content": "\\ --ec2-attributes SubnetId=$SUBNET --bootstrap- action\\ Name=PythonDeps,Path=$BOOTSTRAP_SCRIPT -- steps \\ '[{\"Name\": \"Validate bootstrapaction\", \"ActionOnFailure\":\"TERMINATE_CLUSTER\", \"Jar\": \"command-runner.jar\",\"Properties\":\"\",\"Args\": [\"/usr/bin/python3\",\"-c\",\"exec(\\\"import sklearn'\\ '\\\\nprint(sklearn.__version__)\\\")\"], \"Type\":\"CUSTOM_JAR\"}]')\n\n4. You can use the AWS console, navigate to EMR, select Clusters on the\n\nle menu, and then select the one named BootstrapRecipe. Explore the\n\ndiﬀerent tabs, including Bootstrap Actions and Steps. If all goes\n\ncorrectly, the cluster will start and then run the step con\u0000gured, which\n\nwill check the package version and terminate the cluster automatically.\n\n5. Once the cluster has completed the bootstrap, run the step, and\n\ncompleted successfully, check the step log to verify that the bootstrap\n\nstep did its job and the package was installed. It might take a few\n\nminutes aer the step is executed before the log becomes available. In\n\nthe Steps tab, open the stdout link. Once it becomes available, it should\n\ndisplay the version of the library installed by the bootstrap script (for\n\ninstance, 1.0.2). If something has gone wrong, the stderr log will show\n\nthe error.",
      "content_length": 1221,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 375,
      "content": "Figure 5.8 – EMR steps details\n\n6. If you no longer need it, remove the S3 \u0000les from the shell:\n\naws s3 rm $S3_SCRIPTS_URL/recipe_bootstrap.sh aws s3 rm --recursive $S3_LOGS_URL\n\nHow it works...\n\nIn step 3, you created a basic EMR cluster (using as little\n\ncon\u0000guration as possible) with the minimum number of nodes: a\n\nprimary node (formerly known as master) and a core node. e\n\ndiﬀerence between a core and a task node is that the former is used\n\nfor the HDFS \u0000lesystem, as well as for computing. ese nodes use\n\nthe modest m5.xlarge type to reduce cost.\n\nNotice that you didn’t have to specify any roles. We used the default\n\nones provided by EMR, which serve for the common cases. Also,\n\nyou didn’t specify any image or OS, which means that we have let",
      "content_length": 757,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 376,
      "content": "EMR use the latest version of Amazon Linux 2 and thus don’t have\n\nto worry about patching it for vulnerabilities.\n\ne subnet implicitly determines the VPC that will be used; in this\n\ncase, it sets no security con\u0000guration or ssh access, since there was\n\nno need to access the cluster nodes to complete the recipe.\n\ne cluster instructions included a bootstrap action to call the\n\nscript that you put on S3 following steps 1 and 2. A bootstrap action\n\nis a reference to a script that gets executed as root once the OS is\n\nready but before services in the cluster are started. is allows the\n\nbootstrap script to do OS con\u0000guration and tuning. Once EMR has\n\ncompleted the provisioning of nodes, the bootstrap, along with the\n\ninstallation of the con\u0000gured apps (in this case just Spark), EMR\n\nstarts executing the steps de\u0000ned (if any).\n\nIn this recipe, the Spark step cluster runs the simple shell script that\n\nruns Python code to import the package that was installed on the\n\nOS by the bootstrap script. e step is con\u0000gured to terminate the\n\ncluster if it fails; this will mark the cluster as terminated with errors.\n\nIf there is no issue, the script prints the version of the package,\n\nshowing that the package is installed. Notice that the step only runs\n\non the primary node, but the bootstrap script is applied on all\n\nnodes; therefore, we could use this package’s utilities in a PySpark-\n\ndistributed script.",
      "content_length": 1414,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 377,
      "content": "Finally, because the cluster was started with the --auto-\n\nterminate \u0000ag, it shuts itself down aer completing the steps.\n\nOtherwise, the cluster would be le running until it is manually\n\nshut down or hits the idle timeout (if con\u0000gured).\n\nThere’s more...\n\nRemember that the intent of this feature is not to con\u0000gure the\n\nEMR applications such as Spark conf, as such con\u0000guration would\n\nbe overridden when the apps are installed and when using the\n\napplication options provided by EMR.\n\nIn the recipe, you checked the step output log. However, there are\n\nalso stdout and sterr logs for the bootstrap action. Go check\n\nthem to see the bootstrap script running. You could print messages\n\nin the script and then check in these logs. Notice that there is one\n\nfor each node in the cluster (two in this case). For instance, you can\n\nuse: s3://{S3_LOGS_URL}/<your cluster id>/node/<node\n\nid>/bootstrap-actions/1/.\n\nIt’s possible to pass parameters to the script. at way, you can reuse\n\na bootstrap script and make it con\u0000gurable to your needs. For\n\ninstance, you could indicate the set of packages that the script needs\n\nto install based on the cluster purpose. You can pass shell\n\narguments to the script using an Args parameter aer the script\n\npath. For instance, here, the script will receive the arg1 and arg2",
      "content_length": 1310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 378,
      "content": "arguments, which the script can access using Bash syntax ($1 and\n\n$2):\n\n--boostrap-action \\ Name=PythonDeps,Path=\"$BOOTRAP_SCRIPT}\",Args= [args1,arg2]\n\nFinally, notice that the script runs without user variables such as\n\nPATH. at’s why we used full paths to run the pip and Python\n\ninterpreter.\n\nSee also\n\nBootstrap actions run in all nodes before services are started. In cases\n\nwhere you need to do some setup once the services are running such as\n\nuploading \u0000les to HDFS, instead of a bootstrap action, add a command\n\ntype step (before any other steps), which can also run a script (but unlike\n\nbootstrap, only on the master). See how to do this in the documentation:\n\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-\n\ncommandrunner.html.\n\nTuning Apache Spark resource usage\n\nApache Spark is probably the most-used framework on EMR. It\n\nworks by running YARN containers for executor instances and\n\nanother one for the Application Manager (AM), which oen also\n\nacts as the Spark driver.",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 379,
      "content": "In traditional on-premises clusters, the cluster has many resources\n\nthat are shared by many cluster users, so you use as few resources as\n\npossible for the job at hand. On EMR, it is simple to run clusters on\n\ndemand, which are \u0000t for purpose, and then shut them down when\n\nthe job is complete. at way, the cluster size, nodes, and\n\ncon\u0000guration can be optimized for the speci\u0000c job and negative\n\ninteractions between users, such as resource starvation or\n\nsaturation, can be avoided.\n\nIn such dedicated clusters, you want your application, such as\n\nApache Spark, to make the most of the hardware provided since it\n\ndoesn’t have to share it with other users. at’s why EMR added the\n\nmaximizeResourceAllocation con\u0000guration option for Spark.\n\nIt is enabled by default and indicates to EMR to automatically\n\ncon\u0000gure the Spark executors’ resource usage based on the cluster\n\ncapacity. at includes cores and memory, and, unless\n\nspark.dynamicAllocation.enabled is explicitly enabled, it\n\nwill also calculate the number of instances to use.\n\nHowever, it’s possible when using EMR on EC2 that the cluster can\n\nhave nodes of diﬀerent sizes. at’s why since EMR versions 5.32.0\n\nand 6.5.0, a spark.yarn.heterogeneousExecutors.enabled\n\n\u0000ag was added and enabled by default. is feature adjusts the\n\ncontainers requested by Spark to run executors dynamically.",
      "content_length": 1354,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 380,
      "content": "While these con\u0000guration features provide good defaults and easy\n\ncon\u0000guration, they might not be optimal, and you might want to\n\ntune them yourself; you’ll see how in this recipe.\n\nGetting ready\n\nis recipe assumes that you have de\u0000ned the SUBNET and\n\nS3_LOGS_URL variables in the shell (see the Technical requirements\n\nsection at the beginning of the chapter for more information).\n\nHow to do it...\n\n1. In a shell, run the following command. If all is correct, you should get a\n\nJSON printed with id and arn of the cluster that was just created:\n\naws emr create-cluster --name SparkResourcesRecipe \\ --release-label emr-6.15.0 --auto-terminate \\ --instance-type=m5.2xlarge --log-uri $S3_LOGS_URL \\ --instance-count 2 --applications Name=Spark \\ --use-default-roles --ec2-attributes \\ SubnetId=$SUBNET --steps Type=Spark,Name=SparkPi\\ ,ActionOnFailure=TERMINATE_CLUSTER,Args=\\ --deploy-mode,cluster,-- class,org.apache.spark.\\ examples.SparkPi,/usr/lib/spark/examples/jars/ \\ spark-examples.jar,12",
      "content_length": 998,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 381,
      "content": "2. Go to the AWS EMR console and select Clusters under EMR on EC2 on\n\nthe le menu. Select the cluster that was just created (with the ID\n\nmatching the one returned in step 1). In the Steps tab, there is a Spark\n\napplication to be executed once the cluster is ready. It should \u0000nish in\n\nless than a minute and aer that, the cluster will shut down. Note that we\n\ndidn’t specify any resources for Spark to use.\n\n3. Once the previous step has been completed, on the cluster summary\n\nsection of the page, select the YARN timeline server link. Aer a few\n\nseconds, it will open a new tab with the YARN application history. If you\n\ndon’t see the SparkPl application listed, there might be a bit of a delay in\n\npublishing the logs. Refresh the browser page until it appears.\n\nFigure 5.9 – The cluster summary\n\n4. Click on the application ID, which is named in the format:\n\napplication_<timestamp>_0001. is will take you to the table\n\nof attempts. ere should be only one; click on the attempt ID link. Now,",
      "content_length": 1001,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 382,
      "content": "you should see the containers that this app was allocated in. ere\n\nshould be two. e one ending in 000001 is the driver and the other is\n\nthe executor:\n\nFigure 5.10 – The application container list\n\n5. Click on each container ID. It will take you to the container details.\n\nNotice that each one has a diﬀerent memory allocated, as indicated in\n\nthe Resource line.\n\n6. Now go back to the Cluster summary page tab and use the Spark History\n\nServer link. Aer a few seconds, a new tab will open wherein the\n\nSparkPI application should be listed. Use the link on the app ID to open\n\nSparkUI.\n\n7. In SparkUI, navigate to the Executors tab. e executor is given six cores\n\nand over 7 GB of storage memory, which is half of the JVM heap size by\n\ndefault.\n\n8. You can use the options to control some of the aspects such as the cores,\n\nmemory, and overhead; the system algorithm will try to adapt, but that",
      "content_length": 898,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 383,
      "content": "oen leads to confusing results. Now, you are going to start a cluster\n\nwhere you will disable the automation logic and run it with custom\n\nexecutor resources:\n\naws emr create-cluster --name SparkResourcesRecipe2 \\ --release-label emr-6.15.0 --applications Name=Spark\\ --instance-type=m5.2xlarge --instance-count 2\\ --use-default-roles --ec2-attributes \\ SubnetId=${SUBNET} --log-uri $S3_LOGS_URL \\ --auto-terminate --configurations \\ '[{\"Classification\":\"spark- defaults\",\"Properties\": {\"spark.yarn.heterogeneousExecutors.enabled\":\" false\"}} ]' --steps Type=Spark,Name=SparkPi,ActionOnFailure\\ =TERMINATE_CLUSTER,Args=[--deploy- mode,cluster,\\ --executor-cores,4,--executor-memory,6G,-- conf,\\ 'spark.yarn.executor.memoryOverhead=1G',\\ --class,org.apache.spark.examples.SparkPi,\\ /usr/lib/spark/examples/jars/spark- examples.jar,12]\n\n9. Check the Spark resources like in steps 4 and 5. Notice the diﬀerences in\n\nthe core node resource usage; now you have three executors on the\n\ninstance using 21 GB of RAM and 12 Spark cores (YARN only allocates\n\none per executor, this is explained in the next paragraph).",
      "content_length": 1108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 384,
      "content": "Figure 5.11 – Spark executors’ details\n\n10. Clean up the log \u0000les if you don’t want to keep them:\n\naws s3 rm --recursive $S3_LOGS_URL\n\nHow it works...\n\nIn step 1, you created an EMR on EC2 cluster with default resource\n\npolicies and the minimum number of nodes: a primary node and a\n\ncore node (the diﬀerence between primary and task is that the latter\n\ndoesn’t run the HDFS \u0000lesystem). e cluster was created with a\n\nstep already set up and con\u0000gured to shut itself down automatically\n\nto save on cost. It’s also possible to create the cluster, leave it\n\nrunning (remove the --auto-terminate \u0000ag), and add steps later.\n\ne cluster was created using YARN and Spark con\u0000gurations \u0000t for\n\nthe kind of node used. In addition, the\n\nheterogeneousExecutors feature dynamically adjusts to try to\n\n\u0000t a single executor per node, even if nodes have diﬀerent sizes or",
      "content_length": 857,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 385,
      "content": "are added aer the cluster is con\u0000gured (for instance, if you add a\n\ntask group of a diﬀerent type of instance).\n\nWhen the Spark application was executed, it \u0000rst allocated the AM\n\ncontainer. In this recipe, that container also runs the Spark driver.\n\nis is because of --deploy-mode cluster. en it\n\nautomatically created an executor for the instance trying to use the\n\nrest of the available memory.\n\nIn YARN, by default, it allocates the memory available but doesn’t\n\nenforce vcores, which are sharable thanks to CPU threads. Spark\n\ncores are really threads, so they don’t need to match the cores that\n\nYARN has assigned, since it won’t enforce vcore usage by default.\n\nIf you check the SparkUI Environment tab, you will see that EMR\n\nset the spark.executor.memory property to 4,743M for the\n\ncluster, but that was dynamically overridden by the\n\nheterogeneousExecutors feature to use a JVM of nearly 15 GB\n\n(you can know that because by default, half of the heap is used for\n\nstorage, which you can see in the Executors tab).\n\nere is also a discrepancy with the container size allocated on\n\nYARN, which is 16,896M. at diﬀerence is called the memory\n\noverhead and is used for memory outside of the heap such as the\n\nstack, I/O buﬀers, unmanaged memory, or Python daemons used\n\nfor Python UDFs. e driver also has some memory allocated by",
      "content_length": 1340,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 386,
      "content": "YARN outside of the JVM. If a process exceeds the memory\n\nallocated, YARN will kill it, but it will give you a message suggesting\n\nincreasing the overhead.\n\nYou might have noticed that the two containers on YARN amount\n\nto less than 20 GB of RAM, while m5.2xlarge has 32 GB. Bear in\n\nmind that YARN cannot allocate all the memory on the machine\n\nfor containers, since there are other processes such as Hadoop itself\n\nthat need memory. It depends on the kind of node but, in general,\n\nabout 80% of the memory is used for containers (which is\n\nadjustable in the EMR con\u0000guration but is risky to lower).\n\nIn the end, the automatic allocation used less than 20 GB of the 24\n\nGB available in the worker node and six out of the eight cores. It\n\nwas very conservative.\n\nIn step 7, you started a cluster but this time with the\n\nheterogeneousExecutors feature disabled. In this case, it was\n\ndisabled at the cluster level. However, it is also possible to disable it\n\nin the Spark submit options, like any other Spark con\u0000guration\n\nproperty.\n\nOnce that feature was disabled, you could control the number of\n\ncores, the size of the heap, and the overhead used exactly – 1 G in\n\nthis example, which is plenty for a job that doesn’t require\n\nunmanaged memory use. e only thing that was automatic in this",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 387,
      "content": "case is that by default, EMR is con\u0000gured to maximize resources\n\n(assuming that the cluster is not shared and that you want the best\n\nperformance for your app), so it allocated as many executors as it\n\ncould \u0000t. Each executor container was con\u0000gured with 6 GB heap\n\nand 1 GB overhead for a total of 7 GB. 2.5 GB was used by the\n\ndriver/AM, so that leaves room for three executors.\n\nAlso, notice that each executor is con\u0000gured with four Spark cores,\n\nso in total, the executors will use 12 when the machine has eight\n\nvcores and there are other processes running. is kind of\n\novercommitting can help maximize CPU usage when the job is low\n\non computing and high on I/O. Otherwise, it might hurt\n\nperformance a bit due to the CPU context switching. In this case, it\n\nwas done for demonstration purposes, since the job is purely doing\n\ncomputing so it won’t bene\u0000t from this core con\u0000guration.\n\nThere’s more...\n\nere is another element of \u0000ne-tuning that you can do if you don’t\n\nrequire the Spark driver to be failure-tolerant. If your job has a plan\n\nthat is more complex than the trivial example in this recipe, it is\n\nlikely that you will require multiple GB of memory, while the\n\nprimary node memory is underused at the same time (especially as\n\nyou move to larger instances).",
      "content_length": 1280,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 388,
      "content": "If you run Spark without the --deploy-mode cluster argument\n\n(which we used in this recipe), the Spark driver will run on the\n\nprimary node, and the \u0000rst container is just a small AM using less\n\nthan 1 GB of memory (you can reduce that further with\n\ncon\u0000guration if the job doesn’t need many containers to manage).\n\nis can free up worker node resources for executors.\n\nNow that you know how you can \u0000ne-tune the resource usage.\n\nInstead of trial and error, it’s better to get system-level information\n\nin addition to SparkUI. For that, you can use an application named\n\nGanglia, which you can enable in the cluster application list and\n\nthen access via a browser once you set up a ssh tunnel to access it\n\n(see the documentation at\n\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/view_Gangl\n\nia.html). With Ganglia, you can get detailed information about\n\nsystem CPU and IO usage both overall and per host.\n\nYou might also want to try to run clusters with multiple types of\n\ncore and task nodes without disabling\n\nspark.yarn.heterogeneousExecutors.enabled and see how\n\nthe algorithm reacts and adjusts. is depends on the EMR version,\n\nbut it has a component of trial and error whereby the algorithm\n\nrequests containers that fail to allocate or are discarded.\n\nSee also",
      "content_length": 1276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 389,
      "content": "If you run a job without special requirements and want to simplify\n\nresource usage, you can just use EMR serverless or EMR on EKS. If this\n\nis the case for you, see the corresponding recipes:\n\nRunning jobs using AWS EMR serverless\n\nRunning your AWS EMR cluster on EKS\n\nCode development on EMR using Workspaces\n\nDeveloping data processing code on complex distributed\n\nframeworks is much more productive when it is done in an\n\ninteractive way by using representative data and seeing the results of\n\nthe transformations done on each step. is has led to an increase\n\nin the popularity of languages that can be interpreted interactively,\n\nsuch as Python or Scala.\n\nWhile you can do some interactive development via a shell, as the\n\ncode becomes larger, it stops being practical. e productive way to\n\ndo this is via a notebook with cells, where each cell holds and\n\nexecutes a block of code, but the variables are common to the\n\nnotebook so the work you do in one cell is visible to the others. at\n\nway, you can develop and test a small piece of code at a time and see\n\nthe results.",
      "content_length": 1079,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 390,
      "content": "EMR has traditionally supported this style of development with\n\nApache Zeppelin, which can be installed on the cluster to run\n\nmultiple types of notebooks including Spark or Bash, with multiple\n\nlanguages such as Python or Scala. Accessing the notebooks\n\nrequires opening a port to the primary node (or setting up a proxy\n\nsuch as Apache Knox).\n\ne rise in popularity of Python brought its preferred notebook\n\nenvironment, Jupyter, to EMR, which was added as an alternative to\n\nZeppelin.\n\nEMR Workspaces (previously called notebooks) put the Jupyter\n\nnotebooks at the center of the service, instead of a cluster that runs\n\nJupyter. is service consists of a JupyterLab app, which can be\n\nlinked to existing or new clusters. In this recipe, you will see how to\n\ndo this.\n\nHow to do it...\n\n1. In a shell, create the role that you will use for EMR Studio:\n\nROLE_NAME=EMR_Notebooks_RecipeRole echo '{\"Version\":\"2012-10-17\",\"Statement\": [{\"Effect\": \"Allow\",\"Principal\":{\"Service\": \"elasticmapreduce.amazonaws.com\"},\"Action\": \"sts:AssumeRole\"}]}' > role-assume.json aws iam create-role --role-name $ROLE_NAME \\ --assume-role-policy-document file://role-",
      "content_length": 1147,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 391,
      "content": "assume.json aws iam attach-role-policy --policy-arn \\ arn:aws:iam::aws:policy/service-role/\\ AmazonElasticMapReduceRole --role-name $ROLE_NAME aws iam attach-role-policy --role-name $ROLE_NAME \\ --policy-arn arn:aws:iam::aws:policy/service- role/\\ AmazonElasticMapReduceEditorsRole aws iam attach-role-policy --policy-arn \\ arn:aws:iam::aws:policy/AmazonS3FullAccess \\ --role-name $ROLE_NAME rm role-assume.json\n\n2. Open the AWS console in a browser and navigate to EMR. On the le\n\nmenu, under EMR Studio, select Studios and then Create Studio.\n\n3. Choose the Custom option, name it RecipeStudio and choose\n\nEMR_Notebooks_RecipeRole (you created this in the \u0000rst step).You can\n\nlet it create an S3 bucket or choose an existing one.\n\n4. Enter RecipeStudio_Workspace in the Workspace name \u0000eld,\n\nextend the Networking and security section, and select a VPC and\n\nsubnets (by default, each region has one VPC with three subnets).\n\nChoose Create Studio.\n\n5. Select Workspaces (Notebooks) on the le, choose the one that was just\n\ncreated, and then choose Quick Launch. It will open a new tab with\n\nJupyterLab.\n\n6. In JupyterLab, the le bar shows the \u0000le explorer with a notebook \u0000le\n\nnamed like the workspace by default. Double-click on the name to open",
      "content_length": 1250,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 392,
      "content": "it; it will prompt you to select a kernel. Choose PySpark.\n\n7. Now we need a cluster that Spark can run in. Open the second tab on\n\nthe le bar; it will show the Compute panel:\n\nFigure 5.12 – JupyterLab showing the compute conﬁguration\n\n8. Choose the EMR on EC2 cluster type and expand the Advanced\n\ncon\u0000guration section. Enter the name RecipeCluster, pick a release\n\nand a bucket to store logs, and two m5.xlarge nodes or similar. For\n\nthis kind of cluster, it is especially important to have an auto termination\n\non idle setup, since it is easy to forget about the cluster and build up\n\ncharges. Choose Create cluster and wait until it is ready. It will give you\n\nthe cluster ID, which starts with j-, and then you can go to EMR and\n\nmonitor the progress.\n\n9. Once it is running, it should show a message and update the EMR on\n\nEC2 cluster dropdown. In that dropdown, choose RecipeCluster |",
      "content_length": 892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 393,
      "content": "Attach.\n\n10. Once the action is completed, you will have a cluster powering the\n\nPySpark notebook. To prove this, on the \u0000rst cell, type spark and use\n\nthe Run button on the bar (or use the Shi + Enter shortcut). e icon\n\non the le of the cell will become an asterisk to indicate that it’s in\n\nprocess. Once it \u0000nishes, it will show the YARN ID of the Spark\n\napplication running on the EMR cluster.\n\nFigure 5.13 – JupyterLab showing the notebook executor results\n\n11. Stop the EMR cluster. At the time of writing this book, you cannot do it\n\nfrom JupyterLab. Go to the EMR console, choose Clusters under EMR\n\non EC2 in the le menu, select the cluster named RecipeCluster, and\n\nterminate it.",
      "content_length": 693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 394,
      "content": "12. Go back to the Workshop screen, select the RecipeWorkspace row, and\n\nchoose Delete in the Action menu (or Stop if you want to keep it). You\n\nmight also want to empty any temporary \u0000les on S3, depending on\n\nwhich S3 bucket you chose in step 3.\n\nThere’s more...\n\nIn the \u0000rst step, you created a role to use on EMR Studio. In this\n\nexample, it has unrestricted access to all the S3 buckets in the\n\naccount. In a production system, it is better to narrow down the\n\npermissions to the bucket and path that will be used by Studio.\n\nen you created an EMR Studio. is is something you normally\n\nonly need to do once and can then reuse for all your Workspaces.\n\nAerward, you created a Workspace. is was previously called a\n\nnotebook but has since been renamed, as now it provides an\n\nenvironment where you can have multiple notebooks and\n\nintegrations with other tools such as Git or Presto (if you have them\n\ninstalled on the attached cluster).\n\nNext, you launched the Workspace. In practice, it starts\n\nautomatically when you create it. When using Launch with options\n\ninstead of the quick launch, you could have speci\u0000ed a cluster to\n\nattach to. e cluster must already be running in one of the subnets\n\ncon\u0000gured on Studio and have the JupyterEnterpriseGateway\n\napplication installed (so the Workspace can attach to it). en, if",
      "content_length": 1330,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 395,
      "content": "you want to use Spark, you would need Spark, Hadoop, and Livy\n\ninstalled. Normally, it is easier to just create a \u0000t-for-purpose cluster\n\ninside the Workspace as you did in the recipe and then terminate it\n\nor let it expire. You can create a cluster template so you do not have\n\nto con\u0000gure it every time.\n\nBear in mind that it is possible to share the Workspace with other\n\nusers to enable collaboration on the development, although\n\nworking independently is oen desirable.\n\nSee also\n\nIf you only use PySpark in your notebooks, you have the option to use a\n\nGlue Notebook instead, which provides an integrated Jupyter notebook\n\nand cluster with minimum con\u0000guration and a cluster created on\n\ndemand. You can see an example of Glue notebook usage in the\n\nOptimizing your catalog data retrieval using pushdown \u0000lters and indexes\n\nrecipe in Chapter 3, Ingesting and Transforming Your Data with AWS\n\nGlue.\n\nIf you are a heavy user of SageMaker, you can link your SageMaker\n\nnotebooks with EMR or even easier to use a Glue kernel and run a\n\nserverless cluster. You can learn more about this at\n\nhttps://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-\n\nemr-spark-glue.html.\n\nMonitoring your cluster",
      "content_length": 1204,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 396,
      "content": "e convenience of using EMR to create \u0000t-for-purpose,\n\ndiscardable clusters has signi\u0000cantly reduced the maintenance\n\nneeds for Hadoop clusters compared to long-lived, multitenant on-\n\npremises clusters.\n\nHowever, there is still a need to monitor how the cluster is doing in\n\ndetail, in cases where you need to optimize the use or troubleshoot\n\nan issue. For instance, you might wonder what the limiting factor to\n\nperformance is: is it the CPU, memory, network, disk, or something\n\nelse?\n\nIn this recipe, you will see how to go deep into the cluster metrics\n\nand monitoring tools that it provides out of the box.\n\nGetting ready\n\nTo carry out this recipe, you need to set up the SUBNET,\n\nS3_LOGS_URL, and KEYNAME shell environment variables (see the\n\nTechnical requirements section at the beginning of this chapter to\n\nlearn how to set them up).\n\nTo complete the recipe, you will need a SOCKS5 proxy in your\n\nbrowser to access the cluster. Follow the instructions depending on\n\nyour browser, so that when you access a **.amazonaws.com URL,\n\nit uses the localhost proxy on port 8157 that you’ll create. You can\n\nlearn more at",
      "content_length": 1124,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 397,
      "content": "https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-\n\nconnect-master-node-proxy.html.\n\nFigure 5.14 – A proxy proﬁle\n\nFigure 5.15 – Auto-switch in SwitchyOmega\n\nHow to do it...\n\n1. Create a tiny cluster that will keep running so we can monitor it:\n\nCLUSTER_ID=$(aws emr create-cluster --name \\ MonitorRecipe --release-label emr-6.15.0 \\ --instance-type=m5.xlarge --instance-count 2 \\ --use-default-roles --use-default-roles \\ --ec2-attributes SubnetId=$SUBNET,KeyName=$KEYNAME \\",
      "content_length": 484,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 398,
      "content": "--applications Name=Spark Name=Ganglia --steps \\ Type=Spark,Name=SparkPi,ActionOnFailure=\\ TERMINATE_CLUSTER,Args=--deploy-mode,cluster,\\ --class,org.apache.spark.examples.SparkPi\\ ,/usr/lib/spark/examples/jars/spark- examples.jar\\ ,100000 --log-uri $S3_LOGS_URL \\ --auto-termination-policy IdleTimeout=3600 \\ | grep ClusterArn | grep -o 'j-.*[^\",]') echo $CLUSTER_ID\n\n2. Keep running this command until you get the public DNS name:\n\naws emr describe-cluster --cluster-id $CLUSTER_ID \\ | grep 'Dns'\n\n3. Open the AWS console, navigate to EMR, and \u0000nd the cluster with the\n\nsame ID as the one printed in step 1 (on the same region you are using\n\nthe AWS CLI in). Notice that just by using the console, you can do basic\n\nmonitoring tasks conveniently. You can open the historical YARN and\n\nSpark UIs in the summary, check the list and status of nodes on the\n\nInstances tab, and view several metrics on the Monitoring tab. For\n\ninstance, in this case, the app that is running uses two YARN containers:\n\na Spark driver and an executor. e app does not move any data but\n\ncauses signi\u0000cant system load and memory usage.",
      "content_length": 1113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 399,
      "content": "Figure 5.16 – EMR cluster status metrics\n\n4. To dive deeper and explore timely metrics, you’ll need to access the\n\ncluster. For that, you need to allow SSH access \u0000rst. In the cluster page,\n\nswitch to the Properties tab, expand the EC2 Security groups section,\n\nand open the link under Primary node EMR managed security group.\n\n5. On the security group page, select Edit inbound rules | Add rule. In the\n\nnew role on the dropdowns, select SSH and My IP. Save the rule\n\nchanges.\n\nFigure 5.17 – Choosing a new SSH rule\n\n6. Run the following command to enable port forwarding on your machine\n\ntoward the primary cluster node:\n\naws emr socks --cluster-id $CLUSTER_ID\\",
      "content_length": 663,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 400,
      "content": "--key-pair-file $KEYNAME.pem\n\nIf the command results in Connection timed out, it means that\n\nthe previous step wasn’t completed correctly or that the IP that\n\nwas detected by the browser is not the one that the shell is really\n\nusing to access AWS. In that case, you would need an alternative\n\nmethod to \u0000nd the IP to allow, for instance, running the\n\nfollowing in the shell:\n\ncurl ifconfig.me\n\n7. In your browser, switch the proxy manually if you did not de\u0000ne\n\nautomated rules in the Getting ready section of this chapter. Navigate to\n\nthe master DNS name retrieved in step 2 and specify the 8088 port, for\n\ninstance: http://ec2-X-X-X-X.compute.amazonaws.com:8088.\n\n8. is should open the live YARN UI. It looks like the screen that you see\n\non the YARN timeline server summary link. e diﬀerence is that this\n\none has far more details about the cluster status. Under Cluster Metrics,\n\nyou can see the resources that are available and assigned. In the\n\nfollowing screenshot, under Cluster Node Metrics, you can see the status\n\nof the nodes. ere should be just one active; you can click on the\n\nnumber list of each state and see details of the nodes (for instance, why a\n\nnode is unhealthy).",
      "content_length": 1193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 401,
      "content": "Figure 5.18 – The YARN application list\n\n9. e cluster creation included the Ganglia application. Ganglia can\n\nprovide you with low-level details of the cluster usage globally and per\n\nnode, such as I/O, disk, or CPU usage. On the browser with the proxy\n\nenabled, enter the master node DNS with the /ganglia location. For\n\ninstance, you might enter the following: http://ec2-X-X-X-X.compute-\n\n1.amazonaws.com/ganglia/.\n\n10. Under the time range selector, you should see metrics per node instead\n\nof aggregated. Compare the metrics of the two nodes and note the\n\ndiﬀerence between the primary and the core node (where the Spark\n\napplication ran) CPU usage.",
      "content_length": 655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 402,
      "content": "Figure 5.19 – Ganglia cluster metrics\n\n11. Once you have explored the metrics on your own, stop the proxy in the\n\ncommand line using the Ctrl + C key shortcut, remove the rule you\n\nadded in step 5, terminate the cluster, and clean up the logs:\n\naws emr terminate-clusters –cluster-id $CLUSTER_ID aws s3 rm --recursive $S3_LOGS_URL\n\nThere’s more\n\nIn this recipe, you created a tiny cluster with a Spark step to\n\ngenerate some workload. e cluster also has an automatic idle\n\nauto-shutdown of 30 minutes. is is good practice to limit\n\nexpenses in case you forget to terminate the cluster.",
      "content_length": 587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 403,
      "content": "en, you created a ssh tunnel using the AWS CLI tool. Notice that\n\nthe command indicates the openssh command it is running ssh -\n\no StrictHostKeyChecking=no -o\n\nServerAliveInterval=10 -ND 8157 -I yourkey.pem\n\nhadoop@yourserver.com.\n\nis command tells ssh to accept the server key without asking for\n\ncon\u0000rmation, to keep the connection alive, and to do dynamic port\n\nforwarding from the local 8157 port. If you wanted to use a\n\ndiﬀerent local port for some reason, you would need to install and\n\nrun the ssh command yourself. is port acts as a socks5 proxy.\n\nRemember that you can always ssh into the primary node, copy and\n\npaste the .pem \u0000le content, and then use that \u0000le to ssh to other\n\nhosts on the cluster (you can get the internal names of the other\n\nnodes using one of the tools that you have seen in this recipe). You\n\nhave the option of using OS-speci\u0000c tools such as iostat or\n\nvmstat.\n\nSee also\n\nInstead of monitoring the cluster actively, in most cases, you will want to\n\nreceive an alert when something is abnormal (for instance, if you have\n\nunhealthy YARN nodes). You have the metrics that you see on the\n\nconsole monitor screen on CloudWatch, but the challenge is that they\n\nare cluster-speci\u0000c, so you would need to create these speci\u0000c alarms on",
      "content_length": 1266,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 404,
      "content": "the same script that automates the cluster creation. You can learn more\n\nin the documentation:\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR\n\n_ViewingMetrics.html\n\nProtecting your cluster from security vulnerabilities\n\nHistorically, each version of EMR was provided with an AMI, on\n\nwhich it was tested. In addition, on provisioning the image, it\n\nautomatically checks the repository for security updates. You can\n\nonly disable that behavior using repo-upgrade-on-boot=NONE.\n\nRemember that when using a custom AMI, you must take the\n\nresponsibility of keeping the image patched. However, patches that\n\naﬀect the kernel require a restart to be installed. us, in the past,\n\nyou had two options:\n\nUpgrade to a newer version of EMR, which means taking upgrades in all\n\ncomponents and services. is could cause your application to need to\n\nbe retested and would potentially require a migration.\n\nIndicate EMR to use a newer AMI and risk running the services on an\n\nimage on which it has never been tested.\n\nat has changed since EMR 5.36 and 6.6, if using EMR 5 or 6,\n\nrespectively. From those versions onwards, you can let EMR",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 405,
      "content": "upgrade the AMI automatically with a patched AmazonLinux2 OS\n\nwithout having to do any testing or upgrading yourself.\n\nGetting ready\n\nis recipe assumes that you have de\u0000ned the SUBNET variable in\n\nthe shell (see the Technical requirements section at the beginning of\n\nthis chapter for more information).\n\nHow to do it...\n\n1. Create a cluster that will terminate as soon it runs:\n\nCLUSTER_ID=$(aws emr create-cluster --release- label \\ emr-6.15.0 --auto-terminate --instance-type=\\ m5.xlarge --instance-count 2 --use-default- roles \\ --use-default-roles --ec2-attributes \\ SubnetId=$SUBNET | grep ClusterArn \\ | grep -o 'j-.*[^\",]')\n\n2. Check which Amazon Linux version was used:\n\naws emr describe-cluster --cluster-id $CLUSTER_ID\\ | grep OS\n\n3. List the EMR version details; at the end, there is the\n\nAvailableOSReleases section. Notice that the \u0000rst OS label listed\n\n(the most recent one) is the one that was returned in the previous step:",
      "content_length": 941,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 406,
      "content": "aws emr describe-release-label --release- label\\ emr-6.15.0\n\n4. Stop the cluster to avoid further charges:\n\naws emr terminate-clusters –cluster-id $CLUSTER_ID\n\nHow it works...\n\ne cluster in this example had no steps and was set to auto-\n\nterminate when it had completed all tasks. So, it terminated as soon\n\nas it was started. e point here was to prove that the cluster\n\nautomatically used the most up-to-date version of AmazonLinux2\n\nfor EMR.\n\ne cluster was created with a newer version than the minimum\n\nrequired (6.11 versus 6.6) and you did not specify the --os-\n\nrelease-label \u0000ag. us, EMR was free to automatically select\n\nthe latest veri\u0000ed release for EMR with all the security patches. At\n\nthe time of writing this book, the latest version automatically\n\nselected for 6.11 is AmazonLinux 2 2.0.20230628.0.\n\nis not only provides automatic security but also reduces the EMR\n\nprovisioning time, since the number of patches that must be applied\n\non start is signi\u0000cantly reduced compared to an old image.",
      "content_length": 1014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 407,
      "content": "There’s more...\n\nYou can \u0000nd the details of the OS release picked by EMR on the\n\nAmazonLinux2 release notes at\n\nhttps://docs.aws.amazon.com/AL2/latest/relnotes/relnotes-al2.html.\n\nSee also\n\nIn this case, you have seen how short-lived clusters get OS security\n\npatches. However, that does not cover applications that need patching,\n\nsince in most cases, patches for applications are not security-related. If\n\nthat is the case, you will need to consider upgrading to a version of EMR\n\nor doing custom patching using bootstrap action; see the Customizing\n\nthe cluster nodes easily using bootstrap actions recipe for reference.\n\nOceanofPDF.com",
      "content_length": 639,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 408,
      "content": "6 Governing Your Platform\n\nEnsuring data integrity, compliance, and security is fundamental in\n\ndata engineering, providing a reliable foundation for accurate\n\nanalysis and informed decision-making. Data governance involves\n\nestablishing clear policies and standards to govern data throughout\n\nits life cycle. On the resource management front, governance\n\nensures that best practices and policies are not only de\u0000ned but also\n\nactively applied and enforced. Automation is essential in this\n\nprocess, ensuring consistent adherence to governance policies and\n\nfacilitating smooth implementation of policy updates.\n\nis chapter explores data governance techniques, such as data\n\nmasking, implementing quality checks, and classifying data to\n\nidentify sensitive information. Additionally, it explores resource\n\ngovernance, ensuring eﬀective implementation and enforcement of\n\nbest practices and policies across the AWS environment.\n\ne following recipes will be covered in this chapter:\n\nApplying a data quality check on Glue tables\n\nAutomating the discovery and reporting of sensitive data on your S3\n\nbuckets\n\nEstablishing a tagging strategy for AWS resources",
      "content_length": 1157,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 409,
      "content": "Building your distributed data community with Amazon DataZone\n\nfollowing data mesh principles\n\nHandling security-sensitive data (PII and PHI)\n\nEnsuring S3 compliance with AWS Con\u0000g\n\nTechnical requirements\n\nSeveral recipes in this chapter require having S3 buckets. Glue\n\ntables and Redshi clusters are used in the recipes as well.\n\nYou can \u0000nd the code \u0000les for this chapter on GitHub repository\n\nusing the following link: https://github.com/PacktPublishing/Data-\n\nEngineering-with-AWS-Cookbook/tree/main/Chapter06.\n\nApplying a data quality check on Glue tables\n\nGlue Data Quality is crucial for maintaining the integrity and\n\nreliability of data within the AWS Glue environment. It ensures that\n\ndata conforms to speci\u0000ed quality standards, allowing organizations\n\nto trust and rely on the accuracy of their data-driven insights and\n\ndecision-making processes. Implementing data quality checks helps\n\nidentify and rectify issues, such as missing values, inconsistencies,\n\nand inaccuracies in datasets, promoting data reliability and\n\nreducing the risk of making decisions based on \u0000awed information.",
      "content_length": 1101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 410,
      "content": "AWS Data Quality allows you to enforce quality checks on your\n\ndata on transit and rest using Data Quality De\u0000nition Language\n\n(DQDL). is allows you to proactively apply data quality rules to\n\nyour jobs and tables, helping to identify potential issues early.\n\nAdditionally, you can enforce rules on multiple tables and con\u0000gure\n\nactions or alarms based on detected quality issues, preventing larger\n\nproblems, such as making decisions based on inaccurate data.\n\nIn this recipe, we will learn how to implement data quality checks at\n\nrest on a Glue table.\n\nGetting ready\n\nFor this recipe, you need to have a Glue table and an IAM role with\n\nthe following policies (make sure to replace bucket_name,\n\naws_region_id, and aws_account_id for the Glue source with\n\nyour own values):\n\nis is the Glue Data Quality rule recommendation policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowGlueRuleRecommendationRunActions\", \"Effect\": \"Allow\", \"Action\": [",
      "content_length": 959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 411,
      "content": "\"glue:GetDataQualityRuleRecommendationRun\", \"glue:PublishDataQuality\",\n\n\"glue:CreateDataQualityRuleset\",\n\n\"glue:GetDataQualityRulesetEvaluationRun\", \"glue:GetDataQualityRuleset\" ], \"Resource\": \"arn:aws:glue: <aws_region_id>: <aws_account_id>:dataQualityRuleset/*\" }, { \"Sid\": \"AllowS3GetObjectToRunRuleRecommendationTask\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\" ], \"Resource\": [ \"arn:aws:s3:::aws-glue-*\" ] } ] }\n\nis is the Glue Catalog policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowCatalogPermissions\", \"Effect\": \"Allow\", \"Action\": [ \"glue:GetPartitions\",",
      "content_length": 584,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 412,
      "content": "\"glue:GetTable\" ], \"Resource\": [ \"*\" ] } ]\n\nis is the CloudWatch and S3 policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowPublishingCloudwatchLogs\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogStream\", \"logs:CreateLogGroup\", \"logs:PutLogEvents\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowCloudWatchPutMetricDataToPublishTaskMetr ics\", \"Effect\": \"Allow\", \"Action\": [ \"cloudwatch:PutMetricData\" ], \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"cloudwatch:namespace\":",
      "content_length": 482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 413,
      "content": "\"Glue Data Quality\" } } } ] }\n\nis is the S3 policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::<bucket_name>\", \"arn:aws:s3:::<bucket_name>/*\" ] } ] }\n\nHow to do it…\n\n1. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the Glue service.\n\n2. From the navigation pane on the le, go to Tables and open the table you\n\nneed to set data quality rules for.",
      "content_length": 501,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 414,
      "content": "3. Go to the Data quality tab and click on Create data quality rules.\n\n4. Choose the IAM role you have created and, if required, change the\n\nvalues of Task timeout value and Number of workers (which refers to the\n\nnumber of worker nodes allocated to execute the rule). Optionally, add a\n\n\u0000lter on the table that the task will run on and click on Recommended\n\nrules.\n\n5. Aer a few minutes, the task will \u0000nish scanning the table and will\n\ngenerate a set of recommended data quality rules. Click on Insert rule\n\nrecommendation, review the rules, and if you \u0000nd any rules to be useful\n\nfor your table, select it and click on Add selected rules.\n\n6. In the Ruleset editor, you can de\u0000ne your own rule using DDQL.\n\n7. Click on Save ruleset. A prompt will appear where you can enter a\n\nruleset name and, optionally, a description and tags for the rule. Aer\n\nthat, click again on Save ruleset to complete the process.\n\n8. In the Data quality section, select your ruleset and click on Run. Select\n\nthe IAM role you have created and select the run frequency for your\n\nruleset. Keep the Publish run metrics to Amazon CloudWatch option\n\nselected and click on Run.\n\n9. Once the run is completed, select the ruleset, and from the Actions\n\ndrop-down list, select Download results and review it.\n\nHow it works…\n\nUpon the initial establishment of a data quality ruleset, Glue Data\n\nQuality scanned our table, formulating recommended rules based",
      "content_length": 1430,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 415,
      "content": "on the data within each column of the input table. ese rules serve\n\nas guidelines for pinpointing potential boundaries where data\n\n\u0000ltration can be applied to uphold quality standards. We have\n\nspeci\u0000ed the set of rules to be enforced on our table, initiated a\n\nmanual run that produced a resultset indicating the status of each\n\nrule—whether it has been successfully passed or not, and the Data\n\nquality snapshot section will show the trend of your data quality\n\nscore for the last 10 runs.\n\nThere’s more…\n\nYou can set up alerts on data quality issues to take proactive actions,\n\nusing EventBridge to send noti\u0000cations to a channel of your choice,\n\nsuch as an SNS topic or Lambda. Follow the outlined steps to\n\nachieve this:\n\n1. Navigate to the EventBridge service.\n\n2. Select Rules under Buses from the navigation pane on the le and click\n\non Create rule.\n\n3. Give a name for the rule and optionally a description. Under Rule type,\n\nselect Rule with an event pattern and click on Next.\n\n4. Under Event source, select AWS events or EventBridge partner events.\n\n5. Choose Use pattern form for Creation method.\n\n6. Select AWS service for Event source and then select Glue Data Quality.\n\nUnder Event type, select Data Quality Evaluation Results Available and",
      "content_length": 1258,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 416,
      "content": "then choose Speci\u0000c state(s), select Failed, and click on Next.\n\n7. Choose a target for the rule and create it.\n\nSee also\n\nData Quality De\u0000nition Language (DQDL) reference:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/dqdl.html#dqdl-syntax-\n\nrule-structure\n\nData Quality for ETL jobs in AWS Glue Studio notebooks:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/data-quality-gs-studio-\n\nnotebooks.html\n\nAutomating the discovery and reporting of sensitive data on your S3 buckets\n\ne identi\u0000cation of sensitive data within Amazon S3 is essential\n\nfor maintaining data security and compliance in cloud\n\nenvironments. Given that S3 oen contains extensive datasets,\n\nincluding PII, pinpointing the locations of sensitive data within S3\n\nbuckets is critical for the precise implementation of security\n\nmeasures, the application of relevant access controls, and the\n\neﬀective enforcement of data protection policies. AWS Macie\n\nprovides an automated solution for the discovery, classi\u0000cation, and\n\nprotection of sensitive data through the utilization of machine",
      "content_length": 1051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 417,
      "content": "learning algorithms. is proactive approach aids in mitigating the\n\nrisks associated with data breaches and ensures compliance with\n\nregulatory standards. It enables organizations to respond promptly\n\nto potential threats, constructing a resilient infrastructure to\n\nsafeguard the con\u0000dentiality and integrity of their data in the cloud.\n\nMacie is particularly useful for AWS-based environments with\n\nlarge, unstructured datasets due to its scalability, automated data\n\ndiscovery, and seamless integration with other AWS services. It\n\nprovides built-in data identi\u0000ers, while also allowing the creation of\n\ncustom ones to meet speci\u0000c needs.\n\nIn this recipe, we will learn how to create AWS Macie’s job to\n\nidentify sensitive data in our S3 bucket, speci\u0000cally email addresses.\n\nGetting ready\n\nFor this recipe, you need to have an S3 dataset with email addresses\n\n(dummy data) and other potential PII data (such as credit card\n\ninformation), and you have to enable AWS Macie if you are using it\n\nfor the \u0000rst time following the outlined steps:\n\n1. Log in to the AWS Management Console and navigate to the AWS Macie\n\nservice.\n\n2. Click on Get started.\n\n3. Click on Enable Macie.",
      "content_length": 1177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 418,
      "content": "How to do it…\n\n1. Log in to the AWS Management Console and navigate to the AWS Macie\n\nservice.\n\n2. From the le navigation pane, choose Jobs under the S3 buckets section.\n\n3. Click on Create job. You will get a con\u0000rmation message saying that\n\ncreating a job is not included in the free trial; acknowledge the message\n\nby clicking on Yes.\n\n4. Under Choose S3 buckets, click on Select speci\u0000c buckets, choose your\n\nS3 bucket, and click on Next as shown:\n\nFigure 6.1 – Selecting an S3 bucket for Macie’s job\n\n5. On the Review S3 buckets page, your S3 bucket will be listed with its\n\nestimated cost. Review the values and click on Next.\n\n6. In the Re\u0000ne the scope page, under Sensitive data discovery options, you\n\nchoose either to have a scheduled job where you can set up the update\n\nfrequency for the job or have a one-time job that will run the job only",
      "content_length": 854,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 419,
      "content": "once. Optionally, you can set up a criterion for the object the job will run\n\non by expanding the Additional settings section and choosing Object\n\ncriteria.\n\nFigure 6.2 – Setting up the job run frequency\n\n7. For Select managed data identi\u0000ers, under Managed data identi\u0000er\n\noptions, you can either choose to use all of the managed data identi\u0000ers\n\nMacie provides by clicking on Recommended, you can choose to select\n\nthe speci\u0000c identi\u0000ers you want to apply, or even choose not to apply any\n\nidenti\u0000ers by clicking on Custom. For this recipe, we will go with the\n\nrecommended option to identify any potential PII data. Click on Next.",
      "content_length": 633,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 420,
      "content": "Figure 6.3 – Choosing the managed data identiﬁers\n\n8. Managed data identi\u0000ers will not detect email addresses; so, we will\n\ncreate a custom identi\u0000er for that by clicking on Manage custom\n\nidenti\u0000ers under Custom data identi\u0000ers and do the following:\n\nI. Click on Create.\n\nII. Give a name and description to your identi\u0000er.\n\nIII. Enter (?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.\n\n[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\n\n\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\n\n\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\n\n\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-\n\nz0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\n\n|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-\n\n9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4]\n\n[0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*",
      "content_length": 703,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 421,
      "content": "[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\n\n\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\n\n\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\]) in the Regular\n\nexpression \u0000eld, which will be used to identify the email\n\naddresses.\n\nIV. Optionally, you can choose to add keywords and ignore words,\n\nwhich are words that Macie will ignore in the text even if they\n\nmatch the pattern, and the maximum match distance, which is\n\nthe number of characters that can appear between the keyword\n\nto ignore and the sensitive data. If the distance between the\n\nkeyword and the sensitive data is within this speci\u0000ed limit,\n\nMacie will ignore the match.\n\nV. For Severity, you can keep the default value of using the\n\nMinimum severity setting for any number of matches or use a\n\ncustom setting to determine severity.\n\nVI. Optionally, add tags to label your identi\u0000er and click on Submit.\n\nVII. Go back to the main page and click on the refresh mark to\n\nre\u0000ect the new identi\u0000er, select it, and click on Next.\n\n9. Under Select allow lists, you can optionally create an allow list to ignore\n\nspeci\u0000c text or text patterns by clicking on Manage allow lists.\n\n10. Under the Review and create page, review all the values you have\n\nprovided and click on Submit.\n\n11. e job will be in running status for some time depending on the data\n\nsize. Once it moves to completed status, click on Show results and then",
      "content_length": 1343,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 422,
      "content": "Show \u0000ndings, which will give you a report of the identi\u0000ed sensitive\n\ndata within your S3 bucket.\n\nHow it works…\n\nWe kickstarted the job creation process by de\u0000ning the targeted\n\ndata scope for Macie to scan, specifying a particular S3 bucket.\n\nMoving forward with the job con\u0000guration, we chose to leverage all\n\nof Macie’s managed data identi\u0000ers and introduced our custom data\n\nidenti\u0000er since the existing ones lack a pattern for recognizing\n\nemail addresses. To address this, we de\u0000ned a regular expression:\n\nthe \u0000rst part (before @) matches the local part of email addresses,\n\nallowing both alphanumeric and special characters, which can be\n\nenclosed in quotes or not. e second part matches the domain,\n\nsupporting both standard domain names with subdomains or an IP\n\naddress. Aer submitting the job, Macie scanned the data within\n\nour designated bucket to locate sensitive data in accordance with\n\nour chosen data identi\u0000ers and its machine learning algorithms.\n\nUpon completion, it gave us a comprehensive report detailing the\n\nidenti\u0000ed sensitive data.\n\nThere’s more…\n\nYou can ensure that sensitive data discovered by Macie is promptly\n\nhandled by con\u0000guring alerts and remediation actions. You can set",
      "content_length": 1213,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 423,
      "content": "up an EventBridge rule to be triggered when Macie discovers\n\nsensitive data, which in turn triggers a Lambda function that\n\nremediates it, such as by encrypting the data or deleting it. For\n\nalerts, you can con\u0000gure SNS topics to notify you of Macie’s\n\n\u0000ndings.\n\nSee also\n\nProcessing Macie \u0000ndings with Amazon EventBridge:\n\nhttps://docs.aws.amazon.com/macie/latest/user/\u0000ndings-monitor-\n\nevents-eventbridge.html\n\nStoring and retaining sensitive data discovery results with Amazon Macie:\n\nhttps://docs.aws.amazon.com/macie/latest/user/discovery-results-\n\nrepository-s3.html\n\nEstablishing a tagging strategy for AWS resources\n\nEstablishing a comprehensive resource tagging strategy is pivotal in\n\nthe eﬃcient organization, management, and optimization of AWS\n\nresources. It simpli\u0000es the identi\u0000cation and oversight of resources\n\nwhile facilitating precise cost attribution to designated projects,\n\nteams, or business units. To implement an eﬀective tagging strategy,\n\nit is imperative to assign metadata labels, structured as key-value\n\npairs, to diverse AWS resources based on speci\u0000c attributes or",
      "content_length": 1098,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 424,
      "content": "criteria. is strategic approach is fundamental for achieving\n\noperational eﬃciency, cost-eﬀectiveness, and governance within the\n\ncloud environment. Subsequently, employing proactive and reactive\n\nmeasures aligning with the established tagging strategy ensures the\n\nsystematic tagging of resources.\n\nIn this recipe, we will learn how to proactively tag resources (EC2,\n\nS3, Lambda) using Lambda function.\n\nGetting ready\n\nis recipe assumes you have AWS CLI installed and con\u0000gured\n\nwith the IAM pro\u0000le, and the IAM role used in Lambda must have\n\nthe following outlined policy:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowLambdaTagging\", \"Effect\": \"Allow\", \"Action\": [ \"lambda:TagResource\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowEC2Tagging\", \"Effect\": \"Allow\", \"Action\": [",
      "content_length": 784,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 425,
      "content": "\"ec2:CreateTags\", \"ec2:DescribeInstances\", \"ec2:DescribeVolumes\" ], \"Resource\": \"*\" }, { \"Sid\": \"AllowS3Tagging\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetBucketTagging\", \"s3:PutBucketTagging\" ], \"Resource\": \"arn:aws:s3:::*\" }, { \"Sid\": \"AllowCloudTrailLookup\", \"Effect\": \"Allow\", \"Action\": [ \"cloudtrail:LookupEvents\" ], \"Resource\": \"*\" } ] }\n\nAdditionally, CloudTrail must be enabled. You can follow this guide\n\nto enable it:\n\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudt\n\nrail-create-a-trail-using-the-console-\u0000rst-time.html.\n\nHow to do it…",
      "content_length": 561,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 426,
      "content": "1. Create a Lambda function with the help of the following steps:\n\nI. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?\n\nnc2=h_ct&src=header-signin) and navigate to the Lambda\n\nservice.\n\nII. Click on Create function.\n\nIII. Select Author from scratch in the Basic information section,\n\ngive a name for the function, set the Runtime option to Python\n\n3.11, and select x86_64 as Architecture.\n\nIV. Click on Create function.\n\nV. In the code editor, replace the existing code with the following\n\ncode, which only handles tagging Lambda functions (you can\n\nrefer to the GitHub code for the full code to handle S3 and EC2\n\ntagging):\n\nimport json import boto3 import logging from botocore.exceptions import ClientError logger = logging.getLogger() logger.setLevel(logging.INFO) def get_user_name(event): if 'userIdentity' in event['detail']: if event['detail'] ['userIdentity']['type'] == 'AssumedRole': user_name = str('UserName: '",
      "content_length": 958,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 427,
      "content": "+ event['detail']['userIdentity'] ['principalId'].split(':')[1] + ', Role: ' + event['detail']['userIdentity'] ['sessionContext']['sessionIssuer'] ['userName'] + ' (role)') elif event['detail'] ['userIdentity']['type'] == 'IAMUser': user_name = event['detail'] ['userIdentity']['userName'] elif event['detail'] ['userIdentity']['type'] == 'Root': user_name = 'root' else: logging.info('Could not determine username (unknown iam userIdentity) ') user_name = '' else: logging.info('Could not determine username (no userIdentity data in cloudtrail') user_name = '' return user_name def lambda_handler(event, context): client = boto3.client('cloudtrail') resource_type = event[\"detail\"] [\"eventSource\"] user_name=get_user_name(event) if resource_type == \"lambda.amazonaws.com\": resource_arn = event[\"resources\"][0] resource_name = event[\"detail\"] [\"configurationItem\"][\"configuration\"] [\"functionName\"] try:",
      "content_length": 903,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 428,
      "content": "client = boto3.client('lambda') client.tag_resource( Resource=resource_arn, Tags={'Created_by': user_name} ) logging.info(f\"Lambda function {resource_name} tagged with username : {user_name}\") except ClientError as e: logging.error(f\"Error tagging Lambda function {resource_name}: {e}\")\n\nVI. Click on Deploy.\n\nVII. Go to the Con\u0000guration tab and open the IAM role under\n\nExecution Role.\n\nVIII. Click on Add permissions.\n\n2. Create EventBridge rule with the next steps:\n\nI. Navigate to the EventBridge service and click on Create rule.\n\nII. Give a name and optionally a description for the rule, keep\n\nEvent bus as default, select Rule with an event pattern, and then\n\nclick on Next.\n\nIII. Under Creation method, select Custom pattern (JSON editor),\n\npaste the following rule, and click on Next:\n\n{ \"source\": [\"aws.s3\", \"aws.lambda\", \"aws.ec2\"],",
      "content_length": 844,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 429,
      "content": "\"detail-type\": [\"AWS API Call via CloudTrail\"], \"detail\": { \"eventSource\": [\"s3.amazonaws.com\", \"lambda.amazonaws.com\", \"ec2.amazonaws.com\"], \"eventName\": [\"CreateBucket\", \"CreateFunction\", \"RunInstances\"] } }\n\nIV. Under Target 1, choose AWS service, select Lambda function,\n\nthen select the function you have created, and then click on\n\nNext and Create rule.\n\nIf you go back to your Lambda function, an EventBridge rule\n\ntrigger will be added.\n\nHow it works…\n\nWe enabled CloudTrail to log all write actions to our services,\n\nincluding the creation of Lambda functions, S3 buckets, and EC2\n\ninstances. en, we set up an EventBridge rule that listens for these\n\nCloudTrail events related to the creation of these resources. is\n\nrule triggers a Lambda function, which checks the type of event it\n\nreceives. Based on the event type, the Lambda function retrieves the\n\ncurrent tags for the resource and appends a tag with the user\n\nidentity that created the resource, as obtained from the CloudTrail\n\nevent.",
      "content_length": 1004,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 430,
      "content": "There’s more…\n\nYou can have additional tags added to your resources that you can\n\nget from CloudTrail events, such as the created time or AWS region,\n\nand you can add other taggable resources to the process.\n\nSee also\n\nBuilding your tagging strategy:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/tagging-best-\n\npractices/building-your-tagging-strategy.html\n\nBuilding your distributed data community with Amazon DataZone following data mesh principles\n\nData mesh is a decentralized approach to data architecture, aiming\n\nto address the challenges of traditional centralized models by\n\ndistributing responsibility for data and treating it as a product,\n\nwhich shis the responsibility from a central team to domain-\n\nspeci\u0000c teams, leveraging domain expertise. Key principles in data\n\nmesh involve the following:\n\nDomain-oriented decentralization where each domain will be\n\nresponsible for creating their data as a product and making it available to\n\nothers",
      "content_length": 958,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 431,
      "content": "Self-serve data infrastructure that empowers each domain to\n\nindependently create, manage, and utilize their own data pipelines\n\nwithout relying on a central team\n\nA federated computational governance model where each domain is\n\nresponsible for the governance of its data, ensuring compliance with\n\nglobal standards while maintaining domain-speci\u0000c requirements\n\nAmazon DataZone, based on data mesh principles, provides a\n\nmanaged platform for data governance and access control.\n\nIn this recipe, we will learn how to use DataZone to set up a\n\ndomain, publish Glue tables, and make it accessible to others.\n\nGetting ready\n\nTo complete this recipe, you need to have a Glue table that is\n\nmanaged by Data Lake (Data Lake permission mode, not hybrid\n\nmode), which means the S3 location of the table must be registered\n\non Lake Formation and permission to IAMAllowedPrincipals\n\nmust be revoked (refer to the Getting ready section of the\n\nSynchronizing Glue Data Catalog to a diﬀerent account recipe in\n\nChapter 2, Sharing Your Data Across Environments and Accounts).\n\nHow to do it…",
      "content_length": 1077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 432,
      "content": "is recipe involves diﬀerent tasks, as discussed under each\n\nsubsection.\n\nCreating a domain with the next steps\n\n1. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-\n\nsignin) and navigate to the DataZone service.\n\n2. Click on Create domain.\n\n3. Enter a name and optionally a description for your domain.\n\n4. Under the Quick setup section, select Set-up this account for data\n\nconsumption and publishing, which will enable Data Lake and Data\n\nWarehouse blueprints. It will create default environment pro\u0000les that are\n\nopen to all users.\n\n5. Click on Create domain. Once your domain is created, you can click on\n\nOpen data portal to start building the catalog.\n\nFigure 6.4 – Domain summary\n\nPublishing data to your domain",
      "content_length": 773,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 433,
      "content": "1. Create a project:\n\nI. Open the data portal of your domain.\n\nII. Click on Create project.\n\nIII. Enter a name and optionally a description for your project.\n\nFigure 6.5 – Creating a project\n\n2. Create environment:\n\nI. On your project page, click on CREATE ENVIRONMENT.\n\nII. Enter a name and optionally a description for your\n\nenvironment.\n\nIII. Under Environment pro\u0000le, choose DataLakeProfile.\n\nIV. Click on CREATE ENVIRONMENT.",
      "content_length": 429,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 434,
      "content": "Figure 6.6 – Creating an environment for the project\n\n3. Create data source:\n\nI. Go to the Data tab in the portal.\n\nII. Enter a name and optionally a description for your data source.\n\nIII. Under Data Source type, choose AWS Glue.\n\nIV. Select the environment you created in the previous step in the\n\nSelect an environment section.\n\nV. Under Data Selection, write down the name of the Glue\n\ndatabase you need to add, and under Table selection criteria,\n\nchoose Include * to include all the tables in your database and\n\nclick on Next.",
      "content_length": 532,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 435,
      "content": "Figure 6.7 – Selecting a data source type\n\nVI. Under the Publishing setting section, select No for Publish assets to the\n\ncatalog, which will allow us to review and edit the assets before\n\npublishing them. Select automated business name generation and click\n\non Next.",
      "content_length": 267,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 436,
      "content": "Figure 6.8 – Publishing setting\n\nVII. Under Run preference, you can choose to run the data source on\n\ndemand or to run it based on a schedule, and then click on Next.\n\nVIII. Review all the details you have provided and click on Create.\n\nIX. Click on RUN to trigger the data source. Once the run is complete, your\n\nGlue database tables will be added as assets.\n\nX. Open each asset created on your data source and review the business\n\nname that was generated for you and the business attribute names in the\n\nSchema tab. Accept or edit them if required. You can also provide\n\nmetadata to your assets such as a README \u0000le and glossary terms.\n\nOnce done with the changes, click on PUBLISH ASSET.\n\nXI. To allow DataZone to grant access to consumers on your behalf, grant\n\nthe DataZone access role, <AmazonDataZoneGlueAccess-\n\n<region>-<domainId>, the following permissions through Lake\n\nFormation, as shown in Figure 6.9:\n\nDatabase permissions\n\nTable permissions",
      "content_length": 956,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 437,
      "content": "Figure 6.9 – DataZone Lake Formation permissions\n\nConsuming data from the domain\n\n1. Log in to the AWS Management Console using the consumer user or\n\nrole and navigate to the DataZone service.\n\n2. Create a new project and environment for the consumer following the\n\nsame steps done for the producer.\n\n3. In the search bar, search for the table/asset you have added as a\n\nproducer, add a justi\u0000cation for subscribing to the data under the\n\nReason for request \u0000eld, and click on SUBSCRIBE.\n\nApproving consumer access request",
      "content_length": 522,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 438,
      "content": "1. Navigate to the DataZone portal and choose the project with the\n\npublished data asset.\n\n2. Go to the Data tab and click on Incoming requests from the navigation\n\npane on the le. You should be able to see the request under\n\nREQUESTED – click on View request and approve it.\n\nHow it works…\n\nWe initiated a domain through the quick setup, allowing DataZone\n\nto automate the creation of IAM roles and facilitate domain access\n\nto AWS resources such as Glue, Athena, and Redshi. As\n\npublishers, we con\u0000gured a collaborative project for managing asset\n\naccess and ownership. Within the project, we created an\n\nenvironment using a prede\u0000ned template pro\u0000le to host our\n\nresources. DataZone background processes generated IAM roles, S3\n\nsuﬃxes, Athena workgroups, and AWS Glue databases,\n\nstreamlining Data Lake operations. We proceeded to create a data\n\nsource that de\u0000ned the data to bring, adding assets visible only to\n\nproject members until published to the catalog for wider discovery.\n\nAs a subscriber, we established our own project and environment,\n\nsubscribing to assets that notify publishers for approval. Once\n\napproved by publishers, assets were then accessible for querying in\n\nAthena within the environment.\n\nThere’s more…",
      "content_length": 1235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 439,
      "content": "Aer publishing an asset, you can still edit its business or technical\n\nvalues, which will create a new version of your asset that you will\n\nhave to publish to make it discoverable.\n\nSee also\n\nSecurity in DataZone:\n\nhttps://docs.aws.amazon.com/datazone/latest/userguide/security.html\n\nMonitoring Amazon DataZone:\n\nhttps://docs.aws.amazon.com/datazone/latest/userguide/monitoring-\n\noverview.html\n\nHandling security-sensitive data (PII and PHI)\n\nRedshi data tokenization is a crucial aspect of data security and\n\nprivacy within AWS Redshi to protect the con\u0000dentiality of\n\ncritical data. Tokenization involves substituting sensitive\n\ninformation with unique identi\u0000ers or tokens, preserving the\n\nformat and length of the original data without revealing the actual\n\nsensitive information. Tokenization is necessary to mitigate the risk\n\nof data breaches, comply with regulatory requirements, and\n\nmaintain customer trust by ensuring their personal information is\n\nsecure. Leveraging AWS Lambda user-de\u0000ned functions (UDFs) for",
      "content_length": 1025,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 440,
      "content": "data tokenization in Redshi provides a scalable and eﬃcient\n\nsolution for protecting the data.\n\nIn this recipe, we will learn how to create a Lambda UDF to use in\n\nRedshi for tokenization.\n\nGetting ready\n\nTo complete this recipe, you must have a Redshi cluster with a\n\ntable in which you need to tokenize its data. You need to have a\n\nCloud9 environment as well, please create one following the\n\noutlined steps:\n\n1. Log in to the AWS Management Console and navigate to the Cloud9\n\nservice.\n\n2. Click on Create environment.\n\n3. Give a name and a description for the environment.\n\n4. Select New EC2 instance, then select the t2.micro instance and\n\nAmazon Linux 2 for the Platform setting.\n\n5. Keep everything else set to default and click on Create.\n\nHow to do it…\n\n1. Create a secret key in Secrets Manager:\n\nI. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?",
      "content_length": 897,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 441,
      "content": "nc2=h_ct&src=header-signin) and navigate to Secret manager.\n\nII. Click on Store new secret.\n\nIII. Choose Other type of secret for Secret type. Under Key/value\n\npairs, type secret_key and the value you would like to use\n\nfor the key. It should be a long, complex value, at least 32\n\ncharacters of mixed uppercase and lowercase characters, digits,\n\nand special characters. en, click on Next.\n\nIV. Give a name for the secret and then click on Next. Review the\n\ndetails and click on Store.\n\n2. Create a Lambda layer:\n\nI. Navigate to Cloud9 service.\n\nII. Open a Linux environment and run the following commands.\n\nReplace aws_region with the region you will use to create\n\nthe Lambda function:\n\nsudo amazon-linux-extras install python3.8 curl -O https://bootstrap.pypa.io/get- pip.py python3.8 get-pip.py --user mkdir python python3.8 -m pip install pyffx -t python/ zip -r layer.zip python aws lambda publish-layer-version -- layer-name pyffx-layer --zip-file fileb://layer.zip --compatible-runtimes python3.8 --region <aws_region>",
      "content_length": 1027,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 442,
      "content": "3. Create a Lambda function:\n\nI. Navigate to the Lambda service.\n\nII. Click on Create function.\n\nIII. Select Author from scratch.\n\nIV. In the Basic information section, give a name for the function,\n\nset the Runtime setting as Python 3.11, and select x86_64 as\n\nArchitecture.\n\nV. Click on Create function.\n\nVI. Under Layers in your Lambda function’s Code tab, select Add a\n\nlayer.\n\nVII. Select Custom layers, choose pyffx-layer created in the\n\nprevious step, and select the latest version you have created.\n\nVIII. In the code editor, replace the existing code with the following\n\ncode:\n\nimport boto3 import json import numbers import pyffx from botocore.exceptions import ClientError, BotoCoreError def get_secret(): try: client = boto3.client('secretsmanager') response = client.get_secret_value(SecretId='Redshi",
      "content_length": 813,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 443,
      "content": "ftTokenizationSecretKey') secret = json.loads(response['SecretString']) return secret['secret_key'] except (ClientError, BotoCoreError) as e: return json.dumps({'success': False, 'message': f'Failed to retrieve secret key: {e}'}) def encrypt_data(data, secret_key): alphabet = '0123456789abcdefghijklmnopqrstuvwxyz' try: if isinstance(data, numbers.Number): e = pyffx.Integer(secret_key.encode(), length=len(str(data))) else: e = pyffx.String(secret_key.encode(), alphabet=alphabet, length=len(data)) encrypted_text = e.encrypt(data) return encrypted_text except Exception as e: return json.dumps({'success': False, 'message': f'Failed to encrypt text: {e}'}) def lambda_handler(event, context): secret_key = get_secret() return_value = dict() response = [] for argument in event['arguments']: msg = argument[0] encrypted_text = encrypt_data(msg, secret_key)",
      "content_length": 858,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 444,
      "content": "response.append(json.dumps(encrypted_tex t)) return_value['success'] = True return_value['results'] = response return json.dumps(return_value) )\n\nIX. Click on Deploy.\n\n4. Create an IAM role for Redshi:\n\nI. Navigate to the IAM service.\n\nII. Select Policies under Access management from the navigation\n\npane on the le, and click on Create Policy.\n\nIII. Choose the JSON tab and in the Policy editor, replace the code\n\nwith the following code (make sure to add your Lambda\n\nfunction ARN, which you can get from the function overview),\n\nand click on Next:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Invoke\", \"Effect\": \"Allow\", \"Action\": [ \"lambda:InvokeFunction\" ], \"Resource\": \" <lambda_function_arn>\" }",
      "content_length": 711,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 445,
      "content": "] }\n\nIV. Give a policy name and optionally a description, and then click\n\non Create policy.\n\nV. Select Roles under Access management from the navigation\n\npane on the le, and click on Create Role.\n\nVI. Under Trusted entity type, choose AWS service.\n\nVII. Under Use case, choose Redshi - Customizable and click on\n\nNext.\n\nVIII. Select the policy you have created and click on Next.\n\nIX. Enter a role name and optionally a description, and click on\n\nCreate role.\n\n5. Associate the IAM role with the Redshi cluster:\n\nI. Navigate to the Redshi service.\n\nII. Open your Redshi cluster and go to the Properties tab.\n\nIII. Under the Cluster permissions, select Associate IAM roles from\n\nthe Manage IAM roles list. en, select the IAM role you have\n\ncreated in the previous step and click on Associate IAM roles.\n\n6. Create a UDF in Redshi:\n\nI. Connect to your Redshi cluster.\n\nII. Use the following commands to create the UDF:\n\nCREATE OR REPLACE EXTERNAL FUNCTION",
      "content_length": 961,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 446,
      "content": "PII_tokenize_str (value varchar) RETURNS varchar STABLE LAMBDA 'your_lambda_function_name' IAM_ROLE 'your_redshift_role_arn'; CREATE OR REPLACE EXTERNAL FUNCTION PII_tokenize_int (value int) RETURNS varchar STABLE LAMBDA 'your_lambda_function_name' IAM_ROLE 'your_redshift_role_arn';\n\nHow it works…\n\nWe developed a Lambda function capable of encrypting text or\n\nintegers using the pyffx library, which implements format-\n\npreserving encryption (FPE). is ensures that the encrypted data\n\nretains the same structure, type, and length as the original input.\n\nWe used Secrets Manager to securely store our secret key, which is\n\nused by pyffx internally to generate a pseudo-random\n\npermutation that is applied to the input text. We provided the\n\nalphabet for encrypting strings, which is used by pyffx to de\u0000ne\n\nwhich characters should appear in the encrypted text. For numeric\n\ndata identi\u0000ed using the Numbers library, no alphabet is needed, as\n\npyffx will keep the numeric representation of the input. Since\n\nLambda doesn’t include the pyffx library by default, we addressed\n\nthis by installing and packaging it within a Lambda layer, which has\n\nbeen added to the code. To enable Redshi to invoke this Lambda\n\nfunction, we established an IAM role providing the necessary",
      "content_length": 1271,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 447,
      "content": "permissions, which has been associated with the Redshi cluster.\n\nWithin the Redshi cluster, we created a function that references\n\nthe Lambda function, utilizing the IAM role we established. As a\n\nresult, Redshi is now equipped to call the function and encrypt the\n\ndata.\n\nThere’s more…\n\nIf you want to be able to reverse the tokenization, you can create a\n\ndecrypt function by following the same steps from How to do it…\n\nbut in the Lambda function; replace the encrypt_data function\n\nwith a decrypt_data function and update the Lambda handler\n\nusing the following code (refer to DataDecryptionLambda.py\n\n\u0000le in GitHub for the full Lambda function code):\n\ndef decrypt_data(token, is_number, secret_key): alphabet = '0123456789abcdefghijklmnopqrstuvwxyz' try: if is_number: d = pyffx.Integer(secret_key.encode(), length=len(str(token))) else: d = pyffx.String(secret_key.encode(), alphabet=alphabet, length=len(token)) decrypted_text = d.decrypt(token) return decrypted_text except Exception as e:",
      "content_length": 1000,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 448,
      "content": "return { 'success': False, 'error': str(e) } def lambda_handler(event, context): secret_key = get_secret() return_value = dict() response = [] for argument in event['arguments']: token = argument[0] is_number = argument[1] try: result = decrypt_data(token, is_number, secret_key) response.append(json.dumps(result)) except Exception as e: return { 'success': False, 'error': str(e) } return_value['success'] = True return_value['results'] = response return json.dumps(return_value)\n\nSee also\n\ne pyﬃx code repository: https://github.com/emulbreh/pyﬀx\n\nEnsuring S3 compliance with AWS Config",
      "content_length": 590,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 449,
      "content": "Ensuring that your AWS resources are con\u0000gured according to your\n\nspeci\u0000cations and best practices is crucial. AWS Con\u0000g facilitates\n\nthe governance of your resources by continuously evaluating your\n\nresources against your prede\u0000ned rules. It provides a\n\ncomprehensive view of your resources, enabling you to monitor and\n\ntake corrective actions if any resource deviates from these rules and\n\nbecomes non-compliant.\n\nIn Chapter 1, Managing Data Lake Storage, we discussed the\n\nimportance of encryption, life cycle policies, and access control for\n\nS3 buckets. In this recipe, we will learn how to implement AWS\n\nCon\u0000g rules to verify the compliance of S3 buckets with these\n\nstandards.\n\nGetting ready\n\nFor this recipe, you’ll need S3 buckets that you will monitor for\n\ncompliance, and you must enable AWS CloudTrail, as AWS Con\u0000g\n\nrelies on CloudTrail logs to track and record resource\n\ncon\u0000gurations.\n\nHow to do it…\n\n1. Set up the rules:\n\nI. Log in to the AWS Management Console\n\n(https://console.aws.amazon.com/console/home?",
      "content_length": 1026,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 450,
      "content": "nc2=h_ct&src=header-signin) and navigate to the AWS Con\u0000g\n\nservice.\n\nII. Click on Get started.\n\nIII. Under Recording strategy, select Speci\u0000c resource types and\n\nchoose AWS S3 Bucket for Resource type. For the Frequency\n\nsetting, select Daily.\n\nIV. Under Data governance, choose Create AWS Con\u0000g service-\n\nlinked role.\n\nV. Under Delivery method, either select Choose a bucket from\n\nyour account if you have an existing bucket for storing con\u0000g\n\ndata or select Create a bucket to create a new one. Specify the\n\nbucket name and an optional pre\u0000x for storing the data, which\n\nis helpful if you are using an existing bucket.\n\nVI. Click on Next. On the Rule page, under AWS Managed Rules,\n\nselect s3-bucket-level-public-access-\n\nprohibited and s3-bucket-server-side-\n\nencryption-enabled.\n\nVII. Select Next. Review the setting and click on Con\u0000rm.",
      "content_length": 841,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 451,
      "content": "Figure 6.10 – Reviewing conﬁg settings\n\n2. Enable remediation:\n\nI. From the navigation pane on the le of your AWS Con\u0000g\n\ndashboard, select Rules.\n\nII. Open one rule at a time, and from the Actions drop-down list,\n\nselect Manage remediation.\n\nIII. Choose either Automatic remediation for AWS Con\u0000g to take\n\naction automatically on your behalf or Manual remediation if\n\nyou want to review the non-compliant bucket before\n\nremediating it.",
      "content_length": 436,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 452,
      "content": "IV. Under Remediation action details, choose the following\n\nsettings:\n\nFor s3-bucket-level-public-access-\n\nprohibited, choose\n\nAWSConfigRemediation-\n\nConfigureS3BucketPublicAccessBlock\n\nFor s3-bucket-server-side-encryption-\n\nenabled choose aws-\n\nenables3bucketencryption and Bucketname\n\nunder Resource ID parameter\n\nV. Click on Save changes.\n\nHow it works…\n\nWe enabled AWS Con\u0000g speci\u0000cally to track S3 buckets and\n\nselected two managed rules to check if the buckets are publicly\n\naccessible and if server-side encryption is enabled. AWS Con\u0000g\n\ngenerated a dashboard displaying the resources in the account,\n\nfocusing on S3 buckets in this recipe. It showed the compliance\n\nstatus of your S3 buckets based on the rules set, scanning them\n\ndaily and \u0000agging compliant buckets accordingly while highlighting\n\nnon-compliant ones for remediation. Integration with AWS Systems\n\nManager Automation documents allowed us to de\u0000ne remediation\n\nactions for each rule to rectify non-compliant buckets. If you",
      "content_length": 997,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 453,
      "content": "identify a non-compliant bucket within a rule, you can easily\n\nremediate it by selecting it and clicking on the Remediate button.\n\nFigure 6.11 – S3 bucket remediation\n\nAdditionally, AWS Con\u0000g maintains a historical record of\n\ncon\u0000guration changes for your S3 buckets, providing valuable audit\n\ntrails. You can review these changes by accessing the S3 bucket via\n\nResource Inventory in the dashboard and examining Resource\n\nTimeline, which details CloudTrail events and con\u0000guration\n\nchanges.\n\nFigure 6.12 – S3 bucket historical events\n\nThere’s more…",
      "content_length": 549,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 454,
      "content": "If you require a custom rule that isn’t available among AWS-\n\nmanaged rules, you can create it using AWS Lambda. Let’s create a\n\nrule to monitor whether our S3 buckets have life cycle policies\n\ncon\u0000gured to either delete objects aer a speci\u0000ed time or archive\n\nthem:\n\n1. Create a Python Lambda function:\n\nI. Create a Python function from scratch with a new IAM role.\n\nII. Add the following policies for the function IAM role:\n\nAWSConfigRulesExecutionRole AWS-\n\nmanaged policy\n\nRun the following command in AWS CLI or AWS\n\nCloudShell to create a resource-based policy that will\n\nallow Con\u0000g to invoke our function:\n\naws lambda add-permission -- function-name your_function_arn \\ --principal config.amazonaws.com \\ --statement-id AllowConfigInvoke \\ --action lambda:InvokeFunction \\ --source-account your_aws_account_id\n\nCreate an inline policy with the following permission:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\",",
      "content_length": 938,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 455,
      "content": "\"Action\": [\n\n\"s3:GetLifecycleConfiguration\" ], \"Resource\": [ \"*\" ] } ] }\n\nIII. Add the following code, which AWS Con\u0000g will invoke for each\n\nbucket:\n\nimport boto3 import json from botocore.exceptions import ClientError def lambda_handler(event, context): invoking_event = json.loads(event['invokingEvent']) configuration_item = invoking_event['configurationItem'] # Extract the bucket name from the configurationItem bucket_name = configuration_item['configuration'] ['name'] s3_client = boto3.client('s3') compliance_type = 'NON_COMPLIANT' try: lifecycle_policy = s3_client.get_bucket_lifecycle_configura tion(Bucket=bucket_name) for rule in lifecycle_policy.get('Rules', []):",
      "content_length": 677,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 456,
      "content": "if ('Transitions' in rule and any('StorageClass' in transition and transition.get('StorageClass', '') in ['GLACIER', 'GLACIER_DEEP_ARCHIVE'] for transition in rule['Transitions'])) or \\\n\n('NoncurrentVersionTransition' in rule and any('StorageClass' in transition and transition.get('StorageClass', '') in ['GLACIER', 'GLACIER_DEEP_ARCHIVE'] for transition in rule.get('NoncurrentVersionTransition', []))) or \\ ('Expiration' in rule and 'Days' in rule.get('Expiration', {})) or \\\n\n('NoncurrentVersionExpiration' in rule and 'NoncurrentDays' in rule.get('NoncurrentVersionExpiration', {})): compliance_type = 'COMPLIANT' message = 'The bucket has a lifecycle policy with expiration after specified days.' break else: message = 'The bucket lifecycle policy does not archive or delete objects.' except ClientError as e: message = f\"Error getting lifecycle configuration for bucket {bucket_name}: {e}\" evaluation_result = {",
      "content_length": 918,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 457,
      "content": "'ComplianceResourceType': 'AWS::S3::Bucket', 'ComplianceResourceId': bucket_name, 'ComplianceType': compliance_type, 'OrderingTimestamp': configuration_item['configurationItemCap tureTime'], 'Annotation': message } config_client = boto3.client('config') response = config_client.put_evaluations( Evaluations=[evaluation_result], ResultToken=event['resultToken'] ) return response\n\n2. Set up a Con\u0000g custom rule:\n\nI. From the le navigation pane of AWS con\u0000g, select Rules.\n\nII. Select Add rule, choose Create custom Lambda rule for the\n\nRule type, and click on Next.\n\nIII. Provide a name for the rule and enter the ARN of the Lambda\n\nfunction under Lambda function ARN.\n\nIV. Enable Turn on detective evaluation under Evaluation mode,\n\nselect When con\u0000guration changes for Trigger type, and choose\n\nResources for Scope of change. Select AWS resource under\n\nResource category and specify AWS S3 bucket for Resource\n\ntype.",
      "content_length": 919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 458,
      "content": "Figure 6.13 – Custom rule evaluation mode\n\nV. Select Next, review the details, and click on Save.\n\nYou will be able to track this rule along with the managed con\u0000g\n\nrules.\n\nSee also\n\nConformance Packs:\n\nhttps://docs.aws.amazon.com/con\u0000g/latest/developerguide/conformance\n\npacks.html\n\nOceanofPDF.com",
      "content_length": 298,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 459,
      "content": "7 Data Quality Management\n\nUnreliable data can lead to incorrect insights, misguided business\n\ndecisions, and a signi\u0000cant loss of resources. As organizations treat\n\ndata as a product and rely more on data freshness, data engineers\n\nand analysts must implement robust data quality control\n\nmechanisms to ensure the data’s accuracy, completeness,\n\nconsistency, and reliability to maintain high data quality standards.\n\nIn this chapter, we will explore various methods and tools available\n\non AWS for maintaining data quality. We’ll provide step-by-step\n\nrecipes to help you implement these tools eﬀectively in your data\n\nengineering work\u0000ows. e recipes will guide you through practical\n\nexamples, starting with data quality control using AWS DataBrew,\n\nDeequ, and Glue. Before diving into the chapter, it is important to\n\nwork with your stakeholders to build a data quality control\n\nframework and an SLA for your data quality. When you lead a data\n\nquality project, besides identifying the data owners, you need to\n\nwork with the data owners to create a process to manage and ensure\n\nthe data quality. For this chapter, we won’t explore the theoretical\n\ndetails of building the data quality scorecards or de\u0000ning the\n\nmetrics and process for a data quality project. It would be bene\u0000cial",
      "content_length": 1287,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 460,
      "content": "for you to explore the book DMBoK – Data Management Body of\n\nKnowledge for supplementary knowledge on how to work with\n\nstakeholders on running a data quality project.\n\nWe will cover the following recipes in this chapter:\n\nCreating data quality for ETL jobs in AWS Glue Studio notebooks\n\nUnit testing your data quality using Deequ\n\nSchema management for ETL pipelines\n\nBuilding unit test functions for ETL pipelines\n\nBuilding data cleaning and pro\u0000ling jobs with DataBrew\n\nTechnical requirements\n\nYou can \u0000nd the code \u0000les for this chapter on GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter07.\n\ne dataset for this chapter is available at\n\nhttps://github.com/tidyverse/dplyr/blob/main/data-\n\nraw/starwars.csv.\n\nCreating data quality for ETL jobs in AWS Glue Studio notebooks",
      "content_length": 827,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 461,
      "content": "From the Applying a data quality check on Glue tables recipe in\n\nChapter 6, Governing Your Platform, we learned how to set a ruleset\n\nfor the Glue pipeline. In this recipe, we will dive deeper into how to\n\nuse Glue Studio notebooks to build a Data Quality template. Using\n\nGlue Studio is useful because you can see the output along with the\n\ndataset that you are testing. We will also introduce how to use\n\ncaching and produce row-level and rule-level outputs. e row-level\n\noutput would be suitable for using data quality rule violations for\n\neach of the records.\n\nGetting ready\n\nBefore proceeding with this recipe, go through the Applying a data\n\nquality check on Glue tables recipe in Chapter 6, Governing Your\n\nPlatform, and ensure that you have basic knowledge of how Glue\n\nworks as covered in Chapter 3, Ingesting and Transforming Your\n\nData with AWS Glue. In this recipe, we will provide the code to run\n\nthe quality check scenarios, so you do not need to follow the steps\n\nto create ruleset steps from Chapter 6.\n\nIn this recipe, we will assume that you already have the relevant\n\nIAM for Glue and a dataset against which to run Data Quality. e\n\npublic dataset we will use as an example is starwars.csv that is\n\nrenamed as star_wars_characters.csv.",
      "content_length": 1257,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 462,
      "content": "How to do it…\n\n1. Head to the Glue console and click on Author using an interactive code\n\nnotebook:\n\nFigure 7.1 – Clicking on the script editor to edit the code\n\n2. Instead of using the console to create a ruleset as we did in Chapter 6, in\n\nthis recipe, we will examine how to use the Glue Studio notebook. e\n\n\u0000rst step is to import the sample data quality notebook from Recipe1\n\nin this chapter’s GitHub folder and select the relevant IAM role that you\n\ncreated in Chapter 3 or Chapter 6.",
      "content_length": 491,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 463,
      "content": "Figure 7.2 – Uploading the sample notebook from GitHub\n\n3. e next step is to set up an interactive session and import relevant\n\nlibraries. For this recipe, the code is edited.",
      "content_length": 176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 464,
      "content": "Figure 7.3 – Setting up a Glue session\n\n4. For this recipe, we will create rules to compare two DataFrames with\n\neach other as shown. We will call these two DataFrames main and\n\nreference.\n\nFigure 7.4 – Creating two DataFrames for evaluating\n\n5. e following code in this recipe will go through various data quality\n\ncheck scenarios by creating a ruleset in the Data Quality De\u0000nition\n\nLanguage (DQDL) format. We will \u0000rst go through the rules that will be\n\nused for comparing multiple DataFrames. Depending on your\n\norganization, you need to set your own rules. e ruleset de\u0000nes the\n\nfollowing rules:",
      "content_length": 602,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 465,
      "content": "RowCount = 42: Checks that the number of rows in the\n\ndataset is 42\n\nColumnCount = 14: Checks that the number of columns in\n\nthe dataset is 42\n\nReferentialIntegrity \"homeworld\"\n\n\"reference.homeworld\" = 1: Checks the data integrity\n\nwith the homeworld column in the reference dataset\n\nReferentialIntegrity \"species\"\n\n\"reference.species\" = 1: Checks the data integrity\n\nwith the species column in the reference dataset\n\nFigure 7.5 – Setting up data quality rules\n\n6. For the previous rules to work, you need to add the data frame that is\n\nrequired for the data quality evaluation. In this case, the reference\n\ndataset will be used as an additional source:\n\nEvaluateDataQuality_additional_sources = { \"reference\": df_reference",
      "content_length": 723,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 466,
      "content": ", }\n\n7. Next, we will validate the datasets against the ruleset. We will add the\n\nfollowing options:\n\nChange the option\n\nenableDataQualityResultsPublishing': False.\n\nis option should not be True while using Glue Studio\n\nNotebook because you do not have a Glue job to run within a\n\nnotebook. us, if this option is set to True, it will produce an\n\nerror.\n\nSet observations.scope\": \"ALL\". Specify the scope of\n\nthe observations that will be collected during the execution of\n\nthe AWS Glue job. e ALL value indicates that all observations\n\nwill be collected.\n\nUse cache input to improve the performance of the evaluation.\n\nis is useful if the input data is large or the assessment needs\n\nto be run multiple times by enabling the CACHE_INPUT\n\noption.",
      "content_length": 749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 467,
      "content": "Figure 7.6 – Using the EvaluateDataQuality function to evaluate\n\ntwo DataFrames\n\n8. Instead of producing only a single dataset, we will also use\n\nSelectFromCollection to produce rowLevelOutcomes and\n\nruleOutcomes, as shown from cell 21 and onward in the notebook.\n\nYou can use the notebook to reference how these two methods’ outputs\n\nwould be diﬀerent from each other.\n\nFigure 7.7 – A sample of result of ruleOutcomes\n\nFor more information on the output of Glue Data Quality, please\n\ncheck the Applying a data quality check on Glue tables recipe in\n\nChapter 6, Governing Your Platform.\n\nHow it works…\n\nWhen the CACHE_INPUT option is enabled, the input\n\nDynamicFrame is cached in memory during the \u0000rst execution of",
      "content_length": 715,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 468,
      "content": "the data quality evaluation. Subsequent executions of the data\n\nquality evaluation on the same input DynamicFrame will use the\n\ncached data, which can signi\u0000cantly reduce the processing time.\n\nis caching mechanism can be particularly useful in the following\n\nscenarios:\n\nWhen running the data quality evaluation multiple times on the same\n\ninput data, such as during iterative development or testing\n\nWhen the input data is large and loading it from the source every time\n\ncan be time-consuming\n\nWhen you want to optimize the performance of the data quality\n\nevaluation process and reduce the overall processing time\n\ne key diﬀerence between rowLevelOutcomes and\n\nruleOutcomes is that rowLevelOutcomes contains the original\n\ninput data with additional columns that indicate the data quality\n\nevaluation results at the row level.\n\nis output is useful when you want to identify speci\u0000c rows that\n\nfailed the data quality checks and understand the reasons for the\n\nfailures.\n\nRemember, ruleOutcomes contains the overall data quality\n\nevaluation results at the rule level. For each rule in the data quality\n\nruleset, it provides the following information:\n\nRule: e name of the data quality rule",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 469,
      "content": "Outcome: e pass/fail status of the rule\n\nFailureReason: e reason why the rule failed (if applicable)\n\nEvaluatedMetrics: Any metrics or statistics calculated as part of\n\nthe rule evaluation\n\nis output is useful when you want to understand the overall\n\nperformance of the data quality rules and identify which rules are\n\nfailing, along with the reasons for the failures.\n\nThere’s more…\n\nEvaluateDataQuality.DATA_QUALITY_RULE_OUTCOMES_KEY\n\nis used to access the rule-level outcomes from the output of the\n\nEvaluateDataQuality transform in AWS Glue. It provides access\n\nto the overall data quality evaluation results at the rule level. is\n\nincludes information such as the following:\n\ne name of the data quality rule\n\ne pass/fail status of the rule\n\ne reason for rule failure (if applicable)\n\nAny metrics or statistics calculated as part of the rule evaluation\n\ne following code snippet will use the assert statement to evaluate\n\nthe check if there are any failed data quality rules and then raise an\n\nexception error. is code is useful when integrated into an Extract,",
      "content_length": 1074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 470,
      "content": "Transform, Load (ETL) pipeline to ensure the pipeline passes the\n\ndata quality check. To turn the code from Glue Studio into an ETL\n\njob, click on the Script tab. You will see the job.commit() line\n\nadded:\n\nassert EvaluateDataQuality_output[EvaluateDataQuality.DA TA_QUALITY_RULE_OUTCOMES_KEY].filter(lambda x: x[\"Outcome\"] == \"Failed\").count() == 0, \"The job failed due to failing DQ rules\"\n\nIn the Glue Studio notebook , you can save the output to S3 with the\n\nfollowing code:\n\nglueContext.write_dynamic_frame.from_options( frame = rowLevelOutcomes_data, connection_type = \"s3\", connection_options = {\"path\": \"s3://sample-target/dq_outcome\"}, format = \"csv\")\n\nSee also\n\nMeasuring the performance of AWS Glue Data Quality for ETL pipelines:\n\nhttps://aws.amazon.com/blogs/big-data/measure-performance-of-aws-\n\nglue-data-quality-for-etl-pipelines/\n\nEvaluating data quality for ETL jobs in AWS Glue Studio:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/tutorial-data-quality.html",
      "content_length": 975,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 471,
      "content": "Unit testing your data quality using Deequ\n\nAmazon Deequ is an open source data quality library developed\n\ninternally at Amazon. e purpose of Deequ is to unit test data\n\nbefore feeding it to analytics use cases. Several analytics products\n\nsuch as DataBrew and Glue Data Quality were built upon the\n\nDeequ library to help serve the needs of data engineers and data\n\nscientists. See the Deequ GitHub page\n\n(https://github.com/awslabs/deequ) for more information.\n\nIn the previous recipe, we learned about Glue Data Quality. ere\n\nare several key considerations when choosing between AWS Glue\n\nData Quality and Deequ:\n\nManaged service versus open source library: AWS Glue Data Quality is a\n\nfully managed service built on top of the open source Deequ framework.\n\nDeequ is an open source library that you can use to implement data\n\nquality checks in your applications. Also, since Deequ is an open source\n\nlibrary, there are metrics that might be available on Deequ but are not\n\n(yet) available on AWS Glue Data Quality, such as RatioOfSums (at the\n\ntime of writing this book).\n\nDeployment and integration: AWS Glue Data Quality mainly focuses on\n\nthe Glue pipeline. In contrast, Deequ is an open source library that can\n\nbe used in any environment that supports the JVM, which allows users\n\nto have more \u0000exibility in deployment scenarios such as if you need to",
      "content_length": 1360,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 472,
      "content": "check for data quality in an on-prem/hybrid environment. If you’re\n\nlooking for a fully managed and scalable data quality solution that\n\nintegrates well with other AWS services, AWS Glue Data Quality would\n\nbe the better option.\n\nGetting ready\n\nDeequ depends on Java 8. Deequ version 2.x only runs with Spark\n\n3.1. Deequ metrics are computed using Apache Spark running on\n\nGlue or EMR. Scala or Python can be used over Spark for this\n\npurpose. As Deequ is an open source library, the update of new\n\nmetrics might not be the same across the supported language. us,\n\nthere might be metrics that are available in Scala prior to being\n\navailable in Python. In this recipe, we will use Scala to demonstrate\n\nexamples of how Deequ and Scala work together in a Sagemaker\n\nendpoint.\n\nHow to do it…\n\n1. Click on Author code with a script editor in the console:",
      "content_length": 852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 473,
      "content": "Figure 7.8 – Clicking on the script editor in the console\n\n2. Choose Spark engine and create a script.\n\n3. In the next screen, type DQ-test-scala under Name and choose the\n\nrelevant IAM role, Glue 3.0 as Glue version, and Scala as Language, as\n\nshown:\n\nFigure 7.9 – Setting up the Glue pipeline with Scala option",
      "content_length": 312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 474,
      "content": "4. Scroll to the Advance option. Under the Libraries section, add the\n\nDeequ package to the JARs path. e Deequ package is available on the\n\nMVN repository at\n\nhttps://mvnrepository.com/artifact/com.amazon.deequ/deequ. Make\n\nsure that you select the version that is compatible with your Spark\n\nversion.\n\nFigure 7.10 – Importing the S3 bucket to the dependent JARs\n\npath\n\n5. In the Script section, paste the following script:\n\nimport com.amazonaws.services.glue.GlueContext import com.amazonaws.services.glue.util.GlueArgParser import com.amazonaws.services.glue.util.Job import org.apache.spark.SparkContext import org.apache.spark.SparkConf import scala.collection.JavaConverters._ import",
      "content_length": 689,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 475,
      "content": "com.amazonaws.services.glue.log.GlueLogger import scala.util.matching.Regex import com.amazon.deequ.{VerificationSuite, VerificationResult} import com.amazon.deequ.VerificationResult.checkResul tsAsDataFrame import com.amazon.deequ.checks.{Check, CheckLevel} import com.amazon.deequ.constraints.ConstrainableData Types object GlueApp { def main(sysArgs: Array[String]) { val spark: SparkContext = new SparkContext() val glueContext: GlueContext = new GlueContext(spark) val args = GlueArgParser.getResolvedOptions(sysArgs, Seq(\"JOB_NAME\").toArray) Job.init(args(\"JOB_NAME\"), glueContext, args.asJava) val datasource0 = glueContext.getCatalogSource(database = \"dq- test\", tableName = \"test\", redshiftTmpDir = \"\", transformationContext = \"datasource0\").getDynamicFrame() //Deequ start val dataset: DataFrame = dframe.toDF() val verificationResult : VerificationResult = { VerificationSuite() .onData(df) .addCheck( Check(CheckLevel.Error, \"Customer Code Check\")",
      "content_length": 959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 476,
      "content": ".isComplete(\"CUSTOMER_CODE\") .isUnique(\"CUSTOMER_CODE\")) .addCheck( Check(CheckLevel.Error, \"Mobile Check\") .isComplete(\"PHONE_1\") .hasPattern(\"PHONE_1\",\"\"\"^[0][\\d] {9}$\"\"\".r, _>=0.9) .isUnique(\"PHONE_1\")) .addCheck( Check(CheckLevel.Error, \"Tax code Check\") .isComplete(\"TAX_ID\") .isUnique(\"TAX_ID\") .satisfies(\"length(`TAX_ID`) = 10 or length(`TAX_ID`) = 14\", \"is 10 or 14 digits\", Check.IsOne, None)) .run() // retrieve successfully computed metrics as a Spark data frame val resultDataFrame = checkResultsAsDataFrame(spark, verificationResult) glueContext.getSinkWithFormat( connectionType = \"s3\", options = JsonOptions(Map(\"path\" -> \"s3://dq-outputs/results/\")), format = \"csv\" ).writeDynamicFrame(resultDataFrame) Job.commit()\n\nHow it works…",
      "content_length": 747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 477,
      "content": "When you build a Glue pipeline, you can de\u0000ne which stage of the\n\npipeline Deequ will be used to capture data quality metrics.\n\nDeequ has three main components:\n\nMetrics computation: Deequ computes data quality metrics, that is,\n\nstatistics such as completeness, maximum, or correlation.\n\nConstraint veri\u0000cation: As a user, you focus on de\u0000ning a set of data\n\nquality constraints to be veri\u0000ed. Deequ takes care of deriving the\n\nrequired set of metrics and verifying the constraints.\n\nConstraint suggestion: You can choose to de\u0000ne your custom data\n\nquality constraints, or use the automated constraint suggestion methods\n\nthat pro\u0000le the data to infer useful constraints with\n\nConstraintSuggestionRunner.\n\ne code example in step 5 showcases how to use constraint\n\nveri\u0000cation.\n\nThere’s more…\n\ne following table shows some examples of Deequ rules that you\n\ncan use to check your data:",
      "content_length": 886,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 478,
      "content": "Figure 7.11 – Example of some common data quality checks\n\nSee also\n\nDeequ GitHub: https://github.com/awslabs/deequ/tree/master\n\nProgramming AWS Glue ETL scripts in Scala:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-\n\nscala.html\n\nUsing Scala to program AWS Glue ETL scripts:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/glue-etl-scala-using.html\n\nBuilding a serverless data quality and analysis framework with Deequ and\n\nAWS Glue: https://aws.amazon.com/blogs/big-data/building-a-\n\nserverless-data-quality-and-analysis-framework-with-deequ-and-aws-\n\nglue/\n\nSchema management for ETL pipelines\n\nIn this recipe, we will learn how to perform schema validation using\n\nApache Spark and AWS Glue. Schema validation is essential to\n\nensure the consistency and integrity of your data as it moves\n\nthrough various stages of the data pipeline. By validating schemas,\n\nyou can prevent data quality issues and ensure that downstream\n\napplications receive data in the expected format.",
      "content_length": 990,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 479,
      "content": "Without schema validation, once the data reaches Redshi or\n\nAthena, it will cause data corruption errors from, for example,\n\nduplicate columns or using a wrong datatype. Schema-on-read is a\n\nfeature of the modern data lake, which contrasts with the schema-\n\non-write that is traditionally used in on-prem data warehouses. In\n\nthe data lake environment, when the data moves through layers of\n\nthe data lake, you typically need to de\u0000ne the schema and store the\n\nde\u0000ned scheme either in a JSON con\u0000g \u0000le or in a database that the\n\nETL pipeline could later use to verify the schema of a \u0000le. Using\n\nSpark’s Infer schema is not always a good option, especially when\n\ndealing with columns representing currency. us, this recipe will\n\nintroduce to you how to manage your schema and what is the\n\nbene\u0000t of using Glue Schema Registry.\n\nGetting ready\n\nis recipe assumes that you have a Glue pipeline along with a Glue\n\ncatalog set up from previous recipes.\n\nHow to do it…\n\n1. Head to the Glue console and click on Schemas under Data Catalog:",
      "content_length": 1035,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 480,
      "content": "Figure 7.12 – Selecting schemas under stream schema registries\n\n2. Click on Create schema, \u0000ll in Schema name and Registry, and select\n\nJSON as Data format, as shown:",
      "content_length": 166,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 481,
      "content": "Figure 7.13 – Adding the new schema name, registry, and data\n\nformat\n\n3. In the \u0000rst schema version, add in your schema. Make sure you read the\n\nCreate Schema article\n\n(https://docs.aws.amazon.com/glue/latest/webapi/API_CreateSchema.ht\n\nml) so that you adhere to the syntax. Here is one example of a schema in\n\nJSON format:\n\n{ \"$schema\": \"http://json-schema.org/draft- 07/schema#\", \"type\": \"object\", \"properties\": { \"firstname\": { \"type\": \"string\" }, \"ID\": { \"type\": \"integer\" } } }\n\n4. Click on Create schema and choose Version 1.\n\n5. Once you have created a schema, you can use SchemaReference in\n\nthe request syntax when calling the CreateTable or UpdateTable\n\nAPIs, or through the Glue Data Catalog console under the relevant\n\ndatabase, as shown:",
      "content_length": 750,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 482,
      "content": "Figure 7.14 – Choosing a schema from Glue\n\nHow it works…\n\ne AWS Glue Schema Registry allows you to centrally discover,\n\ncontrol, and evolve data schemas. It helps enforce schema\n\ncompatibility checks when registering new schema versions.\n\nGlue Schema Registry also helps to manage the schema evolution.\n\nWhen a new version of a schema is registered, Schema Registry will\n\ncheck the compatibility based on the con\u0000gured mode. If the new\n\nversion is compatible, it will be registered as a new version. You can\n\nalso specify which version of the schema you want a table to have.\n\nis allows you to evolve your schemas over time without breaking\n\ndownstream applications. Schema evolution comes with the\n\nfollowing modes:\n\nNONE: Any change is allowed",
      "content_length": 747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 483,
      "content": "BACKWARD: New \u0000elds can be added, but existing \u0000elds cannot be\n\nremoved or renamed\n\nFORWARD: Existing \u0000elds can be removed or renamed, but new \u0000elds\n\ncannot be added\n\nFULL: Changes must be backward- and forward-compatible\n\nThere’s more…\n\nSchema Registry is a specialized product for schema management.\n\nIt can integrate with streaming data applications such as Apache\n\nKaa, Amazon MSK, Amazon Kinesis Data Streams, Apache Flink,\n\nand AWS Lambda, providing schema management capabilities for\n\nthese platforms. Data Catalog can also reference schemas stored in\n\nSchema Registry when creating or updating AWS Glue tables or\n\npartitions.\n\nTo expand the architecture further, investigate an architecture of\n\nETL framework where you integrate a control table to keep track of\n\nGlue job run status and a technical and business metadata\n\ndictionary.\n\nSee also\n\nAWS Glue Schema Registry – AWS Glue:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/schema-registry.html",
      "content_length": 955,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 484,
      "content": "Integrating with AWS Glue Schema Registry:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/schema-registry-\n\nintegrations.html\n\nBuilding unit test functions for ETL pipelines\n\nIn this recipe, we will learn how to build some unit test functions\n\nfor your ETL pipeline to help identify and \u0000x issues at an early stage\n\nof your pipeline development. By incorporating unit tests, you can\n\ncatch errors early in the development process, leading to more\n\nrobust and reliable data work\u0000ows. is recipe is particularly useful\n\nfor data engineers who need to validate the functionality of their\n\nETL jobs and ensure data integrity before deploying them to\n\nproduction.\n\ne goal of this recipe is to introduce some code snippets of\n\nfunctions to test Glue Jobs in a unit testing context that you can use\n\nto integrate into your Glue pipeline or your company’s internal\n\nlibraries.\n\nHow to do it…\n\n1. You should create a \u0000le name, such as unit_test.py, that is separate\n\nfrom your ETL code. is \u0000le will contain various functions for unit\n\ntesting.",
      "content_length": 1033,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 485,
      "content": "2. Import the relevant libraries. ese are the libraries that we will use for\n\nthis recipe:\n\nimport logging import os import subprocess import sys from functools import reduce from itertools import zip_longest from pyspark.sql import DataFrame, Row, SparkSession from pandas.testing import assert_frame_equal from pyspark import SparkConf, SparkContext from pyspark.sql.types import ArrayType, MapType from pyspark.testing import assertDataFrameEqual, assertSchemaEqual\n\n3. In this code, we will raise an exception error as follows:\n\nclass DataFrameNotEqualError(Exception): \"\"\"The created dataframes are not equal\"\"\" pass\n\n4. We will create a GlueUnitTest class, as shown, to compare schemas\n\nand row data:\n\nclass GlueUnitTest: @staticmethod def assert_df_equality(df1, df2, ignore_col_order=True, ignore_row_order=True, ignore_nullable=True): \"\"\" Check two PySpark dataframes, and",
      "content_length": 882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 486,
      "content": "raise an exception if they are not equal Handles list/dict style columns, and prints detailed error messages on failure. Do not use this to compare a PySpark dataframe to a Python dict in a test. Instead, collect() the dataframe rows and do a comparison in Python. :param df1: The 1st dataframe to check :param df2: The 2nd dataframe to check :param ignore_col_order: If true, the columns will be sorted prior to checking the schema :param ignore_row_order: If true, the rows will be sorted prior to checking the data :param ignore_nullable: If true, differences in whether or not the schema allows None/null will be ignored. \"\"\" transforms = [] if ignore_col_order: transforms.append(lambda df: df.select(sorted(df.columns))) if ignore_row_order: transforms.append(lambda df: df.sort(df.columns)) df1 = reduce(lambda acc, fn: fn(acc), transforms, df1) df2 = reduce(lambda acc, fn: fn(acc), transforms, df2)\n\nGlueUnitTest.assert_df_schema_equality(df1, df2)\n\nGlueUnitTest.assert_df_data_equality(df1, df2) @staticmethod",
      "content_length": 1019,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 487,
      "content": "def assert_df_data_equality(df1, df2): \"\"\" compare two PySpark dataframes and return if rows are different and the percentage of difference :param df1: the first dataframe :param df2: the second dataframe \"\"\" try: assertDataFrameEqual(df1, df2) except AssertionError as e: raise AssertionError(f\"Dataframes are not equal: {e}\") from e @staticmethod def assert_df_schema_equality(df1, df2): \"\"\" Assert that two pyspark dataframe schemas are equal. this function only compares two schemas , column names, datatypes and nullable property :param df1: the first dataframe :param df2: the second dataframe \"\"\" try:\n\nassertSchemaEqual(actual=df1.schema, expected=df2.schema) except AssertionError as e: raise AssertionError(f\"Schema are not equal: {e}\") from e\n\n5. In your Glue Studio notebook, you can add an additional\n\nunit_test.py \u0000le with the following code:\n\n%%configure",
      "content_length": 869,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 488,
      "content": "{ \"--extra-files\": \"s3://path/to/additional/unit_test.py/\" }\n\nHow it works…\n\nere are three types of testing that you can implement for your\n\npipelines to ensure their accuracy and reliability:\n\nUnit test that tests individual unit code: e standard unit test libraries\n\nare Pytest, uniitest, or scalatest if you write your code in\n\nScala. Unit testing your pipeline is diﬀerent from unit testing your data.\n\nis is why we have two diﬀerent recipes for unit testing in this chapter.\n\nIntegration test that tests for components of your pipeline: It is not\n\nrecommended to build a pipeline with too many components. e best\n\npractice is to standardize your pipeline into a single and modular action\n\nand use an orchestration tool such as Glue Work\u0000ow or Step Functions\n\nto orchestrate the steps of these actions.\n\nEnd-to-end testing whereby you test the entire pipeline from start to\n\n\u0000nish: You will need to adapt the knowledge from Chapter 8, DevOps –\n\nDe\u0000ning IaC and Building CI/CD Pipelines, where you set up a code\n\ndeployment pipeline for CI/CD work\u0000ow.\n\nIn this recipe’s code snippets, we went through two unit test\n\nscenarios that helped you evaluate your DataFrame before passing it\n\nto the next layer of the data lake as follows:",
      "content_length": 1238,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 489,
      "content": "Comparing whether the information of two data frames is identical\n\nComparing whether the schema of two data frames is identical\n\nEach organization will have diﬀerent test scenarios to meet your\n\ndata quality needs. You can build a comprehensive testing\n\nframework with Deequ and Spark’s UDF. You can then add it to\n\nyour pipeline as extra \u0000les.\n\nThere’s more…\n\nTo take this a step further, you should write unit test cases and\n\nautomate the running of these test cases with AWS CodePipeline.\n\nPlease see Chapter 8, DevOps – De\u0000ning IaC and Building CI/CD\n\nPipelines, for more details.\n\nAs an example of writing a unit test, you would need to build mock\n\ndata and pass it into a test case, as shown here:\n\nimport pytest tableName = \"marketing_data\" dbName = \"marketing\" # Does the table exist? def test_tableExists(): assert tableExists(tableName, dbName) is True\n\nSee also",
      "content_length": 872,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 490,
      "content": "Using Python libraries with AWS Glue:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-\n\npython-libraries.html\n\nPySpark Testing:\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.testin\n\ng.html\n\nCon\u0000guring AWS Glue interactive sessions for Jupyter and AWS Glue\n\nStudio notebooks:\n\nhttps://docs.aws.amazon.com/glue/latest/dg/interactive-sessions-\n\nmagics.html\n\nBuilding data cleaning and profiling jobs with DataBrew\n\nAWS Glue DataBrew is a no-code data preparation tool that\n\nsimpli\u0000es data pro\u0000ling, cleansing, and validation, making it an\n\nexcellent choice for data engineers looking to automate data quality\n\nchecks. In this recipe, we’ll use DataBrew to perform data pro\u0000ling\n\nand PII detection.\n\nGetting ready\n\nis recipe assumes that you have a dataset in S3 for testing out\n\nDataBrew.",
      "content_length": 824,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 491,
      "content": "How to do it…\n\n1. Navigate to the AWS DataBrew console, click on Projects | Create\n\nproject, and \u0000nally click on Provide project name.\n\nFigure 7.15 – Clicking on Create Project\n\n2. Select the appropriate dataset that you would like to use for building the\n\nDataBrew project:\n\nFigure 7.16 – Selecting a relevant dataset",
      "content_length": 318,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 492,
      "content": "3. In the Permission section, select the appropriate name under Role name,\n\nthen click on Create project. If you have not created an IAM role for\n\nDataBrew, you can click on Create new IAM role and then click on\n\nCreate project.\n\nFigure 7.17 – Selecting an IAM role\n\n4. You will need to wait a few minutes for the console to start. Once the\n\nconsole \u0000nishes loading, click on RECIPE, then click on Add step:",
      "content_length": 407,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 493,
      "content": "Figure 7.18 – Adding steps to your recipe",
      "content_length": 41,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 494,
      "content": "5. Next, we will see how to build some steps in the recipe. Assume that we\n\nhave a PII column of 10 number and we only want to keep the \u0000rst\n\nnumber and the last 5 numbers for analytics purposes. First, search for\n\nREDACT VALUES:\n\nFigure 7.19 – Choosing the REDACT VALUES option\n\n6. Next, choose the value for Source columns, Value to redact, Starting\n\nposition, and Redact symbol.",
      "content_length": 381,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 495,
      "content": "Figure 7.20 – Selecting rules for Redact Values\n\n7. en click on Preview changes to preview the eﬀect. Once you are happy\n\nwith the result, click on Apply.",
      "content_length": 155,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 496,
      "content": "Figure 7.21 – Previewing output\n\n8. You can also use Substitute values to create new values from your\n\nexisting record. In the following example, you can use shuﬄe value to\n\nshuﬄe customers’ \u0000rst or last names. e Name of columns to group by\n\nwith \u0000eld could be used in a similar way to the GROUP_BY command in\n\nSQL when you want the data to shuﬄe only within the Group by\n\nvalue.",
      "content_length": 380,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 497,
      "content": "Figure 7.22 – Creating rules for substitute values\n\n9. e following screenshot shows an example of a recipe with multiple\n\nsteps, including re-arranging columns to rename, \u0000ltering value, and\n\nmasking data. Depending on your needs, you can also add hashing and\n\nencryption for your dataset. Once you \u0000nish with the recipe steps, click\n\non Publish.",
      "content_length": 347,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 498,
      "content": "Figure 7.23 – Reviewing the steps and publishing\n\n10. Once you publish a recipe, you can head to the Job tab and \u0000ll in the job\n\nname, dataset, and previously created recipe, as shown. Once you \u0000ll in\n\nthe necessary details, click on either Create job or Create and run job.",
      "content_length": 274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 499,
      "content": "Figure 7.24 – Creating a recipe job\n\nHow it works…\n\nAWS DataBrew simpli\u0000es data quality control by providing a visual\n\ninterface for data pro\u0000ling and transformation. e steps outlined\n\nin this recipe show how to leverage AWS Glue DataBrew to\n\nautomate data quality checks by creating a recipe. A recipe could be\n\nadjusted to be reused in multiple datasets, such as creating a recipe\n\nto mask PII data for columns with ID.",
      "content_length": 422,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 500,
      "content": "There’s more…\n\nPro\u0000ling helps you understand the state of your data by generating\n\nstatistics and identifying potential issues. It is advised to run a\n\npro\u0000le job \u0000rst to understand your dataset before applying a ruleset.\n\nClick on Create job on the Pro\u0000le jobs tab to start your pro\u0000ling job\n\nfor the dataset.\n\nFigure 7.25 – Clicking on Create job to automate the DataBrew\n\nrecipe\n\nAs discussed in the Unit testing your data quality using Deequ\n\nrecipe, Data Quality is all built upon the Deequ library. When\n\nchoosing between AWS Glue DataBrew and Deequ for your data\n\nworkloads, there are a few key factors to consider:\n\nData transformation capabilities: AWS Glue DataBrew is a visual data\n\npreparation tool that allows you to easily transform and clean your data,",
      "content_length": 767,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 501,
      "content": "while Deequ is more of a data quality checking tool.\n\nEase of use: AWS Glue DataBrew provides a user-friendly, no-code\n\ninterface that makes it easy for non-technical users to prepare and\n\ntransform data. On the other hand, Deequ requires more technical\n\nexpertise, as it is a library that needs to be integrated into your data\n\nprocessing pipelines.\n\nIntegration with other AWS services: Deequ can be used with a variety\n\nof data processing frameworks, including Apache Spark. It can also be\n\nused within your Glue data pipeline with DataBrew. However, it would\n\nbe a separate step and would need an orchestration tool such as Air\u0000ow\n\nor Step function to trigger the Glue pipeline and DataBrew together.\n\nSee also\n\nGROUP_BY: https://docs.aws.amazon.com/databrew/latest/dg/recipe-\n\nactions.GROUP_BY.html\n\nAWS Glue DataBrew adds binning, skewness, binarization, and transpose\n\ntransformations for pre-processing data for machine learning and\n\nanalytics: https://aws.amazon.com/about-aws/whats-new/2021/03/aws-\n\nglue-databrew-adds-binning-skewness-binarization-transformations-\n\npre-processing-data/\n\nOceanofPDF.com",
      "content_length": 1113,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 502,
      "content": "8 DevOps – Defining IaC and Building CI/CD Pipelines\n\ne idea of combining soware development with production\n\noperations has been around for decades, but it’s since the rise of\n\ncloud computing that it has been established as a best practice,\n\naiming to shorten the soware development cycles from\n\ndevelopment to deployment while improving their reliability and\n\nlowering operational costs.\n\nis methodology involves many aspects, including team\n\norganization, culture, and processes. In this chapter, we will focus\n\non implementing two of the key technical components for\n\nsuccessfully implementing DevOps on AWS:\n\nInfrastructure as Code (IaC): IaC ful\u0000lls the key role of eliminating or\n\nreducing manual operations by coding the deployment and maintenance\n\nof infrastructure and services. is reduces human errors, improves\n\nscalability, and reduces the maintenance burden in general. In recent\n\nyears, AWS Cloud Development Kit (CDK) has become the reference\n\nIaC for AWS. It allows you to code the AWS infrastructure on one of the\n\nmultiple languages it supports, aer which it’s converted into an AWS\n\nCloudFormation template. We will also cover the main alternative,\n\nTerraform, which aims to be a cloud-agnostic tool for IaC.",
      "content_length": 1235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 503,
      "content": "Continuous integration/continuous deployment (CI/CD): is is the set\n\nof techniques that takes care of reliably deploying code to production\n\nsystems. e aim is that once the developer code has been merged into\n\nthe code repository, a pipeline takes care of all the deployment\n\nautomatically. is includes not only deploying the code alongside any\n\ninfrastructure required but also following a deployment process that\n\nnormally includes tests, stages, and potentially manual approvals (full\n\nCI/CD is when there are none).\n\ne traditional tool to implement CI/CD pipelines is Jenkins.\n\nHowever, here, we will use AWS CodePipeline since it is easier\n\nto set up and is well-supported by CDK.\n\ne following recipes will be covered in this chapter:\n\nSetting up a code deployment pipeline using CDK and AWS\n\nCodePipeline\n\nSetting up a CDK pipeline to deploy on multiple accounts and regions\n\nRunning code in a CloudFormation deployment\n\nProtecting resources from accidental deletion\n\nDeploying a data pipeline using Terraform\n\nReverse-engineering IaC\n\nIntegrating AWS Glue and Git version control\n\nTechnical requirements",
      "content_length": 1115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 504,
      "content": "e recipes in this chapter assume you have a bash shell or\n\nequivalent available with the AWS CLI installed (refer to the\n\ninstructions at\n\nhttps://docs.aws.amazon.com/cli/latest/userguide/getting-started-\n\ninstall.html) with access to AWS. If you’re using Microso\n\nWindows, you can enable WSL (https://learn.microso.com/en-\n\nus/windows/wsl/install) or install Git (https://git-\n\nscm.com/downloads) to use the bash shell it brings.\n\nCon\u0000gure the default region and user credentials, ensuring you\n\nhave enough permissions to use the diﬀerent services. You can use\n\naws configure or an AWS CLI pro\u0000le. Alternatively, you can use\n\nenvironment variables to provide the credentials:\n\nAWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and optionally\n\nAWS_SESSION_TOKEN.\n\nFor the recipes that need CDK, you need to install NPM \u0000rst by\n\ndownloading it from https://nodejs.org/en/download (in Linux, it’s\n\nhighly dependent on the GNU Libc version that your OS uses, so\n\nyou might need to use an older NPM version or compile it yourself)\n\nand then, in the command line, run npm install -g aws-cdk\n\nand then cdk --version to verify it’s working. More information\n\nand details can be found in the AWS documentation:\n\nhttps://docs.aws.amazon.com/cdk/v2/guide/cli.html.",
      "content_length": 1249,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 505,
      "content": "You can \u0000nd the code \u0000les for this chapter on GitHub:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter08.\n\nSetting up a code deployment pipeline using CDK and AWS CodePipeline\n\nIn this recipe, you will create a CDK project that de\u0000nes a\n\ndeployment pipeline and a service infrastructure stack with a\n\nsimple Glue Shell job as an example. is recipe uses Python for\n\nboth the Glue Shell script and the CDK script.\n\ne pipeline will monitor the CodeCommit repository to\n\nautomatically deploy both changes to the pipeline itself and the\n\nsolution (the Glue job in this case).\n\nGetting ready\n\nFor this recipe, you’ll require the AWS CLI, CDK, Python 3.8+, and\n\nGit to be installed in your system, as well as a command-line\n\ninterface to invoke them. Check the Technical requirements section\n\nfor indications and guidance on installing the CLI and CDK. You\n\ncan install Python and Git from their respective websites\n\n(www.python.org and git-scm.com). On Windows, make sure that",
      "content_length": 1014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 506,
      "content": "when you install Python, you enable the option to add it to PATH so\n\nthat it can be easily run from the command line. You can verify the\n\nPython version by running python --version.\n\nHow to do it…\n\n1. Bootstrap the CDK infrastructure on the previously con\u0000gured account\n\nand region. is is required so that you can deploy the CDK project\n\nlater:\n\ncdk bootstrap\n\n2. Prepare the CDK Python project by creating a directory for the project\n\nand preparing the Python virtual environment:\n\nmkdir cdk-deployment-recipe cd cdk-deployment-recipe cdk init --language=python source .venv/bin/activate echo git-remote-codecommit >> requirements.txt pip install -r requirements.txt\n\n3. Create the Git CodeDeploy repo where the project will be stored:\n\naws codecommit create-repository --repository- name \\ cdk-deployment-recipe --repository- description \\ \"CDK deployment pipeline recipe\" git remote add codecommit codecommit::$(aws \\ configure get region | \\ sed 's/\\s//g')://cdk-deployment-recipe",
      "content_length": 985,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 507,
      "content": "4. Create a script for the Glue job script:\n\nmkdir glue echo \"print('Hello from GlueShell')\" >\\ ./glue/GlueShellScript.py\n\n5. Override the default CDK script and add the imports:\n\nCDK_FILE=\\ cdk_deployment_recipe/cdk_deployment_recipe_st ack.py cat > $CDK_FILE << EOF from aws_cdk import ( Stack, Stage, aws_codecommit as codecommit, aws_glue as glue, aws_iam as iam, aws_s3 as s3, aws_s3_deployment as s3_deploy ) from aws_cdk.pipelines import ( CodePipeline, CodePipelineSource, ShellStep ) from constructs import Construct EOF\n\n6. Add the deployment stack to the script. Ensure that you respect the\n\nindentation:\n\ncat >> $CDK_FILE << EOF class CdkDeploymentRecipeStack(Stack): def __init__(self, scope: Construct,",
      "content_length": 716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 508,
      "content": "construct_id: str, **kwargs): super().__init__(scope, construct_id, **kwargs) repo = codecommit.Repository.\\\n\nfrom_repository_name(self,\"DeployRecipeRepo\", repository_name=\"cdk-deployment- recipe\") git_source = \\ CodePipelineSource.code_commit(repo, \"master\") pipeline = CodePipeline(self, \"Pipeline\", pipeline_name=\"RecipePipeline\", synth=ShellStep(\"Synth\", input=git_source, commands=[ \"npm install -g aws-cdk\", \"python -m pip install -r requirements.txt\", \"cdk synth\"] ) ) pipeline.add_stage(GlueStage(self, \"prod\")) class GlueStage(Stage): def __init__(self, scope: Construct, construct_id: str, **kwargs): super().__init__(scope, construct_id, **kwargs) GlueAppStack(self, \"GlueAppStack\", stage=self.stage_name) EOF\n\n7. Add the \u0000rst part of the Glue stack to the script:\n\ncat >> $CDK_FILE << EOF",
      "content_length": 800,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 509,
      "content": "class GlueAppStack(Stack): def __init__(self, scope: Construct, construct_id: str, stage: str, **kwargs): super().__init__(scope, construct_id, **kwargs) bucket_name = f\"deployment-recipe- {self.account}-{stage}\" bucket = s3.Bucket(self, id=\"GlueBucket\", bucket_name=bucket_name) deployment = s3_deploy.BucketDeployment(self, \"DeployCode\", destination_bucket=bucket, sources= [s3_deploy.Source.asset(\"./glue\")]) role_name = \\ f\"AWSGlueServiceRole-CdkRecipe- {stage}\" job_role = iam.Role(self, id=role_name, role_name=role_name, managed_policies=[\n\niam.ManagedPolicy.from_managed_policy_arn(self , \"glue-service\", \"arn:aws:iam::aws:policy\\ /service-role/AWSGlueServiceRole\") ],\n\nassumed_by=iam.ServicePrincipal(\n\n\"glue.amazonaws.com\") )\n\njob_role.add_to_policy(iam.PolicyStatement(",
      "content_length": 780,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 510,
      "content": "effect=iam.Effect.ALLOW, resources=[ f'arn:aws:s3::: {bucket_name}', f'arn:aws:s3::: {bucket_name}/*' ], actions=[ 's3:ListBucket', 's3:GetObject', 's3:PutObject' ] ) ) EOF\n\n8. Complete the stack de\u0000nition:\n\ncat >> $CDK_FILE << EOF job = glue.CfnJob( self, \"glue_CDK_job\", command = glue.CfnJob.JobCommandProperty( name = \"pythonshell\", python_version= '3.9', script_location = \\ f's3://{bucket_name}/GlueShellScript.py' ), role= job_role.role_arn, name= \"deployment_recipe_glueshell\", glue_version=\"3.0\" ) EOF",
      "content_length": 510,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 511,
      "content": "9. Test the CDK script to ensure it’s valid. If it isn’t, review the previous\n\nsteps while paying attention to the Python indentation, especially on\n\nnew statements:\n\ncdk synth\n\n10. Commit the changes and push them to the project via the Git repo:\n\ngit add * git commit -m \"Added cdk and Glue code\" git push --set-upstream codecommit master\n\n11. Deploy the pipeline and the stack:\n\ncdk deploy\n\n12. Open the AWS console, navigate to CodePipeline in the same region you\n\nhave the AWS CLI con\u0000gured to use, and locate the RecipePipeline\n\npipeline. If it hasn’t completed yet, wait until all steps up to the prod\n\nstage are green.\n\nNote that you can retry a failed action, but in most cases, you\n\nwill need to make a correction to the CDK code and push to the\n\nrepository to make corrections:",
      "content_length": 788,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 512,
      "content": "Figure 8.1 – Pipeline prod stage\n\n13. Now, you can navigate to Glue on the AWS console, \u0000nd the\n\ndeployment_recipe_glueshell job, select it, and run it. Once it completes\n\nsuccessfully, in the Run details tab, using the Output link, you can view\n\nthe message that the script printed.\n\n14. If you don’t want to keep it, remove the stack, which will delete the\n\nresources:\n\ncdk destroy\n\nHow it works…",
      "content_length": 398,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 513,
      "content": "In this recipe, you created a CDK project and the sample Glue\n\nscript. Because the language of choice was Python, it needed some\n\nCDK dependencies. You downloaded these via pip based on the\n\nrequirement.txt \u0000le, loaded the Python virtual environment\n\ncreated by CDK, and activated them by running source\n\n.venv/bin/activate. is allowed Python to install the speci\u0000c\n\ndependencies just for this project and not globally so that other\n\nCDK projects could use diﬀerent versions. Other languages have\n\ntheir own way of using CDK and managing dependencies.\n\ne cdk bootstrap command deployed the CDKToolkit\n\nCloudFormation stack on the con\u0000gured region and account. is\n\nis a one-time resource setup and is required to be able to deploy\n\nCDK stacks onto that region and account; you only need to\n\nbootstrap again if you’re upgrading the project’s CDK version (the\n\ntool will detect that and remind you if needed).\n\nen, you created a Git repository on AWS CodeCommit to store\n\nthe project. is is not required by CDK since you could run the\n\nCDK project locally, but this is best practice to ensure the\n\ninfrastructure gets treated like any other code – that is, it gets peer-\n\nreviewed, stored in a version management system, and\n\nautomatically deployed.\n\nAer updating the code, you pushed the changes to the Git remote\n\nserver and deployed the CDK project. is was needed because the",
      "content_length": 1382,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 514,
      "content": "pipeline doesn’t exist yet. Once the pipeline is in place, it will\n\nmonitor the repository project to trigger the pipeline on any\n\ncommit that’s done. e pipeline has a stage to self-update, so if you\n\nhave manually deployed changes that have not been pushed to the\n\nGit server, they will be undone. Remember to only invoke cdk\n\ndeploy the very \u0000rst time you create the pipeline.\n\nTo do the deployment (either when you do it manually or with the\n\npipeline), CDK generates a CloudFormation stack or changeset, as\n\nneeded.\n\nOnce the pipeline completes the deployment, the Glue job is ready\n\nto use. All the work to create a bucket, upload the script, and create\n\na role was taken care of by the CDK stack. In the same way, the\n\nstack could have deployed other AWS components or services, such\n\nas an AWS Lambda, an RDS database, or an EMR cluster.\n\nThere’s more…\n\nis recipe combines using CDK and AWS CodePipeline. If you\n\nwanted to use the traditional tool for pipelines instead, such as\n\nJenkins, you would need a Jenkins task that also monitors the Git\n\nrepository and then runs CDK in a project, where the main stack\n\nwould be GlueAppStack instead of CdkDeploymentRecipeStack,\n\nwhich is needed for CodePipeline.",
      "content_length": 1214,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 515,
      "content": "If something fails in the pipeline, you can retry the failed stages –\n\nfor instance, if the issue is that it couldn’t deploy because there was a\n\ncon\u0000ict with some existing resource (such as the S3 bucket) and you\n\nwere able to solve it manually without code changes.\n\nAt the bottom of each stage in the pipeline, the Git commit is listed\n\nwith a message stating that it has been deployed or is currently\n\nbeing deployed. is means that in most cases, you can roll back the\n\nchanges by rolling back the changes in Git.\n\nSee also\n\nFor a more advanced use case of this recipe involving multiple stages\n\nand regions, please refer to the Setting up a CDK pipeline to deploy on\n\nmultiple accounts and regions recipe.\n\nIf you’re going to need to use multiple cloud vendors, consider using\n\nTerraform for your Glue stack on the pipeline instead of CDK. See the\n\nDeploying a data pipeline using Terraform recipe for an example of how\n\nto use that cloud-vendor-agnostic tool.\n\nSetting up a CDK pipeline to deploy on multiple accounts and regions\n\nOnce your pipeline reaches a certain complexity, it’s good practice\n\nto deploy it to diﬀerent environments using diﬀerent accounts or at\n\nleast diﬀerent regions; this separates the environments (especially",
      "content_length": 1243,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 516,
      "content": "when using diﬀerent accounts), which reduces the blast radius if\n\nsomething goes wrong and helps with security separation. For\n\ninstance, you can allow developers to log in to pre-production\n\naccounts for troubleshooting or investigation, but only\n\nadministrators can access production accounts. Another reason to\n\ndo this is to avoid having diﬀerent environments competing for\n\nservice quotas, which in most cases are per account and region.\n\nDeploying on multiple regions is oen used to serve customers in\n\nthat area or if your service needs to survive a regional disaster.\n\nGetting ready\n\nis recipe is an extension of the Setting up a code deployment\n\npipeline using CDK and AWS CodePipeline recipe; therefore, you\n\nneed to follow the Getting ready instructions there to set up the\n\ntools required for this recipe.\n\nIn addition, when using a multi-account deployment, you need the\n\nCDK infrastructure stack to trust the role that’s used by the\n\npipeline. To do so for each account involved (even the main one for\n\nconsistency), you need to do an upgraded CDK bootstrap to add\n\nthis trust to the pipeline. If that account and region have already\n\nbeen CDK bootstrapped, the command will just make the updates\n\nneeded to add that trust.",
      "content_length": 1239,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 517,
      "content": "As indicated in the Technical requirements section, you are expected\n\nto have AWS credentials con\u0000gured by default, either using the\n\ncon\u0000guration \u0000les or environment variables. When you need to\n\naccess the alternative account(s), if you use the con\u0000guration\n\noption, you can update the ~/.aws/config and\n\n~/.aws/credentials \u0000le to add a pro\u0000le for each account so that\n\nyou can specify diﬀerent credentials and con\u0000gurations for each\n\npro\u0000le. At this point, you can reference them in the commands.\n\nPlease check the AWS documentation on CLI pro\u0000les for more\n\ndetails.\n\nIf you’re using environment variables, just override the variables\n\nwhile you run the command and don’t specify any pro\u0000le (omit the\n\n--profile parameter).\n\nDecide on the accounts and regions you will deploy as part of this\n\nrecipe. Ideally, you should use two accounts and regions to fully\n\nexplore this recipe. For each pair of accounts and regions, run the\n\nfollowing bootstrap command, replacing my values with your own:\n\ncdk bootstrap 1234567890/us-west-1 --profile myprofile \\ --trust 11111111111 --cloudformation-execution- policies \\ arn:aws:iam::aws:policy/AdministratorAccess\n\nHere, 1234567890 and us-west-1 are the account and region\n\nwhere the stack will be deployed, and myprofile is the AWS CLI",
      "content_length": 1278,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 518,
      "content": "pro\u0000le with credentials for the 1234567890 account. In this case, it\n\nassumes the account is dedicated to the pipeline deployments and it\n\ngives full-trust administrator access. If the account is shared for\n\nother purposes, the best practice is to grant the minimum required\n\npermissions.\n\nHow to do it…\n\n1. Complete steps 2 to 7 of the Setting up a code deployment pipeline using\n\nCDK and AWS CodePipeline recipe. If you have the resulting project aer\n\ncompleting that recipe, it’s okay if you run the last step to destroy the\n\nstack. Move into the project directory:\n\ncd cdk-deployment-recipe\n\n2. Edit the application \u0000le to specify an account and region for the main\n\nstack – that is, the one that hosts the pipeline. Use a text editor to open\n\nthe \u0000le (for example, vi app.py) and uncomment one of the lines\n\nthat assigns the env variable either using the AWS CLI defaults or by\n\nspecifying your own account and region, like so:\n\nenv=cdk.Environment( account=os.getenv('CDK_DEFAULT_ACCOUNT'), region=os.getenv('CDK_DEFAULT_REGION')),\n\n3. Edit the main CDK \u0000le,\n\ncdk_deployment_recipe/cdk_deployment_recipe_stac",
      "content_length": 1115,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 519,
      "content": "k.py, in the CodePipeline constructor editor and add a new parameter\n\ncalled cross_account_keys=True so that it looks like this:\n\npipeline = CodePipeline(self, \"Pipeline\",\n\npipeline_name=\"RecipePipeline\", cross_account_keys=True, synth=ShellStep(\"Synth\", ...........\n\n4. In the \u0000le, just aer pipeline creation, replace the line containing\n\naddStage with one line for each of the accounts and regions you want\n\nto deploy, specifying an env argument for each one. For instance, if you\n\nwant to deploy the GlueStack to 1234567890 with the us-east-1\n\nregion set to \"integration\" and 0987654321 with the same\n\nregion but set to \"production\", you must do the following (make\n\nsure you respect the Python indentation so that both lines are aligned\n\nwith the pipeline variable assignment):\n\npipeline.add_stage(GlueStage(self, \"integration\", env={\"region\":\"us-east- 1\",\"account\":\"1234567890\"})) pipeline.add_stage(GlueStage(self, \"prod2\", env={\"region\":\"us-east- 1\",\"account\":\"0987654321\"}))\n\n5. Test whether the CDK script is valid. If not, review the previous steps\n\nwhile paying attention to the Python indentation being aligned:\n\ncdk synth",
      "content_length": 1135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 520,
      "content": "6. Commit the changes and push them to Git:\n\ngit add * git commit -m \"Made the pipeline multiregion\" git push --force --set-upstream codecommit master\n\n7. Deploy the pipeline and the stack:\n\ncdk deploy --all\n\nIf you get an “invalid principals” error, revise that the accounts\n\nand regions you speci\u0000ed in step 4 match the bootstrap you did\n\nin the Getting ready section.\n\n8. Open the AWS console, navigate to CodePipeline in the same region\n\nwhere you have AWS CLI con\u0000gured by default, and locate the\n\nRecipePipeline pipeline. If it hasn’t been deployed yet, wait until all steps\n\nup to the prod stage are green.\n\n9. Note that you can retry a failed action, but in most cases, you will need\n\nto make a correction to the CDK code and push it to the repository to\n\nmake corrections.\n\n10. Now, navigate to Glue in the console and go to the accounts in one of the\n\nregions you chose. Review and optionally run the\n\ndeployment_recipe_glueshell Glue shell job.\n\n11. If you no longer need it, remove the stack (which deletes the pipeline as\n\nwell):\n\ncdk destroy --all",
      "content_length": 1061,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 521,
      "content": "How it works…\n\nis recipe extends the previous recipe, Setting up a code\n\ndeployment pipeline using CDK and AWS CodePipeline, so the\n\nconcepts and explanations there apply here as well. We’ll focus on\n\nthe diﬀerences when deploying on multiple accounts and regions.\n\nWhen CodePipeline detects that multiple accounts are involved, the\n\npipeline is considered by CDK as multi-account. is requires an\n\nexplicit environment con\u0000guration, even if you use environment\n\nvariables to take it from the AWS CLI defaults. Also, the pipeline\n\nwill work while assuming roles, even for the current account, thus\n\nthe need for the special bootstrap you did in the Getting ready\n\nsection for each of the accounts and regions involved. is trusts\n\nthe pipeline to do the deployment.\n\nIn the CodePipeline code, you changed the cross_account_keys\n\n\u0000ag to True. is is required so that you can deploy artifacts cross-\n\naccount using trust policies. It creates a KMS key that can be shared\n\nbetween the accounts to encrypt the artifact bucket. In older\n\nversions, this was always done by default, but then it was disabled to\n\navoid the cost of maintaining the key (typically $1/month). is\n\nisn’t needed when the pipeline deploys just in the same account, so\n\nit doesn’t need to impersonate roles.",
      "content_length": 1277,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 522,
      "content": "en, in the main script, you added additional deployments of the\n\nGlue stack for multiple accounts and regions. Note how simple this\n\nis compared to manually propagating changes across environments,\n\neven if you’re using CloudFormation changesets.\n\nFinally, you deployed the project like any other CDK stack.\n\nThere’s more…\n\nCDK does most of the security setup using defaults. You can\n\nexamine the changes and the CloudFormation templates regarding\n\nroles and permissions. It is possible to con\u0000gure the pipeline so that\n\nit uses speci\u0000c roles that already exist. Alternatively, you can create\n\nroles in the pipeline with more \u0000ne-grained control or have a role\n\nname prede\u0000ned.\n\nCheck the CDK documentation for more options. Each new version\n\nprovides both more automation and more customization if you\n\nneed it.\n\nIn this example, we used the s3_deployer utility, which is an easy\n\nway to put \u0000les from the repository into S3. So, in the step where it\n\nbuilds the artifacts, it also deploys them to S3. At the time of\n\nwriting, it doesn’t optimize the build for reuse between the diﬀerent\n\nenvironments and causes more builds than needed.\n\nSee also",
      "content_length": 1149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 523,
      "content": "If you want to learn more about Glue jobs and the diﬀerent usages for\n\nbuilding data pipelines, you can check the recipes dedicated exclusively\n\nto AWS Glue in Chapter 3, Ingesting and Transforming Your Data with\n\nAWS Glue.\n\nRunning code in a CloudFormation deployment\n\nBefore the availability of more sophisticated code-based solutions\n\nsuch as CDK or Terraform, AWS CloudFormation was the standard\n\nto automate AWS deployments for more than a decade. It started as\n\na purely template solution but then introduced the ability to call\n\nfunctions with custom resource deployment.\n\nWhile CloudFormation templates are not a fully \u0000edged IaC\n\nsolution, since it was the only AWS deployment solution for years,\n\nthey are still present on many projects. Oen, the eﬀort of\n\nmigrating to a tool such as CDK doesn’t justify the bene\u0000t, so\n\ninstead, it makes sense to use CloudFormation’s advanced features\n\nand automation infrastructure, despite its limitations.\n\nIn this recipe, you’ll learn how to use CloudFormation custom\n\nresources to run your own code on deployment, which overcomes\n\nthe limitations of templates. In this case, the code will be used to set\n\nup a \u0000le on S3, but the same concept can be used for any of the\n\nfeatures of the AWS SDKs.",
      "content_length": 1246,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 524,
      "content": "Getting ready\n\nFor this recipe, you just need the AWS CLI installed and con\u0000gured,\n\na text editor, and access to the AWS console to view the results. See\n\nthe Technical requirements section for guidance on setting up the\n\ncommand line.\n\nHow to do it…\n\n1. Using a text editor, create a \u0000le named recipe3_template.json\n\nwith the following JSON content. ere are line breaks to \u0000t the code on\n\nthe page. When you enter it in the \u0000le, make sure you keep it as valid\n\nJSON by not introducing line breaks within the string double quotes,\n\n\"\":\n\n{ \"Resources\": { \"S3Bucket\": { \"Type\": \"AWS::S3::Bucket\", \"Properties\": { \"BucketName\": { \"Fn::Sub\": \"recipe-deploy- action-${AWS::AccountId}\" } } }, \"DeployLambdaRole\": { \"Type\": \"AWS::IAM::Role\", \"Properties\": { \"AssumeRolePolicyDocument\": { \"Statement\": [",
      "content_length": 796,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 525,
      "content": "{ \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"lambda.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\" } ] }, \"ManagedPolicyArns\": [ \"arn:aws:iam::aws:policy/service- role/AWSLambdaBasicExecutionRole\" ] } }, \"DeployerPolicy\": { \"Type\": \"AWS::IAM::ManagedPolicy\", \"Properties\": { \"Path\": \"/\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:PutObject\", \"Resource\": { \"Fn::Sub\": \"arn:aws:s3:::${S3Bucket}/*\" } }, { \"Effect\": \"Allow\", \"Action\": \"s3:DeleteObject\", \"Resource\": { \"Fn::Sub\":",
      "content_length": 529,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 526,
      "content": "\"arn:aws:s3:::${S3Bucket}/*\" } } ] }, \"Roles\": [ { \"Ref\": \"DeployLambdaRole\" } ] } }, \"DeployLambda\": { \"DependsOn\": [ \"S3Bucket\", \"DeployLambdaRole\", \"DeployerPolicy\" ], \"Type\": \"AWS::Lambda::Function\", \"Properties\": { \"FunctionName\": \"DeployActionLambda\", \"Description\": \"Intended to run during deploment\", \"Code\": { \"ZipFile\": { \"Fn::Sub\": \"import boto3\\nimport cfnresponse\\n\\ndef handler(event, context):\\n print(f'Event:')\\n print(event)\\n bucket_name = '${S3Bucket}'\\n file_path = 'config/setup.txt'\\n\\n if event['RequestType'] == \\\"Create\\\":\\n print('The bucket ${S3Bucket}')\\n print(f'Using bucket {bucket_name}')\\n s3 = boto3.resource('s3')\\n config = ('''property1=value1\\n property2=value2\\n''')\\n s3.Object(\\n",
      "content_length": 721,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 527,
      "content": "bucket_name=bucket_name, \\n key=file_path\\n ).put(Body=config)\\n elif event['RequestType'] == \\\"Delete\\\":\\n print(f'Deleting file: s3://'\\n '{bucket_name}/{file_path}')\\n boto3.client('s3').delete_object(\\n Bucket=bucket_name,\\n Key=file_path\\n )\\n cfnresponse.send(event, context, cfnresponse.SUCCESS, {})\\n\" } }, \"Handler\": \"index.handler\", \"Role\": { \"Fn::GetAtt\": [ \"DeployLambdaRole\", \"Arn\" ] }, \"Runtime\": \"python3.9\", \"Timeout\": 5 } }, \"DeployRun\": { \"Type\": \"AWS::CloudFormation::CustomResource\", \"DependsOn\": \"DeployLambda\", \"Version\": \"1.0\", \"Properties\": { \"ServiceToken\": { \"Fn::GetAtt\": [ \"DeployLambda\", \"Arn\" ] } } }",
      "content_length": 630,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 528,
      "content": "}, \"Outputs\": { \"RecipeBucketName\": { \"Description\": \"Name of the bucket created by the recipe\", \"Value\": { \"Ref\": \"S3Bucket\" } } } }\n\n2. Validate the template by running the following command:\n\naws cloudformation validate-template \\ --template-body file://recipe3_template.json\n\n3. Check for any errors and address the issue in the template. If everything\n\nis okay, it will return a JSON advising the capabilities required:\n\n{ \"Parameters\": [], \"Capabilities\": [ \"CAPABILITY_IAM\" ], \"CapabilitiesReason\": \"The following resource(s) require capabilities: [AWS::IAM::Role]\" }\n\n4. Deploy the stack and wait until it completes successfully:\n\naws cloudformation deploy --template-file\\ recipe3_template.json --stack-name RecipeCF-",
      "content_length": 726,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 529,
      "content": "Action\\ --capabilities CAPABILITY_IAM\n\n5. Check the bucket that was created based on the con\u0000gured account and\n\nthat the text \u0000le was created and updated by the stack deployment:\n\nBUCKET_NAME=\"recipe-deploy-action-$(aws sts get-\\ caller-identity --query 'Account' --output text)\" aws s3 ls --recursive $BUCKET_NAME\n\n6. Optionally, navigate to the AWS console to check the RecipeCF-Action\n\nstack. Explore the Resources and Events tabs.\n\n7. Delete the stack. is will remove all resources that were created by the\n\nstack, including the S3 object and bucket:\n\naws cloudformation delete-stack \\ --stack-name RecipeCF-Action\n\n8. e delete-stack command triggers the deletion process but\n\ndoesn’t return anything immediately. You can check the deletion’s\n\nprogress by checking the last events until the command errors because\n\nthe stack no longer exists. e deletion can sometimes take a couple of\n\nminutes:\n\naws cloudformation describe-stack-events \\ --stack-name RecipeCF-Action --max-items 3\n\nHow it works…",
      "content_length": 1003,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 530,
      "content": "When you deployed the CloudFormation template, it applied the\n\nFn::Sub and Fn::GetAtt functions, resolving the actual values at\n\ndeployment time. en, it proceeded to create the resources,\n\nconsidering the dependencies de\u0000ned. e last to be created was the\n\nDeployRun custom resource, which causes DeployLambda to run\n\non stack creation and deletion.\n\nis Lambda de\u0000nes the Python code inline for convenience,\n\nwithin a call to Fn::Sub so that it can dynamically insert the name\n\nof the bucket using ${S3Bucket}. In addition, the code is de\u0000ned\n\nto be deployed as a ZIP \u0000le via the ZipFile property, so it\n\nautomatically includes module dependencies, which are detected by\n\nCloudFormation using the top import lines.\n\ne code creates a text \u0000le on S3 with some con\u0000guration for an\n\napplication.\n\nTo create the template via the command line, you had to specify the\n\n--capabilities argument with a value of CAPABILITY_IAM,\n\nwhich con\u0000rms that accepting the template might create resources\n\nand aﬀect permissions. is is the equivalent of the con\u0000rmation\n\ncheck box when you create the stack via the console.\n\nWhen the stack is deleted, it deletes the resources in the inverse\n\norder of deployment. To delete the DeployRun custom resource, it\n\ninvokes the Lambda, indicating in the parameters that it is a",
      "content_length": 1302,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 531,
      "content": "deletion run. e code then deletes the \u0000le it created on deployment\n\n(so that the bucket can be deleted by CloudFormation) and executes\n\nthe callback provided in the parameters to specify that the deletion\n\nis complete. If this call isn’t made, the deletion will be stuck in\n\nprogress until it eventually times out.\n\nThere’s more…\n\nCloudFormation also supports using the YAML format, which is\n\nmore compact but also stricter with indentation and spaces. You\n\ncan convert a template from one format into the other using the\n\ntemplate editor in the CloudFormation console.\n\nCustom resources also allow you to specify an SNS ARN instead of\n\na Lambda, which can be used to notify listeners that the stack has\n\nbeen deployed and can take asynchronous actions.\n\nTIP\n\nWhile you develop deployment actions, you will likely make mistakes and\n\nneed to make several attempts since deleting and redeploying each time isn’t\n\npractical.\n\nInstead, you can use the AWS Lambda console to open the speciﬁc Lambda\n\nand do test invocations while passing a similar JSON to what\n\nCloudFormation uses until you’re happy with the code. Then, you can do a\n\nfull redeployment to validate.",
      "content_length": 1162,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 532,
      "content": "See also\n\nCloudFormation templates have limitations because you need to follow\n\nthe template structure instead of doing your own custom code, and thus\n\nare not ideal for complex scenarios. When starting a new project, it’s\n\npreferable to use CDK, even if you don’t need complex logic yet. See the\n\nrecipes in this chapter on using CDK – that is, Setting up a code\n\ndeployment pipeline using CDK and AWS CodePipeline and Setting up a\n\nCDK pipeline to deploy on multiple accounts and regions.\n\nProtecting resources from accidental deletion\n\nIn general, automation is the best way to avoid mistakes that can\n\nresult in loss of service, data, or both. However, there are cases\n\nwhere human intervention is needed to address exceptional\n\nsituations. In a situation of urgency, the operator might need to\n\nbuild an ad hoc script quickly or take manual action under\n\npressure.\n\nSome AWS resources allow resource protection to prevent costly\n\nmistakes that can lead to serious or even irreversible damage. In this\n\nrecipe, you’ll learn how to protect RDS databases, DynamoDB\n\ntables, and CloudFormation stacks from accidental deletion.\n\nGetting ready",
      "content_length": 1142,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 533,
      "content": "To complete this recipe, you need a bash command line with the\n\nAWS CLI, as indicated in the Technical requirements section at the\n\nbeginning of this chapter.\n\nHow to do it…\n\n1. Create a simple RDS database. is will return the full con\u0000guration:\n\naws rds create-db-instance --db-instance-class \\ db.t3.micro --db-instance-identifier \\ recipe-db-protected --deletion-protection -- engine \\ postgres --no-publicly-accessible \\ --allocated-storage 20 --master-username postgres \\ --master-user-password Password1\n\n2. Try to delete the instance – it should refuse to carry out the action:\n\naws rds delete-db-instance --db-instance- identifier \\ recipe-db-protected --delete-automated- backups \\ --skip-final-snapshot\n\n3. Disable the deletion protection:\n\naws rds modify-db-instance --db-instance- identifier \\ recipe-db-protected --no-deletion-protection\n\n4. Repeat step 2. It should delete the database instance.",
      "content_length": 910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 534,
      "content": "5. Create a DynamoDB table:\n\naws dynamodb create-table --table-name\\ recipe-protected --deletion-protection- enabled\\ --key-schema AttributeName=id,KeyType=HASH\\ --attribute-definitions\\ AttributeName=id,AttributeType=S\\ --billing-mode PAY_PER_REQUEST\n\n6. Try to delete the table – it should refuse:\n\naws dynamodb delete-table --table-name \\ recipe-protected\n\n7. Disable the deletion protection on the table:\n\naws dynamodb update-table --table-name \\ recipe-protected --no-deletion-protection- enabled\n\n8. Repeat step 6. Now, it should delete the table. You can verify that it’s been\n\ndeleted:\n\naws dynamodb list-tables | grep recipe- protected\n\n9. De\u0000ne your variables and create the CloudFormation template:\n\nCFN_TEMPLATE_FILE=cfn_sample.yaml STACK_NAME=CfnProtectRecipe cat > $CFN_TEMPLATE_FILE << EOF AWSTemplateFormatVersion: '2010-09-09' Resources: CfnProtectionSample:",
      "content_length": 875,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 535,
      "content": "Type: 'AWS::DynamoDB::Table' Properties: BillingMode: PAY_PER_REQUEST AttributeDefinitions: - AttributeName: id AttributeType: S KeySchema: - AttributeName: id KeyType: HASH EOF\n\n10. Deploy the template just created:\n\naws cloudformation deploy --template-file \\ $CFN_TEMPLATE_FILE --stack-name $STACK_NAME\n\nProtect the stack from deletion:\n\naws cloudformation update-termination- protection \\ --stack-name $STACK_NAME \\ --enable-termination-protection\n\n11. Try to delete the protected stack:\n\naws cloudformation delete-stack\\ --stack-name $STACK_NAME\n\n12. Disable deletion protection on the stack:\n\naws cloudformation update-termination- protection\\ --stack-name $STACK_NAME\\ --no-enable-termination-protection\n\n13. Repeat step 11. It should con\u0000rm the stack was deleted correctly.",
      "content_length": 781,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 536,
      "content": "How it works…\n\nIn this recipe, you created examples for three kinds of AWS data-\n\nrelated resources. For each of the three cases – RDS, DynamoDB,\n\nand CloudFormation – you veri\u0000ed that when protected, they\n\ncannot be deleted by issuing the delete command, even if your\n\nuser has permission; deletion protection had to be explicitly\n\ndisabled \u0000rst.\n\nFor RDS and DynamoDB, the resources were created with the\n\nprotection already enabled, while in the case of CloudFormation, it\n\nrequired a separate call.\n\nThere’s more…\n\nis recipe covered RDS, DynamoDB, and CloudFormation, the\n\nservices that support this kind of protection at the time of writing.\n\nIn the case of S3, the bucket cannot be deleted if there are still\n\nobjects. Note that by enabling versioning or replication, you can\n\nmitigate the damage of accidental S3 deletion.\n\nis still leaves out important systems such as Redshi databases or\n\nKinesis streams. In those cases (and in general), the best practice is\n\napplying the principle of least privilege. is means that both for\n\nautomation and manual access (both via the console and the",
      "content_length": 1099,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 537,
      "content": "command line), the user role should be as restricted as possible to\n\naccomplish its purpose, which also serves to help reduce mistakes.\n\nFor instance, the operations team could have access to read-only\n\nroles, but to get an admin role, they might require approval, review,\n\nor a two-person process.\n\nSee also\n\nIdeally, you want to have environments where deployments and changes\n\nare only made by pipelines and not manually. With that automation in\n\nplace and highly restricted manual intervention on production, the\n\naccidental protections seen in this recipe could be considered\n\nunnecessary. To learn more about deployment automation, see the\n\nSetting up a code deployment pipeline using CDK and AWS CodePipeline\n\nand Deploying a data pipeline using Terraform recipe.\n\nDeploying a data pipeline using Terraform\n\nIn this recipe, you will see the Terraform cloud-agnostic IaC\n\nautomation tool. Agnostic doesn’t mean you can abstract yourself\n\nfrom the underlying vendor but that the tool has support for\n\nmultiple cloud providers, so you can apply the knowledge of the\n\ntool to other providers as well as build multi-cloud projects. But you\n\nstill need to know about the diﬀerent services on each cloud",
      "content_length": 1203,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 538,
      "content": "provider and how to use them. See the oﬃcial vendor\n\ndocumentation for further details: https://www.terraform.io/.\n\ne trade-oﬀ of its multi-cloud support is that because it’s not\n\nprovided directly by each vendor, it may take some time until\n\nTerraform supports new features.\n\nGetting ready\n\nTo complete this recipe, you need a bash command line with the\n\nAWS CLI set up, as indicated in the Technical requirements section\n\nat the beginning of this chapter.\n\nYou also need the Terraform executable in your system. You must\n\nput it in the system’s PATH so that it can be executed easily. Check\n\nthe HashiCorp website for instructions on how to download the\n\nexecutable for your OS and architecture:\n\nhttps://developer.hashicorp.com/terraform/install.\n\nYou also need a user who can log in to the AWS console and use\n\nAWS Glue.\n\nHow to do it…\n\n1. Create a directory for the Terraform project and change to it:\n\nmkdir terraform_recipe && cd terraform_recipe",
      "content_length": 954,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 539,
      "content": "2. Create a \u0000le with con\u0000guration variables (note that the \u0000rst \\ is to break\n\nthe line and the second one is to escape the $ sign so that bash doesn’t\n\ntry to replace it):\n\ncat > var.tf << EOF variable \"region\" { description = \"AWS region\" type = string default = \"us-east-1\" } data \"aws_caller_identity\" \"current\" {} locals { bucket_name = \"terraform-recipe-\\ \\${data.aws_caller_identity.current.account_id }\" } variable \"script_file\" { type = string default = \"ShellScriptRecipe.py\" } EOF\n\n3. Create a placeholder script for the Glue Shell job:\n\necho 'print(\"Running Glue Shell job\")' > \\ ShellScriptRecipe.py\n\n4. Create the main Terraform project \u0000le so that you can deploy the script\n\nto S3 (here, EOF has quotes around it, so it is written literally and doesn’t\n\nhave to escape via $):\n\ncat > main.tf << 'EOF' provider \"aws\" {",
      "content_length": 832,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 540,
      "content": "region = var.region } resource \"aws_s3_bucket\" \"bucket\" { bucket = local.bucket_name } resource \"aws_s3_object\" \"script\" { bucket = local.bucket_name key = \"scripts/${var.script_file}\" source = var.script_file depends_on = [ aws_s3_bucket.bucket ] } EOF\n\n5. Append a Glue Shell job and a new role with minimum permissions it\n\ncan use to run to the main \u0000le (notice >> to append instead of\n\noverwriting):\n\ncat >> main.tf << EOF resource \"aws_iam_role\" \"glue\" { name = \"AWSGlueServiceRoleTerraformRecipe\" managed_policy_arns = [ \"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAcces s\", \"arn:aws:iam::aws:policy/CloudWatchAgentServer Policy\" ] assume_role_policy = jsonencode( { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": \"sts:AssumeRole\", \"Principal\": {",
      "content_length": 754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 541,
      "content": "\"Service\": \"glue.amazonaws.com\" }, \"Effect\": \"Allow\", \"Sid\": \"\" } ] } ) } resource \"aws_glue_job\" \"shell_job\" { name = \"TerraformRecipeShellJob\" role_arn = aws_iam_role.glue.arn command { name = \"pythonshell\" python_version = \"3.9\" script_location = \"s3://\\${local.bucket_name}\\ /scripts/\\${var.script_file}\" } } EOF\n\n6. Initialize the Terraform project. is can take a minute or so since it\n\nneeds to download the AWS provider:\n\nterraform init\n\n7. Validate that the \u0000les that were generated in the previous steps are valid,\n\nas well as the AWS CLI credentials and con\u0000guration:\n\nterraform plan\n\nIf all is correct, the plan summary should look like this:\n\nPlan: 4 to add, 0 to change, 0 to destroy.",
      "content_length": 698,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 542,
      "content": "8. Deploy the stack with the apply command. Using the region\n\nvariable, set the name of your region – for example, us-east-2:\n\nterraform apply -var region=us-east-2\n\n9. In the AWS console, navigate to AWS Glue on the chosen region. Use the\n\nETL Jobs menu and then select TerraformRecipeShellJobs from the job\n\ntable. Use the Run button and then open the Runs tab. Once it completes\n\nsuccessfully, you can use the Output link to view the message the job\n\nprinted:\n\nFigure 8.2 – AWS Glue job deployed by Terraform\n\n10. Back on the command line, clean up if you don’t want to keep the\n\ndeployment.\n\nYou need to specify the same region you used on deployment,\n\nlike so:",
      "content_length": 665,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 543,
      "content": "terraform destroy -var region=us-east-2\n\nHow it works…\n\nIn addition to the trivial Python script, you created two \u0000les with\n\nthe .tf (Terraform Format) extension. ese are text \u0000les that\n\nresemble JSON but it’s not (notice that there are no commas at the\n\nend of the lines and the object de\u0000nition is also diﬀerent). In\n\naddition, this format allows you to easily reference other objects\n\nfrom the same or diﬀerent \u0000les as you don’t need to import other\n\n\u0000les to reference them.\n\ne names of the \u0000les are arbitrary. Terraform will pick up any \u0000les\n\nwith the extension and load them together; as shown in this recipe,\n\nit’s common practice to have a \u0000le that acts as a\n\nvariable/con\u0000guration reference so that it’s easy to \u0000nd what can be\n\ncon\u0000gured.\n\nInside the main \u0000le, the variables can be used either with a direct\n\nreference using the var/local pre\u0000x or inside a string with a\n\nsyntax like bash: ${variable-name}. e bucket name is de\u0000ned\n\nin locals instead of variable so that it can dynamically retrieve\n\nthe account ID and make the bucket unique for your account.\n\nVariables have default values that can be overridden at deployment",
      "content_length": 1139,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 544,
      "content": "time, as you did by specifying the region when applying and\n\ndestroying the stack.\n\nIf you peruse the Terraform \u0000le, you will see that it’s intuitive to\n\nunderstand how it de\u0000nes the components. An important element\n\nis the provider \"aws\" section, which tells Terraform that the\n\nproject needs the libraries for AWS resources.\n\nWhen you ran terraform init, it downloaded the AWS provider\n\nand put it under the project’s hidden .terraform folder.\n\nBy running the AWS Glue job successfully, you proved that the job\n\nwas deployed with all the required con\u0000gurations: it could \u0000nd and\n\nload the script you created locally and was able to impersonate the\n\nIAM role created for it.\n\nThere’s more…\n\nIn this recipe, you used terraform plan to validate the\n\ndeployment on the actual AWS account. If you want to check if the\n\ncon\u0000g is valid but don’t have access to an account (for instance,\n\nbecause you are oﬄine), you can run terraform validate\n\ninstead to make sure the syntax is correct. e terraform fmt\n\ncommand can be used to format the .tf \u0000les to conform to the\n\nTerraform conventions and recommendations.",
      "content_length": 1105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 545,
      "content": "In addition to AWS, Terraform has support for many providers for\n\nthe popular cloud vendors, as well as generic ones such as the one\n\nfor Kubernetes. ere is a registry where they are clearly labeled,\n\nindicating whether they are provided by HashiCorp, a partner, or\n\nthe community. You can use multiple providers on the same project.\n\nTo keep track of deployments, Terraform creates a \u0000le named\n\nterraform.tfstat (as well as a backup when it needs to update\n\nit). e \u0000le is synchronized with the cloud before deployment. On a\n\nproduction project, it is bad practice to have it locally since it can be\n\nlost or get out of sync if diﬀerent people have their own local copy.\n\nYou can con\u0000gure Terraform to save that \u0000le in a shared place (for\n\nexample, S3) and use locks with a tool such as DynamoDB so that\n\nthe status \u0000le is free from con\u0000icts or loss.\n\nSee also\n\nInstead of writing your own Terraform code, you could generate it from\n\nexisting resources. See the Reverse-engineering IaC recipe.\n\ne main alternative to Terraform is the frameworks and tools provided\n\nby each speci\u0000c cloud vendor. For AWS, that’s CloudFormation and\n\nCDK. To learn more about them, take a look at the Setting up a code\n\ndeployment pipeline using CDK and AWS CodePipeline, Setting up a CDK\n\npipeline to deploy on multiple accounts and regions, and Running code in a\n\nCloudFormation deployment recipes in this chapter.",
      "content_length": 1399,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 546,
      "content": "To learn more about data pipelines using AWS Glue, refer to Chapter 3,\n\nIngesting and Transforming Your Data with AWS Glue.\n\nReverse-engineering IaC\n\nOen, we aren’t creating a brand-new data infrastructure but\n\nextending existing resources that we want to automate. So, future\n\nchanges are automatically tracked and deployed using IaC. You may\n\nalso need some help building your IaC and would rather create the\n\nresources using the AWS console than generate the corresponding\n\ncode. You can then use this as the basis instead of writing the code\n\nfrom scratch.\n\nIn this recipe, you will learn how Terraformer can reverse engineer\n\nexisting resources to generate Terraform code. is tool is relatively\n\nnew and still under development, so it’s likely to help you but not do\n\nthe full work for you.\n\nIf you wish to use other IaC tools instead of Terraform, there are\n\nreverse-engineering alternatives that you can check in the See also\n\nsection at the end of this recipe.\n\nGetting ready\n\nTo complete this recipe, you need a bash command line with the\n\nAWS CLI set up, as indicated in the Technical requirements section",
      "content_length": 1117,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 547,
      "content": "at the beginning of this chapter.\n\nYou also need the Terraform executable in our system in the\n\nsystem’s PATH so that it can be executed just using its name. Check\n\nthe HashiCorp indications and download the executable matching\n\nthe OS and architecture from their website:\n\nhttps://developer.hashicorp.com/terraform/install. In addition,\n\nyou’ll need the Terraformer executable, which you can download as\n\na binary or install with a package manager. Check the project on\n\nGitHub for instructions:\n\nhttps://github.com/GoogleCloudPlatform/terraformer.\n\nDISCLAIMER\n\nDespite the project being hosted in the Google Cloud community repository,\n\nit is Apache licensed open source and it’s not aﬃliated nor provided by\n\nAlphabet Inc.\n\nHow to do it…\n\n1. Create the Glue job that you’ll reverse engineer later to Terraform code:\n\naws glue create-job --name recipe-shell- reveng\\ --role arn:aws:iam::$(aws sts get-caller- identity\\ --query 'Account' --output text)\\ :role/SomeRoleForGlue --command \\",
      "content_length": 988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 548,
      "content": "'{\"Name\": \"pythonshell\", \"PythonVersion\":\"3.9\", \"ScriptLocation\": \"s3://somebucket/yourscript.py\"}'\n\n2. View the job you just created. You’ll see that many properties have been\n\n\u0000lled with defaults:\n\naws glue get-job --job-name recipe-shell- reveng\n\n3. Initialize an empty Terraform project for AWS:\n\nmkdir recipe_reveng && cd recipe_reveng echo \"provider \"aws\" {}\" > provider.tf terraform init\n\n4. Import the Glue job you created in step 1 into the Terraform project:\n\nterraformer import aws --path-pattern=.\\ --compact=true --resources=glue --filter\\ \"Name=name;Value=recipe-shell-reveng\"\n\n5. Edit the resources \u0000le using a text editor – for instance, vi\n\nresources.tf. Make the following changes and save them:\n\nRemove number_of_workers = \"0\"\n\nChange the number of retries to 1 with max_retries =\n\n\"1\"\n\n6. Test the Terraform project with the changes and address any errors:\n\nterraform plan",
      "content_length": 892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 549,
      "content": "At the time of writing, the latest version of Terraformer is\n\nv0.8.24 and still references the old Terraform AWS provider.\n\nCheck for the following error:\n\nError: Failed to load plugin schemas Error while loading schemas for plugin components: Failed to obtain provider schema: Could not load the schema for provider registry.terraform.io/-/aws: failed to instantiate provider \"registry.terraform.io/-/aws\" to obtain schema: unavailable provider \"registry.terraform.io/-/aws\"..\n\nIf you can see this error, you need to address it by upgrading the\n\nprovider:\n\nterraform state replace-provider \\ registry.terraform.io/-/aws \\ registry.terraform.io/hashicorp/aws\n\nIf you’re running the code on Microso Windows, you might get\n\nthe following error:\n\nError: Failed to read state file The state file could not be read: read terraform.tfstate: The process cannot access the file because another process has locked a portion of the file.\n\nis is a known issue regarding handling \u0000le locks locally. You\n\ncan work around this by adding -lock=false to the failed\n\ncommand, like so:\n\nterraform plan -lock=false",
      "content_length": 1097,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 550,
      "content": "7. Deploy the changes you made in step 5 and con\u0000rm that the retries is\n\nnow set to 1:\n\nterraform apply aws glue get-job --job-name recipe-shell- reveng\n\n8. Delete the Glue Shell job using Terraform:\n\nterraform destroy\n\nHow it works…\n\nFirst, you created a very simple Glue job to reverse engineer. Note\n\nthat this job can’t run since it references a non-existing AWS role\n\nand S3 path – it is just for demonstration purposes. To learn more\n\nabout Glue, check out Chapter 3, Ingesting and Transforming Your\n\nData with AWS Glue.\n\nen you initialized a minimal Terraform project to download the\n\nAWS provider under the .terraform directory, which\n\nTerraformer requires to access the account. It is possible to put the\n\nprovider on a shared path, so it’s shared by projects.\n\nUsing Terraformer, you imported the job, and underneath it used\n\nthe AWS CLI to access the account using the default credentials and\n\ncon\u0000guration, such as region. at generated multiple \u0000les in the\n\ncurrent directory. is is due to the --path-pattern= parameter,",
      "content_length": 1035,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 551,
      "content": "which places the import \u0000les in the current directory instead of\n\nmultiple subdirectories. Note that --compact=true indicates all\n\nthe Terraform resources are imported into the resources.tf \u0000le.\n\nIn addition, the import statement overwrote the provider.tf \u0000le\n\nto pin the AWS provider version, while Terraform 1.0 or later has a\n\ncombability promise. It is good practice to specify the version for\n\nbetter stability. e ~> operator indicates that the build version (the\n\nlast of the three numbers) can change but not the others. is\n\nallows you to make \u0000xes to the version with a very low risk of\n\nintroducing incompatibilities.\n\nIt’s also noteworthy that it generated a variables.tf \u0000le,\n\nindicating that the con\u0000guration is stored locally on the\n\nterraform.tfstate \u0000le. For a production project, this should be\n\nreplaced with a shared durable repository such as S3.\n\nIf you had to replace the AWS provider, it upgraded the\n\nterraform.tfstate \u0000le as needed to bring it up to the Terraform\n\nversion you’re using. e state \u0000le is a JSON \u0000le, so you can open it\n\nand explore its contents.\n\nYou used Terraform to deploy a change to the Glue job and \u0000nally\n\ndelete it by destroying the stack. Both commands demonstrated that\n\nonce you completed the import, you can continue working as a",
      "content_length": 1282,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 552,
      "content": "regular Terraform project. Running Terraformer again would\n\noverride any changes you made.\n\nThere’s more…\n\nAs you’ve seen, reverse engineering is not trivial and you’ll likely\n\nneed to make adjustments, such as removing the number of\n\nworkers on a Glue Shell job, since it doesn’t have that concept. You\n\nshould use the generated project as a template and revise all the\n\ncode that’s produced for potential mistakes and inconsistencies. In\n\naddition, many resource types are still not available, so you would\n\nhave to add that part of the code manually.\n\nWhen you use the Terraformer import command, you can specify\n\nmultiple resources so that they are generated on the same project.\n\nHowever, the \u0000lters for resources must be speci\u0000c to the resource\n\ntype. Alternatively, you can import all without \u0000lters and then\n\nmanually delete what you don’t need.\n\nSee also\n\nTo learn how to create a Terraform project from scratch, see the\n\nDeploying a data pipeline using Terraform recipe.\n\nIf you’d rather use CDK, CloudFormation, or AWS CLI commands\n\ninstead of Terraform, there are alternative tools to help you generate\n\ncode automatically:",
      "content_length": 1135,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 553,
      "content": "Former 2 (www.former2.com): is connects to your AWS\n\naccount, detects resources, and generates code for multiple\n\nlanguages, including CDK and CloudFormation. It requires you\n\nto provide AWS credentials, so it’s best practice to provide a\n\ntemporary session with read-only limited privileges. e tool is\n\nunder development and doesn’t work for all kinds of resources.\n\nDISCLAIMER\n\nThe Former 2 tool is provided “as-is” without any guarantees, by a reputable\n\ndeveloper (AWS ambassador since 2018).\n\nConsole Recorder for AWS: From the creator of Former 2, it’s no longer\n\nunder development, so it is missing new services but it’s very easy to use.\n\nIt provides a plugin for the main web browsers, which listens to the\n\nactions you do on the AWS console and then can provide the equivalent\n\ncode for many languages, including CDK, CloudFormation, boto3, and\n\nthe AWS CLI.\n\nIntegrating AWS Glue and Git version control\n\nAWS Glue is a serverless data integration service that oﬀers\n\ndiﬀerent engines and tools for diﬀerent personas involved in data\n\nengineering. Git is the industry standard source code version",
      "content_length": 1108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 554,
      "content": "control system. Integrating both enables version handling on Glue\n\njobs and improves DevOps in general.\n\nIn this recipe, you’ll learn how to save and retrieve the status of a\n\nGlue job on a Git repository provided by AWS CodeCommit. is\n\nfeature is supported for other kinds of jobs, including notebooks,\n\nand is also available directly on the AWS console. See the ere’s\n\nmore… section for further details.\n\nGetting ready\n\nTo complete this recipe, you need a command line bash with the\n\nAWS CLI set up, as indicated in the Technical requirements section\n\nat the beginning of this chapter. e AWS user needs permission to\n\nuse AWS CodeCommit.\n\nHow to do it…\n\n1. Set up a placeholder Python script on S3 for the Glue job:\n\nBUCKET_NAME=\"recipe-glue-git-\\ $(aws sts get-caller-identity --query 'Account' \\ --output text)\" AWS_REGION=\"$(aws configure get region)\" aws s3api create-bucket --bucket $BUCKET_NAME \\ --create-bucket-configuration \\ LocationConstraint=$AWS_REGION",
      "content_length": 970,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 555,
      "content": "echo \"print('Running Shell job')\" > ShellScript.py aws s3 cp ShellScript.py s3://$BUCKET_NAME rm ShellScript.py\n\n2. Create a Glue Shell job, referencing the script you uploaded in Step 1:\n\naws glue create-job --name recipe-glue-git- job\\ --role arn:aws:iam::$(aws sts get-caller- identity\\ --query 'Account' --output text)}role/RoleForGlue\\ --command '{\"Name\" : \"pythonshell\",'\\ '\"PythonVersion\": \"3.9\", \"ScriptLocation\":'\\ '\"s3://'$BUCKET_NAME'/ShellScript.py\"}'\n\n3. Create a CodeCommit Git repository and push the job onto the main\n\nbranch:\n\naws codecommit create-repository \\ --repository-name RecipeGlueGit aws glue update-source-control-from-job --job- name \\ recipe-glue-git-job --provider AWS_CODE_COMMIT \\ --repository-name RecipeGlueGit --branch-name main\n\n4. Delete the job script and verify it’s no longer in the bucket:\n\naws s3 rm s3://$BUCKET_NAME/ShellScript.py aws s3 ls s3://$BUCKET_NAME/\n\n5. Recover the script from Git to S3 so that the job can use it:",
      "content_length": 970,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 556,
      "content": "aws glue update-job-from-source-control --job- name \\ recipe-glue-git-job --provider AWS_CODE_COMMIT \\ --repository-name RecipeGlueGit --branch- name main aws s3 ls s3://$BUCKET_NAME/\n\n6. Download the restored script and verify that it contains the simple\n\nprint statement you created it with in step 1:\n\naws s3 cp s3://$BUCKET_NAME/ShellScript.py . cat ShellScript.py\n\n7. Finally, run the cleanup commands:\n\nrm ShellScript.py aws glue delete-job --job-name recipe-glue- git-job aws s3 rm s3://$BUCKET_NAME/ShellScript.py aws s3api delete-bucket --bucket $BUCKET_NAME\n\nHow it works…\n\nIn the \u0000rst two steps, you created a sample Glue Shell job with a\n\nminimal script. Please note that the role that we used wasn’t created\n\nin this recipe, so the job can’t be run. However, this isn’t a\n\nrequirement for this recipe.\n\nen, you created a CodeCommit Git repository using the default\n\nmain branch, where you asked Glue to save the job. is created a",
      "content_length": 944,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 557,
      "content": "commit on the branch with two \u0000les placed under a directory\n\nmatching the name of the job: a JSON \u0000le with the job de\u0000nition\n\nand the Python script. It is possible to con\u0000gure the folder on Git\n\nunder which these \u0000les are saved.\n\nNotice how simple this was compared to having to con\u0000gure a Git\n\nclient, credentials, and connectivity; it’s all integrated using your\n\nIAM user permissions.\n\nTo demonstrate versioning, you simulated a mistake where the\n\nscript on S3 was deleted and then recovered from the latest version\n\nof the Git repository.\n\nThere’s more…\n\nis recipe used the command line to do all the actions to\n\ndemonstrate it can be automated, as well as to make things easier\n\nfor users not familiar with AWS Glue.\n\nHowever, normally, this Git integration is used via the AWS console\n\nwhile the user is developing a data solution. is is especially the\n\ncase for AWS Studio to develop visual jobs, where the user de\u0000nes\n\nthe ETL visually instead of writing a script \u0000le.\n\ne Glue console provides an action to push and pull changes from\n\nGit that can be used in all kinds of Glue jobs, including managed",
      "content_length": 1111,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 558,
      "content": "notebooks. When you use this action in the console, the repository’s\n\ncon\u0000guration is stored on the Version Control tab:\n\nFigure 8.3 – Glue Studio version control actions\n\nIf you wish to use one of the other Git repositories supported\n\ninstead of CodeCommit, such as GitHub or GitLab, you must\n\nspecify further details and authentication mechanisms, depending\n\non each vendor’s requirements.\n\nIn this recipe, you used the default main branch for simplicity; in\n\npractice, development would typically be done on a diﬀerent\n\nbranch. en, once a version is deemed suitable for production, it\n\nwould be merged onto the branch that re\u0000ects production, possibly",
      "content_length": 655,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 559,
      "content": "with an automated process that monitors that branch for commits\n\nand triggers a pipeline to get those changes tested and deployed.\n\nAt the time of writing, branches must already exist for Glue to be\n\nable to use them. You can create one easily if you have the Git client\n\non a checkout project by running git branch <new branch\n\nname> or by using the AWS CLI on CodeCommit, branching from\n\nanother branch, like so:\n\naws codecommit get-branch --repository-name <your repo> \\ --branch-name <branch to branch from> aws codecommit create-branch --repository-name <your repo>\\ --branch-name dev --commit-id <from the first command>\n\nSee also\n\nWhen the Glue job is pushed into the repository, you can have a\n\ndeployment pipeline automatically trigger and run a CI/CD process. e\n\nSetting up a code deployment pipeline using CDK and AWS CodePipeline\n\nrecipe shows how to do this.\n\nTo learn more about Glue and its capabilities, see the recipes dedicated\n\nto this AWS data integration service in Chapter 3, Ingesting and\n\nTransforming Your Data with AWS Glue.\n\nOceanofPDF.com",
      "content_length": 1067,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 560,
      "content": "9 Monitoring Data Lake Cloud Infrastructure\n\nIn this chapter, we will discuss the essential aspects of tracking and\n\nmonitoring your data lake infrastructure. A data lake, oen a\n\nrepository for vast amounts of structured and unstructured data, is\n\na critical component of any data-driven organization. However,\n\nwithout eﬀective monitoring, the data lake can quickly become a\n\ndata swamp, leading to ineﬃciencies, increased costs, and potential\n\ncompliance risks. e recipes covered in this chapter are designed\n\nto address common challenges and ensure your data lake remains\n\nan asset rather than a liability.\n\nis chapter includes the following recipes:\n\nAutomatically setting CloudWatch log group retention to reduce cost\n\nCreating custom dashboards to monitor Data Lake services\n\nSetting up System Manager to remediate non-compliance with AWS\n\nCon\u0000g rules\n\nUsing AWS con\u0000g to automate non-compliance S3 server access logging\n\npolicy\n\nTracking AWS Data Lake cost per analytics workload",
      "content_length": 989,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 561,
      "content": "By the end of this chapter, you will have acquired the knowledge\n\nand skills necessary to monitor your data platform eﬀectively. is\n\nwill help you maintain a robust, eﬃcient, cost-eﬀective data lake\n\noperation.\n\nTechnical requirements\n\ne code \u0000les for this chapter are available on GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter09.\n\nAdditional information\n\nBefore you start this chapter, you need to understand the following\n\nconcepts:\n\nLogs: Logs are vital for diagnosing issues, auditing activities, and\n\nmonitoring the health of your data lake. You can subscribe to speci\u0000c\n\nevents within your log \u0000les, such as errors and warnings, to stay\n\ninformed about critical occurrences. is proactive approach enables\n\nyou to address issues before they escalate, ensuring the smooth\n\noperation of your data lake services.\n\nAlarms: Alarms are used to monitor speci\u0000c metrics and trigger actions\n\nbased on prede\u0000ned thresholds. For example, you can set an alarm for\n\nCPU utilization to notify the development team when usage exceeds a\n\ncertain limit. Additionally, alarms can automate responses, such as",
      "content_length": 1152,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 562,
      "content": "launching new instances to handle increased load, thereby maintaining\n\nperformance and avoiding service disruptions.\n\nEventBridge: It contains events, rules, and targets. Amazon EventBridge\n\nallows you to manage and respond to events across your AWS\n\nenvironment. It comprises three main components:\n\nEvents: Any signi\u0000cant occurrence in your system, such as an\n\nAPI call, console sign-in, auto-scaling state change, EC2\n\ninstance state change, or EBS volume creation, is included here.\n\nRules: ey de\u0000ne the conditions under which events should\n\ntrigger actions. For example, you might create a rule that\n\ntriggers when an EC2 instance changes state.\n\nTargets: ese are the actions that are executed when an event\n\nmatches a rule. Common targets include AWS Lambda\n\nfunctions, Amazon Simple Noti\u0000cation Service (SNS), and\n\nAmazon Simple Queue Service (SQS).\n\nBy con\u0000guring EventBridge with appropriate rules and targets,\n\nyou can automate responses to various events, enhancing the\n\nresilience and eﬃciency of your data lake infrastructure.\n\nAutomatically setting CloudWatch log group retention to reduce cost\n\nAmazon CloudWatch collects metrics, logs, and events from your\n\nresources by default. ese logs could then be used to build",
      "content_length": 1235,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 563,
      "content": "dashboards, alarms, and alerts. By default, Amazon CloudWatch\n\nLogs stores your log data inde\u0000nitely, which can add up the cost,\n\nparticularly when you use detailed monitoring instead of basic\n\nmonitoring. Using Lambda to automatically check log groups\n\nwithin the regions of your services and data lake can help you save\n\non storage costs.\n\nGetting ready\n\nBefore reducing logging costs, you need to have a strategy. Good\n\nlogging leads to good monitoring. A sizable number enables\n\nhumans and machines to analyze information. It would be best to\n\nhave a logging strategy that can answer questions such as “who did\n\nwhat and when?” without including sensitive information such as\n\npasswords or secrets before trying to reduce the number of logs.\n\nHow to do it…\n\n1. In the Home console, click on IAM:",
      "content_length": 799,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 564,
      "content": "Figure 9.1 – Selecting IAM in the portal\n\n2. Click on Policies and then Create policy, as shown:\n\nFigure 9.2 – Creating a policy for the Lambda service\n\n3. Select Lambda from the Select a service drop-down menu:",
      "content_length": 211,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 565,
      "content": "Figure 9.3 – Selecting Lambda\n\n4. On the next page, select JSON formatting:\n\nFigure 9.4 – Creating a policy as JSON\n\n5. Add the following Lambda IAM policy in JSON formatting:\n\n\"Version\": \"2012-10-17\", \"Statement\": [",
      "content_length": 216,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 566,
      "content": "{ \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:PutLogEvents\", \"logs:DescribeLogGroups\", \"logs:PutRetentionPolicy\" ], \"Resource\": \"*\" } ] }\n\n6. Name the policy, for example, lambda-logging. en click on\n\nCreate policy:\n\nFigure 9.5 – Naming the Lambda policy\n\n7. Next, click on Create role from the IAM console:",
      "content_length": 372,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 567,
      "content": "Figure 9.6 – Selecting roles in the IAM portal\n\n8. Select AWS Service and then Lambda under the Service or use case\n\ndropdown, and then click on Next:\n\nFigure 9.7 – Selecting the Lambda service for roles\n\n9. Select the lambda-logging policy that we created in step 6 and click\n\non Next:",
      "content_length": 286,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 568,
      "content": "Figure 9.8 – Adding the lambda-logging policy to roles\n\n10. Fill in Role details and click on Create role. For best supervision, click on\n\nAdd tag and add the purpose of creating the policy:\n\nFigure 9.9 – Clicking on Create role\n\n11. Select Services | Lambda as shown:",
      "content_length": 268,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 569,
      "content": "Figure 9.10 – Selecting the Lambda service\n\n12. en click on the Create a function button:\n\nFigure 9.11 – Selecting the Create a function button\n\n13. Fill in basic information, as shown in Figure 9.12, and choose the IAM\n\nrole that was created in step 10. en click on Next:",
      "content_length": 274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 570,
      "content": "Figure 9.12 – Filling in the function name, runtime, and execution\n\nrole\n\n14. Go to the code editor and paste the following Python code:\n\nimport boto3 from botocore.config import Config import logging import os # Set the number of retention days retention_days = 30 # logging LOGGER = logging.getLogger() LOGGER.setLevel(logging.INFO) logging.getLogger('boto3').setLevel(logging.CR ITICAL) logging.getLogger('botocore').setLevel(logging .CRITICAL) regions = [item.strip() for item in os.environ['AVAILABLE_REGION'].split(\",\") if item]",
      "content_length": 534,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 571,
      "content": "def lambda_handler(event, context): LOGGER.info(f\"start checking= {regions}\") if not regions: return {'statusCode': 200, 'body': 'No regions found '} for region in regions: client = boto3.client('logs', region_name= region) response = client.describe_log_groups() nextToken = response.get('nextToken', None) log_groups = response['logGroups'] # Continue to fetch log groups if nextToken is present while nextToken is not None: response = client.describe_log_groups(nextToken=nextToken ) nextToken = response.get('nextToken', None) log_groups += response['logGroups'] for group in log_groups: if 'retentionInDays' in group.keys(): print(group['logGroupName'], group['retentionInDays'], region) else: print(\"Retention Not found for \", group['logGroupName'], region) set_retention = client.put_retention_policy(\n\nlogGroupName=group['logGroupName'],",
      "content_length": 845,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 572,
      "content": "retentionInDays=retention_days ) LOGGER.info(f\"PutRetention result {set_retention}\") return {'statusCode': 200, 'body': 'completed.'}\n\n15. Go to the CloudWatch console and navigate to the Rules section:\n\nFigure 9.13 – Clicking on Rules under the Events section",
      "content_length": 260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 573,
      "content": "16. Click on Create rule:\n\nFigure 9.14 – Clicking on Create rule\n\n17. Fill in the Name and Description \u0000elds, as shown, and click on Next:\n\nFigure 9.15 – Filling in the rule details\n\n18. When you click on Next, you will be led to the page where you can select\n\nthe event pattern. Select CloudWatch Logs under AWS Service and AWS",
      "content_length": 328,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 574,
      "content": "API Call via CloudTrail under Event Type as shown here:\n\nFigure 9.16 – Selecting the AWS service and Event type options\n\n19. In the Speci\u0000c operations \u0000eld, add CreateLogGroup:",
      "content_length": 176,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 575,
      "content": "Figure 9.17 – Filling in the Speciﬁc operation(s) box\n\n20. On the Create rule page, select the Lambda function that you created to\n\nlink the CloudWatch Events rule with the Lambda function:",
      "content_length": 189,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 576,
      "content": "Figure 9.18 – Selecting Lambda as the target service\n\n21. Con\u0000gure tags if needed. en, under Review and create, review all the\n\nprevious steps and then click on Create rule:",
      "content_length": 174,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 577,
      "content": "Figure 9.19 – Creating the CloudWatch rule\n\nHow it works…\n\ne Lambda function will scan all log groups within the region and\n\napply the 30-day retention rule. CloudWatch will trigger the",
      "content_length": 186,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 578,
      "content": "Lambda function to run every \u0000ve days. ere are two required\n\nparameters, logGroupName and retentionInDays, which de\u0000ne\n\nthe name of the log group you want to target and the number of\n\ndays you are trying to retain a log.\n\nIn the Event pattern, once you are used to the manual select, you\n\ncan re-use the JSON and alter it per use case. e JSON event\n\npattern that was created through steps 8-10 is as follows:\n\n{ \"source\": [\"aws.logs\"], \"detail-type\": [\"AWS API Call via CloudTrail\"], \"detail\": { \"eventSource\": [\"logs.amazonaws.com\"], \"eventName\": [\"CreateLogGroup\"] } }\n\nThere’s more…\n\nAnother architecture option is to combine Event Bridge with\n\nLambda. is architecture creates an event-based Lambda trigger to\n\napply the retention rules right away aer the log groups are created,\n\nsuch as in the reference link. Alternatively, you can also set the S3\n\nlife cycle policy archive log data to S3.\n\nSee also",
      "content_length": 910,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 579,
      "content": "put_retention_policy:\n\nhttps://boto3.amazonaws.com/v1/documentation/api/latest/reference/se\n\nrvices/logs/client/put_retention_policy.html#\n\nReduce log-storage costs by automating retention settings in Amazon\n\nCloudWatch: https://aws.amazon.com/blogs/infrastructure-and-\n\nautomation/reduce-log-storage-costs-by-automating-retention-settings-\n\nin-amazon-cloudwatch/\n\nFilter pattern syntax for metric \u0000lters, subscription \u0000lters, and \u0000lter log\n\nevents:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAn\n\ndPatternSyntax.html\n\nCreating custom dashboards to monitor Data Lake services\n\nCloudWatch dashboards allow you to create interactive\n\nvisualizations of your data, giving you a consolidated view of the\n\nhealth and status of your AWS assets. CloudWatch provides several\n\npre-built dashboards for various services, but you can also create\n\ncustom dashboards to meet your speci\u0000c needs.\n\nIn the daily operation, you should have a high-level dashboard built\n\nfrom CloudWatch log groups metrics to help you understand the\n\ngeneral performance and perform drill-down on services if needed,\n\nsuch as creating a high-level dashboard on ETL performance and",
      "content_length": 1164,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 580,
      "content": "then drilling down to Glue job run monitoring if required for\n\nfurther investigation.\n\nGetting ready\n\nTo proceed with this recipe, you need a CloudWatch log group for\n\nthe services for which you want to build a dashboard.\n\nHow to do it…\n\n1. Open the AWS console and select the CloudWatch service, which will\n\npresent you with a screen that is similar to the following:\n\nFigure 9.20 – Selecting the CloudWatch log group that you want\n\nto monitor\n\n2. Click on Dashboards. Here, you will see two options: Custom\n\ndashboards and Automatic dashboards. For this recipe, we will click on\n\nCreate dashboard next to Custom Dashboards.",
      "content_length": 625,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 581,
      "content": "Figure 9.21 – Selecting Create dashboard\n\n3. On the next page, select the widget that is relevant to your use case. e\n\npopular ones are as follows:\n\nLine chart, which helps you to see the trend over time\n\nAlarm status, which helps you see a set of alarms\n\nBar chart, which helps you compare categories of data",
      "content_length": 310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 582,
      "content": "Figure 9.22 – Selecting the type of chart that is appropriate for\n\nyour dashboard\n\n4. Select whether you want to monitor metrics or log insights, which allow\n\nyou to query log groups. You can click on Choose a sample query to see\n\na sample query that you can run in the CloudWatch console.",
      "content_length": 289,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 583,
      "content": "Figure 9.23 – Creating a query for your dashboard\n\nHow it works…\n\nAmazon CloudWatch dashboards rely on the underlying structure\n\nof log groups and streams to organize and present data. A log group\n\nacts as a container for log streams that share the same properties,\n\noen grouped by application, system component, or similar criteria.\n\nese log groups can be associated with speci\u0000c AWS resources.\n\nWhen creating a dashboard, users can select speci\u0000c log groups and\n\nstreams and then apply metric \u0000lters to transform the raw log data\n\ninto numerical metrics. ese metrics can be visualized in various\n\nforms, such as graphs, charts, or tables within the dashboard. is\n\nprocess allows users to distill large volumes of log data into\n\nmeaningful insights.\n\nThere’s more…",
      "content_length": 769,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 584,
      "content": "Aer you create a dashboard, you can migrate it across multiple\n\nenvironments or accounts by simply copying the source of the\n\nwidget to a new environment using the Copy source option, as\n\nshown. Remember to check that the properties part aligns with the\n\nproperties of the new environment.\n\nFigure 9.24 – Copying the source of a widget to migrate the\n\ndashboard to another environment\n\nFor S3 monitoring, S3 Storage Lens provides metrics and dashboard\n\nability to allow you to understand your storage usage. Some use\n\ncases include observing bucket costs across organizations, life cycle\n\nrules, and incomplete multiple upload parts that are over seven days\n\nold. ere are free and advanced metrics for building the\n\ndashboard.\n\nDepending on your use case, there would be a lot of metrics to\n\nchoose from, such as ActiveJobCount and ActiveWork\u0000owCount,\n\nwhich help you understand Glue resource utilization or YARN\n\nutilization metrics for EMR. Before choosing which logs to monitor,",
      "content_length": 983,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 585,
      "content": "your team must decide on relevant use case logs with the goals of\n\nmonitoring and investigation in mind. Selecting the relevant\n\nmetrics should help give you an overview of your service and\n\nsystem health with an option to drill down further if needed. ere\n\nare \u0000ve key areas to monitor:\n\nPerformance such as bottlenecks and hotspots\n\nSecurity such as assessments and violations\n\nCon\u0000guration such as change history and violations\n\nCost such as drive and control business spending\n\nFault tolerance such as reliability and availability\n\nCloudwatch comes with standard metrics such as CPU and network\n\nutilization for instances, disk read/write operations for Amazon\n\nEBS volumes, and memory and disk activity for managed database\n\n(Amazon RDS) instances. You can also create custom metrics that\n\nare speci\u0000c to the function of your instance and have them\n\nregistered in CloudWatch. For example, if you are running an\n\nHTTP server on the instance, you could publish a statistic on\n\nservice memory usage.\n\nSee also\n\nMonitoring and optimizing the Data Lake environment:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/building-data-",
      "content_length": 1129,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 586,
      "content": "lakes/monitoring-optimizing-data-lake-environment.html\n\nMaximizing the value of your cloud-enabled enterprise Data Lake:\n\nhttps://aws.amazon.com/blogs/apn/maximizing-the-value-of-your-\n\ncloud-enabled-enterprise-data-lake-by-tracking-critical-metrics/\n\nCost and usage analysis – AWS Well-Architected Labs:\n\nhttps://wellarchitectedlabs.com/\n\nMonitoring workload resources:\n\nhttps://docs.aws.amazon.com/wellarchitected/latest/reliability-\n\npillar/monitor-workload-resources.html\n\nSetting up System Manager to remediate non-compliance with AWS Config rules\n\nIn this recipe, we will learn how to set up AWS System Manager\n\n(SSM) so that we can use this setting to automate non-compliance\n\nremediation in the next chapter. AWS SSM automation simpli\u0000es\n\nmaintenance and deployment tasks for AWS services such as S3.\n\nFor a data engineer, one of the common challenges is ensuring that\n\nall resources adhere to organizational policies and regulatory\n\nrequirements. Due to the length of the recipe, it will be broken into\n\ntwo parts: setting up SSM in this recipe and using this setup in the\n\nUsing AWS con\u0000g to automate non-compliance S3 server access\n\nlogging policy recipe later.",
      "content_length": 1172,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 587,
      "content": "AWS SSM is a comprehensive suite of tools designed to help\n\nautomate and streamline various management tasks across your\n\nAWS infrastructure. It provides a secure, remote management\n\nsolution for con\u0000guring your managed instances, ensuring that they\n\nremain compliant with your organization’s standards and policies.\n\nis automation not only enhances eﬃciency but also reduces the\n\nrisk of manual errors, making it an invaluable tool for maintaining\n\nthe health and security of your AWS environment.\n\nGetting ready\n\nEnsure that you enable AWS Con\u0000g and AWS SSM in your account.\n\nHow to do it…\n\n1. In the AWS IAM console, navigate to Roles and click on Create role.\n\n2. Under Select type of trusted entity, choose AWS service and System\n\nManager.\n\n3. Under Role name, search for the\n\nSSMServiceRoleForAutomation policy on the Attached\n\npermissions policy page.",
      "content_length": 859,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 588,
      "content": "Figure 9.25 – Selecting the SSM role\n\n4. You need to add S3 permissions that SSM automation would use, such as\n\nthe following policy. You need to tailor it to your organization’s security\n\nrequirements:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [\n\n\"s3:PutEncryptionConfiguration\",",
      "content_length": 336,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 589,
      "content": "\"s3:PutBucketLogging\", \"s3:GetBucketLogging\" ], \"Resource\": \"arn:aws:s3:::*\" } ] }\n\n5. Go to Trust Relationships and edit the trust entities as shown:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"ssm.amazonaws.com\" }, \"Action\": \"sts:AssumeRole\", \"Condition\": { \"StringEquals\": { \"aws:SourceAccount\": \"YOUR-ACCOUNT-ID\" }, \"ArnLike\": { \"aws:SourceArn\": \"arn:aws:ssm:*:YOUR-ACCOUNT-ID:automation- execution/*\" } } } ] }",
      "content_length": 462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 590,
      "content": "Save and proceed to the next recipe for monitoring non-\n\ncompliant AWS Con\u0000g rules.\n\nThere’s more…\n\nAWS SSM Explorer is an operations dashboard presenting\n\ninformation about your resources. is dashboard consolidates\n\noperations data views from various AWS accounts and across\n\ndiﬀerent AWS regions. By doing so, Explorer oﬀers insights into the\n\ndistribution of operational issues, their trends over a period, and\n\ntheir diﬀerentiation by various categories. It aids in understanding\n\nthe overall functional health and potential areas that may require\n\nattention.\n\nSee also\n\nUse IAM to con\u0000gure roles for automation:\n\nhttps://docs.aws.amazon.com/systems-\n\nmanager/latest/userguide/automation-setup-iam.html\n\nUsing AWS config to automate non- compliance S3 server access logging policy",
      "content_length": 785,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 591,
      "content": "In the Creating custom dashboards to monitor Data Lake services\n\nrecipe, we learned that the S3 Storage Lens provides a general\n\ndashboard to observe your S3 activities. One of the best practices\n\nfor more comprehensive monitoring and auditing of your bucket is\n\nenabling S3 server access logging. is feature gives you detailed\n\nrecords of the requests made to the buckets, which is helpful in\n\nscenarios wherein you need to detect potential security weaknesses\n\nand incidents. is recipe will teach you to use AWS Con\u0000g and\n\nAWS SSM to enforce this feature.\n\nYou can use the idea in this recipe to create more enforcement not\n\nonly for S3 but also for other resources in your Data Lake.\n\nGetting ready\n\nEnsure that you enable AWS Con\u0000g and AWS SSM in your account.\n\nYou also need to \u0000nish setting up the AWS SSM role as covered in\n\nthe Setting up System Manager to remediate non-compliant AWS\n\nCon\u0000g rules recipe.\n\nBesides that, you need to identify the scope of your AWS Con\u0000g in\n\nthe general settings as shown:",
      "content_length": 1014,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 592,
      "content": "Figure 9.26 – Selecting the recording strategy for AWS conﬁg\n\nHow to do it…\n\n1. Select AWS Con\u0000g service; if this is your \u0000rst time starting with AWS\n\nCon\u0000g, you can select 1-click setup as shown:.",
      "content_length": 197,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 593,
      "content": "Figure 9.27 – Selecting 1-click setup\n\n2. In Step 2 – Rules, under AWS Managed Rules, select s3-bucket-logging-\n\nenabled and then click on Review and create:\n\nFigure 9.28 – Selecting relevant rule name in AWS Managed Rules",
      "content_length": 222,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 594,
      "content": "Head to the Rules part in the AWS Con\u0000g console, where you can\n\nsee the buckets that are not compliant under Detective compliance:\n\nFigure 9.29 – The list of buckets that do not align with the rules\n\n3. Select Manage remediation to con\u0000gure automated remediation actions:\n\nFigure 9.30 – Selecting Manage remediation to resolve the non-\n\ncompliance\n\n4. Select Automatic remediation, and in the Remediation action details\n\nsection, choose AWS-Con\u0000gureS3BucketLogging. More options exist,\n\nsuch as Creating a Jira ticket or Publishing an SNS noti\u0000cation. In this\n\nrecipe, we will select AWS-Con\u0000gureS3BucketLogging.",
      "content_length": 612,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 595,
      "content": "Figure 9.31 – Editing the Remediation action section\n\n5. Choose BucketName under Resource ID parameter:\n\nFigure 9.32 – Selecting BucketName for remediation action\n\n6. In the Parameters \u0000eld, enter the values shown in the following image\n\nand then save the changes:",
      "content_length": 264,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 596,
      "content": "Figure 9.33 – Parameters for remediation action\n\nHow it works…\n\nAWS Con\u0000g rules can verify speci\u0000c settings within your AWS\n\nenvironment, such as whether Amazon S3 buckets have logging\n\nenabled. ese rules employ AWS Lambda functions to conduct\n\ncompliance assessments, returning either compliant or non-\n\ncompliant statuses for the inspected resources. If a resource is non-\n\ncompliant, it can be corrected through a remediation action linked\n\nto the AWS Con\u0000g rule. e auto-remediation feature of AWS\n\nCon\u0000g rules allows this corrective action to be triggered",
      "content_length": 561,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 597,
      "content": "automatically, immediately addressing any detected non-\n\ncompliance.\n\nThere’s more…\n\nWith AWS server access logs enabled, you can use Athena to\n\nanalyze these logs.\n\nIn addition to using prede\u0000ned AWS Con\u0000g rules, you have the\n\n\u0000exibility to create custom AWS Con\u0000g rules to enforce your own\n\ncorporate security policies. ese custom rules are linked to an\n\nAWS Lambda function that you develop and manage. When a\n\ncustom rule is triggered, it executes the associated Lambda\n\nfunction, enabling you to enforce speci\u0000c con\u0000gurations tailored to\n\nyour organization’s needs. Furthermore, AWS Con\u0000g provides real-\n\ntime noti\u0000cations whenever a resource is miscon\u0000gured or violates\n\nthe de\u0000ned security policies, allowing for prompt remediation and\n\nenhancing the security posture of your Data Lake infrastructure.\n\nSee also\n\nSetting up AWS Con\u0000g with the console:\n\nhttps://docs.aws.amazon.com/con\u0000g/latest/developerguide/gs-\n\nconsole.html\n\nRemediating non-compliant AWS Con\u0000g rules with AWS SSM\n\nautomation runbooks: https://aws.amazon.com/blogs/mt/remediate-",
      "content_length": 1055,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 598,
      "content": "noncompliant-aws-con\u0000g-rules-with-aws-systems-manager-\n\nautomation-runbooks/\n\nAnalyzing Amazon S3 server access logs using Athena:\n\nhttps://repost.aws/knowledge-center/analyze-logs-athena\n\nAmazon S3 server access log format:\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html\n\nTracking AWS Data Lake cost per analytics workload\n\nIn an enterprise data lake, multiple teams oen use the data lake to\n\nrun several business campaigns. Tying the cost to the business value\n\nit provides is crucial to assess the return on investment later.\n\nIt is essential to have a cost allocation strategy. ere are multiple\n\nways to build the systems, such as implementing cost allocation\n\ntagging to re\u0000ect business units that utilize the resources as well as\n\nclosely monitoring the cost through set budgets, and building\n\nCloud Intelligence dashboards such as Cost and Usage report and\n\nCost Intelligence Dashboards.\n\nis recipe will look at how to track cost per analytics workload.\n\nFirst, we will look at how to create tags and then at how to bulk-add\n\ntags for multiple resources using Tag Editor. Finally, we will use the\n\ncost categories.",
      "content_length": 1141,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 599,
      "content": "Getting ready\n\nEnsure that you can access AWS billing and cost services and\n\ndiscuss with stakeholders to create a strategy for monitoring costs,\n\nsuch as using a tagging strategy.\n\nHow to do it…\n\n1. First, tag your resources in a key-value pair. e Name tag is oen used\n\nin the AWS GUI to identify resources. Each tag should have a single\n\nvalue. For example, if you want to remember that a particular redshi\n\ncluster is for a marketing test environment, you should tag as shown in\n\nthe following \u0000gure instead of combining the tag such as Name:\n\nmarketing-test.\n\nFigure 9.34 – Tagging an example of a key-value pair\n\n2. Select Resource Groups & Tag Editor in the console.",
      "content_length": 675,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 600,
      "content": "Figure 9.35 – Selecting the resource group and tag editor in AWS\n\nportal\n\n3. Go to Tag Editor under Tagging and choose the AWS regions that you\n\ndeployed the resources to. In the Tag key, choose department and tag\n\nvalue as marketing in the Tags section. en click on the Search\n\nresources button. In this example, we created a project named\n\ntokenization; thus, we will \u0000lter the resources search box as Tag:\n\nproject: tokenization and hit the Enter key.",
      "content_length": 455,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 601,
      "content": "Figure 9.36 – The tag editor for resource groups\n\n4. Select all the resources. en click on Manage tags of selected resources.\n\nFigure 9.37 – Managing the tags of selected resources\n\n5. In the Edit tags of all selected resources section as shown next, you can\n\nadd additional tags besides the pre-existing ones. Add the\n\ncost_center tag and then click on Review and apply tag changes to\n\nbulk-apply this to the resources that you previously tagged for the\n\nproject tokenization.",
      "content_length": 478,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 602,
      "content": "Figure 9.38 – Applying changes after editing tags\n\n6. In the following steps, we will start creating cost categories. First, you\n\nneed to go to Cost allocation tags and activate the tags that you want to\n\ncategorize such as the name, department, or cost_center tags,\n\nwhich we created in the previous steps. Click on the tag you want to\n\nactivate, then click on the Activate button. Please note that sometimes\n\ntags take time to show up in Cost allocation tags.",
      "content_length": 461,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 603,
      "content": "Figure 9.39 – Creating cost allocation tags\n\n7. Go to the cost category. In the Cost category details section, give a name\n\nfor your categorization such as Cost per environment to re\u0000ect\n\ncategorization per environment usage.\n\nFigure 9.40 – Naming the cost category",
      "content_length": 265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 604,
      "content": "8. Select the lookback period to apply the rules to the previous date.\n\nFigure 9.41 – Selecting the lookback period\n\n9. On the De\u0000ne category rules page, select Inherited value, then choose\n\nCost Allocation Tag as Dimension and Name as Tag key.",
      "content_length": 244,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 605,
      "content": "Figure 9.42 – Deﬁning the category rules\n\n10. Click on Create cost category and wait for the category to \u0000nish\n\nprocessing. e longer of a lookback period it has, the longer it will take\n\nto process the category.",
      "content_length": 212,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 606,
      "content": "Figure 9.43 – Creating a cost category\n\nHow it works…\n\nCost allocation tags allow you to categorize your AWS resources as\n\nEC2 instances, EMR clusters, Glue jobs, and so on based on the\n\nworkload they are processing. In addition to that, you can de\u0000ne a\n\ntag per BusinessUnit, Workload, and Department, and then use\n\nAWS Tag Editor or SDK/CLI to apply these tags to relevant\n\nresources in your Data Lake.\n\nThere’s more…\n\nOrganizing your costs with cost categories is a foundational step for\n\nyour organization to visualize these categories and apply split",
      "content_length": 555,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 607,
      "content": "charges. You can further create cost visualizations, such as building\n\na QuickSight dashboard or using cost explorer in Cost\n\nManagement. To use cost explorer for dashboard visualization,\n\nsimply use the Cost category option in the Report parameters.\n\nMake sure to save it to the report library for quick access later.\n\nSplit charges let you divide a single cost category’s expenses across\n\nmultiple target values, either proportionally, by \u0000xed percentages, or\n\nas an even split. For example, you might have a general Project cost\n\ncategory. Still, you can apply split charges to allocate speci\u0000c\n\nportions of this category to multiple teams based on their actual\n\nresource utilization or business objectives.\n\nSee also\n\nUser-de\u0000ned tag restrictions:\n\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/allocatio\n\nn-tag-restrictions.html\n\nCost modeling Data Lakes for beginners:\n\nhttps://d1.awsstatic.com/whitepapers/cost-modeling-data-lakes.pdf\n\nBest practices for tagging AWS resources:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/tagging-best-\n\npractices/tagging-best-practices.html\n\nOceanofPDF.com",
      "content_length": 1116,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 608,
      "content": "10 Building a Serving Layer with AWS Analytics Services\n\nIn this chapter, we will explore how to manage a serving layer with\n\nAmazon Redshi and QuickSight. e consumption layer focuses\n\non building solutions for data users to access, deriving data insights,\n\nand building dashboards to demonstrate the insights. Analysts must\n\nquery this data quickly and eﬃciently to generate insights and\n\nreports. However, due to the sheer volume and complexity of the\n\ndata, traditional querying methods are slow and cumbersome.\n\nus, implementing a robust consumption layer can address this\n\nchallenge by enabling fast, eﬃcient access and querying capabilities,\n\nthus empowering your analytics team to derive actionable insights\n\nwithout delay.\n\nis chapter will walk you through the \u0000rst step of managing a\n\nserving layer, from loading the data to Redshi, connecting client\n\napplications using a VPC endpoint, querying using Redshi\n\nServerless, and using AWS SDK to manage QuickSight.\n\ne recipes in this chapter are as follows:\n\nUsing Redshi workload management (WLM) to manage workload\n\npriority",
      "content_length": 1090,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 609,
      "content": "Querying large historical data with Redshi Spectrum\n\nCreating a VPC endpoint to a Redshi cluster\n\nAccessing a Redshi cluster using JDBC to query data\n\nUsing AWS SDK for pandas, the Redshi Data API, and Lambda to\n\nexecute SQL statements\n\nUsing the AWS SDK for Python to manage Amazon QuickSight\n\nTechnical requirements\n\nBefore going ahead with the recipes in this chapter, it would be\n\nuseful to have an understanding of the data lake architecture and a\n\nbasic knowledge of how data warehouses and data ingestion using\n\nGlue work.\n\ne code \u0000les for this chapter are available on GitHub:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter10.\n\nUsing Redshift workload management (WLM) to manage workload priority\n\nAmazon Redshi’s WLM feature is designed to help manage and\n\nprioritize queries and other database operations. By enabling and\n\ncon\u0000guring WLM, users can ensure critical queries receive the",
      "content_length": 944,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 610,
      "content": "necessary resources without being stalled by other less urgent\n\nprocesses. is recipe will guide you through setting up WLM,\n\nincluding the con\u0000guration of automatic WLM, to manage\n\nworkloads in your Redshi environment eﬃciently.\n\nGetting ready\n\nBefore con\u0000guring WLM in Amazon Redshi, you must have\n\nadministrative access to your Redshi cluster and the AWS\n\nManagement Console. Ensure your Redshi cluster is operational\n\nand you have familiarized yourself with the basic concepts of how\n\nRedshi handles queries and operations.\n\nHow to do it…\n\n1. Log in to the Redshi management console.\n\n2. Navigate to your cluster, click on Con\u0000gurations as shown, and click on\n\nWorkload management:",
      "content_length": 691,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 611,
      "content": "Figure 10.1 – Clicking on Workload management on the\n\nconﬁguration\n\n3. On the Workload management screen, you can see the default parameter\n\ngroup that comes with the cluster creation. In this step, we will create a\n\ncustomized parameter group to help us:",
      "content_length": 255,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 612,
      "content": "Figure 10.2 – Clicking on Create in the Parameter groups section\n\n4. In the pop-up menu, \u0000ll in the Parameter group name and Description\n\n\u0000elds:\n\nFigure 10.3 – Creating a parameter group\n\n5. Aer you create the new WLM feature, there will be a new parameter\n\ngroup. On the right-hand side, you have two options: either using\n\nautomatic WLM or creating workload queues. e default, Automatic\n\nWLM, is enabled. Click on Switch WLM mode to change the mode to\n\nManual WLM:",
      "content_length": 468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 613,
      "content": "Figure 10.4 – Switching WLM to manual WLM\n\n6. By switching to manual WLM, you will have full control of memory and\n\nconcurrency for your queues:\n\nFigure 10.5 – Switching from automatic WLM to manual WLM\n\n7. In the Modify workload queues screen, you can create or modify\n\nexisting queues to de\u0000ne workload priorities and resource allocation. On\n\nthis page, you can also set parameters to monitor each queue, such as\n\nmemory allocation, concurrency, and timeout settings. e following",
      "content_length": 482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 614,
      "content": "screenshot is an example of a queue where you can con\u0000gure memory\n\nand concurrency, create a monitoring rule, and use wildcards for the\n\nquery group:\n\nFigure 10.6 – Conﬁguring workload queues\n\nHow it works…\n\nAmazon Redshi WLM is designed to eﬃciently handle multiple\n\nworkloads by ensuring that short, fast-running queries do not get\n\ndelayed by longer, resource-intensive queries. By prioritizing\n\nqueries, Redshi WLM helps maintain optimal performance for\n\ncritical workloads and user groups.\n\nSome workloads or user groups may require higher performance\n\nthan others. Redshi allows you to set priorities for diﬀerent\n\nworkloads using a system of queues. Each queue can be assigned a",
      "content_length": 688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 615,
      "content": "priority level, which is then inherited by all queries associated with\n\nthat queue. e priority levels, from highest to lowest, are the\n\nfollowing:\n\nHIGHEST\n\nHIGH\n\nNORMAL\n\nLOW\n\nLOWEST\n\nTo manage the prioritization of queries, you can map user groups\n\nand query groups to speci\u0000c queues. is ensures that queries from\n\nimportant users or critical workloads are processed with the\n\nappropriate priority. For instance, you might assign executive\n\nreports to a queue with the highest priority, while routine data loads\n\ncould be assigned to a low-priority queue. Redshi allows up to 50\n\nconcurrent queries, so your concurrent queue needs to be a\n\nsummary of 50. For an example JSON template for WLM con\u0000g,\n\nplease access the following GitHub repo:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/blob/main/Chapter10/Recipe1.\n\nThere’s more…",
      "content_length": 861,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 616,
      "content": "Implementing eﬀective WLM in Redshi goes beyond the basic\n\ncon\u0000guration. Here are additional tips and advanced techniques:\n\nUse short query acceleration (SQA): SQA is a feature in Amazon\n\nRedshi that prioritizes short-running queries over longer-running\n\nqueries. is can help improve query performance and reduce query\n\nwait times. e default SQA maximum runtime is dynamic but can be\n\nmanually set to a \u0000xed value between 1-20 seconds. SQA is set\n\nautomatically in automatic WLM to reduce overall query latency or\n\nthrough clicking on the Enable short query acceleration for queries\n\nwhose maximum runtime is checkbox to a value of your choice:\n\nFigure 10.7 – Enabling SQA\n\nCustom WLM rules: Create custom rules to manage speci\u0000c scenarios,\n\nsuch as automatically aborting long-running queries or reallocating\n\nresources during peak times. You can further monitor query\n\nperformance and resource utilization using system tables such as\n\nSTL_WLM_QUERY, STL_QUERY, and STV_WLM_QUERY_STATE.\n\nFor this recipe, it might be helpful to know queries to get users grouped\n\ntogether. You can use the pg_group table from the system catalog\n\ntable:",
      "content_length": 1140,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 617,
      "content": "Select * from pg_group where groname='reporting_users'\n\nSee also\n\nCon\u0000guring the WLM parameter using the AWS CLI:\n\nhttps://docs.aws.amazon.com/redshi/latest/mgmt/workload-mgmt-\n\ncon\u0000g.html#Con\u0000guring-the-wlm-json-con\u0000guration-Parameter\n\nWLM dynamic and static con\u0000guration properties:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/cm-c-wlm-dynamic-\n\nproperties.html\n\nWLM queue assignment rules:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/cm-c-wlm-queue-\n\nassignment-rules.html\n\nQuerying large historical data with Redshift Spectrum\n\nAmazon Redshi Spectrum is a feature of Amazon Redshi that\n\nallows you to query exabytes of data stored in Amazon S3 directly\n\nwithout prior loading to Redshi tables. is can be useful for\n\nvarious reasons, such as querying historical datasets that have\n\nexpanded to multiple years or having multiple Redshi workgroups\n\nto query the same dataset. You can directly query scalar data or\n\nnested data formats stored in Amazon S3.",
      "content_length": 968,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 618,
      "content": "By using Redshi Spectrum, you will launch a cluster that is\n\nindependent of your existing cluster.\n\nGetting ready\n\nTo use Amazon Redshi Spectrum, you need to have the following:\n\nAn SQL client: is cluster is independent of your existing Redshi\n\ncluster.\n\nAt least three subnets: Each subnet should be associated with diﬀerent\n\nAvailability Zones (AZs) for your Redshi Serverless workspace. Make\n\nsure you understand how the subnet mask and subnet Classless Inter-\n\nDomain Routing (CIDR) block work prior to creating these subnets.\n\nFundamental knowledge of networking is required. Please see this link\n\nfor more information:\n\nhttps://docs.aws.amazon.com/vpc/latest/userguide/subnet-sizing.html.\n\nS3 bucket: e data that you want to query from the S3 bucket must be\n\nin the same region as your Amazon Redshi cluster.\n\nAn Identity and Access Management (IAM) role for Redshi: To create\n\nan IAM role for Redshi, follow the instructions at\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/c-spectrum-iam-\n\npolicies.html.\n\nIn this recipe, we will use a default-namespace namespace, so\n\nmake sure your default-namespace namespace has the relevant",
      "content_length": 1146,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 619,
      "content": "IAM roles attached. You can check for the same under Security and\n\nencryption in your default-namespace namespace:\n\nFigure 10.8 – Creating an IAM role for Redshift\n\nHow to do it…\n\n1. Select Redshi Serverless from the Amazon Redshi console.\n\n2. Select Create workgroup and follow the next page to \u0000ll in the\n\nworkgroup name, capacity, network and security, and namespace. In\n\nNetwork and security, choose the subnets that you created for Redshi\n\nSpectrum. For the capacity, as this is a test recipe, make sure you only\n\nuse the smallest Redshi Processing Unit (RPU) value, which is 8 RPUs:",
      "content_length": 592,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 620,
      "content": "Figure 10.9 – Selecting Create workgroup\n\n3. When you go to the Redshi console, there is a default namespace\n\navailable. Make sure your default-namespace namespace already\n\nhas the relevant permission to access Glue and S3, as mentioned in the\n\nGetting ready section of this recipe:\n\nFigure 10.10 – Choosing a relevant namespace",
      "content_length": 329,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 621,
      "content": "4. Once your Amazon Redshi Spectrum namespace is created, click on\n\nQuery editor v2 as shown:\n\nFigure 10.11 – Selecting Query editor v2\n\n5. When you \u0000rst start using the query editor, you can choose Federated\n\nuser to start with. Once you select it, you will see the data that exists in\n\nyour Glue data catalog:",
      "content_length": 312,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 622,
      "content": "Figure 10.12 – Selecting Federated user\n\n6. e next step is to create an external schema and external tables. You can\n\nchoose from various external databases, such as an Amazon Athena\n\nData Catalog, an AWS Glue Data Catalog, or an Apache Hive Metastore:\n\ncreate external schema spectrum_data from data catalog database 'sample_db' iam_role 'arn:aws:iam::xxxxxxxx:role/xxxxx' create external table if not exists;\n\n7. Once you have created an external schema, you can create an external\n\ntable using the following SQL statement. e LOCATION clause\n\nspeci\u0000es the location of the data in Amazon S3. e bucket_name\n\nparameter is the name of the S3 bucket where the data is stored:\n\nCREATE EXTERNAL TABLE",
      "content_length": 698,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 623,
      "content": "spectrum_data.your_table_name ( first_and_last_name VARCHAR, email VARCHAR, id BIGINT, id_2 BIGINT, gender VARCHAR, country VARCHAR, fax VARCHAR ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION 's3://sample-test-wf09/' TABLE PROPERTIES ('skip.header.line.count'='1');LOCATION 's3://bucket_name/prefix/';\n\n8. Once you have created the external table, you can use standard SQL\n\nqueries to query the data in Amazon S3. For example, the following SQL\n\nstatement will query the top three rows in the table you just created:\n\nselect top 3 * from spectrum_data.your_table_name\n\nHow it works…\n\nAmazon Redshi Spectrum is a feature of Amazon Redshi that\n\nallows you to run SQL queries on data stored directly in Amazon\n\nS3. Redshi Spectrum supports the same data format as Athena.\n\ne process begins with creating an external schema and an\n\nexternal table to reference the data stored in Amazon S3. Once the\n\nexternal table is set up, you can run SQL queries on this data",
      "content_length": 972,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 624,
      "content": "directly from Amazon Redshi. For more information on the\n\nCREATE EXTERNAL TABLE statement command, refer to the\n\nCreating external tables for Redshi Spectrum blog\n\n(https://docs.aws.amazon.com/redshi/latest/dg/c-spectrum-\n\nexternal-tables.html). Once you create an external table, you can\n\nuse SQL to query data similar to how you would use Athena.\n\nTo summarize, the following table compares Redshi Spectrum,\n\nAmazon Athena, and S3 Select:\n\nCOMPARISON\n\nAWS\n\nS3 SELECT\n\nAMAZON\n\nATHENA\n\nREDSHIFT\n\nSPECTRUM\n\nMain purpose\n\nServerless\n\nBuilt-in S3\n\nServerless\n\nSQL\n\nfeature for\n\nSQL querying\n\nquerying\n\nsimple\n\nof S3 data\n\nof S3 data\n\nqueries\n\nAllowed queries\n\nSQL, based\n\nA limited\n\nSQL, based\n\non Presto\n\nsubset of\n\non\n\nSQL\n\nPostgreSQL\n\ncommands.\n\nS3 select has\n\na maximum",
      "content_length": 774,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 625,
      "content": "COMPARISON\n\nAWS\n\nATHENA\n\nManaged/serverless Serverless\n\nTable con\u0000g\n\nVirtual\n\ntables (for\n\nexample,\n\nusing AWS\n\nGlue)\n\nUse case\n\nAd hoc\n\nquerying;\n\ninteractive\n\nS3 SELECT\n\nlength of 256\n\nKB for SQL\n\nexpressions.\n\nServerless\n\nN/A\n\nRetrieving a\n\nsubset of\n\ndata from a\n\nAMAZON\n\nREDSHIFT\n\nSPECTRUM\n\nManaged\n\nservice\n\nrequires\n\nAmazon\n\nRedshi\n\ncluster\n\nVirtual tables,\n\nmanually\n\ncon\u0000gured\n\nwith external\n\ntables\n\nEnterprise\n\nreporting;\n\nbusiness",
      "content_length": 443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 626,
      "content": "COMPARISON\n\nAWS\n\nS3 SELECT\n\nAMAZON\n\nATHENA\n\nREDSHIFT\n\nSPECTRUM\n\nanalytics;\n\nsingle object;\n\nintelligence\n\nexploring\n\nsimple\n\n(BI); complex\n\ndata in S3\n\nquerying of\n\nanalytical use\n\nan S3 object\n\ncases from\n\napps that\n\nrequire a low\n\nlatency\n\nTable 10.1 – Comparison between Athena, Redshift Spectrum,\n\nand S3 Select\n\nThere’s more…\n\nAmazon Redshi Spectrum also supports federated query access,\n\nwhich allows you to query data stored in Amazon Aurora directly\n\nfrom the Amazon Redshi query editor. To use federated query\n\naccess, follow these steps:\n\n1. You need to create a database user in Amazon Redshi that has the\n\nSUPERUSER role. You then need to create a database user in Amazon\n\nAurora that has the same name and password as the database user in\n\nAmazon Redshi. e LOCATION clause speci\u0000es the location of the",
      "content_length": 819,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 627,
      "content": "data in Amazon Aurora. e HOST and PORT clauses specify the\n\nhostname and port of the Amazon Aurora database. e DATABASE,\n\nSCHEMA, and TABLE clauses specify the name of the database, schema,\n\nand table, respectively, in Amazon Aurora. Once you have created the\n\ndatabase users, you can use the following SQL statement to create a\n\nfederated table in Amazon Redshi:\n\nCREATE TABLE table_name ( column_name1 data_type1, column_name2 data_type2, ... ) LOCATION ( TYPE = aurora HOST = 'aurora_host_name' PORT = 'aurora_port' DATABASE = 'aurora_database_name' SCHEMA = 'aurora_schema_name' TABLE = 'aurora_table_name' )\n\n2. Once you have created the federated table, you can use standard SQL\n\nqueries to query the data in Amazon Aurora directly from the Amazon\n\nRedshi query editor. For example, the following SQL statement will\n\nquery the table_name table and return the column_name1 and\n\ncolumn_name2 columns:\n\nSELECT column_name1, column_name2 FROM table_name;",
      "content_length": 960,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 628,
      "content": "3. To estimate the cost of a query executed using Redshi Spectrum, you\n\ncan use the SVL_S3QUERY_SUMMARY system view. e\n\ns3_scanned_bytes column is the number of bytes scanned; it\n\ndivides it by 1024^4 to convert it to bytes. Multiply by 2.5 to account\n\nfor the cost per GB of data scanned in Amazon Redshi Spectrum:\n\nSELECT s3_scanned_bytes / 1024^4 * 2.5 AS spectrum_cost FROM SVL_S3QUERY_SUMMARY WHERE query = <your_query_id>;\n\n4. e STL_ALERT_EVENT_LOG table monitors performance thresholds\n\nthat are exceeded. You can join STL_ALERT_EVENT_LOG with\n\nSTL_QUERY to get detailed information about queries that trigger\n\nperformance alerts. e following SQL example demonstrates how to\n\n\u0000lter the results based on a speci\u0000c job run ID (to be replaced from your\n\nsystem):\n\nSELECT A.*, B.* FROM STL_ALERT_EVENT_LOG A LEFT JOIN STL_QUERY B ON A.QUERY = B.QUERY WHERE A.PID IN (SELECT PID FROM STL_QUERY WHERE QUERYTXT LIKE '%<job_run_id>%' GROUP BY 1 );\n\n5. To further optimize the performance of Amazon Redshi Spectrum\n\nqueries, it’s essential to pre-\u0000lter the data before joining it with other",
      "content_length": 1094,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 629,
      "content": "tables and ensure that \u0000ltering is done on partition keys. is practice\n\nhelps in reducing the data scanned. Consider the following example\n\nwhere a temporary table is created to pre-\u0000lter the data:\n\nI. First, create a temporary table to pre-\u0000lter data from an external\n\nsource:\n\nCREATE TEMP TABLE TEMP_HITS_A DISTKEY (MARKER_ID) COMPOUND SORTKEY (MARKER_ID) AS (SELECT MARKER_ID, COUNT(1) AS TOTAL_HITS FROM EXTERNAL_SCHEMA.SOURCE_TABLE_A WHERE ENTITY_ID = 101 AND MARKET_ID = 1 AND EVENT_DAY AND IS_EVENT = 1 GROUP BY MARKER_ID);\n\nII. en, create another temporary table to pre-\u0000lter data from a\n\ndiﬀerent source:\n\nCREATE TEMP TABLE TEMP_HITS_B DISTKEY (MARKER_ID) COMPOUND SORTKEY (MARKER_ID) AS SELECT MARKER_ID, COUNT(1) AS TOTAL_HITS FROM DATA_WAREHOUSE.SOURCE_TABLE_B WHERE REGION_ID = 1 AND MARKET_ID = 1 AND EVENT_DAY AND IS_EVENT = 2024-04- 20'::DATE GROUP BY MARKER_ID;",
      "content_length": 880,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 630,
      "content": "III. Lastly, join the pre-\u0000ltered temporary tables and calculate the\n\ndiﬀerence:\n\nSELECT A.MARKER_ID, A.TOTAL_HITS AS HITS_A, B.TOTAL_HITS AS HITS_B, A.TOTAL_HITS - B.TOTAL_HITS AS HIT_DIFFERENCE FROM TEMP_HITS_A A LEFT OUTER JOIN TEMP_HITS_B B ON A.MARKER_ID = B.MARKER_ID;\n\nSee also\n\nBest Practices for Amazon Redshi Spectrum | AWS Big Data Blog:\n\nhttps://aws.amazon.com/blogs/big-data/10-best-practices-for-amazon-\n\nredshi-spectrum/\n\nHow do I calculate the query charges in Amazon Redshi Spectrum?\n\nhttps://repost.aws/knowledge-center/redshi-spectrum-query-charges\n\nSVL_S3QUERY_SUMMARY:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/r_SVL_S3QUERY_SU\n\nMMARY.html\n\nMonitoring queries and workloads with Amazon Redshi Serverless:\n\nhttps://docs.aws.amazon.com/redshi/latest/mgmt/serverless-\n\nmonitoring.html\n\nImproving Amazon Redshi Spectrum query performance:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/c-spectrum-external-\n\nperformance.html",
      "content_length": 953,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 631,
      "content": "Issues of overlapping CIDR ranges for subnets:\n\nhttps://repost.aws/questions/QUjrakcRqgRTGwD4pD0K8v8Q/overlappi\n\nng-cidr-ranges-for-subnets\n\nCreating a VPC endpoint to a Redshift cluster\n\nis recipe will guide you in creating a Redshi VPC endpoint to\n\nensure secure, private connectivity between your Redshi cluster\n\nand client applications within your VPC. A VPC is an isolated\n\nvirtual network within the cloud that provides private access to\n\nyour resources. By creating this endpoint, you enable direct, secure\n\naccess to your Redshi cluster from BI tools such as Tableau\n\nOnline, Qlik Sense, and Looker without exposing your data cluster\n\nto a public IP address or routing traﬃc through the internet.\n\nere are two types of VPC endpoints:\n\nInterface endpoint\n\nGateway endpoint\n\nIn this recipe, we will demo two concepts:\n\nShow you step-by-step how to create an S3 gateway endpoint so that the\n\nRedshi cluster can communicate with S3. You can use it later when you\n\ncreate a Glue connection.",
      "content_length": 999,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 632,
      "content": "Create a Redshi-managed VPC endpoint so that you can further\n\nintegrate your BI application. is scenario would be useful when you\n\nhave your Redshi cluster in a VPC that is diﬀerent from your BI\n\napplications.\n\nIt’s important to note that Redshi-managed VPC endpoints are\n\nonly available for Redshi clusters that are running on the RA3\n\nnode type and have either cluster relocation or Multi-AZ enabled.\n\nAdditionally, the cluster or workgroup must be available within the\n\nvalid port ranges, and the VPC endpoint quota limits the number of\n\nendpoints.\n\nIn the code, AWS Systems Manager (SSM) will be used to help\n\nautomate operational tasks across your AWS resources. e\n\nParameter Store capability within SSM is useful for storing sensitive\n\ndata such as database connection details, credentials, and\n\ncon\u0000guration settings in a secure and structured manner, with\n\noptional encryption using AWS Key Management Service (KMS).\n\nGetting ready\n\nBefore starting, ensure you have the following prerequisites:\n\nAn active AWS account with an IAM role that has permission to manage\n\nRedshi, VPC, Glue, and SSM\n\nKnowledge of SSM",
      "content_length": 1125,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 633,
      "content": "An active Redshi cluster\n\nHow to do it…\n\n1. In the Glue service console, select Data connections and then Create\n\nconnection:\n\nFigure 10.13 – Selecting Data connections and creating a\n\nconnection\n\n2. is step assumes that you have an existing Redshi cluster that you\n\ncreated in the previous recipe. Select Amazon Redshi as the data",
      "content_length": 335,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 634,
      "content": "source, then follow the \u0000ow of Step 2, Step 3, and Step 4, as highlighted\n\non the le, to \u0000nish the setup:\n\nFigure 10.14 – Selecting Amazon Redshift as a data source\n\n3. In Step 2, you need to select the relevant database instances, database\n\nname, and credential type. e best practice is to use AWS Secrets\n\nManager for storing credentials. If you con\u0000gure your Redshi cluster\n\nusing username and password, you can head to the Redshi console,\n\nselect Namespace con\u0000guration, and then Edit admin credentials to\n\nchange the password:",
      "content_length": 535,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 635,
      "content": "Figure 10.15 – Heading to Namespace conﬁguration to ﬁnd out\n\nyour admin credentials or to change your password\n\n4. In Step 3, set properties; if you want to reuse your connection later on in\n\nthe Jupyter notebook, you should create a name without a space. For this\n\nrecipe, we will use a connection name of redshift_serverless\n\ninstead of the default name. Later on, if you want to reuse the\n\nconnection in the Glue session, you should use the %connections\n\nredshift_serverless magic cell:\n\nFigure 10.16 – Editing the connection name\n\n5. In Step 4, when you click on Create connection, you will see a Redshi\n\nconnection successfully created message. However, at this step, your\n\nconnection won’t work. You can \u0000nd out why by clicking on Test\n\nconnection:",
      "content_length": 755,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 636,
      "content": "Figure 10.17 – Testing connection of the Redshift connection\n\ncreated\n\n6. To test the connection, you need to select the relevant IAM role. You will\n\nreceive an InvalidInputException: VPC S3 endpoint validation failed\n\nerror message:",
      "content_length": 233,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 637,
      "content": "Figure 10.18 – Error message prior to creating VPC endpoint\n\n7. Head to the VPC service, click on Endpoints, and then click on Create\n\nendpoint.\n\n8. In Endpoint settings, select AWS services, and for the service name,\n\nselect com.amazonaws.us-east-1.s3. Make sure you select the\n\none marked as the Gateway type:\n\nFigure 10.19 – Selecting an S3 gateway in Services",
      "content_length": 363,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 638,
      "content": "9. Select the VPC that you want to create the endpoint and click on Create.\n\nOnce you create the endpoint, you can go back and test the connection,\n\nand you will see that aer creating the endpoint, your test connection\n\nwill be successfully connected:\n\nFigure 10.20 – Successfully created Redshift connection\n\nHow it works…\n\nCreating connections to Amazon Redshi from AWS Glue requires\n\nan Amazon S3 VPC gateway endpoint. is is to ensure secure and\n\nprivate communication between the AWS Glue job and the Redshi\n\ncluster without exposing the data to the public internet.\n\nTo set up a VPC endpoint for Amazon S3, you would need to do the\n\nfollowing:\n\n1. Create a VPC endpoint for the S3 service in the same VPC as your\n\nRedshi cluster.\n\n2. When you create the VPC endpoint, you can customize the level of\n\naccess by con\u0000guring the route tables to allow the AWS Glue job to\n\naccess the S3 bucket through the VPC endpoint.",
      "content_length": 924,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 639,
      "content": "There’s more…\n\nFor secure, reliable connectivity, you can further integrate an AWS\n\nnetwork \u0000rewall, an AWS shield, and a Network Load Balancer\n\n(NLB) instance to enhance security and performance. Additionally,\n\nyou can employ the AWS Cloud Development Kit (AWS CDK) to\n\nde\u0000ne cloud infrastructure as code (IaC).\n\nAn NLB is not required with a Redshi VPC endpoint, but it can be\n\nused to distribute incoming traﬃc to multiple Redshi clusters or\n\nworkgroups. If you have multiple Redshi clusters or workgroups\n\nthat need to be accessed by the same client tool, you can use an NLB\n\nto distribute incoming traﬃc to diﬀerent Redshi endpoints. is\n\ncan ensure that traﬃc is distributed evenly across the diﬀerent\n\nRedshi clusters or workgroups. To set up an NLB with a Redshi\n\nVPC endpoint, you need to create a load balancer and con\u0000gure it\n\nto distribute traﬃc to diﬀerent Redshi endpoints. You will also\n\nneed to con\u0000gure security groups for the Redshi clusters or\n\nworkgroups to allow traﬃc from the load balancer. It’s important to\n\nnote that the NLB will need to be in the same VPC as the Redshi\n\nclusters or workgroups, and the security groups for the Redshi\n\nclusters or workgroups will need to be con\u0000gured to allow traﬃc\n\nfrom the load balancer.",
      "content_length": 1260,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 640,
      "content": "e script that automates the creation of a Redshi VPC endpoint,\n\nenabling secure, private connectivity for your Redshi cluster\n\nwithin the VPC, can be found in the following GitHub repo:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/blob/main/Chapter10/Recipe3.\n\ne steps involve the following:\n\n1. Initializing Boto3 clients and de\u0000ning environment variables.\n\n2. Fetching necessary parameters from AWS SSM.\n\n3. Checking if the VPC endpoint exists or if the cluster type is RA3.\n\n4. Creating a managed VPC endpoint if one doesn’t already exist.\n\ne script uses create_endpoint_access() to create a VPC\n\nendpoint with the provided cluster ID, subnet, and security group.\n\nIt will wait for 400 seconds aer initiating the creation process to\n\nallow the endpoint to be fully set up, then use an API call in\n\ndescribe_endpoint_access() to get the newly created\n\nendpoint details. e VPC endpoint details are then stored back in\n\nSSM for future reference.\n\nSee also\n\nRedshi-managed VPC endpoints:\n\nhttps://docs.aws.amazon.com/redshi/latest/mgmt/managing-cluster-\n\ncross-vpc.html",
      "content_length": 1105,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 641,
      "content": "Gateway endpoints for Amazon S3:\n\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-\n\ns3.html\n\nDiﬀerent types of VPC endpoints: https://tutorialsdojo.com/vpc-\n\ninterface-endpoint-vs-gateway-endpoint-in-aws/\n\nEnhance data security and governance for Amazon Redshi Spectrum\n\nwith VPC endpoints: https://aws.amazon.com/blogs/big-data/enhance-\n\ndata-security-and-governance-for-amazon-redshi-spectrum-with-vpc-\n\nendpoints/\n\nAccessing a Redshift cluster using JDBC to query data\n\nRedshi has many connection options depending on the use case\n\nand downstream requirements. In this recipe, we will focus on how\n\nto connect Glue to Redshi using JDBC. e main bene\u0000ts of using\n\na JDBC connection are the following:\n\nYou can connect to Redshi using a wide variety of tools\n\nDepending on your use case, you can access Redshi data within your\n\nETL job without going through the Glue crawler, which will help you\n\nsave on cost\n\nGetting ready",
      "content_length": 948,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 642,
      "content": "To connect with Amazon Redshi through JDBC, you’ll need the\n\nfollowing:\n\nAWS Glue notebook: Make sure you know how to use Glue notebooks.\n\nRedshi cluster: You should have a working Redshi cluster with the\n\nnecessary data. If you don’t have one, create it before proceeding.\n\nAccess credentials: You should have access credentials to your cluster.\n\nCon\u0000gure security group and VPC endpoint: You should complete the\n\nsetup of relevant security rules to allow inbound and outbound traﬃc.\n\nPlease follow the steps mentioned at\n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-\n\nconnect-redshi-home.html. You also need to create a VPC endpoint to\n\nconnect your application. Please see the Creating a VPC endpoint to a\n\nRedshi cluster recipe to create a managed VPC endpoint or reuse the\n\nredshift_serverless connection using the %connections\n\nsyntax. For more information, please see\n\nhttps://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-\n\nconnect-redshi-home.html.\n\nSample data: Have sample data to query. For this recipe, we will use a\n\nusers table in Redshi, as shown:\n\nCREATE TABLE public.users ( user_id integer identity(1, 1) ENCODE az64, first_name character varying(50) ENCODE lzo, last_name character varying(50) ENCODE",
      "content_length": 1265,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 643,
      "content": "lzo, email character varying(100) ENCODE lzo, created_at timestamp without time zone DEFAULT ('now':: text):: timestamp with time zone ENCODE az64 ) DISTSTYLE AUTO;\n\nHow to do it…\n\n1. Open the AWS Management Console and type Redshift in the search\n\nbar at the top. Click on the result under Services to access the Amazon\n\nRedshi management console:\n\nFigure 10.21 – Selecting Amazon Redshift\n\n2. On the Redshi page, click on Provisioned clusters dashboard and then\n\nclick on the cluster you would like to connect:",
      "content_length": 514,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 644,
      "content": "Figure 10.22 – Selecting Amazon Redshift cluster to connect\n\n3. On the right side, you will see options to connect to Redshi, such as\n\nEndpoint, JDBC URL, and ODBC URL:\n\nFigure 10.23 – Copying the JDBC URL",
      "content_length": 206,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 645,
      "content": "Depending on your application usage, you could consider a JDBC\n\nor Open Database Connectivity (ODBC) endpoint. e following\n\ncode is an example of how to use Glue to connect to the Redshi\n\ncluster using the JDBC URL. us, for this recipe, you will need the\n\nJDBC URL as shown:\n\nFigure 10.24 – Noting down Redshift JDBC URL\n\n4. Next, we need to enter the code in the Glue crawler.\n\nI. We will start with installing the necessary libraries:\n\nfrom awsglue.context import GlueContext from awsglue.dynamicframe import DynamicFrame, DynamicFrameWriter from awsglue.job import Job",
      "content_length": 574,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 646,
      "content": "from awsglue.transforms import * from awsglue.utils import getResolvedOptions from awsglue.context import GlueContext from awsglue.dynamicframe import DynamicFrame from pyspark.context import SparkContext from pyspark.sql import * import sys import boto3\n\nII. en, we create a Spark session:\n\nsc = SparkContext.getOrCreate() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext)\n\nIII. Set up your Redshi credentials:\n\nredshift_properties = { \"user\": \"your_username\", \"password\": \"your_password\", \"driver\": \"com.amazon.redshift.jdbc.Driver\" } # Define Redshift connection URL redshift_url = \"\"\n\nIV. Read the Redshi table into a Spark DataFrame:\n\ndf = spark.read.jdbc(url=redshift_url, table=\"your_table\", properties=redshift_properties)\n\nV. Create a temporary view for running SQL queries:\n\ndf.createOrReplaceTempView(\"users\") spark.sql(\"\"\" INSERT INTO public.users (first_name,",
      "content_length": 912,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 647,
      "content": "last_name, email) SELECT 'John' AS first_name, 'Doe' AS last_name, 'john.doe@example.com' AS email UNION ALL SELECT 'Jane', 'Smith', 'jane.smith@example.com' UNION ALL SELECT 'Bob', 'Johnson', 'bob.johnson@example.com' \"\"\")\n\nVI. Write it back to Redshi:\n\ndf.write.jdbc(url=redshift_url, table=\"your_data\", mode=\"overwrite\", properties=redshift_properties) \"\"\")\n\nThere’s more…\n\nYou need to analyze requirements clearly to understand whether\n\nyou need a Redshi cluster or if Redshi Spectrum would be\n\nsuﬃcient. We will learn how to use Redshi Spectrum and the pros\n\nand cons of using Redshi Spectrum in the next chapter.\n\nAmazon Redshi provides several system tables that can be used to\n\nmonitor and troubleshoot the performance of your cluster. ese\n\ntables are divided into six categories: STL, STV, SVV, SYS, SVCS,\n\nand SVL. When you migrate to Redshi Serverless, some SVV\n\nviews do not directly apply because they are designed to work with",
      "content_length": 948,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 648,
      "content": "the architecture and management features speci\u0000c to provisioned\n\nclusters. Instead, Redshi Serverless uses SYS system views.\n\nFor this recipe, we have created a Glue notebook on GitHub at\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/blob/main/Chapter10/Recipe4.ipynb.\n\nSee also\n\nTop 10 performance tuning techniques for Amazon Redshi:\n\nhttps://aws.amazon.com/blogs/big-data/top-10-performance-tuning-\n\ntechniques-for-amazon-redshi/\n\nBest practices to optimize your Amazon Redshi and MicroStrategy\n\ndeployment: https://aws.amazon.com/blogs/big-data/best-practices-to-\n\noptimize-your-amazon-redshi-and-microstrategy-deployment/\n\nHow to retain system tables’ data spanning multiple Amazon Redshi\n\nclusters and run cross-cluster diagnostic queries:\n\nhttps://aws.amazon.com/blogs/big-data/how-to-retain-system-tables-\n\ndata-spanning-multiple-amazon-redshi-clusters-and-run-cross-\n\ncluster-diagnostic-queries/\n\nTypes of system tables and views:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/cm_chap_system-\n\ntables.html#c_types-of-system-tables-and-views\n\nHow can I troubleshoot high or full disk usage with Amazon Redshi?:\n\nhttps://repost.aws/knowledge-center/redshi-high-disk-usage",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 649,
      "content": "Using AWS SDK for pandas, the Redshift Data API, and Lambda to execute SQL statements\n\ne AWS SDK for pandas library (previously known as AWS Data\n\nWrangler) is a powerful tool that provides a pandas interface to\n\nvarious AWS services, including Glue, Redshi, Athena, and more\n\nusing pandas syntax. is library is handy for data scientists, and\n\nanalysts already using the pandas library and want to interact with\n\nAWS data and analytics services. One of the use cases of AWS SDK\n\nfor pandas is to simplify the querying and manipulating data stored\n\nin AWS data stores.\n\nIn this recipe, we will demonstrate how to use AWS SDK for pandas\n\nand the Redshi Data API. In this \u0000ow, we will demonstrate a\n\nLambda function with two steps:\n\n1. Execute an SQL statement to retrieve the data.\n\n2. Save the DataFrame to S3.\n\nGetting ready\n\nBefore starting, ensure you have the following prerequisites:\n\nAWS Lambda set up with the necessary execution role permissions to\n\ninteract with Redshi and S3. Your Lambda instance needs to be at the",
      "content_length": 1030,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 650,
      "content": "same VPC subnet as your Redshi instance unless you set the VPC\n\nendpoint. is recipe assumes that you created a VPC endpoint.\n\nAble to use AWS Lambda managed layers. Check out\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.html for\n\nmore information.\n\nHave an Amazon Redshi cluster and associate it with an AWS IAM\n\nrole.\n\nHow to do it…\n\n1. Create a policy in IAM named LambdaRedshiftDataAPIRole and\n\nadd the following permission. Make sure to change the Resource\n\nvalue so that it is relevant to the Redshi cluster that you want the\n\nLambda instance to interact with:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogStream\", \"redshift-data:GetStatementResult\", \"redshift:GetClusterCredentials\", \"redshift-data:DescribeStatement\", \"logs:CreateLogGroup\", \"logs:PutLogEvents\", \"redshift-data:ExecuteStatement\", \"redshift-data:ListStatements\"",
      "content_length": 923,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 651,
      "content": "], \"Resource\": \"*\" } ] }\n\n2. Log in to the Lambda service in your AWS portal. Click on the Create a\n\nfunction button to create a new Lambda function:\n\nFigure 10.25 – Clicking on Create a function\n\n3. Fill in the details shown in the following screenshot. Make sure you\n\ncreate a new role or choose an existing relevant IAM role in the Change\n\ndefault execution role section. If you create a new role, make sure to note\n\nit down for the next step. As of the time of writing this book, AWS Data\n\nWrangler version 2.13.0 works with Python version 3.9, so make sure\n\nyou use Python version 3.9 when creating the function. Fill in the\n\ninformation and then click on the Create function button:",
      "content_length": 688,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 652,
      "content": "Figure 10.26 – Filling in the Runtime and Function name ﬁelds\n\n4. Aer the function is successfully created, go to the Lambda | Roles\n\nsection in IAM and attach the policy we created in step 1:",
      "content_length": 193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 653,
      "content": "Figure 10.27 – Attaching policies to your Lambda function\n\n5. Go back to the created Lambda function, scroll down to the Layers\n\nsection, and click on Add a layer:\n\nFigure 10.28 – Adding a layer to your Lambda function\n\n6. Add a layer under Specify an ARN. You should select the Amazon\n\nResource Name (ARN) that matches the runtime setting. Make sure to\n\n\u0000ll in your Lambda region in the ARN. Aer that, click on Add to add\n\nthe layer:",
      "content_length": 435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 654,
      "content": "Figure 10.29 – Verifying the ARN then clicking on Add\n\n7. Remember to change the Timeout and Memory values so that they \u0000t\n\nyour use case. In this exercise, the timeout will be 10 minutes (maximum\n\nallowed timeout):\n\nFigure 10.30 – Making sure to edit the timeout and memory",
      "content_length": 274,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 655,
      "content": "8. Paste the following script in the Code tab. Edit the SQL to \u0000t your use\n\ncase:\n\nimport logging import awswrangler as wr import json statement = \"\"\" UNLOAD (SELECT * FROM redshift. {table_name}') TO '{S3_location}' IAM_ROLE '{'arn:aws:iam:your-account- id:role/your-role' }' PARQUET PARALLEL ON PARTITION BY (product_category)' \"\"\" def lambda_handler(event, context): logger.info(json.dumps(event)) query_id = event['Input'].get('query_id') con_redshift = wr.data_api.redshift.connect( workgroup_name=\"aws-sdk-pandas\", database=\"test_redshift\", secret_arn=\"arn:aws:secretsmanager:us- east-1:your-account-id:secret:your-secret- name\", ) wr.data_api.redshift.read_sql_query( sql=statement, con=con_redshift, return { 'statusCode': 200, 'body': json.dumps(\"finished\") }",
      "content_length": 768,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 656,
      "content": "How it works…\n\nWith Lambda, you create a serverless, highly available computing\n\narchitecture that can be further elevated by integrating with\n\nEventBridge for event-driven architecture (EDA) or Simple\n\nNoti\u0000cation Service (SNS) for error noti\u0000cation. It can help to\n\ntrigger a stored procedure based on speci\u0000c events or conditions.\n\nYou need to carefully plan your trigger and outcome to \u0000t the use\n\ncase’s goal and outcome.\n\ne Data API enables you to access Redshi data without\n\nmaintaining a persistent connection. is API can execute queries\n\nfrom any application or service that invokes HTTPS requests,\n\nincluding AWS Lambda and web applications. e Data API\n\nreturns the result asynchronously, so ideally, for a complex job, you\n\nshould integrate with Step Functions to get the job status using the\n\nDescribeStatement API and get the GetStatementResult\n\nAPI to see if the execution is \u0000nished. To use in a web application, it\n\nis better to parse the response to JSON instead of a pandas\n\nDataFrame.\n\nIn a real-world application, the S3 bucket that stores the DataFrame\n\nfrom Lambda execution could be the storage for the analytical\n\nworkbench where you store relevant data for your team to explore.\n\nDue to the limitations of Lambda timeout and memory, you should",
      "content_length": 1272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 657,
      "content": "consider appropriate tools such as Glue or SageMaker notebooks if\n\nthe dataset is large.\n\nThere’s more…\n\nWhen working with AWS Lambda, you need to pay attention to the\n\nselected Python version. To take a step further to integrate into a\n\nweb application, you can add an API gateway to leverage\n\nWebSocket API and REST API capability for your application.\n\nYou need to make sure your Python version in runtime matches\n\nwith AWS SDK for pandas. If you make a mistake while choosing\n\nthe runtime version, you can go to the runtime settings to change\n\nthe version.\n\nSee also\n\nManaging Lambda dependencies with layers:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/chapter-layers.html\n\nAWS Lambda Layer: https://aws-sdk-\n\npandas.readthedocs.io/en/2.14.0/install.html#aws-lambda-layer\n\nStored procedure limitations:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/stored-procedure-\n\nconstraints.html\n\nSecurity and privileges for stored procedures:\n\nhttps://docs.aws.amazon.com/redshi/latest/dg/stored-procedure-",
      "content_length": 1005,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 658,
      "content": "security-and-privileges.html\n\nCon\u0000gure Lambda function timeout:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/con\u0000guration-\n\ntimeout.html\n\nUsing the AWS SDK for Python to manage Amazon QuickSight\n\ne AWS SDK for Python (Boto3) could be used to create,\n\ncon\u0000gure, and manage Amazon QuickSight. ere are various use\n\ncases for it, from quickly granting permission to exporting\n\nQuickSight assets from one account to another account. e\n\nfollowing script in this recipe will demonstrate how to update\n\npermissions for data sources, datasets, analyses, dashboards, and\n\nthemes in Amazon QuickSight. It grants a speci\u0000c user or role the\n\nnecessary permissions to access and manage these resources.\n\nGetting ready\n\nBefore starting, ensure you have the following prerequisites:\n\nAn active AWS account with a QuickSight subscription\n\nPython 3 and Jupyter Notebook installed on your system\n\nBoto3 and PyYAML libraries installed (pip install boto3\n\nPyYAML)",
      "content_length": 946,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 659,
      "content": "AWS credentials and pro\u0000le con\u0000gured\n\nHow to do it…\n\n1. From your AWS CLI, retrieve the Amazon QuickSight Principal ARN:\n\naws quicksight list-users --region <aws- region> --aws-account-id <account-id> -- namespace <namespace-name>\n\n2. In your Jupyter notebook, start with import boto3, set up the client,\n\nand input your Principal ARN. Replace the principal value with the\n\nvalue from step 1:\n\nimport boto3 client = boto3.client( \"quicksight\", ) principal = 'arn:aws:quicksight:<aws-region>: <account-id>:user/<namespace- name>/<quicksight-user-name>'\n\n3. Input the recipe code from the Using AWS SDK for pandas, the Redshi\n\nData API, and Lambda to execute SQL statements recipe; please check\n\nthe How it works… section.\n\nHow it works…\n\ne AWS SDK for QuickSight provides a set of APIs that you can\n\nuse to manage your QuickSight resources programmatically. ese\n\nAPIs allow you to perform various tasks, such as creating and",
      "content_length": 926,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 660,
      "content": "deleting users, managing datasets and analyses, and con\u0000guring\n\npermissions and security settings. In this recipe, we went through\n\nseveral important terms such as dataset, analysis, , and dashboard.\n\nLet’s elaborate more on these terms and how the API of these\n\ngroups works:\n\nA dataset is a collection of data used to create visualizations and\n\ndashboards. Datasets can be shared across users and groups and can be\n\nused to create multiple analyses and dashboards. With the API, you can\n\nspecify the data source and con\u0000gure data refresh schedules. You can\n\nalso retrieve a list of datasets in your QuickSight account and search for\n\ndatasets based on various criteria, such as dataset name or data source.\n\nAn analysis is a workspace where you can create and customize\n\nvisualizations and dashboards. It contains a dataset, which is the source\n\nof the data used in the visualizations, and a set of visualizations that you\n\ncan arrange and customize to suit your needs. You can create new\n\nvisualizations, add \u0000lters, remove visualizations, and apply formatting to\n\nthe data.\n\nA template is a reusable set of metadata that de\u0000nes the structure and\n\nsettings of an analysis or dashboard. It includes the dataset,\n\nvisualizations, \u0000lters, and formatting that you have de\u0000ned in the\n\nanalysis or dashboard and can be reused to create a new dashboard.\n\nA dashboard is a read-only version of your data visualization.\n\nDashboards can be scheduled to refresh at regular intervals and can be\n\ndelivered to users via email or other communication channels. ey can",
      "content_length": 1556,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 661,
      "content": "also be embedded in external websites and applications, allowing you to\n\nshare your data with a wider audience.\n\nWhen you use the AWS SDK for QuickSight, you can use the\n\nprogramming interface to manage users, datasets, analyses,\n\ntemplates, and dashboards. is solution can help you to manage\n\nthe access of your visualization in a programmatic way. It is a good\n\nway to apply DevOps practices such as version control and\n\ncontinuous integration / continuous deployment (CI/CD) to your\n\nQuickSight artifacts.\n\nThere’s more…\n\nWith the boto3 library for QuickSight, you can transfer dashboard\n\nownership and create a CI/CD pipeline to move the dashboard\n\nfrom one environment to another or from one region to another\n\nregion.\n\nBesides the previously mentioned API, two frequently useful APIs\n\ncan help you quickly create and download your QuickSight\n\ntemplate:\n\ne create_template API helps you create a template from an\n\nexisting analysis or template.\n\ne describe_template API allows you to retrieve metadata about\n\na speci\u0000c template, including its name, ARN, creation and last updated",
      "content_length": 1087,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 662,
      "content": "times, description, sheets, source entity ARN, status, theme ARN, and\n\nversion number. is API is useful for getting information about a\n\ntemplate that you have created or that has been shared with you.\n\nPlease note that not all visuals are supported over APIs, so you\n\nshould check the API Reference Index page in the See also section\n\nfor more details.\n\nSee also\n\nData Types:\n\nhttps://docs.aws.amazon.com/quicksight/latest/APIReference/API_Type\n\ns.html\n\nAPI Reference Index:\n\nhttps://docs.aws.amazon.com/quicksight/latest/APIReference/API_Refe\n\nrence.html\n\nBoto3 documentation – create_template:\n\nhttps://boto3.amazonaws.com/v1/documentation/api/1.26.85/reference/\n\nservices/quicksight/client/create_template.html\n\nBoto3 documentation – describe_template:\n\nhttps://boto3.amazonaws.com/v1/documentation/api/1.26.93/reference/\n\nservices/quicksight/client/describe_template.html\n\nOceanofPDF.com",
      "content_length": 893,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 663,
      "content": "11 Migrating to AWS – Steps, Strategies, and Best Practices for Modernizing Your Analytics and Big Data Workloads\n\nIn this chapter, we’ll embark on the journey of migrating a data\n\nwarehouse from an on-premises environment to AWS, a\n\ntransformative move that brings numerous bene\u0000ts and a few\n\nchallenges. is migration process represents a crucial step for\n\norganizations seeking to leverage the scalability, \u0000exibility, and cost-\n\neﬀectiveness of cloud computing.\n\nWe’ll explore the key considerations, strategies, and best practices\n\nfor a successful transition, including data migration methods,\n\nsecurity concerns, performance optimization, and leveraging AWS-\n\nspeci\u0000c services such as Amazon Redshi.\n\nOur focus will be on ensuring a seamless migration with minimal\n\ndisruption, while also optimizing the data warehouse to harness the\n\nfull potential of AWS’s cloud capabilities. is process not only\n\ninvolves physically transferring data but also a strategic shi in data\n\nmanagement and operations, aiming to enhance accessibility,",
      "content_length": 1041,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 664,
      "content": "analytics, and overall business intelligence (BI) in the cloud\n\nenvironment.\n\ne following recipes will be covered in this chapter:\n\nReviewing the steps and processes for migrating an on-premises\n\nplatform to AWS\n\nChoosing your AWS analytics stack – the re-platforming approach\n\nPicking the correct migration approach for your workload\n\nPlanning for prototyping and testing\n\nConverting ETL processes with big data frameworks\n\nDe\u0000ning and executing your migration process with Hadoop\n\nMigrating the existing Hadoop security authentication and\n\nauthorization processes\n\nTechnical requirements\n\nBefore initiating a data warehouse migration to AWS, it’s essential\n\nto set a robust foundation by addressing a set of technical\n\nprerequisites. Initially, a comprehensive infrastructure assessment is\n\nnecessary so that you can evaluate the existing data warehouse’s\n\nspeci\u0000cations, network setup, and data volume to ensure the AWS\n\nenvironment is provisioned appropriately. Compatibility is\n\nimportant; hence, in-depth analysis to ensure data types, schemas,\n\nand procedures align with AWS services such as Amazon Redshi",
      "content_length": 1114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 665,
      "content": "is crucial. Employing tools such as AWS Schema Conversion Tool\n\n(SCT) for assessment and conversion planning can signi\u0000cantly aid\n\nin this assessment and conversion planning process. e key\n\nconsiderations are as follows:\n\nNetwork readiness must be ensured. For this, you may potentially\n\nrequire a high-bandwidth, secure, and possibly dedicated connection\n\nsuch as AWS Direct Connect to manage the substantial data transfer\n\nprocess.\n\nSecurity considerations are non-negotiable, demanding a thorough\n\nreview and planning for encryption, compliance, and governance\n\nstandards. A detailed resource inventory, including databases, ETL jobs,\n\nand connected applications, should be documented and accompanied by\n\na performance baseline of the current system to measure post-migration\n\neﬃciency.\n\nA robust backup and recovery strategy is imperative, ensuring data\n\nintegrity during and aer the transition. Operational readiness, which\n\ninvolves setting up monitoring, logging, and maintenance mechanisms\n\nin AWS, will ensure smooth, ongoing operations.\n\nAddressing these prerequisites diligently sets the stage for a\n\nsuccessful, seamless migration where you can leverage AWS’s\n\nscalability, performance, and extensive feature set.\n\nReviewing the steps and processes for migrating an on-premises",
      "content_length": 1291,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 666,
      "content": "platform to AWS\n\nMigrating from an on-premises data warehouse to AWS is a\n\nstrategic move that can yield signi\u0000cant bene\u0000ts such as cost\n\nsavings, enhanced scalability, and improved performance. is\n\nrecipe provides a comprehensive guide to the migration process\n\nwhile covering key considerations, strategies, best practices, and\n\nactionable steps to ensure a successful transition.\n\nGetting ready\n\nMigration consists of \u0000ve stages, as shown in Figure 11.1:\n\nFigure 11.1 – The ﬁve phases of migration\n\nLet’s take a closer look at these phases in detail:\n\n1. Assessment or opportunity evaluation: Begin by taking stock of your\n\ncurrent on-premises environment. is involves cataloging your",
      "content_length": 689,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 667,
      "content": "infrastructure (servers, storage, and networking), applications, data, and\n\nthe dependencies between them. Understand your business goals and\n\nperformance requirements to identify which workloads are suitable for\n\nmigration and which strategy aligns best with your objectives.\n\n2. Planning or portfolio discovery and planning: is phase is where the\n\nblueprint for your migration takes shape. Develop a comprehensive\n\nmigration plan that outlines the chosen strategy, detailed timeline,\n\nresource allocation (both personnel and \u0000nancial), and a thorough risk\n\nassessment. A well-craed plan is your compass, guiding you through\n\nthe complexities of the migration process.\n\n3. Migration or application design and migration: is is the heart of the\n\nprocess and is where you execute your plan. It can involve various\n\nstrategies:\n\nRehosting (li-and-shi): is involves moving your\n\napplications and data to AWS with minimal changes. It’s the\n\nquickest approach but may not fully leverage cloud bene\u0000ts.\n\nReplatforming: is involves making minor modi\u0000cations to\n\nyour applications so that you can take advantage of cloud-native\n\nfeatures such as managed databases or autoscaling.\n\nRefactoring/re-architecting: is is a more comprehensive\n\napproach where you redesign your applications so that they’re\n\ncloud-native, oen while utilizing microservices and serverless\n\ntechnologies.",
      "content_length": 1378,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 668,
      "content": "4. Testing and validation: Aer migrating your workloads, rigorous testing\n\nis paramount. is involves functional testing to ensure applications\n\nbehave as expected, performance testing to validate that they meet your\n\nrequirements under load, and security testing to identify and mitigate\n\nany vulnerabilities.\n\n5. Optimization and cutover: Once testing is complete, it’s time to optimize\n\nyour AWS resources for cost eﬃciency and performance. is can\n\ninvolve right-sizing instances, using reserved instances or savings plans,\n\nand employing autoscaling to adapt to varying workloads. Finally, you’ll\n\ntransition your production environment to AWS, either through a\n\nphased approach or a complete cutover.\n\nYou can read more at https://aws.amazon.com/blogs/apn/tips-for-\n\nbecoming-an-aws-migration-consulting-and-delivery-competency-\n\npartner/.\n\nHow to do it…\n\nFollow these steps to perform a detailed migration:\n\n1. Discovery and planning (assessment phase):\n\nInfrastructure inventory: Start by documenting all your\n\ninfrastructure components, including servers, storage systems,\n\nnetwork equipment, soware licenses, and any other essential\n\nassets.\n\nApplication inventory: List all your applications, their\n\ndependencies, and how they interact with infrastructure and",
      "content_length": 1272,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 669,
      "content": "data.\n\nData assessment: Analyze your data in terms of volume, type,\n\nstructure, and sensitivity. is will help you determine the\n\noptimal migration and storage strategy.\n\nBusiness case development: Clearly de\u0000ne the objectives for the\n\nmigration, whether they’re cost reduction, scalability,\n\nperformance improvements, or faster time to market.\n\nCloud-readiness assessment: Evaluate the organization’s\n\nreadiness for cloud adoption by considering technical skills,\n\nsecurity practices, and governance protocols.\n\n2. Migration planning (planning phase):\n\nMigration strategy selection: Choose the most appropriate\n\nstrategy (rehosting, replatforming, or refactoring) based on the\n\nworkload’s requirements\n\nAWS account structure design: Set up your AWS accounts\n\naccording to your organizational needs, such as separate\n\naccounts for production, development, and testing\n\nenvironments\n\nNetwork architecture design: Plan your virtual private cloud\n\n(VPC), including subnets, security groups, and routing\n\ncon\u0000gurations\n\nResource provisioning: Estimate the AWS resources that are\n\nrequired (EC2 instances, RDS databases, S3 buckets, and so on)\n\nbased on your workload and capacity needs",
      "content_length": 1181,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 670,
      "content": "Cost estimation: Use the AWS Pricing Calculator to predict\n\ncosts and create a budget for your cloud infrastructure\n\nMigration timeline: Develop a timeline outlining key tasks,\n\ndependencies, and deadlines\n\nRisk assessment: Identify potential risks such as downtime, data\n\nloss, or security vulnerabilities, and put mitigation plans in\n\nplace\n\nCommunication plan: Create a communication strategy to keep\n\nstakeholders informed and involved during the migration\n\nprocess\n\n3. Migration execution (migration phase):\n\nAWS environment setup: Build your AWS environment by\n\nsetting up VPCs, subnets, and security groups. Establish\n\nnetwork connectivity (such as Direct Connect or VPN)\n\nbetween your on-premises infrastructure and AWS.\n\nData migration:\n\nUse AWS Database Migration Service (DMS) to move\n\ndatabases from on-premises to AWS (for example,\n\nOracle to Amazon RDS)\n\nImplement AWS DataSync for fast and secure \u0000le\n\ntransfers to Amazon S3 or EFS\n\nFor large-scale transfers, consider AWS Snowball\n\nApplication migration:",
      "content_length": 1020,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 671,
      "content": "Rehosting: Use AWS Server Migration Service (SMS)\n\nor AWS Application Migration Service (MGS) to\n\nreplicate VMs from your data center to AWS\n\nReplatforming: Shi applications to AWS-managed\n\nservices, such as Amazon RDS, ElastiCache, or\n\nAmazon MQ\n\nRefactoring: Redesign applications so that they\n\nleverage cloud-native services such as microservices,\n\ncontainers (Amazon ECS or EKS), or serverless\n\nfunctions (AWS Lambda)\n\n4. Testing and validation (testing phase):\n\nFunctional testing: Develop and run test cases to ensure that all\n\napplication features and business processes function as\n\nexpected in AWS\n\nPerformance testing: Simulate real-world workloads to verify\n\nthat your applications meet performance benchmarks in the\n\ncloud\n\nSecurity testing: Conduct vulnerability scans and penetration\n\ntests to detect and \u0000x potential security risks\n\nData validation: Con\u0000rm data accuracy and integrity between\n\nthe on-premises and AWS environments\n\n5. Optimization and cutover (\u0000nal phase):",
      "content_length": 989,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 672,
      "content": "Resource optimization: Continuously monitor AWS resource\n\nusage (CPU, memory, and storage) and adjust it to optimize\n\nperformance and cost-eﬃciency\n\nCost optimization: Regularly review your AWS billing to\n\nidentify cost-saving opportunities, such as Reserved Instances\n\nor Savings Plans\n\nPerformance tuning: Fine-tune applications and databases to\n\nenhance performance in the AWS cloud\n\nCutover: ere are two cutover approaches we can use:\n\nPhased migration: Gradually migrate applications and\n\ndata in stages, allowing for validation and testing at\n\neach phase.\n\nBig bang migration: Migrate all applications and data\n\nin one go. While faster, this method carries more risk.\n\nBy precisely following these steps and understanding the \u0000ve key\n\nphrases of migration, you can con\u0000dently and successfully\n\ntransition your on-premises platform to AWS.\n\nChoosing your AWS analytics stack – the re-platforming approach\n\nIn this recipe, we’ll focus on the re-platforming approach, which\n\ninvolves migrating to a new platform while making modi\u0000cations to",
      "content_length": 1044,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 673,
      "content": "your architecture so that you can take advantage of cloud bene\u0000ts.\n\nWe’ll examine the rich ecosystem of AWS services that can power\n\nyour modern cloud data warehouse.\n\nGetting ready\n\nBefore you initiate your migration, ensure you have the following in\n\nplace:\n\nClear business objectives: Clearly de\u0000ne your motivations for migrating\n\nto AWS. Are you seeking cost savings, scalability, improved\n\nperformance, or increased agility? ese goals will guide your decision-\n\nmaking throughout the migration process.\n\nAn AWS account: If you don’t already have one, you’ll need to create an\n\nAWS account. is will be the foundation for all your cloud resources.\n\nNetwork connectivity: Establish secure connectivity between your on-\n\npremises data center and AWS. is can be achieved through AWS\n\nDirect Connect (dedicated private network connection) or a site-to-site\n\nVPN.\n\nInventory and assessment:\n\nInfrastructure: Catalog your existing on-premises data\n\nwarehouse infrastructure (servers, storage, and network\n\ndevices)\n\nApplications: Identify and document all applications that\n\ninteract with your data warehouse",
      "content_length": 1108,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 674,
      "content": "Data: Analyze your data assets (volume, type, structure, and\n\nquality) to understand the scope of the migration\n\nDependencies: Map the relationships between applications,\n\ndata, and infrastructure to identify potential migration\n\nchallenges\n\nPerformance requirements: Determine your desired\n\nperformance metrics for the cloud data warehouse\n\nSkills and resources: Ensure you have the necessary technical expertise\n\nin-house or have access to external consultants with experience in AWS\n\ndata warehousing technologies.\n\nWhy re-platform on AWS?\n\nRe-platforming on AWS oﬀers a compelling middle ground\n\nbetween the speed of rehosting and the full transformation of\n\nrefactoring. Here’s why it’s a popular choice:\n\nReduced operational overhead: AWS takes care of infrastructure\n\nmanagement, allowing you to focus on your data and analytics\n\nScalability and elasticity: You can dynamically scale your resources up or\n\ndown to match demand, paying only for what you use\n\nManaged services: You can leverage AWS’s fully managed services, such\n\nas Amazon Redshi, to oﬄoad maintenance tasks and accelerate\n\ninnovation\n\nPerformance and cost optimization: You can choose from a variety of\n\ncompute and storage options to \u0000ne-tune performance and cost-",
      "content_length": 1240,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 675,
      "content": "eﬀectiveness\n\nA rich ecosystem of services: You can integrate with a wide range of\n\nAWS services for data ingestion, transformation, analytics, and\n\nvisualization purposes\n\nThe AWS analytics ecosystem\n\nAWS oﬀers a comprehensive suite of services tailored for building a\n\nmodern data warehouse:\n\nAmazon Redshi: A fast, fully managed, petabyte-scale data warehouse\n\noptimized for analytics. It oﬀers excellent performance for complex\n\nqueries and supports a wide range of data types and SQL functions.\n\nAmazon S3: A scalable, high-durability object storage service for storing\n\nvast amounts of structured and unstructured data. It’s a cost-eﬀective\n\noption for data lakes and can be queried directly by Redshi using\n\nRedshi Spectrum.\n\nAWS Glue: A serverless data integration service that simpli\u0000es\n\ndiscovering, preparing, and combining data for analytics and machine\n\nlearning. Glue oﬀers crawlers for schema discovery, extract, transform,\n\nand load (ETL) capabilities, as well as a data catalog.\n\nAmazon Athena: A serverless interactive query service that allows you to\n\nanalyze data in S3 using standard SQL. Athena is a great option for ad\n\nhoc queries and exploration.\n\nAWS Lake Formation: is is a fully managed service that simpli\u0000es the\n\nprocess of creating, securing, and managing data lakes. It automates data",
      "content_length": 1320,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 676,
      "content": "ingestion, cataloging, and access control, allowing you to organize,\n\nsecure, and analyze large-scale data eﬃciently in a centralized repository.\n\nBy integrating with other AWS services, Lake Formation enables\n\nseamless data governance and analytics in a scalable, cost-eﬀective\n\nmanner.\n\nAWS DynamoDB: is is a serverless, NoSQL database service that\n\nenables you to develop modern applications at any scale.\n\nAWS Lambda: AWS Lambda is a serverless computing service that lets\n\nyou run code without the need to provision or manage servers. You pay\n\nonly for the compute time you consume. is eliminates the need for\n\nupfront infrastructure costs and allows you to focus on building\n\napplications.\n\nAWS Managed Work\u0000ows for Apache Air\u0000ow (MWAA): is is a fully\n\nmanaged service that simpli\u0000es the deployment and operation of Apache\n\nAir\u0000ow, an open source work\u0000ow orchestration platform. It allows you\n\nto build, schedule, and monitor data pipelines and work\u0000ows without\n\nthe need to manage the underlying infrastructure.\n\nAmazon Kinesis: is is a fully managed service that processes and\n\nanalyzes real-time streaming data at scale. It can capture and process\n\nmillions of events per second from various sources, making it ideal for\n\napplications such as clickstream analysis, IoT data processing, and real-\n\ntime analytics.\n\nAmazon EMR: is service provides the cloud big data platform for\n\nprocessing vast amounts of data using open source tools.",
      "content_length": 1449,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 677,
      "content": "Amazon QuickSight: A scalable, serverless BI service for creating\n\nvisualizations, dashboards, and reports from your data warehouse data.\n\nIf you’re aiming to re-platform your analytics stack and wish to\n\nleverage the AWS analytics ecosystem, here’s a guide on choosing\n\nthe right tools and services:\n\n1. Data ingestion and integration:\n\nAWS Glue: is is a serverless data integration service that\n\nsimpli\u0000es the process of discovering, preparing, and combining\n\ndata for analytics. It automates many of the tasks involved in\n\nETL, allowing you to focus on building data-driven\n\napplications.\n\nAWS Data Migration Service (DMS): is service helps migrate\n\non-premises databases to AWS easily and securely.\n\nAmazon Kinesis or MSK: For real-time streaming data.\n\n2. Data storage:\n\nAmazon S3: Scalable storage for data lakes and analytics. S3’s\n\nintegrations and data protection features make it ideal for a data\n\nlake.\n\nAmazon Redshi: A petabyte-scale data warehouse service. For\n\ncomplex analytics workloads, you can analyze all your data\n\nusing your existing BI tools.\n\nAmazon RDS and Aurora: For relational data storage needs.",
      "content_length": 1127,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 678,
      "content": "3. Data processing and analysis:\n\nAWS Glue: With this service, you can visually clean and\n\nnormalize data\n\nAmazon Elastic MapReduce (EMR): You can process large\n\namounts of data with popular distributed frameworks such as\n\nApache Spark and Hadoop\n\nAWS Lambda: You can run code in response to events without\n\nprovisioning servers, which can be useful for lightweight data\n\nprocessing tasks\n\n4. Data querying:\n\nAmazon Athena: A serverless interactive query service that\n\nallows you to analyze data in Amazon S3 using SQL\n\nAmazon Redshi Spectrum: With this service, you can examine\n\nhuge datasets in S3 without having to load or transform them\n\n5. Machine learning and advanced analytics:\n\nAmazon SageMaker: Build, train, and deploy machine learning\n\nmodels at scale\n\nAmazon Comprehend and Recognition: For natural language\n\nprocessing (NLP) and video/image analysis, respectively\n\n6. Data visualization and BI:",
      "content_length": 909,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 679,
      "content": "Amazon QuickSight: A BI service with native machine learning\n\nintegrations\n\nIntegration with third-party tools: AWS supports integration\n\nwith popular BI tools such as Tableau, Looker, and more\n\n7. Data security:\n\nAmazon Macie: Discover, classify, and protect sensitive data\n\nAWS Key Management Service (KMS): Create and manage\n\ncryptographic keys\n\nAWS Identity and Access Management (IAM): Manage user\n\npermissions and access\n\n8. Data governance and cataloging:\n\nAWS Glue Catalog: A central metadata repository integrated\n\nwith a wide range of AWS services\n\nLake Formation: Simpli\u0000es and automates many of the complex\n\ntasks associated with setting up, securing, and managing data\n\nlakes\n\n9. Monitoring and Management:\n\nAmazon CloudWatch: Monitor resources and applications\n\nAWS CloudTrail: Provides governance, compliance, operational\n\nauditing, and risk auditing\n\n10. Optimization and cost management:",
      "content_length": 904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 680,
      "content": "AWS Cost Explorer: View and analyze your costs and usage\n\nAWS Trusted Advisor: Oﬀers real-time guidance to provision\n\nresources while following AWS best practices\n\nHow to do it…\n\nWhen re-platforming, consider these steps:\n\n1. Assess: Understand your current analytics stack and identify areas of\n\nimprovement.\n\n2. Select AWS services: Based on your needs, pick the right services from\n\nthe AWS analytics ecosystem.\n\n3. Data migration: Move your data to AWS, either through batch transfers\n\nor real-time streams.\n\n4. Re-architect and develop: Modify or redesign your analytics work\u0000ows\n\nand processes so that they \u0000t within the AWS ecosystem.\n\n5. Testing: Ensure that the new system works correctly and meets\n\nperformance standards.\n\n6. Deployment and monitoring: Roll out the new system and continuously\n\nmonitor its performance, optimizing as necessary.\n\nAWS analytics services architecture\n\nHarnessing the power of AWS’s comprehensive data analytics\n\nservices, the following diagram outlines a scalable and \u0000exible",
      "content_length": 1016,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 681,
      "content": "solution for processing, storing, and analyzing massive datasets\n\neﬃciently.\n\ne following mind map diagram shows a use case map for\n\nanalytics services:\n\nFigure 11.2 – Analytics service mind map\n\nA modern data analytics architecture on AWS involves integrating\n\nvarious AWS services so that you can ingest, store, process, analyze,\n\nand visualize data in a scalable, secure, and cost-eﬀective manner.\n\nHere’s a breakdown of the components that are typically involved in\n\nsuch a reference architecture:",
      "content_length": 502,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 682,
      "content": "1. Data ingestion: Data comes from multiple sources, such as applications,\n\ndatabases, logs, and third-party systems. AWS provides several services\n\nfor handling diﬀerent types of data ingestion:\n\nAmazon Kinesis or AWS Managed Streaming for Apache Kaa\n\n(MSK): For real-time streaming data such as IoT sensor data or\n\nlog \u0000les\n\nAWS DMS: For migrating or replicating databases to AWS\n\nAWS IoT Core: For ingesting data from IoT devices\n\nAmazon AppFlow: For integrating data from SaaS applications\n\nsuch as Salesforce, ServiceNow, or Google Analytics\n\nAWS DataSync: For moving large amounts of unstructured\n\ndata, such as \u0000les and logs\n\nAWS Snowball: For transferring petabyte-scale data from on-\n\npremises environments\n\n2. Data lake storage: Aer data ingestion, the raw and processed data is\n\nstored in a scalable, durable, and secure data lake.\n\nAmazon S3: Serves as the primary data lake storage platform. It\n\nprovides cost-eﬀective, scalable storage with high durability.\n\nAWS Lake Formation: Helps in building, managing, and\n\nsecuring the data lake on S3.\n\n3. Data cataloging and governance: Metadata management, data\n\ncataloging, and data governance are crucial for managing a data lake at",
      "content_length": 1193,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 683,
      "content": "scale:\n\nAWS Glue: For automatic schema discovery, data cataloging,\n\nand ETL work\u0000ows\n\nAWS Glue DataBrew: For data preparation and cleaning\n\nwithout writing code\n\nAWS Lake Formation: For uni\u0000ed governance, which helps\n\nmanage security, access controls, and audit logs centrally\n\n4. Data processing and transformation: Once the data has been ingested\n\nand cataloged, various processing methods can be applied, depending on\n\nthe data type and latency requirements:\n\nAWS Glue: For running ETL jobs that transform and load data\n\ninto a format suitable for analysis\n\nAmazon EMR: For big data processing using Apache Spark,\n\nHive, and other open source frameworks\n\nAmazon Redshi Spectrum: Enables querying directly from the\n\ndata lake using SQL\n\nAWS Lambda: For serverless data processing, especially for\n\nreal-time or event-driven processing\n\n5. Data warehousing: For structured, high-performance, and scalable\n\nquerying, data is oen transformed into a data warehouse optimized for\n\nanalytics:\n\nAmazon Redshi: AWS’s fully managed data warehouse service,\n\noptimized for querying large datasets and integrating with the",
      "content_length": 1114,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 684,
      "content": "data lake using Redshi Spectrum\n\nAmazon RDS/Aurora: For structured data storage with\n\nrelational databases such as MySQL, PostgreSQL, and Aurora\n\n6. Data analytics: With data ingested, processed, and stored, analytics can\n\nbe performed using various tools to derive insights and perform\n\nmachine learning:\n\nAmazon Athena: An interactive query service for analyzing\n\ndata in S3 using standard SQL\n\nAmazon OpenSearch: For full-text search, log analytics, and\n\nreal-time monitoring\n\nAWS Kinesis Data Analytics: For real-time analytics on\n\nstreaming data\n\nAWS QuickSight: AWS’s BI tool for generating dashboards and\n\nreports\n\n7. Machine learning and AI: Data science and machine learning work\u0000ows\n\nare essential for predictive analytics, recommendation systems, and\n\nanomaly detection:\n\nAmazon SageMaker: A fully managed machine learning service\n\nfor building, training, and deploying machine learning models\n\nAWS AI services: Pre-built AI services such as Amazon\n\nRekognition (image recognition), Amazon Comprehend\n\n(natural language processing), and Amazon Forecast (time\n\nseries forecasting)",
      "content_length": 1091,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 685,
      "content": "8. Security, governance, and monitoring: Security and governance play a\n\ncritical role in modern analytics architecture, ensuring that data is\n\nprotected and compliant with industry standards:\n\nAWS IAM: For controlling access to services and resources\n\nAWS KMS: For encrypting data at rest and in transit\n\nAWS CloudTrail: For monitoring and logging all activities on\n\nAWS accounts for security audits\n\nAmazon CloudWatch: For monitoring the performance of\n\napplications, services, and infrastructure\n\nAWS Con\u0000g: For monitoring compliance and security posture\n\n9. Data visualization and BI: Visualizing insights is key for decision-\n\nmaking processes, and AWS oﬀers a variety of services to accomplish\n\nthis:\n\nAmazon QuickSight: AWS’s scalable BI tool that oﬀers machine\n\nlearning and AI-driven insights, visualizations, and dashboards\n\nird-party tools: Integration with popular BI tools such as\n\nTableau, Power BI, or Looker\n\n10. Cost management: With all services in place, managing costs is critical\n\nto ensure the analytics platform remains eﬃcient:\n\nAWS Cost Explorer: For visualizing and understanding costs\n\nAWS Trusted Advisor: Provides real-time guidance to help\n\noptimize the AWS infrastructure in terms of cost, performance,",
      "content_length": 1234,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 686,
      "content": "security, and fault tolerance\n\nChoosing your architecture\n\ne right architecture for your AWS data warehouse will depend on\n\nyour speci\u0000c needs and use cases. Here are a few popular\n\narchitectural patterns:\n\nFigure 11.3 – AWS data warehouse beneﬁts",
      "content_length": 248,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 687,
      "content": "is Redshi-centric architecture leverages Amazon Redshi’s\n\nmassively parallel processing (MPP) capabilities to process and\n\nanalyze large datasets eﬃciently while supporting advanced BI and\n\nreporting needs.\n\nLet’s explore the diverse range of AWS architectures that are\n\navailable to support your unique cloud computing needs:\n\nRedshi-centric architecture: is is ideal for high-performance\n\nanalytics workloads. All your data is loaded into Redshi, and you use\n\nits built-in capabilities for analysis.\n\nIn a Redshi-centric architecture, data from various operational\n\nsystems, marketing data sources, and other systems is initially\n\nmoved to Amazon S3. AWS Glue is then used to perform ETL\n\nprocesses, transforming and cleansing the data before it’s loaded\n\ninto Amazon Redshi. Redshi acts as the central data\n\nwarehouse, serving as a producer that consolidates and stores\n\nthis processed data. e consolidated data is then made\n\naccessible to multiple consumers, including Consumer 1,\n\nConsumer 2, and Consumer 3, each leveraging the data for\n\nanalytics, reporting, and BI purposes. is setup ensures a\n\nstreamlined, scalable, and eﬃcient data pipeline that supports\n\ndiverse analytical needs across the organization:",
      "content_length": 1227,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 688,
      "content": "Figure 11.4 – AWS Redshift-centric architecture diagram\n\nData lakehouse architecture: Combines the best of data lakes and data\n\nwarehouses. Raw data is stored in S3, and Glue is used for ETL and\n\ntransformation. You can use Redshi, Athena, or both to query the data.\n\ne initial layer, known as the landing layer, serves as the entry\n\npoint for all source \u0000les in their native formats. Subsequently,\n\nthese \u0000les undergo conversion and are stored in a standardized\n\nParquet format within the raw layer. e stage layer maintains\n\nhistorical records of dimensional tables using Slowly Changing\n\nDimension Type 2 (SCD2), which is facilitated by Apache Hudi\n\nin Amazon S3 and AWS Glue jobs. AWS Glue orchestrates ETL\n\ntasks across layers, ensuring seamless data movement, cleansing,\n\nvalidation, and transformation. In the presentation layer, data is\n\nmeticulously re\u0000ned so that it aligns with business requirements",
      "content_length": 912,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 689,
      "content": "through AWS Glue jobs. Finally, the data warehouse layer,\n\npowered by Amazon Redshi, houses the curated and cleansed\n\ndata, accessible either through direct copying via AWS Glue or\n\nby creating Spectrum tables linked to the S3 location.\n\nServerless analytics architecture: Leverages Athena and S3 for a fully\n\nserverless data lake. Glue can be used for ETL if needed. is is a cost-\n\neﬀective option for ad hoc or intermittent workloads.\n\nAWS data mesh architecture: is is a decentralized approach to data\n\nmanagement that empowers business domains to own and manage their\n\ndata. It contrasts with traditional data management, where data is\n\ncentralized and managed by a dedicated team. In data mesh, each\n\ndomain is responsible for the entire life cycle of its data, from ingestion\n\nto consumption. is includes ingesting data from various sources,\n\nprocessing it to make it usable, and serving it to consumers.\n\nData mesh is similar to microservices in that it breaks down data\n\nmanagement into smaller, independent units. is makes it\n\nmore scalable and \u0000exible than traditional data management\n\napproaches. It also makes it easier for business domains to\n\ninnovate and experiment with new data use cases.\n\nAWS Lake Formation and AWS Glue are two services that can\n\nbe used to implement a data mesh architecture on AWS. Lake\n\nFormation is a service that makes it easy to create and manage\n\naccess and permissions for the data lake. AWS Glue is a",
      "content_length": 1450,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 690,
      "content": "serverless ETL service that can be used to extract, transform,\n\nand load data into a data lake.\n\nData mesh is a relatively new concept, but it’s gaining traction as\n\norganizations look for ways to improve their data management\n\npractices. AWS Lake Formation and AWS Glue are two services\n\nthat can be used to implement a data mesh architecture on AWS.\n\nHere are some key considerations when you’re choosing your\n\narchitecture:\n\nWorkload types: Are your workloads primarily batch processing,\n\ninteractive querying, or a mix of both?\n\nData volume: How much data do you need to store and process?\n\nData variety: Do you need to handle structured, semi-structured, or\n\nunstructured data?\n\nPerformance requirements: What are your latency requirements for\n\nqueries and reporting?\n\nCost optimization: What is your budget, and how can you balance cost\n\nwith performance?\n\ne key considerations for data migration services and tools are as\n\nfollows:\n\nSchema conversion: Use the AWS SCT to convert your on-premises data\n\nwarehouse schema into a format compatible with Redshi.",
      "content_length": 1065,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 691,
      "content": "Data migration: Use AWS DMS, S3 Transfer Acceleration, or other tools\n\nto move your data to S3.\n\nETL with Glue: Develop ETL jobs in Glue to transform and load the data\n\ninto Redshi (or prepare it for querying with Athena).\n\nOptimize and validate: Fine-tune your Redshi cluster (or Athena\n\nqueries) for performance. Validate data integrity and functionality.\n\nBy carefully considering your requirements and choosing the right\n\narchitecture, you can use the full power of the AWS analytics\n\necosystem to build a scalable, \u0000exible, and cost-eﬀective data\n\nwarehouse that meets your business needs.\n\nSee also\n\nModern Data Analytics Reference Architecture on AWS Diagram:\n\nhttps://docs.aws.amazon.com/architecture-diagrams/latest/modern-\n\ndata-analytics-on-aws/modern-data-analytics-on-aws.html\n\nUse a reusable ETL framework in your AWS lake house architecture:\n\nhttps://aws.amazon.com/blogs/architecture/use-a-reusable-etl-\n\nframework-in-your-aws-lake-house-architecture/\n\nDesign a data mesh architecture using AWS Lake Formation and AWS\n\nGlue: https://aws.amazon.com/blogs/big-data/design-a-data-mesh-\n\narchitecture-using-aws-lake-formation-and-aws-glue/\n\nServerless data lake centric analytics architecture:\n\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-serverless-data-",
      "content_length": 1276,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 692,
      "content": "analytics-pipeline/serverless-data-lake-centric-analytics-\n\narchitecture.html\n\nPicking the correct migration approach for your workload\n\nChoosing the right migration approach depends on various factors,\n\nincluding the type of workload, existing infrastructure,\n\nperformance needs, future scalability requirements, and budget\n\nconstraints. e choice becomes even more nuanced when you’re\n\nconsidering speci\u0000c workloads, such as data warehouses or Hadoop\n\nclusters. is recipe will guide you through the process of picking\n\nthe correct migration approach for diﬀerent workloads.\n\nIn this recipe, we’ll explore several migration approaches and\n\ndiscuss the factors to consider when you’re choosing the best\n\napproach for your workload. We’ll also provide a step-by-step guide\n\nto planning and executing a migration.\n\nEach migration strategy comes with its pros and cons. e choice\n\ndepends on the current state of your systems, the nature of the\n\nworkload, the desired bene\u0000ts, and any other constraints. A\n\nthorough assessment of the existing environment, coupled with a\n\nclear understanding of business and technical objectives, will guide\n\nthe decision-making process.",
      "content_length": 1168,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 693,
      "content": "Getting ready\n\nSelecting the optimal migration strategy is crucial. Here are some\n\ncommon approaches and when to consider them.\n\nData warehouse migration to AWS oﬀers several approaches,\n\nallowing you to choose the one that best \u0000ts your needs:\n\nLi and shi: Li and shi, also known as rehosting, is a migration\n\nstrategy that involves moving workloads from on-premises\n\ninfrastructure to the cloud without making any signi\u0000cant changes to\n\nthe application code or architecture. is approach is oen used for\n\napplications that are running well on-premises but that could bene\u0000t\n\nfrom the scalability, elasticity, and cost-eﬀectiveness of the cloud:\n\nWhen to use: Your current on-premises data warehouse meets\n\nperformance and scalability requirements, but you desire the\n\nbene\u0000ts of cloud infrastructure, such as operational eﬃciency,\n\nelasticity, and cost savings\n\nAdvantages: Faster migration, lower initial eﬀort, and minimal\n\nchanges to existing systems\n\nExample: Migrating an on-premises Teradata or Oracle data\n\nwarehouse to Amazon Redshi without major transformations\n\nLi and rewrite/re-architect: Li and rewrite/re-architect is a cloud\n\nmigration strategy that involves rewriting or re-architecting applications\n\nto take full advantage of the cloud’s capabilities. is approach is oen",
      "content_length": 1299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 694,
      "content": "used for applications that aren’t running eﬃciently on-premises or that\n\nneed to be modernized to meet new business requirements:\n\nWhen to use: Your current data warehouse has inherent\n\nlimitations, or you wish to leverage cloud-native features,\n\noptimized performance, and scalability\n\nAdvantages: Fully utilizing cloud capabilities, improved\n\nperformance, and potentially reduced operational costs in the\n\nlong run\n\nExample: Transforming an on-premises data warehouse into a\n\nserverless architecture using Amazon Athena and S3\n\nHybrid: A hybrid cloud environment combines on-premises\n\ninfrastructure with a public cloud, such as AWS. is approach allows\n\norganizations to take advantage of the bene\u0000ts of both environments,\n\nsuch as the scalability and elasticity of the cloud, while still maintaining\n\ncontrol over their sensitive data on-premises:\n\nWhen to use: You need to maintain some components on-\n\npremises due to data sovereignty, latency, or other business\n\nrequirements, but you also want to leverage the cloud for\n\nscalability and \u0000exibility\n\nAdvantages: You can balance on-premises control and cloud\n\nscalability\n\nExample: Keeping a subset of sensitive data on-premises while\n\nbursting compute-intensive analytics tasks to Amazon Redshi",
      "content_length": 1252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 695,
      "content": "When choosing a migration approach, it’s important to consider the\n\nfollowing factors:\n\ne size and complexity of the workload: e size and complexity of the\n\nworkload will determine the time and eﬀort required to migrate it to the\n\ncloud. Larger and more complex workloads will typically require more\n\ntime and eﬀort to migrate than smaller and simpler workloads.\n\ne skills and experience of the team: e skills and experience of the\n\nteam will determine how much work they can do on their own and how\n\nmuch help they will need from external consultants. A team with strong\n\ncloud skills and experience will be able to migrate workloads more\n\nquickly and eﬃciently than a team with limited cloud skills and\n\nexperience.\n\ne desired timeframe for migration: e desired timeframe for\n\nmigration will determine the urgency with which the migration needs to\n\nbe completed. If the migration needs to be completed quickly, then a li-\n\nand-shi approach may be the best option. If the migration can be\n\ncompleted more slowly, then a refactoring or re-platforming approach\n\nmay be a better option.\n\nBudget: e budget for the migration will determine the resources that\n\nare available to complete the migration. A larger budget will allow for\n\nmore resources to be allocated to the migration, which may allow for a\n\nmore complex migration approach to be used.\n\nRisk tolerance: e risk tolerance of the organization will determine how\n\nmuch risk is acceptable when migrating workloads to the cloud. A risk-\n\naverse organization may prefer a li-and-shi approach, while a more",
      "content_length": 1569,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 696,
      "content": "risk-tolerant organization may be willing to consider a more complex\n\nmigration approach.\n\nHow to do it…\n\nOnce you’ve chosen a migration approach, you can start planning\n\nand executing the migration. e following is a step-by-step guide\n\nto planning and executing a migration:\n\n1. De\u0000ne the scope of the migration: e \u0000rst step is to de\u0000ne the scope of\n\nthe migration, which includes identifying the workloads that will be\n\nmigrated, the target cloud environment, and the timeframe for the\n\nmigration.\n\n2. Assess the readiness of the organization for migration: e next step is to\n\nassess the readiness of the organization for migration. is includes\n\nassessing the skills and experience of the team, the availability of\n\nresources, and the organizational culture.\n\n3. Design the migration plan: e design of the migration plan will include\n\ndetailed speci\u0000cations for the migration, such as the timeline for each\n\nstep, the tools and technologies that will be used, and the procedures for\n\ntesting and cutover.\n\n4. Execute the migration: e execution of the migration will involve\n\ncarrying out the steps in the migration plan. is includes migrating the\n\nworkloads to the cloud, testing the migrated workloads, and cutting over\n\nto the cloud environment.",
      "content_length": 1256,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 697,
      "content": "5. Monitor and optimize the migration: Once the migration is complete, it’s\n\nimportant to monitor the migrated workloads to ensure that they’re\n\nperforming as expected.\n\nPlanning for prototyping and testing\n\nWhen planning for a migration, especially a signi\u0000cant migration,\n\nsuch as moving to the cloud, prototyping and testing are critical\n\nphases. ese steps allow organizations to validate assumptions,\n\nidentify potential issues, and ensure smooth operations when the\n\nmigration is executed at scale.\n\nGetting ready\n\nPrototyping and testing are crucial phases in the on-premises to\n\nAWS cloud migration process. ese phases ensure that the\n\nmigration is well-planned, executed seamlessly, and meets the\n\norganization’s objectives.\n\nPrototyping involves creating a small-scale replica of the on-\n\npremises environment in the AWS cloud. is allows for thorough\n\ntesting and validation of the migration plan before it’s applied to the\n\nentire production environment. Here are the bene\u0000ts of\n\nprototyping:\n\nIdentifying and resolving potential issues early in the migration process",
      "content_length": 1080,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 698,
      "content": "Validating the migration plan and ensuring it aligns with the\n\norganization’s requirements\n\nGaining hands-on experience with AWS services and tools\n\nTesting is an ongoing process that encompasses both functional and\n\nnon-functional testing. Functional testing veri\u0000es that the migrated\n\napplications are working as expected, while non-functional testing\n\nassesses aspects such as performance, security, and scalability. Here\n\nare some of the bene\u0000ts of testing:\n\nYou can ensure that the migrated applications are fully compatible with\n\nthe AWS environment\n\nYou can identify and address performance bottlenecks or security\n\nvulnerabilities\n\nYou can optimize the applications for maximum eﬃciency and\n\nscalability in the cloud\n\nBy incorporating prototyping and testing into the on-premises to\n\nAWS cloud migration process, organizations can minimize risks,\n\nenhance the migration’s success rate, and achieve a smooth\n\ntransition to the cloud.\n\nHow to do it…\n\nPlanning for prototyping and testing from on-premises to AWS\n\ninvolves several key steps. Here’s how to approach it:",
      "content_length": 1073,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 699,
      "content": "1. De\u0000ne the objective:\n\nUnderstand the primary goals of the migration: Performance\n\nenhancement, cost-saving, scalability, and so on\n\nClearly state what you hope to achieve with the prototype:\n\nValidate migration tools, test performance, ensure data\n\nintegrity, and so on\n\n2. Pick a prototype scope:\n\nRepresentative subset: Choose a subset of your systems/data\n\nthat’s a good representative of the whole. is should include\n\nvarious data types, applications, and workloads.\n\nComplexity: Include both simple and complex components to\n\nensure a comprehensive test.\n\nSize: e dataset should be large enough to simulate real-world\n\nscenarios but not so vast that it becomes cumbersome.\n\n3. Develop a prototype plan:\n\nSetup: De\u0000ne the tools (for example, AWS SCT and DMS) and\n\nresources needed\n\nExecution: Outline the step-by-step process of migration for the\n\nprototype\n\nValidation: Describe how you’ll validate the success of the\n\nmigration prototype",
      "content_length": 948,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 700,
      "content": "4. Execute the prototype migration:\n\nMigrate data: Use the chosen tools to migrate the data\n\nCon\u0000gure environment: Ensure the target environment has\n\nbeen set up similarly to the planned \u0000nal setup\n\nDocument: Keep detailed notes of any challenges or issues that\n\nyou faced\n\n5. Test the prototype:\n\nFunctionality tests: Ensure all applications and services are\n\nworking as expected post-migration\n\nPerformance tests: Compare the performance of the prototype\n\nsystem to the original and your performance goals\n\nSecurity and compliance tests: Ensure that data remains secure\n\nand that all compliance requirements are met post-migration\n\nFailover and recovery tests: Check the reliability and resilience\n\nof the new setup\n\n6. Gather feedback: Involve end users and stakeholders. eir feedback on\n\napplication performance, accessibility, and any potential issues is\n\ninvaluable.\n\n7. Re\u0000ne the migration process: Based on feedback and test results, re\u0000ne\n\nyour migration strategy. Address any issues or challenges that were\n\nnoted during the prototyping phase.\n\n8. Plan for mass migration:",
      "content_length": 1083,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 701,
      "content": "Iterate: If major issues are found during the prototype phase,\n\nconsider running additional prototypes or tests before\n\ncommitting to a mass migration.\n\nScale up: Once con\u0000dent in the prototype’s success, plan out the\n\nmigration for the entire system. Use the prototype as a\n\nblueprint.\n\nBack up: Ensure you have backups of everything before the\n\nmass migration.\n\nHere are a few best practices:\n\nIterative approach: It’s okay to run multiple prototypes if necessary. It’s\n\nbetter to identify issues now than during the mass migration.\n\nStakeholder communication: Keep all stakeholders in the loop. eir\n\ninsights can be invaluable and ensure everyone is aligned on\n\nexpectations.\n\nDocumentation: Maintain detailed documentation throughout the\n\nprototyping and testing phase. is will be invaluable during the mass\n\nmigration.\n\nSafety \u0000rst: Always ensure data integrity and security. Any signs of data\n\nloss or potential breaches should be addressed immediately.\n\nHere are a few key takeaways:\n\nPrototyping is crucial: It oﬀers a glimpse into the potential challenges\n\nand successes of a mass migration",
      "content_length": 1101,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 702,
      "content": "Detailed planning: Detailed and careful planning for the prototype phase\n\nincreases the chances of a successful mass migration\n\nFeedback and iteration: Using feedback and iterating on the migration\n\nstrategy ensures fewer issues during the actual migration\n\nBy following this guide, you’ll be well-prepared to tackle the\n\nchallenges of migration with a well-tested prototype, minimizing\n\nrisks and ensuring a smoother transition.\n\nConverting ETL processes with big data frameworks\n\nAs the volume and complexity of data continue to grow, traditional\n\nETL processes are struggling to keep pace. Big data frameworks,\n\nsuch as Apache Hadoop, Apache Spark, and AWS, oﬀer a powerful\n\nsolution for migrating and managing big data workloads, enabling\n\norganizations to process, analyze, and extract valuable insights\n\neﬀectively from their vast data repositories.\n\nGetting ready\n\nLet’s discover how AWS can help you overcome the limitations of\n\ntraditional ETL processes and unlock new possibilities for data\n\nanalysis:",
      "content_length": 1011,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 703,
      "content": "Challenges of traditional ETL in big data: Traditional ETL processes face\n\nseveral limitations in handling the massive scale and complexity of big\n\ndata:\n\nScalability: Traditional ETL tools aren’t designed to handle the\n\nmassive scale of big data, leading to performance bottlenecks\n\nand slow processing times\n\nFlexibility: Traditional ETL processes are oen rigid and\n\nin\u0000exible, making it diﬃcult to adapt to the ever-changing\n\nnature of big data\n\nReal-time processing: Traditional ETL processes are batch-\n\noriented, making it challenging to handle real-time data\n\nstreams and perform real-time analytics\n\nIntroducing big data frameworks for ETL migration: Big data\n\nframeworks such as Apache Hadoop, Apache Spark, and AWS oﬀer\n\nseveral advantages for migrating ETL processes:\n\nScalability: Big data frameworks are designed to handle massive\n\ndatasets horizontally by distributing data across multiple nodes,\n\nenabling scalability and parallel processing\n\nFlexibility: Big data frameworks provide \u0000exible data processing\n\ncapabilities, allowing for data ingestion, transformation, and\n\nanalysis in a variety of formats and structures\n\nReal-time processing: Big data frameworks enable real-time\n\ndata processing and analytics, making it possible to respond to\n\ndata changes and events in real time",
      "content_length": 1299,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 704,
      "content": "Migrating ETL processes to big data frameworks on AWS: Converting\n\nETL processes into a big data framework oen involves transitioning\n\nfrom traditional, monolithic ETL tools to scalable, distributed\n\nprocessing systems, such as those provided by big data platforms. e\n\ngoal is to handle larger volumes of data more eﬃciently, ensure\n\nscalability, and reduce processing times.\n\nHow to do it…\n\nWhen migrating ETL processes to a big data framework, you’d\n\ntypically follow these steps:\n\n1. Assessment and planning:\n\nIdentify current ETL processes: Understand current data\n\nsources, transformations, and load processes.\n\nDetermine volume and velocity: Estimate data volume and the\n\nrate of incoming data to choose the right big data tools.\n\nSet goals: What do you want to achieve with the migration? It\n\ncould be faster processing, handling larger datasets, cost\n\nsavings, and so on.\n\n2. Choose a big data framework:\n\nApache Hadoop: An open source framework for distributed\n\nstorage and processing. Its ecosystem (for example, Hive for\n\nSQL-like operations and Pig for scripting) can aid in ETL\n\nprocesses.",
      "content_length": 1104,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 705,
      "content": "Apache Spark: It oﬀers fast, in-memory data processing and is\n\nparticularly suited for ETL tasks. It supports various languages,\n\nincluding Python, Scala, and Java.\n\n3. Redesign ETL for scalability:\n\nParallel processing: Instead of sequential processing, design\n\nETL jobs so that they run in parallel, distributing the workload\n\nacross clusters\n\nOptimize transformations: Some transformations might be\n\noptimized or restructured to suit the distributed nature of big\n\ndata frameworks\n\nIncremental loads: Rather than full loads, consider incremental\n\napproaches, processing only new or changed data\n\n4. Data ingestion tools: Use tools such as Apache Kaa for real-time data\n\nstreaming or Apache Flume and Sqoop for batch data ingestion from\n\nvarious sources.\n\n5. Data transformation:\n\nLeverage big data tools: Use Spark’s DataFrame or Dataset API\n\nfor transformations. If you’re using Hadoop, tools such as Hive\n\nand Pig can help.\n\nUser-de\u0000ned functions (UDFs): For complex transformations,\n\nyou can write UDFs tailored to your needs.\n\n6. Data loading:",
      "content_length": 1051,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 706,
      "content": "Storage options: Depending on the nature and usage of the data,\n\nchoose storage options such as Hadoop Distributed File System\n\n(HDFS), cloud storage (for example, Amazon S3), or NoSQL\n\ndatabases\n\nBatch versus real time: Depending on your needs, you can load\n\ndata in batches (using tools such as Sqoop) or in real time\n\n(using Kaa or Spark Streaming)\n\n7. Optimization and performance tuning:\n\nTune cluster con\u0000guration: Adjust con\u0000gurations such as\n\nmemory allocation, number of executors, and so on for optimal\n\nperformance\n\nPartitioning and bucketing: Partitioning divides data into\n\nsubsets, whereas bucketing divides data based on column\n\nvalues, optimizing query performance\n\n8. Testing and validation:\n\nEnd-to-end testing: Ensure that the new ETL process captures\n\ndata correctly, transforms it as expected, and loads it without\n\nissues\n\nPerformance testing: Con\u0000rm that the new process meets the\n\nperformance goals that were set at the beginning\n\n9. Monitoring and maintenance:",
      "content_length": 986,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 707,
      "content": "Monitoring tools: Use tools such as Apache Ambari or\n\nCloudera Manager to monitor health, performance, and failures\n\nAutomate failover and recovery: Ensure that if nodes fail or\n\nprocesses crash, the system can recover automatically without\n\ndata loss\n\n10. Documentation: Ensure every part of the new ETL process is well-\n\ndocumented, from data sources and transformations to load processes\n\nand optimizations.\n\nMigrating traditional ETL processes to a big data framework is a\n\nsigni\u0000cant endeavor that can oﬀer immense bene\u0000ts in terms of\n\nscalability, performance, and \u0000exibility. While the process requires\n\ncareful planning, design, and testing, the result can lead to more\n\neﬃcient and cost-eﬀective data processing and analysis.\n\nDefining and executing your migration process with Hadoop\n\nDe\u0000ning and executing a Hadoop migration process from on-\n\npremises to AWS involves a complex transition that requires careful\n\nplanning and execution. is process encompasses transferring\n\ndata, applications, and infrastructure to AWS cloud-based Hadoop\n\nclusters. In this recipe, we’ll explore the essential steps and\n\nconsiderations involved in successfully migrating your Hadoop",
      "content_length": 1177,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 708,
      "content": "ecosystem to AWS, helping you unlock the bene\u0000ts of cloud\n\nscalability, \u0000exibility, and cost-eﬃciency.\n\nGetting ready\n\nBefore embarking on your migration journey, it’s essential to lay a\n\nstrong foundation with the following prerequisites and technical\n\nrequirements:\n\nPrerequisites:\n\nClear objectives: De\u0000ne your migration goals and expected\n\noutcomes. What are you trying to achieve with this migration\n\n(cost savings, improved scalability, increased agility, and so on)?\n\nStakeholder alignment: Ensure buy-in and collaboration from\n\nkey stakeholders across IT, business, and management.\n\nAWS account: Ensure you have an active AWS account with the\n\nnecessary permissions to create and manage resources.\n\nSkilled team: Assemble a team with expertise in AWS services,\n\nnetworking, security, and your existing on-premises\n\ninfrastructure.\n\nTechnical requirements:\n\nInventory and assessment: oroughly document your current\n\non-premises environment, including servers, applications,",
      "content_length": 981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 709,
      "content": "databases, storage, and network con\u0000gurations. Identify\n\ndependencies and performance requirements.\n\nMigration strategy: Choose the most suitable migration\n\napproach for each application or workload.\n\nRehost (li and shi): Migrate applications without major\n\nchanges, oen using tools such as AWS SMS:\n\nReplatform: Make minor modi\u0000cations to take\n\nadvantage of cloud-native features\n\nRefactor/rearchitect: Redesign applications so that you\n\ncan fully leverage AWS services and optimize for the\n\ncloud\n\nConnectivity: Establish secure and reliable network\n\nconnectivity between your on-premises environment and AWS\n\nusing options such as AWS Direct Connect or a VPN.\n\nMigration tools: Select and con\u0000gure appropriate AWS\n\nmigration tools, such as AWS Migration Hub, DMS, and\n\nApplication Discovery Service.\n\nHow to do it…\n\nWhen de\u0000ning migration processes for a Hadoop framework,\n\nfollow these steps:\n\n1. Planning phase:\n\nAssessment:",
      "content_length": 932,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 710,
      "content": "Analyze existing Hadoop infrastructure, data size,\n\nworkloads, and dependencies\n\nEvaluate AWS services for compatibility and cost-\n\neﬀectiveness (for example, EMR, S3, EC2, and Glue)\n\nConsider performance, scalability, security, and\n\ngovernance requirements\n\nDesign:\n\nChoose appropriate AWS services and architecture (for\n\nexample, multi-cluster EMR, hybrid setup, and so on)\n\nPlan data transfer and storage strategies (for example,\n\nS3 buckets and data partitioning)\n\nAddress network connectivity and security measures\n\nDetermine monitoring and logging tools for post-\n\nmigration analysis\n\nCreate a detailed migration timeline and rollback plan\n\n2. Execution phase:\n\nAWS environment setup:\n\nCreate AWS accounts and con\u0000gure IAM roles for\n\naccess control\n\nSet up the necessary VPCs, subnets, security groups,\n\nand network con\u0000gurations",
      "content_length": 835,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 711,
      "content": "Provision EMR clusters with appropriate hardware and\n\nsoware con\u0000gurations\n\nData migration:\n\nTransfer data from on-premises Hadoop storage to S3\n\nusing tools such as DistCp or S3DistCp\n\nOptimize transfer speed and validate data integrity\n\nduring the transfer\n\nJob migration:\n\nRefactor or modify Hadoop jobs so that they run on\n\nEMR while considering any syntax or API changes\n\nTest modi\u0000ed jobs with sample data in the AWS\n\nenvironment\n\nTesting and validation:\n\noroughly test migrated workloads on EMR to ensure\n\nfunctionality and performance\n\nValidate data integrity and consistency post-migration\n\nMonitor resource usage and costs for optimization\n\n3. Post-migration phase:\n\nMonitoring and optimization:\n\nContinuously monitor EMR clusters for performance,\n\nresource usage, and costs",
      "content_length": 786,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 712,
      "content": "Implement cost optimization strategies (for example,\n\nspot instances, auto-scaling, and so on)\n\nMaintenance and security:\n\nApply security patches and updates to AWS services\n\nand EMR clusters\n\nRegularly review security con\u0000gurations and access\n\ncontrols\n\nTroubleshooting: Address any issues that arise while leveraging\n\nAWS support and documentation\n\nData security: Encrypt sensitive data in transit and at rest while\n\nadhering to compliance requirements\n\nCost optimization: Leverage AWS cost management tools and\n\nexplore spot instances and reserved pricing\n\nPerformance optimization: Optimize EMR con\u0000gurations and\n\nconsider caching and data partitioning techniques\n\nGovernance: Implement data access control and auditing\n\nmechanisms for compliance\n\nNOTE\n\nEvery migration is unique, so tailor your approach so that it ﬁts your speciﬁc\n\nrequirements and challenges. Don’t hesitate to seek assistance from AWS\n\nexperts or certiﬁed partners if needed. With the right strategy and execution,\n\nyour AWS migration can be a smooth and successful journey.",
      "content_length": 1049,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 713,
      "content": "Migrating the existing Hadoop security authentication and authorization processes\n\nMigrating your security domain from on-premises to the AWS\n\ncloud is a pivotal transformation that can enhance the resilience,\n\nscalability, and manageability of your security infrastructure. is\n\nprocess involves transitioning critical security components and\n\nprotocols to the cloud environment, ensuring that your\n\norganization can harness the power of AWS while maintaining\n\nrobust security measures. In this context, we’ll explore the crucial\n\nsteps and considerations involved in successfully migrating your\n\nsecurity domain to AWS, enabling you to safeguard your digital\n\nassets eﬀectively in the cloud.\n\ne steps to migrate security authentication and authorization from\n\nHadoop to AWS are as follows:\n\n1. Understand your current setup: Identify the security mechanisms\n\ncurrently in place in your Hadoop environment. is could include\n\nKerberos for authentication, Apache Ranger or Apache Sentry for\n\nauthorization, and encryption methods.\n\n2. Perform authentication and authorization: Assess your Hadoop\n\necosystem’s components (such as HDFS, Hive, Spark, and so on) and\n\ntheir security con\u0000gurations:",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 714,
      "content": "Authentication mechanisms: Identify the methods being used\n\n(Kerberos, LDAP, and so on)\n\nAuthorization mechanisms: Determine how permissions are\n\ngranted and enforced (HDFS ACLs, Ranger, and so on)\n\nIntegrations: Map any external systems that interact with\n\nHadoop security\n\n3. Choose AWS services:\n\nIAM: Centrally manages users, groups, and permissions for\n\nAWS services and resources\n\nS3 access points: Create access points with distinct permissions\n\nfor S3 buckets, simplifying access management\n\nLake Formation: Optionally, create a secure data lake with \u0000ne-\n\ngrained access control and auditing\n\nEncryption:\n\nUnderstand how encryption is handled in AWS using\n\nservices such as AWS KMS and AWS Certi\u0000cate\n\nManager.\n\nPlan how to migrate data securely. AWS oﬀers services\n\nsuch as AWS DataSync, AWS Transfer Family, and\n\nAWS Snowball for large-scale data migrations.\n\nEnsure data is encrypted during transit and at rest in\n\nAWS.",
      "content_length": 931,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 715,
      "content": "4. Integrate with AWS security services:\n\nIntegrate your Hadoop applications with AWS security services.\n\nis might require some recon\u0000guration or code changes.\n\nUse AWS IAM for managing users and permissions. You can\n\ncreate IAM roles and policies that align with your existing\n\nHadoop permissions.\n\n5. Testing and validation:\n\noroughly test the security con\u0000gurations in a staging\n\nenvironment before moving to production\n\nValidate that authentication and authorization work as expected\n\nand that data is accessed securely\n\nUtilize AWS CloudTrail and AWS Con\u0000g for monitoring and\n\nauditing security con\u0000gurations\n\nContinuously monitor for any security threats or vulnerabilities\n\n6. Migrate authentication:\n\nAWS credential providers: Con\u0000gure Hadoop so that it uses\n\nAWS credentials for S3 access via the following methods:\n\nAccess keys and secret keys\n\nIAM roles for EC2 instances\n\nTemporary security credentials",
      "content_length": 916,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 716,
      "content": "Instance pro\u0000le credentials provider: For EC2 instances, use\n\nIAM roles for authentication\n\n7. Migrate authorization:\n\nIAM policies: De\u0000ne permissions for S3 buckets and objects\n\nusing IAM policies\n\nS3 access points: Simplify permission management with access\n\npoints for speci\u0000c use cases\n\nLake Formation: For a data lake, manage permissions centrally\n\nwith Lake Formation\n\n8. Map existing permissions:\n\nTranslate existing Hadoop permissions into IAM policies or S3\n\naccess points\n\nConsider using tools such as Apache Ranger for policy\n\nmanagement across environments\n\n9. Migrate data:\n\nTransfer data to S3, preserving existing permissions or mapping\n\nthem to IAM policies\n\nEncrypt sensitive data at rest and in transit\n\n10. Others:\n\nEncryption: Use S3 encryption features for data protection",
      "content_length": 793,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 717,
      "content": "Auditing: Enable AWS CloudTrail for logging and auditing of\n\nS3 access\n\nird-party tools: Consider tools for managing authentication\n\nand authorization across Hadoop and AWS (for example,\n\nApache Ranger and Cloudera Navigator)\n\nAccess patterns: Analyze access patterns to optimize S3 access\n\npolicies and access points\n\nCompliance requirements: Adhere to regulatory or industry-\n\nspeci\u0000c compliance requirements\n\nGetting ready\n\nTo ensure a successful migration, it’s essential to understand the\n\ntechnical and security requirements outlined here.\n\noroughly document your on-premises Hadoop security setup,\n\nincluding the following aspects:\n\nAuthentication methods: How users and services authenticate (for\n\nexample, Kerberos, LDAP, and Active Directory).\n\nAuthorization mechanisms: How access to data and resources is\n\ncontrolled (for example, Sentry and Ranger).\n\nData encryption: Any encryption methods in place for data at rest and in\n\ntransit.\n\nAWS account and IAM: An active AWS account with necessary IAM\n\npermissions to create and manage resources. Ensure you have familiarity",
      "content_length": 1084,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 718,
      "content": "with IAM roles and policies for \u0000ne-grained access control.\n\nTarget AWS services: Knowledge of relevant AWS services for Hadoop\n\nmigration (for example, EMR, S3, Glue, and Lake Formation). You must\n\nalso understand how security integrates with these services.\n\ne technical requirements are as follows:\n\nNetwork connectivity: Secure network connectivity between your on-\n\npremises environment and AWS (for example, Direct Connect and\n\nVPN)\n\nMigration tools: Tools such as AWS SCT, DMS, and DataSync for data\n\nand metadata transfer\n\ne AWS CLI or SDK: For interacting with AWS services\n\nprogrammatically\n\nBackup and recovery: Ensure proper backups of your on-premises\n\nHadoop security con\u0000gurations and data\n\nHow to do it…\n\nWhen migrating existing Hadoop security authentication and\n\nauthorization, you’d typically follow these steps:\n\n1. Planning and design:\n\nMap security controls: Identify the equivalent AWS security\n\nservices and con\u0000gurations to replicate your on-premises\n\nsecurity model",
      "content_length": 993,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 719,
      "content": "Authentication: Consider AWS IAM, AWS SSO, or integrating\n\nwith existing identity providers using SAML or OpenID\n\nConnect\n\nAuthorization: Evaluate AWS Lake Formation for \u0000ne-grained\n\naccess control on data stored in S3, or continue using tools such\n\nas Ranger if you’re migrating them to EMR\n\nEncryption: Utilize server-side encryption with Amazon S3 or\n\nKMS for data at rest and in transit\n\nProvide a migration strategy:\n\nPhased approach: Migrate components gradually,\n\nstarting with non-critical data and workloads\n\nParallel testing: Run your migrated environment in\n\nparallel with the on-premises setup for testing and\n\nvalidation purposes\n\n2. Implementation:\n\nIAM:\n\nCreate IAM roles: Create IAM roles for users, services,\n\nand applications, granting appropriate permissions\n\nCon\u0000gure authentication: Set up authentication\n\nmechanisms such as IAM users, federated identities, or\n\nSSO integration\n\nAWS Lake Formation:",
      "content_length": 919,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 720,
      "content": "Fine-grained control: Set up Lake Formation to\n\nmanage permissions on data stored in S3 data lakes\n\nRanger (optional): If you’re using Ranger, migrate it to\n\nyour EMR cluster, and con\u0000gure your policies\n\nEncryption:\n\nS3 encryption: Enable server-side encryption (SSE-S3\n\nor SSE-KMS) for S3 buckets that store sensitive data\n\nEMR encryption: Con\u0000gure encryption for EBS\n\nvolumes attached to EMR instances\n\nIn-transit encryption: Ensure secure communication\n\nchannels between your on-premises environment and\n\nAWS\n\n3. Testing and validation:\n\norough testing: Test authentication and authorization\n\nmechanisms in the AWS environment\n\nSecurity review: Conduct security assessments and penetration\n\ntesting to identify vulnerabilities\n\n4. Cutover and go-live:\n\nUser communication: Communicate the migration plan and\n\nany changes in authentication or access procedures to users\n\nGradual cutover: Gradually shi workloads to the AWS\n\nenvironment, monitoring them closely for any issues",
      "content_length": 979,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 721,
      "content": "Rollback plan: Have a well-de\u0000ned rollback plan in case of\n\nunforeseen problems during the cutover\n\n5. Post-migration:\n\nMonitoring and auditing: Continuously monitor access logs and\n\nsecurity events\n\nRe\u0000nement: Re\u0000ne security con\u0000gurations and policies based\n\non real-world usage patterns and evolving threats\n\n6. Decommission on-premises: Once you’re con\u0000dent in the AWS setup,\n\ndecommission the on-premises Hadoop security infrastructure.\n\nOceanofPDF.com",
      "content_length": 456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 722,
      "content": "12 Harnessing the Power of AWS for Seamless Data Warehouse Migration\n\nIn the ever-evolving landscape of data management, the migration\n\nof your data warehouse to the cloud is an essential step toward\n\nunlocking scalability, agility, and cost eﬃciency. As we embark on\n\nthis journey, we’ll examine the powerful tools that AWS oﬀers to\n\nstreamline and optimize this transition. In this chapter, we’ll explore\n\nthree key services: the AWS Schema Conversion Tool (SCT) and\n\nAWS Database Migration Service (DMS), as well as the AWS Snow\n\nFamily. Each of these tools plays a crucial role in ensuring a\n\nseamless and successful migration of your valuable data assets to\n\nthe AWS cloud.\n\nWe will cover the following recipes:\n\nCreating SCT migration assessment report with AWS SCT\n\nExtracting Data with AWS DMS\n\nLive example – migrating an Oracle database from a local laptop to AWS\n\nRDS using AWS SCT\n\nLeveraging AWS Snow Family for large-scale data migration\n\nTechnical requirements",
      "content_length": 975,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 723,
      "content": "Make sure you have an active AWS account. If you don’t already\n\nhave one, sign up for an AWS account at\n\nhttps://aws.amazon.com/resources/create-account/ before\n\nproceeding. You can access the code for this project in the GitHub\n\nrepository at https://github.com/PacktPublishing/Data-\n\nEngineering-with-AWS-Cookbook/tree/main/Chapter12.\n\nCreating SCT migration assessment report with AWS SCT\n\nAWS SCT is a valuable utility for automating the migration of\n\ndatabase schemas between diﬀerent database engines. Whether\n\nyou’re moving from an on-premises database to a cloud-based\n\nsolution or converting between diﬀerent types of databases, SCT\n\nsimpli\u0000es the process by analyzing the source schema and\n\ngenerating a compatible schema for the target database. SCT\n\nautomates much of the process. It analyzes the source database\n\nschema and converts it into a format compatible with the target\n\ndatabase as shown, ensuring a smooth transition with minimal\n\nmanual intervention.\n\nUnderstand the data migration journey with the following \u0000ow\n\ndiagram outlining the use of AWS SCT tools:",
      "content_length": 1080,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 724,
      "content": "Figure 12.1 – SCT for data migration\n\nis recipe demonstrates how to migrate an on-premises database\n\n(Source) to AWS (Target) using the AWS SCT. We’ll focus on\n\nconverting the database schema and generating comprehensive\n\nreports to streamline the migration process.\n\nFigure 12.2 – SCT ﬂow for data migration\n\nWhen you run AWS SCT, it assesses your source database and\n\nprovides a detailed report on what can be automatically converted\n\nand what needs manual intervention.\n\nGetting ready",
      "content_length": 488,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 725,
      "content": "Before you begin generating your migration assessment report,\n\nensure you have the following in place:\n\nAWS account:\n\nActive AWS account: You need an active AWS account to use\n\nAWS SCT, as it’s an AWS service.\n\nSource and target database connectivity:\n\nEnsure that your source and target databases are reachable. You\n\nneed to con\u0000gure network settings (for example, VPC and\n\n\u0000rewall rules) to allow SCT to connect to both the source and\n\ntarget databases.\n\nSupported databases include Oracle, Microso SQL Server,\n\nMySQL, MariaDB, PostgreSQL, Amazon Aurora, and many\n\nothers.\n\nAWS SCT installation:\n\nDownload and install the AWS SCT application on your local\n\nmachine (Windows, Linux, or macOS). It requires Java Runtime\n\nEnvironment (JRE) to run.\n\nEnsure that your machine meets these system requirements:\n\nAt least 4 GB of RAM\n\nAt least 500 MB of disk space\n\nAWS IAM role and permissions:",
      "content_length": 890,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 726,
      "content": "Create or use an AWS Identity and Access Management (IAM)\n\nrole that has the required permissions for SCT to access\n\nresources in AWS, such as AWS Glue, Amazon Relational\n\nDatabase Service (RDS), and Amazon S3.\n\ne IAM role should have permissions for AWS Glue, Amazon\n\nS3, Amazon Redshi, or any other target service you are using.\n\nSchema conversion project setup:\n\nDe\u0000ne your source and target database types to con\u0000gure the\n\ncorrect conversion project within SCT.\n\nMake sure SCT is compatible with both the source and target\n\ndatabase versions.\n\nAWS DMS: If you are planning to use AWS DMS for data migration aer\n\nschema conversion, ensure that the AWS DMS endpoints are properly\n\nset up, and that your replication instance has access to both the source\n\nand target databases.\n\nSuﬃcient disk space for SCT project \u0000les: SCT generates project \u0000les\n\nduring the schema conversion process, which can require signi\u0000cant\n\nstorage space depending on the size of the schema and data to be\n\nconverted.\n\nSource database privileges: Ensure that the user accounts for both the\n\nsource and target databases have the necessary privileges to access\n\nschemas, tables, and other database objects for schema conversion.\n\nSource and target databases:",
      "content_length": 1236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 727,
      "content": "Database details: Have the connection details (hostname/IP,\n\nport, username, and password) ready for both your source and\n\ntarget databases.\n\nCompatibility: Ensure that the source and target database\n\nplatforms are supported by AWS SCT. Consult the AWS SCT\n\ndocumentation\n\n(https://docs.aws.amazon.com/SchemaConversionTool/latest/u\n\nserguide/CHAP_Welcome.html) for the latest compatibility\n\ninformation.\n\nDatabase access: Make sure you have appropriate access\n\npermissions to the source and target databases to run queries,\n\nextract schemas, and potentially modify data (if necessary).\n\nSoware and drivers:\n\nAWS SCT installation: Download and install the latest version\n\nof AWS SCT on a machine that has network access to both your\n\nsource and target databases:\n\nDownload for Windows:\n\nhttps://s3.amazonaws.com/publicsctdownload/Windo\n\nws/aws-schema-conversion-tool-1.0.latest.zip\n\nDownload for Linux:\n\nhttps://s3.amazonaws.com/publicsctdownload/Fedora/\n\naws-schema-conversion-tool-1.0.latest.zip\n\nDownload for Ubuntu:\n\nhttps://s3.amazonaws.com/publicsctdownload/Ubuntu",
      "content_length": 1070,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 728,
      "content": "/aws-schema-conversion-tool-1.0.latest.zip\n\nJDBC drivers: Install the JDBC drivers for both your source and\n\ntarget database platforms. ese drivers are essential for AWS\n\nSCT to connect to and interact with your databases:\n\nDownload the JDBC driver for your Oracle database:\n\nhttps://www.oracle.com/jdbc\n\nDownload the PostgreSQL driver:\n\nhttps://jdbc.postgresql.org/download/postgresql-\n\n42.2.19.jar\n\nFor other JDBC drivers, please check the database\n\ndriver page. All supported database drivers are listed in\n\nthe AWS SCT user guide:\n\nhttps://docs.aws.amazon.com/SchemaConversionTool/\n\nlatest/userguide/CHAP_Installing.html#CHAP_Install\n\ning.JDBCDrivers\n\nNetwork connectivity: Verify that the machine running\n\nAWS SCT can connect to both your source and target\n\ndatabases over the network. is may involve adjusting\n\n\u0000rewall rules or security group settings.\n\nSchema understanding:\n\nSource schema: Have a good understanding of your source\n\ndatabase schema, including tables, columns, relationships, data\n\ntypes, and any custom objects or stored procedures.",
      "content_length": 1058,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 729,
      "content": "Target schema (optional): If you’re migrating to a speci\u0000c target\n\ndatabase, familiarize yourself with its schema as well to\n\nanticipate potential mapping challenges.\n\nHow to do it…\n\nWith all the necessary setup in place, we are now ready to begin\n\ncon\u0000guring the AWS SCT tool.\n\nConfiguring JDBC drivers in AWS SCT\n\nCon\u0000guring JDBC drivers in AWS SCT involves several steps.\n\nJDBC drivers are essential for AWS SCT to connect to your source\n\nand target databases. Here’s a general guide to help you con\u0000gure\n\nJDBC drivers in AWS SCT global settings:\n\n1. Download the JDBC drivers: First, you need to download the\n\nappropriate JDBC drivers for your source and target databases. ese\n\ndrivers are typically available on the database vendors’ websites. For\n\nexample, if you are converting a schema from an Oracle database, you\n\nwould download the Oracle JDBC driver from Oracle’s website.\n\n2. Launch AWS SCT: Open the AWS SCT on your computer. Once you\n\nhave installed and opened SCT, you will see the following screen:",
      "content_length": 1016,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 730,
      "content": "Figure 12.3 – Opening SCT after installation\n\n3. Access Global Settings: In the AWS SCT, go to Global Settings on the\n\nSettings menu. is is usually found in the top menu bar. Locate the\n\ndriver con\u0000guration. Inside Global Settings, look for a tab or section\n\nrelated to JDBC drivers. is might be labeled something such as JDBC\n\nDrivers, Driver Con\u0000guration, or similar.\n\n4. Add the JDBC drivers: In the JDBC drivers section, you will likely see\n\ndiﬀerent categories for diﬀerent database types (for example, Oracle,\n\nMySQL, and PostgreSQL). Click on the Add or Install button next to the\n\ndatabase type for which you downloaded the JDBC driver. Navigate to\n\nthe location where you downloaded the JDBC driver \u0000le, select it, and\n\nthen open or install it as prompted.\n\n5. Verify or test the driver: Aer adding the driver, there might be an\n\noption to test the driver to ensure that it’s correctly installed and\n\nfunctional. Use this feature to verify the setup.",
      "content_length": 962,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 731,
      "content": "6. Save the Con\u0000guration: Once the drivers are added and tested, save your\n\ncon\u0000guration settings.\n\n7. Restart AWS SCT (if required): In some cases, you may need to restart\n\nAWS SCT for the changes to take eﬀect.\n\n8. Proceed with database connection: Now, when setting up a new\n\nconnection to a database in AWS SCT, it should use the JDBC driver\n\nyou’ve con\u0000gured.\n\nCHECKING THE COMPATIBILITY\n\nEnsure the JDBC driver version is compatible with your version of AWS SCT.\n\nUse AWS SCT documentation and your database’s documentation for any\n\nspeciﬁc instructions or requirements.\n\n9. Add source and target connections: Adding source and target\n\nconnections in AWS SCT is a critical step in the database migration\n\nprocess. is allows AWS SCT to analyze your source database and\n\ncreate a corresponding schema in your target database. Here’s how you\n\ncan add these connections:\n\nI. Adding a source connection: Open AWS SCT and launch it on\n\nyour computer:\n\nFigure 12.4 – AWS SCT",
      "content_length": 974,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 732,
      "content": "i. Create a new project or open an existing one: If you haven’t already,\n\ncreate a new project by going to File | New project or open an existing\n\nproject.\n\nii. Initiate adding a source connection: Go to Database | Add source or\n\nsimply click on the Add source button if it’s available on your toolbar.\n\niii. Choose the source database type: Select the type of your source database\n\n(for example, Oracle, Microso SQL Server, and MySQL) from the list\n\nprovided.\n\niv. Enter connection details: Fill in the necessary details such as server\n\nname, port, database name, user, and password. ese details are speci\u0000c\n\nto your database environment.",
      "content_length": 641,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 733,
      "content": "Figure 12.5 – Adding data source in SCT\n\nOnce you add the data source, it will be visible in the SCT editor.\n\nTest the connection: Click on the Test connection button to ensure that\n\nAWS SCT can successfully connect to your source database.\n\nTroubleshoot any connection issues if the test fails.\n\nFinish and save: Once the connection is successful, click on OK or Save\n\nto store the source database connection.\n\nII. Adding a target connection:",
      "content_length": 443,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 734,
      "content": "i. Initiate adding a target connection: Go to Database | Add target\n\nor use the Add target button.\n\nii. Select the target database type: Choose the type of your target\n\ndatabase. is could be an AWS service such as Amazon RDS,\n\nAmazon Redshi, Aurora, or another database type.\n\niii. Input target connection details: Just like with the source, input\n\nthe required details for your target database: server name, port,\n\ndatabase name, user, and password.\n\niv. Test the target connection: Use the Test connection feature to\n\nverify connectivity to the target database.\n\nv. Save the target connection: Once the test is successful, save the\n\nconnection by clicking on Save.\n\nNOTE\n\nMake sure you meet the following conditions:\n\nJDBC drivers: Ensure you have the correct JDBC drivers installed for both\n\nthe source and target databases before attempting to connect.\n\nNetwork accessibility: Make sure that your source and target databases\n\nare accessible from where AWS SCT is running. This might involve\n\nconﬁguring network settings or VPNs.\n\nSecurity and permissions: Verify that the user credentials used for both\n\nsource and target have the necessary permissions for the migration\n\nprocess.\n\nFirewall and security groups: Adjust ﬁrewall settings or security groups\n\nas needed to allow AWS SCT to access the databases.",
      "content_length": 1313,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 735,
      "content": "Once you have successfully added both the source and target\n\nconnections, you can proceed with the schema conversion process\n\nand any other migration activities using AWS SCT.\n\nMapping source and target schemas\n\nMapping source and target schemas in the AWS SCT is a crucial\n\nstep in the database migration process. is process involves\n\naligning the schemas of your source database with those in your\n\ntarget database, which helps ensure a smooth and accurate transfer\n\nof data. Here’s how you can map source and target schemas in AWS\n\nSCT:\n\n1. Open your project in AWS SCT: Start by opening your project in the\n\nAWS SCT where you have already established the source and target\n\nconnections.\n\nI. Access the schema mapping interface: Look for an option to\n\nview or manage your schemas, oen found under a menu\n\nlabeled something such as Schema or Database.\n\nII. Select your source schema: In the schema view, you’ll see a list\n\nof schemas from your source database. Select the schema you\n\nwant to migrate to.\n\nIII. Initiate the mapping process: Right-click on the selected\n\nschema and look for an option to map it to a target schema.\n\nis might be labeled as Map to target or something similar.\n\nIV. Choose the corresponding target schema:",
      "content_length": 1238,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 736,
      "content": "i. A list of available target schemas (from your target\n\ndatabase) will be displayed. Select the schema in the\n\ntarget database that corresponds to or will host the\n\nschema from the source database.\n\nii. If the target schema does not exist, you may have an\n\noption to create a new schema.\n\nV. Con\u0000gure schema mapping options: Depending on the\n\ncomplexity of your database, you might have additional options\n\nto con\u0000gure during the mapping. is can include data type\n\nmappings, key conversions, and so on.\n\ni. Review and adjust mappings: Carefully review the\n\nproposed schema mappings. AWS SCT might provide\n\nsuggestions or automatic mappings, but it’s essential to\n\nverify that these are correct and adjust as necessary.\n\nii. Apply and save the mappings: Once you are satis\u0000ed\n\nwith the mappings, apply and save them.\n\niii. Repeat for additional schemas: If you have multiple\n\nschemas to migrate, repeat this process for each one.\n\n2. Generate an assessment report:\n\nI. In AWS SCT, go to the View menu and select Main view.\n\nII. In the le panel, choose the schema objects you want to include\n\nin the report. You can select individual objects, schemas, or the\n\nentire database.\n\nIII. Go to the View menu and select Assessment Report view.",
      "content_length": 1238,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 737,
      "content": "e report will provide a summary of the following:\n\nConversion success: Objects converted successfully\n\nAction items: Objects requiring manual intervention or code\n\nchanges\n\nComplexity estimates: Eﬀort levels for addressing action items\n\nRecommendations: Suggestions for optimization and best\n\npractices\n\n3. Save the report: You have the following options to save the report:\n\nExport options: You can save the assessment report as a PDF or\n\na CSV \u0000le\n\nPDF format: is provides a comprehensive, formatted report\n\nsuitable for sharing\n\nCSV format: is allows you to analyze the report data in a\n\nspreadsheet\n\n4. Review the assessment report: In a multi-server project, the assessment\n\nreport provides consolidated views of all the mapped schemas and\n\ntargets from a single source database.\n\ne following screenshot illustrates a sample assessment report\n\nfrom a multi-server project. is report consolidates\n\ninformation from a single source database, mapping various\n\nschemas to diﬀerent target databases such as Aurora PostgreSQL\n\nand Amazon Redshi. e summary section oﬀers a",
      "content_length": 1077,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 738,
      "content": "comprehensive overview of all target databases, allowing you to\n\nquickly select a speci\u0000c server or scroll through detailed\n\ninformation for each.\n\nFigure 12.6 – Viewing report in SCT\n\ne details report is grouped by each target database. You can save\n\nthis report in CSV and PDF format for viewing outside AWS SCT\n\nor sharing with others.",
      "content_length": 339,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 739,
      "content": "Figure 12.7 – Report in SCT",
      "content_length": 27,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 740,
      "content": "Figure 12.8 – Report details in SCT\n\n5. Convert schemas: When you’re ready to convert your schemas, complete\n\nthe following steps:\n\nI. Select the desired objects from the source tree (right-click).\n\nII. Choose Convert schema.",
      "content_length": 225,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 741,
      "content": "III. AWS SCT doesn’t apply the converted code to the target\n\ndatabase directly. You can view and edit the converted code\n\ninside AWS SCT or save it to a SQL script.\n\nIV. To view or edit the converted code inside AWS SCT, choose an\n\nobject in the source tree. e converted code is displayed in the\n\nlower-center panel.\n\nV. To save the converted code to a SQL script, choose the intended\n\nschema on the target tree (right-click), and choose Save as SQL.\n\nVI. To apply the converted code to the database, choose the\n\nintended schema on the target tree (right-click), and choose\n\nApply to database. is option is only available for non-virtual\n\ntargets.\n\nSee also\n\nWhat is the AWS SCT?:\n\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/\n\nCHAP_Source.htm\n\nCategory: AWS SCT:\n\nhttps://aws.amazon.com/blogs/database/category/database/aws-schema-\n\nconversion-tool/\n\nExtracting data with AWS DMS\n\nAWS DMS is a versatile tool designed to simplify database\n\nmigrations and data transformations. While it’s primarily known for",
      "content_length": 1032,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 742,
      "content": "database migrations, it’s also a powerful tool for extracting data\n\nfrom your source databases and loading it into other destinations,\n\nsuch as Amazon S3, for further processing, analysis, or archival.\n\nIn this recipe, you’ll learn how to use AWS DMS to extract data and\n\nmigrate it to the cloud, along with converting database schemas\n\nusing AWS SCT. DMS handles the extraction and migration of your\n\ndatabase data, while SCT helps convert the schema when moving\n\nbetween diﬀerent database engines. By the end of this recipe, you’ll\n\nhave a streamlined approach to converting database structures and\n\nextracting data eﬃciently, ensuring a smooth transition to AWS\n\nservices such as Amazon RDS.\n\nIn this recipe, we will explore how to extract data from an on-\n\npremises database and migrate it to AWS using DMS. is process\n\nis divided into two parts:\n\nAWS SCT for database schema conversion (we already covered it in the\n\nprevious recipe, Creating SCT migration assessment report with AWS\n\nSCT)\n\nAWS DMS for data migration\n\ne following diagram illustrates how on-premises data can be\n\nmigrated to the AWS cloud using an internet gateway, AWS SCT,\n\nand AWS DMS, ultimately landing in an Amazon RDS instance:",
      "content_length": 1208,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 743,
      "content": "Figure 12.9 – AWS DMS architecture\n\nLet’s get started!\n\nGetting ready\n\nBefore diving into data extraction with AWS DMS, ensure you have\n\nthe following in place:\n\nActive AWS account: You’ll need an active AWS account with suﬃcient\n\npermissions to create and manage DMS resources. If you don’t have an\n\naccount, you can create one for free at https://aws.amazon.com.\n\nSource database:\n\nSupported database: Verify that your source database is\n\ncompatible with AWS DMS. DMS supports a wide range of",
      "content_length": 494,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 744,
      "content": "database engines, including popular options such as MySQL,\n\nPostgreSQL, Oracle, Microso SQL Server, and more. You can\n\n\u0000nd the full list of supported engines in the AWS DMS\n\ndocumentation (https://docs.aws.amazon.com/dms/).\n\nConnectivity: Ensure that your DMS replication instance can\n\nconnect to your source database. is may involve adjusting\n\nnetwork settings or security groups to allow inbound\n\nconnections.\n\nCredentials: Have valid database credentials (username and\n\npassword) with suﬃcient permissions to read the data you want\n\nto extract.\n\nAmazon S3 bucket: Create an Amazon S3 bucket in the same AWS\n\nregion as your DMS replication instance to store the extracted data.\n\nPermissions: Ensure that your DMS replication instance has\n\npermission to write to the S3 bucket.\n\nIAM roles: To securely interact with your resources, you’ll need to create\n\nand con\u0000gure two IAM roles:\n\nDMS replication instance role: is role allows your DMS\n\nreplication instance to access your source and target resources.\n\nAttach the following managed policies to this role:\n\nAmazonS3FullAccess: is grants full access to the S3\n\nbucket for storing extracted data.",
      "content_length": 1151,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 745,
      "content": "AmazonDMSReplicationInstance: is provides the\n\nnecessary permissions for DMS to operate.\n\nDMS task role: If you’re accessing your source database through\n\na VPC, create a custom IAM role and attach policies that grant\n\naccess to your VPC and the source database.\n\nHow to do it…\n\n1. Create a DMS replication instance: An AWS DMS replication instance is\n\na key component in the DMS that performs the actual data migration\n\ntasks. It provides the necessary compute and memory resources for AWS\n\nDMS to migrate data from the source database to the target database.\n\ne replication instance handles the migration process, including\n\nextracting data, transforming it (if needed), and loading it into the\n\ntarget. e steps are as follows:\n\nI. Open the AWS DMS console at https://us-east-\n\n1.console.aws.amazon.com/dms/v2/home?region=us-east-\n\n1#\u0000rstRun.\n\nII. Click on Create replication instance.",
      "content_length": 890,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 746,
      "content": "Figure 12.10 – DMS replication instance\n\nIII. Provide a name, choose an instance class based on your data volume,\n\nand select your VPC (if applicable):\n\nParameter\n\nValue\n\nName\n\nOracle-to-aurora-replication-\n\ninstance\n\nDescription\n\nOracle to Aurora DMS replication\n\ninstance\n\nInstance Class\n\ndms.c4.xlarge\n\nReplication engine\n\nLatest\n\nversion\n\nVPC\n\nYour VPC id",
      "content_length": 359,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 747,
      "content": "Parameter\n\nValue\n\nAllocated storage (GB)\n\nLeave default\n\nMulti-AZ\n\nUnchecked\n\nPublicly accessible\n\nUnchecked\n\nTable 12.1 – Replication instance conﬁguration\n\nI. Select the Publicly accessible option if your source database is in a\n\ndiﬀerent network.\n\nII. Assign the DMS replication instance role you created.\n\n1. Create a source endpoint: AWS DMS source endpoints are con\u0000gurations\n\nthat de\u0000ne the source database systems from which you want to migrate\n\ndata. ese endpoints specify the details of your on-premises or cloud-\n\nbased databases, such as the database type, hostname, port number, and\n\nauthentication credentials. It’s easy to create your own source endpoint.\n\nIn the DMS console, navigate to Endpoints and click on Create\n\nendpoint.",
      "content_length": 745,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 748,
      "content": "Figure 12.11 – DMS endpoints\n\n3. Select your source database engine: When using AWS DMS to migrate\n\ndata, the \u0000rst critical step is selecting your source database engine. e\n\nsource database is the system from which you’re migrating data to AWS.\n\nAWS DMS supports a wide range of both commercial and open source\n\ndatabase engines, allowing for migrations between heterogeneous or\n\nhomogeneous systems. Here’s what to do:\n\nI. Provide the endpoint identi\u0000er, server name or IP address, port,\n\nusername, and password.\n\nII. Test the connection to ensure it’s successful.\n\n4. Create target endpoint: In AWS DMS, the target endpoint refers to the\n\ndestination database where your data will be migrated. It is the system or\n\nenvironment that receives the data from the source database during the\n\nmigration process. Here are the steps:\n\nI. Click on Create endpoint again.\n\nII. Select S3 as the target endpoint type.",
      "content_length": 908,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 749,
      "content": "III. Provide the endpoint identi\u0000er and S3 bucket name for storing\n\nextracted data.\n\nIV. Specify the output format (for example, CSV and Parquet) and\n\ncon\u0000gure any data format or compression settings:",
      "content_length": 200,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 751,
      "content": "Figure 12.12 – Create endpoints\n\nV. Once the information has been entered, click on Run test under Test\n\nendpoint connection (optional). When the status turns to successful,\n\nclick on Create endpoint.\n\n5. Create DMS task:\n\nI. Go to Tasks and click on Create task.\n\nII. Provide a task identi\u0000er and select the replication instance,\n\nsource endpoint, and target endpoint you created.\n\nIII. Choose Migrate existing data as the migration type.\n\nIV. Select the tables or schemas you want to extract.\n\nV. Optionally, con\u0000gure task settings such as logging, error\n\nhandling, and validation.",
      "content_length": 583,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 752,
      "content": "Figure 12.13 – DMS task\n\n6. Start task:\n\nI. Review the task settings and click on Start task.\n\nII. AWS DMS will initiate the extraction process, transferring data\n\nfrom your source database to the S3 bucket.",
      "content_length": 207,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 753,
      "content": "Figure 12.14 – DMS task running\n\n7. Monitor and verify:\n\nI. Monitor the task progress in the DMS console.\n\nII. Check the S3 bucket to ensure data is being extracted as\n\nexpected.\n\nIII. Review task logs for any errors or warnings.\n\nTYPE OF MIGRATIONS\n\nThere are the following migrations types:\n\nLarge migrations: For very large migrations, consider using AWS SCT’s\n\nreplication agent in conjunction with AWS DMS to stream data directly to\n\nthe target database or a Snowball Edge device\n\nHeterogeneous migrations: If you’re migrating between diﬀerent\n\ndatabase types, AWS SCT can assist with schema conversion before using\n\nDMS for data extraction\n\nData transformations: AWS DMS supports various data\n\ntransformations, such as ﬁltering, masking, and converting data types,\n\nwhich can be applied during extraction\n\nroughout this recipe, we explored key components such as the\n\nreplication instance, source and target endpoints, and various\n\nmigration strategies, helping you understand how to successfully\n\nplan and execute database migrations to AWS. By leveraging DMS\n\nalongside complementary tools like the AWS SCT, you can ensure a",
      "content_length": 1133,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 754,
      "content": "smooth transition of both data and schema, while maintaining high\n\navailability and data integrity. As you plan your migration, AWS\n\nDMS empowers you to scale your data workloads eﬀortlessly,\n\noptimizing performance and cost in the cloud.\n\nSee also\n\nMigrating an On-Premises Oracle Database to Amazon Aurora MySQL:\n\nhttps://docs.aws.amazon.com/dms/latest/sbs/chap-on-\n\npremoracle2aurora.html\n\nLive example – migrating an Oracle database from a local laptop to AWS RDS using AWS SCT\n\nIn this recipe, we will walk through the process of migrating an\n\nOracle database installed on your local laptop to AWS RDS using\n\nAWS SCT. is involves installing Oracle on your laptop, setting up\n\nthe AWS RDS Oracle instance, using SCT for schema conversion,\n\nand completing the migration process with AWS DMS.\n\nGetting ready\n\nBefore diving into data extraction with AWS DMS, ensure you have\n\nthe following in place:",
      "content_length": 901,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 755,
      "content": "AWS account with the necessary permissions (RDS, DMS, SCT, and\n\nnetworking)\n\nProper networking con\u0000gurations to ensure connectivity between your\n\nlocal machine and the AWS RDS instance\n\nHow to do it…\n\nInstalling the Oracle database on your laptop\n\n1. Download the Oracle database installer:\n\nI. Visit the Oracle website\n\n(https://www.oracle.com/database/technologies/oracle-\n\ndatabase-soware-downloads.html).\n\nII. Download the database soware (choose the version suitable\n\nfor your OS such as Windows or macOS).\n\n2. Install Oracle database: Follow the installation instructions provided by\n\nOracle to install the database on your laptop. You’ll need to do the\n\nfollowing:\n\nI. Set up a System Identi\u0000er (SID) for your Oracle instance.\n\nII. Con\u0000gure the listener for database connections (usually port\n\n1521).\n\nIII. Create a database username and password for access (such as\n\nsystem or another DBA account).",
      "content_length": 908,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 756,
      "content": "3. Verify Oracle database: Aer installation, verify that the Oracle Database\n\nis running correctly by logging into the database using Oracle SQL\n\nDeveloper. For detailed steps on how to con\u0000gure SQL Developer, you\n\ncan visit https://docs.oracle.com/en/cloud/paas/exadata-express-\n\ncloud/csdbp/connect-sql-developer.html#GUID-00D45398-2BF3-48D5-\n\nB0E9-11979D5EAFFC.\n\nSetting up an AWS RDS Oracle instance\n\n1. Create an RDS instance: Log into your AWS Management Console, go to\n\nRDS Dashboard, and click on Create Database.\n\n2. Select Oracle: Choose Oracle as the database engine. Select the desired\n\nversion of Oracle, keeping it compatible with your local installation.\n\n3. Con\u0000gure instance settings: Con\u0000gure your database settings:\n\nDB instance class: Choose the appropriate instance size (for\n\nexample, db.t3.medium)\n\nStorage: Choose the storage size (based on your data size)\n\nMaster username/password: Set up the admin credentials for\n\nthe RDS instance\n\n4. Security and network:\n\nI. Ensure the RDS instance is accessible by con\u0000guring the VPC\n\nand Security Group options to allow incoming traﬃc on port\n\n1521.",
      "content_length": 1116,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 757,
      "content": "II. If you’re migrating from your laptop, you’ll need to con\u0000gure\n\nthe laptop’s IP address in the security group to allow access.\n\n5. Launch RDS instance: Click on Create database and wait for the RDS\n\ninstance to be created. Note down the endpoint of your RDS Oracle\n\ninstance for later use.\n\nInstalling and setting up AWS SCT\n\n1. Download and install SCT:\n\nI. Download SCT from the AWS SCT download page\n\n(https://aws.amazon.com/dms/schema-conversion-tool/) for\n\nyour OS and install it on your laptop.\n\nII. You need to follow the detailed steps given in the previous\n\nrecipe, Creating SCT migration assessment report with AWS\n\nSCT.\n\n2. Launch SCT: Open SCT and create a new project. Go to File | New\n\nProject, name your project, and select Oracle as the source database.\n\n3. Connect to Oracle on Laptop:\n\nI. Add a connection to your local Oracle database:\n\nEndpoint: localhost\n\nPort: 1521\n\nUsername: System or another user with DBA privileges\n\nSID: Use the SID or service name of your local Oracle\n\ndatabase",
      "content_length": 1009,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 758,
      "content": "II. Test the connection to ensure it is working correctly.\n\n4. Connect to AWS RDS Oracle:\n\nI. Add a target database connection to your RDS instance:\n\nEndpoint: Use the RDS endpoint from the AWS\n\nconsole\n\nPort: 1521\n\nUsername/Password: Use the credentials you set\n\nduring RDS instance creation\n\nII. Test the connection to ensure it is properly con\u0000gured.\n\nSchema conversion with SCT\n\n1. Convert the schema:\n\nI. Aer setting up connections to both your local Oracle and RDS\n\nOracle, SCT will display the schemas in your local Oracle\n\ndatabase.\n\nII. Select the schema(s) or objects you wish to migrate (tables,\n\nviews, procedures, and so on).\n\nIII. Right-click and select Convert schema. SCT will attempt to\n\nconvert the schema into a format compatible with the AWS\n\nRDS Oracle database.\n\n2. Analyze compatibility: Review the compatibility report generated by\n\nSCT. If there are issues with converting certain features, SCT will",
      "content_length": 925,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 759,
      "content": "highlight them. You may need to manually edit some parts of the\n\nschema if they are incompatible with RDS.\n\n3. Apply the schema to RDS:\n\nI. Once the conversion is complete, right-click on the converted\n\nschema and select Apply to database.\n\nII. SCT will then apply the converted schema to your AWS RDS\n\nOracle instance, creating tables, indexes, and other schema\n\nobjects in the cloud.\n\nData migration with AWS DMS\n\n1. Set up a DMS replication instance: Go to the AWS DMS Console and\n\nclick on Create replication instance. is instance serves as the engine\n\nfor migrating your data between the source (local Oracle) and target\n\n(RDS Oracle).\n\nFor all con\u0000guration steps, please refer to the detailed steps in\n\nthe Extracting data with AWS DMS recipe.\n\n2. Create source endpoint: In the DMS console, create a source endpoint\n\nusing the connection details of your local Oracle database:\n\nEndpoint type: Source\n\nDatabase engine: Oracle\n\nServer name: Your laptop’s IP address or localhost (if\n\ncon\u0000gured)\n\nPort: 1521",
      "content_length": 1012,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 760,
      "content": "Username/Password: Oracle DB credentials\n\n3. Create target endpoint: Create a target endpoint pointing to your AWS\n\nRDS Oracle instance:\n\nEndpoint type: Target\n\nDatabase engine: Oracle\n\nServer name: RDS Endpoint.\n\nPort: 1521\n\nUsername/Password: RDS admin credentials.\n\n4. Create a migration task: Create a new migration task in DMS to migrate\n\ndata from your local Oracle database to your RDS Oracle instance:\n\nI. Choose whether to perform a full load or continuous data\n\nreplication.\n\nII. Select the tables or schemas to migrate.\n\n5. Start the migration task: Start the task and monitor its progress through\n\nthe DMS console. DMS will transfer the data from your local Oracle\n\ndatabase to the AWS RDS Oracle instance.\n\nVerification and post-migration steps\n\n1. Verify schema and data:\n\nI. Once the migration task is complete, use an Oracle SQL client\n\nsuch as SQL*Plus or Oracle SQL Developer to log into your\n\nAWS RDS Oracle instance.",
      "content_length": 936,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 761,
      "content": "II. Verify that all schema objects (tables, indexes, and so on) have\n\nbeen created and that the data has been migrated successfully.\n\n2. Update application con\u0000guration: If your application was pointing to\n\nyour local Oracle database, update it to point to the new AWS RDS\n\nOracle endpoint.\n\n3. Monitoring and tuning: Monitor your RDS instance performance and\n\n\u0000ne-tune parameters such as storage auto-scaling, backup policies, and\n\nperformance metrics using Amazon CloudWatch.\n\nBy following these detailed steps, you have successfully migrated an\n\nOracle database from your local laptop to AWS RDS using AWS\n\nSCT for schema conversion and AWS DMS for data migration. is\n\napproach minimizes downtime and ensures a smooth migration to\n\nthe cloud.\n\nLeveraging AWS Snow Family for large-scale data migration\n\nIn the dynamic landscape of data management and migration,\n\nbusinesses oen face the challenge of moving massive volumes of\n\ndata securely and eﬃciently. AWS oﬀers a suite of physical devices\n\nunder the Snow Family, designed to facilitate large-scale data\n\ntransfers.",
      "content_length": 1074,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 762,
      "content": "is recipe explores the scenarios in which AWS Snow Family\n\nproducts should be considered for data migration. Additionally, we\n\nwill delve into the Snow Family Large Data Migration Manager, a\n\nvaluable tool for managing migrations of 500 TB or more.\n\nAWS Snow Family comprises a range of physical devices designed\n\nto transfer large amounts of data to and from AWS. ese devices\n\ninclude Snowcone, Snowball Edge, and Snowmobile, each tailored\n\nfor diﬀerent scales and types of data migration.\n\nHere’s a comparison of the key features of Snowball, Snowball Edge,\n\nand Snowmobile:\n\nSnowball:\n\nCapacity: 80 TB\n\nNetworking: 10 GB+ network connection\n\nIdeal for: Medium-sized data transfers\n\nSnowball Edge:\n\nCapacity: 100 TB\n\nNetworking: 10 GB+ network connection\n\nIdeal for: Medium to large-scale data transfers and edge\n\ncomputing\n\nSnowmobile:",
      "content_length": 840,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 763,
      "content": "Capacity: Exabyte-scale\n\nIdeal for: Extremely large data transfers\n\nThe Snow Family Large Data Migration Manager as a service\n\nis service simpli\u0000es the process of migrating petabytes of data\n\nfrom your on-premises data centers to AWS. It eliminates the need\n\nfor manual tracking and provides a centralized platform to manage\n\nmultiple Snowball Edge or Snowmobile jobs simultaneously.\n\nLet’s talk about choosing the right device:\n\nConsider data size: Choose Snowball for medium-sized data, Snowball\n\nEdge for medium to large, and Snowmobile for exabyte-scale\n\nEvaluate edge computing requirements: Snowball Edge oﬀers both\n\nstorage and compute capabilities\n\nPrioritize security: Snowmobile provides the highest level of security\n\nBy understanding these diﬀerences, you can select the most\n\nappropriate Snow Family device for your data transfer needs.\n\nA typical Snowball import job \u0000ow is as follows:",
      "content_length": 900,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 764,
      "content": "Figure 12.15 – Snow family job ﬂow\n\nLet’s walk through the process of ordering AWS Snow Family\n\ndevices:\n\n1. Request: In the AWS web console, order one or more snowballs to your\n\nlocation.\n\n2. Setup:\n\nI. Install the Snowball client app on a workstation.\n\nII. Do an import speed test.\n\nIII. Connect the Snowball to your local network.\n\n3. Load it:\n\nI. Download the job manifest \u0000le from the AWS web console.\n\nII. Connect the Snowball client to the snowball device.\n\nIII. Start the import process.\n\n4. Ship it:\n\nI. Power oﬀ and close the snowball device.\n\nII. Arrange for UPS pickup. Shipping is prepaid.",
      "content_length": 602,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 765,
      "content": "III. UPS provides a tracking number.\n\n5. S3 import:\n\nI. AWS loads the Snowball contents into your S3 bucket.\n\nII. S3 access is controlled via the IAM role and bucket policy.\n\nSubsteps involved in using the Large Data Migration Manager\n\ninclude the following:\n\n1. Create a data migration plan:\n\nI. Access the AWS Snow Family Management Console.\n\nII. Provide details about your data migration goals, including the\n\nfollowing:\n\nTotal data size to be migrated\n\nDesired migration timeframe\n\nNumber of Snow devices you plan to use\n\ne Large Data Migration Manager will analyze your input and\n\ngenerate a projected schedule for your migration project. It will\n\nalso recommend a job ordering schedule to optimize the process\n\nand meet your deadlines.\n\n2. Create and monitor jobs:\n\nI. Based on the recommended schedule, create individual jobs for\n\neach Snow device.",
      "content_length": 856,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 766,
      "content": "II. Specify the source and destination locations for your data.\n\nIII. Track the status of each job in real time through the console.\n\nYou can monitor progress, identify any issues, and receive\n\nnoti\u0000cations about important events.\n\n3. Manage and track devices:\n\nI. e service provides tools to manage the Snow devices\n\nassociated with your migration plan.\n\nII. Track the physical location of devices, their status (for example,\n\nin transit, at AWS, and so on), and any maintenance updates.\n\n4. Optimize and adjust:\n\nI. e Large Data Migration Manager allows you to adjust your\n\nplan as needed.\n\nII. If your data size or timeframe changes, you can modify the plan\n\nand the service will update the projected schedule and job\n\nordering recommendations.\n\nGetting ready\n\nLet’s get started with the AWS Snow Family! First, here’s what you’ll\n\nneed to have in place:\n\nAWS account:\n\nActive account: An active AWS account is required to create\n\nand manage Snow Family jobs",
      "content_length": 963,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 767,
      "content": "Permissions: Ensure your IAM user or role has the necessary\n\npermissions to create Snow Family jobs, interact with S3\n\nbuckets, and use other relevant AWS services (for example,\n\nKMS for encryption)\n\nData preparation:\n\nIdentify data: Determine the data you want to transfer and its\n\nlocation (on-premises or in the cloud)\n\nEstimate size: Estimate the total data size accurately to order the\n\nappropriate Snow Family device or cluster\n\nData format: Ensure your data is in a format compatible with\n\nAWS S3 or other supported data formats\n\nNetwork and logistics:\n\nNetwork connectivity: Ensure your location has adequate\n\nnetwork connectivity to support data transfer to/from the\n\nSnowball device\n\nShipping address: Provide a valid shipping address that can\n\nreceive and ship large packages\n\nPhysical space: Allocate suﬃcient space to accommodate the\n\nSnowball device, especially for larger Snowball Edge or\n\nSnowmobile options\n\nSoware and tools:",
      "content_length": 943,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 768,
      "content": "AWS Snow Family client: Download and install the appropriate\n\nAWS Snow Family client soware for your operating system\n\nData transfer tools: If you’re not using the Snow Family client,\n\nensure you have compatible tools for data transfer (for example,\n\nS3Cmd and AWS s3 sync)\n\nHow to do it…\n\n1. Plan your migration: Before starting your migration, ensure you have\n\nthe following prepared:\n\nI. Estimate data size: Measure how much data you need to\n\ntransfer. If it’s 500 TB or more, AWS will recommend using\n\nmultiple Snowball or Snowball Edge devices, or even\n\nSnowmobile for petabyte-scale transfers.\n\nII. Identify source data: Determine where the data is currently\n\nstored (on-premises, data center, and so on) and the size of each\n\ndataset.\n\nIII. Decide target location: Decide where you want the data stored\n\nin AWS, typically in Amazon S3 or another AWS service.\n\nIV. Choose your Snow Family device:\n\nSnowball: Ideal for data transfers ranging from 10 TB\n\nto multiple petabytes\n\nSnowball Edge: Includes both storage and local\n\ncompute capabilities, making it suitable for edge\n\nprocessing before migration",
      "content_length": 1109,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 769,
      "content": "Snowmobile: e best option for migrations of 100 PB\n\nor more, typically used in large-scale data center\n\nmigrations\n\n2. Create a Large Data Migration Job in AWS Snow Family Console:\n\nI. Log in to the AWS Snow Family Console: Go to the AWS Snow\n\nFamily section in the AWS Management Console.\n\nII. Create a new job: Click on Create Job to start the migration\n\nprocess.\n\nIII. Select job type: Choose between Import into Amazon S3 (if\n\nyou’re transferring data to AWS) or Export from Amazon S3 (if\n\nyou’re moving data from AWS to your location).\n\nFigure 12.16 – AWS snow creates new job",
      "content_length": 582,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 770,
      "content": "IV. Choose the Snowball device: Under Device options, choose Snowball\n\nEdge devices, which are designed for large data migrations (such as\n\nSnowball Edge Storage Optimized or Compute Optimized).\n\nV. Select the Large Data Migration option: In the Job options section, you\n\nwill be asked to specify your use case. Choose Large Data Migration to\n\nindicate that you’re using Snowball to transfer large volumes of data (for\n\ndata migrations of 500 TB or more, choose Large Data Migration\n\nManager).\n\nVI. De\u0000ne job details:\n\nS3 bucket: Select the S3 bucket where you want to transfer the\n\ndata.\n\nEncryption: Select or create an AWS KMS key for encryption.\n\nis key ensures that all data is encrypted while in transit.\n\nAdd IAM: Create and add IM policy to the job. Please check the\n\nsample policy in the following GitHub path:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-\n\nAWS-Cookbook/blob/main/Chapter12/snowball-policy.json.\n\nShipping details: Provide the shipping address for where AWS\n\nwill deliver the Snowball or Snowball Edge devices.",
      "content_length": 1050,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 771,
      "content": "Figure 12.17 – AWS job details\n\n3. AWS ships the devices to you:\n\nI. Receive Snowball devices: AWS will ship the required number\n\nof Snowball or Snowball Edge devices to your location based on\n\nyour data size. You can track the shipment via the AWS console.",
      "content_length": 257,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 772,
      "content": "II. Unpack and set up devices: Follow the instructions provided\n\nwith the devices to set them up in your data center. You will\n\nneed to plug them into your network.\n\n4. Load data onto Snowball devices:\n\nI. Install the AWS Snowball Client:\n\ni. AWS provides a Snowball Client that allows you to\n\ntransfer data to the Snowball device using a\n\nCommand-Line Interface (CLI).\n\nii. Download the client from the AWS Snow Family\n\ndocumentation\n\n(https://docs.aws.amazon.com/snowball/).\n\nII. Connect to Snowball: Use the Snowball Client to connect to the\n\ndevice. You will need the job ID and unlock code (provided in\n\nthe AWS console).\n\nIII. Start data transfer: Begin loading data onto the Snowball using\n\nthe CLI or APIs. For large datasets, consider segmenting the\n\ndata to load it in parallel across multiple devices. Here’s an\n\nexample command for transferring data:\n\nbash snowball cp /path/to/data snowball://snowball_device/data -- recursive\n\nIV. Verify data transfer: Ensure all data has been transferred\n\nsuccessfully. Use checksums or other data validation techniques",
      "content_length": 1068,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 773,
      "content": "to verify data integrity.\n\n5. Ship devices back to AWS\n\nI. Pack the Snowball devices: Aer completing the data transfer,\n\npack the devices securely using the provided packaging\n\nmaterials.\n\nII. Return shipment: AWS provides pre-paid shipping labels.\n\nAttach them to the Snowball devices and schedule a pickup or\n\ndrop them oﬀ at the speci\u0000ed shipping carrier.\n\nIII. Track the return: Track the shipment via the AWS console to\n\nmonitor the device’s return to AWS data centers.\n\n6. Data import to Amazon S3:\n\nI. AWS receives the devices: Once AWS receives the devices, they\n\nwill automatically begin transferring the data to the speci\u0000ed\n\nAmazon S3 bucket.\n\nII. Monitor progress: Use the AWS Snow Family Console to\n\nmonitor the progress of the data transfer. You can check\n\nwhether the import is complete or whether any issues have\n\noccurred.\n\nIII. Validation: AWS will validate the integrity of the data and\n\nnotify you once the transfer is successful.\n\n7. Post-migration operations",
      "content_length": 981,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 774,
      "content": "I. Access data in Amazon S3: Once the data transfer is complete,\n\nyou can access your data in Amazon S3. is data can now be\n\nused for further processing, analysis, or storage.\n\nII. Edge compute results (optional): If you used Snowball Edge for\n\nedge computing, you can also access the results of any compute\n\ntasks that were performed locally during the migration.\n\nIII. Data life cycle management: Con\u0000gure S3 storage classes, life\n\ncycle policies, and versioning to optimize your storage costs.\n\nMigrating 500 TB or more of data using the AWS Snow Family is a\n\nsecure, eﬃcient way to handle large-scale data migrations where\n\nnetwork transfer is impractical. By leveraging the Large Data\n\nMigration Manager, you can streamline the process of managing\n\nmultiple Snowball devices, ensuring that your data transfer is\n\nseamless and secure. Whether you’re moving to Amazon S3,\n\nperforming edge compute tasks, or managing long-term storage,\n\nAWS Snow Family provides the tools needed to perform large-scale\n\ndata migrations with minimal impact on your operations.\n\nKEY CONSIDERATIONS\n\nHere are a few key considerations:\n\nPlanning: Accurate data estimation is crucial to avoid ordering too few\n\nor too many devices\n\nData preparation: Ensure your data is organized and ready for transfer\n\nbefore the devices arrive",
      "content_length": 1310,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 775,
      "content": "Security: Use strong encryption keys and IAM roles to protect your data\n\nduring transit and in the cloud\n\nNetwork: Adequate network bandwidth is essential for eﬃcient data\n\ntransfer\n\nTracking: Use the Large Data Migration Manager’s dashboard to\n\nmonitor the progress of your migration and address any issues promptly\n\nSee also\n\nAWS Snowball Edge Primer:\n\nhttps://explore.skillbuilder.aws/learn/course/external/view/elearning/45/\n\naws-snowball-edge-getting-started\n\nAWS Snowball Edge Logistics and Planning:\n\nhttps://explore.skillbuilder.aws/learn/course/external/view/elearning/11\n\n5/aws-snowball-edge-logistics-and-planning\n\nOceanofPDF.com",
      "content_length": 640,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 776,
      "content": "13 Strategizing Hadoop Migrations – Cost, Data, and Workflow Modernization with AWS\n\nMigrating large-scale data and machine learning processes to cloud\n\nplatforms such as Amazon Web Services (AWS) brings multiple\n\nbene\u0000ts. AWS and similar services oﬀer a wide variety of on-\n\ndemand computing resources, cost-eﬀective and durable storage\n\nsolutions, and managed environments that are up to date and user-\n\nfriendly for big data applications. is setup allows data engineers,\n\ndevelopers, data scientists, and IT staﬀ to concentrate on data\n\npreparation and insight extraction.\n\nTools such as Amazon Elastic MapReduce (EMR), AWS Glue, and\n\nAmazon Simple Storage Service (S3) facilitate the separation and\n\nindependent scaling of computing and storage resources while\n\nensuring a cohesive, robust, and well-managed environment. is\n\nsigni\u0000cantly reduces many of the challenges associated with on-\n\npremises methods. Adopting this cloud-based approach results in\n\nquicker, more \u0000exible, user-friendly, and cost-eﬀective solutions for\n\nbig data and data lake projects.",
      "content_length": 1064,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 777,
      "content": "In this chapter, we’ll guide you through various strategies for\n\nmigration, covering how to transfer your cluster data, catalog\n\nmetadata, extract, transform, and load (ETL) jobs, and work\u0000ow\n\nservices. You’ll also discover methods to incorporate quality checks\n\nto ensure your migration is successful.\n\ne recipes we’ll explore include the following:\n\nCalculating total cost of ownership (TCO) using AWS TCO calculators\n\nConducting a Hadoop migration assessment using the TCO simulator\n\nSelecting how to store your data\n\nMigrating on-premises HDFS data using AWS DataSync\n\nMigrating the Hive Metastore to AWS\n\nMigrating and running Apache Oozie work\u0000ows on Amazon EMR\n\nMigrating an Oozie database to the Amazon RDS MySQL\n\nSetting up networking – establishing a secure connection to your EMR\n\ncluster\n\nPerforming a seamless HBase migration to AWS\n\nMigrating HBase to DynamoDB on AWS\n\nGaining insight into these areas will equip you with a\n\ncomprehensive understanding of the crucial factors to consider\n\nduring cloud migration planning. Additionally, you’ll learn about",
      "content_length": 1069,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 778,
      "content": "the necessary modi\u0000cations to adapt your existing applications for a\n\ncloud-native architecture.\n\nTechnical requirements\n\nMake sure you have an active AWS account. If you don’t already\n\nhave one, sign up for an AWS account at\n\nhttps://aws.amazon.com/resources/create-account/ before\n\nproceeding.\n\nYou can access the code for this project in the GitHub repository:\n\nhttps://github.com/PacktPublishing/Data-Engineering-with-AWS-\n\nCookbook/tree/main/Chapter13.\n\nCalculating total cost of ownership (TCO) using AWS TCO calculators\n\nNavigating the \u0000nancial landscape of cloud migration can be\n\ncomplex. AWS total cost of ownership (TCO) calculators simplify\n\nthis process by oﬀering an estimate of your potential cost savings\n\nwhen transitioning from on-premises infrastructure to AWS cloud\n\nservices. ese tools allow you to compare the expenses associated\n\nwith maintaining your current infrastructure to the projected costs\n\nof running similar workloads on AWS.",
      "content_length": 959,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 779,
      "content": "By factoring in variables such as server costs, storage, networking,\n\nsoware licenses, and even personnel expenses, TCO calculators\n\npaint a comprehensive picture of your current IT expenditure. ey\n\nthen contrast this with the potential costs of AWS, considering\n\nvarious pricing models, usage patterns, and optimizations available\n\nin the cloud.\n\nWhile a TCO calculator provides a useful starting point, it’s crucial\n\nto consider it as an estimate. Your actual costs will depend on your\n\nunique con\u0000guration and how you use the service and\n\ncon\u0000guration. Nonetheless, these tools serve as a crucial guide in\n\nyour cloud migration journey, helping you make informed\n\ndecisions based on potential cost savings and a clearer\n\nunderstanding of the \u0000nancial implications of moving to AWS.\n\nIn this recipe, we aim to guide you through the process of\n\nevaluating the cost of migrating an on-premises Hadoop cluster to\n\nAmazon EMR using the AWS TCO calculator. By following a step-\n\nby-step approach, you will learn how to input your current\n\ninfrastructure details, compare on-premises costs with AWS\n\nservices, and assess potential savings.\n\nIn this migration from an on-premises Hadoop cluster to Amazon\n\nEMR, we assume a 10-server Hadoop setup with 200 TB of Hadoop\n\nDistributed File System (HDFS) storage. e EMR cluster is\n\ncon\u0000gured with r5.2xlarge instances for master, core, and task",
      "content_length": 1387,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 780,
      "content": "nodes. Amazon S3 will replace HDFS for data storage, with 200 TB\n\nof data transferred during the initial migration. Additionally, 500\n\nGB of Amazon Elastic Block Store (EBS) storage is allocated per\n\nEMR instance for local storage needs.\n\nGetting ready\n\nTo facilitate the migration decision-making process, ensure that you\n\ncollect the following metrics from your current Hadoop clusters:\n\nTotal count of physical CPUs\n\nCPU clock speed and core counts\n\nTotal memory capacity\n\nHDFS storage volume (excluding replication)\n\nMaximum aggregate network throughput\n\nUtilization graphs span at least one week for the resources\n\nCurrent Hadoop cluster inventory: Detailed information on the existing\n\nHadoop cluster, including the following:\n\nHardware speci\u0000cations (servers, storage, and network)\n\nSoware versions (Hadoop distribution, related tools, and so\n\non)\n\nCurrent utilization and performance metrics",
      "content_length": 900,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 781,
      "content": "AWS cost estimation tools: Access to AWS Pricing Calculator, AWS Cost\n\nExplorer, or similar tools to estimate the cost of running equivalent\n\nworkloads on AWS\n\nTCO analysis framework: A methodology or tool to compare on-\n\npremises costs (hardware, soware, maintenance, and personnel) against\n\nestimated AWS costs\n\nHow to do it…\n\nHere’s a step-by-step guide on how to use the TCO calculator to\n\nassess the cost of moving your Hadoop cluster to AWS:\n\n1. Access the AWS TCO calculator:\n\nI. Navigate to https://calculator.aws/#/.\n\nII. Click on Create estimate.\n\nFigure 13.1 – The pricing calculator",
      "content_length": 595,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 782,
      "content": "2. Input on-premises Cloudera Hadoop details:\n\nI. Select the workload: Choose Hadoop (for example, Cloudera or\n\nHortonworks) as the current workload.\n\nII. Add server details: Let’s assume you have 10 physical servers\n\nrunning your Cloudera Hadoop cluster. Enter the following\n\nserver details in the calculator:\n\nNumber of servers: 10\n\nCPU cores: 64 cores per server\n\nMemory: 256 GB RAM per server\n\nStorage: 20 TB per server (SSD or HDD)\n\nUtilization: Choose High if your servers are running at\n\nhigh capacity\n\nIII. Storage details: Let’s assume that the HDFS storage on your\n\nHadoop cluster amounts to 200 TB in total. Specify the type of\n\nstorage used for Hadoop, whether SSD or HDD.\n\nIV. Network costs: Estimate the network bandwidth used for data\n\ntransfers between your cluster nodes and other systems. In this\n\nexample, assume a bandwidth usage of 10 Gbps.\n\n3. Add additional on-premises costs:\n\nI. Data center costs: For electricity, cooling, and physical space,\n\nassume the following estimates:\n\nPower and cooling: $100 per month per server",
      "content_length": 1047,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 783,
      "content": "Data center space: $500 per month for rack space\n\nII. Labor costs: If you have an IT team managing the Hadoop\n\ncluster, input labor costs. For example:\n\nIT staﬀ: 2 full-time employees, each with a salary of\n\n$100,000/year\n\nCloudera licensing: $50,000/year for Cloudera\n\nManager licenses\n\nIII. Hardware refresh costs: If you refresh hardware every 4-5 years,\n\ninput an average of $10,000 per server for hardware\n\nreplacement.\n\n4. Con\u0000gure the AWS cloud settings for EMR:\n\nI. Amazon EMR cluster setup: Let’s estimate that you will use\n\nAmazon EMR with r5.2xlarge instances for your new\n\nsetup:\n\nNumber of instances: You need 10 Amazon Elastic\n\nCompute Cloud (EC2) instances, similar to your\n\ncurrent on-premises setup\n\nInstance type: Select r5.2xlarge (8 vCPUs, 64 GB\n\nof RAM)\n\nAdd a con\u0000guration for one master node, six core nodes,\n\nand three task nodes",
      "content_length": 853,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 784,
      "content": "II. Amazon S3 for storage: You plan to move your HDFS storage\n\n(200 TB) to Amazon S3. Add the estimated cost of S3 storage\n\nbased on your data size. In this example, you’ll be using\n\nstandard S3 storage for 200 TB of data.\n\nIII. EBS storage for EMR nodes: Specify the amount of EBS storage\n\nneeded for the EMR nodes, assuming 500 GB per instance.\n\nIV. Network and data transfer costs: Estimate the costs of\n\ntransferring your data from on-premises to AWS using AWS\n\nDirect Connect or public internet. Assume you’ll transfer 200\n\nTB of data initially.\n\nV. Additional AWS services: You may use AWS Glue for ETL\n\nprocessing or Amazon CloudWatch for monitoring. Add these\n\ncosts based on your projected usage.\n\nVI. Auto-scaling for EMR: If you plan to use auto-scaling, con\u0000gure\n\nyour scaling policy (for example, scaling up to 20 instances\n\nduring peak times).\n\nOnce you’ve entered all the required information, the TCO\n\ncalculator will generate the estimated total cost:",
      "content_length": 968,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 785,
      "content": "Figure 13.2 – The pricing calculator estimate summary\n\n5. Run the TCO calculation: Aer inputting the required details for both\n\nyour on-premises Cloudera Hadoop cluster and your Amazon EMR\n\ncon\u0000guration, click on Calculate TCO:\n\nI. View the results: e AWS TCO calculator will provide a\n\ncomparison of the cost of running your Hadoop cluster on-\n\npremises versus running it on Amazon EMR. You will see a\n\nbreakdown of the costs, such as the following:\n\nCompute costs (for EMR EC2 instances)\n\nStorage costs (for S3 and EBS)\n\nNetworking costs (for data transfer)\n\nLabor and licensing savings (since Cloudera licensing\n\nand IT staﬀ costs may decrease)\n\n6. Review and analyze the results:",
      "content_length": 685,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 786,
      "content": "I. Estimated savings: e TCO report will show how much you\n\ncould potentially save by moving to Amazon EMR. For\n\nexample, it might show that your on-premises Cloudera cluster\n\ncosts $500,000/year, while running Amazon EMR could cost\n\n$300,000/year, representing a 40% savings.\n\nII. Download the report: You can export the results as a PDF or\n\nExcel \u0000le to share with stakeholders:",
      "content_length": 380,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 787,
      "content": "Figure 13.3 – The pricing calculator estimate summary",
      "content_length": 53,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 788,
      "content": "Summary:\n\nYou have a 10-node on-premises Cloudera Hadoop cluster with 200 TB\n\nof data and a total cost of $500,000/year\n\nMoving to Amazon EMR with r5.2xlarge instances, S3 storage for 200\n\nTB, and auto-scaling con\u0000gured, the estimated annual cost is around\n\n$300,000/year\n\nis migration could potentially save you around 40% in operational\n\nand hardware costs\n\nBy following these steps and inputting similar values into the AWS\n\nTCO calculator, you can create your own estimates for migrating\n\nfrom Cloudera Hadoop to Amazon EMR. is hands-on approach\n\nhelps in understanding the potential cost bene\u0000ts of moving\n\nworkloads to the cloud.\n\nSee also\n\nCreate and con\u0000gure an estimate: https://docs.aws.amazon.com/pricing-\n\ncalculator/latest/userguide/create-con\u0000gure-estimate.html#create-\n\nestimate\n\nConducting a Hadoop migration assessment using the TCO simulator",
      "content_length": 861,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 789,
      "content": "Using AWS’s TCO assessment tool to evaluate the cost of migrating\n\nyour current Hadoop cluster to AWS is an eﬀective approach to\n\nunderstanding and comparing the expenses involved. e TCO\n\nassessment helps you get a comprehensive view of the \u0000nancial\n\nimpact of the migration. Here’s how you can use the AWS TCO\n\nassessment for your Hadoop cluster migration:\n\nFigure 13.4 – The TCO cost-saving summary\n\nHadoop to Amazon EMR TCO simulator\n\ne AWS ProServe Hadoop Migration Delivery Kit (HMDK) TCO\n\ntool is designed to assist organizations in migrating from on-\n\npremises Hadoop clusters to Amazon EMR. is tool helps in\n\nassessing the TCO by simulating the resource usage of future EMR\n\nclusters based on historical Hadoop job data. It provides insights\n\ninto workload patterns, job timelines, and resource utilization,",
      "content_length": 818,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 790,
      "content": "enabling organizations to design cost-eﬀective and optimized EMR\n\nclusters.\n\nFor a more in-depth analysis of migration, AWS professional\n\nservices oﬀer a valuable asset: the Hadoop migration assessment\n\nTCO tool. is tool is now integrated into the AWS ProServe\n\nHMDK. We’ll explore this further in this recipe.\n\nis step-by-step recipe provides a comprehensive approach to\n\nassessing and optimizing the migration of Hadoop workloads to\n\nAmazon EMR using the TCO simulator tool.\n\nGetting ready\n\nBefore using the TCO simulator, ensure that you have the following\n\nprerequisites in place:\n\nAccess to Hadoop cluster logs: You need access to the YARN logs from\n\nyour existing Hadoop cluster. ese logs provide essential data on job\n\nexecution and resource usage, which the TCO tool will analyze.\n\nPython environment: A Python environment is required to run the log\n\ncollector and analyzer scripts. e tool supports execution through both\n\nnative Python environments and Docker containers. You can download\n\nit from Python’s oﬃcial website (https://www.python.org/downloads/).\n\nAWS account: An active AWS account with necessary permissions to\n\ncreate and manage EMR clusters and to use services, such as Amazon\n\nQuickSight for visualization.",
      "content_length": 1236,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 791,
      "content": "Excel: e \u0000nal TCO calculations are done using an Excel template with\n\nmacros, so you need a working installation of Excel that supports macro\n\nexecution.\n\nTechnical requirements are as follows:\n\nYARN log collector: is tool collects logs from the Hadoop YARN\n\nResource Manager. e logs are securely transported using HTTPS and\n\nconverted into CSV format, which serves as input for further analysis.\n\nYARN log analyzer: Aer collecting the logs, this analyzer processes the\n\ndata to extract key metrics such as job timelines, user activity, and\n\nresource usage. ese metrics are visualized using Amazon QuickSight\n\ndashboards.\n\nOptimized TCO calculator: is calculator processes the aggregated log\n\ndata to estimate the costs of running equivalent workloads on Amazon\n\nEMR. e calculator uses inputs such as instance types, storage costs,\n\nand resource allocation to provide detailed cost estimates.\n\nHow to do it…\n\n1. Set up the environment:\n\nI. Clone the repository: Start by cloning the TCO simulator\n\nrepository from GitHub using the following command:\n\ngit clone https://github.com/awslabs/migration- hadoop-to-emr-tco-simulator.git",
      "content_length": 1137,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 792,
      "content": "II. Install Python dependencies: Navigate to the cloned directory\n\nand install the required Python packages. If using a virtual\n\nenvironment, you can do the following:\n\ncd migration-hadoop-to-emr-tco-simulator\n\nUnder the migration-hadoop-to-emr-tco-\n\nsimulator folder, you will \u0000nd\n\nrequirements.txt:\n\npip install -r requirements.txt\n\nIII. Set up Docker (optional): If you prefer using Docker, ensure\n\nDocker is installed on your system. You can follow the\n\ninstructions from Docker’s oﬃcial installation guide:\n\nhttps://www.docker.com/get-started/.\n\n2. Collect YARN logs:\n\nI. Run the YARN log collector: Use the provided script to collect\n\nlogs from your Hadoop cluster. Ensure you have the necessary\n\npermissions to access the YARN ResourceManager API. e\n\ncommand looks like this:\n\npython yarn-log-collector.py --resource- manager <YARN_RESOURCE_MANAGER_ENDPOINT> --output-dir ./logs\n\nYou can \u0000nd details about commands and how to run\n\nthem in the README \u0000le at\n\nhttps://github.com/awslabs/migration-hadoop-to-emr-\n\ntco-simulator/tree/main/yarn-log-collector.",
      "content_length": 1062,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 793,
      "content": "II. Convert logs to CSV: e script will convert the collected JSON\n\nlogs into CSV format, ready for analysis.\n\n3. Analyze logs:\n\nI. Run the log analyzer: Use the yarn-log-analysis script\n\nto analyze the collected logs. is will generate detailed metrics\n\nthat you can use to understand your current Hadoop workload.\n\npython yarn-log-analysis.py --input-dir ./logs --output-dir ./analysis\n\nII. Visualize with Amazon QuickSight: Set up Amazon QuickSight\n\nusing the provided CloudFormation template to visualize the\n\nanalysis results.\n\n4. Run the TCO calculator:\n\nI. Aggregate log data: Use the tco-input-generator.py\n\nscript to aggregate the log data:\n\npython tco-input-generator.py --input- dir ./analysis --output-file tco- input.csv\n\nII. Calculate TCO in Excel: Open the provided Excel template,\n\nenable macros, and input your data. Use the green cells to enter\n\nparameters such as instance types, data size, and other relevant\n\ncon\u0000gurations.",
      "content_length": 944,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 794,
      "content": "5. Interpret results: Use the insights from the QuickSight dashboards and\n\nExcel-based TCO estimates to design your future EMR architecture.\n\nAdjust your cluster design based on workload types, peak usage times,\n\nand resource needs to optimize for both performance and cost.\n\nSee also\n\nAmazon EMR pricing: https://aws.amazon.com/emr/pricing/\n\nIntroducing the AWS ProServe Hadoop Migration Delivery Kit TCO tool:\n\nhttps://aws.amazon.com/blogs/big-data/introducing-the-aws-proserve-\n\nhadoop-migration-delivery-kit-tco-tool/\n\nHadoop Migration Assessment (TCO-Simulator):\n\nhttps://github.com/awslabs/migration-hadoop-to-emr-tco-simulator\n\nSelecting how to store your data\n\nWhen migrating from an on-premises Hadoop cluster to AWS, one\n\nof the crucial decisions to be made is selecting the appropriate\n\nstorage solution for your data. Amazon S3 and HDFS both oﬀer\n\nrobust data storage capabilities, but they diﬀer in their architecture,\n\nfeatures, and use cases. is recipe will help you navigate this choice\n\nby comparing S3 and HDFS, examining their technical\n\nrequirements, and oﬀering guidance on how to make an informed\n\ndecision based on your speci\u0000c needs.",
      "content_length": 1158,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 795,
      "content": "Choosing between Amazon S3 and Hadoop HDFS depends on your\n\nspeci\u0000c use case, performance requirements, and long-term goals.\n\nAmazon S3 oﬀers unmatched scalability and integration with AWS\n\nservices, making it ideal for cloud-native workloads and data lakes.\n\nHDFS, on the other hand, is well suited for high-throughput big\n\ndata processing within a Hadoop ecosystem. By carefully evaluating\n\nyour needs against the capabilities of each storage solution, you can\n\nselect the most appropriate technology for your environment.\n\nHere’s a brief overview of HDFS and S3:\n\nAWS presents Amazon S3 as an object storage service. S3 boasts\n\nindustry-leading attributes, including remarkable scalability, data\n\navailability, security, and performance. is comprehensive feature set\n\nempowers customers to securely store and safeguard vast volumes of\n\ndata, catering to a wide spectrum of use cases. Amazon S3 is best for\n\ncloud-native applications, multi-tenant environments, and when\n\nleveraging other AWS analytics and machine learning services.\n\nHDFS serves as a distributed \u0000lesystem speci\u0000cally craed for the\n\neﬃcient management of extensive data sets spread across multiple\n\nnodes. Within the Apache Hadoop ecosystem, HDFS plays a pivotal role,\n\nensuring robust and high-throughput access to application data while\n\ngracefully managing failures. HDFS is ideal for on-premises\n\ndeployments, traditional data warehousing, and when using a Hadoop-\n\ncentric processing tool.",
      "content_length": 1466,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 796,
      "content": "When deciding between Amazon S3 and Hadoop HDFS for storing\n\nyour data, it’s crucial to consider various factors, such as data size,\n\naccess patterns, cost, performance, scalability, and your speci\u0000c use\n\ncase requirements. Here’s a step-by-step guide to help you make an\n\ninformed choice.\n\nIn an HDFS versus S3 comparison, it’s vital to recognize that each\n\nstorage system possesses its unique strengths and weaknesses,\n\ntailored to distinct scenarios and objectives. HDFS excels in\n\nenvironments prioritizing data locality and replication, while S3\n\nstands out due to its exceptional scalability, durability, and seamless\n\nintegration with various AWS services:\n\nUnderstand the basics of Amazon S3 and Hadoop HDFS:\n\nAmazon S3:\n\nType: Object storage service\n\nScalability: Highly scalable, designed for\n\n99.999999999% durability\n\nAccess: Accessible via HTTP/HTTPS; supports a wide\n\nrange of APIs\n\nCost: Pay-as-you-go pricing; no upfront costs; diﬀerent\n\nstorage classes (Standard, Intelligent-Tiering, Glacier)\n\ndepending on access frequency\n\nPerformance: Optimized for high availability and\n\nscalability, with the ability to manage large volumes of",
      "content_length": 1149,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 797,
      "content": "data\n\nHard limits:\n\nNumber of buckets: You can have up to\n\n100,000 buckets per AWS account\n\nObject size: Individual objects can be up to 5\n\nTB in size\n\nNumber of objects per bucket: ere is no\n\nstrict limit on the number of objects per\n\nbucket, but performance and scalability might\n\nbe aﬀected by extremely large numbers\n\nQuotas:\n\nStorage capacity: e amount of storage you\n\ncan use depends on your AWS account plan\n\nand usage. AWS provides \u0000exible storage\n\noptions, including Standard, Infrequent\n\nAccess (IA), Glacier, and Glacier Deep\n\nArchive.\n\nData transfer: ere are limits on the amount\n\nof data you can transfer in and out of S3.\n\nese limits can vary based on your region\n\nand usage patterns.\n\nAPI requests: e number of API requests you\n\ncan make to S3 is subject to quotas. ese",
      "content_length": 790,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 798,
      "content": "quotas are generally adjusted based on your\n\nusage patterns.\n\nHadoop HDFS:\n\nType: Distributed \u0000lesystem designed for large-scale\n\ndata processing\n\nScalability: Scales across multiple nodes; provides high\n\nthroughput\n\nAccess: Typically accessed within a Hadoop ecosystem;\n\nsupports POSIX-like \u0000le operations\n\nCost: Costs are tied to the underlying hardware,\n\nincluding storage devices and network infrastructure\n\nPerformance: Optimized for batch processing with\n\nhigh throughput; can be con\u0000gured for low-latency\n\naccess depending on the hardware\n\nHard limits:\n\nCluster size: e number of nodes in your\n\nHDFS cluster will determine the overall\n\nstorage capacity and processing power. As you\n\nadd more nodes, your HDFS cluster can scale\n\nto handle larger datasets.\n\nData volume: While HDFS can handle\n\nmassive amounts of data, there are practical\n\nlimits based on the hardware resources of\n\nyour cluster. Factors such as disk space,",
      "content_length": 930,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 799,
      "content": "network bandwidth, and processing power\n\nwill in\u0000uence how much data your HDFS\n\ncluster can eﬀectively store and process.\n\nFile size: Individual \u0000les in HDFS can be very\n\nlarge, but there might be limitations based on\n\nthe underlying \u0000lesystem and hardware.\n\nQuotas: HDFS doesn’t have prede\u0000ned quotas like\n\nAWS S3. However, there are practical considerations\n\nand limitations that can aﬀect your HDFS cluster’s\n\nperformance and capacity:\n\nStorage capacity: e total storage capacity of\n\nyour HDFS cluster is determined by the\n\ncombined storage of all nodes. While there’s\n\nno \u0000xed limit, you might need to add more\n\nnodes to accommodate larger datasets.\n\nNetwork bandwidth: e network bandwidth\n\nbetween nodes in your HDFS cluster can\n\nimpact data transfer speeds. If your network is\n\ncongested, it can aﬀect performance.\n\nProcessing power: e processing power of\n\nyour HDFS nodes will in\u0000uence how quickly\n\ndata can be processed and analyzed. If your\n\nworkloads are demanding, you might need to\n\nupgrade your hardware.\n\nAssess your use case requirements:",
      "content_length": 1056,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 800,
      "content": "Data size and growth:\n\nAmazon S3: Ideal for storing vast amounts of data that\n\ncan scale in\u0000nitely without concern for the underlying\n\ninfrastructure.\n\nHDFS: Suitable for large datasets that are part of a\n\nHadoop ecosystem. However, scaling requires adding\n\nmore hardware nodes.\n\nData access patterns:\n\nAmazon S3: Best for scenarios where data is accessed\n\ninfrequently or requires global access. Suitable for\n\nobject storage, backups, logs, and data lakes.\n\nHDFS: Ideal for high-throughput data processing\n\ntasks, such as big data analytics, where data is\n\nprocessed in large batches within a cluster.\n\nPerformance requirements:\n\nAmazon S3: is provides scalable performance for\n\nread and write operations but is typically slower than\n\nHDFS for high-speed, sequential access required by\n\nsome big data workloads\n\nHDFS: is is optimized for sequential reads and\n\nwrites, making it highly eﬃcient for data processing\n\ntasks such as Spark and MapReduce jobs in an EMR",
      "content_length": 965,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 801,
      "content": "Hadoop cluster, where large-scale data is processed in\n\nparallel.\n\nCost considerations:\n\nAmazon S3: is is cost-eﬀective for storing large\n\nvolumes of data with diﬀerent storage classes to\n\noptimize costs based on access patterns.\n\nHDFS: Costs are higher if you need to manage and\n\nscale the physical hardware and associated\n\ninfrastructure. However, HDFS can be more cost-\n\neﬀective for on-premises deployments if you already\n\nhave the hardware.\n\nScalability and management:\n\nAmazon S3: Automatically scales to accommodate\n\ngrowing data volumes without manual intervention\n\nHDFS: is requires manual management to scale by\n\nadding more nodes to the cluster, which increases\n\noperational complexity\n\nConsider data security and compliance:\n\nAmazon S3 security:\n\nis oﬀers advanced security features such as\n\nencryption at rest (Server-Side Encryption with\n\nAmazon S3-Managed Keys (SSE-S3) and Server-Side\n\nEncryption with AWS Key Management Service-",
      "content_length": 948,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 802,
      "content": "Managed Keys (SSE-KMS)) and in transit (Secure\n\nSockets Layer/Transport Layer Security (SSL/TLS))\n\nis supports \u0000ne-grained access control with Identity\n\nand Access Management (IAM) policies, bucket\n\npolicies, and Access Control List (ACLs)\n\nCompliance certi\u0000cations such as Health Insurance\n\nPortability and Accountability Act (HIPAA), General\n\nData Protection Regulation (GDPR), and others make\n\nit suitable for storing sensitive data\n\nHDFS security:\n\nis provides encryption at rest using Hadoop’s\n\ntransparent encryption features\n\nis supports Kerberos for authentication and Apache\n\nRanger for \u0000ne-grained access control\n\nSecurity depends on the underlying environment;\n\nadditional con\u0000gurations may be needed for\n\ncompliance\n\nEvaluate integration with your ecosystem:\n\nAmazon S3:\n\nis integrates seamlessly with AWS services such as\n\nAWS Lambda, Amazon Athena, Amazon EMR, and\n\nAWS Glue",
      "content_length": 892,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 803,
      "content": "Suitable for building serverless architectures, data\n\nlakes, and integrating with cloud-native applications\n\nis supports native integration with big data tools via\n\nS3 connectors (for example, Spark and Hive)\n\nHDFS:\n\nTight integration with the Hadoop ecosystem and tools\n\nsuch as Apache Hive, Apache HBase, and Apache\n\nSpark\n\nBest for environments where Hadoop is central to data\n\nprocessing, and you rely on its ecosystem for analytics\n\nMatch the solution to your speci\u0000c use case:\n\nUse cases for Amazon S3:\n\nData lakes: Store structured and unstructured data for\n\nanalytics and reporting\n\nBackup and archive: Cost-eﬀective storage for long-\n\nterm backups and archival\n\nWeb content hosting: Serve static content such as\n\nimages, videos, and HTML \u0000les globally\n\nBig data analytics: Store raw data for processing using\n\nAWS services such as Amazon EMR or Athena\n\nUse cases for HDFS:",
      "content_length": 882,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 804,
      "content": "Big data processing: Ideal for running Hadoop-based\n\nanalytics, including MapReduce, Spark jobs, and\n\nmachine learning workloads, providing scalable and\n\nhigh-throughput storage optimized for processing\n\nlarge datasets in a distributed environment\n\nBatch processing: High-throughput data processing for\n\nETL jobs and large-scale data transformations\n\nData warehousing: Used as a storage layer in Hadoop-\n\nbased data warehouses\n\nMake your decision:\n\nChoose Amazon S3 if the following is the case:\n\nYou need highly scalable, durable, and globally\n\naccessible storage\n\nYour data access patterns are varied, and you require\n\n\u0000exibility in storage classes\n\nYou want to minimize infrastructure management and\n\nfocus on cloud-native solutions\n\nChoose HDFS if the following is the case:\n\nYou are operating within a Hadoop ecosystem and\n\nneed high-throughput access to large datasets\n\nYou prefer or require an on-premises solution with\n\ncontrol over the underlying hardware",
      "content_length": 964,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 805,
      "content": "Your use case involves large-scale batch processing\n\nwith speci\u0000c performance requirements\n\nGetting ready\n\ne technical requirements for this recipe are as follows:\n\nAWS account: An active AWS account to provide and manage S3\n\nbuckets and EMR clusters\n\nNetwork connectivity: Secure network connectivity between your on-\n\npremises environment and AWS, if you need to transfer data\n\nData transfer tools: Tools such as AWS DataSync or S3 Transfer\n\nAcceleration for eﬃcient data movement\n\nHow to do it…\n\nHere’s a step-by-step guide to help you make an informed choice.\n\nLet’s start with the steps to implement Amazon S3 storage:\n\n1. Set up Amazon S3 bucket:\n\nI. In the AWS Management Console, navigate to S3 and create a\n\nbucket.\n\nII. De\u0000ne the bucket region, con\u0000gure permissions, and set up\n\npolicies as per your security requirements.\n\n2. Transfer data to S3:",
      "content_length": 858,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 806,
      "content": "I. Use the AWS Command Line Interface (CLI) or S3 console to\n\nupload data.\n\nCommand example: aws s3 cp\n\n/path/to/data s3://your-bucket-name/\n\n--recursive\n\nII. For large-scale data transfer, use AWS DataSync or AWS\n\nSnowball to migrate data eﬃciently from on-premises systems.\n\n3. Enable life cycle policies:\n\nI. De\u0000ne storage policies to automatically move older data to\n\ncheaper storage classes, such as S3 Glacier, or delete unneeded\n\nobjects.\n\nII. Con\u0000gure versioning and replication as needed for backup and\n\ndisaster recovery.\n\n4. Integrate with AWS services:\n\nI. Set up AWS Glue, Athena, or EMR to interact with the data\n\nstored in S3 for analysis and processing.\n\nII. Use Amazon S3 Select to retrieve speci\u0000c data directly from S3\n\nobjects, reducing data transfer and processing overhead.\n\n5. Monitor and manage storage:\n\nI. Use Amazon CloudWatch and S3 Storage Lens to monitor\n\nusage, performance, and storage costs.",
      "content_length": 924,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 807,
      "content": "II. Set up alerts for storage thresholds or unexpected data transfers.\n\ne steps to implement HDFS storage on Amazon EMR are as\n\nfollows:\n\n1. Set up an EMR cluster with HDFS:\n\nI. In the AWS Management Console, navigate to EMR and create\n\na cluster (https://console.aws.amazon.com/emr).\n\nII. Choose Hadoop as the framework and con\u0000gure the cluster\n\nwith HDFS.\n\nIII. Select your instance types (for example, r5.2xlarge for\n\nmaster and core nodes) and con\u0000gure the number of nodes\n\nbased on your processing requirements.\n\n2. Con\u0000gure HDFS on EMR:\n\nI. HDFS is automatically con\u0000gured as the default storage for the\n\nEMR cluster.\n\nII. Data stored on HDFS will be local to the cluster and will not\n\npersist aer cluster termination unless backed up.\n\n3. Upload data to HDFS:\n\nI. Aer the cluster is running, Secure Shell (SSH) into the Master\n\nNode.\n\nII. Use the following command to upload data to HDFS:\n\nbash",
      "content_length": 904,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 808,
      "content": "hadoop fs -put /local/data/path /hdfs/path\n\nIII. For large datasets, use AWS DataSync\n\n(https://docs.aws.amazon.com/datasync/) or DistCp\n\n(https://docs.aws.amazon.com/prescriptive-\n\nguidance/latest/patterns/migrate-data-from-an-on-premises-\n\nhadoop-environment-to-amazon-s3-using-distcp-with-aws-\n\nprivatelink-for-amazon-s3.html) to eﬃciently migrate data to\n\nHDFS.\n\n4. Managing and monitoring HDFS:\n\nI. Use the Hadoop ResourceManager and HDFS NameNode\n\ndashboards in EMR to monitor disk space, replication status,\n\nand node health.\n\nII. Adjust the replication factor for fault tolerance (default is 3):\n\nbash hadoop fs -setrep -R 3 /hdfs/path\n\n5. Job execution on HDFS:\n\nI. Submit your data processing jobs (for example, MapReduce and\n\nSpark) on the EMR cluster using the EMR console or via the\n\ncommand line.\n\nII. e jobs will read and write data directly from and to HDFS.\n\n6. Backup and data persistence: As HDFS data is tied to the EMR cluster,\n\nback up your data to Amazon S3 before shutting down the cluster:",
      "content_length": 1015,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 809,
      "content": "bash hadoop distcp hdfs:///path s3://your-s3- bucket/\n\nBy following these action steps, you can successfully implement and\n\nmanage either HDFS or Amazon S3 based on your storage needs.\n\nSee also\n\nHDFS con\u0000guration:\n\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-\n\ncon\u0000g.html\n\nMigrating on-premises HDFS data using AWS DataSync\n\nMigrating large datasets from an on-premises HDFS environment\n\nto Amazon S3 can be complex, but AWS DataSync simpli\u0000es and\n\naccelerates this process. In this recipe, you’ll learn how to use AWS\n\nDataSync to seamlessly transfer data from Hadoop HDFS to\n\nAmazon S3, ensuring a secure and cost-eﬀective migration.\n\nAWS DataSync automates the tasks involved in data transfers, such\n\nas managing encryption, handling scripts, optimizing networks,\n\nand ensuring data integrity. It supports one-time migrations,\n\nongoing work\u0000ows, and automatic replication for disaster recovery,",
      "content_length": 915,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 810,
      "content": "oﬀering transfer speeds up to 10 times faster than open source\n\ntools.\n\nAWS DataSync supports the following for HDFS:\n\nCopying \u0000les and folders between Hadoop clusters and AWS storage\n\nDataSync agents running external to the cluster\n\nTransferring over internet, Direct Connect, or VPN\n\nEnd-to-end data validation\n\nIncremental transfers, \u0000ltering, and scheduling\n\nKerberos authentication\n\nHadoop in-\u0000ight encryption\n\nHadoop at-rest encryption transparent data encryption (TDE) when\n\nusing simple authentication\n\nGetting ready\n\nTo successfully execute DataSync, the following prerequisites must\n\nbe met:\n\nEnsure that you have an AWS account\n\nYour Hadoop cluster should be operational, and you should have\n\nadministrative access to it\n\nNetwork connectivity between your Hadoop environment and AWS",
      "content_length": 793,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 811,
      "content": "DataSync agent: Installation and con\u0000guration of the DataSync agent on\n\nan on-premises server with access to the HDFS cluster\n\nNetwork connectivity: Ensure network connectivity between the\n\nDataSync agent and your AWS VPC\n\nS3 bucket: An S3 bucket to store the migrated data\n\nIAM role: IAM role for DataSync with permissions to access the S3\n\nbucket and perform data transfer operations\n\nDataSync IAM sample policy\n\nHow to do it...\n\nTo move data from Hadoop HDFS to Amazon S3 using AWS\n\nDataSync, you’ll go through a series of steps that involve setting up\n\nDataSync, con\u0000guring your Hadoop environment, and executing\n\nthe data transfer. Here are the comprehensive steps to walk you\n\nthrough the process:\n\n1. Create a DataSync IAM policy: Create an IAM role with a policy that\n\ngrants DataSync the necessary permissions to access your S3 bucket. e\n\npolicy should look something like this:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\",",
      "content_length": 980,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 812,
      "content": "\"s3:GetObject\", \"s3:ListBucket\", \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::your-s3-bucket-name\", \"arn:aws:s3:::your-s3-bucket-name/*\" ] }, { \"Effect\": \"Allow\", \"Action\": [ \"datasync:StartTaskExecution\", \"datasync:ListTasks\", \"datasync:DescribeTaskExecution\" ], \"Resource\": \"*\" } ]\n\n2. Deploy and con\u0000gure the DataSync agent: Deploy the DataSync agent as\n\na virtual machine (VM) in your environment. e agent is available in\n\nvarious formats, such as Open Virtual Appliance (OVA) for VMware,\n\nHyper-V, and AWS EC2.\n\nI. Open the AWS DataSync console\n\nhttps://console.aws.amazon.com/datasync/.\n\nII. In the le navigation pane, choose Agents, and then choose\n\nCreate agent. e following page will open:",
      "content_length": 705,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 813,
      "content": "Figure 13.5 – Creating a DataSync agent\n\n3. Activate the agent: Aer installing the agent, an activating agent is\n\nneeded. To activate the agent and secure the connection between an\n\nagent and the DataSync service, several network ports should be opened\n\nby the \u0000rewall. For example, if your Hadoop network was inside the on-\n\npremises \u0000rewall, open the outbound traﬃc (Transmission Control\n\nProtocol (TCP) ports 1024–1064) from the DataSync agent to the\n\nVirtual Private Cloud (VPC) endpoint. You also had to open the TCP\n\nport 443 for the entire subnet where the VPC endpoint is located\n\nbecause dynamic IP is assigned to Elastic Network Interface (ENI) for\n\ndata transmission of DataSync. For more detailed network requirements,\n\nyou can refer to the network requirement document",
      "content_length": 782,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 814,
      "content": "(https://docs.aws.amazon.com/datasync/latest/userguide/agent-\n\nrequirements.html):\n\nOnce deployed, activate the agent using the AWS Management\n\nConsole by entering the agent’s IP address. is step registers the\n\nagent with your AWS account.\n\n4. Con\u0000gure network settings: Ensure the agent can connect to both your\n\nHDFS cluster and the AWS DataSync service by con\u0000guring the\n\nnecessary network settings, such as Domain Name System (DNS) and\n\nrouting.\n\nFigure 13.6 – DataSync agent running\n\n5. Create a DataSync task:\n\nI. De\u0000ne source location (HDFS):\n\ni. In the AWS Management Console, navigate to the\n\nDataSync service and create a new task.\n\nii. Select HDFS as the source location and enter the\n\nrequired details, such as the HDFS NameNode address\n\nand the path to the data you want to migrate.",
      "content_length": 796,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 815,
      "content": "II. De\u0000ne destination location (Amazon S3):\n\ni. Select Amazon S3 as the destination location.\n\nii. Choose the S3 bucket you created earlier and specify\n\nthe target directory where the data will be stored.\n\nIII. Con\u0000gure data transfer settings:\n\ni. Set transfer options, including bandwidth throttling,\n\ntask scheduling, and data integrity checks.\n\nii. Enable encryption and con\u0000gure other security\n\nsettings to protect your data during transfer.\n\n6. Start and monitor the data transfer:\n\nI. Start the DataSync task:\n\ni. Start the task manually or schedule it to run at a\n\nspeci\u0000c time.\n\nii. Monitor the progress of the data transfer through the\n\nAWS Management Console, where you can view logs\n\nand statistics.",
      "content_length": 710,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 816,
      "content": "Figure 13.7 – DataSync data transfer monitor\n\nII. Verify data integrity:\n\ni. Once the transfer is complete, verify that all data has been\n\naccurately copied to Amazon S3.\n\nii. Check for any errors or discrepancies in the transfer logs and\n\nresolve them as needed.\n\n7. Verify data transfer:\n\nI. Aer the transfer is complete, verify the data in your S3 bucket\n\nto ensure everything is transferred correctly and completely.\n\nII. Perform any necessary data validation or integrity checks.\n\n8. Post-transfer cleanup and optimization:\n\nI. If it was a one-time transfer, you might want to delete the\n\nDataSync task or agent.\n\nII. For recurring transfers, review the performance and make any\n\nnecessary adjustments.",
      "content_length": 708,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 817,
      "content": "9. Security and compliance:\n\nI. Ensure that your data transfer complies with your organization’s\n\ndata security and compliance policies.\n\nII. Consider encrypting data both in transit and at rest.\n\nMigrating data from HDFS to Amazon S3 using AWS DataSync is a\n\nstraightforward process that signi\u0000cantly reduces the complexity\n\nand time required for large-scale data transfers. By following the\n\nsteps outlined in this recipe, you can eﬃciently and securely move\n\nyour data to the cloud, taking advantage of Amazon S3’s scalability,\n\ndurability, and integration with other AWS services.\n\nSee also\n\nAWS DataSync FAQs: https://aws.amazon.com/datasync/faqs/\n\nPrerequisites to start running the labs: https://cloudone-\n\ndatamigr.awsworkshop.io/20_prerequisites.html\n\nMigrating the Hive Metastore to AWS\n\ne Hive Metastore is a crucial component within Hadoop, acting\n\nas a centralized storehouse for metadata related to Hive tables,\n\nschemas, and partitions. When transitioning your Hadoop cluster\n\nto the AWS cloud, you can opt to either establish a dedicated Hive\n\nMetastore on AWS or utilize the managed AWS Glue Data Catalog.",
      "content_length": 1123,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 818,
      "content": "Migrating to the AWS Glue Data Catalog provides bene\u0000ts such as\n\nschema versioning and eﬃcient integration with Amazon EMR,\n\nespecially for transient clusters. is migration ensures high\n\navailability, fault tolerance, and better data governance.\n\nWhen it comes to data discovery and management, two data\n\ncatalogs are among the most popular choices:\n\nHive Metastore: is repository houses essential information about Hive\n\ntables and their underlying data structures, including partition names\n\nand data types. Hive, an open source data warehousing and analytics\n\ntool built on Hadoop, can be deployed on platforms such as EMR.\n\nAWS Glue Data Catalog: A fully managed service by AWS, the Glue\n\nData Catalog oﬀers a blend of \u0000exibility and reliability. It’s particularly\n\nwell suited for those starting with metastore creation or management, as\n\nit reduces the need for dedicated resources and hands-on con\u0000guration.\n\nIt’s designed for high availability and fault tolerance, ensuring data\n\ndurability through replication and automatic scaling based on usage. It\n\nalso provides granular control over features such as encryption and\n\naccess management.\n\nChoose migration methods as follows:\n\nDirect migration:\n\nUse AWS Glue ETL jobs to extract metadata from the Hive\n\nMetastore and load it into the Glue Data Catalog\n\nStraightforward for smaller datasets",
      "content_length": 1352,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 819,
      "content": "Federation:\n\nConnect AWS Glue to your existing Hive Metastore for\n\nseamless access without full migration\n\nIdeal for maintaining on-premises metastore or gradual\n\nmigration\n\nird-party tools:\n\nExplore tools such as AWS Schema Conversion Tool (SCT) for\n\nspeci\u0000c migration scenarios\n\nGetting ready\n\nBefore you begin the migration process of your Hive Metastore to\n\nthe AWS Glue Data Catalog, ensure that you have met the following\n\nprerequisites:\n\nAWS account: You need an active AWS account with the necessary\n\npermissions to create and manage AWS Glue, Amazon S3, and related\n\nresources.\n\nExisting Hive Metastore: Ensure you have access to your existing Hive\n\nMetastore, which contains the metadata for your Hive tables and\n\nschemas.\n\nAmazon S3 bucket: Create an Amazon S3 bucket in your AWS account\n\nto store any data that needs to be persisted during the migration.",
      "content_length": 867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 820,
      "content": "AWS Glue Data Catalog: Optionally, set up the AWS Glue Data Catalog,\n\nwhich will serve as your centralized metadata repository in the cloud.\n\nis managed service provides a scalable and reliable alternative to a\n\nself-managed Hive Metastore.\n\nNetwork connectivity: Ensure that your on-premises Hadoop cluster,\n\nwhere the Hive Metastore resides, has network connectivity to AWS\n\nservices, including AWS Glue and Amazon S3.\n\nAccess and permissions: Verify that you have the appropriate IAM roles\n\nand policies in place, allowing the migration tool to access the Hive\n\nMetastore, the AWS Glue Data Catalog, and Amazon S3.\n\nBy ful\u0000lling these prerequisites, you’ll be well prepared to migrate\n\nyour Hive Metastore to the AWS Glue Data Catalog, leveraging the\n\nbene\u0000ts of a fully managed, cloud-native metadata repository.\n\nHow to do it…\n\nHere is a step-by-step guide to help you implement metadata\n\nfederation using AWS Glue and Hive Metastore:\n\n1. Create an AWS Glue connection to the Hive Metastore:\n\nI. Set up a JDBC connection in AWS Glue to the Hive Metastore\n\ndatabase (MySQL, PostgreSQL, and so on): Create a Glue\n\nconnection that allows Glue to access your Hive Metastore\n\nusing JDBC:\n\nbash",
      "content_length": 1194,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 821,
      "content": "aws glue create-connection --name hive- metastore-connection \\ --connection-input '{ \"Name\": \"hive-metastore-connection\", \"Description\": \"JDBC connection to Hive Metastore\", \"ConnectionType\": \"JDBC\", \"ConnectionProperties\": { \"JDBC_CONNECTION_URL\": \"jdbc:mysql://<hive-metastore- host>:3306/hive_metastore\", \"USERNAME\": \"hiveuser\", \"PASSWORD\": \"hivepassword\" }, \"PhysicalConnectionRequirements\": { \"AvailabilityZone\": \"us-east- 1a\", \"SubnetId\": \"subnet- 0bb1c79de3EXAMPLE\" } }'\n\nNOTE TO REPLACE\n\n<hive-metastore-host>: The host or IP where your Hive\n\nMetastore database is located\n\nhiveuser: Your Hive Metastore database username\n\nhivepassword: Your Hive Metastore database password\n\nII. Verify the connection to ensure it’s set up correctly:\n\nbash",
      "content_length": 748,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 822,
      "content": "aws glue get-connections --name hive- metastore-connection\n\n2. Con\u0000gure AWS Glue to use the connection:\n\nI. Create an AWS Glue job that will access the Hive Metastore\n\nusing the connection you just set up:\n\nbash aws glue create-job --name GlueFederationJob \\ --role arn:aws:iam::<your-account- id>:role/GlueServiceRole \\ --command '{\"Name\": \"glueetl\", \"ScriptLocation\": \"s3://your- bucket/scripts/hive-metadata- federation.py\"}' \\ --connections '{\"Connections\": [\"hive- metastore-connection\"]}' \\ --default-arguments '{ \"--TempDir\": \"s3://your- bucket/temp/\", \"--enable-glue-datacatalog\": \"true\" }'\n\nLet’s break this down:\n\nScriptLocation: e S3 location of your ETL\n\nscript, which we will de\u0000ne next\n\n--connections: is speci\u0000es the JDBC\n\nconnection to your Hive Metastore\n\n--enable-glue-datacatalog: is enables the\n\nuse of the AWS Glue Data Catalog as the metadata",
      "content_length": 867,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 823,
      "content": "repository\n\nII. Create the ETL script that reads from the Hive Metastore and\n\nperforms metadata federation. Save this script in the S3 bucket\n\nspeci\u0000ed in ScriptLocation.\n\nYou can \u0000nd the full ETL script (hive-metadata-\n\nfederation.py) here:\n\nhttps://github.com/PacktPublishing/Data-Engineering-\n\nwith-AWS-Cookbook/tree/main/Chapter13.\n\n# Reading from the Hive Metastore datasource = glueContext.create_dynamic_frame.from_ca talog( database=\"hive_database_name\", table_name=\"hive_table_name\" ) # Writing data to S3 (you can modify the output path or format) glueContext.write_dynamic_frame.from_opt ions( frame=datasource, connection_type=\"s3\", connection_options={\"path\": \"s3://your-bucket/output-path\"}, format=\"parquet\" ) job.commit()\n\nLet’s break this down:\n\ndatabase and table_name: Replace with your\n\nHive Metastore’s database and table names",
      "content_length": 848,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 824,
      "content": "output-path: Specify the S3 bucket where you\n\nwant to store the output\n\n3. Set up AWS Glue crawlers (optional): If you want AWS Glue to\n\nautomatically crawl your Hive Metastore tables and update the Glue\n\nData Catalog, then perform the following steps:\n\nI. Create a Glue crawler that crawls the data in your Hive\n\nMetastore:\n\nbash aws glue create-crawler --name hive- crawler \\ --role arn:aws:iam::<your-account- id>:role/GlueServiceRole \\ --database-name hive_database \\ --targets '{ \"JdbcTargets\": [{ \"ConnectionName\": \"hive- metastore-connection\", \"Path\": \"hive_metastore\" }] }'\n\nII. Run the crawler to populate the Glue Data Catalog with\n\nmetadata from your Hive Metastore:\n\nbash aws glue start-crawler --name hive- crawler\n\n4. Run the AWS Glue job: Once the Glue job and the script are ready, run\n\nthe job to begin metadata federation:",
      "content_length": 840,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 825,
      "content": "bash aws glue start-job-run --job-name GlueFederationJob\n\n5. Monitor job execution: You can monitor the job’s progress either in the\n\nAWS Glue console or by using the following command:\n\nbash aws glue get-job-runs --job-name GlueFederationJob\n\nis will give you the status of your Glue job, including whether\n\nit succeeded or failed.\n\n6. Verify the federated metadata: Once the Glue job \u0000nishes, do the\n\nfollowing:\n\nI. Check the AWS Glue Data Catalog: You will see the federated\n\nmetadata from your Hive Metastore in the Glue Data Catalog.\n\nII. Validate the results: Make sure that all your Hive tables,\n\nschemas, and partitions are correctly federated and accessible\n\nvia the Glue Data Catalog.\n\nBy following these steps, you have successfully federated your Hive\n\nMetastore metadata with the AWS Glue Data Catalog. is allows\n\nyou to leverage AWS Glue’s scalable, fully managed capabilities\n\nwithout needing to fully migrate your Hive Metastore. e Glue\n\nData Catalog now acts as a centralized metadata store, facilitating",
      "content_length": 1024,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 826,
      "content": "eﬃcient analytics work\u0000ows and seamless access across AWS\n\nservices, such as Amazon Athena, EMR, and Redshi.\n\nMigrating and running Apache Oozie workflows on Amazon EMR\n\nApache Oozie is a popular work\u0000ow scheduler for Hadoop\n\necosystems, orchestrating complex data processing tasks and\n\ndependencies. When migrating from your on-premises Hadoop\n\ncluster to Amazon EMR, you can seamlessly continue using Oozie\n\nor explore alternative AWS services for work\u0000ow orchestration.\n\nIf your migration strategy is “li and shi” and your ETL scripts are\n\nset up to interact with HDFS for both input and output, then your\n\nexisting scripts – including those for Hive, EMR, and Spark –\n\nshould operate eﬀectively in EMR without signi\u0000cant modi\u0000cations.\n\nHowever, if you’ve chosen to re-architect your system during the\n\nmove to AWS and switch to using Amazon S3 as your persistent\n\nstorage layer instead of HDFS, you’ll need to update your scripts.\n\ney must be adapted to work with Amazon S3 (using the s3://\n\nprotocol) via Elastic MapReduce File System (EMRFS).\n\nIn addition to migrating your Hive and Spark scripts, if Apache\n\nOozie is your chosen tool for orchestrating ETL job work\u0000ows, it’s\n\nessential to also strategize for its migration. Let’s explore the",
      "content_length": 1252,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 827,
      "content": "available options for this process and the methodologies for\n\nmigrating to Oozie.\n\nGetting ready\n\nBefore you begin migrating Apache Oozie to Amazon EMR, ensure\n\nthat you meet the following prerequisites:\n\nActive AWS account: You’ll need an active AWS account with the\n\nnecessary permissions to create and manage EMR clusters and related\n\nresources\n\nOozie work\u0000ows: Access to your existing Oozie work\u0000ows, including\n\nwork\u0000ow de\u0000nitions (XML \u0000les), coordinator de\u0000nitions (if applicable),\n\nand any associated scripts or con\u0000gurations\n\nEMR cluster: An EMR cluster con\u0000gured with the desired Hadoop\n\nversion and components, including Oozie\n\nS3 bucket: An S3 bucket to store your Oozie work\u0000ow de\u0000nitions and\n\nany required libraries or dependencies\n\nNetwork connectivity: Ensure network connectivity between your EMR\n\ncluster and any external systems or data sources required by your Oozie\n\nwork\u0000ows\n\nIAM roles: Con\u0000gure IAM roles with appropriate permissions for EMR\n\nto access S3 and other AWS services used by your work\u0000ows\n\nExisting Oozie work\u0000ows ready for migration",
      "content_length": 1066,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 828,
      "content": "How to do it…\n\ne following crucial phase in your migration process involves\n\nthorough testing and validation. is ensures that your cluster setup\n\nis functioning correctly and that the data migration was accurate\n\nand complete:\n\n1. Launch an Elastic MapReduce (EMR) cluster:\n\nI. Use the AWS Management Console, Command Line Interface\n\n(CLI), or Soware Development Kit (SDK) to create an EMR\n\ncluster.\n\nII. Choose a compatible EMR release that supports Oozie (for\n\nexample, emr-5.30.0 or later).\n\nIII. Optionally, customize soware settings and instance types\n\nbased on workload requirements.\n\nYou can launch an EMR cluster using the AWS Management\n\nConsole, AWS CLI, or AWS SDK. Here is a CLI example:\n\naws emr create-cluster --name \"Oozie Cluster\" \\ --release-label emr-5.30.0 \\ --applications Name=Hadoop Name=Hive Name=Pig \\ --use-default-roles \\ --instance-type m5.xlarge \\ --instance-count 3",
      "content_length": 898,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 829,
      "content": "Release version: Choose an EMR release that supports Oozie\n\n(for example, emr-5.30.0 or later)\n\nInstance types: Customize instance types and the number of\n\ninstances based on your workload requirements\n\n2. Install Oozie:\n\nI. Oozie isn’t installed by default on EMR. Use bootstrap actions\n\nduring cluster creation to install Oozie manually. Follow the\n\nAWS documentation\n\n(https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-\n\noozie.html) for speci\u0000c installation steps and con\u0000guration\n\noptions.\n\nII. Create a bootstrap action script to install Oozie and its\n\ndependencies. Here’s an example script:\n\n!/bin/bash sudo yum install -y oozie oozie-client sudo service oozie start\n\nIII. Add this script during the EMR cluster creation:\n\naws emr create-cluster --name \"Oozie Cluster with Bootstrap\" \\ --release-label emr-5.30.0 \\ --applications Name=Hadoop Name=Hive Name=Pig \\ --bootstrap-actions Path=s3://your- bucket/path-to-bootstrap.sh \\ --use-default-roles \\ --instance-type m5.xlarge \\ --instance-count 3",
      "content_length": 1010,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 830,
      "content": "IV. Follow the AWS documentation\n\n(https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-\n\noozie.html) to con\u0000gure Oozie properly, including setting up\n\noozie-site.xml and ensuring that the Oozie server is\n\nrunning on the master node of the cluster.\n\n3. Prepare work\u0000ows:\n\nI. Review work\u0000ows for compatibility with EMR’s Hadoop\n\ndistribution and con\u0000guration.\n\nII. Address any version-speci\u0000c diﬀerences or dependencies.\n\nIII. Test work\u0000ows in a staging environment before migrating to\n\nproduction.\n\n4. Transfer data:\n\nI. Move the data required for work\u0000ows to S3 or other accessible\n\nstorage within EMR.\n\nII. Ensure proper permissions and access from the cluster.\n\ni. Move data to S3: Transfer the datasets required by your\n\nwork\u0000ows to Amazon S3, making them accessible\n\nfrom the EMR cluster:\n\naws s3 cp /local-data-path s3://your-bucket/data-path -- recursive\n\nii. Permissions: Ensure that the EMR cluster has the\n\ncorrect IAM role permissions to access the S3 bucket",
      "content_length": 972,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 831,
      "content": "where your data is stored.\n\n5. Modify the default EMR role:\n\nI. Go to the IAM console: https://console.aws.amazon.com/iam/.\n\nII. In the le sidebar, select Roles.\n\nIII. Search for the role named EMR_EC2_DefaultRole.\n\nIV. Under the role’s Permissions tab, choose Add inline policy to\n\nde\u0000ne custom permissions for S3 access.\n\nYou can de\u0000ne the permissions using an IAM policy that grants\n\nthe EMR cluster access to speci\u0000c S3 buckets. Here is a sample\n\npolicy that grants read and write access to S3:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:ListBucket\" ], \"Resource\": [ \"arn:aws:s3:::your-bucket- name\", \"arn:aws:s3:::your-bucket- name/*\" ] }, {",
      "content_length": 700,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 832,
      "content": "\"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:DeleteObject\" ], \"Resource\": \"arn:aws:s3:::your- bucket-name/*\" } ] }\n\nNOTE\n\nPlease update your-bucket-name with the bucket you are planning to\n\nuse.\n\n6. Submit work\u0000ows:\n\nI. Use Oozie’s command-line tools (oozie job -submit) or\n\nweb user interface (UI) to submit work\u0000ows to the EMR cluster.\n\nII. Monitor work\u0000ow execution through Oozie’s web UI or logs:\n\ni. Submit work\u0000ows: Use Oozie’s command-line tools or\n\nweb UI to submit work\u0000ows to your EMR cluster.\n\nHere’s an example of submitting a work\u0000ow using the\n\ncommand line:\n\noozie job -oozie http://<EMR- MasterNode-DNS>:11000/oozie -config /path/to/job.properties -run",
      "content_length": 673,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 833,
      "content": "ii. Monitor execution: Monitor the execution of your\n\nwork\u0000ows through Oozie’s web UI or by reviewing logs\n\ngenerated in the cluster.\n\nALTERNATIVE OPTIONS TO MOVE OUT OF OOZIE\n\nConsider managed work\u0000ow services or AWS Step Functions for\n\nsimpliﬁed orchestration if appropriate for your workloads.\n\nMigrating an Oozie database to the Amazon RDS MySQL\n\nApache Oozie, a widely used work\u0000ow scheduler in the Hadoop\n\necosystem, orchestrates a variety of Hadoop jobs, including Hive,\n\nPig, Sqoop, Spark, DistCp, Linux shell actions, and more. It stands\n\nout in the Hadoop community for its scalability and reliability.\n\nOozie operates with two key components: work\u0000ow jobs, which\n\nallow you to map out work\u0000ow steps in the form of Directed\n\nAcyclic Graphs (DAGs), and the Oozie Coordinator, designed for\n\nscheduling these work\u0000ow jobs based on events or timed triggers.\n\nUsing XML de\u0000nitions, Oozie enables the creation of work\u0000ows\n\nand has been available on Amazon EMR since the 5.0.0 release.",
      "content_length": 988,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 834,
      "content": "Like Hive, Oozie also relies on a Metastore database, a crucial\n\naspect to consider during migration. When moving Oozie\n\nwork\u0000ows to EMR, it’s essential to transfer both the work\u0000ow\n\nde\u0000nition \u0000les and the Metastore database.\n\nis recipe provides a step-by-step walkthrough of migrating your\n\nOozie database to Amazon Relational Database Service (RDS) for\n\nMySQL. You’ll learn how this migration enhances scalability,\n\nreliability, and maintenance by automating tasks such as backups,\n\npatching, and recovery. By moving to RDS, you can seamlessly\n\nintegrate your Oozie work\u0000ows with AWS services, reduce\n\nmanagement overhead, and create more eﬃcient and secure data\n\npipelines. Follow along to learn how to make this transition and\n\nreap the bene\u0000ts of a managed database service for your Oozie\n\nwork\u0000ows.\n\nGetting ready\n\nBefore migrating your Apache Oozie database to Amazon RDS\n\nMySQL, ensure you have the following prerequisites in place:\n\nAWS account: An active AWS account with permissions to create and\n\nmanage RDS instances and associated resources.\n\nExisting Oozie database: Access to your current Oozie database, which\n\ncould be running on MySQL or another supported database system.",
      "content_length": 1192,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 835,
      "content": "MySQL client: Install a MySQL client on your local machine to interact\n\nwith both your current Oozie database and the RDS MySQL instance.\n\nDatabase dump: Ensure you can export a dump of your existing Oozie\n\ndatabase. is is typically done using tools such as mysqldump for\n\nMySQL databases.\n\nVPC and security groups: Basic understanding of AWS VPCs and\n\nsecurity groups, as they will be needed to con\u0000gure your RDS instance’s\n\nnetwork access.\n\nBackup plan: A backup strategy in place before migrating, ensuring that\n\nyou have a fallback option in case of any issues during the migration\n\nprocess.\n\nAmazon RDS MySQL instance: You will need to create an Amazon RDS\n\nMySQL instance where the Oozie database will be migrated.\n\nIAM roles and permissions: Ensure your AWS account has IAM roles\n\nand permissions to create, manage, and access RDS instances.\n\nNetwork con\u0000guration: Ensure your RDS instance is con\u0000gured to allow\n\nconnections from your current Oozie environment and any\n\nmanagement machines.\n\nDatabase migration tools: Install database migration tools such as\n\nmysqldump for exporting your database and a mysql client for\n\nimporting it into RDS.\n\nHow to do it…",
      "content_length": 1167,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 836,
      "content": "1. Setting up the Amazon RDS MySQL instance: Create a RDS instance\n\nwith the following steps:\n\nI. Go to the AWS Management Console and navigate to the RDS\n\nservice.\n\nII. Click on Create database and choose MySQL as the database\n\nengine.\n\nIII. Select Standard create and con\u0000gure your instance details, such\n\nas database instance class, storage, and VPC settings.\n\nIV. Set the database name, username, and password. is will be\n\nused later for migration.\n\nHere’s an example using AWS CLI:\n\naws rds create-db-instance \\ --db-instance-identifier oozie-db-instance \\ --db-instance-class db.m5.large \\ --engine mysql \\ --allocated-storage 20 \\ --master-username admin \\ --master-user-password yourpassword \\ --vpc-security-group-ids sg-xxxxxxx \\ --availability-zone us-west-2a \\ --db-name oozie\n\n2. Con\u0000gure security groups: Ensure that your RDS instance is accessible\n\nfrom your current Oozie environment. Modify the security group\n\nassociated with the RDS instance to allow incoming MySQL traﬃc (port\n\n3306) from your IP range.",
      "content_length": 1024,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 837,
      "content": "3. Migrating an Oozie database to the Amazon RDS: For the migration of\n\nan Oozie database to the Amazon RDS MySQL engine, follow these\n\nsteps, which will lead you through the export and import procedure:\n\nI. Access the Oozie server node in your on-premises cluster, go to\n\nthe directory containing oozie-setup.sh, and run the\n\nspeci\u0000ed Oozie command to export the Metastore database:\n\n./oozie-setup.sh export /<path>/<oozie- exported-db>.zip\n\nII. In EMR, the oozie-setup.sh \u0000le is in the\n\n/usr/lib/oozie/bin/ directory.\n\nIII. Next, upload the exported database ZIP \u0000le to Amazon S3,\n\nfrom which you can import the database using the following\n\ncommand:\n\naws s3 cp <oozie-exported-db>.zip s3://<bucket-name-path>/<oozie-exported- db>.zip\n\nIV. en, SSH into the EMR master node using Putty to download\n\nthe \u0000le:\n\naws s3 cp s3://<bucket-name- path>/<oozie-exported-db>.zip <oozie- exported-db>.zip\n\nV. Following that, it’s necessary to establish the Oozie database in\n\nAmazon RDS and assign the necessary permissions. Access\n\nyour database with root privileges and run the following\n\ncommands in the MySQL prompt:",
      "content_length": 1110,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 838,
      "content": "mysql> create database oozie default character set utf8; mysql> grant all privileges on oozie.* to 'oozie'@'localhost' identified by 'oozie'; mysql> grant all privileges on oozie.* to 'oozie'@'%' identified by 'oozie';\n\nVI. Once you’ve successfully set up the database with all the\n\nessential permissions, the next step involves importing the\n\ndatabase \u0000le. is can be done using the same oozie-\n\nsetup.sh utility, as demonstrated in the following command:\n\n./oozie-setup.sh import <oozie-exported- db>.zip\n\n4. Update Oozie con\u0000guration: Once the database is prepared with all the\n\nimported metadata, it’s necessary to update the Oozie con\u0000guration in\n\nEMR to redirect it to the new Amazon RDS database. To do this, apply\n\nthe following modi\u0000cations to the oozie-site.xml con\u0000guration\n\n\u0000le:\n\n<property>\n\n<name>oozie.service.JPAService.jdbc.driver</na me> <value>com.mysql.jdbc.Driver</value> </property> <property>\n\n<name>oozie.service.JPAService.jdbc.url</name> <value>jdbc:mysql://<amazon-rds- host>:3306/oozie</value> </property>",
      "content_length": 1032,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 839,
      "content": "<property>\n\n<name>oozie.service.JPAService.jdbc.username</ name> <value><mysql-db-username></value> </property> <property>\n\n<name>oozie.service.JPAService.jdbc.password</ name> <value><mysql-db-password></value> </property>\n\nNOTE\n\nEnsure that you substitute the placeholders with original values in the\n\nprevious code with your speciﬁc database connection details:\n\n<amazon-rds-host>\n\n<mysql-db-username>\n\n<mysql-db-password>\n\nTo apply the updates to Oozie, the ﬁnal step involves restarting\n\nthe Oozie service. This can be done using the following\n\ncommand:\n\nsudo restart oozie\n\nThe outlined steps assist in transferring the Oozie Metastore\n\ndatabase to a remote database on EMR RDS. Following this, your\n\nnext task is to relocate all workﬂow deﬁnition ﬁles to EMR.",
      "content_length": 766,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 840,
      "content": "5. Migrating Oozie work\u0000ow de\u0000nitions:\n\nFor each work\u0000ow de\u0000nition, you will have a set of essential\n\n\u0000les, including the following:\n\njob.properties\n\nworkflow.xml\n\ncoordinator.xml\n\nAny other related dependent \u0000les\n\nTo back up these \u0000les, you can archive them into a single ZIP\n\n\u0000le. Upload this archive to Amazon S3, and then transfer it to\n\nEMR using the aws s3 cp command to integrate it into your\n\nsystem. Remember to update the work\u0000ow con\u0000guration \u0000le to\n\nalign with the EMR connection.\n\nSubsequently, you can submit your jobs to EMR in the same\n\nmanner as you did in your on-premises setup. e Hue interface is\n\navailable for monitoring your Oozie work\u0000ows on EMR.\n\nSee also\n\nConverting Oozie work\u0000ows to AWS Step Functions with AWS Schema\n\nConversion Tool:\n\nhttps://docs.aws.amazon.com/SchemaConversionTool/latest/userguide/\n\nbig-data-oozie.html",
      "content_length": 852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 841,
      "content": "Setting up networking – establishing a secure connection to your EMR cluster\n\nAmazon EMR provides a managed Hadoop framework in the\n\ncloud, enabling powerful big data processing and analytics.\n\nHowever, ensuring the security of your EMR cluster is crucial to\n\nprotect sensitive data and prevent unauthorized access. is recipe\n\nwill guide you through the essential considerations and steps to\n\nestablish a secure connection to your EMR cluster, safeguarding\n\nyour valuable information.\n\nIn this recipe, we introduce a more secure method for engineers to\n\naccess Amazon EMR cluster instances located in a private subnet,\n\nalong with the traditional approach of using a bastion host or jump\n\nserver with open SSH inbound ports.\n\nGetting ready\n\nBefore setting up a secure network connection, ensure you have the\n\nfollowing prerequisites in place:\n\nActive AWS account: You’ll need an active AWS account with the\n\nnecessary permissions to create and manage EMR clusters and related\n\nresources",
      "content_length": 987,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 842,
      "content": "EMR cluster: An existing or newly created EMR cluster in your AWS\n\naccount\n\nSSH client: An SSH client installed on your local machine (for example,\n\nPuTTY or OpenSSH)\n\nKey pair: An SSH key pair generated within your AWS account for\n\nsecure authentication\n\nTo ensure secure communication with your EMR cluster, carefully\n\ncon\u0000gure the following network settings:\n\nPublic subnet: If you need to access the EMR cluster from the internet,\n\nensure it’s launched in a public subnet with appropriate security groups\n\nto allow inbound SSH traﬃc (typically on port 22)\n\nPrivate subnet: If your cluster is in a private subnet, consider using a\n\nbastion host or AWS Systems Manager Session Manager for secure\n\naccess\n\nSecurity groups: Con\u0000gure security groups to restrict inbound and\n\noutbound traﬃc to your EMR cluster based on your speci\u0000c needs\n\nIAM roles: Assign appropriate IAM roles to your EMR instances and\n\nusers to control access to AWS services and resources\n\nHow to do it…\n\nYou have three options for establishing a secure connection to your\n\nEMR cluster, each oﬀering diﬀerent levels of security and \u0000exibility.\n\nWe will discuss each method in detail.",
      "content_length": 1153,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 843,
      "content": "Method 1 – SSH connection (for clusters in public subnets)\n\n1. Retrieve master node public DNS:\n\nI. Open the Amazon EMR console.\n\nII. Navigate to your cluster details and locate the Master public\n\nDNS \u0000eld.\n\n2. Connect using SSH:\n\nI. Open your SSH client.\n\nII. Enter the master node’s public DNS as the hostname.\n\nIII. Specify the appropriate username (for example, hadoop or\n\nec2-user, depending on your EMR con\u0000guration).\n\nIV. Select the private key \u0000le associated with the key pair you\n\ncreated for the cluster.\n\nV. Click on Connect or initiate the SSH connection.\n\nMethod 2 – bastion host (for clusters in private subnets)\n\n1. Launch bastion host:\n\nI. Create an EC2 instance (bastion host) in a public subnet within\n\nthe same VPC as your EMR cluster.\n\nII. Con\u0000gure security groups to allow inbound SSH access from\n\nyour IP address and outbound SSH access to the EMR master",
      "content_length": 876,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 844,
      "content": "node’s private IP.\n\n2. Connect to bastion host: Use SSH to connect to the bastion host’s public\n\nIP address using its associated key pair.\n\n3. Connect to EMR master node: From the bastion host, use SSH to\n\nconnect to the EMR master node’s private IP address using the EMR\n\ncluster’s key pair.\n\nMethod 3 – AWS Systems Manager Session Manager (recommended for private subnets)\n\nAWS Systems Manager oﬀers a consolidated UI, enabling you to\n\nmonitor and control your Amazon EC2 instances eﬀectively. Within\n\nthis, Session Manager enhances security and auditability, for\n\ninstance, management.\n\nWhen integrated with IAM, Systems Manager facilitates centralized\n\naccess control to your EMR cluster. However, by default, Systems\n\nManager lacks the necessary permissions to execute actions on\n\ncluster instances. To enable this functionality, you must assign an\n\nIAM role to the instance that carries the required access\n\npermissions. Before beginning this process, it’s important to\n\nestablish an IAM service role for your cluster’s EC2 instances,\n\nensuring it adheres to the principle of least privilege in its access\n\npolicy. e steps are as follows:",
      "content_length": 1145,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 845,
      "content": "1. Enable Session Manager:\n\nI. Ensure Session Manager is enabled for your EMR cluster (it’s\n\noen enabled by default for newer EMR versions).\n\nII. Con\u0000gure an IAM role with necessary permissions for Session\n\nManager access.\n\nIII. Connect using Session Manager:\n\ni. Open the AWS Systems Manager console.\n\nii. Navigate to Session Manager and click on Start session.\n\niii. Select your EMR master node instance from the list.\n\niv. A browser-based shell session will open, providing\n\nsecure access to your EMR cluster without needing\n\nSSH keys or open ports.\n\nIV. Create an IAM service role speci\u0000cally for EMR cluster EC2\n\ninstances (known as the Amazon EMR role for EC2) and\n\nassociate it with the AWS-managed Systems Manager core\n\ninstance policy (AmazonSSMManagedInstanceCore):\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [\n\n\"ssm:DescribeInstanceProperties\", \"ssm:DescribeSessions\",",
      "content_length": 911,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 846,
      "content": "\"ec2:describeInstances\", \"ssm:GetConnectionStatus\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"ssm:StartSession\" ], \"Resource\": [\n\n\"arn:aws:ec2:${Region}:${Account- Id}:instance/*\" ], \"Condition\": { \"StringEquals\": { \"ssm:resourceTag/ClusterType\": [ \"QACluster\" ] } } } ] }\n\nNow, attach the least privilege policy to the IAM principal (role\n\nor user).\n\nYou can set up the AWS Systems Manager Agent (SSM Agent)\n\non your Amazon EMR cluster nodes using bootstrap actions.",
      "content_length": 480,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 847,
      "content": "e SSM Agent allows Session Manager to control and con\u0000gure\n\nthese nodes.\n\nUsing Session Manager doesn’t incur extra costs for managing\n\nAmazon EC2 instances, though other features might have\n\nadditional charges (check the Systems Manager pricing page for\n\ndetails). e agent receives and executes instructions from the\n\nSession Manager service in the AWS cloud according to user\n\nrequests. By installing the Systems Manager plugin on your local\n\nmachine, you can use dynamic port forwarding. Also, IAM\n\npolicies help in centrally controlling access to the EMR cluster.\n\n2. Con\u0000guring SSM Agent on an EMR cluster: To set up the SSM Agent on\n\nyour cluster, follow these steps:\n\nI. When you are launching the EMR cluster, go to the Bootstrap\n\nActions section and select Add bootstrap action. Choose\n\nCustom action.\n\nII. Include a bootstrap action that executes a script from Amazon\n\nS3. is script will install and set up the SSM Agent on your\n\nAmazon EMR cluster instances.\n\nIII. e SSM Agent requires a localhost entry in the host’s \u0000le. is\n\nis necessary for redirecting traﬃc from your local computer to\n\nthe EMR cluster instance when you’re using dynamic port\n\nforwarding.\n\nIV. e bootstrap script is as follows:",
      "content_length": 1215,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 848,
      "content": "#!/bin/bash ## Name: SSM Agent Installer Script ## Description: Installs SSM Agent on EMR cluster EC2 instances and update hosts file ## sudo yum install -y https://s3.amazonaws.com/ec2-downloads- windows/SSMAgent/latest/linux_amd64/amaz on-ssm-agent.rpm sudo status amazon-ssm-agent >>/tmp/ssm- status.log ## Update hosts file echo \"\\n ########### localhost mapping check ########### \\n\" > /tmp/localhost.log lhost=`sudo cat /etc/hosts | grep localhost | grep '127.0.0.1' | grep -v '^#'` v_ipaddr=`hostname --ip-address` lhostmapping=`sudo cat /etc/hosts | grep $v_ipaddr | grep -v '^#'` if [ -z \"${lhostmapping}\" ]; then echo \"\\n ########### IP address to localhost mapping NOT defined in hosts files. add now ########### \\n \" >> /tmp/localhost.log sudo echo \"${v_ipaddr} localhost\" >>/etc/hosts else echo \"\\n IP address to localhost mapping already defined in hosts file \\n\" >> /tmp/localhost.log fi",
      "content_length": 902,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 849,
      "content": "echo \"\\n ########### IP Address to localhost mapping check complete and below is the content ########### \" >> /tmp/localhost.log sudo cat /etc/hosts >> /tmp/localhost.log echo \"\\n ########### Exit script ########### \" >> /tmp/localhost.log\n\nV. In the Security options section, navigate to Permissions and\n\nselect Custom. en, for the EMR role, choose the IAM role\n\nthat you previously created.\n\nVI. Once the cluster has successfully launched, go to the Session\n\nManager console and select Managed instances. en, choose\n\nyour cluster instance. From the Actions menu, click on Start\n\nsession.\n\ne following \u0000gure shows AWS Systems Manager:",
      "content_length": 638,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 850,
      "content": "Figure 13.8 – AWS Systems Manager\n\nVII. To access web UIs of Hadoop applications, such as YARN Resource\n\nManager and the Spark Job Server, on the Amazon EMR primary node,\n\nyou can establish a secure tunnel between your computer and the\n\nprimary node using Session Manager.\n\n3. Install AWS CLI: Please use the instructions at\n\nhttps://docs.aws.amazon.com/cli/v1/userguide/cli-chap-install.html to\n\nset up AWS CLI. Once done, run the following command:\n\naws ssm start-session --target \"Your Instance ID\" --document-name AWS- StartPortForwardingSession --parameters \"portNumber\"=[\"8080\"],\"localPortNumber\"= [\"8158\"]\n\nIn an environment with a multi-tenant Amazon EMR cluster,\n\nyou can limit access to the cluster instances using speci\u0000c\n\nAmazon EC2 tags.\n\nFor instance, in the example code provided, an IAM principal\n\n(either an IAM user or role) is permitted to initiate a session on\n\nany instance (as indicated by the resource Amazon Resource\n\nName (ARN): arn: aws:ec2:::instance/*) under the\n\ncondition that the instance is tagged as TestCluster (denoted\n\nby ssm:resourceTag/ClusterType: TestCluster).",
      "content_length": 1100,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 851,
      "content": "If the IAM principal attempts to start a session on an instance\n\nthat either lacks a tag or has a tag diﬀerent from ClusterType:\n\nTestCluster, the outcome will display a message indicating\n\nthat they are not authorized to perform the\n\nssm:StartSession action:\n\n{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ssm:DescribeInstanceProperties\", \"ssm:DescribeSessions\", \"ec2:describeInstances\", \"ssm:GetConnectionStatus\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"ssm:StartSession\" ], \"Resource\": [ \"arn:aws:ec2:${Region}:${Account- Id}:instance/*\" ], \"Condition\": { \"StringEquals\": { \"aws:username\": \"${aws:username}\" }, \"StringLike\": { \"ssm:resourceTag/ClusterType\": [",
      "content_length": 707,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 852,
      "content": "\"TestCluster\" ] } } } ] }\n\nYou can modify the default login settings to limit root access in user\n\nsessions. Normally, sessions start with ssm-user, but you can opt\n\nto use an operating system account’s credentials by tagging an IAM\n\nuser or role with SSMSessionRunAs or specifying a username.\n\nis is enabled through updates to Session Manager preferences. An\n\nexample con\u0000guration allows the appdev2 IAM user to start\n\nsessions as ec2-user rather than the standard ssm-user.\n\nSee also\n\nBest Practices for Securing Amazon EMR:\n\nhttps://aws.amazon.com/blogs/big-data/best-practices-for-securing-\n\namazon-emr/\n\nAWS Systems Manager: https://aws.amazon.com/systems-manager/\n\nPerforming a seamless HBase migration to AWS",
      "content_length": 716,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 853,
      "content": "HBase provides a Hadoop-based solution for managing large-scale,\n\nsparse datasets. is non-relational, distributed database excels at\n\nrandom read/write access to data with high volume and variety,\n\nmaking it a valuable tool for big data applications.\n\nMigrating an on-premises Apache HBase database to AWS allows\n\norganizations to leverage the scalability, \u0000exibility, and cost-\n\neﬀectiveness of the cloud. AWS provides several options for\n\nmigrating to HBase, including moving to HBase on Amazon S3\n\n(running on Amazon EMR), or migrating to a fully managed\n\nservice such as Amazon DynamoDB. is recipe outlines the\n\ndetailed steps to perform this migration.\n\nGetting ready\n\nBefore beginning the migration process, ensure the following\n\nprerequisites are met:\n\nAWS account: Ensure you have an AWS account with permissions to\n\ncreate and manage resources such as Amazon EMR, S3, and DynamoDB\n\nExisting HBase cluster: Access to your on-premises HBase cluster,\n\nincluding the HBase tables, con\u0000guration \u0000les, and data\n\nData backup: Back up your HBase data to ensure that you have a fallback\n\noption in case of any issues during migration\n\nAWS CLI and SDK: Install the AWS CLI and AWS SDK if you plan to\n\nautomate or script parts of the migration process",
      "content_length": 1251,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 854,
      "content": "Network connectivity: Ensure there is secure and reliable network\n\nconnectivity between your on-premises environment and AWS\n\ne technical requirements are as follows:\n\nAmazon S3 bucket: Create an Amazon S3 bucket where you will store\n\nHBase snapshots or data \u0000les during the migration process\n\nAmazon EMR cluster: If migrating to HBase on Amazon EMR, ensure\n\nyou can provision an EMR cluster with HBase and Hadoop installed\n\nHBase compatibility: Ensure that the version of HBase you are migrating\n\nfrom is compatible with the version of HBase available on Amazon EMR\n\nor that necessary adjustments are planned\n\nIAM roles and permissions: Set up IAM roles with permissions to access\n\nS3, DynamoDB (if applicable), and other AWS services required during\n\nthe migration process\n\nHow to do it…\n\ne migration process involves exporting data from your on-\n\npremises HBase cluster, transferring it to AWS, and then importing\n\nit into the target AWS service. e detailed steps, including sample\n\ncommands and code snippets, are as follows:\n\n1. Migrate HBase on EMR: Export HBase data:\n\nI. Take a snapshot:",
      "content_length": 1098,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 855,
      "content": "i. Create a snapshot of your HBase tables to capture the\n\ncurrent state of your data.\n\nii. Use the HBase shell to create snapshots:\n\necho \"snapshot 'tableName', 'snapshotName'\" | hbase shell\n\niii. Replace tableName with your HBase table’s name\n\nand snapshotName with your desired snapshot\n\nname.\n\nII. Export the snapshot:\n\ni. Use the ExportSnapshot utility to export the\n\nsnapshot to your on-premises \u0000lesystem or directly to\n\nan S3 bucket:\n\nhbase org.apache.hadoop.hbase.snapshot.Ex portSnapshot \\ -snapshot snapshotName \\ -copy-to hdfs://namenode:8020/path- to-backup\n\nii. Utilize the ExportSnapshot tool provided by\n\nHBase to extract data from the snapshot. is tool\n\nconverts the snapshot data into sequence \u0000les, which\n\ncan be processed further.\n\n2. Transfer data to Amazon S3:\n\nI. Copy data to S3: If you have exported the snapshot to a local\n\n\u0000lesystem, use the AWS CLI to copy the snapshot \u0000les to",
      "content_length": 905,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 856,
      "content": "Amazon S3:\n\naws s3 cp /local-snapshot-path/ s3://your-s3-bucket/snapshot-path/ -- recursive\n\nNOTE\n\nReplace /local-snapshot-path/ with the local directory containing\n\nthe snapshot ﬁles and -s3-bucket/snapshot-path/ with your S3\n\nbucket and path.\n\n3. Set up Amazon EMR cluster:\n\nI. Launch the EMR cluster: Launch an EMR cluster con\u0000gured\n\nwith HBase and Hadoop using the AWS Management Console:\n\naws emr create-cluster --name \"HBase Cluster\" \\ --release-label emr-6.3.0 \\ --applications Name=HBase Name=Hadoop \\ --use-default-roles \\ --instance-type m5.xlarge \\ --instance-count 3\n\nII. Con\u0000gure HBase on EMR: Once the cluster is up, connect to\n\nthe master node and con\u0000gure HBase to restore from the\n\nsnapshot stored in S3.\n\n4. Import HBase data to EMR:",
      "content_length": 751,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 857,
      "content": "I. Restore snapshot to HBase on EMR: Use the\n\nCloneSnapshot utility to restore the snapshot to HBase on\n\nEMR:\n\nhbase org.apache.hadoop.hbase.snapshot.CloneSn apshot \\ -snapshot snapshotName \\ -restore-to tableName\n\n5. Validate and test the migration:\n\nI. Verify data integrity: Perform checks to ensure that all data has\n\nbeen migrated successfully and that the data integrity is\n\nmaintained.\n\nII. Run tests: Run tests on the migrated data to con\u0000rm that your\n\napplications can interact with it as expected in the new\n\nenvironment.\n\nMigrating HBase to DynamoDB on AWS\n\nMigrating from HBase to DynamoDB can provide signi\u0000cant\n\nadvantages in terms of scalability, managed infrastructure, and cost-\n\neﬀectiveness. HBase is a powerful, distributed, NoSQL database\n\noen used in on-premises or Hadoop-based environments.\n\nHowever, managing an HBase cluster can be complex and resource-\n\nintensive. Amazon DynamoDB oﬀers a fully managed, serverless",
      "content_length": 942,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 858,
      "content": "alternative with built-in replication, scalability, and integration with\n\nother AWS services, making it a strong candidate for workloads that\n\nneed high availability and elasticity.\n\nSuppose a media streaming company that stores user session logs\n\nand interaction data in an HBase cluster on an on-premises Hadoop\n\nenvironment is looking to migrate to a cloud-based solution. e\n\ncompany needs the ability to handle sudden spikes in traﬃc (such\n\nas during new content releases), while also reducing the overhead of\n\nmanaging their own HBase clusters. Migrating to Amazon\n\nDynamoDB provides the elasticity to handle these spikes, along\n\nwith automatic scaling and reduced operational complexity.\n\nGetting ready\n\nBefore beginning the migration process, ensure the following\n\nprerequisites are met:\n\nAWS account: Ensure you have an AWS account with appropriate\n\npermissions to create and manage DynamoDB, IAM, and data migration\n\nservices\n\nHBase data analysis: Review your existing HBase data model, including\n\ntables, columns, and key structures, as these will need to be restructured\n\nin DynamoDB\n\nAWS CLI installed: Install and con\u0000gure the AWS CLI to interact with\n\nAWS services",
      "content_length": 1179,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 859,
      "content": "DynamoDB table design: Understand DynamoDB’s table structure,\n\nprimary keys, and global/local secondary indexes, as they diﬀer from\n\nHBase’s schema\n\nHow to do it…\n\n1. ExportHBase snapshot: First, you need to create and export a snapshot\n\nof your HBase tables. To create a snapshot in HBase, please follow the\n\nsteps as explained in the previous recipe, Seamless HBase migration to\n\nAWS, under Migrate HBase on EMR (step 1).\n\n2. Transfer data to Amazon S3: Next, you need to transfer the exported\n\nsnapshot data to an Amazon S3 bucket. Please follow the same steps as\n\nexplained previously.\n\n3. Set up AWS Glue for data transformation: Amazon DynamoDB and\n\nHBase have diﬀerent data models, so you may need to transform the\n\ndata before importing it into DynamoDB. AWS Glue is a suitable service\n\nfor this task:\n\nI. Create a Glue crawler: Set up a Glue crawler to catalog the data\n\nyou uploaded to S3. is will automatically infer the schema\n\nand make the data accessible for ETL processes. Use the AWS\n\nManagement Console or CLI to create the crawler:\n\naws glue create-crawler \\ --name your-crawler-name \\ --role your-glue-role \\ --database-name your-database-name \\ --targets S3Targets=[{Path=s3://your-s3- bucket/snapshot-path/}]",
      "content_length": 1230,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 860,
      "content": "II. Run the Glue ETL job: Aer the crawler has created the\n\nmetadata catalog, use a Glue ETL job to transform and load the\n\ndata into DynamoDB. You can use Python scripts in AWS Glue\n\nto perform this transformation.\n\nYou can \u0000nd the full code on the following path:\n\nhttps://github.com/PacktPublishing/Data-Engineering-\n\nwith-AWS-\n\nCookbook/blob/main/Chapter13/dynamodb-import-\n\nglue-job.py.\n\ndynamic_frame = glueContext.create_dynamic_frame.from_ca talog(database = \"your-database-name\", table_name = \"your-table-name\") # Transformation logic here glueContext.write_dynamic_frame.from_opt ions(frame = dynamic_frame, connection_type = \"dynamodb\", connection_options = {\"dynamodb.output.tableName\": \"your- dynamodb-table-name\"})\n\n4. Import data into Amazon DynamoDB: Finally, load the transformed\n\ndata into your DynamoDB table:\n\nI. Create a DynamoDB table: Ensure you have a DynamoDB table\n\ncreated that matches the schema requirements of your\n\ntransformed data. An example CLI command to create a\n\nDynamoDB table is as follows:\n\nbash",
      "content_length": 1035,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 861,
      "content": "aws dynamodb create-table \\ --table-name your-dynamodb-table-name \\ --attribute-definitions AttributeName=PrimaryKey,AttributeType=S \\ --key-schema AttributeName=PrimaryKey,KeyType=HASH \\ --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5\n\nII. Load data into DynamoDB: Use AWS Glue, as shown in the\n\npreceding script, to load the transformed data directly into\n\nDynamoDB.\n\nIII. Verify the data import: Aer the ETL job completes, verify the\n\ndata in DynamoDB using the AWS Management Console, CLI,\n\nor by running queries against your DynamoDB table.\n\n5. Validate and optimize:\n\nI. Validation:\n\ni. Run queries and validate that all the data has been\n\nsuccessfully imported into DynamoDB.\n\nii. Check for data integrity and consistency across your\n\nnew DynamoDB tables.\n\nII. Optimization:\n\ni. Adjust DynamoDB read/write capacity based on your\n\napplication’s needs.",
      "content_length": 876,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 862,
      "content": "ii. Set up indexes and optimize the table design for\n\neﬃcient querying.\n\niii. Use Amazon CloudWatch and other AWS tools to\n\nmonitor the performance and health.\n\nMigrating from HBase to Amazon DynamoDB oﬀers operational\n\nsimplicity, scalability, and cost bene\u0000ts. By following this recipe, you\n\ncan eﬀectively move your data from an on-premises HBase cluster\n\nto a fully managed DynamoDB environment. is transition not\n\nonly reduces infrastructure management but also improves your\n\napplication’s availability and elasticity on AWS.\n\nSee also\n\nComparing the Use of Amazon DynamoDB and Apache HBase for\n\nNoSQL:\n\nhttps://d1.awsstatic.com/whitepapers/AWS_Comparing_the_Use_of_Dy\n\nnamoDB_and_HBase_for_NoSQL.pdf\n\nMigrating and restoring Apache HBase tables on Apache HBase on\n\nAmazon S3: https://docs.aws.amazon.com/whitepapers/latest/migrate-\n\napache-hbase-s3/migrating-and-restoring-apache-hbase-tables-on-\n\napache-hbase-on-amazon-s3.html\n\nIntroduction to Apache HBase Snapshots:\n\nhttps://blog.cloudera.com/introduction-to-apache-hbase-snapshots/\n\nAmazon DynamoDB: https://aws.amazon.com/dynamodb/",
      "content_length": 1095,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 863,
      "content": "OceanofPDF.com",
      "content_length": 14,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 864,
      "content": "Index\n\nAs this ebook edition doesn't have \u0000xed pagination, the page numbers\n\nbelow are hyperlinked for reference only, based on the printed edition\n\nof this book.\n\nA\n\nAccess Control List (ACLs) 453\n\nAmazon AppFlow 381\n\nAmazon Athena 378, 379, 382\n\ncomparing, with Redshi Spectrum and S3 Select 345\n\nAmazon CloudWatch 380, 383\n\nreference link 314\n\nAmazon CloudWatch Events 119\n\nAmazon Comprehend 379\n\nAmazon DataZone\n\nconsumer access request, approving 224\n\ndata, consuming from domain 223\n\ndata, publishing to domain 219-222",
      "content_length": 525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 865,
      "content": "domain, creating 219\n\nAmazon DataZone, with data mesh principles\n\nused, for building distributed data community 218-224\n\nAmazon Elastic Block Store (EBS) 441\n\nAmazon Elastic Compute Cloud (EC2) instances 443\n\nAmazon Elastic MapReduce (EMR) 378, 379, 382\n\nApache Oozie, migrating and running on 467-471\n\nAmazon EMR TCO simulator\n\nHadoop migration assessment to 446\n\nAmazon EventBridge 300\n\nevent-driven orchestration, setting up with 119-127\n\nAmazon Kinesis 378-381\n\nAmazon Machine Images (AMIs) 183\n\nAmazon Macie 380\n\nAmazon Managed Work\u0000ows for Apache Air\u0000ow (MWAA) 82\n\ndata pipelines, managing with 132-146\n\nAmazon OpenSearch 382\n\nAmazon QuickSight 378, 379, 383\n\nAmazon RDS/Aurora 379, 382",
      "content_length": 692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 866,
      "content": "Amazon RDS MySQL\n\nApache Oozie database, migrating to 471-475\n\nAmazon Recognition 379\n\nAmazon Redshi 377, 379, 382\n\nsystem tables 361\n\nAmazon Redshi Spectrum 341, 379, 382\n\nlarge historical data, querying 342-345\n\nrequisites 341\n\nAmazon Resource Name (ARN) 10, 365, 481\n\nAmazon SageMaker 379, 383\n\nAmazon Simple Storage Service (Amazon S3) 1, 378, 379, 382, 449\n\ndata, replicating 19-22\n\ndata, versioning 15-19\n\nenabling and monitoring 22-29\n\nencryption, enforcing 7-11\n\nstorage types, for optimized storage costs 4-7\n\nAmazon States Language (ASL) 131\n\nAmazon Web Services (AWS) 439\n\nHive Metastore, migrating to 462-467",
      "content_length": 622,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 867,
      "content": "seamless HBase migration, performing to 482-484\n\nanalysis 369\n\nApache Kaa 381\n\nApache Oozie\n\ndatabase, migrating to Amazon RDS MySQL 471-475\n\nmigrating and running, on Amazon EMR 467-471\n\nApache Spark\n\nresource usage, tuning 187-192\n\nApplication Manager (AM) 187\n\natomicity, consistency, isolation, and durability (ACID) 94\n\nAvailability Zones (AZs) 341\n\nAWS AI services 383\n\nAWS analytics ecosystem 377, 378\n\nAWS analytics services architecture 380\n\ncost management 383\n\ndata analytics 382\n\ndata cataloging and governance 382\n\ndata ingestion 381\n\ndata lake storage 382",
      "content_length": 570,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 868,
      "content": "data processing and transformation 382\n\ndata visualization and BI 383\n\ndata warehousing 382\n\nmachine learning and AI 383\n\nsecurity, governance, and monitoring 383\n\nAWS Application Migration Service (MGS) 375\n\nAWS architectures\n\nAWS data mesh architecture 386\n\nconsiderations 386\n\ndata lakehouse architecture 385\n\nRedshi-centric architecture 384, 385\n\nserverless analytics architecture 385\n\nAWS CLI\n\nreference link 167\n\nAWS Cloud Development Kit (AWS CDK) 355\n\nAWS CloudTrail 380, 383\n\nAWS CodePipeline\n\nused, for setting up code deployment pipeline 266-272\n\nAWS Con\u0000g 383",
      "content_length": 572,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 869,
      "content": "reference link 327\n\nS3 compliance, ensuring with 230-236\n\nusing, to automate non-compliance S3 server access logging\n\npolicy 322-326\n\nAWS Con\u0000g rules\n\nSystem Manager, setting up to remediate non-compliance with\n\n319-321\n\nAWS Cost Explorer 380, 383\n\nAWS Database Migration Service (DMS) 375\n\nAWS Data Lake\n\ncost per analytics workload, tracking 327-333\n\nAWS data mesh architecture 386\n\nAWS Data Migration Service (DMS) 379\n\nAWS DataSync 381\n\nreference link 456\n\nused, for migrating on-premises HDFS data 457-462\n\nAWS data warehouse\n\nbene\u0000ts 384\n\nAWS DMS tool 381",
      "content_length": 561,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 870,
      "content": "used, for data migration 428\n\nused, for extracting data 417-424\n\nAWS DynamoDB 378\n\nAWS EMR clusters\n\nhigh availability 177-180\n\nrunning, on EKS 167-170\n\nAWS EMR serverless\n\nused, for running jobs 163-167\n\nAWS Glue 295, 378, 382\n\nintegrating 295-298\n\nused, for pipeline setup to ingest data from JDBC database 150-\n\n160\n\nused, for running pandas code for Ray 103-106\n\nAWS Glue Catalog 380\n\nusing, from another account 171-176\n\nAWS Glue DataBrew 382, 379\n\nAWS Glue Data Catalog 462\n\nAWS Glue Data Quality\n\nversus Deequ 244",
      "content_length": 520,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 871,
      "content": "AWS Glue Data Quality, performance for ETL pipelines\n\nreference link 244\n\nAWS Glue Schema Registry\n\nreferences 252\n\nAWS Glue Studio\n\nused, for creating ETL visual jobs 71-75\n\nAWS Glue work\u0000ows\n\nused, for de\u0000ning simple work\u0000ow 108-119\n\nAWS Identity and Access Management (IAM) 380, 383\n\nAWS IoT Core 381\n\nAWS Key Management Service (KMS) 349, 380, 383, 434\n\nAWS Kinesis Data Analytics 382\n\nAWS Lake Formation 378, 382\n\nAWSLakeFormationDataAdmin 42\n\nAWS Lambda 378, 379, 382\n\nAWS Managed Streaming 381\n\nAWS Managed Work\u0000ows for Apache Air\u0000ow (MWAA) 378\n\nAWS QuickSight 382\n\nAWS RDS Oracle instance",
      "content_length": 596,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 872,
      "content": "setting up 426\n\nAWS resources\n\ntagging strategy, establishing 214-218\n\nAWS Schema Conversion Tool (SCT) 372\n\ninstalling and setting up 426, 427\n\nJDBC drivers, con\u0000guring 409-412\n\nSCT migration assessment report, creating with 406-409\n\nsource and target schemas, mapping 412-417\n\nused, for migrating Oracle database from local laptop to AWS\n\nRDS 425\n\nAWS SDK for pandas\n\nusing, to execute SQL statements 362-367\n\nAWS SDK for Python 368\n\nusing, to manage Amazon QuickSight 368\n\nAWS Server Migration Service (SMS) 375\n\nAWS Snowball 381\n\nAWS Snow Family\n\nlarge data migration manager, as service 430-437\n\nleveraging, for large-scale data migration 429, 430",
      "content_length": 652,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 873,
      "content": "AWS SSM Explorer 321\n\nAWS Step Functions 471\n\ndata work\u0000ow, creating with 127-132\n\nAWS Systems Manager Session Manager 477-482\n\nAWS Systems Manager (SSM) 349\n\nAWS TCO calculators 441\n\nused, for calculating total cost of ownership (TCO) 440-445\n\nAWS Trusted Advisor 380, 383\n\nAWS Well-Architected Labs\n\nreference link 318\n\nB\n\nbastion host 477\n\nbig bang migration 376\n\nbig data frameworks\n\nETL processes, converting with 392-395\n\nbookmarks and bounded execution\n\nused, for processing data incrementally 83-87\n\nbootstrap actions",
      "content_length": 525,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 874,
      "content": "used, for customizing cluster 183-187\n\nboto3 library 369\n\nbusiness intelligence (BI) 346, 371\n\nC\n\ncatalog data retrieval\n\noptimizing, with pushdown \u0000lters and indexes 98-102\n\ncatalog table\n\ndata ingestion, from JDBC database to 150-160\n\ncdk bootstrap command 272\n\nCDK pipeline\n\nsetting up, to deploy multiple accounts and regions 273-276\n\nCDKToolkit 272\n\nClassless Inter-Domain Routing (CIDR) 341\n\nclient-side encryption 8\n\nCloud Development Kit (CDK) 265\n\nused, for setting up code deployment pipeline 267-272\n\ncloud-enabled enterprise Data Lake\n\nreference link 318",
      "content_length": 566,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 875,
      "content": "CloudFormation deployment\n\ncode, running 277-282\n\nCloudWatch data\n\nread-only access, sharing to 64-67\n\nCloudWatch log group retention\n\nsetting, automatically to reduce cost 300-314\n\ncluster\n\ncustomizing, with bootstrap actions 183-187\n\nmonitoring 196-201\n\nprotecting, from security vulnerabilities 201, 202\n\nscaling, based on workload 181-183\n\ncode deployment pipeline\n\nsetting up, with AWS CodePipeline 266-272\n\nsetting up, with CDK 266-272\n\nCommand-Line Interface (CLI) 436, 455, 468\n\nConsole Recorder for AWS 294\n\nconsumer access request\n\napproving 224\n\ncontinuous integration/continuous deployment (CI/CD) 265, 369",
      "content_length": 618,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 876,
      "content": "Cross-Region Replication (CRR) 19\n\ncustom dashboards\n\ncreating, to monitor Data Lake services 314-318\n\ncustom WLM rules 340\n\nD\n\ndashboard 369\n\ndata\n\nconsuming, from domain 223\n\nextracting, with AWS DMS 417-424\n\nprocessing, incrementally with bookmarks and bounded\n\nexecution 83-87\n\npublishing, to domain 219-222\n\nreplicating 19-21\n\nstoring, considerations 449-457\n\nstoring, with data lake formats 94-97\n\nversioning 15-19\n\nDatabase Migration Service (DMS) 405\n\nDataBrew",
      "content_length": 468,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 877,
      "content": "used, for building data cleaning and pro\u0000ling jobs 256-263\n\ndata cleaning\n\nbuilding, with DataBrew 256-263\n\ndata ingestion, from JDBC database\n\nAWS Glue, used for setting up pipeline for 150-160\n\nData Lake environment\n\nreference link 318\n\ndata lake formats\n\nusing, to store data 94-97\n\ndata lakehouse architecture 385\n\nData Lake services\n\ncustom dashboards, creating to monitor 314-318\n\ndata migration services, and tools\n\nconsiderations 386\n\ndata pipelines\n\ndeploying, with Terraform 285-290\n\nhealth, monitoring 146-150\n\nmanaging, with MWAA 132-146\n\ndata quality",
      "content_length": 563,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 878,
      "content": "creating, for ETL jobs in AWS Glue Studio notebooks 238-243\n\nunit testing, with Deequ 244-248\n\nData Quality De\u0000nition Language (DQDL) 206, 240\n\ndataset 369\n\ndata work\u0000ow\n\ncreating, with AWS Step Functions 127-132\n\nDeequ\n\ncomponents 248\n\nexamples 248\n\nused, for unit testing data quality 244-248\n\nversus AWS Glue Data Quality 244\n\nDeequ GitHub page\n\nreference link 244\n\nDirected Acyclic Graphs (DAGs) 471\n\nDistCp\n\nreference link 456\n\ndistributed data community\n\nbuilding, with Amazon DataZone following data mesh principles\n\n218-224",
      "content_length": 531,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 879,
      "content": "domain\n\ncreating 219\n\ndata, consuming from 223\n\ndata, publishing to 219-222\n\nDomain Name System (DNS) 460\n\nDynamoDB, on AWS\n\nHBase, migrating to 485-487\n\nE\n\nEKS CLI\n\nreference link 167\n\nElastic Container Registry (ECR) 97\n\nElastic Kubernetes Service (EKS)\n\nAWS EMR cluster, running on 167-170\n\nElastic MapReduce (EMR) 439\n\nElastic MapReduce File System (EMRFS) 467\n\nElastic Network Interface (ENI) 460\n\nEMR cluster\n\nsecure connection, establishing to 475, 476",
      "content_length": 459,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 880,
      "content": "EMR cluster, secure network connection\n\nAWS Systems Manager Session Manager 477-482\n\nbastion host 477\n\nSSH connection 476\n\nEMR, with Workspaces\n\ncode development on 192-196\n\nEMR Workspaces 193\n\nETL jobs\n\nparameters, making more \u0000exible and reusable 75-78\n\nETL jobs, in AWS Glue Studio notebooks\n\ndata quality, creating for 238-243\n\nETL pipelines\n\nschema management for 249-252\n\nunit test functions, building 252-255\n\nETL processes\n\nconverting, with big data frameworks 392-395\n\nETL visual jobs\n\ncreating, with AWS Glue Studio 71-75\n\nEvent Bridge, with Lambda",
      "content_length": 558,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 881,
      "content": "combining, architecture option 314\n\nevent-driven architecture (EDA) 367\n\nevent-driven orchestration\n\nsetting up, with Amazon EventBridge 120-127\n\nextract, transform, load (ETL) 69, 108, , 243, 335, 378, 439\n\nF\n\nfederated query access\n\nusing 346-348\n\n\u0000ne-grained permissions, on S3 data sharing\n\nenforcing, with Lake Formation 49-57\n\nformat-preserving encryption (FPE) 228\n\nFormer 2\n\nreference link 294\n\nG\n\nGanglia 192\n\nGeneral Data Protection Regulation (GDPR) 453\n\nGit version control\n\nintegrating 295-298",
      "content_length": 506,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 882,
      "content": "Glue Data Catalog\n\nsynchronizing, to diﬀerent account 41-47\n\nGlue Data Quality 206\n\nGlue job\n\nlibraries, reusing in 91-93\n\nretry stages 82\n\nGlue tables\n\ndata quality check, applying on 206-209\n\nH\n\nHadoop\n\nmigration process, de\u0000ning and executing 395-398\n\nsecurity authentication and authorization processes, migrating\n\n398-403\n\nHadoop Distributed File System (HDFS) 82, 394, 441, 450\n\nHadoop migration assessment\n\nconducting, with TCO simulator 446-449\n\ndata, storing considerations 449-457\n\nto Amazon EMR TCO simulator 446",
      "content_length": 523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 883,
      "content": "Hadoop Migration Delivery Kit (HMDK) 446\n\nHBase\n\nmigrating, to DynamoDB on AWS 485-487\n\nHealth Insurance Portability and Accountability Act (HIPAA) 453\n\nHigh Availability (HA) 177\n\nHive Metastore 462\n\nmigrating, to AWS 462-467\n\nhybrid cloud environment 388\n\nI\n\nIaC reverse-engineering 291-294\n\nIAMAllowedPrincipals principle 42\n\nIAM only con\u0000guration\n\nusing 175\n\nIdentity and Access Management (IAM) 2, 407, 453\n\nfor Redshi 341\n\nInfrastructure as Code (IaC) 265, 355\n\nInfrequent Access (IA) 450\n\nIntelligent-Tiering 6",
      "content_length": 518,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 884,
      "content": "J\n\nJava Database Connectivity (JDBC) 87, 335\n\ndrivers, con\u0000guring, in AWS SCT tool 409-412\n\nJava Runtime Environment (JRE) 407\n\njob failures and reruns\n\nhandling, for partial results 78-82\n\njobs\n\nrunning, with AWS EMR serverless 163-166\n\nK\n\nKey Management Service (AWS KMS) 8\n\nKubernetes client\n\ndownload link 170\n\nL\n\nLake Formation 380\n\nused, for enforcing \u0000ne-grained permission on S3 data sharing\n\n49-56\n\nLambda\n\nusing, to execute SQL statements 362-367",
      "content_length": 456,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 885,
      "content": "large historical data\n\nquerying, with Redshi Spectrum 342-345\n\nlarge-scale data migration\n\nAWS Snow Family, leveraging for 429, 430\n\nlogging costs\n\nreducing, by setting automatically CloudWatch log group\n\nretention 300-314\n\nM\n\nmachine learning (ML) 106\n\nmanaged work\u0000ow services 471\n\nmassively parallel processing (MPP) 384\n\nmaster 177\n\nmaster nodes 180\n\nmemory overhead 191\n\nmigration approach\n\nselecting, for workload 387-389\n\nmigration methods\n\nselecting 463",
      "content_length": 462,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 886,
      "content": "migration process\n\nde\u0000ning and executing, with Hadoop 395-398\n\nmigrations\n\ntypes 424\n\nMulti-Factor Authentication (MFA) Delete 19\n\nmulti-primary mode 32\n\nN\n\nnatural language processing (NLP) 379\n\nNetwork Load Balancer (NLB) 355\n\nnon-compliance S3 server access logging policy\n\nautomating, with AWS con\u0000g 322-326\n\nnotebooks 193\n\nO\n\non-premise platform to AWS, migration\n\ndiscovery and planning (assessment phase) 373, 374\n\nmigration execution (migration phase) 373-375\n\nmigration planning (planning phase) 373-375\n\noptimization and cutover 374-376",
      "content_length": 546,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 887,
      "content": "testing and validation (testing phase) 374, 375\n\non-premises HDFS data\n\nmigrating, with AWS DataSync 457-462\n\non-premises platform to AWS, migration\n\nstages 373-376\n\nOozie Coordinator 471\n\nOpen Database Connectivity (ODBC) endpoint 359\n\nOpen Virtual Appliance (OVA) 459\n\nOracle database\n\nAWS RDS Oracle instance, setting up 426\n\nAWS SCT, installing and setting up 426, 427\n\ndata migration, with AWS DMS 428\n\ninstalling, on laptop 425\n\nmigrating, from local laptop to AWS RDS with AWS SCT 425\n\nschema conversion, with SCT 427\n\nveri\u0000cation and post-migration steps 428, 429\n\nP\n\npandas code",
      "content_length": 587,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 888,
      "content": "running, with AWS Glue for Ray 103-106\n\npending authorization status 40\n\nphased migration 376\n\npresigned URL\n\nused, for sharing S3 data 57, 58\n\nprimary replication modes, RDS read replicas\n\nmulti-primary mode 32\n\nsingle-primary mode 32\n\npro\u0000ling jobs\n\nbuilding, with DataBrew 256-263\n\nprototyping and testing\n\nplanning for 390-392\n\npushdown \u0000lters and indexes\n\nused, for optimizing catalog data retrieval 98-102\n\nput_retention_policy\n\nreference link 314\n\nPySpark Testing\n\nreference link 256\n\nPython libraries, with AWS Glue",
      "content_length": 523,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 889,
      "content": "reference link 256\n\nR\n\nRatioOfSums 244\n\nRay\n\nAWS Glue, used for running pandas code for 103-106\n\nread-only replicas\n\ncreating, for RDS 32-35\n\nre-architecting 373, 388\n\nRedshi-centric architecture 384, 385\n\nRedshi cluster\n\naccessing, with JDBC to query data 356-360\n\nVPC endpoint, creating to 350-355\n\nRedshi Data API\n\nusing, to execute SQL statements 362-367\n\nRedshi Processing Unit (RPU) 342\n\nRedshi producer cluster\n\nlive data sharing 36-40\n\nRedshi Spectrum 382",
      "content_length": 469,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 890,
      "content": "comparing, with Amazon Athena and S3 Select 345\n\nRedshi workload management (WLM)\n\nusing, to manage workload priority 336-340\n\nrefactoring 373\n\nrehosting 373, 388\n\nRelational Database Service (RDS) 407, 471\n\nread-only replicas, creating for 32-35\n\nreplatforming 373, 376\n\nconsiderations 380\n\non AWS 377\n\nResource Access Manager (RAM) 41\n\nresource protection\n\nfrom accidental deletion 282-285\n\nretention policies, for objects\n\nsetting up 11-15\n\nS\n\nS3 bucket 341\n\naccess, controlling to 2-4",
      "content_length": 489,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 891,
      "content": "S3 compliance\n\nensuring, with AWS Con\u0000g 230-236\n\nS3 data\n\nreal-time sharing 59-64\n\nS3 data sharing\n\nLake Formation, used for enforcing \u0000ne-grained permissions on\n\n49-56\n\npresigned URL, using for 57, 58\n\ns3_deployer utility 276\n\nS3 logging options\n\nreference link 30\n\nS3 monitoring tools\n\nreference link 30\n\nS3 Select\n\ncomparing, with Redshi Spectrum and Amazon Athena 345\n\nSame-Region Replication 19\n\nSchema Conversion Tool (SCT) 405, 463\n\nschema management\n\nfor ETL pipelines 249-252",
      "content_length": 485,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 892,
      "content": "SCT migration assessment report\n\ncreating, with AWS SCT 406-409\n\nseamless HBase migration\n\nperforming, to AWS 482-484\n\nSecure Shell (SSH) 456\n\nSecure Sockets Layer/Transport Layer Security (SSL/TLS) 453\n\nsecure tunnel 480\n\nsecurity-sensitive data (PII and PHI)\n\nhandling 224-229\n\nsensitive data, on S3 buckets\n\ndiscovery and report, automating of 210-213\n\nserverless analytics architecture 385\n\nserver-side encryption (SSE) 8\n\nServer-Side Encryption with Amazon S3-Managed Keys (SSE-S3) 8,\n\n453\n\nServer-Side Encryption with AWS Key Management Service-\n\nManaged Keys (SSE-KMS) 453\n\nservice-level agreement (SLA) 22\n\nshort query acceleration (SQA) 340",
      "content_length": 649,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 893,
      "content": "Simple Noti\u0000cation Service (SNS) 300, 367\n\nSimple Queue Service (SQS) 300\n\nSimple Storage Service (S3) 335, 439\n\nsimple work\u0000ow\n\nde\u0000ning, with AWS Glue work\u0000ows 108-119\n\nsingle-primary mode 32\n\nSlowly Changing Dimension Type 2 (SCD2) 385\n\nsmall \u0000les, in job\n\nhigh quantity, handling of 87-90\n\nSOCKS5 proxy 196\n\nSoware Development Kit (SDK) 468\n\nSparkUI job run 171\n\nSQL client 341\n\nSQL statements\n\nexecuting, with AWS SDK for pandas 362-367\n\nexecuting, with Lambda 362-367\n\nexecuting, with Redshi Data API 362-367\n\nSSH connection 476\n\nstandard workers 115",
      "content_length": 557,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 894,
      "content": "SUBNET variable 177\n\nSUPERUSER role 346\n\nSwitchyOmega\n\nauto-switch 197\n\nSystem Identi\u0000er (SID) 425\n\nSystem Manager (SSM)\n\nsetting up, to remediate non-compliance with AWS Con\u0000g rules\n\n319-321\n\nSystems Manager Agent (SSM Agent) 478\n\nT\n\nTCO simulator\n\nused, for conducting Hadoop migration assessment 446-449\n\ntemplate 369\n\nTerraform\n\nused, for deploying data pipeline 285-290\n\nterraform fmt command 290\n\ntotal cost of ownership (TCO) 71\n\ncalculating, with AWS TCO calculators 440-445",
      "content_length": 482,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 895,
      "content": "transformation_ctx parameter 86\n\nTransmission Control Protocol (TCP) ports 460\n\ntransparent data encryption (TDE) 457\n\nU\n\nunit test 244\n\nunit test functions\n\nbuilding, for ETL pipelines 252-255\n\nuser-de\u0000ned function (UDF) 81, 224\n\nV\n\nvirtual machine (VM) 459\n\nVirtual Private Cloud (VPC) 374, 460\n\nVPC endpoints 349\n\ncreating, to Redshi cluster 350-355\n\nW\n\nwork\u0000ow jobs 471\n\nworkload\n\ncorrect migration approach, selecting for 387-389",
      "content_length": 435,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 896,
      "content": "workload priority\n\nmanaging, with Redshi workload management (WLM) 336-340\n\nY\n\nYARN application list 199\n\nOceanofPDF.com",
      "content_length": 121,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 897,
      "content": "packtpub.com\n\nSubscribe to our online digital library for full access to over 7,000\n\nbooks and videos, as well as industry leading tools to help you plan\n\nyour personal development and advance your career. For more\n\ninformation, please visit our website.\n\nWhy subscribe?\n\nSpend less time learning and more time coding with practical eBooks\n\nand Videos from over 4,000 industry professionals\n\nImprove your learning with Skill Plans built especially for you\n\nGet a free eBook or video every month\n\nFully searchable for easy access to vital information\n\nCopy and paste, print, and bookmark content\n\nDid you know that Packt oﬀers eBook versions of every book\n\npublished, with PDF and ePub \u0000les available? You can upgrade to\n\nthe eBook version at packtpub.com and as a print book customer,",
      "content_length": 784,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 898,
      "content": "you are entitled to a discount on the eBook copy. Get in touch with\n\nus at customercare@packtpub.com for more details.\n\nAt www.packtpub.com, you can also read a collection of free\n\ntechnical articles, sign up for a range of free newsletters, and receive\n\nexclusive discounts and oﬀers on Packt books and eBooks.\n\nOther Books You May Enjoy\n\nIf you enjoyed this book, you may be interested in these other\n\nbooks by Packt:",
      "content_length": 419,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 899,
      "content": "Data Engineering with Databricks Cookbook\n\nPulkit Chadha",
      "content_length": 56,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 900,
      "content": "ISBN: 978-1-83763-335-7\n\nPerform data loading, ingestion, and processing with Apache Spark\n\nDiscover data transformation techniques and custom user-de\u0000ned\n\nfunctions (UDFs) in Apache Spark\n\nManage and optimize Delta tables with Apache Spark and Delta Lake\n\nAPIs\n\nUse Spark Structured Streaming for real-time data processing\n\nOptimize Apache Spark application and Delta table query performance\n\nImplement DataOps and DevOps practices on Databricks\n\nOrchestrate data pipelines with Delta Live Tables and Databricks\n\nWork\u0000ows\n\nImplement data governance policies with Unity Catalog",
      "content_length": 577,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 901,
      "content": "Data Engineering with Google Cloud Platform\n\nAdi Wijaya",
      "content_length": 55,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 902,
      "content": "ISBN: 978-1-83508-011-5\n\nLoad data into BigQuery and materialize its output\n\nFocus on data pipeline orchestration using Cloud Composer\n\nFormulate Air\u0000ow jobs to orchestrate and automate a data warehouse\n\nEstablish a Hadoop data lake, generate ephemeral clusters, and execute\n\njobs on the Dataproc cluster\n\nHarness Pub/Sub for messaging and ingestion for event-driven systems\n\nApply Data\u0000ow to conduct ETL on streaming data\n\nImplement data governance services on Google Cloud\n\nPackt is searching for authors like you\n\nIf you’re interested in becoming an author for Packt, please visit\n\nauthors.packtpub.com and apply today. We have worked with\n\nthousands of developers and tech professionals, just like you, to\n\nhelp them share their insight with the global tech community. You\n\ncan make a general application, apply for a speci\u0000c hot topic that we\n\nare recruiting an author for, or submit your own idea.\n\nShare Your Thoughts",
      "content_length": 924,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 903,
      "content": "Now you’ve \u0000nished Data Engineering with AWS Cookbook, we’d\n\nlove to hear your thoughts! If you purchased the book from\n\nAmazon, please click here to go straight to the Amazon review page\n\nfor this book and share your feedback or leave a review on the site\n\nthat you purchased it from.\n\nYour review is important to us and the tech community and will\n\nhelp us make sure we’re delivering excellent quality content.\n\nDownload a free PDF copy of this book\n\nanks for purchasing this book!\n\nDo you like to read on the go but are unable to carry your print\n\nbooks everywhere?\n\nIs your eBook purchase not compatible with the device of your\n\nchoice?\n\nDon’t worry, now with every Packt book you get a DRM-free PDF\n\nversion of that book at no cost.\n\nRead anywhere, any place, on any device. Search, copy, and paste\n\ncode from your favorite technical books directly into your\n\napplication.",
      "content_length": 878,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 904,
      "content": "e perks don’t stop there, you can get exclusive access to discounts,\n\nnewsletters, and great free content in your inbox daily\n\nFollow these simple steps to get the bene\u0000ts:\n\n1. Scan the QR code or visit the link below\n\nhttps://packt.link/free-ebook/9781805127284\n\n2. Submit your proof of purchase\n\n3. at’s it! We’ll send your free PDF and other bene\u0000ts to your email\n\ndirectly\n\nOceanofPDF.com",
      "content_length": 394,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 905,
      "content": "Contents\n\n1. Data Engineering with AWS Cookbook 2. Contributors 3. About the authors 4. About the reviewers 5. Preface\n\n1. Who this book is for 2. What this book covers 3. To get the most out of this book 4. Download the example code files 5. Conventions used 6. Sections\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also 7. Get in touch 8. Share Your Thoughts 9. Download a free PDF copy of this book\n\n6. Chapter 1: Managing Data Lake Storage\n\n1. Technical requirements 2. Controlling access to S3 buckets\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Storage types in S3 for optimized storage costs",
      "content_length": 670,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 906,
      "content": "1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n4. Enforcing encryption on S3 buckets\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Setting up retention policies for your objects\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n6. Versioning your data 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n7. Replicating your data 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n8. Monitoring your S3 bucket\n\n1. Getting ready 2. How to do it…",
      "content_length": 607,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 907,
      "content": "3. How it works… 4. There’s more… 5. See also\n\n7. Chapter 2: Sharing Your Data Across Environments and Accounts\n\n1. Technical requirements 2. Creating read-only replicas for RDS\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Redshift live data sharing among your clusters\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more…\n\n4. Synchronizing Glue Data Catalog to a different account\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Enforcing fine-grained permissions on S3 data sharing using Lake Formation 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n6. Sharing your S3 data temporarily using a presigned URL\n\n1. Getting ready",
      "content_length": 754,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 908,
      "content": "2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n7. Real-time sharing of S3 data\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n8. Sharing read-only access to your CloudWatch data with another AWS account 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n8. Chapter 3: Ingesting and Transforming Your Data with AWS Glue\n\n1. Technical requirements 2. Creating ETL jobs visually using AWS Glue Studio\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Parameterizing jobs to make them more flexible and reusable\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more…",
      "content_length": 693,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 909,
      "content": "5. See also\n\n4. Handling job failures and reruns for partial results\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Processing data incrementally using bookmarks and bounded execution 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n6. Handling a high quantity of small files in your job\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n7. Reusing libraries in your Glue job\n\n1. Getting ready 2. How to do it... 3. How it works… 4. There’s more... 5. See also\n\n8. Using data lake formats to store your data\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also",
      "content_length": 690,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 910,
      "content": "9. Optimizing your catalog data retrieval using pushdown filters and indexes 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n10. Running pandas code using AWS Glue for Ray\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n9. Chapter 4: A Deep Dive into AWS Orchestration Frameworks\n\n1. Technical requirements 2. Defining a simple workflow using AWS Glue workflows\n\n1. Getting ready 2. How to do it… 3. See also\n\n3. Setting up event-driven orchestration with Amazon EventBridge\n\n1. Getting ready 2. How to do it…\n\n4. Creating a data workflow using AWS Step Functions\n\n1. How to do it… 2. See also\n\n5. Managing data pipelines with MWAA\n\n1. How to do it… 2. See also\n\n6. Monitoring your pipeline’s health\n\n1. How to do it…",
      "content_length": 784,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 911,
      "content": "7. Setting up a pipeline using AWS Glue to ingest data from a JDBC database into a catalog table\n\n1. How to do it…\n\n10. Chapter 5: Running Big Data Workloads with Amazon EMR\n\n1. Technical requirements 2. Running jobs using AWS EMR serverless\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Running your AWS EMR cluster on EKS\n\n1. Getting ready 2. How to do it... 3. How it works... 4. There’s more... 5. See also\n\n4. Using the AWS Glue catalog from another account\n\n1. Getting ready 2. How to do it... 3. How it works... 4. There’s more... 5. See also\n\n5. Making your cluster highly available\n\n1. Getting ready 2. How to do it... 3. How it works... 4. There’s more... 5. See also\n\n6. Scaling your cluster based on workload\n\n1. Getting ready 2. How to do it...",
      "content_length": 796,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 912,
      "content": "3. How it works... 4. There’s more... 5. See also\n\n7. Customizing the cluster nodes easily using bootstrap actions\n\n1. Getting ready 2. How to do it... 3. How it works... 4. There’s more... 5. See also\n\n8. Tuning Apache Spark resource usage\n\n1. Getting ready 2. How to do it... 3. How it works... 4. There’s more... 5. See also\n\n9. Code development on EMR using Workspaces\n\n1. How to do it... 2. There’s more... 3. See also\n\n10. Monitoring your cluster 1. Getting ready 2. How to do it... 3. There’s more 4. See also\n\n11. Protecting your cluster from security vulnerabilities\n\n1. Getting ready 2. How to do it... 3. How it works... 4. There’s more... 5. See also\n\n11. Chapter 6: Governing Your Platform",
      "content_length": 702,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 913,
      "content": "1. Technical requirements 2. Applying a data quality check on Glue tables\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Automating the discovery and reporting of sensitive data on your S3 buckets 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n4. Establishing a tagging strategy for AWS resources\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Building your distributed data community with Amazon DataZone following data mesh principles\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n6. Handling security-sensitive data (PII and PHI)\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more…",
      "content_length": 747,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 914,
      "content": "5. See also\n\n7. Ensuring S3 compliance with AWS Config\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n12. Chapter 7: Data Quality Management\n\n1. Technical requirements 2. Creating data quality for ETL jobs in AWS Glue Studio notebooks\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Unit testing your data quality using Deequ\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n4. Schema management for ETL pipelines\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Building unit test functions for ETL pipelines\n\n1. How to do it… 2. How it works… 3. There’s more…",
      "content_length": 702,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 915,
      "content": "4. See also\n\n6. Building data cleaning and profiling jobs with DataBrew\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n13. Chapter 8: DevOps – Defining IaC and Building CI/CD Pipelines\n\n1. Technical requirements 2. Setting up a code deployment pipeline using CDK and AWS CodePipeline 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Setting up a CDK pipeline to deploy on multiple accounts and regions\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n4. Running code in a CloudFormation deployment\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Protecting resources from accidental deletion\n\n1. Getting ready",
      "content_length": 749,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 916,
      "content": "2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n6. Deploying a data pipeline using Terraform\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n7. Reverse-engineering IaC 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n8. Integrating AWS Glue and Git version control\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n14. Chapter 9: Monitoring Data Lake Cloud Infrastructure\n\n1. Technical requirements\n\n1. Additional information\n\n2. Automatically setting CloudWatch log group retention to reduce cost\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also",
      "content_length": 692,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 917,
      "content": "3. Creating custom dashboards to monitor Data Lake services\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n4. Setting up System Manager to remediate non-compliance with AWS Config rules\n\n1. Getting ready 2. How to do it… 3. There’s more… 4. See also\n\n5. Using AWS config to automate non-compliance S3 server access logging policy 1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n6. Tracking AWS Data Lake cost per analytics workload\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n15. Chapter 10: Building a Serving Layer with AWS Analytics Services\n\n1. Technical requirements 2. Using Redshift workload management (WLM) to manage workload priority 1. Getting ready",
      "content_length": 766,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 918,
      "content": "2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n3. Querying large historical data with Redshift Spectrum\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n4. Creating a VPC endpoint to a Redshift cluster\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n5. Accessing a Redshift cluster using JDBC to query data\n\n1. Getting ready 2. How to do it… 3. There’s more… 4. See also\n\n6. Using AWS SDK for pandas, the Redshift Data API, and Lambda to execute SQL statements\n\n1. Getting ready 2. How to do it… 3. How it works… 4. There’s more… 5. See also\n\n7. Using the AWS SDK for Python to manage Amazon QuickSight\n\n1. Getting ready 2. How to do it…",
      "content_length": 721,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 919,
      "content": "3. How it works… 4. There’s more… 5. See also\n\n16. Chapter 11: Migrating to AWS – Steps, Strategies, and Best Practices for Modernizing Your Analytics and Big Data Workloads\n\n1. Technical requirements 2. Reviewing the steps and processes for migrating an on- premises platform to AWS\n\n1. Getting ready 2. How to do it…\n\n3. Choosing your AWS analytics stack – the re-platforming approach\n\n1. Getting ready 2. How to do it… 3. See also\n\n4. Picking the correct migration approach for your workload\n\n1. Getting ready 2. How to do it…\n\n5. Planning for prototyping and testing\n\n1. Getting ready 2. How to do it…\n\n6. Converting ETL processes with big data frameworks\n\n1. Getting ready 2. How to do it…\n\n7. Defining and executing your migration process with Hadoop\n\n1. Getting ready 2. How to do it…\n\n8. Migrating the existing Hadoop security authentication and authorization processes\n\n1. Getting ready",
      "content_length": 895,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 920,
      "content": "2. How to do it…\n\n17. Chapter 12: Harnessing the Power of AWS for Seamless Data Warehouse Migration\n\n1. Technical requirements 2. Creating SCT migration assessment report with AWS SCT\n\n1. Getting ready 2. How to do it… 3. See also\n\n3. Extracting data with AWS DMS\n\n1. Getting ready 2. How to do it… 3. See also\n\n4. Live example – migrating an Oracle database from a local laptop to AWS RDS using AWS SCT\n\n1. Getting ready 2. How to do it…\n\n5. Leveraging AWS Snow Family for large-scale data migration\n\n1. The Snow Family Large Data Migration Manager as a service\n\n2. Getting ready 3. How to do it… 4. See also\n\n18. Chapter 13: Strategizing Hadoop Migrations – Cost, Data, and Workflow Modernization with AWS\n\n1. Technical requirements 2. Calculating total cost of ownership (TCO) using AWS TCO calculators\n\n1. Getting ready 2. How to do it… 3. See also",
      "content_length": 852,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 921,
      "content": "3. Conducting a Hadoop migration assessment using the TCO simulator\n\n1. Hadoop to Amazon EMR TCO simulator 2. Getting ready 3. How to do it… 4. See also\n\n4. Selecting how to store your data\n\n1. Getting ready 2. How to do it… 3. See also\n\n5. Migrating on-premises HDFS data using AWS DataSync\n\n1. Getting ready 2. How to do it... 3. See also\n\n6. Migrating the Hive Metastore to AWS\n\n1. Getting ready 2. How to do it…\n\n7. Migrating and running Apache Oozie workflows on Amazon EMR\n\n1. Getting ready 2. How to do it…\n\n8. Migrating an Oozie database to the Amazon RDS MySQL\n\n1. Getting ready 2. How to do it… 3. See also\n\n9. Setting up networking – establishing a secure connection to your EMR cluster 1. Getting ready 2. How to do it… 3. See also\n\n10. Performing a seamless HBase migration to AWS\n\n1. Getting ready",
      "content_length": 811,
      "extraction_method": "Unstructured"
    },
    {
      "page_number": 922,
      "content": "2. How to do it…\n\n11. Migrating HBase to DynamoDB on AWS\n\n1. Getting ready 2. How to do it… 3. See also\n\n19. Index\n\n1. Why subscribe?\n\n20. Other Books You May Enjoy\n\n1. Packt is searching for authors like you 2. Share Your Thoughts 3. Download a free PDF copy of this book\n\nLandmarks\n\n1. Cover 2. Table of Contents 3. Index\n\nOceanofPDF.com",
      "content_length": 339,
      "extraction_method": "Unstructured"
    }
  ]
}