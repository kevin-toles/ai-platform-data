{
  "metadata": {
    "title": "Practical Statistics for Data Scientists",
    "author": "Peter Bruce & Andrew Bruce",
    "publisher": "Unknown Publisher",
    "edition": "1st Edition",
    "isbn": "",
    "total_pages": 562,
    "conversion_date": "2025-12-25T18:16:04.139576",
    "conversion_method": "PyMuPDF + OCR fallback",
    "source_pdf": "Practical Statistics for Data Scientists.pdf",
    "extraction_method": "PyMuPDF_fallback (Unstructured failed)"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Exploratory Data Analysis",
      "start_page": 13,
      "end_page": 92,
      "detection_method": "regex_chapter_title",
      "content": "Chapter 1. Exploratory Data Analysis\nAs a discipline, statistics has mostly developed in the past century. Probability\ntheory — the mathematical foundation for statistics — was developed in the 17th\nto 19th centuries based on work by Thomas Bayes, Pierre-Simon Laplace, and\nCarl Gauss. In contrast to the purely theoretical nature of probability, statistics is\nan applied science concerned with analysis and modeling of data. Modern\nstatistics as a rigorous scientific discipline traces its roots back to the late 1800s\nand Francis Galton and Karl Pearson. R. A. Fisher, in the early 20th century, was\na leading pioneer of modern statistics, introducing key ideas of experimental\ndesign and maximum likelihood estimation. These and many other statistical\nconcepts live largely in the recesses of data science. The main goal of this book is\nto help illuminate these concepts and clarify their importance — or lack thereof\n— in the context of data science and big data.\nThis chapter focuses on the first step in any data science project: exploring the\ndata. Exploratory data analysis, or EDA, is a comparatively new area of\nstatistics. Classical statistics focused almost exclusively on inference, a\nsometimes complex set of procedures for drawing conclusions about large\npopulations based on small samples. In 1962, John W. Tukey (Figure 1-1) called\nfor a reformation of statistics in his seminal paper “The Future of Data Analysis”\n[Tukey-1962]. He proposed a new scientific discipline called data analysis that\nincluded statistical inference as just one component. Tukey forged links to the\nengineering and computer science communities (he coined the terms bit, short for\nbinary digit, and software), and his original tenets are suprisingly durable and\nform part of the foundation for data science. The field of exploratory data analysis\nwas established with Tukey’s 1977 now-classic book Exploratory Data Analysis\n[Tukey-1977].\n\n\nFigure 1-1. John Tukey, the eminent statistician whose ideas developed over 50 years ago form the\nfoundation of data science.\nWith the ready availablility of computing power and expressive data analysis\nsoftware, exploratory data analysis has evolved well beyond its original scope.\nKey drivers of this discipline have been the rapid development of new technology,\naccess to more and bigger data, and the greater use of quantitative analysis in a\nvariety of disciplines. David Donoho, professor of statistics at Stanford\nUniversity and former undergraduate student of Tukey’s, authored an excellent\narticle based on his presentation at the Tukey Centennial workshop in Princeton,\nNew Jersey [Donoho-2015]. Donoho traces the genesis of data science back to\nTukey’s pioneering work in data analysis.\n\n\nElements of Structured Data\nData comes from many sources: sensor measurements, events, text, images, and\nvideos. The Internet of Things (IoT) is spewing out streams of information. Much\nof this data is unstructured: images are a collection of pixels with each pixel\ncontaining RGB (red, green, blue) color information. Texts are sequences of\nwords and nonword characters, often organized by sections, subsections, and so\non. Clickstreams are sequences of actions by a user interacting with an app or\nweb page. In fact, a major challenge of data science is to harness this torrent of\nraw data into actionable information. To apply the statistical concepts covered in\nthis book, unstructured raw data must be processed and manipulated into a\nstructured form — as it might emerge from a relational database — or be\ncollected for a study.\nKEY TERMS FOR DATA TYPES\nContinuous\nData that can take on any value in an interval.\nSynonyms\ninterval, float, numeric\nDiscrete\nData that can take on only integer values, such as counts.\nSynonyms\ninteger, count\nCategorical\nData that can take on only a specific set of values representing a set of possible categories.\nSynonyms\nenums, enumerated, factors, nominal, polychotomous\nBinary\nA special case of categorical data with just two categories of values (0/1, true/false).\nSynonyms\ndichotomous, logical, indicator, boolean\nOrdinal\nCategorical data that has an explicit ordering.\n\n\nSynonyms\nordered factor\nThere are two basic types of structured data: numeric and categorical. Numeric\ndata comes in two forms: continuous, such as wind speed or time duration, and\ndiscrete, such as the count of the occurrence of an event. Categorical data takes\nonly a fixed set of values, such as a type of TV screen (plasma, LCD, LED, etc.)\nor a state name (Alabama, Alaska, etc.). Binary data is an important special case\nof categorical data that takes on only one of two values, such as 0/1, yes/no, or\ntrue/false. Another useful type of categorical data is ordinal data in which the\ncategories are ordered; an example of this is a numerical rating (1, 2, 3, 4, or 5).\nWhy do we bother with a taxonomy of data types? It turns out that for the purposes\nof data analysis and predictive modeling, the data type is important to help\ndetermine the type of visual display, data analysis, or statistical model. In fact,\ndata science software, such as R and Python, uses these data types to improve\ncomputational performance. More important, the data type for a variable\ndetermines how software will handle computations for that variable.\nSoftware engineers and database programmers may wonder why we even need the\nnotion of categorical and ordinal data for analytics. After all, categories are\nmerely a collection of text (or numeric) values, and the underlying database\nautomatically handles the internal representation. However, explicit identification\nof data as categorical, as distinct from text, does offer some advantages:\nKnowing that data is categorical can act as a signal telling software how\nstatistical procedures, such as producing a chart or fitting a model, should\nbehave. In particular, ordinal data can be represented as an ordered.factor\nin R and Python, preserving a user-specified ordering in charts, tables, and\nmodels.\nStorage and indexing can be optimized (as in a relational database).\nThe possible values a given categorical variable can take are enforced in the\nsoftware (like an enum).\nThe third “benefit” can lead to unintended or unexpected behavior: the default\nbehavior of data import functions in R (e.g., read.csv) is to automatically\nconvert a text column into a factor. Subsequent operations on that column will\n\n\nassume that the only allowable values for that column are the ones originally\nimported, and assigning a new text value will introduce a warning and produce an\nNA (missing value).\nKEY IDEAS\nData is typically classified in software by type.\nData types include continuous, discrete, categorical (which includes binary), and ordinal.\nData typing in software acts as a signal to the software on how to process the data.\n\n\nFurther Reading\nData types can be confusing, since types may overlap, and the taxonomy in\none software may differ from that in another. The R-Tutorial website covers\nthe taxonomy for R.\nDatabases are more detailed in their classification of data types,\nincorporating considerations of precision levels, fixed- or variable-length\nfields, and more; see the W3Schools guide for SQL.\n\n\nRectangular Data\nThe typical frame of reference for an analysis in data science is a rectangular\ndata object, like a spreadsheet or database table.\nKEY TERMS FOR RECTANGULAR DATA\nData frame\nRectangular data (like a spreadsheet) is the basic data structure for statistical and machine\nlearning models.\nFeature\nA column in the table is commonly referred to as a feature.\nSynonyms\nattribute, input, predictor, variable\nOutcome\nMany data science projects involve predicting an outcome — often a yes/no outcome (in Table 1-\n1, it is “auction was competitive or not”). The features are sometimes used to predict the outcome\nin an experiment or study.\nSynonyms\ndependent variable, response, target, output\nRecords\nA row in the table is commonly referred to as a record.\nSynonyms\ncase, example, instance, observation, pattern, sample\nRectangular data is essentially a two-dimensional matrix with rows indicating\nrecords (cases) and columns indicating features (variables). The data doesn’t\nalways start in this form: unstructured data (e.g., text) must be processed and\nmanipulated so that it can be represented as a set of features in the rectangular\ndata (see “Elements of Structured Data”). Data in relational databases must be\nextracted and put into a single table for most data analysis and modeling tasks.\nIn Table 1-1, there is a mix of measured or counted data (e.g., duration and price),\nand categorical data (e.g., category and currency). As mentioned earlier, a special\nform of categorical variable is a binary (yes/no or 0/1) variable, seen in the\nrightmost column in Table 1-1 — an indicator variable showing whether an\n\n\nauction was competitive or not.\nTable 1-1. A typical data format\nCategory\ncurrency sellerRating Duration endDay ClosePrice OpenPrice Competitive?\nMusic/Movie/Game US\n3249\n5\nMon\n0.01\n0.01\n0\nMusic/Movie/Game US\n3249\n5\nMon\n0.01\n0.01\n0\nAutomotive\nUS\n3115\n7\nTue\n0.01\n0.01\n0\nAutomotive\nUS\n3115\n7\nTue\n0.01\n0.01\n0\nAutomotive\nUS\n3115\n7\nTue\n0.01\n0.01\n0\nAutomotive\nUS\n3115\n7\nTue\n0.01\n0.01\n0\nAutomotive\nUS\n3115\n7\nTue\n0.01\n0.01\n1\nAutomotive\nUS\n3115\n7\nTue\n0.01\n0.01\n1\n\n\nData Frames and Indexes\nTraditional database tables have one or more columns designated as an index.\nThis can vastly improve the efficiency of certain SQL queries. In Python, with the\npandas library, the basic rectangular data structure is a DataFrame object. By\ndefault, an automatic integer index is created for a DataFrame based on the order\nof the rows. In pandas, it is also possible to set multilevel/hierarchical indexes to\nimprove the efficiency of certain operations.\nIn R, the basic rectangular data structure is a data.frame object. A data.frame\nalso has an implicit integer index based on the row order. While a custom key can\nbe created through the row.names attribute, the native R data.frame does not\nsupport user-specified or multilevel indexes. To overcome this deficiency, two\nnew packages are gaining widespread use: data.table and dplyr. Both support\nmultilevel indexes and offer significant speedups in working with a data.frame.\n\n\nTERMINOLOGY DIFFERENCES\nTerminology for rectangular data can be confusing. Statisticians and data scientists use different\nterms for the same thing. For a statistician, predictor variables are used in a model to predict a\nresponse or dependent variable. For a data scientist, features are used to predict a target. One\nsynonym is particularly confusing: computer scientists will use the term sample for a single row; a\nsample to a statistician means a collection of rows.\n\n\nNonrectangular Data Structures\nThere are other data structures besides rectangular data.\nTime series data records successive measurements of the same variable. It is the\nraw material for statistical forecasting methods, and it is also a key component of\nthe data produced by devices — the Internet of Things.\nSpatial data structures, which are used in mapping and location analytics, are\nmore complex and varied than rectangular data structures. In the object\nrepresentation, the focus of the data is an object (e.g., a house) and its spatial\ncoordinates. The field view, by contrast, focuses on small units of space and the\nvalue of a relevant metric (pixel brightness, for example).\nGraph (or network) data structures are used to represent physical, social, and\nabstract relationships. For example, a graph of a social network, such as\nFacebook or LinkedIn, may represent connections between people on the network.\nDistribution hubs connected by roads are an example of a physical network.\nGraph structures are useful for certain types of problems, such as network\noptimization and recommender systems.\nEach of these data types has its specialized methodology in data science. The\nfocus of this book is on rectangular data, the fundamental building block of\npredictive modeling.\n\n\nGRAPHS IN STATISTICS\nIn computer science and information technology, the term graph typically refers to a depiction of\nthe connections among entities, and to the underlying data structure. In statistics, graph is used to\nrefer to a variety of plots and visualizations, not just of connections among entities, and the term\napplies just to the visualization, not to the data structure.\nKEY IDEAS\nThe basic data structure in data science is a rectangular matrix in which rows are records and\ncolumns are variables (features).\nTerminology can be confusing; there are a variety of synonyms arising from the different disciplines\nthat contribute to data science (statistics, computer science, and information technology).\n\n\nFurther Reading\nDocumentation on data frames in R\nDocumentation on data frames in Python\n\n\nEstimates of Location\nVariables with measured or count data might have thousands of distinct values. A\nbasic step in exploring your data is getting a “typical value” for each feature\n(variable): an estimate of where most of the data is located (i.e., its central\ntendency).\nKEY TERMS FOR ESTIMATES OF LOCATION\nMean\nThe sum of all values divided by the number of values.\nSynonyms\naverage\nWeighted mean\nThe sum of all values times a weight divided by the sum of the weights.\nSynonyms\nweighted average\nMedian\nThe value such that one-half of the data lies above and below.\nSynonyms\n50th percentile\nWeighted median\nThe value such that one-half of the sum of the weights lies above and below the sorted data.\nTrimmed mean\nThe average of all values after dropping a fixed number of extreme values.\nSynonyms\ntruncated mean\nRobust\nNot sensitive to extreme values.\nSynonyms\nresistant\nOutlier\nA data value that is very different from most of the data.\nSynonyms\n\n\nextreme value\nAt first glance, summarizing data might seem fairly trivial: just take the mean of\nthe data (see “Mean”). In fact, while the mean is easy to compute and expedient to\nuse, it may not always be the best measure for a central value. For this reason,\nstatisticians have developed and promoted several alternative estimates to the\nmean.\n\n\nMETRICS AND ESTIMATES\nStatisticians often use the term estimates for values calculated from the data at hand, to draw a\ndistinction between what we see from the data, and the theoretical true or exact state of affairs.\nData scientists and business analysts are more likely to refer to such values as a metric. The\ndifference reflects the approach of statistics versus data science: accounting for uncertainty lies\nat the heart of the discipline of statistics, whereas concrete business or organizational objectives\nare the focus of data science. Hence, statisticians estimate, and data scientists measure.\n\n\nMean\nThe most basic estimate of location is the mean, or average value. The mean is the\nsum of all the values divided by the number of values. Consider the following set\nof numbers: {3 5 1 2}. The mean is (3 + 5 + 1 + 2) / 4 = 11 / 4 = 2.75. You will\nencounter the symbol \n (pronounced “x-bar”) to represent the mean of a sample\nfrom a population. The formula to compute the mean for a set of n values \n is:\nNOTE\nN (or n) refers to the total number of records or observations. In statistics it is capitalized if it is\nreferring to a population, and lowercase if it refers to a sample from a population. In data science,\nthat distinction is not vital so you may see it both ways.\nA variation of the mean is a trimmed mean, which you calculate by dropping a\nfixed number of sorted values at each end and then taking an average of the\nremaining values. Representing the sorted values by \n where \n is the smallest value and \n the largest, the formula to compute the trimmed\nmean with  smallest and largest values omitted is:\nA trimmed mean eliminates the influence of extreme values. For example, in\ninternational diving the top and bottom scores from five judges are dropped, and\nthe final score is the average of the three remaining judges [Wikipedia-2016].\n\n\nThis makes it difficult for a single judge to manipulate the score, perhaps to favor\nhis country’s contestant. Trimmed means are widely used, and in many cases, are\npreferable to use instead of the ordinary mean: see “Median and Robust\nEstimates” for further discussion.\nAnother type of mean is a weighted mean, which you calculate by multiplying\neach data value \n by a weight \n and dividing their sum by the sum of the\nweights. The formula for a weighted mean is:\nThere are two main motivations for using a weighted mean:\nSome values are intrinsically more variable than others, and highly variable\nobservations are given a lower weight. For example, if we are taking the\naverage from multiple sensors and one of the sensors is less accurate, then\nwe might downweight the data from that sensor.\nThe data collected does not equally represent the different groups that we are\ninterested in measuring. For example, because of the way an online\nexperiment was conducted, we may not have a set of data that accurately\nreflects all groups in the user base. To correct that, we can give a higher\nweight to the values from the groups that were underrepresented.\nwww.allitebooks.com\n\n\nMedian and Robust Estimates\nThe median is the middle number on a sorted list of the data. If there is an even\nnumber of data values, the middle value is one that is not actually in the data set,\nbut rather the average of the two values that divide the sorted data into upper and\nlower halves. Compared to the mean, which uses all observations, the median\ndepends only on the values in the center of the sorted data. While this might seem\nto be a disadvantage, since the mean is much more sensitive to the data, there are\nmany instances in which the median is a better metric for location. Let’s say we\nwant to look at typical household incomes in neighborhoods around Lake\nWashington in Seattle. In comparing the Medina neighborhood to the Windermere\nneighborhood, using the mean would produce very different results because Bill\nGates lives in Medina. If we use the median, it won’t matter how rich Bill Gates\nis — the position of the middle observation will remain the same.\nFor the same reasons that one uses a weighted mean, it is also possible to compute\na weighted median. As with the median, we first sort the data, although each data\nvalue has an associated weight. Instead of the middle number, the weighted\nmedian is a value such that the sum of the weights is equal for the lower and upper\nhalves of the sorted list. Like the median, the weighted median is robust to\noutliers.\nOutliers\nThe median is referred to as a robust estimate of location since it is not influenced\nby outliers (extreme cases) that could skew the results. An outlier is any value\nthat is very distant from the other values in a data set. The exact definition of an\noutlier is somewhat subjective, although certain conventions are used in various\ndata summaries and plots (see “Percentiles and Boxplots”). Being an outlier in\nitself does not make a data value invalid or erroneous (as in the previous example\nwith Bill Gates). Still, outliers are often the result of data errors such as mixing\ndata of different units (kilometers versus meters) or bad readings from a sensor.\nWhen outliers are the result of bad data, the mean will result in a poor estimate of\nlocation, while the median will be still be valid. In any case, outliers should be\nidentified and are usually worthy of further investigation.\n\n\nANOMALY DETECTION\nIn contrast to typical data analysis, where outliers are sometimes informative and sometimes a\nnuisance, in anomaly detection the points of interest are the outliers, and the greater mass of\ndata serves primarily to define the “normal” against which anomalies are measured.\nThe median is not the only robust estimate of location. In fact, a trimmed mean is\nwidely used to avoid the influence of outliers. For example, trimming the bottom\nand top 10% (a common choice) of the data will provide protection against\noutliers in all but the smallest data sets. The trimmed mean can be thought of as a\ncompromise between the median and the mean: it is robust to extreme values in\nthe data, but uses more data to calculate the estimate for location.\n\n\nOTHER ROBUST METRICS FOR LOCATION\nStatisticians have developed a plethora of other estimators for location, primarily with the goal of\ndeveloping an estimator more robust than the mean and also more efficient (i.e., better able to\ndiscern small location differences between data sets). While these methods are potentially useful\nfor small data sets, they are not likely to provide added benefit for large or even moderately sized\ndata sets.\n\n\nExample: Location Estimates of Population and Murder Rates\nTable 1-2 shows the first few rows in the data set containing population and\nmurder rates (in units of murders per 100,000 people per year) for each state.\nTable 1-2. A few rows of the\ndata.frame state of population\nand murder rate by state\nState\nPopulation Murder rate\n1 Alabama\n4,779,736\n5.7\n2 Alaska\n710,231\n5.6\n3 Arizona\n6,392,017\n4.7\n4 Arkansas\n2,915,918\n5.6\n5 California\n37,253,956\n4.4\n6 Colorado\n5,029,196\n2.8\n7 Connecticut 3,574,097\n2.4\n8 Delaware\n897,934\n5.8\nCompute the mean, trimmed mean, and median for the population using R:\n> state <- read.csv(file=\"/Users/andrewbruce1/book/state.csv\")\n> mean(state[[\"Population\"]])\n[1] 6162876\n> mean(state[[\"Population\"]], trim=0.1)\n[1] 4783697\n> median(state[[\"Population\"]])\n[1] 4436370\nThe mean is bigger than the trimmed mean, which is bigger than the median.\nThis is because the trimmed mean excludes the largest and smallest five states\n(trim=0.1 drops 10% from each end). If we want to compute the average murder\nrate for the country, we need to use a weighted mean or median to account for\ndifferent populations in the states. Since base R doesn’t have a function for\nweighted median, we need to install a package such as matrixStats:\n\n\n> weighted.mean(state[[\"Murder.Rate\"]], w=state[[\"Population\"]])\n[1] 4.445834\n> library(\"matrixStats\")\n> weightedMedian(state[[\"Murder.Rate\"]], w=state[[\"Population\"]])\n[1] 4.4\nIn this case, the weighted mean and median are about the same.\nKEY IDEAS\nThe basic metric for location is the mean, but it can be sensitive to extreme values (outlier).\nOther metrics (median, trimmed mean) are more robust.\n\n\nFurther Reading\nMichael Levine (Purdue University) has posted some useful slides on basic\ncalculations for measures of location.\nJohn Tukey’s 1977 classic Exploratory Data Analysis (Pearson) is still\nwidely read.\n\n\nEstimates of Variability\nLocation is just one dimension in summarizing a feature. A second dimension,\nvariability, also referred to as dispersion, measures whether the data values are\ntightly clustered or spread out. At the heart of statistics lies variability: measuring\nit, reducing it, distinguishing random from real variability, identifying the various\nsources of real variability, and making decisions in the presence of it.\nKEY TERMS FOR VARIABILITY METRICS\nDeviations\nThe difference between the observed values and the estimate of location.\nSynonyms\nerrors, residuals\nVariance\nThe sum of squared deviations from the mean divided by n – 1 where n is the number of data\nvalues.\nSynonyms\nmean-squared-error\nStandard deviation\nThe square root of the variance.\nSynonyms\nl2-norm, Euclidean norm\nMean absolute deviation\nThe mean of the absolute value of the deviations from the mean.\nSynonyms\nl1-norm, Manhattan norm\nMedian absolute deviation from the median\nThe median of the absolute value of the deviations from the median.\nRange\nThe difference between the largest and the smallest value in a data set.\nOrder statistics\nMetrics based on the data values sorted from smallest to biggest.\nSynonyms\nranks\n\n\nPercentile\nThe value such that P percent of the values take on this value or less and (100–P) percent take on\nthis value or more.\nSynonyms\nquantile\nInterquartile range\nThe difference between the 75th percentile and the 25th percentile.\nSynonyms\nIQR\nJust as there are different ways to measure location (mean, median, etc.) there are\nalso different ways to measure variability.\n\n\nStandard Deviation and Related Estimates\nThe most widely used estimates of variation are based on the differences, or\ndeviations, between the estimate of location and the observed data. For a set of\ndata {1, 4, 4}, the mean is 3 and the median is 4. The deviations from the mean\nare the differences: 1 – 3 = –2, 4 – 3 = 1 , 4 – 3 = 1. These deviations tell us how\ndispersed the data is around the central value.\nOne way to measure variability is to estimate a typical value for these deviations.\nAveraging the deviations themselves would not tell us much — the negative\ndeviations offset the positive ones. In fact, the sum of the deviations from the mean\nis precisely zero. Instead, a simple approach is to take the average of the absolute\nvalues of the deviations from the mean. In the preceding example, the absolute\nvalue of the deviations is {2 1 1} and their average is (2 + 1 + 1) / 3 = 1.33. This\nis known as the mean absolute deviation and is computed with the formula:\nwhere \n is the sample mean.\nThe best-known estimates for variability are the variance and the standard\ndeviation, which are based on squared deviations. The variance is an average of\nthe squared deviations, and the standard deviation is the square root of the\nvariance.\nThe standard deviation is much easier to interpret than the variance since it is on\nthe same scale as the original data. Still, with its more complicated and less\nintuitive formula, it might seem peculiar that the standard deviation is preferred in\nstatistics over the mean absolute deviation. It owes its preeminence to statistical\ntheory: mathematically, working with squared values is much more convenient\n\n\nthan absolute values, especially for statistical models.\nDEGREES OF FREEDOM, AND N OR N – 1?\nIn statistics books, there is always some discussion of why we have n – 1 in the denominator in the\nvariance formula, instead of n, leading into the concept of degrees of freedom. This distinction is not\nimportant since n is generally large enough that it won’t make much difference whether you divide by n\nor n – 1. But in case you are interested, here is the story. It is based on the premise that you want to\nmake estimates about a population, based on a sample.\nIf you use the intuitive denominator of n in the variance formula, you will underestimate the true value of\nthe variance and the standard deviation in the population. This is referred to as a biased estimate.\nHowever, if you divide by n – 1 instead of n, the standard deviation becomes an unbiased estimate.\nTo fully explain why using n leads to a biased estimate involves the notion of degrees of freedom, which\ntakes into account the number of constraints in computing an estimate. In this case, there are n – 1\ndegrees of freedom since there is one constraint: the standard deviation depends on calculating the\nsample mean. For many problems, data scientists do not need to worry about degrees of freedom, but\nthere are cases where the concept is important (see “Choosing K”).\nNeither the variance, the standard deviation, nor the mean absolute deviation is\nrobust to outliers and extreme values (see “Median and Robust Estimates” for a\ndiscussion of robust estimates for location). The variance and standard deviation\nare especially sensitive to outliers since they are based on the squared deviations.\nA robust estimate of variability is the median absolute deviation from the median\nor MAD:\nwhere m is the median. Like the median, the MAD is not influenced by extreme\nvalues. It is also possible to compute a trimmed standard deviation analogous to\nthe trimmed mean (see “Mean”).\nNOTE\nThe variance, the standard deviation, mean absolute deviation, and median absolute deviation\nfrom the median are not equivalent estimates, even in the case where the data comes from a\nnormal distribution. In fact, the standard deviation is always greater than the mean absolute\ndeviation, which itself is greater than the median absolute deviation. Sometimes, the median\nabsolute deviation is multiplied by a constant scaling factor (it happens to work out to 1.4826) to\nput MAD on the same scale as the standard deviation in the case of a normal distribution.\n\n\nEstimates Based on Percentiles\nA different approach to estimating dispersion is based on looking at the spread of\nthe sorted data. Statistics based on sorted (ranked) data are referred to as order\nstatistics. The most basic measure is the range: the difference between the largest\nand smallest number. The minimum and maximum values themselves are useful to\nknow, and helpful in identifying outliers, but the range is extremely sensitive to\noutliers and not very useful as a general measure of dispersion in the data.\nTo avoid the sensitivity to outliers, we can look at the range of the data after\ndropping values from each end. Formally, these types of estimates are based on\ndifferences between percentiles. In a data set, the Pth percentile is a value such\nthat at least P percent of the values take on this value or less and at least (100 – P)\npercent of the values take on this value or more. For example, to find the 80th\npercentile, sort the data. Then, starting with the smallest value, proceed 80 percent\nof the way to the largest value. Note that the median is the same thing as the 50th\npercentile. The percentile is essentially the same as a quantile, with quantiles\nindexed by fractions (so the .8 quantile is the same as the 80th percentile).\nA common measurement of variability is the difference between the 25th\npercentile and the 75th percentile, called the interquartile range (or IQR). Here\nis a simple example: 3,1,5,3,6,7,2,9. We sort these to get 1,2,3,3,5,6,7,9. The 25th\npercentile is at 2.5, and the 75th percentile is at 6.5, so the interquartile range is\n6.5 – 2.5 = 4. Software can have slightly differing approaches that yield different\nanswers (see the following note); typically, these differences are smaller.\nFor very large data sets, calculating exact percentiles can be computationally very\nexpensive since it requires sorting all the data values. Machine learning and\nstatistical software use special algorithms, such as [Zhang-Wang-2007], to get an\napproximate percentile that can be calculated very quickly and is guaranteed to\nhave a certain accuracy.\n\n\nPERCENTILE: PRECISE DEFINITION\nIf we have an even number of data (n is even), then the percentile is ambiguous under the\npreceding definition. In fact, we could take on any value between the order statistics \n and \n where j satisfies:\nFormally, the percentile is the weighted average:\nfor some weight w between 0 and 1. Statistical software has slightly differing approaches to\nchoosing w. In fact, the R function quantile offers nine different alternatives to compute the\nquantile. Except for small data sets, you don’t usually need to worry about the precise way a\npercentile is calculated.\n\n\nExample: Variability Estimates of State Population\nTable 1-3 shows the first few rows in the data set containing population and\nmurder rates for each state.\nTable 1-3. A few rows of the\ndata.frame state of population\nand murder rate by state\nState\nPopulation Murder rate\n1 Alabama\n4,779,736\n5.7\n2 Alaska\n710,231\n5.6\n3 Arizona\n6,392,017\n4.7\n4 Arkansas\n2,915,918\n5.6\n5 California\n37,253,956\n4.4\n6 Colorado\n5,029,196\n2.8\n7 Connecticut 3,574,097\n2.4\n8 Delaware\n897,934\n5.8\nUsing R’s built-in functions for the standard deviation, interquartile range (IQR),\nand the median absolution deviation from the median (MAD), we can compute\nestimates of variability for the state population data:\n> sd(state[[\"Population\"]])\n[1] 6848235\n> IQR(state[[\"Population\"]])\n[1] 4847308\n> mad(state[[\"Population\"]])\n[1] 3849870\nThe standard deviation is almost twice as large as the MAD (in R, by default, the\nscale of the MAD is adjusted to be on the same scale as the mean). This is not\nsurprising since the standard deviation is sensitive to outliers.\nKEY IDEAS\n\n\nThe variance and standard deviation are the most widespread and routinely reported statistics of\nvariability.\nBoth are sensitive to outliers.\nMore robust metrics include mean and median absolute deviations from the mean and percentiles\n(quantiles).\n\n\nFurther Reading\n1. David Lane’s online statistics resource has a section on percentiles.\n2. Kevin Davenport has a useful post on deviations from the median, and\ntheir robust properties in R-Bloggers.\n\n\nExploring the Data Distribution\nEach of the estimates we’ve covered sums up the data in a single number to\ndescribe the location or variability of the data. It is also useful to explore how the\ndata is distributed overall.\nKEY TERMS FOR EXPLORING THE DISTRIBUTION\nBoxplot\nA plot introduced by Tukey as a quick way to visualize the distribution of data.\nSynonyms\nBox and whiskers plot\nFrequency table\nA tally of the count of numeric data values that fall into a set of intervals (bins).\nHistogram\nA plot of the frequency table with the bins on the x-axis and the count (or proportion) on the y-\naxis.\nDensity plot\nA smoothed version of the histogram, often based on a kernal density estimate.\n\n\nPercentiles and Boxplots\nIn “Estimates Based on Percentiles”, we explored how percentiles can be used to\nmeasure the spread of the data. Percentiles are also valuable to summarize the\nentire distribution. It is common to report the quartiles (25th, 50th, and 75th\npercentiles) and the deciles (the 10th, 20th, …, 90th percentiles). Percentiles are\nespecially valuable to summarize the tails (the outer range) of the distribution.\nPopular culture has coined the term one-percenters to refer to the people in the\ntop 99th percentile of wealth.\nTable 1-4 displays some percentiles of the murder rate by state. In R, this would\nbe produced by the quantile function:\nquantile(state[[\"Murder.Rate\"]], p=c(.05, .25, .5, .75, .95))\n   5%   25%   50%   75%   95%\n1.600 2.425 4.000 5.550 6.510\nTable 1-4. Percentiles\nof murder rate by\nstate\n5%\n25% 50% 75% 95%\n1.60 2.42 4.00 5.55 6.51\nThe median is 4 murders per 100,000 people, although there is quite a bit of\nvariability: the 5th percentile is only 1.6 and the 95th percentile is 6.51.\nBoxplots, introduced by Tukey [Tukey-1977], are based on percentiles and give a\nquick way to visualize the distribution of data. Figure 1-2 shows a boxplot of the\npopulation by state produced by R:\nboxplot(state[[\"Population\"]]/1000000, ylab=\"Population (millions)\")\n\n\nFigure 1-2. Boxplot of state populations\nThe top and bottom of the box are the 75th and 25th percentiles, respectively. The\nmedian is shown by the horizontal line in the box. The dashed lines, referred to as\nwhiskers, extend from the top and bottom to indicate the range for the bulk of the\ndata. There are many variations of a boxplot; see, for example, the documentation\nfor the R function boxplot [R-base-2015]. By default, the R function extends the\nwhiskers to the furthest point beyond the box, except that it will not go beyond 1.5\ntimes the IQR (other software may use a different rule). Any data outside of the\nwhiskers is plotted as single points.\n\n\nFrequency Table and Histograms\nA frequency table of a variable divides up the variable range into equally spaced\nsegments, and tells us how many values fall in each segment. Table 1-5 shows a\nfrequency table of the population by state computed in R:\nbreaks <- seq(from=min(state[[\"Population\"]]),\n                to=max(state[[\"Population\"]]), length=11)\npop_freq <- cut(state[[\"Population\"]], breaks=breaks,\n                right=TRUE, include.lowest = TRUE)\ntable(pop_freq)\nTable 1-5. A frequency table of population by state\nBinNumber BinRange\nCount States\n1\n563,626–\n4,232,658\n24\nWY,VT,ND,AK,SD,DE,MT,RI,NH,ME,HI,ID,NE,WV,NM,NV,UT,KS,AR\n2\n4,232,659–\n7,901,691\n14\nKY,LA,SC,AL,CO,MN,WI,MD,MO,TN,AZ,IN,MA,WA\n3\n7,901,692–\n11,570,724\n6\nVA,NJ,NC,GA,MI,OH\n4\n11,570,725–\n15,239,757\n2\nPA,IL\n5\n15,239,758–\n18,908,790\n1\nFL\n6\n18,908,791–\n22,577,823\n1\nNY\n7\n22,577,824–\n26,246,856\n1\nTX\n8\n26,246,857–\n29,915,889\n0\n9\n29,915,890–\n33,584,922\n0\n10\n33,584,923–\n37,253,956\n1\nCA\nwww.allitebooks.com\n\n\nThe least populous state is Wyoming, with 563,626 people (2010 Census) and the\nmost populous is California, with 37,253,956 people. This gives us a range of\n37,253,956 – 563,626 = 36,690,330, which we must divide up into equal size\nbins — let’s say 10 bins. With 10 equal size bins, each bin will have a width of\n3,669,033, so the first bin will span from 563,626 to 4,232,658. By contrast, the\ntop bin, 33,584,923 to 37,253,956, has only one state: California. The two bins\nimmediately below California are empty, until we reach Texas. It is important to\ninclude the empty bins; the fact that there are no values in those bins is useful\ninformation. It can also be useful to experiment with different bin sizes. If they are\ntoo large, important features of the distribution can be obscured. It they are too\nsmall, the result is too granular and the ability to see bigger pictures is lost.\nNOTE\nBoth frequency tables and percentiles summarize the data by creating bins. In general, quartiles\nand deciles will have the same count in each bin (equal-count bins), but the bin sizes will be\ndifferent. The frequency table, by contrast, will have different counts in the bins (equal-size bins).\n\n\nFigure 1-3. Histogram of state populations\nA histogram is a way to visualize a frequency table, with bins on the x-axis and\ndata count on the y-axis. To create a histogram corresponding to Table 1-5 in R,\nuse the hist function with the breaks argument:\nhist(state[[\"Population\"]], breaks=breaks)\nThe histogram is shown in Figure 1-3. In general, histograms are plotted such that:\n\n\nEmpty bins are included in the graph.\nBins are equal width.\nNumber of bins (or, equivalently, bin size) is up to the user.\nBars are contiguous — no empty space shows between bars, unless there is\nan empty bin.\n\n\nSTATISTICAL MOMENTS\nIn statistical theory, location and variability are referred to as the first and second moments of a\ndistribution. The third and fourth moments are called skewness and kurtosis. Skewness refers to\nwhether the data is skewed to larger or smaller values and kurtosis indicates the propensity of the\ndata to have extreme values. Generally, metrics are not used to measure skewness and kurtosis;\ninstead, these are discovered through visual displays such as Figures 1-2 and 1-3.\n\n\nDensity Estimates\nRelated to the histogram is a density plot, which shows the distribution of data\nvalues as a continuous line. A density plot can be thought of as a smoothed\nhistogram, although it is typically computed directly from the data through a\nkernal density estimate (see [Duong-2001] for a short tutorial). Figure 1-4\ndisplays a density estimate superposed on a histogram. In R, you can compute a\ndensity estimate using the density function:\nhist(state[[\"Murder.Rate\"]], freq=FALSE)\nlines(density(state[[\"Murder.Rate\"]]), lwd=3, col=\"blue\")\nA key distinction from the histogram plotted in Figure 1-3 is the scale of the y-\naxis: a density plot corresponds to plotting the histogram as a proportion rather\nthan counts (you specify this in R using the argument freq=FALSE).\n\n\nDENSITY ESTIMATION\nDensity estimation is a rich topic with a long history in statistical literature. In fact, over 20 R\npackages have been published that offer functions for density estimation. [Deng-Wickham-2011]\ngive a comprehesive review of R packages, with a particular recommendation for ASH or\nKernSmooth. For many data science problems, there is no need to worry about the various types\nof density estimates; it suffices to use the base functions.\n\n\nFigure 1-4. Density of state murder rates\nKEY IDEAS\nA frequency histogram plots frequency counts on the y-axis and variable values on the x-axis; it\ngives a sense of the distribution of the data at a glance.\nA frequency table is a tabular version of the frequency counts found in a histogram.\nA boxplot — with the top and bottom of the box at the 75th and 25th percentiles, respectively —\nalso gives a quick sense of the distribution of the data; it is often used in side-by-side displays to\ncompare distributions.\n\n\nA density plot is a smoothed version of a histogram; it requires a function to estimate a plot based\non the data (multiple estimates are possible, of course).\n\n\nFurther Reading\nA SUNY Oswego professor provides a step-by-step guide to creating a\nboxplot.\nDensity estimation in R is covered in Henry Deng and Hadley Wickham’s\npaper of the same name.\nR-Bloggers has a useful post on histograms in R, including customization\nelements, such as binning (breaks)\nR-Bloggers also has similar post on boxplots in R.\n\n\nExploring Binary and Categorical Data\nFor categorical data, simple proportions or percentages tell the story of the data.\nKEY TERMS FOR EXPLORING CATEGORICAL DATA\nMode\nThe most commonly occurring category or value in a data set.\nExpected value\nWhen the categories can be associated with a numeric value, this gives an average value based on\na category’s probability of occurrence.\nBar charts\nThe frequency or proportion for each category plotted as bars.\nPie charts\nThe frequency or proportion for each category plotted as wedges in a pie.\nGetting a summary of a binary variable or a categorical variable with a few\ncategories is a fairly easy matter: we just figure out the proportion of 1s, or of the\nimportant categories. For example, Table 1-6 shows the percentage of delayed\nflights by the cause of delay at Dallas/Fort Worth airport since 2010. Delays are\ncategorized as being due to factors under carrier control, air traffic control (ATC)\nsystem delays, weather, security, or a late inbound aircraft.\nTable 1-6. Percentage of delays\nby cause at Dallas-Fort Worth\nairport\nCarrier ATC\nWeather Security Inbound\n23.02\n30.40 4.03\n0.12\n42.43\nBar charts are a common visual tool for displaying a single categorical variable,\noften seen in the popular press. Categories are listed on the x-axis, and\nfrequencies or proportions on the y-axis. Figure 1-5 shows the airport delays per\nyear by cause for Dallas/Fort Worth, and it is produced with the R function\nbarplot:\n\n\nbarplot(as.matrix(dfw)/6, cex.axis=.5)\n\n\nFigure 1-5. Bar plot airline delays at DFW by cause\nNote that a bar chart resembles a histogram; in a bar chart the x-axis represents\ndifferent categories of a factor variable, while in a histogram the x-axis represents\nvalues of a single variable on a numeric scale. In a histogram, the bars are\ntypically shown touching each other, with gaps indicating values that did not occur\n\n\nin the data. In a bar chart, the bars are shown separate from one another.\nPie charts are an alternative to bar charts, although statisticians and data\nvisualization experts generally eschew pie charts as less visually informative (see\n[Few-2007]).\n\n\nNUMERICAL DATA AS CATEGORICAL DATA\nIn “Frequency Table and Histograms”, we looked at frequency tables based on binning the data.\nThis implicitly converts the numeric data to an ordered factor. In this sense, histograms and bar\ncharts are similar, except that the categories on the x-axis in the bar chart are not ordered.\nConverting numeric data to categorical data is an important and widely used step in data analysis\nsince it reduces the complexity (and size) of the data. This aids in the discovery of relationships\nbetween features, particularly at the initial stages of an analysis.\n\n\nMode\nThe mode is the value — or values in case of a tie — that appears most often in\nthe data. For example, the mode of the cause of delay at Dallas/Fort Worth airport\nis “Inbound.” As another example, in most parts of the United States, the mode for\nreligious preference would be Christian. The mode is a simple summary statistic\nfor categorical data, and it is generally not used for numeric data.\n\n\nExpected Value\nA special type of categorical data is data in which the categories represent or can\nbe mapped to discrete values on the same scale. A marketer for a new cloud\ntechnology, for example, offers two levels of service, one priced at $300/month\nand another at $50/month. The marketer offers free webinars to generate leads,\nand the firm figures that 5% of the attendees will sign up for the $300 service,\n15% for the $50 service, and 80% will not sign up for anything. This data can be\nsummed up, for financial purposes, in a single “expected value,” which is a form\nof weighted mean in which the weights are probabilities.\nThe expected value is calculated as follows:\n1. Multiply each outcome by its probability of occurring.\n2. Sum these values.\nIn the cloud service example, the expected value of a webinar attendee is thus\n$22.50 per month, calculated as follows:\nThe expected value is really a form of weighted mean: it adds the ideas of future\nexpectations and probability weights, often based on subjective judgment.\nExpected value is a fundamental concept in business valuation and capital\nbudgeting — for example, the expected value of five years of profits from a new\nacquisition, or the expected cost savings from new patient management software at\na clinic.\nKEY IDEAS\nCategorical data is typically summed up in proportions, and can be visualized in a bar chart.\nCategories might represent distinct things (apples and oranges, male and female), levels of a factor\nvariable (low, medium, and high), or numeric data that has been binned.\nExpected value is the sum of values times their probability of occurrence, often used to sum up\nfactor variable levels.\n\n\nFurther Reading\nNo statistics course is complete without a lesson on misleading graphs, which\noften involve bar charts and pie charts.\n\n\nCorrelation\nExploratory data analysis in many modeling projects (whether in data science or\nin research) involves examining correlation among predictors, and between\npredictors and a target variable. Variables X and Y (each with measured data) are\nsaid to be positively correlated if high values of X go with high values of Y, and\nlow values of X go with low values of Y. If high values of X go with low values\nof Y, and vice versa, the variables are negatively correlated.\nKEY TERMS FOR CORRELATION\nCorrelation coefficient\nA metric that measures the extent to which numeric variables are associated with one another\n(ranges from –1 to +1).\nCorrelation matrix\nA table where the variables are shown on both rows and columns, and the cell values are the\ncorrelations between the variables.\nScatterplot\nA plot in which the x-axis is the value of one variable, and the y-axis the value of another.\nConsider these two variables, perfectly correlated in the sense that each goes from\nlow to high:\nv1: {1, 2, 3}\nv2: {4, 5, 6}\nThe vector sum of products is 4 + 10 + 18 = 32. Now try shuffling one of them and\nrecalculating — the vector sum of products will never be higher than 32. So this\nsum of products could be used as a metric; that is, the observed sum of 32 could\nbe compared to lots of random shufflings (in fact, this idea relates to a\nresampling-based estimate: see “Permutation Test”). Values produced by this\nmetric, though, are not that meaningful, except by reference to the resampling\ndistribution.\nMore useful is a standardized variant: the correlation coefficient, which gives an\nestimate of the correlation between two variables that always lies on the same\n\n\nscale. To compute Pearson’s correlation coefficient, we multiply deviations from\nthe mean for variable 1 times those for variable 2, and divide by the product of the\nstandard deviations:\nNote that we divide by n – 1 instead of n; see “Degrees of Freedom, and n or n –\n1?” for more details. The correlation coefficient always lies between +1 (perfect\npositive correlation) and –1 (perfect negative correlation); 0 indicates no\ncorrelation.\nVariables can have an association that is not linear, in which case the correlation\ncoefficient may not be a useful metric. The relationship between tax rates and\nrevenue raised is an example: as tax rates increase from 0, the revenue raised also\nincreases. However, once tax rates reach a high level and approach 100%, tax\navoidance increases and tax revenue actually declines.\nTable 1-7, called a correlation matrix, shows the correlation between the daily\nreturns for telecommunication stocks from July 2012 through June 2015. From the\ntable, you can see that Verizon (VZ) and ATT (T) have the highest correlation.\nLevel Three (LVLT), which is an infrastructure company, has the lowest\ncorrelation. Note the diagonal of 1s (the correlation of a stock with itself is 1),\nand the redundancy of the information above and below the diagonal.\nTable 1-7. Correlation between\ntelecommunication stock\nreturns\nT\nCTL FTR VZ\nLVLT\nT\n1.000 0.475 0.328 0.678 0.279\nCTL\n0.475 1.000 0.420 0.417 0.287\nFTR\n0.328 0.420 1.000 0.287 0.260\n\n\nVZ\n0.678 0.417 0.287 1.000 0.242\nLVLT 0.279 0.287 0.260 0.242 1.000\nA table of correlations like Table 1-7 is commonly plotted to visually display the\nrelationship between multiple variables. Figure 1-6 shows the correlation\nbetween the daily returns for major exchange traded funds (ETFs). In R, we can\neasily create this using the package corrplot:\netfs <- sp500_px[row.names(sp500_px)>\"2012-07-01\",\n                 sp500_sym[sp500_sym$sector==\"etf\", 'symbol']]\nlibrary(corrplot)\ncorrplot(cor(etfs), method = \"ellipse\")\nThe ETFs for the S&P 500 (SPY) and the Dow Jones Index (DIA) have a high\ncorrelation. Similary, the QQQ and the XLK, composed mostly of technology\ncompanies, are postively correlated. Defensive ETFs, such as those tracking gold\nprices (GLD), oil prices (USO), or market volatility (VXX) tend to be negatively\ncorrelated with the other ETFs. The orientation of the ellipse indicates whether\ntwo variables are positively correlated (ellipse is pointed right) or negatively\ncorrelated (ellipse is pointed left). The shading and width of the ellipse indicate\nthe strength of the association: thinner and darker ellipses correspond to stronger\nrelationships.\nLike the mean and standard deviation, the correlation coefficient is sensitive to\noutliers in the data. Software packages offer robust alternatives to the classical\ncorrelation coefficient. For example, the R function cor has a trim argument\nsimilar to that for computing a trimmed mean (see [R-base-2015]).\n\n\nFigure 1-6. Correlation between ETF returns\n\n\nOTHER CORRELATION ESTIMATES\nStatisticians have long ago proposed other types of correlation coefficients, such as Spearman’s\nrho or Kendall’s tau. These are correlation coefficients based on the rank of the data. Since\nthey work with ranks rather than values, these estimates are robust to outliers and can handle\ncertain types of nonlinearities. However, data scientists can generally stick to Pearson’s\ncorrelation coefficient, and its robust alternatives, for exploratory analysis. The appeal of rank-\nbased estimates is mostly for smaller data sets and specific hypothesis tests.\n\n\nScatterplots\nThe standard way to visualize the relationship between two measured data\nvariables is with a scatterplot. The x-axis represents one variable, the y-axis\nanother, and each point on the graph is a record. See Figure 1-7 for a plot between\nthe daily returns for ATT and Verizon. This is produced in R with the command:\nplot(telecom$T, telecom$VZ, xlab=\"T\", ylab=\"VZ\")\nThe returns have a strong positive relationship: on most days, both stocks go up or\ngo down in tandem. There are very few days where one stock goes down\nsignificantly while the other stock goes up (and vice versa).\n\n\nFigure 1-7. Scatterplot between returns for ATT and Verizon\nKEY IDEAS FOR CORRELATION\nThe correlation coefficient measures the extent to which two variables are associated with one\nanother.\nWhen high values of v1 go with high values of v2, v1 and v2 are positively associated.\nWhen high values of v1 are associated with low values of v2, v1 and v2 are negatively associated.\n\n\nThe correlation coefficient is a standardized metric so that it always ranges from –1 (perfect\nnegative correlation) to +1 (perfect positive correlation).\nA correlation coefficient of 0 indicates no correlation, but be aware that random arrangements of\ndata will produce both positive and negative values for the correlation coefficient just by chance.\n\n\nFurther Reading\nStatistics, 4th ed., by David Freedman, Robert Pisani, and Roger Purves (W. W.\nNorton, 2007), has an excellent discussion of correlation.\n\n\nExploring Two or More Variables\nFamiliar estimators like mean and variance look at variables one at a time\n(univariate analysis). Correlation analysis (see “Correlation”) is an important\nmethod that compares two variables (bivariate analysis). In this section we look\nat additional estimates and plots, and at more than two variables (multivariate\nanalysis).\nKEY TERMS FOR EXPLORING TWO OR MORE VARIABLES\nContingency tables\nA tally of counts between two or more categorical variables.\nHexagonal binning\nA plot of two numeric variables with the records binned into hexagons.\nContour plots\nA plot showing the density of two numeric variables like a topographical map.\nViolin plots\nSimilar to a boxplot but showing the density estimate.\nLike univariate analysis, bivariate analysis involves both computing summary\nstatistics and producing visual displays. The appropriate type of bivariate or\nmultivariate analysis depends on the nature of the data: numeric versus\ncategorical.\n\n\nHexagonal Binning and Contours (Plotting Numeric versus\nNumeric Data)\nScatterplots are fine when there is a relatively small number of data values. The\nplot of stock returns in Figure 1-7 involves only about 750 points. For data sets\nwith hundreds of thousands or millions of records, a scatterplot will be too dense,\nso we need a different way to visualize the relationship. To illustrate, consider the\ndata set kc_tax, which contains the tax-assessed values for residential properties\nin King County, Washington. In order to focus on the main part of the data, we strip\nout very expensive and very small or large residences using the subset function:\nkc_tax0 <- subset(kc_tax, TaxAssessedValue < 750000 & SqFtTotLiving>100 &\n                  SqFtTotLiving<3500)\nnrow(kc_tax0)\n[1] 432733\nFigure 1-8 is a hexagon binning plot of the relationship between the finished\nsquare feet versus the tax-assessed value for homes in King County. Rather than\nplotting points, which would appear as a monolithic dark cloud, we grouped the\nrecords into hexagonal bins and plotted the hexagons with a color indicating the\nnumber of records in that bin. In this chart, the positive relationship between\nsquare feet and tax-assessed value is clear. An interesting feature is the hint of a\nsecond cloud above the main cloud, indicating homes that have the same square\nfootage as those in the main cloud, but a higher tax-assessed value.\nFigure 1-8 was generated by the powerful R package ggplot2, developed by\nHadley Wickham [ggplot2]. ggplot2 is one of several new software libraries for\nadvanced exploratory visual analysis of data; see “Visualizing Multiple\nVariables”.\nggplot(kc_tax0, (aes(x=SqFtTotLiving, y=TaxAssessedValue))) +\n  stat_binhex(colour=\"white\") +\n  theme_bw() +\n  scale_fill_gradient(low=\"white\", high=\"black\") +\n  labs(x=\"Finished Square Feet\", y=\"Tax Assessed Value\")\n\n\nFigure 1-8. Hexagonal binning for tax-assessed value versus finished square feet\nFigure 1-9 uses contours overlaid on a scatterplot to visualize the relationship\nbetween two numeric variables. The contours are essentially a topographical map\nto two variables; each contour band represents a specific density of points,\nincreasing as one nears a “peak.” This plot shows a similar story as Figure 1-8:\nthere is a secondary peak “north” of the main peak. This chart was also created\nusing ggplot2 with the built-in geom_density2d function.\n\n\nggplot(kc_tax0, aes(SqFtTotLiving, TaxAssessedValue)) +\n  theme_bw() +\n  geom_point( alpha=0.1) +\n  geom_density2d(colour=\"white\") +\n  labs(x=\"Finished Square Feet\", y=\"Tax Assessed Value\")\n\n\nFigure 1-9. Contour plot for tax-assessed value versus finished square feet\nOther types of charts are used to show the relationship between two numeric\nvariables, including heat maps. Heat maps, hexagonal binning, and contour plots\nall give a visual representation of a two-dimensional density. In this way, they are\nnatural analogs to histograms and density plots.\n\n\nTwo Categorical Variables\nA useful way to summarize two categorical variables is a contingency table — a\ntable of counts by category. Table 1-8 shows the contingency table between the\ngrade of a personal loan and the outcome of that loan. This is taken from data\nprovided by Lending Club, a leader in the peer-to-peer lending business. The\ngrade goes from A (high) to G (low). The outcome is either paid off, current, late,\nor charged off (the balance of the loan is not expected to be collected). This table\nshows the count and row percentages. High-grade loans have a very low\nlate/charge-off percentage as compared with lower-grade loans. Contingency\ntables can look at just counts, or also include column and total percentages. Pivot\ntables in Excel are perhaps the most common tool used to create contingency\ntables. In R, the CrossTable function in the descr package produces contingency\ntables, and the following code was used to create Table 1-8:\nlibrary(descr)\nx_tab <- CrossTable(lc_loans$grade, lc_loans$status,\n                    prop.c=FALSE, prop.chisq=FALSE, prop.t=FALSE)\nTable 1-8. Contingency table of loan grade\nand status\nGrade Fully paid Current Late\nCharged off Total\nA\n20715\n52058\n494\n1588\n74855\n0.277\n0.695\n0.007 0.021\n0.161\nB\n31782\n97601\n2149\n5384\n136916\n0.232\n0.713\n0.016 0.039\n0.294\nC\n23773\n92444\n2895\n6163\n125275\n0.190\n0.738\n0.023 0.049\n0.269\nD\n14036\n55287\n2421\n5131\n76875\n0.183\n0.719\n0.031 0.067\n0.165\nE\n6089\n25344\n1421\n2898\n35752\n0.170\n0.709\n0.040 0.081\n0.077\n\n\nF\n2376\n8675\n621\n1556\n13228\n0.180\n0.656\n0.047 0.118\n0.028\nG\n655\n2042\n206\n419\n3322\n0.197\n0.615\n0.062 0.126\n0.007\nTotal\n99426\n333451\n10207 23139\n466223\n\n\nCategorical and Numeric Data\nBoxplots (see “Percentiles and Boxplots”) are a simple way to visually compare\nthe distributions of a numeric variable grouped according to a categorical\nvariable. For example, we might want to compare how the percentage of flight\ndelays varies across airlines. Figure 1-10 shows the percentage of flights in a\nmonth that were delayed where the delay was within the carrier’s control.\nboxplot(pct_delay ~ airline, data=airline_stats, ylim=c(0, 50))\n\n\nFigure 1-10. Boxplot of percent of airline delays by carrier\nAlaska stands out as having the fewest delays, while American has the most\ndelays: the lower quartile for American is higher than the upper quartile for\nAlaska.\nA violin plot, introduced by [Hintze-Nelson-1998], is an enhancement to the\nboxplot and plots the density estimate with the density on the y-axis. The density is\nmirrored and flipped over and the resulting shape is filled in, creating an image\nresembling a violin. The advantage of a violin plot is that it can show nuances in\nthe distribution that aren’t perceptible in a boxplot. On the other hand, the boxplot\nmore clearly shows the outliers in the data. In ggplot2, the function geom_violin\n\n\ncan be used to create a violin plot as follows:\nggplot(data=airline_stats, aes(airline, pct_carrier_delay)) +\n  ylim(0, 50) +\n  geom_violin() +\n  labs(x=\"\", y=\"Daily % of Delayed Flights\")\nThe corresponding plot is shown in Figure 1-11. The violin plot shows a\nconcentration in the distribution near zero for Alaska, and to a lesser extent, Delta.\nThis phenomenon is not as obvious in the boxplot. You can combine a violin plot\nwith a boxplot by adding geom_boxplot to the plot (although this is best when\ncolors are used).\n\n\nFigure 1-11. Combination of boxplot and violin plot of percent of airline delays by carrier\n\n\nVisualizing Multiple Variables\nThe types of charts used to compare two variables — scatterplots, hexagonal\nbinning, and boxplots — are readily extended to more variables through the notion\nof conditioning. As an example, look back at Figure 1-8, which showed the\nrelationship between homes’ finished square feet and tax-assessed values. We\nobserved that there appears to be a cluster of homes that have higher tax-assessed\nvalue per square foot. Diving deeper, Figure 1-12 accounts for the effect of\nlocation by plotting the data for a set of zip codes. Now the picture is much\nclearer: tax-assessed value is much higher in some zip codes (98112, 98105) than\nin others (98108, 98057). This disparity gives rise to the clusters observed in\nFigure 1-8.\nWe created Figure 1-12 using ggplot2 and the idea of facets, or a conditioning\nvariable (in this case zip code):\nggplot(subset(kc_tax0, ZipCode %in% c(98188, 98105, 98108, 98126)),\n         aes(x=SqFtTotLiving, y=TaxAssessedValue)) +\n  stat_binhex(colour=\"white\") +\n  theme_bw() +\n  scale_fill_gradient( low=\"white\", high=\"blue\") +\n  labs(x=\"Finished Square Feet\", y=\"Tax Assessed Value\") +\n  facet_wrap(\"ZipCode\")\n\n\nFigure 1-12. Tax-assessed value versus finished square feet by zip code\nThe concept of conditioning variables in a graphics system was pioneered with\nTrellis graphics, developed by Rick Becker, Bill Cleveland, and others at Bell\nLabs [Trellis-Graphics]. This idea has propogated to various modern graphics\nsystems, such as the lattice ([lattice]) and ggplot2 packages in R and the\nSeaborn ([seaborne]) and Bokeh ([bokeh]) modules in Python. Conditioning\nvariables are also integral to business intelligence platforms such as Tableau and\nSpotfire. With the advent of vast computing power, modern visualization platforms\nhave moved well beyond the humble beginnings of exploratory data analysis.\nHowever, key concepts and tools developed over the years still form a foundation\nfor these systems.\nKEY IDEAS\n\n\nHexagonal binning and contour plots are useful tools that permit graphical examination of two\nnumeric variables at a time, without being overwhelmed by huge amounts of data.\nContingency tables are the standard tool for looking at the counts of two categorical variables.\nBoxplots and violin plots allow you to plot a numeric variable against a categorical variable.\n\n\nFurther Reading\nModern Data Science with R, by Benjamin Baumer, Daniel Kaplan, and\nNicholas Horton (CRC Press, 2017), has an excellent presentation of “a\ngrammar for graphics” (the “gg” in ggplot).\nGgplot2: Elegant Graphics for Data Analysis, by Hadley Wickham, is an\nexcellent resource from the creator of ggplot2 (Springer, 2009).\nJosef Fruehwald has a web-based tutorial on ggplot2.\n\n\nSummary\nWith the development of exploratory data analysis (EDA), pioneered by John\nTukey, statistics set a foundation that was a precursor to the field of data science.\nThe key idea of EDA is that the first and most important step in any project based\non data is to look at the data. By summarizing and visualizing the data, you can\ngain valuable intuition and understanding of the project.\nThis chapter has reviewed concepts ranging from simple metrics, such as\nestimates of location and variability, to rich visual displays to explore the\nrelationships between multiple variables, as in Figure 1-12. The diverse set of\ntools and techniques being developed by the open source community, combined\nwith the expressiveness of the R and Python languages, has created a plethora of\nways to explore and analyze data. Exploratory analysis should be a cornerstone of\nany data science project.\n\n\nChapter 2. Data and Sampling\nDistributions\nA popular misconception holds that the era of big data means the end of a need for\nsampling. In fact, the proliferation of data of varying quality and relevance\nreinforces the need for sampling as a tool to work efficiently with a variety of data\nand to minimize bias. Even in a big data project, predictive models are typically\ndeveloped and piloted with samples. Samples are also used in tests of various\nsorts (e.g., pricing, web treatments).\nFigure 2-1 shows a schematic that underpins the concepts in this chapter. The\nlefthand side represents a population that, in statistics, is assumed to follow an\nunderlying but unknown distribution. The only thing available is the sample data\nand its empirical distribution, shown on the righthand side. To get from the\nlefthand side to the righthand side, a sampling procedure is used (represented by\ndashed arrows). Traditional statistics focused very much on the lefthand side,\nusing theory based on strong assumptions about the population. Modern statistics\nhas moved to the righthand side, where such assumptions are not needed.\n",
      "page_number": 13
    },
    {
      "number": 2,
      "title": "Data and Sampling",
      "start_page": 93,
      "end_page": 153,
      "detection_method": "regex_chapter_title",
      "content": "Figure 2-1. Population versus sample\nIn general, data scientists need not worry about the theoretical nature of the\nlefthand side, and instead should focus on the sampling procedures and the data at\nhand. There are some notable exceptions. Sometimes data is generated from a\nphysical process that can be modeled. The simplest example is flipping a coin:\nthis follows a binomial distribution. Any real-life binomial situation (buy or don’t\nbuy, fraud or no fraud, click or don’t click) can be modeled effectively by a coin\n(with modified probability of landing heads, of course). In these cases, we can\ngain additional insight by using our understanding of the population.\n\n\nRandom Sampling and Sample Bias\nA sample is a subset of data from a larger data set; statisticians call this larger\ndata set the population. A population in statistics is not the same thing as in\nbiology — it is a large, defined but sometimes theoretical or imaginary, set of\ndata.\nKEY TERMS FOR RANDOM SAMPLING\nSample\nA subset from a larger data set.\nPopulation\nThe larger data set or idea of a data set.\nN (n)\nThe size of the population (sample).\nRandom sampling\nDrawing elements into a sample at random.\nStratified sampling\nDividing the population into strata and randomly sampling from each strata.\nSimple random sample\nThe sample that results from random sampling without stratifying the population.\nSample bias\nA sample that misrepresents the population.\nRandom sampling is a process in which each available member of the population\nbeing sampled has an equal chance of being chosen for the sample at each draw.\nThe sample that results is called a simple random sample. Sampling can be done\nwith replacement, in which observations are put back in the population after each\ndraw for possible future reselection. Or it can be done without replacement, in\nwhich case observations, once selected, are unavailable for future draws.\nData quality often matters more than data quantity when making an estimate or a\nmodel based on a sample. Data quality in data science involves completeness,\nconsistency of format, cleanliness, and accuracy of individual data points.\nStatistics adds the notion of representativeness.\n\n\nThe classic example is the Literary Digest poll of 1936 that predicted a victory of\nAl Landon against Franklin Roosevelt. The Literary Digest, a leading periodical\nof the day, polled its entire subscriber base, plus additional lists of individuals, a\ntotal of over 10 million, and predicted a landslide victory for Landon. George\nGallup, founder of the Gallup Poll, conducted biweekly polls of just 2,000, and\naccurately predicted a Roosevelt victory. The difference lay in the selection of\nthose polled.\nThe Literary Digest opted for quantity, paying little attention to the method of\nselection. They ended up polling those with relatively high socioeconomic status\n(their own subscribers, plus those who, by virtue of owning luxuries like\ntelephones and automobiles, appeared in marketers’ lists). The result was sample\nbias; that is, the sample was different in some meaningful nonrandom way from\nthe larger population it was meant to represent. The term nonrandom is important\n— hardly any sample, including random samples, will be exactly representative of\nthe population. Sample bias occurs when the difference is meaningful, and can be\nexpected to continue for other samples drawn in the same way as the first.\n\n\nSELF-SELECTION SAMPLING BIAS\nThe reviews of restaurants, hotels, cafes, and so on that you read on social media sites like Yelp\nare prone to bias because the people submitting them are not randomly selected; rather, they\nthemselves have taken the initiative to write. This leads to self-selection bias — the people\nmotivated to write reviews may be those who had poor experiences, may have an association\nwith the establishment, or may simply be a different type of person from those who do not write\nreviews. Note that while self-selection samples can be unreliable indicators of the true state of\naffairs, they may be more reliable in simply comparing one establishment to a similar one; the\nsame self-selection bias might apply to each.\n\n\nBias\nStatistical bias refers to measurement or sampling errors that are systematic and\nproduced by the measurement or sampling process. An important distinction\nshould be made between errors due to random chance, and errors due to bias.\nConsider the physical process of a gun shooting at a target. It will not hit the\nabsolute center of the target every time, or even much at all. An unbiased process\nwill produce error, but it is random and does not tend strongly in any direction\n(see Figure 2-2). The results shown in Figure 2-3 show a biased process — there\nis still random error in both the x and y direction, but there is also a bias. Shots\ntend to fall in the upper-right quadrant.\nFigure 2-2. Scatterplot of shots from a gun with true aim\n\n\nFigure 2-3. Scatterplot of shots from a gun with biased aim\nBias comes in different forms, and may be observable or invisible. When a result\ndoes suggest bias (e.g., by reference to a benchmark or actual values), it is often\nan indicator that a statistical or machine learning model has been misspecified, or\nan important variable left out.\n\n\nRandom Selection\nTo avoid the problem of sample bias that led the Literary Digest to predict\nLandon over Roosevelt, George Gallup (shown in Figure 2-4) opted for more\nscientifically chosen methods to achieve a sample that was representative of the\nUS voter. There are now a variety of methods to achieve representativeness, but at\nthe heart of all of them lies random sampling.\nFigure 2-4. George Gallup, catapulted to fame by the Literary Digest’s “big data” failure\nRandom sampling is not always easy. Proper definition of an accessible\npopulation is key. Suppose we want to generate a representative profile of\ncustomers and we need to conduct a pilot customer survey. The survey needs to be\nrepresentative but is labor intensive.\nFirst we need to define who a customer is. We might select all customer records\nwhere purchase amount > 0. Do we include all past customers? Do we include\nrefunds? Internal test purchases? Resellers? Both billing agent and customer?\nNext we need to specify a sampling procedure. It might be “select 100 customers\nat random.” Where a sampling from a flow is involved (e.g., real-time customer\ntransactions or web visitors), timing considerations may be important (e.g., a web\nvisitor at 10 a.m. on a weekday may be different from a web visitor at 10 p.m. on\na weekend).\nIn stratified sampling, the population is divided up into strata, and random\nsamples are taken from each stratum. Political pollsters might seek to learn the\nelectoral preferences of whites, blacks, and Hispanics. A simple random sample\nwww.allitebooks.com\n\n\ntaken from the population would yield too few blacks and Hispanics, so those\nstrata could be overweighted in stratified sampling to yield equivalent sample\nsizes.\n\n\nSize versus Quality: When Does Size Matter?\nIn the era of big data, it is sometimes surprising that smaller is better. Time and\neffort spent on random sampling not only reduce bias, but also allow greater\nattention to data exploration and data quality. For example, missing data and\noutliers may contain useful information. It might be prohibitively expensive to\ntrack down missing values or evaluate outliers in millions of records, but doing so\nin a sample of several thousand records may be feasible. Data plotting and manual\ninspection bog down if there is too much data.\nSo when are massive amounts of data needed?\nThe classic scenario for the value of big data is when the data is not only big, but\nsparse as well. Consider the search queries received by Google, where columns\nare terms, rows are individual search queries, and cell values are either 0 or 1,\ndepending on whether a query contains a term. The goal is to determine the best\npredicted search destination for a given query. There are over 150,000 words in\nthe English language, and Google processes over 1 trillion queries per year. This\nyields a huge matrix, the vast majority of whose entries are “0.”\nThis is a true big data problem — only when such enormous quantities of data are\naccumulated can effective search results be returned for most queries. And the\nmore data accumulates, the better the results. For popular search terms this is not\nsuch a problem — effective data can be found fairly quickly for the handful of\nextremely popular topics trending at a particular time. The real value of modern\nsearch technology lies in the ability to return detailed and useful results for a huge\nvariety of search queries, including those that occur only with a frequency, say, of\none in a million.\nConsider the search phrase “Ricky Ricardo and Little Red Riding Hood.” In the\nearly days of the internet, this query would probably have returned results on\nRicky Ricardo the band leader, the television show I Love Lucy in which he\nstarred, and the children’s story Little Red Riding Hood. Later, now that trillions\nof search queries have been accumulated, this search query returns the exact I\nLove Lucy episode in which Ricky narrates, in dramatic fashion, the Little Red\nRiding Hood story to his infant son in a comic mix of English and Spanish.\nKeep in mind that the number of actual pertinent records — ones in which this\n\n\nexact search query, or something very similar, appears (together with information\non what link people ultimately clicked on) — might need only be in the thousands\nto be effective. However, many trillions of data points are needed in order to\nobtain these pertinent records (and random sampling, of course, will not help).\nSee also “Long-Tailed Distributions”.\n\n\nSample Mean versus Population Mean\nThe symbol \n (pronounced x-bar) is used to represent the mean of a sample from\na population, whereas  is used to represent the mean of a population. Why make\nthe distinction? Information about samples is observed, and information about\nlarge populations is often inferred from smaller samples. Statisticians like to keep\nthe two things separate in the symbology.\nKEY IDEAS\nEven in the era of big data, random sampling remains an important arrow in the data scientist’s\nquiver.\nBias occurs when measurements or observations are systematically in error because they are not\nrepresentative of the full population.\nData quality is often more important than data quantity, and random sampling can reduce bias and\nfacilitate quality improvement that would be prohibitively expensive.\n\n\nFurther Reading\nA useful review of sampling procedures can be found in Ronald Fricker’s\nchapter “Sampling Methods for Web and E-mail Surveys,” found in the Sage\nHandbook of Online Research Methods. This chapter includes a review of\nthe modifications to random sampling that are often used for practical\nreasons of cost or feasibility.\nThe story of the Literary Digest poll failure can be found on the Capital\nCentury website.\n\n\nSelection Bias\nTo paraphrase Yogi Berra, “If you don’t know what you’re looking for, look hard\nenough and you’ll find it.”\nSelection bias refers to the practice of selectively choosing data — consciously or\nunconsciously — in a way that that leads to a conclusion that is misleading or\nephemeral.\nKEY TERMS\nBias\nSystematic error.\nData snooping\nExtensive hunting through data in search of something interesting.\nVast search effect\nBias or nonreproducibility resulting from repeated data modeling, or modeling data with large\nnumbers of predictor variables.\nIf you specify a hypothesis and conduct a well-designed experiment to test it, you\ncan have high confidence in the conclusion. Such is often not the case, however.\nOften, one looks at available data and tries to discern patterns. But is the pattern\nfor real, or just the product of data snooping — that is, extensive hunting through\nthe data until something interesting emerges? There is a saying among statisticians:\n“If you torture the data long enough, sooner or later it will confess.”\nThe difference between a phenomenon that you verify when you test a hypothesis\nusing an experiment, versus a phenomenon that you discover by perusing available\ndata, can be illuminated with the following thought experiment.\nImagine that someone tells you she can flip a coin and have it land heads on the\nnext 10 tosses. You challenge her (the equivalent of an experiment), and she\nproceeds to toss it 10 times, all landing heads. Clearly you ascribe some special\ntalent to her — the probability that 10 coin tosses will land heads just by chance\nis 1 in 1,000.\nNow imagine that the announcer at a sports stadium asks the 20,000 people in\nattendance each to toss a coin 10 times, and report to an usher if they get 10 heads\n\n\nin a row. The chance that somebody in the stadium will get 10 heads is extremely\nhigh (more than 99% — it’s 1 minus the probability that nobody gets 10 heads).\nClearly, selecting, after the fact, the person (or persons) who gets 10 heads at the\nstadium does not indicate they have any special talent — it’s most likely luck.\nSince repeated review of large data sets is a key value proposition in data\nscience, selection bias is something to worry about. A form of selection bias of\nparticular concern to data scientists is what John Elder (founder of Elder\nResearch, a respected data mining consultancy) calls the vast search effect. If you\nrepeatedly run different models and ask different questions with a large data set,\nyou are bound to find something interesting. Is the result you found truly something\ninteresting, or is it the chance outlier?\nWe can guard against this by using a holdout set, and sometimes more than one\nholdout set, against which to validate performance. Elder also advocates the use\nof what he calls target shuffling (a permutation test, in essence) to test the validity\nof predictive associations that a data mining model suggests.\nTypical forms of selection bias in statistics, in addition to the vast search effect,\ninclude nonrandom sampling (see sampling bias), cherry-picking data, selection\nof time intervals that accentuate a partiular statistical effect, and stopping an\nexperiment when the results look “interesting.”\n\n\nRegression to the Mean\nRegression to the mean refers to a phenomenon involving successive\nmeasurements on a given variable: extreme observations tend to be followed by\nmore central ones. Attaching special focus and meaning to the extreme value can\nlead to a form of selection bias.\nSports fans are familiar with the “rookie of the year, sophomore slump”\nphenomenon. Among the athletes who begin their career in a given season (the\nrookie class), there is always one who performs better than all the rest. Generally,\nthis “rookie of the year” does not do as well in his second year. Why not?\nIn nearly all major sports, at least those played with a ball or puck, there are two\nelements that play a role in overall performance:\nSkill\nLuck\nRegression to the mean is a consequence of a particular form of selection bias.\nWhen we select the rookie with the best performance, skill and good luck are\nprobably contributing. In his next season, the skill will still be there but, in most\ncases, the luck will not, so his performance will decline — it will regress. The\nphenomenon was first identified by Francis Galton in 1886 [Galton-1886], who\nwrote of it in connection with genetic tendencies; for example, the children of\nextremely tall men tend not to be as tall as their father (see Figure 2-5).\n\n\nFigure 2-5. Galton’s study that identified the phenomenon of regression to the mean\nWARNING\nRegression to the mean, meaning to “go back,” is distinct from the statistical modeling method of\nlinear regression, in which a linear relationship is estimated between predictor variables and an\noutcome variable.\nKEY IDEAS\nSpecifying a hypothesis, then collecting data following randomization and random sampling\nprinciples, ensures against bias.\nAll other forms of data analysis run the risk of bias resulting from the data collection/analysis\n\n\nprocess (repeated running of models in data mining, data snooping in research, and after-the-fact\nselection of interesting events).\n\n\nFurther Reading\nChristopher J. Pannucci and Edwin G. Wilkins’ article “Identifying and\nAvoiding Bias in Research” in (surprisingly) Plastic and Reconstructive\nSurgery (August 2010) has an excellent review of various types of bias that\ncan enter into research, including selection bias.\nMichael Harris’s article “Fooled by Randomness Through Selection Bias”\nprovides an interesting review of selection bias considerations in stock\nmarket trading schemes, from the perspective of traders.\n\n\nSampling Distribution of a Statistic\nThe term sampling distribution of a statistic refers to the distribution of some\nsample statistic, over many samples drawn from the same population. Much of\nclassical statistics is concerned with making inferences from (small) samples to\n(very large) populations.\nKEY TERMS\nSample statistic\nA metric calculated for a sample of data drawn from a larger population.\nData distribution\nThe frequency distribution of individual values in a data set.\nSampling distribution\nThe frequency distribution of a sample statistic over many samples or resamples.\nCentral limit theorem\nThe tendency of the sampling distribution to take on a normal shape as sample size rises.\nStandard error\nThe variability (standard deviation) of a sample statistic over many samples (not to be confused\nwith standard deviation, which, by itself, refers to variability of individual data values).\nTypically, a sample is drawn with the goal of measuring something (with a sample\nstatistic) or modeling something (with a statistical or machine learning model).\nSince our estimate or model is based on a sample, it might be in error; it might be\ndifferent if we were to draw a different sample. We are therefore interested in\nhow different it might be — a key concern is sampling variability. If we had lots\nof data, we could draw additional samples and observe the distribution of a\nsample statistic directly. Typically, we will calculate our estimate or model using\nas much data as is easily available, so the option of drawing additional samples\nfrom the population is not readily available.\nWARNING\nIt is important to distinguish between the distribution of the individual data points, known as the\ndata distribution, and the distribution of a sample statistic, known as the sampling distribution.\n\n\nThe distribution of a sample statistic such as the mean is likely to be more regular\nand bell-shaped than the distribution of the data itself. The larger the sample that\nthe statistic is based on, the more this is true. Also, the larger the sample, the\nnarrower the distribution of the sample statistic.\nThis is illustrated in an example using annual income for loan applicants to\nLending Club (see “A Small Example: Predicting Loan Default” for a description\nof the data). Take three samples from this data: a sample of 1,000 values, a\nsample of 1,000 means of 5 values, and a sample of 1,000 means of 20 values.\nThen plot a histogram of each sample to produce Figure 2-6.\n\n\nFigure 2-6. Histogram of annual incomes of 1,000 loan applicants (top), then 1000 means of n=5\napplicants (middle), and n=20 (bottom)\nThe histogram of the individual data values is broadly spread out and skewed\ntoward higher values as is to be expected with income data. The histograms of the\nmeans of 5 and 20 are increasingly compact and more bell-shaped. Here is the R\ncode to generate these histograms, using the visualization package ggplot2.\nlibrary(ggplot2)\n# take a simple random sample\nsamp_data <- data.frame(income=sample(loans_income, 1000),\n                        type='data_dist')\n# take a sample of means of 5 values\nsamp_mean_05 <- data.frame(\n  income = tapply(sample(loans_income, 1000*5),\n                  rep(1:1000, rep(5, 1000)), FUN=mean),\n  type = 'mean_of_5')\n# take a sample of means of 20 values\nsamp_mean_20 <- data.frame(\n  income = tapply(sample(loans_income, 1000*20),\n                  rep(1:1000, rep(20, 1000)), FUN=mean),\n  type = 'mean_of_20')\n# bind the data.frames and convert type to a factor\nincome <- rbind(samp_data, samp_mean_05, samp_mean_20)\nincome$type = factor(income$type,\n                     levels=c('data_dist', 'mean_of_5', 'mean_of_20'),\n                     labels=c('Data', 'Mean of 5', 'Mean of 20'))\n# plot the histograms\nggplot(income, aes(x=income)) +\n  geom_histogram(bins=40) +\n  facet_grid(type ~ .)\n\n\nCentral Limit Theorem\nThis phenomenon is termed the central limit theorem. It says that the means drawn\nfrom multiple samples will resemble the familiar bell-shaped normal curve (see\n“Normal Distribution”), even if the source population is not normally distributed,\nprovided that the sample size is large enough and the departure of the data from\nnormality is not too great. The central limit theorem allows normal-approximation\nformulas like the t-distribution to be used in calculating sampling distributions for\ninference — that is, confidence intervals and hypothesis tests.\nThe central limit theorem receives a lot of attention in traditional statistics texts\nbecause it underlies the machinery of hypothesis tests and confidence intervals,\nwhich themselves consume half the space in such texts. Data scientists should be\naware of this role, but, since formal hypothesis tests and confidence intervals play\na small role in data science, and the bootstrap is available in any case, the central\nlimit theorem is not so central in the practice of data science.\n\n\nStandard Error\nThe standard error is a single metric that sums up the variability in the sampling\ndistribution for a statistic. The standard error can be estimated using a statistic\nbased on the standard deviation s of the sample values, and the sample size n:\nAs the sample size increases, the standard error decreases, corresponding to what\nwas observed in Figure 2-6. The relationship between standard error and sample\nsize is sometimes referred to as the square-root of n rule: in order to reduce the\nstandard error by a factor of 2, the sample size must be increased by a factor of 4.\nThe validity of the standard error formula arises from the central limit theorem\n(see “Central Limit Theorem”). In fact, you don’t need to rely on the central limit\ntheorem to understand standard error. Consider the following approach to measure\nstandard error:\n1. Collect a number of brand new samples from the population.\n2. For each new sample, calculate the statistic (e.g., mean).\n3. Calculate the standard deviation of the statistics computed in step 2; use\nthis as your estimate of standard error.\nIn practice, this approach of collecting new samples to estimate the standard error\nis typically not feasible (and statistically very wasteful). Fortunately, it turns out\nthat it is not necessary to draw brand new samples; instead, you can use bootstrap\nresamples (see “The Bootstrap”). In modern statistics, the bootstrap has become\nthe standard way to to estimate standard error. It can be used for virtually any\nstatistic and does not rely on the central limit theorem or other distributional\nassumptions.\n\n\nSTANDARD DEVIATION VERSUS STANDARD ERROR\nDo not confuse standard deviation (which measures the variability of individual data points) with\nstandard error (which measures the variability of a sample metric).\nKEY IDEAS\nThe frequency distribution of a sample statistic tells us how that metric would turn out differently\nfrom sample to sample.\nThis sampling distribution can be estimated via the bootstrap, or via formulas that rely on the central\nlimit theorem.\nA key metric that sums up the variability of a sample statistic is its standard error.\n\n\nFurther Reading\nDavid Lane’s online multimedia resource in statistics has a useful simulation that\nallows you to select a sample statistic, a sample size and number of iterations and\nvisualize a histogram of the resulting frequency distribution.\n\n\nThe Bootstrap\nOne easy and effective way to estimate the sampling distribution of a statistic, or\nof model parameters, is to draw additional samples, with replacement, from the\nsample itself and recalculate the statistic or model for each resample. This\nprocedure is called the bootstrap, and it does not necessarily involve any\nassumptions about the data or the sample statistic being normally distributed.\nKEY TERMS\nBootstrap sample\nA sample taken with replacement from an observed data set.\nResampling\nThe process of taking repeated samples from observed data; includes both bootstrap and\npermutation (shuffling) procedures.\nConceptually, you can imagine the bootstrap as replicating the original sample\nthousands or millions of times so that you have a hypothetical population that\nembodies all the knowledge from your original sample (it’s just larger). You can\nthen draw samples from this hypothetical population for the purpose of estimating\na sampling distribution. See Figure 2-7.\nFigure 2-7. The idea of the bootstrap\n\n\nIn practice, it is not necessary to actually replicate the sample a huge number of\ntimes. We simply replace each observation after each draw; that is, we sample\nwith replacement. In this way we effectively create an infinite population in\nwhich the probability of an element being drawn remains unchanged from draw to\ndraw. The algorithm for a bootstrap resampling of the mean is as follows, for a\nsample of size n:\n1. Draw a sample value, record, replace it.\n2. Repeat n times.\n3. Record the mean of the n resampled values.\n4. Repeat steps 1–3 R times.\n5. Use the R results to:\na. Calculate their standard deviation (this estimates sample mean\nstandard error).\nb. Produce a histogram or boxplot.\nc. Find a confidence interval.\nR, the number of iterations of the bootstrap, is set somewhat arbitrarily. The more\niterations you do, the more accurate the estimate of the standard error, or the\nconfidence interval. The result from this procedure is a bootstrap set of sample\nstatistics or estimated model parameters, which you can then examine to see how\nvariable they are.\nThe R package boot combines these steps in one function. For example, the\nfollowing applies the bootstrap to the incomes of people taking out loans:\nlibrary(boot)\nstat_fun <- function(x, idx) median(x[idx])\nboot_obj <- boot(loans_income, R = 1000, statistic=stat_fun)\nThe function stat_fun computes the median for a given sample specified by the\nindex idx. The result is as follows:\nBootstrap Statistics :\n    original   bias    std. error\n\n\nt1*    62000 -70.5595    209.1515\nThe original estimate of the median is $62,000. The bootstrap distribution\nindicates that the estimate has a bias of about –$70 and a standard error of $209.\nThe bootstrap can be used with multivariate data, where the rows are sampled as\nunits (see Figure 2-8). A model might then be run on the bootstrapped data, for\nexample, to estimate the stability (variability) of model parameters, or to improve\npredictive power. With classification and regression trees (also called decision\ntrees), running multiple trees on bootstrap samples and then averaging their\npredictions (or, with classification, taking a majority vote) generally performs\nbetter than using a single tree. This process is called bagging (short for\n“bootstrap aggregating”: see “Bagging and the Random Forest”).\nFigure 2-8. Multivariate bootstrap sampling\nThe repeated resampling of the bootstrap is conceptually simple, and Julian\nSimon, an economist and demographer, published a compendium of resampling\nexamples, including the bootstrap, in his 1969 text Basic Research Methods in\n\n\nSocial Science (Random House). However, it is also computationally intensive,\nand was not a feasible option before the widespread availability of computing\npower. The technique gained its name and took off with the publication of several\njournal articles and a book by Stanford statistician Bradley Efron in the late 1970s\nand early 1980s. It was particularly popular among researchers who use statistics\nbut are not statisticians, and for use with metrics or models where mathematical\napproximations are not readily available. The sampling distribution of the mean\nhas been well established since 1908; the sampling distribution of many other\nmetrics has not. The bootstrap can be used for sample size determination;\nexperiment with different values for n to see how the sampling distribution is\naffected.\nThe bootstrap met with considerable skepticism when it was first introduced; it\nhad the aura to many of spinning gold from straw. This skepticism stemmed from a\nmisunderstanding of the bootstrap’s purpose.\nWARNING\nThe bootstrap does not compensate for a small sample size; it does not create new data, nor does\nit fill in holes in an existing data set. It merely informs us about how lots of additional samples\nwould behave when drawn from a population like our original sample.\n\n\nResampling versus Bootstrapping\nSometimes the term resampling is used synonymously with the term\nbootstrapping, as just outlined. More often, the term resampling also includes\npermutation procedures (see “Permutation Test”), where multiple samples are\ncombined and the sampling may be done without replacement. In any case, the\nterm bootstrap always implies sampling with replacement from an observed data\nset.\nKEY IDEAS\nThe bootstrap (sampling with replacement from a data set) is a powerful tool for assessing the\nvariability of a sample statistic.\nThe bootstrap can be applied in similar fashion in a wide variety of circumstances, without\nextensive study of mathematical approximations to sampling distributions.\nIt also allows us to estimate sampling distributions for statistics where no mathematical\napproximation has been developed.\nWhen applied to predictive models, aggregating multiple bootstrap sample predictions (bagging)\noutperforms the use of a single model.\n\n\nFurther Reading\nAn Introduction to the Bootstrap by Bradley Efron and Robert Tibshirani\n(Chapman Hall, 1993) was the first book-length treatment of the bootstrap. It\nis still widely read.\nThe retrospective on the bootstrap in the May 2003 issue of Statistical\nScience, (vol. 18, no. 2), discusses (among other antecedents, in Peter Hall’s\n“Prehistory”) Julian Simon’s first publication of the bootstrap in 1969.\nSee An Introduction to Statistical Learning by Gareth James et al.\n(Springer, 2013) for sections on the bootstrap and, in particular, bagging.\n\n\nConfidence Intervals\nFrequency tables, histograms, boxplots, and standard errors are all ways to\nunderstand the potential error in a sample estimate. Confidence intervals are\nanother.\nKEY TERMS\nConfidence level\nThe percentage of confidence intervals, constructed in the same way from the same population,\nexpected to contain the statistic of interest.\nInterval endpoints\nThe top and bottom of the confidence interval.\nThere is a natural human aversion to uncertainty; people (especially experts) say,\n“I don’t know” far too rarely. Analysts and managers, while acknowledging\nuncertainty, nonetheless place undue faith in an estimate when it is presented as a\nsingle number (a point estimate). Presenting an estimate not as a single number\nbut as a range is one way to counteract this tendency. Confidence intervals do this\nin a manner grounded in statistical sampling principles.\nConfidence intervals always come with a coverage level, expressed as a (high)\npercentage, say 90% or 95%. One way to think of a 90% confidence interval is as\nfollows: it is the interval that encloses the central 90% of the bootstrap sampling\ndistribution of a sample statistic (see “The Bootstrap”). More generally, an x%\nconfidence interval around a sample estimate should, on average, contain similar\nsample estimates x% of the time (when a similar sampling procedure is\nfollowed).\nGiven a sample of size n, and a sample statistic of interest, the algorithm for a\nbootstrap confidence interval is as follows:\n1. Draw a random sample of size n with replacement from the data (a\nresample).\n2. Record the statistic of interest for the resample.\n3. Repeat steps 1–2 many (R) times.\n\n\n4. For an x% confidence interval, trim [(1 – [x/100]) / 2]% of the R\nresample results from either end of the distribution.\n5. The trim points are the endpoints of an x% bootstrap confidence interval.\nFigure 2-9 shows a a 90% confidence interval for the mean annual income of loan\napplicants, based on a sample of 20 for which the mean was $57,573.\nFigure 2-9. Bootstrap confidence interval for the annual income of loan applicants, based on a sample\nof 20\nThe bootstrap is a general tool that can be used to generate confidence intervals\nfor most statistics, or model parameters. Statistical textbooks and software, with\nroots in over a half-century of computerless statistical analysis, will also\nreference confidence intervals generated by formulas, especially the t-distribution\n(see “Student’s t-Distribution”).\nNOTE\nOf course, what we are really interested in when we have a sample result is “what is the\nprobability that the true value lies within a certain interval?” This is not really the question that a\nconfidence interval answers, but it ends up being how most people interpret the answer.\nThe probability question associated with a confidence interval starts out with the phrase “Given a\nsampling procedure and a population, what is the probability that…” To go in the opposite\ndirection, “Given a sample result, what is the probability that (something is true about the\npopulation),” involves more complex calculations and deeper imponderables.\n\n\nThe percentage associated with the confidence interval is termed the level of\nconfidence. The higher the level of confidence, the wider the interval. Also, the\nsmaller the sample, the wider the interval (i.e., the more uncertainty). Both make\nsense: the more confident you want to be, and the less data you have, the wider\nyou must make the confidence interval to be sufficiently assured of capturing the\ntrue value.\nNOTE\nFor a data scientist, a confidence interval is a tool to get an idea of how variable a sample result\nmight be. Data scientists would use this information not to publish a scholarly paper or submit a\nresult to a regulatory agency (as a researcher might), but most likely to communicate the potential\nerror in an estimate, and, perhaps, learn whether a larger sample is needed.\nKEY IDEAS\nConfidence intervals are the typical way to present estimates as an interval range.\nThe more data you have, the less variable a sample estimate will be.\nThe lower the level of confidence you can tolerate, the narrower the confidence interval will be.\nThe bootstrap is an effective way to construct confidence intervals.\n\n\nFurther Reading\nFor a bootstrap approach to confidence intervals, see Introductory Statistics\nand Analytics: A Resampling Perspective by Peter Bruce (Wiley, 2014) or\nStatistics by Robin Lock and four other Lock family members (Wiley, 2012).\nEngineers, who have a need to understand the precision of their\nmeasurements, use confidence intervals perhaps more than most disciplines,\nand Modern Engineering Statistics by Tom Ryan (Wiley, 2007) discusses\nconfidence intervals. It also reviews a tool that is just as useful and gets less\nattention: prediction intervals (intervals around a single value, as opposed to\na mean or other summary statistic).\n\n\nNormal Distribution\nThe bell-shaped normal distribution is iconic in traditional statistics.1 The fact\nthat distributions of sample statistics are often normally shaped has made it a\npowerful tool in the development of mathematical formulas that approximate those\ndistributions.\nKEY TERMS\nError\nThe difference between a data point and a predicted or average value.\nStandardize\nSubtract the mean and divide by the standard deviation.\nz-score\nThe result of standardizing an individual data point.\nStandard normal\nA normal distribution with mean = 0 and standard deviation = 1.\nQQ-Plot\nA plot to visualize how close a sample distribution is to a normal distribution.\nIn a normal distribution (Figure 2-10), 68% of the data lies within one standard\ndeviation of the mean, and 95% lies within two standard deviations.\nWARNING\nIt is a common misconception that the normal distribution is called that because most data follows\na normal distribution — that is, it is the normal thing. Most of the variables used in a typical data\nscience project — in fact most raw data as a whole — are not normally distributed: see “Long-\nTailed Distributions”. The utility of the normal distribution derives from the fact that many\nstatistics are normally distributed in their sampling distribution. Even so, assumptions of normality\nare generally a last resort, used when empirical probability distributions, or bootstrap distributions,\nare not available.\n\n\nFigure 2-10. Normal curve\nNOTE\nThe normal distribution is also referred to as a Gaussian distribution after Carl Friedrich Gauss, a\nprodigous German mathematician from the late 18th and early 19th century. Another name\npreviously used for the normal distribution was the “error” distribution. Statistically speaking, an\nerror is the difference between an actual value and a statistical estimate like the sample mean.\nFor example, the standard deviation (see “Estimates of Variability”) is based on the errors from\nthe mean of the data. Gauss’s development of the normal distribution came from his study of the\nerrors of astronomical measurements that were found to be normally distributed.\n\n\nStandard Normal and QQ-Plots\nA standard normal distribution is one in which the units on the x-axis are\nexpressed in terms of standard deviations away from the mean. To compare data\nto a standard normal distribution, you subtract the mean then divide by the\nstandard deviation; this is also called normalization or standardization (see\n“Standardization (Normalization, Z-Scores)”). Note that “standardization” in this\nsense is unrelated to database record standardization (conversion to a common\nformat). The transformed value is termed a z-score, and the normal distribution is\nsometimes called the z-distribution.\nA QQ-Plot is used to visually determine how close a sample is to the normal\ndistribution. The QQ-Plot orders the z-scores from low to high, and plots each\nvalue’s z-score on the y-axis; the x-axis is the corresponding quantile of a normal\ndistribution for that value’s rank. Since the data is normalized, the units\ncorrespond to the number of standard deviations away of the data from the mean.\nIf the points roughly fall on the diagonal line, then the sample distribution can be\nconsidered close to normal. Figure 2-11 shows a QQ-Plot for a sample of 100\nvalues randomly generated from a normal distribution; as expected, the points\nclosely follow the line. This figure can be produced in R with the qqnorm\nfunction:\nnorm_samp <- rnorm(100)\nqqnorm(norm_samp)\nabline(a=0, b=1, col='grey')\n\n\nFigure 2-11. QQ-Plot of a sample of 100 values drawn from a normal distribution\nWARNING\nConverting data to z-scores (i.e., standardizing or normalizing the data) does not make the data\nnormally distributed. It just puts the data on the same scale as the standard normal distribution,\noften for comparison purposes.\n\n\nKEY IDEAS\nThe normal distribution was essential to the historical development of statistics, as it permitted\nmathematical approximation of uncertainty and variability.\nWhile raw data is typically not normally distributed, errors often are, as are averages and totals in\nlarge samples.\nTo convert data to z-scores, you subtract the mean of the data and divide by the standard deviation;\nyou can then compare the data to a normal distribution.\n\n\nLong-Tailed Distributions\nDespite the importance of the normal distribution historically in statistics, and in\ncontrast to what the name would suggest, data is generally not normally\ndistributed.\nKEY TERMS FOR LONG-TAIL DISTRIBUTION\nTail\nThe long narrow portion of a frequency distribution, where relatively extreme values occur at low\nfrequency.\nSkew\nWhere one tail of a distribution is longer than the other.\nWhile the normal distribution is often appropriate and useful with respect to the\ndistribution of errors and sample statistics, it typically does not characterize the\ndistribution of raw data. Sometimes, the distribution is highly skewed\n(asymmetric), such as with income data, or the distribution can be discrete, as\nwith binomial data. Both symmetric and asymmetric distributions may have long\ntails. The tails of a distribution correspond to the extreme values (small and\nlarge). Long tails, and guarding against them, are widely recognized in practical\nwork. Nassim Taleb has proposed the black swan theory, which predicts that\nanamolous events, such as a stock market crash, are much more likely to occur\nthan would be predicted by the normal distribution.\nA good example to illustrate the long-tailed nature of data is stock returns.\nFigure 2-12 shows the QQ-Plot for the daily stock returns for Netflix (NFLX).\nThis is generated in R by:\nnflx <- sp500_px[,'NFLX']\nnflx <- diff(log(nflx[nflx>0]))\nqqnorm(nflx)\nabline(a=0, b=1, col='grey')\n\n\nFigure 2-12. QQ-Plot of the returns for NFLX\nIn contrast to Figure 2-11, the points are far below the line for low values and far\nabove the line for high values. This means that we are much more likely to\nobserve extreme values than would be expected if the data had a normal\ndistribution. Figure 2-12 shows another common phenomena: the points are close\nto the line for the data within one standard deviation of the mean. Tukey refers to\nthis phenomenon as data being “normal in the middle,” but having much longer\ntails (see [Tukey-1987]).\n\n\nNOTE\nThere is much statistical literature about the task of fitting statistical distributions to observed\ndata. Beware an excessively data-centric approach to this job, which is as much art as science.\nData is variable, and often consistent, on its face, with more than one shape and type of\ndistribution. It is typically the case that domain and statistical knowledge must be brought to bear\nto determine what type of distribution is appropriate to model a given situation. For example, we\nmight have data on the level of internet traffic on a server over many consecutive 5-second\nperiods. It is useful to know that the best distribution to model “events per time period” is the\nPoisson (see “Poisson Distributions”).\nKEY IDEAS FOR LONG-TAIL DISTRIBUTION\nMost data is not normally distributed.\nAssuming a normal distribution can lead to underestimation of extreme events (“black swans”).\n\n\nFurther Reading\nThe Black Swan, 2nd ed., by Nassim Taleb (Random House, 2010).\nHandbook of Statistical Distributions with Applications, 2nd ed., by K.\nKrishnamoorthy (CRC Press, 2016)\n\n\nStudent’s t-Distribution\nThe t-distribution is a normally shaped distribution, but a bit thicker and longer\non the tails. It is used extensively in depicting distributions of sample statistics.\nDistributions of sample means are typically shaped like a t-distribution, and there\nis a family of t-distributions that differ depending on how large the sample is. The\nlarger the sample, the more normally shaped the t-distribution becomes.\nKEY TERMS FOR STUDENT’S T-DISTRIBUTION\nn\nSample size.\nDegrees of freedom\nA parameter that allows the t-distribution to adjust to different sample sizes, statistics, and number\nof groups.\nThe t-distribution is often called Student’s t because it was published in 1908 in\nBiometrika by W. S. Gossett under the name “Student.” Gossett’s employer, the\nGuinness brewery, did not want competitors to know that it was using statistical\nmethods, so insisted that Gossett not use his name on the article.\nGossett wanted to answer the question “What is the sampling distribution of the\nmean of a sample, drawn from a larger population?” He started out with a\nresampling experiment — drawing random samples of 4 from a data set of 3,000\nmeasurements of criminals’ height and left-middle-finger lengths. (This being the\nera of eugenics, there was much interest in data on criminals, and in discovering\ncorrelations between criminal tendencies and physical or psychological\nattributes.) He plotted the standardized results (the z-scores) on the x-axis and the\nfrequency on the y-axis. Separately, he had derived a function, now known as\nStudent’s t, and he fit this function over the sample results, plotting the comparison\n(see Figure 2-13).\n\n\nFigure 2-13. Gossett’s resampling experiment results and fitted t-curve (from his 1908 Biometrika paper)\nA number of different statistics can be compared, after standardization, to the t-\ndistribution, to estimate confidence intervals in light of sampling variation.\nConsider a sample of size n for which the sample mean \n has been calculated. If\ns is the sample standard deviation, a 90% confidence interval around the sample\nmean is given by:\nwhere \n is the value of the t-statistic, with (n – 1) degrees of freedom\n(see “Degrees of Freedom”), that “chops off” 5% of the t-distribution at either\nend. The t-distribution has been used as a reference for the distribution of a\nsample mean, the difference between two sample means, regression parameters,\nand other statistics.\nHad computing power been widely available in 1908, statistics would no doubt\nhave relied much more heavily on computationally intensive resampling methods\nfrom the start. Lacking computers, statisticians turned to mathematics and functions\nsuch as the t-distribution to approximate sampling distributions. Computer power\nenabled practical resampling experiments in the 1980s, but by then, use of the t-\ndistribution and similar distributions had become deeply embedded in textbooks\n\n\nand software.\nThe t-distribution’s accuracy in depicting the behavior of a sample statistic\nrequires that the distribution of that statistic for that sample be shaped like a\nnormal distribution. It turns out that sample statistics are often normally\ndistributed, even when the underlying population data is not (a fact which led to\nwidespread application of the t-distribution). This phenomenon is termed the\ncentral limit theorem (see “Central Limit Theorem”).\nNOTE\nWhat do data scientists need to know about the t-distribution and the central limit theorem? Not a\nwhole lot. These distributions are used in classical statistical inference, but are not as central to\nthe purposes of data science. Understanding and quantifying uncertainty and variation are\nimportant to data scientists, but empirical bootstrap sampling can answer most questions about\nsampling error. However, data scientists will routinely encounter t-statistics in output from\nstatistical software and statistical procedures in R, for example in A-B tests and regressions, so\nfamiliarity with its purpose is helpful.\nKEY IDEAS\nThe t-distribution is actually a family of distributions resembling the normal distribution, but with\nthicker tails.\nIt is widely used as a reference basis for the distribution of sample means, differerences between\ntwo sample means, regression parameters, and more.\n\n\nFurther Reading\nThe original Gossett paper in Biometrica from 1908 is available as a PDF.\nA standard treatment of the t-distribution can be found in David Lane’s online\nresource.\n\n\nBinomial Distribution\nKEY TERMS FOR BINOMIAL DISTRIBUTION\nTrial\nAn event with a discrete outcome (e.g., a coin flip).\nSuccess\nThe outcome of interest for a trial.\nSynonyms\n“1” (as opposed to “0”)\nBinomial\nHaving two outcomes.\nSynonyms\nyes/no, 0/1, binary\nBinomial trial\nA trial with two outcomes.\nSynonym\nBernoulli trial\nBinomial distribution\nDistribution of number of successes in x trials.\nSynonym\nBernoulli distribution\nYes/no (binomial) outcomes lie at the heart of analytics since they are often the\nculmination of a decision or other process; buy/don’t buy, click/don’t click,\nsurvive/die, and so on. Central to understanding the binomial distribution is the\nidea of a set of trials, each trial having two possible outcomes with definite\nprobabilities.\nFor example, flipping a coin 10 times is a binomial experiment with 10 trials,\neach trial having two possible outcomes (heads or tails); see Figure 2-14. Such\nyes/no or 0/1 outcomes are termed binary outcomes, and they need not have 50/50\nprobabilities. Any probabilities that sum to 1.0 are possible. It is conventional in\nstatistics to term the “1” outcome the success outcome; it is also common practice\nto assign “1” to the more rare outcome. Use of the term success does not imply\n\n\nthat the outcome is desirable or beneficial, but it does tend to indicate the outcome\nof interest. For example, loan defaults or fraudulent transactions are relatively\nuncommon events that we may be interested in predicting, so they are termed “1s”\nor “successes.”\nFigure 2-14. The tails side of a buffalo nickel\nThe binomial distribution is the frequency distribution of the number of successes\n(x) in a given number of trials (n) with specified probability (p) of success in\neach trial. There is a family of binomial distributions, depending on the values of\nx, n, and p. The binomial distribution would answer a question like:\nIf the probability of a click converting to a sale is 0.02, what is the probability\nof observing 0 sales in 200 clicks?\nThe R function dbinom calculates binomial probabilities. For example:\ndbinom(x=2, n=5, p=0.1)\nwould return 0.0729, the probability of observing exactly x = 2 successes in n = 5\ntrials, where the probability of success for each trial is p = 0.1.\nOften we are interested in determining the probability of x or fewer successes in n\ntrials. In this case, we use the function pbinom:\npbinom(2, 5, 0.1)\nThis would return 0.9914, the probability of observing two or fewer successes in\nfive trials, where the probability of success for each trial is 0.1.\nThe mean of a binomial distribution is \n; you can also think of this as the\nexpected number of successes in n trials, for success probability = p.\nThe variance is \n. With a large enough number of trials\n\n\n(particularly when p is close to 0.50), the binomial distribution is virtually\nindistinguishable from the normal distribution. In fact, calculating binomial\nprobabilities with large sample sizes is computationally demanding, and most\nstatistical procedures use the normal distribution, with mean and variance, as an\napproximation.\nKEY IDEAS\nBinomial outcomes are important to model, since they represent, among other things, fundamental\ndecisions (buy or don’t buy, click or don’t click, survive or die, etc.).\nA binomial trial is an experiment with two possible outcomes: one with probability p and the other\nwith probability 1 – p.\nWith large n, and provided p is not too close to 0 or 1, the binomial distribution can be approximated\nby the normal distribution.\n\n\nFurther Reading\nRead about the “quincunx”, a pinball-like simulation device for illustrating\nthe binomial distribution.\nThe binomial distribution is a staple of introductory statistics, and all\nintroductory statistics texts will have a chapter or two on it.\n\n\nPoisson and Related Distributions\nMany processes produce events randomly at a given overall rate — visitors\narriving at a website, cars arriving at a toll plaza (events spread over time),\nimperfections in a square meter of fabric, or typos per 100 lines of code (events\nspread over space).\nKEY TERMS FOR POISSON AND RELATED DISTRIBUTIONS\nLambda\nThe rate (per unit of time or space) at which events occur.\nPoisson distribution\nThe frequency distribution of the number of events in sampled units of time or space.\nExponential distribution\nThe frequency distribution of the time or distance from one event to the next event.\nWeibull distribution\nA generalized version of the exponential, in which the event rate is allowed to shift over time.\n\n\nPoisson Distributions\nFrom prior data we can estimate the average number of events per unit of time or\nspace, but we might also want to know how different this might be from one unit\nof time/space to another. The Poisson distribution tells us the distribution of\nevents per unit of time or space when we sample many such units. It is useful\nwhen addressing queuing questions like “How much capacity do we need to be\n95% sure of fully processing the internet traffic that arrives on a server in any 5-\nsecond period?”\nThe key parameter in a Poisson distribution is , or lambda. This is the mean\nnumber of events that occurs in a specified interval of time or space. The variance\nfor a Poisson distribution is also .\nA common technique is to generate random numbers from a Poisson distribution as\npart of a queuing simulation. The rpois function in R does this, taking only two\narguments — the quantity of random numbers sought, and lambda:\nrpois(100, lambda = 2)\nThis code will generate 100 random numbers from a Poisson distribution with \n= 2. For example, if incoming customer service calls average 2 per minute, this\ncode will simulate 100 minutes, returning the number of calls in each of those 100\nminutes.\n\n\nExponential Distribution\nUsing the same parameter  that we used in the Poisson distribution, we can also\nmodel the distribution of the time between events: time between visits to a\nwebsite or between cars arriving at a toll plaza. It is also used in engineering to\nmodel time to failure, and in process management to model, for example, the time\nrequired per service call. The R code to generate random numbers from an\nexponential distribution takes two arguments, n (the quantity of numbers to be\ngenerated), and rate, the number of events per time period. For example:\nrexp(n = 100, rate = .2)\nThis code would generate 100 random numbers from an exponential distribution\nwhere the mean number of events per time period is 2. So you could use it to\nsimulate 100 intervals, in minutes, between service calls, where the average rate\nof incoming calls is 0.2 per minute.\nA key assumption in any simulation study for either the Poisson or exponential\ndistribution is that the rate, , remains constant over the period being considered.\nThis is rarely reasonable in a global sense; for example, traffic on roads or data\nnetworks varies by time of day and day of week. However, the time periods, or\nareas of space, can usually be divided into segments that are sufficiently\nhomogeneous so that analysis or simulation within those periods is valid.\n\n\nEstimating the Failure Rate\nIn many applications, the event rate, , is known or can be estimated from prior\ndata. However, for rare events, this is not necessarily so. Aircraft engine failure,\nfor example, is sufficiently rare (thankfully) that, for a given engine type, there\nmay be little data on which to base an estimate of time between failures. With no\ndata at all, there is little basis on which to estimate an event rate. However, you\ncan make some guesses: if no events have been seen after 20 hours, you can be\npretty sure that the rate is not 1 per hour. Via simulation, or direct calculation of\nprobabilities, you can assess different hypothetical event rates and estimate\nthreshold values below which the rate is very unlikely to fall. If there is some data\nbut not enough to provide a precise, reliable estimate of the rate, a goodness-of-fit\ntest (see “Chi-Square Test”) can be applied to various rates to determine how\nwell they fit the observed data.\n\n\nWeibull Distribution\nIn many cases, the event rate does not remain constant over time. If the period\nover which it changes is much longer than the typical interval between events,\nthere is no problem; you just subdivide the analysis into the segments where rates\nare relatively constant, as mentioned before. If, however, the event rate changes\nover the time of the interval, the exponential (or Poisson) distributions are no\nlonger useful. This is likely to be the case in mechanical failure — the risk of\nfailure increases as time goes by. The Weibull distribution is an extension of the\nexponential distribution, in which the event rate is allowed to change, as specified\nby a shape parameter, . If  > 1, the probability of an event increases over\ntime, if  < 1, it decreases. Because the Weibull distribution is used with time-to-\nfailure analysis instead of event rate, the second parameter is expressed in terms\nof characteristic life, rather than in terms of the rate of events per interval. The\nsymbol used is , the Greek letter eta. It is also called the scale parameter.\nWith the Weibull, the estimation task now includes estimation of both parameters, \n and . Software is used to model the data and yield an estimate of the best-\nfitting Weibull distribution.\nThe R code to generate random numbers from a Weibull distribution takes three\narguments, n (the quantity of numbers to be generated), shape, and scale. For\nexample, the following code would generate 100 random numbers (lifetimes) from\na Weibull distribution with shape of 1.5 and characteristic life of 5,000:\nrweibull(100,1.5,5000)\nKEY IDEAS\nFor events that occur at a constant rate, the number of events per unit of time or space can be\nmodeled as a Poisson distribution.\nIn this scenario, you can also model the time or distance between one event and the next as an\nexponential distribution.\nA changing event rate over time (e.g., an increasing probability of device failure) can be modeled\nwith the Weibull distribution.\n\n\nFurther Reading\nModern Engineering Statistics by Tom Ryan (Wiley, 2007) has a chapter\ndevoted to the probability distributions used in engineering applications.\nRead an engineering-based perspective on the use of the Weibull distribution\n(mainly from an engineering perspective) here and here.\n\n\nSummary\nIn the era of big data, the principles of random sampling remain important when\naccurate estimates are needed. Random selection of data can reduce bias and\nyield a higher quality data set than would result from just using the conveniently\navailable data. Knowledge of various sampling and data generating distributions\nallows us to quantify potential errors in an estimate that might be due to random\nvariation. At the same time, the bootstrap (sampling with replacement from an\nobserved data set) is an attractive “one size fits all” method to determine possible\nerror in sample estimates.\nThe bell curve is iconic but perhaps overrated. George W. Cobb, the Mount Holyoke statistician noted for\nhis contribution to the philosophy of teaching introductory statistics, argued in a November 2015 editorial in\nthe American Statistician that the “standard introductory course, which puts the normal distribution at its\ncenter, had outlived the usefulness of its centrality.”\n1\n\n\nChapter 3. Statistical Experiments\nand Significance Testing\nDesign of experiments is a cornerstone of the practice of statistics, with\napplications in virtually all areas of research. The goal is to design an experiment\nin order to confirm or reject a hypothesis. Data scientists are faced with the need\nto conduct continual experiments, particularly regarding user interface and product\nmarketing. This chapter reviews traditional experimental design and discusses\nsome common challenges in data science. It also covers some oft-cited concepts\nin statistical inference and explains their meaning and relevance (or lack of\nrelevance) to data science.\nWhenever you see references to statistical significance, t-tests, or p-values, it is\ntypically in the context of the classical statistical inference “pipeline” (see\nFigure 3-1). This process starts with a hypothesis (“drug A is better than the\nexisting standard drug,” “price A is more profitable than the existing price B”).\nAn experiment (it might be an A/B test) is designed to test the hypothesis —\ndesigned in such a way that, hopefully, will deliver conclusive results. The data is\ncollected and analyzed, and then a conclusion is drawn. The term inference\nreflects the intention to apply the experiment results, which involve a limited set\nof data, to a larger process or population.\nFigure 3-1. The classical statistical inference pipeline\n\n\nA/B Testing\nAn A/B test is an experiment with two groups to establish which of two\ntreatments, products, procedures, or the like is superior. Often one of the two\ntreatments is the standard existing treatment, or no treatment. If a standard (or no)\ntreatment is used, it is called the control. A typical hypothesis is that treatment is\nbetter than control.\nKEY TERMS FOR A/B TESTING\nTreatment\nSomething (drug, price, web headline) to which a subject is exposed.\nTreatment group\nA group of subjects exposed to a specific treatment.\nControl group\nA group of subjects exposed to no (or standard) treatment.\nRandomization\nThe process of randomly assigning subjects to treatments.\nSubjects\nThe items (web visitors, patients, etc.) that are exposed to treatments.\nTest statistic\nThe metric used to measure the effect of the treatment.\nA/B tests are common in web design and marketing, since results are so readily\nmeasured. Some examples of A/B testing include:\nTesting two soil treatments to determine which produces better seed\ngermination\nTesting two therapies to determine which suppresses cancer more effectively\nTesting two prices to determine which yields more net profit\nTesting two web headlines to determine which produces more clicks\n(Figure 3-2)\n",
      "page_number": 93
    },
    {
      "number": 3,
      "title": "Statistical Experiments",
      "start_page": 154,
      "end_page": 230,
      "detection_method": "regex_chapter_title",
      "content": "Testing two web ads to determine which generates more conversions\nFigure 3-2. Marketers continually test one web presentation against another\nA proper A/B test has subjects that can be assigned to one treatment or another.\nThe subject might be a person, a plant seed, a web visitor; the key is that the\nsubject is exposed to the treatment. Ideally, subjects are randomized (assigned\nrandomly) to treatments. In this way, you know that any difference between the\ntreatment groups is due to one of two things:\n\n\nThe effect of the different treatments\nLuck of the draw in which subjects are assigned to which treatments (i.e., the\nrandom assignment may have resulted in the naturally better-performing\nsubjects being concentrated in A or B)\nYou also need to pay attention to the test statistic or metric you use to compare\ngroup A to group B. Perhaps the most common metric in data science is a binary\nvariable: click or no-click, buy or don’t buy, fraud or no fraud, and so on. Those\nresults would be summed up in a 2×2 table. Table 3-1 is a 2×2 table for an actual\nprice test.\nTable 3-1. 2×2 table for\necommerce experiment\nresults\nOutcome\nPrice A Price B\nConversion\n200\n182\nNo conversion 23,539\n22,406\nIf the metric is a continuous variable (purchase amount, profit, etc.), or a count\n(e.g., days in hospital, pages visited) the result might be displayed differently. If\none were interested not in conversion, but in revenue per page view, the results of\nthe price test in Table 3-1 might look like this in typical default software output:\nRevenue/page-view with price A: mean = 3.87, SD = 51.10\nRevenue/page-view with price B: mean = 4.11, SD = 62.98\n“SD” refers to the standard deviation of the values within each group.\nWARNING\nJust because statistical software — including R — generates output by default does not mean\nthat all the output is useful or relevant. You can see that the preceding standard deviations are not\nthat useful; on their face they suggest that numerous values might be negative, when negative\nrevenue is not feasible. This data consists of a small set of relatively high values (page views with\nconversions) and a huge number of 0-values (page views with no conversion). It is difficult to\nsum up the variability of such data with a single number, though the mean absolute deviation from\nthe mean (7.68 for A and 8.15 for B) is more reasonable than the standard deviation.\n\n\nWhy Have a Control Group?\nWhy not skip the control group and just run an experiment applying the treatment\nof interest to only one group, and compare the outcome to prior experience?\nWithout a control group, there is no assurance that “other things are equal” and\nthat any difference is really due to the treatment (or to chance). When you have a\ncontrol group, it is subject to the same conditions (except for the treatment of\ninterest) as the treatment group. If you simply make a comparison to “baseline” or\nprior experience, other factors, besides the treatment, might differ.\n\n\nBLINDING IN STUDIES\nA blind study is one in which the subjects are unaware of whether they are getting treatment A\nor treatment B. Awareness of receiving a particular treatment can affect response. A double\nblind study is one in which the investigators and facilitators (e.g., doctors and nurses in a medical\nstudy) are unaware which subjects are getting which treatment. Blinding is not possible when the\nnature of the treatment is transparent — for example, cognitive therapy from a computer versus\na psychologist.\nThe use of A/B testing in data science is typically in a web context. Treatments\nmight be the design of a web page, the price of a product, the wording of a\nheadline, or some other item. Some thought is required to preserve the principles\nof randomization. Typically the subject in the experiment is the web visitor, and\nthe outcomes we are interested in measuring are clicks, purchases, visit duration,\nnumber of pages visited, whether a particular page is visited, and the like. In a\nstandard A/B experiment, you need to decide on one metric ahead of time.\nMultiple behavior metrics might be collected and be of interest, but if the\nexperiment is expected to lead to a decision between treatment A and treatment B,\na single metric, or test statistic, needs to be established beforehand. Selecting a\ntest statistic after the experiment is conducted opens the door to researcher bias.\n\n\nWhy Just A/B? Why Not C, D…?\nA/B tests are popular in the marketing and ecommerce worlds, but are far from the\nonly type of statistical experiment. Additional treatments can be included.\nSubjects might have repeated measurements taken. Pharmaceutical trials where\nsubjects are scarce, expensive, and acquired over time are sometimes designed\nwith multiple opportunities to stop the experiment and reach a conclusion.\nTraditional statistical experimental designs focus on answering a static question\nabout the efficacy of specified treatments. Data scientists are less interested in the\nquestion:\nIs the difference between price A and price B statistically significant?\nthan in the question:\nWhich, out of multiple possible prices, is best?\nFor this, a relatively new type of experimental design is used: the multi-arm\nbandit (see “Multi-Arm Bandit Algorithm”).\n\n\nGETTING PERMISSION\nIn scientific and medical research involving human subjects, it is typically necessary to get their\npermission, as well as obtain the approval of an institutional review board. Experiments in\nbusiness that are done as a part of ongoing operations almost never do this. In most cases (e.g.,\npricing experiments, or experiments about which headline to show or which offer should be\nmade), this practice is widely accepted. Facebook, however, ran afoul of this general acceptance\nin 2014 when it experimented with the emotional tone in users’ newsfeeds. Facebook used\nsentiment analysis to classify newsfeed posts as positive or negative, then altered the\npositive/negative balance in what it showed users. Some randomly selected users experienced\nmore positive posts, while others experienced more negative posts. Facebook found that the users\nwho experienced a more positive newsfeed were more likely to post positively themselves, and\nvice versa. The magnitude of the effect was small, however, and Facebook faced much criticism\nfor conducting the experiment without users’ knowledge. Some users speculated that Facebook\nmight have pushed some extremely depressed users over the edge, if they got the negative\nversion of their feed.\nKEY IDEAS\nSubjects are assigned to two (or more) groups that are treated exactly alike, except that the\ntreatment under study differs from one to another.\nIdeally, subjects are assigned randomly to the groups.\n\n\nFor Further Reading\nTwo-group comparisons (A/B tests) are a staple of traditional statistics, and\njust about any introductory statistics text will have extensive coverage of\ndesign principles and inference procedures. For a discussion that places A/B\ntests in more of a data science context and uses resampling, see Introductory\nStatistics and Analytics: A Resampling Perspective by Peter Bruce (Wiley,\n2014).\nFor web testing, the logistical aspects of testing can be just as challenging as\nthe statistical ones. A good place to start is the Google Analytics help section\non Experiments.\nBeware advice found in the ubiquitous guides to A/B testing that you see on\nthe web, such as these words in one such guide: “Wait for about 1,000 total\nvisitors and make sure you run the test for a week.” Such general rules of\nthumb are not statistically meaningful; see “Power and Sample Size” for\nmore detail.\n\n\nHypothesis Tests\nHypothesis tests, also called significance tests, are ubiquitous in the traditional\nstatistical analysis of published research. Their purpose is to help you learn\nwhether random chance might be responsible for an observed effect.\nKEY TERMS\nNull hypothesis\nThe hypothesis that chance is to blame.\nAlternative hypothesis\nCounterpoint to the null (what you hope to prove).\nOne-way test\nHypothesis test that counts chance results only in one direction.\nTwo-way test\nHypothesis test that counts chance results in two directions.\nAn A/B test (see “A/B Testing”) is typically constructed with a hypothesis in\nmind. For example, the hypothesis might be that price B produces higher profit.\nWhy do we need a hypothesis? Why not just look at the outcome of the experiment\nand go with whichever treatment does better?\nThe answer lies in the tendency of the human mind to underestimate the scope of\nnatural random behavior. One manifestation of this is the failure to anticipate\nextreme events, or so-called “black swans” (see “Long-Tailed Distributions”).\nAnother manifestation is the tendency to misinterpret random events as having\npatterns of some significance. Statistical hypothesis testing was invented as a way\nto protect researchers from being fooled by random chance.\nMISINTERPRETING RANDOMNESS\nYou can observe the human tendency to underestimate randomness in this experiment. Ask several\nfriends to invent a series of 50 coin flips: have them write down a series of random Hs and Ts. Then ask\nthem to actually flip a coin 50 times and write down the results. Have them put the real coin flip results in\none pile, and the made-up results in another. It is easy to tell which results are real: the real ones will have\nlonger runs of Hs or Ts. In a set of 50 real coin flips, it is not at all unusual to see five or six Hs or Ts in a\nrow. However, when most of us are inventing random coin flips and we have gotten three or four Hs in a\nrow, we tell ourselves that, for the series to look random, we had better switch to T.\n\n\nThe other side of this coin, so to speak, is that when we do see the real-world equivalent of six Hs in a\nrow (e.g., when one headline outperforms another by 10%), we are inclined to attribute it to something\nreal, not just chance.\nIn a properly designed A/B test, you collect data on treatments A and B in such a\nway that any observed difference between A and B must be due to either:\nRandom chance in assignment of subjects\nA true difference between A and B\nA statistical hypothesis test is further analysis of an A/B test, or any randomized\nexperiment, to assess whether random chance is a reasonable explanation for the\nobserved difference between groups A and B.\n\n\nThe Null Hypothesis\nHypothesis tests use the following logic: “Given the human tendency to react to\nunusual but random behavior and interpret it as something meaningful and real, in\nour experiments we will require proof that the difference between groups is more\nextreme than what chance might reasonably produce.” This involves a baseline\nassumption that the treatments are equivalent, and any difference between the\ngroups is due to chance. This baseline assumption is termed the null hypothesis.\nOur hope is then that we can, in fact, prove the null hypothesis wrong, and show\nthat the outcomes for groups A and B are more different than what chance might\nproduce.\nOne way to do this is via a resampling permutation procedure, in which we\nshuffle together the results from groups A and B and then repeatedly deal out the\ndata in groups of similar sizes, then observe how often we get a difference as\nextreme as the observed difference. See “Resampling” for more detail.\n\n\nAlternative Hypothesis\nHypothesis tests by their nature involve not just a null hypothesis, but also an\noffsetting alternative hypothesis. Here are some examples:\nNull = “no difference between the means of group A and group B,”\nalternative = “A is different from B” (could be bigger or smaller)\nNull = “A \n B,” alternative = “B > A”\nNull = “B is not X% greater than A,” alternative = “B is X% greater than A”\nTaken together, the null and alternative hypotheses must account for all\npossibilities. The nature of the null hypothesis determines the structure of the\nhypothesis test.\n\n\nOne-Way, Two-Way Hypothesis Test\nOften, in an A/B test, you are testing a new option (say B), against an established\ndefault option (A) and the presumption is that you will stick with the default\noption unless the new option proves itself definitively better. In such a case, you\nwant a hypothesis test to protect you from being fooled by chance in the direction\nfavoring B. You don’t care about being fooled by chance in the other direction,\nbecause you would be sticking with A unless B proves definitively better. So you\nwant a directional alternative hypothesis (B is better than A). In such a case, you\nuse a one-way (or one-tail) hypothesis test. This means that extreme chance results\nin only one direction direction count toward the p-value.\nIf you want a hypothesis test to protect you from being fooled by chance in either\ndirection, the alternative hypothesis is bidirectional (A is different from B; could\nbe bigger or smaller). In such a case, you use a two-way (or two-tail) hypothesis.\nThis means that extreme chance results in either direction count toward the p-\nvalue.\nA one-tail hypothesis test often fits the nature of A/B decision making, in which a\ndecision is required and one option is typically assigned “default” status unless\nthe other proves better. Software, however, including R, typically provides a two-\ntail test in its default output, and many statisticians opt for the more conservative\ntwo-tail test just to avoid argument. One-tail versus two-tail is a confusing\nsubject, and not that relevant for data science, where the precision of p-value\ncalculations is not terribly important.\nKEY IDEAS\nA null hypothesis is a logical construct embodying the notion that nothing special has happened,\nand any effect you observe is due to random chance.\nThe hypothesis test assumes that the null hypothesis is true, creates a “null model” (a probability\nmodel), and tests whether the effect you observe is a reasonable outcome of that model.\n\n\nFurther Reading\nThe Drunkard’s Walk by Leonard Mlodinow (Vintage Books, 2008) is a\nreadable survey of the ways in which “randomness rules our lives.”\nDavid Freedman, Robert Pisani, and Roger Purves’s classic statistics text\nStatistics, 4th ed. (W. W. Norton, 2007) has excellent nonmathematical\ntreatments of most statistics topics, including hypothesis testing.\nIntroductory Statistics and Analytics: A Resampling Perspective by Peter\nBruce (Wiley, 2014) develops hypothesis testing concepts using resampling.\n\n\nResampling\nResampling in statistics means to repeatedly sample values from observed data,\nwith a general goal of assessing random variability in a statistic. It can also be\nused to assess and improve the accuracy of some machine-learning models (e.g.,\nthe predictions from decision tree models built on multiple bootstrapped data sets\ncan be averaged in a process known as bagging: see “Bagging and the Random\nForest”).\nThere are two main types of resampling procedures: the bootstrap and\npermutation tests. The bootstrap is used to assess the reliability of an estimate; it\nwas discussed in the previous chapter (see “The Bootstrap”). Permutation tests\nare used to test hypotheses, typically involving two or more groups, and we\ndiscuss those in this section.\nKEY TERMS\nPermutation test\nThe procedure of combining two or more samples together, and randomly (or exhaustively)\nreallocating the observations to resamples.\nSynonyms\nRandomization test, random permutation test, exact test.\nWith or without replacement\nIn sampling, whether or not an item is returned to the sample before the next draw.\n\n\nPermutation Test\nIn a permutation procedure, two or more samples are involved, typically the\ngroups in an A/B or other hypothesis test. Permute means to change the order of a\nset of values. The first step in a permutation test of a hypothesis is to combine the\nresults from groups A and B (and, if used, C, D, …) together. This is the logical\nembodiment of the null hypothesis that the treatments to which the groups were\nexposed do not differ. We then test that hypothesis by randomly drawing groups\nfrom this combined set, and seeing how much they differ from one another. The\npermutation procedure is as follows:\n1. Combine the results from the different groups in a single data set.\n2. Shuffle the combined data, then randomly draw (without replacing) a\nresample of the same size as group A.\n3. From the remaining data, randomly draw (without replacing) a resample\nof the same size as group B.\n4. Do the same for groups C, D, and so on.\n5. Whatever statistic or estimate was calculated for the original samples\n(e.g., difference in group proportions), calculate it now for the\nresamples, and record; this constitutes one permutation iteration.\n6. Repeat the previous steps R times to yield a permutation distribution of\nthe test statistic.\nNow go back to the observed difference between groups and compare it to the set\nof permuted differences. If the observed difference lies well within the set of\npermuted differences, then we have not proven anything — the observed\ndifference is within the range of what chance might produce. However, if the\nobserved difference lies outside most of the permutation distribution, then we\nconclude that chance is not responsible. In technical terms, the difference is\nstatistically significant. (See “Statistical Significance and P-Values”.)\n\n\nExample: Web Stickiness\nA company selling a relatively high-value service wants to test which of two web\npresentations does a better selling job. Due to the high value of the service being\nsold, sales are infrequent and the sales cycle is lengthy; it would take too long to\naccumulate enough sales to know which presentation is superior. So the company\ndecides to measure the results with a proxy variable, using the detailed interior\npage that describes the service.\nTIP\nA proxy variable is one that stands in for the true variable of interest, which may be unavailable,\ntoo costly, or too time-consuming to measure. In climate research, for example, the oxygen\ncontent of ancient ice cores is used as a proxy for temperature. It is useful to have at least some\ndata on the true variable of interest, so the strength of its association with the proxy can be\nassessed.\nOne potential proxy variable for our company is the number of clicks on the\ndetailed landing page. A better one is how long people spend on the page. It is\nreasonable to think that a web presentation (page) that holds people’s attention\nlonger will lead to more sales. Hence, our metric is average session time,\ncomparing page A to page B.\nDue to the fact that this is an interior, special-purpose page, it does not receive a\nhuge number of visitors. Also note that Google Analytics, which is how we\nmeasure session time, cannot measure session time for the last session a person\nvisits. Instead of deleting that session from the data, though, GA records it as a\nzero, so the data requires additional processing to remove those sessions. The\nresult is a total of 36 sessions for the two different presentations, 21 for page A\nand 15 for page B. Using ggplot, we can visually compare the session times\nusing side-by-side boxplots:\nggplot(session_times, aes(x=Page, y=Time)) +\n  geom_boxplot()\nThe boxplot, shown in Figure 3-3, indicates that page B leads to longer sessions\nthan page A. The means for each group can be computed as follows:\n\n\nmean_a <- mean(session_times[session_times['Page']=='Page A', 'Time'])\nmean_b <- mean(session_times[session_times['Page']=='Page B', 'Time'])\nmean_b - mean_a\n[1] 21.4\nPage B has session times greater, on average, by 21.4 seconds versus page A. The\nquestion is whether this difference is within the range of what random chance\nmight produce, or, alternatively, is statistically significant. One way to answer this\nis to apply a permutation test — combine all the session times together, then\nrepeatedly shuffle and divide them into groups of 21 (recall that n = 21 for page\nA) and 15 (n = 15 for B).\nTo apply a permutation test, we need a function to randomly assign the 36 session\ntimes to a group of 21 (page A) and a group of 15 (page B):\nperm_fun <- function(x, n1, n2)\n{\n  n <- n1 + n2\n  idx_b <- sample(1:n, n1)\n  idx_a <- setdiff(1:n, idx_b)\n  mean_diff <- mean(x[idx_b]) - mean(x[idx_a])\n  return(mean_diff)\n}\n\n\nFigure 3-3. Session times for web pages A and B\nThis function works by sampling without replacement n2 indices and assigning\nthem to the B group; the remaining n1 indices are assigned to group A. The\ndifference between the two means is returned. Calling this function R = 1,000\ntimes and specifying n2 = 15 and n1 = 21 leads to a distribution of differences in\nthe session times that can be plotted as a histogram.\nperm_diffs <- rep(0, 1000)\nfor(i in 1:1000)\n  perm_diffs[i] = perm_fun(session_times[,'Time'], 21, 15)\n\n\nhist(perm_diffs, xlab='Session time differences (in seconds)')\nabline(v = mean_b - mean_a)\nThe histogram, shown in Figure 3-4 shows that mean difference of random\npermutations often exceeds the observed difference in session times (the vertical\nline). This suggests that the oberved difference in session time between page A\nand page B is well within the range of chance variation, thus is not statistically\nsignificant.\n\n\nFigure 3-4. Frequency distribution for session time differences between pages A and B\n\n\nExhaustive and Bootstrap Permutation Test\nIn addition to the preceding random shuffling procedure, also called a random\npermutation test or a randomization test, there are two variants of the\npermutation test:\nAn exhaustive permutation test\nA bootstrap permutation test\nIn an exhaustive permutation test, instead of just randomly shuffling and dividing\nthe data, we actually figure out all the possible ways it could be divided. This is\npractical only for relatively small sample sizes. With a large number of repeated\nshufflings, the random permutation test results approximate those of the exhaustive\npermutation test, and approach them in the limit. Exhaustive permutation tests are\nalso sometimes called exact tests, due to their statistical property of guaranteeing\nthat the null model will not test as “significant” more than the alpha level of the\ntest (see “Statistical Significance and P-Values”).\nIn a bootstrap permutation test, the draws outlined in steps 2 and 3 of the random\npermutation test are made with replacement instead of without replacement. In\nthis way the resampling procedure models not just the random element in the\nassignment of treatment to subject, but also the random element in the selection of\nsubjects from a population. Both procedures are encountered in statistics, and the\ndistinction between them is somewhat convoluted and not of consequence in the\npractice of data science.\n\n\nPermutation Tests: The Bottom Line for Data Science\nPermutation tests are useful heuristic procedures for exploring the role of random\nvariation. They are relatively easy to code, interpret and explain, and they offer a\nuseful detour around the formalism and “false determinism” of formula-based\nstatistics.\nOne virtue of resampling, in contrast to formula approaches, is that it comes much\ncloser to a “one size fits all” approach to inference. Data can be numeric or\nbinary. Sample sizes can be the same or different. Assumptions about normally-\ndistributed data are not needed.\nKEY IDEAS\nIn a permutation test, multiple samples are combined, then shuffled.\nThe shuffled values are then divided into resamples, and the statistic of interest is calculated.\nThis process is then repeated, and the resampled statistic is tabulated.\nComparing the observed value of the statistic to the resampled distribution allows you to judge\nwhether an observed difference between samples might occur by chance.\n\n\nFor Further Reading\nRandomization Tests, 4th ed., by Eugene Edgington and Patrick Onghena\n(Chapman Hall, 2007), but don’t get too drawn into the thicket of nonrandom\nsampling.\nIntroductory Statistics and Analytics: A Resampling Perspective by Peter\nBruce (Wiley, 2015).\n\n\nStatistical Significance and P-Values\nStatistical significance is how statisticians measure whether an experiment (or\neven a study of existing data) yields a result more extreme than what chance might\nproduce. If the result is beyond the realm of chance variation, it is said to be\nstatistically significant.\nKEY TERMS\nP-value\nGiven a chance model that embodies the null hypothesis, the p-value is the probability of obtaining\nresults as unusual or extreme as the observed results.\nAlpha\nThe probability threshold of “unusualness” that chance results must surpass, for actual outcomes\nto be deemed statistically significant.\nType 1 error\nMistakenly concluding an effect is real (when it is due to chance).\nType 2 error\nMistakenly concluding an effect is due to chance (when it is real).\nConsider in Table 3-2 the results of the web test shown earlier.\nTable 3-2. 2×2 table for\necommerce experiment\nresults\nOutcome\nPrice A Price B\nConversion\n200\n182\nNo conversion 23539\n22406\nPrice A converts almost 5% better than price B (0.8425% versus 0.8057% — a\ndifference of 0.0368 percentage points), big enough to be meaningful in a high-\nvolume business. We have over 45,000 data points here, and it is tempting to\nconsider this as “big data,” not requiring tests of statistical significance (needed\nmainly to account for sampling variability in small samples). However, the\n\n\nconversion rates are so low (less than 1%) that the actual meaningful values —\nthe conversions — are only in the 100s, and the sample size needed is really\ndetermined by these conversions. We can test whether the difference in\nconversions between prices A and B is within the range of chance variation, using\na resampling procedure. By “chance variation,” we mean the random variation\nproduced by a probability model that embodies the null hypothesis that there is no\ndifference between the rates (see “The Null Hypothesis”). The following\npermutation procedure asks “if the two prices share the same conversion rate,\ncould chance variation produce a difference as big as 5%?”\n1. Create an urn with all sample results: this represents the supposed shared\nconversion rate of 382 ones and 45,945 zeros = 0.008246 = 0.8246%.\n2. Shuffle and draw out a resample of size 23,739 (same n as price A), and\nrecord how many 1s.\n3. Record the number of 1s in the remaining 22,588 (same n as price B).\n4. Record the difference in proportion 1s.\n5. Repeat steps 2–4.\n6. How often was the difference >= 0.0368?\nReusing the function perm_fun defined in “Example: Web Stickiness”, we can\ncreate a histogram of randomly permuted differences in conversion rate:\nobs_pct_diff <- 100*(200/23739 - 182/22588)\nconversion <- c(rep(0, 45945), rep(1, 382))\nperm_diffs <- rep(0, 1000)\nfor(i in 1:1000)\n  perm_diffs[i] = 100*perm_fun(conversion, 23739, 22588 )\nhist(perm_diffs, xlab='Session time differences (in seconds)')\nabline(v = obs_pct_diff)\nSee the histogram of 1,000 resampled results in Figure 3-5: as it happens, in this\ncase the observed difference of 0.0368% is well within the range of chance\nvariation.\n\n\nFigure 3-5. Frequency distribution for the difference in conversion rates between pages A and B\n\n\nP-Value\nSimply looking at the graph is not a very precise way to measure statistical\nsignificance, so of more interest is the p-value. This is the frequency with which\nthe chance model produces a result more extreme than the observed result. We can\nestimate a p-value from our permutation test by taking the proportion of times that\nthe permutation test produces a difference equal to or greater than the observed\ndifference:\nmean(perm_diffs > obs_pct_diff)\n[1] 0.308\nThe p-value is 0.308, which means that we would expect to achieve the same\nresult by random chance over 30% of the time.\nIn this case, we didn’t need to use a permutation test to get a p-value. Since we\nhave a binomial distribution, we can approximate the p-value using the normal\ndistribution. In R code, we do this using the function prop.test:\n> prop.test(x=c(200,182), n=c(23739,22588), alternative=\"greater\")\n \n2-sample test for equality of proportions with continuity correction\ndata:  c(200, 182) out of c(23739, 22588)\nX-squared = 0.14893, df = 1, p-value = 0.3498\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.001057439  1.000000000\nsample estimates:\n     prop 1      prop 2\n0.008424955 0.008057376\nThe argument x is the number of successes for each group and the argument n is\nthe number of trials. The normal approximation yields a p-value of 0.3498, which\nis close to the p-value obtained from the permutation test.\n\n\nAlpha\nStatisticians frown on the practice of leaving it to the researcher’s discretion to\ndetermine whether a result is “too unusual” to happen by chance. Rather, a\nthreshold is specified in advance, as in “more extreme than 5% of the chance (null\nhypothesis) results”; this threshold is known as alpha. Typical alpha levels are\n5% and 1%. Any chosen level is an arbitrary decision — there is nothing about\nthe process that will guarantee correct decisions x% of the time. This is because\nthe probability question being answered is not “what is the probability that this\nhappened by chance?” but rather “given a chance model, what is the probability of\na result this extreme?” We then deduce backward about the appropriateness of the\nchance model, but that judgment does not carry a probability. This point has been\nthe subject of much confusion.\nValue of the p-value\nConsiderable controversy has surrounded the use of the p-value in recent years.\nOne psychology journal has gone so far as to “ban” the use of p-values in\nsubmitted papers on the grounds that publication decisions based solely on the p-\nvalue were resulting in the publication of poor research. Too many researchers,\nonly dimly aware of what a p-value really means, root around in the data and\namong different possible hypotheses to test, until they find a combination that\nyields a significant p-value and, hence, a paper suitable for publication.\nThe real problem is that people want more meaning from the p-value than it\ncontains. Here’s what we would like the p-value to convey:\nThe probability that the result is due to chance.\nWe hope for a low value, so we can conclude that we’ve proved something. This\nis how many journal editors were interpreting the p-value. But here’s what the p-\nvalue actually represents:\nThe probability that, given a chance model, results as extreme as the observed\nresults could occur.\nThe difference is subtle, but real. A significant p-value does not carry you quite as\nfar along the road to “proof” as it seems to promise. The logical foundation for the\nconclusion “statistically significant” is somewhat weaker when the real meaning\n\n\nof the p-value is understood.\nIn March 2016, the American Statistical Association, after much internal\ndeliberation, revealed the extent of misunderstanding about p-values when it\nissued a cautionary statement regarding their use.\nThe ASA statement stressed six principles for researchers and journal editors:\n1. P-values can indicate how incompatible the data are with a specified\nstatistical model.\n2. P-values do not measure the probability that the studied hypothesis is\ntrue, or the probability that the data were produced by random chance\nalone.\n3. Scientific conclusions and business or policy decisions should not be\nbased only on whether a p-value passes a specific threshold.\n4. Proper inference requires full reporting and transparency.\n5. A p-value, or statistical significance, does not measure the size of an\neffect or the importance of a result.\n6. By itself, a p-value does not provide a good measure of evidence\nregarding a model or hypothesis.\n\n\nType 1 and Type 2 Errors\nIn assessing statistical significance, two types of error are possible:\nType 1 error, in which you mistakenly conclude an effect is real, when it is\nreally just due to chance\nType 2 error, in which you mistakenly conclude that an effect is not real (i.e.,\ndue to chance), when it really is real\nActually, a Type 2 error is not so much an error as a judgment that the sample size\nis too small to detect the effect. When a p-value falls short of statistical\nsignificance (e.g., it exceeds 5%), what we are really saying is “effect not\nproven.” It could be that a larger sample would yield a smaller p-value.\nThe basic function of significance tests (also called hypothesis tests) is to protect\nagainst being fooled by random chance; thus they are typically structured to\nminimize Type 1 errors.\n\n\nData Science and P-Values\nThe work that data scientists do is typically not destined for publication in\nscientific journals, so the debate over the value of a p-value is somewhat\nacademic. For a data scientist, a p-value is a useful metric in situations where you\nwant to know whether a model result that appears interesting and useful is within\nthe range of normal chance variability. As a decision tool in an experiment, a p-\nvalue should not be considered controlling, but merely another point of\ninformation bearing on a decision. For example, p-values are sometimes used as\nintermediate inputs in some statistical or machine learning models — a feature\nnight be included in or excluded from a model depending on its p-value.\nKEY IDEAS\nSignificance tests are used to determine whether an observed effect is within the range of chance\nvariation for a null hypothesis model.\nThe p-value is the probability that results as extreme as the observed results might occur, given a\nnull hypothesis model.\nThe alpha value is the threshold of “unusualness” in a null hypothesis chance model.\nSignificance testing has been much more relevant for formal reporting of research than for data\nscience (but has been fading recently, even for the former).\n\n\nFurther Reading\nStephen Stigler, “Fisher and the 5% Level,” Chance vol. 21, no. 4 (2008):\n12. This article is a short commentary on Ronald Fisher’s 1925 book\nStatistical Methods for Research Workers, and his emphasis on the 5% level\nof significance.\nSee also “Hypothesis Tests” and the further reading mentioned there.\n\n\nt-Tests\nThere are numerous types of significance tests, depending on whether the data\ncomprises count data or measured data, how many samples there are, and what’s\nbeing measured. A very common one is the t-test, named after Student’s t-\ndistribution, originally developed by W. S. Gossett to approximate the distribution\nof a single sample mean (see “Student’s t-Distribution”).\nKEY TERMS\nTest statistic\nA metric for the difference or effect of interest.\nt-statistic\nA standardized version of the test statistic.\nt-distribution\nA reference distribution (in this case derived from the null hypothesis), to which the observed t-\nstatistic can be compared.\nAll significance tests require that you specify a test statistic to measure the effect\nyou are interested in, and help you determine whether that observed effect lies\nwithin the range of normal chance variation. In a resampling test (see the\ndiscussion of permutation in “Permutation Test”), the scale of the data does not\nmatter. You create the reference (null hypothesis) distribution from the data itself,\nand use the test statistic as is.\nIn the 1920s and 30s, when statistical hypothesis testing was being developed, it\nwas not feasible to randomly shuffle data thousands of times to do a resampling\ntest. Statisticians found that a good approximation to the permutation (shuffled)\ndistribution was the t-test, based on Gossett’s t-distribution. It is used for the very\ncommon two-sample comparison — A/B test — in which the data is numeric. But\nin order for the t-distribution to be used without regard to scale, a standardized\nform of the test statistic must be used.\nA classic statistics text would at this stage show various formulas that incorporate\nGossett’s distribution and demonstrate how to standardize your data to compare it\nto the standard t-distribution. These formulas are not shown here because all\n\n\nstatistical software, as well as R and Python, include commands that embody the\nformula. In R, the function is t.test:\n> t.test(Time ~ Page, data=session_times, alternative='less' )\n \nWelch Two Sample t-test\ndata:  Time by Page\nt = -1.0983, df = 27.693, p-value = 0.1408\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n     -Inf 19.59674\nsample estimates:\nmean in group Page A mean in group Page B\n            126.3333             162.0000\nThe alternative hypothesis is that the session time mean for page A is less than for\npage B. This is fairly close to the permutation test p-value of 0.124 (see\n“Example: Web Stickiness”).\nIn a resampling mode, we structure the solution to reflect the observed data and\nthe hypothesis to be tested, not worrying about whether the data is numeric or\nbinary, sample sizes are balanced or not, sample variances, or a variety of other\nfactors. In the formula world, many variations present themselves, and they can be\nbewildering. Statisticians need to navigate that world and learn its map, but data\nscientists do not — they are typically not in the business of sweating the details of\nhypothesis tests and confidence intervals the way a researcher preparing a paper\nfor presentation might.\nKEY IDEAS\nBefore the advent of computers, resampling tests were not practical and statisticians used standard\nreference distributions.\nA test statistic could then be standardized and compared to the reference distribution.\nOne such widely used standardized statistic is the t-statistic.\n\n\nFurther Reading\nAny introductory statistics text will have illustrations of the t-statistic and its\nuses; two good ones are Statistics, 4th ed., by David Freedman, Robert\nPisani, and Roger Purves (W. W. Norton, 2007) and The Basic Practice of\nStatistics by David S. Moore (Palgrave Macmillan, 2010).\nFor a treatment of both the t-test and resampling procedures in parallel, see\nIntroductory Statistics and Analytics: A Resampling Perspective by Peter\nBruce (Wiley, 2014) or Statistics by Robin Lock and four other Lock family\nmembers (Wiley, 2012).\n\n\nMultiple Testing\nAs we’ve mentioned previously, there is a saying in statistics: “torture the data\nlong enough, and it will confess.” This means that if you look at the data through\nenough different perspectives, and ask enough questions, you can almost\ninvariably find a statistically significant effect.\nKEY TERMS\nType 1 error\nMistakenly concluding that an effect is statistically significant.\nFalse discovery rate\nAcross multiple tests, the rate of making a Type 1 error.\nAdjustment of p-values\nAccounting for doing multiple tests on the same data.\nOverfitting\nFitting the noise.\nFor example, if you have 20 predictor variables and one outcome variable, all\nrandomly generated, the odds are pretty good that at least one predictor will\n(falsely) turn out to be statistically significant if you do a series of 20 significance\ntests at the alpha = 0.05 level. As previously discussed, this is called a Type 1\nerror. You can calculate this probability by first finding the probability that all\nwill correctly test nonsignificant at the 0.05 level. The probability that one will\ncorrectly test nonsignificant is 0.95, so the probability that all 20 will correctly\ntest nonsignificant is 0.95 × 0.95 × 0.95 … or 0.9520 = 0.36.1 The probability that\nat least one predictor will (falsely) test significant is the flip side of this\nprobability, or 1 – (probability that all will be nonsignificant) = 0.64.\nThis issue is related to the problem of overfitting in data mining, or “fitting the\nmodel to the noise.” The more variables you add, or the more models you run, the\ngreater the probability that something will emerge as “significant” just by chance.\nIn supervised learning tasks, a holdout set where models are assessed on data that\nthe model has not seen before mitigates this risk. In statistical and machine\nlearning tasks not involving a labeled holdout set, the risk of reaching conclusions\n\n\nbased on statistical noise persists.\nIn statistics, there are some procedures intended to deal with this problem in very\nspecific circumstances. For example, if you are comparing results across multiple\ntreatment groups you might ask multiple questions. So, for treatments A–C, you\nmight ask:\nIs A different from B?\nIs B different from C?\nIs A different from C?\nOr, in a clinical trial, you might want to look at results from a therapy at multiple\nstages. In each case, you are asking multiple questions, and with each question,\nyou are increasing the chance of being fooled by chance. Adjustment procedures\nin statistics can compensate for this by setting the bar for statistical significance\nmore stringently than it would be set for a single hypothesis test. These adjustment\nprocedures typically involve “dividing up the alpha” according to the number of\ntests. This results in a smaller alpha (i.e., a more stringent bar for statistical\nsignificance) for each test. One such procedure, the Bonferroni adjustment, simply\ndivides the alpha by the number of observations n.\nHowever, the problem of multiple comparisons goes beyond these highly\nstructured cases and is related to the phenomenon of repeated data “dredging” that\ngives rise to the saying about torturing the data. Put another way, given sufficiently\ncomplex data, if you haven’t found something interesting, you simply haven’t\nlooked long and hard enough. More data is available now than ever before, and\nthe number of journal articles published nearly doubled between 2002 and 2010.\nThis gives rise to lots of opportunities to find something interesting in the data,\nincluding multiplicity issues such as:\nChecking for multiple pairwise differences across groups\nLooking at multiple subgroup results (“we found no significant treatment\neffect overall, but we did find an effect for unmarried women younger than\n30”)\nTrying lots of statistical models\n\n\nIncluding lots of variables in models\nAsking a number of different questions (i.e., different possible outcomes)\n\n\nFALSE DISCOVERY RATE\nThe term false discovery rate was originally used to describe the rate at which a given set of\nhypothesis tests would falsely identify a significant effect. It became particularly useful with the\nadvent of genomic research, in which massive numbers of statistical tests might be conducted as\npart of a gene sequencing project. In these cases, the term applies to the testing protocol, and a\nsingle false “discovery” refers to the outcome of a hypothesis test (e.g., between two samples).\nResearchers sought to set the parameters of the testing process to control the false discovery\nrate at a specified level. The term has also been used in the data mining community in a\nclassification context, in which a false discovery is a mislabeling of a single record — in particular\nthe mislabeling of 0s as 1s (see Chapter 5 and “The Rare Class Problem”).\nFor a variety of reasons, including especially this general issue of “multiplicity,”\nmore research does not necessarily mean better research. For example, the\npharmaceutical company Bayer found in 2011 that when it tried to replicate 67\nscientific studies, it could fully replicate only 14 of them. Nearly two-thirds could\nnot be replicated at all.\nIn any case, the adjustment procedures for highly defined and structured statistical\ntests are too specific and inflexible to be of general use to data scientists. The\nbottom line for data scientists on multiplicity is:\nFor predictive modeling, the risk of getting an illusory model whose apparent\nefficacy is largely a product of random chance is mitigated by cross-\nvalidation (see “Cross-Validation”), and use of a holdout sample.\nFor other procedures without a labeled holdout set to check the model, you\nmust rely on:\nAwareness that the more you query and manipulate the data, the greater the\nrole that chance might play; and\nResampling and simulation heuristics to provide random chance\nbenchmarks against which observed results can be compared.\nKEY IDEAS\nMultiplicity in a research study or data mining project (multiple comparisons, many variables, many\nmodels, etc.) increases the risk of concluding that something is significant just by chance.\nFor situations involving multiple statistical comparisons (i.e., multiple tests of significance) there are\n\n\nstatistical adjustment procedures.\nIn a data mining situation, use of a holdout sample with labeled outcome variables can help avoid\nmisleading results.\n\n\nFurther Reading\n1. For a short exposition of one procedure (Dunnett’s) to adjust for multiple\ncomparisons, see David Lane’s online statistics text.\n2. Megan Goldman offers a slightly longer treatment of the Bonferroni\nadjustment procedure.\n3. For an in-depth treatment of more flexible statistical procedures to adjust\np-values, see Resampling-Based Multiple Testing by Peter Westfall and\nStanley Young (Wiley, 1993).\n4. For a discussion of data partitioning and the use of holdout samples in\npredictive modeling, see Data Mining for Business Analytics, Chapter\n2, by Galit Shmueli, Peter Bruce, and Nitin Patel (Wiley, 2016).\n\n\nDegrees of Freedom\nIn the documentation and settings to many statistical tests, you will see reference\nto “degrees of freedom.” The concept is applied to statistics calculated from\nsample data, and refers to the number of values free to vary. For example, if you\nknow the mean for a sample of 10 values, and you also know 9 of the values, you\nalso know the 10th value. Only 9 are free to vary.\nKEY TERMS\nn or sample size\nThe number of observations (also called rows or records) in the data.\nd.f.\nDegrees of freedom.\nThe number of degrees of freedom is an input to many statistical tests. For\nexample, degrees of freedom is the name given to the n – 1 denominator seen in\nthe calculations for variance and standard deviation. Why does it matter? When\nyou use a sample to estimate the variance for a population, you will end up with\nan estimate that is slightly biased downward if you use n in the denominator. If\nyou use n – 1 in the denominator, the estimate will be free of that bias.\nA large share of a traditional statistics course or text is consumed by various\nstandard tests of hypotheses (t-test, F-test, etc.). When sample statistics are\nstandardized for use in traditional statistical formulas, degrees of freedom is part\nof the standardization calculation to ensure that your standardized data matches the\nappropriate reference distribution (t-distribution, F-distribution, etc.).\nIs it important for data science? Not really, at least in the context of significance\ntesting. For one thing, formal statistical tests are used only sparingly in data\nscience. For another, the data size is usually large enough that it rarely makes a\nreal difference for a data scientist whether, for example, the denominator has n or\nn – 1.\nThere is one context, though, in which it is relevant: the use of factored variables\nin regression (including logistic regression). Regression algorithms choke if\nexactly redundant predictor variables are present. This most commonly occurs\n\n\nwhen factoring categorical variables into binary indicators (dummies). Consider\nday of week. Although there are seven days of the week, there are only six degrees\nof freedom in specifying day of week. For example, once you know that day of\nweek is not Monday through Saturday, you know it must be Sunday. Inclusion of\nthe Mon–Sat indicators thus means that also including Sunday would cause the\nregression to fail, due to a multicollinearity error.\nKEY IDEAS\nThe number of degrees of freedom (d.f.) forms part of the calculation to standardize test statistics\nso they can be compared to reference distributions (t-distribution, F-distribution, etc.).\nThe concept of degrees of freedom lies behind the factoring of categorical variables into n – 1\nindicator or dummy variables when doing a regression (to avoid multicollinearity).\n\n\nFurther Reading\nThere are several web tutorials on degrees of freedom.\n\n\nANOVA\nSuppose that, instead of an A/B test, we had a comparison of multiple groups, say\nA-B-C-D, each with numeric data. The statistical procedure that tests for a\nstatistically significant difference among the groups is called analysis of\nvariance, or ANOVA.\nKEY TERMS FOR ANOVA\nPairwise comparison\nA hypothesis test (e.g., of means) between two groups among multiple groups.\nOmnibus test\nA single hypothesis test of the overall variance among multiple group means.\nDecomposition of variance\nSeparation of components. contributing to an individual value (e.g., from the overall average, from\na treatment mean, and from a residual error).\nF-statistic\nA standardized statistic that measures the extent to which differences among group means\nexceeds what might be expected in a chance model.\nSS\n“Sum of squares,” referring to deviations from some average value.\nTable 3-3 shows the stickiness of four web pages, in numbers of seconds spent on\nthe page. The four pages are randomly switched out so that each web visitor\nreceives one at random. There are a total of five visitors for each page, and, in\nTable 3-3, each column is an independent set of data. The first viewer for page 1\nhas no connection to the first viewer for page 2. Note that in a web test like this,\nwe cannot fully implement the classic randomized sampling design in which each\nvisitor is selected at random from some huge population. We must take the visitors\nas they come. Visitors may systematically differ depending on time of day, time of\nweek, season of the year, conditions of their internet, what device they are using,\nand so on. These factors should be considered as potential bias when the\nexperiment results are reviewed.\nTable 3-3. Stickiness (in seconds) for\nfour web pages\n\n\nPage 1 Page 2 Page 3 Page 4\n164\n178\n175\n155\n172\n191\n193\n166\n177\n182\n171\n164\n156\n185\n163\n170\n195\n177\n176\n168\nAverage\n172\n185\n176\n162\nGrand average\n173.75\nNow, we have a conundrum (see Figure 3-6). When we were comparing just two\ngroups, it was a simple matter; we merely looked at the difference between the\nmeans of each group. With four means, there are six possible comparisons\nbetween groups:\nPage 1 compared to page 2\nPage 1 compared to page 3\nPage 1 compared to page 4\nPage 2 compared to page 3\nPage 2 compared to page 4\nPage 3 compared to page 4\n\n\nFigure 3-6. Boxplots of the four groups show considerable differences among them\nThe more such pairwise comparisons we make, the greater the potential for being\nfooled by random chance (see “Multiple Testing”). Instead of worrying about all\nthe different comparisons between individual pages we could possibly make, we\ncan do a single overall omnibus test that addresses the question, “Could all the\npages have the same underlying stickiness, and the differences among them be due\nto the random way in which a common set of session times got allocated among\nthe four pages?”\n\n\nThe procedure used to test this is ANOVA. The basis for it can be seen in the\nfollowing resampling procedure (specified here for the A-B-C-D test of web page\nstickiness):\n1. Combine all the data together in a single box\n2. Shuffle and draw out four resamples of five values each\n3. Record the mean of each of the four groups\n4. Record the variance among the four group means\n5. Repeat steps 2–4 many times (say 1,000)\nWhat proportion of the time did the resampled variance exceed the observed\nvariance? This is the p-value.\nThis type of permutation test is a bit more involved than the type used in\n“Permutation Test”. Fortunately, the aovp function in the lmPerm package\ncomputes a permutation test for this case:\n> library(lmPerm)\n> summary(aovp(Time ~ Page, data=four_sessions))\n[1] \"Settings:  unique SS \"\nComponent 1 :\n            Df R Sum Sq R Mean Sq Iter Pr(Prob)\nPage         3    831.4    277.13 3104  0.09278 .\nResiduals   16   1618.4    101.15\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nThe p-value, given by Pr(Prob), is 0.09278. The column Iter lists the number of\niterations taken in the permutation test. The other columns correspond to a\ntraditional ANOVA table and are described next.\n\n\nF-Statistic\nJust like the t-test can be used instead of a permutation test for comparing the mean\nof two groups, there is a statistical test for ANOVA based on the F-statistic. The\nF-statistic is based on the ratio of the variance across group means (i.e., the\ntreatment effect) to the variance due to residual error. The higher this ratio, the\nmore statistically significant the result. If the data follows a normal distribution,\nthen statistical theory dictates that the statistic should have a certain distribution.\nBased on this, it is possible to compute a p-value.\nIn R, we can compute an ANOVA table using the aov function:\n> summary(aov(Time ~ Page, data=four_sessions))\n            Df Sum Sq Mean Sq F value Pr(>F)\nPage         3  831.4   277.1    2.74 0.0776 .\nResiduals   16 1618.4   101.2\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\nDf is “degrees of freedom,” Sum Sq is “sum of squares,” Mean Sq is “mean\nsquares” (short for mean-squared deviations), and F value is the F-statistic. For\nthe grand average, sum of squares is the departure of the grand average from 0,\nsquared, times 20 (the number of observations). The degrees of freedom for the\ngrand average is 1, by definition. For the treatment means, the degrees of freedom\nis 3 (once three values are set, and then the grand average is set, the other\ntreatment mean cannot vary). Sum of squares for the treatment means is the sum of\nsquared departures between the treatment means and the grand average. For the\nresiduals, degrees of freedom is 20 (all observations can vary), and SS is the sum\nof squared difference between the individual observations and the treatment\nmeans. Mean squares (MS) is the sum of squares divided by the degrees of\nfreedom. The F-statistic is MS(treatment)/MS(error). The F value thus depends\nonly on this ratio, and can be compared to a standard F distribution to determine\nwhether the differences among treatment means is greater than would be expected\nin random chance variation.\n\n\nDECOMPOSITION OF VARIANCE\nObserved values in a data set can be considered sums of different components. For any observed\ndata value within a data set, we can break it down into the grand average, the treatment effect,\nand the residual error. We call this a “decomposition of variance.”\n1. Start with grand average (173.75 for web page stickiness data).\n2. Add treatment effect, which might be negative (independent variable = web page).\n3. Add residual error, which might be negative.\nThus, the decomposition of the variance for the top-left value in the A-B-C-D test table is as\nfollows:\n1. Start with grand average: 173.75\n2. Add treatment (group) effect: –1.75 (172 – 173.75).\n3. Add residual: –8 (164 – 172).\n4. Equals: 164.\n\n\nTwo-Way ANOVA\nThe A-B-C-D test just described is a “one-way” ANOVA, in which we have one\nfactor (group) that is varying. We could have a second factor involved — say,\n“weekend versus weekday” — with data collected on each combination (group A\nweekend, group A weekday, group B weekend, etc.). This would be a “two-way\nANOVA,” and we would handle it in similar fashion to the one-way ANOVA by\nidentifying the “interaction effect.” After identifying the grand average effect, and\nthe treatment effect, we then separate the weekend and the weekday observations\nfor each group, and find the difference between the averages for those subsets and\nthe treatment average.\nYou can see that ANOVA, then two-way ANOVA, are the first steps on the road\ntoward a full statistical model, such as regression and logistic regression, in\nwhich multiple factors and their effects can be modeled (see Chapter 4).\nKEY IDEAS\nANOVA is a statistical proecdure for analyzing the results of an experiment with multiple groups.\nIt is the extension of similar procedures for the A/B test, used to assess whether the overall\nvariation among groups is within the range of chance variation.\nA useful outcome of an ANOVA is the identification of variance components associated with group\ntreatments, interaction effects, and errors.\n\n\nFurther Reading\n1. Introductory Statistics: A Resampling Perspective by Peter Bruce\n(Wiley, 2014) has a chapter on ANOVA.\n2. Introduction to Design and Analysis of Experiments by George Cobb\n(Wiley, 2008) is a comprehensive and readable treatment of its subject.\n\n\nChi-Square Test\nWeb testing often goes beyond A/B testing and tests multiple treatments at once.\nThe chi-square test is used with count data to test how well it fits some expected\ndistribution. The most common use of the chi-square statistic in statistical\npractice is with \n contingency tables, to assess whether the null hypothesis\nof independence among variables is reasonable.\nThe chi-square test was originally developed by Karl Pearson in 1900. The term\n“chi” comes from the greek letter  used by Pearson in the article.\nKEY TERMS\nChi-square statistic\nA measure of the extent to which some observed data departs from expectation.\nExpectation or expected\nHow we would expect the data to turn out under some assumption, typically the null hypothesis.\nd.f.\nDegrees of freedom.\nNOTE\n means “rows by columns” — a 2×3 table has two rows and three columns.\n\n\nChi-Square Test: A Resampling Approach\nSuppose you are testing three different headlines — A, B, and C — and you run\nthem each on 1,000 visitors, with the results shown in Table 3-4.\nTable 3-4. Web testing results of three\ndifferent headlines\nHeadline A Headline B Headline C\nClick\n14\n8\n12\nNo-click 986\n992\n988\nThe headlines certainly appear to differ. Headline A returns nearly twice the click\nrate of B. The actual numbers are small, though. A resampling procedure can test\nwhether the click rates differ to an extent greater than chance might cause. For this\ntest, we need to have the “expected” distribution of clicks, and, in this case, that\nwould be under the null hypothesis assumption that all three headlines share the\nsame click rate, for an overall click rate of 34/3,000. Under this assumption, our\ncontingency table would look like Table 3-5.\nTable 3-5. Expected if all three\nheadlines have the same click rate\n(null hypothesis)\nHeadline A Headline B Headline C\nClick\n11.33\n11.33\n11.33\nNo-click 988.67\n988.67\n988.67\nThe Pearson residual is defined as:\n\n\nR measures the extent to which the actual counts differ from these expected counts\n(see Table 3-6).\nTable 3-6. Pearson residuals\nHeadline A Headline B Headline C\nClick\n0.792\n-0.990\n0.198\nNo-click -0.085\n0.106\n-0.021\nThe chi-squared statistic is defined as the sum of the squared Pearson residuals:\nwhere r and c are the number of rows and columns, respectively. The chi-squared\nstatistic for this example is 1.666. Is that more than could reasonably occur in a\nchance model?\nWe can test with this resampling algorithm:\n1. Constitute a box with 34 ones (clicks) and 2,966 zeros (no clicks).\n2. Shuffle, take three separate samples of 1,000, and count the clicks in\neach.\n3. Find the squared differences between the shuffled counts and the\nexpected counts, and sum them.\n4. Repeat steps 2 and 3, say, 1,000 times.\n5. How often does the resampled sum of squared deviations exceed the\nobserved? That’s the p-value.\nThe function chisq.test can be used to compute a resampled chi-square\nstatistic. For the click data, the chi-square test is:\n\n\n> chisq.test(clicks, simulate.p.value=TRUE)\n \nPearson's Chi-squared test with simulated p-value (based on 2000 replicates)\ndata:  clicks\nX-squared = 1.6659, df = NA, p-value = 0.4853\nThe test shows that this result could easily have been obtained by randomness.\n\n\nChi-Squared Test: Statistical Theory\nAsymptotic statistical theory shows that the distribution of the chi-squared statistic\ncan be approximated by a chi-square distribution. The appropriate standard chi-\nsquare distribution is determined by the degrees of freedom (see “Degrees of\nFreedom”). For a contingency table, the degrees of freedom are related to the\nnumber of rows (r) and columns (s) as follows:\nThe chi-square distribution is typically skewed, with a long tail to the right; see\nFigure 3-7 for the distribution with 1, 2, 5, and 10 degrees of freedom. The further\nout on the chi-square distribution the observed statistic is, the lower the p-value.\nThe function chisq.test can be used to compute the p-value using the chi-\nsquared distribution as a reference:\n> chisq.test(clicks, simulate.p.value=FALSE)\n \nPearson's Chi-squared test\ndata:  clicks\nX-squared = 1.6659, df = 2, p-value = 0.4348\nThe p-value is a little less than the resampling p-value: this is because the chi-\nsquare distribution is only an approximation of the actual distribution of the\nstatistic.\n\n\nFigure 3-7. Chi-square distribution with various degrees of freedom (probability on y-axis, value of chi-\nsquare statistic on x-axis)\n\n\nFisher’s Exact Test\nThe chi-square distribution is a good approximation of the shuffled resampling\ntest just described, except when counts are extremely low (single digits,\nespecially five or fewer). In such cases, the resampling procedure will yield more\naccurate p-values. In fact, most statistical software has a procedure to actually\nenumerate all the possible rearrangements (permutations) that can occur, tabulate\ntheir frequencies, and determine exactly how extreme the observed result is. This\nis called Fisher’s exact test after the great statistician R. A. Fisher. R code for\nFisher’s exact test is simple in its basic form:\n> fisher.test(clicks)\n \nFisher's Exact Test for Count Data\ndata:  clicks\np-value = 0.4824\nalternative hypothesis: two.sided\nThe p-value is very close to the p-value of 0.4853 obtained using the resampling\nmethod.\nWhere some counts are very low but others are quite high (e.g., the denominator in\na conversion rate), it may be necessary to do a shuffled permutation test instead of\na full exact test, due to the difficulty of calculating all possible permutations. The\npreceding R function has several arguments that control whether to use this\napproximation (simulate.p.value=TRUE or FALSE), how many iterations\nshould be used (B=...), and a computational constraint (workspace=...) that\nlimits how far calculations for the exact result should go.\nDETECTING SCIENTIFIC FRAUD\nAn interesting example is provided by Tufts University researcher Thereza Imanishi-Kari, who was\naccused in 1991 of fabricating data in her research. Congressman John Dingell became involved, and the\ncase eventually led to the resignation of her colleague, David Baltimore, from the presidency of\nRockefeller University.\nImanishi-Kari was ultimately exonerated after a lengthy proceeding. However, one element in the case\nrested on statistical evidence regarding the expected distribution of digits in her laboratory data, where\neach observation had many digits. Investigators focused on the interior digits, which would be expected\nto follow a uniform random distribution. That is, they would occur randomly, with each digit having equal\nprobability of occurring (the lead digit might be predominantly one value, and the final digits might be\naffected by rounding). Table 3-7 lists the frequencies of interior digits from the actual data in the case.\n\n\nTable 3-7. Central\ndigit in laboratory\ndata\nDigit Frequency\n0\n14\n1\n71\n2\n7\n3\n65\n4\n23\n5\n19\n6\n12\n7\n45\n8\n53\n9\n6\nThe distribution of the 315 digits, shown in Figure 3-8 certainly looks nonrandom:\nInvestigators calculated the departure from expectation (31.5 — that’s how often each digit would occur\nin a strictly uniform distribution) and used a chi-square test (a resampling procedure could equally have\nbeen used) to show that the actual distribution was well beyond the range of normal chance variation.\n\n\nFigure 3-8. Frequency histogram for Imanishi-Kari lab data\n\n\nRelevance for Data Science\nMost standard uses of the chi-square test, or Fisher’s exact test, are not terribly\nrelevant for data science. In most experiments, whether A-B or A-B-C…, the goal\nis not simply to establish statistical significance, but rather to arive at the best\ntreatment. For this purpose, multi-armed bandits (see “Multi-Arm Bandit\nAlgorithm”) offer a more complete solution.\nOne data science application of the chi-square test, especially Fisher’s exact\nversion, is in determining appropriate sample sizes for web experiments. These\nexperiments often have very low click rates and, despite thousands of exposures,\ncount rates might be too small to yield definitive conclusions in an experiment. In\nsuch cases, Fisher’s exact test, the chi-square test, and other tests can be useful as\na component of power and sample size calculations (see “Power and Sample\nSize”).\nChi-square tests are used widely in research by investigators in search of the\nelusive statistically significant p-value that will allow publication. Chi-square\ntests, or similar resampling simulations, are used in data science applications\nmore as a filter to determine whether an effect or feature is worthy of further\nconsideration than as a formal test of significance. For example, they are used in\nspatial statistics and mapping to determine whether spatial data conforms to a\nspecified null distribution (e.g., are crimes concentrated in a certain area to a\ngreater degree than random chance would allow?). They can also be used in\nautomated feature selection in machine learning, to assess class prevalence across\nfeatures and identify features where the prevalence of a certain class is unusually\nhigh or low, in a way that is not compatible with random variation.\nKEY IDEAS\nA common procedure in statistics is to test whether observed data counts are consistent with an\nassumption of independence (e.g., propensity to buy a particular item is independent of gender).\nThe chi-square distribution is the reference distribution (which embodies the assumption of\nindependence) to which the observed calculated chi-square statistic must be compared.\n\n\nFurther Reading\nR. A. Fisher’s famous “Lady Tasting Tea” example from the beginning of the\n20th century remains a simple and effective illustration of his exact test.\nGoogle “Lady Tasting Tea,” and you will find a number of good writeups.\nStat Trek offers a good tutorial on the chi-square test.\n\n\nMulti-Arm Bandit Algorithm\nMulti-arm bandits offer an approach to testing, especially web testing, that allows\nexplicit optimization and more rapid decision making than the traditional\nstatistical approach to designing experiments.\nKEY TERMS\nMulti-arm bandit\nAn imaginary slot machine with multiple arms for the customer to choose from, each with different\npayoffs, here taken to be an analogy for a multitreatment experiment.\nArm\nA treatment in an experiment (e.g., “headline A in a web test”).\nWin\nThe experimental analog of a win at the slot machine (e.g., “customer clicks on the link”).\nA traditional A/B test involves data collected in an experiment, according to a\nspecified design, to answer a specific question such as, “Which is better,\ntreatment A or treatment B?” The presumption is that once we get an answer to\nthat question, the experimenting is over and we proceed to act on the results.\nYou can probably perceive several difficulties with that approach. First, our\nanswer may be inconclusive: “effect not proven.” In other words, the results from\nthe experiment may suggest an effect, but if there is an effect, we don’t have a big\nenough sample to prove it (to the satisfaction of the traditional statistical\nstandards). What decision do we take? Second, we might want to begin taking\nadvantage of results that come in prior to the conclusion of the experiment. Third,\nwe might want the right to change our minds or to try something different based on\nadditional data that comes in after the experiment is over. The traditional\napproach to experiments and hypothesis tests dates from the 1920s, and is rather\ninflexible. The advent of computer power and software has enabled more\npowerful flexible approaches. Moreover, data science (and business in general) is\nnot so worried about statistical significance, but more concerned with optimizing\noverall effort and results.\nBandit algorithms, which are very popular in web testing, allow you to test\n\n\nmultiple treatments at once and reach conclusions faster than traditional statistical\ndesigns. They take their name from slot machines used in gambling, also termed\none-armed bandits (since they are configured in such a way that they extract\nmoney from the gambler in a steady flow). If you imagine a slot machine with\nmore than one arm, each arm paying out at a different rate, you would have a\nmulti-armed bandit, which is the full name for this algorithm.\nYour goal is to win as much money as possible, and more specifically, to identify\nand settle on the winning arm sooner rather than later. The challenge is that you\ndon’t know at what rate the arms pay out — you only know the results of pulling\nthe arm. Suppose each “win” is for the same amount, no matter which arm. What\ndiffers is the probability of a win. Suppose further that you initially try each arm\n50 times and get the following results:\nArm A: 10 wins out of 50\nArm B: 2 win out of 50\nArm C: 4 wins out of 50\nOne extreme approach is to say, “Looks like arm A is a winner — let’s quit trying\nthe other arms and stick with A.” This takes full advantage of the information from\nthe initial trial. If A is truly superior, we get the benefit of that early on. On the\nother hand, if B or C is truly better, we lose any opportunity to discover that.\nAnother extreme approach is to say, “This all looks to be within the realm of\nchance — let’s keep pulling them all equally.” This gives maximum opportunity\nfor alternates to A to show themselves. However, in the process, we are deploying\nwhat seem to be inferior treatments. How long do we permit that? Bandit\nalgorithms take a hybrid approach: we start pulling A more often, to take\nadvantage of its apparent superiority, but we don’t abandon B and C. We just pull\nthem less often. If A continues to outperform, we continue to shift resources\n(pulls) away from B and C and pull A more often. If, on the other hand, C starts to\ndo better, and A starts to do worse, we can shift pulls from A back to C. If one of\nthem turns out to be superior to A and this was hidden in the initial trial due to\nchance, it now has an opportunity to emerge with further testing.\nNow think of applying this to web testing. Instead of multiple slot machine arms,\nyou might have multiple offers, headlines, colors, and so on, being tested on a\n\n\nwebsite. Customers either click (a “win” for the merchant) or don’t click. Initially,\nthe offers are shown randomly and equally. If, however, one offer starts to\noutperform the others, it can be shown (“pulled”) more often. But what should the\nparameters of the algorithm that modifies the pull rates be? What “pull rates”\nshould we change to, and when should we change?\nHere is one simple algorithm, the epsilon-greedy algorithm for an A/B test:\n1. Generate a random number between 0 and 1.\n2. If the number lies between 0 and epsilon (where epsilon is a number\nbetween 0 and 1, typically fairly small), flip a fair coin (50/50\nprobability), and:\na. If the coin is heads, show offer A.\nb. If the coin is tails, show offer B.\n3. If the number is ≥ epsilon, show whichever offer has had the highest\nresponse rate to date.\nEpsilon is the single parameter that governs this algorithm. If epsilon is 1, we end\nup with a standard simple A/B experiment (random allocation between A and B\nfor each subject). If epsilon is 0, we end up with a purely greedy algorithm — it\nseeks no further experimentation, simply assigning subjects (web visitors) to the\nbest-performing treatment.\nA more sophisticated algorithm uses “Thompson’s sampling.” This procedure\n“samples” (pulls a bandit arm) at each stage to maximize the probability of\nchoosing the best arm. Of course you don’t know which is the best arm — that’s\nthe whole problem! — but as you observe the payoff with each successive draw,\nyou gain more information. Thompson’s sampling uses a Bayesian approach: some\nprior distribution of rewards is assumed initially, using what is called a beta\ndistribution (this is a common mechanism for specifying prior information in a\nBayesian problem). As information accumulates from each draw, this information\ncan be updated, allowing the selection of the next draw to be better optimized as\nfar as choosing the right arm.\nBandit algorithms can efficiently handle 3+ treatments and move toward optimal\nselection of the “best.” For traditional statistical testing procedures, the\n\n\ncomplexity of decision making for 3+ treatments far outstrips that of the traditional\nA/B test, and the advantage of bandit algorithms is much greater.\nKEY IDEAS\nTraditional A/B tests envision a random sampling process, which can lead to excessive exposure to\nthe inferior treatment.\nMulti-arm bandits, in contrast, alter the sampling process to incorporate information learned during\nthe experiment and reduce the frequency of the inferior treatment.\nThey also facilitate efficient treatment of more than two treatments.\nThere are different algorithms for shifting sampling probability away from the inferior treatment(s)\nand to the (presumed) superior one.\n\n\nFurther Reading\nAn excellent short treatment of multi-arm bandit algorithms is found in\nBandit Algorithms, by John Myles White (O’Reilly, 2012). White includes\nPython code, as well as the results of simulations to assess the performance\nof bandits.\nFor more (somewhat technical) information about Thompson sampling, see\n“Analysis of Thompson Sampling for the Multi-armed Bandit Problem” by\nShipra Agrawal and Navin Goyal.\n\n\nPower and Sample Size\nIf you run a web test, how do you decide how long it should run (i.e., how many\nimpressions per treatment are needed)? Despite what you may read in many\nguides to web testing on the web, there is no good general guidance — it depends,\nmainly, on the frequency with which the desired goal is attained.\nKEY TERMS\nEffect size\nThe minimum size of the effect that you hope to be able to detect in a statistical test, such as “a\n20% improvement in click rates”.\nPower\nThe probability of detecting a given effect size with a given sample size.\nSignificance level\nThe statistical significance level at which the test will be conducted.\nOne step in statistical calculations for sample size is to ask “Will a hypothesis test\nactually reveal a difference between treatments A and B?” The outcome of a\nhypothesis test — the p-value — depends on what the real difference is between\ntreatment A and treatment B. It also depends on the luck of the draw — who gets\nselected for the groups in the experiment. But it makes sense that the bigger the\nactual difference between treatments A and B, the greater the probability that our\nexperiment will reveal it; and the smaller the difference, the more data will be\nneeded to detect it. To distinguish between a .350 hitter in baseball, and a .200\nhitter, not that many at-bats are needed. To distinguish between a .300 hitter and a\n.280 hitter, a good many more at-bats will be needed.\nPower is the probability of detecting a specified effect size with specified sample\ncharacteristics (size and variability). For example, we might say (hypothetically)\nthat the probability of distinguishing between a .330 hitter and a .200 hitter in 25\nat-bats is 0.75. The effect size here is a difference of .130. And “detecting” means\nthat a hypothesis test will reject the null hypothesis of “no difference” and\nconclude there is a real effect. So the experiment of 25 at-bats (n = 25) for two\nhitters, with an effect size of 0.130, has (hypothetical) power of 0.75 or 75%.\n\n\nYou can see that there are several moving parts here, and it is easy to get tangled\nup with the numerous statistical assumptions and formulas that will be needed (to\nspecify sample variability, effect size, sample size, alpha-level for the hypothesis\ntest, etc., and to calculate power). Indeed, there is special-purpose statistical\nsoftware to calculate power. Most data scientists will not need to go through all\nthe formal steps needed to report power, for example, in a published paper.\nHowever, they may face occasions where they want to collect some data for an\nA/B test, and collecting or processing the data involves some cost. In that case,\nknowing approximately how much data to collect can help avoid the situation\nwhere you collect data at some effort, and the result ends up being inconclusive.\nHere’s a fairly intuitive alternative approach:\n1. Start with some hypothetical data that represents your best guess about\nthe data that will result (perhaps based on prior data) — for example, a\nbox with 20 ones and 80 zeros to represent a .200 hitter, or a box with\nsome observations of “time spent on website.”\n2. Create a second sample simply by adding the desired effect size to the\nfirst sample — for example, a second box with 33 ones and 67 zeros, or\na second box with 25 seconds added to each initial “time spent on\nwebsite.”\n3. Draw a bootstrap sample of size n from each box.\n4. Conduct a permutation (or formula-based) hypothesis test on the two\nbootstrap samples and record whether the difference between them is\nstatistically significant.\n5. Repeat the preceding two steps many times and determine how often the\ndifference was significant — that’s the estimated power.\n\n\nSample Size\nThe most common use of power calculations is to estimate how big a sample you\nwill need.\nFor example, suppose you are looking at click-through rates (clicks as a\npercentage of exposures), and testing a new ad against an existing ad. How many\nclicks do you need to accumulate in the study? If you are only interested in results\nthat show a huge difference (say a 50% difference), a relatively small sample\nmight do the trick. If, on the other hand, even a minor difference would be of\ninterest, then a much larger sample is needed. A standard approach is to establish\na policy that a new ad must do better than an existing ad by some percentage, say\n10%; otherwise, the existing ad will remain in place. This goal, the “effect size,”\nthen drives the sample size.\nFor example, suppose current click-through rates are about 1.1%, and you are\nseeking a 10% boost to 1.21%. So we have two boxes, box A with 1.1% ones\n(say 110 ones and 9,890 zeros), and box B with 1.21% ones (say 121 ones and\n9,879 zeros). For starters, let’s try 300 draws from each box (this would be like\n300 “impressions” for each ad). Suppose our first draw yields the following:\nBox A: 3 ones\nBox B: 5 ones\nRight away we can see that any hypothesis test would reveal this difference (5\nversus 3) to be well within the range of chance variation. This combination of\nsample size (n = 300 in each group) and effect size (10% difference) is too small\nfor any hypothesis test to reliably show a difference.\nSo we can try increasing the sample size (let’s try 2,000 impressions), and require\na larger improvement (30% instead of 10%).\nFor example, suppose current click-through rates are still 1.1%, but we are now\nseeking a 50% boost to 1.65%. So we have two boxes: box A still with 1.1% ones\n(say 110 ones and 9,890 zeros), and box B with 1.65% ones (say 165 ones and\n9,868 zeros). Now we’ll try 2,000 draws from each box. Suppose our first draw\nyields the following:\nBox A: 19 ones\n\n\nBox B: 34 ones\nA significance test on this difference (34–19) shows it still registers as “not\nsignificant” (though much closer to significance than the earlier difference of 5–3).\nTo calculate power, we would need to repeat the previous procedure many times,\nor use statistical software that can calculate power, but our initial draw suggests\nto us that even detecting a 50% improvement will require several thousand ad\nimpressions.\nIn summary, for calculating power or required sample size, there are four moving\nparts:\nSample size\nEffect size you want to detect\nSignificance level (alpha) at which the test will be conducted\nPower\nSpecify any three of them, and the fourth can be calculated. Most commonly, you\nwould want to calculate sample size, so you must specify the other three. Here is\nR code for a test involving two proportions, where both samples are the same size\n(this uses the pwr package):\npwr.2p.test(h = ..., n = ..., sig.level = ..., power = )\nh= effect size (as a proportion)\nn = sample size\nsig.level = the significance level (alpha) at which the test will be conducted\npower = power (probability of detecting the effect size)\nKEY IDEAS\nFinding out how big a sample size you need requires thinking ahead to the statistical test you plan to\nconduct.\nYou must specify the minimum size of the effect that you want to detect.\nYou must also specify the required probability of detecting that effect size (power).\nFinally, you must specify the significance level (alpha) at which the test will be conducted.\n\n\nFurther Reading\n1. Sample Size Determination and Power, by Tom Ryan (Wiley, 2013), is a\ncomprehensive and readable review of this subject.\n2. Steve Simon, a statistical consultant, has written a very engaging\nnarrative-style post on the subject.\n\n\nSummary\nThe principles of experimental design — randomization of subjects into two or\nmore groups receiving different treatments — allow us to draw valid conclusions\nabout how well the treatments work. It is best to include a control treatment of\n“making no change.” The subject of formal statistical inference — hypothesis\ntesting, p-values, t-tests, and much more along these lines — occupies much time\nand space in a traditional statistics course or text, and the formality is mostly\nunneeded from a data science perspective. However, it remains important to\nrecognize the role that random variation can play in fooling the human brain.\nIntuitive resampling procedures (permutation and bootstrap) allow data scientists\nto gauge the extent to which chance variation can play a role in their data analysis.\nThe multiplication rule states that the probability of n independent events all happening is the product of the\nindividual probabilities. For example, if you and I each flip a coin once, the probability that your coin and my\ncoin will both land heads is 0.5 × 0.5 = 0.25.\n1\n\n\nChapter 4. Regression and Prediction\nPerhaps the most common goal in statistics is to answer the question: Is the\nvariable X (or more likely, \n) associated with a variable Y, and, if so,\nwhat is the relationship and can we use it to predict Y?\nNowhere is the nexus between statistics and data science stronger than in the\nrealm of prediction — specifically the prediction of an outcome (target) variable\nbased on the values of other “predictor” variables. Another important connection\nis in the area of anomaly detection, where regression diagnostics originally\nintended for data analysis and improving the regression model can be used to\ndetect unusual records. The antecedents of correlation and linear regression date\nback over a century.\n\n\nSimple Linear Regression\nSimple linear regression models the relationship between the magnitude of one\nvariable and that of a second — for example, as X increases, Y also increases. Or\nas X increases, Y decreases.1 Correlation is another way to measure how two\nvariables are related: see the section “Correlation”. The difference is that while\ncorrelation measures the strength of an association between two variables,\nregression quantifies the nature of the relationship.\nKEY TERMS FOR SIMPLE LINEAR REGRESSION\nResponse\nThe variable we are trying to predict.\nSynonyms\ndependent variable, Y-variable, target, outcome\nIndependent variable\nThe variable used to predict the response.\nSynonyms\nindependent variable, X-variable, feature, attribute\nRecord\nThe vector of predictor and outcome values for a specific individual or case.\nSynonyms\nrow, case, instance, example\nIntercept\nThe intercept of the regression line — that is, the predicted value when \n.\nSynonyms\n, \nRegression coefficient\nThe slope of the regression line.\nSynonyms\nslope, \n, \n, parameter estimates, weights\nFitted values\nThe estimates \n obtained from the regression line.\n\n\nSynonyms\npredicted values\nResiduals\nThe difference between the observed values and the fitted values.\nSynonyms\nerrors\nLeast squares\nThe method of fitting a regression by minimizing the sum of squared residuals.\nSynonyms\nordinary least squares\n",
      "page_number": 154
    },
    {
      "number": 4,
      "title": "Regression and Prediction",
      "start_page": 231,
      "end_page": 309,
      "detection_method": "regex_chapter_title",
      "content": "The Regression Equation\nSimple linear regression estimates exactly how much Y will change when X\nchanges by a certain amount. With the correlation coefficient, the variables X and\nY are interchangable. With regression, we are trying to predict the Y variable from\nX using a linear relationship (i.e., a line):\nWe read this as “Y equals b1 times X, plus a constant b0.” The symbol \n is\nknown as the intercept (or constant), and the symbol \n as the slope for X. Both\nappear in R output as coefficients, though in general use the term coefficient is\noften reserved for \n. The Y variable is known as the response or dependent\nvariable since it depends on X. The X variable is known as the predictor or\nindependent variable. The machine learning community tends to use other terms,\ncalling Y the target and X a feature vector.\nConsider the scatterplot in Figure 4-1 displaying the number of years a worker\nwas exposed to cotton dust (Exposure) versus a measure of lung capacity (PEFR\nor “peak expiratory flow rate”). How is PEFR related to Exposure? It’s hard to\ntell just based on the picture.\n\n\nFigure 4-1. Cotton exposure versus lung capacity\nSimple linear regression tries to find the “best” line to predict the response PEFR\nas a function of the predictor variable Exposure.\nThe lm function in R can be used to fit a linear regression.\n\n\nmodel <- lm(PEFR ~ Exposure, data=lung)\nlm standards for linear model and the ~ symbol denotes that PEFR is predicted by\nExposure.\nPrinting the model object produces the following output:\nCall:\nlm(formula = PEFR ~ Exposure, data = lung)\nCoefficients:\n(Intercept)     Exposure\n    424.583       -4.185\nThe intercept, or \n, is 424.583 and can be interpreted as the predicted PEFR for a\nworker with zero years exposure. The regression coefficient, or \n, can be\ninterpreted as follows: for each additional year that a worker is exposed to cotton\ndust, the worker’s PEFR measurement is reduced by –4.185.\nThe regression line from this model is displayed in Figure 4-2.\nFigure 4-2. Slope and intercept for the regression fit to the lung data\n\n\nFitted Values and Residuals\nImportant concepts in regression analysis are the fitted values and residuals. In\ngeneral, the data doesn’t fall exactly on a line, so the regression equation should\ninclude an explicit error term :\nThe fitted values, also referred to as the predicted values, are typically denoted\nby  (Y-hat). These are given by:\nThe notation  and  indicates that the coefficients are estimated versus known.\n\n\nHAT NOTATION: ESTIMATES VERSUS KNOWN VALUES\nThe “hat” notation is used to differentiate between estimates and known values. So the symbol \n(“b-hat”) is an estimate of the unknown parameter \n. Why do statisticians differentiate between\nthe estimate and the true value? The estimate has uncertainty, whereas the true value is fixed.2\nWe compute the residuals  by subtracting the predicted values from the original\ndata:\nIn R, we can obtain the fitted values and residuals using the functions predict\nand residuals:\nfitted <- predict(model)\nresid <- residuals(model)\nFigure 4-3 illustrates the residuals from the regression line fit to the lung data. The\nresiduals are the length of the vertical dashed lines from the data to the line.\n\n\nFigure 4-3. Residuals from a regression line (note the different y-axis scale from Figure 4-2, hence the\napparently different slope)\n\n\nLeast Squares\nHow is the model fit to the data? When there is a clear relationship, you could\nimagine fitting the line by hand. In practice, the regression line is the estimate that\nminimizes the sum of squared residual values, also called the residual sum of\nsquares or RSS:\nThe estimates  and  are the values that minimize RSS.\nThe method of minimizing the sum of the squared residuals is termed least\nsquares regression, or ordinary least squares (OLS) regression. It is often\nattributed to Carl Friedrich Gauss, the German mathmetician, but was first\npublished by the French mathmetician Adrien-Marie Legendre in 1805. Least\nsquares regression leads to a simple formula to compute the coefficients:\n\n\nHistorically, computational convenience is one reason for the widespread use of\nleast squares in regression. With the advent of big data, computational speed is\nstill an important factor. Least squares, like the mean (see “Median and Robust\nEstimates”), are sensitive to outliers, although this tends to be a signicant problem\nonly in small or moderate-sized problems. See “Outliers” for a discussion of\noutliers in regression.\n\n\nREGRESSION TERMINOLOGY\nWhen analysts and researchers use the term regression by itself, they are typically referring to\nlinear regression; the focus is usually on developing a linear model to explain the relationship\nbetween predictor variables and a numeric outcome variable. In its formal statistical sense,\nregression also includes nonlinear models that yield a functional relationship between predictors\nand outcome variables. In the machine learning community, the term is also occasionally used\nloosely to refer to the use of any predictive model that produces a predicted numeric outcome\n(standing in distinction from classification methods that predict a binary or categorical outcome).\n\n\nPrediction versus Explanation (Profiling)\nHistorically, a primary use of regression was to illuminate a supposed linear\nrelationship between predictor variables and an outcome variable. The goal has\nbeen to understand a relationship and explain it using the data that the regression\nwas fit to. In this case, the primary focus is on the estimated slope of the\nregression equation, . Economists want to know the relationship between\nconsumer spending and GDP growth. Public health officials might want to\nunderstand whether a public information campaign is effective in promoting safe\nsex practices. In such cases, the focus is not on predicting individual cases, but\nrather on understanding the overall relationship.\nWith the advent of big data, regression is widely used to form a model to predict\nindividual outcomes for new data, rather than explain data in hand (i.e., a\npredictive model). In this instance, the main items of interest are the fitted values \n. In marketing, regression can be used to predict the change in revenue in\nresponse to the size of an ad campaign. Universities use regression to predict\nstudents’ GPA based on their SAT scores.\nA regression model that fits the data well is set up such that changes in X lead to\nchanges in Y. However, by itself, the regression equation does not prove the\ndirection of causation. Conclusions about causation must come from a broader\ncontext of understanding about the relationship. For example, a regression\nequation might show a definite relationship between number of clicks on a web ad\nand number of conversions. It is our knowledge of the marketing process, not the\nregression equation, that leads us to the conclusion that clicks on the ad lead to\nsales, and not vice versa.\nKEY IDEAS\nThe regression equation models the relationship between a response variable Y and a predictor\nvariable X as a line.\nA regression model yields fitted values and residuals — predictions of the response and the errors\nof the predictions.\nRegression models are typically fit by the method of least squares.\nRegression is used both for prediction and explanation.\n\n\nFurther Reading\nFor an in-depth treatment of prediction versus explanation, see Galit Shmueli’s\narticle “To Explain or to Predict”.\n\n\nMultiple Linear Regression\nWhen there are multiple predictors, the equation is simply extended to\naccommodate them:\nInstead of a line, we now have a linear model — the relationship between each\ncoefficient and its variable (feature) is linear.\nKEY TERMS FOR MULTIPLE LINEAR REGRESSION\nRoot mean squared error\nThe square root of the average squared error of the regression (this is the most widely used metric\nto compare regression models).\nSynonyms\nRMSE\nResidual standard error\nThe same as the root mean squared error, but adjusted for degrees of freedom.\nSynonyms\nRSE\nR-squared\nThe proportion of variance explained by the model, from 0 to 1.\nSynonyms\ncoefficient of determination, \nt-statistic\nThe coefficient for a predictor, divided by the standard error of the coefficient, giving a metric to\ncompare the importance of variables in the model.\nWeighted regression\nRegression with the records having different weights.\nAll of the other concepts in simple linear regression, such as fitting by least\nsquares and the definition of fitted values and residuals, extend to the multiple\nlinear regression setting. For example, the fitted values are given by:\n\n\nExample: King County Housing Data\nAn example of using regression is in estimating the value of houses. County\nassessors must estimate the value of a house for the purposes of assessing taxes.\nReal estate consumers and professionals consult popular websites such as Zillow\nto ascertain a fair price. Here are a few rows of housing data from King County\n(Seattle), Washington, from the house data.frame:\nhead(house[, c(\"AdjSalePrice\", \"SqFtTotLiving\", \"SqFtLot\", \"Bathrooms\",\n               \"Bedrooms\", \"BldgGrade\")])\nSource: local data frame [6 x 6]\n  AdjSalePrice SqFtTotLiving SqFtLot Bathrooms Bedrooms BldgGrade\n         (dbl)         (int)   (int)     (dbl)    (int)     (int)\n1       300805          2400    9373      3.00        6         7\n2      1076162          3764   20156      3.75        4        10\n3       761805          2060   26036      1.75        4         8\n4       442065          3200    8618      3.75        5         7\n5       297065          1720    8620      1.75        4         7\n6       411781           930    1012      1.50        2         8\nThe goal is to predict the sales price from the other variables. The lm handles the\nmultiple regression case simply by including more terms on the righthand side of\nthe equation; the argument na.action=na.omit causes the model to drop records\nthat have missing values:\nhouse_lm <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n                 Bedrooms + BldgGrade,\n               data=house, na.action=na.omit)\nPrinting house_lm object produces the following output:\nhouse_lm\nCall:\nlm(formula = AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n    Bedrooms + BldgGrade, data = house, na.action = na.omit)\nCoefficients:\n  (Intercept)  SqFtTotLiving        SqFtLot      Bathrooms\n   -5.219e+05      2.288e+02     -6.051e-02     -1.944e+04\n     Bedrooms      BldgGrade\n   -4.778e+04      1.061e+05\nThe interpretation of the coefficients is as with simple linear regression: the\n\n\npredicted value  changes by the coefficient \n for each unit change in \nassuming all the other variables, \n for \n, remain the same. For example,\nadding an extra finished square foot to a house increases the estimated value by\nroughly $229; adding 1,000 finished square feet implies the value will increase by\n$228,800.\n\n\nAssessing the Model\nThe most important performance metric from a data science perspective is root\nmean squared error, or RMSE. RMSE is the square root of the average squared\nerror in the predicted  values:\nThis measures the overall accuracy of the model, and is a basis for comparing it to\nother models (including models fit using machine learning techniques). Similar to\nRMSE is the residual standard error, or RSE. In this case we have p predictors,\nand the RSE is given by:\nThe only difference is that the denominator is the degrees of freedom, as opposed\nto number of records (see “Degrees of Freedom”). In practice, for linear\nregression, the difference between RMSE and RSE is very small, particularly for\nbig data applications.\nThe summary function in R computes RSE as well as other metrics for a\nregression model:\nsummary(house_lm)\nCall:\nlm(formula = AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n    Bedrooms + BldgGrade, data = house, na.action = na.omit)\n\n\nResiduals:\n     Min       1Q   Median       3Q      Max\n-1199508  -118879   -20982    87414  9472982\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -5.219e+05  1.565e+04 -33.349  < 2e-16 ***\nSqFtTotLiving  2.288e+02  3.898e+00  58.699  < 2e-16 ***\nSqFtLot       -6.051e-02  6.118e-02  -0.989    0.323\nBathrooms     -1.944e+04  3.625e+03  -5.362 8.32e-08 ***\nBedrooms      -4.778e+04  2.489e+03 -19.194  < 2e-16 ***\nBldgGrade      1.061e+05  2.396e+03  44.287  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 261200 on 22683 degrees of freedom\nMultiple R-squared:  0.5407, \nAdjusted R-squared:  0.5406\nF-statistic:  5340 on 5 and 22683 DF,  p-value: < 2.2e-16\nAnother useful metric that you will see in software output is the coefficient of\ndetermination, also called the R-squared statistic or \n. R-squared ranges from\n0 to 1 and measures the proportion of variation in the data that is accounted for in\nthe model. It is useful mainly in explanatory uses of regression where you want to\nassess how well the model fits the data. The formula for \n is:\nThe denominator is proportional to the variance of Y. The output from R also\nreports an adjusted R-squared, which adjusts for the degrees of freedom; seldom\nis this significantly different in multiple regression.\nAlong with the estimated coefficients, R reports the standard error of the\ncoefficients (SE) and a t-statistic:\n\n\nThe t-statistic — and its mirror image, the p-value — measures the extent to\nwhich a coefficient is “statistically significant” — that is, outside the range of\nwhat a random chance arrangement of predictor and target variable might produce.\nThe higher the t-statistic (and the lower the p-value), the more significant the\npredictor. Since parsimony is a valuable model feature, it is useful to have a tool\nlike this to guide choice of variables to include as predictors (see “Model\nSelection and Stepwise Regression”).\nWARNING\nIn addition to the t-statistic, R and other packages will often report a p-value (Pr(>|t|) in the R\noutput) and F-statistic. Data scientists do not generally get too involved with the interpretation of\nthese statistics, nor with the issue of statistical significance. Data scientists primarily focus on the\nt-statistic as a useful guide for whether to include a predictor in a model or not. High t-statistics\n(which go with p-values near 0) indicate a predictor should be retained in a model, while very low\nt-statistics indicate a predictor could be dropped. See “P-Value” for more discussion.\n\n\nCross-Validation\nClassic statistical regression metrics (R2, F-statistics, and p-values) are all “in-\nsample” metrics — they are applied to the same data that was used to fit the\nmodel. Intuitively, you can see that it would make a lot of sense to set aside some\nof the original data, not use it to fit the model, and then apply the model to the set-\naside (holdout) data to see how well it does. Normally, you would use a majority\nof the data to fit the model, and use a smaller portion to test the model.\nThis idea of “out-of-sample” validation is not new, but it did not really take hold\nuntil larger data sets became more prevalent; with a small data set, analysts\ntypically want to use all the data and fit the best possible model.\nUsing a holdout sample, though, leaves you subject to some uncertainty that arises\nsimply from variability in the small holdout sample. How different would the\nassessment be if you selected a different holdout sample?\nCross-validation extends the idea of a holdout sample to multiple sequential\nholdout samples. The algorithm for basic k-fold cross-validation is as follows:\n1. Set aside 1/k of the data as a holdout sample.\n2. Train the model on the remaining data.\n3. Apply (score) the model to the 1/k holdout, and record needed model\nassessment metrics.\n4. Restore the first 1/k of the data, and set aside the next 1/k (excluding any\nrecords that got picked the first time).\n5. Repeat steps 2 and 3.\n6. Repeat until each record has been used in the holdout portion.\n7. Average or otherwise combine the model assessment metrics.\nThe division of the data into the training sample and the holdout sample is also\ncalled a fold.\n\n\nModel Selection and Stepwise Regression\nIn some problems, many variables could be used as predictors in a regression.\nFor example, to predict house value, additional variables such as the basement\nsize or year built could be used. In R, these are easy to add to the regression\nequation:\nhouse_full <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n                 Bedrooms + BldgGrade + PropertyType + NbrLivingUnits +\n                 SqFtFinBasement + YrBuilt + YrRenovated +\n                 NewConstruction,\n               data=house, na.action=na.omit)\nAdding more variables, however, does not necessarily mean we have a better\nmodel. Statisticians use the principle of Occam’s razor to guide the choice of a\nmodel: all things being equal, a simpler model should be used in preference to a\nmore complicated model.\nIncluding additional variables always reduces RMSE and increases \n. Hence,\nthese are not appropriate to help guide the model choice. In the 1970s, Hirotugu\nAkaike, the eminent Japanese statistician, deveoped a metric called AIC (Akaike’s\nInformation Criteria) that penalizes adding terms to a model. In the case of\nregression, AIC has the form:\nAIC = 2P + n log(RSS/n)\nwhere p is the number of variables and n is the number of records. The goal is to\nfind the model that minimizes AIC; models with k more extra variables are\npenalized by 2k.\n\n\nAIC, BIC AND MALLOWS CP\nThe formula for AIC may seem a bit mysterious, but in fact it is based on asymptotic results in\ninformation theory. There are several variants to AIC:\nAICc: a version of AIC corrected for small sample sizes.\nBIC or Bayesian information criteria: similar to AIC with a stronger penalty for including\nadditional variables to the model.\nMallows Cp: A variant of AIC developed by Colin Mallows.\nData scientists generally do not need to worry about the differences among these in-sample\nmetrics or the underlying theory behind them.\nHow do we find the model that minimizes AIC? One approach is to search through\nall possible models, called all subset regression. This is computationally\nexpensive and is not feasible for problems with large data and many variables. An\nattractive alternative is to use stepwise regression, which successively adds and\ndrops predictors to find a model that lowers AIC. The MASS package by Venebles\nand Ripley offers a stepwise regression function called stepAIC:\nlibrary(MASS)\nstep <- stepAIC(house_full, direction=\"both\")\nstep\nCall:\nlm(formula = AdjSalePrice ~ SqFtTotLiving + Bathrooms + Bedrooms +\n    BldgGrade + PropertyType + SqFtFinBasement + YrBuilt, data = house0,\n    na.action = na.omit)\nCoefficients:\n              (Intercept)              SqFtTotLiving\n               6227632.22                     186.50\n                Bathrooms                   Bedrooms\n                 44721.72                  -49807.18\n                BldgGrade  PropertyTypeSingle Family\n                139179.23                   23328.69\n    PropertyTypeTownhouse            SqFtFinBasement\n                 92216.25                       9.04\n                  YrBuilt\n                 -3592.47\nThe function chose a model in which several variables were dropped from\nhouse_full: SqFtLot, NbrLivingUnits, YrRenovated, and NewConstruction.\nSimpler yet are forward selection and backward selection. In forward selection,\n\n\nyou start with no predictors and add them one-by-one, at each step adding the\npredictor that has the largest contribution to \n, stopping when the contribution is\nno longer statistically significant. In backward selection, or backward\nelimination, you start with the full model and take away predictors that are not\nstatistically significant until you are left with a model in which all predictors are\nstatistically significant.\nPenalized regression is similar in spirit to AIC. Instead of explicitly searching\nthrough a discrete set of models, the model-fitting equation incorporates a\nconstraint that penalizes the model for too many variables (parameters). Rather\nthan eliminating predictor variables entirely — as with stepwise, forward, and\nbackward selection — penalized regression applies the penalty by reducing\ncoefficients, in some cases to near zero. Common penalized regression methods\nare ridge regression and lasso regression.\nStepwise regression and all subset regression are in-sample methods to assess\nand tune models. This means the model selection is possibly subject to overfitting\nand may not perform as well when applied to new data. One common approach to\navoid this is to use cross-validation to validate the models. In linear regression,\noverfitting is typically not a major issue, due to the simple (linear) global\nstructure imposed on the data. For more sophisticated types of models,\nparticularly iterative procedures that respond to local data structure, cross-\nvalidation is a very important tool; see “Cross-Validation” for details.\n\n\nWeighted Regression\nWeighted regression is used by statisticians for a variety of purposes; in\nparticular, it is important for analysis of complex surveys. Data scientists may find\nweighted regression useful in two cases:\nInverse-variance weighting when different observations have been measured\nwith different precision.\nAnalysis of data in an aggregated form such that the weight variable encodes\nhow many original observations each row in the aggregated data represents.\nFor example, with the housing data, older sales are less reliable than more recent\nsales. Using the DocumentDate to determine the year of the sale, we can compute\na Weight as the number of years since 2005 (the beginning of the data).\nlibrary(lubridate)\nhouse$Year = year(house$DocumentDate)\nhouse$Weight = house$Year - 2005\nWe can compute a weighted regression with the lm function using the weight\nargument.\nhouse_wt <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n                 Bedrooms + BldgGrade,\n               data=house, weight=Weight)\nround(cbind(house_lm=house_lm$coefficients,\n            house_wt=house_wt$coefficients), digits=3)\n                   house_lm    house_wt\n(Intercept)   -521924.722 -584265.244\nSqFtTotLiving     228.832     245.017\nSqFtLot            -0.061      -0.292\nBathrooms      -19438.099  -26079.171\nBedrooms       -47781.153  -53625.404\nBldgGrade      106117.210  115259.026\nThe coefficents in the weighted regression are slightly different from the original\nregression.\nKEY IDEAS\nMultiple linear regression models the relationship between a response variable Y and multiple\npredictor variables \n.\n\n\nThe most important metrics to evaluate a model are root mean squared error (RMSE) and R-\nsquared (R2).\nThe standard error of the coefficients can be used to measure the reliability of a variable’s\ncontribution to a model.\nStepwise regression is a way to automatically determine which variables should be included in the\nmodel.\nWeighted regression is used to give certain records more or less weight in fitting the equation.\n\n\nPrediction Using Regression\nThe primary purpose of regression in data science is prediction. This is useful to\nkeep in mind, since regression, being an old and established statistical method,\ncomes with baggage that is more relevant to its traditional explanatory modeling\nrole than to prediction.\nKEY TERMS FOR PREDICTION USING REGRESSION\nPrediction interval\nAn uncertainty interval around an individual predicted value.\nExtrapolation\nExtension of a model beyond the range of the data used to fit it.\n\n\nThe Dangers of Extrapolation\nRegression models should not be used to extrapolate beyond the range of the data.\nThe model is valid only for predictor values for which the data has sufficient\nvalues (even in the case that sufficient data is available, there could be other\nproblems: see “Testing the Assumptions: Regression Diagnostics”). As an\nextreme case, suppose model_lm is used to predict the value of a 5,000-square-\nfoot empty lot. In such a case, all the predictors related to the building would have\na value of 0 and the regression equation would yield an absurd prediction of –\n521,900 + 5,000 × –.0605 = –$522,202. Why did this happen? The data contains\nonly parcels with buildings — there are no records corresponding to vacant land.\nConsequently, the model has no information to tell it how to predict the sales price\nfor vacant land.\n\n\nConfidence and Prediction Intervals\nMuch of statistics involves understanding and measuring variability (uncertainty).\nThe t-statistics and p-values reported in regression output deal with this in a\nformal way, which is sometimes useful for variable selection (see “Assessing the\nModel”). More useful metrics are confidence intervals, which are uncertainty\nintervals placed around regression coefficients and predictions. An easy way to\nunderstand this is via the bootstrap (see “The Bootstrap” for more details about\nthe general bootstrap procedure). The most common regression confidence\nintervals encountered in software output are those for regression parameters\n(coefficients). Here is a bootstrap algorithm for generating confidence intervals\nfor regression parameters (coefficients) for a data set with P predictors and n\nrecords (rows):\n1. Consider each row (including outcome variable) as a single “ticket” and\nplace all the n tickets in a box.\n2. Draw a ticket at random, record the values, and replace it in the box.\n3. Repeat step 2 n times; you now have one bootstrap resample.\n4. Fit a regression to the bootstrap sample, and record the estimated\ncoefficients.\n5. Repeat steps 2 through 4, say, 1,000 times.\n6. You now have 1,000 bootstrap values for each coefficient; find the\nappropriate percentiles for each one (e.g., 5th and 95th for a 90%\nconfidence interval).\nYou can use the Boot function in R to generate actual bootstrap confidence\nintervals for the coefficients, or you can simply use the formula-based intervals\nthat are a routine R output. The conceptual meaning and interpretation are the\nsame, and not of central importance to data scientists, because they concern the\nregression coefficients. Of greater interest to data scientists are intervals around\npredicted y values ( ). The uncertainty around  comes from two sources:\nUncertainty about what the relevant predictor variables and their coefficients\n\n\nare (see the preceding bootstrap algorithm)\nAdditional error inherent in individual data points\nThe individual data point error can be thought of as follows: even if we knew for\ncertain what the regression equation was (e.g., if we had a huge number of records\nto fit it), the actual outcome values for a given set of predictor values will vary.\nFor example, several houses — each with 8 rooms, a 6,500 square foot lot, 3\nbathrooms, and a basement — might have different values. We can model this\nindividual error with the residuals from the fitted values. The bootstrap algorithm\nfor modeling both the regression model error and the individual data point error\nwould look as follows:\n1. Take a bootstrap sample from the data (spelled out in greater detail\nearlier).\n2. Fit the regression, and predict the new value.\n3. Take a single residual at random from the original regression fit, add it to\nthe predicted value, and record the result.\n4. Repeat steps 1 through 3, say, 1,000 times.\n5. Find the 2.5th and the 97.5th percentiles of the results.\n\n\nPREDICTION INTERVAL OR CONFIDENCE INTERVAL?\nA prediction interval pertains to uncertainty around a single value, while a confidence interval\npertains to a mean or other statistic calculated from multiple values. Thus, a prediction interval\nwill typically be much wider than a confidence interval for the same value. We model this\nindividual value error in the bootstrap model by selecting an individual residual to tack on to the\npredicted value. Which should you use? That depends on the context and the purpose of the\nanalysis, but, in general, data scientists are interested in specific individual predictions, so a\nprediction interval would be more appropriate. Using a confidence interval when you should be\nusing a prediction interval will greatly underestimate the uncertainty in a given predicted value.\nKEY IDEAS\nExtrapolation beyond the range of the data can lead to error.\nConfidence intervals quantify uncertainty around regression coefficients.\nPrediction intervals quantify uncertainty in individual predictions.\nMost software, R included, will produce prediction and confidence intervals in default or specified\noutput, using formulas.\nThe bootstrap can also be used; the interpretation and idea are the same.\n\n\nFactor Variables in Regression\nFactor variables, also termed categorical variables, take on a limited number of\ndiscrete values. For example, a loan purpose can be “debt consolidation,”\n“wedding,” “car,” and so on. The binary (yes/no) variable, also called an\nindicator variable, is a special case of a factor variable. Regression requires\nnumerical inputs, so factor variables need to be recoded to use in the model. The\nmost common approach is to convert a variable into a set of binary dummy\nvariables.\nKEY TERMS FOR FACTOR VARIABLES\nDummy variables\nBinary 0–1 variables derived by recoding factor data for use in regression and other models.\nReference coding\nThe most common type of coding used by statisticians, in which one level of a factor is used as a\nreference and other factors are compared to that level.\nSynonyms\ntreatment coding\nOne hot encoder\nA common type of coding used in the machine learning community in which all factors levels are\nretained. While useful for certain machine learning algorithms, this approach is not appropriate for\nmultiple linear regression.\nDeviation coding\nA type of coding that compares each level against the overall mean as opposed to the reference\nlevel.\nSynonyms\nsum contrasts\n\n\nDummy Variables Representation\nIn the King County housing data, there is a factor variable for the property type; a\nsmall subset of six records is shown below.\nhead(house[, 'PropertyType'])\nSource: local data frame [6 x 1]\n   PropertyType\n         (fctr)\n1     Multiplex\n2 Single Family\n3 Single Family\n4 Single Family\n5 Single Family\n6     Townhouse\nThere are three possible values: Multiplex, Single Family, and Townhouse.\nTo use this factor variable, we need to convert it to a set of binary variables. We\ndo this by creating a binary variable for each possible value of the factor variable.\nTo do this in R, we use the model.matrix function:3\nprop_type_dummies <- model.matrix(~PropertyType -1, data=house)\nhead(prop_type_dummies)\n  PropertyTypeMultiplex PropertyTypeSingle Family PropertyTypeTownhouse\n1                     1                         0                     0\n2                     0                         1                     0\n3                     0                         1                     0\n4                     0                         1                     0\n5                     0                         1                     0\n6                     0                         0                     1\nThe function model.matrix converts a data frame into a matrix suitable to a\nlinear model. The factor variable PropertyType, which has three distinct levels,\nis represented as a matrix with three columns. In the machine learning community,\nthis representation is referred to as one hot encoding (see “One Hot Encoder”). In\ncertain machine learning algorithms, such as nearest neighbors and tree models,\none hot encoding is the standard way to represent factor variables (for example,\nsee “Tree Models”).\nIn the regression setting, a factor variable with P distinct levels is usually\nrepresented by a matrix with only P – 1 columns. This is because a regression\nmodel typically includes an intercept term. With an intercept, once you have\ndefined the values for P – 1 binaries, the value for the Pth is known and could be\n\n\nconsidered redundant. Adding the Pth column will cause a multicollinearity error\n(see “Multicollinearity”).\nThe default representation in R is to use the first factor level as a reference and\ninterpret the remaining levels relative to that factor.\nlm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n+      Bedrooms +  BldgGrade + PropertyType, data=house)\nCall:\nlm(formula = AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n    Bedrooms + BldgGrade + PropertyType, data = house)\nCoefficients:\n              (Intercept)              SqFtTotLiving\n               -4.469e+05                  2.234e+02\n                  SqFtLot                  Bathrooms\n               -7.041e-02                 -1.597e+04\n                 Bedrooms                  BldgGrade\n               -5.090e+04                  1.094e+05\nPropertyTypeSingle Family      PropertyTypeTownhouse\n               -8.469e+04                 -1.151e+05\nThe output from the R regression shows two coefficients corresponding to\nPropertyType: PropertyTypeSingle Family and PropertyTypeTownhouse.\nThere is no coefficient of Multiplex since it is implicitly defined when\nPropertyTypeSingle Family == 0 and PropertyTypeTownhouse == 0. The\ncoefficients are interpreted as relative to Multiplex, so a home that is Single\nFamily is worth almost $85,000 less, and a home that is Townhouse is worth over\n$150,000 less.4\n\n\nDIFFERENT FACTOR CODINGS\nThere are several different ways to encode factor variables, known as contrast coding systems.\nFor example, deviation coding, also know as sum contrasts, compares each level against the\noverall mean. Another contrast is polynomial coding, which is appropriate for ordered factors;\nsee the section “Ordered Factor Variables”. With the exception of ordered factors, data scientists\nwill generally not encounter any type of coding besides reference coding or one hot encoder.\n\n\nFactor Variables with Many Levels\nSome factor variables can produce a huge number of binary dummies — zip codes\nare a factor variable and there are 43,000 zip codes in the US. In such cases, it is\nuseful to explore the data, and the relationships between predictor variables and\nthe outcome, to determine whether useful information is contained in the\ncategories. If so, you must further decide whether it is useful to retain all factors,\nor whether the levels should be consolidated.\nIn King County, there are 82 zip codes with a house sale:\ntable(house$ZipCode)\n 9800 89118 98001 98002 98003 98004 98005 98006 98007 98008 98010 98011\n    1     1   358   180   241   293   133   460   112   291    56   163\n98014 98019 98022 98023 98024 98027 98028 98029 98030 98031 98032 98033\n   85   242   188   455    31   366   252   475   263   308   121   517\n98034 98038 98039 98040 98042 98043 98045 98047 98050 98051 98052 98053\n  575   788    47   244   641     1   222    48     7    32   614   499\n98055 98056 98057 98058 98059 98065 98068 98070 98072 98074 98075 98077\n  332   402     4   420   513   430     1    89   245   502   388   204\n98092 98102 98103 98105 98106 98107 98108 98109 98112 98113 98115 98116\n  289   106   671   313   361   296   155   149   357     1   620   364\n98117 98118 98119 98122 98125 98126 98133 98136 98144 98146 98148 98155\n  619   492   260   380   409   473   465   310   332   287    40   358\n98166 98168 98177 98178 98188 98198 98199 98224 98288 98354\n  193   332   216   266   101   225   393     3     4     9\nZipCode is an important variable, since it is a proxy for the effect of location on\nthe value of a house. Including all levels requires 81 coefficients corresponding to\n81 degrees of freedom. The original model house_lm has only 5 degress of\nfreedom; see “Assessing the Model”. Moreover, several zip codes have only one\nsale. In some problems, you can consolidate a zip code using the first two or three\ndigits, corresponding to a submetropolitan geographic region. For King County,\nalmost all of the sales occur in 980xx or 981xx, so this doesn’t help.\nAn alternative approach is to group the zip codes according to another variable,\nsuch as sale price. Even better is to form zip code groups using the residuals from\nan initial model. The following dplyr code consolidates the 82 zip codes into\nfive groups based on the median of the residual from the house_lm regression:\nzip_groups <- house %>%\n  mutate(resid = residuals(house_lm)) %>%\n  group_by(ZipCode) %>%\n  summarize(med_resid = median(resid),\n\n\n            cnt = n()) %>%\n  arrange(med_resid) %>%\n  mutate(cum_cnt = cumsum(cnt),\n         ZipGroup = ntile(cum_cnt, 5))\nhouse <- house %>%\n  left_join(select(zip_groups, ZipCode, ZipGroup), by='ZipCode')\nThe median residual is computed for each zip and the ntile function is used to\nsplit the zip codes, sorted by the median, into five groups. See “Confounding\nVariables” for an example of how this is used as a term in a regression improving\nupon the original fit.\nThe concept of using the residuals to help guide the regression fitting is a\nfundamental step in the modeling process; see “Testing the Assumptions:\nRegression Diagnostics”.\n\n\nOrdered Factor Variables\nSome factor variables reflect levels of a factor; these are termed ordered factor\nvariables or ordered categorical variables. For example, the loan grade could be\nA, B, C, and so on — each grade carries more risk than the prior grade. Ordered\nfactor variables can typically be converted to numerical values and used as is. For\nexample, the variable BldgGrade is an ordered factor variable. Several of the\ntypes of grades are shown in Table 4-1. While the grades have specific meaning,\nthe numeric value is ordered from low to high, corresponding to higher-grade\nhomes. With the regression model house_lm, fit in “Multiple Linear Regression”,\nBldgGrade was treated as a numeric variable.\nTable 4-1. A\ntypical data\nformat\nValue Description\n1\nCabin\n2\nSubstandard\n5\nFair\n10\nVery good\n12\nLuxury\n13\nMansion\nTreating ordered factors as a numeric variable preserves the information\ncontained in the ordering that would be lost if it were converted to a factor.\nKEY IDEAS\nFactor variables need to be converted into numeric variables for use in a regression.\nThe most common method to encode a factor variable with P distinct values is to represent them\nusing P-1 dummy variables.\nA factor variable with many levels, even in very big data sets, may need to be consolidated into a\nvariable with fewer levels.\n\n\nSome factors have levels that are ordered and can be represented as a single numeric variable.\n\n\nInterpreting the Regression Equation\nIn data science, the most important use of regression is to predict some dependent\n(outcome) variable. In some cases, however, gaining insight from the equation\nitself to understand the nature of the relationship between the predictors and the\noutcome can be of value. This section provides guidance on examining the\nregression equation and interpreting it.\nKEY TERMS FOR INTERPRETING THE REGRESSION EQUATION\nCorrelated variables\nWhen the predictor variables are highly correlated, it is difficult to interpret the individual\ncoefficients.\nMulticollinearity\nWhen the predictor variables have perfect, or near-perfect, correlation, the regression can be\nunstable or impossible to compute.\nSynonyms\ncollinearity\nConfounding variables\nAn important predictor that, when omitted, leads to spurious relationships in a regression equation.\nMain effects\nThe relationship between a predictor and the outcome variable, independent from other variables.\nInteractions\nAn interdependent relationship between two or more predictors and the response.\n\n\nCorrelated Predictors\nIn multiple regression, the predictor variables are often correlated with each\nother. As an example, examine the regression coefficients for the model step_lm,\nfit in “Model Selection and Stepwise Regression”:\nstep_lm$coefficients\n              (Intercept)             SqFtTotLiving\n             6.227632e+06              1.865012e+02\n                Bathrooms                  Bedrooms\n             4.472172e+04             -4.980718e+04\n                BldgGrade PropertyTypeSingle Family\n             1.391792e+05              2.332869e+04\n    PropertyTypeTownhouse           SqFtFinBasement\n             9.221625e+04              9.039911e+00\n                  YrBuilt\n            -3.592468e+03\nThe coefficient for Bedrooms is negative! This implies that adding a bedroom to a\nhouse will reduce its value. How can this be? This is because the predictor\nvariables are correlated: larger houses tend to have more bedrooms, and it is the\nsize that drives house value, not the number of bedrooms. Consider two homes of\nthe exact same size: it is reasonable to expect that a home with more, but smaller,\nbedrooms would be considered less desirable.\nHaving correlated predictors can make it difficult to interpret the sign and value of\nregression coefficients (and can inflate the standard error of the estimates). The\nvariables for bedrooms, house size, and number of bathrooms are all correlated.\nThis is illustrated by the following example, which fits another regression\nremoving the variables SqFtTotLiving, SqFtFinBasement, and Bathrooms\nfrom the equation:\nupdate(step_lm, . ~ . -SqFtTotLiving - SqFtFinBasement - Bathrooms)\nCall:\nlm(formula = AdjSalePrice ~ Bedrooms + BldgGrade + PropertyType +\n    YrBuilt, data = house0, na.action = na.omit)\nCoefficients:\n              (Intercept)                   Bedrooms\n                  4834680                      27657\n                BldgGrade  PropertyTypeSingle Family\n                   245709                     -17604\n    PropertyTypeTownhouse                    YrBuilt\n                   -47477                      -3161\n\n\nThe update function can be used to add or remove variables from a model. Now\nthe coefficient for bedrooms is positive — in line with what we would expect\n(though it is really acting as a proxy for house size, now that those variables have\nbeen removed).\nCorrelated variables are only one issue with interpreting regression coefficients.\nIn house_lm, there is no variable to account for the location of the home, and the\nmodel is mixing together very different types of regions. Location may be a\nconfounding variable; see “Confounding Variables” for further discussion.\n\n\nMulticollinearity\nAn extreme case of correlated variables produces multicollinearity — a condition\nin which there is redundance among the predictor variables. Perfect\nmulticollinearity occurs when one predictor variable can be expressed as a linear\ncombination of others. Multicollinearity occurs when:\nA variable is included multiple times by error.\nP dummies, instead of P – 1 dummies, are created from a factor variable (see\n“Factor Variables in Regression”).\nTwo variables are nearly perfectly correlated with one another.\nMulticollinearity in regression must be addressed — variables should be removed\nuntil the multicollinearity is gone. A regression does not have a well-defined\nsolution in the presence of perfect multicollinearity. Many software packages,\nincluding R, automatically handle certain types of multicolliearity. For example, if\nSqFtTotLiving is included twice in the regression of the house data, the results\nare the same as for the house_lm model. In the case of nonperfect\nmulticollinearity, the software may obtain a solution but the results may be\nunstable.\nNOTE\nMulticollinearity is not such a problem for nonregression methods like trees, clustering, and\nnearest-neighbors, and in such methods it may be advisable to retain P dummies (instead of P –\n1). That said, even in those methods, nonredundancy in predictor variables is still a virtue.\n\n\nConfounding Variables\nWith correlated variables, the problem is one of commission: including different\nvariables that have a similar predictive relationship with the response. With\nconfounding variables, the problem is one of omission: an important variable is\nnot included in the regression equation. Naive interpretation of the equation\ncoefficients can lead to invalid conclusions.\nTake, for example, the King County regression equation house_lm from\n“Example: King County Housing Data”. The regression coefficients of SqFtLot,\nBathrooms, and Bedrooms are all negative. The original regression model does\nnot contain a variable to represent location — a very important predictor of house\nprice. To model location, include a variable ZipGroup that categorizes the zip\ncode into one of five groups, from least expensive (1) to most expensive (5).5\nlm(AdjSalePrice ~  SqFtTotLiving + SqFtLot +\n     Bathrooms + Bedrooms +\n     BldgGrade + PropertyType + ZipGroup,\n   data=house, na.action=na.omit)\nCoefficients:\n              (Intercept)              SqFtTotLiving\n               -6.709e+05                  2.112e+02\n                  SqFtLot                  Bathrooms\n                4.692e-01                  5.537e+03\n                 Bedrooms                  BldgGrade\n               -4.139e+04                  9.893e+04\nPropertyTypeSingle Family      PropertyTypeTownhouse\n                2.113e+04                 -7.741e+04\n                ZipGroup2                  ZipGroup3\n                5.169e+04                  1.142e+05\n                ZipGroup4                  ZipGroup5\n                1.783e+05                  3.391e+05\nZipGroup is clearly an important variable: a home in the most expensive zip code\ngroup is estimated to have a higher sales price by almost $340,000. The\ncoefficients of SqFtLot and Bathrooms are now positive and adding a bathroom\nincreases the sale price by $7,500.\nThe coefficient for Bedrooms is still negative. While this is unintuitive, this is a\nwell-known phenomenon in real estate. For homes of the same livable area and\nnumber of bathrooms, having more, and therefore smaller, bedrooms is associated\nwith less valuable homes.\n\n\nInteractions and Main Effects\nStatisticians like to distinguish between main effects, or independent variables,\nand the interactions between the main effects. Main effects are what are often\nreferred to as the predictor variables in the regression equation. An implicit\nassumption when only main effects are used in a model is that the relationship\nbetween a predictor variable and the response is independent of the other\npredictor variables. This is often not the case.\nFor example, the model fit to the King County Housing Data in “Confounding\nVariables” includes several variables as main effects, including ZipCode.\nLocation in real estate is everything, and it is natural to presume that the\nrelationship between, say, house size and the sale price depends on location. A\nbig house built in a low-rent district is not going to retain the same value as a big\nhouse built in an expensive area. You include interactions between variables in R\nusing the * operator. For the King County data, the following fits an interaction\nbetween SqFtTotLiving and ZipGroup:\nlm(AdjSalePrice ~  SqFtTotLiving*ZipGroup + SqFtLot +\n     Bathrooms + Bedrooms + BldgGrade + PropertyType,\n   data=house, na.action=na.omit)\n Coefficients:\n              (Intercept)              SqFtTotLiving\n               -4.919e+05                  1.176e+02\n                ZipGroup2                  ZipGroup3\n               -1.342e+04                  2.254e+04\n                ZipGroup4                  ZipGroup5\n                1.776e+04                 -1.555e+05\n                  SqFtLot                  Bathrooms\n                7.176e-01                 -5.130e+03\n                 Bedrooms                  BldgGrade\n               -4.181e+04                  1.053e+05\nPropertyTypeSingle Family      PropertyTypeTownhouse\n                1.603e+04                 -5.629e+04\n  SqFtTotLiving:ZipGroup2    SqFtTotLiving:ZipGroup3\n                3.165e+01                  3.893e+01\n  SqFtTotLiving:ZipGroup4    SqFtTotLiving:ZipGroup5\n                7.051e+01                  2.298e+02\nThe resulting model has four new terms: SqFtTotLiving:ZipGroup2,\nSqFtTotLiving:ZipGroup3, and so on.\nLocation and house size appear to have a strong interaction. For a home in the\nlowest ZipGroup, the slope is the same as the slope for the main effect\n\n\nSqFtTotLiving, which is $177 per square foot (this is because R uses reference\ncoding for factor variables; see “Factor Variables in Regression”). For a home in\nthe highest ZipGroup, the slope is the sum of the main effect plus\nSqFtTotLiving:ZipGroup5, or $177 + $230 = $447 per square foot. In other\nwords, adding a square foot in the most expensive zip code group boosts the\npredicted sale price by a factor of almost 2.7, compared to the boost in the least\nexpensive zip code group.\n\n\nMODEL SELECTION WITH INTERACTION TERMS\nIn problems involving many variables, it can be challenging to decide which interaction terms\nshould be included in the model. Several different approaches are commonly taken:\nIn some problems, prior knowledge and intuition can guide the choice of which interaction\nterms to include in the model.\nStepwise selection (see “Model Selection and Stepwise Regression”) can be used to sift\nthrough the various models.\nPenalized regression can automatically fit to a large set of possible interaction terms.\nPerhaps the most common approach is the use tree models, as well as their descendents,\nrandom forest and gradient boosted trees. This class of models automatically searches\nfor optimal interaction terms; see “Tree Models”.\nKEY IDEAS\nBecause of correlation between predictors, care must be taken in the interpretation of the\ncoefficients in multiple linear regression.\nMulticollinearity can cause numerical instability in fitting the regression equation.\nA confounding variable is an important predictor that is omitted from a model and can lead to a\nregression equation with spurious relationships.\nAn interaction term between two variables is needed if the relationship between the variables and\nthe response is interdependent.\n\n\nTesting the Assumptions: Regression Diagnostics\nIn explanatory modeling (i.e., in a research context), various steps, in addition to\nthe metrics mentioned previously (see “Assessing the Model”), are taken to assess\nhow well the model fits the data. Most are based on analysis of the residuals,\nwhich can test the assumptions underlying the model. These steps do not directly\naddress predictive accuracy, but they can provide useful insight in a predictive\nsetting.\nKEY TERMS FOR REGRESSION DIAGNOSTICS\nStandardized residuals\nResiduals divided by the standard error of the residuals.\nOutliers\nRecords (or outcome values) that are distant from the rest of the data (or the predicted outcome).\nInfluential value\nA value or record whose presence or absence makes a big difference in the regression equation.\nLeverage\nThe degree of influence that a single record has on a regression equation.\nSynonyms\nhat-value\nNon-normal residuals\nNon-normally distributed residuals can invalidate some technical requirements of regression, but\nare usually not a concern in data science.\nHeteroskedasticity\nWhen some ranges of the outcome experience residuals with higher variance (may indicate a\npredictor missing from the equation).\nPartial residual plots\nA diagnostic plot to illuminate the relationship between the outcome variable and a single predictor.\nSynonyms\nadded variables plot\n\n\nOutliers\nGenerally speaking, an extreme value, also called an outlier, is one that is distant\nfrom most of the other observations. Just as outliers need to be handled for\nestimates of location and variability (see “Estimates of Location” and “Estimates\nof Variability”), outliers can cause problems with regression models. In\nregression, an outlier is a record whose actual y value is distant from the\npredicted value. You can detect outliers by examining the standardized residual,\nwhich is the residual divided by the standard error of the residuals.\nThere is no statistical theory that separates outliers from nonoutliers. Rather, there\nare (arbitrary) rules of thumb for how distant from the bulk of the data an\nobservation needs to be in order to be called an outlier. For example, with the\nboxplot, outliers are those data points that are too far above or below the box\nboundaries (see “Percentiles and Boxplots”), where “too far” = “more than 1.5\ntimes the inter-quartile range.” In regression, the standardized residual is the\nmetric that is typically used to determine whether a record is classified as an\noutlier. Standardized residuals can be interpreted as “the number of standard\nerrors away from the regression line.”\nLet’s fit a regression to the King County house sales data for all sales in zip code\n98105:\nhouse_98105 <- house[house$ZipCode == 98105,]\nlm_98105 <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n                 Bedrooms + BldgGrade, data=house_98105)\nWe extract the standardized residuals using the rstandard function and obtain the\nindex of the smallest residual using the order function:\nsresid <- rstandard(lm_98105)\nidx <- order(sresid)\nsresid[idx[1]]\n    20431\n-4.326732\nThe biggest overestimate from the model is more than four standard errors above\nthe regression line, corresponding to an overestimate of $757,753. The original\ndata record corresponding to this outlier is as follows:\n\n\nhouse_98105[idx[1], c('AdjSalePrice', 'SqFtTotLiving', 'SqFtLot',\n              'Bathrooms', 'Bedrooms', 'BldgGrade')]\nAdjSalePrice SqFtTotLiving SqFtLot Bathrooms Bedrooms BldgGrade\n         (dbl)         (int)   (int)     (dbl)    (int)     (int)\n1       119748          2900    7276         3        6         7\nIn this case, it appears that there is something wrong with the record: a house of\nthat size typically sells for much more than $119,748 in that zip code. Figure 4-4\nshows an excerpt from the statuatory deed from this sale: it is clear that the sale\ninvolved only partial interest in the property. In this case, the outlier corresonds to\na sale that is anomalous and should not be included in the regression. Outliers\ncould also be the result of other problems, such as a “fat-finger” data entry or a\nmismatch of units (e.g., reporting a sale in thousands of dollars versus simply\ndollars).\nFigure 4-4. Statutory warrant of deed for the largest negative residual\nFor big data problems, outliers are generally not a problem in fitting the\nregression to be used in predicting new data. However, outliers are central to\nanomaly detection, where finding outliers is the whole point. The outlier could\nalso correspond to a case of fraud or an accidental action. In any case, detecting\noutliers can be a critical business need.\n\n\nInfluential Values\nA value whose absence would significantly change the regression equation is\ntermed an infuential observation. In regression, such a value need not be\nassociated with a large residual. As an example, consider the regression lines in\nFigure 4-5. The solid line corresponds to the regression with all the data, while\nthe dashed line corresonds to the regression with the point in the upper-right\nremoved. Clearly, that data value has a huge influence on the regression even\nthough it is not associated with a large outlier (from the full regression). This data\nvalue is considered to have high leverage on the regression.\nIn addition to standardized residuals (see “Outliers”), statisticians have\ndeveloped several metrics to determine the influence of a single record on a\nregression. A common measure of leverage is the hat-value; values above \n indicate a high-leverage data value.6\n\n\nFigure 4-5. An example of an influential data point in regression\nAnother metric is Cook’s distance, which defines influence as a combination of\nleverage and residual size. A rule of thumb is that an observation has high\ninfluence if Cook’s distance exceeds \n.\nAn influence plot or bubble plot combines standardized residuals, the hat-value,\nand Cook’s distance in a single plot. Figure 4-6 shows the influence plot for the\nKing County house data, and can be created by the following R code.\n\n\nstd_resid <- rstandard(lm_98105)\ncooks_D <- cooks.distance(lm_98105)\nhat_values <- hatvalues(lm_98105)\nplot(hat_values, std_resid, cex=10*sqrt(cooks_D))\nabline(h=c(-2.5, 2.5), lty=2)\nThere are apparently several data points that exhibit large influence in the\nregression. Cook’s distance can be computed using the function cooks.distance,\nand you can use hatvalues to compute the diagnostics. The hat values are plotted\non the x-axis, the residuals are plotted on the y-axis, and the size of the points is\nrelated to the value of Cook’s distance.\n\n\nFigure 4-6. A plot to determine which observations have high influence\nTable 4-2 compares the regression with the full data set and with highly influential\ndata points removed. The regression coefficient for Bathrooms changes quite\ndramatically.7\nTable 4-2. Comparison of\nregression coefficients with the full\ndata and with influential data\nremoved\n\n\nremoved\nOriginal Influential removed\n(Intercept)\n–772550 –647137\nSqFtTotLiving 210\n230\nSqFtLot\n39\n33\nBathrooms\n2282\n–16132\nBedrooms\n–26320\n–22888\nBldgGrade\n130000\n114871\nFor purposes of fitting a regression that reliably predicts future data, identifying\ninfluential observations is only useful in smaller data sets. For regressions\ninvolving many records, it is unlikely that any one observation will carry\nsufficient weight to cause extreme influence on the fitted equation (although the\nregression may still have big outliers). For purposes of anomaly detection, though,\nidentifying influential observations can be very useful.\n\n\nHeteroskedasticity, Non-Normality and Correlated Errors\nStatisticians pay considerable attention to the distribution of the residuals. It turns\nout that ordinary least squares (see “Least Squares”) are unbiased, and in some\ncases the “optimal” estimator, under a wide range of distributional assumptions.\nThis means that in most problems, data scientists do not need to be too concerned\nwith the distribution of the residuals.\nThe distribution of the residuals is relevant mainly for the validity of formal\nstatistical inference (hypothesis tests and p-values), which is of minimal\nimportance to data scientists concerned mainly with predictive accuracy. For\nformal inference to be fully valid, the residuals are assumed to be normally\ndistributed, have the same variance, and be independent. One area where this may\nbe of concern to data scientists is the standard calculation of confidence intervals\nfor predicted values, which are based upon the assumptions about the residuals\n(see “Confidence and Prediction Intervals”).\nHeteroskedasticity is the lack of constant residual variance across the range of the\npredicted values. In other words, errors are greater for some portions of the range\nthan for others. The ggplot2 package has some convenient tools to analyze\nresiduals.\nThe following code plots the absolute residuals versus the predicted values for\nthe lm_98105 regression fit in “Outliers”.\ndf <- data.frame(\n  resid = residuals(lm_98105),\n  pred = predict(lm_98105))\nggplot(df, aes(pred, abs(resid))) +\n  geom_point() +\n  geom_smooth()\nFigure 4-7 shows the resulting plot. Using geom_smooth, it is easy to superpose a\nsmooth of the absolute residuals. The function calls the loess method to produce\na visual smooth to estimate the relationship between the variables on the x-axis\nand y-axis in a scatterplot (see Scatterplot Smoothers).\n\n\nFigure 4-7. A plot of the absolute value of the residuals versus the predicted values\nEvidently, the variance of the residuals tends to increase for higher-valued homes,\nbut is also large for lower-valued homes. This plot indicates that lm_98105 has\nheteroskedastic errors.\n\n\nWHY WOULD A DATA SCIENTIST CARE ABOUT\nHETEROSKEDASTICITY?\nHeteroskedasticity indicates that prediction errors differ for different ranges of the predicted\nvalue, and may suggest an incomplete model. For example, the heteroskedasticity in lm_98105\nmay indicate that the regression has left something unaccounted for in high- and low-range\nhomes.\nFigure 4-8 is a histogram of the standarized residuals for the lm_98105\nregression. The distribution has decidely longer tails than the normal distribution,\nand exhibits mild skewness toward larger residuals.\n\n\nFigure 4-8. A histogram of the residuals from the regression of the housing data\nStatisticians may also check the assumption that the errors are independent. This is\nparticularly true for data that is collected over time. The Durbin-Watson statistic\ncan be used to detect if there is significant autocorrelation in a regression\ninvolving time series data.\nEven though a regression may violate one of the distributional assumptions, should\nwe care? Most often in data science, the interest is primarily in predictive\n\n\naccuracy, so some review of heteroskedasticity may be in order. You may\ndiscover that there is some signal in the data that your model has not captured.\nSatisfying distributional assumptions simply for the sake of validating formal\nstatistical inference (p-values, F-statistics, etc.), however, is not that important for\nthe data scientist.\n\n\nSCATTERPLOT SMOOTHERS\nRegression is about modeling the relationship between the response and predictor variables. In\nevaluating a regression model, it is useful to use a scatterplot smoother to visually highlight\nrelationships between two variables.\nFor example, in Figure 4-7, a smooth of the relationship between the absolute residuals and the\npredicted value shows that the variance of the residuals depends on the value of the residual. In\nthis case, the loess function was used; loess works by repeatedly fitting a series of local\nregressions to contiguous subsets to come up with a smooth. While loess is probably the most\ncommonly used smoother, other scatterplot smoothers are available in R, such as super smooth\n(supsmu) and kernal smoothing (ksmooth). For the purposes of evaluating a regression model,\nthere is typically no need to worry about the details of these scatterplot smooths.\n\n\nPartial Residual Plots and Nonlinearity\nPartial residual plots are a way to visualize how well the estimated fit explains\nthe relationship between a predictor and the outcome. Along with detection of\noutliers, this is probably the most important diagnostic for data scientists. The\nbasic idea of a partial residual plot is to isolate the relationship between a\npredictor variable and the response, taking into account all of the other\npredictor variables. A partial residual might be thought of as a “synthetic\noutcome” value, combining the prediction based on a single predictor with the\nactual residual from the full regression equation. A partial residual for predictor \n is the ordinary residual plus the regression term associated with \n:\nwhere  is the estimated regression coefficient. The predict function in R has an\noption to return the individual regression terms \n:\nterms <- predict(lm_98105, type='terms')\npartial_resid <- resid(lm_98105) + terms\nThe partial residual plot displays the \n on the x-axis and the partial residuals on\nthe y-axis. Using ggplot2 makes it easy to superpose a smooth of the partial\nresiduals.\ndf <- data.frame(SqFtTotLiving = house_98105[, 'SqFtTotLiving'],\n                 Terms = terms[, 'SqFtTotLiving'],\n                 PartialResid = partial_resid[, 'SqFtTotLiving'])\nggplot(df, aes(SqFtTotLiving, PartialResid)) +\n  geom_point(shape=1) + scale_shape(solid = FALSE) +\n  geom_smooth(linetype=2) +\n  geom_line(aes(SqFtTotLiving, Terms))\nThe resulting plot is shown in Figure 4-9. The partial residual is an estimate of the\ncontribution that SqFtTotLiving adds to the sales price. The relationship\nbetween SqFtTotLiving and the sales price is evidently nonlinear. The\nregression line underestimates the sales price for homes less than 1,000 square\nfeet and overestimates the price for homes between 2,000 and 3,000 square feet.\n\n\nThere are too few data points above 4,000 square feet to draw conclusions for\nthose homes.\n\n\nFigure 4-9. A partial residual plot for the variable SqFtTotLiving\nThis nonlinearity makes sense in this case: adding 500 feet in a small home makes\na much bigger difference than adding 500 feet in a large home. This suggests that,\ninstead of a simple linear term for SqFtTotLiving, a nonlinear term should be\nconsidered (see “Polynomial and Spline Regression”).\nKEY IDEAS\nWhile outliers can cause problems for small data sets, the primary interest with outliers is to identify\n\n\nproblems with the data, or locate anomalies.\nSingle records (including regression outliers) can have a big influence on a regression equation with\nsmall data, but this effect washes out in big data.\nIf the regression model is used for formal inference (p-values and the like), then certain\nassumptions about the distribution of the residuals should be checked. In general, however, the\ndistribution of residuals is not critical in data science.\nThe partial residuals plot can be used to qualitatively assess the fit for each regression term,\npossibly leading to alternative model specification.\n\n\nPolynomial and Spline Regression\nThe relationship between the response and a predictor variable is not necessarily\nlinear. The response to the dose of a drug is often nonlinear: doubling the dosage\ngenerally doesn’t lead to a doubled response. The demand for a product is not a\nlinear function of marketing dollars spent since, at some point, demand is likely to\nbe saturated. There are several ways that regression can be extended to capture\nthese nonlinear effects.\nKEY TERMS FOR NONLINEAR REGRESSION\nPolynomial regression\nAdds polynomial terms (squares, cubes, etc.) to a regression.\nSpline regression\nFitting a smooth curve with a series of polynomial segments.\nKnots\nValues that separate spline segments.\nGeneralized additive models\nSpline models with automated selection of knots.\nSynonyms\nGAM\n\n\nNONLINEAR REGRESSION\nWhen statisticians talk about nonlinear regression, they are referring to models that can’t be fit\nusing least squares. What kind of models are nonlinear? Essentially all models where the\nresponse cannot be expressed as a linear combination of the predictors or some transform of the\npredictors. Nonlinear regression models are harder and computationally more intensive to fit,\nsince they require numerical optimization. For this reason, it is generally preferred to use a linear\nmodel if possible.\n\n\nPolynomial\nPolynomial regression involves including polynomial terms to a regression\nequation. The use of polynomial regression dates back almost to the development\nof regression itself with a paper by Gergonne in 1815. For example, a quadratic\nregression between the response Y and the predictor X would take the form:\nPolynomial regression can be fit in R through the poly function. For example, the\nfollowing fits a quadratic polynomial for SqFtTotLiving with the King County\nhousing data:\nlm(AdjSalePrice ~  poly(SqFtTotLiving, 2) + SqFtLot +\n                BldgGrade +  Bathrooms +  Bedrooms,\n                    data=house_98105)\nCall:\nlm(formula = AdjSalePrice ~ poly(SqFtTotLiving, 2) + SqFtLot +\n    BldgGrade + Bathrooms + Bedrooms, data = house_98105)\nCoefficients:\n            (Intercept)  poly(SqFtTotLiving, 2)1\n             -402530.47               3271519.49\npoly(SqFtTotLiving, 2)2                  SqFtLot\n              776934.02                    32.56\n              BldgGrade                Bathrooms\n              135717.06                 -1435.12\n               Bedrooms\n               -9191.94\nThere are now two coefficients associated with SqFtTotLiving: one for the\nlinear term and one for the quadratic term.\nThe partial residual plot (see “Partial Residual Plots and Nonlinearity”) indicates\nsome curvature in the regression equation associated with SqFtTotLiving. The\nfitted line more closely matches the smooth (see “Splines”) of the partial residuals\nas compared to a linear fit (see Figure 4-10).\n\n\nFigure 4-10. A polynomial regression fit for the variable SqFtTotLiving (solid line) versus a smooth\n(dashed line; see the following section about splines)\n\n\nSplines\nPolynomial regression only captures a certain amount of curvature in a nonlinear\nrelationship. Adding in higher-order terms, such as a cubic quartic polynomial,\noften leads to undesirable “wiggliness” in the regression equation. An alternative,\nand often superior, approach to modeling nonlinear relationships is to use splines.\nSplines provide a way to smoothly interpolate between fixed points. Splines were\noriginally used by draftsmen to draw a smooth curve, particularly in ship and\naircraft building.\nThe splines were created by bending a thin piece of wood using weights, referred\nto as “ducks”; see Figure 4-11.\nFigure 4-11. Splines were originally created using bendable wood and “ducks,” and were used as a\ndraftsman tool to fit curves. Photo courtesy Bob Perry.\nThe technical definition of a spline is a series of piecewise continuous\npolynomials. They were first developed during World War II at the US Aberdeen\nProving Grounds by I. J. Schoenberg, a Romanian mathematician. The polynomial\npieces are smoothly connected at a series of fixed points in a predictor variable,\nreferred to as knots. Formulation of splines is much more complicated than\npolynomial regression; statistical software usually handles the details of fitting a\nspline. The R package splines includes the function bs to create a b-spline term\nin a regression model. For example, the following adds a b-spline term to the\nhouse regression model:\n\n\nlibrary(splines)\nknots <- quantile(house_98105$SqFtTotLiving, p=c(.25, .5, .75))\nlm_spline <- lm(AdjSalePrice ~ bs(SqFtTotLiving, knots=knots, degree=3) +\n  SqFtLot + Bathrooms + Bedrooms + BldgGrade,  data=house_98105)\nTwo parameters need to be specified: the degree of the polynomial and the\nlocation of the knots. In this case, the predictor SqFtTotLiving is included in the\nmodel using a cubic spline (degree=3). By default, bs places knots at the\nboundaries; in addition, knots were also placed at the lower quartile, the median\nquartile, and the upper quartile.\nIn contrast to a linear term, for which the coefficient has a direct meaning, the\ncoefficients for a spline term are not interpretable. Instead, it is more useful to use\nthe visual display to reveal the nature of the spline fit. Figure 4-12 displays the\npartial residual plot from the regression. In contrast to the polynomial model, the\nspline model more closely matches the smooth, demonstrating the greater\nflexibility of splines. In this case, the line more closely fits the data. Does this\nmean the spline regression is a better model? Not necessarily: it doesn’t make\neconomic sense that very small homes (less than 1,000 square feet) would have\nhigher value than slightly larger homes. This is possibly an artifact of a\nconfounding variable; see “Confounding Variables”.\n\n\nFigure 4-12. A spline regression fit for the variable SqFtTotLiving (solid line) compared to a smooth\n(dashed line)\n\n\nGeneralized Additive Models\nSuppose you suspect a nonlinear relationship between the response and a\npredictor variable, either by a priori knowledge or by examining the regression\ndiagnostics. Polynomial terms may not flexible enough to capture the relationship,\nand spline terms require specifying the knots. Generalized additive models, or\nGAM, are a technique to automatically fit a spline regression. The gam package in\nR can be used to fit a GAM model to the housing data:\nlibrary(mgcv)\nlm_gam <- gam(AdjSalePrice ~ s(SqFtTotLiving) + SqFtLot +\n                      Bathrooms +  Bedrooms + BldgGrade,\n                    data=house_98105)\nThe term s(SqFtTotLiving) tells the gam function to find the “best” knots for a\nspline term (see Figure 4-13).\n\n\nFigure 4-13. A GAM regression fit for the variable SqFtTotLiving (solid line) compared to a smooth\n(dashed line)\nKEY IDEAS\nOutliers in a regression are records with a large residual.\nMulticollinearity can cause numerical instability in fitting the regression equation.\nA confounding variable is an important predictor that is omitted from a model and can lead to a\nregression equation with spurious relationships.\nAn interaction term between two variables is needed if the effect of one variable depends on the\n\n\nlevel of the other.\nPolynomial regression can fit nonlinear relationships between predictors and the outcome variable.\nSplines are series of polynomial segments strung together, joining at knots.\nGeneralized additive models (GAM) automate the process of specifying the knots in splines.\n\n\nFurther Reading\nFor more on spline models and GAMS, see The Elements of Statistical Learning\nby Trevor Hastie, Robert Tibshirani, and Jerome Friedman, and its shorter cousin\nbased on R, An Introduction to Statistical Learning by Gareth James, Daniela\nWitten, Trevor Hastie, and Robert Tibshirani; both are Springer books.\n\n\nSummary\nPerhaps no other statistical method has seen greater use over the years than\nregression — the process of establishing a relationship between multiple\npredictor variables and an outcome variable. The fundamental form is linear: each\npredictor variable has a coefficient that describes a linear relationship between\nthe predictor and the outcome. More advanced forms of regression, such as\npolynomial and spline regression, permit the relationship to be nonlinear. In\nclassical statistics, the emphasis is on finding a good fit to the observed data to\nexplain or describe some phenomenon, and the strength of this fit is how\ntraditional (“in-sample”) metrics are used to assess the model. In data science, by\ncontrast, the goal is typically to predict values for new data, so metrics based on\npredictive accuracy for out-of-sample data are used. Variable selection methods\nare used to reduce dimensionality and create more compact models.\nThis and subsequent sections in this chapter © 2017 Datastats, LLC, Peter Bruce and Andrew Bruce,\nused by permission.\nIn Bayesian statistics, the true value is assumed to be a random variable with a specified distribution. In the\nBayesian context, instead of estimates of unknown parameters, there are posterior and prior distributions.\nThe -1 argument in the model.matrix produces one hot encoding representation (by removing the\nintercept, hence the “-”). Otherwise, the default in R is to produce a matrix with P – 1 columns with the\nfirst factor level as a reference.\nThis is unintuitive, but can be explained by the impact of location as a confounding variable; see\n“Confounding Variables”.\nThere are 82 zip codes in King County, several with just a handful of sales. An alternative to directly using\nzip code as a factor variable, ZipGroup clusters similar zip codes into a single group. See “Factor Variables\nwith Many Levels” for details.\nThe term hat-value comes from the notion of the hat matrix in regression. Multiple linear regression can\nbe expressed by the formula \n where \n is the hat matrix. The hat-values correspond to the\ndiagonal of \n.\nThe coefficient for Bathrooms becomes negative, which is unintuitive. Location has not been taken into\naccount and the zip code 98105 contains areas of disparate types of homes. See “Confounding Variables”\nfor a discussion of confounding variables.\n1\n2\n3\n4\n5\n6\n7\n\n\nChapter 5. Classification\nData scientists are often faced with a problem that requires an automated decision.\nIs an email an attempt at phishing? Is a customer likely to churn? Is the web user\nlikely to click on an advertisement? These are all classification problems.\nClassification is perhaps the most important form of prediction: the goal is to\npredict whether a record is a 0 or a 1 (phishing/not-phishing, click/don’t click,\nchurn/don’t churn), or in some cases, one of several categories (for example,\nGmail’s filtering of your inbox into “primary,” “social,” “promotional,” or\n“forums”).\nOften, we need more than a simple binary classification: we want to know the\npredicted probability that a case belongs to a class.\nRather than having a model simply assign a binary classification, most algorithms\ncan return a probability score (propensity) of belonging to the class of interest. In\nfact, with logistic regression, the default output from R is on the log-odds scale,\nand this must be transformed to a propensity. A sliding cutoff can then be used to\nconvert the propensity score to a decision. The general approach is as follows:\n1. Establish a cutoff probability for the class of interest above which we\nconsider a record as belonging to that class.\n2. Estimate (with any model) the probability that a record belongs to the\nclass of interest.\n3. If that probability is above the cutoff probability, assign the new record\nto the class of interest.\nThe higher the cutoff, the fewer records predicted as 1 — that is, belonging to the\nclass of interest. The lower the cutoff, the more records predicted as 1.\nThis chapter covers several key techniques for classification and estimating\npropensities; additional methods that can be used both for classification and\nnumerical prediction are described in the next chapter.\nMORE THAN TWO CATEGORIES?\n\n\nThe vast majority of problems involve a binary response. Some classification problems, however, involve\na response with more than two possible outcomes. For example, at the anniversary of a customer’s\nsubscription contract, there might be three outcomes: the customer leaves, or “churns” (Y=2), goes on a\nmonth-to-month (Y=1) contract, or signs a new long-term contract (Y=0). The goal is to predict Y = j for\nj = 0, 1 or 2. Most of the classification methods in this chapter can be applied, either directly or with\nmodest adaptations, to responses that have more than two outcomes. Even in the case of more than two\noutcomes, the problem can often be recast into a series of binary problems using conditional probabilities.\nFor example, to predict the outcome of the contract, you can solve two binary prediction problems:\nPredict whether Y = 0 or Y > 0.\nGiven that Y > 0, predict whether Y = 1 or Y = 2.\nIn this case, it makes sense to break up the problem into two cases: whether the customer churns, and if\nthey don’t churn, what type of contract they will choose. From a model-fitting viewpoint, it is often\nadvantageous to convert the multiclass problem to a series of binary problems. This is particularly true\nwhen one category is much more common than the other categories.\n\n\nNaive Bayes\nThe naive Bayes algorithm uses the probability of observing predictor values,\ngiven an outcome, to estimate the probability of observing outcome Y = i, given a\nset of predictor values.1\nKEY TERMS FOR NAIVE BAYES\nConditional probability\nThe probability of observing some event (say X = i) given some other event (say Y = i), written as\n.\nPosterior probability\nThe probability of an outcome after the predictor information has been incorporated (in contrast to\nthe prior probability of outcomes, not taking predictor information into account).\nTo understand Bayesian classification, we can start out by imagining “non-naive”\nBayesian classification. For each record to be classified:\n1. Find all the other records with the same predictor profile (i.e., where the\npredictor values are the same).\n2. Determine what classes those records belong to and which class is most\nprevalent (i.e., probable).\n3. Assign that class to the new record.\nThe preceding approach amounts to finding all the records in the sample that are\nexactly like the new record to be classified in the sense that all the predictor\nvalues are identical.\nNOTE\nPredictor variables must be categorical (factor) variables in the standard naive Bayes algorithm.\nSee “Numeric Predictor Variables” for two workarounds for using continuous variables.\n\n\nWhy Exact Bayesian Classification Is Impractical\nWhen the number of predictor variables exceeds a handful, many of the records to\nbe classified will be without exact matches. This can be understood in the context\nof a model to predict voting on the basis of demographic variables. Even a sizable\nsample may not contain even a single match for a new record who is a male\nHispanic with high income from the US Midwest who voted in the last election,\ndid not vote in the prior election, has three daughters and one son, and is\ndivorced. And this is just eight variables, a small number for most classification\nproblems. The addition of just a single new variable with five equally frequent\ncategories reduces the probability of a match by a factor of 5.\nWARNING\nDespite its name, naive Bayes is not considered a method of Bayesian statistics. Naive Bayes is\na data–driven, empirical method requiring relatively little statistical expertise. The name comes\nfrom the Bayes rule–like calculation in forming the predictions — specifically the initial\ncalculation of predictor value probabilities given an outcome, and then the final calculation of\noutcome probabilities.\n\n\nThe Naive Solution\nIn the naive Bayes solution, we no longer restrict the probability calculation to\nthose records that match the record to be classified. Instead, we use the entire data\nset. The naive Bayes modification is as follows:\n1. For a binary response Y = i (i = 0 or 1), estimate the individual\nconditional probabilities for each predictor \n; these are\nthe probabilities that the predictor value is in the record when we\nobserve Y = i. This probability is estimated by the proportion of Xj\nvalues among the Y = i records in the training set.\n2. Multiply these probabilities by each other, and then by the proportion of\nrecords belonging to Y = i.\n3. Repeat steps 1 and 2 for all the classes.\n4. Estimate a probability for outcome i by taking the value calculated in step\n2 for class i and dividing it by the sum of such values for all classes.\n5. Assign the record to the class with the highest probability for this set of\npredictor values.\nThis naive Bayes algorithm can also be stated as an equation for the probability of\nobserving outcome Y = i, given a set of predictor values \n:\nThe value of \n is a scaling factor to ensure the probability\nis between 0 and 1 and does not depend on Y:\nWhy is this formula called “naive”? We have made a simplifying assumption that\nthe exact conditional probability of a vector of predictor values, given observing\nan outcome, is sufficiently well estimated by the product of the individual\nconditional probabilities \n. In other words, in estimating \n",
      "page_number": 231
    },
    {
      "number": 5,
      "title": "Classification",
      "start_page": 310,
      "end_page": 370,
      "detection_method": "regex_chapter_title",
      "content": " instead of \n, we are assuming \nis independent of all the other predictor variables \n for \n.\nSeveral packages in R can be used to estimate a naive Bayes model. The\nfollowing fits a model using the klaR package:\nlibrary(klaR)\nnaive_model <- NaiveBayes(outcome ~ purpose_ + home_ + emp_len_,\n                          data = na.omit(loan_data))\nnaive_model$table\n$purpose_\n          var\ngrouping   credit_card debt_consolidation home_improvement major_purchase\n  paid off   0.1857711          0.5523427       0.07153354     0.05541148\n  default    0.1517548          0.5777144       0.05956086     0.03708506\n          var\ngrouping      medical      other small_business\n  paid off 0.01236169 0.09958506     0.02299447\n  default  0.01434993 0.11415111     0.04538382\n$home_\n          var\ngrouping    MORTGAGE        OWN      RENT\n  paid off 0.4966286 0.08043741 0.4229340\n  default  0.4327455 0.08363589 0.4836186\n$emp_len_\n          var\ngrouping    > 1 Year   < 1 Year\n  paid off 0.9690526 0.03094744\n  default  0.9523686 0.04763140\nThe output from the model is the conditional probabilities \n. The\nmodel can be used to predict the outcome of a new loan:\nnew_loan\n        purpose_    home_  emp_len_\n1 small_business MORTGAGE  > 1 Year\nIn this case, the model predicts a default:\npredict(naive_model, new_loan)\n$class\n[1] default\nLevels: paid off default\n$posterior\n      paid off   default\n[1,] 0.3717206 0.6282794\nThe prediction also returns a posterior estimate of the probability of default.\n\n\nThe naive Bayesian classifier is known to produce biased estimates. However,\nwhere the goal is to rank records according to the probability that Y = 1, unbiased\nestimates of probability are not needed and naive Bayes produces good results.\n\n\nNumeric Predictor Variables\nFrom the definition, we see that the Bayesian classifier works only with\ncategorical predictors (e.g., with spam classification, where presence or absence\nof words, phrases, characters, and so on, lies at the heart of the predictive task).\nTo apply naive Bayes to numerical predictors, one of two approaches must be\ntaken:\nBin and convert the numerical predictors to categorical predictors and apply\nthe algorithm of the previous section.\nUse a probability model — for example, the normal distribution (see\n“Normal Distribution”) — to estimate the conditional probability \n.\nCAUTION\nWhen a predictor category is absent in the training data, the algorithm assigns zero probability to\nthe outcome variable in new data, rather than simply ignoring this variable and using the\ninformation from other variables, as other methods might. This is something to pay attention to\nwhen binning continuous variables.\nKEY IDEAS\nNaive Bayes works with categorical (factor) predictors and outcomes.\nIt asks, “Within each outcome category, which predictor categories are most probable?”\nThat information is then inverted to estimate probabilities of outcome categories, given predictor\nvalues.\n\n\nFurther Reading\nElements of Statistical Learning, 2nd ed., by Trevor Hastie, Robert\nTibshirani, and Jerome Friedman (Springer, 2009).\nThere is a full chapter on naive Bayes in Data Mining for Business\nAnalytics, 3rd ed., by Galit Shmueli, Peter Bruce, and Nitin Patel (Wiley,\n2016, with variants for R, Excel, and JMP).\n\n\nDiscriminant Analysis\nDiscriminant analysis is the earliest statistical classifier; it was introduced by R.\nA. Fisher in 1936 in an article published in the Annals of Eugenics journal.2\nKEY TERMS FOR DISCRIMINANT ANALYSIS\nCovariance\nA measure of the extent to which one variable varies in concert with another (i.e., similar\nmagnitude and direction).\nDiscriminant function\nThe function that, when applied to the predictor variables, maximizes the separation of the classes.\nDiscriminant weights\nThe scores that result from the application of the discriminant function, and are used to estimate\nprobabilities of belonging to one class or another.\nWhile discriminant analysis encompasses several techniques, the most commonly\nused is linear discriminant analysis, or LDA. The original method proposed by\nFisher was actually slightly different from LDA, but the mechanics are essentially\nthe same. LDA is now less widely used with the advent of more sophisticated\ntechniques, such as tree models and logistic regression.\nHowever, you may still encounter LDA in some applications and it has links to\nother more widely used methods (such as principal components analysis; see\n“Principal Components Analysis”). In addition, discriminant analysis can provide\na measure of predictor importance, and it is used as a computationally efficient\nmethod of feature selection.\nWARNING\nLinear discriminant analysis should not be confused with Latent Dirichlet Allocation, also referred\nto as LDA. Latent Dirichlet Allocation is used in text and natural language processing and is\nunrelated to linear discriminant analysis.\n\n\nCovariance Matrix\nTo understand discriminant analysis, it is first necessary to introduce the concept\nof covariance between two or more variables. The covariance measures the\nrelationship between two variables  and . Denote the mean for each variable\nby \n and  (see “Mean”). The covariance \n between  and  is given by:\nwhere n is the number of records (note that we divide by n – 1 instead of n: see\n“Degrees of Freedom, and n or n – 1?”).\nAs with the correlation coefficient (see “Correlation”), positive values indicate a\npositive relationship and negative values indicate a negative relationship.\nCorrelation, however, is constrained to be between –1 and 1, whereas covariance\nis on the same scale as the variables  and . The covariance matrix \n for \nand  consists of the individual variable variances, \n and \n, on the diagonal\n(where row and column are the same variable) and the covariances between\nvariable pairs on the off-diagonals.\nNOTE\nRecall that the standard deviation is used to normalize a variable to a z-score; the covariance\nmatrix is used in a multivariate extension of this standardization process. This is known as\n\n\nMahalanobis distance (see Other Distance Metrics) and is related to the LDA function.\n\n\nFisher’s Linear Discriminant\nFor simplicity, we focus on a classification problem in which we want to predict\na binary outcome y using just two continuous numeric variables \n.\nTechnically, discriminant analysis assumes the predictor variables are normally\ndistributed continuous variables, but, in practice, the method works well even for\nnonextreme departures from normality, and for binary predictors. Fisher’s linear\ndiscriminant distinguishes variation between groups, on the one hand, from\nvariation within groups on the other. Specifically, seeking to divide the records\ninto two groups, LDA focuses on maximizing the “between” sum of squares \n (measuring the variation between the two groups) relative to the\n“within” sum of squares \n (measuring the within-group variation). In this\ncase, the two groups correspond to the records \n for which y = 0 and the\nrecords \n for which y = 1. The method finds the linear combination \n that maximizes that sum of squares ratio.\nThe between sum of squares is the squared distance between the two group means,\nand the within sum of squares is the spread around the means within each group,\nweighted by the covariance matrix. Intuitively, by maximizing the between sum of\nsquares and minimizing the within sum of squares, this method yields the greatest\nseparation between the two groups.\n\n\nA Simple Example\nThe MASS package, associated with the book Modern Applied Statistics with S by\nW. N. Venables and B. D. Ripley (Springer, 1994), provides a function for LDA\nwith R. The following applies this function to a sample of loan data using two\npredictor variables, borrower_score and payment_inc_ratio, and prints out\nthe estimated linear discriminator weights.\nlibrary(MASS)\nloan_lda <- lda(outcome ~ borrower_score + payment_inc_ratio,\n                     data=loan3000)\nloan_lda$scaling\n                         LD1\nborrower_score    -6.2962811\npayment_inc_ratio  0.1288243\n\n\nUSING DISCRIMINANT ANALYSIS FOR FEATURE\nSELECTION\nIf the predictor variables are normalized prior to running LDA, the discriminator weights are\nmeasures of variable importance, thus providing a computationally efficient method of feature\nselection.\nThe lda function can predict the probability of “default” versus “paid off”:\npred <- predict(loan_lda)\nhead(pred$posterior)\n       paid off   default\n25333 0.5554293 0.4445707\n27041 0.6274352 0.3725648\n7398  0.4014055 0.5985945\n35625 0.3411242 0.6588758\n17058 0.6081592 0.3918408\n2986  0.6733245 0.3266755\nA plot of the predictions helps illustrate how LDA works. Using the output from\nthe predict function, a plot of the estimated probability of default is produced as\nfollows:\nlda_df <- cbind(loan3000, prob_default=pred$posterior[,'default'])\nggplot(data=lda_df,\n       aes(x=borrower_score, y=payment_inc_ratio, color=prob_default)) +\n  geom_point(alpha=.6) +\n  scale_color_gradient2(low='white', high='blue') +\ngeom_line(data=lda_df0, col='green', size=2, alpha=.8) +\nThe resulting plot is shown in Figure 5-1.\n\n\nFigure 5-1. LDA prediction of loan default using two variables: a score of the borrower’s\ncreditworthiness and the payment to income ratio.\nUsing the discriminant function weights, LDA splits the predictor space into two\nregions as shown by the solid line. The predictions farther away from the line\nhave a higher level of confidence (i.e., a probability further away from 0.5).\n\n\nEXTENSIONS OF DISCRIMINANT ANALYSIS\nMore predictor variables: while the text and example in this section used just two predictor\nvariables, LDA works just as well with more than two predictor variables. The only limiting factor\nis the number of records (estimating the covariance matrix requires a sufficient number of\nrecords per variable, which is typically not an issue in data science applications).\nQuadratic Discriminant Analysis: There are other variants of discriminant analysis. The best\nknown is quadratic discriminant analysis (QDA). Despite its name, QDA is still a linear\ndiscriminant function. The main difference is that in LDA, the covariance matrix is assumed to be\nthe same for the two groups corresponding to Y = 0 and Y = 1. In QDA, the covariance matrix is\nallowed to be different for the two groups. In practice, the difference in most applications is not\ncritical.\nKEY IDEAS FOR DISCRIMINANT ANALYSIS\nDiscriminant analysis works with continuous or categorical predictors, as well as categorical\noutcomes.\nUsing the covariance matrix, it calculates a linear discriminant function, which is used to\ndistinguish records belonging to one class from those belonging to another.\nThis function is applied to the records to derive weights, or scores, for each record (one weight for\neach possible class) that determines its estimated class.\n\n\nFurther Reading\nElements of Statistical Learning, 2nd ed., by Trevor Hastie, Robert\nTibshirani, Jerome Freidman, and its shorter cousin, An Introduction to\nStatistical Learning, by Gareth James, Daniela Witten, Trevor Hastie, and\nRobert Tibshirani (both from Springer). Both have a section on discriminant\nanalysis.\nData Mining for Business Analytics, 3rd ed., by Galit Shmueli, Peter Bruce,\nand Nitin Patel (Wiley, 2016, with variants for R, Excel, and JMP) has a full\nchapter on discriminant analysis.\nFor historical interest, Fisher’s original article on the topic, “The Use of\nMultiple Measurements in Taxonomic Problems,” as published in 1936 in\nAnnals of Eugenics (now called Annals of Genetics) can be found online.\n\n\nLogistic Regression\nLogistic regression is analogous to multiple linear regression, except the outcome\nis binary. Various transformations are employed to convert the problem to one in\nwhich a linear model can be fit. Like discriminant analysis, and unlike K-Nearest\nNeighbor and naive Bayes, logistic regression is a structured model approach,\nrather than a data-centric approach. Due to its fast computational speed and its\noutput of a model that lends itself to rapid scoring of new data, it is a popular\nmethod.\nKEY TERMS FOR LOGISTIC REGRESSION\nLogit\nThe function that maps the probability of belonging to a class with a range from ± ∞ (instead of 0\nto 1).\nSynonym\nLog odds (see below)\nOdds\nThe ratio of “success” (1) to “not success” (0).\nLog odds\nThe response in the transformed model (now linear), which gets mapped back to a probability.\nHow do we get from a binary outcome variable to an outcome variable that can be\nmodeled in linear fashion, then back again to a binary outcome?\n\n\nLogistic Response Function and Logit\nThe key ingredients are the logistic response function and the logit, in which we\nmap a probability (which is on a 0–1 scale) to a more expansive scale suitable for\nlinear modeling.\nThe first step is to think of the outcome variable not as a binary label, but as the\nprobability p that the label is a “1.” Naively, we might be tempted to model p as a\nlinear function of the predictor variables:\nHowever, fitting this model does not ensure that p will end up between 0 and 1, as\na probability must.\nInstead, we model p by applying a logistic response or inverse logit function to\nthe predictors:\nThis transform ensures that the p stays between 0 and 1.\nTo get the exponential expression out of the denominator, we consider odds\ninstead of probabilities. Odds, familiar to bettors everywhere, are the ratio of\n“successes” (1) to “nonsuccesses” (0). In terms of probabilities, odds are the\nprobability of an event divided by the probability that the event will not occur.\nFor example, if the probability that a horse will win is 0.5, the probability of\n“won’t win” is (1 – 0.5) = 0.5, and the odds are 1.0.\nWe can obtain the probability from the odds using the inverse odds function:\n\n\nWe combine this with the logistic response function, shown earlier, to get:\nFinally, taking the logarithm of both sides, we get an expression that involves a\nlinear function of the predictors:\nThe log-odds function, also known as the logit function, maps the probability p\nfrom \n to any value \n: see Figure 5-2. The\ntransformation circle is complete; we have used a linear model to predict a\nprobability, which, in turn, we can map to a class label by applying a cutoff rule\n— any record with a probability greater than the cutoff is classified as a 1.\n\n\nFigure 5-2. The function that maps a probability to a scale suitable for a linear model (logit)\n\n\nLogistic Regression and the GLM\nThe response in the logistic regression formula is the log odds of a binary\noutcome of 1. We only observe the binary outcome, not the log odds, so special\nstatistical methods are needed to fit the equation. Logistic regression is a special\ninstance of a generalized linear model (GLM) developed to extend linear\nregression to other settings.\nIn R, to fit a logistic regression, the glm function is used with the family\nparameter set to binomial. The following code fits a logistic regression to the\npersonal loan data introduced in “K-Nearest Neighbors”.\nlogistic_model\nCall:  glm(formula = outcome ~ payment_inc_ratio + purpose_ + home_ +\n    emp_len_ + borrower_score, family = \"binomial\", data = loan_data)\nCoefficients:\n               (Intercept)           payment_inc_ratio\n                   1.26982                     0.08244\npurpose_debt_consolidation    purpose_home_improvement\n                   0.25216                     0.34367\n    purpose_major_purchase             purpose_medical\n                   0.24373                     0.67536\n             purpose_other      purpose_small_business\n                   0.59268                     1.21226\n                  home_OWN                   home_RENT\n                   0.03132                     0.16867\n         emp_len_ < 1 Year              borrower_score\n                   0.44489                    -4.63890\nDegrees of Freedom: 46271 Total (i.e. Null);  46260 Residual\nNull Deviance:     64150\nResidual Deviance: 58530  \nAIC: 58550\nThe response is outcome, which takes a 0 if the loan is paid off and 1 if the loan\ndefaults. purpose_ and home_ are factor variables representing the purpose of the\nloan and the home ownership status. As in regression, a factor variable with P\nlevels is represented with P – 1 columns. By default in R, the reference coding is\nused and the levels are all compared to the reference level (see “Factor Variables\nin Regression”). The reference levels for these factors are credit_card and\nMORTGAGE, respectively. The variable borrower_score is a score from 0 to 1\nrepresenting the creditworthiness of the borrower (from poor to excellent). This\nvariable was created from several other variables using K-Nearest Neighbor: see\n“KNN as a Feature Engine”.\n\n\nGeneralized Linear Models\nGeneralized linear models (GLMs) are the second most important class of models\nbesides regression. GLMs are characterized by two main components:\nA probability distribution or family (binomial in the case of logistic\nregression)\nA link function mapping the response to the predictors (logit in the case of\nlogistic regression)\nLogistic regression is by far the most common form of GLM. A data scientist will\nencounter other types of GLMs. Sometimes a log link function is used instead of\nthe logit; in practice, use of a log link is unlikely to lead to very different results\nfor most applications. The poisson distribution is commonly used to model count\ndata (e.g., the number of times a user visits a web page in a certain amount of\ntime). Other families include negative binomial and gamma, often used to model\nelapsed time (e.g., time to failure). In contrast to logistic regression, application\nof GLMs with these models is more nuanced and involves greater care. These are\nbest avoided unless you are familiar with and understand the utility and pitfalls of\nthese methods.\n\n\nPredicted Values from Logistic Regression\nThe predicted value from logistic regression is in terms of the log odds: \n. The predicted probability is given by the logistic\nresponse function:\nFor example, look at the predictions from the model logistic_model:\npred <- predict(logistic_model)\nsummary(pred)\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.\n-2.728000 -0.525100 -0.005235  0.002599  0.513700  3.658000\nConverting these values to probabilities is a simple transform:\nprob <- 1/(1 + exp(-pred))\n> summary(prob)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n0.06132 0.37170 0.49870 0.50000 0.62570 0.97490\nThese are on a scale from 0 to 1 and don’t yet declare whether the predicted value\nis default or paid off. We could declare any value greater than 0.5 as default,\nanalogous to the K-Nearest Neighbors classifier. In practice, a lower cutoff is\noften appropriate if the goal is to identify members of a rare class (see “The Rare\nClass Problem”).\n\n\nInterpreting the Coefficients and Odds Ratios\nOne advantage of logistic regression is that it produces a model that can be scored\nto new data rapidly, without recomputation. Another is the relative ease of\ninterpretation of the model, as compared with other classification methods. The\nkey conceptual idea is understanding an odds ratio. The odds ratio is easiest to\nunderstand for a binary factor variable X:\nThis is interpreted as the odds that Y = 1 when X = 1 versus the odds that Y = 1\nwhen X = 0. If the odds ratio is 2, then the odds that Y = 1 are two times higher\nwhen X = 1 versus X = 0.\nWhy bother with an odds ratio, instead of probabilities? We work with odds\nbecause the coefficient \n in the logistic regression is the log of the odds ratio for \n.\nAn example will make this more explicit. For the model fit in “Logistic\nRegression and the GLM”, the regression coefficient for\npurpose_small_business is 1.21226. This means that a loan to a small business\ncompared to a loan to pay off credit card debt reduces the odds of defaulting\nversus being paid off by \n. Clearly, loans for the\npurpose of creating or expanding a small business are considerably riskier than\nother types of loans.\nFigure 5-3 shows the relationship between the odds ratio and log-odds ratio for\nodds ratios greater than 1. Because the coefficients are on the log scale, an\nincrease of 1 in the coefficient results in an increase of \n in the\nodds ratio.\n\n\nFigure 5-3. The relationship between the odds ratio and the log-odds ratio\nOdds ratios for numeric variables X can be interpreted similarly: they measure the\nchange in the odds ratio for a unit change in X. For example, the effect of\nincreasing the payment to income ratio from, say, 5 to 6 increases the odds of the\nloan defaulting by a factor of \n. The variable\nborrower_score is a score on the borrowers’ creditworthiness and ranges from 0\n(low) to 1 (high). The odds of the best borrowers relative to the worst borrowers\ndefaulting on their loans is smaller by a factor of \n. In other words, the default risk from the borrowers with the poorest\ncreditworthiness is 100 times greater than that of the best borrowers!\n\n\nLinear and Logistic Regression: Similarities and Differences\nMultiple linear regression and logistic regression share many commonalities. Both\nassume a parametric linear form relating the predictors with the response.\nExploring and finding the best model are done in very similar ways. Generalities\nto the linear model to use a spline transform of the predictor are equally\napplicable in the logistic regression setting. Logistic regression differs in two\nfundamental ways:\nThe way the model is fit (least squares is not applicable)\nThe nature and analysis of the residuals from the model\nFitting the model\nLinear regression is fit using least squares, and the quality of the fit is evaluated\nusing RMSE and R-squared statistics. In logistic regression (unlike in linear\nregression), there is no closed-form solution and the model must be fit using\nmaximum likelihood estimation (MLE). Maximum likelihood estimation is a\nprocess that tries to find the model that is most likely to have produced the data\nwe see. In the logistic regression equation, the response is not 0 or 1 but rather an\nestimate of the log odds that the response is 1. The MLE finds the solution such\nthat the estimated log odds best describes the observed outcome. The mechanics\nof the algorithm involve a quasi-Newton optimization that iterates between a\nscoring step (Fisher’s scoring), based on the current parameters, and an update to\nthe parameters to improve the fit.\nMAXIMUM LIKELIHOOD ESTIMATION\nMore detail, if you like statistical symbols: start with a set of data \n and a\nprobability model \n that depends on a set of parameters \n. The goal of\nMLE is to find the set of parameters  that maximizes the value of \n; that\nis, it maximizes the probability of observing \n given the model ..] In the\nfitting process, the model is evaluated using a metric called deviance:\n\n\nLower deviance corresponds to a better fit.\nFortunately, most users don’t need to concern themselves with the details of the\nfitting algorithm since this is handled by the software. Most data scientists will not\nneed to worry about the fitting method, other than understanding that it is a way to\nfind a good model under certain assumptions.\n\n\nHANDLING FACTOR VARIABLES\nIn logistic regression, factor variables should be coded as in linear regression; see “Factor\nVariables in Regression”. In R and other software, this is normally handled automatically and\ngenerally reference encoding is used. All of the other classification methods covered in this\nchapter typically use the one hot encoder representation (see “One Hot Encoder”).\n\n\nAssessing the Model\nLike other classification methods, logistic regression is assessed by how\naccurately the model classifies new data (see “Evaluating Classification\nModels”). As with linear regression, some additional standard statistical tools are\navailable to assess and improve the model. Along with the estimated coefficients,\nR reports the standard error of the coefficients (SE), a z-value, and a p-value:\nsummary(logistic_model)\nCall:\nglm(formula = outcome ~ payment_inc_ratio + purpose_ + home_ +\n    emp_len_ + borrower_score, family = \"binomial\", data = loan_data)\nDeviance Residuals:\n     Min        1Q    Median        3Q       Max\n-2.71430  -1.06806  -0.04482   1.07446   2.11672\nCoefficients:\n                            Estimate Std. Error z value Pr(>|z|)\n(Intercept)                 1.269822   0.051929  24.453  < 2e-16 ***\npayment_inc_ratio           0.082443   0.002485  33.177  < 2e-16 ***\npurpose_debt_consolidation  0.252164   0.027409   9.200  < 2e-16 ***\npurpose_home_improvement    0.343674   0.045951   7.479 7.48e-14 ***\npurpose_major_purchase      0.243728   0.053314   4.572 4.84e-06 ***\npurpose_medical             0.675362   0.089803   7.520 5.46e-14 ***\npurpose_other               0.592678   0.039109  15.154  < 2e-16 ***\npurpose_small_business      1.212264   0.062457  19.410  < 2e-16 ***\nhome_OWN                    0.031320   0.037479   0.836    0.403\nhome_RENT                   0.168670   0.021041   8.016 1.09e-15 ***\nemp_len_ < 1 Year           0.444892   0.053342   8.340  < 2e-16 ***\nborrower_score             -4.638902   0.082433 -56.275  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Dispersion parameter for binomial family taken to be 1)\n    Null deviance: 64147  on 46271  degrees of freedom\nResidual deviance: 58531  on 46260  degrees of freedom\nAIC: 58555\nNumber of Fisher Scoring iterations: 4\nInterpretation of the p-value comes with the same caveat as in regression, and\nshould be viewed more as a relative indicator of variable importance (see\n“Assessing the Model”) than as a formal measure of statistical significance. A\nlogistic regression model, which has a binary response, does not have an\nassociated RMSE or R-squared. Instead, a logistic regression model is typically\nevaluated using more general metrics for classification; see “Evaluating\nClassification Models”.\n\n\nMany other concepts for linear regression carry over to the logistic regression\nsetting (and other GLMs). For example, you can use stepwise regression, fit\ninteraction terms, or include spline terms. The same concerns regarding\nconfounding and correlated variables apply to logistic regression (see\n“Interpreting the Regression Equation”). You can fit generalized additive models\n(see “Generalized Additive Models”) using the mgcv package:\nlogistic_gam <- gam(outcome ~ s(payment_inc_ratio) + purpose_ +\n                        home_ + emp_len_ + s(borrower_score),\n                      data=loan_data, family='binomial')\nOne area where logistic regression differs is in the analysis of the residuals. As in\nregression (see Figure 4-9), it is straightforward to compute partial residuals:\nterms <- predict(logistic_gam, type='terms')\npartial_resid <- resid(logistic_model) + terms\ndf <- data.frame(payment_inc_ratio = loan_data[, 'payment_inc_ratio'],\n                 terms = terms[, 's(payment_inc_ratio)'],\n                 partial_resid = partial_resid[, 's(payment_inc_ratio)'])\nggplot(df, aes(x=payment_inc_ratio, y=partial_resid, solid = FALSE)) +\n  geom_point(shape=46, alpha=.4) +\n  geom_line(aes(x=payment_inc_ratio, y=terms),\n            color='red', alpha=.5, size=1.5) +\n  labs(y='Partial Residual')\nThe resulting plot is displayed in Figure 5-4. The estimated fit, shown by the line,\ngoes between two sets of point clouds. The top cloud corresponds to a response\nof 1 (defaulted loans), and the bottom cloud corresponds to a response of 0 (loans\npaid off). This is very typical of residuals from a logistic regression since the\noutput is binary. Partial residuals in logistic regression, while less valuable than\nin regression, are still useful to confirm nonlinear behavior and identify highly\ninfluential records.\n\n\nFigure 5-4. Partial residuals from logistic regression\nWARNING\nSome of the output from the summary function can effectively be ignored. The dispersion\nparameter does not apply to logistic regression and is there for other types of GLMs. The residual\ndeviance and the number of scoring iterations are related to the maximum likelihood fitting\nmethod; see “Maximum Likelihood Estimation”.\nKEY IDEAS FOR LOGISTIC REGRESSION\nLogistic regression is like linear regression, except that the outcome is a binary variable.\nSeveral transformations are needed to get the model into a form that can be fit as a linear model,\nwith the log of the odds ratio as the response variable.\n\n\nAfter the linear model is fit (by an iterative process), the log odds is mapped back to a probability.\nLogistic regression is popular because it is computationally fast, and produces a model that can be\nscored to new data without recomputation.\n\n\nFurther Reading\n1. The standard reference on logistic regression is Applied Logistic\nRegression, 3rd ed., by David Hosmer, Stanley Lemeshow, and Rodney\nSturdivant (Wiley).\n2. Also popular are two books by Joseph Hilbe: Logistic Regression\nModels (very comprehensive) and Practical Guide to Logistic\nRegression (compact), both from CRC Press.\n3. Elements of Statistical Learning, 2nd ed., by Trevor Hastie, Robert\nTibshirani, Jerome Freidman, and its shorter cousin, An Introduction to\nStatistical Learning, by Gareth James, Daniela Witten, Trevor Hastie,\nand Robert Tibshirani (both from Springer) both have a section on\nlogistic regression.\n4. Data Mining for Business Analytics, 3rd ed., by Galit Shmueli, Peter\nBruce, and Nitin Patel (Wiley, 2016, with variants for R, Excel, and\nJMP) has a full chapter on logistic regression.\n\n\nEvaluating Classification Models\nIt is common in predictive modeling to try out a number of different models, apply\neach to a holdout sample (also called a test or validation sample), and assess\ntheir performance. Fundamentally, this amounts to seeing which produces the most\naccurate predictions.\nKEY TERMS FOR EVALUATING CLASSIFICATION MODELS\nAccuracy\nThe percent (or proportion) of cases classified correctly.\nConfusion matrix\nA tabular display (2×2 in the binary case) of the record counts by their predicted and actual\nclassification status.\nSensitivity\nThe percent (or proportion) of 1s correctly classified.\nSynonym\nRecall\nSpecificity\nThe percent (or proportion) of 0s correctly classified.\nPrecision\nThe percent (proportion) of predicted 1s that are actually 1s.\nROC curve\nA plot of sensitivity versus specificity.\nLift\nA measure of how effective the model is at identifying (comparitively rare) 1s at different\nprobability cutoffs.\nA simple way to measure classification performance is to count the proportion of\npredictions that are correct.\nIn most classification algorithms, each case is assigned an “estimated probability\nof being a 1.”3 The default decision point, or cutoff, is typically 0.50 or 50%. If\nthe probability is above 0.5, the classification is “1,” otherwise it is “0.” An\nalternative default cutoff is the prevalent probability of 1s in the data.\n\n\nAccuracy is simply a measure of total error:\n\n\nConfusion Matrix\nAt the heart of classification metrics is the confusion matrix. The confusion\nmatrix is a table showing the number of correct and incorrect predictions\ncategorized by type of response. Several packages are available in R to compute a\nconfusion matrix, but in the binary case, it is simple to compute one by hand.\nTo illustrate the confusion matrix, consider the logistic_gam model that was\ntrained on a balanced data set with an equal number of defaulted and paid-off\nloans (see Figure 5-4). Following the usual conventions Y = 1 corresponds to the\nevent of interest (e.g., default) and Y = 0 corresponds to a negative (or usual)\nevent (e.g., paid off). The following computes the confusion matrix for the\nlogistic_gam model applied to the entire (unbalanced) training set:\npred <- predict(logistic_gam, newdata=train_set)\npred_y <- as.numeric(pred > 0)\ntrue_y <- as.numeric(train_set$outcome=='default')\ntrue_pos <- (true_y==1) & (pred_y==1)\ntrue_neg <- (true_y==0) & (pred_y==0)\nfalse_pos <- (true_y==0) & (pred_y==1)\nfalse_neg <- (true_y==1) & (pred_y==0)\nconf_mat <- matrix(c(sum(true_pos), sum(false_pos),\n                     sum(false_neg), sum(true_neg)), 2, 2)\ncolnames(conf_mat) <- c('Yhat = 1', 'Yhat = 0')\nrownames(conf_mat) <- c('Y = 1', 'Y = 0')\nconf_mat\n      Yhat = 1 Yhat = 0\nY = 1 14635    8501\nY = 0 8236     14900\nThe predicted outcomes are columns and the true outcomes are the rows. The\ndiagonal elements of the matrix show the number of correct predictions and the\noff-diagonal elements show the number of incorrect predictions. For example,\n6,126 defaulted loans were correctly predicted as a default, but 17,010 defaulted\nloans were incorrectly predicted as paid off.\nFigure 5-5 shows the relationship between the confusion matrix for a binary\nreponse Y and different metrics (see “Precision, Recall, and Specificity” for more\non the metrics). As with the example for the loan data, the actual response is along\nthe rows and the predicted response is along the columns. (You may see confusion\nmatrices with this reversed.) The diagonal boxes (upper left, lower right) show\nwhen the predictions  correctly predict the response. One important metric not\n\n\nexplicitly called out is the false positive rate (the mirror image of precision).\nWhen 1s are rare, the ratio of false positives to all predicted positives can be\nhigh, leading to the unintuitive situation where a predicted 1 is most likely a 0.\nThis problem plagues medical screening tests (e.g., mammograms) that are widely\napplied: due to the relative rarity of the condition, positive test results most likely\ndo not mean breast cancer. This leads to much confusion in the public.\nFigure 5-5. Confusion matrix for a binary response and various metrics\n\n\nThe Rare Class Problem\nIn many cases, there is an imbalance in the classes to be predicted, with one class\nmuch more prevalent than the other — for example, legitimate insurance claims\nversus fraudulent ones, or browsers versus purchasers at a website. The rare class\n(e.g., the fraudulent claims) is usually the class of more interest, and is typically\ndesignated 1, in contrast to the more prevalent 0s. In the typical scenario, the 1s\nare the more important case, in the sense that misclassifying them as 0s is costlier\nthan misclassfying 0s as 1s. For example, correctly identifying a fraudulent\ninsurance claim may save thousands of dollars. On the other hand, correctly\nidentifying a nonfraudulent claim merely saves you the cost and effort of going\nthrough the claim by hand with a more careful review (which is what you would\ndo if the claim were tagged as “fraudulent”).\nIn such cases, unless the classes are easily separable, the most accurate\nclassification model may be one that simply classifies everything as a 0. For\nexample, if only 0.1% of the browsers at a web store end up purchasing, a model\nthat predicts that each browser will leave without purchasing will be 99.9%\naccurate. However, it will be useless. Instead, we would be happy with a model\nthat is less accurate overall, but is good at picking out the purchasers, even if it\nmisclassifies some nonpurchasers along the way.\n\n\nPrecision, Recall, and Specificity\nMetrics other than pure accuracy — metrics that are more nuanced — are\ncommonly used in evaluating classification models. Several of these have a long\nhistory in statistics — especially biostatistics, where they are used to describe the\nexpected performance of diagnostic tests. The precision measures the accuracy of\na predicted positive outcome (see Figure 5-5):\nThe recall, also known as sensitivity, measures the strength of the model to\npredict a positive outcome — the proportion of the 1s that it correctly identifies\n(see Figure 5-5). The term sensitivity is used a lot in biostatistics and medical\ndiagnostics, whereas recall is used more in the machine learning community. The\ndefinition of recall is:\nAnother metric used is specificity, which measures a model’s ability to predict a\nnegative outcome:\n# precision\nconf_mat[1,1]/sum(conf_mat[,1])\n# recall\nconf_mat[1,1]/sum(conf_mat[1,])\n# specificity\nconf_mat[2,2]/sum(conf_mat[2,])\n\n\nROC Curve\nYou can see that there is a tradeoff between recall and specificity. Capturing more\n1s generally means misclassifying more 0s as 1s. The ideal classifier would do an\nexcellent job of classifying the 1s, without misclassifying more 0s as 1s.\nThe metric that captures this tradeoff is the “Receiver Operating Characteristics”\ncurve, usually referred to as the ROC curve. The ROC curve plots recall\n(sensitivity) on the y-axis against specificity on the x-axis.4 The ROC curve shows\nthe trade-off between recall and specificity as you change the cutoff to determine\nhow to classify a record. Sensitivity (recall) is plotted on the y-axis, and you may\nencounter two forms in which the x-axis is labeled:\nSpecificity plotted on the x-axis, with 1 on the left and 0 on the right\nSpecificity plotted on the x-axis, with 0 on the left and 1 on the right\nThe curve looks identical whichever way it is done. The process to compute the\nROC curve is:\n1. Sort the records by the predicted probability of being a 1, starting with\nthe most probable and ending with the least probable.\n2. Compute the cumulative specificity and recall based on the sorted\nrecords.\nComputing the ROC curve in R is straightforward. The following code computes\nROC for the loan data:\nidx <- order(-pred)\nrecall <- cumsum(true_y[idx]==1)/sum(true_y==1)\nspecificity <- (sum(true_y==0) - cumsum(true_y[idx]==0))/sum(true_y==0)\nroc_df <- data.frame(recall = recall, specificity = specificity)\nggplot(roc_df, aes(x=specificity, y=recall)) +\n  geom_line(color='blue') +\n  scale_x_reverse(expand=c(0, 0)) +\n  scale_y_continuous(expand=c(0, 0)) +\n  geom_line(data=data.frame(x=(0:100)/100), aes(x=x, y=1-x),\n            linetype='dotted', color='red')\nThe result is shown in Figure 5-6. The dotted diagonal line corresponds to a\nclassifier no better than random chance. An extremely effective classifier (or, in\nmedical situations, an extremely effective diagnostic test) will have an ROC that\n\n\nhugs the upper-left corner — it will correctly identify lots of 1s without\nmisclassifying lots of 0s as 1s. For this model, if we want a classifier with a\nspecificity of at least 50%, then the recall is about 75%.\n\n\nFigure 5-6. ROC curve for the loan data\n\n\nPRECISION-RECALL CURVE\nIn addition to ROC curves, it can be illuminating to examine the precision-recall (PR) curve. PR\ncurves are computed in a similar way except that the data is ordered from least to most probable\nand cumulative precision and recall statistics are computed. PR curves are especially useful in\nevaluating data with highly unbalanced outcomes.\n\n\nAUC\nThe ROC curve is a valuable graphical tool but, by itself, doesn’t constitute a\nsingle measure for the performance of a classifier. The ROC curve can be used,\nhowever, to produce the area underneath the curve (AUC) metric. AUC is simply\nthe total area under the ROC curve. The larger the value of AUC, the more\neffective the classifier. An AUC of 1 indicates a perfect classifier: it gets all the\n1s correctly classified, and doesn’t misclassify any 0s as 1s.\nA completely ineffective classifier — the diagonal line — will have an AUC of\n0.5.\nFigure 5-7 shows the area under the ROC curve for the loan model. The value of\nAUC can be computed by a numerical integration:\nsum(roc_df$recall[-1] * diff(1-roc_df$specificity))\n[1] 0.5924072\nThe model has an AUC of about 0.59, corresponding to a relatively weak\nclassifier.\n\n\nFigure 5-7. Area under the ROC curve for the loan data\n\n\nFALSE POSITIVE RATE CONFUSION\nFalse positive/negative rates are often confused or conflated with specificity or sensitivity (even\nin publications and software!). Sometimes the false positive rate is defined as the proportion of\ntrue negatives that test positive. In many cases (such as network intrusion detection), the term is\nused to refer to the proportion of positive signals that are true negatives.\n\n\nLift\nUsing the AUC as a metric is an improvement over simple accuracy, as it can\nassess how well a classifier handles the tradeoff between overall accuracy and\nthe need to identify the more important 1s. But it does not completely address the\nrare-case problem, where you need to lower the model’s probability cutoff below\n0.5 to avoid having all records classified as 0. In such cases, for a record to be\nclassified as a 1, it might be sufficient to have a probability of 0.4, 0.3, or lower.\nIn effect, we end up overidentifying 1s, reflecting their greater importance.\nChanging this cutoff will improve your chances of catching the 1s (at the cost of\nmisclassifying more 0s as 1s). But what is the optimum cutoff?\nThe concept of lift lets you defer answering that question. Instead, you consider\nthe records in order of their predicted probability of being 1s. Say, of the top 10%\nclassified as 1s, how much better did the algorithm do, compared to the\nbenchmark of simply picking blindly? If you can get 0.3% response in this top\ndecile instead of the 0.1% you get overall picking randomly, the algorithm is said\nto have a lift (also called gains) of 3 in the top decile. A lift chart (gains chart)\nquantifies this over the range of the data. It can be produced decile by decile, or\ncontinuously over the range of the data.\nTo compute a lift chart, you first produce a cumulative gains chart that shows the\nrecall on the y-axis and the total number of records on the x-axis. The lift curve is\nthe ratio of the cumulative gains to the diagonal line corresponding to random\nselection. Decile gains charts are one of the oldest techniques in predictive\nmodeling, dating from the days before internet commerce. They were particularly\npopular among direct mail professionals. Direct mail is an expensive method of\nadvertising if applied indiscriminantly, and advertisers used predictive models\n(quite simple ones, in the early days) to identify the potential customers with the\nlikeliest prospect of payoff.\n\n\nUPLIFT\nSometimes the term uplift is used to mean the same thing as lift. An alternate meaning is used in\na more restrictive setting, when an A-B test has been conducted and the treatment (A or B) is\nthen used as a predictor variable in a predictive model. The uplift is the improvement in response\npredicted for an individual case with treatment A versus treatment B. This is determined by\nscoring the individual case first with the predictor set to A, and then again with the predictor\ntoggled to B. Marketers and political campaign consultants use this method to determine which of\ntwo messaging treatments should be used with which customers or voters.\nA lift curve lets you look at the consequences of setting different probability\ncutoffs for classifying records as 1s. It can be an intermediate step in settling on an\nappropriate cutoff level. For example, a tax authority might only have a certain\namount of resources that it can spend on tax audits, and want to spend them on the\nlikeliest tax cheats. With its resource constraint in mind, the authority would use a\nlift chart to estimate where to draw the line between tax returns selected for audit\nand those left alone.\nKEY IDEAS FOR EVALUATING CLASSIFICATION MODELS\nAccuracy (the percent of predicted classifications that are correct) is but a first step in evaluating a\nmodel.\nOther metrics (recall, specificity, precision) focus on more specific performance characteristics\n(e.g., recall measures how good a model is at correctly identifying 1s).\nAUC (area under the ROC curve) is a common metric for the ability of a model to distinguish 1s\nfrom 0s.\nSimilarly, lift measures how effective a model is in identifying the 1s, and it is often calculated decile\nby decile, starting with the most probable 1s.\n\n\nFurther Reading\nEvaluation and assessment are typically covered in the context of a particular\nmodel (e.g., K-Nearest Neighbors or decision trees); three books that handle it in\nits own chapter are:\nData Mining, 3rd ed., by Ian Whitten, Elbe Frank, and Mark Hall (Morgan\nKaufmann, 2011).\nModern Data Science with R by Benjamin Baumer, Daniel Kaplan, and\nNicholas Horton (CRC Press, 2017).\nData Mining for Business Analytics, 3rd ed., by Galit Shmueli, Peter Bruce,\nand Nitin Patel (Wiley, 2016, with variants for R, Excel, and JMP).\nAn excellent treatment of cross-validation and resampling can be found in:\nAn Introduction to Statistical Learning by Gareth James, et al. (Springer,\n2013).\n\n\nStrategies for Imbalanced Data\nThe previous section dealt with evaluation of classification models using metrics\nthat go beyond simple accuracy, and are suitable for imbalanced data — data in\nwhich the outcome of interest (purchase on a website, insurance fraud, etc.) is\nrare. In this section, we look at additional strategies that can improve predictive\nmodeling performance with imbalanced data.\nKEY TERMS FOR IMBALANCED DATA\nUndersample\nUse fewer of the prevalent class records in the classification model.\nSynonym\nDownsample\nOversample\nUse more of the rare class records in the classification model, bootstrapping if necessary.\nSynonym\nUpsample\nUp weight or down weight\nAttach more (or less) weight to the rare (or prevalent) class in the model.\nData generation\nLike bootstrapping, except each new bootstrapped record is slightly different from its source.\nZ-score\nThe value that results after standardization.\nK\nThe number of neighbors considered in the nearest neighbor calculation.\n\n\nUndersampling\nIf you have enough data, as is the case with the loan data, one solution is to\nundersample (or downsample) the prevalent class, so the data to be modeled is\nmore balanced between 0s and 1s. The basic idea in undersampling is that the data\nfor the dominant class has many redundant records. Dealing with a smaller, more\nbalanced data set yields benefits in model performance, and makes it easier to\nprepare the data, and to explore and pilot models.\nHow much data is enough? It depends on the application, but in general, having\ntens of thousands of records for the less dominant class is enough. The more easily\ndistinguishable the 1s are from the 0s, the less data needed.\nThe loan data analyzed in “Logistic Regression” was based on a balanced training\nset: half of the loans were paid off and the other half were in default. The\npredicted values were similar: half of the probabilities were less than 0.5 and half\nwere greater than 0.5. In the full data set, only about 5% of the loans were in\ndefault:\nmean(loan_all_data$outcome == 'default')\n[1] 0.05024048\nWhat happens if we use the full data set to train the model?\nfull_model <- glm(outcome ~ payment_inc_ratio + purpose_ +\n                        home_ + emp_len_+ dti + revol_bal + revol_util,\n                      data=train_set, family='binomial')\npred <- predict(full_model)\nmean(pred > 0)\n[1] 0.00386009\nOnly 0.39% of the loans are predicted to be in default, or less than 1/12 of the\nexpected number. The loans that were paid off overwhelm the loans in default\nbecause the model is trained using all the data equally. Thinking about it\nintuitively, the presence of so many nondefaulting loans, coupled with the\ninevitable variability in predictor data, means that, even for a defaulting loan, the\nmodel is likely to find some nondefaulting loans that it is similar to, by chance.\nWhen a balanced sample was used, roughly 50% of the loans were predicted to be\nin default.\n\n\nOversampling and Up/Down Weighting\nOne criticism of the undersampling method is that it throws away data and is not\nusing all the information at hand. If you have a relatively small data set, and the\nrarer class contains a few hundred or a few thousand records, then undersampling\nthe dominant class has the risk of throwing out useful information. In this case,\ninstead of downsampling the dominant case, you should oversample (upsample)\nthe rarer class by drawing additional rows with replacement (bootstrapping).\nYou can achieve a similar effect by weighting the data. Many classification\nalgorithms take a weight argument that will allow you to up/down weight the data.\nFor example, apply a weight vector to the loan data using the weight argument to\nglm:\nwt <- ifelse(loan_all_data$outcome=='default',\n             1/mean(loan_all_data$outcome == 'default'), 1)\nfull_model <- glm(outcome ~ payment_inc_ratio + purpose_ +\n                    home_ + emp_len_+ dti + revol_bal + revol_util,\n                  data=loan_all_data, weight=wt, family='binomial')\npred <- predict(full_model)\nmean(pred > 0)\n[1] 0.4344177\nThe weights for loans that default are set to  where p is the probability of\ndefault. The nondefaulting loans have a weight of 1. The sum of the weights for the\ndefaulted loans and nondefaulted loans are roughly equal. The mean of the\npredicted values is now 43% instead of 0.39%.\nNote that weighting provides an alternative to both upsampling the rarer class and\ndownsampling the dominant class.\n\n\nADAPTING THE LOSS FUNCTION\nMany classification and regression algorithms optimize a certain criteria or loss function. For\nexample, logistic regression attempts to minimize the deviance. In the literature, some propose to\nmodify the loss function in order to avoid the problems caused by a rare class. In practice, this is\nhard to do: classification algorithms can be complex and difficult to modify. Weighting is an easy\nway to change the loss function, discounting errors for records with low weights in favor of\nrecords of higher weights.\n\n\nData Generation\nA variation of upsampling via bootstrapping (see “Undersampling”) is data\ngeneration by perturbing existing records to create new records. The intuition\nbehind this idea is that since we only observe a limited set of instances, the\nalgorithm doesn’t have a rich set of information to build classification “rules.” By\ncreating new records that are similar but not identical to existing records, the\nalgorithm has a chance to learn a more robust set of rules. This notion is similar in\nspirit to ensemble statistical models such as boosting and bagging (see Chapter 6).\nThe idea gained traction with the publication of the SMOTE algorithm, which\nstands for “Synthetic Minority Oversampling Technique.” The SMOTE algorithm\nfinds a record that is similar to the record being upsampled (see “K-Nearest\nNeighbors”) and creates a synthetic record that is a randomly weighted average of\nthe original record and the neighboring record, where the weight is generated\nseparately for each predictor. The number of synthetic oversampled records\ncreated depends on the oversampling ratio required to bring the data set into\napproximate balance, with respect to outcome classes.\nThere are several implementations of SMOTE in R. The most comprehensive\npackage for handling unbalanced data is unbalanced. It offers a variety of\ntechniques, including a “Racing” algorithm to select the best method. However,\nthe SMOTE algorithm is simple enough that it can be implemented directly in R\nusing the knn package.\n\n\nCost-Based Classification\nIn practice, accuracy and AUC are a poor man’s way to choose a classification\nrule. Often, an estimated cost can be assigned to false positives versus false\nnegatives, and it is more appropriate to incorporate these costs to determine the\nbest cutoff when classifying 1s and 0s. For example, suppose the expected cost of\na default of a new loan is \n and the expected return from a paid-off loan is \n.\nThen the expected return for that loan is:\nInstead of simply labeling a loan as default or paid off, or determining the\nprobability of default, it makes more sense to determine if the loan has a positive\nexpected return. Predicted probability of default is an intermediate step, and it\nmust be combined with the loan’s total value to determine expected profit, which\nis the ultimate planning metric of business. For example, a smaller value loan\nmight be passed over in favor of a larger one with a slightly higher predicted\ndefault probability.\n\n\nExploring the Predictions\nA single metric, such as AUC, cannot capture all aspects of the appropriateness of\na model for a situation. Figure 5-8 displays the decision rules for four different\nmodels fit to the loan data using just two predictor variables: borrower_score\nand payment_inc_ratio. The models are linear discriminant analysis (LDA),\nlogistic linear regression, logistic regression fit using a generalized additive\nmodel (GAM) and a tree model (see “Tree Models”). The region to the upper-left\nof the lines corresponds to a predicted default. LDA and logistic linear regression\ngive nearly identical results in this case. The tree model produces the least regular\nrule: in fact, there are situations in which increasing the borrower score shifts the\nprediction from “paid-off” to “default”! Finally, the GAM fit of the logistic\nregression represents a compromise between the tree models and the linear\nmodels.\n\n\nFigure 5-8. Comparison of the classification rules for four different methods\nIt is not easy to visualize the prediction rules in higher dimensions, or in the case\nof the GAM and tree model, even generate the regions for such rules.\nIn any case, exploratory analysis of predicted values is always warranted.\nKEY IDEAS FOR IMBALANCED DATA STRATEGIES\nHighly imbalanced data (i.e., where the interesting outcomes, the 1s, are rare) are problematic for\nclassification algorithms.\nOne strategy is to balance the training data via undersampling the abundant case (or oversampling\nthe rare case).\nIf using all the 1s still leaves you with too few 1s, you can bootstrap the rare cases, or use SMOTE\nto create synthetic data similar to existing rare cases.\nImbalanced data usually indicates that correctly classifying one class (the 1s) has higher value, and\nthat value ratio should be built into the assessment metric.\n\n\nFurther Reading\nTom Fawcett, author of Data Science for Business, has a good article on\nimbalanced classes.\nFor more on SMOTE, see Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.\nHall, and W. Philip Kegelmeyer, “SMOTE: Synthetic Minority Over-\nsampling Technique,” Journal of Artificial Intelligence Research 16 (2002):\n321–357.\nAlso see the Analytics Vidya Content Team’s “Practical Guide to deal with\nImbalanced Classification Problems in R,” March 28, 2016.\n\n\nSummary\nClassification, the process of predicting which of two (or a small number of)\ncategories a record belongs to, is a fundamental tool of predictive analytics. Will\na loan default (yes or no)? Will it prepay? Will a web visitor click on a link? Will\nshe purchase something? Is an insurance claim fraudulent? Often in classification\nproblems, one class is of primary interest (e.g., the fraudulent insurance claim)\nand, in binary classification, this class is designated as a 1, with the other, more\nprevalent class being a 0. Often, a key part of the process is estimating a\npropensity score, a probability of belonging to the class of interest. A common\nscenario is one in which the class of interest is relatively rare. The chapter\nconcludes with a discussion of a variety of model assessment metrics that go\nbeyond simple accuracy; these are important in the rare-class situation, when\nclassifying all records as 0s can yield high accuracy.\nThis and subsequent sections in this chapter © 2017 Datastats, LLC, Peter Bruce and Andrew Bruce,\nused by permission.\nIt is certainly surprising that the first article on statistical classification was published in a journal devoted to\neugenics. Indeed, there is a disconcerting connection between the early development of statistics and\neugenics.\nNot all methods provide unbiased estimates of probability. In most cases, it is sufficient that the method\nprovide a ranking equivalent to the rankings that would result from an unbiased probability estimate; the\ncutoff method is then functionally equivalent.\nThe ROC curve was first used during World War II to describe the performance of radar receiving\nstations, whose job was to correctly identify (classify) reflected radar signals, and alert defense forces to\nincoming aircraft.\n1\n2\n3\n4\n\n\nChapter 6. Statistical Machine\nLearning\nRecent advances in statistics have been devoted to developing more powerful\nautomated techniques for predictive modeling — both regression and\nclassification. These methods fall under the umbrella of statistical machine\nlearning, and are distinguished from classical statistical methods in that they are\ndata-driven and do not seek to impose linear or other overall structure on the data.\nThe K-Nearest Neighbors method, for example, is quite simple: classify a record\nin accordance with how similar records are classified. The most successful and\nwidely used techniques are based on ensemble learning applied to decision trees.\nThe basic idea of ensemble learning is to use many models to form a prediction as\nopposed to just a single model. Decision trees are a flexible and automatic\ntechnique to learn rules about the relationships between predictor variables and\noutcome variables. It turns out that the combination of ensemble learning with\ndecision trees leads to the top-performing off-the-shelf predictive modeling\ntechniques.\nThe development of many of the techniques in statistical machine learning can be\ntraced back to the statisticians Leo Breiman (see Figure 6-1) at the University of\nCalifornia at Berkeley and Jerry Friedman at Stanford University. Their work,\nalong with other researchers at Berkeley and Stanford, started with the\ndevelopment of tree models in 1984. The subsequent development of ensemble\nmethods of bagging and boosting in the 1990s established the foundation of\nstatistical machine learning.\n\n\nFigure 6-1. Leo Breiman, who was a professor of statistics at Berkeley, was at the forefront in\ndevelopment of many techniques at the core of a data scientist’s toolkit\n\n\nMACHINE LEARNING VERSUS STATISTICS\nIn the context of predictive modeling, what is the difference between machine learning and\nstatistics? There is not a bright line dividing the two disciplines. Machine learning tends to be\nmore focused on developing efficient algorithms that scale to large data in order to optimize the\npredictive model. Statistics generally pays more attention to the probabilistic theory and underlying\nstructure of the model. Bagging, and the random forest (see “Bagging and the Random Forest”),\ngrew up firmly in the statistics camp. Boosting (see “Boosting”), on the other hand, has been\ndeveloped in both disciplines but receives more attention on the machine learning side of the\ndivide. Regardless of the history, the promise of boosting ensures that it will thrive as a technique\nin both statistics and machine learning.\n\n\nK-Nearest Neighbors\nThe idea behind K-Nearest Neighbors (KNN) is very simple.1 For each record to\nbe classified or predicted:\n1. Find K records that have similar features (i.e., similar predictor values).\n2. For classification: Find out what the majority class is among those\nsimilar records, and assign that class to the new record.\n3. For prediction (also called KNN regression): Find the average among\nthose similar records, and predict that average for the new record.\nKEY TERMS FOR K-NEAREST NEIGHBORS\nNeighbor\nA record that has similar predictor values to another record.\nDistance metrics\nMeasures that sum up in a single number how far one record is from another.\nStandardization\nSubtract the mean and divide by the standard deviation.\nSynonym\nNormalization\nZ-score\nThe value that results after standardization.\nK\nThe number of neighbors considered in the nearest neighbor calculation.\nKNN is one of the simpler prediction/classification techniques: there is no model\nto be fit (as in regression). This doesn’t mean that using KNN is an automatic\nprocedure. The prediction results depend on how the features are scaled, how\nsimilarity is measured, and how big K is set. Also, all predictors must be in\nnumeric form. We will illustrate it with a classification example.\n\n\nA Small Example: Predicting Loan Default\nTable 6-1 shows a few records of personal loan data from the Lending Club.\nLending Club is a leader in peer-to-peer lending in which pools of investors make\npersonal loans to individuals. The goal of an analysis would be to predict the\noutcome of a new potential loan: paid-off versus default.\nTable 6-1. A few records and columns for Lending Club loan data\nOutcome Loan amount Income Purpose\nYears employed Home ownership State\nPaid off\n10000\n79100\ndebt_consolidation 11\nMORTGAGE\nNV\nPaid off\n9600\n48000\nmoving\n5\nMORTGAGE\nTN\nPaid off\n18800\n120036\ndebt_consolidation 11\nMORTGAGE\nMD\nDefault\n15250\n232000\nsmall_business\n9\nMORTGAGE\nCA\nPaid off\n17050\n35000\ndebt_consolidation 4\nRENT\nMD\nPaid off\n5500\n43000\ndebt_consolidation 4\nRENT\nKS\nConsider a very simple model with just two predictor variables: dti, which is the\nratio of debt payments (excluding mortgage) to income, and payment_inc_ratio,\nwhich is the ratio of the loan payment to income. Both ratios are multiplied by\n100. Using a small set of 200 loans, loan200, with known binary outcomes\n(default or no-default, specified in the predictor outcome200), and with K set to\n20, the KNN estimate for a new loan to be predicted, newloan, with dti=22.5\nand payment_inc_ratio=9 can be calculated in R as follows:\nlibrary(FNN)\nknn_pred <- knn(train=loan200, test=newloan, cl=outcome200, k=20)\nknn_pred == 'default'\n[1] TRUE\nThe KNN prediction is for the loan to default.\nWhile R has a native knn function, the contributed R package FNN, for Fast\nNearest Neighbor, scales to big data better and provides more flexibility.\n",
      "page_number": 310
    },
    {
      "number": 6,
      "title": "Statistical Machine",
      "start_page": 371,
      "end_page": 431,
      "detection_method": "regex_chapter_title",
      "content": "Figure 6-2 gives a visual display of this example. The new loan to be predicted is\nthe square in the middle. The circles (default) and triangles (paid off) are the\ntraining data. The black line shows the boundary of the nearest 20 points. In this\ncase, 14 defaulted loans lie within the circle as compared with only 6 paid-off\nloans. Hence, the predicted outcome of the loan is default.\nNOTE\nWhile the output of KNN for classification is typically a binary decision, such as default or paid\noff in the loan data, KNN routines usually offer the opportunity to output a probability (propensity)\nbetween 0 and 1. The probability is based on the fraction of one class in the K nearest neighbors.\nIn the preceding example, this probability of default would have been estimated at \n or 0.7.\nUsing a probability score lets you use classification rules other than simple majority votes\n(probability of 0.5). This is especially important in problems with imbalanced classes; see\n“Strategies for Imbalanced Data”. For example, if the goal is to identify members of a rare class,\nthe cutoff would typically be set below 50%. One common approach is to set the cutoff at the\nprobability of the rare event.\n\n\nFigure 6-2. KNN prediction of loan default using two variables: debt-to-income ratio and loan\npayment-to-income ratio\n\n\nDistance Metrics\nSimilarity (nearness) is determined using a distance metric, which is a function\nthat measures how far two records (x1, x2, … xp) and (u1, u2, … up) are from one\nanother. The most popular distance metric between two vectors is Euclidean\ndistance. To measure the Euclidean distance between two vectors, subtract one\nfrom the other, square the differences, sum them, and take the square root:\nEuclidean distance offers special computational advantages. This is particularly\nimportant for large data sets since KNN involves K × n pairwise comparisons\nwhere n is the number of rows.\nAnother common distance metric for numeric data is Manhattan distance:\nEuclidean distance corresponds to the straight-line distance between two points\n(e.g., as the crow flies). Manhattan distance is the distance between two points\ntraversed in a single direction at a time (e.g., traveling along rectangular city\nblocks). For this reason, Manhattan distance is a useful approximation if similarity\nis defined as point-to-point travel time.\nIn measuring distance between two vectors, variables (features) that are measured\nwith comparatively large scale will dominate the measure. For example, for the\nloan data, the distance would be almost solely a function of the income and loan\namount variables, which are measured in tens or hundreds of thousands. Ratio\nvariables would count for practically nothing in comparison. We address this\nproblem by standardizing the data; see “Standardization (Normalization, Z-\nScores)”.\n\n\nOTHER DISTANCE METRICS\nThere are numerous other metrics for measuring distance between vectors. For numeric data,\nMahalanobis distance is attractive since it accounts for the correlation between two variables.\nThis is useful since if two variables are highly correlated, Mahalanobis will essentially treat these\nas a single variable in terms of distance. Euclidean and Manhattan distance do not account for the\ncorrelation, effectively placing greater weight on the attribute that underlies those features. The\ndownside of using Mahalanobis distance is increased computational effort and complexity; it is\ncomputed using the covariance matrix; see “Covariance Matrix”.\n\n\nOne Hot Encoder\nThe loan data in Table 6-1 includes several factor (string) variables. Most\nstatistical and machine learning models require this type of variable to be\nconverted to a series of binary dummy variables conveying the same information,\nas in Table 6-2. Instead of a single variable denoting the home occupant status as\n“owns with a mortage,” “owns with no mortgage,” “rents,” or “other,” we end up\nwith four binary variables. The first would be “owns with a mortgage — Y/N,”\nthe second would be “owns with no mortgage — Y/N,” and so on. This one\npredictor, home occupant status, thus yields a vector with one 1 and three 0s, that\ncan be used in statistical and machine learning algorithms. The phrase one hot\nencoding comes from digital circuit terminology, where it describes circuit\nsettings in which only one bit is allowed to be positive (hot).\nTable 6-2. Representing home\nownership factor data as a\nnumeric dummy variable\nMORTGAGE OTHER OWN RENT\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\nNOTE\nIn linear and logistic regression, one hot encoding causes problems with multicollinearity; see\n“Multicollinearity”. In such cases, one dummy is omitted (its value can be inferred from the other\nvalues). This is not an issue with KNN and other methods.\n\n\nStandardization (Normalization, Z-Scores)\nIn measurement, we are often not so much interested in “how much” but “how\ndifferent from the average.” Standardization, also called normalization, puts all\nvariables on similar scales by subtracting the mean and dividing by the standard\ndeviation. In this way, we ensure that a variable does not overly influence a model\nsimply due to the scale of its original measurement.\nThese are commonly refered to as z-scores. Measurements are then stated in terms\nof “standard deviations away from the mean.” In this way, a variable’s impact on\na model is not affected by the scale of its original measurement.\nCAUTION\nNormalization in this statistical context is not to be confused with database normalization,\nwhich is the removal of redundant data and the verification of data dependencies.\nFor KNN and a few other procedures (e.g., principal components analysis and\nclustering), it is essential to consider standardizing the data prior to applying the\nprocedure. To illustrate this idea, KNN is applied to the loan data using dti and\npayment_inc_ratio (see “A Small Example: Predicting Loan Default”) plus two\nother variables: revol_bal, the total revolving credit available to the applicant in\ndollars, and revol_util, the percent of the credit being used. The new record to\nbe predicted is shown here:\nnewloan\n  payment_inc_ratio dti revol_bal revol_util\n1            2.3932   1      1687        9.4\nThe magnitude of revol_bal, which is in dollars, is much bigger than the other\nvariables. The knn function returns the index of the nearest neighbors as an\nattribute nn.index, and this can be used to show the top-five closest rows in\n\n\nloan_df:\nloan_df <- model.matrix(~ -1 + payment_inc_ratio + dti + revol_bal +\n                           revol_util, data=loan_data)\nknn_pred <- knn(train=loan_df, test=newloan, cl=outcome, k=5)\nloan_df[attr(knn_pred,\"nn.index\"),]\n      payment_inc_ratio  dti revol_bal revol_util\n36054           2.22024 0.79      1687        8.4\n33233           5.97874 1.03      1692        6.2\n28989           5.65339 5.40      1694        7.0\n29572           5.00128 1.84      1695        5.1\n20962           9.42600 7.14      1683        8.6\nThe value of revol_bal in these neighbors is very close to its value in the new\nrecord, but the other predictor variables are all over the map and essentially play\nno role in determining neighbors.\nCompare this to KNN applied to the standardized data using the R function scale,\nwhich computes the z-score for each variable:\nloan_std <- scale(loan_df)\nknn_pred <- knn(train=loan_std, test=newloan_std, cl=outcome, k=5)\nloan_df[attr(knn_pred,\"nn.index\"),]\n      payment_inc_ratio  dti revol_bal revol_util\n2081            2.61091 1.03      1218        9.7\n36054           2.22024 0.79      1687        8.4\n23655           2.34286 1.12       523       10.7\n41327           2.15987 0.69      2115        8.1\n39555           2.76891 0.75      2129        9.5\nThe five nearest neighbors are much more alike in all the variables providing a\nmore sensible result. Note that the results are displayed on the original scale, but\nKNN was applied to the scaled data and the new loan to be predicted.\nTIP\nUsing the z-score is just one way to rescale variables. Instead of the mean, a more robust\nestimate of location could be used, such as the median. Likewise, a different estimate of scale\nsuch as the interquartile range could be used instead of the standard deviation. Sometimes,\nvariables are “squashed” into the 0–1 range. It’s also important to realize that scaling each\nvariable to have unit variance is somewhat arbitrary. This implies that each variable is thought to\nhave the same importance in predictive power. If you have subjective knowledge that some\nvariables are more important than others, then these could be scaled up. For example, with the\nloan data, it is reasonable to expect that the payment-to-income ratio is very important.\n\n\nNOTE\nNormalization (standardization) does not change the distributional shape of the data; it does not\nmake it normally shaped if it was not already normally shaped (see “Normal Distribution”).\n\n\nChoosing K\nThe choice of K is very important to the performance of KNN. The simplest\nchoice is to set \n, known as the 1-nearest neighbor classifier. The\nprediction is intuitive: it is based on finding the data record in the training set\nmost similar to the new record to be predicted. Setting \n is rarely the best\nchoice; you’ll almost always obtain superior performance by using K > 1-nearest\nneighbors.\nGenerally speaking, if K is too low, we may be overfitting: including the noise in\nthe data. Higher values of K provide smoothing that reduces the risk of overfitting\nin the training data. On the other hand, if K is too high, we may oversmooth the\ndata and miss out on KNN’s ability to capture the local structure in the data, one\nof its main advantages.\nThe K that best balances between overfitting and oversmoothing is typically\ndetermined by accuracy metrics and, in particular, accuracy with holdout or\nvalidation data. There is no general rule about the best K — it depends greatly on\nthe nature of the data. For highly structured data with little noise, smaller values of\nK work best. Borrowing a term from the signal processing community, this type of\ndata is sometimes referred to as having a high signal-to-noise ratio (SNR).\nExamples of data with typically high SNR are handwriting and speech\nrecognition. For noisy data with less structure (data with a low SNR), such as the\nloan data, larger values of K are appropriate. Typically, values of K fall in the\nrange 1 to 20. Often, an odd number is chosen to avoid ties.\n\n\nBIAS-VARIANCE TRADEOFF\nThe tension between oversmoothing and overfitting is an instance of the bias-variance tradeoff,\nan ubiquitous problem in statistical model fitting. Variance refers to the modeling error that occurs\nbecause of the choice of training data; that is, if you were to choose a different set of training\ndata, the resulting model would be different. Bias refers to the modeling error that occurs\nbecause you have not properly identified the underlying real-world scenario; this error would not\ndisappear if you simply added more training data. When a flexible model is overfit, the variance\nincreases. You can reduce this by using a simpler model, but the bias may increase due to the loss\nof flexibility in modeling the real underlying situation. A general approach to handling this tradeoff\nis through cross-validation. See “Cross-Validation” for more details.\n\n\nKNN as a Feature Engine\nKNN gained its popularity due to its simplicity and intuitive nature. In terms of\nperformance, KNN by itself is usually not competitive with more sophisticated\nclassification techniques. In practical model fitting, however, KNN can be used to\nadd “local knowledge” in a staged process with other classification techniques.\n1. KNN is run on the data, and for each record, a classification (or quasi-\nprobability of a class) is derived.\n2. That result is added as a new feature to the record, and another\nclassification method is then run on the data. The original predictor\nvariables are thus used twice.\nAt first you might wonder whether this process, since it uses some predictors\ntwice, causes a problem with multicollinearity (see “Multicollinearity”). This is\nnot an issue, since the information being incorporated into the second-stage model\nis highly local, derived only from a few nearby records, and is therefore\nadditional information, and not redundant.\nNOTE\nYou can think of this staged use of KNN as a form of ensemble learning, in which multiple\npredictive modeling methods are used in conjunction with one another. It can also be considered\nas a form of feature engineering where the aim is to derive features (predictor variables) that\nhave predictive power. Often this involves some manual review of the data; KNN gives a fairly\nautomatic way to do this.\nFor example, consider the King County housing data. In pricing a home for sale, a\nrealtor will base the price on similar homes recently sold, known as “comps.” In\nessence, realtors are doing a manual version of KNN: by looking at the sale prices\nof similar homes, they can estimate what a home will sell for. We can create a\nnew feature for a statistical model to mimic the real estate professional by\napplying KNN to recent sales. The predicted value is the sales price and the\nexisting predictor variables could include location, total square feet, type of\nstructure, lot size, and number of bedrooms and bathrooms. The new predictor\nvariable (feature) that we add via KNN is the KNN predictor for each record\n\n\n(analogous to the realtors’ comps). Since we are predicting a numerical value, the\naverage of the K-Nearest Neighbors is used instead of a majority vote (known as\nKNN regression).\nSimilarly, for the loan data, we can create features that represent different aspects\nof the loan process. For example, the following would build a feature that\nrepresents a borrower’s creditworthiness:\nborrow_df <- model.matrix(~ -1 + dti + revol_bal + revol_util + open_acc +\n                          delinq_2yrs_zero + pub_rec_zero, data=loan_data)\nborrow_knn <- knn(borrow_df, test=borrow_df, cl=loan_data[, 'outcome'],\n                prob=TRUE, k=10)\nprob <- attr(borrow_knn, \"prob\")\nborrow_feature <- ifelse(borrow_knn=='default', prob, 1-prob)\nsummary(borrow_feature)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n 0.0000  0.4000  0.5000  0.5012  0.6000  1.0000\nThe result is a feature that predicts the likelihood a borrower will default based\non his credit history.\nKEY IDEAS FOR K-NEAREST NEIGHBORS\nK-Nearest Neighbors (KNN) classifies a record by assigning it to the class that similar records\nbelong to.\nSimilarity (distance) is determined by Euclidian distance or other related metrics.\nThe number of nearest neighbors to compare a record to, K, is determined by how well the\nalgorithm performs on training data, using different values for K.\nTypically, the predictor variables are standardized so that variables of large scale do not dominate\nthe distance metric.\nKNN is often used as a first stage in predictive modeling, and the predicted value is added back\ninto the data as a predictor for second-stage (non-KNN) modeling.\n\n\nTree Models\nTree models, also called Classification and Regression Trees (CART),2 decision\ntrees, or just trees, are an effective and popular classification (and regression)\nmethod initially developed by Leo Breiman and others in 1984. Tree models, and\ntheir more powerful descendents random forests and boosting (see “Bagging and\nthe Random Forest” and “Boosting”), form the basis for the most widely used and\npowerful predictive modeling tools in data science for both regression and\nclassification.\nKEY TERMS FOR TREES\nRecursive partitioning\nRepeatedly dividing and subdividing the data with the goal of making the outcomes in each final\nsubdivision as homogeneous as possible.\nSplit value\nA predictor value that divides the records into those where that predictor is less than the split\nvalue, and those where it is more.\nNode\nIn the decision tree, or in the set of corresponding branching rules, a node is the graphical or rule\nrepresentation of a split value.\nLeaf\nThe end of a set of if-then rules, or branches of a tree — the rules that bring you to that leaf\nprovide one of the classification rules for any record in a tree.\nLoss\nThe number of misclassifications at a stage in the splitting process; the more losses, the more\nimpurity.\nImpurity\nThe extent to which a mix of classes is found in a subpartition of the data (the more mixed, the\nmore impure).\nSynonym\nHeterogeneity\nAntonym\nHomogeneity, purity\nPruning\nThe process of taking a fully grown tree and progressively cutting its branches back, to reduce\noverfitting.\n\n\nA tree model is a set of “if-then-else” rules that are easy to understand and to\nimplement. In contrast to regression and logistic regression, trees have the ability\nto discover hidden patterns corresponding to complex interactions in the data.\nHowever, unlike KNN or naive Bayes, simple tree models can be expressed in\nterms of predictor relationships that are easily interpretable.\n\n\nDECISION TREES IN OPERATIONS RESEARCH\nThe term decision trees has a different (and older) meaning in decision science and operations\nresearch, where it refers to a human decision analysis process. In this meaning, decision points,\npossible outcomes, and their estimated probabilities are laid out in a branching diagram, and the\ndecision path with the maximum expected value is chosen.\n\n\nA Simple Example\nThe two main packages to fit tree models in R are rpart and tree. Using the\nrpart package, a model is fit to a sample of 3,000 records of the loan data using\nthe variables payment_inc_ratio and borrower_score (see “K-Nearest\nNeighbors” for a description of the data).\nlibrary(rpart)\nloan_tree <- rpart(outcome ~ borrower_score + payment_inc_ratio,\n                   data=loan_data, control = rpart.control(cp=.005))\nplot(loan_tree, uniform=TRUE, margin=.05)\ntext(loan_tree)\nThe resulting tree is shown in Figure 6-3. These classification rules are\ndetermined by traversing through a hierarchical tree, starting at the root until a leaf\nis reached.\nFigure 6-3. The rules for a simple tree model fit to the loan data\nTypically, the tree is plotted upside-down, so the root is at the top and the leaves\nare at the bottom. For example, if we get a loan with borrower_score of 0.6 and\n\n\na payment_inc_ratio of 8.0, we end up at the leftmost leaf and predict the loan\nwill be paid off.\nA nicely printed version of the tree is also easily produced:\nloan_tree\nn= 3000\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n 1) root 3000 1467 paid off (0.5110000 0.4890000)\n   2) borrower_score>=0.525 1283  474 paid off (0.6305534 0.3694466)\n     4) payment_inc_ratio< 8.772305 845  249 paid off (0.7053254 0.2946746) *\n     5) payment_inc_ratio>=8.772305 438  213 default (0.4863014 0.5136986)\n      10) borrower_score>=0.625 149   60 paid off (0.5973154 0.4026846) *\n      11) borrower_score< 0.625 289  124 default (0.4290657 0.5709343) *\n   3) borrower_score< 0.525 1717  724 default (0.4216657 0.5783343)\n     6) payment_inc_ratio< 9.73236 1082  517 default (0.4778189 0.5221811)\n      12) borrower_score>=0.375 784  384 paid off (0.5102041 0.4897959) *\n      13) borrower_score< 0.375 298  117 default (0.3926174 0.6073826) *\n     7) payment_inc_ratio>=9.73236 635  207 default (0.3259843 0.6740157) *\nThe depth of the tree is shown by the indent. Each node corresponds to a\nprovisional classification determined by the prevalent outcome in that partition.\nThe “loss” is the number of misclassifications yielded by the provisional\nclassification in a partition. For example, in node 2, there were 474\nmisclassification out of a total of 1,467 total records. The values in the\nparentheses correspond to the proportion of records that are paid off and default,\nrespectively. For example, in node 13, which predicts default, over 60 percent of\nthe records are loans that are in default.\n\n\nThe Recursive Partitioning Algorithm\nThe algorithm to construct a decision tree, called recursive partitioning , is\nstraightforward and intuitive. The data is repeatedly partitioned using predictor\nvalues that do the best job of separating the data into relatively homogeneous\npartitions. Figure 6-4 shows a picture of the partitions created for the tree in\nFigure 6-3. The first rule is borrower_score >= 0.525 and is depicted by rule 1\nin the plot. The second rule is payment_inc_ratio < 9.732 and divides the\nrighthand region in two.\n\n\nFigure 6-4. The rules for a simple tree model fit to the loan data\nSuppose we have a response variable Y and a set of P predictor variables Xj for \n. For a partition A of records, recursive partitioning will find\nthe best way to partition A into two subpartitions:\n1. For each predictor variable Xj,\na. For each value sj of Xj:\ni. Split the records in A with Xj values < sj as one partition, and\nthe remaining records where Xj ≥ sj as another partition.\nii. Measure the homogeneity of classes within each subpartition of\nA.\nb. Select the value of sj that produces maximum within-partition\nhomogeneity of class.\n2. Select the variable Xj and the split value sj that produces maximum\n\n\nwithin-partition homogeneity of class.\nNow comes the recursive part:\n1. Initialize A with the entire data set.\n2. Apply the partitioning algorithm to split A into two subpartitions, A1 and\nA2.\n3. Repeat step 2 on subpartitions A1 and A2.\n4. The algorithm terminates when no further partition can be made that\nsufficiently improves the homogeneity of the partitions.\nThe end result is a partitioning of the data, as in Figure 6-4 except in P-\ndimensions, with each partition predicting an outcome of 0 or 1 depending on the\nmajority vote of the reponse in that partition.\nNOTE\nIn addition to a binary 0/1 prediction, tree models can produce a probability estimate based on the\nnumber of 0s and 1s in the partition. The estimate is simply the sum of 0s or 1s in the partition\ndivided by the number of observations in the partition.\nThe estimated \n can then be converted to a binary decision; for example,\nset the estimate to 1 if Prob(Y = 1) > 0.5.\n\n\nMeasuring Homogeneity or Impurity\nTree models recursively create partitions (sets of records), A, that predict an\noutcome of Y = 0 or Y = 1. You can see from the preceding algorithm that we need\na way to measure homogeneity, also called class purity, within a partition. Or,\nequivalently, we need to measure the impurity of a partition. The accuracy of the\npredictions is the proportion p of misclassified records within that partition,\nwhich ranges from 0 (perfect) to 0.5 (purely random guessing).\nIt turns out that accuracy is not a good measure for impurity. Instead, two common\nmeasures for impurity are the Gini impurity and entropy or information. While\nthese (and other) impurity measures apply to classification problems with more\nthan two classes, we focus on the binary case. The Gini impurity for a set of\nrecords A is:\nThe entropy measure is given by:\nFigure 6-5 shows that Gini impurity (rescaled) and entropy measures are similar,\nwith entropy giving higher impurity scores for moderate and high accuracy rates.\n\n\nFigure 6-5. Gini impurity and entropy measures\n\n\nGINI COEFFICIENT\nGini impurity is not to be confused with the Gini coefficient. They represent similar concepts, but\nthe Gini coefficient is limited to the binary classification problem and is related to the AUC metric\n(see “AUC”).\nThe impurity metric is used in the splitting algorithm described earlier. For each\nproposed partition of the data, impurity is measured for each of the partitions that\nresult from the split. A weighted average is then calculated, and whichever\npartition (at each stage) yields the lowest weighted average is selected.\n\n\nStopping the Tree from Growing\nAs the tree grows bigger, the splitting rules become more detailed, and the tree\ngradually shifts from identifying “big” rules that identify real and reliable\nrelationships in the data to “tiny” rules that reflect only noise. A fully grown tree\nresults in completely pure leaves and, hence, 100% accuracy in classifying the\ndata that it is trained on. This accuracy is, of course, illusory — we have overfit\n(see Bias-Variance Tradeoff) the data, fitting the noise in the training data, not the\nsignal that we want to identify in new data.\n\n\nPRUNING\nA simple and intuitive method of reducing tree size is to prune back the terminal and smaller\nbranches of the tree, leaving a smaller tree. How far should the pruning proceed? A common\ntechnique is to prune the tree back to the point where the error on holdout data is minimized.\nWhen we combine predictions from multiple trees (see “Bagging and the Random Forest”),\nhowever, we will need a way to stop tree growth. Pruning plays a role in the process of cross-\nvalidation to determine how far to grow trees that are used in ensemble methods.\nWe need some way to determine when to stop growing a tree at a stage that will\ngeneralize to new data. There are two common ways to stop splitting:\nAvoid splitting a partition if a resulting subpartition is too small, or if a\nterminal leaf is too small. In rpart, these constraints are controlled\nseparately by the parameters minsplit and minbucket, respectively, with\ndefaults of 20 and 7.\nDon’t split a partition if the new partition does not “significantly” reduce the\nimpurity. In rpart, this is controlled by the complexity parameter cp, which\nis a measure of how complex a tree is — the more complex, the greater the\nvalue of cp. In practice, cp is used to limit tree growth by attaching a penalty\nto additional complexity (splits) in a tree.\nThe first method involves arbitrary rules, and can be usful for exploratory work,\nbut we can’t easily determine optimum values (i.e., values that maximize\npredictive accuracy with new data). With the complexity parameter, cp, we can\nestimate what size tree will perform best with new data.\nIf cp is too small, then the tree will overfit the data, fitting noise and not signal.\nOn the other hand, if cp is too large, then the tree will be too small and have little\npredictive power. The default in rpart is 0.01, although for larger data sets, you\nare likely to find this is too large. In the previous example, cp was set to 0.005\nsince the default led to a tree with a single split. In exploratory analysis, it is\nsufficient to simply try a few values.\nDetermining the optimum cp is an instance of the bias-variance tradeoff (see Bias-\nVariance Tradeoff). The most common way to estimate a good value of cp is via\n\n\ncross-validation (see “Cross-Validation”):\n1. Partition the data into training and validation (holdout) sets.\n2. Grow the tree with the training data.\n3. Prune it successively, step by step, recording cp (using the training data)\nat each step.\n4. Note the cp that corresponds to the minimum error (loss) on the\nvalidation data.\n5. Repartition the data into training and validation, and repeat the growing,\npruning, and cp recording process.\n6. Do this again and again, and average the cps that reflect minimum error\nfor each tree.\n7. Go back to the original data, or future data, and grow a tree, stopping at\nthis optimum cp value.\nIn rpart, you can use the argument cptable to produce a table of the CP values\nand their associated cross-validation error (xerror in R), from which you can\ndetermine the CP value that has the lowest cross-validation error.\n\n\nPredicting a Continuous Value\nPredicting a continuous value (also termed regression) with a tree follows the\nsame logic and procedure, except that impurity is measured by squared deviations\nfrom the mean (squared errors) in each subpartition, and predictive performance\nis judged by the square root of the mean squared error (RMSE) (see “Assessing\nthe Model”) in each partition.\n\n\nHow Trees Are Used\nOne of the big obstacles faced by predictive modelers in organizations is the\nperceived “black box” nature of the methods they use, which gives rise to\nopposition from other elements of the organization. In this regard, the tree model\nhas two appealing aspects.\nTree models provide a visual tool for exploring the data, to gain an idea of\nwhat variables are important and how they relate to one another. Trees can\ncapture nonlinear relationships among predictor variables.\nTree models provide a set of rules that can be effectively communicated to\nnonspecialists, either for implementation or to “sell” a data mining project.\nWhen it comes to prediction, however, harnassing the results from multiple trees\nis typically more powerful than just using a single tree. In particular, the random\nforest and boosted tree algorithms almost always provide superior predictive\naccuracy and performance (see “Bagging and the Random Forest” and\n“Boosting”), but the aforementioned advantages of a single tree are lost.\nKEY IDEAS\nDecision trees produce a set of rules to classify or predict an outcome.\nThe rules correspond to successive partitioning of the data into subpartitions.\nEach partition, or split, references a specific value of a predictor variable and divides the data into\nrecords where that predictor value is above or below that split value.\nAt each stage, the tree algorithm chooses the split that minimizes the outcome impurity within each\nsubpartition.\nWhen no further splits can be made, the tree is fully grown and each terminal node, or leaf, has\nrecords of a single class; new cases following that rule (split) path would be assigned that class.\nA fully grown tree overfits the data and must be pruned back so that it captures signal and not\nnoise.\nMultiple-tree algorithms like random forests and boosted trees yield better predictive performance,\nbut lose the rule-based communicative power of single trees.\n\n\nFurther Reading\nAnalytics Vidhya Content Team, “A Complete Tutorial on Tree Based\nModeling from Scratch (in R & Python)”, April 12, 2016.\nTerry M. Therneau, Elizabeth J. Atkinson, and the Mayo Foundation, “An\nIntroduction to Recursive Partitioning Using the RPART Routines”, June 29,\n2015.\n\n\nBagging and the Random Forest\nIn 1907, the statistician Sir Francis Galton was visiting a county fair in England,\nat which a contest was being held to guess the dressed weight of an ox that was on\nexhibit. There were 800 guesses, and, while the individual guesses varied widely,\nboth the mean and the median came out within 1% of the ox’s true weight. James\nSuroweicki has explored this phenomenon in his book The Wisdom of Crowds\n(Doubleday, 2004). This principle applies to predictive models, as well:\naveraging (or taking majority votes) of multiple models — an ensemble of models\n— turns out to be more accurate than just selecting one model.\nKEY TERMS FOR BAGGING AND THE RANDOM FOREST\nEnsemble\nForming a prediction by using a collection of models.\nSynonym\nModel averaging\nBagging\nA general technique to form a collection of models by bootstrapping the data.\nSynonym\nBootstrap aggregation\nRandom forest\nA type of bagged estimate based on decision tree models.\nSynonym\nBagged decision trees\nVariable importance\nA measure of the importance of a predictor variable in the performance of the model.\nThe ensemble approach has been applied to and across many different modeling\nmethods, most publicly in the Netflix Contest, in which Netflix offered a $1\nmillion prize to any contestant who came up with a model that produced a 10%\nimprovement in predicting the rating that a Netflix customer would award a\nmovie. The simple version of ensembles is as follows:\n1. Develop a predictive model and record the predictions for a given data\n\n\nset.\n2. Repeat for multiple models, on the same data.\n3. For each record to be predicted, take an average (or a weighted average,\nor a majority vote) of the predictions.\nEnsemble methods have been applied most systematically and effectively to\ndecision trees. Ensemble tree models are so powerful that they provide a way to\nbuild good predictive models with relatively little effort.\nGoing beyond the simple ensemble algorithm, there are two main variants of\nensemble models: bagging and boosting. In the case of ensemble tree models,\nthese are refered to as random forest models and boosted tree models. This\nsection focuses on bagging; boosting is covered in “Boosting”.\n\n\nBagging\nBagging, which stands for “bootstrap aggregating,” was introduced by Leo\nBreiman in 1994. Suppose we have a response Y and P predictor variables \n with n records.\nBagging is like the basic algorithm for ensembles, except that, instead of fitting the\nvarious models to the same data, each new model is fit to a bootstrap resample.\nHere is the algorithm presented more formally:\n1. Initialize M, the number of models to be fit, and n, the number of records\nto choose (n < N). Set the iteration \n.\n2. Take a bootstrap resample (i.e., with replacement) of n records from the\ntraining data to form a subsample \n and \n (the bag).\n3. Train a model using \n and \n to create a set of decision rules \n.\n4. Increment the model counter \n. If m <= M, go to step 1.\nIn the case where \n predicts the probability \n, the bagged estimate is\ngiven by:\n\n\nRandom Forest\nThe random forest is based on applying bagging to decision trees with one\nimportant extension: in addition to sampling the records, the algorithm also\nsamples the variables.3 In traditional decision trees, to determine how to create a\nsubpartition of a partition A, the algorithm makes the choice of variable and split\npoint by minimizing a criterion such as Gini impurity (see “Measuring\nHomogeneity or Impurity”). With random forests, at each stage of the algorithm,\nthe choice of variable is limited to a random subset of variables. Compared to\nthe basic tree algorithm (see “The Recursive Partitioning Algorithm”), the random\nforest algorithm adds two more steps: the bagging discussed earlier (see “Bagging\nand the Random Forest”), and the bootstrap sampling of variables at each split:\n1. Take a bootstrap (with replacement) subsample from the records.\n2. For the first split, sample p < P variables at random without\nreplacement.\n3. For each of the sampled variables \n, apply the\nsplitting algorithm:\na. For each value \n of \n:\ni. Split the records in partition A with Xj(k) < sj(k) as one partition,\nand the remaining records where \n as another\npartition.\nii. Measure the homogeneity of classes within each subpartition of\nA.\nb. Select the value of \n that produces maximum within-partition\nhomogeneity of class.\n4. Select the variable \n and the split value \n that produces maximum\nwithin-partition homogeneity of class.\n5. Proceed to the next split and repeat the previous steps, starting with step\n2.\n\n\n6. Continue with additional splits following the same procedure until the\ntree is grown.\n7. Go back to step 1, take another bootstrap subsample, and start the\nprocess over again.\nHow many variables to sample at each step? A rule of thumb is to choose \nwhere P is the number of predictor variables. The package randomForest\nimplements the random forest in R. The following applies this package to the loan\ndata (see “K-Nearest Neighbors” for a description of the data).\n> library(randomForest)\n> rf <- randomForest(outcome ~ borrower_score + payment_inc_ratio,\n                     data=loan3000)\nCall:\n randomForest(formula = outcome ~ borrower_score + payment_inc_ratio,\n              data = loan3000)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n        OOB estimate of  error rate: 38.53%\nConfusion matrix:\n         paid off default class.error\npaid off     1089     425   0.2807133\ndefault       731     755   0.4919246\nBy default, 500 trees are trained. Since there are only two variables in the\npredictor set, the algorithm randomly selects the variable on which to split at each\nstage (i.e., a bootstrap subsample of size 1).\nThe out-of-bag (OOB) estimate of error is the error rate for the trained models,\napplied to the data left out of the training set for that tree. Using the output from the\nmodel, the OOB error can be plotted versus the number of trees in the random\nforest:\nerror_df = data.frame(error_rate = rf$err.rate[,'OOB'],\n                      num_trees = 1:rf$ntree)\nggplot(error_df, aes(x=num_trees, y=error_rate)) +\n  geom_line()\nThe result is shown in Figure 6-6. The error rate rapidly decreases from over .44\nbefore stabilizing around .385. The predicted values can be obtained from the\npredict function and plotted as follows:\n\n\npred <- predict(loan_lda)\nrf_df <- cbind(loan3000, pred_default=pred[,'default']>.5)\nggplot(data=rf_df, aes(x=borrower_score, y=payment_inc_ratio,\n                       color=pred_default, shape=pred_default)) +\n  geom_point(alpha=.6, size=2) +\n  scale_shape_manual( values=c( 46, 4))\n\n\nFigure 6-6. The improvement in accuracy of the random forest with the addition of more trees\nThe plot, shown in Figure 6-7, is quite revealing about the nature of the random\nforest.\n\n\nFigure 6-7. The predicted outcomes from the random forest applied to the loan default data\nThe random forest method is a “black box” method. It produces more accurate\npredictions than a simple tree, but the simple tree’s intuitive decision rules are\nlost. The predictions are also somewhat noisy: note that some borrowers with a\nvery high score, indicating high creditworthiness, still end up with a prediction of\ndefault. This is a result of some unusual records in the data and demonstrates the\ndanger of overfitting by the random forest (see Bias-Variance Tradeoff).\n\n\nVariable Importance\nThe power of the random forest algorithm shows itself when you build predictive\nmodels for data with many features and records. It has the ability to automatically\ndetermine which predictors are important and discover complex relationships\nbetween predictors corresponding to interaction terms (see “Interactions and Main\nEffects”). For example, fit a model to the loan default data with all columns\nincluded:\n> rf_all <- randomForest(outcome ~ ., data=loan_data, importance=TRUE)\n> rf_all\nCall:\n randomForest(formula = outcome ~ ., data = loan_data, importance = TRUE)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 3\n        OOB estimate of  error rate: 34.38%\nConfusion matrix:\n         paid off default class.error\npaid off    15078    8058   0.3482884\ndefault      7849   15287   0.3392548\nThe argument importance=TRUE requests that the randomForest store additional\ninformation about the importance of different variables. The function varImpPlot\nwill plot the relative performance of the variables:\nvarImpPlot(rf_all, type=1)\nvarImpPlot(rf_all, type=2)\nThe result is shown in Figure 6-8.\n\n\nFigure 6-8. The importance of variables for the full model fit to the loan data\nThere are two ways to measure variable importance:\nBy the decrease in accuracy of the model if the values of a variable are\nrandomly permuted (type=1). Randomly permuting the values has the effect\nof removing all predictive power for that variable. The accuracy is computed\nfrom the out-of-bag data (so this measure is effectively a cross-validated\nestimate).\nBy the mean decrease in the Gini impurity score (see “Measuring\nHomogeneity or Impurity”) for all of the nodes that were split on a variable\n(type=2). This measures how much improvement to the purity of the nodes\nthat variable contributes. This measure is based on the training set, and\ntherefore less reliable than a measure calculated on out-of-bag data.\nThe top and bottom panels of Figure 6-8 show variable importance according to\nthe decrease in accuracy and in Gini impurity, respectively. The variables in both\npanels are ranked by the decrease in accuracy. The variable importance scores\nproduced by these two measures are quite different.\nSince the accuracy decrease is a more reliable metric, why should we use the Gini\nimpurity decrease measure? By default, randomForest only computes this Gini\nimpurity: Gini impurity is a byproduct of the algorithm, whereas model accuracy\nby variable requires extra computations (randomly permuting the data and\npredicting this data). In cases where computational complexity is important, such\nas in a production setting where thousands of models are being fit, it may not be\nworth the extra computational effort. In addition, the Gini decrease sheds light on\nwhich variables the random forest is using to make its splitting rules (recall that\nthis information, readily visible in a simple tree, is effectively lost in a random\nforest). Examining the difference between Gini decrease and model accuracy\nvariable importance may suggest ways to improve the model.\n\n\nHyperparameters\nThe random forest, as with many statistical machine learning algorithms, can be\nconsidered a black-box algorithm with knobs to adjust how the box works. These\nknobs are called hyperparameters, which are parameters that you need to set\nbefore fitting a model; they are not optimized as part of the training process. While\ntraditional statistical models require choices (e.g., the choice of predictors to use\nin a regression model), the hyperparameters for random forest are more critical,\nespecially to avoid overfitting. In particular, the two most important\nhyperparemters for the random forest are:\nnodesize\nThe minimum size for terminal nodes (leaves in the tree). The default is 1 for\nclassification and 5 for regression.\nmaxnodes\nThe maximum number of nodes in each decision tree. By default, there is no\nlimit and the largest tree will be fit subject to the constraints of nodesize.\nIt may be tempting to ignore these parameters and simply go with the default\nvalues. However, using the default may lead to overfitting when you apply the\nrandom forest to noisy data. When you increase nodesize or set maxnodes, the\nalgorithm will fit smaller trees and is less likely to create spurious predictive\nrules. Cross-validation (see “Cross-Validation”) can be used to test the effects of\nsetting different values for hyperparameters.\nKEY IDEAS FOR BAGGING AND THE RANDOM FOREST\nEnsemble models improve model accuracy by combining the results from many models.\nBagging is a particular type of ensemble model based on fitting many models to bootstrapped\nsamples of the data and averaging the models.\nRandom forest is a special type of bagging applied to decision trees. In addition to resampling the\ndata, the random forest algorithm samples the predictor variables when splitting the trees.\nA useful output from the random forest is a measure of variable importance that ranks the\npredictors in terms of their contribution to model accuracy.\nThe random forest has a set of hyperparameters that should be tuned using cross-validation to\navoid overfitting.\n\n\nBoosting\nEnsemble models have become a standard tool for predictive modeling. Boosting\nis a general technique to create an ensemble of models. It was developed around\nthe same time as bagging (see “Bagging and the Random Forest”). Like bagging,\nboosting is most commonly used with decision trees. Despite their similarities,\nboosting takes a very different approach — one that comes with many more bells\nand whistles. As a result, while bagging can be done with relatively little tuning,\nboosting requires much greater care in its application. If these two methods were\ncars, bagging could be considered a Honda Accord (reliable and steady), whereas\nboosting could be considered a Porsche (powerful but requires more care).\nIn linear regression models, the residuals are often examined to see if the fit can\nbe improved (see “Partial Residual Plots and Nonlinearity”). Boosting takes this\nconcept much further and fits a series of models with each successive model fit to\nminimize the error of the previous models. Several variants of the algorithm are\ncommonly used: Adaboost, gradient boosting, and stochastic gradient boosting.\nThe latter, stochastic gradient boosting, is the most general and widely used.\nIndeed, with the right choice of parameters, the algorithm can emulate the random\nforest.\nKEY TERMS FOR BOOSTING\nEnsemble\nForming a prediction by using a collection of models.\nSynonym\nModel averaging\nBoosting\nA general technique to fit a sequence of models by giving more weight to the records with large\nresiduals for each successive round.\nAdaboost\nAn early version of boosting based on reweighting the data based on the residuals.\nGradient boosting\nA more general form of boosting that is cast in terms of minimizing a cost function.\nStochastic gradient boosting\nThe most general algorithm for boosting that incorporates resampling of records and columns in\n\n\neach round.\nRegularization\nA technique to avoid overfitting by adding a penalty term to the cost function on the number of\nparameters in the model.\nHyperparameters\nParameters that need to be set before fitting the algorithm.\n\n\nThe Boosting Algorithm\nThe basic idea behind the various boosting algorithms is essentially the same. The\neasiest to understand is Adaboost, which proceeds as follows:\n1. Initialize M, the maximum number of models to be fit, and set the\niteration counter \n. Initialize the observation weights \nfor \n. Initialize the ensemble model \n.\n2. Train a model using \n using the observation weights \nthat minimizes the weighted error \n defined by summing the weights for\nthe misclassified observations.\n3. Add the model to the ensemble: \n where \n.\n4. Update the weights \n so that the weights are increased\nfor the observations that were misclassfied. The size of the increase\ndepends on \n with larger values of \n leading to bigger weights.\n5. Increment the model counter \n. If \n, go to step 1.\nThe boosted estimate is given by:\nBy increasing the weights for the observations that were misclassified, the\nalgorithm forces the models to train more heavily on the data for which it\nperformed poorly. The factor \n ensures that models with lower error have a\nbigger weight.\nGradient boosting is similar to Adaboost but casts the problem as an optimization\nof a cost function. Instead of adjusting weights, gradient boosting fits models to a\npseudo-residual, which has the effect of training more heavily on the larger\nresiduals. In the spirit of the random forest, stochastic gradient boosting adds\nrandomness to the algorithm by sampling observations and predictor variables at\neach stage.\n\n\nXGBoost\nThe most widely used public domain software for boosting is XGBoost, an\nimplementation of stochastic gradient boosting originally developed by Tianqi\nChen and Carlos Guestrin at the University of Washington. A computationally\nefficient implementation with many options, it is available as a package for most\nmajor data science software languages. In R, XGBoost is available as the package\nxgboost.\nThe function xgboost has many parameters that can, and should, be adjusted (see\n“Hyperparameters and Cross-Validation”). Two very important parameters are\nsubsample, which controls the fraction of observations that should be sampled at\neach iteration, and eta, a shrinkage factor applied to \n in the boosting algorithm\n(see “The Boosting Algorithm”). Using subsample makes boosting act like the\nrandom forest except that the sampling is done without replacement. The shrinkage\nparameter eta is helpful to prevent overfitting by reducing the change in the\nweights (a smaller change in the weights means the algorithm is less likely to\noverfit to the training set). The following applies xgboost to the loan data with\njust two predictor variables:\nlibrary(xgboost)\npredictors <- data.matrix(loan3000[, c('borrower_score',\n                                       'payment_inc_ratio')])\nlabel <- as.numeric(loan3000[,'outcome'])-1\nxgb <- xgboost(data=predictors, label=label,\n               objective = \"binary:logistic\",\n               params=list(subsample=.63, eta=0.1), nrounds=100)\nNote that xgboost does not support the formula syntax, so the predictors need to\nbe converted to a data.matrix and the response needs to be converted to 0/1\nvariables. The objective argument tells xgboost what type of problem this is;\nbased on this, xgboost will choose a metric to optimize.\nThe predicted values can be obtained from the predict function and, since there\nare only two variables, plotted versus the predictors:\npred <- predict(xgb, newdata=predictors)\nxgb_df <- cbind(loan3000, pred_default=pred>.5, prob_default=pred)\nggplot(data=xgb_df, aes(x=borrower_score, y=payment_inc_ratio,\n                       color=pred_default, shape=pred_default)) +\n\n\n       geom_point(alpha=.6, size=2)\nThe result is shown in Figure 6-9. Qualitatively, this is similar to the predictions\nfrom the random forest; see Figure 6-7. The predictions are somewhat noisy in\nthat some borrowers with a very high borrower score still end up with a\nprediction of default.\n\n\nFigure 6-9. The predicted outcomes from XGBoost applied to the loan default data\n\n\nRegularization: Avoiding Overfitting\nBlind application of xgboost can lead to unstable models as a result of\noverfitting to the training data. The problem with overfitting is twofold:\nThe accuracy of the model on new data not in the training set will be\ndegraded.\nThe predictions from the model are highly variable, leading to unstable\nresults.\nAny modeling technique is potentially prone to overfitting. For example, if too\nmany variables are included in a regression equation, the model may end up with\nspurious predictions. However, for most statistical techniques, overfitting can be\navoided by a judicious selection of predictor variables. Even the random forest\ngenerally produces a reasonable model without tuning the parameters. This,\nhowever, is not the case for xgboost. Fit xgboost to the loan data for a training\nset with all of the variables included in the model:\n> predictors <- data.matrix(loan_data[,-which(names(loan_data) %in%\n                                       'outcome')])\n> label <- as.numeric(loan_data$outcome)-1\n> test_idx <- sample(nrow(loan_data), 10000)\n> xgb_default <- xgboost(data=predictors[-test_idx,],\n                         label=label[-test_idx],\n                         objective = \"binary:logistic\", nrounds=250)\n> pred_default <- predict(xgb_default, predictors[test_idx,])\n> error_default <- abs(label[test_idx] - pred_default) > 0.5\n> xgb_default$evaluation_log[250,]\n   iter train_error\n1:  250    0.145622\n> mean(error_default)\n[1] 0.3715\nThe test set consists of 10,000 randomly sampled records from the full data, and\nthe training set consists of the remaining records. Boosting leads to an error rate\nof only 14.6% for the training set. The test set, however, has a much higher error\nrate of 36.2%. This is a result of overfitting: while boosting can explain the\nvariability in the training set very well, the prediction rules do not apply to new\ndata.\nBoosting provides several parameters to avoid overfitting, including the\nparameters eta and subsample (see “XGBoost”). Another approach is\n\n\nregularization, a technique that modifies the cost function in order to penalize the\ncomplexity of the model. Decision trees are fit by minimizing cost criteria such as\nGini’s impurity score (see “Measuring Homogeneity or Impurity”). In xgboost, it\nis possible to modify the cost function by adding a term that measures the\ncomplexity of the model.\nThere are two parameters in xgboost to regularize the model: alpha and lambda,\nwhich correspond to Manhattan distance and squared Euclidean distance,\nrespectively (see “Distance Metrics”). Increasing these parameters will penalize\nmore complex models and reduce the size of the trees that are fit. For example,\nsee what happens if we set lambda to 1,000:\n> xgb_penalty <- xgboost(data=predictors[-test_idx,],\n                         label=label[-test_idx],\n                         params=list(eta=.1, subsample=.63, lambda=1000),\n                         objective = \"binary:logistic\", nrounds=250)\n> pred_penalty <- predict(xgb_penalty, predictors[test_idx,])\n> error_penalty <- abs(label[test_idx] - pred_penalty) > 0.5\n> xgb_penalty$evaluation_log[250,]\n   iter train_error\n1:  250    0.332405\n> mean(error_penalty)\n[1] 0.3483\nNow the training error is only slightly lower than the error on the test set.\nThe predict method offers a convenient argument, ntreelimit, that forces only\nthe first i trees to be used in the prediction. This lets us directly compare the in-\nsample versus out-of-sample error rates as more models are included:\n> error_default <- rep(0, 250)\n> error_penalty <- rep(0, 250)\n> for(i in 1:250){\n  pred_def <- predict(xgb_default, predictors[test_idx,], ntreelimit=i)\n  error_default[i] <- mean(abs(label[test_idx] - pred_def) >= 0.5)\n  pred_pen <- predict(xgb_penalty, predictors[test_idx,], ntreelimit = i)\n  error_penalty[i] <- mean(abs(label[test_idx] - pred_pen) >= 0.5)\n}\nThe output from the model returns the error for the training set in the component\nxgb_default$evaluation_log. By combining this with the out-of-sample\nerrors, we can plot the errors versus the number of iterations:\n> errors <- rbind(xgb_default$evaluation_log,\n                  xgb_penalty$evaluation_log,\n                  data.frame(iter=1:250, train_error=error_default),\n\n\n                data.frame(iter=1:250, train_error=error_penalty))\n> errors$type <- rep(c('default train', 'penalty train',\n                       'default test', 'penalty test'), rep(250, 4))\n> ggplot(errors, aes(x=iter, y=train_error, group=type)) +\n  geom_line(aes(linetype=type, color=type))\nThe result, displayed in Figure 6-10, shows how the default model steadily\nimproves the accuracy for the training set but actually gets worse for the test set.\nThe penalized model does not exhibit this behavior.\n\n\nFigure 6-10. The error rate of the default XGBoost versus a penalized version of XGBoost\nRIDGE REGRESSION AND THE LASSO\nAdding a penalty on the complexity of a model to help avoid overfitting dates back to the 1970s. Least\nsquares regression minimizes the residual sum of squares (RSS); see “Least Squares”. Ridge regression\nminimizes the sum of squared residuals plus a penalty on the number and size of the coefficients:\nThe value of \n determines how much the coefficients are penalized; larger values produce models that\nare less likely to overfit the data. The Lasso is similar, except that it uses Manhattan distance instead of\nEuclidean distance as a penalty term:\nThe xgboost parameters lambda and alpha are acting in a similar mannger.\n\n\nHyperparameters and Cross-Validation\nxgboost has a daunting array of hyperparameters; see “XGBoost\nHyperparameters” for a discussion. As seen in “Regularization: Avoiding\nOverfitting”, the specific choice can dramatically change the model fit. Given a\nhuge combination of hyperparameters to choose from, how should we be guided in\nour choice? A standard solution to this problem is to use cross-validation; see\n“Cross-Validation”. Cross-validation randomly splits up the data into K different\ngroups, also called folds. For each fold, a model is trained on the data not in the\nfold and then evaluated on the data in the fold. This yields a measure of accuracy\nof the model on out-of-sample data. The best set of hyperparameters is the one\ngiven by the model with the lowest overall error as computed by averaging the\nerrors from each of the folds.\nTo illustrate the technique, we apply it to parameter selection for xgboost. In this\nexample, we explore two parameters: the shrinkage parameter eta (see\n“XGBoost”) and the maximum depth of trees max_depth. The parameter\nmax_depth is the maximum depth of a leaf node to the root of the tree with a\ndefault value of 6. This gives us another way to control overfitting: deep trees\ntend to be more complex and may overfit the data. First we set up the folds and\nparameter list:\n> N <- nrow(loan_data)\n> fold_number <- sample(1:5, N, replace = TRUE)\n> params <- data.frame(eta = rep(c(.1, .5, .9), 3),\n                       max_depth = rep(c(3, 6, 12), rep(3,3)))\nNow we apply the preceding algorithm to compute the error for each model and\neach fold using five folds:\n> error <- matrix(0, nrow=9, ncol=5)\n> for(i in 1:nrow(params)){\n>   for(k in 1:5){\n>     fold_idx <- (1:N)[fold_number == k]\n>     xgb <- xgboost(data=predictors[-fold_idx,], label=label[-fold_idx],\n                     params = list(eta = params[i, 'eta'],\n                                   max_depth = params[i, 'max_depth']),\n                   objective = \"binary:logistic\", nrounds=100, verbose=0)\n>     pred <- predict(xgb, predictors[fold_idx,])\n>     error[i, k] <- mean(abs(label[fold_idx] - pred) >= 0.5)\n>   }\n> }\n\n\nSince we are fitting 45 total models, this can take a while. The errors are stored\nas a matrix with the models along the rows and folds along the columns. Using the\nfunction rowMeans, we can compare the error rate for the different parameter sets:\n> avg_error <- 100 * rowMeans(error)\n> cbind(params, avg_error)\n  eta max_depth avg_error\n1 0.1         3     35.41\n2 0.5         3     35.84\n3 0.9         3     36.48\n4 0.1         6     35.37\n5 0.5         6     37.33\n6 0.9         6     39.41\n7 0.1        12     36.70\n8 0.5        12     38.85\n9 0.9        12     40.19\nCross-validation suggests that using shallower trees with a smaller value of eta\nyields more accurate results. Since these models are also more stable, the best\nparameters to use are eta=0.1 and max_depth=3 (or possibly max_depth=6).\nXGBOOST HYPERPARAMETERS\nThe hyperparameters for xgboost are primarily used to balance overfitting with the accuracy and\ncomputational complexity. For a complete discussion of the parameters, refer to the xgboost\ndocumentation.\neta\nThe shrinkage factor between 0 and 1 applied to \n in the boosting algorithm. The default is 0.3,\nbut for noisy data, smaller values are recommended (e.g., 0.1).\nnrounds\nThe number of boosting rounds. If eta is set to a small value, it is important to increase the number\nof rounds since the algorithm learns more slowly. As long as some parameters are included to\nprevent overfitting, having more rounds doesn’t hurt.\nmax_depth\nThe maximum depth of the tree (the default is 6). In contrast to the random forest, which fits very\ndeep trees, boosting usually fits shallow trees. This has the advantage of avoiding spurious\ncomplex interactions in the model that can arise from noisy data.\nsubsample and colsample_bytree\nFraction of the records to sample without replacement and the fraction of predictors to sample for\nuse in fitting the trees. These parameters, which are similar to those in random forests, help avoid\noverfitting.\nlambda and alpha\nThe regularization parameters to help control overfitting (see “Regularization: Avoiding\nOverfitting”).\n\n\nKEY IDEAS FOR BOOSTING\nBoosting is a class of ensemble models based on fitting a sequence of models, with more weight\ngiven to records with large errors in successive rounds.\nStochastic gradient boosting is the most general type of boosting and offers the best performance.\nThe most common form of stochastic gradient boosting uses tree models.\nXGBoost is a popular and computationally efficient software package for stochastic gradient\nboosting; it is available in all common languages used in data science.\nBoosting is prone to overfitting the data, and the hyperparameters need to be tuned to avoid this.\nRegularization is one way to avoid overfitting by including a penalty term on the number of\nparameters (e.g., tree size) in a model.\nCross-validation is especially important for boosting due to the large number of hyperparameters\nthat need to be set.\n\n\nSummary\nThis chapter describes two classification and prediction methods that “learn”\nflexibly and locally from data, rather than starting with a structural model (e.g., a\nlinear regression) that is fit to the entire data set. K-Nearest Neighbors is a simple\nprocess that simply looks around at similar records and assigns their majority\nclass (or average value) to the record being predicted. Trying various cutoff\n(split) values of predictor variables, tree models iteratively divide the data into\nsections and subsections that are increasingly homogeneous with respect to class.\nThe most effective split values form a path, and also a “rule,” to a classification\nor prediction. Tree models are a very powerful and popular predictive tool, often\noutperforming other methods. They have given rise to various ensemble methods\n(random forests, boosting, bagging) that sharpen the predictive power of trees.\nThis and subsequent sections in this chapter © 2017 Datastats, LLC, Peter Bruce and Andrew Bruce,\nused by permission.\nThe term CART is a registered trademark of Salford Systems related to their specific implementation of\ntree models.\nThe term random forest is a trademark of Leo Breiman and Adele Cutler and licensed to Salford\nSystems. There is no standard nontrademark name, and the term random forest is as synonymous with the\nalgorithm as Kleenex is with facial tissues.\n1\n2\n3\n\n\nChapter 7. Unsupervised Learning\nThe term unsupervised learning refers to statistical methods that extract meaning\nfrom data without training a model on labeled data (data where an outcome of\ninterest is known). In Chapters 4 and 5, the goal is to build a model (set of rules)\nto predict a response from a set of predictor variables. Unsupervised learning\nalso constructs a model of the data, but does not distinguish between a response\nvariable and predictor variables.\nUnsupervised learning can have different possible goals. In some cases, it can be\nused to create a predictive rule in the absence of a labeled response. Clustering\nmethods can be used to identify meaningful groups of data. For example, using the\nweb clicks and demographic data of a user on a website, we may be able to group\ntogether different types of users. The website could then be personalized to these\ndifferent types.\nIn other cases, the goal may be to reduce the dimension of the data to a more\nmanageable set of variables. This reduced set could then be used as input into a\npredictive model, such as regression or classification. For example, we may have\nthousands of sensors to monitor an industrial process. By reducing the data to a\nsmaller set of features, we may be able to build a more powerful and interpretable\nmodel to predict process failure than by including data streams from thousands of\nsensors.\nFinally, unsupervised learning can be viewed as an extension of the exploratory\ndata analysis (see Chapter 1) to situations where you are confronted with a large\nnumber of variables and records. The aim is to gain insight into a set of data and\nhow the different variables relate to each other. Unsupervised techniques give\nways to sift through and analyze these variables and discover relationships.\nUNSUPERVISED LEARNING AND PREDICTION\nUnsupervised learning can play an important role for prediction, both for regression and classification\nproblems. In some cases, we want to predict a category in the absence of any labeled data. For example,\nwe might want to predict the type of vegetation in an area from a set of satellite sensory data. Since we\ndon’t have a response variable to train a model, clustering gives us a way to identify common patterns\nand categorize the regions.\n\n\nClustering is an especially important tool for the “cold-start problem.” In these types of problems, such as\nlaunching a new marketing campaign or identifying potential new types of fraud or spam, we initially may\nnot have any response to train a model. Over time, as data is collected, we can learn more about the\nsystem and build a traditional predictive model. But clustering helps us start the learning process more\nquickly by identifying population segments.\nUnsupervised learning is also important as a building block for regression and classification techniques.\nWith big data, if a small subpopulation is not well represented in the overall population, the trained model\nmay not perform well for that subpopulation. With clustering, it is possible to identify and label\nsubpopulations. Separate models can then be fit to the different subpopulations. Alternatively, the\nsubpopulation can be represented with its own feature, forcing the overall model to explicitly consider\nsubpopulation identity as a predictor.\n\n\nPrincipal Components Analysis\nOften, variables will vary together (covary), and some of the variation in one is\nactually duplicated by variation in another. Principal components analysis (PCA)\nis a technique to discover the way in which numeric variables covary.1\nKEY TERMS FOR PRINCIPAL COMPONENTS ANALYSIS\nPrincipal component\nA linear combination of the predictor variables.\nLoadings\nThe weights that transform the predictors into the components.\nSynonym\nWeights\nScreeplot\nA plot of the variances of the components, showing the relative importance of the components.\nThe idea in PCA is to combine multiple numeric predictor variables into a smaller\nset of variables, which are weighted linear combinations of the original set. The\nsmaller set of variables, the principal components, “explains” most of the\nvariability of the full set of variables, reducing the dimension of the data. The\nweights used to form the principal components reveal the relative contributions of\nthe original variables to the new principal components.\nPCA was first proposed by Karl Pearson. In what was perhaps the first paper on\nunsupervised learning, Pearson recognized that in many problems there is\nvariability in the predictor variables, so he developed PCA as a technique to\nmodel this variability. PCA can be viewed as the unsupervised version of linear\ndiscriminant analysis; see“Discriminant Analysis”.\n\n\nA Simple Example\nFor two variables, \n and \n, there are two principal components \n (\n or\n2):\nThe weights \n are known as the component loadings. These transform\nthe original variables into the principal components. The first principal\ncomponent, \n, is the linear combination that best explains the total variation. The\nsecond principal component, \n, explains the remaining variation (it is also the\nlinear combination that is the worst fit).\nNOTE\nIt is also common to compute principal components on deviations from the means of the predictor\nvariables, rather than on the values themselves.\nYou can compute principal components in R using the princomp function. The\nfollowing performs a PCA on the stock price returns for Chevron (CVX) and\nExxonMobil (XOM):\noil_px <- sp500_px[, c('CVX', 'XOM')]\npca <- princomp(oil_px)\npca$loadings\nLoadings:\n    Comp.1 Comp.2\nCVX -0.747  0.665\nXOM -0.665 -0.747\nThe weights for CVX and XOM for the first principal component are –0.747 and –\n0.665 and for the second principal component they are 0.665 and –0.747. How to\ninterpret this? The first principal component is essentially an average of CVX and\nXOM, reflecting the correlation between the two energy companies. The second\nprincipal component measures when the stock prices of CVX and XOM diverge.\n\n\nIt is instructive to plot the principal components with the data:\nloadings <- pca$loadings\nggplot(data=oil_px, aes(x=CVX, y=XOM)) +\n  geom_point(alpha=.3) +\n  stat_ellipse(type='norm', level=.99) +\n  geom_abline(intercept = 0, slope = loadings[2,1]/loadings[1,1]) +\n  geom_abline(intercept = 0, slope = loadings[2,2]/loadings[1,2])\nThe result is shown in Figure 7-1.\n\n\nFigure 7-1. The principal components for the stock returns for Chevron and ExxonMobil\nThe solid dashed lines show the two principal components: the first one is along\nthe long axis of the ellipse and the second one is along the short axis. You can see\nthat a majority of the variability in the two stock returns is explained by the first\nprincipal component. This makes sense since energy stock prices tend to move as\na group.\n\n\nNOTE\nThe weights for the first principal component are both negative, but reversing the sign of all the\nweights does not change the principal component. For example, using weights of 0.747 and 0.665\nfor the first principal component is equivalent to the negative weights, just as an infinite line\ndefined by the origin and 1,1 is the same as one defined by the origin and –1, –1.\n",
      "page_number": 371
    },
    {
      "number": 7,
      "title": "Unsupervised Learning",
      "start_page": 432,
      "end_page": 562,
      "detection_method": "regex_chapter_title",
      "content": "Computing the Principal Components\nGoing from two variables to more variables is straightforward. For the first\ncomponent, simply include the additional predictor variables in the linear\ncombination, assigning weights that optimize the collection of the covariation from\nall the predictor variables into this first principal component (covariance is the\nstatistical term; see “Covariance Matrix”). Calculation of principal components is\na classic statistical method, relying on either the correlation matrix of the data or\nthe covariance matrix, and it executes rapidly, not relying on iteration. As noted\nearlier, it works only with numeric variables, not categorical ones. The full\nprocess can be described as follows:\n1. In creating the first principal component, PCA arrives at the linear\ncombination of predictor variables that maximizes the percent of total\nvariance explained.\n2. This linear combination then becomes the first “new” predictor, Z1.\n3. PCA repeats this process, using the same variables, with different\nweights to create a second new predictor, Z2. The weighting is done such\nthat Z1 and Z2 are uncorrelated.\n4. The process continues until you have as many new variables, or\ncomponents, Zi as original variables Xi.\n5. Choose to retain as many components as are needed to account for most\nof the variance.\n6. The result so far is a set of weights for each component. The final step is\nto convert the original data into new principal component scores by\napplying the weights to the original values. These new scores can then be\nused as the reduced set of predictor variables.\n\n\nInterpreting Principal Components\nThe nature of the principal components often reveals information about the\nstructure of the data. There are a couple of standard visualization displays to help\nyou glean insight about the principal components. One such method is a Screeplot\nto visualize the relative importance of principal components (the name derives\nfrom the resemblance of the plot to a scree slope). The following is an example\nfor a few top companies in the S&P 500:\nsyms <- c( 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM',\n   'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\ntop_sp <- sp500_px[row.names(sp500_px)>='2005-01-01', syms]\nsp_pca <- princomp(top_sp)\nscreeplot(sp_pca)\nAs seen in Figure 7-2, the variance of the first principal component is quite large\n(as is often the case), but the other top principal components are significant.\n\n\nFigure 7-2. A screeplot for a PCA of top stocks from the S&P 500\nIt can be especially revealing to plot the weights of the top principal components.\nOne way to do this is to use the gather function from the tidyr package in\nconjunction with ggplot:\nlibrary(tidyr)\nloadings <- sp_pca$loadings[,1:5]\nloadings$Symbol <- row.names(loadings)\nloadings <- gather(loadings, \"Component\", \"Weight\", -Symbol)\nggplot(loadings, aes(x=Symbol, y=Weight)) +\n  geom_bar(stat='identity') +\n\n\n  facet_grid(Component ~ ., scales='free_y')\nThe loadings for the top five components are shown in Figure 7-3. The loadings\nfor the first principal component have the same sign: this is typical for data in\nwhich all the columns share a common factor (in this case, the overall stock\nmarket trend). The second component captures the price changes of energy stocks\nas compared to the other stocks. The third component is primarily a contrast in the\nmovements of Apple and CostCo. The fourth component contrasts the movements\nof Schlumberger to the other energy stocks. Finally, the fifth component is mostly\ndominated by financial companies.\n\n\nFigure 7-3. The loadings for the top five principal components of stock price returns\n\n\nHOW MANY COMPONENTS TO CHOOSE?\nIf your goal is to reduce the dimension of the data, you must decide how many principal\ncomponents to select. The most common approach is to use an ad hoc rule to select the\ncomponents that explain “most” of the variance. You can do this visually through the screeplot;\nfor example, in Figure 7-2, it would be natural to restrict the analysis to the top five components.\nAlternatively, you could select the top components such that the cumulative variance exceeds a\nthreshold, such as 80%. Also, you can inspect the loadings to determine if the component has an\nintuitive interpretation. Cross-validation provides a more formal method to select the number of\nsignificant components (see “Cross-Validation” for more).\nKEY IDEAS FOR PRINCIPAL COMPONENTS\nPrincipal components are linear combinations of the predictor variables (numeric data only).\nThey are calculated so as to minimize correlation between components, reducing redundancy.\nA limited number of components will typically explain most of the variance in the outcome variable.\nThe limited set of principal components can then be used in place of the (more numerous) original\npredictors, reducing dimensionality.\n\n\nFurther Reading\nFor a detailed look at the use of cross-validation in principal components, see\nRasmus Bro, K. Kjeldahl, A.K. Smilde, and Henk A. L. Kiers, “Cross-Validation\nof Component Models: A Critical Look at Current Methods”, Analytical and\nBioanalytical Chemistry 390, no. 5 (2008).\n\n\nK-Means Clustering\nClustering is a technique to divide data into different groups, where the records in\neach group are similar to one another. A goal of clustering is to identify significant\nand meaningful groups of data. The groups can be used directly, analyzed in more\ndepth, or passed as a feature or an outcome to a predictive regression or\nclassification model. K-means is the first clustering method to be developed; it is\nstill widely used, owing its popularity to the relative simplicity of the algorithm\nand its ability to scale to large data sets.\nKEY TERMS FOR K-MEANS CLUSTERING\nCluster\nA group of records that are similar.\nCluster mean\nThe vector of variable means for the records in a cluster.\nK\nThe number of clusters.\nK-means divides the data into K clusters by minimizing the sum of the squared\ndistances of each record to the mean of its assigned cluster. The is referred to as\nthe within-cluster sum of squares or within-cluster SS. K-means does not ensure\nthe clusters will have the same size, but finds the clusters that are the best\nseparated.\n\n\nNORMALIZATION\nIt is typical to normalize (standardize) continuous variables by subtracting the mean and dividing\nby the standard deviation. Otherwise, variables with large scale will dominate the clustering\nprocess (see “Standardization (Normalization, Z-Scores)”).\n\n\nA Simple Example\nStart by considering a data set with n records and just two variables,  and .\nSuppose we want to split the data into \n clusters. This means assigning\neach record \n to a cluster k. Given an assignment of \n records to cluster k,\nthe center of the cluster \n is the mean of the points in the cluster:\n\n\nCLUSTER MEAN\nIn clustering records with multiple variables (the typical case), the term cluster mean refers not\nto a single number, but to the vector of means of the variables.\nThe sum of squares within a cluster is given by:\nK-means finds the assignment of records that minimizes within-cluster sum of\nsquares across all four clusters \n.\nK-means clustering can be used to gain insight into how the price movements of\nstocks tend to cluster. Note that stock returns are reported in a fashion that is, in\neffect, standardized, so we do not need to normalize the data. In R, K-means\nclustering can be performed using the kmeans function. For example, the\nfollowing finds four clusters based on two variables: the stock returns for\nExxonMobil (XOM) and Chevron (CVX):\ndf <- sp500_px[row.names(sp500_px)>='2011-01-01', c('XOM', 'CVX')]\nkm <- kmeans(df, centers=4)\nThe cluster assignment for each record is returned as the cluster component:\n> df$cluster <- factor(km$cluster)\n> head(df)\n                  XOM        CVX cluster\n2011-01-03 0.73680496  0.2406809       2\n\n\n2011-01-04 0.16866845 -0.5845157       1\n2011-01-05 0.02663055  0.4469854       2\n2011-01-06 0.24855834 -0.9197513       1\n2011-01-07 0.33732892  0.1805111       2\n2011-01-10 0.00000000 -0.4641675       1\nThe first six records are assigned to either cluster 1 or cluster 2. The means of the\nclusters are also returned:\n> centers <- data.frame(cluster=factor(1:4), km$centers)\n> centers\n  cluster        XOM        CVX\n1       1 -0.3284864 -0.5669135\n2       2  0.2410159  0.3342130\n3       3 -1.1439800 -1.7502975\n4       4  0.9568628  1.3708892\nClusters 1 and 3 represent “down” markets, while clusters 2 and 4 represent “up\nmarkets.” In this example, with just two variables, it is straightforward to\nvisualize the clusters and their means:\nggplot(data=df, aes(x=XOM, y=CVX, color=cluster, shape=cluster)) +\n  geom_point(alpha=.3) +\n  geom_point(data=centers,  aes(x=XOM, y=CVX), size=3, stroke=2)\nThe resulting plot, given by Figure 7-4, shows the cluster assignments and the\ncluster means.\n\n\nFigure 7-4. The clusters of K-means applied to stock price data for ExxonMobil and Chevron (the two\ncluster centers in the dense area are hard to distinguish)\n\n\nK-Means Algorithm\nIn general, K-means can be applied to a data set with p variables \n.\nWhile the exact solution to K-means is computationally very difficult, heuristic\nalgorithms provide an efficient way to compute a locally optimal solution.\nThe algorithm starts with a user-specified K and an initial set of cluster means,\nthen iterates the following steps:\n1. Assign each record to the nearest cluster mean as measured by squared\ndistance.\n2. Compute the new cluster means based on the assignment of records.\nThe algorithm converges when the assignment of records to clusters does not\nchange.\nFor the first iteration, you need to specify an initial set of cluster means. Usually\nyou do this by randomly assigning each record to one of the K clusters, then\nfinding the means of those clusters.\nSince this algorithm isn’t guaranteed to find the best possible solution, it is\nrecommended to run the algorithm several times using different random samples to\ninitialize the algorithm. When more than one set of iterations is used, the K-means\nresult is given by the iteration that has the lowest within-cluster sum of squares.\nThe nstart parameter to the R function kmeans allows you to specify the number\nof random starts to try. For example, the following code runs K-means to find 5\nclusters using 10 different starting cluster means:\nsyms <- c( 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM', 'SLB', 'COP',\n           'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\ndf <- sp500_px[row.names(sp500_px)>='2011-01-01', syms]\nkm <- kmeans(df, centers=5, nstart=10)\nThe function automatically returns the best solution out of the 10 different starting\npoints. You can use the argument iter.max to set the maximum number of\niterations the algorithm is allowed for each random start.\n\n\nInterpreting the Clusters\nAn important part of cluster analysis can involve the interpretation of the clusters.\nThe two most important outputs from kmeans are the sizes of the clusters and the\ncluster means. For the example in the previous subsection, the sizes of resulting\nclusters are given by this R command:\nkm$size\n[1] 186 106 285 288 266\nThe cluster sizes are relatively balanced. Imbalanced clusters can result from\ndistant outliers, or groups of records very distinct from the rest of the data — both\nmay warrant further inspection.\nYou can plot the centers of the clusters using the gather function in conjunction\nwith ggplot:\ncenters <- as.data.frame(t(centers))\nnames(centers) <- paste(\"Cluster\", 1:5)\ncenters$Symbol <- row.names(centers)\ncenters <- gather(centers, \"Cluster\", \"Mean\", -Symbol)\ncenters$Color = centers$Mean > 0\nggplot(centers, aes(x=Symbol, y=Mean, fill=Color)) +\n  geom_bar(stat='identity', position = \"identity\", width=.75) +\n  facet_grid(Cluster ~ ., scales='free_y')\nThe resulting plot is shown in Figure 7-5 and reveals the nature of each cluster.\nFor example, clusters 1 and 2 correspond to days on which the market is down\nand up, respectively. Clusters 3 and 5 are characterized by up-market days for\nconsumer stocks and down-market days for energy stocks, respectively. Finally,\ncluster 4 captures the days in which energy stocks were up and consumer stocks\nwere down.\n\n\nFigure 7-5. The means of the variables in each cluster (“cluster means”)\n\n\nCLUSTER ANALYSIS VERSUS PCA\nThe plot of cluster means is similar in spirit to looking at the loadings for principal component\nanalysis (PCA); see “Interpreting Principal Components”. A major distinction is that unlike with\nPCA, the sign of the cluster means is meaningful. PCA identifies principal directions of variation,\nwhereas cluster analysis finds groups of records located near one another.\n\n\nSelecting the Number of Clusters\nThe K-means algorithm requires that you specify the number of clusters K.\nSometimes the number of clusters is driven by the application. For example, a\ncompany managing a sales force might want to cluster customers into “personas”\nto focus and guide sales calls. In such a case, managerial considerations would\ndictate the number of desired customer segments — for example, two might not\nyield useful differentiation of customers, while eight might be too many to manage.\nIn the absence of a cluster number dictated by practical or managerial\nconsiderations, a statistical approach could be used. There is no single standard\nmethod to find the “best” number of clusters.\nA common approach, called the elbow method, is to identify when the set of\nclusters explains “most” of the variance in the data. Adding new clusters beyond\nthis set contributes relatively little incremental contribution in the variance\nexplained. The elbow is the point where the cumulative variance explained\nflattens out after rising steeply, hence the name of the method.\nFigure 7-6 shows the cumulative percent of variance explained for the default data\nfor the number of clusters ranging from 2 to 15. Where is the elbow in this\nexample? There is no obvious candidate, since the incremental increase in\nvariance explained drops gradually. This is fairly typical in data that does not\nhave well-defined clusters. This is perhaps a drawback of the elbow method, but\nit does reveal the nature of the data.\n\n\nFigure 7-6. The elbow method applied to the stock data\nIn R, the kmeans function doesn’t provide a single command for applying the\nelbow method, but it can be readily applied from the output of kmeans as shown\nhere:\npct_var <- data.frame(pct_var = 0,\n                      num_clusters=2:14)\ntotalss <- kmeans(df, centers=14, nstart=50, iter.max = 100)$totss\nfor(i in 2:14){\n  pct_var[i-1, 'pct_var'] <- kmeans(df, centers=i, nstart=50, iter.max = 100)\n    $betweenss/totalss\n}\nIn evaluating how many clusters to retain, perhaps the most important test is this:\nhow likely are the clusters to be replicated on new data? Are the clusters\ninterpretable, and do they relate to a general characteristic of the data, or do they\njust reflect a specific instance? You can assess this, in part, using cross-\n\n\nvalidation; see “Cross-Validation”.\nIn general, there is no single rule that will reliably guide how many clusters to\nproduce.\nNOTE\nThere are several more formal ways to determine the number of clusters based on statistical or\ninformation theory. For example, Robert Tibshirani, Guenther Walther, and Trevor Hastie\n(http://www.stanford.edu/~hastie/Papers/gap.pdf) propose a “gap” statistic based on statistical\ntheory to identify the elbow. For most applications, a theoretical approach is probably not\nnecessary, or even appropriate.\nKEY IDEAS FOR K-MEANS CLUSTERING\nThe number of desired clusters, K, is chosen by the user.\nThe algorithm develops clusters by iteratively assigning records to the nearest cluster mean until\ncluster assignments do not change.\nPractical considerations usually dominate the choice of K; there is no statistically determined\noptimal number of clusters.\n\n\nHierarchical Clustering\nHierarchical clustering is an alternative to K-means that can yield very different\nclusters. Hierarchical clustering is more flexible than K-means and more easily\naccommodates non-numerical variables. It is more sensitive in discovering\noutlying or aberrant groups or records. Hierarchical clustering also lends itself to\nan intuitive graphical display, leading to easier interpretation of the clusters.\nKEY TERMS FOR HIERARCHICAL CLUSTERING\nDendrogram\nA visual representation of the records and the hierarchy of clusters to which they belong.\nDistance\nA measure of how close one record is to another.\nDissimilarity\nA measure of how close one cluster is to another.\nHierarchical clustering’s flexibility comes with a cost, and hierarchical clustering\ndoes not scale well to large data sets with millions of records. For even modest-\nsized data with just tens of thousands of records, hierarchical clustering can\nrequire intensive computing resources. Indeed, most of the applications of\nhierarchical clustering are focused on relatively small data sets.\n\n\nA Simple Example\nHierarchical clustering works on a data set with n records and p variables and is\nbased on two basic building blocks:\nA distance metric \n to measure the distance beween two records i and j.\nA dissimilarity metric \n to measure the difference between two clusters\nA and B based on the distances \n between the members of each cluster.\nFor applications involving numeric data, the most importance choice is the\ndissimilarity metric. Hierarchical clustering starts by setting each record as its\nown cluster and iterates to combine the least dissimilar clusters.\nIn R, the hclust function can be used to perform hierarchical clustering. One big\ndifference with hclust versus kmeans is that it operates on the pairwise\ndistances \n rather than the data itself. You can compute these using the dist\nfunction. For example, the following applies hierarchical clustering to the stock\nreturns for a set of companies:\nsyms1 <- c('GOOGL', 'AMZN', 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX',\n           'XOM', 'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP',\n           'WMT', 'TGT', 'HD', 'COST')\n# take transpose: to cluster companies, we need the stocks along the rows\ndf <- t(sp500_px[row.names(sp500_px)>='2011-01-01', syms1])\nd <- dist(df)\nhcl <- hclust(d)\nClustering algorithms will cluster the records (rows) of a data frame. Since we\nwant to cluster the companies, we need to transpose the data frame and put the\nstocks along the rows and the dates along the columns.\n\n\nThe Dendrogram\nHierarchical clustering lends itself to a natural graphical display as a tree,\nreferred to as a dendrogram. The name comes from the Greek words dendro\n(tree) and gramma (drawing). In R, you can easily produce this using the plot\ncommand:\nplot(hcl)\nThe result is shown in Figure 7-7. The leaves of the tree correspond to the\nrecords. The length of the branch in the tree indicates the degree of dissimilarity\nbetween corresponding clusters. The returns for Google and Amazon are quite\ndissimilar to the returns for the other stocks. The other stocks fall into natural\ngroups: energy stocks, financial stocks, and consumer stocks are all separated into\ntheir own subtrees.\n\n\nFigure 7-7. A dendogram of stocks\nIn contrast to K-means, it is not necessary to prespecify the number of clusters. To\nextract a specific number of clusters, you can use the cutree function:\ncutree(hcl, k=4)\nGOOGL  AMZN  AAPL  MSFT  CSCO  INTC   CVX   XOM   SLB   COP   JPM   WFC\n    1     2     3     3     3     3     4     4     4     4     3     3\n  USB   AXP   WMT   TGT    HD  COST\n    3     3     3     3     3     3\n\n\nThe number of clusters to extract is set to 4, and you can see that Google and\nAmazon each belong to their own cluster. The oil stocks (XOM, CVS, SLB, COP)\nall belong to another cluster. The remaining stocks are in the fourth cluster.\n\n\nThe Agglomerative Algorithm\nThe main algorithm for hierarchical clustering is the agglomerative algorithm,\nwhich iteratively merges similar clusters. The agglomerative algorithm begins\nwith each record constituting its own single-record cluster, then builds up larger\nand larger clusters. The first step is to calculate distances between all pairs of\nrecords.\nFor each pair of records \n and \n, we measure\nthe distance between the two records, \n, using a distance metric (see “Distance\nMetrics”). For example, we can use Euclidian distance:\nWe now turn to inter-cluster distance. Consider two clusters A and B, each with a\ndistinctive set of records, \n and \n.\nWe can measure the dissimilarity between the clusters \n by using the\ndistances between the members of A and the members of B.\nOne measure of dissimilarity is the complete-linkage method, which is the\nmaximum distance across all pairs of records between A and B:\nThis defines the dissimilarity as the biggest difference between all pairs.\nThe main steps of the agglomerative algorithm are:\n1. Create an initial set of clusters with each cluster consisting of a single\nrecord for all records in the data.\n2. Compute the dissimilarity \n) between all pairs of clusters \n.\n3. Merge the two clusters \n and \n that are least dissimilar as measured\nby \n).\n\n\n4. If we have more than one cluster remaining, return to step 2. Otherwise,\nwe are done.\n\n\nMeasures of Dissimilarity\nThere are four common measures of dissimilarity: complete linkage, single\nlinkage, average linkage, and minimum variance. These (plus other measures)\nare all supported by most hierarchical clustering software, including hclust. The\ncomplete linkage method defined earlier tends to produce clusters with members\nthat are similar. The single linkage method is the minimum distance between the\nrecords in two clusters:\nThis is a “greedy” method and produces clusters that can contain quite disparate\nelements. The average linkage method is the average of all distance pairs and\nrepresents a compromise between the single and complete linkage methods.\nFinally, the minimum variance method, also referred to as Ward’s method, is\nsimilar to K-means since it minimizes the within-cluster sum of squares (see “K-\nMeans Clustering”).\nFigure 7-8 applies hierarchical clustering using the four measures to the\nExxonMobil and Chevron stock returns. For each measure, four clusters are\nretained.\n\n\nFigure 7-8. A comparison of measures of dissimilarity applied to stock data\nThe results are strikingly different: the single linkage measure assigns almost all\nof the points to a single cluster. Except for the minimum variance method\n(Ward.D), all measures end up with at least one cluster with just a few outlying\npoints. The minimum variance method is most similar to the K-means cluster;\ncompare with Figure 7-4.\nKEY IDEAS FOR HIERARCHICAL CLUSTERING\nStart with every record in its own cluster.\nProgressively, clusters are joined to nearby clusters until all records belong to a single cluster (the\nagglomerative algorithm).\nThe agglomeration history is retained and plotted, and the user (without specifying the number of\nclusters beforehand) can visualize the number and structure of clusters at different stages.\nInter-cluster distances are computed in different ways, all relying on the set of all inter-record\ndistances.\n\n\nModel-Based Clustering\nClustering methods such as hierarchical clustering and K-means are based on\nheuristics and rely primarily on finding clusters whose members are close to one\nanother, as measured directly with the data (no probability model involved). In the\npast 20 years, significant effort has been devoted to developing model-based\nclustering methods. Adrian Raftery and other researchers at the University of\nWashington made critical contributions to model-based clustering, including both\ntheory and software. The techniques are grounded in statistical theory and provide\nmore rigorous ways to determine the nature and number of clusters. They could be\nused, for example, in cases where there might be one group of records that are\nsimilar to one another but not necessarily close to one another (e.g., tech stocks\nwith high variance of returns), and another group of records that is similar, and\nalso close (e.g., utility stocks with low variance).\n\n\nMultivariate Normal Distribution\nThe most widely used model-based clustering methods rest on the multivariate\nnormal distribution. The multivariate normal distribution is a generalization of the\nnormal distribution to set of p variables \n. The distribution is\ndefined by a set of means \n and a covariance matrix \n. The\ncovariance matrix is a measure of how the variables correlate with each other\n(see “Covariance Matrix” for details on the covariance). The covariance matrix \n consists of p variances \n and covariances \n for all pairs of\nvariables \n. With the variables put along the rows and duplicated along the\ncolumns, the matrix looks like this:\nSince a covariance matrix is symmetric, and \n, there are only \n covariance terms. In total, the covariance matrix has \n parameters. The distribution is denoted by:\n\n\nThis is a symbolic way of saying that the variables are all normally distributed,\nand the overall distribution is fully described by the vector of variable means and\nthe covariance matrix.\nFigure 7-9 shows the probability contours for a multivariate normal distribution\nfor two variables X and Y (the 0.5 probability contour, for example, contains 50%\nof the distribution).\nThe means are \n and \n and the covariance matrix is:\nSince the covariance \n is positive, X and Y are positively correlated.\n\n\nFigure 7-9. Probability contours for a two-dimensional normal distribution\n\n\nMixtures of Normals\nThe key idea behind model-based clustering is that each record is assumed to be\ndistributed as one of K multivariate-normal distributions, where K is the number\nof clusters. Each distribution has a different mean  and covariance matrix \n.\nFor example, if you have two variables, X and Y, then each row \n is\nmodeled as having been sampled from one of K distributions \n.\nR has a very rich package for model-based clustering called mclust, originally\ndeveloped by Chris Fraley and Adrian Raftery. With this package, we can apply\nmodel-based clustering to the stock return data we previously analyzed using K-\nmeans and hierarchical clustering:\n> library(mclust)\n> df <- sp500_px[row.names(sp500_px)>='2011-01-01', c('XOM', 'CVX')]\n> mcl <- Mclust(df)\n> summary(mcl)\nMclust VEE (ellipsoidal, equal shape and orientation) model with 2 components:\n log.likelihood    n df       BIC       ICL\n      -2255.134 1131  9 -4573.546 -5076.856\nClustering table:\n  1   2\n963 168\nIf you execute this code, you will notice that the computation takes significiantly\nlonger than other procedures. Extracting the cluster assignments using the predict\nfunction, we can visualize the clusters:\ncluster <- factor(predict(mcl)$classification)\nggplot(data=df, aes(x=XOM, y=CVX, color=cluster, shape=cluster)) +\n  geom_point(alpha=.8)\nThe resulting plot is shown in Figure 7-10. There are two clusters: one cluster in\nthe middle of the data, and a second cluster in the outer edge of the data. This is\nvery different from the clusters obtained using K-means (Figure 7-4) and\nhierarchical clustering (Figure 7-8), which find clusters that are compact.\n\n\nFigure 7-10. Two clusters are obtained for stock return data using mclust\nYou can extract the parameters to the normal distributions using the summary\nfunction:\n> summary(mcl, parameters=TRUE)$mean\n          [,1]        [,2]\nXOM 0.05783847 -0.04374944\nCVX 0.07363239 -0.21175715\n> summary(mcl, parameters=TRUE)$variance\n, , 1\n          XOM       CVX\nXOM 0.3002049 0.3060989\nCVX 0.3060989 0.5496727\n, , 2\n         XOM      CVX\nXOM 1.046318 1.066860\nCVX 1.066860 1.915799\n\n\nThe distributions have similar means and correlations, but the second distribution\nhas much larger variances and covariances.\nThe clusters from mclust may seem surprising, but in fact, they illustrate the\nstatistical nature of the method. The goal of model-based clustering is to find the\nbest-fitting set of multivariate normal distributions. The stock data appears to have\na normal-looking shape: see the contours of Figure 7-9. In fact, though, stock\nreturns have a longer-tailed distribution than a normal distribution. To handle this,\nmclust fits a distribution to the bulk of the data, but then fits a second distribution\nwith a bigger variance.\n\n\nSelecting the Number of Clusters\nUnlike K-means and hierarchical clustering, mclust automatically selects the\nnumber of clusters (in this case, two). It does this by choosing the number of\nclusters for which the Bayesian Information Criteria (BIC) has the largest value.\nBIC (similar to AIC) is a general tool to find the best model amongst a candidate\nset of models. For example, AIC (or BIC) is commonly used to select a model in\nstepwise regression; see “Model Selection and Stepwise Regression”. BIC works\nby selecting the best-fitting model with a penalty for the number of parameters in\nthe model. In the case of model-based clustering, adding more clusters will\nalways improve the fit at the expense of introducing additional parameters in the\nmodel.\nYou can plot the BIC value for each cluster size using a function in hclust:\nplot(mcl, what='BIC', ask=FALSE)\nThe number of clusters — or number of different multivariate normal models\n(components) — is shown on the x-axis (see Figure 7-11).\n\n\nFigure 7-11. BIC scores for the stock return data for different numbers of clusters (components)\nThis plot is similar to the elbow plot used to identify the number of clusters to\nchoose for K-means, except the value being plotted is BIC instead of percent of\nvariance explained (see Figure 7-6). One big difference is that instead of one line,\nmclust shows 14 different lines! This is because mclust is actually fitting 14\ndifferent models for each cluster size, and ultimately it chooses the best-fitting\n\n\nmodel.\nWhy does mclust fit so many models to determine the best set of multivariate\nnormals? It’s because there are different ways to parameterize the covariance\nmatrix \n for fitting a model. For the most part, you do not need to worry about the\ndetails of the models and can simply use the model chosen by mclust. In this\nexample, according to BIC, three different models (called VEE, VEV, and VVE)\ngive the best fit using two components.\nNOTE\nModel-based clustering is a rich and rapidly developing area of study, and the coverage in this text\nonly spans a small part of the field. Indeed, the mclust help file is currently 154 pages long.\nNavigating the nuances of model-based clustering is probably more effort than is needed for most\nproblems encountered by data scientists.\nModel-based clustering techniques do have some limitations. The methods require\nan underlying assumption of a model for the data, and the cluster results are very\ndependent on that assumption. The computations requirements are higher than even\nhierarchical clustering, making it difficult to scale to large data. Finally, the\nalgorithm is more sophisticated and less accessible than that of other methods.\nKEY IDEAS FOR MODEL-BASED CLUSTERING\nClusters are assumed to derive from different data-generating processes with different probability\ndistributions.\nDifferent models are fit, assuming different numbers of (typically normal) distributions.\nThe method chooses the model (and the associated number of clusters) that fits the data well\nwithout using too many parameters (i.e., overfitting).\n\n\nFurther Reading\nFor more detail on model-based clustering, see the mclust documentation.\n\n\nScaling and Categorical Variables\nUnsupervised learning techniques generally require that the data be appropriately\nscaled. This is different from many of the techniques for regression and\nclassification in which scaling is not important (an exception is K-nearest\nneighbors; see “K-Nearest Neighbors”).\nKEY TERMS FOR SCALING DATA\nScaling\nSquashing or expanding data, usually to bring multiple variables to the same scale.\nNormalization\nOne method of scaling — subtracting the mean and dividing by the standard deviation.\nSynonym\nStandardization\nGower’s distance\nA scaling algorithm applied to mixed numeric and categoprical data to bring all variables to a 0–1\nrange.\nFor example, with the personal loan data, the variables have widely different units\nand magnitude. Some variables have relatively small values (e.g., number of years\nemployed), while others have very large values (e.g., loan amount in dollars). If\nthe data is not scaled, then the PCA, K-means, and other clustering methods will\nbe dominated by the variables with large values and ignore the variables with\nsmall values.\nCategorical data can pose a special problem for some clustering procedures. As\nwith K-nearest neighbors, unordered factor variables are generally converted to a\nset of binary (0/1) variables using one hot encoding (see “One Hot Encoder”). Not\nonly are the binary variables likely on a different scale from other data, the fact\nthat binary variables have only two values can prove problematic with techniques\nsuch as PCA and K-means.\n\n\nScaling the Variables\nVariables with very different scale and units need to be normalized appropriately\nbefore you apply a clustering procedure. For example, let’s apply kmeans to a set\nof data of loan defaults without normalizing:\ndf <- defaults[, c('loan_amnt', 'annual_inc', 'revol_bal', 'open_acc',\n                   'dti', 'revol_util')]\nkm <- kmeans(df, centers=4, nstart=10)\ncenters <- data.frame( size=km$size, km$centers)\nround(centers, digits=2)\n  size loan_amnt annual_inc revol_bal open_acc   dti revol_util\n1    55  23157.27  491522.49  83471.07    13.35  6.89      58.74\n2  1218  21900.96  165748.53  38299.44    12.58 13.43      63.58\n3  7686  18311.55   83504.68  19685.28    11.68 16.80      62.18\n4 14177  10610.43   42539.36  10277.97     9.60 17.73      58.05\nThe variables annual_inc and revol_bal dominate the clusters, and the clusters\nhave very different sizes. Cluster 1 has only 55 members with comparatively high\nincome and revolving credit balance.\nA common approach to scaling the variables is to convert them to z-scores by\nsubtracting the mean and dividing by the standard deviation. This is termed\nstandardization or normalization (see “Standardization (Normalization, Z-\nScores)” for more discussion about using z-scores):\nSee what happens to the clusters when kmeans is applied to the normalized data:\ndf0 <- scale(df)\nkm0 <- kmeans(df0, centers=4, nstart=10)\ncenters0 <-scale(km0$centers, center=FALSE,\n                 scale=1/attr(df0, 'scaled:scale'))\ncenters0 <- scale(centers0, center=-attr(df0, 'scaled:center'), scale=F)\ndata.frame(size=km0$size, centers0)\n  size loan_amnt annual_inc revol_bal open_acc   dti revol_util\n1 5429  10393.60   53689.54   6077.77     8.69 11.35      30.69\n2 6396  13310.43   55522.76  16310.95    14.25 24.27      59.57\n3 7493  10482.19   51216.95  11530.17     7.48 15.79      77.68\n4 3818  25933.01  116144.63  32617.81    12.44 16.25      66.01\nThe cluster sizes are more balanced, and the clusters are not just dominated by\n\n\nannual_inc and revol_bal, revealing more interesting structure in the data.\nNote that the centers are rescaled to the original units in the preceding code. If we\nhad left them unscaled, the resulting values would be in terms of z-scores, and\ntherefore less interpretable.\nNOTE\nScaling is also important for PCA. Using the z-scores is equivalent to using the correlation matrix\n(see “Correlation”) instead of the covariance matrix in computing the principal components.\nSoftware to compute PCA usually has an option to use the correlation matrix (in R, the princomp\nfunction has the argument cor).\n\n\nDominant Variables\nEven in cases where the variables are measured on the same scale and accurately\nreflect relative importance (e.g., movement to stock prices), it can sometimes be\nuseful to rescale the variables.\nSuppose we add Alphabet (GOOGL) and Amazon (AMZN) to the analysis in\n“Interpreting Principal Components”.\nsyms <- c('AMZN', 'GOOGL' 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM',\n   'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\ntop_sp1 <- sp500_px[row.names(sp500_px)>='2005-01-01', syms]\nsp_pca1 <- princomp(top_sp1)\nscreeplot(sp_pca1)\nThe screeplot displays the variances for the top principal components. In this\ncase, the screeplot in Figure 7-12 reveals that the variances of the first and second\ncomponents are much larger than the others. This often indicates that one or two\nvariables dominate the loadings. This is, indeed, the case in this example:\nround(sp_pca1$loadings[,1:2], 3)\n      Comp.1 Comp.2\nGOOGL  0.781  0.609\nAMZN   0.593 -0.792\nAAPL   0.078  0.004\nMSFT   0.029  0.002\nCSCO   0.017 -0.001\nINTC   0.020 -0.001\nCVX    0.068 -0.021\nXOM    0.053 -0.005\n...\nThe first two principal components are almost completely dominated by GOOGL\nand AMZN. This is because the stock price movements of GOOGL and AMZN\ndominate the variability.\nTo handle this situation, you can either include them as is, rescale the variables\n(see “Scaling the Variables”), or exclude the dominant variables from the analysis\nand handle them separately. There is no “correct” approach, and the treatment\ndepends on the application.\n\n\nFigure 7-12. A screeplot for a PCA of top stocks from the S&P 500 including GOOGL and AMZN\n\n\nCategorical Data and Gower’s Distance\nIn the case of categorical data, you must convert it to numeric data, either by\nranking (for an ordered factor) or by encoding as a set of binary (dummy)\nvariables. If the data consists of mixed continuous and binary variables, you will\nusually want to scale the variables so that the ranges are similar; see “Scaling the\nVariables”. One popular method is to use Gower’s distance.\nThe basic idea behind Gower’s distance is to apply a different distance metric to\neach variable depending on the type of data:\nFor numeric variables and ordered factors, distance is calculated as the\nabsolute value of the difference between two records (Manhattan distance).\nFor categorical variables, the distance is 1 if the categories between two\nrecords are different and the distance is 0 if the categories are the same.\nGower’s distance is computed as follows:\n1. Compute the distance \n for all pairs of variables i and j for each\nrecord.\n2. Scale each pair \n so the minimum is 0 and the maximum is 1.\n3. Add the pairwise scaled distances between variables together, either\nusing a simple or weighted mean, to create the distance matrix.\nTo illustrate Gower’s distance, take a few rows from the loan data:\n> x = defaults[1:5, c('dti', 'payment_inc_ratio', 'home', 'purpose')]\n> x\n# A tibble: 5 × 4\n    dti payment_inc_ratio   home            purpose\n  <dbl>             <dbl> <fctr>             <fctr>\n1  1.00           2.39320   RENT                car\n2  5.55           4.57170    OWN     small_business\n3 18.08           9.71600   RENT              other\n4 10.08          12.21520   RENT debt_consolidation\n5  7.06           3.90888   RENT              other\nThe function daisy in the cluster package can be used to compute Gower’s\ndistance:\n\n\n> library(cluster)\n> daisy(x, metric='gower')\nDissimilarities :\n          1         2         3         4\n2 0.6220479\n3 0.6863877 0.8143398\n4 0.6329040 0.7608561 0.4307083\n5 0.3772789 0.5389727 0.3091088 0.5056250\nAll distances are between 0 and 1. The pair of records with the biggest distance is\n2 and 3: neither has the same values for home or purpose and they have very\ndifferent levels of dti (debt-to-income) and payment_inc_ratio. Records 3 and\n5 have the smallest distance because they share the same values for home or\npurpose.\nYou can apply hierarchical clustering (see “Hierarchical Clustering”) to the\nresulting distance matrix using hclust to the output from daisy:\ndf <- defaults[sample(nrow(defaults), 250),\n               c('dti', 'payment_inc_ratio', 'home', 'purpose')]\nd = daisy(df, metric='gower')\nhcl <- hclust(d)\ndnd <- as.dendrogram(hcl)\nplot(dnd, leaflab='none')\nThe resulting dendrogram is shown in Figure 7-13. The individual records are not\ndistinguishable on the x-axis, but we can examine the records in one of the\nsubtrees (on the left, using a “cut” of 0.5), with this code:\n> df[labels(dnd_cut$lower[[1]]),]\n# A tibble: 9 × 4\n    dti payment_inc_ratio   home purpose\n  <dbl>             <dbl> <fctr>  <fctr>\n1 24.57           0.83550   RENT   other\n2 34.95           5.02763   RENT   other\n3  1.51           2.97784   RENT   other\n4  8.73          14.42070   RENT   other\n5 12.05           9.96750   RENT   other\n6 10.15          11.43180   RENT   other\n7 19.61          14.04420   RENT   other\n8 20.92           6.90123   RENT   other\n9 22.49           9.36000   RENT   other\nThis subtree entirely consists of renters with a loan purpose labeled as “other.”\nWhile strict separation is not true of all subtrees, this illustrates that the\ncategorical variables tend to be grouped together in the clusters.\n\n\nFigure 7-13. A dendrogram of hclust applied to a sample of loan default data with mixed variable types\n\n\nProblems with Clustering Mixed Data\nK-means and PCA are most appropriate for continuous variables. For smaller data\nsets, it is better to use hierarchical clustering with Gower’s distance. In principle,\nthere is no reason why K-means can’t be applied to binary or categorical data.\nYou would usually use the “one hot encoder” representation (see “One Hot\nEncoder”) to convert the categorical data to numeric values. In practice, however,\nusing K-means and PCA with binary data can be difficult.\nIf the standard z-scores are used, the binary variables will dominate the definition\nof the clusters. This is because 0/1 variables take on only two values and K-means\ncan obtain a small within-cluster sum-of-squares by assigning all the records with\na 0 or 1 to a single cluster. For example, apply kmeans to loan default data\nincluding factor variables home and pub_rec_zero:\ndf <- model.matrix(~ -1 + dti + payment_inc_ratio + home + pub_rec_zero,\n                   data=defaults)\ndf0 <- scale(df)\nkm0 <- kmeans(df0, centers=4, nstart=10)\ncenters0 <-scale(km0$centers, center=FALSE,\n                 scale=1/attr(df0, 'scaled:scale'))\nscale(centers0, center=-attr(df0, 'scaled:center'), scale=F)\n    dti payment_inc_ratio homeMORTGAGE homeOWN homeRENT pub_rec_zero\n1 17.02              9.10         0.00       0     1.00         1.00\n2 17.47              8.43         1.00       0     0.00         1.00\n3 17.23              9.28         0.00       1     0.00         0.92\n4 16.50              8.09         0.52       0     0.48         0.00\nThe top four clusters are essentially proxies for the different levels of the factor\nvariables. To avoid this behavior, you could scale the binary variables to have a\nsmaller variance than other variables. Alternatively, for very large data sets, you\ncould apply clustering to different subsets of data taking on specific categorical\nvalues. For example, you could apply clustering separately to those loans made to\nsomeone who has a mortgage, owns a home outright, or rents.\nKEY IDEAS FOR SCALING DATA\nVariables measured on different scales need to be transformed to similar scales, so that their impact\non algorithms is not determined mainly by their scale.\nA common scaling method is normalization (standardization) — subtracting the mean and dividing\nby the standard deviation.\nAnother method is Gower’s distance, which scales all variables to the 0–1 range (it is often used\n\n\nwith mixed numeric and categorical data).\n\n\nSummary\nFor dimension reduction of numeric data, the main tools are either principal\ncomponents analysis or K-means clustering. Both require attention to proper\nscaling of the data to ensure meaningful data reduction.\nFor clustering with highly structured data in which the clusters are well separated,\nall methods will likely produce a similar result. Each method offers its own\nadvantage. K-means scales to very large data and is easily understood.\nHierarchical clustering can be applied to mixed data types — numeric and\ncategorical — and lends itself to an intuitive display (the dendrogram). Model-\nbased clustering is founded on statistical theory and provides a more rigorous\napproach, as opposed to the heuristic methods. For very large data, however, K-\nmeans is the main method used.\nWith noisy data, such as the loan and stock data (and much of the data that a data\nscientist will face), the choice is more stark. K-means, hierarchical clustering, and\nespecially model-based clustering all produce very different solutions. How\nshould a data scientist proceed? Unfortunately, there is no simple rule of thumb to\nguide the choice. Ultimately, the method used will depend on the data size and the\ngoal of the application.\nThis and subsequent sections in this chapter © 2017 Datastats, LLC, Peter Bruce and Andrew Bruce,\nused by permission.\n1\n\n\nBibliography\n[bokeh] Bokeh Development Team. “Bokeh: Python library for interactive\nvisualization” (2014). http://www.bokeh.pydata.org.\n[Deng-Wickham-2011] Deng, H. and Wickham, H. “Density estimation in R”\n(2011). http://vita.had.co.nz/papers/density-estimation.pdf.\n[Wikipedia-2016] “Diving.” Wikipedia: The Free Encyclopedia. Wikimedia\nFoundation, Inc. 10 Mar 2016. Web. 19 Mar 2016.\n[Donoho-2015] Donoho, David. “50 Years of Data Science” (2015).\nhttp://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf.\n[Duong-2001] Duang, Tarn. “An introduction to kernel density estimation”\n(2001). http://www.mvstat.net/tduong/research/seminars/seminar-2001-\n05.pdf.\n[Few-2007] Few, Stephen. “Save the Pies for Dessert.” Visual Intelligence\nNewsletter, Perceptual Edge (2007).\nhttps://www.perceptualedge.com/articles/visual_business_intelligence/save\n[Hintze-Nelson-1998] Hintze, J. and Nelson, R. “Violin Plots: A Box Plot-\nDensity Trace Synergism.” The American Statistician 52.2 (May 1998):\n181–184.\n[Galton-1886] Galton, Francis. “Regression towards mediocrity in\nHereditary stature.” The Journal of the Anthropological Institute of Great\nBritain and Ireland, 15:246-273. JSTOR 2841583.\n[ggplot2] Wickham, Hadley. ggplot2: Elegant Graphics for Data Analysis.\nSpringer-Verlag New York (2009). ISBN: 978-0-387-98140-6.\nhttp://had.co.nz/ggplot2/book.\n[Hyndman-Fan-1996] Hyndman, R. J. and Fan, Y. “Sample quantiles in\nstatistical packages,” American Statistician 50, (1996) 361–365.\n\n\n[lattice] Sarkar, Deepayan. Lattice: Multivariate Data Visualization with R.\nSpringer (2008). ISBN 978-0-387-75968-5. http://lmdvr.r-forge.r-\nproject.org.\n[Legendre] Legendre, Adrien-Marie. Nouvelle methodes pour la\ndetermination des orbites des cometes. F. Didot, Paris (1805).\n[NIST-Handbook-2012] NIST/SEMATECH e-Handbook of Statistical\nMethods,\nhttp://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm (2012).\n[R-base-2015] R Core Team. “R: A Language and Environment for\nStatistical Computing,” R Foundation for Statistical Computing (2015).\nhttp://www.R-project.org/.\n[seaborne] Wasdom, Michael. “Seaborn: statistical data visualization”\n(2015). http://stanford.edu/~mwaskom/software/seaborn/#.\n[Stigler-Gauss] Stigler, Stephen M. “Gauss and the Invention of Least\nSquares.” Ann. Stat. 9(3), 465–474 (1981).\n[Trellis-Graphics] Becker, R., Cleveland, W, Shyu, M. and Kaluzny, S. “A\nTour of Trellis Graphics” (1996).\nhttp://polisci.msu.edu/jacoby/icpsr/graphics/manuscripts/Trellis_tour.pdf.\n[Tukey-1962] Tukey, John W. “The Future of Data Analysis.” Ann. Math.\nStatist. 33 (1962), no. 1, 1–67.\nhttps://projecteuclid.org/download/pdf_1/euclid.aoms/1177704711\n[Tukey-1977] Tukey, John W. Exploratory Data Analysis. Pearson (1977).\nISBN: 978-0-201-07616-5.\n[Tukey-1987] Tukey, John W. Edited by Jones, L. V. The collected works of\nJohn W. Tukey: Philosophy and Principles of Data Analysis 1965–1986,\nVolume IV. Chapman and Hall/CRC (1987). ISBN: 978-0-534-05101-3.\n[UCLA] “R Library: Contrast Coding Systems for Categorical Variables.”\nUCLA: Statistical Consulting Group.\nhttp://www.ats.ucla.edu/stat/r/library/contrast_coding.htm. Accessed June\n2016.\n\n\n[Zhang-Wang-2007] Zhang, Qi and Wang, Wei. 19th International Conference\non Scientific and Statistical Database Management, IEEE Computer Society\n(2007).\n\n\nIndex\nA\nA/B testing, A/B Testing-For Further Reading\ncontrol group, advantages of using, Why Have a Control Group?\nepsilon-greedy algorithm, Multi-Arm Bandit Algorithm\nimportance of permissions, Why Just A/B? Why Not C, D…?\ntraditional, shortcoming of, Multi-Arm Bandit Algorithm\naccuracy, Evaluating Classification Models\nimproving in random forests, Random Forest\nAdaboost, Boosting\nboosting algorithm, The Boosting Algorithm\nadjusted R-squared, Assessing the Model\nadjustment of p-values, Multiple Testing, Multiple Testing\nagglomerative algorithm, The Agglomerative Algorithm\nAIC (Akaike's Information Criteria), Model Selection and Stepwise\nRegression, Selecting the Number of Clusters\nvariants of, Model Selection and Stepwise Regression\nAkike, Hirotugu, Model Selection and Stepwise Regression\nall subset regression, Model Selection and Stepwise Regression\n\n\nalpha, Statistical Significance and P-Values, Alpha\ndividing up in multiple testing, Multiple Testing\nalternative hypothesis, Hypothesis Tests, Alternative Hypothesis\nAmerican Statistical Association (ASA), statement on p-values, Value of the\np-value\nanomaly detection, Outliers, Regression and Prediction\nANOVA (analysis of variance\nstatististical test based on F-statistic, F-Statistic\nANOVA (analysis of variance), ANOVA-Further Reading\ncomputing ANOVA table in R, F-Statistic\ndecomposition of variance, F-Statistic\ntwo-way, Two-Way ANOVA\narms (multi-arm bandits), Multi-Arm Bandit Algorithm\nAUC (area under the ROC curve), AUC\naverage linkage, Measures of Dissimilarity\nB\nbackward elimination, Model Selection and Stepwise Regression\nbackward selection, Model Selection and Stepwise Regression\nbagging, The Bootstrap, Resampling, Statistical Machine Learning, Bagging\nbetter predictive performance than single trees, How Trees Are Used\n\n\nboosting vs., Boosting\nusing with random forests, Random Forest\nbandit algorithms, Multi-Arm Bandit Algorithm\n(see also multi-arm bandits)\nbar charts, Exploring Binary and Categorical Data\nBayesian classification, Naive Bayes\n(see also naive Bayes algorithm)\nimpracticality of exact Bayesian classification, Why Exact Bayesian\nClassification Is Impractical\nBayesian infomation criteria (BIC), Model Selection and Stepwise\nRegression, Selecting the Number of Clusters\nbeta distribution, Multi-Arm Bandit Algorithm\nbias, Bias\nselection bias, Selection Bias-Further Reading\nbias-variance tradeoff, Choosing K\nbiased estimates, Standard Deviation and Related Estimates\nfrom naive Bayes classifier, The Naive Solution\nBIC (Bayesian information criteria), Model Selection and Stepwise\nRegression, Selecting the Number of Clusters\nbidirectional alternative hypothesis, One-Way, Two-Way Hypothesis Test\nbig data\n\n\nand outliers in regression, Outliers\nuse of regression in, Prediction versus Explanation (Profiling)\nvalue of, Size versus Quality: When Does Size Matter?\nbinary data, Elements of Structured Data\nexploring, Exploring Binary and Categorical Data-Correlation\nbinomial, Binomial Distribution\nbinomial distribution, Binomial Distribution-Further Reading\nbinomial trials, Binomial Distribution\nbins\nhexagonal binning, Hexagonal Binning and Contours (Plotting Numeric\nversus Numeric Data)\nin frequency tables, Frequency Table and Histograms\nin histograms, Frequency Table and Histograms\nbivariate analysis, Exploring Two or More Variables\nblack swan theory, Long-Tailed Distributions\nblind studies, Why Have a Control Group?\nboosting, Statistical Machine Learning, Tree Models, Boosting-Summary\nbagging vs., Boosting\nboosting algorithm, The Boosting Algorithm\nhyperparameters and cross-validation, Hyperparameters and Cross-\n\n\nValidation\noverfitting, avoiding using regularization, Regularization: Avoiding\nOverfitting\nXGBoost, XGBoost\nbootstrap, The Bootstrap-Further Reading, Resampling\nconfidence interval generation, Confidence Intervals, Confidence and\nPrediction Intervals\npermutation tests, Exhaustive and Bootstrap Permutation Test\nresampling vs. bootstrapping, Resampling versus Bootstrapping\nusing with random forests, Random Forest\nbootstrap sample, The Bootstrap\nboxplots, Exploring the Data Distribution\ncombining with a violin plot, example, Categorical and Numeric Data\nexample, percent of airline delays by carrier, Categorical and Numeric\nData\noutliers in, Outliers\npercentiles and, Percentiles and Boxplots\nBreiman, Leo, Statistical Machine Learning\nbubble plots, Influential Values\nC\ncategorical data, Elements of Structured Data\n\n\nexploring, Exploring Binary and Categorical Data-Correlation\nexpected value, Expected Value\nmode, Mode\nnumerical data as categorical data, Exploring Binary and Categorical\nData\nexploring two categorical variables, Two Categorical Variables\nimportance of the concept, Elements of Structured Data\nnumeric variable grouped by categorical variable, Categorical and Numeric\nData\nscaling and categorical variables, Scaling and Categorical Variables-\nSummary\ndominant variables, Dominant Variables\nGower's distance, Categorical Data and Gower’s Distance\nscaling the variables, Scaling the Variables\ncategorical variables, Factor Variables in Regression\n(see also factor variables)\ncausation, regression and, Prediction versus Explanation (Profiling)\ncentral limit theorem, Sampling Distribution of a Statistic, Central Limit\nTheorem, Student’s t-Distribution\ndata science and, Student’s t-Distribution\nchi-square distribution, Chi-Squared Test: Statistical Theory\n\n\nchi-square statistic, Chi-Square Test\nchi-square test, Chi-Square Test-Further Reading\ndetecting scientific fraud, Fisher’s Exact Test\nFisher's exact test, Fisher’s Exact Test\nrelevance for data science, Relevance for Data Science\nresampling approach, Chi-Square Test: A Resampling Approach\nstatistical theory, Chi-Squared Test: Statistical Theory\nclass purity, Measuring Homogeneity or Impurity\nclassification, Classification-Summary\ndiscriminant analysis, Discriminant Analysis-Further Reading\ncovariance matrix, Covariance Matrix\nFisher's linear discriminant, Fisher’s Linear Discriminant\nsimple example, A Simple Example\nevaluating models, Evaluating Classification Models-Further Reading\nAUC metric, AUC\nconfusion matrix, Confusion Matrix\nlift, Lift\nprecision, recall, and specificity, Precision, Recall, and Specificity\nrare class problem, The Rare Class Problem\nROC curve, ROC Curve\n\n\nK-Nearest Neighbors, K-Nearest Neighbors\nlogistic regression, Logistic Regression-Further Reading\nand the GLM, Logistic Regression and the GLM\nassessing the model, Assessing the Model\ncomparison to linear regression, Linear and Logistic Regression:\nSimilarities and Differences\ninterpreting coefficients and odds ratios, Interpreting the Coefficients\nand Odds Ratios\nlogistic response function and logit, Logistic Response Function and\nLogit\npredicted values from, Predicted Values from Logistic Regression\nmore than two possible outcomes, Classification\nnaive Bayes algorithm, Naive Bayes-Further Reading\nimpracticality of exact Bayesian classification, Why Exact Bayesian\nClassification Is Impractical\nusing numeric predictor variables, Numeric Predictor Variables\nstrategies for imbalanced data, Strategies for Imbalanced Data-Further\nReading\ncost-based classification, Cost-Based Classification\ndata generation, Data Generation\nexploring the predictions, Exploring the Predictions\noversampling and up/down weighting, Oversampling and Up/Down\n\n\nWeighting\nundersampling, Undersampling\nunsupervised learning as building block, Unsupervised Learning\ncluster mean, K-Means Clustering, A Simple Example, Interpreting the\nClusters\nclustering, Unsupervised Learning\napplication to cold-start problems, Unsupervised Learning\ncluster analysis vs. PCA, Interpreting the Clusters\nhierarchical, Hierarchical Clustering-Measures of Dissimilarity,\nCategorical Data and Gower’s Distance\nagglomerative algorithm, The Agglomerative Algorithm\ndendrogram, The Dendrogram\ndissimilarity measures, Measures of Dissimilarity\nsimple example, A Simple Example\nK-means, K-Means Clustering-Selecting the Number of Clusters, Scaling\nthe Variables\ninterpreting the clusters, Interpreting the Clusters\nK-means algorithm, K-Means Algorithm\nselecting the number of customers, Selecting the Number of Clusters\nsimple example, A Simple Example-K-Means Algorithm\nmodel-based, Model-Based Clustering-Further Reading\n\n\nmixtures of normals, Mixtures of Normals\nselecting the number of clusters, Selecting the Number of Clusters\nproblems with mixed data, Problems with Clustering Mixed Data\nstandardizing data, Standardization (Normalization, Z-Scores)\nclusters, K-Means Clustering\ncoefficient of determination, Assessing the Model\ncoefficients\nin logistic regression, Interpreting the Coefficients and Odds Ratios\nin simple linear regression, The Regression Equation\nestimates vs. known, Fitted Values and Residuals\ninterpretation in multiple linear regression, Example: King County Housing\nData\ncomplete linkage, The Agglomerative Algorithm\ncomplexity parameter (cp), Stopping the Tree from Growing\nconditional probabilities, Naive Bayes\nconditioning variables, Visualizing Multiple Variables\nconfidence intervals, Confidence Intervals-Further Reading, Confidence and\nPrediction Intervals\ngenerating with bootstrap, Confidence Intervals\nlevel of confidence, Confidence Intervals\n\n\nprediction intervals vs., Confidence and Prediction Intervals\nconfidence level, Confidence Intervals-Confidence Intervals\nconfounding variables, Interpreting the Regression Equation, Confounding\nVariables\nconfusion matrix, Evaluating Classification Models-Confusion Matrix\ncontingency tables, Exploring Two or More Variables\nexample, loan grade and status, Two Categorical Variables\ncontinuous data, Elements of Structured Data\ncontinuous variable as test metric, A/B Testing\npredicting continuous value with a tree, Predicting a Continuous Value\ncontour plots, Exploring Two or More Variables\nusing with hexagonal binning, Hexagonal Binning and Contours (Plotting\nNumeric versus Numeric Data)\ncontrast coding systems, Dummy Variables Representation\ncontrol group, A/B Testing\nadvantages of using, Why Have a Control Group?\nCook's distance, Influential Values\ncorrelated variables, Interpreting the Regression Equation\nmulticollinearity, Multicollinearity\npredictor variables, Correlated Predictors\ncorrelation, Correlation-Further Reading\n\n\nkey terms for, Correlation\nregression vs., Simple Linear Regression\nscatterplots, Scatterplots\ncorrelation coefficient, Correlation\ncomputing Pearson's correlation coefficient, Correlation\nkey concepts, Scatterplots\nother types of, Correlation\ncorrelation matrix, Correlation\nexample, correlation between telecommunication stock returns,\nCorrelation\ncost-based classification, Cost-Based Classification\ncount data\nas test metric, A/B Testing\nFisher's exact test for, Fisher’s Exact Test\ncovariance, Discriminant Analysis, Covariance Matrix, Computing the\nPrincipal Components\ncovariance matrix\nin discriminant analysis, Covariance Matrix\nin model-based clustering, Multivariate Normal Distribution\nusing to compute Mahalanobis distance, Distance Metrics\ncross-validation, Cross-Validation, Choosing K\n\n\nfor selection of principal components, Interpreting Principal Components\nusing for hyperparameters in boosting, Hyperparameters and Cross-\nValidation\nusing to estimate value of complexity parameter, Stopping the Tree from\nGrowing\ncumulative gains charts, Lift\nD\nd.f. (degrees of freedom), Degrees of Freedom, Chi-Square Test\n(see also degrees of freedom)\ndata analysis, Exploratory Data Analysis\n(see also exploratory data analysis)\ndata distribution, Exploring the Data Distribution-Further Reading, Sampling\nDistribution of a Statistic\nfrequency tables and histograms, Frequency Table and Histograms\nkey terms for, Exploring the Data Distribution\npercentiles and boxplots, Percentiles and Boxplots\nsampling distribution vs., Sampling Distribution of a Statistic\ndata frames, Rectangular Data\nand indexes, Data Frames and Indexes\ntypical data format, Rectangular Data\ndata generation, Strategies for Imbalanced Data, Data Generation\n\n\ndata snoopng, Selection Bias\ndata types\nkey terms for, Elements of Structured Data\nresources for further reading, Further Reading\ndatabase normalization, Standardization (Normalization, Z-Scores)\ndecile gains charts, Lift\ndecision trees, The Bootstrap, Statistical Machine Learning\nmeaning in operations research, Tree Models\nrecursive partitioning algorithm, Random Forest\ndecomposition of variance, ANOVA, F-Statistic\ndegrees of freedom, Standard Deviation and Related Estimates, Student’s t-\nDistribution, Degrees of Freedom-Further Reading\nin chi-square test, Chi-Squared Test: Statistical Theory\ndendrograms, Hierarchical Clustering\nexample, dendrogram of stocks, The Dendrogram\nhierarchical clustering with mixed variable types, Categorical Data and\nGower’s Distance\ndensity plots, Exploring the Data Distribution, Density Estimates\nexample, density of state murder rates, Density Estimates\ndependent variable, The Regression Equation\n(see also response)\n\n\ndeviation coding, Factor Variables in Regression, Dummy Variables\nRepresentation\ndeviations, Estimates of Variability\nstandard deviation and related estimates, Standard Deviation and Related\nEstimates\ndirectional alternative hypothesis, One-Way, Two-Way Hypothesis Test\ndiscrete data, Elements of Structured Data\ndiscriminant analysis, Discriminant Analysis-Further Reading\ncovariance matrix, Covariance Matrix\nextensions of, A Simple Example\nFisher's linear discriminant, Fisher’s Linear Discriminant\nsimple example, A Simple Example-A Simple Example\ndiscriminant function, Discriminant Analysis\ndiscriminant weights, Discriminant Analysis\ndispersion, Estimates of Variability\n(see also variability, estimates of)\ndissimilarity, Hierarchical Clustering\ncommon measures of, Measures of Dissimilarity\nmeasuring with, complete-linkage method, The Agglomerative Algorithm\nmetric in hierarchical clustering, A Simple Example\ndistance metrics, K-Nearest Neighbors, Hierarchical Clustering\n\n\nGower's distance and categorical data, Categorical Data and Gower’s\nDistance\nin hierarchical clustering, A Simple Example, The Agglomerative Algorithm\nin K-Nearest Neighbors, Distance Metrics\nDonoho, David, Exploratory Data Analysis\ndouble blind studies, Why Have a Control Group?\ndummy variables, Factor Variables in Regression\nrepresentation of factor variables in regression, Dummy Variables\nRepresentation\nrepresenting string factor data as numbers, One Hot Encoder\nDurbin-Watson statistic, Heteroskedasticity, Non-Normality and Correlated\nErrors\nE\nEDA (see exploratory data analysis)\neffect size, Power and Sample Size, Sample Size\nelbow method, Selecting the Number of Clusters\nensemble learning, Statistical Machine Learning\nstaged used of K-Nearest Neighbors, KNN as a Feature Engine\nensemble models, Boosting\nentropy, Measuring Homogeneity or Impurity\nepsilon-greedy algorithm, Multi-Arm Bandit Algorithm\n\n\nerrors, Normal Distribution\nestimates, Estimates of Location\nindicated by hat notation, Fitted Values and Residuals\nEuclidean distance, Distance Metrics\nexact tests, Exhaustive and Bootstrap Permutation Test\nExcel, pivot tables, Two Categorical Variables\nexhaustive permutation tests, Exhaustive and Bootstrap Permutation Test\nexpectation or expected, Chi-Square Test\nexpected value, Exploring Binary and Categorical Data, Expected Value\ncalculating, Expected Value\nexplanation vs. prediction (in regression), Prediction versus Explanation\n(Profiling)\nexploratory data analysis, Exploratory Data Analysis-Summary\nbinary and categorical data, Exploring Binary and Categorical Data-\nCorrelation\ncorrelation, Correlation-Further Reading\ndata distribution, Exploring the Data Distribution-Further Reading\nestimates of location, Estimates of Location-Further Reading\nestimates of variability, Estimates of Variability-Further Reading\nexploring two or more variables, Exploring Two or More Variables-\nSummary\n\n\nrectangular data, Rectangular Data-Estimates of Location\nExploratory Data Analysis (Tukey), Exploratory Data Analysis\nexponential distribution, Poisson and Related Distributions\ncalculating, Exponential Distribution\nextrapolation\ndangers of, The Dangers of Extrapolation\ndefinition of, Prediction Using Regression\nF\nF-statistic, ANOVA, F-Statistic, Assessing the Model\nfacets, Visualizing Multiple Variables\nfactor variables, Factor Variables in Regression-Ordered Factor Variables\ndifferent codings, Dummy Variables Representation\ndummy variables representation, Dummy Variables Representation\nhandling in logistic regression, Fitting the model\nin naive Bayes algorithm, Naive Bayes\nordered, Ordered Factor Variables\nreference coding, Interactions and Main Effects\nwith many levels, Factor Variables with Many Levels\nfactors, conversion of text columns to, Elements of Structured Data\nfailure rate, estimating, Estimating the Failure Rate\n\n\nfalse discovery rate, Multiple Testing, Multiple Testing\nfalse positive rate, AUC\nfeature selection\nchi-square tests in, Relevance for Data Science\nusing discriminant analysis, A Simple Example\nfeatures, Rectangular Data\nterminology differences, Data Frames and Indexes\nfield view (spatial data), Nonrectangular Data Structures\nFisher's exact test, Fisher’s Exact Test\nFisher's linear discriminant, Fisher’s Linear Discriminant\nFisher's scoring, Fitting the model\nFisher, R.A., Fisher’s Exact Test, Discriminant Analysis\nfitted values, Simple Linear Regression, Fitted Values and Residuals\nfolds, Cross-Validation, Hyperparameters and Cross-Validation\nforward selection and backward selection, Model Selection and Stepwise\nRegression\nfrequency tables, Exploring the Data Distribution\nexample, population by state, Frequency Table and Histograms\nFriedman, Jerome H. (Jerry), Statistical Machine Learning\nG\n\n\ngains, Lift\n(see also lift)\nGallup Poll, Random Sampling and Sample Bias\nGallup, George, Random Sampling and Sample Bias, Random Selection\nGalton, Francis, Regression to the Mean\nGAM (see generalized additive models)\nGaussian distribution, Normal Distribution\n(see also normal distribution)\ngeneralized additive models, Polynomial and Spline Regression, Generalized\nAdditive Models, Exploring the Predictions\ngeneralized linear model (GLM), Logistic Regression and the GLM\nGini coefficient, Measuring Homogeneity or Impurity\nGini impurity, Measuring Homogeneity or Impurity\nGLM (see generalized linear model)\nGossett, W.S., Student’s t-Distribution\nGower's distance, Scaling and Categorical Variables\ncategorical data and, Categorical Data and Gower’s Distance\ngradient boosted trees, Interactions and Main Effects\ngradient boosting, The Boosting Algorithm\ndefinition of, Boosting\n\n\ngraphs, Nonrectangular Data Structures\ncomputer science versus statistics, Nonrectangular Data Structures\nlesson on misleading graphs, Further Reading\ngreedy algorithms, Multi-Arm Bandit Algorithm\nH\nhat notation, Fitted Values and Residuals\nhat-value, Testing the Assumptions: Regression Diagnostics, Influential\nValues\nheat maps, Hexagonal Binning and Contours (Plotting Numeric versus\nNumeric Data)\nheteroskedastic errors, Heteroskedasticity, Non-Normality and Correlated\nErrors\nheteroskedasticity, Testing the Assumptions: Regression Diagnostics,\nHeteroskedasticity, Non-Normality and Correlated Errors\nhexagonal binning, Exploring Two or More Variables\nexample, using with contour plot, Hexagonal Binning and Contours\n(Plotting Numeric versus Numeric Data)\nhierarchical clustering, Hierarchical Clustering-Measures of Dissimilarity,\nCategorical Data and Gower’s Distance\nagglomerative algorithm, The Agglomerative Algorithm\nmeasures of dissimilarity, Measures of Dissimilarity\nsimple example, A Simple Example\n\n\nhistograms, Exploring the Data Distribution\nexample, population by state, Frequency Table and Histograms\nhomogeneity, measuring, Measuring Homogeneity or Impurity\nhyperparameters\nand cross-validation in boosting, Hyperparameters and Cross-Validation\nfor HGBoost, Hyperparameters and Cross-Validation\nin random forests, Hyperparameters\nhypothesis tests, Hypothesis Tests-Further Reading\nalternative hypothesis, Alternative Hypothesis\nfalse discovery rate, Multiple Testing\nnull hypothesis, The Null Hypothesis\none-way and two-way tests, One-Way, Two-Way Hypothesis Test\nI\nimpurity, Tree Models\nmeasuring, Measuring Homogeneity or Impurity\nin-sample methods to assess and tune models, Model Selection and Stepwise\nRegression\nindependent variables, Simple Linear Regression, The Regression Equation\nmain effects, Interactions and Main Effects\nindexes, data frames and, Data Frames and Indexes\nindicator variables, Factor Variables in Regression\n\n\ninference, Exploratory Data Analysis, Statistical Experiments and\nSignificance Testing\ninfluence plots, Influential Values\ninfluential values, Testing the Assumptions: Regression Diagnostics,\nInfluential Values\ninformation, Measuring Homogeneity or Impurity\ninteractions, Interpreting the Regression Equation\nand main effects, Interactions and Main Effects\ndeciding which interaction terms to include in the model, Interactions and\nMain Effects\nintercepts, Simple Linear Regression\nin cotton exposure and lung capacity example, The Regression Equation\nInternet of Things (IoT), Elements of Structured Data\ninterquantile range (IQR), Estimates of Variability, Estimates Based on\nPercentiles\ninterval endpoints, Confidence Intervals\nK\nK (in K-Nearest Neighbors), K-Nearest Neighbors\nk-fold cross-validation, Cross-Validation\nK-means clustering, K-Means Clustering-Selecting the Number of Clusters\ninterpreting the clusters, Interpreting the Clusters\n\n\nK-means algorithm, K-Means Algorithm\nselecting the number of clusters, Selecting the Number of Clusters\nsimple example, A Simple Example-K-Means Algorithm\nusing on unnormalized and normalized variables, Scaling the Variables\nK-Nearest Neighbors, Predicted Values from Logistic Regression, K-Nearest\nNeighbors-KNN as a Feature Engine\nas a feature engine, KNN as a Feature Engine\nchoosing K, Choosing K\ndistance metrics, Distance Metrics\nexample, predicting loan default, A Small Example: Predicting Loan\nDefault\none hot encoder, One Hot Encoder\nstandardization, Standardization (Normalization, Z-Scores)\nkernal density estimates, Density Estimates\nKernSmooth package, Density Estimates\nKNN (see K-Nearest Neighbors)\nknots, Polynomial and Spline Regression, Splines\nkurtosis, Frequency Table and Histograms\nL\nlambda, in Poisson and related distributions, Poisson and Related\nDistributions\n\n\nLasso regression, Model Selection and Stepwise Regression, Regularization:\nAvoiding Overfitting\nLatent Dirichlet Allocation (LDA), Discriminant Analysis\nleaf, Tree Models\nleast squares, Simple Linear Regression, Least Squares\nleverage, Testing the Assumptions: Regression Diagnostics\ninfluential values in regression, Influential Values\nlift, Evaluating Classification Models, Lift\nlift curve, Lift\nlinear discriminant analysis (LDA), Discriminant Analysis, Exploring the\nPredictions\nlinear regression, Simple Linear Regression-Weighted Regression\ncomparison to logistic regression, Linear and Logistic Regression:\nSimilarities and Differences\nfitted values and residuals, Fitted Values and Residuals\ngeneralized linear model (GLM), Logistic Regression and the GLM\nleast squares, Least Squares\nmultiple, Multiple Linear Regression-Weighted Regression\nassessing the model, Assessing the Model\ncross-validation, Cross-Validation\nexample, King County housing data, Example: King County Housing\n\n\nData\nmodel selection and stepwise regression, Model Selection and Stepwise\nRegression\nweighted regression, Weighted Regression\nprediction vs. explanation, Prediction versus Explanation (Profiling)\nregression equation, The Regression Equation\nLiterary Digest poll of 1936, Random Sampling and Sample Bias, Random\nSelection\nloadings, Principal Components Analysis, A Simple Example\nfor top five components (example), Interpreting Principal Components\nlog odds, Logistic Regression\nlog-odds function (see logit function)\nlog-odds ratio, Interpreting the Coefficients and Odds Ratios\nlogistic regression, Logistic Regression-Further Reading, Exploring the\nPredictions\nand the generalized linear model (GLM), Logistic Regression and the\nGLM\nassessing the model, Assessing the Model\ncomparison to linear regression, Linear and Logistic Regression:\nSimilarities and Differences\ninterpreting the coefficients and odds ratios, Interpreting the Coefficients\nand Odds Ratios\n\n\nlogistic response function and logit, Logistic Response Function and Logit\npredicted values from, Predicted Values from Logistic Regression\nlogit function, Logistic Regression, Logistic Response Function and Logit\nlong-tail distributions, Long-Tailed Distributions-Further Reading\nloss, Tree Models\nloss function, Oversampling and Up/Down Weighting\nM\nmachine learning\nstatistics vs., Statistical Machine Learning\nmachine learnng, Statistical Machine Learning\n(see also statistical machine learning)\nMahalanobis distance, Covariance Matrix, Distance Metrics\nmain effects, Interpreting the Regression Equation\ninteractions and, Interactions and Main Effects\nMallows Cp, Model Selection and Stepwise Regression\nManhattan distance, Distance Metrics, Regularization: Avoiding Overfitting,\nCategorical Data and Gower’s Distance\nmaximum likelihood estimation (MLE), Fitting the model\nmean, Estimates of Location\nformula for, Mean\n\n\nregression to, Regression to the Mean\nsample mean vs. population mean, Sample Mean versus Population Mean\ntrimmed mean, Mean\nweighted mean, Mean\nmean absolute deviation, Estimates of Variability, A/B Testing\nformula for calculating, Standard Deviation and Related Estimates\nmean absolute deviation from the median (MAD), Standard Deviation and\nRelated Estimates\nmedian, Estimates of Location\nand robust estimates, Median and Robust Estimates\nmedian absolute deviation, Estimates of Variability\nmetrics, Estimates of Location\nminimum variance, Measures of Dissimilarity\nMLE (see maximum likelihood estimation)\nmode, Exploring Binary and Categorical Data\nexamples in categorical data, Mode\nmodel-based clustering, Model-Based Clustering-Further Reading\nlimitations, Selecting the Number of Clusters\nmixtures of normals, Mixtures of Normals\nmultivariate normal distribution, Multivariate Normal Distribution\n\n\nselecting the number of clusters, Selecting the Number of Clusters\nmoments, Frequency Table and Histograms\nmulti-arm bandits, Why Just A/B? Why Not C, D…?, Multi-Arm Bandit\nAlgorithm-Further Reading\ndefinition of, Multi-Arm Bandit Algorithm\nmulticollinearity, Interpreting the Regression Equation, Multicollinearity\nproblems with one hot encoding, One Hot Encoder\nmulticollinearity errors, Degrees of Freedom, Dummy Variables\nRepresentation\nmultiple linear regression (see linear regression)\nmultiple testing, Multiple Testing-Further Reading\nbottom line for data scientists, Multiple Testing\nmultivariate analysis, Exploring Two or More Variables\nmultivariate normal distribution, Multivariate Normal Distribution\nN\nn (sample size), Student’s t-Distribution\nn or sample size, Degrees of Freedom\nnaive Bayes algorithm, Naive Bayes-Further Reading\napplying to numeric predictor variables, Numeric Predictor Variables\nneighbors, K-Nearest Neighbors\nnetwork data structures, Nonrectangular Data Structures\n\n\nnodes, Tree Models\nnon-normal residuals, Testing the Assumptions: Regression Diagnostics\nnonlinear regression, Polynomial and Spline Regression-Further Reading\ndefinition of, Polynomial and Spline Regression\nnonrectangular data structures, Nonrectangular Data Structures\nnormal distribution, Normal Distribution-Standard Normal and QQ-Plots\nkey concepts, Standard Normal and QQ-Plots\nstandard normal and QQ-Plots, Standard Normal and QQ-Plots\nnormalization, Standard Normal and QQ-Plots, Standardization\n(Normalization, Z-Scores), K-Means Clustering\ncategorical variables before clustering, Scaling the Variables\ndata distribution and, Standardization (Normalization, Z-Scores)\nin statistics vs. database context, Standardization (Normalization, Z-\nScores)\nnull hypothesis, Hypothesis Tests, The Null Hypothesis\nnumeric variables\ngrouped according to a categorical variable, Categorical and Numeric Data\nnumeric predictor variables for naive Bayes, Numeric Predictor Variables\nnumerical data as categorical data, Exploring Binary and Categorical Data\nO\nobject representation (spatial data), Nonrectangular Data Structures\n\n\nOccam's razor, Model Selection and Stepwise Regression\nodds, Logistic Regression, Logistic Response Function and Logit\nodds ratios, Interpreting the Coefficients and Odds Ratios\nlog-odds ratio and, Interpreting the Coefficients and Odds Ratios\nomnibus tests, ANOVA\none hot encoder, Factor Variables in Regression, One Hot Encoder\none hot encoding, Dummy Variables Representation\none-way tests, Hypothesis Tests, One-Way, Two-Way Hypothesis Test\norder statistics, Estimates of Variability, Estimates Based on Percentiles\nordered factor variables, Ordered Factor Variables\nordinal data, Elements of Structured Data\nimportance of the concept, Elements of Structured Data\nordinary least squares (OLS), Least Squares, Heteroskedasticity, Non-\nNormality and Correlated Errors\n(see also least squares)\nout-of-bag (OOB) estimate of error, Random Forest\noutcome, Rectangular Data\noutliers, Estimates of Location, Outliers, Testing the Assumptions:\nRegression Diagnostics\nin regression, Outliers\nsensitivity of correlation coefficient to, Correlation\n\n\nsensitivity of least squares to, Least Squares\nvariance, standard deviation, mean absolute deviation and, Standard\nDeviation and Related Estimates\noverfitting, Multiple Testing\navoiding in boosting using regularization, Regularization: Avoiding\nOverfitting\nin linear regression, Model Selection and Stepwise Regression\noversampling, Strategies for Imbalanced Data, Oversampling and Up/Down\nWeighting\nP\np-values, Statistical Significance and P-Values, P-Value\nadjusting, Multiple Testing\ndata science and, Data Science and P-Values\nt-statistic and, Assessing the Model\nvalue of, Value of the p-value\npairwise comparisons, ANOVA\npartial residual plots, Testing the Assumptions: Regression Diagnostics,\nPartial Residual Plots and Nonlinearity\nin logistic regression, Assessing the Model\nPCA (see principal components analysis)\nPearson residuals, Chi-Square Test: A Resampling Approach\n\n\nPearson's chi-squared test, Chi-Squared Test: Statistical Theory\nPearson's correlation coefficient, Correlation\nPearson, Karl, Chi-Square Test, Principal Components Analysis\npenalized regression, Model Selection and Stepwise Regression\npercentiles, Estimates of Variability\nand boxplots, Percentiles and Boxplots\nestimates based on, Estimates Based on Percentiles\nprecise definition of, Estimates Based on Percentiles\npermission, obtaining for human subject testing, Why Just A/B? Why Not C,\nD…?\npermutation tests, Resampling\nexhaustive and bootstrap, Exhaustive and Bootstrap Permutation Test\nfor ANOVA, ANOVA\nvalue for data science, Permutation Tests: The Bottom Line for Data\nScience\nweb stickiness example, Example: Web Stickiness\npertinent records (in searches), Size versus Quality: When Does Size\nMatter?\nphysical networks, Nonrectangular Data Structures\npie charts, Exploring Binary and Categorical Data\n\n\npivot tables (Excel), Two Categorical Variables\npoint estimates, Confidence Intervals\nPoisson distributions, Poisson and Related Distributions, Generalized Linear\nModels\ncalculating, Poisson Distributions\npolynomial coding, Dummy Variables Representation\npolynomial regression, Polynomial and Spline Regression, Polynomial\npopulation, Random Sampling and Sample Bias\nsample mean vs. population mean, Sample Mean versus Population Mean\nposterior probability, Naive Bayes, The Naive Solution\npower and sample size, Power and Sample Size-Further Reading\nprecision, Evaluating Classification Models\nin classification models, Precision, Recall, and Specificity\npredicted values, Fitted Values and Residuals\n(see also fitted values)\nprediction\nexplanation vs., in linear regression, Prediction versus Explanation\n(Profiling)\nharnessing results from multiple trees, How Trees Are Used\nK-Nearest Neighbors, K-Nearest Neighbors\nusing as first stage, KNN as a Feature Engine\n\n\npredicted values from logistic regression, Predicted Values from Logistic\nRegression\nunsupervised learning and, Unsupervised Learning\nusing regression, Prediction Using Regression-Factor Variables in\nRegression\nconfidence and prediction intervals, Confidence and Prediction Intervals\ndangers of extrapolation, The Dangers of Extrapolation\nprediction intervals, Prediction Using Regression\nconfidence intervals vs., Confidence and Prediction Intervals\npredictor variables, Data Frames and Indexes, The Regression Equation\n(see also independent variables)\ncorrelated, Correlated Predictors\nin linear discriminant analysis, more than two, A Simple Example\nin naive Bayes algorithm, Naive Bayes\nmain effects, Interactions and Main Effects\nnumeric, applying naive Bayes to, Numeric Predictor Variables\nrelationship between response and, Partial Residual Plots and Nonlinearity\nprincipal components, Principal Components Analysis\nprincipal components analysis, Principal Components Analysis-Further\nReading\ncluster analysis vs., Interpreting the Clusters\n\n\ncomputing the principal components, Computing the Principal Components\ninterpreting principal components, Interpreting Principal Components\nscaling the variables, Scaling the Variables\nsimple example, A Simple Example-A Simple Example\nstandardizing data, Standardization (Normalization, Z-Scores)\nprobability theory, Exploratory Data Analysis\nprofiling vs. explanation, Prediction versus Explanation (Profiling)\npropensity score, Classification\nproxy variables, Example: Web Stickiness\npruning, Tree Models, Stopping the Tree from Growing\npseudo-residuals, The Boosting Algorithm\nQ\nQQ-Plots, Normal Distribution\nexample, returns for Netflix, Long-Tailed Distributions\nstandard normal and, Standard Normal and QQ-Plots\nquadratic discriminant analysis, A Simple Example\nquantiles, Estimates Based on Percentiles\nR function, quantile, Estimates Based on Percentiles\nR\nR-squared, Multiple Linear Regression, Assessing the Model\n\n\nrandom forests, Interactions and Main Effects, Tree Models, Random Forest-\nHyperparameters\nbetter predictive performance than single trees, How Trees Are Used\ndetermining variable importance, Variable Importance\nhyperparameters, Hyperparameters\nrandom sampling, Random Sampling and Sample Bias-Further Reading\nbias, Bias\nkey terms for, Random Sampling and Sample Bias\nrandom selection, Random Selection\nsample mean vs. population mean, Sample Mean versus Population Mean\nsize versus quality, Size versus Quality: When Does Size Matter?\nrandom subset of variables, Random Forest\nrandomization, A/B Testing\nrandomization tests, Resampling\n(see also permutation tests)\nrandomness, misinterpreting, Hypothesis Tests\nrange, Estimates of Variability, Estimates Based on Percentiles\nrare class problem, The Rare Class Problem\nrecall, Evaluating Classification Models, Precision, Recall, and Specificity\nreceiver operating characteristics (see ROC curve)\n\n\nrecords, Rectangular Data, Simple Linear Regression\nrectangular data, Rectangular Data-Estimates of Location\nterminology differences, Data Frames and Indexes\nrecursive partitioning, Tree Models, The Recursive Partitioning Algorithm,\nRandom Forest\nreference coding, Factor Variables in Regression-Dummy Variables\nRepresentation, Interactions and Main Effects, Logistic Regression and the\nGLM\nregression, Regression and Prediction-Summary\ncausation and, Prediction versus Explanation (Profiling)\ndiagnostics, Testing the Assumptions: Regression Diagnostics-Polynomial\nand Spline Regression\nheteroskedasticity, non-normality, and correlated errors,\nHeteroskedasticity, Non-Normality and Correlated Errors\ninfluential values, Influential Values\noutliers, Outliers\nparial residual plots and nonlinearity, Partial Residual Plots and\nNonlinearity\nusing scatterplot smoothers, Heteroskedasticity, Non-Normality and\nCorrelated Errors\ndifferent meanings of the term, Least Squares\nfactor variables in, Factor Variables in Regression-Ordered Factor\nVariables\n\n\nordered factor variables, Ordered Factor Variables\nwith many levels, Factor Variables with Many Levels\ninterpreting the regression equation, Interpreting the Regression Equation-\nInteractions and Main Effects\nconfounding variables, Confounding Variables\ncorrelated predictors, Correlated Predictors\ninteractions and main effects, Interactions and Main Effects\nmulticollinearity, Multicollinearity\nKNN (K-Nearest Neighbors), KNN as a Feature Engine\nlogistic regression, Logistic Regression-Further Reading\ncomparison to linear regression, Linear and Logistic Regression:\nSimilarities and Differences\nmultiple linear regression, Multiple Linear Regression-Weighted\nRegression\npolynomial and spline regression, Polynomial and Spline Regression-\nSummary\ngeneralized additive models, Generalized Additive Models\npolynomial regression, Polynomial\nsplines, Splines\nprediction with, Prediction Using Regression-Factor Variables in\nRegression\nconfidence and prediction intervals, Confidence and Prediction Intervals\n\n\ndangers of extrapolation, The Dangers of Extrapolation\nridge regression, Regularization: Avoiding Overfitting\nsimple linear regression, Simple Linear Regression-Further Reading\nfitted values and residuals, Fitted Values and Residuals\nleast squares, Least Squares\nprediction vs. explanation, Prediction versus Explanation (Profiling)\nregression equation, The Regression Equation\nunsupervised learning as building block, Unsupervised Learning\nwith a tree, Predicting a Continuous Value\nregression coefficient, Simple Linear Regression\nin cotton exposure and lung capacity example, The Regression Equation\nregression to the mean, Regression to the Mean\nregularization, Boosting\navoding overfitting with, Regularization: Avoiding Overfitting\nreplacement (in sampling), Random Sampling and Sample Bias\nbootstrap, The Bootstrap\nrepresentativeness, Random Sampling and Sample Bias\nresampling, The Bootstrap, Resampling-For Further Reading\nbootstrapping vs., Resampling versus Bootstrapping\npermutation tests, Permutation Test\n\n\nexhaustive and bootstrap tests, Exhaustive and Bootstrap Permutation\nTest\nvalue for data science, Permutation Tests: The Bottom Line for Data\nScience\nweb stickiness example, Example: Web Stickiness\nusing in chi-square test, Chi-Square Test: A Resampling Approach\nresidual standard error, Multiple Linear Regression, Assessing the Model\nresidual sum of squares, Least Squares\n(see also least squares)\nresiduals, Simple Linear Regression, Fitted Values and Residuals\ncomputing, Fitted Values and Residuals\ndistribution of, Heteroskedasticity, Non-Normality and Correlated Errors\nstandardized, Testing the Assumptions: Regression Diagnostics\nresponse, Simple Linear Regression, The Regression Equation\nrelationship between predictor variable and, Partial Residual Plots and\nNonlinearity\nridge regression, Model Selection and Stepwise Regression, Regularization:\nAvoiding Overfitting\nrobust, Estimates of Location\nrobust estimates of location\nexample, population and murder rate by state, Example: Location\nEstimates of Population and Murder Rates\n\n\nmean absolute deviation from the median, Standard Deviation and Related\nEstimates\nmedian, Median and Robust Estimates\noutliers and, Outliers\nROC curve, ROC Curve\nroot mean squared error (RMSE), Multiple Linear Regression, Assessing the\nModel, Predicting a Continuous Value\nRSE (see residual standard error)\nRSS (residual sum of squares), Least Squares\n(see also least squares)\nS\nsample bias, Random Sampling and Sample Bias, Random Sampling and\nSample Bias\nsample statistic, Sampling Distribution of a Statistic\nsamples\ndefinition of, Random Sampling and Sample Bias\nsample size, power and, Power and Sample Size-Further Reading\nterminology differences, Data Frames and Indexes\nsampling, Data and Sampling Distributions-Summary\nbinomial distribution, Binomial Distribution-Further Reading\nbootstrap, The Bootstrap-Further Reading\n\n\nconfidence intervals, Confidence Intervals-Further Reading\nlong-tail distributions, Long-Tailed Distributions-Further Reading\nnormal distribution, Normal Distribution-Standard Normal and QQ-Plots\noversampling imbalanced data, Oversampling and Up/Down Weighting\nPoisson and related distributions, Poisson and Related Distributions-\nSummary\nestimating failure rate, Estimating the Failure Rate\nexponential distribution, Exponential Distribution\nPoisson distribution, Poisson Distributions\nWeibull distribution, Weibull Distribution\npopulation versus sample, Data and Sampling Distributions\nrandom sampling and sample bias, Random Sampling and Sample Bias-\nFurther Reading\nsampling distribution of a statistic, Sampling Distribution of a Statistic-\nFurther Reading\nselection bias, Selection Bias-Further Reading\nStudent's t-distribution, Student’s t-Distribution-Further Reading\nThompson's sampling, Multi-Arm Bandit Algorithm\nundersampling imbalanced data, Undersampling\nwith and without replacement, Random Sampling and Sample Bias, The\nBootstrap, Resampling\n\n\nsampling distribution, Sampling Distribution of a Statistic-Further Reading\ncentral limit theorem, Central Limit Theorem\ndata distribution vs., Sampling Distribution of a Statistic\nstandard error, Standard Error\nscale parameter (Weibull distribution), Weibull Distribution\nscaling data and categorical variables, Scaling and Categorical Variables-\nSummary\ndominant variables, Dominant Variables\nGower's distance and categorical data, Categorical Data and Gower’s\nDistance\nproblems clustering mixed data, Problems with Clustering Mixed Data\nscaling the variables, Scaling the Variables\nscatterplot smoothers, Heteroskedasticity, Non-Normality and Correlated\nErrors\nscatterplots, Correlation\nexample, returns for ATT and Verizon, Scatterplots\nscientific fraud, detecting, Fisher’s Exact Test\nscreeplots, Principal Components Analysis, Interpreting Principal\nComponents\nfor PCA of top stocks, Dominant Variables\nsearches\nsearch queries on Google, Size versus Quality: When Does Size Matter?\n\n\nvast search effect, Selection Bias\nselection bias, Selection Bias-Further Reading\nregression to the mean, Regression to the Mean\nself-selection sampling bias, Random Sampling and Sample Bias\nsensitivity, Evaluating Classification Models, Precision, Recall, and\nSpecificity\nshape parameter (Weibull distribution), Weibull Distribution\nsignal-to-noise ratio, Choosing K\nsignificance level, Power and Sample Size, Sample Size\nsignificance tests, Hypothesis Tests, Data Science and P-Values\n(see also hypothesis tests)\nsimple random sample, Random Sampling and Sample Bias\nsingle linkage, Measures of Dissimilarity\nskew, Long-Tailed Distributions\nskewness, Frequency Table and Histograms\nslope, Simple Linear Regression\n(see also regression coefficient)\nin regression equation, The Regression Equation\nSMOTE algorithm, Data Generation\nspatial data structures, Nonrectangular Data Structures\n\n\nspecificity, Evaluating Classification Models, Precision, Recall, and\nSpecificity\nspline regression, Polynomial and Spline Regression, Splines\nsplines, Splines\nsplit value, Tree Models\nsquare-root of n rule, Standard Error\nSS (sum of squares), ANOVA\nwithing cluster sum of squares, K-Means Clustering\nstandard deviation, Estimates of Variability\nand related estimates, Standard Deviation and Related Estimates\ncovariance matrix and, Covariance Matrix\nin statistical testing output, A/B Testing\nsensitivity to outliers, Standard Deviation and Related Estimates\nstandard error vs., Standard Error\nstandard error, Sampling Distribution of a Statistic\nformula for calculating, Standard Error\nstandard deviation vs., Standard Error\nstandard normal distribution, Normal Distribution, Standard Normal and QQ-\nPlots\nstandardization, Standard Normal and QQ-Plots, K-Nearest Neighbors, K-\nMeans Clustering\n\n\nin K-Nearest Neighbors, Standardization (Normalization, Z-Scores)\nstandardized residuals, Testing the Assumptions: Regression Diagnostics\nexamining to detect outliers, Outliers\nstatistical experiments and significance testing, Statistical Experiments and\nSignificance Testing-Summary\nA/B testing, A/B Testing-For Further Reading\nchi-square test, Chi-Square Test-Further Reading\ndegrees of freedom, Degrees of Freedom-Further Reading\nhypothesis tests, Hypothesis Tests-Further Reading\nmulti-arm bandit algorithm, Multi-Arm Bandit Algorithm-Further Reading\nmultiple tests, Multiple Testing-Further Reading\npower and sample size, Power and Sample Size-Further Reading\nresampling, Resampling-Statistical Significance and P-Values\nstatistical significance and p-values, Statistical Significance and P-Values-\nFurther Reading\nalpha, Alpha\ndata science and p-values, Data Science and P-Values\np-values, P-Value\ntype 1 and type 2 errors, Type 1 and Type 2 Errors\nvalue of p-values, Value of the p-value\n\n\nt-tests, t-Tests-Further Reading\nstatistical inference, classical inference pipeline, Statistical Experiments and\nSignificance Testing\nstatistical machine learning, Statistical Machine Learning-Summary\nbagging and the random forest, Bagging and the Random Forest-\nHyperparameters\nboosting, Boosting-Summary\navoiding overfitting using regularization, Regularization: Avoiding\nOverfitting\nhyperparameters and cross-validation, Hyperparameters and Cross-\nValidation\nXGBoost, XGBoost\nK-Nearest Neighbors, K-Nearest Neighbors-KNN as a Feature Engine\nas a feature engine, KNN as a Feature Engine\nchoosing K, Choosing K\ndistance metrics, Distance Metrics\nexample, predicting loan default, A Small Example: Predicting Loan\nDefault\none hot encoder, One Hot Encoder\nstandardization, Standardization (Normalization, Z-Scores)\ntree models, Tree Models-Further Reading\nmeasuring homogeneity or impurity, Measuring Homogeneity or\n\n\nImpurity\npredicting a continuous value, Predicting a Continuous Value\nrecursive partitioning algorithm, The Recursive Partitioning Algorithm\nsimple example, A Simple Example\nstopping tree growth, Stopping the Tree from Growing\nuses of trees, How Trees Are Used\nstatistical moments, Frequency Table and Histograms\nstatistical significance, Permutation Test\nstatistics vs. machine learning, Statistical Machine Learning\nstepwise regression, Model Selection and Stepwise Regression\nstochastic gradient boosting, The Boosting Algorithm\ndefinition of, Boosting\nXGBoost implementation, XGBoost-Hyperparameters and Cross-\nValidation\nstratified sampling, Random Sampling and Sample Bias, Random Selection\nstructured data, Elements of Structured Data-Further Reading\nStudent's t-distribution, Student’s t-Distribution-Further Reading\nsubjects, A/B Testing\nsuccess, Binomial Distribution\n\n\nsum contrasts, Dummy Variables Representation\nT\nt-distributions, Student’s t-Distribution-Further Reading, t-Tests\ndata science and, Student’s t-Distribution\nt-statistic, t-Tests, Multiple Linear Regression, Assessing the Model\nt-tests, t-Tests-Further Reading\ntail, Long-Tailed Distributions\ntarget shuffling, Selection Bias\ntest sample, Evaluating Classification Models\ntest statistic, A/B Testing, t-Tests\nselecting before the experiment, Why Have a Control Group?\nThompson sampling, Multi-Arm Bandit Algorithm\ntime series data, Nonrectangular Data Structures\ntime-to-failure analysis, Weibull Distribution\ntreatment, A/B Testing\ntreatment group, A/B Testing\ntree models, Interactions and Main Effects, Exploring the Predictions, Tree\nModels\nhow trees are used, How Trees Are Used\nmeasuring homogeneity or impurity, Measuring Homogeneity or Impurity\n\n\npredicting a continuous value, Predicting a Continuous Value\nrecursive partitioning algorithm, The Recursive Partitioning Algorithm\nsimple example, A Simple Example\nstopping tree growth, Stopping the Tree from Growing\nTrellis graphics, Visualizing Multiple Variables\ntrials, Binomial Distribution\ntrimmed mean, Estimates of Location\nformula for, Mean\nTukey, John Wilder, Exploratory Data Analysis\ntwo-way tests, Hypothesis Tests, One-Way, Two-Way Hypothesis Test\ntype 1 errors, Statistical Significance and P-Values, Type 1 and Type 2\nErrors, Multiple Testing\ntype 2 errors, Statistical Significance and P-Values, Type 1 and Type 2 Errors\nU\nunbiased estimates, Standard Deviation and Related Estimates\nundersampling, Undersampling\nuniform random distribution, Fisher’s Exact Test\nunivariate analysis, Exploring Two or More Variables\nunsupervised learning, Unsupervised Learning-Summary\nand prediction, Unsupervised Learning\n\n\nhierarchical clustering, Hierarchical Clustering-Measures of Dissimilarity\nagglomerative algorithm, The Agglomerative Algorithm\ndendrogram, The Dendrogram\ndissimilarity measures, Measures of Dissimilarity\nsimple example, A Simple Example\nK-means clustering, K-Means Clustering-Selecting the Number of Clusters\ninterpreting the clusters, Interpreting the Clusters\nK-means algorithm, K-Means Algorithm\nselecting the number of customers, Selecting the Number of Clusters\nsimple example, A Simple Example-K-Means Algorithm\nmodel-based clustering, Model-Based Clustering-Further Reading\nmixtures of normals, Mixtures of Normals\nmultivariate normal distribution, Multivariate Normal Distribution\nselecting the number of clusters, Selecting the Number of Clusters\nprincipal components analysis, Principal Components Analysis-Further\nReading\ncomputing the principal components, Computing the Principal\nComponents\ninterpreting principal components, Interpreting Principal Components\nsimple example, A Simple Example-A Simple Example\nscaling and categorical variables, Scaling and Categorical Variables-\n\n\nSummary\ndominant variables, Dominant Variables\nGower's distance and categorical data, Categorical Data and Gower’s\nDistance\nproblems clustering mixed data, Problems with Clustering Mixed Data\nscaling the variables, Scaling the Variables\nup weight or down weight, Strategies for Imbalanced Data, Oversampling and\nUp/Down Weighting\nuplift vs. lift, Lift\nV\nvalidation sample, Evaluating Classification Models\nvariability\nvariability, estimates of, Estimates of Variability-Further Reading\nexample, murder rate by state population, Example: Variability Estimates\nof State Population\nkey terminology, Estimates of Variability\npercentiles, Estimates Based on Percentiles\nstandard deviation and related estimates, Standard Deviation and Related\nEstimates\nvariables\nexploring two or more, Exploring Two or More Variables-Summary\ncategorical and numeric data, Categorical and Numeric Data\n\n\nhexagonal binning and contours, Hexagonal Binning and Contours\n(Plotting Numeric versus Numeric Data)\nkey concepts, Visualizing Multiple Variables\nvisualizing multiple variables, Visualizing Multiple Variables\nimportance of, determining in random forests, Variable Importance\nrescaling with z-scores, Standardization (Normalization, Z-Scores)\nvariance, Estimates of Variability\nanalysis of (ANOVA), ANOVA\nformula for calculating, Standard Deviation and Related Estimates\nsensitivity to outliers, Standard Deviation and Related Estimates\nvast search effect, Selection Bias\nviolin plots, Exploring Two or More Variables\ncombining with a boxplot, example, Categorical and Numeric Data\nW\nWard's method, Measures of Dissimilarity\nweb stickiness example (permutation test), Example: Web Stickiness\nweb testing\nbandit algorithms in, Multi-Arm Bandit Algorithm\ndeciding how long a test should run, Power and Sample Size\nWeibull distribution, Poisson and Related Distributions\n\n\ncalculating, Weibull Distribution\nweighted mean, Estimates of Location\nexpected value, Expected Value\nweighted median, Estimates of Location, Median and Robust Estimates\nformula for calculating, Mean\nweighted regression, Multiple Linear Regression, Weighted Regression\nweights, Simple Linear Regression\ncomponent loadings, A Simple Example\nwhiskers (in boxplots), Percentiles and Boxplots\nwins, Multi-Arm Bandit Algorithm\nwithin cluster sum of squares (SS), K-Means Clustering\nX\nXGBoost, XGBoost-Hyperparameters and Cross-Validation\nhyperparameters, Hyperparameters and Cross-Validation\nZ\nz-distribution, Standard Normal and QQ-Plots\n(see also normal distribution)\nz-score, Normal Distribution, Strategies for Imbalanced Data, K-Nearest\nNeighbors, Standardization (Normalization, Z-Scores)\nconverting data to, Standard Normal and QQ-Plots\nrescaling variables, Standardization (Normalization, Z-Scores)\n\n\nAbout the Authors\nPeter Bruce founded and grew the Institute for Statistics Education at\nStatistics.com, which now offers about 100 courses in statistics, roughly a third of\nwhich are aimed at the data scientist. In recruiting top authors as instructors and\nforging a marketing strategy to reach professional data scientists, Peter has\ndeveloped both a broad view of the target market and his own expertise to reach\nit.\nAndrew Bruce has over 30 years of experience in statistics and data science in\nacademia, government, and business. He has a PhD in statistics from the\nUniversity of Washington and has published numerous papers in refereed journals.\nHe has developed statistical-based solutions to a wide range of problems faced\nby a variety of industries, from established financial firms to internet startups, and\noffers a deep understanding of the practice of data science.\n\n\nColophon\nThe animal on the cover of Practical Statistics for Data Scientists is a lined\nshore crab (Pachygrapsus crassipes), also known as a striped shore crab. It is\nfound along the coasts and beaches of the Pacific Ocean in North America,\nCentral America, Korea, and Japan. These crustaceans live under rocks, in\ntidepools, and within crevices. They spend about half their time on land, and\nperiodically return to the water to wet their gills.\nThe lined shore crab is named for the green stripes on its brown-black carapace.\nIt has red claws and purple legs, which also have a striped or mottled pattern. The\ncrab generally grows to be 3–5 centimeters in size; females are slightly smaller.\nTheir eyes are on flexible stalks that can rotate to give them a full field of vision\nas they walk.\nCrabs are omnivores, feeding primarily on algae, but also mollusks, worms, fungi,\ndead animals, and other crustaceans (depending on what is available). They moult\nmany times as they grow to adulthood, taking in water to expand and crack open\ntheir old shell. Once this is achieved, they spend several difficult hours getting\nfree, and then must hide until the new shell hardens.\nMany of the animals on O’Reilly covers are endangered; all of them are important\nto the world. To learn more about how you can help, go to animals.oreilly.com.\nThe cover image is from Pictorial Museum of Animated Nature. The cover fonts\nare URW Typewriter and Guardian Sans. The text font is Adobe Minion Pro; the\nheading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s\nUbuntu Mono.\n\n\nPreface\nWhat to Expect\nConventions Used in This Book\nUsing Code Examples\nSafari® Books Online\nHow to Contact Us\nAcknowledgments\n1. Exploratory Data Analysis\nElements of Structured Data\nFurther Reading\nRectangular Data\nData Frames and Indexes\nNonrectangular Data Structures\nFurther Reading\nEstimates of Location\nMean\nMedian and Robust Estimates\nExample: Location Estimates of Population and Murder Rates\nFurther Reading\nEstimates of Variability\nStandard Deviation and Related Estimates\nEstimates Based on Percentiles\n\n\nExample: Variability Estimates of State Population\nFurther Reading\nExploring the Data Distribution\nPercentiles and Boxplots\nFrequency Table and Histograms\nDensity Estimates\nFurther Reading\nExploring Binary and Categorical Data\nMode\nExpected Value\nFurther Reading\nCorrelation\nScatterplots\nFurther Reading\nExploring Two or More Variables\nHexagonal Binning and Contours (Plotting Numeric versus\nNumeric Data)\nTwo Categorical Variables\nCategorical and Numeric Data\nVisualizing Multiple Variables\nFurther Reading\nSummary\n\n\n2. Data and Sampling Distributions\nRandom Sampling and Sample Bias\nBias\nRandom Selection\nSize versus Quality: When Does Size Matter?\nSample Mean versus Population Mean\nFurther Reading\nSelection Bias\nRegression to the Mean\nFurther Reading\nSampling Distribution of a Statistic\nCentral Limit Theorem\nStandard Error\nFurther Reading\nThe Bootstrap\nResampling versus Bootstrapping\nFurther Reading\nConfidence Intervals\nFurther Reading\nNormal Distribution\nStandard Normal and QQ-Plots\nLong-Tailed Distributions\nFurther Reading\n\n\nStudent’s t-Distribution\nFurther Reading\nBinomial Distribution\nFurther Reading\nPoisson and Related Distributions\nPoisson Distributions\nExponential Distribution\nEstimating the Failure Rate\nWeibull Distribution\nFurther Reading\nSummary\n3. Statistical Experiments and Significance Testing\nA/B Testing\nWhy Have a Control Group?\nWhy Just A/B? Why Not C, D…?\nFor Further Reading\nHypothesis Tests\nThe Null Hypothesis\nAlternative Hypothesis\nOne-Way, Two-Way Hypothesis Test\nFurther Reading\nResampling\nPermutation Test\n\n\nExample: Web Stickiness\nExhaustive and Bootstrap Permutation Test\nPermutation Tests: The Bottom Line for Data Science\nFor Further Reading\nStatistical Significance and P-Values\nP-Value\nAlpha\nType 1 and Type 2 Errors\nData Science and P-Values\nFurther Reading\nt-Tests\nFurther Reading\nMultiple Testing\nFurther Reading\nDegrees of Freedom\nFurther Reading\nANOVA\nF-Statistic\nTwo-Way ANOVA\nFurther Reading\nChi-Square Test\nChi-Square Test: A Resampling Approach\nChi-Squared Test: Statistical Theory\n\n\nFisher’s Exact Test\nRelevance for Data Science\nFurther Reading\nMulti-Arm Bandit Algorithm\nFurther Reading\nPower and Sample Size\nSample Size\nFurther Reading\nSummary\n4. Regression and Prediction\nSimple Linear Regression\nThe Regression Equation\nFitted Values and Residuals\nLeast Squares\nPrediction versus Explanation (Profiling)\nFurther Reading\nMultiple Linear Regression\nExample: King County Housing Data\nAssessing the Model\nCross-Validation\nModel Selection and Stepwise Regression\nWeighted Regression\n\n\nPrediction Using Regression\nThe Dangers of Extrapolation\nConfidence and Prediction Intervals\nFactor Variables in Regression\nDummy Variables Representation\nFactor Variables with Many Levels\nOrdered Factor Variables\nInterpreting the Regression Equation\nCorrelated Predictors\nMulticollinearity\nConfounding Variables\nInteractions and Main Effects\nTesting the Assumptions: Regression Diagnostics\nOutliers\nInfluential Values\nHeteroskedasticity, Non-Normality and Correlated Errors\nPartial Residual Plots and Nonlinearity\nPolynomial and Spline Regression\nPolynomial\nSplines\nGeneralized Additive Models\nFurther Reading\n\n\nSummary\n5. Classification\nNaive Bayes\nWhy Exact Bayesian Classification Is Impractical\nThe Naive Solution\nNumeric Predictor Variables\nFurther Reading\nDiscriminant Analysis\nCovariance Matrix\nFisher’s Linear Discriminant\nA Simple Example\nFurther Reading\nLogistic Regression\nLogistic Response Function and Logit\nLogistic Regression and the GLM\nGeneralized Linear Models\nPredicted Values from Logistic Regression\nInterpreting the Coefficients and Odds Ratios\nLinear and Logistic Regression: Similarities and Differences\nAssessing the Model\nFurther Reading\nEvaluating Classification Models\n\n\nConfusion Matrix\nThe Rare Class Problem\nPrecision, Recall, and Specificity\nROC Curve\nAUC\nLift\nFurther Reading\nStrategies for Imbalanced Data\nUndersampling\nOversampling and Up/Down Weighting\nData Generation\nCost-Based Classification\nExploring the Predictions\nFurther Reading\nSummary\n6. Statistical Machine Learning\nK-Nearest Neighbors\nA Small Example: Predicting Loan Default\nDistance Metrics\nOne Hot Encoder\nStandardization (Normalization, Z-Scores)\nChoosing K\n\n\nKNN as a Feature Engine\nTree Models\nA Simple Example\nThe Recursive Partitioning Algorithm\nMeasuring Homogeneity or Impurity\nStopping the Tree from Growing\nPredicting a Continuous Value\nHow Trees Are Used\nFurther Reading\nBagging and the Random Forest\nBagging\nRandom Forest\nVariable Importance\nHyperparameters\nBoosting\nThe Boosting Algorithm\nXGBoost\nRegularization: Avoiding Overfitting\nHyperparameters and Cross-Validation\nSummary\n7. Unsupervised Learning\nPrincipal Components Analysis\n\n\nA Simple Example\nComputing the Principal Components\nInterpreting Principal Components\nFurther Reading\nK-Means Clustering\nA Simple Example\nK-Means Algorithm\nInterpreting the Clusters\nSelecting the Number of Clusters\nHierarchical Clustering\nA Simple Example\nThe Dendrogram\nThe Agglomerative Algorithm\nMeasures of Dissimilarity\nModel-Based Clustering\nMultivariate Normal Distribution\nMixtures of Normals\nSelecting the Number of Clusters\nFurther Reading\nScaling and Categorical Variables\nScaling the Variables\nDominant Variables\n\n\nCategorical Data and Gower’s Distance\nProblems with Clustering Mixed Data\nSummary\nBibliography\nIndex\n",
      "page_number": 432
    }
  ],
  "pages": [
    {
      "page_number": 1,
      "content": "www.allitebooks.com\n",
      "content_length": 20,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 2,
      "content": "Practical Statistics for Data Scientists\n50 Essential Concepts\nPeter Bruce and Andrew Bruce\nwww.allitebooks.com\n",
      "content_length": 112,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 3,
      "content": "Practical Statistics for Data Scientists\nby Peter Bruce and Andrew Bruce\nCopyright © 2017 Peter Bruce and Andrew Bruce. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North,\nSebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional\nuse. Online editions are also available for most titles (http://oreilly.com/safari).\nFor more information, contact our corporate/institutional sales department: 800-\n998-9938 or corporate@oreilly.com.\nEditor: Shannon Cutt\nProduction Editor: Kristen Brown\nCopyeditor: Rachel Monaghan\nProofreader: Eliahu Sussman\nIndexer: Ellen Troutman-Zaig\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nMay 2017: First Edition\nwww.allitebooks.com\n",
      "content_length": 830,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 4,
      "content": "Revision History for the First Edition\n2017-05-09: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781491952962 for release\ndetails.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Practical\nStatistics for Data Scientists, the cover image, and related trade dress are\ntrademarks of O’Reilly Media, Inc.\nWhile the publisher and the authors have used good faith efforts to ensure that the\ninformation and instructions contained in this work are accurate, the publisher and\nthe authors disclaim all responsibility for errors or omissions, including without\nlimitation responsibility for damages resulting from the use of or reliance on this\nwork. Use of the information and instructions contained in this work is at your\nown risk. If any code samples or other technology this work contains or describes\nis subject to open source licenses or the intellectual property rights of others, it is\nyour responsibility to ensure that your use thereof complies with such licenses\nand/or rights.\n978-1-491-95296-2\n[M]\nwww.allitebooks.com\n",
      "content_length": 1059,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 5,
      "content": "Dedication\nWe would like to dedicate this book to the memories of our parents Victor G.\nBruce and Nancy C. Bruce, who cultivated a passion for math and science; and to\nour early mentors John W. Tukey and Julian Simon, and our lifelong friend Geoff\nWatson, who helped inspire us to pursue a career in statistics.\nwww.allitebooks.com\n",
      "content_length": 332,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 6,
      "content": "Preface\nThis book is aimed at the data scientist with some familiarity with the R\nprogramming language, and with some prior (perhaps spotty or ephemeral)\nexposure to statistics. Both of us came to the world of data science from the world\nof statistics, so we have some appreciation of the contribution that statistics can\nmake to the art of data science. At the same time, we are well aware of the\nlimitations of traditional statistics instruction: statistics as a discipline is a century\nand a half old, and most statistics textbooks and courses are laden with the\nmomentum and inertia of an ocean liner.\nTwo goals underlie this book:\nTo lay out, in digestible, navigable, and easily referenced form, key concepts\nfrom statistics that are relevant to data science.\nTo explain which concepts are important and useful from a data science\nperspective, which are less so, and why.\nwww.allitebooks.com\n",
      "content_length": 898,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 7,
      "content": "What to Expect\nKEY TERMS\nData science is a fusion of multiple disciplines, including statistics, computer science, information\ntechnology, and domain-specific fields. As a result, several different terms could be used to reference a\ngiven concept. Key terms and their synonyms will be highlighted throughout the book in a sidebar such as\nthis.\nwww.allitebooks.com\n",
      "content_length": 364,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 8,
      "content": "Conventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program\nelements such as variable or function names, databases, data types,\nenvironment variables, statements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nConstant width italic\nShows text that should be replaced with user-supplied values or by values\ndetermined by context.\nTIP\nThis element signifies a tip or suggestion.\nNOTE\nThis element signifies a general note.\nWARNING\nThis element indicates a warning or caution.\nwww.allitebooks.com\n",
      "content_length": 761,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 9,
      "content": "Using Code Examples\nSupplemental material (code examples, exercises, etc.) is available for download\nat https://github.com/andrewgbruce/statistics-for-data-scientists.\nThis book is here to help you get your job done. In general, if example code is\noffered with this book, you may use it in your programs and documentation. You\ndo not need to contact us for permission unless you’re reproducing a significant\nportion of the code. For example, writing a program that uses several chunks of\ncode from this book does not require permission. Selling or distributing a CD-\nROM of examples from O’Reilly books does require permission. Answering a\nquestion by citing this book and quoting example code does not require\npermission. Incorporating a significant amount of example code from this book\ninto your product’s documentation does require permission.\nWe appreciate, but do not require, attribution. An attribution usually includes the\ntitle, author, publisher, and ISBN. For example: “Practical Statistics for Data\nScientists by Peter Bruce and Andrew Bruce (O’Reilly). Copyright 2017 Peter\nBruce and Andrew Bruce, 978-1-491-95296-2.”\nIf you feel your use of code examples falls outside fair use or the permission given\nabove, feel free to contact us at permissions@oreilly.com.\nwww.allitebooks.com\n",
      "content_length": 1296,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 10,
      "content": "Safari® Books Online\nNOTE\nSafari Books Online is an on-demand digital library that delivers expert content\nin both book and video form from the world’s leading authors in technology and\nbusiness.\nTechnology professionals, software developers, web designers, and business and\ncreative professionals use Safari Books Online as their primary resource for\nresearch, problem solving, learning, and certification training.\nSafari Books Online offers a range of plans and pricing for enterprise,\ngovernment, education, and individuals.\nMembers have access to thousands of books, training videos, and prepublication\nmanuscripts in one fully searchable database from publishers like O’Reilly\nMedia, Prentice Hall Professional, Addison-Wesley Professional, Microsoft\nPress, Sams, Que, Peachpit Press, Focal Press, Cisco Press, John Wiley & Sons,\nSyngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT Press,\nApress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, Course\nTechnology, and hundreds more. For more information about Safari Books Online,\nplease visit us online.\nwww.allitebooks.com\n",
      "content_length": 1097,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 11,
      "content": "How to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any\nadditional information. You can access this page at\nhttp://bit.ly/practicalStats_for_DataScientists.\nTo comment or ask technical questions about this book, send email to\nbookquestions@oreilly.com.\nFor more information about our books, courses, conferences, and news, see our\nwebsite at http://www.oreilly.com.\nFind us on Facebook: http://facebook.com/oreilly\nFollow us on Twitter: http://twitter.com/oreillymedia\nWatch us on YouTube: http://www.youtube.com/oreillymedia\n",
      "content_length": 815,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 12,
      "content": "Acknowledgments\nThe authors acknowledge the many people who helped make this book a reality.\nGerhard Pilcher, CEO of the data mining firm Elder Research, saw early drafts of\nthe book and gave us detailed and helpful corrections and comments. Likewise,\nAnya McGuirk and Wei Xiao, statisticians at SAS, and Jay Hilfiger, fellow\nO’Reilly author, provided helpful feedback on initial drafts of the book.\nAt O’Reilly, Shannon Cutt has shepherded us through the publication process with\ngood cheer and the right amount of prodding, while Kristen Brown smoothly took\nour book through the production phase. Rachel Monaghan and Eliahu Sussman\ncorrected and improved our writing with care and patience, while Ellen\nTroutman-Zaig prepared the index. We also thank Marie Beaugureau, who\ninitiated our project at O’Reilly, as well as Ben Bengfort, O’Reilly author and\nstatistics.com instructor, who introduced us to O’Reilly.\nWe, and this book, have also benefited from the many conversations Peter has had\nover the years with Galit Shmueli, coauthor on other book projects.\nFinally, we would like to especially thank Elizabeth Bruce and Deborah Donnell,\nwhose patience and support made this endeavor possible.\n",
      "content_length": 1198,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 13,
      "content": "Chapter 1. Exploratory Data Analysis\nAs a discipline, statistics has mostly developed in the past century. Probability\ntheory — the mathematical foundation for statistics — was developed in the 17th\nto 19th centuries based on work by Thomas Bayes, Pierre-Simon Laplace, and\nCarl Gauss. In contrast to the purely theoretical nature of probability, statistics is\nan applied science concerned with analysis and modeling of data. Modern\nstatistics as a rigorous scientific discipline traces its roots back to the late 1800s\nand Francis Galton and Karl Pearson. R. A. Fisher, in the early 20th century, was\na leading pioneer of modern statistics, introducing key ideas of experimental\ndesign and maximum likelihood estimation. These and many other statistical\nconcepts live largely in the recesses of data science. The main goal of this book is\nto help illuminate these concepts and clarify their importance — or lack thereof\n— in the context of data science and big data.\nThis chapter focuses on the first step in any data science project: exploring the\ndata. Exploratory data analysis, or EDA, is a comparatively new area of\nstatistics. Classical statistics focused almost exclusively on inference, a\nsometimes complex set of procedures for drawing conclusions about large\npopulations based on small samples. In 1962, John W. Tukey (Figure 1-1) called\nfor a reformation of statistics in his seminal paper “The Future of Data Analysis”\n[Tukey-1962]. He proposed a new scientific discipline called data analysis that\nincluded statistical inference as just one component. Tukey forged links to the\nengineering and computer science communities (he coined the terms bit, short for\nbinary digit, and software), and his original tenets are suprisingly durable and\nform part of the foundation for data science. The field of exploratory data analysis\nwas established with Tukey’s 1977 now-classic book Exploratory Data Analysis\n[Tukey-1977].\n",
      "content_length": 1930,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 14,
      "content": "Figure 1-1. John Tukey, the eminent statistician whose ideas developed over 50 years ago form the\nfoundation of data science.\nWith the ready availablility of computing power and expressive data analysis\nsoftware, exploratory data analysis has evolved well beyond its original scope.\nKey drivers of this discipline have been the rapid development of new technology,\naccess to more and bigger data, and the greater use of quantitative analysis in a\nvariety of disciplines. David Donoho, professor of statistics at Stanford\nUniversity and former undergraduate student of Tukey’s, authored an excellent\narticle based on his presentation at the Tukey Centennial workshop in Princeton,\nNew Jersey [Donoho-2015]. Donoho traces the genesis of data science back to\nTukey’s pioneering work in data analysis.\n",
      "content_length": 798,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 15,
      "content": "Elements of Structured Data\nData comes from many sources: sensor measurements, events, text, images, and\nvideos. The Internet of Things (IoT) is spewing out streams of information. Much\nof this data is unstructured: images are a collection of pixels with each pixel\ncontaining RGB (red, green, blue) color information. Texts are sequences of\nwords and nonword characters, often organized by sections, subsections, and so\non. Clickstreams are sequences of actions by a user interacting with an app or\nweb page. In fact, a major challenge of data science is to harness this torrent of\nraw data into actionable information. To apply the statistical concepts covered in\nthis book, unstructured raw data must be processed and manipulated into a\nstructured form — as it might emerge from a relational database — or be\ncollected for a study.\nKEY TERMS FOR DATA TYPES\nContinuous\nData that can take on any value in an interval.\nSynonyms\ninterval, float, numeric\nDiscrete\nData that can take on only integer values, such as counts.\nSynonyms\ninteger, count\nCategorical\nData that can take on only a specific set of values representing a set of possible categories.\nSynonyms\nenums, enumerated, factors, nominal, polychotomous\nBinary\nA special case of categorical data with just two categories of values (0/1, true/false).\nSynonyms\ndichotomous, logical, indicator, boolean\nOrdinal\nCategorical data that has an explicit ordering.\n",
      "content_length": 1414,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 16,
      "content": "Synonyms\nordered factor\nThere are two basic types of structured data: numeric and categorical. Numeric\ndata comes in two forms: continuous, such as wind speed or time duration, and\ndiscrete, such as the count of the occurrence of an event. Categorical data takes\nonly a fixed set of values, such as a type of TV screen (plasma, LCD, LED, etc.)\nor a state name (Alabama, Alaska, etc.). Binary data is an important special case\nof categorical data that takes on only one of two values, such as 0/1, yes/no, or\ntrue/false. Another useful type of categorical data is ordinal data in which the\ncategories are ordered; an example of this is a numerical rating (1, 2, 3, 4, or 5).\nWhy do we bother with a taxonomy of data types? It turns out that for the purposes\nof data analysis and predictive modeling, the data type is important to help\ndetermine the type of visual display, data analysis, or statistical model. In fact,\ndata science software, such as R and Python, uses these data types to improve\ncomputational performance. More important, the data type for a variable\ndetermines how software will handle computations for that variable.\nSoftware engineers and database programmers may wonder why we even need the\nnotion of categorical and ordinal data for analytics. After all, categories are\nmerely a collection of text (or numeric) values, and the underlying database\nautomatically handles the internal representation. However, explicit identification\nof data as categorical, as distinct from text, does offer some advantages:\nKnowing that data is categorical can act as a signal telling software how\nstatistical procedures, such as producing a chart or fitting a model, should\nbehave. In particular, ordinal data can be represented as an ordered.factor\nin R and Python, preserving a user-specified ordering in charts, tables, and\nmodels.\nStorage and indexing can be optimized (as in a relational database).\nThe possible values a given categorical variable can take are enforced in the\nsoftware (like an enum).\nThe third “benefit” can lead to unintended or unexpected behavior: the default\nbehavior of data import functions in R (e.g., read.csv) is to automatically\nconvert a text column into a factor. Subsequent operations on that column will\n",
      "content_length": 2246,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 17,
      "content": "assume that the only allowable values for that column are the ones originally\nimported, and assigning a new text value will introduce a warning and produce an\nNA (missing value).\nKEY IDEAS\nData is typically classified in software by type.\nData types include continuous, discrete, categorical (which includes binary), and ordinal.\nData typing in software acts as a signal to the software on how to process the data.\n",
      "content_length": 415,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 18,
      "content": "Further Reading\nData types can be confusing, since types may overlap, and the taxonomy in\none software may differ from that in another. The R-Tutorial website covers\nthe taxonomy for R.\nDatabases are more detailed in their classification of data types,\nincorporating considerations of precision levels, fixed- or variable-length\nfields, and more; see the W3Schools guide for SQL.\n",
      "content_length": 380,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 19,
      "content": "Rectangular Data\nThe typical frame of reference for an analysis in data science is a rectangular\ndata object, like a spreadsheet or database table.\nKEY TERMS FOR RECTANGULAR DATA\nData frame\nRectangular data (like a spreadsheet) is the basic data structure for statistical and machine\nlearning models.\nFeature\nA column in the table is commonly referred to as a feature.\nSynonyms\nattribute, input, predictor, variable\nOutcome\nMany data science projects involve predicting an outcome — often a yes/no outcome (in Table 1-\n1, it is “auction was competitive or not”). The features are sometimes used to predict the outcome\nin an experiment or study.\nSynonyms\ndependent variable, response, target, output\nRecords\nA row in the table is commonly referred to as a record.\nSynonyms\ncase, example, instance, observation, pattern, sample\nRectangular data is essentially a two-dimensional matrix with rows indicating\nrecords (cases) and columns indicating features (variables). The data doesn’t\nalways start in this form: unstructured data (e.g., text) must be processed and\nmanipulated so that it can be represented as a set of features in the rectangular\ndata (see “Elements of Structured Data”). Data in relational databases must be\nextracted and put into a single table for most data analysis and modeling tasks.\nIn Table 1-1, there is a mix of measured or counted data (e.g., duration and price),\nand categorical data (e.g., category and currency). As mentioned earlier, a special\nform of categorical variable is a binary (yes/no or 0/1) variable, seen in the\nrightmost column in Table 1-1 — an indicator variable showing whether an\n",
      "content_length": 1625,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 20,
      "content": "auction was competitive or not.\nTable 1-1. A typical data format\nCategory\ncurrency sellerRating Duration endDay ClosePrice OpenPrice Competitive?\nMusic/Movie/Game US\n3249\n5\nMon\n0.01\n0.01\n0\nMusic/Movie/Game US\n3249\n5\nMon\n0.01\n0.01\n0\nAutomotive\nUS\n3115\n7\nTue\n0.01\n0.01\n0\nAutomotive\nUS\n3115\n7\nTue\n0.01\n0.01\n0\nAutomotive\nUS\n3115\n7\nTue\n0.01\n0.01\n0\nAutomotive\nUS\n3115\n7\nTue\n0.01\n0.01\n0\nAutomotive\nUS\n3115\n7\nTue\n0.01\n0.01\n1\nAutomotive\nUS\n3115\n7\nTue\n0.01\n0.01\n1\n",
      "content_length": 454,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 21,
      "content": "Data Frames and Indexes\nTraditional database tables have one or more columns designated as an index.\nThis can vastly improve the efficiency of certain SQL queries. In Python, with the\npandas library, the basic rectangular data structure is a DataFrame object. By\ndefault, an automatic integer index is created for a DataFrame based on the order\nof the rows. In pandas, it is also possible to set multilevel/hierarchical indexes to\nimprove the efficiency of certain operations.\nIn R, the basic rectangular data structure is a data.frame object. A data.frame\nalso has an implicit integer index based on the row order. While a custom key can\nbe created through the row.names attribute, the native R data.frame does not\nsupport user-specified or multilevel indexes. To overcome this deficiency, two\nnew packages are gaining widespread use: data.table and dplyr. Both support\nmultilevel indexes and offer significant speedups in working with a data.frame.\n",
      "content_length": 951,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 22,
      "content": "TERMINOLOGY DIFFERENCES\nTerminology for rectangular data can be confusing. Statisticians and data scientists use different\nterms for the same thing. For a statistician, predictor variables are used in a model to predict a\nresponse or dependent variable. For a data scientist, features are used to predict a target. One\nsynonym is particularly confusing: computer scientists will use the term sample for a single row; a\nsample to a statistician means a collection of rows.\n",
      "content_length": 472,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 23,
      "content": "Nonrectangular Data Structures\nThere are other data structures besides rectangular data.\nTime series data records successive measurements of the same variable. It is the\nraw material for statistical forecasting methods, and it is also a key component of\nthe data produced by devices — the Internet of Things.\nSpatial data structures, which are used in mapping and location analytics, are\nmore complex and varied than rectangular data structures. In the object\nrepresentation, the focus of the data is an object (e.g., a house) and its spatial\ncoordinates. The field view, by contrast, focuses on small units of space and the\nvalue of a relevant metric (pixel brightness, for example).\nGraph (or network) data structures are used to represent physical, social, and\nabstract relationships. For example, a graph of a social network, such as\nFacebook or LinkedIn, may represent connections between people on the network.\nDistribution hubs connected by roads are an example of a physical network.\nGraph structures are useful for certain types of problems, such as network\noptimization and recommender systems.\nEach of these data types has its specialized methodology in data science. The\nfocus of this book is on rectangular data, the fundamental building block of\npredictive modeling.\n",
      "content_length": 1281,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 24,
      "content": "GRAPHS IN STATISTICS\nIn computer science and information technology, the term graph typically refers to a depiction of\nthe connections among entities, and to the underlying data structure. In statistics, graph is used to\nrefer to a variety of plots and visualizations, not just of connections among entities, and the term\napplies just to the visualization, not to the data structure.\nKEY IDEAS\nThe basic data structure in data science is a rectangular matrix in which rows are records and\ncolumns are variables (features).\nTerminology can be confusing; there are a variety of synonyms arising from the different disciplines\nthat contribute to data science (statistics, computer science, and information technology).\n",
      "content_length": 716,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 25,
      "content": "Further Reading\nDocumentation on data frames in R\nDocumentation on data frames in Python\n",
      "content_length": 89,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 26,
      "content": "Estimates of Location\nVariables with measured or count data might have thousands of distinct values. A\nbasic step in exploring your data is getting a “typical value” for each feature\n(variable): an estimate of where most of the data is located (i.e., its central\ntendency).\nKEY TERMS FOR ESTIMATES OF LOCATION\nMean\nThe sum of all values divided by the number of values.\nSynonyms\naverage\nWeighted mean\nThe sum of all values times a weight divided by the sum of the weights.\nSynonyms\nweighted average\nMedian\nThe value such that one-half of the data lies above and below.\nSynonyms\n50th percentile\nWeighted median\nThe value such that one-half of the sum of the weights lies above and below the sorted data.\nTrimmed mean\nThe average of all values after dropping a fixed number of extreme values.\nSynonyms\ntruncated mean\nRobust\nNot sensitive to extreme values.\nSynonyms\nresistant\nOutlier\nA data value that is very different from most of the data.\nSynonyms\n",
      "content_length": 950,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 27,
      "content": "extreme value\nAt first glance, summarizing data might seem fairly trivial: just take the mean of\nthe data (see “Mean”). In fact, while the mean is easy to compute and expedient to\nuse, it may not always be the best measure for a central value. For this reason,\nstatisticians have developed and promoted several alternative estimates to the\nmean.\n",
      "content_length": 346,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 28,
      "content": "METRICS AND ESTIMATES\nStatisticians often use the term estimates for values calculated from the data at hand, to draw a\ndistinction between what we see from the data, and the theoretical true or exact state of affairs.\nData scientists and business analysts are more likely to refer to such values as a metric. The\ndifference reflects the approach of statistics versus data science: accounting for uncertainty lies\nat the heart of the discipline of statistics, whereas concrete business or organizational objectives\nare the focus of data science. Hence, statisticians estimate, and data scientists measure.\n",
      "content_length": 606,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 29,
      "content": "Mean\nThe most basic estimate of location is the mean, or average value. The mean is the\nsum of all the values divided by the number of values. Consider the following set\nof numbers: {3 5 1 2}. The mean is (3 + 5 + 1 + 2) / 4 = 11 / 4 = 2.75. You will\nencounter the symbol \n (pronounced “x-bar”) to represent the mean of a sample\nfrom a population. The formula to compute the mean for a set of n values \n is:\nNOTE\nN (or n) refers to the total number of records or observations. In statistics it is capitalized if it is\nreferring to a population, and lowercase if it refers to a sample from a population. In data science,\nthat distinction is not vital so you may see it both ways.\nA variation of the mean is a trimmed mean, which you calculate by dropping a\nfixed number of sorted values at each end and then taking an average of the\nremaining values. Representing the sorted values by \n where \n is the smallest value and \n the largest, the formula to compute the trimmed\nmean with  smallest and largest values omitted is:\nA trimmed mean eliminates the influence of extreme values. For example, in\ninternational diving the top and bottom scores from five judges are dropped, and\nthe final score is the average of the three remaining judges [Wikipedia-2016].\n",
      "content_length": 1256,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 30,
      "content": "This makes it difficult for a single judge to manipulate the score, perhaps to favor\nhis country’s contestant. Trimmed means are widely used, and in many cases, are\npreferable to use instead of the ordinary mean: see “Median and Robust\nEstimates” for further discussion.\nAnother type of mean is a weighted mean, which you calculate by multiplying\neach data value \n by a weight \n and dividing their sum by the sum of the\nweights. The formula for a weighted mean is:\nThere are two main motivations for using a weighted mean:\nSome values are intrinsically more variable than others, and highly variable\nobservations are given a lower weight. For example, if we are taking the\naverage from multiple sensors and one of the sensors is less accurate, then\nwe might downweight the data from that sensor.\nThe data collected does not equally represent the different groups that we are\ninterested in measuring. For example, because of the way an online\nexperiment was conducted, we may not have a set of data that accurately\nreflects all groups in the user base. To correct that, we can give a higher\nweight to the values from the groups that were underrepresented.\nwww.allitebooks.com\n",
      "content_length": 1175,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 31,
      "content": "Median and Robust Estimates\nThe median is the middle number on a sorted list of the data. If there is an even\nnumber of data values, the middle value is one that is not actually in the data set,\nbut rather the average of the two values that divide the sorted data into upper and\nlower halves. Compared to the mean, which uses all observations, the median\ndepends only on the values in the center of the sorted data. While this might seem\nto be a disadvantage, since the mean is much more sensitive to the data, there are\nmany instances in which the median is a better metric for location. Let’s say we\nwant to look at typical household incomes in neighborhoods around Lake\nWashington in Seattle. In comparing the Medina neighborhood to the Windermere\nneighborhood, using the mean would produce very different results because Bill\nGates lives in Medina. If we use the median, it won’t matter how rich Bill Gates\nis — the position of the middle observation will remain the same.\nFor the same reasons that one uses a weighted mean, it is also possible to compute\na weighted median. As with the median, we first sort the data, although each data\nvalue has an associated weight. Instead of the middle number, the weighted\nmedian is a value such that the sum of the weights is equal for the lower and upper\nhalves of the sorted list. Like the median, the weighted median is robust to\noutliers.\nOutliers\nThe median is referred to as a robust estimate of location since it is not influenced\nby outliers (extreme cases) that could skew the results. An outlier is any value\nthat is very distant from the other values in a data set. The exact definition of an\noutlier is somewhat subjective, although certain conventions are used in various\ndata summaries and plots (see “Percentiles and Boxplots”). Being an outlier in\nitself does not make a data value invalid or erroneous (as in the previous example\nwith Bill Gates). Still, outliers are often the result of data errors such as mixing\ndata of different units (kilometers versus meters) or bad readings from a sensor.\nWhen outliers are the result of bad data, the mean will result in a poor estimate of\nlocation, while the median will be still be valid. In any case, outliers should be\nidentified and are usually worthy of further investigation.\n",
      "content_length": 2287,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 32,
      "content": "ANOMALY DETECTION\nIn contrast to typical data analysis, where outliers are sometimes informative and sometimes a\nnuisance, in anomaly detection the points of interest are the outliers, and the greater mass of\ndata serves primarily to define the “normal” against which anomalies are measured.\nThe median is not the only robust estimate of location. In fact, a trimmed mean is\nwidely used to avoid the influence of outliers. For example, trimming the bottom\nand top 10% (a common choice) of the data will provide protection against\noutliers in all but the smallest data sets. The trimmed mean can be thought of as a\ncompromise between the median and the mean: it is robust to extreme values in\nthe data, but uses more data to calculate the estimate for location.\n",
      "content_length": 761,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 33,
      "content": "OTHER ROBUST METRICS FOR LOCATION\nStatisticians have developed a plethora of other estimators for location, primarily with the goal of\ndeveloping an estimator more robust than the mean and also more efficient (i.e., better able to\ndiscern small location differences between data sets). While these methods are potentially useful\nfor small data sets, they are not likely to provide added benefit for large or even moderately sized\ndata sets.\n",
      "content_length": 441,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 34,
      "content": "Example: Location Estimates of Population and Murder Rates\nTable 1-2 shows the first few rows in the data set containing population and\nmurder rates (in units of murders per 100,000 people per year) for each state.\nTable 1-2. A few rows of the\ndata.frame state of population\nand murder rate by state\nState\nPopulation Murder rate\n1 Alabama\n4,779,736\n5.7\n2 Alaska\n710,231\n5.6\n3 Arizona\n6,392,017\n4.7\n4 Arkansas\n2,915,918\n5.6\n5 California\n37,253,956\n4.4\n6 Colorado\n5,029,196\n2.8\n7 Connecticut 3,574,097\n2.4\n8 Delaware\n897,934\n5.8\nCompute the mean, trimmed mean, and median for the population using R:\n> state <- read.csv(file=\"/Users/andrewbruce1/book/state.csv\")\n> mean(state[[\"Population\"]])\n[1] 6162876\n> mean(state[[\"Population\"]], trim=0.1)\n[1] 4783697\n> median(state[[\"Population\"]])\n[1] 4436370\nThe mean is bigger than the trimmed mean, which is bigger than the median.\nThis is because the trimmed mean excludes the largest and smallest five states\n(trim=0.1 drops 10% from each end). If we want to compute the average murder\nrate for the country, we need to use a weighted mean or median to account for\ndifferent populations in the states. Since base R doesn’t have a function for\nweighted median, we need to install a package such as matrixStats:\n",
      "content_length": 1253,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 35,
      "content": "> weighted.mean(state[[\"Murder.Rate\"]], w=state[[\"Population\"]])\n[1] 4.445834\n> library(\"matrixStats\")\n> weightedMedian(state[[\"Murder.Rate\"]], w=state[[\"Population\"]])\n[1] 4.4\nIn this case, the weighted mean and median are about the same.\nKEY IDEAS\nThe basic metric for location is the mean, but it can be sensitive to extreme values (outlier).\nOther metrics (median, trimmed mean) are more robust.\n",
      "content_length": 400,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 36,
      "content": "Further Reading\nMichael Levine (Purdue University) has posted some useful slides on basic\ncalculations for measures of location.\nJohn Tukey’s 1977 classic Exploratory Data Analysis (Pearson) is still\nwidely read.\n",
      "content_length": 213,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 37,
      "content": "Estimates of Variability\nLocation is just one dimension in summarizing a feature. A second dimension,\nvariability, also referred to as dispersion, measures whether the data values are\ntightly clustered or spread out. At the heart of statistics lies variability: measuring\nit, reducing it, distinguishing random from real variability, identifying the various\nsources of real variability, and making decisions in the presence of it.\nKEY TERMS FOR VARIABILITY METRICS\nDeviations\nThe difference between the observed values and the estimate of location.\nSynonyms\nerrors, residuals\nVariance\nThe sum of squared deviations from the mean divided by n – 1 where n is the number of data\nvalues.\nSynonyms\nmean-squared-error\nStandard deviation\nThe square root of the variance.\nSynonyms\nl2-norm, Euclidean norm\nMean absolute deviation\nThe mean of the absolute value of the deviations from the mean.\nSynonyms\nl1-norm, Manhattan norm\nMedian absolute deviation from the median\nThe median of the absolute value of the deviations from the median.\nRange\nThe difference between the largest and the smallest value in a data set.\nOrder statistics\nMetrics based on the data values sorted from smallest to biggest.\nSynonyms\nranks\n",
      "content_length": 1205,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 38,
      "content": "Percentile\nThe value such that P percent of the values take on this value or less and (100–P) percent take on\nthis value or more.\nSynonyms\nquantile\nInterquartile range\nThe difference between the 75th percentile and the 25th percentile.\nSynonyms\nIQR\nJust as there are different ways to measure location (mean, median, etc.) there are\nalso different ways to measure variability.\n",
      "content_length": 377,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 39,
      "content": "Standard Deviation and Related Estimates\nThe most widely used estimates of variation are based on the differences, or\ndeviations, between the estimate of location and the observed data. For a set of\ndata {1, 4, 4}, the mean is 3 and the median is 4. The deviations from the mean\nare the differences: 1 – 3 = –2, 4 – 3 = 1 , 4 – 3 = 1. These deviations tell us how\ndispersed the data is around the central value.\nOne way to measure variability is to estimate a typical value for these deviations.\nAveraging the deviations themselves would not tell us much — the negative\ndeviations offset the positive ones. In fact, the sum of the deviations from the mean\nis precisely zero. Instead, a simple approach is to take the average of the absolute\nvalues of the deviations from the mean. In the preceding example, the absolute\nvalue of the deviations is {2 1 1} and their average is (2 + 1 + 1) / 3 = 1.33. This\nis known as the mean absolute deviation and is computed with the formula:\nwhere \n is the sample mean.\nThe best-known estimates for variability are the variance and the standard\ndeviation, which are based on squared deviations. The variance is an average of\nthe squared deviations, and the standard deviation is the square root of the\nvariance.\nThe standard deviation is much easier to interpret than the variance since it is on\nthe same scale as the original data. Still, with its more complicated and less\nintuitive formula, it might seem peculiar that the standard deviation is preferred in\nstatistics over the mean absolute deviation. It owes its preeminence to statistical\ntheory: mathematically, working with squared values is much more convenient\n",
      "content_length": 1658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 40,
      "content": "than absolute values, especially for statistical models.\nDEGREES OF FREEDOM, AND N OR N – 1?\nIn statistics books, there is always some discussion of why we have n – 1 in the denominator in the\nvariance formula, instead of n, leading into the concept of degrees of freedom. This distinction is not\nimportant since n is generally large enough that it won’t make much difference whether you divide by n\nor n – 1. But in case you are interested, here is the story. It is based on the premise that you want to\nmake estimates about a population, based on a sample.\nIf you use the intuitive denominator of n in the variance formula, you will underestimate the true value of\nthe variance and the standard deviation in the population. This is referred to as a biased estimate.\nHowever, if you divide by n – 1 instead of n, the standard deviation becomes an unbiased estimate.\nTo fully explain why using n leads to a biased estimate involves the notion of degrees of freedom, which\ntakes into account the number of constraints in computing an estimate. In this case, there are n – 1\ndegrees of freedom since there is one constraint: the standard deviation depends on calculating the\nsample mean. For many problems, data scientists do not need to worry about degrees of freedom, but\nthere are cases where the concept is important (see “Choosing K”).\nNeither the variance, the standard deviation, nor the mean absolute deviation is\nrobust to outliers and extreme values (see “Median and Robust Estimates” for a\ndiscussion of robust estimates for location). The variance and standard deviation\nare especially sensitive to outliers since they are based on the squared deviations.\nA robust estimate of variability is the median absolute deviation from the median\nor MAD:\nwhere m is the median. Like the median, the MAD is not influenced by extreme\nvalues. It is also possible to compute a trimmed standard deviation analogous to\nthe trimmed mean (see “Mean”).\nNOTE\nThe variance, the standard deviation, mean absolute deviation, and median absolute deviation\nfrom the median are not equivalent estimates, even in the case where the data comes from a\nnormal distribution. In fact, the standard deviation is always greater than the mean absolute\ndeviation, which itself is greater than the median absolute deviation. Sometimes, the median\nabsolute deviation is multiplied by a constant scaling factor (it happens to work out to 1.4826) to\nput MAD on the same scale as the standard deviation in the case of a normal distribution.\n",
      "content_length": 2511,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 41,
      "content": "Estimates Based on Percentiles\nA different approach to estimating dispersion is based on looking at the spread of\nthe sorted data. Statistics based on sorted (ranked) data are referred to as order\nstatistics. The most basic measure is the range: the difference between the largest\nand smallest number. The minimum and maximum values themselves are useful to\nknow, and helpful in identifying outliers, but the range is extremely sensitive to\noutliers and not very useful as a general measure of dispersion in the data.\nTo avoid the sensitivity to outliers, we can look at the range of the data after\ndropping values from each end. Formally, these types of estimates are based on\ndifferences between percentiles. In a data set, the Pth percentile is a value such\nthat at least P percent of the values take on this value or less and at least (100 – P)\npercent of the values take on this value or more. For example, to find the 80th\npercentile, sort the data. Then, starting with the smallest value, proceed 80 percent\nof the way to the largest value. Note that the median is the same thing as the 50th\npercentile. The percentile is essentially the same as a quantile, with quantiles\nindexed by fractions (so the .8 quantile is the same as the 80th percentile).\nA common measurement of variability is the difference between the 25th\npercentile and the 75th percentile, called the interquartile range (or IQR). Here\nis a simple example: 3,1,5,3,6,7,2,9. We sort these to get 1,2,3,3,5,6,7,9. The 25th\npercentile is at 2.5, and the 75th percentile is at 6.5, so the interquartile range is\n6.5 – 2.5 = 4. Software can have slightly differing approaches that yield different\nanswers (see the following note); typically, these differences are smaller.\nFor very large data sets, calculating exact percentiles can be computationally very\nexpensive since it requires sorting all the data values. Machine learning and\nstatistical software use special algorithms, such as [Zhang-Wang-2007], to get an\napproximate percentile that can be calculated very quickly and is guaranteed to\nhave a certain accuracy.\n",
      "content_length": 2092,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 42,
      "content": "PERCENTILE: PRECISE DEFINITION\nIf we have an even number of data (n is even), then the percentile is ambiguous under the\npreceding definition. In fact, we could take on any value between the order statistics \n and \n where j satisfies:\nFormally, the percentile is the weighted average:\nfor some weight w between 0 and 1. Statistical software has slightly differing approaches to\nchoosing w. In fact, the R function quantile offers nine different alternatives to compute the\nquantile. Except for small data sets, you don’t usually need to worry about the precise way a\npercentile is calculated.\n",
      "content_length": 593,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 43,
      "content": "Example: Variability Estimates of State Population\nTable 1-3 shows the first few rows in the data set containing population and\nmurder rates for each state.\nTable 1-3. A few rows of the\ndata.frame state of population\nand murder rate by state\nState\nPopulation Murder rate\n1 Alabama\n4,779,736\n5.7\n2 Alaska\n710,231\n5.6\n3 Arizona\n6,392,017\n4.7\n4 Arkansas\n2,915,918\n5.6\n5 California\n37,253,956\n4.4\n6 Colorado\n5,029,196\n2.8\n7 Connecticut 3,574,097\n2.4\n8 Delaware\n897,934\n5.8\nUsing R’s built-in functions for the standard deviation, interquartile range (IQR),\nand the median absolution deviation from the median (MAD), we can compute\nestimates of variability for the state population data:\n> sd(state[[\"Population\"]])\n[1] 6848235\n> IQR(state[[\"Population\"]])\n[1] 4847308\n> mad(state[[\"Population\"]])\n[1] 3849870\nThe standard deviation is almost twice as large as the MAD (in R, by default, the\nscale of the MAD is adjusted to be on the same scale as the mean). This is not\nsurprising since the standard deviation is sensitive to outliers.\nKEY IDEAS\n",
      "content_length": 1042,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 44,
      "content": "The variance and standard deviation are the most widespread and routinely reported statistics of\nvariability.\nBoth are sensitive to outliers.\nMore robust metrics include mean and median absolute deviations from the mean and percentiles\n(quantiles).\n",
      "content_length": 249,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 45,
      "content": "Further Reading\n1. David Lane’s online statistics resource has a section on percentiles.\n2. Kevin Davenport has a useful post on deviations from the median, and\ntheir robust properties in R-Bloggers.\n",
      "content_length": 200,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 46,
      "content": "Exploring the Data Distribution\nEach of the estimates we’ve covered sums up the data in a single number to\ndescribe the location or variability of the data. It is also useful to explore how the\ndata is distributed overall.\nKEY TERMS FOR EXPLORING THE DISTRIBUTION\nBoxplot\nA plot introduced by Tukey as a quick way to visualize the distribution of data.\nSynonyms\nBox and whiskers plot\nFrequency table\nA tally of the count of numeric data values that fall into a set of intervals (bins).\nHistogram\nA plot of the frequency table with the bins on the x-axis and the count (or proportion) on the y-\naxis.\nDensity plot\nA smoothed version of the histogram, often based on a kernal density estimate.\n",
      "content_length": 692,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 47,
      "content": "Percentiles and Boxplots\nIn “Estimates Based on Percentiles”, we explored how percentiles can be used to\nmeasure the spread of the data. Percentiles are also valuable to summarize the\nentire distribution. It is common to report the quartiles (25th, 50th, and 75th\npercentiles) and the deciles (the 10th, 20th, …, 90th percentiles). Percentiles are\nespecially valuable to summarize the tails (the outer range) of the distribution.\nPopular culture has coined the term one-percenters to refer to the people in the\ntop 99th percentile of wealth.\nTable 1-4 displays some percentiles of the murder rate by state. In R, this would\nbe produced by the quantile function:\nquantile(state[[\"Murder.Rate\"]], p=c(.05, .25, .5, .75, .95))\n   5%   25%   50%   75%   95%\n1.600 2.425 4.000 5.550 6.510\nTable 1-4. Percentiles\nof murder rate by\nstate\n5%\n25% 50% 75% 95%\n1.60 2.42 4.00 5.55 6.51\nThe median is 4 murders per 100,000 people, although there is quite a bit of\nvariability: the 5th percentile is only 1.6 and the 95th percentile is 6.51.\nBoxplots, introduced by Tukey [Tukey-1977], are based on percentiles and give a\nquick way to visualize the distribution of data. Figure 1-2 shows a boxplot of the\npopulation by state produced by R:\nboxplot(state[[\"Population\"]]/1000000, ylab=\"Population (millions)\")\n",
      "content_length": 1296,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 49,
      "content": "Figure 1-2. Boxplot of state populations\nThe top and bottom of the box are the 75th and 25th percentiles, respectively. The\nmedian is shown by the horizontal line in the box. The dashed lines, referred to as\nwhiskers, extend from the top and bottom to indicate the range for the bulk of the\ndata. There are many variations of a boxplot; see, for example, the documentation\nfor the R function boxplot [R-base-2015]. By default, the R function extends the\nwhiskers to the furthest point beyond the box, except that it will not go beyond 1.5\ntimes the IQR (other software may use a different rule). Any data outside of the\nwhiskers is plotted as single points.\n",
      "content_length": 658,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 50,
      "content": "Frequency Table and Histograms\nA frequency table of a variable divides up the variable range into equally spaced\nsegments, and tells us how many values fall in each segment. Table 1-5 shows a\nfrequency table of the population by state computed in R:\nbreaks <- seq(from=min(state[[\"Population\"]]),\n                to=max(state[[\"Population\"]]), length=11)\npop_freq <- cut(state[[\"Population\"]], breaks=breaks,\n                right=TRUE, include.lowest = TRUE)\ntable(pop_freq)\nTable 1-5. A frequency table of population by state\nBinNumber BinRange\nCount States\n1\n563,626–\n4,232,658\n24\nWY,VT,ND,AK,SD,DE,MT,RI,NH,ME,HI,ID,NE,WV,NM,NV,UT,KS,AR\n2\n4,232,659–\n7,901,691\n14\nKY,LA,SC,AL,CO,MN,WI,MD,MO,TN,AZ,IN,MA,WA\n3\n7,901,692–\n11,570,724\n6\nVA,NJ,NC,GA,MI,OH\n4\n11,570,725–\n15,239,757\n2\nPA,IL\n5\n15,239,758–\n18,908,790\n1\nFL\n6\n18,908,791–\n22,577,823\n1\nNY\n7\n22,577,824–\n26,246,856\n1\nTX\n8\n26,246,857–\n29,915,889\n0\n9\n29,915,890–\n33,584,922\n0\n10\n33,584,923–\n37,253,956\n1\nCA\nwww.allitebooks.com\n",
      "content_length": 981,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 51,
      "content": "The least populous state is Wyoming, with 563,626 people (2010 Census) and the\nmost populous is California, with 37,253,956 people. This gives us a range of\n37,253,956 – 563,626 = 36,690,330, which we must divide up into equal size\nbins — let’s say 10 bins. With 10 equal size bins, each bin will have a width of\n3,669,033, so the first bin will span from 563,626 to 4,232,658. By contrast, the\ntop bin, 33,584,923 to 37,253,956, has only one state: California. The two bins\nimmediately below California are empty, until we reach Texas. It is important to\ninclude the empty bins; the fact that there are no values in those bins is useful\ninformation. It can also be useful to experiment with different bin sizes. If they are\ntoo large, important features of the distribution can be obscured. It they are too\nsmall, the result is too granular and the ability to see bigger pictures is lost.\nNOTE\nBoth frequency tables and percentiles summarize the data by creating bins. In general, quartiles\nand deciles will have the same count in each bin (equal-count bins), but the bin sizes will be\ndifferent. The frequency table, by contrast, will have different counts in the bins (equal-size bins).\n",
      "content_length": 1190,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 52,
      "content": "Figure 1-3. Histogram of state populations\nA histogram is a way to visualize a frequency table, with bins on the x-axis and\ndata count on the y-axis. To create a histogram corresponding to Table 1-5 in R,\nuse the hist function with the breaks argument:\nhist(state[[\"Population\"]], breaks=breaks)\nThe histogram is shown in Figure 1-3. In general, histograms are plotted such that:\n",
      "content_length": 380,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 53,
      "content": "Empty bins are included in the graph.\nBins are equal width.\nNumber of bins (or, equivalently, bin size) is up to the user.\nBars are contiguous — no empty space shows between bars, unless there is\nan empty bin.\n",
      "content_length": 210,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 54,
      "content": "STATISTICAL MOMENTS\nIn statistical theory, location and variability are referred to as the first and second moments of a\ndistribution. The third and fourth moments are called skewness and kurtosis. Skewness refers to\nwhether the data is skewed to larger or smaller values and kurtosis indicates the propensity of the\ndata to have extreme values. Generally, metrics are not used to measure skewness and kurtosis;\ninstead, these are discovered through visual displays such as Figures 1-2 and 1-3.\n",
      "content_length": 495,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 55,
      "content": "Density Estimates\nRelated to the histogram is a density plot, which shows the distribution of data\nvalues as a continuous line. A density plot can be thought of as a smoothed\nhistogram, although it is typically computed directly from the data through a\nkernal density estimate (see [Duong-2001] for a short tutorial). Figure 1-4\ndisplays a density estimate superposed on a histogram. In R, you can compute a\ndensity estimate using the density function:\nhist(state[[\"Murder.Rate\"]], freq=FALSE)\nlines(density(state[[\"Murder.Rate\"]]), lwd=3, col=\"blue\")\nA key distinction from the histogram plotted in Figure 1-3 is the scale of the y-\naxis: a density plot corresponds to plotting the histogram as a proportion rather\nthan counts (you specify this in R using the argument freq=FALSE).\n",
      "content_length": 783,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 56,
      "content": "DENSITY ESTIMATION\nDensity estimation is a rich topic with a long history in statistical literature. In fact, over 20 R\npackages have been published that offer functions for density estimation. [Deng-Wickham-2011]\ngive a comprehesive review of R packages, with a particular recommendation for ASH or\nKernSmooth. For many data science problems, there is no need to worry about the various types\nof density estimates; it suffices to use the base functions.\n",
      "content_length": 455,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 57,
      "content": "Figure 1-4. Density of state murder rates\nKEY IDEAS\nA frequency histogram plots frequency counts on the y-axis and variable values on the x-axis; it\ngives a sense of the distribution of the data at a glance.\nA frequency table is a tabular version of the frequency counts found in a histogram.\nA boxplot — with the top and bottom of the box at the 75th and 25th percentiles, respectively —\nalso gives a quick sense of the distribution of the data; it is often used in side-by-side displays to\ncompare distributions.\n",
      "content_length": 515,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 58,
      "content": "A density plot is a smoothed version of a histogram; it requires a function to estimate a plot based\non the data (multiple estimates are possible, of course).\n",
      "content_length": 159,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 59,
      "content": "Further Reading\nA SUNY Oswego professor provides a step-by-step guide to creating a\nboxplot.\nDensity estimation in R is covered in Henry Deng and Hadley Wickham’s\npaper of the same name.\nR-Bloggers has a useful post on histograms in R, including customization\nelements, such as binning (breaks)\nR-Bloggers also has similar post on boxplots in R.\n",
      "content_length": 346,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 60,
      "content": "Exploring Binary and Categorical Data\nFor categorical data, simple proportions or percentages tell the story of the data.\nKEY TERMS FOR EXPLORING CATEGORICAL DATA\nMode\nThe most commonly occurring category or value in a data set.\nExpected value\nWhen the categories can be associated with a numeric value, this gives an average value based on\na category’s probability of occurrence.\nBar charts\nThe frequency or proportion for each category plotted as bars.\nPie charts\nThe frequency or proportion for each category plotted as wedges in a pie.\nGetting a summary of a binary variable or a categorical variable with a few\ncategories is a fairly easy matter: we just figure out the proportion of 1s, or of the\nimportant categories. For example, Table 1-6 shows the percentage of delayed\nflights by the cause of delay at Dallas/Fort Worth airport since 2010. Delays are\ncategorized as being due to factors under carrier control, air traffic control (ATC)\nsystem delays, weather, security, or a late inbound aircraft.\nTable 1-6. Percentage of delays\nby cause at Dallas-Fort Worth\nairport\nCarrier ATC\nWeather Security Inbound\n23.02\n30.40 4.03\n0.12\n42.43\nBar charts are a common visual tool for displaying a single categorical variable,\noften seen in the popular press. Categories are listed on the x-axis, and\nfrequencies or proportions on the y-axis. Figure 1-5 shows the airport delays per\nyear by cause for Dallas/Fort Worth, and it is produced with the R function\nbarplot:\n",
      "content_length": 1467,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 61,
      "content": "barplot(as.matrix(dfw)/6, cex.axis=.5)\n",
      "content_length": 39,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 62,
      "content": "Figure 1-5. Bar plot airline delays at DFW by cause\nNote that a bar chart resembles a histogram; in a bar chart the x-axis represents\ndifferent categories of a factor variable, while in a histogram the x-axis represents\nvalues of a single variable on a numeric scale. In a histogram, the bars are\ntypically shown touching each other, with gaps indicating values that did not occur\n",
      "content_length": 381,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 63,
      "content": "in the data. In a bar chart, the bars are shown separate from one another.\nPie charts are an alternative to bar charts, although statisticians and data\nvisualization experts generally eschew pie charts as less visually informative (see\n[Few-2007]).\n",
      "content_length": 249,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 64,
      "content": "NUMERICAL DATA AS CATEGORICAL DATA\nIn “Frequency Table and Histograms”, we looked at frequency tables based on binning the data.\nThis implicitly converts the numeric data to an ordered factor. In this sense, histograms and bar\ncharts are similar, except that the categories on the x-axis in the bar chart are not ordered.\nConverting numeric data to categorical data is an important and widely used step in data analysis\nsince it reduces the complexity (and size) of the data. This aids in the discovery of relationships\nbetween features, particularly at the initial stages of an analysis.\n",
      "content_length": 589,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 65,
      "content": "Mode\nThe mode is the value — or values in case of a tie — that appears most often in\nthe data. For example, the mode of the cause of delay at Dallas/Fort Worth airport\nis “Inbound.” As another example, in most parts of the United States, the mode for\nreligious preference would be Christian. The mode is a simple summary statistic\nfor categorical data, and it is generally not used for numeric data.\n",
      "content_length": 400,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 66,
      "content": "Expected Value\nA special type of categorical data is data in which the categories represent or can\nbe mapped to discrete values on the same scale. A marketer for a new cloud\ntechnology, for example, offers two levels of service, one priced at $300/month\nand another at $50/month. The marketer offers free webinars to generate leads,\nand the firm figures that 5% of the attendees will sign up for the $300 service,\n15% for the $50 service, and 80% will not sign up for anything. This data can be\nsummed up, for financial purposes, in a single “expected value,” which is a form\nof weighted mean in which the weights are probabilities.\nThe expected value is calculated as follows:\n1. Multiply each outcome by its probability of occurring.\n2. Sum these values.\nIn the cloud service example, the expected value of a webinar attendee is thus\n$22.50 per month, calculated as follows:\nThe expected value is really a form of weighted mean: it adds the ideas of future\nexpectations and probability weights, often based on subjective judgment.\nExpected value is a fundamental concept in business valuation and capital\nbudgeting — for example, the expected value of five years of profits from a new\nacquisition, or the expected cost savings from new patient management software at\na clinic.\nKEY IDEAS\nCategorical data is typically summed up in proportions, and can be visualized in a bar chart.\nCategories might represent distinct things (apples and oranges, male and female), levels of a factor\nvariable (low, medium, and high), or numeric data that has been binned.\nExpected value is the sum of values times their probability of occurrence, often used to sum up\nfactor variable levels.\n",
      "content_length": 1676,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 67,
      "content": "Further Reading\nNo statistics course is complete without a lesson on misleading graphs, which\noften involve bar charts and pie charts.\n",
      "content_length": 135,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 68,
      "content": "Correlation\nExploratory data analysis in many modeling projects (whether in data science or\nin research) involves examining correlation among predictors, and between\npredictors and a target variable. Variables X and Y (each with measured data) are\nsaid to be positively correlated if high values of X go with high values of Y, and\nlow values of X go with low values of Y. If high values of X go with low values\nof Y, and vice versa, the variables are negatively correlated.\nKEY TERMS FOR CORRELATION\nCorrelation coefficient\nA metric that measures the extent to which numeric variables are associated with one another\n(ranges from –1 to +1).\nCorrelation matrix\nA table where the variables are shown on both rows and columns, and the cell values are the\ncorrelations between the variables.\nScatterplot\nA plot in which the x-axis is the value of one variable, and the y-axis the value of another.\nConsider these two variables, perfectly correlated in the sense that each goes from\nlow to high:\nv1: {1, 2, 3}\nv2: {4, 5, 6}\nThe vector sum of products is 4 + 10 + 18 = 32. Now try shuffling one of them and\nrecalculating — the vector sum of products will never be higher than 32. So this\nsum of products could be used as a metric; that is, the observed sum of 32 could\nbe compared to lots of random shufflings (in fact, this idea relates to a\nresampling-based estimate: see “Permutation Test”). Values produced by this\nmetric, though, are not that meaningful, except by reference to the resampling\ndistribution.\nMore useful is a standardized variant: the correlation coefficient, which gives an\nestimate of the correlation between two variables that always lies on the same\n",
      "content_length": 1668,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 69,
      "content": "scale. To compute Pearson’s correlation coefficient, we multiply deviations from\nthe mean for variable 1 times those for variable 2, and divide by the product of the\nstandard deviations:\nNote that we divide by n – 1 instead of n; see “Degrees of Freedom, and n or n –\n1?” for more details. The correlation coefficient always lies between +1 (perfect\npositive correlation) and –1 (perfect negative correlation); 0 indicates no\ncorrelation.\nVariables can have an association that is not linear, in which case the correlation\ncoefficient may not be a useful metric. The relationship between tax rates and\nrevenue raised is an example: as tax rates increase from 0, the revenue raised also\nincreases. However, once tax rates reach a high level and approach 100%, tax\navoidance increases and tax revenue actually declines.\nTable 1-7, called a correlation matrix, shows the correlation between the daily\nreturns for telecommunication stocks from July 2012 through June 2015. From the\ntable, you can see that Verizon (VZ) and ATT (T) have the highest correlation.\nLevel Three (LVLT), which is an infrastructure company, has the lowest\ncorrelation. Note the diagonal of 1s (the correlation of a stock with itself is 1),\nand the redundancy of the information above and below the diagonal.\nTable 1-7. Correlation between\ntelecommunication stock\nreturns\nT\nCTL FTR VZ\nLVLT\nT\n1.000 0.475 0.328 0.678 0.279\nCTL\n0.475 1.000 0.420 0.417 0.287\nFTR\n0.328 0.420 1.000 0.287 0.260\n",
      "content_length": 1461,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 70,
      "content": "VZ\n0.678 0.417 0.287 1.000 0.242\nLVLT 0.279 0.287 0.260 0.242 1.000\nA table of correlations like Table 1-7 is commonly plotted to visually display the\nrelationship between multiple variables. Figure 1-6 shows the correlation\nbetween the daily returns for major exchange traded funds (ETFs). In R, we can\neasily create this using the package corrplot:\netfs <- sp500_px[row.names(sp500_px)>\"2012-07-01\",\n                 sp500_sym[sp500_sym$sector==\"etf\", 'symbol']]\nlibrary(corrplot)\ncorrplot(cor(etfs), method = \"ellipse\")\nThe ETFs for the S&P 500 (SPY) and the Dow Jones Index (DIA) have a high\ncorrelation. Similary, the QQQ and the XLK, composed mostly of technology\ncompanies, are postively correlated. Defensive ETFs, such as those tracking gold\nprices (GLD), oil prices (USO), or market volatility (VXX) tend to be negatively\ncorrelated with the other ETFs. The orientation of the ellipse indicates whether\ntwo variables are positively correlated (ellipse is pointed right) or negatively\ncorrelated (ellipse is pointed left). The shading and width of the ellipse indicate\nthe strength of the association: thinner and darker ellipses correspond to stronger\nrelationships.\nLike the mean and standard deviation, the correlation coefficient is sensitive to\noutliers in the data. Software packages offer robust alternatives to the classical\ncorrelation coefficient. For example, the R function cor has a trim argument\nsimilar to that for computing a trimmed mean (see [R-base-2015]).\n",
      "content_length": 1485,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 71,
      "content": "Figure 1-6. Correlation between ETF returns\n",
      "content_length": 44,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 72,
      "content": "OTHER CORRELATION ESTIMATES\nStatisticians have long ago proposed other types of correlation coefficients, such as Spearman’s\nrho or Kendall’s tau. These are correlation coefficients based on the rank of the data. Since\nthey work with ranks rather than values, these estimates are robust to outliers and can handle\ncertain types of nonlinearities. However, data scientists can generally stick to Pearson’s\ncorrelation coefficient, and its robust alternatives, for exploratory analysis. The appeal of rank-\nbased estimates is mostly for smaller data sets and specific hypothesis tests.\n",
      "content_length": 584,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 73,
      "content": "Scatterplots\nThe standard way to visualize the relationship between two measured data\nvariables is with a scatterplot. The x-axis represents one variable, the y-axis\nanother, and each point on the graph is a record. See Figure 1-7 for a plot between\nthe daily returns for ATT and Verizon. This is produced in R with the command:\nplot(telecom$T, telecom$VZ, xlab=\"T\", ylab=\"VZ\")\nThe returns have a strong positive relationship: on most days, both stocks go up or\ngo down in tandem. There are very few days where one stock goes down\nsignificantly while the other stock goes up (and vice versa).\n",
      "content_length": 593,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 74,
      "content": "Figure 1-7. Scatterplot between returns for ATT and Verizon\nKEY IDEAS FOR CORRELATION\nThe correlation coefficient measures the extent to which two variables are associated with one\nanother.\nWhen high values of v1 go with high values of v2, v1 and v2 are positively associated.\nWhen high values of v1 are associated with low values of v2, v1 and v2 are negatively associated.\n",
      "content_length": 375,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 75,
      "content": "The correlation coefficient is a standardized metric so that it always ranges from –1 (perfect\nnegative correlation) to +1 (perfect positive correlation).\nA correlation coefficient of 0 indicates no correlation, but be aware that random arrangements of\ndata will produce both positive and negative values for the correlation coefficient just by chance.\n",
      "content_length": 353,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 76,
      "content": "Further Reading\nStatistics, 4th ed., by David Freedman, Robert Pisani, and Roger Purves (W. W.\nNorton, 2007), has an excellent discussion of correlation.\n",
      "content_length": 154,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 77,
      "content": "Exploring Two or More Variables\nFamiliar estimators like mean and variance look at variables one at a time\n(univariate analysis). Correlation analysis (see “Correlation”) is an important\nmethod that compares two variables (bivariate analysis). In this section we look\nat additional estimates and plots, and at more than two variables (multivariate\nanalysis).\nKEY TERMS FOR EXPLORING TWO OR MORE VARIABLES\nContingency tables\nA tally of counts between two or more categorical variables.\nHexagonal binning\nA plot of two numeric variables with the records binned into hexagons.\nContour plots\nA plot showing the density of two numeric variables like a topographical map.\nViolin plots\nSimilar to a boxplot but showing the density estimate.\nLike univariate analysis, bivariate analysis involves both computing summary\nstatistics and producing visual displays. The appropriate type of bivariate or\nmultivariate analysis depends on the nature of the data: numeric versus\ncategorical.\n",
      "content_length": 975,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 78,
      "content": "Hexagonal Binning and Contours (Plotting Numeric versus\nNumeric Data)\nScatterplots are fine when there is a relatively small number of data values. The\nplot of stock returns in Figure 1-7 involves only about 750 points. For data sets\nwith hundreds of thousands or millions of records, a scatterplot will be too dense,\nso we need a different way to visualize the relationship. To illustrate, consider the\ndata set kc_tax, which contains the tax-assessed values for residential properties\nin King County, Washington. In order to focus on the main part of the data, we strip\nout very expensive and very small or large residences using the subset function:\nkc_tax0 <- subset(kc_tax, TaxAssessedValue < 750000 & SqFtTotLiving>100 &\n                  SqFtTotLiving<3500)\nnrow(kc_tax0)\n[1] 432733\nFigure 1-8 is a hexagon binning plot of the relationship between the finished\nsquare feet versus the tax-assessed value for homes in King County. Rather than\nplotting points, which would appear as a monolithic dark cloud, we grouped the\nrecords into hexagonal bins and plotted the hexagons with a color indicating the\nnumber of records in that bin. In this chart, the positive relationship between\nsquare feet and tax-assessed value is clear. An interesting feature is the hint of a\nsecond cloud above the main cloud, indicating homes that have the same square\nfootage as those in the main cloud, but a higher tax-assessed value.\nFigure 1-8 was generated by the powerful R package ggplot2, developed by\nHadley Wickham [ggplot2]. ggplot2 is one of several new software libraries for\nadvanced exploratory visual analysis of data; see “Visualizing Multiple\nVariables”.\nggplot(kc_tax0, (aes(x=SqFtTotLiving, y=TaxAssessedValue))) +\n  stat_binhex(colour=\"white\") +\n  theme_bw() +\n  scale_fill_gradient(low=\"white\", high=\"black\") +\n  labs(x=\"Finished Square Feet\", y=\"Tax Assessed Value\")\n",
      "content_length": 1873,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 79,
      "content": "Figure 1-8. Hexagonal binning for tax-assessed value versus finished square feet\nFigure 1-9 uses contours overlaid on a scatterplot to visualize the relationship\nbetween two numeric variables. The contours are essentially a topographical map\nto two variables; each contour band represents a specific density of points,\nincreasing as one nears a “peak.” This plot shows a similar story as Figure 1-8:\nthere is a secondary peak “north” of the main peak. This chart was also created\nusing ggplot2 with the built-in geom_density2d function.\n",
      "content_length": 537,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 80,
      "content": "ggplot(kc_tax0, aes(SqFtTotLiving, TaxAssessedValue)) +\n  theme_bw() +\n  geom_point( alpha=0.1) +\n  geom_density2d(colour=\"white\") +\n  labs(x=\"Finished Square Feet\", y=\"Tax Assessed Value\")\n",
      "content_length": 190,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 81,
      "content": "Figure 1-9. Contour plot for tax-assessed value versus finished square feet\nOther types of charts are used to show the relationship between two numeric\nvariables, including heat maps. Heat maps, hexagonal binning, and contour plots\nall give a visual representation of a two-dimensional density. In this way, they are\nnatural analogs to histograms and density plots.\n",
      "content_length": 366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 82,
      "content": "Two Categorical Variables\nA useful way to summarize two categorical variables is a contingency table — a\ntable of counts by category. Table 1-8 shows the contingency table between the\ngrade of a personal loan and the outcome of that loan. This is taken from data\nprovided by Lending Club, a leader in the peer-to-peer lending business. The\ngrade goes from A (high) to G (low). The outcome is either paid off, current, late,\nor charged off (the balance of the loan is not expected to be collected). This table\nshows the count and row percentages. High-grade loans have a very low\nlate/charge-off percentage as compared with lower-grade loans. Contingency\ntables can look at just counts, or also include column and total percentages. Pivot\ntables in Excel are perhaps the most common tool used to create contingency\ntables. In R, the CrossTable function in the descr package produces contingency\ntables, and the following code was used to create Table 1-8:\nlibrary(descr)\nx_tab <- CrossTable(lc_loans$grade, lc_loans$status,\n                    prop.c=FALSE, prop.chisq=FALSE, prop.t=FALSE)\nTable 1-8. Contingency table of loan grade\nand status\nGrade Fully paid Current Late\nCharged off Total\nA\n20715\n52058\n494\n1588\n74855\n0.277\n0.695\n0.007 0.021\n0.161\nB\n31782\n97601\n2149\n5384\n136916\n0.232\n0.713\n0.016 0.039\n0.294\nC\n23773\n92444\n2895\n6163\n125275\n0.190\n0.738\n0.023 0.049\n0.269\nD\n14036\n55287\n2421\n5131\n76875\n0.183\n0.719\n0.031 0.067\n0.165\nE\n6089\n25344\n1421\n2898\n35752\n0.170\n0.709\n0.040 0.081\n0.077\n",
      "content_length": 1491,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 83,
      "content": "F\n2376\n8675\n621\n1556\n13228\n0.180\n0.656\n0.047 0.118\n0.028\nG\n655\n2042\n206\n419\n3322\n0.197\n0.615\n0.062 0.126\n0.007\nTotal\n99426\n333451\n10207 23139\n466223\n",
      "content_length": 149,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 84,
      "content": "Categorical and Numeric Data\nBoxplots (see “Percentiles and Boxplots”) are a simple way to visually compare\nthe distributions of a numeric variable grouped according to a categorical\nvariable. For example, we might want to compare how the percentage of flight\ndelays varies across airlines. Figure 1-10 shows the percentage of flights in a\nmonth that were delayed where the delay was within the carrier’s control.\nboxplot(pct_delay ~ airline, data=airline_stats, ylim=c(0, 50))\n",
      "content_length": 478,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 85,
      "content": "Figure 1-10. Boxplot of percent of airline delays by carrier\nAlaska stands out as having the fewest delays, while American has the most\ndelays: the lower quartile for American is higher than the upper quartile for\nAlaska.\nA violin plot, introduced by [Hintze-Nelson-1998], is an enhancement to the\nboxplot and plots the density estimate with the density on the y-axis. The density is\nmirrored and flipped over and the resulting shape is filled in, creating an image\nresembling a violin. The advantage of a violin plot is that it can show nuances in\nthe distribution that aren’t perceptible in a boxplot. On the other hand, the boxplot\nmore clearly shows the outliers in the data. In ggplot2, the function geom_violin\n",
      "content_length": 717,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 86,
      "content": "can be used to create a violin plot as follows:\nggplot(data=airline_stats, aes(airline, pct_carrier_delay)) +\n  ylim(0, 50) +\n  geom_violin() +\n  labs(x=\"\", y=\"Daily % of Delayed Flights\")\nThe corresponding plot is shown in Figure 1-11. The violin plot shows a\nconcentration in the distribution near zero for Alaska, and to a lesser extent, Delta.\nThis phenomenon is not as obvious in the boxplot. You can combine a violin plot\nwith a boxplot by adding geom_boxplot to the plot (although this is best when\ncolors are used).\n",
      "content_length": 524,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 87,
      "content": "Figure 1-11. Combination of boxplot and violin plot of percent of airline delays by carrier\n",
      "content_length": 92,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 88,
      "content": "Visualizing Multiple Variables\nThe types of charts used to compare two variables — scatterplots, hexagonal\nbinning, and boxplots — are readily extended to more variables through the notion\nof conditioning. As an example, look back at Figure 1-8, which showed the\nrelationship between homes’ finished square feet and tax-assessed values. We\nobserved that there appears to be a cluster of homes that have higher tax-assessed\nvalue per square foot. Diving deeper, Figure 1-12 accounts for the effect of\nlocation by plotting the data for a set of zip codes. Now the picture is much\nclearer: tax-assessed value is much higher in some zip codes (98112, 98105) than\nin others (98108, 98057). This disparity gives rise to the clusters observed in\nFigure 1-8.\nWe created Figure 1-12 using ggplot2 and the idea of facets, or a conditioning\nvariable (in this case zip code):\nggplot(subset(kc_tax0, ZipCode %in% c(98188, 98105, 98108, 98126)),\n         aes(x=SqFtTotLiving, y=TaxAssessedValue)) +\n  stat_binhex(colour=\"white\") +\n  theme_bw() +\n  scale_fill_gradient( low=\"white\", high=\"blue\") +\n  labs(x=\"Finished Square Feet\", y=\"Tax Assessed Value\") +\n  facet_wrap(\"ZipCode\")\n",
      "content_length": 1166,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 89,
      "content": "Figure 1-12. Tax-assessed value versus finished square feet by zip code\nThe concept of conditioning variables in a graphics system was pioneered with\nTrellis graphics, developed by Rick Becker, Bill Cleveland, and others at Bell\nLabs [Trellis-Graphics]. This idea has propogated to various modern graphics\nsystems, such as the lattice ([lattice]) and ggplot2 packages in R and the\nSeaborn ([seaborne]) and Bokeh ([bokeh]) modules in Python. Conditioning\nvariables are also integral to business intelligence platforms such as Tableau and\nSpotfire. With the advent of vast computing power, modern visualization platforms\nhave moved well beyond the humble beginnings of exploratory data analysis.\nHowever, key concepts and tools developed over the years still form a foundation\nfor these systems.\nKEY IDEAS\n",
      "content_length": 804,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 90,
      "content": "Hexagonal binning and contour plots are useful tools that permit graphical examination of two\nnumeric variables at a time, without being overwhelmed by huge amounts of data.\nContingency tables are the standard tool for looking at the counts of two categorical variables.\nBoxplots and violin plots allow you to plot a numeric variable against a categorical variable.\n",
      "content_length": 366,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 91,
      "content": "Further Reading\nModern Data Science with R, by Benjamin Baumer, Daniel Kaplan, and\nNicholas Horton (CRC Press, 2017), has an excellent presentation of “a\ngrammar for graphics” (the “gg” in ggplot).\nGgplot2: Elegant Graphics for Data Analysis, by Hadley Wickham, is an\nexcellent resource from the creator of ggplot2 (Springer, 2009).\nJosef Fruehwald has a web-based tutorial on ggplot2.\n",
      "content_length": 386,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 92,
      "content": "Summary\nWith the development of exploratory data analysis (EDA), pioneered by John\nTukey, statistics set a foundation that was a precursor to the field of data science.\nThe key idea of EDA is that the first and most important step in any project based\non data is to look at the data. By summarizing and visualizing the data, you can\ngain valuable intuition and understanding of the project.\nThis chapter has reviewed concepts ranging from simple metrics, such as\nestimates of location and variability, to rich visual displays to explore the\nrelationships between multiple variables, as in Figure 1-12. The diverse set of\ntools and techniques being developed by the open source community, combined\nwith the expressiveness of the R and Python languages, has created a plethora of\nways to explore and analyze data. Exploratory analysis should be a cornerstone of\nany data science project.\n",
      "content_length": 886,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 93,
      "content": "Chapter 2. Data and Sampling\nDistributions\nA popular misconception holds that the era of big data means the end of a need for\nsampling. In fact, the proliferation of data of varying quality and relevance\nreinforces the need for sampling as a tool to work efficiently with a variety of data\nand to minimize bias. Even in a big data project, predictive models are typically\ndeveloped and piloted with samples. Samples are also used in tests of various\nsorts (e.g., pricing, web treatments).\nFigure 2-1 shows a schematic that underpins the concepts in this chapter. The\nlefthand side represents a population that, in statistics, is assumed to follow an\nunderlying but unknown distribution. The only thing available is the sample data\nand its empirical distribution, shown on the righthand side. To get from the\nlefthand side to the righthand side, a sampling procedure is used (represented by\ndashed arrows). Traditional statistics focused very much on the lefthand side,\nusing theory based on strong assumptions about the population. Modern statistics\nhas moved to the righthand side, where such assumptions are not needed.\n",
      "content_length": 1122,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 94,
      "content": "Figure 2-1. Population versus sample\nIn general, data scientists need not worry about the theoretical nature of the\nlefthand side, and instead should focus on the sampling procedures and the data at\nhand. There are some notable exceptions. Sometimes data is generated from a\nphysical process that can be modeled. The simplest example is flipping a coin:\nthis follows a binomial distribution. Any real-life binomial situation (buy or don’t\nbuy, fraud or no fraud, click or don’t click) can be modeled effectively by a coin\n(with modified probability of landing heads, of course). In these cases, we can\ngain additional insight by using our understanding of the population.\n",
      "content_length": 672,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 95,
      "content": "Random Sampling and Sample Bias\nA sample is a subset of data from a larger data set; statisticians call this larger\ndata set the population. A population in statistics is not the same thing as in\nbiology — it is a large, defined but sometimes theoretical or imaginary, set of\ndata.\nKEY TERMS FOR RANDOM SAMPLING\nSample\nA subset from a larger data set.\nPopulation\nThe larger data set or idea of a data set.\nN (n)\nThe size of the population (sample).\nRandom sampling\nDrawing elements into a sample at random.\nStratified sampling\nDividing the population into strata and randomly sampling from each strata.\nSimple random sample\nThe sample that results from random sampling without stratifying the population.\nSample bias\nA sample that misrepresents the population.\nRandom sampling is a process in which each available member of the population\nbeing sampled has an equal chance of being chosen for the sample at each draw.\nThe sample that results is called a simple random sample. Sampling can be done\nwith replacement, in which observations are put back in the population after each\ndraw for possible future reselection. Or it can be done without replacement, in\nwhich case observations, once selected, are unavailable for future draws.\nData quality often matters more than data quantity when making an estimate or a\nmodel based on a sample. Data quality in data science involves completeness,\nconsistency of format, cleanliness, and accuracy of individual data points.\nStatistics adds the notion of representativeness.\n",
      "content_length": 1516,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 96,
      "content": "The classic example is the Literary Digest poll of 1936 that predicted a victory of\nAl Landon against Franklin Roosevelt. The Literary Digest, a leading periodical\nof the day, polled its entire subscriber base, plus additional lists of individuals, a\ntotal of over 10 million, and predicted a landslide victory for Landon. George\nGallup, founder of the Gallup Poll, conducted biweekly polls of just 2,000, and\naccurately predicted a Roosevelt victory. The difference lay in the selection of\nthose polled.\nThe Literary Digest opted for quantity, paying little attention to the method of\nselection. They ended up polling those with relatively high socioeconomic status\n(their own subscribers, plus those who, by virtue of owning luxuries like\ntelephones and automobiles, appeared in marketers’ lists). The result was sample\nbias; that is, the sample was different in some meaningful nonrandom way from\nthe larger population it was meant to represent. The term nonrandom is important\n— hardly any sample, including random samples, will be exactly representative of\nthe population. Sample bias occurs when the difference is meaningful, and can be\nexpected to continue for other samples drawn in the same way as the first.\n",
      "content_length": 1218,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 97,
      "content": "SELF-SELECTION SAMPLING BIAS\nThe reviews of restaurants, hotels, cafes, and so on that you read on social media sites like Yelp\nare prone to bias because the people submitting them are not randomly selected; rather, they\nthemselves have taken the initiative to write. This leads to self-selection bias — the people\nmotivated to write reviews may be those who had poor experiences, may have an association\nwith the establishment, or may simply be a different type of person from those who do not write\nreviews. Note that while self-selection samples can be unreliable indicators of the true state of\naffairs, they may be more reliable in simply comparing one establishment to a similar one; the\nsame self-selection bias might apply to each.\n",
      "content_length": 740,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 98,
      "content": "Bias\nStatistical bias refers to measurement or sampling errors that are systematic and\nproduced by the measurement or sampling process. An important distinction\nshould be made between errors due to random chance, and errors due to bias.\nConsider the physical process of a gun shooting at a target. It will not hit the\nabsolute center of the target every time, or even much at all. An unbiased process\nwill produce error, but it is random and does not tend strongly in any direction\n(see Figure 2-2). The results shown in Figure 2-3 show a biased process — there\nis still random error in both the x and y direction, but there is also a bias. Shots\ntend to fall in the upper-right quadrant.\nFigure 2-2. Scatterplot of shots from a gun with true aim\n",
      "content_length": 747,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 99,
      "content": "Figure 2-3. Scatterplot of shots from a gun with biased aim\nBias comes in different forms, and may be observable or invisible. When a result\ndoes suggest bias (e.g., by reference to a benchmark or actual values), it is often\nan indicator that a statistical or machine learning model has been misspecified, or\nan important variable left out.\n",
      "content_length": 341,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 100,
      "content": "Random Selection\nTo avoid the problem of sample bias that led the Literary Digest to predict\nLandon over Roosevelt, George Gallup (shown in Figure 2-4) opted for more\nscientifically chosen methods to achieve a sample that was representative of the\nUS voter. There are now a variety of methods to achieve representativeness, but at\nthe heart of all of them lies random sampling.\nFigure 2-4. George Gallup, catapulted to fame by the Literary Digest’s “big data” failure\nRandom sampling is not always easy. Proper definition of an accessible\npopulation is key. Suppose we want to generate a representative profile of\ncustomers and we need to conduct a pilot customer survey. The survey needs to be\nrepresentative but is labor intensive.\nFirst we need to define who a customer is. We might select all customer records\nwhere purchase amount > 0. Do we include all past customers? Do we include\nrefunds? Internal test purchases? Resellers? Both billing agent and customer?\nNext we need to specify a sampling procedure. It might be “select 100 customers\nat random.” Where a sampling from a flow is involved (e.g., real-time customer\ntransactions or web visitors), timing considerations may be important (e.g., a web\nvisitor at 10 a.m. on a weekday may be different from a web visitor at 10 p.m. on\na weekend).\nIn stratified sampling, the population is divided up into strata, and random\nsamples are taken from each stratum. Political pollsters might seek to learn the\nelectoral preferences of whites, blacks, and Hispanics. A simple random sample\nwww.allitebooks.com\n",
      "content_length": 1560,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 101,
      "content": "taken from the population would yield too few blacks and Hispanics, so those\nstrata could be overweighted in stratified sampling to yield equivalent sample\nsizes.\n",
      "content_length": 163,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 102,
      "content": "Size versus Quality: When Does Size Matter?\nIn the era of big data, it is sometimes surprising that smaller is better. Time and\neffort spent on random sampling not only reduce bias, but also allow greater\nattention to data exploration and data quality. For example, missing data and\noutliers may contain useful information. It might be prohibitively expensive to\ntrack down missing values or evaluate outliers in millions of records, but doing so\nin a sample of several thousand records may be feasible. Data plotting and manual\ninspection bog down if there is too much data.\nSo when are massive amounts of data needed?\nThe classic scenario for the value of big data is when the data is not only big, but\nsparse as well. Consider the search queries received by Google, where columns\nare terms, rows are individual search queries, and cell values are either 0 or 1,\ndepending on whether a query contains a term. The goal is to determine the best\npredicted search destination for a given query. There are over 150,000 words in\nthe English language, and Google processes over 1 trillion queries per year. This\nyields a huge matrix, the vast majority of whose entries are “0.”\nThis is a true big data problem — only when such enormous quantities of data are\naccumulated can effective search results be returned for most queries. And the\nmore data accumulates, the better the results. For popular search terms this is not\nsuch a problem — effective data can be found fairly quickly for the handful of\nextremely popular topics trending at a particular time. The real value of modern\nsearch technology lies in the ability to return detailed and useful results for a huge\nvariety of search queries, including those that occur only with a frequency, say, of\none in a million.\nConsider the search phrase “Ricky Ricardo and Little Red Riding Hood.” In the\nearly days of the internet, this query would probably have returned results on\nRicky Ricardo the band leader, the television show I Love Lucy in which he\nstarred, and the children’s story Little Red Riding Hood. Later, now that trillions\nof search queries have been accumulated, this search query returns the exact I\nLove Lucy episode in which Ricky narrates, in dramatic fashion, the Little Red\nRiding Hood story to his infant son in a comic mix of English and Spanish.\nKeep in mind that the number of actual pertinent records — ones in which this\n",
      "content_length": 2394,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 103,
      "content": "exact search query, or something very similar, appears (together with information\non what link people ultimately clicked on) — might need only be in the thousands\nto be effective. However, many trillions of data points are needed in order to\nobtain these pertinent records (and random sampling, of course, will not help).\nSee also “Long-Tailed Distributions”.\n",
      "content_length": 360,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 104,
      "content": "Sample Mean versus Population Mean\nThe symbol \n (pronounced x-bar) is used to represent the mean of a sample from\na population, whereas  is used to represent the mean of a population. Why make\nthe distinction? Information about samples is observed, and information about\nlarge populations is often inferred from smaller samples. Statisticians like to keep\nthe two things separate in the symbology.\nKEY IDEAS\nEven in the era of big data, random sampling remains an important arrow in the data scientist’s\nquiver.\nBias occurs when measurements or observations are systematically in error because they are not\nrepresentative of the full population.\nData quality is often more important than data quantity, and random sampling can reduce bias and\nfacilitate quality improvement that would be prohibitively expensive.\n",
      "content_length": 813,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 105,
      "content": "Further Reading\nA useful review of sampling procedures can be found in Ronald Fricker’s\nchapter “Sampling Methods for Web and E-mail Surveys,” found in the Sage\nHandbook of Online Research Methods. This chapter includes a review of\nthe modifications to random sampling that are often used for practical\nreasons of cost or feasibility.\nThe story of the Literary Digest poll failure can be found on the Capital\nCentury website.\n",
      "content_length": 426,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 106,
      "content": "Selection Bias\nTo paraphrase Yogi Berra, “If you don’t know what you’re looking for, look hard\nenough and you’ll find it.”\nSelection bias refers to the practice of selectively choosing data — consciously or\nunconsciously — in a way that that leads to a conclusion that is misleading or\nephemeral.\nKEY TERMS\nBias\nSystematic error.\nData snooping\nExtensive hunting through data in search of something interesting.\nVast search effect\nBias or nonreproducibility resulting from repeated data modeling, or modeling data with large\nnumbers of predictor variables.\nIf you specify a hypothesis and conduct a well-designed experiment to test it, you\ncan have high confidence in the conclusion. Such is often not the case, however.\nOften, one looks at available data and tries to discern patterns. But is the pattern\nfor real, or just the product of data snooping — that is, extensive hunting through\nthe data until something interesting emerges? There is a saying among statisticians:\n“If you torture the data long enough, sooner or later it will confess.”\nThe difference between a phenomenon that you verify when you test a hypothesis\nusing an experiment, versus a phenomenon that you discover by perusing available\ndata, can be illuminated with the following thought experiment.\nImagine that someone tells you she can flip a coin and have it land heads on the\nnext 10 tosses. You challenge her (the equivalent of an experiment), and she\nproceeds to toss it 10 times, all landing heads. Clearly you ascribe some special\ntalent to her — the probability that 10 coin tosses will land heads just by chance\nis 1 in 1,000.\nNow imagine that the announcer at a sports stadium asks the 20,000 people in\nattendance each to toss a coin 10 times, and report to an usher if they get 10 heads\n",
      "content_length": 1770,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 107,
      "content": "in a row. The chance that somebody in the stadium will get 10 heads is extremely\nhigh (more than 99% — it’s 1 minus the probability that nobody gets 10 heads).\nClearly, selecting, after the fact, the person (or persons) who gets 10 heads at the\nstadium does not indicate they have any special talent — it’s most likely luck.\nSince repeated review of large data sets is a key value proposition in data\nscience, selection bias is something to worry about. A form of selection bias of\nparticular concern to data scientists is what John Elder (founder of Elder\nResearch, a respected data mining consultancy) calls the vast search effect. If you\nrepeatedly run different models and ask different questions with a large data set,\nyou are bound to find something interesting. Is the result you found truly something\ninteresting, or is it the chance outlier?\nWe can guard against this by using a holdout set, and sometimes more than one\nholdout set, against which to validate performance. Elder also advocates the use\nof what he calls target shuffling (a permutation test, in essence) to test the validity\nof predictive associations that a data mining model suggests.\nTypical forms of selection bias in statistics, in addition to the vast search effect,\ninclude nonrandom sampling (see sampling bias), cherry-picking data, selection\nof time intervals that accentuate a partiular statistical effect, and stopping an\nexperiment when the results look “interesting.”\n",
      "content_length": 1455,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 108,
      "content": "Regression to the Mean\nRegression to the mean refers to a phenomenon involving successive\nmeasurements on a given variable: extreme observations tend to be followed by\nmore central ones. Attaching special focus and meaning to the extreme value can\nlead to a form of selection bias.\nSports fans are familiar with the “rookie of the year, sophomore slump”\nphenomenon. Among the athletes who begin their career in a given season (the\nrookie class), there is always one who performs better than all the rest. Generally,\nthis “rookie of the year” does not do as well in his second year. Why not?\nIn nearly all major sports, at least those played with a ball or puck, there are two\nelements that play a role in overall performance:\nSkill\nLuck\nRegression to the mean is a consequence of a particular form of selection bias.\nWhen we select the rookie with the best performance, skill and good luck are\nprobably contributing. In his next season, the skill will still be there but, in most\ncases, the luck will not, so his performance will decline — it will regress. The\nphenomenon was first identified by Francis Galton in 1886 [Galton-1886], who\nwrote of it in connection with genetic tendencies; for example, the children of\nextremely tall men tend not to be as tall as their father (see Figure 2-5).\n",
      "content_length": 1294,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 109,
      "content": "Figure 2-5. Galton’s study that identified the phenomenon of regression to the mean\nWARNING\nRegression to the mean, meaning to “go back,” is distinct from the statistical modeling method of\nlinear regression, in which a linear relationship is estimated between predictor variables and an\noutcome variable.\nKEY IDEAS\nSpecifying a hypothesis, then collecting data following randomization and random sampling\nprinciples, ensures against bias.\nAll other forms of data analysis run the risk of bias resulting from the data collection/analysis\n",
      "content_length": 538,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 110,
      "content": "process (repeated running of models in data mining, data snooping in research, and after-the-fact\nselection of interesting events).\n",
      "content_length": 132,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 111,
      "content": "Further Reading\nChristopher J. Pannucci and Edwin G. Wilkins’ article “Identifying and\nAvoiding Bias in Research” in (surprisingly) Plastic and Reconstructive\nSurgery (August 2010) has an excellent review of various types of bias that\ncan enter into research, including selection bias.\nMichael Harris’s article “Fooled by Randomness Through Selection Bias”\nprovides an interesting review of selection bias considerations in stock\nmarket trading schemes, from the perspective of traders.\n",
      "content_length": 487,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 112,
      "content": "Sampling Distribution of a Statistic\nThe term sampling distribution of a statistic refers to the distribution of some\nsample statistic, over many samples drawn from the same population. Much of\nclassical statistics is concerned with making inferences from (small) samples to\n(very large) populations.\nKEY TERMS\nSample statistic\nA metric calculated for a sample of data drawn from a larger population.\nData distribution\nThe frequency distribution of individual values in a data set.\nSampling distribution\nThe frequency distribution of a sample statistic over many samples or resamples.\nCentral limit theorem\nThe tendency of the sampling distribution to take on a normal shape as sample size rises.\nStandard error\nThe variability (standard deviation) of a sample statistic over many samples (not to be confused\nwith standard deviation, which, by itself, refers to variability of individual data values).\nTypically, a sample is drawn with the goal of measuring something (with a sample\nstatistic) or modeling something (with a statistical or machine learning model).\nSince our estimate or model is based on a sample, it might be in error; it might be\ndifferent if we were to draw a different sample. We are therefore interested in\nhow different it might be — a key concern is sampling variability. If we had lots\nof data, we could draw additional samples and observe the distribution of a\nsample statistic directly. Typically, we will calculate our estimate or model using\nas much data as is easily available, so the option of drawing additional samples\nfrom the population is not readily available.\nWARNING\nIt is important to distinguish between the distribution of the individual data points, known as the\ndata distribution, and the distribution of a sample statistic, known as the sampling distribution.\n",
      "content_length": 1804,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 113,
      "content": "The distribution of a sample statistic such as the mean is likely to be more regular\nand bell-shaped than the distribution of the data itself. The larger the sample that\nthe statistic is based on, the more this is true. Also, the larger the sample, the\nnarrower the distribution of the sample statistic.\nThis is illustrated in an example using annual income for loan applicants to\nLending Club (see “A Small Example: Predicting Loan Default” for a description\nof the data). Take three samples from this data: a sample of 1,000 values, a\nsample of 1,000 means of 5 values, and a sample of 1,000 means of 20 values.\nThen plot a histogram of each sample to produce Figure 2-6.\n",
      "content_length": 674,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 115,
      "content": "Figure 2-6. Histogram of annual incomes of 1,000 loan applicants (top), then 1000 means of n=5\napplicants (middle), and n=20 (bottom)\nThe histogram of the individual data values is broadly spread out and skewed\ntoward higher values as is to be expected with income data. The histograms of the\nmeans of 5 and 20 are increasingly compact and more bell-shaped. Here is the R\ncode to generate these histograms, using the visualization package ggplot2.\nlibrary(ggplot2)\n# take a simple random sample\nsamp_data <- data.frame(income=sample(loans_income, 1000),\n                        type='data_dist')\n# take a sample of means of 5 values\nsamp_mean_05 <- data.frame(\n  income = tapply(sample(loans_income, 1000*5),\n                  rep(1:1000, rep(5, 1000)), FUN=mean),\n  type = 'mean_of_5')\n# take a sample of means of 20 values\nsamp_mean_20 <- data.frame(\n  income = tapply(sample(loans_income, 1000*20),\n                  rep(1:1000, rep(20, 1000)), FUN=mean),\n  type = 'mean_of_20')\n# bind the data.frames and convert type to a factor\nincome <- rbind(samp_data, samp_mean_05, samp_mean_20)\nincome$type = factor(income$type,\n                     levels=c('data_dist', 'mean_of_5', 'mean_of_20'),\n                     labels=c('Data', 'Mean of 5', 'Mean of 20'))\n# plot the histograms\nggplot(income, aes(x=income)) +\n  geom_histogram(bins=40) +\n  facet_grid(type ~ .)\n",
      "content_length": 1365,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 116,
      "content": "Central Limit Theorem\nThis phenomenon is termed the central limit theorem. It says that the means drawn\nfrom multiple samples will resemble the familiar bell-shaped normal curve (see\n“Normal Distribution”), even if the source population is not normally distributed,\nprovided that the sample size is large enough and the departure of the data from\nnormality is not too great. The central limit theorem allows normal-approximation\nformulas like the t-distribution to be used in calculating sampling distributions for\ninference — that is, confidence intervals and hypothesis tests.\nThe central limit theorem receives a lot of attention in traditional statistics texts\nbecause it underlies the machinery of hypothesis tests and confidence intervals,\nwhich themselves consume half the space in such texts. Data scientists should be\naware of this role, but, since formal hypothesis tests and confidence intervals play\na small role in data science, and the bootstrap is available in any case, the central\nlimit theorem is not so central in the practice of data science.\n",
      "content_length": 1063,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 117,
      "content": "Standard Error\nThe standard error is a single metric that sums up the variability in the sampling\ndistribution for a statistic. The standard error can be estimated using a statistic\nbased on the standard deviation s of the sample values, and the sample size n:\nAs the sample size increases, the standard error decreases, corresponding to what\nwas observed in Figure 2-6. The relationship between standard error and sample\nsize is sometimes referred to as the square-root of n rule: in order to reduce the\nstandard error by a factor of 2, the sample size must be increased by a factor of 4.\nThe validity of the standard error formula arises from the central limit theorem\n(see “Central Limit Theorem”). In fact, you don’t need to rely on the central limit\ntheorem to understand standard error. Consider the following approach to measure\nstandard error:\n1. Collect a number of brand new samples from the population.\n2. For each new sample, calculate the statistic (e.g., mean).\n3. Calculate the standard deviation of the statistics computed in step 2; use\nthis as your estimate of standard error.\nIn practice, this approach of collecting new samples to estimate the standard error\nis typically not feasible (and statistically very wasteful). Fortunately, it turns out\nthat it is not necessary to draw brand new samples; instead, you can use bootstrap\nresamples (see “The Bootstrap”). In modern statistics, the bootstrap has become\nthe standard way to to estimate standard error. It can be used for virtually any\nstatistic and does not rely on the central limit theorem or other distributional\nassumptions.\n",
      "content_length": 1604,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 118,
      "content": "STANDARD DEVIATION VERSUS STANDARD ERROR\nDo not confuse standard deviation (which measures the variability of individual data points) with\nstandard error (which measures the variability of a sample metric).\nKEY IDEAS\nThe frequency distribution of a sample statistic tells us how that metric would turn out differently\nfrom sample to sample.\nThis sampling distribution can be estimated via the bootstrap, or via formulas that rely on the central\nlimit theorem.\nA key metric that sums up the variability of a sample statistic is its standard error.\n",
      "content_length": 547,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 119,
      "content": "Further Reading\nDavid Lane’s online multimedia resource in statistics has a useful simulation that\nallows you to select a sample statistic, a sample size and number of iterations and\nvisualize a histogram of the resulting frequency distribution.\n",
      "content_length": 246,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 120,
      "content": "The Bootstrap\nOne easy and effective way to estimate the sampling distribution of a statistic, or\nof model parameters, is to draw additional samples, with replacement, from the\nsample itself and recalculate the statistic or model for each resample. This\nprocedure is called the bootstrap, and it does not necessarily involve any\nassumptions about the data or the sample statistic being normally distributed.\nKEY TERMS\nBootstrap sample\nA sample taken with replacement from an observed data set.\nResampling\nThe process of taking repeated samples from observed data; includes both bootstrap and\npermutation (shuffling) procedures.\nConceptually, you can imagine the bootstrap as replicating the original sample\nthousands or millions of times so that you have a hypothetical population that\nembodies all the knowledge from your original sample (it’s just larger). You can\nthen draw samples from this hypothetical population for the purpose of estimating\na sampling distribution. See Figure 2-7.\nFigure 2-7. The idea of the bootstrap\n",
      "content_length": 1028,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 121,
      "content": "In practice, it is not necessary to actually replicate the sample a huge number of\ntimes. We simply replace each observation after each draw; that is, we sample\nwith replacement. In this way we effectively create an infinite population in\nwhich the probability of an element being drawn remains unchanged from draw to\ndraw. The algorithm for a bootstrap resampling of the mean is as follows, for a\nsample of size n:\n1. Draw a sample value, record, replace it.\n2. Repeat n times.\n3. Record the mean of the n resampled values.\n4. Repeat steps 1–3 R times.\n5. Use the R results to:\na. Calculate their standard deviation (this estimates sample mean\nstandard error).\nb. Produce a histogram or boxplot.\nc. Find a confidence interval.\nR, the number of iterations of the bootstrap, is set somewhat arbitrarily. The more\niterations you do, the more accurate the estimate of the standard error, or the\nconfidence interval. The result from this procedure is a bootstrap set of sample\nstatistics or estimated model parameters, which you can then examine to see how\nvariable they are.\nThe R package boot combines these steps in one function. For example, the\nfollowing applies the bootstrap to the incomes of people taking out loans:\nlibrary(boot)\nstat_fun <- function(x, idx) median(x[idx])\nboot_obj <- boot(loans_income, R = 1000, statistic=stat_fun)\nThe function stat_fun computes the median for a given sample specified by the\nindex idx. The result is as follows:\nBootstrap Statistics :\n    original   bias    std. error\n",
      "content_length": 1512,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 122,
      "content": "t1*    62000 -70.5595    209.1515\nThe original estimate of the median is $62,000. The bootstrap distribution\nindicates that the estimate has a bias of about –$70 and a standard error of $209.\nThe bootstrap can be used with multivariate data, where the rows are sampled as\nunits (see Figure 2-8). A model might then be run on the bootstrapped data, for\nexample, to estimate the stability (variability) of model parameters, or to improve\npredictive power. With classification and regression trees (also called decision\ntrees), running multiple trees on bootstrap samples and then averaging their\npredictions (or, with classification, taking a majority vote) generally performs\nbetter than using a single tree. This process is called bagging (short for\n“bootstrap aggregating”: see “Bagging and the Random Forest”).\nFigure 2-8. Multivariate bootstrap sampling\nThe repeated resampling of the bootstrap is conceptually simple, and Julian\nSimon, an economist and demographer, published a compendium of resampling\nexamples, including the bootstrap, in his 1969 text Basic Research Methods in\n",
      "content_length": 1085,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 123,
      "content": "Social Science (Random House). However, it is also computationally intensive,\nand was not a feasible option before the widespread availability of computing\npower. The technique gained its name and took off with the publication of several\njournal articles and a book by Stanford statistician Bradley Efron in the late 1970s\nand early 1980s. It was particularly popular among researchers who use statistics\nbut are not statisticians, and for use with metrics or models where mathematical\napproximations are not readily available. The sampling distribution of the mean\nhas been well established since 1908; the sampling distribution of many other\nmetrics has not. The bootstrap can be used for sample size determination;\nexperiment with different values for n to see how the sampling distribution is\naffected.\nThe bootstrap met with considerable skepticism when it was first introduced; it\nhad the aura to many of spinning gold from straw. This skepticism stemmed from a\nmisunderstanding of the bootstrap’s purpose.\nWARNING\nThe bootstrap does not compensate for a small sample size; it does not create new data, nor does\nit fill in holes in an existing data set. It merely informs us about how lots of additional samples\nwould behave when drawn from a population like our original sample.\n",
      "content_length": 1286,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 124,
      "content": "Resampling versus Bootstrapping\nSometimes the term resampling is used synonymously with the term\nbootstrapping, as just outlined. More often, the term resampling also includes\npermutation procedures (see “Permutation Test”), where multiple samples are\ncombined and the sampling may be done without replacement. In any case, the\nterm bootstrap always implies sampling with replacement from an observed data\nset.\nKEY IDEAS\nThe bootstrap (sampling with replacement from a data set) is a powerful tool for assessing the\nvariability of a sample statistic.\nThe bootstrap can be applied in similar fashion in a wide variety of circumstances, without\nextensive study of mathematical approximations to sampling distributions.\nIt also allows us to estimate sampling distributions for statistics where no mathematical\napproximation has been developed.\nWhen applied to predictive models, aggregating multiple bootstrap sample predictions (bagging)\noutperforms the use of a single model.\n",
      "content_length": 975,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 125,
      "content": "Further Reading\nAn Introduction to the Bootstrap by Bradley Efron and Robert Tibshirani\n(Chapman Hall, 1993) was the first book-length treatment of the bootstrap. It\nis still widely read.\nThe retrospective on the bootstrap in the May 2003 issue of Statistical\nScience, (vol. 18, no. 2), discusses (among other antecedents, in Peter Hall’s\n“Prehistory”) Julian Simon’s first publication of the bootstrap in 1969.\nSee An Introduction to Statistical Learning by Gareth James et al.\n(Springer, 2013) for sections on the bootstrap and, in particular, bagging.\n",
      "content_length": 555,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 126,
      "content": "Confidence Intervals\nFrequency tables, histograms, boxplots, and standard errors are all ways to\nunderstand the potential error in a sample estimate. Confidence intervals are\nanother.\nKEY TERMS\nConfidence level\nThe percentage of confidence intervals, constructed in the same way from the same population,\nexpected to contain the statistic of interest.\nInterval endpoints\nThe top and bottom of the confidence interval.\nThere is a natural human aversion to uncertainty; people (especially experts) say,\n“I don’t know” far too rarely. Analysts and managers, while acknowledging\nuncertainty, nonetheless place undue faith in an estimate when it is presented as a\nsingle number (a point estimate). Presenting an estimate not as a single number\nbut as a range is one way to counteract this tendency. Confidence intervals do this\nin a manner grounded in statistical sampling principles.\nConfidence intervals always come with a coverage level, expressed as a (high)\npercentage, say 90% or 95%. One way to think of a 90% confidence interval is as\nfollows: it is the interval that encloses the central 90% of the bootstrap sampling\ndistribution of a sample statistic (see “The Bootstrap”). More generally, an x%\nconfidence interval around a sample estimate should, on average, contain similar\nsample estimates x% of the time (when a similar sampling procedure is\nfollowed).\nGiven a sample of size n, and a sample statistic of interest, the algorithm for a\nbootstrap confidence interval is as follows:\n1. Draw a random sample of size n with replacement from the data (a\nresample).\n2. Record the statistic of interest for the resample.\n3. Repeat steps 1–2 many (R) times.\n",
      "content_length": 1660,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 127,
      "content": "4. For an x% confidence interval, trim [(1 – [x/100]) / 2]% of the R\nresample results from either end of the distribution.\n5. The trim points are the endpoints of an x% bootstrap confidence interval.\nFigure 2-9 shows a a 90% confidence interval for the mean annual income of loan\napplicants, based on a sample of 20 for which the mean was $57,573.\nFigure 2-9. Bootstrap confidence interval for the annual income of loan applicants, based on a sample\nof 20\nThe bootstrap is a general tool that can be used to generate confidence intervals\nfor most statistics, or model parameters. Statistical textbooks and software, with\nroots in over a half-century of computerless statistical analysis, will also\nreference confidence intervals generated by formulas, especially the t-distribution\n(see “Student’s t-Distribution”).\nNOTE\nOf course, what we are really interested in when we have a sample result is “what is the\nprobability that the true value lies within a certain interval?” This is not really the question that a\nconfidence interval answers, but it ends up being how most people interpret the answer.\nThe probability question associated with a confidence interval starts out with the phrase “Given a\nsampling procedure and a population, what is the probability that…” To go in the opposite\ndirection, “Given a sample result, what is the probability that (something is true about the\npopulation),” involves more complex calculations and deeper imponderables.\n",
      "content_length": 1459,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 128,
      "content": "The percentage associated with the confidence interval is termed the level of\nconfidence. The higher the level of confidence, the wider the interval. Also, the\nsmaller the sample, the wider the interval (i.e., the more uncertainty). Both make\nsense: the more confident you want to be, and the less data you have, the wider\nyou must make the confidence interval to be sufficiently assured of capturing the\ntrue value.\nNOTE\nFor a data scientist, a confidence interval is a tool to get an idea of how variable a sample result\nmight be. Data scientists would use this information not to publish a scholarly paper or submit a\nresult to a regulatory agency (as a researcher might), but most likely to communicate the potential\nerror in an estimate, and, perhaps, learn whether a larger sample is needed.\nKEY IDEAS\nConfidence intervals are the typical way to present estimates as an interval range.\nThe more data you have, the less variable a sample estimate will be.\nThe lower the level of confidence you can tolerate, the narrower the confidence interval will be.\nThe bootstrap is an effective way to construct confidence intervals.\n",
      "content_length": 1128,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 129,
      "content": "Further Reading\nFor a bootstrap approach to confidence intervals, see Introductory Statistics\nand Analytics: A Resampling Perspective by Peter Bruce (Wiley, 2014) or\nStatistics by Robin Lock and four other Lock family members (Wiley, 2012).\nEngineers, who have a need to understand the precision of their\nmeasurements, use confidence intervals perhaps more than most disciplines,\nand Modern Engineering Statistics by Tom Ryan (Wiley, 2007) discusses\nconfidence intervals. It also reviews a tool that is just as useful and gets less\nattention: prediction intervals (intervals around a single value, as opposed to\na mean or other summary statistic).\n",
      "content_length": 648,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 130,
      "content": "Normal Distribution\nThe bell-shaped normal distribution is iconic in traditional statistics.1 The fact\nthat distributions of sample statistics are often normally shaped has made it a\npowerful tool in the development of mathematical formulas that approximate those\ndistributions.\nKEY TERMS\nError\nThe difference between a data point and a predicted or average value.\nStandardize\nSubtract the mean and divide by the standard deviation.\nz-score\nThe result of standardizing an individual data point.\nStandard normal\nA normal distribution with mean = 0 and standard deviation = 1.\nQQ-Plot\nA plot to visualize how close a sample distribution is to a normal distribution.\nIn a normal distribution (Figure 2-10), 68% of the data lies within one standard\ndeviation of the mean, and 95% lies within two standard deviations.\nWARNING\nIt is a common misconception that the normal distribution is called that because most data follows\na normal distribution — that is, it is the normal thing. Most of the variables used in a typical data\nscience project — in fact most raw data as a whole — are not normally distributed: see “Long-\nTailed Distributions”. The utility of the normal distribution derives from the fact that many\nstatistics are normally distributed in their sampling distribution. Even so, assumptions of normality\nare generally a last resort, used when empirical probability distributions, or bootstrap distributions,\nare not available.\n",
      "content_length": 1435,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 131,
      "content": "Figure 2-10. Normal curve\nNOTE\nThe normal distribution is also referred to as a Gaussian distribution after Carl Friedrich Gauss, a\nprodigous German mathematician from the late 18th and early 19th century. Another name\npreviously used for the normal distribution was the “error” distribution. Statistically speaking, an\nerror is the difference between an actual value and a statistical estimate like the sample mean.\nFor example, the standard deviation (see “Estimates of Variability”) is based on the errors from\nthe mean of the data. Gauss’s development of the normal distribution came from his study of the\nerrors of astronomical measurements that were found to be normally distributed.\n",
      "content_length": 690,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 132,
      "content": "Standard Normal and QQ-Plots\nA standard normal distribution is one in which the units on the x-axis are\nexpressed in terms of standard deviations away from the mean. To compare data\nto a standard normal distribution, you subtract the mean then divide by the\nstandard deviation; this is also called normalization or standardization (see\n“Standardization (Normalization, Z-Scores)”). Note that “standardization” in this\nsense is unrelated to database record standardization (conversion to a common\nformat). The transformed value is termed a z-score, and the normal distribution is\nsometimes called the z-distribution.\nA QQ-Plot is used to visually determine how close a sample is to the normal\ndistribution. The QQ-Plot orders the z-scores from low to high, and plots each\nvalue’s z-score on the y-axis; the x-axis is the corresponding quantile of a normal\ndistribution for that value’s rank. Since the data is normalized, the units\ncorrespond to the number of standard deviations away of the data from the mean.\nIf the points roughly fall on the diagonal line, then the sample distribution can be\nconsidered close to normal. Figure 2-11 shows a QQ-Plot for a sample of 100\nvalues randomly generated from a normal distribution; as expected, the points\nclosely follow the line. This figure can be produced in R with the qqnorm\nfunction:\nnorm_samp <- rnorm(100)\nqqnorm(norm_samp)\nabline(a=0, b=1, col='grey')\n",
      "content_length": 1405,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 133,
      "content": "Figure 2-11. QQ-Plot of a sample of 100 values drawn from a normal distribution\nWARNING\nConverting data to z-scores (i.e., standardizing or normalizing the data) does not make the data\nnormally distributed. It just puts the data on the same scale as the standard normal distribution,\noften for comparison purposes.\n",
      "content_length": 315,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 134,
      "content": "KEY IDEAS\nThe normal distribution was essential to the historical development of statistics, as it permitted\nmathematical approximation of uncertainty and variability.\nWhile raw data is typically not normally distributed, errors often are, as are averages and totals in\nlarge samples.\nTo convert data to z-scores, you subtract the mean of the data and divide by the standard deviation;\nyou can then compare the data to a normal distribution.\n",
      "content_length": 442,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 135,
      "content": "Long-Tailed Distributions\nDespite the importance of the normal distribution historically in statistics, and in\ncontrast to what the name would suggest, data is generally not normally\ndistributed.\nKEY TERMS FOR LONG-TAIL DISTRIBUTION\nTail\nThe long narrow portion of a frequency distribution, where relatively extreme values occur at low\nfrequency.\nSkew\nWhere one tail of a distribution is longer than the other.\nWhile the normal distribution is often appropriate and useful with respect to the\ndistribution of errors and sample statistics, it typically does not characterize the\ndistribution of raw data. Sometimes, the distribution is highly skewed\n(asymmetric), such as with income data, or the distribution can be discrete, as\nwith binomial data. Both symmetric and asymmetric distributions may have long\ntails. The tails of a distribution correspond to the extreme values (small and\nlarge). Long tails, and guarding against them, are widely recognized in practical\nwork. Nassim Taleb has proposed the black swan theory, which predicts that\nanamolous events, such as a stock market crash, are much more likely to occur\nthan would be predicted by the normal distribution.\nA good example to illustrate the long-tailed nature of data is stock returns.\nFigure 2-12 shows the QQ-Plot for the daily stock returns for Netflix (NFLX).\nThis is generated in R by:\nnflx <- sp500_px[,'NFLX']\nnflx <- diff(log(nflx[nflx>0]))\nqqnorm(nflx)\nabline(a=0, b=1, col='grey')\n",
      "content_length": 1456,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 136,
      "content": "Figure 2-12. QQ-Plot of the returns for NFLX\nIn contrast to Figure 2-11, the points are far below the line for low values and far\nabove the line for high values. This means that we are much more likely to\nobserve extreme values than would be expected if the data had a normal\ndistribution. Figure 2-12 shows another common phenomena: the points are close\nto the line for the data within one standard deviation of the mean. Tukey refers to\nthis phenomenon as data being “normal in the middle,” but having much longer\ntails (see [Tukey-1987]).\n",
      "content_length": 542,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 137,
      "content": "NOTE\nThere is much statistical literature about the task of fitting statistical distributions to observed\ndata. Beware an excessively data-centric approach to this job, which is as much art as science.\nData is variable, and often consistent, on its face, with more than one shape and type of\ndistribution. It is typically the case that domain and statistical knowledge must be brought to bear\nto determine what type of distribution is appropriate to model a given situation. For example, we\nmight have data on the level of internet traffic on a server over many consecutive 5-second\nperiods. It is useful to know that the best distribution to model “events per time period” is the\nPoisson (see “Poisson Distributions”).\nKEY IDEAS FOR LONG-TAIL DISTRIBUTION\nMost data is not normally distributed.\nAssuming a normal distribution can lead to underestimation of extreme events (“black swans”).\n",
      "content_length": 890,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 138,
      "content": "Further Reading\nThe Black Swan, 2nd ed., by Nassim Taleb (Random House, 2010).\nHandbook of Statistical Distributions with Applications, 2nd ed., by K.\nKrishnamoorthy (CRC Press, 2016)\n",
      "content_length": 184,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 139,
      "content": "Student’s t-Distribution\nThe t-distribution is a normally shaped distribution, but a bit thicker and longer\non the tails. It is used extensively in depicting distributions of sample statistics.\nDistributions of sample means are typically shaped like a t-distribution, and there\nis a family of t-distributions that differ depending on how large the sample is. The\nlarger the sample, the more normally shaped the t-distribution becomes.\nKEY TERMS FOR STUDENT’S T-DISTRIBUTION\nn\nSample size.\nDegrees of freedom\nA parameter that allows the t-distribution to adjust to different sample sizes, statistics, and number\nof groups.\nThe t-distribution is often called Student’s t because it was published in 1908 in\nBiometrika by W. S. Gossett under the name “Student.” Gossett’s employer, the\nGuinness brewery, did not want competitors to know that it was using statistical\nmethods, so insisted that Gossett not use his name on the article.\nGossett wanted to answer the question “What is the sampling distribution of the\nmean of a sample, drawn from a larger population?” He started out with a\nresampling experiment — drawing random samples of 4 from a data set of 3,000\nmeasurements of criminals’ height and left-middle-finger lengths. (This being the\nera of eugenics, there was much interest in data on criminals, and in discovering\ncorrelations between criminal tendencies and physical or psychological\nattributes.) He plotted the standardized results (the z-scores) on the x-axis and the\nfrequency on the y-axis. Separately, he had derived a function, now known as\nStudent’s t, and he fit this function over the sample results, plotting the comparison\n(see Figure 2-13).\n",
      "content_length": 1665,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 140,
      "content": "Figure 2-13. Gossett’s resampling experiment results and fitted t-curve (from his 1908 Biometrika paper)\nA number of different statistics can be compared, after standardization, to the t-\ndistribution, to estimate confidence intervals in light of sampling variation.\nConsider a sample of size n for which the sample mean \n has been calculated. If\ns is the sample standard deviation, a 90% confidence interval around the sample\nmean is given by:\nwhere \n is the value of the t-statistic, with (n – 1) degrees of freedom\n(see “Degrees of Freedom”), that “chops off” 5% of the t-distribution at either\nend. The t-distribution has been used as a reference for the distribution of a\nsample mean, the difference between two sample means, regression parameters,\nand other statistics.\nHad computing power been widely available in 1908, statistics would no doubt\nhave relied much more heavily on computationally intensive resampling methods\nfrom the start. Lacking computers, statisticians turned to mathematics and functions\nsuch as the t-distribution to approximate sampling distributions. Computer power\nenabled practical resampling experiments in the 1980s, but by then, use of the t-\ndistribution and similar distributions had become deeply embedded in textbooks\n",
      "content_length": 1258,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 141,
      "content": "and software.\nThe t-distribution’s accuracy in depicting the behavior of a sample statistic\nrequires that the distribution of that statistic for that sample be shaped like a\nnormal distribution. It turns out that sample statistics are often normally\ndistributed, even when the underlying population data is not (a fact which led to\nwidespread application of the t-distribution). This phenomenon is termed the\ncentral limit theorem (see “Central Limit Theorem”).\nNOTE\nWhat do data scientists need to know about the t-distribution and the central limit theorem? Not a\nwhole lot. These distributions are used in classical statistical inference, but are not as central to\nthe purposes of data science. Understanding and quantifying uncertainty and variation are\nimportant to data scientists, but empirical bootstrap sampling can answer most questions about\nsampling error. However, data scientists will routinely encounter t-statistics in output from\nstatistical software and statistical procedures in R, for example in A-B tests and regressions, so\nfamiliarity with its purpose is helpful.\nKEY IDEAS\nThe t-distribution is actually a family of distributions resembling the normal distribution, but with\nthicker tails.\nIt is widely used as a reference basis for the distribution of sample means, differerences between\ntwo sample means, regression parameters, and more.\n",
      "content_length": 1364,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 142,
      "content": "Further Reading\nThe original Gossett paper in Biometrica from 1908 is available as a PDF.\nA standard treatment of the t-distribution can be found in David Lane’s online\nresource.\n",
      "content_length": 179,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 143,
      "content": "Binomial Distribution\nKEY TERMS FOR BINOMIAL DISTRIBUTION\nTrial\nAn event with a discrete outcome (e.g., a coin flip).\nSuccess\nThe outcome of interest for a trial.\nSynonyms\n“1” (as opposed to “0”)\nBinomial\nHaving two outcomes.\nSynonyms\nyes/no, 0/1, binary\nBinomial trial\nA trial with two outcomes.\nSynonym\nBernoulli trial\nBinomial distribution\nDistribution of number of successes in x trials.\nSynonym\nBernoulli distribution\nYes/no (binomial) outcomes lie at the heart of analytics since they are often the\nculmination of a decision or other process; buy/don’t buy, click/don’t click,\nsurvive/die, and so on. Central to understanding the binomial distribution is the\nidea of a set of trials, each trial having two possible outcomes with definite\nprobabilities.\nFor example, flipping a coin 10 times is a binomial experiment with 10 trials,\neach trial having two possible outcomes (heads or tails); see Figure 2-14. Such\nyes/no or 0/1 outcomes are termed binary outcomes, and they need not have 50/50\nprobabilities. Any probabilities that sum to 1.0 are possible. It is conventional in\nstatistics to term the “1” outcome the success outcome; it is also common practice\nto assign “1” to the more rare outcome. Use of the term success does not imply\n",
      "content_length": 1245,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 144,
      "content": "that the outcome is desirable or beneficial, but it does tend to indicate the outcome\nof interest. For example, loan defaults or fraudulent transactions are relatively\nuncommon events that we may be interested in predicting, so they are termed “1s”\nor “successes.”\nFigure 2-14. The tails side of a buffalo nickel\nThe binomial distribution is the frequency distribution of the number of successes\n(x) in a given number of trials (n) with specified probability (p) of success in\neach trial. There is a family of binomial distributions, depending on the values of\nx, n, and p. The binomial distribution would answer a question like:\nIf the probability of a click converting to a sale is 0.02, what is the probability\nof observing 0 sales in 200 clicks?\nThe R function dbinom calculates binomial probabilities. For example:\ndbinom(x=2, n=5, p=0.1)\nwould return 0.0729, the probability of observing exactly x = 2 successes in n = 5\ntrials, where the probability of success for each trial is p = 0.1.\nOften we are interested in determining the probability of x or fewer successes in n\ntrials. In this case, we use the function pbinom:\npbinom(2, 5, 0.1)\nThis would return 0.9914, the probability of observing two or fewer successes in\nfive trials, where the probability of success for each trial is 0.1.\nThe mean of a binomial distribution is \n; you can also think of this as the\nexpected number of successes in n trials, for success probability = p.\nThe variance is \n. With a large enough number of trials\n",
      "content_length": 1500,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 145,
      "content": "(particularly when p is close to 0.50), the binomial distribution is virtually\nindistinguishable from the normal distribution. In fact, calculating binomial\nprobabilities with large sample sizes is computationally demanding, and most\nstatistical procedures use the normal distribution, with mean and variance, as an\napproximation.\nKEY IDEAS\nBinomial outcomes are important to model, since they represent, among other things, fundamental\ndecisions (buy or don’t buy, click or don’t click, survive or die, etc.).\nA binomial trial is an experiment with two possible outcomes: one with probability p and the other\nwith probability 1 – p.\nWith large n, and provided p is not too close to 0 or 1, the binomial distribution can be approximated\nby the normal distribution.\n",
      "content_length": 765,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 146,
      "content": "Further Reading\nRead about the “quincunx”, a pinball-like simulation device for illustrating\nthe binomial distribution.\nThe binomial distribution is a staple of introductory statistics, and all\nintroductory statistics texts will have a chapter or two on it.\n",
      "content_length": 258,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 147,
      "content": "Poisson and Related Distributions\nMany processes produce events randomly at a given overall rate — visitors\narriving at a website, cars arriving at a toll plaza (events spread over time),\nimperfections in a square meter of fabric, or typos per 100 lines of code (events\nspread over space).\nKEY TERMS FOR POISSON AND RELATED DISTRIBUTIONS\nLambda\nThe rate (per unit of time or space) at which events occur.\nPoisson distribution\nThe frequency distribution of the number of events in sampled units of time or space.\nExponential distribution\nThe frequency distribution of the time or distance from one event to the next event.\nWeibull distribution\nA generalized version of the exponential, in which the event rate is allowed to shift over time.\n",
      "content_length": 740,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 148,
      "content": "Poisson Distributions\nFrom prior data we can estimate the average number of events per unit of time or\nspace, but we might also want to know how different this might be from one unit\nof time/space to another. The Poisson distribution tells us the distribution of\nevents per unit of time or space when we sample many such units. It is useful\nwhen addressing queuing questions like “How much capacity do we need to be\n95% sure of fully processing the internet traffic that arrives on a server in any 5-\nsecond period?”\nThe key parameter in a Poisson distribution is , or lambda. This is the mean\nnumber of events that occurs in a specified interval of time or space. The variance\nfor a Poisson distribution is also .\nA common technique is to generate random numbers from a Poisson distribution as\npart of a queuing simulation. The rpois function in R does this, taking only two\narguments — the quantity of random numbers sought, and lambda:\nrpois(100, lambda = 2)\nThis code will generate 100 random numbers from a Poisson distribution with \n= 2. For example, if incoming customer service calls average 2 per minute, this\ncode will simulate 100 minutes, returning the number of calls in each of those 100\nminutes.\n",
      "content_length": 1211,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 149,
      "content": "Exponential Distribution\nUsing the same parameter  that we used in the Poisson distribution, we can also\nmodel the distribution of the time between events: time between visits to a\nwebsite or between cars arriving at a toll plaza. It is also used in engineering to\nmodel time to failure, and in process management to model, for example, the time\nrequired per service call. The R code to generate random numbers from an\nexponential distribution takes two arguments, n (the quantity of numbers to be\ngenerated), and rate, the number of events per time period. For example:\nrexp(n = 100, rate = .2)\nThis code would generate 100 random numbers from an exponential distribution\nwhere the mean number of events per time period is 2. So you could use it to\nsimulate 100 intervals, in minutes, between service calls, where the average rate\nof incoming calls is 0.2 per minute.\nA key assumption in any simulation study for either the Poisson or exponential\ndistribution is that the rate, , remains constant over the period being considered.\nThis is rarely reasonable in a global sense; for example, traffic on roads or data\nnetworks varies by time of day and day of week. However, the time periods, or\nareas of space, can usually be divided into segments that are sufficiently\nhomogeneous so that analysis or simulation within those periods is valid.\n",
      "content_length": 1342,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 150,
      "content": "Estimating the Failure Rate\nIn many applications, the event rate, , is known or can be estimated from prior\ndata. However, for rare events, this is not necessarily so. Aircraft engine failure,\nfor example, is sufficiently rare (thankfully) that, for a given engine type, there\nmay be little data on which to base an estimate of time between failures. With no\ndata at all, there is little basis on which to estimate an event rate. However, you\ncan make some guesses: if no events have been seen after 20 hours, you can be\npretty sure that the rate is not 1 per hour. Via simulation, or direct calculation of\nprobabilities, you can assess different hypothetical event rates and estimate\nthreshold values below which the rate is very unlikely to fall. If there is some data\nbut not enough to provide a precise, reliable estimate of the rate, a goodness-of-fit\ntest (see “Chi-Square Test”) can be applied to various rates to determine how\nwell they fit the observed data.\n",
      "content_length": 968,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 151,
      "content": "Weibull Distribution\nIn many cases, the event rate does not remain constant over time. If the period\nover which it changes is much longer than the typical interval between events,\nthere is no problem; you just subdivide the analysis into the segments where rates\nare relatively constant, as mentioned before. If, however, the event rate changes\nover the time of the interval, the exponential (or Poisson) distributions are no\nlonger useful. This is likely to be the case in mechanical failure — the risk of\nfailure increases as time goes by. The Weibull distribution is an extension of the\nexponential distribution, in which the event rate is allowed to change, as specified\nby a shape parameter, . If  > 1, the probability of an event increases over\ntime, if  < 1, it decreases. Because the Weibull distribution is used with time-to-\nfailure analysis instead of event rate, the second parameter is expressed in terms\nof characteristic life, rather than in terms of the rate of events per interval. The\nsymbol used is , the Greek letter eta. It is also called the scale parameter.\nWith the Weibull, the estimation task now includes estimation of both parameters, \n and . Software is used to model the data and yield an estimate of the best-\nfitting Weibull distribution.\nThe R code to generate random numbers from a Weibull distribution takes three\narguments, n (the quantity of numbers to be generated), shape, and scale. For\nexample, the following code would generate 100 random numbers (lifetimes) from\na Weibull distribution with shape of 1.5 and characteristic life of 5,000:\nrweibull(100,1.5,5000)\nKEY IDEAS\nFor events that occur at a constant rate, the number of events per unit of time or space can be\nmodeled as a Poisson distribution.\nIn this scenario, you can also model the time or distance between one event and the next as an\nexponential distribution.\nA changing event rate over time (e.g., an increasing probability of device failure) can be modeled\nwith the Weibull distribution.\n",
      "content_length": 1996,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 152,
      "content": "Further Reading\nModern Engineering Statistics by Tom Ryan (Wiley, 2007) has a chapter\ndevoted to the probability distributions used in engineering applications.\nRead an engineering-based perspective on the use of the Weibull distribution\n(mainly from an engineering perspective) here and here.\n",
      "content_length": 294,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 153,
      "content": "Summary\nIn the era of big data, the principles of random sampling remain important when\naccurate estimates are needed. Random selection of data can reduce bias and\nyield a higher quality data set than would result from just using the conveniently\navailable data. Knowledge of various sampling and data generating distributions\nallows us to quantify potential errors in an estimate that might be due to random\nvariation. At the same time, the bootstrap (sampling with replacement from an\nobserved data set) is an attractive “one size fits all” method to determine possible\nerror in sample estimates.\nThe bell curve is iconic but perhaps overrated. George W. Cobb, the Mount Holyoke statistician noted for\nhis contribution to the philosophy of teaching introductory statistics, argued in a November 2015 editorial in\nthe American Statistician that the “standard introductory course, which puts the normal distribution at its\ncenter, had outlived the usefulness of its centrality.”\n1\n",
      "content_length": 981,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 154,
      "content": "Chapter 3. Statistical Experiments\nand Significance Testing\nDesign of experiments is a cornerstone of the practice of statistics, with\napplications in virtually all areas of research. The goal is to design an experiment\nin order to confirm or reject a hypothesis. Data scientists are faced with the need\nto conduct continual experiments, particularly regarding user interface and product\nmarketing. This chapter reviews traditional experimental design and discusses\nsome common challenges in data science. It also covers some oft-cited concepts\nin statistical inference and explains their meaning and relevance (or lack of\nrelevance) to data science.\nWhenever you see references to statistical significance, t-tests, or p-values, it is\ntypically in the context of the classical statistical inference “pipeline” (see\nFigure 3-1). This process starts with a hypothesis (“drug A is better than the\nexisting standard drug,” “price A is more profitable than the existing price B”).\nAn experiment (it might be an A/B test) is designed to test the hypothesis —\ndesigned in such a way that, hopefully, will deliver conclusive results. The data is\ncollected and analyzed, and then a conclusion is drawn. The term inference\nreflects the intention to apply the experiment results, which involve a limited set\nof data, to a larger process or population.\nFigure 3-1. The classical statistical inference pipeline\n",
      "content_length": 1399,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 155,
      "content": "A/B Testing\nAn A/B test is an experiment with two groups to establish which of two\ntreatments, products, procedures, or the like is superior. Often one of the two\ntreatments is the standard existing treatment, or no treatment. If a standard (or no)\ntreatment is used, it is called the control. A typical hypothesis is that treatment is\nbetter than control.\nKEY TERMS FOR A/B TESTING\nTreatment\nSomething (drug, price, web headline) to which a subject is exposed.\nTreatment group\nA group of subjects exposed to a specific treatment.\nControl group\nA group of subjects exposed to no (or standard) treatment.\nRandomization\nThe process of randomly assigning subjects to treatments.\nSubjects\nThe items (web visitors, patients, etc.) that are exposed to treatments.\nTest statistic\nThe metric used to measure the effect of the treatment.\nA/B tests are common in web design and marketing, since results are so readily\nmeasured. Some examples of A/B testing include:\nTesting two soil treatments to determine which produces better seed\ngermination\nTesting two therapies to determine which suppresses cancer more effectively\nTesting two prices to determine which yields more net profit\nTesting two web headlines to determine which produces more clicks\n(Figure 3-2)\n",
      "content_length": 1252,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 156,
      "content": "Testing two web ads to determine which generates more conversions\nFigure 3-2. Marketers continually test one web presentation against another\nA proper A/B test has subjects that can be assigned to one treatment or another.\nThe subject might be a person, a plant seed, a web visitor; the key is that the\nsubject is exposed to the treatment. Ideally, subjects are randomized (assigned\nrandomly) to treatments. In this way, you know that any difference between the\ntreatment groups is due to one of two things:\n",
      "content_length": 508,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 157,
      "content": "The effect of the different treatments\nLuck of the draw in which subjects are assigned to which treatments (i.e., the\nrandom assignment may have resulted in the naturally better-performing\nsubjects being concentrated in A or B)\nYou also need to pay attention to the test statistic or metric you use to compare\ngroup A to group B. Perhaps the most common metric in data science is a binary\nvariable: click or no-click, buy or don’t buy, fraud or no fraud, and so on. Those\nresults would be summed up in a 2×2 table. Table 3-1 is a 2×2 table for an actual\nprice test.\nTable 3-1. 2×2 table for\necommerce experiment\nresults\nOutcome\nPrice A Price B\nConversion\n200\n182\nNo conversion 23,539\n22,406\nIf the metric is a continuous variable (purchase amount, profit, etc.), or a count\n(e.g., days in hospital, pages visited) the result might be displayed differently. If\none were interested not in conversion, but in revenue per page view, the results of\nthe price test in Table 3-1 might look like this in typical default software output:\nRevenue/page-view with price A: mean = 3.87, SD = 51.10\nRevenue/page-view with price B: mean = 4.11, SD = 62.98\n“SD” refers to the standard deviation of the values within each group.\nWARNING\nJust because statistical software — including R — generates output by default does not mean\nthat all the output is useful or relevant. You can see that the preceding standard deviations are not\nthat useful; on their face they suggest that numerous values might be negative, when negative\nrevenue is not feasible. This data consists of a small set of relatively high values (page views with\nconversions) and a huge number of 0-values (page views with no conversion). It is difficult to\nsum up the variability of such data with a single number, though the mean absolute deviation from\nthe mean (7.68 for A and 8.15 for B) is more reasonable than the standard deviation.\n",
      "content_length": 1888,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 159,
      "content": "Why Have a Control Group?\nWhy not skip the control group and just run an experiment applying the treatment\nof interest to only one group, and compare the outcome to prior experience?\nWithout a control group, there is no assurance that “other things are equal” and\nthat any difference is really due to the treatment (or to chance). When you have a\ncontrol group, it is subject to the same conditions (except for the treatment of\ninterest) as the treatment group. If you simply make a comparison to “baseline” or\nprior experience, other factors, besides the treatment, might differ.\n",
      "content_length": 581,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 160,
      "content": "BLINDING IN STUDIES\nA blind study is one in which the subjects are unaware of whether they are getting treatment A\nor treatment B. Awareness of receiving a particular treatment can affect response. A double\nblind study is one in which the investigators and facilitators (e.g., doctors and nurses in a medical\nstudy) are unaware which subjects are getting which treatment. Blinding is not possible when the\nnature of the treatment is transparent — for example, cognitive therapy from a computer versus\na psychologist.\nThe use of A/B testing in data science is typically in a web context. Treatments\nmight be the design of a web page, the price of a product, the wording of a\nheadline, or some other item. Some thought is required to preserve the principles\nof randomization. Typically the subject in the experiment is the web visitor, and\nthe outcomes we are interested in measuring are clicks, purchases, visit duration,\nnumber of pages visited, whether a particular page is visited, and the like. In a\nstandard A/B experiment, you need to decide on one metric ahead of time.\nMultiple behavior metrics might be collected and be of interest, but if the\nexperiment is expected to lead to a decision between treatment A and treatment B,\na single metric, or test statistic, needs to be established beforehand. Selecting a\ntest statistic after the experiment is conducted opens the door to researcher bias.\n",
      "content_length": 1402,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 161,
      "content": "Why Just A/B? Why Not C, D…?\nA/B tests are popular in the marketing and ecommerce worlds, but are far from the\nonly type of statistical experiment. Additional treatments can be included.\nSubjects might have repeated measurements taken. Pharmaceutical trials where\nsubjects are scarce, expensive, and acquired over time are sometimes designed\nwith multiple opportunities to stop the experiment and reach a conclusion.\nTraditional statistical experimental designs focus on answering a static question\nabout the efficacy of specified treatments. Data scientists are less interested in the\nquestion:\nIs the difference between price A and price B statistically significant?\nthan in the question:\nWhich, out of multiple possible prices, is best?\nFor this, a relatively new type of experimental design is used: the multi-arm\nbandit (see “Multi-Arm Bandit Algorithm”).\n",
      "content_length": 861,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 162,
      "content": "GETTING PERMISSION\nIn scientific and medical research involving human subjects, it is typically necessary to get their\npermission, as well as obtain the approval of an institutional review board. Experiments in\nbusiness that are done as a part of ongoing operations almost never do this. In most cases (e.g.,\npricing experiments, or experiments about which headline to show or which offer should be\nmade), this practice is widely accepted. Facebook, however, ran afoul of this general acceptance\nin 2014 when it experimented with the emotional tone in users’ newsfeeds. Facebook used\nsentiment analysis to classify newsfeed posts as positive or negative, then altered the\npositive/negative balance in what it showed users. Some randomly selected users experienced\nmore positive posts, while others experienced more negative posts. Facebook found that the users\nwho experienced a more positive newsfeed were more likely to post positively themselves, and\nvice versa. The magnitude of the effect was small, however, and Facebook faced much criticism\nfor conducting the experiment without users’ knowledge. Some users speculated that Facebook\nmight have pushed some extremely depressed users over the edge, if they got the negative\nversion of their feed.\nKEY IDEAS\nSubjects are assigned to two (or more) groups that are treated exactly alike, except that the\ntreatment under study differs from one to another.\nIdeally, subjects are assigned randomly to the groups.\n",
      "content_length": 1462,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 163,
      "content": "For Further Reading\nTwo-group comparisons (A/B tests) are a staple of traditional statistics, and\njust about any introductory statistics text will have extensive coverage of\ndesign principles and inference procedures. For a discussion that places A/B\ntests in more of a data science context and uses resampling, see Introductory\nStatistics and Analytics: A Resampling Perspective by Peter Bruce (Wiley,\n2014).\nFor web testing, the logistical aspects of testing can be just as challenging as\nthe statistical ones. A good place to start is the Google Analytics help section\non Experiments.\nBeware advice found in the ubiquitous guides to A/B testing that you see on\nthe web, such as these words in one such guide: “Wait for about 1,000 total\nvisitors and make sure you run the test for a week.” Such general rules of\nthumb are not statistically meaningful; see “Power and Sample Size” for\nmore detail.\n",
      "content_length": 900,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 164,
      "content": "Hypothesis Tests\nHypothesis tests, also called significance tests, are ubiquitous in the traditional\nstatistical analysis of published research. Their purpose is to help you learn\nwhether random chance might be responsible for an observed effect.\nKEY TERMS\nNull hypothesis\nThe hypothesis that chance is to blame.\nAlternative hypothesis\nCounterpoint to the null (what you hope to prove).\nOne-way test\nHypothesis test that counts chance results only in one direction.\nTwo-way test\nHypothesis test that counts chance results in two directions.\nAn A/B test (see “A/B Testing”) is typically constructed with a hypothesis in\nmind. For example, the hypothesis might be that price B produces higher profit.\nWhy do we need a hypothesis? Why not just look at the outcome of the experiment\nand go with whichever treatment does better?\nThe answer lies in the tendency of the human mind to underestimate the scope of\nnatural random behavior. One manifestation of this is the failure to anticipate\nextreme events, or so-called “black swans” (see “Long-Tailed Distributions”).\nAnother manifestation is the tendency to misinterpret random events as having\npatterns of some significance. Statistical hypothesis testing was invented as a way\nto protect researchers from being fooled by random chance.\nMISINTERPRETING RANDOMNESS\nYou can observe the human tendency to underestimate randomness in this experiment. Ask several\nfriends to invent a series of 50 coin flips: have them write down a series of random Hs and Ts. Then ask\nthem to actually flip a coin 50 times and write down the results. Have them put the real coin flip results in\none pile, and the made-up results in another. It is easy to tell which results are real: the real ones will have\nlonger runs of Hs or Ts. In a set of 50 real coin flips, it is not at all unusual to see five or six Hs or Ts in a\nrow. However, when most of us are inventing random coin flips and we have gotten three or four Hs in a\nrow, we tell ourselves that, for the series to look random, we had better switch to T.\n",
      "content_length": 2038,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 165,
      "content": "The other side of this coin, so to speak, is that when we do see the real-world equivalent of six Hs in a\nrow (e.g., when one headline outperforms another by 10%), we are inclined to attribute it to something\nreal, not just chance.\nIn a properly designed A/B test, you collect data on treatments A and B in such a\nway that any observed difference between A and B must be due to either:\nRandom chance in assignment of subjects\nA true difference between A and B\nA statistical hypothesis test is further analysis of an A/B test, or any randomized\nexperiment, to assess whether random chance is a reasonable explanation for the\nobserved difference between groups A and B.\n",
      "content_length": 668,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 166,
      "content": "The Null Hypothesis\nHypothesis tests use the following logic: “Given the human tendency to react to\nunusual but random behavior and interpret it as something meaningful and real, in\nour experiments we will require proof that the difference between groups is more\nextreme than what chance might reasonably produce.” This involves a baseline\nassumption that the treatments are equivalent, and any difference between the\ngroups is due to chance. This baseline assumption is termed the null hypothesis.\nOur hope is then that we can, in fact, prove the null hypothesis wrong, and show\nthat the outcomes for groups A and B are more different than what chance might\nproduce.\nOne way to do this is via a resampling permutation procedure, in which we\nshuffle together the results from groups A and B and then repeatedly deal out the\ndata in groups of similar sizes, then observe how often we get a difference as\nextreme as the observed difference. See “Resampling” for more detail.\n",
      "content_length": 973,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 167,
      "content": "Alternative Hypothesis\nHypothesis tests by their nature involve not just a null hypothesis, but also an\noffsetting alternative hypothesis. Here are some examples:\nNull = “no difference between the means of group A and group B,”\nalternative = “A is different from B” (could be bigger or smaller)\nNull = “A \n B,” alternative = “B > A”\nNull = “B is not X% greater than A,” alternative = “B is X% greater than A”\nTaken together, the null and alternative hypotheses must account for all\npossibilities. The nature of the null hypothesis determines the structure of the\nhypothesis test.\n",
      "content_length": 580,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 168,
      "content": "One-Way, Two-Way Hypothesis Test\nOften, in an A/B test, you are testing a new option (say B), against an established\ndefault option (A) and the presumption is that you will stick with the default\noption unless the new option proves itself definitively better. In such a case, you\nwant a hypothesis test to protect you from being fooled by chance in the direction\nfavoring B. You don’t care about being fooled by chance in the other direction,\nbecause you would be sticking with A unless B proves definitively better. So you\nwant a directional alternative hypothesis (B is better than A). In such a case, you\nuse a one-way (or one-tail) hypothesis test. This means that extreme chance results\nin only one direction direction count toward the p-value.\nIf you want a hypothesis test to protect you from being fooled by chance in either\ndirection, the alternative hypothesis is bidirectional (A is different from B; could\nbe bigger or smaller). In such a case, you use a two-way (or two-tail) hypothesis.\nThis means that extreme chance results in either direction count toward the p-\nvalue.\nA one-tail hypothesis test often fits the nature of A/B decision making, in which a\ndecision is required and one option is typically assigned “default” status unless\nthe other proves better. Software, however, including R, typically provides a two-\ntail test in its default output, and many statisticians opt for the more conservative\ntwo-tail test just to avoid argument. One-tail versus two-tail is a confusing\nsubject, and not that relevant for data science, where the precision of p-value\ncalculations is not terribly important.\nKEY IDEAS\nA null hypothesis is a logical construct embodying the notion that nothing special has happened,\nand any effect you observe is due to random chance.\nThe hypothesis test assumes that the null hypothesis is true, creates a “null model” (a probability\nmodel), and tests whether the effect you observe is a reasonable outcome of that model.\n",
      "content_length": 1967,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 169,
      "content": "Further Reading\nThe Drunkard’s Walk by Leonard Mlodinow (Vintage Books, 2008) is a\nreadable survey of the ways in which “randomness rules our lives.”\nDavid Freedman, Robert Pisani, and Roger Purves’s classic statistics text\nStatistics, 4th ed. (W. W. Norton, 2007) has excellent nonmathematical\ntreatments of most statistics topics, including hypothesis testing.\nIntroductory Statistics and Analytics: A Resampling Perspective by Peter\nBruce (Wiley, 2014) develops hypothesis testing concepts using resampling.\n",
      "content_length": 511,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 170,
      "content": "Resampling\nResampling in statistics means to repeatedly sample values from observed data,\nwith a general goal of assessing random variability in a statistic. It can also be\nused to assess and improve the accuracy of some machine-learning models (e.g.,\nthe predictions from decision tree models built on multiple bootstrapped data sets\ncan be averaged in a process known as bagging: see “Bagging and the Random\nForest”).\nThere are two main types of resampling procedures: the bootstrap and\npermutation tests. The bootstrap is used to assess the reliability of an estimate; it\nwas discussed in the previous chapter (see “The Bootstrap”). Permutation tests\nare used to test hypotheses, typically involving two or more groups, and we\ndiscuss those in this section.\nKEY TERMS\nPermutation test\nThe procedure of combining two or more samples together, and randomly (or exhaustively)\nreallocating the observations to resamples.\nSynonyms\nRandomization test, random permutation test, exact test.\nWith or without replacement\nIn sampling, whether or not an item is returned to the sample before the next draw.\n",
      "content_length": 1098,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 171,
      "content": "Permutation Test\nIn a permutation procedure, two or more samples are involved, typically the\ngroups in an A/B or other hypothesis test. Permute means to change the order of a\nset of values. The first step in a permutation test of a hypothesis is to combine the\nresults from groups A and B (and, if used, C, D, …) together. This is the logical\nembodiment of the null hypothesis that the treatments to which the groups were\nexposed do not differ. We then test that hypothesis by randomly drawing groups\nfrom this combined set, and seeing how much they differ from one another. The\npermutation procedure is as follows:\n1. Combine the results from the different groups in a single data set.\n2. Shuffle the combined data, then randomly draw (without replacing) a\nresample of the same size as group A.\n3. From the remaining data, randomly draw (without replacing) a resample\nof the same size as group B.\n4. Do the same for groups C, D, and so on.\n5. Whatever statistic or estimate was calculated for the original samples\n(e.g., difference in group proportions), calculate it now for the\nresamples, and record; this constitutes one permutation iteration.\n6. Repeat the previous steps R times to yield a permutation distribution of\nthe test statistic.\nNow go back to the observed difference between groups and compare it to the set\nof permuted differences. If the observed difference lies well within the set of\npermuted differences, then we have not proven anything — the observed\ndifference is within the range of what chance might produce. However, if the\nobserved difference lies outside most of the permutation distribution, then we\nconclude that chance is not responsible. In technical terms, the difference is\nstatistically significant. (See “Statistical Significance and P-Values”.)\n",
      "content_length": 1783,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 172,
      "content": "Example: Web Stickiness\nA company selling a relatively high-value service wants to test which of two web\npresentations does a better selling job. Due to the high value of the service being\nsold, sales are infrequent and the sales cycle is lengthy; it would take too long to\naccumulate enough sales to know which presentation is superior. So the company\ndecides to measure the results with a proxy variable, using the detailed interior\npage that describes the service.\nTIP\nA proxy variable is one that stands in for the true variable of interest, which may be unavailable,\ntoo costly, or too time-consuming to measure. In climate research, for example, the oxygen\ncontent of ancient ice cores is used as a proxy for temperature. It is useful to have at least some\ndata on the true variable of interest, so the strength of its association with the proxy can be\nassessed.\nOne potential proxy variable for our company is the number of clicks on the\ndetailed landing page. A better one is how long people spend on the page. It is\nreasonable to think that a web presentation (page) that holds people’s attention\nlonger will lead to more sales. Hence, our metric is average session time,\ncomparing page A to page B.\nDue to the fact that this is an interior, special-purpose page, it does not receive a\nhuge number of visitors. Also note that Google Analytics, which is how we\nmeasure session time, cannot measure session time for the last session a person\nvisits. Instead of deleting that session from the data, though, GA records it as a\nzero, so the data requires additional processing to remove those sessions. The\nresult is a total of 36 sessions for the two different presentations, 21 for page A\nand 15 for page B. Using ggplot, we can visually compare the session times\nusing side-by-side boxplots:\nggplot(session_times, aes(x=Page, y=Time)) +\n  geom_boxplot()\nThe boxplot, shown in Figure 3-3, indicates that page B leads to longer sessions\nthan page A. The means for each group can be computed as follows:\n",
      "content_length": 2008,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 173,
      "content": "mean_a <- mean(session_times[session_times['Page']=='Page A', 'Time'])\nmean_b <- mean(session_times[session_times['Page']=='Page B', 'Time'])\nmean_b - mean_a\n[1] 21.4\nPage B has session times greater, on average, by 21.4 seconds versus page A. The\nquestion is whether this difference is within the range of what random chance\nmight produce, or, alternatively, is statistically significant. One way to answer this\nis to apply a permutation test — combine all the session times together, then\nrepeatedly shuffle and divide them into groups of 21 (recall that n = 21 for page\nA) and 15 (n = 15 for B).\nTo apply a permutation test, we need a function to randomly assign the 36 session\ntimes to a group of 21 (page A) and a group of 15 (page B):\nperm_fun <- function(x, n1, n2)\n{\n  n <- n1 + n2\n  idx_b <- sample(1:n, n1)\n  idx_a <- setdiff(1:n, idx_b)\n  mean_diff <- mean(x[idx_b]) - mean(x[idx_a])\n  return(mean_diff)\n}\n",
      "content_length": 917,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 174,
      "content": "Figure 3-3. Session times for web pages A and B\nThis function works by sampling without replacement n2 indices and assigning\nthem to the B group; the remaining n1 indices are assigned to group A. The\ndifference between the two means is returned. Calling this function R = 1,000\ntimes and specifying n2 = 15 and n1 = 21 leads to a distribution of differences in\nthe session times that can be plotted as a histogram.\nperm_diffs <- rep(0, 1000)\nfor(i in 1:1000)\n  perm_diffs[i] = perm_fun(session_times[,'Time'], 21, 15)\n",
      "content_length": 518,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 175,
      "content": "hist(perm_diffs, xlab='Session time differences (in seconds)')\nabline(v = mean_b - mean_a)\nThe histogram, shown in Figure 3-4 shows that mean difference of random\npermutations often exceeds the observed difference in session times (the vertical\nline). This suggests that the oberved difference in session time between page A\nand page B is well within the range of chance variation, thus is not statistically\nsignificant.\n",
      "content_length": 421,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 176,
      "content": "Figure 3-4. Frequency distribution for session time differences between pages A and B\n",
      "content_length": 86,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 177,
      "content": "Exhaustive and Bootstrap Permutation Test\nIn addition to the preceding random shuffling procedure, also called a random\npermutation test or a randomization test, there are two variants of the\npermutation test:\nAn exhaustive permutation test\nA bootstrap permutation test\nIn an exhaustive permutation test, instead of just randomly shuffling and dividing\nthe data, we actually figure out all the possible ways it could be divided. This is\npractical only for relatively small sample sizes. With a large number of repeated\nshufflings, the random permutation test results approximate those of the exhaustive\npermutation test, and approach them in the limit. Exhaustive permutation tests are\nalso sometimes called exact tests, due to their statistical property of guaranteeing\nthat the null model will not test as “significant” more than the alpha level of the\ntest (see “Statistical Significance and P-Values”).\nIn a bootstrap permutation test, the draws outlined in steps 2 and 3 of the random\npermutation test are made with replacement instead of without replacement. In\nthis way the resampling procedure models not just the random element in the\nassignment of treatment to subject, but also the random element in the selection of\nsubjects from a population. Both procedures are encountered in statistics, and the\ndistinction between them is somewhat convoluted and not of consequence in the\npractice of data science.\n",
      "content_length": 1415,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 178,
      "content": "Permutation Tests: The Bottom Line for Data Science\nPermutation tests are useful heuristic procedures for exploring the role of random\nvariation. They are relatively easy to code, interpret and explain, and they offer a\nuseful detour around the formalism and “false determinism” of formula-based\nstatistics.\nOne virtue of resampling, in contrast to formula approaches, is that it comes much\ncloser to a “one size fits all” approach to inference. Data can be numeric or\nbinary. Sample sizes can be the same or different. Assumptions about normally-\ndistributed data are not needed.\nKEY IDEAS\nIn a permutation test, multiple samples are combined, then shuffled.\nThe shuffled values are then divided into resamples, and the statistic of interest is calculated.\nThis process is then repeated, and the resampled statistic is tabulated.\nComparing the observed value of the statistic to the resampled distribution allows you to judge\nwhether an observed difference between samples might occur by chance.\n",
      "content_length": 997,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 179,
      "content": "For Further Reading\nRandomization Tests, 4th ed., by Eugene Edgington and Patrick Onghena\n(Chapman Hall, 2007), but don’t get too drawn into the thicket of nonrandom\nsampling.\nIntroductory Statistics and Analytics: A Resampling Perspective by Peter\nBruce (Wiley, 2015).\n",
      "content_length": 270,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 180,
      "content": "Statistical Significance and P-Values\nStatistical significance is how statisticians measure whether an experiment (or\neven a study of existing data) yields a result more extreme than what chance might\nproduce. If the result is beyond the realm of chance variation, it is said to be\nstatistically significant.\nKEY TERMS\nP-value\nGiven a chance model that embodies the null hypothesis, the p-value is the probability of obtaining\nresults as unusual or extreme as the observed results.\nAlpha\nThe probability threshold of “unusualness” that chance results must surpass, for actual outcomes\nto be deemed statistically significant.\nType 1 error\nMistakenly concluding an effect is real (when it is due to chance).\nType 2 error\nMistakenly concluding an effect is due to chance (when it is real).\nConsider in Table 3-2 the results of the web test shown earlier.\nTable 3-2. 2×2 table for\necommerce experiment\nresults\nOutcome\nPrice A Price B\nConversion\n200\n182\nNo conversion 23539\n22406\nPrice A converts almost 5% better than price B (0.8425% versus 0.8057% — a\ndifference of 0.0368 percentage points), big enough to be meaningful in a high-\nvolume business. We have over 45,000 data points here, and it is tempting to\nconsider this as “big data,” not requiring tests of statistical significance (needed\nmainly to account for sampling variability in small samples). However, the\n",
      "content_length": 1367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 181,
      "content": "conversion rates are so low (less than 1%) that the actual meaningful values —\nthe conversions — are only in the 100s, and the sample size needed is really\ndetermined by these conversions. We can test whether the difference in\nconversions between prices A and B is within the range of chance variation, using\na resampling procedure. By “chance variation,” we mean the random variation\nproduced by a probability model that embodies the null hypothesis that there is no\ndifference between the rates (see “The Null Hypothesis”). The following\npermutation procedure asks “if the two prices share the same conversion rate,\ncould chance variation produce a difference as big as 5%?”\n1. Create an urn with all sample results: this represents the supposed shared\nconversion rate of 382 ones and 45,945 zeros = 0.008246 = 0.8246%.\n2. Shuffle and draw out a resample of size 23,739 (same n as price A), and\nrecord how many 1s.\n3. Record the number of 1s in the remaining 22,588 (same n as price B).\n4. Record the difference in proportion 1s.\n5. Repeat steps 2–4.\n6. How often was the difference >= 0.0368?\nReusing the function perm_fun defined in “Example: Web Stickiness”, we can\ncreate a histogram of randomly permuted differences in conversion rate:\nobs_pct_diff <- 100*(200/23739 - 182/22588)\nconversion <- c(rep(0, 45945), rep(1, 382))\nperm_diffs <- rep(0, 1000)\nfor(i in 1:1000)\n  perm_diffs[i] = 100*perm_fun(conversion, 23739, 22588 )\nhist(perm_diffs, xlab='Session time differences (in seconds)')\nabline(v = obs_pct_diff)\nSee the histogram of 1,000 resampled results in Figure 3-5: as it happens, in this\ncase the observed difference of 0.0368% is well within the range of chance\nvariation.\n",
      "content_length": 1690,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 182,
      "content": "Figure 3-5. Frequency distribution for the difference in conversion rates between pages A and B\n",
      "content_length": 96,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 183,
      "content": "P-Value\nSimply looking at the graph is not a very precise way to measure statistical\nsignificance, so of more interest is the p-value. This is the frequency with which\nthe chance model produces a result more extreme than the observed result. We can\nestimate a p-value from our permutation test by taking the proportion of times that\nthe permutation test produces a difference equal to or greater than the observed\ndifference:\nmean(perm_diffs > obs_pct_diff)\n[1] 0.308\nThe p-value is 0.308, which means that we would expect to achieve the same\nresult by random chance over 30% of the time.\nIn this case, we didn’t need to use a permutation test to get a p-value. Since we\nhave a binomial distribution, we can approximate the p-value using the normal\ndistribution. In R code, we do this using the function prop.test:\n> prop.test(x=c(200,182), n=c(23739,22588), alternative=\"greater\")\n \n2-sample test for equality of proportions with continuity correction\ndata:  c(200, 182) out of c(23739, 22588)\nX-squared = 0.14893, df = 1, p-value = 0.3498\nalternative hypothesis: greater\n95 percent confidence interval:\n -0.001057439  1.000000000\nsample estimates:\n     prop 1      prop 2\n0.008424955 0.008057376\nThe argument x is the number of successes for each group and the argument n is\nthe number of trials. The normal approximation yields a p-value of 0.3498, which\nis close to the p-value obtained from the permutation test.\n",
      "content_length": 1418,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 184,
      "content": "Alpha\nStatisticians frown on the practice of leaving it to the researcher’s discretion to\ndetermine whether a result is “too unusual” to happen by chance. Rather, a\nthreshold is specified in advance, as in “more extreme than 5% of the chance (null\nhypothesis) results”; this threshold is known as alpha. Typical alpha levels are\n5% and 1%. Any chosen level is an arbitrary decision — there is nothing about\nthe process that will guarantee correct decisions x% of the time. This is because\nthe probability question being answered is not “what is the probability that this\nhappened by chance?” but rather “given a chance model, what is the probability of\na result this extreme?” We then deduce backward about the appropriateness of the\nchance model, but that judgment does not carry a probability. This point has been\nthe subject of much confusion.\nValue of the p-value\nConsiderable controversy has surrounded the use of the p-value in recent years.\nOne psychology journal has gone so far as to “ban” the use of p-values in\nsubmitted papers on the grounds that publication decisions based solely on the p-\nvalue were resulting in the publication of poor research. Too many researchers,\nonly dimly aware of what a p-value really means, root around in the data and\namong different possible hypotheses to test, until they find a combination that\nyields a significant p-value and, hence, a paper suitable for publication.\nThe real problem is that people want more meaning from the p-value than it\ncontains. Here’s what we would like the p-value to convey:\nThe probability that the result is due to chance.\nWe hope for a low value, so we can conclude that we’ve proved something. This\nis how many journal editors were interpreting the p-value. But here’s what the p-\nvalue actually represents:\nThe probability that, given a chance model, results as extreme as the observed\nresults could occur.\nThe difference is subtle, but real. A significant p-value does not carry you quite as\nfar along the road to “proof” as it seems to promise. The logical foundation for the\nconclusion “statistically significant” is somewhat weaker when the real meaning\n",
      "content_length": 2138,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 185,
      "content": "of the p-value is understood.\nIn March 2016, the American Statistical Association, after much internal\ndeliberation, revealed the extent of misunderstanding about p-values when it\nissued a cautionary statement regarding their use.\nThe ASA statement stressed six principles for researchers and journal editors:\n1. P-values can indicate how incompatible the data are with a specified\nstatistical model.\n2. P-values do not measure the probability that the studied hypothesis is\ntrue, or the probability that the data were produced by random chance\nalone.\n3. Scientific conclusions and business or policy decisions should not be\nbased only on whether a p-value passes a specific threshold.\n4. Proper inference requires full reporting and transparency.\n5. A p-value, or statistical significance, does not measure the size of an\neffect or the importance of a result.\n6. By itself, a p-value does not provide a good measure of evidence\nregarding a model or hypothesis.\n",
      "content_length": 962,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 186,
      "content": "Type 1 and Type 2 Errors\nIn assessing statistical significance, two types of error are possible:\nType 1 error, in which you mistakenly conclude an effect is real, when it is\nreally just due to chance\nType 2 error, in which you mistakenly conclude that an effect is not real (i.e.,\ndue to chance), when it really is real\nActually, a Type 2 error is not so much an error as a judgment that the sample size\nis too small to detect the effect. When a p-value falls short of statistical\nsignificance (e.g., it exceeds 5%), what we are really saying is “effect not\nproven.” It could be that a larger sample would yield a smaller p-value.\nThe basic function of significance tests (also called hypothesis tests) is to protect\nagainst being fooled by random chance; thus they are typically structured to\nminimize Type 1 errors.\n",
      "content_length": 818,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 187,
      "content": "Data Science and P-Values\nThe work that data scientists do is typically not destined for publication in\nscientific journals, so the debate over the value of a p-value is somewhat\nacademic. For a data scientist, a p-value is a useful metric in situations where you\nwant to know whether a model result that appears interesting and useful is within\nthe range of normal chance variability. As a decision tool in an experiment, a p-\nvalue should not be considered controlling, but merely another point of\ninformation bearing on a decision. For example, p-values are sometimes used as\nintermediate inputs in some statistical or machine learning models — a feature\nnight be included in or excluded from a model depending on its p-value.\nKEY IDEAS\nSignificance tests are used to determine whether an observed effect is within the range of chance\nvariation for a null hypothesis model.\nThe p-value is the probability that results as extreme as the observed results might occur, given a\nnull hypothesis model.\nThe alpha value is the threshold of “unusualness” in a null hypothesis chance model.\nSignificance testing has been much more relevant for formal reporting of research than for data\nscience (but has been fading recently, even for the former).\n",
      "content_length": 1242,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 188,
      "content": "Further Reading\nStephen Stigler, “Fisher and the 5% Level,” Chance vol. 21, no. 4 (2008):\n12. This article is a short commentary on Ronald Fisher’s 1925 book\nStatistical Methods for Research Workers, and his emphasis on the 5% level\nof significance.\nSee also “Hypothesis Tests” and the further reading mentioned there.\n",
      "content_length": 319,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 189,
      "content": "t-Tests\nThere are numerous types of significance tests, depending on whether the data\ncomprises count data or measured data, how many samples there are, and what’s\nbeing measured. A very common one is the t-test, named after Student’s t-\ndistribution, originally developed by W. S. Gossett to approximate the distribution\nof a single sample mean (see “Student’s t-Distribution”).\nKEY TERMS\nTest statistic\nA metric for the difference or effect of interest.\nt-statistic\nA standardized version of the test statistic.\nt-distribution\nA reference distribution (in this case derived from the null hypothesis), to which the observed t-\nstatistic can be compared.\nAll significance tests require that you specify a test statistic to measure the effect\nyou are interested in, and help you determine whether that observed effect lies\nwithin the range of normal chance variation. In a resampling test (see the\ndiscussion of permutation in “Permutation Test”), the scale of the data does not\nmatter. You create the reference (null hypothesis) distribution from the data itself,\nand use the test statistic as is.\nIn the 1920s and 30s, when statistical hypothesis testing was being developed, it\nwas not feasible to randomly shuffle data thousands of times to do a resampling\ntest. Statisticians found that a good approximation to the permutation (shuffled)\ndistribution was the t-test, based on Gossett’s t-distribution. It is used for the very\ncommon two-sample comparison — A/B test — in which the data is numeric. But\nin order for the t-distribution to be used without regard to scale, a standardized\nform of the test statistic must be used.\nA classic statistics text would at this stage show various formulas that incorporate\nGossett’s distribution and demonstrate how to standardize your data to compare it\nto the standard t-distribution. These formulas are not shown here because all\n",
      "content_length": 1875,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 190,
      "content": "statistical software, as well as R and Python, include commands that embody the\nformula. In R, the function is t.test:\n> t.test(Time ~ Page, data=session_times, alternative='less' )\n \nWelch Two Sample t-test\ndata:  Time by Page\nt = -1.0983, df = 27.693, p-value = 0.1408\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n     -Inf 19.59674\nsample estimates:\nmean in group Page A mean in group Page B\n            126.3333             162.0000\nThe alternative hypothesis is that the session time mean for page A is less than for\npage B. This is fairly close to the permutation test p-value of 0.124 (see\n“Example: Web Stickiness”).\nIn a resampling mode, we structure the solution to reflect the observed data and\nthe hypothesis to be tested, not worrying about whether the data is numeric or\nbinary, sample sizes are balanced or not, sample variances, or a variety of other\nfactors. In the formula world, many variations present themselves, and they can be\nbewildering. Statisticians need to navigate that world and learn its map, but data\nscientists do not — they are typically not in the business of sweating the details of\nhypothesis tests and confidence intervals the way a researcher preparing a paper\nfor presentation might.\nKEY IDEAS\nBefore the advent of computers, resampling tests were not practical and statisticians used standard\nreference distributions.\nA test statistic could then be standardized and compared to the reference distribution.\nOne such widely used standardized statistic is the t-statistic.\n",
      "content_length": 1562,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 191,
      "content": "Further Reading\nAny introductory statistics text will have illustrations of the t-statistic and its\nuses; two good ones are Statistics, 4th ed., by David Freedman, Robert\nPisani, and Roger Purves (W. W. Norton, 2007) and The Basic Practice of\nStatistics by David S. Moore (Palgrave Macmillan, 2010).\nFor a treatment of both the t-test and resampling procedures in parallel, see\nIntroductory Statistics and Analytics: A Resampling Perspective by Peter\nBruce (Wiley, 2014) or Statistics by Robin Lock and four other Lock family\nmembers (Wiley, 2012).\n",
      "content_length": 549,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 192,
      "content": "Multiple Testing\nAs we’ve mentioned previously, there is a saying in statistics: “torture the data\nlong enough, and it will confess.” This means that if you look at the data through\nenough different perspectives, and ask enough questions, you can almost\ninvariably find a statistically significant effect.\nKEY TERMS\nType 1 error\nMistakenly concluding that an effect is statistically significant.\nFalse discovery rate\nAcross multiple tests, the rate of making a Type 1 error.\nAdjustment of p-values\nAccounting for doing multiple tests on the same data.\nOverfitting\nFitting the noise.\nFor example, if you have 20 predictor variables and one outcome variable, all\nrandomly generated, the odds are pretty good that at least one predictor will\n(falsely) turn out to be statistically significant if you do a series of 20 significance\ntests at the alpha = 0.05 level. As previously discussed, this is called a Type 1\nerror. You can calculate this probability by first finding the probability that all\nwill correctly test nonsignificant at the 0.05 level. The probability that one will\ncorrectly test nonsignificant is 0.95, so the probability that all 20 will correctly\ntest nonsignificant is 0.95 × 0.95 × 0.95 … or 0.9520 = 0.36.1 The probability that\nat least one predictor will (falsely) test significant is the flip side of this\nprobability, or 1 – (probability that all will be nonsignificant) = 0.64.\nThis issue is related to the problem of overfitting in data mining, or “fitting the\nmodel to the noise.” The more variables you add, or the more models you run, the\ngreater the probability that something will emerge as “significant” just by chance.\nIn supervised learning tasks, a holdout set where models are assessed on data that\nthe model has not seen before mitigates this risk. In statistical and machine\nlearning tasks not involving a labeled holdout set, the risk of reaching conclusions\n",
      "content_length": 1896,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 193,
      "content": "based on statistical noise persists.\nIn statistics, there are some procedures intended to deal with this problem in very\nspecific circumstances. For example, if you are comparing results across multiple\ntreatment groups you might ask multiple questions. So, for treatments A–C, you\nmight ask:\nIs A different from B?\nIs B different from C?\nIs A different from C?\nOr, in a clinical trial, you might want to look at results from a therapy at multiple\nstages. In each case, you are asking multiple questions, and with each question,\nyou are increasing the chance of being fooled by chance. Adjustment procedures\nin statistics can compensate for this by setting the bar for statistical significance\nmore stringently than it would be set for a single hypothesis test. These adjustment\nprocedures typically involve “dividing up the alpha” according to the number of\ntests. This results in a smaller alpha (i.e., a more stringent bar for statistical\nsignificance) for each test. One such procedure, the Bonferroni adjustment, simply\ndivides the alpha by the number of observations n.\nHowever, the problem of multiple comparisons goes beyond these highly\nstructured cases and is related to the phenomenon of repeated data “dredging” that\ngives rise to the saying about torturing the data. Put another way, given sufficiently\ncomplex data, if you haven’t found something interesting, you simply haven’t\nlooked long and hard enough. More data is available now than ever before, and\nthe number of journal articles published nearly doubled between 2002 and 2010.\nThis gives rise to lots of opportunities to find something interesting in the data,\nincluding multiplicity issues such as:\nChecking for multiple pairwise differences across groups\nLooking at multiple subgroup results (“we found no significant treatment\neffect overall, but we did find an effect for unmarried women younger than\n30”)\nTrying lots of statistical models\n",
      "content_length": 1917,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 194,
      "content": "Including lots of variables in models\nAsking a number of different questions (i.e., different possible outcomes)\n",
      "content_length": 113,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 195,
      "content": "FALSE DISCOVERY RATE\nThe term false discovery rate was originally used to describe the rate at which a given set of\nhypothesis tests would falsely identify a significant effect. It became particularly useful with the\nadvent of genomic research, in which massive numbers of statistical tests might be conducted as\npart of a gene sequencing project. In these cases, the term applies to the testing protocol, and a\nsingle false “discovery” refers to the outcome of a hypothesis test (e.g., between two samples).\nResearchers sought to set the parameters of the testing process to control the false discovery\nrate at a specified level. The term has also been used in the data mining community in a\nclassification context, in which a false discovery is a mislabeling of a single record — in particular\nthe mislabeling of 0s as 1s (see Chapter 5 and “The Rare Class Problem”).\nFor a variety of reasons, including especially this general issue of “multiplicity,”\nmore research does not necessarily mean better research. For example, the\npharmaceutical company Bayer found in 2011 that when it tried to replicate 67\nscientific studies, it could fully replicate only 14 of them. Nearly two-thirds could\nnot be replicated at all.\nIn any case, the adjustment procedures for highly defined and structured statistical\ntests are too specific and inflexible to be of general use to data scientists. The\nbottom line for data scientists on multiplicity is:\nFor predictive modeling, the risk of getting an illusory model whose apparent\nefficacy is largely a product of random chance is mitigated by cross-\nvalidation (see “Cross-Validation”), and use of a holdout sample.\nFor other procedures without a labeled holdout set to check the model, you\nmust rely on:\nAwareness that the more you query and manipulate the data, the greater the\nrole that chance might play; and\nResampling and simulation heuristics to provide random chance\nbenchmarks against which observed results can be compared.\nKEY IDEAS\nMultiplicity in a research study or data mining project (multiple comparisons, many variables, many\nmodels, etc.) increases the risk of concluding that something is significant just by chance.\nFor situations involving multiple statistical comparisons (i.e., multiple tests of significance) there are\n",
      "content_length": 2281,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 196,
      "content": "statistical adjustment procedures.\nIn a data mining situation, use of a holdout sample with labeled outcome variables can help avoid\nmisleading results.\n",
      "content_length": 153,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 197,
      "content": "Further Reading\n1. For a short exposition of one procedure (Dunnett’s) to adjust for multiple\ncomparisons, see David Lane’s online statistics text.\n2. Megan Goldman offers a slightly longer treatment of the Bonferroni\nadjustment procedure.\n3. For an in-depth treatment of more flexible statistical procedures to adjust\np-values, see Resampling-Based Multiple Testing by Peter Westfall and\nStanley Young (Wiley, 1993).\n4. For a discussion of data partitioning and the use of holdout samples in\npredictive modeling, see Data Mining for Business Analytics, Chapter\n2, by Galit Shmueli, Peter Bruce, and Nitin Patel (Wiley, 2016).\n",
      "content_length": 627,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 198,
      "content": "Degrees of Freedom\nIn the documentation and settings to many statistical tests, you will see reference\nto “degrees of freedom.” The concept is applied to statistics calculated from\nsample data, and refers to the number of values free to vary. For example, if you\nknow the mean for a sample of 10 values, and you also know 9 of the values, you\nalso know the 10th value. Only 9 are free to vary.\nKEY TERMS\nn or sample size\nThe number of observations (also called rows or records) in the data.\nd.f.\nDegrees of freedom.\nThe number of degrees of freedom is an input to many statistical tests. For\nexample, degrees of freedom is the name given to the n – 1 denominator seen in\nthe calculations for variance and standard deviation. Why does it matter? When\nyou use a sample to estimate the variance for a population, you will end up with\nan estimate that is slightly biased downward if you use n in the denominator. If\nyou use n – 1 in the denominator, the estimate will be free of that bias.\nA large share of a traditional statistics course or text is consumed by various\nstandard tests of hypotheses (t-test, F-test, etc.). When sample statistics are\nstandardized for use in traditional statistical formulas, degrees of freedom is part\nof the standardization calculation to ensure that your standardized data matches the\nappropriate reference distribution (t-distribution, F-distribution, etc.).\nIs it important for data science? Not really, at least in the context of significance\ntesting. For one thing, formal statistical tests are used only sparingly in data\nscience. For another, the data size is usually large enough that it rarely makes a\nreal difference for a data scientist whether, for example, the denominator has n or\nn – 1.\nThere is one context, though, in which it is relevant: the use of factored variables\nin regression (including logistic regression). Regression algorithms choke if\nexactly redundant predictor variables are present. This most commonly occurs\n",
      "content_length": 1972,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 199,
      "content": "when factoring categorical variables into binary indicators (dummies). Consider\nday of week. Although there are seven days of the week, there are only six degrees\nof freedom in specifying day of week. For example, once you know that day of\nweek is not Monday through Saturday, you know it must be Sunday. Inclusion of\nthe Mon–Sat indicators thus means that also including Sunday would cause the\nregression to fail, due to a multicollinearity error.\nKEY IDEAS\nThe number of degrees of freedom (d.f.) forms part of the calculation to standardize test statistics\nso they can be compared to reference distributions (t-distribution, F-distribution, etc.).\nThe concept of degrees of freedom lies behind the factoring of categorical variables into n – 1\nindicator or dummy variables when doing a regression (to avoid multicollinearity).\n",
      "content_length": 830,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 200,
      "content": "Further Reading\nThere are several web tutorials on degrees of freedom.\n",
      "content_length": 71,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 201,
      "content": "ANOVA\nSuppose that, instead of an A/B test, we had a comparison of multiple groups, say\nA-B-C-D, each with numeric data. The statistical procedure that tests for a\nstatistically significant difference among the groups is called analysis of\nvariance, or ANOVA.\nKEY TERMS FOR ANOVA\nPairwise comparison\nA hypothesis test (e.g., of means) between two groups among multiple groups.\nOmnibus test\nA single hypothesis test of the overall variance among multiple group means.\nDecomposition of variance\nSeparation of components. contributing to an individual value (e.g., from the overall average, from\na treatment mean, and from a residual error).\nF-statistic\nA standardized statistic that measures the extent to which differences among group means\nexceeds what might be expected in a chance model.\nSS\n“Sum of squares,” referring to deviations from some average value.\nTable 3-3 shows the stickiness of four web pages, in numbers of seconds spent on\nthe page. The four pages are randomly switched out so that each web visitor\nreceives one at random. There are a total of five visitors for each page, and, in\nTable 3-3, each column is an independent set of data. The first viewer for page 1\nhas no connection to the first viewer for page 2. Note that in a web test like this,\nwe cannot fully implement the classic randomized sampling design in which each\nvisitor is selected at random from some huge population. We must take the visitors\nas they come. Visitors may systematically differ depending on time of day, time of\nweek, season of the year, conditions of their internet, what device they are using,\nand so on. These factors should be considered as potential bias when the\nexperiment results are reviewed.\nTable 3-3. Stickiness (in seconds) for\nfour web pages\n",
      "content_length": 1755,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 202,
      "content": "Page 1 Page 2 Page 3 Page 4\n164\n178\n175\n155\n172\n191\n193\n166\n177\n182\n171\n164\n156\n185\n163\n170\n195\n177\n176\n168\nAverage\n172\n185\n176\n162\nGrand average\n173.75\nNow, we have a conundrum (see Figure 3-6). When we were comparing just two\ngroups, it was a simple matter; we merely looked at the difference between the\nmeans of each group. With four means, there are six possible comparisons\nbetween groups:\nPage 1 compared to page 2\nPage 1 compared to page 3\nPage 1 compared to page 4\nPage 2 compared to page 3\nPage 2 compared to page 4\nPage 3 compared to page 4\n",
      "content_length": 552,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 203,
      "content": "Figure 3-6. Boxplots of the four groups show considerable differences among them\nThe more such pairwise comparisons we make, the greater the potential for being\nfooled by random chance (see “Multiple Testing”). Instead of worrying about all\nthe different comparisons between individual pages we could possibly make, we\ncan do a single overall omnibus test that addresses the question, “Could all the\npages have the same underlying stickiness, and the differences among them be due\nto the random way in which a common set of session times got allocated among\nthe four pages?”\n",
      "content_length": 575,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 204,
      "content": "The procedure used to test this is ANOVA. The basis for it can be seen in the\nfollowing resampling procedure (specified here for the A-B-C-D test of web page\nstickiness):\n1. Combine all the data together in a single box\n2. Shuffle and draw out four resamples of five values each\n3. Record the mean of each of the four groups\n4. Record the variance among the four group means\n5. Repeat steps 2–4 many times (say 1,000)\nWhat proportion of the time did the resampled variance exceed the observed\nvariance? This is the p-value.\nThis type of permutation test is a bit more involved than the type used in\n“Permutation Test”. Fortunately, the aovp function in the lmPerm package\ncomputes a permutation test for this case:\n> library(lmPerm)\n> summary(aovp(Time ~ Page, data=four_sessions))\n[1] \"Settings:  unique SS \"\nComponent 1 :\n            Df R Sum Sq R Mean Sq Iter Pr(Prob)\nPage         3    831.4    277.13 3104  0.09278 .\nResiduals   16   1618.4    101.15\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nThe p-value, given by Pr(Prob), is 0.09278. The column Iter lists the number of\niterations taken in the permutation test. The other columns correspond to a\ntraditional ANOVA table and are described next.\n",
      "content_length": 1227,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 205,
      "content": "F-Statistic\nJust like the t-test can be used instead of a permutation test for comparing the mean\nof two groups, there is a statistical test for ANOVA based on the F-statistic. The\nF-statistic is based on the ratio of the variance across group means (i.e., the\ntreatment effect) to the variance due to residual error. The higher this ratio, the\nmore statistically significant the result. If the data follows a normal distribution,\nthen statistical theory dictates that the statistic should have a certain distribution.\nBased on this, it is possible to compute a p-value.\nIn R, we can compute an ANOVA table using the aov function:\n> summary(aov(Time ~ Page, data=four_sessions))\n            Df Sum Sq Mean Sq F value Pr(>F)\nPage         3  831.4   277.1    2.74 0.0776 .\nResiduals   16 1618.4   101.2\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\nDf is “degrees of freedom,” Sum Sq is “sum of squares,” Mean Sq is “mean\nsquares” (short for mean-squared deviations), and F value is the F-statistic. For\nthe grand average, sum of squares is the departure of the grand average from 0,\nsquared, times 20 (the number of observations). The degrees of freedom for the\ngrand average is 1, by definition. For the treatment means, the degrees of freedom\nis 3 (once three values are set, and then the grand average is set, the other\ntreatment mean cannot vary). Sum of squares for the treatment means is the sum of\nsquared departures between the treatment means and the grand average. For the\nresiduals, degrees of freedom is 20 (all observations can vary), and SS is the sum\nof squared difference between the individual observations and the treatment\nmeans. Mean squares (MS) is the sum of squares divided by the degrees of\nfreedom. The F-statistic is MS(treatment)/MS(error). The F value thus depends\nonly on this ratio, and can be compared to a standard F distribution to determine\nwhether the differences among treatment means is greater than would be expected\nin random chance variation.\n",
      "content_length": 2003,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 206,
      "content": "DECOMPOSITION OF VARIANCE\nObserved values in a data set can be considered sums of different components. For any observed\ndata value within a data set, we can break it down into the grand average, the treatment effect,\nand the residual error. We call this a “decomposition of variance.”\n1. Start with grand average (173.75 for web page stickiness data).\n2. Add treatment effect, which might be negative (independent variable = web page).\n3. Add residual error, which might be negative.\nThus, the decomposition of the variance for the top-left value in the A-B-C-D test table is as\nfollows:\n1. Start with grand average: 173.75\n2. Add treatment (group) effect: –1.75 (172 – 173.75).\n3. Add residual: –8 (164 – 172).\n4. Equals: 164.\n",
      "content_length": 729,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 207,
      "content": "Two-Way ANOVA\nThe A-B-C-D test just described is a “one-way” ANOVA, in which we have one\nfactor (group) that is varying. We could have a second factor involved — say,\n“weekend versus weekday” — with data collected on each combination (group A\nweekend, group A weekday, group B weekend, etc.). This would be a “two-way\nANOVA,” and we would handle it in similar fashion to the one-way ANOVA by\nidentifying the “interaction effect.” After identifying the grand average effect, and\nthe treatment effect, we then separate the weekend and the weekday observations\nfor each group, and find the difference between the averages for those subsets and\nthe treatment average.\nYou can see that ANOVA, then two-way ANOVA, are the first steps on the road\ntoward a full statistical model, such as regression and logistic regression, in\nwhich multiple factors and their effects can be modeled (see Chapter 4).\nKEY IDEAS\nANOVA is a statistical proecdure for analyzing the results of an experiment with multiple groups.\nIt is the extension of similar procedures for the A/B test, used to assess whether the overall\nvariation among groups is within the range of chance variation.\nA useful outcome of an ANOVA is the identification of variance components associated with group\ntreatments, interaction effects, and errors.\n",
      "content_length": 1301,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 208,
      "content": "Further Reading\n1. Introductory Statistics: A Resampling Perspective by Peter Bruce\n(Wiley, 2014) has a chapter on ANOVA.\n2. Introduction to Design and Analysis of Experiments by George Cobb\n(Wiley, 2008) is a comprehensive and readable treatment of its subject.\n",
      "content_length": 263,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 209,
      "content": "Chi-Square Test\nWeb testing often goes beyond A/B testing and tests multiple treatments at once.\nThe chi-square test is used with count data to test how well it fits some expected\ndistribution. The most common use of the chi-square statistic in statistical\npractice is with \n contingency tables, to assess whether the null hypothesis\nof independence among variables is reasonable.\nThe chi-square test was originally developed by Karl Pearson in 1900. The term\n“chi” comes from the greek letter  used by Pearson in the article.\nKEY TERMS\nChi-square statistic\nA measure of the extent to which some observed data departs from expectation.\nExpectation or expected\nHow we would expect the data to turn out under some assumption, typically the null hypothesis.\nd.f.\nDegrees of freedom.\nNOTE\n means “rows by columns” — a 2×3 table has two rows and three columns.\n",
      "content_length": 856,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 210,
      "content": "Chi-Square Test: A Resampling Approach\nSuppose you are testing three different headlines — A, B, and C — and you run\nthem each on 1,000 visitors, with the results shown in Table 3-4.\nTable 3-4. Web testing results of three\ndifferent headlines\nHeadline A Headline B Headline C\nClick\n14\n8\n12\nNo-click 986\n992\n988\nThe headlines certainly appear to differ. Headline A returns nearly twice the click\nrate of B. The actual numbers are small, though. A resampling procedure can test\nwhether the click rates differ to an extent greater than chance might cause. For this\ntest, we need to have the “expected” distribution of clicks, and, in this case, that\nwould be under the null hypothesis assumption that all three headlines share the\nsame click rate, for an overall click rate of 34/3,000. Under this assumption, our\ncontingency table would look like Table 3-5.\nTable 3-5. Expected if all three\nheadlines have the same click rate\n(null hypothesis)\nHeadline A Headline B Headline C\nClick\n11.33\n11.33\n11.33\nNo-click 988.67\n988.67\n988.67\nThe Pearson residual is defined as:\n",
      "content_length": 1065,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 211,
      "content": "R measures the extent to which the actual counts differ from these expected counts\n(see Table 3-6).\nTable 3-6. Pearson residuals\nHeadline A Headline B Headline C\nClick\n0.792\n-0.990\n0.198\nNo-click -0.085\n0.106\n-0.021\nThe chi-squared statistic is defined as the sum of the squared Pearson residuals:\nwhere r and c are the number of rows and columns, respectively. The chi-squared\nstatistic for this example is 1.666. Is that more than could reasonably occur in a\nchance model?\nWe can test with this resampling algorithm:\n1. Constitute a box with 34 ones (clicks) and 2,966 zeros (no clicks).\n2. Shuffle, take three separate samples of 1,000, and count the clicks in\neach.\n3. Find the squared differences between the shuffled counts and the\nexpected counts, and sum them.\n4. Repeat steps 2 and 3, say, 1,000 times.\n5. How often does the resampled sum of squared deviations exceed the\nobserved? That’s the p-value.\nThe function chisq.test can be used to compute a resampled chi-square\nstatistic. For the click data, the chi-square test is:\n",
      "content_length": 1036,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 212,
      "content": "> chisq.test(clicks, simulate.p.value=TRUE)\n \nPearson's Chi-squared test with simulated p-value (based on 2000 replicates)\ndata:  clicks\nX-squared = 1.6659, df = NA, p-value = 0.4853\nThe test shows that this result could easily have been obtained by randomness.\n",
      "content_length": 262,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 213,
      "content": "Chi-Squared Test: Statistical Theory\nAsymptotic statistical theory shows that the distribution of the chi-squared statistic\ncan be approximated by a chi-square distribution. The appropriate standard chi-\nsquare distribution is determined by the degrees of freedom (see “Degrees of\nFreedom”). For a contingency table, the degrees of freedom are related to the\nnumber of rows (r) and columns (s) as follows:\nThe chi-square distribution is typically skewed, with a long tail to the right; see\nFigure 3-7 for the distribution with 1, 2, 5, and 10 degrees of freedom. The further\nout on the chi-square distribution the observed statistic is, the lower the p-value.\nThe function chisq.test can be used to compute the p-value using the chi-\nsquared distribution as a reference:\n> chisq.test(clicks, simulate.p.value=FALSE)\n \nPearson's Chi-squared test\ndata:  clicks\nX-squared = 1.6659, df = 2, p-value = 0.4348\nThe p-value is a little less than the resampling p-value: this is because the chi-\nsquare distribution is only an approximation of the actual distribution of the\nstatistic.\n",
      "content_length": 1077,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 214,
      "content": "Figure 3-7. Chi-square distribution with various degrees of freedom (probability on y-axis, value of chi-\nsquare statistic on x-axis)\n",
      "content_length": 134,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 215,
      "content": "Fisher’s Exact Test\nThe chi-square distribution is a good approximation of the shuffled resampling\ntest just described, except when counts are extremely low (single digits,\nespecially five or fewer). In such cases, the resampling procedure will yield more\naccurate p-values. In fact, most statistical software has a procedure to actually\nenumerate all the possible rearrangements (permutations) that can occur, tabulate\ntheir frequencies, and determine exactly how extreme the observed result is. This\nis called Fisher’s exact test after the great statistician R. A. Fisher. R code for\nFisher’s exact test is simple in its basic form:\n> fisher.test(clicks)\n \nFisher's Exact Test for Count Data\ndata:  clicks\np-value = 0.4824\nalternative hypothesis: two.sided\nThe p-value is very close to the p-value of 0.4853 obtained using the resampling\nmethod.\nWhere some counts are very low but others are quite high (e.g., the denominator in\na conversion rate), it may be necessary to do a shuffled permutation test instead of\na full exact test, due to the difficulty of calculating all possible permutations. The\npreceding R function has several arguments that control whether to use this\napproximation (simulate.p.value=TRUE or FALSE), how many iterations\nshould be used (B=...), and a computational constraint (workspace=...) that\nlimits how far calculations for the exact result should go.\nDETECTING SCIENTIFIC FRAUD\nAn interesting example is provided by Tufts University researcher Thereza Imanishi-Kari, who was\naccused in 1991 of fabricating data in her research. Congressman John Dingell became involved, and the\ncase eventually led to the resignation of her colleague, David Baltimore, from the presidency of\nRockefeller University.\nImanishi-Kari was ultimately exonerated after a lengthy proceeding. However, one element in the case\nrested on statistical evidence regarding the expected distribution of digits in her laboratory data, where\neach observation had many digits. Investigators focused on the interior digits, which would be expected\nto follow a uniform random distribution. That is, they would occur randomly, with each digit having equal\nprobability of occurring (the lead digit might be predominantly one value, and the final digits might be\naffected by rounding). Table 3-7 lists the frequencies of interior digits from the actual data in the case.\n",
      "content_length": 2362,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 216,
      "content": "Table 3-7. Central\ndigit in laboratory\ndata\nDigit Frequency\n0\n14\n1\n71\n2\n7\n3\n65\n4\n23\n5\n19\n6\n12\n7\n45\n8\n53\n9\n6\nThe distribution of the 315 digits, shown in Figure 3-8 certainly looks nonrandom:\nInvestigators calculated the departure from expectation (31.5 — that’s how often each digit would occur\nin a strictly uniform distribution) and used a chi-square test (a resampling procedure could equally have\nbeen used) to show that the actual distribution was well beyond the range of normal chance variation.\n",
      "content_length": 503,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 217,
      "content": "Figure 3-8. Frequency histogram for Imanishi-Kari lab data\n",
      "content_length": 59,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 218,
      "content": "Relevance for Data Science\nMost standard uses of the chi-square test, or Fisher’s exact test, are not terribly\nrelevant for data science. In most experiments, whether A-B or A-B-C…, the goal\nis not simply to establish statistical significance, but rather to arive at the best\ntreatment. For this purpose, multi-armed bandits (see “Multi-Arm Bandit\nAlgorithm”) offer a more complete solution.\nOne data science application of the chi-square test, especially Fisher’s exact\nversion, is in determining appropriate sample sizes for web experiments. These\nexperiments often have very low click rates and, despite thousands of exposures,\ncount rates might be too small to yield definitive conclusions in an experiment. In\nsuch cases, Fisher’s exact test, the chi-square test, and other tests can be useful as\na component of power and sample size calculations (see “Power and Sample\nSize”).\nChi-square tests are used widely in research by investigators in search of the\nelusive statistically significant p-value that will allow publication. Chi-square\ntests, or similar resampling simulations, are used in data science applications\nmore as a filter to determine whether an effect or feature is worthy of further\nconsideration than as a formal test of significance. For example, they are used in\nspatial statistics and mapping to determine whether spatial data conforms to a\nspecified null distribution (e.g., are crimes concentrated in a certain area to a\ngreater degree than random chance would allow?). They can also be used in\nautomated feature selection in machine learning, to assess class prevalence across\nfeatures and identify features where the prevalence of a certain class is unusually\nhigh or low, in a way that is not compatible with random variation.\nKEY IDEAS\nA common procedure in statistics is to test whether observed data counts are consistent with an\nassumption of independence (e.g., propensity to buy a particular item is independent of gender).\nThe chi-square distribution is the reference distribution (which embodies the assumption of\nindependence) to which the observed calculated chi-square statistic must be compared.\n",
      "content_length": 2138,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 219,
      "content": "Further Reading\nR. A. Fisher’s famous “Lady Tasting Tea” example from the beginning of the\n20th century remains a simple and effective illustration of his exact test.\nGoogle “Lady Tasting Tea,” and you will find a number of good writeups.\nStat Trek offers a good tutorial on the chi-square test.\n",
      "content_length": 296,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 220,
      "content": "Multi-Arm Bandit Algorithm\nMulti-arm bandits offer an approach to testing, especially web testing, that allows\nexplicit optimization and more rapid decision making than the traditional\nstatistical approach to designing experiments.\nKEY TERMS\nMulti-arm bandit\nAn imaginary slot machine with multiple arms for the customer to choose from, each with different\npayoffs, here taken to be an analogy for a multitreatment experiment.\nArm\nA treatment in an experiment (e.g., “headline A in a web test”).\nWin\nThe experimental analog of a win at the slot machine (e.g., “customer clicks on the link”).\nA traditional A/B test involves data collected in an experiment, according to a\nspecified design, to answer a specific question such as, “Which is better,\ntreatment A or treatment B?” The presumption is that once we get an answer to\nthat question, the experimenting is over and we proceed to act on the results.\nYou can probably perceive several difficulties with that approach. First, our\nanswer may be inconclusive: “effect not proven.” In other words, the results from\nthe experiment may suggest an effect, but if there is an effect, we don’t have a big\nenough sample to prove it (to the satisfaction of the traditional statistical\nstandards). What decision do we take? Second, we might want to begin taking\nadvantage of results that come in prior to the conclusion of the experiment. Third,\nwe might want the right to change our minds or to try something different based on\nadditional data that comes in after the experiment is over. The traditional\napproach to experiments and hypothesis tests dates from the 1920s, and is rather\ninflexible. The advent of computer power and software has enabled more\npowerful flexible approaches. Moreover, data science (and business in general) is\nnot so worried about statistical significance, but more concerned with optimizing\noverall effort and results.\nBandit algorithms, which are very popular in web testing, allow you to test\n",
      "content_length": 1966,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 221,
      "content": "multiple treatments at once and reach conclusions faster than traditional statistical\ndesigns. They take their name from slot machines used in gambling, also termed\none-armed bandits (since they are configured in such a way that they extract\nmoney from the gambler in a steady flow). If you imagine a slot machine with\nmore than one arm, each arm paying out at a different rate, you would have a\nmulti-armed bandit, which is the full name for this algorithm.\nYour goal is to win as much money as possible, and more specifically, to identify\nand settle on the winning arm sooner rather than later. The challenge is that you\ndon’t know at what rate the arms pay out — you only know the results of pulling\nthe arm. Suppose each “win” is for the same amount, no matter which arm. What\ndiffers is the probability of a win. Suppose further that you initially try each arm\n50 times and get the following results:\nArm A: 10 wins out of 50\nArm B: 2 win out of 50\nArm C: 4 wins out of 50\nOne extreme approach is to say, “Looks like arm A is a winner — let’s quit trying\nthe other arms and stick with A.” This takes full advantage of the information from\nthe initial trial. If A is truly superior, we get the benefit of that early on. On the\nother hand, if B or C is truly better, we lose any opportunity to discover that.\nAnother extreme approach is to say, “This all looks to be within the realm of\nchance — let’s keep pulling them all equally.” This gives maximum opportunity\nfor alternates to A to show themselves. However, in the process, we are deploying\nwhat seem to be inferior treatments. How long do we permit that? Bandit\nalgorithms take a hybrid approach: we start pulling A more often, to take\nadvantage of its apparent superiority, but we don’t abandon B and C. We just pull\nthem less often. If A continues to outperform, we continue to shift resources\n(pulls) away from B and C and pull A more often. If, on the other hand, C starts to\ndo better, and A starts to do worse, we can shift pulls from A back to C. If one of\nthem turns out to be superior to A and this was hidden in the initial trial due to\nchance, it now has an opportunity to emerge with further testing.\nNow think of applying this to web testing. Instead of multiple slot machine arms,\nyou might have multiple offers, headlines, colors, and so on, being tested on a\n",
      "content_length": 2335,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 222,
      "content": "website. Customers either click (a “win” for the merchant) or don’t click. Initially,\nthe offers are shown randomly and equally. If, however, one offer starts to\noutperform the others, it can be shown (“pulled”) more often. But what should the\nparameters of the algorithm that modifies the pull rates be? What “pull rates”\nshould we change to, and when should we change?\nHere is one simple algorithm, the epsilon-greedy algorithm for an A/B test:\n1. Generate a random number between 0 and 1.\n2. If the number lies between 0 and epsilon (where epsilon is a number\nbetween 0 and 1, typically fairly small), flip a fair coin (50/50\nprobability), and:\na. If the coin is heads, show offer A.\nb. If the coin is tails, show offer B.\n3. If the number is ≥ epsilon, show whichever offer has had the highest\nresponse rate to date.\nEpsilon is the single parameter that governs this algorithm. If epsilon is 1, we end\nup with a standard simple A/B experiment (random allocation between A and B\nfor each subject). If epsilon is 0, we end up with a purely greedy algorithm — it\nseeks no further experimentation, simply assigning subjects (web visitors) to the\nbest-performing treatment.\nA more sophisticated algorithm uses “Thompson’s sampling.” This procedure\n“samples” (pulls a bandit arm) at each stage to maximize the probability of\nchoosing the best arm. Of course you don’t know which is the best arm — that’s\nthe whole problem! — but as you observe the payoff with each successive draw,\nyou gain more information. Thompson’s sampling uses a Bayesian approach: some\nprior distribution of rewards is assumed initially, using what is called a beta\ndistribution (this is a common mechanism for specifying prior information in a\nBayesian problem). As information accumulates from each draw, this information\ncan be updated, allowing the selection of the next draw to be better optimized as\nfar as choosing the right arm.\nBandit algorithms can efficiently handle 3+ treatments and move toward optimal\nselection of the “best.” For traditional statistical testing procedures, the\n",
      "content_length": 2065,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 223,
      "content": "complexity of decision making for 3+ treatments far outstrips that of the traditional\nA/B test, and the advantage of bandit algorithms is much greater.\nKEY IDEAS\nTraditional A/B tests envision a random sampling process, which can lead to excessive exposure to\nthe inferior treatment.\nMulti-arm bandits, in contrast, alter the sampling process to incorporate information learned during\nthe experiment and reduce the frequency of the inferior treatment.\nThey also facilitate efficient treatment of more than two treatments.\nThere are different algorithms for shifting sampling probability away from the inferior treatment(s)\nand to the (presumed) superior one.\n",
      "content_length": 659,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 224,
      "content": "Further Reading\nAn excellent short treatment of multi-arm bandit algorithms is found in\nBandit Algorithms, by John Myles White (O’Reilly, 2012). White includes\nPython code, as well as the results of simulations to assess the performance\nof bandits.\nFor more (somewhat technical) information about Thompson sampling, see\n“Analysis of Thompson Sampling for the Multi-armed Bandit Problem” by\nShipra Agrawal and Navin Goyal.\n",
      "content_length": 422,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 225,
      "content": "Power and Sample Size\nIf you run a web test, how do you decide how long it should run (i.e., how many\nimpressions per treatment are needed)? Despite what you may read in many\nguides to web testing on the web, there is no good general guidance — it depends,\nmainly, on the frequency with which the desired goal is attained.\nKEY TERMS\nEffect size\nThe minimum size of the effect that you hope to be able to detect in a statistical test, such as “a\n20% improvement in click rates”.\nPower\nThe probability of detecting a given effect size with a given sample size.\nSignificance level\nThe statistical significance level at which the test will be conducted.\nOne step in statistical calculations for sample size is to ask “Will a hypothesis test\nactually reveal a difference between treatments A and B?” The outcome of a\nhypothesis test — the p-value — depends on what the real difference is between\ntreatment A and treatment B. It also depends on the luck of the draw — who gets\nselected for the groups in the experiment. But it makes sense that the bigger the\nactual difference between treatments A and B, the greater the probability that our\nexperiment will reveal it; and the smaller the difference, the more data will be\nneeded to detect it. To distinguish between a .350 hitter in baseball, and a .200\nhitter, not that many at-bats are needed. To distinguish between a .300 hitter and a\n.280 hitter, a good many more at-bats will be needed.\nPower is the probability of detecting a specified effect size with specified sample\ncharacteristics (size and variability). For example, we might say (hypothetically)\nthat the probability of distinguishing between a .330 hitter and a .200 hitter in 25\nat-bats is 0.75. The effect size here is a difference of .130. And “detecting” means\nthat a hypothesis test will reject the null hypothesis of “no difference” and\nconclude there is a real effect. So the experiment of 25 at-bats (n = 25) for two\nhitters, with an effect size of 0.130, has (hypothetical) power of 0.75 or 75%.\n",
      "content_length": 2015,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 226,
      "content": "You can see that there are several moving parts here, and it is easy to get tangled\nup with the numerous statistical assumptions and formulas that will be needed (to\nspecify sample variability, effect size, sample size, alpha-level for the hypothesis\ntest, etc., and to calculate power). Indeed, there is special-purpose statistical\nsoftware to calculate power. Most data scientists will not need to go through all\nthe formal steps needed to report power, for example, in a published paper.\nHowever, they may face occasions where they want to collect some data for an\nA/B test, and collecting or processing the data involves some cost. In that case,\nknowing approximately how much data to collect can help avoid the situation\nwhere you collect data at some effort, and the result ends up being inconclusive.\nHere’s a fairly intuitive alternative approach:\n1. Start with some hypothetical data that represents your best guess about\nthe data that will result (perhaps based on prior data) — for example, a\nbox with 20 ones and 80 zeros to represent a .200 hitter, or a box with\nsome observations of “time spent on website.”\n2. Create a second sample simply by adding the desired effect size to the\nfirst sample — for example, a second box with 33 ones and 67 zeros, or\na second box with 25 seconds added to each initial “time spent on\nwebsite.”\n3. Draw a bootstrap sample of size n from each box.\n4. Conduct a permutation (or formula-based) hypothesis test on the two\nbootstrap samples and record whether the difference between them is\nstatistically significant.\n5. Repeat the preceding two steps many times and determine how often the\ndifference was significant — that’s the estimated power.\n",
      "content_length": 1691,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 227,
      "content": "Sample Size\nThe most common use of power calculations is to estimate how big a sample you\nwill need.\nFor example, suppose you are looking at click-through rates (clicks as a\npercentage of exposures), and testing a new ad against an existing ad. How many\nclicks do you need to accumulate in the study? If you are only interested in results\nthat show a huge difference (say a 50% difference), a relatively small sample\nmight do the trick. If, on the other hand, even a minor difference would be of\ninterest, then a much larger sample is needed. A standard approach is to establish\na policy that a new ad must do better than an existing ad by some percentage, say\n10%; otherwise, the existing ad will remain in place. This goal, the “effect size,”\nthen drives the sample size.\nFor example, suppose current click-through rates are about 1.1%, and you are\nseeking a 10% boost to 1.21%. So we have two boxes, box A with 1.1% ones\n(say 110 ones and 9,890 zeros), and box B with 1.21% ones (say 121 ones and\n9,879 zeros). For starters, let’s try 300 draws from each box (this would be like\n300 “impressions” for each ad). Suppose our first draw yields the following:\nBox A: 3 ones\nBox B: 5 ones\nRight away we can see that any hypothesis test would reveal this difference (5\nversus 3) to be well within the range of chance variation. This combination of\nsample size (n = 300 in each group) and effect size (10% difference) is too small\nfor any hypothesis test to reliably show a difference.\nSo we can try increasing the sample size (let’s try 2,000 impressions), and require\na larger improvement (30% instead of 10%).\nFor example, suppose current click-through rates are still 1.1%, but we are now\nseeking a 50% boost to 1.65%. So we have two boxes: box A still with 1.1% ones\n(say 110 ones and 9,890 zeros), and box B with 1.65% ones (say 165 ones and\n9,868 zeros). Now we’ll try 2,000 draws from each box. Suppose our first draw\nyields the following:\nBox A: 19 ones\n",
      "content_length": 1959,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 228,
      "content": "Box B: 34 ones\nA significance test on this difference (34–19) shows it still registers as “not\nsignificant” (though much closer to significance than the earlier difference of 5–3).\nTo calculate power, we would need to repeat the previous procedure many times,\nor use statistical software that can calculate power, but our initial draw suggests\nto us that even detecting a 50% improvement will require several thousand ad\nimpressions.\nIn summary, for calculating power or required sample size, there are four moving\nparts:\nSample size\nEffect size you want to detect\nSignificance level (alpha) at which the test will be conducted\nPower\nSpecify any three of them, and the fourth can be calculated. Most commonly, you\nwould want to calculate sample size, so you must specify the other three. Here is\nR code for a test involving two proportions, where both samples are the same size\n(this uses the pwr package):\npwr.2p.test(h = ..., n = ..., sig.level = ..., power = )\nh= effect size (as a proportion)\nn = sample size\nsig.level = the significance level (alpha) at which the test will be conducted\npower = power (probability of detecting the effect size)\nKEY IDEAS\nFinding out how big a sample size you need requires thinking ahead to the statistical test you plan to\nconduct.\nYou must specify the minimum size of the effect that you want to detect.\nYou must also specify the required probability of detecting that effect size (power).\nFinally, you must specify the significance level (alpha) at which the test will be conducted.\n",
      "content_length": 1524,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 229,
      "content": "Further Reading\n1. Sample Size Determination and Power, by Tom Ryan (Wiley, 2013), is a\ncomprehensive and readable review of this subject.\n2. Steve Simon, a statistical consultant, has written a very engaging\nnarrative-style post on the subject.\n",
      "content_length": 246,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 230,
      "content": "Summary\nThe principles of experimental design — randomization of subjects into two or\nmore groups receiving different treatments — allow us to draw valid conclusions\nabout how well the treatments work. It is best to include a control treatment of\n“making no change.” The subject of formal statistical inference — hypothesis\ntesting, p-values, t-tests, and much more along these lines — occupies much time\nand space in a traditional statistics course or text, and the formality is mostly\nunneeded from a data science perspective. However, it remains important to\nrecognize the role that random variation can play in fooling the human brain.\nIntuitive resampling procedures (permutation and bootstrap) allow data scientists\nto gauge the extent to which chance variation can play a role in their data analysis.\nThe multiplication rule states that the probability of n independent events all happening is the product of the\nindividual probabilities. For example, if you and I each flip a coin once, the probability that your coin and my\ncoin will both land heads is 0.5 × 0.5 = 0.25.\n1\n",
      "content_length": 1082,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 231,
      "content": "Chapter 4. Regression and Prediction\nPerhaps the most common goal in statistics is to answer the question: Is the\nvariable X (or more likely, \n) associated with a variable Y, and, if so,\nwhat is the relationship and can we use it to predict Y?\nNowhere is the nexus between statistics and data science stronger than in the\nrealm of prediction — specifically the prediction of an outcome (target) variable\nbased on the values of other “predictor” variables. Another important connection\nis in the area of anomaly detection, where regression diagnostics originally\nintended for data analysis and improving the regression model can be used to\ndetect unusual records. The antecedents of correlation and linear regression date\nback over a century.\n",
      "content_length": 742,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 232,
      "content": "Simple Linear Regression\nSimple linear regression models the relationship between the magnitude of one\nvariable and that of a second — for example, as X increases, Y also increases. Or\nas X increases, Y decreases.1 Correlation is another way to measure how two\nvariables are related: see the section “Correlation”. The difference is that while\ncorrelation measures the strength of an association between two variables,\nregression quantifies the nature of the relationship.\nKEY TERMS FOR SIMPLE LINEAR REGRESSION\nResponse\nThe variable we are trying to predict.\nSynonyms\ndependent variable, Y-variable, target, outcome\nIndependent variable\nThe variable used to predict the response.\nSynonyms\nindependent variable, X-variable, feature, attribute\nRecord\nThe vector of predictor and outcome values for a specific individual or case.\nSynonyms\nrow, case, instance, example\nIntercept\nThe intercept of the regression line — that is, the predicted value when \n.\nSynonyms\n, \nRegression coefficient\nThe slope of the regression line.\nSynonyms\nslope, \n, \n, parameter estimates, weights\nFitted values\nThe estimates \n obtained from the regression line.\n",
      "content_length": 1137,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 233,
      "content": "Synonyms\npredicted values\nResiduals\nThe difference between the observed values and the fitted values.\nSynonyms\nerrors\nLeast squares\nThe method of fitting a regression by minimizing the sum of squared residuals.\nSynonyms\nordinary least squares\n",
      "content_length": 243,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 234,
      "content": "The Regression Equation\nSimple linear regression estimates exactly how much Y will change when X\nchanges by a certain amount. With the correlation coefficient, the variables X and\nY are interchangable. With regression, we are trying to predict the Y variable from\nX using a linear relationship (i.e., a line):\nWe read this as “Y equals b1 times X, plus a constant b0.” The symbol \n is\nknown as the intercept (or constant), and the symbol \n as the slope for X. Both\nappear in R output as coefficients, though in general use the term coefficient is\noften reserved for \n. The Y variable is known as the response or dependent\nvariable since it depends on X. The X variable is known as the predictor or\nindependent variable. The machine learning community tends to use other terms,\ncalling Y the target and X a feature vector.\nConsider the scatterplot in Figure 4-1 displaying the number of years a worker\nwas exposed to cotton dust (Exposure) versus a measure of lung capacity (PEFR\nor “peak expiratory flow rate”). How is PEFR related to Exposure? It’s hard to\ntell just based on the picture.\n",
      "content_length": 1090,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 235,
      "content": "Figure 4-1. Cotton exposure versus lung capacity\nSimple linear regression tries to find the “best” line to predict the response PEFR\nas a function of the predictor variable Exposure.\nThe lm function in R can be used to fit a linear regression.\n",
      "content_length": 244,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 236,
      "content": "model <- lm(PEFR ~ Exposure, data=lung)\nlm standards for linear model and the ~ symbol denotes that PEFR is predicted by\nExposure.\nPrinting the model object produces the following output:\nCall:\nlm(formula = PEFR ~ Exposure, data = lung)\nCoefficients:\n(Intercept)     Exposure\n    424.583       -4.185\nThe intercept, or \n, is 424.583 and can be interpreted as the predicted PEFR for a\nworker with zero years exposure. The regression coefficient, or \n, can be\ninterpreted as follows: for each additional year that a worker is exposed to cotton\ndust, the worker’s PEFR measurement is reduced by –4.185.\nThe regression line from this model is displayed in Figure 4-2.\nFigure 4-2. Slope and intercept for the regression fit to the lung data\n",
      "content_length": 736,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 237,
      "content": "Fitted Values and Residuals\nImportant concepts in regression analysis are the fitted values and residuals. In\ngeneral, the data doesn’t fall exactly on a line, so the regression equation should\ninclude an explicit error term :\nThe fitted values, also referred to as the predicted values, are typically denoted\nby  (Y-hat). These are given by:\nThe notation  and  indicates that the coefficients are estimated versus known.\n",
      "content_length": 422,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 238,
      "content": "HAT NOTATION: ESTIMATES VERSUS KNOWN VALUES\nThe “hat” notation is used to differentiate between estimates and known values. So the symbol \n(“b-hat”) is an estimate of the unknown parameter \n. Why do statisticians differentiate between\nthe estimate and the true value? The estimate has uncertainty, whereas the true value is fixed.2\nWe compute the residuals  by subtracting the predicted values from the original\ndata:\nIn R, we can obtain the fitted values and residuals using the functions predict\nand residuals:\nfitted <- predict(model)\nresid <- residuals(model)\nFigure 4-3 illustrates the residuals from the regression line fit to the lung data. The\nresiduals are the length of the vertical dashed lines from the data to the line.\n",
      "content_length": 733,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 239,
      "content": "Figure 4-3. Residuals from a regression line (note the different y-axis scale from Figure 4-2, hence the\napparently different slope)\n",
      "content_length": 133,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 240,
      "content": "Least Squares\nHow is the model fit to the data? When there is a clear relationship, you could\nimagine fitting the line by hand. In practice, the regression line is the estimate that\nminimizes the sum of squared residual values, also called the residual sum of\nsquares or RSS:\nThe estimates  and  are the values that minimize RSS.\nThe method of minimizing the sum of the squared residuals is termed least\nsquares regression, or ordinary least squares (OLS) regression. It is often\nattributed to Carl Friedrich Gauss, the German mathmetician, but was first\npublished by the French mathmetician Adrien-Marie Legendre in 1805. Least\nsquares regression leads to a simple formula to compute the coefficients:\n",
      "content_length": 703,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 241,
      "content": "Historically, computational convenience is one reason for the widespread use of\nleast squares in regression. With the advent of big data, computational speed is\nstill an important factor. Least squares, like the mean (see “Median and Robust\nEstimates”), are sensitive to outliers, although this tends to be a signicant problem\nonly in small or moderate-sized problems. See “Outliers” for a discussion of\noutliers in regression.\n",
      "content_length": 428,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 242,
      "content": "REGRESSION TERMINOLOGY\nWhen analysts and researchers use the term regression by itself, they are typically referring to\nlinear regression; the focus is usually on developing a linear model to explain the relationship\nbetween predictor variables and a numeric outcome variable. In its formal statistical sense,\nregression also includes nonlinear models that yield a functional relationship between predictors\nand outcome variables. In the machine learning community, the term is also occasionally used\nloosely to refer to the use of any predictive model that produces a predicted numeric outcome\n(standing in distinction from classification methods that predict a binary or categorical outcome).\n",
      "content_length": 695,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 243,
      "content": "Prediction versus Explanation (Profiling)\nHistorically, a primary use of regression was to illuminate a supposed linear\nrelationship between predictor variables and an outcome variable. The goal has\nbeen to understand a relationship and explain it using the data that the regression\nwas fit to. In this case, the primary focus is on the estimated slope of the\nregression equation, . Economists want to know the relationship between\nconsumer spending and GDP growth. Public health officials might want to\nunderstand whether a public information campaign is effective in promoting safe\nsex practices. In such cases, the focus is not on predicting individual cases, but\nrather on understanding the overall relationship.\nWith the advent of big data, regression is widely used to form a model to predict\nindividual outcomes for new data, rather than explain data in hand (i.e., a\npredictive model). In this instance, the main items of interest are the fitted values \n. In marketing, regression can be used to predict the change in revenue in\nresponse to the size of an ad campaign. Universities use regression to predict\nstudents’ GPA based on their SAT scores.\nA regression model that fits the data well is set up such that changes in X lead to\nchanges in Y. However, by itself, the regression equation does not prove the\ndirection of causation. Conclusions about causation must come from a broader\ncontext of understanding about the relationship. For example, a regression\nequation might show a definite relationship between number of clicks on a web ad\nand number of conversions. It is our knowledge of the marketing process, not the\nregression equation, that leads us to the conclusion that clicks on the ad lead to\nsales, and not vice versa.\nKEY IDEAS\nThe regression equation models the relationship between a response variable Y and a predictor\nvariable X as a line.\nA regression model yields fitted values and residuals — predictions of the response and the errors\nof the predictions.\nRegression models are typically fit by the method of least squares.\nRegression is used both for prediction and explanation.\n",
      "content_length": 2111,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 245,
      "content": "Further Reading\nFor an in-depth treatment of prediction versus explanation, see Galit Shmueli’s\narticle “To Explain or to Predict”.\n",
      "content_length": 132,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 246,
      "content": "Multiple Linear Regression\nWhen there are multiple predictors, the equation is simply extended to\naccommodate them:\nInstead of a line, we now have a linear model — the relationship between each\ncoefficient and its variable (feature) is linear.\nKEY TERMS FOR MULTIPLE LINEAR REGRESSION\nRoot mean squared error\nThe square root of the average squared error of the regression (this is the most widely used metric\nto compare regression models).\nSynonyms\nRMSE\nResidual standard error\nThe same as the root mean squared error, but adjusted for degrees of freedom.\nSynonyms\nRSE\nR-squared\nThe proportion of variance explained by the model, from 0 to 1.\nSynonyms\ncoefficient of determination, \nt-statistic\nThe coefficient for a predictor, divided by the standard error of the coefficient, giving a metric to\ncompare the importance of variables in the model.\nWeighted regression\nRegression with the records having different weights.\nAll of the other concepts in simple linear regression, such as fitting by least\nsquares and the definition of fitted values and residuals, extend to the multiple\nlinear regression setting. For example, the fitted values are given by:\n",
      "content_length": 1155,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 248,
      "content": "Example: King County Housing Data\nAn example of using regression is in estimating the value of houses. County\nassessors must estimate the value of a house for the purposes of assessing taxes.\nReal estate consumers and professionals consult popular websites such as Zillow\nto ascertain a fair price. Here are a few rows of housing data from King County\n(Seattle), Washington, from the house data.frame:\nhead(house[, c(\"AdjSalePrice\", \"SqFtTotLiving\", \"SqFtLot\", \"Bathrooms\",\n               \"Bedrooms\", \"BldgGrade\")])\nSource: local data frame [6 x 6]\n  AdjSalePrice SqFtTotLiving SqFtLot Bathrooms Bedrooms BldgGrade\n         (dbl)         (int)   (int)     (dbl)    (int)     (int)\n1       300805          2400    9373      3.00        6         7\n2      1076162          3764   20156      3.75        4        10\n3       761805          2060   26036      1.75        4         8\n4       442065          3200    8618      3.75        5         7\n5       297065          1720    8620      1.75        4         7\n6       411781           930    1012      1.50        2         8\nThe goal is to predict the sales price from the other variables. The lm handles the\nmultiple regression case simply by including more terms on the righthand side of\nthe equation; the argument na.action=na.omit causes the model to drop records\nthat have missing values:\nhouse_lm <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n                 Bedrooms + BldgGrade,\n               data=house, na.action=na.omit)\nPrinting house_lm object produces the following output:\nhouse_lm\nCall:\nlm(formula = AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n    Bedrooms + BldgGrade, data = house, na.action = na.omit)\nCoefficients:\n  (Intercept)  SqFtTotLiving        SqFtLot      Bathrooms\n   -5.219e+05      2.288e+02     -6.051e-02     -1.944e+04\n     Bedrooms      BldgGrade\n   -4.778e+04      1.061e+05\nThe interpretation of the coefficients is as with simple linear regression: the\n",
      "content_length": 1967,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 249,
      "content": "predicted value  changes by the coefficient \n for each unit change in \nassuming all the other variables, \n for \n, remain the same. For example,\nadding an extra finished square foot to a house increases the estimated value by\nroughly $229; adding 1,000 finished square feet implies the value will increase by\n$228,800.\n",
      "content_length": 318,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 250,
      "content": "Assessing the Model\nThe most important performance metric from a data science perspective is root\nmean squared error, or RMSE. RMSE is the square root of the average squared\nerror in the predicted  values:\nThis measures the overall accuracy of the model, and is a basis for comparing it to\nother models (including models fit using machine learning techniques). Similar to\nRMSE is the residual standard error, or RSE. In this case we have p predictors,\nand the RSE is given by:\nThe only difference is that the denominator is the degrees of freedom, as opposed\nto number of records (see “Degrees of Freedom”). In practice, for linear\nregression, the difference between RMSE and RSE is very small, particularly for\nbig data applications.\nThe summary function in R computes RSE as well as other metrics for a\nregression model:\nsummary(house_lm)\nCall:\nlm(formula = AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n    Bedrooms + BldgGrade, data = house, na.action = na.omit)\n",
      "content_length": 974,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 251,
      "content": "Residuals:\n     Min       1Q   Median       3Q      Max\n-1199508  -118879   -20982    87414  9472982\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -5.219e+05  1.565e+04 -33.349  < 2e-16 ***\nSqFtTotLiving  2.288e+02  3.898e+00  58.699  < 2e-16 ***\nSqFtLot       -6.051e-02  6.118e-02  -0.989    0.323\nBathrooms     -1.944e+04  3.625e+03  -5.362 8.32e-08 ***\nBedrooms      -4.778e+04  2.489e+03 -19.194  < 2e-16 ***\nBldgGrade      1.061e+05  2.396e+03  44.287  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nResidual standard error: 261200 on 22683 degrees of freedom\nMultiple R-squared:  0.5407, \nAdjusted R-squared:  0.5406\nF-statistic:  5340 on 5 and 22683 DF,  p-value: < 2.2e-16\nAnother useful metric that you will see in software output is the coefficient of\ndetermination, also called the R-squared statistic or \n. R-squared ranges from\n0 to 1 and measures the proportion of variation in the data that is accounted for in\nthe model. It is useful mainly in explanatory uses of regression where you want to\nassess how well the model fits the data. The formula for \n is:\nThe denominator is proportional to the variance of Y. The output from R also\nreports an adjusted R-squared, which adjusts for the degrees of freedom; seldom\nis this significantly different in multiple regression.\nAlong with the estimated coefficients, R reports the standard error of the\ncoefficients (SE) and a t-statistic:\n",
      "content_length": 1465,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 252,
      "content": "The t-statistic — and its mirror image, the p-value — measures the extent to\nwhich a coefficient is “statistically significant” — that is, outside the range of\nwhat a random chance arrangement of predictor and target variable might produce.\nThe higher the t-statistic (and the lower the p-value), the more significant the\npredictor. Since parsimony is a valuable model feature, it is useful to have a tool\nlike this to guide choice of variables to include as predictors (see “Model\nSelection and Stepwise Regression”).\nWARNING\nIn addition to the t-statistic, R and other packages will often report a p-value (Pr(>|t|) in the R\noutput) and F-statistic. Data scientists do not generally get too involved with the interpretation of\nthese statistics, nor with the issue of statistical significance. Data scientists primarily focus on the\nt-statistic as a useful guide for whether to include a predictor in a model or not. High t-statistics\n(which go with p-values near 0) indicate a predictor should be retained in a model, while very low\nt-statistics indicate a predictor could be dropped. See “P-Value” for more discussion.\n",
      "content_length": 1122,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 253,
      "content": "Cross-Validation\nClassic statistical regression metrics (R2, F-statistics, and p-values) are all “in-\nsample” metrics — they are applied to the same data that was used to fit the\nmodel. Intuitively, you can see that it would make a lot of sense to set aside some\nof the original data, not use it to fit the model, and then apply the model to the set-\naside (holdout) data to see how well it does. Normally, you would use a majority\nof the data to fit the model, and use a smaller portion to test the model.\nThis idea of “out-of-sample” validation is not new, but it did not really take hold\nuntil larger data sets became more prevalent; with a small data set, analysts\ntypically want to use all the data and fit the best possible model.\nUsing a holdout sample, though, leaves you subject to some uncertainty that arises\nsimply from variability in the small holdout sample. How different would the\nassessment be if you selected a different holdout sample?\nCross-validation extends the idea of a holdout sample to multiple sequential\nholdout samples. The algorithm for basic k-fold cross-validation is as follows:\n1. Set aside 1/k of the data as a holdout sample.\n2. Train the model on the remaining data.\n3. Apply (score) the model to the 1/k holdout, and record needed model\nassessment metrics.\n4. Restore the first 1/k of the data, and set aside the next 1/k (excluding any\nrecords that got picked the first time).\n5. Repeat steps 2 and 3.\n6. Repeat until each record has been used in the holdout portion.\n7. Average or otherwise combine the model assessment metrics.\nThe division of the data into the training sample and the holdout sample is also\ncalled a fold.\n",
      "content_length": 1665,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 254,
      "content": "Model Selection and Stepwise Regression\nIn some problems, many variables could be used as predictors in a regression.\nFor example, to predict house value, additional variables such as the basement\nsize or year built could be used. In R, these are easy to add to the regression\nequation:\nhouse_full <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n                 Bedrooms + BldgGrade + PropertyType + NbrLivingUnits +\n                 SqFtFinBasement + YrBuilt + YrRenovated +\n                 NewConstruction,\n               data=house, na.action=na.omit)\nAdding more variables, however, does not necessarily mean we have a better\nmodel. Statisticians use the principle of Occam’s razor to guide the choice of a\nmodel: all things being equal, a simpler model should be used in preference to a\nmore complicated model.\nIncluding additional variables always reduces RMSE and increases \n. Hence,\nthese are not appropriate to help guide the model choice. In the 1970s, Hirotugu\nAkaike, the eminent Japanese statistician, deveoped a metric called AIC (Akaike’s\nInformation Criteria) that penalizes adding terms to a model. In the case of\nregression, AIC has the form:\nAIC = 2P + n log(RSS/n)\nwhere p is the number of variables and n is the number of records. The goal is to\nfind the model that minimizes AIC; models with k more extra variables are\npenalized by 2k.\n",
      "content_length": 1371,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 255,
      "content": "AIC, BIC AND MALLOWS CP\nThe formula for AIC may seem a bit mysterious, but in fact it is based on asymptotic results in\ninformation theory. There are several variants to AIC:\nAICc: a version of AIC corrected for small sample sizes.\nBIC or Bayesian information criteria: similar to AIC with a stronger penalty for including\nadditional variables to the model.\nMallows Cp: A variant of AIC developed by Colin Mallows.\nData scientists generally do not need to worry about the differences among these in-sample\nmetrics or the underlying theory behind them.\nHow do we find the model that minimizes AIC? One approach is to search through\nall possible models, called all subset regression. This is computationally\nexpensive and is not feasible for problems with large data and many variables. An\nattractive alternative is to use stepwise regression, which successively adds and\ndrops predictors to find a model that lowers AIC. The MASS package by Venebles\nand Ripley offers a stepwise regression function called stepAIC:\nlibrary(MASS)\nstep <- stepAIC(house_full, direction=\"both\")\nstep\nCall:\nlm(formula = AdjSalePrice ~ SqFtTotLiving + Bathrooms + Bedrooms +\n    BldgGrade + PropertyType + SqFtFinBasement + YrBuilt, data = house0,\n    na.action = na.omit)\nCoefficients:\n              (Intercept)              SqFtTotLiving\n               6227632.22                     186.50\n                Bathrooms                   Bedrooms\n                 44721.72                  -49807.18\n                BldgGrade  PropertyTypeSingle Family\n                139179.23                   23328.69\n    PropertyTypeTownhouse            SqFtFinBasement\n                 92216.25                       9.04\n                  YrBuilt\n                 -3592.47\nThe function chose a model in which several variables were dropped from\nhouse_full: SqFtLot, NbrLivingUnits, YrRenovated, and NewConstruction.\nSimpler yet are forward selection and backward selection. In forward selection,\n",
      "content_length": 1963,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 256,
      "content": "you start with no predictors and add them one-by-one, at each step adding the\npredictor that has the largest contribution to \n, stopping when the contribution is\nno longer statistically significant. In backward selection, or backward\nelimination, you start with the full model and take away predictors that are not\nstatistically significant until you are left with a model in which all predictors are\nstatistically significant.\nPenalized regression is similar in spirit to AIC. Instead of explicitly searching\nthrough a discrete set of models, the model-fitting equation incorporates a\nconstraint that penalizes the model for too many variables (parameters). Rather\nthan eliminating predictor variables entirely — as with stepwise, forward, and\nbackward selection — penalized regression applies the penalty by reducing\ncoefficients, in some cases to near zero. Common penalized regression methods\nare ridge regression and lasso regression.\nStepwise regression and all subset regression are in-sample methods to assess\nand tune models. This means the model selection is possibly subject to overfitting\nand may not perform as well when applied to new data. One common approach to\navoid this is to use cross-validation to validate the models. In linear regression,\noverfitting is typically not a major issue, due to the simple (linear) global\nstructure imposed on the data. For more sophisticated types of models,\nparticularly iterative procedures that respond to local data structure, cross-\nvalidation is a very important tool; see “Cross-Validation” for details.\n",
      "content_length": 1563,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 257,
      "content": "Weighted Regression\nWeighted regression is used by statisticians for a variety of purposes; in\nparticular, it is important for analysis of complex surveys. Data scientists may find\nweighted regression useful in two cases:\nInverse-variance weighting when different observations have been measured\nwith different precision.\nAnalysis of data in an aggregated form such that the weight variable encodes\nhow many original observations each row in the aggregated data represents.\nFor example, with the housing data, older sales are less reliable than more recent\nsales. Using the DocumentDate to determine the year of the sale, we can compute\na Weight as the number of years since 2005 (the beginning of the data).\nlibrary(lubridate)\nhouse$Year = year(house$DocumentDate)\nhouse$Weight = house$Year - 2005\nWe can compute a weighted regression with the lm function using the weight\nargument.\nhouse_wt <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n                 Bedrooms + BldgGrade,\n               data=house, weight=Weight)\nround(cbind(house_lm=house_lm$coefficients,\n            house_wt=house_wt$coefficients), digits=3)\n                   house_lm    house_wt\n(Intercept)   -521924.722 -584265.244\nSqFtTotLiving     228.832     245.017\nSqFtLot            -0.061      -0.292\nBathrooms      -19438.099  -26079.171\nBedrooms       -47781.153  -53625.404\nBldgGrade      106117.210  115259.026\nThe coefficents in the weighted regression are slightly different from the original\nregression.\nKEY IDEAS\nMultiple linear regression models the relationship between a response variable Y and multiple\npredictor variables \n.\n",
      "content_length": 1623,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 258,
      "content": "The most important metrics to evaluate a model are root mean squared error (RMSE) and R-\nsquared (R2).\nThe standard error of the coefficients can be used to measure the reliability of a variable’s\ncontribution to a model.\nStepwise regression is a way to automatically determine which variables should be included in the\nmodel.\nWeighted regression is used to give certain records more or less weight in fitting the equation.\n",
      "content_length": 424,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 259,
      "content": "Prediction Using Regression\nThe primary purpose of regression in data science is prediction. This is useful to\nkeep in mind, since regression, being an old and established statistical method,\ncomes with baggage that is more relevant to its traditional explanatory modeling\nrole than to prediction.\nKEY TERMS FOR PREDICTION USING REGRESSION\nPrediction interval\nAn uncertainty interval around an individual predicted value.\nExtrapolation\nExtension of a model beyond the range of the data used to fit it.\n",
      "content_length": 502,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 260,
      "content": "The Dangers of Extrapolation\nRegression models should not be used to extrapolate beyond the range of the data.\nThe model is valid only for predictor values for which the data has sufficient\nvalues (even in the case that sufficient data is available, there could be other\nproblems: see “Testing the Assumptions: Regression Diagnostics”). As an\nextreme case, suppose model_lm is used to predict the value of a 5,000-square-\nfoot empty lot. In such a case, all the predictors related to the building would have\na value of 0 and the regression equation would yield an absurd prediction of –\n521,900 + 5,000 × –.0605 = –$522,202. Why did this happen? The data contains\nonly parcels with buildings — there are no records corresponding to vacant land.\nConsequently, the model has no information to tell it how to predict the sales price\nfor vacant land.\n",
      "content_length": 847,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 261,
      "content": "Confidence and Prediction Intervals\nMuch of statistics involves understanding and measuring variability (uncertainty).\nThe t-statistics and p-values reported in regression output deal with this in a\nformal way, which is sometimes useful for variable selection (see “Assessing the\nModel”). More useful metrics are confidence intervals, which are uncertainty\nintervals placed around regression coefficients and predictions. An easy way to\nunderstand this is via the bootstrap (see “The Bootstrap” for more details about\nthe general bootstrap procedure). The most common regression confidence\nintervals encountered in software output are those for regression parameters\n(coefficients). Here is a bootstrap algorithm for generating confidence intervals\nfor regression parameters (coefficients) for a data set with P predictors and n\nrecords (rows):\n1. Consider each row (including outcome variable) as a single “ticket” and\nplace all the n tickets in a box.\n2. Draw a ticket at random, record the values, and replace it in the box.\n3. Repeat step 2 n times; you now have one bootstrap resample.\n4. Fit a regression to the bootstrap sample, and record the estimated\ncoefficients.\n5. Repeat steps 2 through 4, say, 1,000 times.\n6. You now have 1,000 bootstrap values for each coefficient; find the\nappropriate percentiles for each one (e.g., 5th and 95th for a 90%\nconfidence interval).\nYou can use the Boot function in R to generate actual bootstrap confidence\nintervals for the coefficients, or you can simply use the formula-based intervals\nthat are a routine R output. The conceptual meaning and interpretation are the\nsame, and not of central importance to data scientists, because they concern the\nregression coefficients. Of greater interest to data scientists are intervals around\npredicted y values ( ). The uncertainty around  comes from two sources:\nUncertainty about what the relevant predictor variables and their coefficients\n",
      "content_length": 1934,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 262,
      "content": "are (see the preceding bootstrap algorithm)\nAdditional error inherent in individual data points\nThe individual data point error can be thought of as follows: even if we knew for\ncertain what the regression equation was (e.g., if we had a huge number of records\nto fit it), the actual outcome values for a given set of predictor values will vary.\nFor example, several houses — each with 8 rooms, a 6,500 square foot lot, 3\nbathrooms, and a basement — might have different values. We can model this\nindividual error with the residuals from the fitted values. The bootstrap algorithm\nfor modeling both the regression model error and the individual data point error\nwould look as follows:\n1. Take a bootstrap sample from the data (spelled out in greater detail\nearlier).\n2. Fit the regression, and predict the new value.\n3. Take a single residual at random from the original regression fit, add it to\nthe predicted value, and record the result.\n4. Repeat steps 1 through 3, say, 1,000 times.\n5. Find the 2.5th and the 97.5th percentiles of the results.\n",
      "content_length": 1049,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 263,
      "content": "PREDICTION INTERVAL OR CONFIDENCE INTERVAL?\nA prediction interval pertains to uncertainty around a single value, while a confidence interval\npertains to a mean or other statistic calculated from multiple values. Thus, a prediction interval\nwill typically be much wider than a confidence interval for the same value. We model this\nindividual value error in the bootstrap model by selecting an individual residual to tack on to the\npredicted value. Which should you use? That depends on the context and the purpose of the\nanalysis, but, in general, data scientists are interested in specific individual predictions, so a\nprediction interval would be more appropriate. Using a confidence interval when you should be\nusing a prediction interval will greatly underestimate the uncertainty in a given predicted value.\nKEY IDEAS\nExtrapolation beyond the range of the data can lead to error.\nConfidence intervals quantify uncertainty around regression coefficients.\nPrediction intervals quantify uncertainty in individual predictions.\nMost software, R included, will produce prediction and confidence intervals in default or specified\noutput, using formulas.\nThe bootstrap can also be used; the interpretation and idea are the same.\n",
      "content_length": 1225,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 264,
      "content": "Factor Variables in Regression\nFactor variables, also termed categorical variables, take on a limited number of\ndiscrete values. For example, a loan purpose can be “debt consolidation,”\n“wedding,” “car,” and so on. The binary (yes/no) variable, also called an\nindicator variable, is a special case of a factor variable. Regression requires\nnumerical inputs, so factor variables need to be recoded to use in the model. The\nmost common approach is to convert a variable into a set of binary dummy\nvariables.\nKEY TERMS FOR FACTOR VARIABLES\nDummy variables\nBinary 0–1 variables derived by recoding factor data for use in regression and other models.\nReference coding\nThe most common type of coding used by statisticians, in which one level of a factor is used as a\nreference and other factors are compared to that level.\nSynonyms\ntreatment coding\nOne hot encoder\nA common type of coding used in the machine learning community in which all factors levels are\nretained. While useful for certain machine learning algorithms, this approach is not appropriate for\nmultiple linear regression.\nDeviation coding\nA type of coding that compares each level against the overall mean as opposed to the reference\nlevel.\nSynonyms\nsum contrasts\n",
      "content_length": 1225,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 265,
      "content": "Dummy Variables Representation\nIn the King County housing data, there is a factor variable for the property type; a\nsmall subset of six records is shown below.\nhead(house[, 'PropertyType'])\nSource: local data frame [6 x 1]\n   PropertyType\n         (fctr)\n1     Multiplex\n2 Single Family\n3 Single Family\n4 Single Family\n5 Single Family\n6     Townhouse\nThere are three possible values: Multiplex, Single Family, and Townhouse.\nTo use this factor variable, we need to convert it to a set of binary variables. We\ndo this by creating a binary variable for each possible value of the factor variable.\nTo do this in R, we use the model.matrix function:3\nprop_type_dummies <- model.matrix(~PropertyType -1, data=house)\nhead(prop_type_dummies)\n  PropertyTypeMultiplex PropertyTypeSingle Family PropertyTypeTownhouse\n1                     1                         0                     0\n2                     0                         1                     0\n3                     0                         1                     0\n4                     0                         1                     0\n5                     0                         1                     0\n6                     0                         0                     1\nThe function model.matrix converts a data frame into a matrix suitable to a\nlinear model. The factor variable PropertyType, which has three distinct levels,\nis represented as a matrix with three columns. In the machine learning community,\nthis representation is referred to as one hot encoding (see “One Hot Encoder”). In\ncertain machine learning algorithms, such as nearest neighbors and tree models,\none hot encoding is the standard way to represent factor variables (for example,\nsee “Tree Models”).\nIn the regression setting, a factor variable with P distinct levels is usually\nrepresented by a matrix with only P – 1 columns. This is because a regression\nmodel typically includes an intercept term. With an intercept, once you have\ndefined the values for P – 1 binaries, the value for the Pth is known and could be\n",
      "content_length": 2059,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 266,
      "content": "considered redundant. Adding the Pth column will cause a multicollinearity error\n(see “Multicollinearity”).\nThe default representation in R is to use the first factor level as a reference and\ninterpret the remaining levels relative to that factor.\nlm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n+      Bedrooms +  BldgGrade + PropertyType, data=house)\nCall:\nlm(formula = AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n    Bedrooms + BldgGrade + PropertyType, data = house)\nCoefficients:\n              (Intercept)              SqFtTotLiving\n               -4.469e+05                  2.234e+02\n                  SqFtLot                  Bathrooms\n               -7.041e-02                 -1.597e+04\n                 Bedrooms                  BldgGrade\n               -5.090e+04                  1.094e+05\nPropertyTypeSingle Family      PropertyTypeTownhouse\n               -8.469e+04                 -1.151e+05\nThe output from the R regression shows two coefficients corresponding to\nPropertyType: PropertyTypeSingle Family and PropertyTypeTownhouse.\nThere is no coefficient of Multiplex since it is implicitly defined when\nPropertyTypeSingle Family == 0 and PropertyTypeTownhouse == 0. The\ncoefficients are interpreted as relative to Multiplex, so a home that is Single\nFamily is worth almost $85,000 less, and a home that is Townhouse is worth over\n$150,000 less.4\n",
      "content_length": 1382,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 267,
      "content": "DIFFERENT FACTOR CODINGS\nThere are several different ways to encode factor variables, known as contrast coding systems.\nFor example, deviation coding, also know as sum contrasts, compares each level against the\noverall mean. Another contrast is polynomial coding, which is appropriate for ordered factors;\nsee the section “Ordered Factor Variables”. With the exception of ordered factors, data scientists\nwill generally not encounter any type of coding besides reference coding or one hot encoder.\n",
      "content_length": 498,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 268,
      "content": "Factor Variables with Many Levels\nSome factor variables can produce a huge number of binary dummies — zip codes\nare a factor variable and there are 43,000 zip codes in the US. In such cases, it is\nuseful to explore the data, and the relationships between predictor variables and\nthe outcome, to determine whether useful information is contained in the\ncategories. If so, you must further decide whether it is useful to retain all factors,\nor whether the levels should be consolidated.\nIn King County, there are 82 zip codes with a house sale:\ntable(house$ZipCode)\n 9800 89118 98001 98002 98003 98004 98005 98006 98007 98008 98010 98011\n    1     1   358   180   241   293   133   460   112   291    56   163\n98014 98019 98022 98023 98024 98027 98028 98029 98030 98031 98032 98033\n   85   242   188   455    31   366   252   475   263   308   121   517\n98034 98038 98039 98040 98042 98043 98045 98047 98050 98051 98052 98053\n  575   788    47   244   641     1   222    48     7    32   614   499\n98055 98056 98057 98058 98059 98065 98068 98070 98072 98074 98075 98077\n  332   402     4   420   513   430     1    89   245   502   388   204\n98092 98102 98103 98105 98106 98107 98108 98109 98112 98113 98115 98116\n  289   106   671   313   361   296   155   149   357     1   620   364\n98117 98118 98119 98122 98125 98126 98133 98136 98144 98146 98148 98155\n  619   492   260   380   409   473   465   310   332   287    40   358\n98166 98168 98177 98178 98188 98198 98199 98224 98288 98354\n  193   332   216   266   101   225   393     3     4     9\nZipCode is an important variable, since it is a proxy for the effect of location on\nthe value of a house. Including all levels requires 81 coefficients corresponding to\n81 degrees of freedom. The original model house_lm has only 5 degress of\nfreedom; see “Assessing the Model”. Moreover, several zip codes have only one\nsale. In some problems, you can consolidate a zip code using the first two or three\ndigits, corresponding to a submetropolitan geographic region. For King County,\nalmost all of the sales occur in 980xx or 981xx, so this doesn’t help.\nAn alternative approach is to group the zip codes according to another variable,\nsuch as sale price. Even better is to form zip code groups using the residuals from\nan initial model. The following dplyr code consolidates the 82 zip codes into\nfive groups based on the median of the residual from the house_lm regression:\nzip_groups <- house %>%\n  mutate(resid = residuals(house_lm)) %>%\n  group_by(ZipCode) %>%\n  summarize(med_resid = median(resid),\n",
      "content_length": 2552,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 269,
      "content": "            cnt = n()) %>%\n  arrange(med_resid) %>%\n  mutate(cum_cnt = cumsum(cnt),\n         ZipGroup = ntile(cum_cnt, 5))\nhouse <- house %>%\n  left_join(select(zip_groups, ZipCode, ZipGroup), by='ZipCode')\nThe median residual is computed for each zip and the ntile function is used to\nsplit the zip codes, sorted by the median, into five groups. See “Confounding\nVariables” for an example of how this is used as a term in a regression improving\nupon the original fit.\nThe concept of using the residuals to help guide the regression fitting is a\nfundamental step in the modeling process; see “Testing the Assumptions:\nRegression Diagnostics”.\n",
      "content_length": 643,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 270,
      "content": "Ordered Factor Variables\nSome factor variables reflect levels of a factor; these are termed ordered factor\nvariables or ordered categorical variables. For example, the loan grade could be\nA, B, C, and so on — each grade carries more risk than the prior grade. Ordered\nfactor variables can typically be converted to numerical values and used as is. For\nexample, the variable BldgGrade is an ordered factor variable. Several of the\ntypes of grades are shown in Table 4-1. While the grades have specific meaning,\nthe numeric value is ordered from low to high, corresponding to higher-grade\nhomes. With the regression model house_lm, fit in “Multiple Linear Regression”,\nBldgGrade was treated as a numeric variable.\nTable 4-1. A\ntypical data\nformat\nValue Description\n1\nCabin\n2\nSubstandard\n5\nFair\n10\nVery good\n12\nLuxury\n13\nMansion\nTreating ordered factors as a numeric variable preserves the information\ncontained in the ordering that would be lost if it were converted to a factor.\nKEY IDEAS\nFactor variables need to be converted into numeric variables for use in a regression.\nThe most common method to encode a factor variable with P distinct values is to represent them\nusing P-1 dummy variables.\nA factor variable with many levels, even in very big data sets, may need to be consolidated into a\nvariable with fewer levels.\n",
      "content_length": 1323,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 271,
      "content": "Some factors have levels that are ordered and can be represented as a single numeric variable.\n",
      "content_length": 95,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 272,
      "content": "Interpreting the Regression Equation\nIn data science, the most important use of regression is to predict some dependent\n(outcome) variable. In some cases, however, gaining insight from the equation\nitself to understand the nature of the relationship between the predictors and the\noutcome can be of value. This section provides guidance on examining the\nregression equation and interpreting it.\nKEY TERMS FOR INTERPRETING THE REGRESSION EQUATION\nCorrelated variables\nWhen the predictor variables are highly correlated, it is difficult to interpret the individual\ncoefficients.\nMulticollinearity\nWhen the predictor variables have perfect, or near-perfect, correlation, the regression can be\nunstable or impossible to compute.\nSynonyms\ncollinearity\nConfounding variables\nAn important predictor that, when omitted, leads to spurious relationships in a regression equation.\nMain effects\nThe relationship between a predictor and the outcome variable, independent from other variables.\nInteractions\nAn interdependent relationship between two or more predictors and the response.\n",
      "content_length": 1073,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 273,
      "content": "Correlated Predictors\nIn multiple regression, the predictor variables are often correlated with each\nother. As an example, examine the regression coefficients for the model step_lm,\nfit in “Model Selection and Stepwise Regression”:\nstep_lm$coefficients\n              (Intercept)             SqFtTotLiving\n             6.227632e+06              1.865012e+02\n                Bathrooms                  Bedrooms\n             4.472172e+04             -4.980718e+04\n                BldgGrade PropertyTypeSingle Family\n             1.391792e+05              2.332869e+04\n    PropertyTypeTownhouse           SqFtFinBasement\n             9.221625e+04              9.039911e+00\n                  YrBuilt\n            -3.592468e+03\nThe coefficient for Bedrooms is negative! This implies that adding a bedroom to a\nhouse will reduce its value. How can this be? This is because the predictor\nvariables are correlated: larger houses tend to have more bedrooms, and it is the\nsize that drives house value, not the number of bedrooms. Consider two homes of\nthe exact same size: it is reasonable to expect that a home with more, but smaller,\nbedrooms would be considered less desirable.\nHaving correlated predictors can make it difficult to interpret the sign and value of\nregression coefficients (and can inflate the standard error of the estimates). The\nvariables for bedrooms, house size, and number of bathrooms are all correlated.\nThis is illustrated by the following example, which fits another regression\nremoving the variables SqFtTotLiving, SqFtFinBasement, and Bathrooms\nfrom the equation:\nupdate(step_lm, . ~ . -SqFtTotLiving - SqFtFinBasement - Bathrooms)\nCall:\nlm(formula = AdjSalePrice ~ Bedrooms + BldgGrade + PropertyType +\n    YrBuilt, data = house0, na.action = na.omit)\nCoefficients:\n              (Intercept)                   Bedrooms\n                  4834680                      27657\n                BldgGrade  PropertyTypeSingle Family\n                   245709                     -17604\n    PropertyTypeTownhouse                    YrBuilt\n                   -47477                      -3161\n",
      "content_length": 2104,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 274,
      "content": "The update function can be used to add or remove variables from a model. Now\nthe coefficient for bedrooms is positive — in line with what we would expect\n(though it is really acting as a proxy for house size, now that those variables have\nbeen removed).\nCorrelated variables are only one issue with interpreting regression coefficients.\nIn house_lm, there is no variable to account for the location of the home, and the\nmodel is mixing together very different types of regions. Location may be a\nconfounding variable; see “Confounding Variables” for further discussion.\n",
      "content_length": 570,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 275,
      "content": "Multicollinearity\nAn extreme case of correlated variables produces multicollinearity — a condition\nin which there is redundance among the predictor variables. Perfect\nmulticollinearity occurs when one predictor variable can be expressed as a linear\ncombination of others. Multicollinearity occurs when:\nA variable is included multiple times by error.\nP dummies, instead of P – 1 dummies, are created from a factor variable (see\n“Factor Variables in Regression”).\nTwo variables are nearly perfectly correlated with one another.\nMulticollinearity in regression must be addressed — variables should be removed\nuntil the multicollinearity is gone. A regression does not have a well-defined\nsolution in the presence of perfect multicollinearity. Many software packages,\nincluding R, automatically handle certain types of multicolliearity. For example, if\nSqFtTotLiving is included twice in the regression of the house data, the results\nare the same as for the house_lm model. In the case of nonperfect\nmulticollinearity, the software may obtain a solution but the results may be\nunstable.\nNOTE\nMulticollinearity is not such a problem for nonregression methods like trees, clustering, and\nnearest-neighbors, and in such methods it may be advisable to retain P dummies (instead of P –\n1). That said, even in those methods, nonredundancy in predictor variables is still a virtue.\n",
      "content_length": 1372,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 276,
      "content": "Confounding Variables\nWith correlated variables, the problem is one of commission: including different\nvariables that have a similar predictive relationship with the response. With\nconfounding variables, the problem is one of omission: an important variable is\nnot included in the regression equation. Naive interpretation of the equation\ncoefficients can lead to invalid conclusions.\nTake, for example, the King County regression equation house_lm from\n“Example: King County Housing Data”. The regression coefficients of SqFtLot,\nBathrooms, and Bedrooms are all negative. The original regression model does\nnot contain a variable to represent location — a very important predictor of house\nprice. To model location, include a variable ZipGroup that categorizes the zip\ncode into one of five groups, from least expensive (1) to most expensive (5).5\nlm(AdjSalePrice ~  SqFtTotLiving + SqFtLot +\n     Bathrooms + Bedrooms +\n     BldgGrade + PropertyType + ZipGroup,\n   data=house, na.action=na.omit)\nCoefficients:\n              (Intercept)              SqFtTotLiving\n               -6.709e+05                  2.112e+02\n                  SqFtLot                  Bathrooms\n                4.692e-01                  5.537e+03\n                 Bedrooms                  BldgGrade\n               -4.139e+04                  9.893e+04\nPropertyTypeSingle Family      PropertyTypeTownhouse\n                2.113e+04                 -7.741e+04\n                ZipGroup2                  ZipGroup3\n                5.169e+04                  1.142e+05\n                ZipGroup4                  ZipGroup5\n                1.783e+05                  3.391e+05\nZipGroup is clearly an important variable: a home in the most expensive zip code\ngroup is estimated to have a higher sales price by almost $340,000. The\ncoefficients of SqFtLot and Bathrooms are now positive and adding a bathroom\nincreases the sale price by $7,500.\nThe coefficient for Bedrooms is still negative. While this is unintuitive, this is a\nwell-known phenomenon in real estate. For homes of the same livable area and\nnumber of bathrooms, having more, and therefore smaller, bedrooms is associated\nwith less valuable homes.\n",
      "content_length": 2182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 277,
      "content": "Interactions and Main Effects\nStatisticians like to distinguish between main effects, or independent variables,\nand the interactions between the main effects. Main effects are what are often\nreferred to as the predictor variables in the regression equation. An implicit\nassumption when only main effects are used in a model is that the relationship\nbetween a predictor variable and the response is independent of the other\npredictor variables. This is often not the case.\nFor example, the model fit to the King County Housing Data in “Confounding\nVariables” includes several variables as main effects, including ZipCode.\nLocation in real estate is everything, and it is natural to presume that the\nrelationship between, say, house size and the sale price depends on location. A\nbig house built in a low-rent district is not going to retain the same value as a big\nhouse built in an expensive area. You include interactions between variables in R\nusing the * operator. For the King County data, the following fits an interaction\nbetween SqFtTotLiving and ZipGroup:\nlm(AdjSalePrice ~  SqFtTotLiving*ZipGroup + SqFtLot +\n     Bathrooms + Bedrooms + BldgGrade + PropertyType,\n   data=house, na.action=na.omit)\n Coefficients:\n              (Intercept)              SqFtTotLiving\n               -4.919e+05                  1.176e+02\n                ZipGroup2                  ZipGroup3\n               -1.342e+04                  2.254e+04\n                ZipGroup4                  ZipGroup5\n                1.776e+04                 -1.555e+05\n                  SqFtLot                  Bathrooms\n                7.176e-01                 -5.130e+03\n                 Bedrooms                  BldgGrade\n               -4.181e+04                  1.053e+05\nPropertyTypeSingle Family      PropertyTypeTownhouse\n                1.603e+04                 -5.629e+04\n  SqFtTotLiving:ZipGroup2    SqFtTotLiving:ZipGroup3\n                3.165e+01                  3.893e+01\n  SqFtTotLiving:ZipGroup4    SqFtTotLiving:ZipGroup5\n                7.051e+01                  2.298e+02\nThe resulting model has four new terms: SqFtTotLiving:ZipGroup2,\nSqFtTotLiving:ZipGroup3, and so on.\nLocation and house size appear to have a strong interaction. For a home in the\nlowest ZipGroup, the slope is the same as the slope for the main effect\n",
      "content_length": 2321,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 278,
      "content": "SqFtTotLiving, which is $177 per square foot (this is because R uses reference\ncoding for factor variables; see “Factor Variables in Regression”). For a home in\nthe highest ZipGroup, the slope is the sum of the main effect plus\nSqFtTotLiving:ZipGroup5, or $177 + $230 = $447 per square foot. In other\nwords, adding a square foot in the most expensive zip code group boosts the\npredicted sale price by a factor of almost 2.7, compared to the boost in the least\nexpensive zip code group.\n",
      "content_length": 486,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 279,
      "content": "MODEL SELECTION WITH INTERACTION TERMS\nIn problems involving many variables, it can be challenging to decide which interaction terms\nshould be included in the model. Several different approaches are commonly taken:\nIn some problems, prior knowledge and intuition can guide the choice of which interaction\nterms to include in the model.\nStepwise selection (see “Model Selection and Stepwise Regression”) can be used to sift\nthrough the various models.\nPenalized regression can automatically fit to a large set of possible interaction terms.\nPerhaps the most common approach is the use tree models, as well as their descendents,\nrandom forest and gradient boosted trees. This class of models automatically searches\nfor optimal interaction terms; see “Tree Models”.\nKEY IDEAS\nBecause of correlation between predictors, care must be taken in the interpretation of the\ncoefficients in multiple linear regression.\nMulticollinearity can cause numerical instability in fitting the regression equation.\nA confounding variable is an important predictor that is omitted from a model and can lead to a\nregression equation with spurious relationships.\nAn interaction term between two variables is needed if the relationship between the variables and\nthe response is interdependent.\n",
      "content_length": 1269,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 280,
      "content": "Testing the Assumptions: Regression Diagnostics\nIn explanatory modeling (i.e., in a research context), various steps, in addition to\nthe metrics mentioned previously (see “Assessing the Model”), are taken to assess\nhow well the model fits the data. Most are based on analysis of the residuals,\nwhich can test the assumptions underlying the model. These steps do not directly\naddress predictive accuracy, but they can provide useful insight in a predictive\nsetting.\nKEY TERMS FOR REGRESSION DIAGNOSTICS\nStandardized residuals\nResiduals divided by the standard error of the residuals.\nOutliers\nRecords (or outcome values) that are distant from the rest of the data (or the predicted outcome).\nInfluential value\nA value or record whose presence or absence makes a big difference in the regression equation.\nLeverage\nThe degree of influence that a single record has on a regression equation.\nSynonyms\nhat-value\nNon-normal residuals\nNon-normally distributed residuals can invalidate some technical requirements of regression, but\nare usually not a concern in data science.\nHeteroskedasticity\nWhen some ranges of the outcome experience residuals with higher variance (may indicate a\npredictor missing from the equation).\nPartial residual plots\nA diagnostic plot to illuminate the relationship between the outcome variable and a single predictor.\nSynonyms\nadded variables plot\n",
      "content_length": 1370,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 281,
      "content": "Outliers\nGenerally speaking, an extreme value, also called an outlier, is one that is distant\nfrom most of the other observations. Just as outliers need to be handled for\nestimates of location and variability (see “Estimates of Location” and “Estimates\nof Variability”), outliers can cause problems with regression models. In\nregression, an outlier is a record whose actual y value is distant from the\npredicted value. You can detect outliers by examining the standardized residual,\nwhich is the residual divided by the standard error of the residuals.\nThere is no statistical theory that separates outliers from nonoutliers. Rather, there\nare (arbitrary) rules of thumb for how distant from the bulk of the data an\nobservation needs to be in order to be called an outlier. For example, with the\nboxplot, outliers are those data points that are too far above or below the box\nboundaries (see “Percentiles and Boxplots”), where “too far” = “more than 1.5\ntimes the inter-quartile range.” In regression, the standardized residual is the\nmetric that is typically used to determine whether a record is classified as an\noutlier. Standardized residuals can be interpreted as “the number of standard\nerrors away from the regression line.”\nLet’s fit a regression to the King County house sales data for all sales in zip code\n98105:\nhouse_98105 <- house[house$ZipCode == 98105,]\nlm_98105 <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +\n                 Bedrooms + BldgGrade, data=house_98105)\nWe extract the standardized residuals using the rstandard function and obtain the\nindex of the smallest residual using the order function:\nsresid <- rstandard(lm_98105)\nidx <- order(sresid)\nsresid[idx[1]]\n    20431\n-4.326732\nThe biggest overestimate from the model is more than four standard errors above\nthe regression line, corresponding to an overestimate of $757,753. The original\ndata record corresponding to this outlier is as follows:\n",
      "content_length": 1937,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 282,
      "content": "house_98105[idx[1], c('AdjSalePrice', 'SqFtTotLiving', 'SqFtLot',\n              'Bathrooms', 'Bedrooms', 'BldgGrade')]\nAdjSalePrice SqFtTotLiving SqFtLot Bathrooms Bedrooms BldgGrade\n         (dbl)         (int)   (int)     (dbl)    (int)     (int)\n1       119748          2900    7276         3        6         7\nIn this case, it appears that there is something wrong with the record: a house of\nthat size typically sells for much more than $119,748 in that zip code. Figure 4-4\nshows an excerpt from the statuatory deed from this sale: it is clear that the sale\ninvolved only partial interest in the property. In this case, the outlier corresonds to\na sale that is anomalous and should not be included in the regression. Outliers\ncould also be the result of other problems, such as a “fat-finger” data entry or a\nmismatch of units (e.g., reporting a sale in thousands of dollars versus simply\ndollars).\nFigure 4-4. Statutory warrant of deed for the largest negative residual\nFor big data problems, outliers are generally not a problem in fitting the\nregression to be used in predicting new data. However, outliers are central to\nanomaly detection, where finding outliers is the whole point. The outlier could\nalso correspond to a case of fraud or an accidental action. In any case, detecting\noutliers can be a critical business need.\n",
      "content_length": 1337,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 283,
      "content": "Influential Values\nA value whose absence would significantly change the regression equation is\ntermed an infuential observation. In regression, such a value need not be\nassociated with a large residual. As an example, consider the regression lines in\nFigure 4-5. The solid line corresponds to the regression with all the data, while\nthe dashed line corresonds to the regression with the point in the upper-right\nremoved. Clearly, that data value has a huge influence on the regression even\nthough it is not associated with a large outlier (from the full regression). This data\nvalue is considered to have high leverage on the regression.\nIn addition to standardized residuals (see “Outliers”), statisticians have\ndeveloped several metrics to determine the influence of a single record on a\nregression. A common measure of leverage is the hat-value; values above \n indicate a high-leverage data value.6\n",
      "content_length": 902,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 284,
      "content": "Figure 4-5. An example of an influential data point in regression\nAnother metric is Cook’s distance, which defines influence as a combination of\nleverage and residual size. A rule of thumb is that an observation has high\ninfluence if Cook’s distance exceeds \n.\nAn influence plot or bubble plot combines standardized residuals, the hat-value,\nand Cook’s distance in a single plot. Figure 4-6 shows the influence plot for the\nKing County house data, and can be created by the following R code.\n",
      "content_length": 492,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 285,
      "content": "std_resid <- rstandard(lm_98105)\ncooks_D <- cooks.distance(lm_98105)\nhat_values <- hatvalues(lm_98105)\nplot(hat_values, std_resid, cex=10*sqrt(cooks_D))\nabline(h=c(-2.5, 2.5), lty=2)\nThere are apparently several data points that exhibit large influence in the\nregression. Cook’s distance can be computed using the function cooks.distance,\nand you can use hatvalues to compute the diagnostics. The hat values are plotted\non the x-axis, the residuals are plotted on the y-axis, and the size of the points is\nrelated to the value of Cook’s distance.\n",
      "content_length": 547,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 286,
      "content": "Figure 4-6. A plot to determine which observations have high influence\nTable 4-2 compares the regression with the full data set and with highly influential\ndata points removed. The regression coefficient for Bathrooms changes quite\ndramatically.7\nTable 4-2. Comparison of\nregression coefficients with the full\ndata and with influential data\nremoved\n",
      "content_length": 349,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 287,
      "content": "removed\nOriginal Influential removed\n(Intercept)\n–772550 –647137\nSqFtTotLiving 210\n230\nSqFtLot\n39\n33\nBathrooms\n2282\n–16132\nBedrooms\n–26320\n–22888\nBldgGrade\n130000\n114871\nFor purposes of fitting a regression that reliably predicts future data, identifying\ninfluential observations is only useful in smaller data sets. For regressions\ninvolving many records, it is unlikely that any one observation will carry\nsufficient weight to cause extreme influence on the fitted equation (although the\nregression may still have big outliers). For purposes of anomaly detection, though,\nidentifying influential observations can be very useful.\n",
      "content_length": 631,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 288,
      "content": "Heteroskedasticity, Non-Normality and Correlated Errors\nStatisticians pay considerable attention to the distribution of the residuals. It turns\nout that ordinary least squares (see “Least Squares”) are unbiased, and in some\ncases the “optimal” estimator, under a wide range of distributional assumptions.\nThis means that in most problems, data scientists do not need to be too concerned\nwith the distribution of the residuals.\nThe distribution of the residuals is relevant mainly for the validity of formal\nstatistical inference (hypothesis tests and p-values), which is of minimal\nimportance to data scientists concerned mainly with predictive accuracy. For\nformal inference to be fully valid, the residuals are assumed to be normally\ndistributed, have the same variance, and be independent. One area where this may\nbe of concern to data scientists is the standard calculation of confidence intervals\nfor predicted values, which are based upon the assumptions about the residuals\n(see “Confidence and Prediction Intervals”).\nHeteroskedasticity is the lack of constant residual variance across the range of the\npredicted values. In other words, errors are greater for some portions of the range\nthan for others. The ggplot2 package has some convenient tools to analyze\nresiduals.\nThe following code plots the absolute residuals versus the predicted values for\nthe lm_98105 regression fit in “Outliers”.\ndf <- data.frame(\n  resid = residuals(lm_98105),\n  pred = predict(lm_98105))\nggplot(df, aes(pred, abs(resid))) +\n  geom_point() +\n  geom_smooth()\nFigure 4-7 shows the resulting plot. Using geom_smooth, it is easy to superpose a\nsmooth of the absolute residuals. The function calls the loess method to produce\na visual smooth to estimate the relationship between the variables on the x-axis\nand y-axis in a scatterplot (see Scatterplot Smoothers).\n",
      "content_length": 1850,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 289,
      "content": "Figure 4-7. A plot of the absolute value of the residuals versus the predicted values\nEvidently, the variance of the residuals tends to increase for higher-valued homes,\nbut is also large for lower-valued homes. This plot indicates that lm_98105 has\nheteroskedastic errors.\n",
      "content_length": 274,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 290,
      "content": "WHY WOULD A DATA SCIENTIST CARE ABOUT\nHETEROSKEDASTICITY?\nHeteroskedasticity indicates that prediction errors differ for different ranges of the predicted\nvalue, and may suggest an incomplete model. For example, the heteroskedasticity in lm_98105\nmay indicate that the regression has left something unaccounted for in high- and low-range\nhomes.\nFigure 4-8 is a histogram of the standarized residuals for the lm_98105\nregression. The distribution has decidely longer tails than the normal distribution,\nand exhibits mild skewness toward larger residuals.\n",
      "content_length": 554,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 291,
      "content": "Figure 4-8. A histogram of the residuals from the regression of the housing data\nStatisticians may also check the assumption that the errors are independent. This is\nparticularly true for data that is collected over time. The Durbin-Watson statistic\ncan be used to detect if there is significant autocorrelation in a regression\ninvolving time series data.\nEven though a regression may violate one of the distributional assumptions, should\nwe care? Most often in data science, the interest is primarily in predictive\n",
      "content_length": 516,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 292,
      "content": "accuracy, so some review of heteroskedasticity may be in order. You may\ndiscover that there is some signal in the data that your model has not captured.\nSatisfying distributional assumptions simply for the sake of validating formal\nstatistical inference (p-values, F-statistics, etc.), however, is not that important for\nthe data scientist.\n",
      "content_length": 341,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 293,
      "content": "SCATTERPLOT SMOOTHERS\nRegression is about modeling the relationship between the response and predictor variables. In\nevaluating a regression model, it is useful to use a scatterplot smoother to visually highlight\nrelationships between two variables.\nFor example, in Figure 4-7, a smooth of the relationship between the absolute residuals and the\npredicted value shows that the variance of the residuals depends on the value of the residual. In\nthis case, the loess function was used; loess works by repeatedly fitting a series of local\nregressions to contiguous subsets to come up with a smooth. While loess is probably the most\ncommonly used smoother, other scatterplot smoothers are available in R, such as super smooth\n(supsmu) and kernal smoothing (ksmooth). For the purposes of evaluating a regression model,\nthere is typically no need to worry about the details of these scatterplot smooths.\n",
      "content_length": 898,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 294,
      "content": "Partial Residual Plots and Nonlinearity\nPartial residual plots are a way to visualize how well the estimated fit explains\nthe relationship between a predictor and the outcome. Along with detection of\noutliers, this is probably the most important diagnostic for data scientists. The\nbasic idea of a partial residual plot is to isolate the relationship between a\npredictor variable and the response, taking into account all of the other\npredictor variables. A partial residual might be thought of as a “synthetic\noutcome” value, combining the prediction based on a single predictor with the\nactual residual from the full regression equation. A partial residual for predictor \n is the ordinary residual plus the regression term associated with \n:\nwhere  is the estimated regression coefficient. The predict function in R has an\noption to return the individual regression terms \n:\nterms <- predict(lm_98105, type='terms')\npartial_resid <- resid(lm_98105) + terms\nThe partial residual plot displays the \n on the x-axis and the partial residuals on\nthe y-axis. Using ggplot2 makes it easy to superpose a smooth of the partial\nresiduals.\ndf <- data.frame(SqFtTotLiving = house_98105[, 'SqFtTotLiving'],\n                 Terms = terms[, 'SqFtTotLiving'],\n                 PartialResid = partial_resid[, 'SqFtTotLiving'])\nggplot(df, aes(SqFtTotLiving, PartialResid)) +\n  geom_point(shape=1) + scale_shape(solid = FALSE) +\n  geom_smooth(linetype=2) +\n  geom_line(aes(SqFtTotLiving, Terms))\nThe resulting plot is shown in Figure 4-9. The partial residual is an estimate of the\ncontribution that SqFtTotLiving adds to the sales price. The relationship\nbetween SqFtTotLiving and the sales price is evidently nonlinear. The\nregression line underestimates the sales price for homes less than 1,000 square\nfeet and overestimates the price for homes between 2,000 and 3,000 square feet.\n",
      "content_length": 1870,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 295,
      "content": "There are too few data points above 4,000 square feet to draw conclusions for\nthose homes.\n",
      "content_length": 91,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 296,
      "content": "Figure 4-9. A partial residual plot for the variable SqFtTotLiving\nThis nonlinearity makes sense in this case: adding 500 feet in a small home makes\na much bigger difference than adding 500 feet in a large home. This suggests that,\ninstead of a simple linear term for SqFtTotLiving, a nonlinear term should be\nconsidered (see “Polynomial and Spline Regression”).\nKEY IDEAS\nWhile outliers can cause problems for small data sets, the primary interest with outliers is to identify\n",
      "content_length": 478,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 297,
      "content": "problems with the data, or locate anomalies.\nSingle records (including regression outliers) can have a big influence on a regression equation with\nsmall data, but this effect washes out in big data.\nIf the regression model is used for formal inference (p-values and the like), then certain\nassumptions about the distribution of the residuals should be checked. In general, however, the\ndistribution of residuals is not critical in data science.\nThe partial residuals plot can be used to qualitatively assess the fit for each regression term,\npossibly leading to alternative model specification.\n",
      "content_length": 595,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 298,
      "content": "Polynomial and Spline Regression\nThe relationship between the response and a predictor variable is not necessarily\nlinear. The response to the dose of a drug is often nonlinear: doubling the dosage\ngenerally doesn’t lead to a doubled response. The demand for a product is not a\nlinear function of marketing dollars spent since, at some point, demand is likely to\nbe saturated. There are several ways that regression can be extended to capture\nthese nonlinear effects.\nKEY TERMS FOR NONLINEAR REGRESSION\nPolynomial regression\nAdds polynomial terms (squares, cubes, etc.) to a regression.\nSpline regression\nFitting a smooth curve with a series of polynomial segments.\nKnots\nValues that separate spline segments.\nGeneralized additive models\nSpline models with automated selection of knots.\nSynonyms\nGAM\n",
      "content_length": 800,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 299,
      "content": "NONLINEAR REGRESSION\nWhen statisticians talk about nonlinear regression, they are referring to models that can’t be fit\nusing least squares. What kind of models are nonlinear? Essentially all models where the\nresponse cannot be expressed as a linear combination of the predictors or some transform of the\npredictors. Nonlinear regression models are harder and computationally more intensive to fit,\nsince they require numerical optimization. For this reason, it is generally preferred to use a linear\nmodel if possible.\n",
      "content_length": 520,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 300,
      "content": "Polynomial\nPolynomial regression involves including polynomial terms to a regression\nequation. The use of polynomial regression dates back almost to the development\nof regression itself with a paper by Gergonne in 1815. For example, a quadratic\nregression between the response Y and the predictor X would take the form:\nPolynomial regression can be fit in R through the poly function. For example, the\nfollowing fits a quadratic polynomial for SqFtTotLiving with the King County\nhousing data:\nlm(AdjSalePrice ~  poly(SqFtTotLiving, 2) + SqFtLot +\n                BldgGrade +  Bathrooms +  Bedrooms,\n                    data=house_98105)\nCall:\nlm(formula = AdjSalePrice ~ poly(SqFtTotLiving, 2) + SqFtLot +\n    BldgGrade + Bathrooms + Bedrooms, data = house_98105)\nCoefficients:\n            (Intercept)  poly(SqFtTotLiving, 2)1\n             -402530.47               3271519.49\npoly(SqFtTotLiving, 2)2                  SqFtLot\n              776934.02                    32.56\n              BldgGrade                Bathrooms\n              135717.06                 -1435.12\n               Bedrooms\n               -9191.94\nThere are now two coefficients associated with SqFtTotLiving: one for the\nlinear term and one for the quadratic term.\nThe partial residual plot (see “Partial Residual Plots and Nonlinearity”) indicates\nsome curvature in the regression equation associated with SqFtTotLiving. The\nfitted line more closely matches the smooth (see “Splines”) of the partial residuals\nas compared to a linear fit (see Figure 4-10).\n",
      "content_length": 1531,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 301,
      "content": "Figure 4-10. A polynomial regression fit for the variable SqFtTotLiving (solid line) versus a smooth\n(dashed line; see the following section about splines)\n",
      "content_length": 156,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 302,
      "content": "Splines\nPolynomial regression only captures a certain amount of curvature in a nonlinear\nrelationship. Adding in higher-order terms, such as a cubic quartic polynomial,\noften leads to undesirable “wiggliness” in the regression equation. An alternative,\nand often superior, approach to modeling nonlinear relationships is to use splines.\nSplines provide a way to smoothly interpolate between fixed points. Splines were\noriginally used by draftsmen to draw a smooth curve, particularly in ship and\naircraft building.\nThe splines were created by bending a thin piece of wood using weights, referred\nto as “ducks”; see Figure 4-11.\nFigure 4-11. Splines were originally created using bendable wood and “ducks,” and were used as a\ndraftsman tool to fit curves. Photo courtesy Bob Perry.\nThe technical definition of a spline is a series of piecewise continuous\npolynomials. They were first developed during World War II at the US Aberdeen\nProving Grounds by I. J. Schoenberg, a Romanian mathematician. The polynomial\npieces are smoothly connected at a series of fixed points in a predictor variable,\nreferred to as knots. Formulation of splines is much more complicated than\npolynomial regression; statistical software usually handles the details of fitting a\nspline. The R package splines includes the function bs to create a b-spline term\nin a regression model. For example, the following adds a b-spline term to the\nhouse regression model:\n",
      "content_length": 1436,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 303,
      "content": "library(splines)\nknots <- quantile(house_98105$SqFtTotLiving, p=c(.25, .5, .75))\nlm_spline <- lm(AdjSalePrice ~ bs(SqFtTotLiving, knots=knots, degree=3) +\n  SqFtLot + Bathrooms + Bedrooms + BldgGrade,  data=house_98105)\nTwo parameters need to be specified: the degree of the polynomial and the\nlocation of the knots. In this case, the predictor SqFtTotLiving is included in the\nmodel using a cubic spline (degree=3). By default, bs places knots at the\nboundaries; in addition, knots were also placed at the lower quartile, the median\nquartile, and the upper quartile.\nIn contrast to a linear term, for which the coefficient has a direct meaning, the\ncoefficients for a spline term are not interpretable. Instead, it is more useful to use\nthe visual display to reveal the nature of the spline fit. Figure 4-12 displays the\npartial residual plot from the regression. In contrast to the polynomial model, the\nspline model more closely matches the smooth, demonstrating the greater\nflexibility of splines. In this case, the line more closely fits the data. Does this\nmean the spline regression is a better model? Not necessarily: it doesn’t make\neconomic sense that very small homes (less than 1,000 square feet) would have\nhigher value than slightly larger homes. This is possibly an artifact of a\nconfounding variable; see “Confounding Variables”.\n",
      "content_length": 1346,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 304,
      "content": "Figure 4-12. A spline regression fit for the variable SqFtTotLiving (solid line) compared to a smooth\n(dashed line)\n",
      "content_length": 116,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 305,
      "content": "Generalized Additive Models\nSuppose you suspect a nonlinear relationship between the response and a\npredictor variable, either by a priori knowledge or by examining the regression\ndiagnostics. Polynomial terms may not flexible enough to capture the relationship,\nand spline terms require specifying the knots. Generalized additive models, or\nGAM, are a technique to automatically fit a spline regression. The gam package in\nR can be used to fit a GAM model to the housing data:\nlibrary(mgcv)\nlm_gam <- gam(AdjSalePrice ~ s(SqFtTotLiving) + SqFtLot +\n                      Bathrooms +  Bedrooms + BldgGrade,\n                    data=house_98105)\nThe term s(SqFtTotLiving) tells the gam function to find the “best” knots for a\nspline term (see Figure 4-13).\n",
      "content_length": 756,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 306,
      "content": "Figure 4-13. A GAM regression fit for the variable SqFtTotLiving (solid line) compared to a smooth\n(dashed line)\nKEY IDEAS\nOutliers in a regression are records with a large residual.\nMulticollinearity can cause numerical instability in fitting the regression equation.\nA confounding variable is an important predictor that is omitted from a model and can lead to a\nregression equation with spurious relationships.\nAn interaction term between two variables is needed if the effect of one variable depends on the\n",
      "content_length": 511,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 307,
      "content": "level of the other.\nPolynomial regression can fit nonlinear relationships between predictors and the outcome variable.\nSplines are series of polynomial segments strung together, joining at knots.\nGeneralized additive models (GAM) automate the process of specifying the knots in splines.\n",
      "content_length": 287,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 308,
      "content": "Further Reading\nFor more on spline models and GAMS, see The Elements of Statistical Learning\nby Trevor Hastie, Robert Tibshirani, and Jerome Friedman, and its shorter cousin\nbased on R, An Introduction to Statistical Learning by Gareth James, Daniela\nWitten, Trevor Hastie, and Robert Tibshirani; both are Springer books.\n",
      "content_length": 322,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 309,
      "content": "Summary\nPerhaps no other statistical method has seen greater use over the years than\nregression — the process of establishing a relationship between multiple\npredictor variables and an outcome variable. The fundamental form is linear: each\npredictor variable has a coefficient that describes a linear relationship between\nthe predictor and the outcome. More advanced forms of regression, such as\npolynomial and spline regression, permit the relationship to be nonlinear. In\nclassical statistics, the emphasis is on finding a good fit to the observed data to\nexplain or describe some phenomenon, and the strength of this fit is how\ntraditional (“in-sample”) metrics are used to assess the model. In data science, by\ncontrast, the goal is typically to predict values for new data, so metrics based on\npredictive accuracy for out-of-sample data are used. Variable selection methods\nare used to reduce dimensionality and create more compact models.\nThis and subsequent sections in this chapter © 2017 Datastats, LLC, Peter Bruce and Andrew Bruce,\nused by permission.\nIn Bayesian statistics, the true value is assumed to be a random variable with a specified distribution. In the\nBayesian context, instead of estimates of unknown parameters, there are posterior and prior distributions.\nThe -1 argument in the model.matrix produces one hot encoding representation (by removing the\nintercept, hence the “-”). Otherwise, the default in R is to produce a matrix with P – 1 columns with the\nfirst factor level as a reference.\nThis is unintuitive, but can be explained by the impact of location as a confounding variable; see\n“Confounding Variables”.\nThere are 82 zip codes in King County, several with just a handful of sales. An alternative to directly using\nzip code as a factor variable, ZipGroup clusters similar zip codes into a single group. See “Factor Variables\nwith Many Levels” for details.\nThe term hat-value comes from the notion of the hat matrix in regression. Multiple linear regression can\nbe expressed by the formula \n where \n is the hat matrix. The hat-values correspond to the\ndiagonal of \n.\nThe coefficient for Bathrooms becomes negative, which is unintuitive. Location has not been taken into\naccount and the zip code 98105 contains areas of disparate types of homes. See “Confounding Variables”\nfor a discussion of confounding variables.\n1\n2\n3\n4\n5\n6\n7\n",
      "content_length": 2365,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 310,
      "content": "Chapter 5. Classification\nData scientists are often faced with a problem that requires an automated decision.\nIs an email an attempt at phishing? Is a customer likely to churn? Is the web user\nlikely to click on an advertisement? These are all classification problems.\nClassification is perhaps the most important form of prediction: the goal is to\npredict whether a record is a 0 or a 1 (phishing/not-phishing, click/don’t click,\nchurn/don’t churn), or in some cases, one of several categories (for example,\nGmail’s filtering of your inbox into “primary,” “social,” “promotional,” or\n“forums”).\nOften, we need more than a simple binary classification: we want to know the\npredicted probability that a case belongs to a class.\nRather than having a model simply assign a binary classification, most algorithms\ncan return a probability score (propensity) of belonging to the class of interest. In\nfact, with logistic regression, the default output from R is on the log-odds scale,\nand this must be transformed to a propensity. A sliding cutoff can then be used to\nconvert the propensity score to a decision. The general approach is as follows:\n1. Establish a cutoff probability for the class of interest above which we\nconsider a record as belonging to that class.\n2. Estimate (with any model) the probability that a record belongs to the\nclass of interest.\n3. If that probability is above the cutoff probability, assign the new record\nto the class of interest.\nThe higher the cutoff, the fewer records predicted as 1 — that is, belonging to the\nclass of interest. The lower the cutoff, the more records predicted as 1.\nThis chapter covers several key techniques for classification and estimating\npropensities; additional methods that can be used both for classification and\nnumerical prediction are described in the next chapter.\nMORE THAN TWO CATEGORIES?\n",
      "content_length": 1855,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 311,
      "content": "The vast majority of problems involve a binary response. Some classification problems, however, involve\na response with more than two possible outcomes. For example, at the anniversary of a customer’s\nsubscription contract, there might be three outcomes: the customer leaves, or “churns” (Y=2), goes on a\nmonth-to-month (Y=1) contract, or signs a new long-term contract (Y=0). The goal is to predict Y = j for\nj = 0, 1 or 2. Most of the classification methods in this chapter can be applied, either directly or with\nmodest adaptations, to responses that have more than two outcomes. Even in the case of more than two\noutcomes, the problem can often be recast into a series of binary problems using conditional probabilities.\nFor example, to predict the outcome of the contract, you can solve two binary prediction problems:\nPredict whether Y = 0 or Y > 0.\nGiven that Y > 0, predict whether Y = 1 or Y = 2.\nIn this case, it makes sense to break up the problem into two cases: whether the customer churns, and if\nthey don’t churn, what type of contract they will choose. From a model-fitting viewpoint, it is often\nadvantageous to convert the multiclass problem to a series of binary problems. This is particularly true\nwhen one category is much more common than the other categories.\n",
      "content_length": 1283,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 312,
      "content": "Naive Bayes\nThe naive Bayes algorithm uses the probability of observing predictor values,\ngiven an outcome, to estimate the probability of observing outcome Y = i, given a\nset of predictor values.1\nKEY TERMS FOR NAIVE BAYES\nConditional probability\nThe probability of observing some event (say X = i) given some other event (say Y = i), written as\n.\nPosterior probability\nThe probability of an outcome after the predictor information has been incorporated (in contrast to\nthe prior probability of outcomes, not taking predictor information into account).\nTo understand Bayesian classification, we can start out by imagining “non-naive”\nBayesian classification. For each record to be classified:\n1. Find all the other records with the same predictor profile (i.e., where the\npredictor values are the same).\n2. Determine what classes those records belong to and which class is most\nprevalent (i.e., probable).\n3. Assign that class to the new record.\nThe preceding approach amounts to finding all the records in the sample that are\nexactly like the new record to be classified in the sense that all the predictor\nvalues are identical.\nNOTE\nPredictor variables must be categorical (factor) variables in the standard naive Bayes algorithm.\nSee “Numeric Predictor Variables” for two workarounds for using continuous variables.\n",
      "content_length": 1320,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 313,
      "content": "Why Exact Bayesian Classification Is Impractical\nWhen the number of predictor variables exceeds a handful, many of the records to\nbe classified will be without exact matches. This can be understood in the context\nof a model to predict voting on the basis of demographic variables. Even a sizable\nsample may not contain even a single match for a new record who is a male\nHispanic with high income from the US Midwest who voted in the last election,\ndid not vote in the prior election, has three daughters and one son, and is\ndivorced. And this is just eight variables, a small number for most classification\nproblems. The addition of just a single new variable with five equally frequent\ncategories reduces the probability of a match by a factor of 5.\nWARNING\nDespite its name, naive Bayes is not considered a method of Bayesian statistics. Naive Bayes is\na data–driven, empirical method requiring relatively little statistical expertise. The name comes\nfrom the Bayes rule–like calculation in forming the predictions — specifically the initial\ncalculation of predictor value probabilities given an outcome, and then the final calculation of\noutcome probabilities.\n",
      "content_length": 1164,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 314,
      "content": "The Naive Solution\nIn the naive Bayes solution, we no longer restrict the probability calculation to\nthose records that match the record to be classified. Instead, we use the entire data\nset. The naive Bayes modification is as follows:\n1. For a binary response Y = i (i = 0 or 1), estimate the individual\nconditional probabilities for each predictor \n; these are\nthe probabilities that the predictor value is in the record when we\nobserve Y = i. This probability is estimated by the proportion of Xj\nvalues among the Y = i records in the training set.\n2. Multiply these probabilities by each other, and then by the proportion of\nrecords belonging to Y = i.\n3. Repeat steps 1 and 2 for all the classes.\n4. Estimate a probability for outcome i by taking the value calculated in step\n2 for class i and dividing it by the sum of such values for all classes.\n5. Assign the record to the class with the highest probability for this set of\npredictor values.\nThis naive Bayes algorithm can also be stated as an equation for the probability of\nobserving outcome Y = i, given a set of predictor values \n:\nThe value of \n is a scaling factor to ensure the probability\nis between 0 and 1 and does not depend on Y:\nWhy is this formula called “naive”? We have made a simplifying assumption that\nthe exact conditional probability of a vector of predictor values, given observing\nan outcome, is sufficiently well estimated by the product of the individual\nconditional probabilities \n. In other words, in estimating \n",
      "content_length": 1499,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 315,
      "content": " instead of \n, we are assuming \nis independent of all the other predictor variables \n for \n.\nSeveral packages in R can be used to estimate a naive Bayes model. The\nfollowing fits a model using the klaR package:\nlibrary(klaR)\nnaive_model <- NaiveBayes(outcome ~ purpose_ + home_ + emp_len_,\n                          data = na.omit(loan_data))\nnaive_model$table\n$purpose_\n          var\ngrouping   credit_card debt_consolidation home_improvement major_purchase\n  paid off   0.1857711          0.5523427       0.07153354     0.05541148\n  default    0.1517548          0.5777144       0.05956086     0.03708506\n          var\ngrouping      medical      other small_business\n  paid off 0.01236169 0.09958506     0.02299447\n  default  0.01434993 0.11415111     0.04538382\n$home_\n          var\ngrouping    MORTGAGE        OWN      RENT\n  paid off 0.4966286 0.08043741 0.4229340\n  default  0.4327455 0.08363589 0.4836186\n$emp_len_\n          var\ngrouping    > 1 Year   < 1 Year\n  paid off 0.9690526 0.03094744\n  default  0.9523686 0.04763140\nThe output from the model is the conditional probabilities \n. The\nmodel can be used to predict the outcome of a new loan:\nnew_loan\n        purpose_    home_  emp_len_\n1 small_business MORTGAGE  > 1 Year\nIn this case, the model predicts a default:\npredict(naive_model, new_loan)\n$class\n[1] default\nLevels: paid off default\n$posterior\n      paid off   default\n[1,] 0.3717206 0.6282794\nThe prediction also returns a posterior estimate of the probability of default.\n",
      "content_length": 1495,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 316,
      "content": "The naive Bayesian classifier is known to produce biased estimates. However,\nwhere the goal is to rank records according to the probability that Y = 1, unbiased\nestimates of probability are not needed and naive Bayes produces good results.\n",
      "content_length": 240,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 317,
      "content": "Numeric Predictor Variables\nFrom the definition, we see that the Bayesian classifier works only with\ncategorical predictors (e.g., with spam classification, where presence or absence\nof words, phrases, characters, and so on, lies at the heart of the predictive task).\nTo apply naive Bayes to numerical predictors, one of two approaches must be\ntaken:\nBin and convert the numerical predictors to categorical predictors and apply\nthe algorithm of the previous section.\nUse a probability model — for example, the normal distribution (see\n“Normal Distribution”) — to estimate the conditional probability \n.\nCAUTION\nWhen a predictor category is absent in the training data, the algorithm assigns zero probability to\nthe outcome variable in new data, rather than simply ignoring this variable and using the\ninformation from other variables, as other methods might. This is something to pay attention to\nwhen binning continuous variables.\nKEY IDEAS\nNaive Bayes works with categorical (factor) predictors and outcomes.\nIt asks, “Within each outcome category, which predictor categories are most probable?”\nThat information is then inverted to estimate probabilities of outcome categories, given predictor\nvalues.\n",
      "content_length": 1205,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 318,
      "content": "Further Reading\nElements of Statistical Learning, 2nd ed., by Trevor Hastie, Robert\nTibshirani, and Jerome Friedman (Springer, 2009).\nThere is a full chapter on naive Bayes in Data Mining for Business\nAnalytics, 3rd ed., by Galit Shmueli, Peter Bruce, and Nitin Patel (Wiley,\n2016, with variants for R, Excel, and JMP).\n",
      "content_length": 320,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 319,
      "content": "Discriminant Analysis\nDiscriminant analysis is the earliest statistical classifier; it was introduced by R.\nA. Fisher in 1936 in an article published in the Annals of Eugenics journal.2\nKEY TERMS FOR DISCRIMINANT ANALYSIS\nCovariance\nA measure of the extent to which one variable varies in concert with another (i.e., similar\nmagnitude and direction).\nDiscriminant function\nThe function that, when applied to the predictor variables, maximizes the separation of the classes.\nDiscriminant weights\nThe scores that result from the application of the discriminant function, and are used to estimate\nprobabilities of belonging to one class or another.\nWhile discriminant analysis encompasses several techniques, the most commonly\nused is linear discriminant analysis, or LDA. The original method proposed by\nFisher was actually slightly different from LDA, but the mechanics are essentially\nthe same. LDA is now less widely used with the advent of more sophisticated\ntechniques, such as tree models and logistic regression.\nHowever, you may still encounter LDA in some applications and it has links to\nother more widely used methods (such as principal components analysis; see\n“Principal Components Analysis”). In addition, discriminant analysis can provide\na measure of predictor importance, and it is used as a computationally efficient\nmethod of feature selection.\nWARNING\nLinear discriminant analysis should not be confused with Latent Dirichlet Allocation, also referred\nto as LDA. Latent Dirichlet Allocation is used in text and natural language processing and is\nunrelated to linear discriminant analysis.\n",
      "content_length": 1607,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 320,
      "content": "Covariance Matrix\nTo understand discriminant analysis, it is first necessary to introduce the concept\nof covariance between two or more variables. The covariance measures the\nrelationship between two variables  and . Denote the mean for each variable\nby \n and  (see “Mean”). The covariance \n between  and  is given by:\nwhere n is the number of records (note that we divide by n – 1 instead of n: see\n“Degrees of Freedom, and n or n – 1?”).\nAs with the correlation coefficient (see “Correlation”), positive values indicate a\npositive relationship and negative values indicate a negative relationship.\nCorrelation, however, is constrained to be between –1 and 1, whereas covariance\nis on the same scale as the variables  and . The covariance matrix \n for \nand  consists of the individual variable variances, \n and \n, on the diagonal\n(where row and column are the same variable) and the covariances between\nvariable pairs on the off-diagonals.\nNOTE\nRecall that the standard deviation is used to normalize a variable to a z-score; the covariance\nmatrix is used in a multivariate extension of this standardization process. This is known as\n",
      "content_length": 1135,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 321,
      "content": "Mahalanobis distance (see Other Distance Metrics) and is related to the LDA function.\n",
      "content_length": 86,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 322,
      "content": "Fisher’s Linear Discriminant\nFor simplicity, we focus on a classification problem in which we want to predict\na binary outcome y using just two continuous numeric variables \n.\nTechnically, discriminant analysis assumes the predictor variables are normally\ndistributed continuous variables, but, in practice, the method works well even for\nnonextreme departures from normality, and for binary predictors. Fisher’s linear\ndiscriminant distinguishes variation between groups, on the one hand, from\nvariation within groups on the other. Specifically, seeking to divide the records\ninto two groups, LDA focuses on maximizing the “between” sum of squares \n (measuring the variation between the two groups) relative to the\n“within” sum of squares \n (measuring the within-group variation). In this\ncase, the two groups correspond to the records \n for which y = 0 and the\nrecords \n for which y = 1. The method finds the linear combination \n that maximizes that sum of squares ratio.\nThe between sum of squares is the squared distance between the two group means,\nand the within sum of squares is the spread around the means within each group,\nweighted by the covariance matrix. Intuitively, by maximizing the between sum of\nsquares and minimizing the within sum of squares, this method yields the greatest\nseparation between the two groups.\n",
      "content_length": 1332,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 323,
      "content": "A Simple Example\nThe MASS package, associated with the book Modern Applied Statistics with S by\nW. N. Venables and B. D. Ripley (Springer, 1994), provides a function for LDA\nwith R. The following applies this function to a sample of loan data using two\npredictor variables, borrower_score and payment_inc_ratio, and prints out\nthe estimated linear discriminator weights.\nlibrary(MASS)\nloan_lda <- lda(outcome ~ borrower_score + payment_inc_ratio,\n                     data=loan3000)\nloan_lda$scaling\n                         LD1\nborrower_score    -6.2962811\npayment_inc_ratio  0.1288243\n",
      "content_length": 587,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 324,
      "content": "USING DISCRIMINANT ANALYSIS FOR FEATURE\nSELECTION\nIf the predictor variables are normalized prior to running LDA, the discriminator weights are\nmeasures of variable importance, thus providing a computationally efficient method of feature\nselection.\nThe lda function can predict the probability of “default” versus “paid off”:\npred <- predict(loan_lda)\nhead(pred$posterior)\n       paid off   default\n25333 0.5554293 0.4445707\n27041 0.6274352 0.3725648\n7398  0.4014055 0.5985945\n35625 0.3411242 0.6588758\n17058 0.6081592 0.3918408\n2986  0.6733245 0.3266755\nA plot of the predictions helps illustrate how LDA works. Using the output from\nthe predict function, a plot of the estimated probability of default is produced as\nfollows:\nlda_df <- cbind(loan3000, prob_default=pred$posterior[,'default'])\nggplot(data=lda_df,\n       aes(x=borrower_score, y=payment_inc_ratio, color=prob_default)) +\n  geom_point(alpha=.6) +\n  scale_color_gradient2(low='white', high='blue') +\ngeom_line(data=lda_df0, col='green', size=2, alpha=.8) +\nThe resulting plot is shown in Figure 5-1.\n",
      "content_length": 1065,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 325,
      "content": "Figure 5-1. LDA prediction of loan default using two variables: a score of the borrower’s\ncreditworthiness and the payment to income ratio.\nUsing the discriminant function weights, LDA splits the predictor space into two\nregions as shown by the solid line. The predictions farther away from the line\nhave a higher level of confidence (i.e., a probability further away from 0.5).\n",
      "content_length": 379,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 326,
      "content": "EXTENSIONS OF DISCRIMINANT ANALYSIS\nMore predictor variables: while the text and example in this section used just two predictor\nvariables, LDA works just as well with more than two predictor variables. The only limiting factor\nis the number of records (estimating the covariance matrix requires a sufficient number of\nrecords per variable, which is typically not an issue in data science applications).\nQuadratic Discriminant Analysis: There are other variants of discriminant analysis. The best\nknown is quadratic discriminant analysis (QDA). Despite its name, QDA is still a linear\ndiscriminant function. The main difference is that in LDA, the covariance matrix is assumed to be\nthe same for the two groups corresponding to Y = 0 and Y = 1. In QDA, the covariance matrix is\nallowed to be different for the two groups. In practice, the difference in most applications is not\ncritical.\nKEY IDEAS FOR DISCRIMINANT ANALYSIS\nDiscriminant analysis works with continuous or categorical predictors, as well as categorical\noutcomes.\nUsing the covariance matrix, it calculates a linear discriminant function, which is used to\ndistinguish records belonging to one class from those belonging to another.\nThis function is applied to the records to derive weights, or scores, for each record (one weight for\neach possible class) that determines its estimated class.\n",
      "content_length": 1356,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 327,
      "content": "Further Reading\nElements of Statistical Learning, 2nd ed., by Trevor Hastie, Robert\nTibshirani, Jerome Freidman, and its shorter cousin, An Introduction to\nStatistical Learning, by Gareth James, Daniela Witten, Trevor Hastie, and\nRobert Tibshirani (both from Springer). Both have a section on discriminant\nanalysis.\nData Mining for Business Analytics, 3rd ed., by Galit Shmueli, Peter Bruce,\nand Nitin Patel (Wiley, 2016, with variants for R, Excel, and JMP) has a full\nchapter on discriminant analysis.\nFor historical interest, Fisher’s original article on the topic, “The Use of\nMultiple Measurements in Taxonomic Problems,” as published in 1936 in\nAnnals of Eugenics (now called Annals of Genetics) can be found online.\n",
      "content_length": 723,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 328,
      "content": "Logistic Regression\nLogistic regression is analogous to multiple linear regression, except the outcome\nis binary. Various transformations are employed to convert the problem to one in\nwhich a linear model can be fit. Like discriminant analysis, and unlike K-Nearest\nNeighbor and naive Bayes, logistic regression is a structured model approach,\nrather than a data-centric approach. Due to its fast computational speed and its\noutput of a model that lends itself to rapid scoring of new data, it is a popular\nmethod.\nKEY TERMS FOR LOGISTIC REGRESSION\nLogit\nThe function that maps the probability of belonging to a class with a range from ± ∞ (instead of 0\nto 1).\nSynonym\nLog odds (see below)\nOdds\nThe ratio of “success” (1) to “not success” (0).\nLog odds\nThe response in the transformed model (now linear), which gets mapped back to a probability.\nHow do we get from a binary outcome variable to an outcome variable that can be\nmodeled in linear fashion, then back again to a binary outcome?\n",
      "content_length": 990,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 329,
      "content": "Logistic Response Function and Logit\nThe key ingredients are the logistic response function and the logit, in which we\nmap a probability (which is on a 0–1 scale) to a more expansive scale suitable for\nlinear modeling.\nThe first step is to think of the outcome variable not as a binary label, but as the\nprobability p that the label is a “1.” Naively, we might be tempted to model p as a\nlinear function of the predictor variables:\nHowever, fitting this model does not ensure that p will end up between 0 and 1, as\na probability must.\nInstead, we model p by applying a logistic response or inverse logit function to\nthe predictors:\nThis transform ensures that the p stays between 0 and 1.\nTo get the exponential expression out of the denominator, we consider odds\ninstead of probabilities. Odds, familiar to bettors everywhere, are the ratio of\n“successes” (1) to “nonsuccesses” (0). In terms of probabilities, odds are the\nprobability of an event divided by the probability that the event will not occur.\nFor example, if the probability that a horse will win is 0.5, the probability of\n“won’t win” is (1 – 0.5) = 0.5, and the odds are 1.0.\nWe can obtain the probability from the odds using the inverse odds function:\n",
      "content_length": 1218,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 330,
      "content": "We combine this with the logistic response function, shown earlier, to get:\nFinally, taking the logarithm of both sides, we get an expression that involves a\nlinear function of the predictors:\nThe log-odds function, also known as the logit function, maps the probability p\nfrom \n to any value \n: see Figure 5-2. The\ntransformation circle is complete; we have used a linear model to predict a\nprobability, which, in turn, we can map to a class label by applying a cutoff rule\n— any record with a probability greater than the cutoff is classified as a 1.\n",
      "content_length": 553,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 331,
      "content": "Figure 5-2. The function that maps a probability to a scale suitable for a linear model (logit)\n",
      "content_length": 96,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 332,
      "content": "Logistic Regression and the GLM\nThe response in the logistic regression formula is the log odds of a binary\noutcome of 1. We only observe the binary outcome, not the log odds, so special\nstatistical methods are needed to fit the equation. Logistic regression is a special\ninstance of a generalized linear model (GLM) developed to extend linear\nregression to other settings.\nIn R, to fit a logistic regression, the glm function is used with the family\nparameter set to binomial. The following code fits a logistic regression to the\npersonal loan data introduced in “K-Nearest Neighbors”.\nlogistic_model\nCall:  glm(formula = outcome ~ payment_inc_ratio + purpose_ + home_ +\n    emp_len_ + borrower_score, family = \"binomial\", data = loan_data)\nCoefficients:\n               (Intercept)           payment_inc_ratio\n                   1.26982                     0.08244\npurpose_debt_consolidation    purpose_home_improvement\n                   0.25216                     0.34367\n    purpose_major_purchase             purpose_medical\n                   0.24373                     0.67536\n             purpose_other      purpose_small_business\n                   0.59268                     1.21226\n                  home_OWN                   home_RENT\n                   0.03132                     0.16867\n         emp_len_ < 1 Year              borrower_score\n                   0.44489                    -4.63890\nDegrees of Freedom: 46271 Total (i.e. Null);  46260 Residual\nNull Deviance:     64150\nResidual Deviance: 58530  \nAIC: 58550\nThe response is outcome, which takes a 0 if the loan is paid off and 1 if the loan\ndefaults. purpose_ and home_ are factor variables representing the purpose of the\nloan and the home ownership status. As in regression, a factor variable with P\nlevels is represented with P – 1 columns. By default in R, the reference coding is\nused and the levels are all compared to the reference level (see “Factor Variables\nin Regression”). The reference levels for these factors are credit_card and\nMORTGAGE, respectively. The variable borrower_score is a score from 0 to 1\nrepresenting the creditworthiness of the borrower (from poor to excellent). This\nvariable was created from several other variables using K-Nearest Neighbor: see\n“KNN as a Feature Engine”.\n",
      "content_length": 2289,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 333,
      "content": "Generalized Linear Models\nGeneralized linear models (GLMs) are the second most important class of models\nbesides regression. GLMs are characterized by two main components:\nA probability distribution or family (binomial in the case of logistic\nregression)\nA link function mapping the response to the predictors (logit in the case of\nlogistic regression)\nLogistic regression is by far the most common form of GLM. A data scientist will\nencounter other types of GLMs. Sometimes a log link function is used instead of\nthe logit; in practice, use of a log link is unlikely to lead to very different results\nfor most applications. The poisson distribution is commonly used to model count\ndata (e.g., the number of times a user visits a web page in a certain amount of\ntime). Other families include negative binomial and gamma, often used to model\nelapsed time (e.g., time to failure). In contrast to logistic regression, application\nof GLMs with these models is more nuanced and involves greater care. These are\nbest avoided unless you are familiar with and understand the utility and pitfalls of\nthese methods.\n",
      "content_length": 1106,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 334,
      "content": "Predicted Values from Logistic Regression\nThe predicted value from logistic regression is in terms of the log odds: \n. The predicted probability is given by the logistic\nresponse function:\nFor example, look at the predictions from the model logistic_model:\npred <- predict(logistic_model)\nsummary(pred)\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.\n-2.728000 -0.525100 -0.005235  0.002599  0.513700  3.658000\nConverting these values to probabilities is a simple transform:\nprob <- 1/(1 + exp(-pred))\n> summary(prob)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n0.06132 0.37170 0.49870 0.50000 0.62570 0.97490\nThese are on a scale from 0 to 1 and don’t yet declare whether the predicted value\nis default or paid off. We could declare any value greater than 0.5 as default,\nanalogous to the K-Nearest Neighbors classifier. In practice, a lower cutoff is\noften appropriate if the goal is to identify members of a rare class (see “The Rare\nClass Problem”).\n",
      "content_length": 970,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 335,
      "content": "Interpreting the Coefficients and Odds Ratios\nOne advantage of logistic regression is that it produces a model that can be scored\nto new data rapidly, without recomputation. Another is the relative ease of\ninterpretation of the model, as compared with other classification methods. The\nkey conceptual idea is understanding an odds ratio. The odds ratio is easiest to\nunderstand for a binary factor variable X:\nThis is interpreted as the odds that Y = 1 when X = 1 versus the odds that Y = 1\nwhen X = 0. If the odds ratio is 2, then the odds that Y = 1 are two times higher\nwhen X = 1 versus X = 0.\nWhy bother with an odds ratio, instead of probabilities? We work with odds\nbecause the coefficient \n in the logistic regression is the log of the odds ratio for \n.\nAn example will make this more explicit. For the model fit in “Logistic\nRegression and the GLM”, the regression coefficient for\npurpose_small_business is 1.21226. This means that a loan to a small business\ncompared to a loan to pay off credit card debt reduces the odds of defaulting\nversus being paid off by \n. Clearly, loans for the\npurpose of creating or expanding a small business are considerably riskier than\nother types of loans.\nFigure 5-3 shows the relationship between the odds ratio and log-odds ratio for\nodds ratios greater than 1. Because the coefficients are on the log scale, an\nincrease of 1 in the coefficient results in an increase of \n in the\nodds ratio.\n",
      "content_length": 1437,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 336,
      "content": "Figure 5-3. The relationship between the odds ratio and the log-odds ratio\nOdds ratios for numeric variables X can be interpreted similarly: they measure the\nchange in the odds ratio for a unit change in X. For example, the effect of\nincreasing the payment to income ratio from, say, 5 to 6 increases the odds of the\nloan defaulting by a factor of \n. The variable\nborrower_score is a score on the borrowers’ creditworthiness and ranges from 0\n(low) to 1 (high). The odds of the best borrowers relative to the worst borrowers\ndefaulting on their loans is smaller by a factor of \n. In other words, the default risk from the borrowers with the poorest\ncreditworthiness is 100 times greater than that of the best borrowers!\n",
      "content_length": 720,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 337,
      "content": "Linear and Logistic Regression: Similarities and Differences\nMultiple linear regression and logistic regression share many commonalities. Both\nassume a parametric linear form relating the predictors with the response.\nExploring and finding the best model are done in very similar ways. Generalities\nto the linear model to use a spline transform of the predictor are equally\napplicable in the logistic regression setting. Logistic regression differs in two\nfundamental ways:\nThe way the model is fit (least squares is not applicable)\nThe nature and analysis of the residuals from the model\nFitting the model\nLinear regression is fit using least squares, and the quality of the fit is evaluated\nusing RMSE and R-squared statistics. In logistic regression (unlike in linear\nregression), there is no closed-form solution and the model must be fit using\nmaximum likelihood estimation (MLE). Maximum likelihood estimation is a\nprocess that tries to find the model that is most likely to have produced the data\nwe see. In the logistic regression equation, the response is not 0 or 1 but rather an\nestimate of the log odds that the response is 1. The MLE finds the solution such\nthat the estimated log odds best describes the observed outcome. The mechanics\nof the algorithm involve a quasi-Newton optimization that iterates between a\nscoring step (Fisher’s scoring), based on the current parameters, and an update to\nthe parameters to improve the fit.\nMAXIMUM LIKELIHOOD ESTIMATION\nMore detail, if you like statistical symbols: start with a set of data \n and a\nprobability model \n that depends on a set of parameters \n. The goal of\nMLE is to find the set of parameters  that maximizes the value of \n; that\nis, it maximizes the probability of observing \n given the model ..] In the\nfitting process, the model is evaluated using a metric called deviance:\n",
      "content_length": 1846,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 338,
      "content": "Lower deviance corresponds to a better fit.\nFortunately, most users don’t need to concern themselves with the details of the\nfitting algorithm since this is handled by the software. Most data scientists will not\nneed to worry about the fitting method, other than understanding that it is a way to\nfind a good model under certain assumptions.\n",
      "content_length": 342,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 339,
      "content": "HANDLING FACTOR VARIABLES\nIn logistic regression, factor variables should be coded as in linear regression; see “Factor\nVariables in Regression”. In R and other software, this is normally handled automatically and\ngenerally reference encoding is used. All of the other classification methods covered in this\nchapter typically use the one hot encoder representation (see “One Hot Encoder”).\n",
      "content_length": 390,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 340,
      "content": "Assessing the Model\nLike other classification methods, logistic regression is assessed by how\naccurately the model classifies new data (see “Evaluating Classification\nModels”). As with linear regression, some additional standard statistical tools are\navailable to assess and improve the model. Along with the estimated coefficients,\nR reports the standard error of the coefficients (SE), a z-value, and a p-value:\nsummary(logistic_model)\nCall:\nglm(formula = outcome ~ payment_inc_ratio + purpose_ + home_ +\n    emp_len_ + borrower_score, family = \"binomial\", data = loan_data)\nDeviance Residuals:\n     Min        1Q    Median        3Q       Max\n-2.71430  -1.06806  -0.04482   1.07446   2.11672\nCoefficients:\n                            Estimate Std. Error z value Pr(>|z|)\n(Intercept)                 1.269822   0.051929  24.453  < 2e-16 ***\npayment_inc_ratio           0.082443   0.002485  33.177  < 2e-16 ***\npurpose_debt_consolidation  0.252164   0.027409   9.200  < 2e-16 ***\npurpose_home_improvement    0.343674   0.045951   7.479 7.48e-14 ***\npurpose_major_purchase      0.243728   0.053314   4.572 4.84e-06 ***\npurpose_medical             0.675362   0.089803   7.520 5.46e-14 ***\npurpose_other               0.592678   0.039109  15.154  < 2e-16 ***\npurpose_small_business      1.212264   0.062457  19.410  < 2e-16 ***\nhome_OWN                    0.031320   0.037479   0.836    0.403\nhome_RENT                   0.168670   0.021041   8.016 1.09e-15 ***\nemp_len_ < 1 Year           0.444892   0.053342   8.340  < 2e-16 ***\nborrower_score             -4.638902   0.082433 -56.275  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Dispersion parameter for binomial family taken to be 1)\n    Null deviance: 64147  on 46271  degrees of freedom\nResidual deviance: 58531  on 46260  degrees of freedom\nAIC: 58555\nNumber of Fisher Scoring iterations: 4\nInterpretation of the p-value comes with the same caveat as in regression, and\nshould be viewed more as a relative indicator of variable importance (see\n“Assessing the Model”) than as a formal measure of statistical significance. A\nlogistic regression model, which has a binary response, does not have an\nassociated RMSE or R-squared. Instead, a logistic regression model is typically\nevaluated using more general metrics for classification; see “Evaluating\nClassification Models”.\n",
      "content_length": 2364,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 341,
      "content": "Many other concepts for linear regression carry over to the logistic regression\nsetting (and other GLMs). For example, you can use stepwise regression, fit\ninteraction terms, or include spline terms. The same concerns regarding\nconfounding and correlated variables apply to logistic regression (see\n“Interpreting the Regression Equation”). You can fit generalized additive models\n(see “Generalized Additive Models”) using the mgcv package:\nlogistic_gam <- gam(outcome ~ s(payment_inc_ratio) + purpose_ +\n                        home_ + emp_len_ + s(borrower_score),\n                      data=loan_data, family='binomial')\nOne area where logistic regression differs is in the analysis of the residuals. As in\nregression (see Figure 4-9), it is straightforward to compute partial residuals:\nterms <- predict(logistic_gam, type='terms')\npartial_resid <- resid(logistic_model) + terms\ndf <- data.frame(payment_inc_ratio = loan_data[, 'payment_inc_ratio'],\n                 terms = terms[, 's(payment_inc_ratio)'],\n                 partial_resid = partial_resid[, 's(payment_inc_ratio)'])\nggplot(df, aes(x=payment_inc_ratio, y=partial_resid, solid = FALSE)) +\n  geom_point(shape=46, alpha=.4) +\n  geom_line(aes(x=payment_inc_ratio, y=terms),\n            color='red', alpha=.5, size=1.5) +\n  labs(y='Partial Residual')\nThe resulting plot is displayed in Figure 5-4. The estimated fit, shown by the line,\ngoes between two sets of point clouds. The top cloud corresponds to a response\nof 1 (defaulted loans), and the bottom cloud corresponds to a response of 0 (loans\npaid off). This is very typical of residuals from a logistic regression since the\noutput is binary. Partial residuals in logistic regression, while less valuable than\nin regression, are still useful to confirm nonlinear behavior and identify highly\ninfluential records.\n",
      "content_length": 1831,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 342,
      "content": "Figure 5-4. Partial residuals from logistic regression\nWARNING\nSome of the output from the summary function can effectively be ignored. The dispersion\nparameter does not apply to logistic regression and is there for other types of GLMs. The residual\ndeviance and the number of scoring iterations are related to the maximum likelihood fitting\nmethod; see “Maximum Likelihood Estimation”.\nKEY IDEAS FOR LOGISTIC REGRESSION\nLogistic regression is like linear regression, except that the outcome is a binary variable.\nSeveral transformations are needed to get the model into a form that can be fit as a linear model,\nwith the log of the odds ratio as the response variable.\n",
      "content_length": 670,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 343,
      "content": "After the linear model is fit (by an iterative process), the log odds is mapped back to a probability.\nLogistic regression is popular because it is computationally fast, and produces a model that can be\nscored to new data without recomputation.\n",
      "content_length": 245,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 344,
      "content": "Further Reading\n1. The standard reference on logistic regression is Applied Logistic\nRegression, 3rd ed., by David Hosmer, Stanley Lemeshow, and Rodney\nSturdivant (Wiley).\n2. Also popular are two books by Joseph Hilbe: Logistic Regression\nModels (very comprehensive) and Practical Guide to Logistic\nRegression (compact), both from CRC Press.\n3. Elements of Statistical Learning, 2nd ed., by Trevor Hastie, Robert\nTibshirani, Jerome Freidman, and its shorter cousin, An Introduction to\nStatistical Learning, by Gareth James, Daniela Witten, Trevor Hastie,\nand Robert Tibshirani (both from Springer) both have a section on\nlogistic regression.\n4. Data Mining for Business Analytics, 3rd ed., by Galit Shmueli, Peter\nBruce, and Nitin Patel (Wiley, 2016, with variants for R, Excel, and\nJMP) has a full chapter on logistic regression.\n",
      "content_length": 831,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 345,
      "content": "Evaluating Classification Models\nIt is common in predictive modeling to try out a number of different models, apply\neach to a holdout sample (also called a test or validation sample), and assess\ntheir performance. Fundamentally, this amounts to seeing which produces the most\naccurate predictions.\nKEY TERMS FOR EVALUATING CLASSIFICATION MODELS\nAccuracy\nThe percent (or proportion) of cases classified correctly.\nConfusion matrix\nA tabular display (2×2 in the binary case) of the record counts by their predicted and actual\nclassification status.\nSensitivity\nThe percent (or proportion) of 1s correctly classified.\nSynonym\nRecall\nSpecificity\nThe percent (or proportion) of 0s correctly classified.\nPrecision\nThe percent (proportion) of predicted 1s that are actually 1s.\nROC curve\nA plot of sensitivity versus specificity.\nLift\nA measure of how effective the model is at identifying (comparitively rare) 1s at different\nprobability cutoffs.\nA simple way to measure classification performance is to count the proportion of\npredictions that are correct.\nIn most classification algorithms, each case is assigned an “estimated probability\nof being a 1.”3 The default decision point, or cutoff, is typically 0.50 or 50%. If\nthe probability is above 0.5, the classification is “1,” otherwise it is “0.” An\nalternative default cutoff is the prevalent probability of 1s in the data.\n",
      "content_length": 1375,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 346,
      "content": "Accuracy is simply a measure of total error:\n",
      "content_length": 45,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 347,
      "content": "Confusion Matrix\nAt the heart of classification metrics is the confusion matrix. The confusion\nmatrix is a table showing the number of correct and incorrect predictions\ncategorized by type of response. Several packages are available in R to compute a\nconfusion matrix, but in the binary case, it is simple to compute one by hand.\nTo illustrate the confusion matrix, consider the logistic_gam model that was\ntrained on a balanced data set with an equal number of defaulted and paid-off\nloans (see Figure 5-4). Following the usual conventions Y = 1 corresponds to the\nevent of interest (e.g., default) and Y = 0 corresponds to a negative (or usual)\nevent (e.g., paid off). The following computes the confusion matrix for the\nlogistic_gam model applied to the entire (unbalanced) training set:\npred <- predict(logistic_gam, newdata=train_set)\npred_y <- as.numeric(pred > 0)\ntrue_y <- as.numeric(train_set$outcome=='default')\ntrue_pos <- (true_y==1) & (pred_y==1)\ntrue_neg <- (true_y==0) & (pred_y==0)\nfalse_pos <- (true_y==0) & (pred_y==1)\nfalse_neg <- (true_y==1) & (pred_y==0)\nconf_mat <- matrix(c(sum(true_pos), sum(false_pos),\n                     sum(false_neg), sum(true_neg)), 2, 2)\ncolnames(conf_mat) <- c('Yhat = 1', 'Yhat = 0')\nrownames(conf_mat) <- c('Y = 1', 'Y = 0')\nconf_mat\n      Yhat = 1 Yhat = 0\nY = 1 14635    8501\nY = 0 8236     14900\nThe predicted outcomes are columns and the true outcomes are the rows. The\ndiagonal elements of the matrix show the number of correct predictions and the\noff-diagonal elements show the number of incorrect predictions. For example,\n6,126 defaulted loans were correctly predicted as a default, but 17,010 defaulted\nloans were incorrectly predicted as paid off.\nFigure 5-5 shows the relationship between the confusion matrix for a binary\nreponse Y and different metrics (see “Precision, Recall, and Specificity” for more\non the metrics). As with the example for the loan data, the actual response is along\nthe rows and the predicted response is along the columns. (You may see confusion\nmatrices with this reversed.) The diagonal boxes (upper left, lower right) show\nwhen the predictions  correctly predict the response. One important metric not\n",
      "content_length": 2194,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 348,
      "content": "explicitly called out is the false positive rate (the mirror image of precision).\nWhen 1s are rare, the ratio of false positives to all predicted positives can be\nhigh, leading to the unintuitive situation where a predicted 1 is most likely a 0.\nThis problem plagues medical screening tests (e.g., mammograms) that are widely\napplied: due to the relative rarity of the condition, positive test results most likely\ndo not mean breast cancer. This leads to much confusion in the public.\nFigure 5-5. Confusion matrix for a binary response and various metrics\n",
      "content_length": 556,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 349,
      "content": "The Rare Class Problem\nIn many cases, there is an imbalance in the classes to be predicted, with one class\nmuch more prevalent than the other — for example, legitimate insurance claims\nversus fraudulent ones, or browsers versus purchasers at a website. The rare class\n(e.g., the fraudulent claims) is usually the class of more interest, and is typically\ndesignated 1, in contrast to the more prevalent 0s. In the typical scenario, the 1s\nare the more important case, in the sense that misclassifying them as 0s is costlier\nthan misclassfying 0s as 1s. For example, correctly identifying a fraudulent\ninsurance claim may save thousands of dollars. On the other hand, correctly\nidentifying a nonfraudulent claim merely saves you the cost and effort of going\nthrough the claim by hand with a more careful review (which is what you would\ndo if the claim were tagged as “fraudulent”).\nIn such cases, unless the classes are easily separable, the most accurate\nclassification model may be one that simply classifies everything as a 0. For\nexample, if only 0.1% of the browsers at a web store end up purchasing, a model\nthat predicts that each browser will leave without purchasing will be 99.9%\naccurate. However, it will be useless. Instead, we would be happy with a model\nthat is less accurate overall, but is good at picking out the purchasers, even if it\nmisclassifies some nonpurchasers along the way.\n",
      "content_length": 1400,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 350,
      "content": "Precision, Recall, and Specificity\nMetrics other than pure accuracy — metrics that are more nuanced — are\ncommonly used in evaluating classification models. Several of these have a long\nhistory in statistics — especially biostatistics, where they are used to describe the\nexpected performance of diagnostic tests. The precision measures the accuracy of\na predicted positive outcome (see Figure 5-5):\nThe recall, also known as sensitivity, measures the strength of the model to\npredict a positive outcome — the proportion of the 1s that it correctly identifies\n(see Figure 5-5). The term sensitivity is used a lot in biostatistics and medical\ndiagnostics, whereas recall is used more in the machine learning community. The\ndefinition of recall is:\nAnother metric used is specificity, which measures a model’s ability to predict a\nnegative outcome:\n# precision\nconf_mat[1,1]/sum(conf_mat[,1])\n# recall\nconf_mat[1,1]/sum(conf_mat[1,])\n# specificity\nconf_mat[2,2]/sum(conf_mat[2,])\n",
      "content_length": 978,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 351,
      "content": "ROC Curve\nYou can see that there is a tradeoff between recall and specificity. Capturing more\n1s generally means misclassifying more 0s as 1s. The ideal classifier would do an\nexcellent job of classifying the 1s, without misclassifying more 0s as 1s.\nThe metric that captures this tradeoff is the “Receiver Operating Characteristics”\ncurve, usually referred to as the ROC curve. The ROC curve plots recall\n(sensitivity) on the y-axis against specificity on the x-axis.4 The ROC curve shows\nthe trade-off between recall and specificity as you change the cutoff to determine\nhow to classify a record. Sensitivity (recall) is plotted on the y-axis, and you may\nencounter two forms in which the x-axis is labeled:\nSpecificity plotted on the x-axis, with 1 on the left and 0 on the right\nSpecificity plotted on the x-axis, with 0 on the left and 1 on the right\nThe curve looks identical whichever way it is done. The process to compute the\nROC curve is:\n1. Sort the records by the predicted probability of being a 1, starting with\nthe most probable and ending with the least probable.\n2. Compute the cumulative specificity and recall based on the sorted\nrecords.\nComputing the ROC curve in R is straightforward. The following code computes\nROC for the loan data:\nidx <- order(-pred)\nrecall <- cumsum(true_y[idx]==1)/sum(true_y==1)\nspecificity <- (sum(true_y==0) - cumsum(true_y[idx]==0))/sum(true_y==0)\nroc_df <- data.frame(recall = recall, specificity = specificity)\nggplot(roc_df, aes(x=specificity, y=recall)) +\n  geom_line(color='blue') +\n  scale_x_reverse(expand=c(0, 0)) +\n  scale_y_continuous(expand=c(0, 0)) +\n  geom_line(data=data.frame(x=(0:100)/100), aes(x=x, y=1-x),\n            linetype='dotted', color='red')\nThe result is shown in Figure 5-6. The dotted diagonal line corresponds to a\nclassifier no better than random chance. An extremely effective classifier (or, in\nmedical situations, an extremely effective diagnostic test) will have an ROC that\n",
      "content_length": 1960,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 352,
      "content": "hugs the upper-left corner — it will correctly identify lots of 1s without\nmisclassifying lots of 0s as 1s. For this model, if we want a classifier with a\nspecificity of at least 50%, then the recall is about 75%.\n",
      "content_length": 214,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 353,
      "content": "Figure 5-6. ROC curve for the loan data\n",
      "content_length": 40,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 354,
      "content": "PRECISION-RECALL CURVE\nIn addition to ROC curves, it can be illuminating to examine the precision-recall (PR) curve. PR\ncurves are computed in a similar way except that the data is ordered from least to most probable\nand cumulative precision and recall statistics are computed. PR curves are especially useful in\nevaluating data with highly unbalanced outcomes.\n",
      "content_length": 362,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 355,
      "content": "AUC\nThe ROC curve is a valuable graphical tool but, by itself, doesn’t constitute a\nsingle measure for the performance of a classifier. The ROC curve can be used,\nhowever, to produce the area underneath the curve (AUC) metric. AUC is simply\nthe total area under the ROC curve. The larger the value of AUC, the more\neffective the classifier. An AUC of 1 indicates a perfect classifier: it gets all the\n1s correctly classified, and doesn’t misclassify any 0s as 1s.\nA completely ineffective classifier — the diagonal line — will have an AUC of\n0.5.\nFigure 5-7 shows the area under the ROC curve for the loan model. The value of\nAUC can be computed by a numerical integration:\nsum(roc_df$recall[-1] * diff(1-roc_df$specificity))\n[1] 0.5924072\nThe model has an AUC of about 0.59, corresponding to a relatively weak\nclassifier.\n",
      "content_length": 823,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 356,
      "content": "Figure 5-7. Area under the ROC curve for the loan data\n",
      "content_length": 55,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 357,
      "content": "FALSE POSITIVE RATE CONFUSION\nFalse positive/negative rates are often confused or conflated with specificity or sensitivity (even\nin publications and software!). Sometimes the false positive rate is defined as the proportion of\ntrue negatives that test positive. In many cases (such as network intrusion detection), the term is\nused to refer to the proportion of positive signals that are true negatives.\n",
      "content_length": 405,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 358,
      "content": "Lift\nUsing the AUC as a metric is an improvement over simple accuracy, as it can\nassess how well a classifier handles the tradeoff between overall accuracy and\nthe need to identify the more important 1s. But it does not completely address the\nrare-case problem, where you need to lower the model’s probability cutoff below\n0.5 to avoid having all records classified as 0. In such cases, for a record to be\nclassified as a 1, it might be sufficient to have a probability of 0.4, 0.3, or lower.\nIn effect, we end up overidentifying 1s, reflecting their greater importance.\nChanging this cutoff will improve your chances of catching the 1s (at the cost of\nmisclassifying more 0s as 1s). But what is the optimum cutoff?\nThe concept of lift lets you defer answering that question. Instead, you consider\nthe records in order of their predicted probability of being 1s. Say, of the top 10%\nclassified as 1s, how much better did the algorithm do, compared to the\nbenchmark of simply picking blindly? If you can get 0.3% response in this top\ndecile instead of the 0.1% you get overall picking randomly, the algorithm is said\nto have a lift (also called gains) of 3 in the top decile. A lift chart (gains chart)\nquantifies this over the range of the data. It can be produced decile by decile, or\ncontinuously over the range of the data.\nTo compute a lift chart, you first produce a cumulative gains chart that shows the\nrecall on the y-axis and the total number of records on the x-axis. The lift curve is\nthe ratio of the cumulative gains to the diagonal line corresponding to random\nselection. Decile gains charts are one of the oldest techniques in predictive\nmodeling, dating from the days before internet commerce. They were particularly\npopular among direct mail professionals. Direct mail is an expensive method of\nadvertising if applied indiscriminantly, and advertisers used predictive models\n(quite simple ones, in the early days) to identify the potential customers with the\nlikeliest prospect of payoff.\n",
      "content_length": 2006,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 359,
      "content": "UPLIFT\nSometimes the term uplift is used to mean the same thing as lift. An alternate meaning is used in\na more restrictive setting, when an A-B test has been conducted and the treatment (A or B) is\nthen used as a predictor variable in a predictive model. The uplift is the improvement in response\npredicted for an individual case with treatment A versus treatment B. This is determined by\nscoring the individual case first with the predictor set to A, and then again with the predictor\ntoggled to B. Marketers and political campaign consultants use this method to determine which of\ntwo messaging treatments should be used with which customers or voters.\nA lift curve lets you look at the consequences of setting different probability\ncutoffs for classifying records as 1s. It can be an intermediate step in settling on an\nappropriate cutoff level. For example, a tax authority might only have a certain\namount of resources that it can spend on tax audits, and want to spend them on the\nlikeliest tax cheats. With its resource constraint in mind, the authority would use a\nlift chart to estimate where to draw the line between tax returns selected for audit\nand those left alone.\nKEY IDEAS FOR EVALUATING CLASSIFICATION MODELS\nAccuracy (the percent of predicted classifications that are correct) is but a first step in evaluating a\nmodel.\nOther metrics (recall, specificity, precision) focus on more specific performance characteristics\n(e.g., recall measures how good a model is at correctly identifying 1s).\nAUC (area under the ROC curve) is a common metric for the ability of a model to distinguish 1s\nfrom 0s.\nSimilarly, lift measures how effective a model is in identifying the 1s, and it is often calculated decile\nby decile, starting with the most probable 1s.\n",
      "content_length": 1769,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 360,
      "content": "Further Reading\nEvaluation and assessment are typically covered in the context of a particular\nmodel (e.g., K-Nearest Neighbors or decision trees); three books that handle it in\nits own chapter are:\nData Mining, 3rd ed., by Ian Whitten, Elbe Frank, and Mark Hall (Morgan\nKaufmann, 2011).\nModern Data Science with R by Benjamin Baumer, Daniel Kaplan, and\nNicholas Horton (CRC Press, 2017).\nData Mining for Business Analytics, 3rd ed., by Galit Shmueli, Peter Bruce,\nand Nitin Patel (Wiley, 2016, with variants for R, Excel, and JMP).\nAn excellent treatment of cross-validation and resampling can be found in:\nAn Introduction to Statistical Learning by Gareth James, et al. (Springer,\n2013).\n",
      "content_length": 690,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 361,
      "content": "Strategies for Imbalanced Data\nThe previous section dealt with evaluation of classification models using metrics\nthat go beyond simple accuracy, and are suitable for imbalanced data — data in\nwhich the outcome of interest (purchase on a website, insurance fraud, etc.) is\nrare. In this section, we look at additional strategies that can improve predictive\nmodeling performance with imbalanced data.\nKEY TERMS FOR IMBALANCED DATA\nUndersample\nUse fewer of the prevalent class records in the classification model.\nSynonym\nDownsample\nOversample\nUse more of the rare class records in the classification model, bootstrapping if necessary.\nSynonym\nUpsample\nUp weight or down weight\nAttach more (or less) weight to the rare (or prevalent) class in the model.\nData generation\nLike bootstrapping, except each new bootstrapped record is slightly different from its source.\nZ-score\nThe value that results after standardization.\nK\nThe number of neighbors considered in the nearest neighbor calculation.\n",
      "content_length": 990,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 362,
      "content": "Undersampling\nIf you have enough data, as is the case with the loan data, one solution is to\nundersample (or downsample) the prevalent class, so the data to be modeled is\nmore balanced between 0s and 1s. The basic idea in undersampling is that the data\nfor the dominant class has many redundant records. Dealing with a smaller, more\nbalanced data set yields benefits in model performance, and makes it easier to\nprepare the data, and to explore and pilot models.\nHow much data is enough? It depends on the application, but in general, having\ntens of thousands of records for the less dominant class is enough. The more easily\ndistinguishable the 1s are from the 0s, the less data needed.\nThe loan data analyzed in “Logistic Regression” was based on a balanced training\nset: half of the loans were paid off and the other half were in default. The\npredicted values were similar: half of the probabilities were less than 0.5 and half\nwere greater than 0.5. In the full data set, only about 5% of the loans were in\ndefault:\nmean(loan_all_data$outcome == 'default')\n[1] 0.05024048\nWhat happens if we use the full data set to train the model?\nfull_model <- glm(outcome ~ payment_inc_ratio + purpose_ +\n                        home_ + emp_len_+ dti + revol_bal + revol_util,\n                      data=train_set, family='binomial')\npred <- predict(full_model)\nmean(pred > 0)\n[1] 0.00386009\nOnly 0.39% of the loans are predicted to be in default, or less than 1/12 of the\nexpected number. The loans that were paid off overwhelm the loans in default\nbecause the model is trained using all the data equally. Thinking about it\nintuitively, the presence of so many nondefaulting loans, coupled with the\ninevitable variability in predictor data, means that, even for a defaulting loan, the\nmodel is likely to find some nondefaulting loans that it is similar to, by chance.\nWhen a balanced sample was used, roughly 50% of the loans were predicted to be\nin default.\n",
      "content_length": 1951,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 363,
      "content": "Oversampling and Up/Down Weighting\nOne criticism of the undersampling method is that it throws away data and is not\nusing all the information at hand. If you have a relatively small data set, and the\nrarer class contains a few hundred or a few thousand records, then undersampling\nthe dominant class has the risk of throwing out useful information. In this case,\ninstead of downsampling the dominant case, you should oversample (upsample)\nthe rarer class by drawing additional rows with replacement (bootstrapping).\nYou can achieve a similar effect by weighting the data. Many classification\nalgorithms take a weight argument that will allow you to up/down weight the data.\nFor example, apply a weight vector to the loan data using the weight argument to\nglm:\nwt <- ifelse(loan_all_data$outcome=='default',\n             1/mean(loan_all_data$outcome == 'default'), 1)\nfull_model <- glm(outcome ~ payment_inc_ratio + purpose_ +\n                    home_ + emp_len_+ dti + revol_bal + revol_util,\n                  data=loan_all_data, weight=wt, family='binomial')\npred <- predict(full_model)\nmean(pred > 0)\n[1] 0.4344177\nThe weights for loans that default are set to  where p is the probability of\ndefault. The nondefaulting loans have a weight of 1. The sum of the weights for the\ndefaulted loans and nondefaulted loans are roughly equal. The mean of the\npredicted values is now 43% instead of 0.39%.\nNote that weighting provides an alternative to both upsampling the rarer class and\ndownsampling the dominant class.\n",
      "content_length": 1516,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 364,
      "content": "ADAPTING THE LOSS FUNCTION\nMany classification and regression algorithms optimize a certain criteria or loss function. For\nexample, logistic regression attempts to minimize the deviance. In the literature, some propose to\nmodify the loss function in order to avoid the problems caused by a rare class. In practice, this is\nhard to do: classification algorithms can be complex and difficult to modify. Weighting is an easy\nway to change the loss function, discounting errors for records with low weights in favor of\nrecords of higher weights.\n",
      "content_length": 542,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 365,
      "content": "Data Generation\nA variation of upsampling via bootstrapping (see “Undersampling”) is data\ngeneration by perturbing existing records to create new records. The intuition\nbehind this idea is that since we only observe a limited set of instances, the\nalgorithm doesn’t have a rich set of information to build classification “rules.” By\ncreating new records that are similar but not identical to existing records, the\nalgorithm has a chance to learn a more robust set of rules. This notion is similar in\nspirit to ensemble statistical models such as boosting and bagging (see Chapter 6).\nThe idea gained traction with the publication of the SMOTE algorithm, which\nstands for “Synthetic Minority Oversampling Technique.” The SMOTE algorithm\nfinds a record that is similar to the record being upsampled (see “K-Nearest\nNeighbors”) and creates a synthetic record that is a randomly weighted average of\nthe original record and the neighboring record, where the weight is generated\nseparately for each predictor. The number of synthetic oversampled records\ncreated depends on the oversampling ratio required to bring the data set into\napproximate balance, with respect to outcome classes.\nThere are several implementations of SMOTE in R. The most comprehensive\npackage for handling unbalanced data is unbalanced. It offers a variety of\ntechniques, including a “Racing” algorithm to select the best method. However,\nthe SMOTE algorithm is simple enough that it can be implemented directly in R\nusing the knn package.\n",
      "content_length": 1507,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 366,
      "content": "Cost-Based Classification\nIn practice, accuracy and AUC are a poor man’s way to choose a classification\nrule. Often, an estimated cost can be assigned to false positives versus false\nnegatives, and it is more appropriate to incorporate these costs to determine the\nbest cutoff when classifying 1s and 0s. For example, suppose the expected cost of\na default of a new loan is \n and the expected return from a paid-off loan is \n.\nThen the expected return for that loan is:\nInstead of simply labeling a loan as default or paid off, or determining the\nprobability of default, it makes more sense to determine if the loan has a positive\nexpected return. Predicted probability of default is an intermediate step, and it\nmust be combined with the loan’s total value to determine expected profit, which\nis the ultimate planning metric of business. For example, a smaller value loan\nmight be passed over in favor of a larger one with a slightly higher predicted\ndefault probability.\n",
      "content_length": 973,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 367,
      "content": "Exploring the Predictions\nA single metric, such as AUC, cannot capture all aspects of the appropriateness of\na model for a situation. Figure 5-8 displays the decision rules for four different\nmodels fit to the loan data using just two predictor variables: borrower_score\nand payment_inc_ratio. The models are linear discriminant analysis (LDA),\nlogistic linear regression, logistic regression fit using a generalized additive\nmodel (GAM) and a tree model (see “Tree Models”). The region to the upper-left\nof the lines corresponds to a predicted default. LDA and logistic linear regression\ngive nearly identical results in this case. The tree model produces the least regular\nrule: in fact, there are situations in which increasing the borrower score shifts the\nprediction from “paid-off” to “default”! Finally, the GAM fit of the logistic\nregression represents a compromise between the tree models and the linear\nmodels.\n",
      "content_length": 921,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 368,
      "content": "Figure 5-8. Comparison of the classification rules for four different methods\nIt is not easy to visualize the prediction rules in higher dimensions, or in the case\nof the GAM and tree model, even generate the regions for such rules.\nIn any case, exploratory analysis of predicted values is always warranted.\nKEY IDEAS FOR IMBALANCED DATA STRATEGIES\nHighly imbalanced data (i.e., where the interesting outcomes, the 1s, are rare) are problematic for\nclassification algorithms.\nOne strategy is to balance the training data via undersampling the abundant case (or oversampling\nthe rare case).\nIf using all the 1s still leaves you with too few 1s, you can bootstrap the rare cases, or use SMOTE\nto create synthetic data similar to existing rare cases.\nImbalanced data usually indicates that correctly classifying one class (the 1s) has higher value, and\nthat value ratio should be built into the assessment metric.\n",
      "content_length": 911,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 369,
      "content": "Further Reading\nTom Fawcett, author of Data Science for Business, has a good article on\nimbalanced classes.\nFor more on SMOTE, see Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.\nHall, and W. Philip Kegelmeyer, “SMOTE: Synthetic Minority Over-\nsampling Technique,” Journal of Artificial Intelligence Research 16 (2002):\n321–357.\nAlso see the Analytics Vidya Content Team’s “Practical Guide to deal with\nImbalanced Classification Problems in R,” March 28, 2016.\n",
      "content_length": 460,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 370,
      "content": "Summary\nClassification, the process of predicting which of two (or a small number of)\ncategories a record belongs to, is a fundamental tool of predictive analytics. Will\na loan default (yes or no)? Will it prepay? Will a web visitor click on a link? Will\nshe purchase something? Is an insurance claim fraudulent? Often in classification\nproblems, one class is of primary interest (e.g., the fraudulent insurance claim)\nand, in binary classification, this class is designated as a 1, with the other, more\nprevalent class being a 0. Often, a key part of the process is estimating a\npropensity score, a probability of belonging to the class of interest. A common\nscenario is one in which the class of interest is relatively rare. The chapter\nconcludes with a discussion of a variety of model assessment metrics that go\nbeyond simple accuracy; these are important in the rare-class situation, when\nclassifying all records as 0s can yield high accuracy.\nThis and subsequent sections in this chapter © 2017 Datastats, LLC, Peter Bruce and Andrew Bruce,\nused by permission.\nIt is certainly surprising that the first article on statistical classification was published in a journal devoted to\neugenics. Indeed, there is a disconcerting connection between the early development of statistics and\neugenics.\nNot all methods provide unbiased estimates of probability. In most cases, it is sufficient that the method\nprovide a ranking equivalent to the rankings that would result from an unbiased probability estimate; the\ncutoff method is then functionally equivalent.\nThe ROC curve was first used during World War II to describe the performance of radar receiving\nstations, whose job was to correctly identify (classify) reflected radar signals, and alert defense forces to\nincoming aircraft.\n1\n2\n3\n4\n",
      "content_length": 1790,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 371,
      "content": "Chapter 6. Statistical Machine\nLearning\nRecent advances in statistics have been devoted to developing more powerful\nautomated techniques for predictive modeling — both regression and\nclassification. These methods fall under the umbrella of statistical machine\nlearning, and are distinguished from classical statistical methods in that they are\ndata-driven and do not seek to impose linear or other overall structure on the data.\nThe K-Nearest Neighbors method, for example, is quite simple: classify a record\nin accordance with how similar records are classified. The most successful and\nwidely used techniques are based on ensemble learning applied to decision trees.\nThe basic idea of ensemble learning is to use many models to form a prediction as\nopposed to just a single model. Decision trees are a flexible and automatic\ntechnique to learn rules about the relationships between predictor variables and\noutcome variables. It turns out that the combination of ensemble learning with\ndecision trees leads to the top-performing off-the-shelf predictive modeling\ntechniques.\nThe development of many of the techniques in statistical machine learning can be\ntraced back to the statisticians Leo Breiman (see Figure 6-1) at the University of\nCalifornia at Berkeley and Jerry Friedman at Stanford University. Their work,\nalong with other researchers at Berkeley and Stanford, started with the\ndevelopment of tree models in 1984. The subsequent development of ensemble\nmethods of bagging and boosting in the 1990s established the foundation of\nstatistical machine learning.\n",
      "content_length": 1570,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 372,
      "content": "Figure 6-1. Leo Breiman, who was a professor of statistics at Berkeley, was at the forefront in\ndevelopment of many techniques at the core of a data scientist’s toolkit\n",
      "content_length": 169,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 373,
      "content": "MACHINE LEARNING VERSUS STATISTICS\nIn the context of predictive modeling, what is the difference between machine learning and\nstatistics? There is not a bright line dividing the two disciplines. Machine learning tends to be\nmore focused on developing efficient algorithms that scale to large data in order to optimize the\npredictive model. Statistics generally pays more attention to the probabilistic theory and underlying\nstructure of the model. Bagging, and the random forest (see “Bagging and the Random Forest”),\ngrew up firmly in the statistics camp. Boosting (see “Boosting”), on the other hand, has been\ndeveloped in both disciplines but receives more attention on the machine learning side of the\ndivide. Regardless of the history, the promise of boosting ensures that it will thrive as a technique\nin both statistics and machine learning.\n",
      "content_length": 849,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 374,
      "content": "K-Nearest Neighbors\nThe idea behind K-Nearest Neighbors (KNN) is very simple.1 For each record to\nbe classified or predicted:\n1. Find K records that have similar features (i.e., similar predictor values).\n2. For classification: Find out what the majority class is among those\nsimilar records, and assign that class to the new record.\n3. For prediction (also called KNN regression): Find the average among\nthose similar records, and predict that average for the new record.\nKEY TERMS FOR K-NEAREST NEIGHBORS\nNeighbor\nA record that has similar predictor values to another record.\nDistance metrics\nMeasures that sum up in a single number how far one record is from another.\nStandardization\nSubtract the mean and divide by the standard deviation.\nSynonym\nNormalization\nZ-score\nThe value that results after standardization.\nK\nThe number of neighbors considered in the nearest neighbor calculation.\nKNN is one of the simpler prediction/classification techniques: there is no model\nto be fit (as in regression). This doesn’t mean that using KNN is an automatic\nprocedure. The prediction results depend on how the features are scaled, how\nsimilarity is measured, and how big K is set. Also, all predictors must be in\nnumeric form. We will illustrate it with a classification example.\n",
      "content_length": 1276,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 375,
      "content": "A Small Example: Predicting Loan Default\nTable 6-1 shows a few records of personal loan data from the Lending Club.\nLending Club is a leader in peer-to-peer lending in which pools of investors make\npersonal loans to individuals. The goal of an analysis would be to predict the\noutcome of a new potential loan: paid-off versus default.\nTable 6-1. A few records and columns for Lending Club loan data\nOutcome Loan amount Income Purpose\nYears employed Home ownership State\nPaid off\n10000\n79100\ndebt_consolidation 11\nMORTGAGE\nNV\nPaid off\n9600\n48000\nmoving\n5\nMORTGAGE\nTN\nPaid off\n18800\n120036\ndebt_consolidation 11\nMORTGAGE\nMD\nDefault\n15250\n232000\nsmall_business\n9\nMORTGAGE\nCA\nPaid off\n17050\n35000\ndebt_consolidation 4\nRENT\nMD\nPaid off\n5500\n43000\ndebt_consolidation 4\nRENT\nKS\nConsider a very simple model with just two predictor variables: dti, which is the\nratio of debt payments (excluding mortgage) to income, and payment_inc_ratio,\nwhich is the ratio of the loan payment to income. Both ratios are multiplied by\n100. Using a small set of 200 loans, loan200, with known binary outcomes\n(default or no-default, specified in the predictor outcome200), and with K set to\n20, the KNN estimate for a new loan to be predicted, newloan, with dti=22.5\nand payment_inc_ratio=9 can be calculated in R as follows:\nlibrary(FNN)\nknn_pred <- knn(train=loan200, test=newloan, cl=outcome200, k=20)\nknn_pred == 'default'\n[1] TRUE\nThe KNN prediction is for the loan to default.\nWhile R has a native knn function, the contributed R package FNN, for Fast\nNearest Neighbor, scales to big data better and provides more flexibility.\n",
      "content_length": 1608,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 376,
      "content": "Figure 6-2 gives a visual display of this example. The new loan to be predicted is\nthe square in the middle. The circles (default) and triangles (paid off) are the\ntraining data. The black line shows the boundary of the nearest 20 points. In this\ncase, 14 defaulted loans lie within the circle as compared with only 6 paid-off\nloans. Hence, the predicted outcome of the loan is default.\nNOTE\nWhile the output of KNN for classification is typically a binary decision, such as default or paid\noff in the loan data, KNN routines usually offer the opportunity to output a probability (propensity)\nbetween 0 and 1. The probability is based on the fraction of one class in the K nearest neighbors.\nIn the preceding example, this probability of default would have been estimated at \n or 0.7.\nUsing a probability score lets you use classification rules other than simple majority votes\n(probability of 0.5). This is especially important in problems with imbalanced classes; see\n“Strategies for Imbalanced Data”. For example, if the goal is to identify members of a rare class,\nthe cutoff would typically be set below 50%. One common approach is to set the cutoff at the\nprobability of the rare event.\n",
      "content_length": 1193,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 377,
      "content": "Figure 6-2. KNN prediction of loan default using two variables: debt-to-income ratio and loan\npayment-to-income ratio\n",
      "content_length": 118,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 378,
      "content": "Distance Metrics\nSimilarity (nearness) is determined using a distance metric, which is a function\nthat measures how far two records (x1, x2, … xp) and (u1, u2, … up) are from one\nanother. The most popular distance metric between two vectors is Euclidean\ndistance. To measure the Euclidean distance between two vectors, subtract one\nfrom the other, square the differences, sum them, and take the square root:\nEuclidean distance offers special computational advantages. This is particularly\nimportant for large data sets since KNN involves K × n pairwise comparisons\nwhere n is the number of rows.\nAnother common distance metric for numeric data is Manhattan distance:\nEuclidean distance corresponds to the straight-line distance between two points\n(e.g., as the crow flies). Manhattan distance is the distance between two points\ntraversed in a single direction at a time (e.g., traveling along rectangular city\nblocks). For this reason, Manhattan distance is a useful approximation if similarity\nis defined as point-to-point travel time.\nIn measuring distance between two vectors, variables (features) that are measured\nwith comparatively large scale will dominate the measure. For example, for the\nloan data, the distance would be almost solely a function of the income and loan\namount variables, which are measured in tens or hundreds of thousands. Ratio\nvariables would count for practically nothing in comparison. We address this\nproblem by standardizing the data; see “Standardization (Normalization, Z-\nScores)”.\n",
      "content_length": 1518,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 379,
      "content": "OTHER DISTANCE METRICS\nThere are numerous other metrics for measuring distance between vectors. For numeric data,\nMahalanobis distance is attractive since it accounts for the correlation between two variables.\nThis is useful since if two variables are highly correlated, Mahalanobis will essentially treat these\nas a single variable in terms of distance. Euclidean and Manhattan distance do not account for the\ncorrelation, effectively placing greater weight on the attribute that underlies those features. The\ndownside of using Mahalanobis distance is increased computational effort and complexity; it is\ncomputed using the covariance matrix; see “Covariance Matrix”.\n",
      "content_length": 669,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 380,
      "content": "One Hot Encoder\nThe loan data in Table 6-1 includes several factor (string) variables. Most\nstatistical and machine learning models require this type of variable to be\nconverted to a series of binary dummy variables conveying the same information,\nas in Table 6-2. Instead of a single variable denoting the home occupant status as\n“owns with a mortage,” “owns with no mortgage,” “rents,” or “other,” we end up\nwith four binary variables. The first would be “owns with a mortgage — Y/N,”\nthe second would be “owns with no mortgage — Y/N,” and so on. This one\npredictor, home occupant status, thus yields a vector with one 1 and three 0s, that\ncan be used in statistical and machine learning algorithms. The phrase one hot\nencoding comes from digital circuit terminology, where it describes circuit\nsettings in which only one bit is allowed to be positive (hot).\nTable 6-2. Representing home\nownership factor data as a\nnumeric dummy variable\nMORTGAGE OTHER OWN RENT\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\nNOTE\nIn linear and logistic regression, one hot encoding causes problems with multicollinearity; see\n“Multicollinearity”. In such cases, one dummy is omitted (its value can be inferred from the other\nvalues). This is not an issue with KNN and other methods.\n",
      "content_length": 1270,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 381,
      "content": "Standardization (Normalization, Z-Scores)\nIn measurement, we are often not so much interested in “how much” but “how\ndifferent from the average.” Standardization, also called normalization, puts all\nvariables on similar scales by subtracting the mean and dividing by the standard\ndeviation. In this way, we ensure that a variable does not overly influence a model\nsimply due to the scale of its original measurement.\nThese are commonly refered to as z-scores. Measurements are then stated in terms\nof “standard deviations away from the mean.” In this way, a variable’s impact on\na model is not affected by the scale of its original measurement.\nCAUTION\nNormalization in this statistical context is not to be confused with database normalization,\nwhich is the removal of redundant data and the verification of data dependencies.\nFor KNN and a few other procedures (e.g., principal components analysis and\nclustering), it is essential to consider standardizing the data prior to applying the\nprocedure. To illustrate this idea, KNN is applied to the loan data using dti and\npayment_inc_ratio (see “A Small Example: Predicting Loan Default”) plus two\nother variables: revol_bal, the total revolving credit available to the applicant in\ndollars, and revol_util, the percent of the credit being used. The new record to\nbe predicted is shown here:\nnewloan\n  payment_inc_ratio dti revol_bal revol_util\n1            2.3932   1      1687        9.4\nThe magnitude of revol_bal, which is in dollars, is much bigger than the other\nvariables. The knn function returns the index of the nearest neighbors as an\nattribute nn.index, and this can be used to show the top-five closest rows in\n",
      "content_length": 1674,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 382,
      "content": "loan_df:\nloan_df <- model.matrix(~ -1 + payment_inc_ratio + dti + revol_bal +\n                           revol_util, data=loan_data)\nknn_pred <- knn(train=loan_df, test=newloan, cl=outcome, k=5)\nloan_df[attr(knn_pred,\"nn.index\"),]\n      payment_inc_ratio  dti revol_bal revol_util\n36054           2.22024 0.79      1687        8.4\n33233           5.97874 1.03      1692        6.2\n28989           5.65339 5.40      1694        7.0\n29572           5.00128 1.84      1695        5.1\n20962           9.42600 7.14      1683        8.6\nThe value of revol_bal in these neighbors is very close to its value in the new\nrecord, but the other predictor variables are all over the map and essentially play\nno role in determining neighbors.\nCompare this to KNN applied to the standardized data using the R function scale,\nwhich computes the z-score for each variable:\nloan_std <- scale(loan_df)\nknn_pred <- knn(train=loan_std, test=newloan_std, cl=outcome, k=5)\nloan_df[attr(knn_pred,\"nn.index\"),]\n      payment_inc_ratio  dti revol_bal revol_util\n2081            2.61091 1.03      1218        9.7\n36054           2.22024 0.79      1687        8.4\n23655           2.34286 1.12       523       10.7\n41327           2.15987 0.69      2115        8.1\n39555           2.76891 0.75      2129        9.5\nThe five nearest neighbors are much more alike in all the variables providing a\nmore sensible result. Note that the results are displayed on the original scale, but\nKNN was applied to the scaled data and the new loan to be predicted.\nTIP\nUsing the z-score is just one way to rescale variables. Instead of the mean, a more robust\nestimate of location could be used, such as the median. Likewise, a different estimate of scale\nsuch as the interquartile range could be used instead of the standard deviation. Sometimes,\nvariables are “squashed” into the 0–1 range. It’s also important to realize that scaling each\nvariable to have unit variance is somewhat arbitrary. This implies that each variable is thought to\nhave the same importance in predictive power. If you have subjective knowledge that some\nvariables are more important than others, then these could be scaled up. For example, with the\nloan data, it is reasonable to expect that the payment-to-income ratio is very important.\n",
      "content_length": 2271,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 383,
      "content": "NOTE\nNormalization (standardization) does not change the distributional shape of the data; it does not\nmake it normally shaped if it was not already normally shaped (see “Normal Distribution”).\n",
      "content_length": 194,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 384,
      "content": "Choosing K\nThe choice of K is very important to the performance of KNN. The simplest\nchoice is to set \n, known as the 1-nearest neighbor classifier. The\nprediction is intuitive: it is based on finding the data record in the training set\nmost similar to the new record to be predicted. Setting \n is rarely the best\nchoice; you’ll almost always obtain superior performance by using K > 1-nearest\nneighbors.\nGenerally speaking, if K is too low, we may be overfitting: including the noise in\nthe data. Higher values of K provide smoothing that reduces the risk of overfitting\nin the training data. On the other hand, if K is too high, we may oversmooth the\ndata and miss out on KNN’s ability to capture the local structure in the data, one\nof its main advantages.\nThe K that best balances between overfitting and oversmoothing is typically\ndetermined by accuracy metrics and, in particular, accuracy with holdout or\nvalidation data. There is no general rule about the best K — it depends greatly on\nthe nature of the data. For highly structured data with little noise, smaller values of\nK work best. Borrowing a term from the signal processing community, this type of\ndata is sometimes referred to as having a high signal-to-noise ratio (SNR).\nExamples of data with typically high SNR are handwriting and speech\nrecognition. For noisy data with less structure (data with a low SNR), such as the\nloan data, larger values of K are appropriate. Typically, values of K fall in the\nrange 1 to 20. Often, an odd number is chosen to avoid ties.\n",
      "content_length": 1534,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 385,
      "content": "BIAS-VARIANCE TRADEOFF\nThe tension between oversmoothing and overfitting is an instance of the bias-variance tradeoff,\nan ubiquitous problem in statistical model fitting. Variance refers to the modeling error that occurs\nbecause of the choice of training data; that is, if you were to choose a different set of training\ndata, the resulting model would be different. Bias refers to the modeling error that occurs\nbecause you have not properly identified the underlying real-world scenario; this error would not\ndisappear if you simply added more training data. When a flexible model is overfit, the variance\nincreases. You can reduce this by using a simpler model, but the bias may increase due to the loss\nof flexibility in modeling the real underlying situation. A general approach to handling this tradeoff\nis through cross-validation. See “Cross-Validation” for more details.\n",
      "content_length": 879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 386,
      "content": "KNN as a Feature Engine\nKNN gained its popularity due to its simplicity and intuitive nature. In terms of\nperformance, KNN by itself is usually not competitive with more sophisticated\nclassification techniques. In practical model fitting, however, KNN can be used to\nadd “local knowledge” in a staged process with other classification techniques.\n1. KNN is run on the data, and for each record, a classification (or quasi-\nprobability of a class) is derived.\n2. That result is added as a new feature to the record, and another\nclassification method is then run on the data. The original predictor\nvariables are thus used twice.\nAt first you might wonder whether this process, since it uses some predictors\ntwice, causes a problem with multicollinearity (see “Multicollinearity”). This is\nnot an issue, since the information being incorporated into the second-stage model\nis highly local, derived only from a few nearby records, and is therefore\nadditional information, and not redundant.\nNOTE\nYou can think of this staged use of KNN as a form of ensemble learning, in which multiple\npredictive modeling methods are used in conjunction with one another. It can also be considered\nas a form of feature engineering where the aim is to derive features (predictor variables) that\nhave predictive power. Often this involves some manual review of the data; KNN gives a fairly\nautomatic way to do this.\nFor example, consider the King County housing data. In pricing a home for sale, a\nrealtor will base the price on similar homes recently sold, known as “comps.” In\nessence, realtors are doing a manual version of KNN: by looking at the sale prices\nof similar homes, they can estimate what a home will sell for. We can create a\nnew feature for a statistical model to mimic the real estate professional by\napplying KNN to recent sales. The predicted value is the sales price and the\nexisting predictor variables could include location, total square feet, type of\nstructure, lot size, and number of bedrooms and bathrooms. The new predictor\nvariable (feature) that we add via KNN is the KNN predictor for each record\n",
      "content_length": 2107,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 387,
      "content": "(analogous to the realtors’ comps). Since we are predicting a numerical value, the\naverage of the K-Nearest Neighbors is used instead of a majority vote (known as\nKNN regression).\nSimilarly, for the loan data, we can create features that represent different aspects\nof the loan process. For example, the following would build a feature that\nrepresents a borrower’s creditworthiness:\nborrow_df <- model.matrix(~ -1 + dti + revol_bal + revol_util + open_acc +\n                          delinq_2yrs_zero + pub_rec_zero, data=loan_data)\nborrow_knn <- knn(borrow_df, test=borrow_df, cl=loan_data[, 'outcome'],\n                prob=TRUE, k=10)\nprob <- attr(borrow_knn, \"prob\")\nborrow_feature <- ifelse(borrow_knn=='default', prob, 1-prob)\nsummary(borrow_feature)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.\n 0.0000  0.4000  0.5000  0.5012  0.6000  1.0000\nThe result is a feature that predicts the likelihood a borrower will default based\non his credit history.\nKEY IDEAS FOR K-NEAREST NEIGHBORS\nK-Nearest Neighbors (KNN) classifies a record by assigning it to the class that similar records\nbelong to.\nSimilarity (distance) is determined by Euclidian distance or other related metrics.\nThe number of nearest neighbors to compare a record to, K, is determined by how well the\nalgorithm performs on training data, using different values for K.\nTypically, the predictor variables are standardized so that variables of large scale do not dominate\nthe distance metric.\nKNN is often used as a first stage in predictive modeling, and the predicted value is added back\ninto the data as a predictor for second-stage (non-KNN) modeling.\n",
      "content_length": 1625,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 388,
      "content": "Tree Models\nTree models, also called Classification and Regression Trees (CART),2 decision\ntrees, or just trees, are an effective and popular classification (and regression)\nmethod initially developed by Leo Breiman and others in 1984. Tree models, and\ntheir more powerful descendents random forests and boosting (see “Bagging and\nthe Random Forest” and “Boosting”), form the basis for the most widely used and\npowerful predictive modeling tools in data science for both regression and\nclassification.\nKEY TERMS FOR TREES\nRecursive partitioning\nRepeatedly dividing and subdividing the data with the goal of making the outcomes in each final\nsubdivision as homogeneous as possible.\nSplit value\nA predictor value that divides the records into those where that predictor is less than the split\nvalue, and those where it is more.\nNode\nIn the decision tree, or in the set of corresponding branching rules, a node is the graphical or rule\nrepresentation of a split value.\nLeaf\nThe end of a set of if-then rules, or branches of a tree — the rules that bring you to that leaf\nprovide one of the classification rules for any record in a tree.\nLoss\nThe number of misclassifications at a stage in the splitting process; the more losses, the more\nimpurity.\nImpurity\nThe extent to which a mix of classes is found in a subpartition of the data (the more mixed, the\nmore impure).\nSynonym\nHeterogeneity\nAntonym\nHomogeneity, purity\nPruning\nThe process of taking a fully grown tree and progressively cutting its branches back, to reduce\noverfitting.\n",
      "content_length": 1532,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 389,
      "content": "A tree model is a set of “if-then-else” rules that are easy to understand and to\nimplement. In contrast to regression and logistic regression, trees have the ability\nto discover hidden patterns corresponding to complex interactions in the data.\nHowever, unlike KNN or naive Bayes, simple tree models can be expressed in\nterms of predictor relationships that are easily interpretable.\n",
      "content_length": 384,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 390,
      "content": "DECISION TREES IN OPERATIONS RESEARCH\nThe term decision trees has a different (and older) meaning in decision science and operations\nresearch, where it refers to a human decision analysis process. In this meaning, decision points,\npossible outcomes, and their estimated probabilities are laid out in a branching diagram, and the\ndecision path with the maximum expected value is chosen.\n",
      "content_length": 386,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 391,
      "content": "A Simple Example\nThe two main packages to fit tree models in R are rpart and tree. Using the\nrpart package, a model is fit to a sample of 3,000 records of the loan data using\nthe variables payment_inc_ratio and borrower_score (see “K-Nearest\nNeighbors” for a description of the data).\nlibrary(rpart)\nloan_tree <- rpart(outcome ~ borrower_score + payment_inc_ratio,\n                   data=loan_data, control = rpart.control(cp=.005))\nplot(loan_tree, uniform=TRUE, margin=.05)\ntext(loan_tree)\nThe resulting tree is shown in Figure 6-3. These classification rules are\ndetermined by traversing through a hierarchical tree, starting at the root until a leaf\nis reached.\nFigure 6-3. The rules for a simple tree model fit to the loan data\nTypically, the tree is plotted upside-down, so the root is at the top and the leaves\nare at the bottom. For example, if we get a loan with borrower_score of 0.6 and\n",
      "content_length": 898,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 392,
      "content": "a payment_inc_ratio of 8.0, we end up at the leftmost leaf and predict the loan\nwill be paid off.\nA nicely printed version of the tree is also easily produced:\nloan_tree\nn= 3000\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n 1) root 3000 1467 paid off (0.5110000 0.4890000)\n   2) borrower_score>=0.525 1283  474 paid off (0.6305534 0.3694466)\n     4) payment_inc_ratio< 8.772305 845  249 paid off (0.7053254 0.2946746) *\n     5) payment_inc_ratio>=8.772305 438  213 default (0.4863014 0.5136986)\n      10) borrower_score>=0.625 149   60 paid off (0.5973154 0.4026846) *\n      11) borrower_score< 0.625 289  124 default (0.4290657 0.5709343) *\n   3) borrower_score< 0.525 1717  724 default (0.4216657 0.5783343)\n     6) payment_inc_ratio< 9.73236 1082  517 default (0.4778189 0.5221811)\n      12) borrower_score>=0.375 784  384 paid off (0.5102041 0.4897959) *\n      13) borrower_score< 0.375 298  117 default (0.3926174 0.6073826) *\n     7) payment_inc_ratio>=9.73236 635  207 default (0.3259843 0.6740157) *\nThe depth of the tree is shown by the indent. Each node corresponds to a\nprovisional classification determined by the prevalent outcome in that partition.\nThe “loss” is the number of misclassifications yielded by the provisional\nclassification in a partition. For example, in node 2, there were 474\nmisclassification out of a total of 1,467 total records. The values in the\nparentheses correspond to the proportion of records that are paid off and default,\nrespectively. For example, in node 13, which predicts default, over 60 percent of\nthe records are loans that are in default.\n",
      "content_length": 1612,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 393,
      "content": "The Recursive Partitioning Algorithm\nThe algorithm to construct a decision tree, called recursive partitioning , is\nstraightforward and intuitive. The data is repeatedly partitioned using predictor\nvalues that do the best job of separating the data into relatively homogeneous\npartitions. Figure 6-4 shows a picture of the partitions created for the tree in\nFigure 6-3. The first rule is borrower_score >= 0.525 and is depicted by rule 1\nin the plot. The second rule is payment_inc_ratio < 9.732 and divides the\nrighthand region in two.\n",
      "content_length": 537,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 394,
      "content": "Figure 6-4. The rules for a simple tree model fit to the loan data\nSuppose we have a response variable Y and a set of P predictor variables Xj for \n. For a partition A of records, recursive partitioning will find\nthe best way to partition A into two subpartitions:\n1. For each predictor variable Xj,\na. For each value sj of Xj:\ni. Split the records in A with Xj values < sj as one partition, and\nthe remaining records where Xj ≥ sj as another partition.\nii. Measure the homogeneity of classes within each subpartition of\nA.\nb. Select the value of sj that produces maximum within-partition\nhomogeneity of class.\n2. Select the variable Xj and the split value sj that produces maximum\n",
      "content_length": 682,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 395,
      "content": "within-partition homogeneity of class.\nNow comes the recursive part:\n1. Initialize A with the entire data set.\n2. Apply the partitioning algorithm to split A into two subpartitions, A1 and\nA2.\n3. Repeat step 2 on subpartitions A1 and A2.\n4. The algorithm terminates when no further partition can be made that\nsufficiently improves the homogeneity of the partitions.\nThe end result is a partitioning of the data, as in Figure 6-4 except in P-\ndimensions, with each partition predicting an outcome of 0 or 1 depending on the\nmajority vote of the reponse in that partition.\nNOTE\nIn addition to a binary 0/1 prediction, tree models can produce a probability estimate based on the\nnumber of 0s and 1s in the partition. The estimate is simply the sum of 0s or 1s in the partition\ndivided by the number of observations in the partition.\nThe estimated \n can then be converted to a binary decision; for example,\nset the estimate to 1 if Prob(Y = 1) > 0.5.\n",
      "content_length": 947,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 396,
      "content": "Measuring Homogeneity or Impurity\nTree models recursively create partitions (sets of records), A, that predict an\noutcome of Y = 0 or Y = 1. You can see from the preceding algorithm that we need\na way to measure homogeneity, also called class purity, within a partition. Or,\nequivalently, we need to measure the impurity of a partition. The accuracy of the\npredictions is the proportion p of misclassified records within that partition,\nwhich ranges from 0 (perfect) to 0.5 (purely random guessing).\nIt turns out that accuracy is not a good measure for impurity. Instead, two common\nmeasures for impurity are the Gini impurity and entropy or information. While\nthese (and other) impurity measures apply to classification problems with more\nthan two classes, we focus on the binary case. The Gini impurity for a set of\nrecords A is:\nThe entropy measure is given by:\nFigure 6-5 shows that Gini impurity (rescaled) and entropy measures are similar,\nwith entropy giving higher impurity scores for moderate and high accuracy rates.\n",
      "content_length": 1027,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 397,
      "content": "Figure 6-5. Gini impurity and entropy measures\n",
      "content_length": 47,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 398,
      "content": "GINI COEFFICIENT\nGini impurity is not to be confused with the Gini coefficient. They represent similar concepts, but\nthe Gini coefficient is limited to the binary classification problem and is related to the AUC metric\n(see “AUC”).\nThe impurity metric is used in the splitting algorithm described earlier. For each\nproposed partition of the data, impurity is measured for each of the partitions that\nresult from the split. A weighted average is then calculated, and whichever\npartition (at each stage) yields the lowest weighted average is selected.\n",
      "content_length": 550,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 399,
      "content": "Stopping the Tree from Growing\nAs the tree grows bigger, the splitting rules become more detailed, and the tree\ngradually shifts from identifying “big” rules that identify real and reliable\nrelationships in the data to “tiny” rules that reflect only noise. A fully grown tree\nresults in completely pure leaves and, hence, 100% accuracy in classifying the\ndata that it is trained on. This accuracy is, of course, illusory — we have overfit\n(see Bias-Variance Tradeoff) the data, fitting the noise in the training data, not the\nsignal that we want to identify in new data.\n",
      "content_length": 571,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 400,
      "content": "PRUNING\nA simple and intuitive method of reducing tree size is to prune back the terminal and smaller\nbranches of the tree, leaving a smaller tree. How far should the pruning proceed? A common\ntechnique is to prune the tree back to the point where the error on holdout data is minimized.\nWhen we combine predictions from multiple trees (see “Bagging and the Random Forest”),\nhowever, we will need a way to stop tree growth. Pruning plays a role in the process of cross-\nvalidation to determine how far to grow trees that are used in ensemble methods.\nWe need some way to determine when to stop growing a tree at a stage that will\ngeneralize to new data. There are two common ways to stop splitting:\nAvoid splitting a partition if a resulting subpartition is too small, or if a\nterminal leaf is too small. In rpart, these constraints are controlled\nseparately by the parameters minsplit and minbucket, respectively, with\ndefaults of 20 and 7.\nDon’t split a partition if the new partition does not “significantly” reduce the\nimpurity. In rpart, this is controlled by the complexity parameter cp, which\nis a measure of how complex a tree is — the more complex, the greater the\nvalue of cp. In practice, cp is used to limit tree growth by attaching a penalty\nto additional complexity (splits) in a tree.\nThe first method involves arbitrary rules, and can be usful for exploratory work,\nbut we can’t easily determine optimum values (i.e., values that maximize\npredictive accuracy with new data). With the complexity parameter, cp, we can\nestimate what size tree will perform best with new data.\nIf cp is too small, then the tree will overfit the data, fitting noise and not signal.\nOn the other hand, if cp is too large, then the tree will be too small and have little\npredictive power. The default in rpart is 0.01, although for larger data sets, you\nare likely to find this is too large. In the previous example, cp was set to 0.005\nsince the default led to a tree with a single split. In exploratory analysis, it is\nsufficient to simply try a few values.\nDetermining the optimum cp is an instance of the bias-variance tradeoff (see Bias-\nVariance Tradeoff). The most common way to estimate a good value of cp is via\n",
      "content_length": 2214,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 401,
      "content": "cross-validation (see “Cross-Validation”):\n1. Partition the data into training and validation (holdout) sets.\n2. Grow the tree with the training data.\n3. Prune it successively, step by step, recording cp (using the training data)\nat each step.\n4. Note the cp that corresponds to the minimum error (loss) on the\nvalidation data.\n5. Repartition the data into training and validation, and repeat the growing,\npruning, and cp recording process.\n6. Do this again and again, and average the cps that reflect minimum error\nfor each tree.\n7. Go back to the original data, or future data, and grow a tree, stopping at\nthis optimum cp value.\nIn rpart, you can use the argument cptable to produce a table of the CP values\nand their associated cross-validation error (xerror in R), from which you can\ndetermine the CP value that has the lowest cross-validation error.\n",
      "content_length": 856,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 402,
      "content": "Predicting a Continuous Value\nPredicting a continuous value (also termed regression) with a tree follows the\nsame logic and procedure, except that impurity is measured by squared deviations\nfrom the mean (squared errors) in each subpartition, and predictive performance\nis judged by the square root of the mean squared error (RMSE) (see “Assessing\nthe Model”) in each partition.\n",
      "content_length": 379,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 403,
      "content": "How Trees Are Used\nOne of the big obstacles faced by predictive modelers in organizations is the\nperceived “black box” nature of the methods they use, which gives rise to\nopposition from other elements of the organization. In this regard, the tree model\nhas two appealing aspects.\nTree models provide a visual tool for exploring the data, to gain an idea of\nwhat variables are important and how they relate to one another. Trees can\ncapture nonlinear relationships among predictor variables.\nTree models provide a set of rules that can be effectively communicated to\nnonspecialists, either for implementation or to “sell” a data mining project.\nWhen it comes to prediction, however, harnassing the results from multiple trees\nis typically more powerful than just using a single tree. In particular, the random\nforest and boosted tree algorithms almost always provide superior predictive\naccuracy and performance (see “Bagging and the Random Forest” and\n“Boosting”), but the aforementioned advantages of a single tree are lost.\nKEY IDEAS\nDecision trees produce a set of rules to classify or predict an outcome.\nThe rules correspond to successive partitioning of the data into subpartitions.\nEach partition, or split, references a specific value of a predictor variable and divides the data into\nrecords where that predictor value is above or below that split value.\nAt each stage, the tree algorithm chooses the split that minimizes the outcome impurity within each\nsubpartition.\nWhen no further splits can be made, the tree is fully grown and each terminal node, or leaf, has\nrecords of a single class; new cases following that rule (split) path would be assigned that class.\nA fully grown tree overfits the data and must be pruned back so that it captures signal and not\nnoise.\nMultiple-tree algorithms like random forests and boosted trees yield better predictive performance,\nbut lose the rule-based communicative power of single trees.\n",
      "content_length": 1940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 404,
      "content": "Further Reading\nAnalytics Vidhya Content Team, “A Complete Tutorial on Tree Based\nModeling from Scratch (in R & Python)”, April 12, 2016.\nTerry M. Therneau, Elizabeth J. Atkinson, and the Mayo Foundation, “An\nIntroduction to Recursive Partitioning Using the RPART Routines”, June 29,\n2015.\n",
      "content_length": 290,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 405,
      "content": "Bagging and the Random Forest\nIn 1907, the statistician Sir Francis Galton was visiting a county fair in England,\nat which a contest was being held to guess the dressed weight of an ox that was on\nexhibit. There were 800 guesses, and, while the individual guesses varied widely,\nboth the mean and the median came out within 1% of the ox’s true weight. James\nSuroweicki has explored this phenomenon in his book The Wisdom of Crowds\n(Doubleday, 2004). This principle applies to predictive models, as well:\naveraging (or taking majority votes) of multiple models — an ensemble of models\n— turns out to be more accurate than just selecting one model.\nKEY TERMS FOR BAGGING AND THE RANDOM FOREST\nEnsemble\nForming a prediction by using a collection of models.\nSynonym\nModel averaging\nBagging\nA general technique to form a collection of models by bootstrapping the data.\nSynonym\nBootstrap aggregation\nRandom forest\nA type of bagged estimate based on decision tree models.\nSynonym\nBagged decision trees\nVariable importance\nA measure of the importance of a predictor variable in the performance of the model.\nThe ensemble approach has been applied to and across many different modeling\nmethods, most publicly in the Netflix Contest, in which Netflix offered a $1\nmillion prize to any contestant who came up with a model that produced a 10%\nimprovement in predicting the rating that a Netflix customer would award a\nmovie. The simple version of ensembles is as follows:\n1. Develop a predictive model and record the predictions for a given data\n",
      "content_length": 1534,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 406,
      "content": "set.\n2. Repeat for multiple models, on the same data.\n3. For each record to be predicted, take an average (or a weighted average,\nor a majority vote) of the predictions.\nEnsemble methods have been applied most systematically and effectively to\ndecision trees. Ensemble tree models are so powerful that they provide a way to\nbuild good predictive models with relatively little effort.\nGoing beyond the simple ensemble algorithm, there are two main variants of\nensemble models: bagging and boosting. In the case of ensemble tree models,\nthese are refered to as random forest models and boosted tree models. This\nsection focuses on bagging; boosting is covered in “Boosting”.\n",
      "content_length": 673,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 407,
      "content": "Bagging\nBagging, which stands for “bootstrap aggregating,” was introduced by Leo\nBreiman in 1994. Suppose we have a response Y and P predictor variables \n with n records.\nBagging is like the basic algorithm for ensembles, except that, instead of fitting the\nvarious models to the same data, each new model is fit to a bootstrap resample.\nHere is the algorithm presented more formally:\n1. Initialize M, the number of models to be fit, and n, the number of records\nto choose (n < N). Set the iteration \n.\n2. Take a bootstrap resample (i.e., with replacement) of n records from the\ntraining data to form a subsample \n and \n (the bag).\n3. Train a model using \n and \n to create a set of decision rules \n.\n4. Increment the model counter \n. If m <= M, go to step 1.\nIn the case where \n predicts the probability \n, the bagged estimate is\ngiven by:\n",
      "content_length": 840,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 408,
      "content": "Random Forest\nThe random forest is based on applying bagging to decision trees with one\nimportant extension: in addition to sampling the records, the algorithm also\nsamples the variables.3 In traditional decision trees, to determine how to create a\nsubpartition of a partition A, the algorithm makes the choice of variable and split\npoint by minimizing a criterion such as Gini impurity (see “Measuring\nHomogeneity or Impurity”). With random forests, at each stage of the algorithm,\nthe choice of variable is limited to a random subset of variables. Compared to\nthe basic tree algorithm (see “The Recursive Partitioning Algorithm”), the random\nforest algorithm adds two more steps: the bagging discussed earlier (see “Bagging\nand the Random Forest”), and the bootstrap sampling of variables at each split:\n1. Take a bootstrap (with replacement) subsample from the records.\n2. For the first split, sample p < P variables at random without\nreplacement.\n3. For each of the sampled variables \n, apply the\nsplitting algorithm:\na. For each value \n of \n:\ni. Split the records in partition A with Xj(k) < sj(k) as one partition,\nand the remaining records where \n as another\npartition.\nii. Measure the homogeneity of classes within each subpartition of\nA.\nb. Select the value of \n that produces maximum within-partition\nhomogeneity of class.\n4. Select the variable \n and the split value \n that produces maximum\nwithin-partition homogeneity of class.\n5. Proceed to the next split and repeat the previous steps, starting with step\n2.\n",
      "content_length": 1523,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 409,
      "content": "6. Continue with additional splits following the same procedure until the\ntree is grown.\n7. Go back to step 1, take another bootstrap subsample, and start the\nprocess over again.\nHow many variables to sample at each step? A rule of thumb is to choose \nwhere P is the number of predictor variables. The package randomForest\nimplements the random forest in R. The following applies this package to the loan\ndata (see “K-Nearest Neighbors” for a description of the data).\n> library(randomForest)\n> rf <- randomForest(outcome ~ borrower_score + payment_inc_ratio,\n                     data=loan3000)\nCall:\n randomForest(formula = outcome ~ borrower_score + payment_inc_ratio,\n              data = loan3000)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n        OOB estimate of  error rate: 38.53%\nConfusion matrix:\n         paid off default class.error\npaid off     1089     425   0.2807133\ndefault       731     755   0.4919246\nBy default, 500 trees are trained. Since there are only two variables in the\npredictor set, the algorithm randomly selects the variable on which to split at each\nstage (i.e., a bootstrap subsample of size 1).\nThe out-of-bag (OOB) estimate of error is the error rate for the trained models,\napplied to the data left out of the training set for that tree. Using the output from the\nmodel, the OOB error can be plotted versus the number of trees in the random\nforest:\nerror_df = data.frame(error_rate = rf$err.rate[,'OOB'],\n                      num_trees = 1:rf$ntree)\nggplot(error_df, aes(x=num_trees, y=error_rate)) +\n  geom_line()\nThe result is shown in Figure 6-6. The error rate rapidly decreases from over .44\nbefore stabilizing around .385. The predicted values can be obtained from the\npredict function and plotted as follows:\n",
      "content_length": 1847,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 410,
      "content": "pred <- predict(loan_lda)\nrf_df <- cbind(loan3000, pred_default=pred[,'default']>.5)\nggplot(data=rf_df, aes(x=borrower_score, y=payment_inc_ratio,\n                       color=pred_default, shape=pred_default)) +\n  geom_point(alpha=.6, size=2) +\n  scale_shape_manual( values=c( 46, 4))\n",
      "content_length": 286,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 411,
      "content": "Figure 6-6. The improvement in accuracy of the random forest with the addition of more trees\nThe plot, shown in Figure 6-7, is quite revealing about the nature of the random\nforest.\n",
      "content_length": 182,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 412,
      "content": "Figure 6-7. The predicted outcomes from the random forest applied to the loan default data\nThe random forest method is a “black box” method. It produces more accurate\npredictions than a simple tree, but the simple tree’s intuitive decision rules are\nlost. The predictions are also somewhat noisy: note that some borrowers with a\nvery high score, indicating high creditworthiness, still end up with a prediction of\ndefault. This is a result of some unusual records in the data and demonstrates the\ndanger of overfitting by the random forest (see Bias-Variance Tradeoff).\n",
      "content_length": 570,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 413,
      "content": "Variable Importance\nThe power of the random forest algorithm shows itself when you build predictive\nmodels for data with many features and records. It has the ability to automatically\ndetermine which predictors are important and discover complex relationships\nbetween predictors corresponding to interaction terms (see “Interactions and Main\nEffects”). For example, fit a model to the loan default data with all columns\nincluded:\n> rf_all <- randomForest(outcome ~ ., data=loan_data, importance=TRUE)\n> rf_all\nCall:\n randomForest(formula = outcome ~ ., data = loan_data, importance = TRUE)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 3\n        OOB estimate of  error rate: 34.38%\nConfusion matrix:\n         paid off default class.error\npaid off    15078    8058   0.3482884\ndefault      7849   15287   0.3392548\nThe argument importance=TRUE requests that the randomForest store additional\ninformation about the importance of different variables. The function varImpPlot\nwill plot the relative performance of the variables:\nvarImpPlot(rf_all, type=1)\nvarImpPlot(rf_all, type=2)\nThe result is shown in Figure 6-8.\n",
      "content_length": 1201,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 415,
      "content": "Figure 6-8. The importance of variables for the full model fit to the loan data\nThere are two ways to measure variable importance:\nBy the decrease in accuracy of the model if the values of a variable are\nrandomly permuted (type=1). Randomly permuting the values has the effect\nof removing all predictive power for that variable. The accuracy is computed\nfrom the out-of-bag data (so this measure is effectively a cross-validated\nestimate).\nBy the mean decrease in the Gini impurity score (see “Measuring\nHomogeneity or Impurity”) for all of the nodes that were split on a variable\n(type=2). This measures how much improvement to the purity of the nodes\nthat variable contributes. This measure is based on the training set, and\ntherefore less reliable than a measure calculated on out-of-bag data.\nThe top and bottom panels of Figure 6-8 show variable importance according to\nthe decrease in accuracy and in Gini impurity, respectively. The variables in both\npanels are ranked by the decrease in accuracy. The variable importance scores\nproduced by these two measures are quite different.\nSince the accuracy decrease is a more reliable metric, why should we use the Gini\nimpurity decrease measure? By default, randomForest only computes this Gini\nimpurity: Gini impurity is a byproduct of the algorithm, whereas model accuracy\nby variable requires extra computations (randomly permuting the data and\npredicting this data). In cases where computational complexity is important, such\nas in a production setting where thousands of models are being fit, it may not be\nworth the extra computational effort. In addition, the Gini decrease sheds light on\nwhich variables the random forest is using to make its splitting rules (recall that\nthis information, readily visible in a simple tree, is effectively lost in a random\nforest). Examining the difference between Gini decrease and model accuracy\nvariable importance may suggest ways to improve the model.\n",
      "content_length": 1949,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 416,
      "content": "Hyperparameters\nThe random forest, as with many statistical machine learning algorithms, can be\nconsidered a black-box algorithm with knobs to adjust how the box works. These\nknobs are called hyperparameters, which are parameters that you need to set\nbefore fitting a model; they are not optimized as part of the training process. While\ntraditional statistical models require choices (e.g., the choice of predictors to use\nin a regression model), the hyperparameters for random forest are more critical,\nespecially to avoid overfitting. In particular, the two most important\nhyperparemters for the random forest are:\nnodesize\nThe minimum size for terminal nodes (leaves in the tree). The default is 1 for\nclassification and 5 for regression.\nmaxnodes\nThe maximum number of nodes in each decision tree. By default, there is no\nlimit and the largest tree will be fit subject to the constraints of nodesize.\nIt may be tempting to ignore these parameters and simply go with the default\nvalues. However, using the default may lead to overfitting when you apply the\nrandom forest to noisy data. When you increase nodesize or set maxnodes, the\nalgorithm will fit smaller trees and is less likely to create spurious predictive\nrules. Cross-validation (see “Cross-Validation”) can be used to test the effects of\nsetting different values for hyperparameters.\nKEY IDEAS FOR BAGGING AND THE RANDOM FOREST\nEnsemble models improve model accuracy by combining the results from many models.\nBagging is a particular type of ensemble model based on fitting many models to bootstrapped\nsamples of the data and averaging the models.\nRandom forest is a special type of bagging applied to decision trees. In addition to resampling the\ndata, the random forest algorithm samples the predictor variables when splitting the trees.\nA useful output from the random forest is a measure of variable importance that ranks the\npredictors in terms of their contribution to model accuracy.\nThe random forest has a set of hyperparameters that should be tuned using cross-validation to\navoid overfitting.\n",
      "content_length": 2069,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 418,
      "content": "Boosting\nEnsemble models have become a standard tool for predictive modeling. Boosting\nis a general technique to create an ensemble of models. It was developed around\nthe same time as bagging (see “Bagging and the Random Forest”). Like bagging,\nboosting is most commonly used with decision trees. Despite their similarities,\nboosting takes a very different approach — one that comes with many more bells\nand whistles. As a result, while bagging can be done with relatively little tuning,\nboosting requires much greater care in its application. If these two methods were\ncars, bagging could be considered a Honda Accord (reliable and steady), whereas\nboosting could be considered a Porsche (powerful but requires more care).\nIn linear regression models, the residuals are often examined to see if the fit can\nbe improved (see “Partial Residual Plots and Nonlinearity”). Boosting takes this\nconcept much further and fits a series of models with each successive model fit to\nminimize the error of the previous models. Several variants of the algorithm are\ncommonly used: Adaboost, gradient boosting, and stochastic gradient boosting.\nThe latter, stochastic gradient boosting, is the most general and widely used.\nIndeed, with the right choice of parameters, the algorithm can emulate the random\nforest.\nKEY TERMS FOR BOOSTING\nEnsemble\nForming a prediction by using a collection of models.\nSynonym\nModel averaging\nBoosting\nA general technique to fit a sequence of models by giving more weight to the records with large\nresiduals for each successive round.\nAdaboost\nAn early version of boosting based on reweighting the data based on the residuals.\nGradient boosting\nA more general form of boosting that is cast in terms of minimizing a cost function.\nStochastic gradient boosting\nThe most general algorithm for boosting that incorporates resampling of records and columns in\n",
      "content_length": 1871,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 419,
      "content": "each round.\nRegularization\nA technique to avoid overfitting by adding a penalty term to the cost function on the number of\nparameters in the model.\nHyperparameters\nParameters that need to be set before fitting the algorithm.\n",
      "content_length": 225,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 420,
      "content": "The Boosting Algorithm\nThe basic idea behind the various boosting algorithms is essentially the same. The\neasiest to understand is Adaboost, which proceeds as follows:\n1. Initialize M, the maximum number of models to be fit, and set the\niteration counter \n. Initialize the observation weights \nfor \n. Initialize the ensemble model \n.\n2. Train a model using \n using the observation weights \nthat minimizes the weighted error \n defined by summing the weights for\nthe misclassified observations.\n3. Add the model to the ensemble: \n where \n.\n4. Update the weights \n so that the weights are increased\nfor the observations that were misclassfied. The size of the increase\ndepends on \n with larger values of \n leading to bigger weights.\n5. Increment the model counter \n. If \n, go to step 1.\nThe boosted estimate is given by:\nBy increasing the weights for the observations that were misclassified, the\nalgorithm forces the models to train more heavily on the data for which it\nperformed poorly. The factor \n ensures that models with lower error have a\nbigger weight.\nGradient boosting is similar to Adaboost but casts the problem as an optimization\nof a cost function. Instead of adjusting weights, gradient boosting fits models to a\npseudo-residual, which has the effect of training more heavily on the larger\nresiduals. In the spirit of the random forest, stochastic gradient boosting adds\nrandomness to the algorithm by sampling observations and predictor variables at\neach stage.\n",
      "content_length": 1476,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 421,
      "content": "XGBoost\nThe most widely used public domain software for boosting is XGBoost, an\nimplementation of stochastic gradient boosting originally developed by Tianqi\nChen and Carlos Guestrin at the University of Washington. A computationally\nefficient implementation with many options, it is available as a package for most\nmajor data science software languages. In R, XGBoost is available as the package\nxgboost.\nThe function xgboost has many parameters that can, and should, be adjusted (see\n“Hyperparameters and Cross-Validation”). Two very important parameters are\nsubsample, which controls the fraction of observations that should be sampled at\neach iteration, and eta, a shrinkage factor applied to \n in the boosting algorithm\n(see “The Boosting Algorithm”). Using subsample makes boosting act like the\nrandom forest except that the sampling is done without replacement. The shrinkage\nparameter eta is helpful to prevent overfitting by reducing the change in the\nweights (a smaller change in the weights means the algorithm is less likely to\noverfit to the training set). The following applies xgboost to the loan data with\njust two predictor variables:\nlibrary(xgboost)\npredictors <- data.matrix(loan3000[, c('borrower_score',\n                                       'payment_inc_ratio')])\nlabel <- as.numeric(loan3000[,'outcome'])-1\nxgb <- xgboost(data=predictors, label=label,\n               objective = \"binary:logistic\",\n               params=list(subsample=.63, eta=0.1), nrounds=100)\nNote that xgboost does not support the formula syntax, so the predictors need to\nbe converted to a data.matrix and the response needs to be converted to 0/1\nvariables. The objective argument tells xgboost what type of problem this is;\nbased on this, xgboost will choose a metric to optimize.\nThe predicted values can be obtained from the predict function and, since there\nare only two variables, plotted versus the predictors:\npred <- predict(xgb, newdata=predictors)\nxgb_df <- cbind(loan3000, pred_default=pred>.5, prob_default=pred)\nggplot(data=xgb_df, aes(x=borrower_score, y=payment_inc_ratio,\n                       color=pred_default, shape=pred_default)) +\n",
      "content_length": 2152,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 422,
      "content": "       geom_point(alpha=.6, size=2)\nThe result is shown in Figure 6-9. Qualitatively, this is similar to the predictions\nfrom the random forest; see Figure 6-7. The predictions are somewhat noisy in\nthat some borrowers with a very high borrower score still end up with a\nprediction of default.\n",
      "content_length": 294,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 423,
      "content": "Figure 6-9. The predicted outcomes from XGBoost applied to the loan default data\n",
      "content_length": 81,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 424,
      "content": "Regularization: Avoiding Overfitting\nBlind application of xgboost can lead to unstable models as a result of\noverfitting to the training data. The problem with overfitting is twofold:\nThe accuracy of the model on new data not in the training set will be\ndegraded.\nThe predictions from the model are highly variable, leading to unstable\nresults.\nAny modeling technique is potentially prone to overfitting. For example, if too\nmany variables are included in a regression equation, the model may end up with\nspurious predictions. However, for most statistical techniques, overfitting can be\navoided by a judicious selection of predictor variables. Even the random forest\ngenerally produces a reasonable model without tuning the parameters. This,\nhowever, is not the case for xgboost. Fit xgboost to the loan data for a training\nset with all of the variables included in the model:\n> predictors <- data.matrix(loan_data[,-which(names(loan_data) %in%\n                                       'outcome')])\n> label <- as.numeric(loan_data$outcome)-1\n> test_idx <- sample(nrow(loan_data), 10000)\n> xgb_default <- xgboost(data=predictors[-test_idx,],\n                         label=label[-test_idx],\n                         objective = \"binary:logistic\", nrounds=250)\n> pred_default <- predict(xgb_default, predictors[test_idx,])\n> error_default <- abs(label[test_idx] - pred_default) > 0.5\n> xgb_default$evaluation_log[250,]\n   iter train_error\n1:  250    0.145622\n> mean(error_default)\n[1] 0.3715\nThe test set consists of 10,000 randomly sampled records from the full data, and\nthe training set consists of the remaining records. Boosting leads to an error rate\nof only 14.6% for the training set. The test set, however, has a much higher error\nrate of 36.2%. This is a result of overfitting: while boosting can explain the\nvariability in the training set very well, the prediction rules do not apply to new\ndata.\nBoosting provides several parameters to avoid overfitting, including the\nparameters eta and subsample (see “XGBoost”). Another approach is\n",
      "content_length": 2045,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 425,
      "content": "regularization, a technique that modifies the cost function in order to penalize the\ncomplexity of the model. Decision trees are fit by minimizing cost criteria such as\nGini’s impurity score (see “Measuring Homogeneity or Impurity”). In xgboost, it\nis possible to modify the cost function by adding a term that measures the\ncomplexity of the model.\nThere are two parameters in xgboost to regularize the model: alpha and lambda,\nwhich correspond to Manhattan distance and squared Euclidean distance,\nrespectively (see “Distance Metrics”). Increasing these parameters will penalize\nmore complex models and reduce the size of the trees that are fit. For example,\nsee what happens if we set lambda to 1,000:\n> xgb_penalty <- xgboost(data=predictors[-test_idx,],\n                         label=label[-test_idx],\n                         params=list(eta=.1, subsample=.63, lambda=1000),\n                         objective = \"binary:logistic\", nrounds=250)\n> pred_penalty <- predict(xgb_penalty, predictors[test_idx,])\n> error_penalty <- abs(label[test_idx] - pred_penalty) > 0.5\n> xgb_penalty$evaluation_log[250,]\n   iter train_error\n1:  250    0.332405\n> mean(error_penalty)\n[1] 0.3483\nNow the training error is only slightly lower than the error on the test set.\nThe predict method offers a convenient argument, ntreelimit, that forces only\nthe first i trees to be used in the prediction. This lets us directly compare the in-\nsample versus out-of-sample error rates as more models are included:\n> error_default <- rep(0, 250)\n> error_penalty <- rep(0, 250)\n> for(i in 1:250){\n  pred_def <- predict(xgb_default, predictors[test_idx,], ntreelimit=i)\n  error_default[i] <- mean(abs(label[test_idx] - pred_def) >= 0.5)\n  pred_pen <- predict(xgb_penalty, predictors[test_idx,], ntreelimit = i)\n  error_penalty[i] <- mean(abs(label[test_idx] - pred_pen) >= 0.5)\n}\nThe output from the model returns the error for the training set in the component\nxgb_default$evaluation_log. By combining this with the out-of-sample\nerrors, we can plot the errors versus the number of iterations:\n> errors <- rbind(xgb_default$evaluation_log,\n                  xgb_penalty$evaluation_log,\n                  data.frame(iter=1:250, train_error=error_default),\n",
      "content_length": 2231,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 426,
      "content": "                data.frame(iter=1:250, train_error=error_penalty))\n> errors$type <- rep(c('default train', 'penalty train',\n                       'default test', 'penalty test'), rep(250, 4))\n> ggplot(errors, aes(x=iter, y=train_error, group=type)) +\n  geom_line(aes(linetype=type, color=type))\nThe result, displayed in Figure 6-10, shows how the default model steadily\nimproves the accuracy for the training set but actually gets worse for the test set.\nThe penalized model does not exhibit this behavior.\n",
      "content_length": 508,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 427,
      "content": "Figure 6-10. The error rate of the default XGBoost versus a penalized version of XGBoost\nRIDGE REGRESSION AND THE LASSO\nAdding a penalty on the complexity of a model to help avoid overfitting dates back to the 1970s. Least\nsquares regression minimizes the residual sum of squares (RSS); see “Least Squares”. Ridge regression\nminimizes the sum of squared residuals plus a penalty on the number and size of the coefficients:\nThe value of \n determines how much the coefficients are penalized; larger values produce models that\nare less likely to overfit the data. The Lasso is similar, except that it uses Manhattan distance instead of\nEuclidean distance as a penalty term:\nThe xgboost parameters lambda and alpha are acting in a similar mannger.\n",
      "content_length": 744,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 428,
      "content": "Hyperparameters and Cross-Validation\nxgboost has a daunting array of hyperparameters; see “XGBoost\nHyperparameters” for a discussion. As seen in “Regularization: Avoiding\nOverfitting”, the specific choice can dramatically change the model fit. Given a\nhuge combination of hyperparameters to choose from, how should we be guided in\nour choice? A standard solution to this problem is to use cross-validation; see\n“Cross-Validation”. Cross-validation randomly splits up the data into K different\ngroups, also called folds. For each fold, a model is trained on the data not in the\nfold and then evaluated on the data in the fold. This yields a measure of accuracy\nof the model on out-of-sample data. The best set of hyperparameters is the one\ngiven by the model with the lowest overall error as computed by averaging the\nerrors from each of the folds.\nTo illustrate the technique, we apply it to parameter selection for xgboost. In this\nexample, we explore two parameters: the shrinkage parameter eta (see\n“XGBoost”) and the maximum depth of trees max_depth. The parameter\nmax_depth is the maximum depth of a leaf node to the root of the tree with a\ndefault value of 6. This gives us another way to control overfitting: deep trees\ntend to be more complex and may overfit the data. First we set up the folds and\nparameter list:\n> N <- nrow(loan_data)\n> fold_number <- sample(1:5, N, replace = TRUE)\n> params <- data.frame(eta = rep(c(.1, .5, .9), 3),\n                       max_depth = rep(c(3, 6, 12), rep(3,3)))\nNow we apply the preceding algorithm to compute the error for each model and\neach fold using five folds:\n> error <- matrix(0, nrow=9, ncol=5)\n> for(i in 1:nrow(params)){\n>   for(k in 1:5){\n>     fold_idx <- (1:N)[fold_number == k]\n>     xgb <- xgboost(data=predictors[-fold_idx,], label=label[-fold_idx],\n                     params = list(eta = params[i, 'eta'],\n                                   max_depth = params[i, 'max_depth']),\n                   objective = \"binary:logistic\", nrounds=100, verbose=0)\n>     pred <- predict(xgb, predictors[fold_idx,])\n>     error[i, k] <- mean(abs(label[fold_idx] - pred) >= 0.5)\n>   }\n> }\n",
      "content_length": 2141,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 429,
      "content": "Since we are fitting 45 total models, this can take a while. The errors are stored\nas a matrix with the models along the rows and folds along the columns. Using the\nfunction rowMeans, we can compare the error rate for the different parameter sets:\n> avg_error <- 100 * rowMeans(error)\n> cbind(params, avg_error)\n  eta max_depth avg_error\n1 0.1         3     35.41\n2 0.5         3     35.84\n3 0.9         3     36.48\n4 0.1         6     35.37\n5 0.5         6     37.33\n6 0.9         6     39.41\n7 0.1        12     36.70\n8 0.5        12     38.85\n9 0.9        12     40.19\nCross-validation suggests that using shallower trees with a smaller value of eta\nyields more accurate results. Since these models are also more stable, the best\nparameters to use are eta=0.1 and max_depth=3 (or possibly max_depth=6).\nXGBOOST HYPERPARAMETERS\nThe hyperparameters for xgboost are primarily used to balance overfitting with the accuracy and\ncomputational complexity. For a complete discussion of the parameters, refer to the xgboost\ndocumentation.\neta\nThe shrinkage factor between 0 and 1 applied to \n in the boosting algorithm. The default is 0.3,\nbut for noisy data, smaller values are recommended (e.g., 0.1).\nnrounds\nThe number of boosting rounds. If eta is set to a small value, it is important to increase the number\nof rounds since the algorithm learns more slowly. As long as some parameters are included to\nprevent overfitting, having more rounds doesn’t hurt.\nmax_depth\nThe maximum depth of the tree (the default is 6). In contrast to the random forest, which fits very\ndeep trees, boosting usually fits shallow trees. This has the advantage of avoiding spurious\ncomplex interactions in the model that can arise from noisy data.\nsubsample and colsample_bytree\nFraction of the records to sample without replacement and the fraction of predictors to sample for\nuse in fitting the trees. These parameters, which are similar to those in random forests, help avoid\noverfitting.\nlambda and alpha\nThe regularization parameters to help control overfitting (see “Regularization: Avoiding\nOverfitting”).\n",
      "content_length": 2089,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 430,
      "content": "KEY IDEAS FOR BOOSTING\nBoosting is a class of ensemble models based on fitting a sequence of models, with more weight\ngiven to records with large errors in successive rounds.\nStochastic gradient boosting is the most general type of boosting and offers the best performance.\nThe most common form of stochastic gradient boosting uses tree models.\nXGBoost is a popular and computationally efficient software package for stochastic gradient\nboosting; it is available in all common languages used in data science.\nBoosting is prone to overfitting the data, and the hyperparameters need to be tuned to avoid this.\nRegularization is one way to avoid overfitting by including a penalty term on the number of\nparameters (e.g., tree size) in a model.\nCross-validation is especially important for boosting due to the large number of hyperparameters\nthat need to be set.\n",
      "content_length": 859,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 431,
      "content": "Summary\nThis chapter describes two classification and prediction methods that “learn”\nflexibly and locally from data, rather than starting with a structural model (e.g., a\nlinear regression) that is fit to the entire data set. K-Nearest Neighbors is a simple\nprocess that simply looks around at similar records and assigns their majority\nclass (or average value) to the record being predicted. Trying various cutoff\n(split) values of predictor variables, tree models iteratively divide the data into\nsections and subsections that are increasingly homogeneous with respect to class.\nThe most effective split values form a path, and also a “rule,” to a classification\nor prediction. Tree models are a very powerful and popular predictive tool, often\noutperforming other methods. They have given rise to various ensemble methods\n(random forests, boosting, bagging) that sharpen the predictive power of trees.\nThis and subsequent sections in this chapter © 2017 Datastats, LLC, Peter Bruce and Andrew Bruce,\nused by permission.\nThe term CART is a registered trademark of Salford Systems related to their specific implementation of\ntree models.\nThe term random forest is a trademark of Leo Breiman and Adele Cutler and licensed to Salford\nSystems. There is no standard nontrademark name, and the term random forest is as synonymous with the\nalgorithm as Kleenex is with facial tissues.\n1\n2\n3\n",
      "content_length": 1387,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 432,
      "content": "Chapter 7. Unsupervised Learning\nThe term unsupervised learning refers to statistical methods that extract meaning\nfrom data without training a model on labeled data (data where an outcome of\ninterest is known). In Chapters 4 and 5, the goal is to build a model (set of rules)\nto predict a response from a set of predictor variables. Unsupervised learning\nalso constructs a model of the data, but does not distinguish between a response\nvariable and predictor variables.\nUnsupervised learning can have different possible goals. In some cases, it can be\nused to create a predictive rule in the absence of a labeled response. Clustering\nmethods can be used to identify meaningful groups of data. For example, using the\nweb clicks and demographic data of a user on a website, we may be able to group\ntogether different types of users. The website could then be personalized to these\ndifferent types.\nIn other cases, the goal may be to reduce the dimension of the data to a more\nmanageable set of variables. This reduced set could then be used as input into a\npredictive model, such as regression or classification. For example, we may have\nthousands of sensors to monitor an industrial process. By reducing the data to a\nsmaller set of features, we may be able to build a more powerful and interpretable\nmodel to predict process failure than by including data streams from thousands of\nsensors.\nFinally, unsupervised learning can be viewed as an extension of the exploratory\ndata analysis (see Chapter 1) to situations where you are confronted with a large\nnumber of variables and records. The aim is to gain insight into a set of data and\nhow the different variables relate to each other. Unsupervised techniques give\nways to sift through and analyze these variables and discover relationships.\nUNSUPERVISED LEARNING AND PREDICTION\nUnsupervised learning can play an important role for prediction, both for regression and classification\nproblems. In some cases, we want to predict a category in the absence of any labeled data. For example,\nwe might want to predict the type of vegetation in an area from a set of satellite sensory data. Since we\ndon’t have a response variable to train a model, clustering gives us a way to identify common patterns\nand categorize the regions.\n",
      "content_length": 2275,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 433,
      "content": "Clustering is an especially important tool for the “cold-start problem.” In these types of problems, such as\nlaunching a new marketing campaign or identifying potential new types of fraud or spam, we initially may\nnot have any response to train a model. Over time, as data is collected, we can learn more about the\nsystem and build a traditional predictive model. But clustering helps us start the learning process more\nquickly by identifying population segments.\nUnsupervised learning is also important as a building block for regression and classification techniques.\nWith big data, if a small subpopulation is not well represented in the overall population, the trained model\nmay not perform well for that subpopulation. With clustering, it is possible to identify and label\nsubpopulations. Separate models can then be fit to the different subpopulations. Alternatively, the\nsubpopulation can be represented with its own feature, forcing the overall model to explicitly consider\nsubpopulation identity as a predictor.\n",
      "content_length": 1021,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 434,
      "content": "Principal Components Analysis\nOften, variables will vary together (covary), and some of the variation in one is\nactually duplicated by variation in another. Principal components analysis (PCA)\nis a technique to discover the way in which numeric variables covary.1\nKEY TERMS FOR PRINCIPAL COMPONENTS ANALYSIS\nPrincipal component\nA linear combination of the predictor variables.\nLoadings\nThe weights that transform the predictors into the components.\nSynonym\nWeights\nScreeplot\nA plot of the variances of the components, showing the relative importance of the components.\nThe idea in PCA is to combine multiple numeric predictor variables into a smaller\nset of variables, which are weighted linear combinations of the original set. The\nsmaller set of variables, the principal components, “explains” most of the\nvariability of the full set of variables, reducing the dimension of the data. The\nweights used to form the principal components reveal the relative contributions of\nthe original variables to the new principal components.\nPCA was first proposed by Karl Pearson. In what was perhaps the first paper on\nunsupervised learning, Pearson recognized that in many problems there is\nvariability in the predictor variables, so he developed PCA as a technique to\nmodel this variability. PCA can be viewed as the unsupervised version of linear\ndiscriminant analysis; see“Discriminant Analysis”.\n",
      "content_length": 1390,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 435,
      "content": "A Simple Example\nFor two variables, \n and \n, there are two principal components \n (\n or\n2):\nThe weights \n are known as the component loadings. These transform\nthe original variables into the principal components. The first principal\ncomponent, \n, is the linear combination that best explains the total variation. The\nsecond principal component, \n, explains the remaining variation (it is also the\nlinear combination that is the worst fit).\nNOTE\nIt is also common to compute principal components on deviations from the means of the predictor\nvariables, rather than on the values themselves.\nYou can compute principal components in R using the princomp function. The\nfollowing performs a PCA on the stock price returns for Chevron (CVX) and\nExxonMobil (XOM):\noil_px <- sp500_px[, c('CVX', 'XOM')]\npca <- princomp(oil_px)\npca$loadings\nLoadings:\n    Comp.1 Comp.2\nCVX -0.747  0.665\nXOM -0.665 -0.747\nThe weights for CVX and XOM for the first principal component are –0.747 and –\n0.665 and for the second principal component they are 0.665 and –0.747. How to\ninterpret this? The first principal component is essentially an average of CVX and\nXOM, reflecting the correlation between the two energy companies. The second\nprincipal component measures when the stock prices of CVX and XOM diverge.\n",
      "content_length": 1289,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 436,
      "content": "It is instructive to plot the principal components with the data:\nloadings <- pca$loadings\nggplot(data=oil_px, aes(x=CVX, y=XOM)) +\n  geom_point(alpha=.3) +\n  stat_ellipse(type='norm', level=.99) +\n  geom_abline(intercept = 0, slope = loadings[2,1]/loadings[1,1]) +\n  geom_abline(intercept = 0, slope = loadings[2,2]/loadings[1,2])\nThe result is shown in Figure 7-1.\n",
      "content_length": 367,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 437,
      "content": "Figure 7-1. The principal components for the stock returns for Chevron and ExxonMobil\nThe solid dashed lines show the two principal components: the first one is along\nthe long axis of the ellipse and the second one is along the short axis. You can see\nthat a majority of the variability in the two stock returns is explained by the first\nprincipal component. This makes sense since energy stock prices tend to move as\na group.\n",
      "content_length": 427,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 438,
      "content": "NOTE\nThe weights for the first principal component are both negative, but reversing the sign of all the\nweights does not change the principal component. For example, using weights of 0.747 and 0.665\nfor the first principal component is equivalent to the negative weights, just as an infinite line\ndefined by the origin and 1,1 is the same as one defined by the origin and –1, –1.\n",
      "content_length": 380,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 439,
      "content": "Computing the Principal Components\nGoing from two variables to more variables is straightforward. For the first\ncomponent, simply include the additional predictor variables in the linear\ncombination, assigning weights that optimize the collection of the covariation from\nall the predictor variables into this first principal component (covariance is the\nstatistical term; see “Covariance Matrix”). Calculation of principal components is\na classic statistical method, relying on either the correlation matrix of the data or\nthe covariance matrix, and it executes rapidly, not relying on iteration. As noted\nearlier, it works only with numeric variables, not categorical ones. The full\nprocess can be described as follows:\n1. In creating the first principal component, PCA arrives at the linear\ncombination of predictor variables that maximizes the percent of total\nvariance explained.\n2. This linear combination then becomes the first “new” predictor, Z1.\n3. PCA repeats this process, using the same variables, with different\nweights to create a second new predictor, Z2. The weighting is done such\nthat Z1 and Z2 are uncorrelated.\n4. The process continues until you have as many new variables, or\ncomponents, Zi as original variables Xi.\n5. Choose to retain as many components as are needed to account for most\nof the variance.\n6. The result so far is a set of weights for each component. The final step is\nto convert the original data into new principal component scores by\napplying the weights to the original values. These new scores can then be\nused as the reduced set of predictor variables.\n",
      "content_length": 1597,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 440,
      "content": "Interpreting Principal Components\nThe nature of the principal components often reveals information about the\nstructure of the data. There are a couple of standard visualization displays to help\nyou glean insight about the principal components. One such method is a Screeplot\nto visualize the relative importance of principal components (the name derives\nfrom the resemblance of the plot to a scree slope). The following is an example\nfor a few top companies in the S&P 500:\nsyms <- c( 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM',\n   'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\ntop_sp <- sp500_px[row.names(sp500_px)>='2005-01-01', syms]\nsp_pca <- princomp(top_sp)\nscreeplot(sp_pca)\nAs seen in Figure 7-2, the variance of the first principal component is quite large\n(as is often the case), but the other top principal components are significant.\n",
      "content_length": 873,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 441,
      "content": "Figure 7-2. A screeplot for a PCA of top stocks from the S&P 500\nIt can be especially revealing to plot the weights of the top principal components.\nOne way to do this is to use the gather function from the tidyr package in\nconjunction with ggplot:\nlibrary(tidyr)\nloadings <- sp_pca$loadings[,1:5]\nloadings$Symbol <- row.names(loadings)\nloadings <- gather(loadings, \"Component\", \"Weight\", -Symbol)\nggplot(loadings, aes(x=Symbol, y=Weight)) +\n  geom_bar(stat='identity') +\n",
      "content_length": 472,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 442,
      "content": "  facet_grid(Component ~ ., scales='free_y')\nThe loadings for the top five components are shown in Figure 7-3. The loadings\nfor the first principal component have the same sign: this is typical for data in\nwhich all the columns share a common factor (in this case, the overall stock\nmarket trend). The second component captures the price changes of energy stocks\nas compared to the other stocks. The third component is primarily a contrast in the\nmovements of Apple and CostCo. The fourth component contrasts the movements\nof Schlumberger to the other energy stocks. Finally, the fifth component is mostly\ndominated by financial companies.\n",
      "content_length": 640,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 443,
      "content": "Figure 7-3. The loadings for the top five principal components of stock price returns\n",
      "content_length": 86,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 444,
      "content": "HOW MANY COMPONENTS TO CHOOSE?\nIf your goal is to reduce the dimension of the data, you must decide how many principal\ncomponents to select. The most common approach is to use an ad hoc rule to select the\ncomponents that explain “most” of the variance. You can do this visually through the screeplot;\nfor example, in Figure 7-2, it would be natural to restrict the analysis to the top five components.\nAlternatively, you could select the top components such that the cumulative variance exceeds a\nthreshold, such as 80%. Also, you can inspect the loadings to determine if the component has an\nintuitive interpretation. Cross-validation provides a more formal method to select the number of\nsignificant components (see “Cross-Validation” for more).\nKEY IDEAS FOR PRINCIPAL COMPONENTS\nPrincipal components are linear combinations of the predictor variables (numeric data only).\nThey are calculated so as to minimize correlation between components, reducing redundancy.\nA limited number of components will typically explain most of the variance in the outcome variable.\nThe limited set of principal components can then be used in place of the (more numerous) original\npredictors, reducing dimensionality.\n",
      "content_length": 1202,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 445,
      "content": "Further Reading\nFor a detailed look at the use of cross-validation in principal components, see\nRasmus Bro, K. Kjeldahl, A.K. Smilde, and Henk A. L. Kiers, “Cross-Validation\nof Component Models: A Critical Look at Current Methods”, Analytical and\nBioanalytical Chemistry 390, no. 5 (2008).\n",
      "content_length": 290,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 446,
      "content": "K-Means Clustering\nClustering is a technique to divide data into different groups, where the records in\neach group are similar to one another. A goal of clustering is to identify significant\nand meaningful groups of data. The groups can be used directly, analyzed in more\ndepth, or passed as a feature or an outcome to a predictive regression or\nclassification model. K-means is the first clustering method to be developed; it is\nstill widely used, owing its popularity to the relative simplicity of the algorithm\nand its ability to scale to large data sets.\nKEY TERMS FOR K-MEANS CLUSTERING\nCluster\nA group of records that are similar.\nCluster mean\nThe vector of variable means for the records in a cluster.\nK\nThe number of clusters.\nK-means divides the data into K clusters by minimizing the sum of the squared\ndistances of each record to the mean of its assigned cluster. The is referred to as\nthe within-cluster sum of squares or within-cluster SS. K-means does not ensure\nthe clusters will have the same size, but finds the clusters that are the best\nseparated.\n",
      "content_length": 1067,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 447,
      "content": "NORMALIZATION\nIt is typical to normalize (standardize) continuous variables by subtracting the mean and dividing\nby the standard deviation. Otherwise, variables with large scale will dominate the clustering\nprocess (see “Standardization (Normalization, Z-Scores)”).\n",
      "content_length": 266,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 448,
      "content": "A Simple Example\nStart by considering a data set with n records and just two variables,  and .\nSuppose we want to split the data into \n clusters. This means assigning\neach record \n to a cluster k. Given an assignment of \n records to cluster k,\nthe center of the cluster \n is the mean of the points in the cluster:\n",
      "content_length": 314,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 449,
      "content": "CLUSTER MEAN\nIn clustering records with multiple variables (the typical case), the term cluster mean refers not\nto a single number, but to the vector of means of the variables.\nThe sum of squares within a cluster is given by:\nK-means finds the assignment of records that minimizes within-cluster sum of\nsquares across all four clusters \n.\nK-means clustering can be used to gain insight into how the price movements of\nstocks tend to cluster. Note that stock returns are reported in a fashion that is, in\neffect, standardized, so we do not need to normalize the data. In R, K-means\nclustering can be performed using the kmeans function. For example, the\nfollowing finds four clusters based on two variables: the stock returns for\nExxonMobil (XOM) and Chevron (CVX):\ndf <- sp500_px[row.names(sp500_px)>='2011-01-01', c('XOM', 'CVX')]\nkm <- kmeans(df, centers=4)\nThe cluster assignment for each record is returned as the cluster component:\n> df$cluster <- factor(km$cluster)\n> head(df)\n                  XOM        CVX cluster\n2011-01-03 0.73680496  0.2406809       2\n",
      "content_length": 1065,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 450,
      "content": "2011-01-04 0.16866845 -0.5845157       1\n2011-01-05 0.02663055  0.4469854       2\n2011-01-06 0.24855834 -0.9197513       1\n2011-01-07 0.33732892  0.1805111       2\n2011-01-10 0.00000000 -0.4641675       1\nThe first six records are assigned to either cluster 1 or cluster 2. The means of the\nclusters are also returned:\n> centers <- data.frame(cluster=factor(1:4), km$centers)\n> centers\n  cluster        XOM        CVX\n1       1 -0.3284864 -0.5669135\n2       2  0.2410159  0.3342130\n3       3 -1.1439800 -1.7502975\n4       4  0.9568628  1.3708892\nClusters 1 and 3 represent “down” markets, while clusters 2 and 4 represent “up\nmarkets.” In this example, with just two variables, it is straightforward to\nvisualize the clusters and their means:\nggplot(data=df, aes(x=XOM, y=CVX, color=cluster, shape=cluster)) +\n  geom_point(alpha=.3) +\n  geom_point(data=centers,  aes(x=XOM, y=CVX), size=3, stroke=2)\nThe resulting plot, given by Figure 7-4, shows the cluster assignments and the\ncluster means.\n",
      "content_length": 994,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 451,
      "content": "Figure 7-4. The clusters of K-means applied to stock price data for ExxonMobil and Chevron (the two\ncluster centers in the dense area are hard to distinguish)\n",
      "content_length": 159,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 452,
      "content": "K-Means Algorithm\nIn general, K-means can be applied to a data set with p variables \n.\nWhile the exact solution to K-means is computationally very difficult, heuristic\nalgorithms provide an efficient way to compute a locally optimal solution.\nThe algorithm starts with a user-specified K and an initial set of cluster means,\nthen iterates the following steps:\n1. Assign each record to the nearest cluster mean as measured by squared\ndistance.\n2. Compute the new cluster means based on the assignment of records.\nThe algorithm converges when the assignment of records to clusters does not\nchange.\nFor the first iteration, you need to specify an initial set of cluster means. Usually\nyou do this by randomly assigning each record to one of the K clusters, then\nfinding the means of those clusters.\nSince this algorithm isn’t guaranteed to find the best possible solution, it is\nrecommended to run the algorithm several times using different random samples to\ninitialize the algorithm. When more than one set of iterations is used, the K-means\nresult is given by the iteration that has the lowest within-cluster sum of squares.\nThe nstart parameter to the R function kmeans allows you to specify the number\nof random starts to try. For example, the following code runs K-means to find 5\nclusters using 10 different starting cluster means:\nsyms <- c( 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM', 'SLB', 'COP',\n           'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\ndf <- sp500_px[row.names(sp500_px)>='2011-01-01', syms]\nkm <- kmeans(df, centers=5, nstart=10)\nThe function automatically returns the best solution out of the 10 different starting\npoints. You can use the argument iter.max to set the maximum number of\niterations the algorithm is allowed for each random start.\n",
      "content_length": 1785,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 453,
      "content": "Interpreting the Clusters\nAn important part of cluster analysis can involve the interpretation of the clusters.\nThe two most important outputs from kmeans are the sizes of the clusters and the\ncluster means. For the example in the previous subsection, the sizes of resulting\nclusters are given by this R command:\nkm$size\n[1] 186 106 285 288 266\nThe cluster sizes are relatively balanced. Imbalanced clusters can result from\ndistant outliers, or groups of records very distinct from the rest of the data — both\nmay warrant further inspection.\nYou can plot the centers of the clusters using the gather function in conjunction\nwith ggplot:\ncenters <- as.data.frame(t(centers))\nnames(centers) <- paste(\"Cluster\", 1:5)\ncenters$Symbol <- row.names(centers)\ncenters <- gather(centers, \"Cluster\", \"Mean\", -Symbol)\ncenters$Color = centers$Mean > 0\nggplot(centers, aes(x=Symbol, y=Mean, fill=Color)) +\n  geom_bar(stat='identity', position = \"identity\", width=.75) +\n  facet_grid(Cluster ~ ., scales='free_y')\nThe resulting plot is shown in Figure 7-5 and reveals the nature of each cluster.\nFor example, clusters 1 and 2 correspond to days on which the market is down\nand up, respectively. Clusters 3 and 5 are characterized by up-market days for\nconsumer stocks and down-market days for energy stocks, respectively. Finally,\ncluster 4 captures the days in which energy stocks were up and consumer stocks\nwere down.\n",
      "content_length": 1406,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 454,
      "content": "Figure 7-5. The means of the variables in each cluster (“cluster means”)\n",
      "content_length": 73,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 455,
      "content": "CLUSTER ANALYSIS VERSUS PCA\nThe plot of cluster means is similar in spirit to looking at the loadings for principal component\nanalysis (PCA); see “Interpreting Principal Components”. A major distinction is that unlike with\nPCA, the sign of the cluster means is meaningful. PCA identifies principal directions of variation,\nwhereas cluster analysis finds groups of records located near one another.\n",
      "content_length": 398,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 456,
      "content": "Selecting the Number of Clusters\nThe K-means algorithm requires that you specify the number of clusters K.\nSometimes the number of clusters is driven by the application. For example, a\ncompany managing a sales force might want to cluster customers into “personas”\nto focus and guide sales calls. In such a case, managerial considerations would\ndictate the number of desired customer segments — for example, two might not\nyield useful differentiation of customers, while eight might be too many to manage.\nIn the absence of a cluster number dictated by practical or managerial\nconsiderations, a statistical approach could be used. There is no single standard\nmethod to find the “best” number of clusters.\nA common approach, called the elbow method, is to identify when the set of\nclusters explains “most” of the variance in the data. Adding new clusters beyond\nthis set contributes relatively little incremental contribution in the variance\nexplained. The elbow is the point where the cumulative variance explained\nflattens out after rising steeply, hence the name of the method.\nFigure 7-6 shows the cumulative percent of variance explained for the default data\nfor the number of clusters ranging from 2 to 15. Where is the elbow in this\nexample? There is no obvious candidate, since the incremental increase in\nvariance explained drops gradually. This is fairly typical in data that does not\nhave well-defined clusters. This is perhaps a drawback of the elbow method, but\nit does reveal the nature of the data.\n",
      "content_length": 1512,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 457,
      "content": "Figure 7-6. The elbow method applied to the stock data\nIn R, the kmeans function doesn’t provide a single command for applying the\nelbow method, but it can be readily applied from the output of kmeans as shown\nhere:\npct_var <- data.frame(pct_var = 0,\n                      num_clusters=2:14)\ntotalss <- kmeans(df, centers=14, nstart=50, iter.max = 100)$totss\nfor(i in 2:14){\n  pct_var[i-1, 'pct_var'] <- kmeans(df, centers=i, nstart=50, iter.max = 100)\n    $betweenss/totalss\n}\nIn evaluating how many clusters to retain, perhaps the most important test is this:\nhow likely are the clusters to be replicated on new data? Are the clusters\ninterpretable, and do they relate to a general characteristic of the data, or do they\njust reflect a specific instance? You can assess this, in part, using cross-\n",
      "content_length": 800,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 458,
      "content": "validation; see “Cross-Validation”.\nIn general, there is no single rule that will reliably guide how many clusters to\nproduce.\nNOTE\nThere are several more formal ways to determine the number of clusters based on statistical or\ninformation theory. For example, Robert Tibshirani, Guenther Walther, and Trevor Hastie\n(http://www.stanford.edu/~hastie/Papers/gap.pdf) propose a “gap” statistic based on statistical\ntheory to identify the elbow. For most applications, a theoretical approach is probably not\nnecessary, or even appropriate.\nKEY IDEAS FOR K-MEANS CLUSTERING\nThe number of desired clusters, K, is chosen by the user.\nThe algorithm develops clusters by iteratively assigning records to the nearest cluster mean until\ncluster assignments do not change.\nPractical considerations usually dominate the choice of K; there is no statistically determined\noptimal number of clusters.\n",
      "content_length": 884,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 459,
      "content": "Hierarchical Clustering\nHierarchical clustering is an alternative to K-means that can yield very different\nclusters. Hierarchical clustering is more flexible than K-means and more easily\naccommodates non-numerical variables. It is more sensitive in discovering\noutlying or aberrant groups or records. Hierarchical clustering also lends itself to\nan intuitive graphical display, leading to easier interpretation of the clusters.\nKEY TERMS FOR HIERARCHICAL CLUSTERING\nDendrogram\nA visual representation of the records and the hierarchy of clusters to which they belong.\nDistance\nA measure of how close one record is to another.\nDissimilarity\nA measure of how close one cluster is to another.\nHierarchical clustering’s flexibility comes with a cost, and hierarchical clustering\ndoes not scale well to large data sets with millions of records. For even modest-\nsized data with just tens of thousands of records, hierarchical clustering can\nrequire intensive computing resources. Indeed, most of the applications of\nhierarchical clustering are focused on relatively small data sets.\n",
      "content_length": 1078,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 460,
      "content": "A Simple Example\nHierarchical clustering works on a data set with n records and p variables and is\nbased on two basic building blocks:\nA distance metric \n to measure the distance beween two records i and j.\nA dissimilarity metric \n to measure the difference between two clusters\nA and B based on the distances \n between the members of each cluster.\nFor applications involving numeric data, the most importance choice is the\ndissimilarity metric. Hierarchical clustering starts by setting each record as its\nown cluster and iterates to combine the least dissimilar clusters.\nIn R, the hclust function can be used to perform hierarchical clustering. One big\ndifference with hclust versus kmeans is that it operates on the pairwise\ndistances \n rather than the data itself. You can compute these using the dist\nfunction. For example, the following applies hierarchical clustering to the stock\nreturns for a set of companies:\nsyms1 <- c('GOOGL', 'AMZN', 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX',\n           'XOM', 'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP',\n           'WMT', 'TGT', 'HD', 'COST')\n# take transpose: to cluster companies, we need the stocks along the rows\ndf <- t(sp500_px[row.names(sp500_px)>='2011-01-01', syms1])\nd <- dist(df)\nhcl <- hclust(d)\nClustering algorithms will cluster the records (rows) of a data frame. Since we\nwant to cluster the companies, we need to transpose the data frame and put the\nstocks along the rows and the dates along the columns.\n",
      "content_length": 1466,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 461,
      "content": "The Dendrogram\nHierarchical clustering lends itself to a natural graphical display as a tree,\nreferred to as a dendrogram. The name comes from the Greek words dendro\n(tree) and gramma (drawing). In R, you can easily produce this using the plot\ncommand:\nplot(hcl)\nThe result is shown in Figure 7-7. The leaves of the tree correspond to the\nrecords. The length of the branch in the tree indicates the degree of dissimilarity\nbetween corresponding clusters. The returns for Google and Amazon are quite\ndissimilar to the returns for the other stocks. The other stocks fall into natural\ngroups: energy stocks, financial stocks, and consumer stocks are all separated into\ntheir own subtrees.\n",
      "content_length": 686,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 462,
      "content": "Figure 7-7. A dendogram of stocks\nIn contrast to K-means, it is not necessary to prespecify the number of clusters. To\nextract a specific number of clusters, you can use the cutree function:\ncutree(hcl, k=4)\nGOOGL  AMZN  AAPL  MSFT  CSCO  INTC   CVX   XOM   SLB   COP   JPM   WFC\n    1     2     3     3     3     3     4     4     4     4     3     3\n  USB   AXP   WMT   TGT    HD  COST\n    3     3     3     3     3     3\n",
      "content_length": 424,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 463,
      "content": "The number of clusters to extract is set to 4, and you can see that Google and\nAmazon each belong to their own cluster. The oil stocks (XOM, CVS, SLB, COP)\nall belong to another cluster. The remaining stocks are in the fourth cluster.\n",
      "content_length": 235,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 464,
      "content": "The Agglomerative Algorithm\nThe main algorithm for hierarchical clustering is the agglomerative algorithm,\nwhich iteratively merges similar clusters. The agglomerative algorithm begins\nwith each record constituting its own single-record cluster, then builds up larger\nand larger clusters. The first step is to calculate distances between all pairs of\nrecords.\nFor each pair of records \n and \n, we measure\nthe distance between the two records, \n, using a distance metric (see “Distance\nMetrics”). For example, we can use Euclidian distance:\nWe now turn to inter-cluster distance. Consider two clusters A and B, each with a\ndistinctive set of records, \n and \n.\nWe can measure the dissimilarity between the clusters \n by using the\ndistances between the members of A and the members of B.\nOne measure of dissimilarity is the complete-linkage method, which is the\nmaximum distance across all pairs of records between A and B:\nThis defines the dissimilarity as the biggest difference between all pairs.\nThe main steps of the agglomerative algorithm are:\n1. Create an initial set of clusters with each cluster consisting of a single\nrecord for all records in the data.\n2. Compute the dissimilarity \n) between all pairs of clusters \n.\n3. Merge the two clusters \n and \n that are least dissimilar as measured\nby \n).\n",
      "content_length": 1306,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 465,
      "content": "4. If we have more than one cluster remaining, return to step 2. Otherwise,\nwe are done.\n",
      "content_length": 89,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 466,
      "content": "Measures of Dissimilarity\nThere are four common measures of dissimilarity: complete linkage, single\nlinkage, average linkage, and minimum variance. These (plus other measures)\nare all supported by most hierarchical clustering software, including hclust. The\ncomplete linkage method defined earlier tends to produce clusters with members\nthat are similar. The single linkage method is the minimum distance between the\nrecords in two clusters:\nThis is a “greedy” method and produces clusters that can contain quite disparate\nelements. The average linkage method is the average of all distance pairs and\nrepresents a compromise between the single and complete linkage methods.\nFinally, the minimum variance method, also referred to as Ward’s method, is\nsimilar to K-means since it minimizes the within-cluster sum of squares (see “K-\nMeans Clustering”).\nFigure 7-8 applies hierarchical clustering using the four measures to the\nExxonMobil and Chevron stock returns. For each measure, four clusters are\nretained.\n",
      "content_length": 1009,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 467,
      "content": "Figure 7-8. A comparison of measures of dissimilarity applied to stock data\nThe results are strikingly different: the single linkage measure assigns almost all\nof the points to a single cluster. Except for the minimum variance method\n(Ward.D), all measures end up with at least one cluster with just a few outlying\npoints. The minimum variance method is most similar to the K-means cluster;\ncompare with Figure 7-4.\nKEY IDEAS FOR HIERARCHICAL CLUSTERING\nStart with every record in its own cluster.\nProgressively, clusters are joined to nearby clusters until all records belong to a single cluster (the\nagglomerative algorithm).\nThe agglomeration history is retained and plotted, and the user (without specifying the number of\nclusters beforehand) can visualize the number and structure of clusters at different stages.\nInter-cluster distances are computed in different ways, all relying on the set of all inter-record\ndistances.\n",
      "content_length": 929,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 469,
      "content": "Model-Based Clustering\nClustering methods such as hierarchical clustering and K-means are based on\nheuristics and rely primarily on finding clusters whose members are close to one\nanother, as measured directly with the data (no probability model involved). In the\npast 20 years, significant effort has been devoted to developing model-based\nclustering methods. Adrian Raftery and other researchers at the University of\nWashington made critical contributions to model-based clustering, including both\ntheory and software. The techniques are grounded in statistical theory and provide\nmore rigorous ways to determine the nature and number of clusters. They could be\nused, for example, in cases where there might be one group of records that are\nsimilar to one another but not necessarily close to one another (e.g., tech stocks\nwith high variance of returns), and another group of records that is similar, and\nalso close (e.g., utility stocks with low variance).\n",
      "content_length": 961,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 470,
      "content": "Multivariate Normal Distribution\nThe most widely used model-based clustering methods rest on the multivariate\nnormal distribution. The multivariate normal distribution is a generalization of the\nnormal distribution to set of p variables \n. The distribution is\ndefined by a set of means \n and a covariance matrix \n. The\ncovariance matrix is a measure of how the variables correlate with each other\n(see “Covariance Matrix” for details on the covariance). The covariance matrix \n consists of p variances \n and covariances \n for all pairs of\nvariables \n. With the variables put along the rows and duplicated along the\ncolumns, the matrix looks like this:\nSince a covariance matrix is symmetric, and \n, there are only \n covariance terms. In total, the covariance matrix has \n parameters. The distribution is denoted by:\n",
      "content_length": 816,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 471,
      "content": "This is a symbolic way of saying that the variables are all normally distributed,\nand the overall distribution is fully described by the vector of variable means and\nthe covariance matrix.\nFigure 7-9 shows the probability contours for a multivariate normal distribution\nfor two variables X and Y (the 0.5 probability contour, for example, contains 50%\nof the distribution).\nThe means are \n and \n and the covariance matrix is:\nSince the covariance \n is positive, X and Y are positively correlated.\n",
      "content_length": 497,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 472,
      "content": "Figure 7-9. Probability contours for a two-dimensional normal distribution\n",
      "content_length": 75,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 473,
      "content": "Mixtures of Normals\nThe key idea behind model-based clustering is that each record is assumed to be\ndistributed as one of K multivariate-normal distributions, where K is the number\nof clusters. Each distribution has a different mean  and covariance matrix \n.\nFor example, if you have two variables, X and Y, then each row \n is\nmodeled as having been sampled from one of K distributions \n.\nR has a very rich package for model-based clustering called mclust, originally\ndeveloped by Chris Fraley and Adrian Raftery. With this package, we can apply\nmodel-based clustering to the stock return data we previously analyzed using K-\nmeans and hierarchical clustering:\n> library(mclust)\n> df <- sp500_px[row.names(sp500_px)>='2011-01-01', c('XOM', 'CVX')]\n> mcl <- Mclust(df)\n> summary(mcl)\nMclust VEE (ellipsoidal, equal shape and orientation) model with 2 components:\n log.likelihood    n df       BIC       ICL\n      -2255.134 1131  9 -4573.546 -5076.856\nClustering table:\n  1   2\n963 168\nIf you execute this code, you will notice that the computation takes significiantly\nlonger than other procedures. Extracting the cluster assignments using the predict\nfunction, we can visualize the clusters:\ncluster <- factor(predict(mcl)$classification)\nggplot(data=df, aes(x=XOM, y=CVX, color=cluster, shape=cluster)) +\n  geom_point(alpha=.8)\nThe resulting plot is shown in Figure 7-10. There are two clusters: one cluster in\nthe middle of the data, and a second cluster in the outer edge of the data. This is\nvery different from the clusters obtained using K-means (Figure 7-4) and\nhierarchical clustering (Figure 7-8), which find clusters that are compact.\n",
      "content_length": 1645,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 474,
      "content": "Figure 7-10. Two clusters are obtained for stock return data using mclust\nYou can extract the parameters to the normal distributions using the summary\nfunction:\n> summary(mcl, parameters=TRUE)$mean\n          [,1]        [,2]\nXOM 0.05783847 -0.04374944\nCVX 0.07363239 -0.21175715\n> summary(mcl, parameters=TRUE)$variance\n, , 1\n          XOM       CVX\nXOM 0.3002049 0.3060989\nCVX 0.3060989 0.5496727\n, , 2\n         XOM      CVX\nXOM 1.046318 1.066860\nCVX 1.066860 1.915799\n",
      "content_length": 470,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 475,
      "content": "The distributions have similar means and correlations, but the second distribution\nhas much larger variances and covariances.\nThe clusters from mclust may seem surprising, but in fact, they illustrate the\nstatistical nature of the method. The goal of model-based clustering is to find the\nbest-fitting set of multivariate normal distributions. The stock data appears to have\na normal-looking shape: see the contours of Figure 7-9. In fact, though, stock\nreturns have a longer-tailed distribution than a normal distribution. To handle this,\nmclust fits a distribution to the bulk of the data, but then fits a second distribution\nwith a bigger variance.\n",
      "content_length": 652,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 476,
      "content": "Selecting the Number of Clusters\nUnlike K-means and hierarchical clustering, mclust automatically selects the\nnumber of clusters (in this case, two). It does this by choosing the number of\nclusters for which the Bayesian Information Criteria (BIC) has the largest value.\nBIC (similar to AIC) is a general tool to find the best model amongst a candidate\nset of models. For example, AIC (or BIC) is commonly used to select a model in\nstepwise regression; see “Model Selection and Stepwise Regression”. BIC works\nby selecting the best-fitting model with a penalty for the number of parameters in\nthe model. In the case of model-based clustering, adding more clusters will\nalways improve the fit at the expense of introducing additional parameters in the\nmodel.\nYou can plot the BIC value for each cluster size using a function in hclust:\nplot(mcl, what='BIC', ask=FALSE)\nThe number of clusters — or number of different multivariate normal models\n(components) — is shown on the x-axis (see Figure 7-11).\n",
      "content_length": 1000,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 477,
      "content": "Figure 7-11. BIC scores for the stock return data for different numbers of clusters (components)\nThis plot is similar to the elbow plot used to identify the number of clusters to\nchoose for K-means, except the value being plotted is BIC instead of percent of\nvariance explained (see Figure 7-6). One big difference is that instead of one line,\nmclust shows 14 different lines! This is because mclust is actually fitting 14\ndifferent models for each cluster size, and ultimately it chooses the best-fitting\n",
      "content_length": 506,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 478,
      "content": "model.\nWhy does mclust fit so many models to determine the best set of multivariate\nnormals? It’s because there are different ways to parameterize the covariance\nmatrix \n for fitting a model. For the most part, you do not need to worry about the\ndetails of the models and can simply use the model chosen by mclust. In this\nexample, according to BIC, three different models (called VEE, VEV, and VVE)\ngive the best fit using two components.\nNOTE\nModel-based clustering is a rich and rapidly developing area of study, and the coverage in this text\nonly spans a small part of the field. Indeed, the mclust help file is currently 154 pages long.\nNavigating the nuances of model-based clustering is probably more effort than is needed for most\nproblems encountered by data scientists.\nModel-based clustering techniques do have some limitations. The methods require\nan underlying assumption of a model for the data, and the cluster results are very\ndependent on that assumption. The computations requirements are higher than even\nhierarchical clustering, making it difficult to scale to large data. Finally, the\nalgorithm is more sophisticated and less accessible than that of other methods.\nKEY IDEAS FOR MODEL-BASED CLUSTERING\nClusters are assumed to derive from different data-generating processes with different probability\ndistributions.\nDifferent models are fit, assuming different numbers of (typically normal) distributions.\nThe method chooses the model (and the associated number of clusters) that fits the data well\nwithout using too many parameters (i.e., overfitting).\n",
      "content_length": 1575,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 479,
      "content": "Further Reading\nFor more detail on model-based clustering, see the mclust documentation.\n",
      "content_length": 89,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 480,
      "content": "Scaling and Categorical Variables\nUnsupervised learning techniques generally require that the data be appropriately\nscaled. This is different from many of the techniques for regression and\nclassification in which scaling is not important (an exception is K-nearest\nneighbors; see “K-Nearest Neighbors”).\nKEY TERMS FOR SCALING DATA\nScaling\nSquashing or expanding data, usually to bring multiple variables to the same scale.\nNormalization\nOne method of scaling — subtracting the mean and dividing by the standard deviation.\nSynonym\nStandardization\nGower’s distance\nA scaling algorithm applied to mixed numeric and categoprical data to bring all variables to a 0–1\nrange.\nFor example, with the personal loan data, the variables have widely different units\nand magnitude. Some variables have relatively small values (e.g., number of years\nemployed), while others have very large values (e.g., loan amount in dollars). If\nthe data is not scaled, then the PCA, K-means, and other clustering methods will\nbe dominated by the variables with large values and ignore the variables with\nsmall values.\nCategorical data can pose a special problem for some clustering procedures. As\nwith K-nearest neighbors, unordered factor variables are generally converted to a\nset of binary (0/1) variables using one hot encoding (see “One Hot Encoder”). Not\nonly are the binary variables likely on a different scale from other data, the fact\nthat binary variables have only two values can prove problematic with techniques\nsuch as PCA and K-means.\n",
      "content_length": 1523,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 481,
      "content": "Scaling the Variables\nVariables with very different scale and units need to be normalized appropriately\nbefore you apply a clustering procedure. For example, let’s apply kmeans to a set\nof data of loan defaults without normalizing:\ndf <- defaults[, c('loan_amnt', 'annual_inc', 'revol_bal', 'open_acc',\n                   'dti', 'revol_util')]\nkm <- kmeans(df, centers=4, nstart=10)\ncenters <- data.frame( size=km$size, km$centers)\nround(centers, digits=2)\n  size loan_amnt annual_inc revol_bal open_acc   dti revol_util\n1    55  23157.27  491522.49  83471.07    13.35  6.89      58.74\n2  1218  21900.96  165748.53  38299.44    12.58 13.43      63.58\n3  7686  18311.55   83504.68  19685.28    11.68 16.80      62.18\n4 14177  10610.43   42539.36  10277.97     9.60 17.73      58.05\nThe variables annual_inc and revol_bal dominate the clusters, and the clusters\nhave very different sizes. Cluster 1 has only 55 members with comparatively high\nincome and revolving credit balance.\nA common approach to scaling the variables is to convert them to z-scores by\nsubtracting the mean and dividing by the standard deviation. This is termed\nstandardization or normalization (see “Standardization (Normalization, Z-\nScores)” for more discussion about using z-scores):\nSee what happens to the clusters when kmeans is applied to the normalized data:\ndf0 <- scale(df)\nkm0 <- kmeans(df0, centers=4, nstart=10)\ncenters0 <-scale(km0$centers, center=FALSE,\n                 scale=1/attr(df0, 'scaled:scale'))\ncenters0 <- scale(centers0, center=-attr(df0, 'scaled:center'), scale=F)\ndata.frame(size=km0$size, centers0)\n  size loan_amnt annual_inc revol_bal open_acc   dti revol_util\n1 5429  10393.60   53689.54   6077.77     8.69 11.35      30.69\n2 6396  13310.43   55522.76  16310.95    14.25 24.27      59.57\n3 7493  10482.19   51216.95  11530.17     7.48 15.79      77.68\n4 3818  25933.01  116144.63  32617.81    12.44 16.25      66.01\nThe cluster sizes are more balanced, and the clusters are not just dominated by\n",
      "content_length": 2000,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 482,
      "content": "annual_inc and revol_bal, revealing more interesting structure in the data.\nNote that the centers are rescaled to the original units in the preceding code. If we\nhad left them unscaled, the resulting values would be in terms of z-scores, and\ntherefore less interpretable.\nNOTE\nScaling is also important for PCA. Using the z-scores is equivalent to using the correlation matrix\n(see “Correlation”) instead of the covariance matrix in computing the principal components.\nSoftware to compute PCA usually has an option to use the correlation matrix (in R, the princomp\nfunction has the argument cor).\n",
      "content_length": 597,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 483,
      "content": "Dominant Variables\nEven in cases where the variables are measured on the same scale and accurately\nreflect relative importance (e.g., movement to stock prices), it can sometimes be\nuseful to rescale the variables.\nSuppose we add Alphabet (GOOGL) and Amazon (AMZN) to the analysis in\n“Interpreting Principal Components”.\nsyms <- c('AMZN', 'GOOGL' 'AAPL', 'MSFT', 'CSCO', 'INTC', 'CVX', 'XOM',\n   'SLB', 'COP', 'JPM', 'WFC', 'USB', 'AXP', 'WMT', 'TGT', 'HD', 'COST')\ntop_sp1 <- sp500_px[row.names(sp500_px)>='2005-01-01', syms]\nsp_pca1 <- princomp(top_sp1)\nscreeplot(sp_pca1)\nThe screeplot displays the variances for the top principal components. In this\ncase, the screeplot in Figure 7-12 reveals that the variances of the first and second\ncomponents are much larger than the others. This often indicates that one or two\nvariables dominate the loadings. This is, indeed, the case in this example:\nround(sp_pca1$loadings[,1:2], 3)\n      Comp.1 Comp.2\nGOOGL  0.781  0.609\nAMZN   0.593 -0.792\nAAPL   0.078  0.004\nMSFT   0.029  0.002\nCSCO   0.017 -0.001\nINTC   0.020 -0.001\nCVX    0.068 -0.021\nXOM    0.053 -0.005\n...\nThe first two principal components are almost completely dominated by GOOGL\nand AMZN. This is because the stock price movements of GOOGL and AMZN\ndominate the variability.\nTo handle this situation, you can either include them as is, rescale the variables\n(see “Scaling the Variables”), or exclude the dominant variables from the analysis\nand handle them separately. There is no “correct” approach, and the treatment\ndepends on the application.\n",
      "content_length": 1557,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 484,
      "content": "Figure 7-12. A screeplot for a PCA of top stocks from the S&P 500 including GOOGL and AMZN\n",
      "content_length": 91,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 485,
      "content": "Categorical Data and Gower’s Distance\nIn the case of categorical data, you must convert it to numeric data, either by\nranking (for an ordered factor) or by encoding as a set of binary (dummy)\nvariables. If the data consists of mixed continuous and binary variables, you will\nusually want to scale the variables so that the ranges are similar; see “Scaling the\nVariables”. One popular method is to use Gower’s distance.\nThe basic idea behind Gower’s distance is to apply a different distance metric to\neach variable depending on the type of data:\nFor numeric variables and ordered factors, distance is calculated as the\nabsolute value of the difference between two records (Manhattan distance).\nFor categorical variables, the distance is 1 if the categories between two\nrecords are different and the distance is 0 if the categories are the same.\nGower’s distance is computed as follows:\n1. Compute the distance \n for all pairs of variables i and j for each\nrecord.\n2. Scale each pair \n so the minimum is 0 and the maximum is 1.\n3. Add the pairwise scaled distances between variables together, either\nusing a simple or weighted mean, to create the distance matrix.\nTo illustrate Gower’s distance, take a few rows from the loan data:\n> x = defaults[1:5, c('dti', 'payment_inc_ratio', 'home', 'purpose')]\n> x\n# A tibble: 5 × 4\n    dti payment_inc_ratio   home            purpose\n  <dbl>             <dbl> <fctr>             <fctr>\n1  1.00           2.39320   RENT                car\n2  5.55           4.57170    OWN     small_business\n3 18.08           9.71600   RENT              other\n4 10.08          12.21520   RENT debt_consolidation\n5  7.06           3.90888   RENT              other\nThe function daisy in the cluster package can be used to compute Gower’s\ndistance:\n",
      "content_length": 1770,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 486,
      "content": "> library(cluster)\n> daisy(x, metric='gower')\nDissimilarities :\n          1         2         3         4\n2 0.6220479\n3 0.6863877 0.8143398\n4 0.6329040 0.7608561 0.4307083\n5 0.3772789 0.5389727 0.3091088 0.5056250\nAll distances are between 0 and 1. The pair of records with the biggest distance is\n2 and 3: neither has the same values for home or purpose and they have very\ndifferent levels of dti (debt-to-income) and payment_inc_ratio. Records 3 and\n5 have the smallest distance because they share the same values for home or\npurpose.\nYou can apply hierarchical clustering (see “Hierarchical Clustering”) to the\nresulting distance matrix using hclust to the output from daisy:\ndf <- defaults[sample(nrow(defaults), 250),\n               c('dti', 'payment_inc_ratio', 'home', 'purpose')]\nd = daisy(df, metric='gower')\nhcl <- hclust(d)\ndnd <- as.dendrogram(hcl)\nplot(dnd, leaflab='none')\nThe resulting dendrogram is shown in Figure 7-13. The individual records are not\ndistinguishable on the x-axis, but we can examine the records in one of the\nsubtrees (on the left, using a “cut” of 0.5), with this code:\n> df[labels(dnd_cut$lower[[1]]),]\n# A tibble: 9 × 4\n    dti payment_inc_ratio   home purpose\n  <dbl>             <dbl> <fctr>  <fctr>\n1 24.57           0.83550   RENT   other\n2 34.95           5.02763   RENT   other\n3  1.51           2.97784   RENT   other\n4  8.73          14.42070   RENT   other\n5 12.05           9.96750   RENT   other\n6 10.15          11.43180   RENT   other\n7 19.61          14.04420   RENT   other\n8 20.92           6.90123   RENT   other\n9 22.49           9.36000   RENT   other\nThis subtree entirely consists of renters with a loan purpose labeled as “other.”\nWhile strict separation is not true of all subtrees, this illustrates that the\ncategorical variables tend to be grouped together in the clusters.\n",
      "content_length": 1837,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 487,
      "content": "Figure 7-13. A dendrogram of hclust applied to a sample of loan default data with mixed variable types\n",
      "content_length": 103,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 488,
      "content": "Problems with Clustering Mixed Data\nK-means and PCA are most appropriate for continuous variables. For smaller data\nsets, it is better to use hierarchical clustering with Gower’s distance. In principle,\nthere is no reason why K-means can’t be applied to binary or categorical data.\nYou would usually use the “one hot encoder” representation (see “One Hot\nEncoder”) to convert the categorical data to numeric values. In practice, however,\nusing K-means and PCA with binary data can be difficult.\nIf the standard z-scores are used, the binary variables will dominate the definition\nof the clusters. This is because 0/1 variables take on only two values and K-means\ncan obtain a small within-cluster sum-of-squares by assigning all the records with\na 0 or 1 to a single cluster. For example, apply kmeans to loan default data\nincluding factor variables home and pub_rec_zero:\ndf <- model.matrix(~ -1 + dti + payment_inc_ratio + home + pub_rec_zero,\n                   data=defaults)\ndf0 <- scale(df)\nkm0 <- kmeans(df0, centers=4, nstart=10)\ncenters0 <-scale(km0$centers, center=FALSE,\n                 scale=1/attr(df0, 'scaled:scale'))\nscale(centers0, center=-attr(df0, 'scaled:center'), scale=F)\n    dti payment_inc_ratio homeMORTGAGE homeOWN homeRENT pub_rec_zero\n1 17.02              9.10         0.00       0     1.00         1.00\n2 17.47              8.43         1.00       0     0.00         1.00\n3 17.23              9.28         0.00       1     0.00         0.92\n4 16.50              8.09         0.52       0     0.48         0.00\nThe top four clusters are essentially proxies for the different levels of the factor\nvariables. To avoid this behavior, you could scale the binary variables to have a\nsmaller variance than other variables. Alternatively, for very large data sets, you\ncould apply clustering to different subsets of data taking on specific categorical\nvalues. For example, you could apply clustering separately to those loans made to\nsomeone who has a mortgage, owns a home outright, or rents.\nKEY IDEAS FOR SCALING DATA\nVariables measured on different scales need to be transformed to similar scales, so that their impact\non algorithms is not determined mainly by their scale.\nA common scaling method is normalization (standardization) — subtracting the mean and dividing\nby the standard deviation.\nAnother method is Gower’s distance, which scales all variables to the 0–1 range (it is often used\n",
      "content_length": 2420,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 489,
      "content": "with mixed numeric and categorical data).\n",
      "content_length": 42,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 490,
      "content": "Summary\nFor dimension reduction of numeric data, the main tools are either principal\ncomponents analysis or K-means clustering. Both require attention to proper\nscaling of the data to ensure meaningful data reduction.\nFor clustering with highly structured data in which the clusters are well separated,\nall methods will likely produce a similar result. Each method offers its own\nadvantage. K-means scales to very large data and is easily understood.\nHierarchical clustering can be applied to mixed data types — numeric and\ncategorical — and lends itself to an intuitive display (the dendrogram). Model-\nbased clustering is founded on statistical theory and provides a more rigorous\napproach, as opposed to the heuristic methods. For very large data, however, K-\nmeans is the main method used.\nWith noisy data, such as the loan and stock data (and much of the data that a data\nscientist will face), the choice is more stark. K-means, hierarchical clustering, and\nespecially model-based clustering all produce very different solutions. How\nshould a data scientist proceed? Unfortunately, there is no simple rule of thumb to\nguide the choice. Ultimately, the method used will depend on the data size and the\ngoal of the application.\nThis and subsequent sections in this chapter © 2017 Datastats, LLC, Peter Bruce and Andrew Bruce,\nused by permission.\n1\n",
      "content_length": 1351,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 491,
      "content": "Bibliography\n[bokeh] Bokeh Development Team. “Bokeh: Python library for interactive\nvisualization” (2014). http://www.bokeh.pydata.org.\n[Deng-Wickham-2011] Deng, H. and Wickham, H. “Density estimation in R”\n(2011). http://vita.had.co.nz/papers/density-estimation.pdf.\n[Wikipedia-2016] “Diving.” Wikipedia: The Free Encyclopedia. Wikimedia\nFoundation, Inc. 10 Mar 2016. Web. 19 Mar 2016.\n[Donoho-2015] Donoho, David. “50 Years of Data Science” (2015).\nhttp://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf.\n[Duong-2001] Duang, Tarn. “An introduction to kernel density estimation”\n(2001). http://www.mvstat.net/tduong/research/seminars/seminar-2001-\n05.pdf.\n[Few-2007] Few, Stephen. “Save the Pies for Dessert.” Visual Intelligence\nNewsletter, Perceptual Edge (2007).\nhttps://www.perceptualedge.com/articles/visual_business_intelligence/save\n[Hintze-Nelson-1998] Hintze, J. and Nelson, R. “Violin Plots: A Box Plot-\nDensity Trace Synergism.” The American Statistician 52.2 (May 1998):\n181–184.\n[Galton-1886] Galton, Francis. “Regression towards mediocrity in\nHereditary stature.” The Journal of the Anthropological Institute of Great\nBritain and Ireland, 15:246-273. JSTOR 2841583.\n[ggplot2] Wickham, Hadley. ggplot2: Elegant Graphics for Data Analysis.\nSpringer-Verlag New York (2009). ISBN: 978-0-387-98140-6.\nhttp://had.co.nz/ggplot2/book.\n[Hyndman-Fan-1996] Hyndman, R. J. and Fan, Y. “Sample quantiles in\nstatistical packages,” American Statistician 50, (1996) 361–365.\n",
      "content_length": 1488,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 492,
      "content": "[lattice] Sarkar, Deepayan. Lattice: Multivariate Data Visualization with R.\nSpringer (2008). ISBN 978-0-387-75968-5. http://lmdvr.r-forge.r-\nproject.org.\n[Legendre] Legendre, Adrien-Marie. Nouvelle methodes pour la\ndetermination des orbites des cometes. F. Didot, Paris (1805).\n[NIST-Handbook-2012] NIST/SEMATECH e-Handbook of Statistical\nMethods,\nhttp://www.itl.nist.gov/div898/handbook/eda/section3/eda35b.htm (2012).\n[R-base-2015] R Core Team. “R: A Language and Environment for\nStatistical Computing,” R Foundation for Statistical Computing (2015).\nhttp://www.R-project.org/.\n[seaborne] Wasdom, Michael. “Seaborn: statistical data visualization”\n(2015). http://stanford.edu/~mwaskom/software/seaborn/#.\n[Stigler-Gauss] Stigler, Stephen M. “Gauss and the Invention of Least\nSquares.” Ann. Stat. 9(3), 465–474 (1981).\n[Trellis-Graphics] Becker, R., Cleveland, W, Shyu, M. and Kaluzny, S. “A\nTour of Trellis Graphics” (1996).\nhttp://polisci.msu.edu/jacoby/icpsr/graphics/manuscripts/Trellis_tour.pdf.\n[Tukey-1962] Tukey, John W. “The Future of Data Analysis.” Ann. Math.\nStatist. 33 (1962), no. 1, 1–67.\nhttps://projecteuclid.org/download/pdf_1/euclid.aoms/1177704711\n[Tukey-1977] Tukey, John W. Exploratory Data Analysis. Pearson (1977).\nISBN: 978-0-201-07616-5.\n[Tukey-1987] Tukey, John W. Edited by Jones, L. V. The collected works of\nJohn W. Tukey: Philosophy and Principles of Data Analysis 1965–1986,\nVolume IV. Chapman and Hall/CRC (1987). ISBN: 978-0-534-05101-3.\n[UCLA] “R Library: Contrast Coding Systems for Categorical Variables.”\nUCLA: Statistical Consulting Group.\nhttp://www.ats.ucla.edu/stat/r/library/contrast_coding.htm. Accessed June\n2016.\n",
      "content_length": 1661,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 493,
      "content": "[Zhang-Wang-2007] Zhang, Qi and Wang, Wei. 19th International Conference\non Scientific and Statistical Database Management, IEEE Computer Society\n(2007).\n",
      "content_length": 154,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 494,
      "content": "Index\nA\nA/B testing, A/B Testing-For Further Reading\ncontrol group, advantages of using, Why Have a Control Group?\nepsilon-greedy algorithm, Multi-Arm Bandit Algorithm\nimportance of permissions, Why Just A/B? Why Not C, D…?\ntraditional, shortcoming of, Multi-Arm Bandit Algorithm\naccuracy, Evaluating Classification Models\nimproving in random forests, Random Forest\nAdaboost, Boosting\nboosting algorithm, The Boosting Algorithm\nadjusted R-squared, Assessing the Model\nadjustment of p-values, Multiple Testing, Multiple Testing\nagglomerative algorithm, The Agglomerative Algorithm\nAIC (Akaike's Information Criteria), Model Selection and Stepwise\nRegression, Selecting the Number of Clusters\nvariants of, Model Selection and Stepwise Regression\nAkike, Hirotugu, Model Selection and Stepwise Regression\nall subset regression, Model Selection and Stepwise Regression\n",
      "content_length": 864,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 495,
      "content": "alpha, Statistical Significance and P-Values, Alpha\ndividing up in multiple testing, Multiple Testing\nalternative hypothesis, Hypothesis Tests, Alternative Hypothesis\nAmerican Statistical Association (ASA), statement on p-values, Value of the\np-value\nanomaly detection, Outliers, Regression and Prediction\nANOVA (analysis of variance\nstatististical test based on F-statistic, F-Statistic\nANOVA (analysis of variance), ANOVA-Further Reading\ncomputing ANOVA table in R, F-Statistic\ndecomposition of variance, F-Statistic\ntwo-way, Two-Way ANOVA\narms (multi-arm bandits), Multi-Arm Bandit Algorithm\nAUC (area under the ROC curve), AUC\naverage linkage, Measures of Dissimilarity\nB\nbackward elimination, Model Selection and Stepwise Regression\nbackward selection, Model Selection and Stepwise Regression\nbagging, The Bootstrap, Resampling, Statistical Machine Learning, Bagging\nbetter predictive performance than single trees, How Trees Are Used\n",
      "content_length": 940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 496,
      "content": "boosting vs., Boosting\nusing with random forests, Random Forest\nbandit algorithms, Multi-Arm Bandit Algorithm\n(see also multi-arm bandits)\nbar charts, Exploring Binary and Categorical Data\nBayesian classification, Naive Bayes\n(see also naive Bayes algorithm)\nimpracticality of exact Bayesian classification, Why Exact Bayesian\nClassification Is Impractical\nBayesian infomation criteria (BIC), Model Selection and Stepwise\nRegression, Selecting the Number of Clusters\nbeta distribution, Multi-Arm Bandit Algorithm\nbias, Bias\nselection bias, Selection Bias-Further Reading\nbias-variance tradeoff, Choosing K\nbiased estimates, Standard Deviation and Related Estimates\nfrom naive Bayes classifier, The Naive Solution\nBIC (Bayesian information criteria), Model Selection and Stepwise\nRegression, Selecting the Number of Clusters\nbidirectional alternative hypothesis, One-Way, Two-Way Hypothesis Test\nbig data\n",
      "content_length": 904,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 497,
      "content": "and outliers in regression, Outliers\nuse of regression in, Prediction versus Explanation (Profiling)\nvalue of, Size versus Quality: When Does Size Matter?\nbinary data, Elements of Structured Data\nexploring, Exploring Binary and Categorical Data-Correlation\nbinomial, Binomial Distribution\nbinomial distribution, Binomial Distribution-Further Reading\nbinomial trials, Binomial Distribution\nbins\nhexagonal binning, Hexagonal Binning and Contours (Plotting Numeric\nversus Numeric Data)\nin frequency tables, Frequency Table and Histograms\nin histograms, Frequency Table and Histograms\nbivariate analysis, Exploring Two or More Variables\nblack swan theory, Long-Tailed Distributions\nblind studies, Why Have a Control Group?\nboosting, Statistical Machine Learning, Tree Models, Boosting-Summary\nbagging vs., Boosting\nboosting algorithm, The Boosting Algorithm\nhyperparameters and cross-validation, Hyperparameters and Cross-\n",
      "content_length": 919,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 498,
      "content": "Validation\noverfitting, avoiding using regularization, Regularization: Avoiding\nOverfitting\nXGBoost, XGBoost\nbootstrap, The Bootstrap-Further Reading, Resampling\nconfidence interval generation, Confidence Intervals, Confidence and\nPrediction Intervals\npermutation tests, Exhaustive and Bootstrap Permutation Test\nresampling vs. bootstrapping, Resampling versus Bootstrapping\nusing with random forests, Random Forest\nbootstrap sample, The Bootstrap\nboxplots, Exploring the Data Distribution\ncombining with a violin plot, example, Categorical and Numeric Data\nexample, percent of airline delays by carrier, Categorical and Numeric\nData\noutliers in, Outliers\npercentiles and, Percentiles and Boxplots\nBreiman, Leo, Statistical Machine Learning\nbubble plots, Influential Values\nC\ncategorical data, Elements of Structured Data\n",
      "content_length": 822,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 499,
      "content": "exploring, Exploring Binary and Categorical Data-Correlation\nexpected value, Expected Value\nmode, Mode\nnumerical data as categorical data, Exploring Binary and Categorical\nData\nexploring two categorical variables, Two Categorical Variables\nimportance of the concept, Elements of Structured Data\nnumeric variable grouped by categorical variable, Categorical and Numeric\nData\nscaling and categorical variables, Scaling and Categorical Variables-\nSummary\ndominant variables, Dominant Variables\nGower's distance, Categorical Data and Gower’s Distance\nscaling the variables, Scaling the Variables\ncategorical variables, Factor Variables in Regression\n(see also factor variables)\ncausation, regression and, Prediction versus Explanation (Profiling)\ncentral limit theorem, Sampling Distribution of a Statistic, Central Limit\nTheorem, Student’s t-Distribution\ndata science and, Student’s t-Distribution\nchi-square distribution, Chi-Squared Test: Statistical Theory\n",
      "content_length": 957,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 500,
      "content": "chi-square statistic, Chi-Square Test\nchi-square test, Chi-Square Test-Further Reading\ndetecting scientific fraud, Fisher’s Exact Test\nFisher's exact test, Fisher’s Exact Test\nrelevance for data science, Relevance for Data Science\nresampling approach, Chi-Square Test: A Resampling Approach\nstatistical theory, Chi-Squared Test: Statistical Theory\nclass purity, Measuring Homogeneity or Impurity\nclassification, Classification-Summary\ndiscriminant analysis, Discriminant Analysis-Further Reading\ncovariance matrix, Covariance Matrix\nFisher's linear discriminant, Fisher’s Linear Discriminant\nsimple example, A Simple Example\nevaluating models, Evaluating Classification Models-Further Reading\nAUC metric, AUC\nconfusion matrix, Confusion Matrix\nlift, Lift\nprecision, recall, and specificity, Precision, Recall, and Specificity\nrare class problem, The Rare Class Problem\nROC curve, ROC Curve\n",
      "content_length": 890,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 501,
      "content": "K-Nearest Neighbors, K-Nearest Neighbors\nlogistic regression, Logistic Regression-Further Reading\nand the GLM, Logistic Regression and the GLM\nassessing the model, Assessing the Model\ncomparison to linear regression, Linear and Logistic Regression:\nSimilarities and Differences\ninterpreting coefficients and odds ratios, Interpreting the Coefficients\nand Odds Ratios\nlogistic response function and logit, Logistic Response Function and\nLogit\npredicted values from, Predicted Values from Logistic Regression\nmore than two possible outcomes, Classification\nnaive Bayes algorithm, Naive Bayes-Further Reading\nimpracticality of exact Bayesian classification, Why Exact Bayesian\nClassification Is Impractical\nusing numeric predictor variables, Numeric Predictor Variables\nstrategies for imbalanced data, Strategies for Imbalanced Data-Further\nReading\ncost-based classification, Cost-Based Classification\ndata generation, Data Generation\nexploring the predictions, Exploring the Predictions\noversampling and up/down weighting, Oversampling and Up/Down\n",
      "content_length": 1046,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 502,
      "content": "Weighting\nundersampling, Undersampling\nunsupervised learning as building block, Unsupervised Learning\ncluster mean, K-Means Clustering, A Simple Example, Interpreting the\nClusters\nclustering, Unsupervised Learning\napplication to cold-start problems, Unsupervised Learning\ncluster analysis vs. PCA, Interpreting the Clusters\nhierarchical, Hierarchical Clustering-Measures of Dissimilarity,\nCategorical Data and Gower’s Distance\nagglomerative algorithm, The Agglomerative Algorithm\ndendrogram, The Dendrogram\ndissimilarity measures, Measures of Dissimilarity\nsimple example, A Simple Example\nK-means, K-Means Clustering-Selecting the Number of Clusters, Scaling\nthe Variables\ninterpreting the clusters, Interpreting the Clusters\nK-means algorithm, K-Means Algorithm\nselecting the number of customers, Selecting the Number of Clusters\nsimple example, A Simple Example-K-Means Algorithm\nmodel-based, Model-Based Clustering-Further Reading\n",
      "content_length": 935,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 503,
      "content": "mixtures of normals, Mixtures of Normals\nselecting the number of clusters, Selecting the Number of Clusters\nproblems with mixed data, Problems with Clustering Mixed Data\nstandardizing data, Standardization (Normalization, Z-Scores)\nclusters, K-Means Clustering\ncoefficient of determination, Assessing the Model\ncoefficients\nin logistic regression, Interpreting the Coefficients and Odds Ratios\nin simple linear regression, The Regression Equation\nestimates vs. known, Fitted Values and Residuals\ninterpretation in multiple linear regression, Example: King County Housing\nData\ncomplete linkage, The Agglomerative Algorithm\ncomplexity parameter (cp), Stopping the Tree from Growing\nconditional probabilities, Naive Bayes\nconditioning variables, Visualizing Multiple Variables\nconfidence intervals, Confidence Intervals-Further Reading, Confidence and\nPrediction Intervals\ngenerating with bootstrap, Confidence Intervals\nlevel of confidence, Confidence Intervals\n",
      "content_length": 960,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 504,
      "content": "prediction intervals vs., Confidence and Prediction Intervals\nconfidence level, Confidence Intervals-Confidence Intervals\nconfounding variables, Interpreting the Regression Equation, Confounding\nVariables\nconfusion matrix, Evaluating Classification Models-Confusion Matrix\ncontingency tables, Exploring Two or More Variables\nexample, loan grade and status, Two Categorical Variables\ncontinuous data, Elements of Structured Data\ncontinuous variable as test metric, A/B Testing\npredicting continuous value with a tree, Predicting a Continuous Value\ncontour plots, Exploring Two or More Variables\nusing with hexagonal binning, Hexagonal Binning and Contours (Plotting\nNumeric versus Numeric Data)\ncontrast coding systems, Dummy Variables Representation\ncontrol group, A/B Testing\nadvantages of using, Why Have a Control Group?\nCook's distance, Influential Values\ncorrelated variables, Interpreting the Regression Equation\nmulticollinearity, Multicollinearity\npredictor variables, Correlated Predictors\ncorrelation, Correlation-Further Reading\n",
      "content_length": 1040,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 505,
      "content": "key terms for, Correlation\nregression vs., Simple Linear Regression\nscatterplots, Scatterplots\ncorrelation coefficient, Correlation\ncomputing Pearson's correlation coefficient, Correlation\nkey concepts, Scatterplots\nother types of, Correlation\ncorrelation matrix, Correlation\nexample, correlation between telecommunication stock returns,\nCorrelation\ncost-based classification, Cost-Based Classification\ncount data\nas test metric, A/B Testing\nFisher's exact test for, Fisher’s Exact Test\ncovariance, Discriminant Analysis, Covariance Matrix, Computing the\nPrincipal Components\ncovariance matrix\nin discriminant analysis, Covariance Matrix\nin model-based clustering, Multivariate Normal Distribution\nusing to compute Mahalanobis distance, Distance Metrics\ncross-validation, Cross-Validation, Choosing K\n",
      "content_length": 801,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 506,
      "content": "for selection of principal components, Interpreting Principal Components\nusing for hyperparameters in boosting, Hyperparameters and Cross-\nValidation\nusing to estimate value of complexity parameter, Stopping the Tree from\nGrowing\ncumulative gains charts, Lift\nD\nd.f. (degrees of freedom), Degrees of Freedom, Chi-Square Test\n(see also degrees of freedom)\ndata analysis, Exploratory Data Analysis\n(see also exploratory data analysis)\ndata distribution, Exploring the Data Distribution-Further Reading, Sampling\nDistribution of a Statistic\nfrequency tables and histograms, Frequency Table and Histograms\nkey terms for, Exploring the Data Distribution\npercentiles and boxplots, Percentiles and Boxplots\nsampling distribution vs., Sampling Distribution of a Statistic\ndata frames, Rectangular Data\nand indexes, Data Frames and Indexes\ntypical data format, Rectangular Data\ndata generation, Strategies for Imbalanced Data, Data Generation\n",
      "content_length": 934,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 507,
      "content": "data snoopng, Selection Bias\ndata types\nkey terms for, Elements of Structured Data\nresources for further reading, Further Reading\ndatabase normalization, Standardization (Normalization, Z-Scores)\ndecile gains charts, Lift\ndecision trees, The Bootstrap, Statistical Machine Learning\nmeaning in operations research, Tree Models\nrecursive partitioning algorithm, Random Forest\ndecomposition of variance, ANOVA, F-Statistic\ndegrees of freedom, Standard Deviation and Related Estimates, Student’s t-\nDistribution, Degrees of Freedom-Further Reading\nin chi-square test, Chi-Squared Test: Statistical Theory\ndendrograms, Hierarchical Clustering\nexample, dendrogram of stocks, The Dendrogram\nhierarchical clustering with mixed variable types, Categorical Data and\nGower’s Distance\ndensity plots, Exploring the Data Distribution, Density Estimates\nexample, density of state murder rates, Density Estimates\ndependent variable, The Regression Equation\n(see also response)\n",
      "content_length": 961,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 508,
      "content": "deviation coding, Factor Variables in Regression, Dummy Variables\nRepresentation\ndeviations, Estimates of Variability\nstandard deviation and related estimates, Standard Deviation and Related\nEstimates\ndirectional alternative hypothesis, One-Way, Two-Way Hypothesis Test\ndiscrete data, Elements of Structured Data\ndiscriminant analysis, Discriminant Analysis-Further Reading\ncovariance matrix, Covariance Matrix\nextensions of, A Simple Example\nFisher's linear discriminant, Fisher’s Linear Discriminant\nsimple example, A Simple Example-A Simple Example\ndiscriminant function, Discriminant Analysis\ndiscriminant weights, Discriminant Analysis\ndispersion, Estimates of Variability\n(see also variability, estimates of)\ndissimilarity, Hierarchical Clustering\ncommon measures of, Measures of Dissimilarity\nmeasuring with, complete-linkage method, The Agglomerative Algorithm\nmetric in hierarchical clustering, A Simple Example\ndistance metrics, K-Nearest Neighbors, Hierarchical Clustering\n",
      "content_length": 984,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 509,
      "content": "Gower's distance and categorical data, Categorical Data and Gower’s\nDistance\nin hierarchical clustering, A Simple Example, The Agglomerative Algorithm\nin K-Nearest Neighbors, Distance Metrics\nDonoho, David, Exploratory Data Analysis\ndouble blind studies, Why Have a Control Group?\ndummy variables, Factor Variables in Regression\nrepresentation of factor variables in regression, Dummy Variables\nRepresentation\nrepresenting string factor data as numbers, One Hot Encoder\nDurbin-Watson statistic, Heteroskedasticity, Non-Normality and Correlated\nErrors\nE\nEDA (see exploratory data analysis)\neffect size, Power and Sample Size, Sample Size\nelbow method, Selecting the Number of Clusters\nensemble learning, Statistical Machine Learning\nstaged used of K-Nearest Neighbors, KNN as a Feature Engine\nensemble models, Boosting\nentropy, Measuring Homogeneity or Impurity\nepsilon-greedy algorithm, Multi-Arm Bandit Algorithm\n",
      "content_length": 914,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 510,
      "content": "errors, Normal Distribution\nestimates, Estimates of Location\nindicated by hat notation, Fitted Values and Residuals\nEuclidean distance, Distance Metrics\nexact tests, Exhaustive and Bootstrap Permutation Test\nExcel, pivot tables, Two Categorical Variables\nexhaustive permutation tests, Exhaustive and Bootstrap Permutation Test\nexpectation or expected, Chi-Square Test\nexpected value, Exploring Binary and Categorical Data, Expected Value\ncalculating, Expected Value\nexplanation vs. prediction (in regression), Prediction versus Explanation\n(Profiling)\nexploratory data analysis, Exploratory Data Analysis-Summary\nbinary and categorical data, Exploring Binary and Categorical Data-\nCorrelation\ncorrelation, Correlation-Further Reading\ndata distribution, Exploring the Data Distribution-Further Reading\nestimates of location, Estimates of Location-Further Reading\nestimates of variability, Estimates of Variability-Further Reading\nexploring two or more variables, Exploring Two or More Variables-\nSummary\n",
      "content_length": 1003,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 511,
      "content": "rectangular data, Rectangular Data-Estimates of Location\nExploratory Data Analysis (Tukey), Exploratory Data Analysis\nexponential distribution, Poisson and Related Distributions\ncalculating, Exponential Distribution\nextrapolation\ndangers of, The Dangers of Extrapolation\ndefinition of, Prediction Using Regression\nF\nF-statistic, ANOVA, F-Statistic, Assessing the Model\nfacets, Visualizing Multiple Variables\nfactor variables, Factor Variables in Regression-Ordered Factor Variables\ndifferent codings, Dummy Variables Representation\ndummy variables representation, Dummy Variables Representation\nhandling in logistic regression, Fitting the model\nin naive Bayes algorithm, Naive Bayes\nordered, Ordered Factor Variables\nreference coding, Interactions and Main Effects\nwith many levels, Factor Variables with Many Levels\nfactors, conversion of text columns to, Elements of Structured Data\nfailure rate, estimating, Estimating the Failure Rate\n",
      "content_length": 940,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 512,
      "content": "false discovery rate, Multiple Testing, Multiple Testing\nfalse positive rate, AUC\nfeature selection\nchi-square tests in, Relevance for Data Science\nusing discriminant analysis, A Simple Example\nfeatures, Rectangular Data\nterminology differences, Data Frames and Indexes\nfield view (spatial data), Nonrectangular Data Structures\nFisher's exact test, Fisher’s Exact Test\nFisher's linear discriminant, Fisher’s Linear Discriminant\nFisher's scoring, Fitting the model\nFisher, R.A., Fisher’s Exact Test, Discriminant Analysis\nfitted values, Simple Linear Regression, Fitted Values and Residuals\nfolds, Cross-Validation, Hyperparameters and Cross-Validation\nforward selection and backward selection, Model Selection and Stepwise\nRegression\nfrequency tables, Exploring the Data Distribution\nexample, population by state, Frequency Table and Histograms\nFriedman, Jerome H. (Jerry), Statistical Machine Learning\nG\n",
      "content_length": 905,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 513,
      "content": "gains, Lift\n(see also lift)\nGallup Poll, Random Sampling and Sample Bias\nGallup, George, Random Sampling and Sample Bias, Random Selection\nGalton, Francis, Regression to the Mean\nGAM (see generalized additive models)\nGaussian distribution, Normal Distribution\n(see also normal distribution)\ngeneralized additive models, Polynomial and Spline Regression, Generalized\nAdditive Models, Exploring the Predictions\ngeneralized linear model (GLM), Logistic Regression and the GLM\nGini coefficient, Measuring Homogeneity or Impurity\nGini impurity, Measuring Homogeneity or Impurity\nGLM (see generalized linear model)\nGossett, W.S., Student’s t-Distribution\nGower's distance, Scaling and Categorical Variables\ncategorical data and, Categorical Data and Gower’s Distance\ngradient boosted trees, Interactions and Main Effects\ngradient boosting, The Boosting Algorithm\ndefinition of, Boosting\n",
      "content_length": 881,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 514,
      "content": "graphs, Nonrectangular Data Structures\ncomputer science versus statistics, Nonrectangular Data Structures\nlesson on misleading graphs, Further Reading\ngreedy algorithms, Multi-Arm Bandit Algorithm\nH\nhat notation, Fitted Values and Residuals\nhat-value, Testing the Assumptions: Regression Diagnostics, Influential\nValues\nheat maps, Hexagonal Binning and Contours (Plotting Numeric versus\nNumeric Data)\nheteroskedastic errors, Heteroskedasticity, Non-Normality and Correlated\nErrors\nheteroskedasticity, Testing the Assumptions: Regression Diagnostics,\nHeteroskedasticity, Non-Normality and Correlated Errors\nhexagonal binning, Exploring Two or More Variables\nexample, using with contour plot, Hexagonal Binning and Contours\n(Plotting Numeric versus Numeric Data)\nhierarchical clustering, Hierarchical Clustering-Measures of Dissimilarity,\nCategorical Data and Gower’s Distance\nagglomerative algorithm, The Agglomerative Algorithm\nmeasures of dissimilarity, Measures of Dissimilarity\nsimple example, A Simple Example\n",
      "content_length": 1014,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 515,
      "content": "histograms, Exploring the Data Distribution\nexample, population by state, Frequency Table and Histograms\nhomogeneity, measuring, Measuring Homogeneity or Impurity\nhyperparameters\nand cross-validation in boosting, Hyperparameters and Cross-Validation\nfor HGBoost, Hyperparameters and Cross-Validation\nin random forests, Hyperparameters\nhypothesis tests, Hypothesis Tests-Further Reading\nalternative hypothesis, Alternative Hypothesis\nfalse discovery rate, Multiple Testing\nnull hypothesis, The Null Hypothesis\none-way and two-way tests, One-Way, Two-Way Hypothesis Test\nI\nimpurity, Tree Models\nmeasuring, Measuring Homogeneity or Impurity\nin-sample methods to assess and tune models, Model Selection and Stepwise\nRegression\nindependent variables, Simple Linear Regression, The Regression Equation\nmain effects, Interactions and Main Effects\nindexes, data frames and, Data Frames and Indexes\nindicator variables, Factor Variables in Regression\n",
      "content_length": 942,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 516,
      "content": "inference, Exploratory Data Analysis, Statistical Experiments and\nSignificance Testing\ninfluence plots, Influential Values\ninfluential values, Testing the Assumptions: Regression Diagnostics,\nInfluential Values\ninformation, Measuring Homogeneity or Impurity\ninteractions, Interpreting the Regression Equation\nand main effects, Interactions and Main Effects\ndeciding which interaction terms to include in the model, Interactions and\nMain Effects\nintercepts, Simple Linear Regression\nin cotton exposure and lung capacity example, The Regression Equation\nInternet of Things (IoT), Elements of Structured Data\ninterquantile range (IQR), Estimates of Variability, Estimates Based on\nPercentiles\ninterval endpoints, Confidence Intervals\nK\nK (in K-Nearest Neighbors), K-Nearest Neighbors\nk-fold cross-validation, Cross-Validation\nK-means clustering, K-Means Clustering-Selecting the Number of Clusters\ninterpreting the clusters, Interpreting the Clusters\n",
      "content_length": 948,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 517,
      "content": "K-means algorithm, K-Means Algorithm\nselecting the number of clusters, Selecting the Number of Clusters\nsimple example, A Simple Example-K-Means Algorithm\nusing on unnormalized and normalized variables, Scaling the Variables\nK-Nearest Neighbors, Predicted Values from Logistic Regression, K-Nearest\nNeighbors-KNN as a Feature Engine\nas a feature engine, KNN as a Feature Engine\nchoosing K, Choosing K\ndistance metrics, Distance Metrics\nexample, predicting loan default, A Small Example: Predicting Loan\nDefault\none hot encoder, One Hot Encoder\nstandardization, Standardization (Normalization, Z-Scores)\nkernal density estimates, Density Estimates\nKernSmooth package, Density Estimates\nKNN (see K-Nearest Neighbors)\nknots, Polynomial and Spline Regression, Splines\nkurtosis, Frequency Table and Histograms\nL\nlambda, in Poisson and related distributions, Poisson and Related\nDistributions\n",
      "content_length": 887,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 518,
      "content": "Lasso regression, Model Selection and Stepwise Regression, Regularization:\nAvoiding Overfitting\nLatent Dirichlet Allocation (LDA), Discriminant Analysis\nleaf, Tree Models\nleast squares, Simple Linear Regression, Least Squares\nleverage, Testing the Assumptions: Regression Diagnostics\ninfluential values in regression, Influential Values\nlift, Evaluating Classification Models, Lift\nlift curve, Lift\nlinear discriminant analysis (LDA), Discriminant Analysis, Exploring the\nPredictions\nlinear regression, Simple Linear Regression-Weighted Regression\ncomparison to logistic regression, Linear and Logistic Regression:\nSimilarities and Differences\nfitted values and residuals, Fitted Values and Residuals\ngeneralized linear model (GLM), Logistic Regression and the GLM\nleast squares, Least Squares\nmultiple, Multiple Linear Regression-Weighted Regression\nassessing the model, Assessing the Model\ncross-validation, Cross-Validation\nexample, King County housing data, Example: King County Housing\n",
      "content_length": 991,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 519,
      "content": "Data\nmodel selection and stepwise regression, Model Selection and Stepwise\nRegression\nweighted regression, Weighted Regression\nprediction vs. explanation, Prediction versus Explanation (Profiling)\nregression equation, The Regression Equation\nLiterary Digest poll of 1936, Random Sampling and Sample Bias, Random\nSelection\nloadings, Principal Components Analysis, A Simple Example\nfor top five components (example), Interpreting Principal Components\nlog odds, Logistic Regression\nlog-odds function (see logit function)\nlog-odds ratio, Interpreting the Coefficients and Odds Ratios\nlogistic regression, Logistic Regression-Further Reading, Exploring the\nPredictions\nand the generalized linear model (GLM), Logistic Regression and the\nGLM\nassessing the model, Assessing the Model\ncomparison to linear regression, Linear and Logistic Regression:\nSimilarities and Differences\ninterpreting the coefficients and odds ratios, Interpreting the Coefficients\nand Odds Ratios\n",
      "content_length": 964,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 520,
      "content": "logistic response function and logit, Logistic Response Function and Logit\npredicted values from, Predicted Values from Logistic Regression\nlogit function, Logistic Regression, Logistic Response Function and Logit\nlong-tail distributions, Long-Tailed Distributions-Further Reading\nloss, Tree Models\nloss function, Oversampling and Up/Down Weighting\nM\nmachine learning\nstatistics vs., Statistical Machine Learning\nmachine learnng, Statistical Machine Learning\n(see also statistical machine learning)\nMahalanobis distance, Covariance Matrix, Distance Metrics\nmain effects, Interpreting the Regression Equation\ninteractions and, Interactions and Main Effects\nMallows Cp, Model Selection and Stepwise Regression\nManhattan distance, Distance Metrics, Regularization: Avoiding Overfitting,\nCategorical Data and Gower’s Distance\nmaximum likelihood estimation (MLE), Fitting the model\nmean, Estimates of Location\nformula for, Mean\n",
      "content_length": 923,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 521,
      "content": "regression to, Regression to the Mean\nsample mean vs. population mean, Sample Mean versus Population Mean\ntrimmed mean, Mean\nweighted mean, Mean\nmean absolute deviation, Estimates of Variability, A/B Testing\nformula for calculating, Standard Deviation and Related Estimates\nmean absolute deviation from the median (MAD), Standard Deviation and\nRelated Estimates\nmedian, Estimates of Location\nand robust estimates, Median and Robust Estimates\nmedian absolute deviation, Estimates of Variability\nmetrics, Estimates of Location\nminimum variance, Measures of Dissimilarity\nMLE (see maximum likelihood estimation)\nmode, Exploring Binary and Categorical Data\nexamples in categorical data, Mode\nmodel-based clustering, Model-Based Clustering-Further Reading\nlimitations, Selecting the Number of Clusters\nmixtures of normals, Mixtures of Normals\nmultivariate normal distribution, Multivariate Normal Distribution\n",
      "content_length": 905,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 522,
      "content": "selecting the number of clusters, Selecting the Number of Clusters\nmoments, Frequency Table and Histograms\nmulti-arm bandits, Why Just A/B? Why Not C, D…?, Multi-Arm Bandit\nAlgorithm-Further Reading\ndefinition of, Multi-Arm Bandit Algorithm\nmulticollinearity, Interpreting the Regression Equation, Multicollinearity\nproblems with one hot encoding, One Hot Encoder\nmulticollinearity errors, Degrees of Freedom, Dummy Variables\nRepresentation\nmultiple linear regression (see linear regression)\nmultiple testing, Multiple Testing-Further Reading\nbottom line for data scientists, Multiple Testing\nmultivariate analysis, Exploring Two or More Variables\nmultivariate normal distribution, Multivariate Normal Distribution\nN\nn (sample size), Student’s t-Distribution\nn or sample size, Degrees of Freedom\nnaive Bayes algorithm, Naive Bayes-Further Reading\napplying to numeric predictor variables, Numeric Predictor Variables\nneighbors, K-Nearest Neighbors\nnetwork data structures, Nonrectangular Data Structures\n",
      "content_length": 1003,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 523,
      "content": "nodes, Tree Models\nnon-normal residuals, Testing the Assumptions: Regression Diagnostics\nnonlinear regression, Polynomial and Spline Regression-Further Reading\ndefinition of, Polynomial and Spline Regression\nnonrectangular data structures, Nonrectangular Data Structures\nnormal distribution, Normal Distribution-Standard Normal and QQ-Plots\nkey concepts, Standard Normal and QQ-Plots\nstandard normal and QQ-Plots, Standard Normal and QQ-Plots\nnormalization, Standard Normal and QQ-Plots, Standardization\n(Normalization, Z-Scores), K-Means Clustering\ncategorical variables before clustering, Scaling the Variables\ndata distribution and, Standardization (Normalization, Z-Scores)\nin statistics vs. database context, Standardization (Normalization, Z-\nScores)\nnull hypothesis, Hypothesis Tests, The Null Hypothesis\nnumeric variables\ngrouped according to a categorical variable, Categorical and Numeric Data\nnumeric predictor variables for naive Bayes, Numeric Predictor Variables\nnumerical data as categorical data, Exploring Binary and Categorical Data\nO\nobject representation (spatial data), Nonrectangular Data Structures\n",
      "content_length": 1122,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 524,
      "content": "Occam's razor, Model Selection and Stepwise Regression\nodds, Logistic Regression, Logistic Response Function and Logit\nodds ratios, Interpreting the Coefficients and Odds Ratios\nlog-odds ratio and, Interpreting the Coefficients and Odds Ratios\nomnibus tests, ANOVA\none hot encoder, Factor Variables in Regression, One Hot Encoder\none hot encoding, Dummy Variables Representation\none-way tests, Hypothesis Tests, One-Way, Two-Way Hypothesis Test\norder statistics, Estimates of Variability, Estimates Based on Percentiles\nordered factor variables, Ordered Factor Variables\nordinal data, Elements of Structured Data\nimportance of the concept, Elements of Structured Data\nordinary least squares (OLS), Least Squares, Heteroskedasticity, Non-\nNormality and Correlated Errors\n(see also least squares)\nout-of-bag (OOB) estimate of error, Random Forest\noutcome, Rectangular Data\noutliers, Estimates of Location, Outliers, Testing the Assumptions:\nRegression Diagnostics\nin regression, Outliers\nsensitivity of correlation coefficient to, Correlation\n",
      "content_length": 1041,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 525,
      "content": "sensitivity of least squares to, Least Squares\nvariance, standard deviation, mean absolute deviation and, Standard\nDeviation and Related Estimates\noverfitting, Multiple Testing\navoiding in boosting using regularization, Regularization: Avoiding\nOverfitting\nin linear regression, Model Selection and Stepwise Regression\noversampling, Strategies for Imbalanced Data, Oversampling and Up/Down\nWeighting\nP\np-values, Statistical Significance and P-Values, P-Value\nadjusting, Multiple Testing\ndata science and, Data Science and P-Values\nt-statistic and, Assessing the Model\nvalue of, Value of the p-value\npairwise comparisons, ANOVA\npartial residual plots, Testing the Assumptions: Regression Diagnostics,\nPartial Residual Plots and Nonlinearity\nin logistic regression, Assessing the Model\nPCA (see principal components analysis)\nPearson residuals, Chi-Square Test: A Resampling Approach\n",
      "content_length": 882,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 526,
      "content": "Pearson's chi-squared test, Chi-Squared Test: Statistical Theory\nPearson's correlation coefficient, Correlation\nPearson, Karl, Chi-Square Test, Principal Components Analysis\npenalized regression, Model Selection and Stepwise Regression\npercentiles, Estimates of Variability\nand boxplots, Percentiles and Boxplots\nestimates based on, Estimates Based on Percentiles\nprecise definition of, Estimates Based on Percentiles\npermission, obtaining for human subject testing, Why Just A/B? Why Not C,\nD…?\npermutation tests, Resampling\nexhaustive and bootstrap, Exhaustive and Bootstrap Permutation Test\nfor ANOVA, ANOVA\nvalue for data science, Permutation Tests: The Bottom Line for Data\nScience\nweb stickiness example, Example: Web Stickiness\npertinent records (in searches), Size versus Quality: When Does Size\nMatter?\nphysical networks, Nonrectangular Data Structures\npie charts, Exploring Binary and Categorical Data\n",
      "content_length": 912,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 527,
      "content": "pivot tables (Excel), Two Categorical Variables\npoint estimates, Confidence Intervals\nPoisson distributions, Poisson and Related Distributions, Generalized Linear\nModels\ncalculating, Poisson Distributions\npolynomial coding, Dummy Variables Representation\npolynomial regression, Polynomial and Spline Regression, Polynomial\npopulation, Random Sampling and Sample Bias\nsample mean vs. population mean, Sample Mean versus Population Mean\nposterior probability, Naive Bayes, The Naive Solution\npower and sample size, Power and Sample Size-Further Reading\nprecision, Evaluating Classification Models\nin classification models, Precision, Recall, and Specificity\npredicted values, Fitted Values and Residuals\n(see also fitted values)\nprediction\nexplanation vs., in linear regression, Prediction versus Explanation\n(Profiling)\nharnessing results from multiple trees, How Trees Are Used\nK-Nearest Neighbors, K-Nearest Neighbors\nusing as first stage, KNN as a Feature Engine\n",
      "content_length": 965,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 528,
      "content": "predicted values from logistic regression, Predicted Values from Logistic\nRegression\nunsupervised learning and, Unsupervised Learning\nusing regression, Prediction Using Regression-Factor Variables in\nRegression\nconfidence and prediction intervals, Confidence and Prediction Intervals\ndangers of extrapolation, The Dangers of Extrapolation\nprediction intervals, Prediction Using Regression\nconfidence intervals vs., Confidence and Prediction Intervals\npredictor variables, Data Frames and Indexes, The Regression Equation\n(see also independent variables)\ncorrelated, Correlated Predictors\nin linear discriminant analysis, more than two, A Simple Example\nin naive Bayes algorithm, Naive Bayes\nmain effects, Interactions and Main Effects\nnumeric, applying naive Bayes to, Numeric Predictor Variables\nrelationship between response and, Partial Residual Plots and Nonlinearity\nprincipal components, Principal Components Analysis\nprincipal components analysis, Principal Components Analysis-Further\nReading\ncluster analysis vs., Interpreting the Clusters\n",
      "content_length": 1049,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 529,
      "content": "computing the principal components, Computing the Principal Components\ninterpreting principal components, Interpreting Principal Components\nscaling the variables, Scaling the Variables\nsimple example, A Simple Example-A Simple Example\nstandardizing data, Standardization (Normalization, Z-Scores)\nprobability theory, Exploratory Data Analysis\nprofiling vs. explanation, Prediction versus Explanation (Profiling)\npropensity score, Classification\nproxy variables, Example: Web Stickiness\npruning, Tree Models, Stopping the Tree from Growing\npseudo-residuals, The Boosting Algorithm\nQ\nQQ-Plots, Normal Distribution\nexample, returns for Netflix, Long-Tailed Distributions\nstandard normal and, Standard Normal and QQ-Plots\nquadratic discriminant analysis, A Simple Example\nquantiles, Estimates Based on Percentiles\nR function, quantile, Estimates Based on Percentiles\nR\nR-squared, Multiple Linear Regression, Assessing the Model\n",
      "content_length": 924,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 530,
      "content": "random forests, Interactions and Main Effects, Tree Models, Random Forest-\nHyperparameters\nbetter predictive performance than single trees, How Trees Are Used\ndetermining variable importance, Variable Importance\nhyperparameters, Hyperparameters\nrandom sampling, Random Sampling and Sample Bias-Further Reading\nbias, Bias\nkey terms for, Random Sampling and Sample Bias\nrandom selection, Random Selection\nsample mean vs. population mean, Sample Mean versus Population Mean\nsize versus quality, Size versus Quality: When Does Size Matter?\nrandom subset of variables, Random Forest\nrandomization, A/B Testing\nrandomization tests, Resampling\n(see also permutation tests)\nrandomness, misinterpreting, Hypothesis Tests\nrange, Estimates of Variability, Estimates Based on Percentiles\nrare class problem, The Rare Class Problem\nrecall, Evaluating Classification Models, Precision, Recall, and Specificity\nreceiver operating characteristics (see ROC curve)\n",
      "content_length": 947,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 531,
      "content": "records, Rectangular Data, Simple Linear Regression\nrectangular data, Rectangular Data-Estimates of Location\nterminology differences, Data Frames and Indexes\nrecursive partitioning, Tree Models, The Recursive Partitioning Algorithm,\nRandom Forest\nreference coding, Factor Variables in Regression-Dummy Variables\nRepresentation, Interactions and Main Effects, Logistic Regression and the\nGLM\nregression, Regression and Prediction-Summary\ncausation and, Prediction versus Explanation (Profiling)\ndiagnostics, Testing the Assumptions: Regression Diagnostics-Polynomial\nand Spline Regression\nheteroskedasticity, non-normality, and correlated errors,\nHeteroskedasticity, Non-Normality and Correlated Errors\ninfluential values, Influential Values\noutliers, Outliers\nparial residual plots and nonlinearity, Partial Residual Plots and\nNonlinearity\nusing scatterplot smoothers, Heteroskedasticity, Non-Normality and\nCorrelated Errors\ndifferent meanings of the term, Least Squares\nfactor variables in, Factor Variables in Regression-Ordered Factor\nVariables\n",
      "content_length": 1048,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 532,
      "content": "ordered factor variables, Ordered Factor Variables\nwith many levels, Factor Variables with Many Levels\ninterpreting the regression equation, Interpreting the Regression Equation-\nInteractions and Main Effects\nconfounding variables, Confounding Variables\ncorrelated predictors, Correlated Predictors\ninteractions and main effects, Interactions and Main Effects\nmulticollinearity, Multicollinearity\nKNN (K-Nearest Neighbors), KNN as a Feature Engine\nlogistic regression, Logistic Regression-Further Reading\ncomparison to linear regression, Linear and Logistic Regression:\nSimilarities and Differences\nmultiple linear regression, Multiple Linear Regression-Weighted\nRegression\npolynomial and spline regression, Polynomial and Spline Regression-\nSummary\ngeneralized additive models, Generalized Additive Models\npolynomial regression, Polynomial\nsplines, Splines\nprediction with, Prediction Using Regression-Factor Variables in\nRegression\nconfidence and prediction intervals, Confidence and Prediction Intervals\n",
      "content_length": 1007,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 533,
      "content": "dangers of extrapolation, The Dangers of Extrapolation\nridge regression, Regularization: Avoiding Overfitting\nsimple linear regression, Simple Linear Regression-Further Reading\nfitted values and residuals, Fitted Values and Residuals\nleast squares, Least Squares\nprediction vs. explanation, Prediction versus Explanation (Profiling)\nregression equation, The Regression Equation\nunsupervised learning as building block, Unsupervised Learning\nwith a tree, Predicting a Continuous Value\nregression coefficient, Simple Linear Regression\nin cotton exposure and lung capacity example, The Regression Equation\nregression to the mean, Regression to the Mean\nregularization, Boosting\navoding overfitting with, Regularization: Avoiding Overfitting\nreplacement (in sampling), Random Sampling and Sample Bias\nbootstrap, The Bootstrap\nrepresentativeness, Random Sampling and Sample Bias\nresampling, The Bootstrap, Resampling-For Further Reading\nbootstrapping vs., Resampling versus Bootstrapping\npermutation tests, Permutation Test\n",
      "content_length": 1019,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 534,
      "content": "exhaustive and bootstrap tests, Exhaustive and Bootstrap Permutation\nTest\nvalue for data science, Permutation Tests: The Bottom Line for Data\nScience\nweb stickiness example, Example: Web Stickiness\nusing in chi-square test, Chi-Square Test: A Resampling Approach\nresidual standard error, Multiple Linear Regression, Assessing the Model\nresidual sum of squares, Least Squares\n(see also least squares)\nresiduals, Simple Linear Regression, Fitted Values and Residuals\ncomputing, Fitted Values and Residuals\ndistribution of, Heteroskedasticity, Non-Normality and Correlated Errors\nstandardized, Testing the Assumptions: Regression Diagnostics\nresponse, Simple Linear Regression, The Regression Equation\nrelationship between predictor variable and, Partial Residual Plots and\nNonlinearity\nridge regression, Model Selection and Stepwise Regression, Regularization:\nAvoiding Overfitting\nrobust, Estimates of Location\nrobust estimates of location\nexample, population and murder rate by state, Example: Location\nEstimates of Population and Murder Rates\n",
      "content_length": 1044,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 535,
      "content": "mean absolute deviation from the median, Standard Deviation and Related\nEstimates\nmedian, Median and Robust Estimates\noutliers and, Outliers\nROC curve, ROC Curve\nroot mean squared error (RMSE), Multiple Linear Regression, Assessing the\nModel, Predicting a Continuous Value\nRSE (see residual standard error)\nRSS (residual sum of squares), Least Squares\n(see also least squares)\nS\nsample bias, Random Sampling and Sample Bias, Random Sampling and\nSample Bias\nsample statistic, Sampling Distribution of a Statistic\nsamples\ndefinition of, Random Sampling and Sample Bias\nsample size, power and, Power and Sample Size-Further Reading\nterminology differences, Data Frames and Indexes\nsampling, Data and Sampling Distributions-Summary\nbinomial distribution, Binomial Distribution-Further Reading\nbootstrap, The Bootstrap-Further Reading\n",
      "content_length": 830,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 536,
      "content": "confidence intervals, Confidence Intervals-Further Reading\nlong-tail distributions, Long-Tailed Distributions-Further Reading\nnormal distribution, Normal Distribution-Standard Normal and QQ-Plots\noversampling imbalanced data, Oversampling and Up/Down Weighting\nPoisson and related distributions, Poisson and Related Distributions-\nSummary\nestimating failure rate, Estimating the Failure Rate\nexponential distribution, Exponential Distribution\nPoisson distribution, Poisson Distributions\nWeibull distribution, Weibull Distribution\npopulation versus sample, Data and Sampling Distributions\nrandom sampling and sample bias, Random Sampling and Sample Bias-\nFurther Reading\nsampling distribution of a statistic, Sampling Distribution of a Statistic-\nFurther Reading\nselection bias, Selection Bias-Further Reading\nStudent's t-distribution, Student’s t-Distribution-Further Reading\nThompson's sampling, Multi-Arm Bandit Algorithm\nundersampling imbalanced data, Undersampling\nwith and without replacement, Random Sampling and Sample Bias, The\nBootstrap, Resampling\n",
      "content_length": 1058,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 537,
      "content": "sampling distribution, Sampling Distribution of a Statistic-Further Reading\ncentral limit theorem, Central Limit Theorem\ndata distribution vs., Sampling Distribution of a Statistic\nstandard error, Standard Error\nscale parameter (Weibull distribution), Weibull Distribution\nscaling data and categorical variables, Scaling and Categorical Variables-\nSummary\ndominant variables, Dominant Variables\nGower's distance and categorical data, Categorical Data and Gower’s\nDistance\nproblems clustering mixed data, Problems with Clustering Mixed Data\nscaling the variables, Scaling the Variables\nscatterplot smoothers, Heteroskedasticity, Non-Normality and Correlated\nErrors\nscatterplots, Correlation\nexample, returns for ATT and Verizon, Scatterplots\nscientific fraud, detecting, Fisher’s Exact Test\nscreeplots, Principal Components Analysis, Interpreting Principal\nComponents\nfor PCA of top stocks, Dominant Variables\nsearches\nsearch queries on Google, Size versus Quality: When Does Size Matter?\n",
      "content_length": 988,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 538,
      "content": "vast search effect, Selection Bias\nselection bias, Selection Bias-Further Reading\nregression to the mean, Regression to the Mean\nself-selection sampling bias, Random Sampling and Sample Bias\nsensitivity, Evaluating Classification Models, Precision, Recall, and\nSpecificity\nshape parameter (Weibull distribution), Weibull Distribution\nsignal-to-noise ratio, Choosing K\nsignificance level, Power and Sample Size, Sample Size\nsignificance tests, Hypothesis Tests, Data Science and P-Values\n(see also hypothesis tests)\nsimple random sample, Random Sampling and Sample Bias\nsingle linkage, Measures of Dissimilarity\nskew, Long-Tailed Distributions\nskewness, Frequency Table and Histograms\nslope, Simple Linear Regression\n(see also regression coefficient)\nin regression equation, The Regression Equation\nSMOTE algorithm, Data Generation\nspatial data structures, Nonrectangular Data Structures\n",
      "content_length": 887,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 539,
      "content": "specificity, Evaluating Classification Models, Precision, Recall, and\nSpecificity\nspline regression, Polynomial and Spline Regression, Splines\nsplines, Splines\nsplit value, Tree Models\nsquare-root of n rule, Standard Error\nSS (sum of squares), ANOVA\nwithing cluster sum of squares, K-Means Clustering\nstandard deviation, Estimates of Variability\nand related estimates, Standard Deviation and Related Estimates\ncovariance matrix and, Covariance Matrix\nin statistical testing output, A/B Testing\nsensitivity to outliers, Standard Deviation and Related Estimates\nstandard error vs., Standard Error\nstandard error, Sampling Distribution of a Statistic\nformula for calculating, Standard Error\nstandard deviation vs., Standard Error\nstandard normal distribution, Normal Distribution, Standard Normal and QQ-\nPlots\nstandardization, Standard Normal and QQ-Plots, K-Nearest Neighbors, K-\nMeans Clustering\n",
      "content_length": 896,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 540,
      "content": "in K-Nearest Neighbors, Standardization (Normalization, Z-Scores)\nstandardized residuals, Testing the Assumptions: Regression Diagnostics\nexamining to detect outliers, Outliers\nstatistical experiments and significance testing, Statistical Experiments and\nSignificance Testing-Summary\nA/B testing, A/B Testing-For Further Reading\nchi-square test, Chi-Square Test-Further Reading\ndegrees of freedom, Degrees of Freedom-Further Reading\nhypothesis tests, Hypothesis Tests-Further Reading\nmulti-arm bandit algorithm, Multi-Arm Bandit Algorithm-Further Reading\nmultiple tests, Multiple Testing-Further Reading\npower and sample size, Power and Sample Size-Further Reading\nresampling, Resampling-Statistical Significance and P-Values\nstatistical significance and p-values, Statistical Significance and P-Values-\nFurther Reading\nalpha, Alpha\ndata science and p-values, Data Science and P-Values\np-values, P-Value\ntype 1 and type 2 errors, Type 1 and Type 2 Errors\nvalue of p-values, Value of the p-value\n",
      "content_length": 995,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 541,
      "content": "t-tests, t-Tests-Further Reading\nstatistical inference, classical inference pipeline, Statistical Experiments and\nSignificance Testing\nstatistical machine learning, Statistical Machine Learning-Summary\nbagging and the random forest, Bagging and the Random Forest-\nHyperparameters\nboosting, Boosting-Summary\navoiding overfitting using regularization, Regularization: Avoiding\nOverfitting\nhyperparameters and cross-validation, Hyperparameters and Cross-\nValidation\nXGBoost, XGBoost\nK-Nearest Neighbors, K-Nearest Neighbors-KNN as a Feature Engine\nas a feature engine, KNN as a Feature Engine\nchoosing K, Choosing K\ndistance metrics, Distance Metrics\nexample, predicting loan default, A Small Example: Predicting Loan\nDefault\none hot encoder, One Hot Encoder\nstandardization, Standardization (Normalization, Z-Scores)\ntree models, Tree Models-Further Reading\nmeasuring homogeneity or impurity, Measuring Homogeneity or\n",
      "content_length": 916,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 542,
      "content": "Impurity\npredicting a continuous value, Predicting a Continuous Value\nrecursive partitioning algorithm, The Recursive Partitioning Algorithm\nsimple example, A Simple Example\nstopping tree growth, Stopping the Tree from Growing\nuses of trees, How Trees Are Used\nstatistical moments, Frequency Table and Histograms\nstatistical significance, Permutation Test\nstatistics vs. machine learning, Statistical Machine Learning\nstepwise regression, Model Selection and Stepwise Regression\nstochastic gradient boosting, The Boosting Algorithm\ndefinition of, Boosting\nXGBoost implementation, XGBoost-Hyperparameters and Cross-\nValidation\nstratified sampling, Random Sampling and Sample Bias, Random Selection\nstructured data, Elements of Structured Data-Further Reading\nStudent's t-distribution, Student’s t-Distribution-Further Reading\nsubjects, A/B Testing\nsuccess, Binomial Distribution\n",
      "content_length": 878,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 543,
      "content": "sum contrasts, Dummy Variables Representation\nT\nt-distributions, Student’s t-Distribution-Further Reading, t-Tests\ndata science and, Student’s t-Distribution\nt-statistic, t-Tests, Multiple Linear Regression, Assessing the Model\nt-tests, t-Tests-Further Reading\ntail, Long-Tailed Distributions\ntarget shuffling, Selection Bias\ntest sample, Evaluating Classification Models\ntest statistic, A/B Testing, t-Tests\nselecting before the experiment, Why Have a Control Group?\nThompson sampling, Multi-Arm Bandit Algorithm\ntime series data, Nonrectangular Data Structures\ntime-to-failure analysis, Weibull Distribution\ntreatment, A/B Testing\ntreatment group, A/B Testing\ntree models, Interactions and Main Effects, Exploring the Predictions, Tree\nModels\nhow trees are used, How Trees Are Used\nmeasuring homogeneity or impurity, Measuring Homogeneity or Impurity\n",
      "content_length": 853,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 544,
      "content": "predicting a continuous value, Predicting a Continuous Value\nrecursive partitioning algorithm, The Recursive Partitioning Algorithm\nsimple example, A Simple Example\nstopping tree growth, Stopping the Tree from Growing\nTrellis graphics, Visualizing Multiple Variables\ntrials, Binomial Distribution\ntrimmed mean, Estimates of Location\nformula for, Mean\nTukey, John Wilder, Exploratory Data Analysis\ntwo-way tests, Hypothesis Tests, One-Way, Two-Way Hypothesis Test\ntype 1 errors, Statistical Significance and P-Values, Type 1 and Type 2\nErrors, Multiple Testing\ntype 2 errors, Statistical Significance and P-Values, Type 1 and Type 2 Errors\nU\nunbiased estimates, Standard Deviation and Related Estimates\nundersampling, Undersampling\nuniform random distribution, Fisher’s Exact Test\nunivariate analysis, Exploring Two or More Variables\nunsupervised learning, Unsupervised Learning-Summary\nand prediction, Unsupervised Learning\n",
      "content_length": 924,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 545,
      "content": "hierarchical clustering, Hierarchical Clustering-Measures of Dissimilarity\nagglomerative algorithm, The Agglomerative Algorithm\ndendrogram, The Dendrogram\ndissimilarity measures, Measures of Dissimilarity\nsimple example, A Simple Example\nK-means clustering, K-Means Clustering-Selecting the Number of Clusters\ninterpreting the clusters, Interpreting the Clusters\nK-means algorithm, K-Means Algorithm\nselecting the number of customers, Selecting the Number of Clusters\nsimple example, A Simple Example-K-Means Algorithm\nmodel-based clustering, Model-Based Clustering-Further Reading\nmixtures of normals, Mixtures of Normals\nmultivariate normal distribution, Multivariate Normal Distribution\nselecting the number of clusters, Selecting the Number of Clusters\nprincipal components analysis, Principal Components Analysis-Further\nReading\ncomputing the principal components, Computing the Principal\nComponents\ninterpreting principal components, Interpreting Principal Components\nsimple example, A Simple Example-A Simple Example\nscaling and categorical variables, Scaling and Categorical Variables-\n",
      "content_length": 1094,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 546,
      "content": "Summary\ndominant variables, Dominant Variables\nGower's distance and categorical data, Categorical Data and Gower’s\nDistance\nproblems clustering mixed data, Problems with Clustering Mixed Data\nscaling the variables, Scaling the Variables\nup weight or down weight, Strategies for Imbalanced Data, Oversampling and\nUp/Down Weighting\nuplift vs. lift, Lift\nV\nvalidation sample, Evaluating Classification Models\nvariability\nvariability, estimates of, Estimates of Variability-Further Reading\nexample, murder rate by state population, Example: Variability Estimates\nof State Population\nkey terminology, Estimates of Variability\npercentiles, Estimates Based on Percentiles\nstandard deviation and related estimates, Standard Deviation and Related\nEstimates\nvariables\nexploring two or more, Exploring Two or More Variables-Summary\ncategorical and numeric data, Categorical and Numeric Data\n",
      "content_length": 880,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 547,
      "content": "hexagonal binning and contours, Hexagonal Binning and Contours\n(Plotting Numeric versus Numeric Data)\nkey concepts, Visualizing Multiple Variables\nvisualizing multiple variables, Visualizing Multiple Variables\nimportance of, determining in random forests, Variable Importance\nrescaling with z-scores, Standardization (Normalization, Z-Scores)\nvariance, Estimates of Variability\nanalysis of (ANOVA), ANOVA\nformula for calculating, Standard Deviation and Related Estimates\nsensitivity to outliers, Standard Deviation and Related Estimates\nvast search effect, Selection Bias\nviolin plots, Exploring Two or More Variables\ncombining with a boxplot, example, Categorical and Numeric Data\nW\nWard's method, Measures of Dissimilarity\nweb stickiness example (permutation test), Example: Web Stickiness\nweb testing\nbandit algorithms in, Multi-Arm Bandit Algorithm\ndeciding how long a test should run, Power and Sample Size\nWeibull distribution, Poisson and Related Distributions\n",
      "content_length": 968,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 548,
      "content": "calculating, Weibull Distribution\nweighted mean, Estimates of Location\nexpected value, Expected Value\nweighted median, Estimates of Location, Median and Robust Estimates\nformula for calculating, Mean\nweighted regression, Multiple Linear Regression, Weighted Regression\nweights, Simple Linear Regression\ncomponent loadings, A Simple Example\nwhiskers (in boxplots), Percentiles and Boxplots\nwins, Multi-Arm Bandit Algorithm\nwithin cluster sum of squares (SS), K-Means Clustering\nX\nXGBoost, XGBoost-Hyperparameters and Cross-Validation\nhyperparameters, Hyperparameters and Cross-Validation\nZ\nz-distribution, Standard Normal and QQ-Plots\n(see also normal distribution)\nz-score, Normal Distribution, Strategies for Imbalanced Data, K-Nearest\nNeighbors, Standardization (Normalization, Z-Scores)\nconverting data to, Standard Normal and QQ-Plots\nrescaling variables, Standardization (Normalization, Z-Scores)\n",
      "content_length": 902,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 549,
      "content": "About the Authors\nPeter Bruce founded and grew the Institute for Statistics Education at\nStatistics.com, which now offers about 100 courses in statistics, roughly a third of\nwhich are aimed at the data scientist. In recruiting top authors as instructors and\nforging a marketing strategy to reach professional data scientists, Peter has\ndeveloped both a broad view of the target market and his own expertise to reach\nit.\nAndrew Bruce has over 30 years of experience in statistics and data science in\nacademia, government, and business. He has a PhD in statistics from the\nUniversity of Washington and has published numerous papers in refereed journals.\nHe has developed statistical-based solutions to a wide range of problems faced\nby a variety of industries, from established financial firms to internet startups, and\noffers a deep understanding of the practice of data science.\n",
      "content_length": 879,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 550,
      "content": "Colophon\nThe animal on the cover of Practical Statistics for Data Scientists is a lined\nshore crab (Pachygrapsus crassipes), also known as a striped shore crab. It is\nfound along the coasts and beaches of the Pacific Ocean in North America,\nCentral America, Korea, and Japan. These crustaceans live under rocks, in\ntidepools, and within crevices. They spend about half their time on land, and\nperiodically return to the water to wet their gills.\nThe lined shore crab is named for the green stripes on its brown-black carapace.\nIt has red claws and purple legs, which also have a striped or mottled pattern. The\ncrab generally grows to be 3–5 centimeters in size; females are slightly smaller.\nTheir eyes are on flexible stalks that can rotate to give them a full field of vision\nas they walk.\nCrabs are omnivores, feeding primarily on algae, but also mollusks, worms, fungi,\ndead animals, and other crustaceans (depending on what is available). They moult\nmany times as they grow to adulthood, taking in water to expand and crack open\ntheir old shell. Once this is achieved, they spend several difficult hours getting\nfree, and then must hide until the new shell hardens.\nMany of the animals on O’Reilly covers are endangered; all of them are important\nto the world. To learn more about how you can help, go to animals.oreilly.com.\nThe cover image is from Pictorial Museum of Animated Nature. The cover fonts\nare URW Typewriter and Guardian Sans. The text font is Adobe Minion Pro; the\nheading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s\nUbuntu Mono.\n",
      "content_length": 1574,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 551,
      "content": "Preface\nWhat to Expect\nConventions Used in This Book\nUsing Code Examples\nSafari® Books Online\nHow to Contact Us\nAcknowledgments\n1. Exploratory Data Analysis\nElements of Structured Data\nFurther Reading\nRectangular Data\nData Frames and Indexes\nNonrectangular Data Structures\nFurther Reading\nEstimates of Location\nMean\nMedian and Robust Estimates\nExample: Location Estimates of Population and Murder Rates\nFurther Reading\nEstimates of Variability\nStandard Deviation and Related Estimates\nEstimates Based on Percentiles\n",
      "content_length": 516,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 552,
      "content": "Example: Variability Estimates of State Population\nFurther Reading\nExploring the Data Distribution\nPercentiles and Boxplots\nFrequency Table and Histograms\nDensity Estimates\nFurther Reading\nExploring Binary and Categorical Data\nMode\nExpected Value\nFurther Reading\nCorrelation\nScatterplots\nFurther Reading\nExploring Two or More Variables\nHexagonal Binning and Contours (Plotting Numeric versus\nNumeric Data)\nTwo Categorical Variables\nCategorical and Numeric Data\nVisualizing Multiple Variables\nFurther Reading\nSummary\n",
      "content_length": 516,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 553,
      "content": "2. Data and Sampling Distributions\nRandom Sampling and Sample Bias\nBias\nRandom Selection\nSize versus Quality: When Does Size Matter?\nSample Mean versus Population Mean\nFurther Reading\nSelection Bias\nRegression to the Mean\nFurther Reading\nSampling Distribution of a Statistic\nCentral Limit Theorem\nStandard Error\nFurther Reading\nThe Bootstrap\nResampling versus Bootstrapping\nFurther Reading\nConfidence Intervals\nFurther Reading\nNormal Distribution\nStandard Normal and QQ-Plots\nLong-Tailed Distributions\nFurther Reading\n",
      "content_length": 518,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 554,
      "content": "Student’s t-Distribution\nFurther Reading\nBinomial Distribution\nFurther Reading\nPoisson and Related Distributions\nPoisson Distributions\nExponential Distribution\nEstimating the Failure Rate\nWeibull Distribution\nFurther Reading\nSummary\n3. Statistical Experiments and Significance Testing\nA/B Testing\nWhy Have a Control Group?\nWhy Just A/B? Why Not C, D…?\nFor Further Reading\nHypothesis Tests\nThe Null Hypothesis\nAlternative Hypothesis\nOne-Way, Two-Way Hypothesis Test\nFurther Reading\nResampling\nPermutation Test\n",
      "content_length": 509,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 555,
      "content": "Example: Web Stickiness\nExhaustive and Bootstrap Permutation Test\nPermutation Tests: The Bottom Line for Data Science\nFor Further Reading\nStatistical Significance and P-Values\nP-Value\nAlpha\nType 1 and Type 2 Errors\nData Science and P-Values\nFurther Reading\nt-Tests\nFurther Reading\nMultiple Testing\nFurther Reading\nDegrees of Freedom\nFurther Reading\nANOVA\nF-Statistic\nTwo-Way ANOVA\nFurther Reading\nChi-Square Test\nChi-Square Test: A Resampling Approach\nChi-Squared Test: Statistical Theory\n",
      "content_length": 489,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 556,
      "content": "Fisher’s Exact Test\nRelevance for Data Science\nFurther Reading\nMulti-Arm Bandit Algorithm\nFurther Reading\nPower and Sample Size\nSample Size\nFurther Reading\nSummary\n4. Regression and Prediction\nSimple Linear Regression\nThe Regression Equation\nFitted Values and Residuals\nLeast Squares\nPrediction versus Explanation (Profiling)\nFurther Reading\nMultiple Linear Regression\nExample: King County Housing Data\nAssessing the Model\nCross-Validation\nModel Selection and Stepwise Regression\nWeighted Regression\n",
      "content_length": 500,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 557,
      "content": "Prediction Using Regression\nThe Dangers of Extrapolation\nConfidence and Prediction Intervals\nFactor Variables in Regression\nDummy Variables Representation\nFactor Variables with Many Levels\nOrdered Factor Variables\nInterpreting the Regression Equation\nCorrelated Predictors\nMulticollinearity\nConfounding Variables\nInteractions and Main Effects\nTesting the Assumptions: Regression Diagnostics\nOutliers\nInfluential Values\nHeteroskedasticity, Non-Normality and Correlated Errors\nPartial Residual Plots and Nonlinearity\nPolynomial and Spline Regression\nPolynomial\nSplines\nGeneralized Additive Models\nFurther Reading\n",
      "content_length": 611,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 558,
      "content": "Summary\n5. Classification\nNaive Bayes\nWhy Exact Bayesian Classification Is Impractical\nThe Naive Solution\nNumeric Predictor Variables\nFurther Reading\nDiscriminant Analysis\nCovariance Matrix\nFisher’s Linear Discriminant\nA Simple Example\nFurther Reading\nLogistic Regression\nLogistic Response Function and Logit\nLogistic Regression and the GLM\nGeneralized Linear Models\nPredicted Values from Logistic Regression\nInterpreting the Coefficients and Odds Ratios\nLinear and Logistic Regression: Similarities and Differences\nAssessing the Model\nFurther Reading\nEvaluating Classification Models\n",
      "content_length": 585,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 559,
      "content": "Confusion Matrix\nThe Rare Class Problem\nPrecision, Recall, and Specificity\nROC Curve\nAUC\nLift\nFurther Reading\nStrategies for Imbalanced Data\nUndersampling\nOversampling and Up/Down Weighting\nData Generation\nCost-Based Classification\nExploring the Predictions\nFurther Reading\nSummary\n6. Statistical Machine Learning\nK-Nearest Neighbors\nA Small Example: Predicting Loan Default\nDistance Metrics\nOne Hot Encoder\nStandardization (Normalization, Z-Scores)\nChoosing K\n",
      "content_length": 461,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 560,
      "content": "KNN as a Feature Engine\nTree Models\nA Simple Example\nThe Recursive Partitioning Algorithm\nMeasuring Homogeneity or Impurity\nStopping the Tree from Growing\nPredicting a Continuous Value\nHow Trees Are Used\nFurther Reading\nBagging and the Random Forest\nBagging\nRandom Forest\nVariable Importance\nHyperparameters\nBoosting\nThe Boosting Algorithm\nXGBoost\nRegularization: Avoiding Overfitting\nHyperparameters and Cross-Validation\nSummary\n7. Unsupervised Learning\nPrincipal Components Analysis\n",
      "content_length": 485,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 561,
      "content": "A Simple Example\nComputing the Principal Components\nInterpreting Principal Components\nFurther Reading\nK-Means Clustering\nA Simple Example\nK-Means Algorithm\nInterpreting the Clusters\nSelecting the Number of Clusters\nHierarchical Clustering\nA Simple Example\nThe Dendrogram\nThe Agglomerative Algorithm\nMeasures of Dissimilarity\nModel-Based Clustering\nMultivariate Normal Distribution\nMixtures of Normals\nSelecting the Number of Clusters\nFurther Reading\nScaling and Categorical Variables\nScaling the Variables\nDominant Variables\n",
      "content_length": 525,
      "extraction_method": "PyMuPDF_fallback"
    },
    {
      "page_number": 562,
      "content": "Categorical Data and Gower’s Distance\nProblems with Clustering Mixed Data\nSummary\nBibliography\nIndex\n",
      "content_length": 101,
      "extraction_method": "PyMuPDF_fallback"
    }
  ]
}